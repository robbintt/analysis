---
ver: rpa2
title: 'Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A Multi-Dialectal
  Bavarian Case Study'
arxiv_id: '2501.03863'
source_url: https://arxiv.org/abs/2501.03863
tags:
- bavarian
- data
- tasks
- intent
- slot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of slot and intent detection
  (SID) in dialectal languages, specifically focusing on Bavarian dialects. The authors
  explore zero-shot transfer learning using auxiliary tasks, including syntactic dependencies,
  named entity recognition (NER), and masked language modeling (MLM), to improve SID
  performance.
---

# Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A Multi-Dialectal Bavarian Case Study

## Quick Facts
- **arXiv ID:** 2501.03863
- **Source URL:** https://arxiv.org/abs/2501.03863
- **Reference count:** 34
- **Primary result:** Best approach improves intent classification by 5.1% and slot filling F1 by 8.4% on Bavarian dialects using intermediate-task training with MLM+NER

## Executive Summary
This study tackles slot and intent detection (SID) in Bavarian dialects through zero-shot cross-lingual transfer from English training data. The authors evaluate multi-task learning and intermediate-task training with four auxiliary tasks: syntactic dependencies, named entity recognition (NER), part-of-speech tagging, and masked language modeling. They find that NER provides the strongest auxiliary benefit for slot filling, while intermediate-task training yields more consistent performance gains than joint multi-task learning. The best configuration—intermediate-task training with MLM and NER—improves intent classification by 5.1% and slot filling F1 by 8.4% over baseline.

## Method Summary
The paper uses mDeBERTa-v3-base as the base encoder and MaChAmp 0.4.2 framework for training. They compare joint multi-task learning (MTL) where all tasks are trained simultaneously with equal loss weighting, against intermediate-task training (ITT) where auxiliary tasks are trained first, then the model is fine-tuned on the target SID task. The auxiliary tasks include Bavarian syntactic dependencies (MaiBaam), Bavarian NER (BarNER 1.0), and Bavarian Wikipedia for MLM. The SID task uses xSID-0.5 with English training data and Bavarian test sets (de-muc, de-ba, de-st).

## Key Results
- NER auxiliary task improves slot filling F1 by 7-8 percentage points in ITT configuration
- Intermediate-task training outperforms joint multi-task learning consistently
- MLM-only training degrades performance, but MLM+NER combination works best
- All auxiliary tasks improve slot filling more than intent classification
- 4-task joint MTL configuration degrades all metrics

## Why This Works (Mechanism)

### Mechanism 1: NER-Slot Task Similarity
- NER and slot filling share similar task structures—both are span-labeling sequence tagging problems, enabling effective knowledge transfer
- Break condition: If NER entities bear no relation to slot types (e.g., pure numeric IDs vs. semantic categories), benefits may diminish

### Mechanism 2: Sequential Training Preserves Target-Task Optimization
- ITT allows SID-specific model weights to be optimized in a final phase without gradient interference from auxiliary tasks
- Break condition: When auxiliary task dataset vastly exceeds target task data, joint MTL may outperform ITT

### Mechanism 3: Token-Level Task Alignment
- Token-level auxiliary tasks produce representations tuned for token-level predictions, which transfer better to slot filling than to intent classification
- Break condition: If sentence-level auxiliary tasks were available, intent classification might see greater relative improvement

## Foundational Learning

- **Concept: Slot and Intent Detection (SID)**
  - Why needed: Understanding user commands requires both intent classification (sentence-level) and slot filling (token-level)
  - Quick check: For "Book a flight to Paris tomorrow," which part is the intent and which are slots?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - Why needed: Trains on English SID data and evaluates on Bavarian with no Bavarian SID training data
  - Quick check: What does "zero-shot" mean in this context?

- **Concept: Multi-Task Learning vs. Intermediate-Task Training**
  - Why needed: Understanding trade-offs between simultaneous vs. sequential training of auxiliary and target tasks
  - Quick check: In joint MTL, are auxiliary and target tasks trained simultaneously or sequentially? In ITT, which task is trained last?

## Architecture Onboarding

- **Component map:** mDeBERTa-v3-base encoder → seq head for slot filling/NER/POS → classification head for intent → mlm head for MLM → dependency head for parsing

- **Critical path:**
  1. Load mDeBERTa pretrained weights
  2. For ITT: Train on Bavarian auxiliary task(s), discard task head
  3. Fine-tune on English SID training data (44k sentences)
  4. Evaluate zero-shot on Bavarian test sets (de-muc, de-ba, de-st)

- **Design tradeoffs:**
  - ITT vs. MTL: ITT gives more consistent gains; MTL can fail catastrophically for intent classification
  - Single vs. multiple auxiliary tasks: 4-task joint MTL degraded all metrics
  - PLM selection: mDeBERTa outperformed mBERT, XLM-R, and GBERT on Bavarian baselines

- **Failure signatures:**
  - MLM-only ITT: Harmed both tasks (-1.8 pp intent, -5.8 pp slots)
  - Joint MTL with UD: Large intent classification drops (up to -24.6 pp)
  - Multi-task joint training with all four tasks: Consistent degradation

- **First 3 experiments:**
  1. Baseline: Fine-tune mDeBERTa on English SID only; evaluate on Bavarian test sets
  2. Single auxiliary ITT: Train NER→SID; measure slot filling F1 delta vs. baseline (+7-8 pp expected)
  3. Combined ITT: Train MLM×NER→SID; verify intent accuracy (+5.1 pp) and slot F1 (+8.4 pp) improvements

## Open Questions the Paper Calls Out

- **Question 1:** To what extent would sentence-level auxiliary tasks improve intent classification performance compared to the token-level tasks evaluated?
  - Basis: Authors state they "could not conduct any experiments with sentence-level auxiliary tasks" due to lack of data
  - Resolution: A comparison of performance when integrating available sentence-level classification tasks (e.g., sentiment or topic classification)

- **Question 2:** Do the benefits of intermediate-task training with NER generalize across different pre-trained language model (PLM) architectures?
  - Basis: Authors note they "only carried out the (non-baseline) experiments with a single PLM [mDeBERTa]"
  - Resolution: Replicating the experimental setup using alternative encoders like XLM-R or GBERT

- **Question 3:** Is the negative impact of standalone Masked Language Modeling (MLM) auxiliary tasks caused by data scarcity or a mismatch with the pre-training objective?
  - Basis: Authors hypothesize that the MLM dataset may be "too small" and note that mDeBERTa is pre-trained on Replaced Token Detection (RTD), not MLM
  - Resolution: Experiments varying the size of the MLM dataset or substituting MLM with RTD

## Limitations

- Results are based on a single language pair (English → Bavarian) and a single pre-trained model architecture (mDeBERTa-v3-base)
- Task interactions in multi-task learning are "difficult to predict" with no systematic framework for predicting when auxiliary tasks will help or harm
- The mechanism explanation for MLM-only degradation remains speculative without systematic ablation studies

## Confidence

- **High Confidence**: NER provides strongest auxiliary benefit for slot filling (8.4% F1 improvement)
- **Medium Confidence**: ITT outperforms MTL, but results show task-dependent variability and conditions matter
- **Low Confidence**: Mechanism explanation for MLM-only degradation remains speculative

## Next Checks

1. **Cross-Lingual Generalization**: Replicate auxiliary task experiments on a different language pair (e.g., English → Swiss German or English → Arabic dialects) to test whether NER consistently provides the strongest benefit across language families

2. **Task Interaction Mapping**: Systematically test all combinations of the four auxiliary tasks (MLM, NER, POS, UD) in both joint MTL and sequential ITT configurations to map the space of task interactions

3. **Data Scaling Analysis**: Vary the size of auxiliary task datasets relative to the target SID dataset (e.g., 1:1, 5:1, 10:1 ratios) to empirically validate the paper's own citation that MTL can outperform ITT when auxiliary data is sufficiently larger