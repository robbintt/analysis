---
ver: rpa2
title: Q-Learning with Fine-Grained Gap-Dependent Regret
arxiv_id: '2510.06647'
source_url: https://arxiv.org/abs/2510.06647
tags:
- regret
- algorithm
- learning
- equation
- bounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of establishing fine-grained
  gap-dependent regret bounds for model-free reinforcement learning in episodic tabular
  Markov Decision Processes. Existing methods achieve minimax worst-case regret but
  their gap-dependent bounds are coarse, failing to capture the structure of suboptimality
  gaps.
---

# Q-Learning with Fine-Grained Gap-Dependent Regret

## Quick Facts
- arXiv ID: 2510.06647
- Source URL: https://arxiv.org/abs/2510.06647
- Reference count: 40
- This paper develops the first fine-grained gap-dependent regret bounds for model-free RL algorithms, achieving near-optimal dependence on suboptimality gaps.

## Executive Summary
This paper addresses the challenge of establishing fine-grained gap-dependent regret bounds for model-free reinforcement learning in episodic tabular Markov Decision Processes. Existing methods achieve minimax worst-case regret but their gap-dependent bounds are coarse, failing to capture the structure of suboptimality gaps. The authors develop a novel analytical framework that separates the analysis of optimal and suboptimal state-action pairs, enabling the first fine-grained regret bound for UCB-Hoeffding and a new algorithm ULCB-Hoeffding. They also revisit AMB, identifying issues with improper truncation and violation of martingale conditions, then propose a refined version with rigorous analysis. The resulting algorithm achieves the first fine-grained gap-dependent regret bound for a non-UCB-based method. Empirical results show improved performance over existing methods, with regret curves flattening as the number of episodes increases, indicating logarithmic growth consistent with theoretical guarantees.

## Method Summary
The paper develops three algorithms with fine-grained gap-dependent regret: UCB-Hoeffding (Algorithm 1), ULCB-Hoeffding (Algorithm 2), and Refined AMB (Algorithms 4-5). All algorithms use optimistic Q-estimates with Hoeffding-style bonuses to maintain exploration. The key innovation is a novel analytical framework that separates the analysis of optimal versus suboptimal state-action pairs, exploiting the empirical asymmetry that suboptimal pairs are visited only O(log T) times while optimal pairs may be visited frequently. For Refined AMB, the authors fix truncation placement by moving it from Q-updates to V-updates, and correct the concentration argument by centering estimators on conditional expectations. The algorithms operate in episodic tabular MDPs with horizon H, S states, A actions, and K episodes.

## Key Results
- Achieves first fine-grained regret bound for UCB-Hoeffding: O(∑_h ∑_∆_h(s,a)>0 H^5/∆_h(s,a) + H^5S|A|/∆_min)
- Introduces ULCB-Hoeffding with candidate action elimination, improving empirical performance
- Fixes AMB algorithm by relocating truncation and correcting martingale conditions, achieving O(∑_h ∑_∆_h(s,a)>0 H^6/∆_h(s,a) + H^6|S|opt|/∆_min)
- Empirical results show regret curves flattening (logarithmic growth) for all refined algorithms vs non-flattening curves for original AMB

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating analysis of optimal versus suboptimal state-action pairs yields tighter regret bounds than uniform treatment.
- **Mechanism:** The framework exploits an empirical asymmetry—suboptimal pairs are visited only O(log T) times while optimal pairs may be visited frequently. By conditioning the cumulative weighted estimation error analysis on gap structure, the bound aggregates ∑_∆_h(s,a)>0 1/∆_h(s,a) instead of the coarse SA/∆_min.
- **Core assumption:** The MDP has strictly positive suboptimality gaps for at least some (s,a,h) triples; visitation imbalance emerges under optimistic policies.
- **Evidence anchors:**
  - [abstract] "develop a novel analytical framework that explicitly separates the analysis of optimal and suboptimal state-action pairs"
  - [section 4.1, p.7] "ignoring this imbalance leads to loose bounds and an overly conservative dependence on 1/∆_min"
  - [corpus] "Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs" similarly uses per-gap aggregation, confirming the pattern across gap-dependent analyses.
- **Break condition:** When all gaps ∆_h(s,a) = 0 (fully deterministic optimal paths), the fine-grained term collapses; worst-case √T bounds dominate.

### Mechanism 2
- **Claim:** Moving truncation from Q-updates to V-updates preserves the recursive structure required for optimism/pessimism proofs.
- **Mechanism:** In AMB, clipping Q-values at [0, H] during multi-step bootstrapping breaks the equality linking Q-estimates to historical V-estimates. By instead clipping only the V-functions during aggregation, the update maintains Q^k+1_h = η_0^N H + ∑_i=1^N η_i^N (Q^ki,d_h + V^ki_h + b_i) as an exact unrolling, enabling valid concentration.
- **Core assumption:** The learning rate schedule η_t = (H+1)/(H+t) satisfies Lemma A.2 properties, ensuring the weighted sum remains well-behaved.
- **Evidence anchors:**
  - [section 5, p.9] "improper truncation in the Q-updates... breaks the key link between the Q-estimates and historical V-estimates"
  - [appendix B.2, p.21] "we slightly modify the update rules by shifting the truncation from the Q-estimates to the corresponding V-estimates"
  - [corpus] No direct corpus corroboration for this specific truncation relocation; mechanism is paper-specific.
- **Break condition:** If truncation is re-introduced on Q-values, the inductive proof of optimism (Theorem B.1) fails; bounds become invalid.

### Mechanism 3
- **Claim:** Centering multi-step bootstrapping estimators on conditional expectations (not unconditional expectations) restores martingale difference conditions for Azuma–Hoeffding.
- **Mechanism:** Multi-step bootstrapping induces estimators Q^ki,d_h(s,a) and V^k_h'(s_h'). Their joint concentration requires treating Q^ki,d_h - Q^ki,d_h and V*_h' - Q^ki,ud_h as martingale differences given filtration F_h,k. AMB incorrectly centered on unconditional expectations, violating this condition.
- **Core assumption:** The decomposition Q^ki,d_h(s,a) + Q^ki,ud_h(s,a) = Q*_h(s,a) holds (proven via induction in Theorem B.1).
- **Evidence anchors:**
  - [section 5, p.10] "violation of the martingale difference condition in its concentration argument"
  - [appendix B.1, p.20] "centering the estimators... around their expectations rather than their conditional expectations"
  - [corpus] "Provably Efficient and Agile Randomized Q-Learning" mentions martingale-based analysis for model-free settings, but no direct overlap on multi-step bootstrapping specifics.
- **Break condition:** If the filtration does not include policy and trajectory up to step h, the martingale property is lost; concentration inequalities cannot be applied.

## Foundational Learning

- **Concept: Suboptimality Gap ∆_h(s,a) = V*_h(s) - Q*_h(s,a)**
  - **Why needed here:** The entire fine-grained analysis conditions on ∆_h(s,a) values; without understanding gaps, the improvement from coarse 1/∆_min to per-gap sums is opaque.
  - **Quick check question:** For a state with 3 actions having Q-values [5, 5, 2], what are the gaps?

- **Concept: UCB Exploration via Optimistic Bonuses**
  - **Why needed here:** UCB-Hoeffding and ULCB-Hoeffding both use Hoeffding-style bonuses b_t ∝ √(H^3ι/t) to maintain optimism; the regret bound relies on this.
  - **Quick check question:** Why does adding b_t to Q-updates encourage exploration of uncertain states?

- **Concept: Martingale Difference Sequences and Azuma–Hoeffding**
  - **Why needed here:** The concentration proof for both UCB-based and AMB-style algorithms hinges on bounding martingale differences; misunderstanding here leads to the AMB errors.
  - **Quick check question:** Given X_n = ∑_i=1^n (Y_i - E[Y_i | F_i-1]), is {X_n} a martingale?

## Architecture Onboarding

- **Component map:**
  - Q-estimates (Q^k_h, Q^k_h): Upper/lower bounds on Q*_h; updated via Bellman or multi-step bootstrapping
  - V-estimates (V^k_h, V^k_h): Aggregations over actions; clipped to [0, H]
  - Bonus term b_n = 2√(H^3 log(2SAT/p) / n): Controls exploration-exploitation; halved in Refined AMB vs original AMB
  - Candidate action set A^k_h(s): Eliminates provably suboptimal actions; shrinks over time

- **Critical path:**
  1. Receive initial state s^k_1; for each step h, select a^k_h = argmax_a∈A^k_h(s) (Q^k_h - Q^k_h)(s, a)
  2. Observe transition; increment visit count N^k+1_h(s,a)
  3. Update Q^k+1_h, Q^k+1_h via (multi-step) Bellman update with bonus
  4. Update A^k+1_h(s) by eliminating actions with Q^k+1_h(s,a) < V^k+1_h(s)

- **Design tradeoffs:**
  - UCB-Hoeffding vs ULCB-Hoeffding vs Refined AMB: UCB is simplest; ULCB adds lower bounds and action elimination for tighter empirical performance; Refined AMB uses multi-step bootstrapping for potentially better gap dependence but adds complexity in proving unbiasedness
  - Bonus coefficient: Halving (from 4 to 2 in √· term) improves empirical regret but requires correct joint concentration analysis

- **Failure signatures:**
  - Regret does not flatten in log-scale plots: Indicates incorrect bonus scaling, bug in visit count updates, or failure of optimism (truncation in wrong place)
  - Candidate set A^k_h(s) never shrinks: Suggests Q or Q bounds are too loose; check bonus magnitude
  - Negative Q-values or Q > H after updates: Truncation bug; verify clipping is on V, not Q

- **First 3 experiments:**
  1. Small MDP validation: (H,S,A,K) = (2,3,3,10^5); plot Regret(T)/log(K+1); expect flattening for all algorithms except original AMB
  2. Ablation on bonus coefficient: Run ULCB-Hoeffding with c ∈ {0.5, 1.0, 2.0} in b_t = c√(H^3ι/t); confirm c=1 matches theory, larger c increases regret, smaller c risks optimism failure
  3. Gap structure sensitivity: Construct two MDPs—one with uniform gaps ∆_h(s,a) = ∆_min, one with heterogeneous gaps; compare coarse vs fine-grained term dominance in regret

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the polynomial dependence on the horizon H in the fine-grained regret bounds be reduced from H^5/H^6 to match lower bounds more closely?
- **Basis in paper:** [explicit] The authors state their bounds match lower bounds "up to polynomial factors in H" and note the lower bound shows ˜Ω(∑_h ∑_(s,a) 1/∆_h(s,a) + S/∆_min) without such H dependence.
- **Why unresolved:** The analysis framework introduces polynomial H factors that may be artifacts of the proof technique rather than fundamental algorithmic limitations.
- **What evidence would resolve it:** Either a lower bound showing H^5/H^6 dependence is necessary for model-free methods, or an improved analysis/algorithm reducing H to lower-order terms.

### Open Question 2
- **Question:** Can fine-grained gap-dependent regret bounds be extended to model-free RL with linear or general function approximation beyond the tabular setting?
- **Basis in paper:** [inferred] The paper focuses exclusively on tabular MDPs, and related work mentions gap-dependent analysis exists for linear function approximation in other contexts.
- **Why unresolved:** The analysis relies heavily on state-action counting and concentration arguments specific to the tabular setting.
- **What evidence would resolve it:** An algorithm and analysis extending the fine-grained framework to linear MDPs or other function approximation settings with similar gap-dependent guarantees.

### Open Question 3
- **Question:** Is there a fundamental separation between model-free and model-based methods in achievable fine-grained regret, particularly regarding the |Z_opt|/∆_min term?
- **Basis in paper:** [inferred] The paper compares against model-based results achieving similar forms but does not establish whether model-free methods face inherent limitations.
- **Why unresolved:** Lower bounds specific to model-free methods with fine-grained gap dependence are not established.
- **What evidence would resolve it:** Either a model-free-specific lower bound matching the upper bounds, or an improved model-free algorithm matching model-based guarantees exactly.

## Limitations
- The analysis assumes strictly positive suboptimality gaps and does not extend to continuous or large state spaces
- Gap-dependent bounds degrade when many state-action pairs have near-zero gaps
- The framework does not establish whether the H^5/H^6 polynomial dependence is fundamental to model-free methods

## Confidence
- **High:** UCB-Hoeffding analysis and ULCB-Hoeffding algorithm correctness
- **Medium:** Refined AMB analysis due to multi-step bootstrapping complexity
- **Low:** Specific numerical constants in refined bounds from tight concentration arguments

## Next Checks
1. Reproduce the small-scale experiment (H=2, S=3, A=3, K=10^5) to verify regret curves flatten as predicted
2. Perform an ablation study on the bonus coefficient to confirm optimal value of c=1 for UCB-based methods
3. Test Refined AMB algorithm on an MDP with known heterogeneous gaps to verify gap-dependent terms dominate regret bound