---
ver: rpa2
title: 'GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative
  Language Model'
arxiv_id: '2512.20978'
source_url: https://arxiv.org/abs/2512.20978
tags:
- speech
- arxiv
- speaker
- target
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenTSE addresses target speaker extraction (TSE) by proposing a
  fully generative two-stage decoder-only language model. Stage-1 generates coarse
  semantic tokens, and Stage-2 generates fine acoustic tokens conditioned on those
  predictions.
---

# GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model

## Quick Facts
- **arXiv ID**: 2512.20978
- **Source URL**: https://arxiv.org/abs/2512.20978
- **Reference count**: 0
- **Primary result**: Fully generative two-stage LM-based TSE system surpassing prior LM-based models on Libri2Mix

## Executive Summary
GenTSE introduces a novel approach to target speaker extraction by leveraging a two-stage decoder-only generative language model. The first stage generates coarse semantic tokens, while the second stage refines these into fine acoustic tokens conditioned on the coarse predictions. Both stages utilize continuous SSL or codec embeddings. To mitigate exposure bias during training, GenTSE employs Frozen-LM Conditioning (FLC), and it further refines outputs using Direct Preference Optimization (DPO) to align with human perceptual preferences. The system demonstrates state-of-the-art performance on the Libri2Mix dataset, significantly outperforming previous LM-based TSE systems.

## Method Summary
GenTSE is a fully generative two-stage decoder-only language model designed for target speaker extraction. Stage-1 generates coarse semantic tokens, and Stage-2 generates fine acoustic tokens conditioned on those predictions. Both stages use continuous SSL or codec embeddings. To reduce exposure bias, GenTSE employs Frozen-LM Conditioning (FLC) during training. It also incorporates Direct Preference Optimization (DPO) to align outputs with human perceptual preferences. On Libri2Mix, GenTSE surpasses previous LM-based systems in speech quality, intelligibility, and speaker consistency.

## Key Results
- Achieved UTMOS of 4.135, NISQA of 3.399, and SECS of 0.927 on Libri2Mix
- Outperformed models like LLaSE-G1, Metis, and USEF-SepFormer
- Statistically significant improvements in most metrics

## Why This Works (Mechanism)
The coarse-to-fine generation pipeline allows GenTSE to first capture high-level semantic content before refining into detailed acoustic representations. FLC reduces exposure bias by conditioning on frozen language model embeddings during training, improving generalization. DPO fine-tunes the model to produce outputs that better match human perceptual preferences, enhancing both intelligibility and speaker consistency.

## Foundational Learning
- **Target Speaker Extraction (TSE)**: Isolating a specific speaker's voice from a mixture; needed to focus on one speaker's speech in multi-talker scenarios; quick check: can the model extract speech from a known speaker in a two-speaker mix?
- **Decoder-only Language Models**: Generate tokens sequentially without encoder input; needed for autoregressive token generation in both stages; quick check: does the model generate coherent sequences?
- **SSL/Codec Embeddings**: Continuous representations from self-supervised learning or codecs; needed as input features for the LM stages; quick check: are the embeddings stable and discriminative?
- **Exposure Bias**: Discrepancy between training (teacher-forced) and inference (autoregressive) modes; needed to address for robust generation; quick check: does FLC reduce this gap?
- **Direct Preference Optimization (DPO)**: Reinforcement learning from human feedback; needed to align outputs with perceptual quality; quick check: do DPO-tuned outputs score higher in human evaluations?

## Architecture Onboarding
- **Component Map**: Input SSL embeddings → Stage-1 Coarse Generator → Coarse tokens → Stage-2 Fine Generator → Fine acoustic tokens → Output speech
- **Critical Path**: SSL embeddings → Stage-1 → Stage-2 → Final output; each stage must generate coherent, conditioned tokens
- **Design Tradeoffs**: Two-stage generation adds complexity but improves refinement; FLC adds training overhead but reduces exposure bias; DPO improves perceptual quality but requires preference data
- **Failure Signatures**: Poor speaker consistency (low SECS) if enrollment fails; incoherent speech if Stage-1 fails; exposure bias if FLC is ineffective
- **First Experiments**: 1) Test coarse token generation quality; 2) Validate fine token conditioning on coarse outputs; 3) Measure impact of FLC on training stability

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on single Libri2Mix test set, limiting generalizability
- No external validation on in-the-wild or multi-speaker mixtures
- Robustness to enrollment noise not tested

## Confidence
- **Methodological novelty**: High
- **Quantitative superiority on Libri2Mix**: Medium
- **Robustness and perceptual alignment claims**: Low

## Next Checks
1. Test GenTSE on a second TSE benchmark (e.g., WHAM! or DNS) to assess cross-dataset generalization
2. Conduct ablation studies isolating the impact of SSL embedding choice (HuBERT vs Wav2Vec2) and FLC vs standard teacher forcing
3. Run robustness tests with enrollment utterances under varying SNR and reverberation to measure real-world speaker consistency