---
ver: rpa2
title: What Do Learned Models Measure?
arxiv_id: '2601.18278'
source_url: https://arxiv.org/abs/2601.18278
tags:
- measurement
- learned
- learning
- quantity
- predictive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem that learned models, when used
  as measurement instruments, can exhibit systematic disagreement in their outputs
  despite comparable predictive performance. This is problematic because such disagreement
  indicates that the models are measuring different quantities, rather than a single,
  well-defined quantity.
---

# What Do Learned Models Measure?

## Quick Facts
- arXiv ID: 2601.18278
- Source URL: https://arxiv.org/abs/2601.18278
- Authors: Indrė Žliobaitė
- Reference count: 9
- Primary result: Learned models can achieve comparable predictive performance while implementing systematically different measurement functions, violating measurement stability.

## Executive Summary
This paper addresses a fundamental problem in using learned models as measurement instruments: multiple risk-equivalent models can disagree systematically under distribution shift despite comparable predictive performance. The core insight is that standard evaluation criteria (generalization, calibration, robustness) assess single-model predictive behavior but do not constrain agreement across admissible measurement realizations. The authors introduce measurement stability as a relational property requiring invariance of the measured quantity across admissible realizations and contexts. A real-world case study using the UCI Air Quality dataset demonstrates that two linear models trained on different sensor subsets achieve similar performance but diverge systematically under temporal shift, revealing that standard evaluation is insufficient for validating measurement validity.

## Method Summary
The method defines measurement stability as a relational property requiring agreement across admissible realizations of the learning process. The approach involves: (1) defining the quantity of interest and admissible contexts, (2) training multiple admissible realizations (e.g., different feature subsets or retraining runs), (3) applying all realizations to identical observations across preserved contexts, and (4) measuring structured disagreement. The primary empirical demonstration uses the UCI Air Quality dataset with two linear models trained on different sensor subsets, evaluated under temporal distribution shift. The method reveals that prediction disagreement correlates with the measured temperature value, exposing systematic measurement instability not detected by standard evaluation criteria.

## Key Results
- Two linear models with comparable predictive performance can disagree systematically under temporal distribution shift
- Standard evaluation criteria (generalization, calibration, robustness) do not guarantee measurement stability
- Structured, state-dependent disagreement between models reveals underspecification of the measurement function
- Measurement stability requires relational evaluation across multiple admissible realizations, not single-model assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple learned models can achieve comparable predictive performance while implementing systematically different measurement functions.
- Mechanism: Learning objectives typically impose only risk-based constraints, admitting a set of risk-equivalent solutions. Inductive biases and training data choices—not physical specification—determine which mapping is selected as the measurement function.
- Core assumption: The training objective admits multiple risk-equivalent solutions that are treated as interchangeable by standard evaluation.
- Evidence anchors:
  - [abstract]: "allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria"
  - [section 3.2]: "Learning objectives typically impose only risk-based constraints, admitting a set of risk-equivalent solutions. Inductive and training data choices determine which mapping is selected as the measurement function."
  - [corpus]: Limited direct corpus evidence for this specific mechanism; underspecification literature addresses related concerns.
- Break condition: When the learning objective uniquely constrains the measurement function (rare in practice).

### Mechanism 2
- Claim: Distribution shift exposes systematic disagreement between learned measurement functions that appear equivalent under standard evaluation.
- Mechanism: Under temporal shift, sensor-to-quantity relationships change while the quantity interpretation remains fixed. Different admissible realizations respond differently to this change, revealing state-dependent disagreement that is not detected by performance metrics.
- Core assumption: The shift alters observation-to-quantity relationships without redefining what quantity is being measured.
- Evidence anchors:
  - [abstract]: "with distribution shift providing a concrete illustration of this failure"
  - [section 5.3]: "their outputs diverge in a structured, state-dependent manner as a function of the inferred temperature"
  - [corpus]: Weak corpus evidence; distribution shift literature focuses on performance degradation, not measurement disagreement.
- Break condition: When the shift fundamentally changes what is being measured (not an admissible context).

### Mechanism 3
- Claim: Generalization, calibration, and robustness evaluate single-model predictive behavior but do not constrain agreement across admissible measurement realizations.
- Mechanism: Standard criteria fix one model and assess its predictions; measurement stability fixes the quantity and asks whether different admissible realizations agree. These are orthogonal evaluative dimensions.
- Core assumption: Measurement stability is inherently relational—it cannot be assessed from a single trained model.
- Evidence anchors:
  - [abstract]: "We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability"
  - [section 4.3]: "Generalization does not restrict which predictor is selected among multiple risk-equivalent solutions"
  - [corpus: 68067]: "The Benchmarking Epistemology" discusses construct validity limitations in benchmark-based evaluation.
- Break condition: When only one admissible realization exists (stability becomes trivially satisfied or undefined).

## Foundational Learning

- Concept: Measurement vs. Prediction
  - Why needed here: The paper's core distinction separates accuracy about a fixed target from stable measurement of a latent quantity.
  - Quick check question: Can a model predict accurately yet fail to measure the intended quantity? Explain how.

- Concept: Admissible Realizations
  - Why needed here: Stability requires defining what variations (data splits, sensor subsets, retraining) are treated as legitimate alternatives for the same measurement goal.
  - Quick check question: Give an example of a model variation that is admissible vs. one that redefines the measurement goal.

- Concept: Relational Evaluation
  - Why needed here: Measurement stability is not a property of any single model—it requires comparing outputs across realizations and contexts.
  - Quick check question: Why can't you evaluate measurement stability by inspecting one trained model in isolation?

## Architecture Onboarding

- Component map:
  Observation space (O) → representation mapping (ϕ_m) → feature space (X_m) → measurement output via m(x, c)
  Multiple admissible realizations (m₁, m₂, …) each with their own observation-to-representation mapping
  Context space (C_z) over which quantity z is assumed invariant

- Critical path:
  1. Define quantity of interest z and admissible contexts C_z.
  2. Train multiple admissible realizations (e.g., different feature subsets, retraining runs).
  3. Apply all realizations to identical observations across preserved contexts.
  4. Measure structured disagreement (state-dependent, not zero-mean noise).
  5. If systematic divergence exists, measurement is underspecified.

- Design tradeoffs:
  - Sensor/feature diversity vs. stability: Different feature subsets can maintain predictive equivalence while diverging in measurement.
  - Single-model deployment vs. multi-realization auditing: Production favors one model; stability audits require several.
  - Accuracy vs. interpretability: High predictive accuracy does not guarantee stable, interpretable measurements.

- Failure signatures:
  - State-dependent disagreement: Prediction differences that correlate with the measured quantity value (Figure 1d).
  - Performance equivalence with measurement divergence: Similar MSE, calibration, and robustness curves but structured disagreement.
  - Shift reveals divergence: In-distribution agreement masks instability that appears under temporal or covariate shift.

- First 3 experiments:
  1. Replicate the air quality audit: Train two linear models on different sensor subsets, evaluate on a temporal split, plot disagreement vs. true temperature.
  2. Vary the realization type: Replace sensor-subset variation with random seed/retraining variation and assess whether similar instability appears.
  3. Stress-test under shift magnitude: Systematically increase distribution-shift severity and characterize how disagreement scales relative to predictive metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can statistical guarantees for measurement stability be operationalized and certified for a given learning setup?
- Basis in paper: [explicit] Section 7 states: "Operationalizing statistical guarantees for such setups is left as an open problem outside the scope of the present paper."
- Why unresolved: The paper defines measurement stability as a relational property across admissible realizations but does not provide methods for statistical certification or finite-sample guarantees.
- What evidence would resolve it: Formal statistical tests or bounds quantifying when stability can be certified with high probability given finite samples and a specified class of admissible realizations.

### Open Question 2
- Question: Can learning objectives be designed to explicitly incorporate measurement stability constraints alongside predictive performance?
- Basis in paper: [explicit] Section 7 lists as a future direction: "incorporate assumptions about admissible contexts and preserved quantities directly into learning."
- Why unresolved: Current learning objectives optimize predictive risk only, admitting multiple risk-equivalent solutions with unstable measurements; no framework exists for jointly optimizing stability and accuracy.
- What evidence would resolve it: A learning algorithm with provable stability guarantees under defined admissible variations, or empirical demonstrations that multi-objective training improves stability without catastrophic accuracy loss.

### Open Question 3
- Question: How can measurement instability attributable to causal non-identifiability be distinguished from instability due to epistemic underdetermination?
- Basis in paper: [explicit] Section 7 identifies these two distinct sources but notes "our current framework is agnostic to this distinction."
- Why unresolved: The paper's diagnostic framework detects instability but does not diagnose its cause; disentangling these sources requires different methodological tools.
- What evidence would resolve it: A diagnostic procedure that, given observed instability, determines whether additional data or alternative observational proxies could in principle resolve it (epistemic) versus whether no learning procedure can recover a unique measurement (causal).

### Open Question 4
- Question: What are principled methods for defining and validating the set of admissible realizations for a given measurement task?
- Basis in paper: [inferred] Section 4.2 notes that admissibility "is implicit and determined by modeling assumptions and data availability" and intentionally defined abstractly, leaving practical specification unresolved.
- Why unresolved: Without concrete guidance on what variations count as admissible, practitioners lack systematic criteria for stability evaluation design.
- What evidence would resolve it: Domain-specific frameworks or general principles for enumerating admissible realizations based on measurement semantics, with validation showing that specified sets capture meaningful stability requirements.

## Limitations

- Empirical demonstration relies on a single real-world case study, constraining generalizability
- Linear models used may not capture complexity of nonlinear measurement functions common in modern ML
- Definition of admissible realizations depends on researcher judgment, introducing potential subjectivity

## Confidence

- High confidence: The conceptual framework linking measurement stability to measurement validity is well-founded and logically coherent
- Medium confidence: The UCI Air Quality case study demonstrates the proposed phenomenon, but limited scope means results may not generalize
- Low confidence: Claims about how frequently measurement instability occurs in practice and its prevalence across ML applications lack empirical support

## Next Checks

1. Apply the measurement stability audit to 3-5 diverse datasets (e.g., medical imaging, financial forecasting, environmental monitoring) with both linear and nonlinear models to assess prevalence.

2. Systematically vary the definition of admissible realizations (different random seeds, feature subsets, model architectures) to determine which types of variation most strongly affect measurement stability.

3. Quantify the relationship between predictive performance and measurement stability across a controlled parameter sweep (e.g., regularization strength, model complexity) to understand if there are inherent tradeoffs.