---
ver: rpa2
title: Investigating Training and Generalization in Faithful Self-Explanations of
  Large Language Models
arxiv_id: '2512.07288'
source_url: https://arxiv.org/abs/2512.07288
tags:
- self-explanations
- training
- faithfulness
- style
- redacted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how training affects the faithfulness\
  \ of self-explanations generated by large language models (LLMs) and examines whether\
  \ these improvements generalize across different settings. The authors construct\
  \ pseudo-faithful self-explanations for three styles\u2014attribution, redaction,\
  \ and counterfactual\u2014using feature attribution methods under a one-word constraint."
---

# Investigating Training and Evaluation in Faithful Self-Explanations of Large Language Models

## Quick Facts
- arXiv ID: 2512.07288
- Source URL: https://arxiv.org/abs/2512.07288
- Reference count: 11
- This paper investigates how training affects the faithfulness of self-explanations generated by large language models (LLMs) and examines whether these improvements generalize across different settings.

## Executive Summary
This paper investigates how training affects the faithfulness of self-explanations generated by large language models (LLMs) and examines whether these improvements generalize across different settings. The authors construct pseudo-faithful self-explanations for three styles—attribution, redaction, and counterfactual—using feature attribution methods under a one-word constraint. They then train LLMs using these constructed explanations in a continual learning setup, mixing them with original instruction-tuning data to prevent catastrophic forgetting. Experiments with Tulu-2 models on three classification tasks show that training significantly improves faithfulness across all styles, with generalization observed to unconstrained multi-word settings and unseen tasks, particularly in the attribution style. The study also finds consistent cross-style generalization, indicating that training benefits extend beyond the specific style used during training.

## Method Summary
The authors construct pseudo-faithful self-explanations using feature attribution methods with a one-word constraint across three styles: attribution, redaction, and counterfactual. These explanations are then used to train LLMs in a continual learning setup, where they are mixed with original instruction-tuning data to prevent catastrophic forgetting. The study employs Tulu-2 models and evaluates faithfulness using automated metrics like F1 score on three classification tasks. The experiments assess both in-domain performance and generalization to unconstrained multi-word settings and unseen tasks, with particular attention to cross-style generalization effects.

## Key Results
- Training significantly improves faithfulness across all explanation styles (attribution, redaction, counterfactual)
- Generalization observed to unconstrained multi-word settings and unseen tasks, especially for attribution style
- Cross-style generalization demonstrates that training benefits extend beyond the specific style used during training

## Why This Works (Mechanism)
The paper does not provide a detailed mechanistic explanation for why training on pseudo-faithful explanations improves faithfulness. The authors demonstrate that the approach works empirically but do not deeply explore the underlying mechanisms that enable these improvements.

## Foundational Learning

**Feature Attribution Methods**: Used to construct pseudo-faithful explanations by identifying which input features contribute most to model predictions. Why needed: Provides a way to generate explanations that can be used for training. Quick check: Verify attribution methods correctly identify important features for predictions.

**Continual Learning**: The training approach mixes pseudo-faithful explanations with original instruction data to prevent catastrophic forgetting. Why needed: Allows models to learn new explanation capabilities while retaining original task performance. Quick check: Monitor performance on original tasks during training.

**Faithfulness Evaluation**: Automated metrics (F1 score) are used to assess whether explanations accurately reflect the model's reasoning process. Why needed: Provides quantitative measure of explanation quality. Quick check: Validate metrics correlate with human judgment of explanation quality.

## Architecture Onboarding

**Component Map**: Feature Attribution Methods -> Pseudo-Faithful Explanations -> LLM Training -> Faithfulness Evaluation

**Critical Path**: The key sequence is: generate pseudo-faithful explanations → mix with original data → train LLM → evaluate faithfulness. The success depends on the quality of pseudo-explanations and the effectiveness of the continual learning approach.

**Design Tradeoffs**: The one-word constraint simplifies explanation generation but may limit expressiveness. Mixing original and pseudo-data prevents forgetting but may dilute the training signal for explanations.

**Failure Signatures**: Poor faithfulness scores could indicate: (1) inadequate pseudo-explanation quality from attribution methods, (2) insufficient training data or epochs, (3) catastrophic interference from original data, or (4) evaluation metric limitations.

**First Experiments**: 
1. Test feature attribution methods on a small dataset to verify they identify meaningful features
2. Evaluate whether pseudo-explanations improve faithfulness when trained on a single style
3. Compare faithfulness scores between models trained with and without original data mixing

## Open Questions the Paper Calls Out
None

## Limitations
- The improvements in faithfulness may reflect optimization artifacts rather than genuine model understanding, as the paper relies on feature attribution methods that have inherent limitations
- Findings are based on classification tasks and specific explanation styles, which may not generalize to more complex tasks or other explanation types
- The mechanisms underlying generalization across styles and tasks are not fully explored

## Confidence
- Primary uncertainty: Medium - The extent to which improvements reflect genuine understanding versus optimization artifacts
- Generalization claims: Medium - Limited to classification tasks and specific model sizes
- Cross-style generalization mechanisms: Medium - Underlying reasons for generalization not fully explored

## Next Checks
1. Conduct ablation studies to isolate the contribution of different components of the training setup (e.g., the one-word constraint, the mix of original and pseudo-faithful data) to the observed improvements in faithfulness
2. Evaluate the faithfulness of self-explanations using human evaluation to complement automated metrics and provide a more nuanced assessment of explanation quality
3. Extend the experiments to a wider range of tasks (e.g., reasoning, generation) and model sizes to assess the generalizability of the findings