---
ver: rpa2
title: Better Process Supervision with Bi-directional Rewarding Signals
arxiv_id: '2503.04618'
source_url: https://arxiv.org/abs/2503.04618
tags:
- birm
- zhang
- step
- wang
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiRM, a novel process supervision model for
  large language models that addresses the limitation of existing one-directional
  process reward models. Inspired by the A algorithm, BiRM provides bidirectional
  rewarding signals by simultaneously evaluating the correctness of reasoning steps
  taken so far and estimating the probability of future success.
---

# Better Process Supervision with Bi-directional Rewarding Signals

## Quick Facts
- arXiv ID: 2503.04618
- Source URL: https://arxiv.org/abs/2503.04618
- Reference count: 40
- Primary result: 3.1% accuracy gain on Gaokao2023 over PRM under Best-of-N sampling

## Executive Summary
This paper introduces BiRM, a novel process supervision model for large language models that addresses the limitation of existing one-directional process reward models. Inspired by the A* algorithm, BiRM provides bidirectional rewarding signals by simultaneously evaluating the correctness of reasoning steps taken so far and estimating the probability of future success. The model incorporates both a reward head (for backward supervision) and a value head (for forward supervision) to provide more comprehensive guidance during test-time reasoning. Experiments on mathematical reasoning tasks demonstrate significant improvements over existing methods, with better performance maintenance as sample size increases compared to traditional PRMs.

## Method Summary
BiRM is a process supervision model that extends traditional process reward models by adding a value head to estimate future success probability. The model is trained on MetaMath-MATH dataset with 15 trajectories per query, using DeepSeek-V3 for step correctness annotation and Monte Carlo estimation for value labels. Training involves joint optimization of reward and value heads with MSE loss. At inference, BiRM scores candidates using f(st) = g(st) + β·h(st), where g(st) aggregates rewards over completed steps and h(st) estimates probability of reaching correct final answer. The model is evaluated using Best-of-N sampling and beam search on GSM8K, MATH-500, and Gaokao2023 datasets.

## Key Results
- 3.1% accuracy increase on Gaokao2023 over PRM under Best-of-N sampling
- 5.0% and 3.8% improvements over ORM and PRM respectively on MATH-500 using beam search
- Maintains consistent accuracy improvement as sample size N increases, while traditional PRMs plateau or decline
- Outperforms both PRMs and value models across all reasoning stages

## Why This Works (Mechanism)

### Mechanism 1: A*-inspired Bidirectional Scoring
Combining backward-looking correctness evaluation with forward-looking success probability yields more comprehensive supervision than either alone. BiRM computes f(st) = g(st) + β·h(st), where g(st) aggregates rewards over completed steps (backward supervision) and h(st) estimates probability of reaching correct final answer (forward supervision). This mirrors A* heuristic search where optimal path selection requires both incurred cost and estimated remaining cost.

### Mechanism 2: Stage-Appropriate Signal Dominance
PRMs excel at early-stage error detection while VMs excel at late-stage evaluation; BiRM provides consistent accuracy across all reasoning stages. The reward head directly classifies step correctness (effective when errors are local), while the value head implicitly learns to project partial trajectories to final outcomes (effective when near-completion).

### Mechanism 3: Scaling Decline Avoidance
BiRM maintains accuracy improvement as sample size N increases, while traditional PRMs/ORMs plateau or decline due to misranking by imperfect verifiers. Single-direction verifiers misclassify positive samples at higher rates as N grows, selecting incorrect solutions. BiRM's dual evaluation reduces false positives by requiring both backward correctness AND forward viability.

## Foundational Learning

- **Process Reward Models (PRMs)**
  - Why needed: BiRM extends PRMs by adding a value head; understanding vanilla PRM's one-directional limitation motivates the architecture.
  - Quick check: Given a partial solution, can a standard PRM estimate whether it will reach the correct final answer?

- **Monte Carlo Value Estimation**
  - Why needed: BiRM's value labels require estimating success probability from partial trajectories; MC rollouts provide ground truth for training.
  - Quick check: If you sample 8 completions from an intermediate step and 4 reach correct answers, what is the soft-label value for that step?

- **A* Search Heuristic (f = g + h)**
  - Why needed: BiRM's scoring formula directly maps to A*; g(n) is accumulated cost (backward), h(n) is estimated remaining cost (forward).
  - Quick check: In A* terms, what does a high h(st) but low g(st) indicate about a partial solution?

## Architecture Onboarding

- Component map: Input (question q, partial trajectory τ[1:t]) -> Base LLM (shared parameters θ) -> [Reward Head (ϕR) → score r(st) "Is step st correct?", Value Head (ϕV) → score v(τ[1:t]) "Will this reach correct answer?"] -> Combined Score: f(st) = Agg(r(s1)...r(st)) + β·v(τ[1:t])

- Critical path: 1) Generate training data: Sample 15 solutions per query from SFT-ed generator (~225K total solutions) 2) Annotate reward labels: Use LLM for binary step correctness 3) Annotate value labels: Use MC estimation (8 rollouts per step) or outcome-supervised propagation 4) Train BiRM: Joint MSE loss L = L_PRM + c·L_VM (default c=1.0) 5) Inference: Score candidates using f(st) with tuned β (varies by dataset/model)

- Design tradeoffs: MC vs. outcome-supervised value labels (MC is more accurate but expensive vs. outcome-supervised is cheaper but noisier); Response scaling vs. query scaling (15K queries × 15 responses optimal); β tuning (per-dataset tuning required).

- Failure signatures: Scaling decline at high N (indicates verifier misranking; check if β is too low); OOD degradation (if trained on limited query diversity, value head may overfit); Beam search underperforming BoN (suggests local step-level decisions are misaligned with global trajectory quality).

- First 3 experiments: 1) Ablation on β: Sweep β ∈ {0.5, 1.0, 1.5, 2.0, 3.0} on held-out validation set; plot BoN accuracy vs. β to find task-optimal balance. 2) Label strategy comparison: Train three BiRM variants (outcome-supervised, MC-hard, MC-soft) with identical base model; compare on MATH-500 to quantify annotation quality impact. 3) Scaling curve analysis: Run BoN from N=1 to N=512 with BiRM vs. PRM on GSM8K; verify BiRM avoids scaling decline and identify crossover point where PRM begins degrading.

## Open Questions the Paper Calls Out

### Open Question 1
Can a BiRM supervision model be trained to be independent of specific data patterns and base models, such that it can effectively supervise generators with different architectures or reasoning styles than those used during its training? The current experimental setup ties the supervisor to the generator, leaving generalization capability untested.

### Open Question 2
Under what specific conditions do search-based strategies utilizing BiRM underperform compared to repeated sampling (Best-of-N) due to "verifier failures," and can this degradation be mathematically bounded? While BiRM improves search performance over PRMs, the fundamental issue where the search algorithm follows a verifier into a local optimum or error cascade is not fully solved.

### Open Question 3
What is the optimal trade-off between query scaling (number of distinct problems) and response scaling (number of solutions per problem) when constructing training data for BiRM, and is the finding of 15 responses per query universal? The specific result appears empirically derived for the specific dataset and model size used; it is unclear if this ratio holds for smaller/larger models or different reasoning domains.

## Limitations
- Architecture transparency lacking detailed specifications of head dimensions and initialization schemes
- Heavy dependence on computationally expensive Monte Carlo value estimation for accurate supervision
- Performance generalization to non-mathematical or non-verifiable tasks unexplored
- Dataset representation issues may contribute to performance gains rather than supervision quality alone

## Confidence

**High Confidence**: The core claim that combining backward and forward supervision improves process supervision accuracy is well-supported by ablation studies and comparative results across multiple datasets.

**Medium Confidence**: The mechanism explaining why PRMs perform better early and VMs perform better late is plausible but relies on visual analysis without quantitative stage-wise breakdowns.

**Low Confidence**: The assertion that BiRM maintains accuracy gains as sample size increases is limited to a few dataset/model combinations without broader scaling experiments.

## Next Checks

1. **β Sensitivity Analysis**: Conduct systematic sweep of β values on held-out validation sets for each dataset/model combination; plot Best-of-N accuracy against β to identify optimal points.

2. **Label Strategy Comparison**: Train BiRM variants using outcome-supervised, MC-hard, and MC-soft value labels on identical base models; compare performance on MATH-500 to quantify impact of label quality.

3. **Scaling Curve Validation**: Extend scaling analysis to N=1024 or higher for BiRM vs. PRM on GSM8K and MATH-500; confirm BiRM avoids scaling decline plateau and identify exact N where PRM begins degrading.