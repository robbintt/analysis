---
ver: rpa2
title: Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language
  Models
arxiv_id: '2504.03051'
source_url: https://arxiv.org/abs/2504.03051
tags:
- symptom
- symptoms
- taco
- coding
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate medical symptom
  coding from unstructured clinical text, focusing on extracting and linking symptoms
  to standardized vocabularies like MedDRA. The proposed Task as Context (TACO) prompting
  framework unifies symptom extraction and linking by embedding task-specific context
  into LLM prompts, improving contextual understanding and reducing information loss.
---

# Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models

## Quick Facts
- arXiv ID: 2504.03051
- Source URL: https://arxiv.org/abs/2504.03051
- Authors: Chengyang He; Wenlong Zhang; Violet Xinying Chen; Yue Ning; Ping Wang
- Reference count: 40
- One-line primary result: TACO prompting framework achieves high precision and recall scores for medical symptom coding, with GPT-4-Turbo reaching EM Precision of 0.999 and EM Recall of 0.998.

## Executive Summary
This paper introduces Task as Context (TACO) prompting, a unified framework for medical symptom extraction and linking using large language models. The approach embeds task-specific context into LLM prompts to simultaneously extract symptoms from clinical text and map them to standardized vocabularies like MedDRA. Experiments demonstrate that TACO significantly outperforms sequential approaches, particularly for larger models like GPT-4-Turbo, while highlighting performance variations across different model sizes.

## Method Summary
The TACO framework employs a unified prompt architecture that simultaneously instructs LLMs to extract clinical symptoms and map them to a provided list of standardized terms. The method uses the SYMPCODER dataset of VAERS reports with two-stage evaluation: LINK stage assesses code linking accuracy (EM/Fuzzy Match Precision/Recall), while MATCH stage evaluates extracted text fidelity (BLEU/Fuzzy/Cosine similarity). The study compares TACO against the sequential TASI baseline across multiple models including GPT-4 variants, Llama2, and Jackalope, using prompts with max_new_token=256 and temperature settings of 0.3-0.5.

## Key Results
- GPT-4-Turbo with TACO achieves EM Precision of 0.999 and EM Recall of 0.998
- TACO outperforms sequential TASI approach by reducing error propagation
- Performance varies significantly by model size, with smaller models (Jackalope-7b) showing better results with TASI
- TACO shows better consistency in recalling rare symptoms compared to common symptoms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unifying extraction and linking into a single prompt reduces the error propagation inherent in sequential pipelines.
- **Mechanism:** TACO forces the LLM to attend to the target vocabulary while reading the source text, allowing the model to filter noise and resolve ambiguities using the target codes as immediate context, rather than relying on intermediate outputs that may have dropped critical nuances.
- **Core assumption:** The model possesses sufficient working memory and attention capacity to process the clinical text and the vocabulary list simultaneously without degradation.
- **Evidence anchors:** [abstract] "TACO... unifies extraction and linking tasks by embedding task-specific context into LLM prompts... reducing information loss." [section 4.1.1] "TACO integrates the tasks within the prompt, enabling simultaneous learning and execution... minimiz[ing] error propagation."
- **Break condition:** If the "Suggested List" of symptoms exceeds the LLM's context window or attention resolution, the unification advantage degrades.

### Mechanism 2
- **Claim:** Providing the target vocabulary acts as a semantic guide, improving the recall of rare or ambiguous terms.
- **Mechanism:** By including the "Suggested List" in the prompt, the model is primed to map informal clinical descriptions to specific concepts, acting as an in-context retrieval mechanism that helps overcome frequency bias in pre-training data.
- **Core assumption:** The LLM has sufficient pre-existing medical knowledge to recognize semantic equivalence between raw text and provided terms.
- **Evidence anchors:** [section 5.2.3] "TACO prompting provides better consistency in recall... likely due to their sparse representation in training data." [section 4.1.2] The Prompt Header explicitly instructs the model to "Extract the terms... indicating each of the following terms."
- **Break condition:** If clinical text contains a symptom that is semantically distinct but orthographically similar to a "distractor" term, the model may be misled by context.

### Mechanism 3
- **Claim:** Performance gains are contingent on model capacity; smaller models may suffer from "contextual overload" in unified tasks.
- **Mechanism:** Smaller models (Jackalope-7b) sometimes performed better with sequential TASI approach than unified TACO, suggesting large models have capacity to manage complex dual-objective prompts while smaller models find them harder to optimize.
- **Core assumption:** Model parameter count and instruction-following fine-tuning correlate directly with ability to handle multi-constraint prompts.
- **Evidence anchors:** [section 5.2.1] "Jackalope-7b exhibits a unique trend, where TASI slightly outperforms TACO for precision... suggesting that smaller models... may benefit from the sequential nature." [table 2] Jackalope-7b shows EM-Precision of 0.874 (TASI) vs 0.763 (TACO).
- **Break condition:** As prompt complexity increases, performance of smaller models degrades non-linearly compared to large models.

## Foundational Learning

- **Concept: Entity Linking vs. Named Entity Recognition (NER)**
  - **Why needed here:** The paper distinguishes between extracting text (NER - "fever") and mapping it to a standardized ID (Entity Linking - "MedDRA Code 10016558"). TACO explicitly tries to solve the mapping problem, not just the extraction problem.
  - **Quick check question:** If a model extracts "arm hurt" from text, is that extraction or linking? If it maps "arm hurt" to "Pain in extremity," is that extraction or linking?

- **Concept: Prompt Engineering Architecture (Context vs. Sequential)**
  - **Why needed here:** The core contribution is the TACO prompt structure. Understanding how to construct prompts that embed the "solution space" (the list of valid codes) into the "problem space" (the text analysis) is the primary skill required to implement this.
  - **Quick check question:** In the TACO framework, where does the "Suggested List" of symptoms appear relative to the "Clinical Text" in the prompt flow?

- **Concept: Evaluation Metrics for Coding (Precision/Recall vs. Semantic Similarity)**
  - **Why needed here:** The paper uses a two-stage evaluation (LINK and MATCH). One checks for code accuracy (EM-Precision), the other for text fidelity (BLEU/Fuzzy). An engineer needs to know that getting the *code* right (LINK) is prioritized over getting the exact *mention* right (MATCH), though both are tracked.
  - **Quick check question:** If the text says "high temp" and the model outputs "Pyrexia" (correct code) but extracts the text "temperature" instead of "high temp", which evaluation stage (LINK or MATCH) penalizes this?

## Architecture Onboarding

- **Component map:** Input (VAERS Clinical Text + Suggested Symptom List) -> TACO Prompt Engine -> LLM Processor -> Distillation Layer -> Evaluation Suite (LINK Stage + MATCH Stage)
- **Critical path:** The Prompt Constructor is the single point of failure. If the "Suggested List" is not formatted exactly as the specific LLM expects, the model will hallucinate or fail to follow the linking instruction.
- **Design tradeoffs:** TACO (Unified) vs TASI (Sequential): TACO is faster and more accurate for large models (one inference step) but requires larger context windows. TASI is slower (two inference steps) but more robust for smaller models or extremely long vocabulary lists.
- **Failure signatures:**
  - Small Model Overload: Using TACO with Jackalope-7b resulted in *lower* precision than the baseline (0.763 vs 0.874)
  - Temporal Hallucination: Jackalope-7b adding irrelevant temporal noise ("on Wednesday", "on Friday") to symptoms
  - Rare Term Blindness: Models often return "none" for rare symptoms even when present in the text
- **First 3 experiments:**
  1. Baseline Validation: Run TASI vs. TACO on a sample of 50 reports using a mid-sized model to verify TACO improves EM-Precision
  2. Context Saturation Test: Incrementally increase "Suggested List" size (10, 50, 100 terms) to find breaking point where recall drops
  3. Rare vs. Common Split: Specifically evaluate performance on SYMPCODER-Rare-50 subset to confirm if model is actually linking rare symptoms or ignoring them

## Open Questions the Paper Calls Out
- Can the TACO prompting framework generalize to broader medical coding tasks and datasets beyond vaccine adverse event reporting?
- How can the TACO framework be refined to better handle the extraction of rare or infrequent symptoms?
- Why does the integrated context of TACO prompting degrade performance for smaller models (like Jackalope-7b) compared to sequential prompting?

## Limitations
- Performance depends heavily on proprietary LLMs (GPT-4 variants) that cannot be independently verified
- SYMPCODER dataset originates from a single source (VAERS) with specific reporting patterns that may not generalize to other clinical contexts
- The evaluation framework's "fuzzy match" algorithm remains underspecified, potentially affecting reproducibility

## Confidence
- **High Confidence:** The fundamental mechanism of TACO (unified prompt architecture) is well-supported by direct evidence showing reduced error propagation compared to sequential approaches
- **Medium Confidence:** Claims about rare symptom performance are supported by internal comparisons but depend heavily on the specific SYMPCODER-Rare-50 subset composition
- **Low Confidence:** The scalability assertions regarding context window limits are theoretical extrapolations without empirical breaking point establishment

## Next Checks
1. Cross-Vocabulary Validation: Apply TACO to a different medical coding system (e.g., SNOMED CT or ICD-10) using the same evaluation framework to test generalizability beyond MedDRA
2. Context Window Stress Test: Systematically increase the size of the "Suggested List" from 10 to 500 terms while measuring recall degradation to empirically establish the upper limit of TACO's effectiveness
3. Model Capacity Gradient: Test TACO with intermediate-sized models (7B-13B parameters) to precisely map the performance threshold where unified prompting becomes detrimental, identifying the optimal model size range for deployment