---
ver: rpa2
title: Rethinking Parameter Sharing as Graph Coloring for Structured Compression
arxiv_id: '2511.06786'
source_url: https://arxiv.org/abs/2511.06786
tags:
- sharing
- layer
- basis
- compression
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of parameter sharing in deep learning
  model compression, specifically extending sharing beyond adjacent layers to achieve
  better compression ratios. The core idea is to reformulate parameter sharing as
  introducing structural symmetries in the parameter space, modeled as a graph coloring
  problem where layers sharing the same basis are assigned the same color.
---

# Rethinking Parameter Sharing as Graph Coloring for Structured Compression

## Quick Facts
- **arXiv ID:** 2511.06786
- **Source URL:** https://arxiv.org/abs/2511.06786
- **Reference count:** 37
- **Primary result:** Geometric Hessian-based graph coloring for parameter sharing achieves 28% compression on Swin-L with no accuracy loss and 45% latency reduction

## Executive Summary
This paper addresses the problem of parameter sharing in deep learning model compression by reformulating it as a graph coloring problem that introduces structural symmetries. The core innovation is Geo-Sharing, which uses Hessian spectrum analysis to project perturbations onto low-curvature eigensubspaces, minimizing accuracy loss while enabling efficient cross-layer parameter sharing. The method consistently outperforms existing heuristics across diverse architectures, achieving significant compression ratios while maintaining or improving model performance and inference efficiency.

## Method Summary
Geo-Sharing is a training-free compression method that combines SVD-based low-rank decomposition with Hessian-guided basis selection. For each layer, it computes the Hessian's minor-axis eigenvectors using Lanczos iteration, then selects among candidate bases the one minimizing high-curvature projection error. The method applies a trust-region constraint to bound perturbations along flat directions, reconstructing compressed weights through selective projection and clipping. The entire process is formulated as graph coloring where layers sharing the same basis are assigned the same color, preserving structural symmetry.

## Key Results
- On LLaMA-7B: Lower perplexity and higher zero-shot reasoning scores at 20-50% compression ratios
- On Swin-L: Maintains nearly identical accuracy while achieving 28% compression
- Strong scalability: Effective on 13B and 30B parameter models with 70-80% compression
- Inference efficiency: ~45% latency reduction and up to 70% throughput improvement on real hardware

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-layer parameter sharing can be formalized as a graph coloring problem where layers sharing the same basis are assigned the same color, inducing structural symmetry.
- **Mechanism:** A coloring function α: L → C assigns each layer to a sharing class. Layers in the same class use identical basis matrices (U_b, V_b) with layer-specific coefficients S_ℓ,b, making them permutation-invariant under the automorphism group Aut(Ω_layer).
- **Core assumption:** Layers that can share parameters should be functionally interchangeable—swapping their order preserves model expressivity.
- **Evidence anchors:** [abstract] "A sharing configuration can be described by a coloring function α:L→C... which determines inter-layer sharing groups while preserving structural symmetry."

### Mechanism 2
- **Claim:** Aligning parameter-sharing perturbations with the Hessian's low-curvature eigensubspace minimizes accuracy loss under compression.
- **Mechanism:** The loss change from sharing ΔJ ≈ (1/2)δ^T H δ describes an ellipsoid where major axes (small eigenvalues) are insensitive directions. Projecting perturbations onto these flat directions confines error to regions where the loss surface is most tolerant.
- **Core assumption:** The quadratic Taylor approximation dominates; the first-order term is negligible near convergence (gradient ≈ 0).
- **Evidence anchors:** [abstract] "By projecting perturbations onto the Hessian's low-curvature eigensubspace, the criterion provides an analytic rule for selecting sharing groups that minimize performance impact."

### Mechanism 3
- **Claim:** A trust-region constraint on low-curvature perturbations prevents excessive deviation while allowing expressive flexibility.
- **Mechanism:** Perturbation δ is split into δ_⊥ (high-curvature, minimized) and δ_∥ (low-curvature, bounded by τ_ℓ = β‖W_ℓ‖_F). Clipping δ_∥ ensures controlled movement along flat directions without breaking the quadratic assumption.
- **Core assumption:** Moderate β (~10^-2 to 10^-1) allows beneficial alignment; excessive β injects noise into sensitive directions.
- **Evidence anchors:** [Figure 3(b)] Shows perplexity U-curve: optimal at β ≈ 10^-2, sharp degradation beyond 10^-1.

## Foundational Learning

- **Concept:** Hessian spectrum and eigen-decomposition
  - **Why needed:** The entire geometric criterion depends on interpreting the Hessian's eigenvalues as curvature directions. Small eigenvalues = flat directions = safe for perturbation.
  - **Quick check:** Given H = QΛQ^T, which eigenvector corresponds to the direction most sensitive to parameter changes?

- **Concept:** Low-rank matrix decomposition (SVD)
  - **Why needed:** Geo-Sharing constructs shared bases from SVD (W ≈ USV^T). Understanding rank-constrained approximation is essential for interpreting compression ratios.
  - **Quick check:** If a 4096×4096 weight matrix has only 64 significant singular values, what is the parameter reduction ratio from using truncated SVD?

- **Concept:** Group theory basics (permutations, automorphisms)
  - **Why needed:** The paper formalizes sharing via automorphism groups—permutations that preserve structure. This provides the theoretical justification for why certain layers can share.
  - **Quick check:** If layers {1,3,5} share basis A and {2,4} share basis B, what permutations preserve the sharing structure?

## Architecture Onboarding

- **Component map:** Pre-trained weights → SVD decomposition → Hessian estimation → Basis selection → Trust-region clipping → Compressed weights
- **Critical path:** 1) Hessian eigen-computation (most expensive: ~0.93h for LLaMA-7B, t=550) 2) Per-layer basis selection via high-curvature energy minimization (~0.4h) 3) Trust-region clipping and weight reconstruction
- **Design tradeoffs:**
  - **Minor-axis count (t):** Higher t → better curvature estimation → lower perplexity but ~linear time increase. Paper uses t=550.
  - **Amplitude factor (β):** Controls trust-region radius. Too small → underfitting; too large → instability. Sweet spot: 10^-2 to 10^-1.
  - **Number of bases (K):** Fewer bases → higher compression but more sharing pressure. Paper achieves 32 layers → 12 bases at 50% compression.
- **Failure signatures:**
  - **Perplexity explosion at high compression:** Check if β exceeded valid range or t too small for accurate curvature.
  - **OOM on large models:** Hessian computation memory; reduce t or use more efficient HVP approximations.
  - **Asymmetric degradation across layers:** May indicate incorrect basis assignment; verify coloring function outputs.
- **First 3 experiments:**
  1. **Sanity check on small model:** Apply Geo-Sharing to a 6-layer Transformer with t=100, β=0.05. Verify perplexity stays within 5% of original at 20% compression.
  2. **Ablation on minor-axis count:** Fix compression at 30%, vary t ∈ {100, 200, 400, 550}. Plot perplexity vs. computation time to find efficiency frontier.
  3. **Cross-architecture validation:** Test on both attention-heavy (LLaMA) and convolutional-hybrid (Swin) models to confirm geometric criterion generalizes across architectures.

## Open Questions the Paper Calls Out

- **Question:** Can the computational overhead of the Hessian-based alignment be reduced for models larger than 30B parameters?
  - **Basis:** Section 4.3 notes linear growth in computation time with minor-axis count.
  - **Why unresolved:** Linear scaling presents bottleneck for frontier models (70B+).
  - **Evidence needed:** Sub-linear scaling validation or successful application of randomized methods.

- **Question:** In what specific training scenarios does the negligible first-order gradient assumption fail?
  - **Basis:** Section 4.3 validates assumption empirically but acknowledges dependence on well-optimized models.
  - **Why unresolved:** Unclear if criterion holds for under-trained, heavily regularized, or small learning rate fine-tuning scenarios.
  - **Evidence needed:** Ablation study across training trajectories.

- **Question:** Is there a theoretical upper bound for the amplitude factor β that generalizes across architectures?
  - **Basis:** Section 4.3 observes optimal β empirically with degradation beyond threshold.
  - **Why unresolved:** Optimal β appears tuned; threshold likely depends on specific loss landscape curvature.
  - **Evidence needed:** Deriving dynamic, layer-wise bounds based on local spectral properties.

## Limitations

- **Computational bottleneck:** Hessian eigen-analysis introduces significant overhead, particularly for large models where full spectrum estimation becomes infeasible.
- **Assumption dependency:** Trust-region formulation assumes near-quadratic loss landscapes, which may not hold during early training or under extreme compression ratios.
- **Architecture bias:** Evaluation focuses primarily on decoder-only and vision transformers, leaving uncertainty about performance on encoder-only architectures or hybrid models.

## Confidence

**High Confidence:** The core claim that parameter sharing can be formalized as graph coloring with structural symmetry preservation is well-supported by both theoretical framing and empirical results across multiple model families.

**Medium Confidence:** The Hessian-based geometric criterion for basis selection shows consistent perplexity improvements, but the specific values of t=550 and β=0.05 appear tuned to the tested models.

**Low Confidence:** The scalability claims to 30B-parameter models and 80% compression ratios are based on limited testing and may not generalize to all model architectures.

## Next Checks

1. **Cross-architecture stress test:** Apply Geo-Sharing to encoder-only BERT variants and hybrid architectures (e.g., CLIP) to verify the geometric criterion generalizes beyond decoder-only and vision transformers.

2. **Optimization dynamics validation:** Measure gradient norms and loss curvature evolution during fine-tuning of compressed models to confirm the quadratic approximation remains valid under different training regimes and compression levels.

3. **Resource-constrained scalability test:** Implement memory-efficient Hessian approximation (e.g., randomized SVD on approximate Hessians) and benchmark performance degradation on 7B models using single GPU setups to establish practical limits.