---
ver: rpa2
title: 'Seewo''s Submission to MLC-SLM: Lessons learned from Speech Reasoning Language
  Models'
arxiv_id: '2506.13300'
source_url: https://arxiv.org/abs/2506.13300
tags:
- speech
- training
- speaker
- loss
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage training pipeline for multilingual
  conversational speech recognition that enhances reasoning and self-correction in
  speech language models. The approach combines curriculum learning, Chain-of-Thought
  data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR) to
  improve model performance.
---

# Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models

## Quick Facts
- arXiv ID: 2506.13300
- Source URL: https://arxiv.org/abs/2506.13300
- Authors: Bo Li; Chengben Xu; Wufeng Zhang
- Reference count: 0
- One-line primary result: Achieves 11.57% WER/CER for Track 1 (ASR) and 17.67% tcpWER/tcpCER for Track 2 (SD-ASR) on MLC-SLM evaluation set

## Executive Summary
This paper presents a multi-stage training pipeline for multilingual conversational speech recognition that enhances reasoning and self-correction capabilities in speech language models. The approach combines curriculum learning, Chain-of-Thought data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR) to improve model performance. The system demonstrates substantial improvements over official baselines on the MLC-SLM challenge, achieving 11.57% WER/CER for Track 1 and 17.67% tcpWER/tcpCER for Track 2 on the evaluation set.

## Method Summary
The proposed method employs a systematic multi-stage training approach that integrates curriculum learning, Chain-of-Thought data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR). The pipeline includes specialized token adaptation for instruction following, weighted loss functions for Chain-of-Thought training, and carefully designed reward functions for reinforcement learning. This comprehensive strategy aims to improve both recognition accuracy and reasoning capabilities in multilingual conversational speech recognition scenarios.

## Key Results
- Achieves 11.57% WER/CER for Track 1 (ASR) on MLC-SLM evaluation set
- Achieves 17.67% tcpWER/tcpCER for Track 2 (SD-ASR) on MLC-SLM evaluation set
- Demonstrates substantial improvements over official baselines through multi-stage training pipeline

## Why This Works (Mechanism)
The success of this approach stems from the systematic integration of reasoning capabilities and self-correction mechanisms into speech language models through curriculum-based learning progression. The Chain-of-Thought data augmentation enables the model to develop structured reasoning pathways, while RLVR provides feedback-driven optimization that reinforces correct reasoning patterns. The weighted loss functions ensure balanced learning across different stages of the training pipeline, preventing early-stage optimization from dominating the overall learning process.

## Foundational Learning
- Curriculum learning: Why needed - Gradually increases task complexity to prevent overwhelming the model; Quick check - Monitor performance progression across curriculum stages
- Chain-of-Thought reasoning: Why needed - Enables structured problem-solving approach for speech recognition; Quick check - Validate intermediate reasoning steps against ground truth
- Reinforcement Learning with Verifiable Rewards: Why needed - Provides feedback-driven optimization for reasoning patterns; Quick check - Test reward function robustness against edge cases
- Multilingual data handling: Why needed - Ensures model generalizes across different languages and dialects; Quick check - Evaluate performance across language-specific subsets
- Token adaptation for instruction following: Why needed - Improves model's ability to understand and execute spoken instructions; Quick check - Test instruction comprehension accuracy
- Weighted loss functions: Why needed - Balances learning across different training stages; Quick check - Analyze loss contribution from each stage during training

## Architecture Onboarding

Component Map:
Preprocessing -> Curriculum Learning -> Chain-of-Thought Augmentation -> RLVR Training -> Inference

Critical Path:
The critical path flows through the curriculum learning stage to Chain-of-Thought augmentation, as these stages establish the foundational reasoning capabilities that RLVR then optimizes. Any bottleneck in the curriculum learning or reasoning augmentation stages directly impacts the quality of rewards generated during RLVR training.

Design Tradeoffs:
The approach trades computational efficiency for improved reasoning capabilities, requiring multiple training stages and extensive data augmentation. This design prioritizes accuracy and reasoning quality over inference speed, making it suitable for applications where correctness is paramount but potentially limiting its use in real-time scenarios.

Failure Signatures:
Common failure modes include reward hacking in RLVR where the model learns to exploit reward function weaknesses rather than improving genuine reasoning capabilities. Additionally, curriculum progression that's too aggressive can lead to catastrophic forgetting of earlier-learned patterns, while overly conservative progression results in suboptimal utilization of the training pipeline.

First Experiments:
1. Baseline comparison: Run the model without Chain-of-Thought augmentation to isolate its contribution to performance improvements
2. Curriculum ablation: Test different curriculum progression rates to find optimal balance between learning efficiency and stability
3. Reward function sensitivity: Evaluate model performance under different reward function configurations to identify potential reward hacking vulnerabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Limited transparency regarding dataset sizes, training durations, and computational resources used
- Evaluation restricted to MLC-SLM challenge datasets, limiting generalizability to other multilingual conversational scenarios
- Potential biases in multilingual data not addressed, with no analysis of performance disparities across different languages

## Confidence
- High confidence in reported WER/CER and tcpWER/tcpCER improvements on MLC-SLM evaluation set
- Medium confidence in improvements from curriculum learning and Chain-of-Thought data augmentation
- Low confidence in generalization of self-correction capabilities to real-world scenarios

## Next Checks
1. Conduct ablation studies to quantify individual contributions of curriculum learning, Chain-of-Thought data augmentation, and RLVR
2. Evaluate model on additional multilingual conversational speech datasets outside MLC-SLM challenge
3. Perform bias analysis on multilingual data to identify and mitigate performance disparities across languages