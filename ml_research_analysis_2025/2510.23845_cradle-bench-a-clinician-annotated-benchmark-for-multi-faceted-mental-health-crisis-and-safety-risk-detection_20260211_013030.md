---
ver: rpa2
title: 'CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health
  Crisis and Safety Risk Detection'
arxiv_id: '2510.23845'
source_url: https://arxiv.org/abs/2510.23845
tags:
- crisis
- past
- ongoing
- labels
- abuse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CRADLE BENCH is a clinician-annotated benchmark for multi-faceted\
  \ mental health crisis detection on social media, covering seven clinically defined\
  \ crisis types\u2014including passive and active suicide ideation, self-harm, domestic\
  \ violence, rape, sexual harassment, and child abuse/endangerment\u2014with temporal\
  \ annotations (ongoing vs. past)."
---

# CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection

## Quick Facts
- arXiv ID: 2510.23845
- Source URL: https://arxiv.org/abs/2510.23845
- Reference count: 24
- Primary result: Expert-annotated multi-label crisis detection benchmark with ensemble-labeled training data, achieving best Exact Match of 0.8450 with majority voting across strong LLMs

## Executive Summary
CRADLE BENCH is a clinician-annotated benchmark for multi-faceted mental health crisis detection on social media, covering seven clinically defined crisis types—including passive and active suicide ideation, self-harm, domestic violence, rape, sexual harassment, and child abuse/endangerment—with temporal annotations (ongoing vs. past). Expert annotators, including psychologists and social workers, labeled 600 test instances and 420 development instances, producing a high-quality, multi-label, temporally-aware dataset. The benchmark was used to evaluate 15 state-of-the-art LLMs, revealing that closed-source models (e.g., Claude-4-Sonnet, GPT-5) significantly outperform open-source models, with Claude-4-Sonnet achieving the highest Exact Match score of 0.8217. An ensemble majority-vote method combining GPT-5, Claude-4-Sonnet, and Gemini-2.5-Pro achieved an Exact Match of 0.8450 and Jaccard of 0.8794. Using ensemble-labeled data, six fine-tuned crisis detection models were developed, with the best—Llama-3.3-70B trained on consensus labels—achieving an Exact Match of 0.7783, Jaccard of 0.8198, and Micro F1 of 0.8158, surpassing the best baseline model. Fine-tuning consistently improved detection performance, especially for smaller models trained on high-quality labels. The work highlights the value of expert annotation, temporal context, and ensemble labeling in building reliable crisis detection systems, and all data and models are publicly released.

## Method Summary
The benchmark uses 600 clinician-annotated test instances and 420 development instances across seven crisis types with temporal annotations. Three strong LLMs (GPT-5, Claude-4-Sonnet, Gemini-2.5-Pro) generate labels via few-shot prompting, with ensemble majority voting for final labels. Training data is split into Consensus (≥2 models agree, 4,181 instances) and Unanimous (all 3 agree, 3,058 instances). Six fine-tuned models are trained using LoRA on ensemble-labeled data, with Qwen3-14B using full fine-tuning. Models are evaluated using Exact Match, Jaccard Index, Micro/Macro F1, and Micro/Macro Recall.

## Key Results
- Closed-source models significantly outperform open-source models on all metrics
- Ensemble majority voting achieves Exact Match of 0.8450, outperforming individual models
- Fine-tuned Llama-3.3-70B on consensus labels achieves Exact Match of 0.7783, surpassing best baseline
- Fine-tuning consistently improves detection performance, especially for smaller models trained on high-quality labels
- Models show over-extension of child abuse labels and failure to apply single-label severity principle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble majority voting across strong LLMs yields more reliable crisis detection than single-model inference.
- Mechanism: When three models (GPT-5, Claude-4-Sonnet, Gemini-2.5-Pro) predict labels independently, taking the majority vote for each crisis label aggregates complementary strengths—recovering errors where any single model misses a subtle indicator. When all three disagree, defaulting to the strongest individual performer (Claude) provides a safety net.
- Core assumption: Models make independent errors; disagreement correlates with ambiguous or edge-case inputs rather than systematic shared failures.
- Evidence anchors:
  - [abstract] "An ensemble majority-vote method combining GPT-5, Claude-4-Sonnet, and Gemini-2.5-Pro achieved an Exact Match of 0.8450 and Jaccard of 0.8794."
  - [section] Table 4 shows the ensemble outperforms every individual model across all metrics; 68.5% of test instances had full 3-way agreement, only 4.33% had complete disagreement.
  - [corpus] Related work (Chen et al., 2024; Wang et al., 2023) supports majority voting for compound inference systems, though evidence in crisis-specific domains is sparse.
- Break condition: If models share systematic biases (e.g., all over-predict "child abuse" for minors regardless of perpetrator context), ensemble voting amplifies rather than corrects the error.

### Mechanism 2
- Claim: Fine-tuning on ensemble-labeled data transfers closed-source model capability to smaller open-source models.
- Mechanism: High-quality pseudo-labels from strong LLM ensembles serve as supervision for fine-tuning smaller open-source models, teaching them crisis-specific patterns without requiring human annotation at scale.
- Core assumption: Ensemble labels are sufficiently accurate to serve as ground truth; label noise does not overwhelm signal during fine-tuning.
- Evidence anchors:
  - [abstract] "Fine-tuning consistently improved detection performance, especially for smaller models trained on high-quality labels."
  - [section] Table 8 shows Llama-3.3-70B fine-tuned on Consensus labels improved Exact Match from 0.7283 to 0.7783 (+5.0 pp), Micro F1 from 0.7773 to 0.8158 (+3.85 pp).
  - [corpus] Weak direct evidence; Nguyen and Pham (2024) showed LLM pseudo-labels can enhance suicide ideation detection, but generalization to multi-faceted crisis detection remains underexplored.
- Break condition: If ensemble labels contain systematic errors (e.g., over-extension of "child abuse" noted in Section 6), fine-tuning will propagate these biases rather than correct them.

### Mechanism 3
- Claim: Agreement threshold for training data (unanimous vs. consensus) interacts with model scale—smaller models benefit from cleaner labels; larger models benefit from more data.
- Mechanism: Unanimous subsets have higher label precision but smaller size (3,058 instances); consensus subsets have slightly lower precision but more coverage (4,181 instances). Smaller models (14B parameters) overfit to noise and benefit from precision; larger models (70B+) have capacity to learn from noisier but more diverse data.
- Core assumption: The precision-diversity trade-off is real and model-dependent; this interaction is not yet empirically confirmed.
- Evidence anchors:
  - [section] "For Qwen3-14B, the model fine-tuned on the Unanimous subset achieves higher scores... Llama-3.3-70B exhibits the opposite trend: the model trained on the Consensus subset slightly outperforms."
  - [section] Authors explicitly note: "this remains a preliminary observation rather than a confirmed conclusion."
  - [corpus] No direct corpus support; this hypothesis requires controlled experiments.
- Break condition: If the observed effect is driven by class distribution differences between subsets rather than label quality, the interaction claim does not hold.

## Foundational Learning

- **Multi-label classification with temporal dimensions**
  - Why needed here: Crises co-occur (e.g., "rape" + "child abuse") and temporal status ("ongoing" vs. "past") determines intervention urgency. Single-label, atemporal approaches miss clinically critical nuance.
  - Quick check question: Given a post mentioning historical child sexual abuse and current passive suicidal ideation, what labels apply?

- **Annotation schema alignment with clinical standards (C-SSRS)**
  - Why needed here: Distinguishing passive vs. active suicidal ideation maps to C-SSRS levels 1–3 vs. 4–5, directly informing risk assessment protocols.
  - Quick check question: Does "I wish I were dead" map to passive or active suicidal ideation under C-SSRS?

- **Precision-recall trade-offs in safety-critical detection**
  - Why needed here: Over-prediction (false positives) may overwhelm human reviewers; under-prediction (false negatives) misses crises. Table 8 shows fine-tuning improved precision but sometimes reduced recall.
  - Quick check question: For crisis detection, which error type is more costly—false positive or false negative—and how should this inform threshold selection?

## Architecture Onboarding

- **Component map:**
  Input preprocessing -> Prompt construction (few-shot examples + task definition + category definitions) -> LLM inference -> JSON label extraction -> Optional ensemble layer (majority voting across 3 models) -> Optional fine-tuning layer (LoRA or full fine-tuning on ensemble-labeled data) -> Evaluation (multi-label metrics)

- **Critical path:**
  1. Ensure prompt includes precise category boundaries (e.g., "child abuse" requires adult perpetrator; peer conflict excluded)
  2. Extract only valid JSON labels; reject malformed outputs
  3. For ensemble, default to strongest model on 3-way disagreement

- **Design tradeoffs:**
  - Ensemble vs. single model: Higher accuracy vs. 3× inference cost
  - Consensus vs. unanimous training data: More coverage vs. higher label quality
  - LoRA vs. full fine-tuning: Parameter efficiency vs. potential performance ceiling

- **Failure signatures:**
  - Over-extension of "child abuse" for any minor-involved incident (Section 6)
  - Confusion between passive vs. active suicidal ideation in ambiguous posts
  - Redundant multi-labeling (tagging both "rape" and "sexual harassment" for same incident violates single-label severity principle)
  - Gemini content filtering blocking sensitive inputs (14/600 blocked)

- **First 3 experiments:**
  1. Replicate ensemble majority voting on held-out subset; measure agreement rate and per-category error reduction vs. single best model.
  2. Fine-tune a mid-sized model (14B) on both Consensus and Unanimous subsets; compare performance deltas to validate scale-label-quality interaction hypothesis.
  3. Ablate temporal annotations; train/evaluate models without "ongoing"/"past" distinctions to quantify the value of temporal awareness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ensemble-based crisis detection be made more computationally efficient while preserving the performance gains from majority voting across multiple models?
- Basis in paper: [explicit] "While our ensemble-based majority voting approach yields strong results, it comes with increased cost and longer inference time due to the need to run multiple models. We will explore more efficient alternatives in future work."
- Why unresolved: The paper demonstrates that ensemble voting (GPT-5, Claude-4-Sonnet, Gemini-2.5-Pro) achieves the best Exact Match (0.8450), but the approach triples inference cost and latency, making it impractical for real-time deployment.
- What evidence would resolve it: Comparison of distillation techniques, cascaded inference, or single-model approaches that achieve comparable performance to the ensemble with significantly reduced computational overhead.

### Open Question 2
- Question: What is the precise relationship between model scale, training data quality (consensus vs. unanimous labels), and downstream crisis detection performance?
- Basis in paper: [explicit] "Our findings also hint at a possible interaction between model scale and supervision quality — with smaller models appearing to benefit more from label precision, and larger models from data diversity — but this remains a preliminary observation rather than a confirmed conclusion. Future studies with controlled experimental settings are needed to substantiate this hypothesis."
- Why unresolved: The paper observes that Qwen3-14B performs better with Unanimous (smaller, cleaner) data while Llama-3.3-70B performs better with Consensus (larger, noisier) data, but this was not systematically tested with controlled variables.
- What evidence would resolve it: Controlled ablation studies varying model size and label quality independently, with statistical analysis of interaction effects on performance metrics.

### Open Question 3
- Question: How can fine-tuned crisis detection models maintain or improve recall while achieving the precision gains observed from high-quality supervision?
- Basis in paper: [explicit] "Finally, because recall is important in crisis detection tasks, the modest decrease in recall observed in the fine-tuned Llama model shows an area for potential improvement."
- Why unresolved: Fine-tuning improved Exact Match and F1 but reduced Macro Recall for Llama-3.3-70B (from 74.04% to 70.63%), suggesting a precision-recall trade-off that is particularly problematic in high-stakes domains where missing crises carries severe consequences.
- What evidence would resolve it: Development of training objectives or data augmentation strategies specifically designed to preserve recall, with empirical demonstration that recall can be maintained without sacrificing precision gains.

### Open Question 4
- Question: How can LLMs be improved to better align with clinical annotation principles, particularly regarding severity prioritization and age-based distinctions in crisis categorization?
- Basis in paper: [inferred] From Section 6 (Analysis), the paper documents systematic model errors including: (1) over-extension of child abuse labels when minors are involved with peer perpetrators close in age, and (2) failure to apply the single-label severity principle where only the most severe category should be tagged for related incidents.
- Why unresolved: Models like GPT-5 and Claude frequently multi-tag related categories (e.g., both rape and sexual harassment for the same incident) and overgeneralize child abuse, indicating they lack understanding of clinical adjudication rules that require contextual reasoning about perpetrator age and severity hierarchies.
- What evidence would resolve it: Development of specialized training data or prompt engineering approaches that encode clinical prioritization rules, with evaluation on edge cases where clinical judgment diverges from surface-level text patterns.

## Limitations
- **Systematic label bias risk**: Ensemble labeling may propagate shared model biases, creating high-precision but clinically misaligned labels.
- **Temporal annotation reliability**: No reported inter-annotator agreement on temporal labels raises questions about consistency.
- **Dataset representativeness**: Reddit-only data may not capture crisis language patterns in clinical settings or other platforms.
- **Limited clinical validation**: Lacks external clinical validation to confirm schema captures all clinically relevant crisis indicators.

## Confidence
- **High confidence**: Benchmark creation process and observation that closed-source models outperform open-source models
- **Medium confidence**: Ensemble majority voting mechanism and fine-tuning effectiveness
- **Low confidence**: Interaction hypothesis between model scale and training data quality

## Next Checks
1. **Ablation study on ensemble agreement patterns**: Analyze which categories benefit most from ensemble voting and whether disagreements cluster around specific crisis types or input characteristics.
2. **Clinical external validation**: Have independent clinicians review a random sample of model predictions (especially false positives) to assess clinical validity.
3. **Temporal annotation reliability test**: Measure inter-annotator agreement specifically for temporal labels (ongoing vs. past) to validate their consistency and clinical utility.