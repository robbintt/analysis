---
ver: rpa2
title: 'BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship
  Attribution'
arxiv_id: '2511.08085'
source_url: https://arxiv.org/abs/2511.08085
tags:
- bangla
- bard10
- baad16
- authorship
- stop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BARD10, a balanced corpus of contemporary
  Bangla blog and opinion texts, and evaluates both classical and deep learning models
  for authorship attribution. Experiments on BARD10 and the existing BAAD16 corpus
  show that TF-IDF with linear SVM consistently outperforms Bangla BERT, XGBoost,
  and MLP, achieving macro-F1 scores of 0.997 on BAAD16 and 0.921 on BARD10.
---

# BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution

## Quick Facts
- **arXiv ID:** 2511.08085
- **Source URL:** https://arxiv.org/abs/2511.08085
- **Reference count:** 24
- **Primary result:** TF-IDF with linear SVM outperforms Bangla BERT, XGBoost, and MLP, achieving macro-F1 scores of 0.997 on BAAD16 and 0.921 on BARD10.

## Executive Summary
This study introduces BARD10, a balanced corpus of contemporary Bangla blog and opinion texts, and evaluates both classical and deep learning models for authorship attribution. Experiments on BARD10 and the existing BAAD16 corpus show that TF-IDF with linear SVM consistently outperforms Bangla BERT, XGBoost, and MLP, achieving macro-F1 scores of 0.997 on BAAD16 and 0.921 on BARD10. Ablation analysis reveals that removing stop-words significantly degrades performance, especially for short blog posts in BARD10, confirming that Bangla stop-words serve as essential stylistic markers. Error patterns indicate that high-frequency features carry authorial signatures that transformer models suppress. The findings highlight the importance of preserving stop-words and using calibrated sparse models for Bangla authorship attribution, and provide BARD10 as a reproducible benchmark for future research.

## Method Summary
The study constructs BARD10, a balanced corpus of 10 contemporary Bangla blog authors, and evaluates four models—Linear SVM with TF-IDF, XGBoost, MLP, and Bangla BERT—on both BARD10 and the existing BAAD16 corpus of 16 canonical literary authors. All models use stratified 80:20 train-test splits, with preprocessing including Unicode normalization and punctuation removal. Stop-words are removed using the BNLP library for some conditions. Models are trained with fixed hyperparameters: Bangla BERT uses sagorsarker/bangla-bert-base with 512-token limit; SVM uses linear kernel with sublinear_tf and use_idf=True; XGBoost uses max_depth=6, n_estimators=100; MLP uses pretrained Word2Vec embeddings with GlobalAveragePooling1D. Macro-F1 is the primary metric, with Δ-Recall ablation isolating per-stop-word contributions.

## Key Results
- TF-IDF with linear SVM achieves macro-F1 of 0.997 on BAAD16 and 0.921 on BARD10.
- Bangla BERT lags by up to 5 points, especially on longer BAAD16 texts due to 512-token truncation.
- Removing stop-words degrades SVM performance by 3.1 pp on BAAD16 and 4.0 pp on BARD10.
- Ablation analysis shows individual stop-words (e.g., "ম") contribute up to 8.1 pp recall for specific authors.

## Why This Works (Mechanism)

### Mechanism 1: Stop-Words Encode Authorial Style in Bangla Prose
Bangla stop-words function as stylistic discriminators rather than noise, particularly in informal genres. Authors exhibit consistent, idiosyncratic usage patterns of pronouns, discourse particles, and connectors (e.g., "ম", "আম", "অে"). Classical models assign high TF-IDF weights to these frequent tokens, capturing fine-grained usage differences that persist across documents. This mechanism weakens if stop-word distributions converge across authors or if documents are too short to establish reliable frequency patterns.

### Mechanism 2: Sparse Global Vocabularies Outperform Truncated Context Windows
TF-IDF + linear SVM captures more complete stylistic signal than context-limited transformers for attribution tasks. TF-IDF vectorizes all tokens regardless of document length; BERT truncates at 512 subword tokens. For long BAAD16 chapters (~750+ words), BERT discards ~75% of input. For short BARD10 blogs (~150–400 words), the MLP's 4,000-token capacity wastes computation on padding. This advantage may diminish if future long-context transformers (4K+ tokens) are applied or if stylistic signal concentrates in early sentences.

### Mechanism 3: Transformer Pretraining Suppresses High-Frequency Stylistic Cues
Pretrained transformers normalize high-frequency tokens, diminishing author-specific distinctions that classical models exploit. Masked language modeling objectives down-weight frequent subwords; embeddings for common particles converge across authors. TF-IDF's IDF component amplifies rare n-grams while still retaining raw frequency signals. This suppression may be reduced if domain-adapted pretraining explicitly preserves frequency-sensitive representations or if hybrid architectures inject sparse features.

## Foundational Learning

- **Concept: TF-IDF Vectorization with IDF Weighting**
  - Why needed here: The paper's strongest results depend on TF-IDF capturing both term frequency and document rarity. Understanding sublinear_tf and IDF scaling explains why SVM outperforms raw counts.
  - Quick check question: If you double the document length without changing term proportions, does TF-IDF magnitude change? Should it?

- **Concept: Stratified Train-Test Splits for Imbalanced Multi-Class**
  - Why needed here: BAAD16 is severely imbalanced (one author has 4,500+ segments, others <250). Stratified splits preserve author proportions across train/test.
  - Quick check question: Why would random splitting inflate accuracy on BAAD16 even if the model is biased toward majority authors?

- **Concept: Ablation via Feature Masking at Inference Time**
  - Why needed here: The Δ-Recall analysis freezes the trained SVM and zeros individual stop-word columns to isolate per-token contribution without retraining confounds.
  - Quick check question: If you retrained the model after removing each stop-word instead of masking at inference, what additional factors would contaminate the Δ measurement?

## Architecture Onboarding

- **Component map:**
  - Preprocessing: Unicode normalization → punctuation removal → (optional) stop-word masking via BNLP tokenizer
  - Feature extraction: scikit-learn TF-IDF (sublinear_tf=True, use_idf=True) OR Bangla BERT tokenizer (max_length=512)
  - Classifiers: Linear SVM (sklearn SVC kernel="linear"), XGBoost (max_depth=6, n_estimators=100), MLP (embedding → GlobalAveragePooling1D → 64 → 32 → softmax), Bangla BERT (sagorsarker/bangla-bert-base + linear head)
  - Evaluation: Macro-F1, per-author recall, confusion matrices logged to W&B

- **Critical path:**
  1. Load BARD10/BAAD16 → stratified 80:20 split
  2. Preprocess with/without stop-words (controlled variable)
  3. Vectorize (TF-IDF or BERT tokenizer)
  4. Train classifier with fixed hyperparameters across conditions
  5. Evaluate macro-F1; run Δ-Recall ablation on best SVM model

- **Design tradeoffs:**
  - SVM: Fast, interpretable, handles full vocabulary; no GPU needed. Weak for non-linear interactions.
  - BERT: Captures context; 512-token limit discards long-document signal. Fine-tuning slower, requires GPU.
  - MLP with fixed embeddings: Handles variable-length input via pooling; OOV tokens problematic; padding waste on short texts.
  - XGBoost: Non-linear ensemble on sparse features; sensitive to stop-word removal (−3.1% F1 on BAAD16).

- **Failure signatures:**
  - BERT gains +0.002 F1 on BAAD16 when stop-words removed (window freed) but loses −0.006 on BARD10 (stylistic cues lost) → genre-dependent sensitivity.
  - MLP drops −0.040 F1 on BARD10 with stop-word removal (short texts become padding-dominated).
  - XGBoost degrades sharply on both corpora when stop-words removed → frequency-based features critical.

- **First 3 experiments:**
  1. Replicate SVM + TF-IDF baseline on BARD10 with stop-words retained; verify macro-F1 ≈ 0.92.
  2. Run stop-word ablation study: train SVM with stop-words, then mask individual high-Δ tokens at inference to confirm per-author sensitivity.
  3. Test BERT with sliding-window pooling (e.g., average [CLS] across 512-token chunks) to mitigate truncation loss on BAAD16 long chapters.

## Open Questions the Paper Calls Out

### Open Question 1
Can long-context transformer architectures outperform the current TF-IDF baseline by capturing stylistic dependencies across the full length of Bangla texts? The study found that standard Bangla BERT discards nearly 75% of input in longer datasets like BAAD16 due to the 512-token limit, while the superior TF-IDF model utilizes all tokens. Fine-tuning a long-context model (e.g., Longformer) on BARD10 and BAAD16 would determine if it can match or exceed the 0.997 and 0.921 macro-F1 scores of the SVM baseline.

### Open Question 2
Does selectively retaining stop-words based on their per-author utility improve classification accuracy compared to blanket retention? The ablation study revealed that the $\Delta$-Recall values showed that some stop-words act as noise for specific authors. An experiment utilizing the $\Delta$-Recall values to filter the stop-word list dynamically per author could result in higher precision/recall than the standard preprocessing pipeline.

### Open Question 3
Can hybrid systems that combine sparse lexical features with deep contextual embeddings outperform the individual models tested? The conclusion lists "hybrid systems that combine sparse and contextual representations" as a specific direction for future work. The development of a model that concatenates TF-IDF vectors with Bangla BERT embeddings, demonstrating higher accuracy on BARD10 than either component alone, would resolve this question.

## Limitations

- The study demonstrates stop-word importance for contemporary blogs and canonical literature, but generalizability to other Bangla genres and time periods is untested.
- The observed TF-IDF advantage may reflect under-optimized transformer hyperparameters rather than inherent model limitations.
- The claim that BERT pretraining suppresses high-frequency stylistic cues is inferred from performance patterns rather than direct embedding analysis.

## Confidence

- **High Confidence**: BARD10 corpus construction methodology and baseline TF-IDF + SVM performance metrics (0.921 macro-F1 on BARD10, 0.997 on BAAD16) are reproducible given the detailed specifications.
- **Medium Confidence**: The stop-word ablation results showing significant performance degradation are robust, but the exact contribution of individual stop-words may vary with different tokenization approaches or stop-word lists.
- **Low Confidence**: The mechanism explaining why transformers underperform—that pretraining downweights frequent tokens—requires direct embedding analysis to verify, not just performance observation.

## Next Checks

1. **Embedding Analysis**: Extract and visualize stop-word embeddings from the trained Bangla BERT model to test whether high-frequency particles indeed converge across authors, providing direct evidence for the suppression mechanism.
2. **Long-Context Transformer Test**: Implement a sliding-window or long-context BERT variant (4K+ tokens) on BAAD16 to determine if truncation, not model architecture, drives the performance gap.
3. **Cross-Corpus Generalization**: Apply the best-performing models to a third Bangla authorship dataset (e.g., social media or news articles) to validate whether stop-word importance persists across genres and time periods.