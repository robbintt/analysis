---
ver: rpa2
title: 'Using Visual Language Models to Control Bionic Hands: Assessment of Object
  Perception and Grasp Inference'
arxiv_id: '2509.13572'
source_url: https://arxiv.org/abs/2509.13572
tags:
- object
- grasp
- hand
- claude
- sonnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks eight vision-language models (VLMs) for inferring
  object properties and grasp parameters in prosthetic hand control. The authors collected
  a dataset of 34 household objects and prompted VLMs to output structured JSON describing
  object name, shape, dimensions, orientation, and grasp parameters (type, hand rotation,
  aperture, finger count).
---

# Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference

## Quick Facts
- arXiv ID: 2509.13572
- Source URL: https://arxiv.org/abs/2509.13572
- Reference count: 17
- Eight VLMs achieve 88-100% accuracy for object identification and shape classification, but grasp type accuracy varies from 44-91%

## Executive Summary
This paper benchmarks eight vision-language models for inferring object properties and grasp parameters in prosthetic hand control. The authors collected a dataset of 34 household objects and prompted VLMs to output structured JSON describing object name, shape, dimensions, orientation, and grasp parameters. Results show VLMs achieve high accuracy for object identification and shape classification, but accuracy for grasp type varies widely. Object dimension estimation errors are generally under 20 mm, with Gemini 2.5 Flash showing the best performance. The structured JSON output enables interpretable shared control, though latency and grasp parameter estimation limitations remain for real-time prosthetic applications.

## Method Summary
The study evaluated eight vision-language models (GPT-4.1, Gemini 2.5 Flash, Claude 3.5 Sonnet, Llama 4 Maverick, Claude 3.5 Haiku, Gemma 3, Llama 4 Behemoth, Claude 3.5 Haiku) on a dataset of 34 household objects. Each model received a static image and a prompt requesting structured JSON output with object name, shape, dimensions, orientation, and grasp parameters (type, hand rotation, aperture, finger count). The VLMs were evaluated using metrics including accuracy, MAE, MSE, and computational cost across all parameters.

## Key Results
- Object identification accuracy: 82.4-97.1% across models
- Shape classification accuracy: 97-100% across models
- Grasp type inference accuracy: 44.1-91.2% across models
- Hand rotation estimation MAE: 13.2-47.6° across models
- Fastest model (Gemini 2.5 Flash) responds in 2.6 s; cheapest (Gemma 3) costs $0.00016 per query

## Why This Works (Mechanism)

### Mechanism 1: Unified Perception via Vision-Language Integration
- Claim: A single VLM can replace multi-module perception pipelines for prosthetic grasp planning.
- Mechanism: VLMs process visual and textual information jointly, enabling object identification, property extraction, and grasp inference from a single image and prompt—consolidating what traditionally requires separate detection, segmentation, and pose estimation modules.
- Core assumption: The VLM has sufficient visual-semantic understanding to map object properties to grasp parameters without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "evaluating a single VLM to perform tasks that traditionally require complex pipelines with separate modules for object detection, pose estimation, and grasp planning"
  - [section II.C] Structured JSON output with 8 parameters from single prompt
  - [corpus] PartDexTOG paper demonstrates language-driven grasp analysis, supporting unified approaches
- Break condition: Multi-object scenes, occlusion, or poor lighting degrade VLM performance below usable thresholds (untested in this study).

### Mechanism 2: Structured Output for Interpretable Control
- Claim: JSON-structured outputs enable reliable parsing and systematic error correction in shared control.
- Mechanism: By constraining VLM output to a predefined schema, the prosthesis controller can parse parameters directly, and mismatches become identifiable for user correction—supporting the shared control paradigm.
- Core assumption: Syntactically valid JSON implies semantically meaningful parameter values.
- Evidence anchors:
  - [section IV.A] "All 272 model-image pairs produced syntactically valid JSON"
  - [section II.C] Schema defines categorical (shape, orientation, grasp_type) and numerical (dimensions, rotation, aperture) fields
  - [corpus] Weak direct evidence; related work on vision-guided grasp planning lacks structured output evaluation
- Break condition: Valid JSON with physically implausible values (e.g., 500mm aperture for a pen) that escape automatic detection.

### Mechanism 3: Categorical-Numerical Performance Asymmetry
- Claim: VLMs excel at categorical reasoning but exhibit higher variance in numerical estimation.
- Mechanism: Pre-trained VLMs have strong semantic classification capabilities from vision-language pretraining, but lack calibrated metric understanding—leading to high categorical accuracy (88–100%) but substantial numerical errors (MAE up to 47.6° rotation).
- Core assumption: Metric estimation requires depth cues or calibration data absent from single static images.
- Evidence anchors:
  - [section III.A] Object naming: 82.4–97.1%; shape: 97–100%; grasp type: 44.1–91.2%
  - [section III.C] Hand rotation MAE: 13.2–47.6°; aperture MAE: 5.74–23.4mm
  - [section IV.B] "relatively large errors in rotation estimation can be tolerated since the user can correct this through compensatory motions"
  - [corpus] Vision-Guided Grasp Planning paper notes similar challenges in unstructured environments
- Break condition: Applications requiring sub-centimeter precision or <10° orientation accuracy without user correction.

## Foundational Learning

- **Vision Language Models (VLMs)**
  - Why needed here: Understanding how multimodal models process images and text jointly explains both their flexibility and limitations in prosthetic perception.
  - Quick check question: Can you explain why a VLM might correctly identify "mug" but misestimate its width by 15mm?

- **Shared Control in Prosthetics**
  - Why needed here: The paper's proposed deployment model relies on VLM providing initial estimates that users refine—augmenting rather than replacing human control.
  - Quick check question: What types of errors are acceptable in shared control vs. fully autonomous grasp planning?

- **Grasp Taxonomy and Parameters**
  - Why needed here: The benchmark evaluates specific grasp parameters (palmar vs. lateral, aperture, rotation, finger count) that map to physical prosthesis configuration.
  - Quick check question: Why might a model correctly classify grasp type but incorrectly estimate hand rotation?

## Architecture Onboarding

- **Component map:**
  Image capture -> VLM API call -> JSON parsing -> Prosthesis controller -> Hand preshaping -> User correction

- **Critical path:**
  1. Image capture → prompt construction → API call (2.6–9.4s latency)
  2. JSON parsing → parameter validation
  3. Command dispatch to prosthesis controller
  4. User adjustment if mismatch detected

- **Design tradeoffs:**
  - **Latency vs. accuracy:** Gemini 2.5 Flash 20-05 offers fastest response (2.6s) but middle-tier accuracy; Claude models offer top categorical accuracy but 7–9s latency
  - **Cost vs. capability:** Gemma 3 costs $0.00016/query but 44.1% grasp type accuracy; Claude costs $0.012/query with 85.3% accuracy
  - **Bias direction:** Positive mean error in aperture (overestimation) preferable to negative (user can close further)

- **Failure signatures:**
  - Grasp type errors on slender objects (models predict palmar when lateral preferred)
  - High rotation variance (±31.9–71.8° STD across models)
  - Depth estimation less reliable than width/height
  - Latency exceeding 5s may frustrate users in real-time scenarios

- **First 3 experiments:**
  1. **Replicate benchmark on your target prosthesis camera:** Validate whether controlled lighting performance transfers to your imaging conditions.
  2. **Test multi-object and cluttered scenes:** The 34-object dataset uses single objects; evaluate performance degradation with occlusion.
  3. **Implement closed-loop correction:** Add user feedback mechanism to measure how quickly shared control converges on successful grasps vs. VLM-only prediction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can VLMs maintain high accuracy in grasp inference when deployed in cluttered, multi-object environments with occlusions and variable lighting?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "Real-world scenarios involving clutter, occlusion, varying illumination, and multiple objects remain to be tested," as the current study relied on a dataset of single-object scenes with consistent lighting.
- **Why unresolved:** The benchmark was limited to 34 images of isolated objects, which does not reflect the visual complexity of daily activities where a prosthesis user must identify target objects amidst distractors and shadows.
- **What evidence would resolve it:** A new benchmark study evaluating the top-performing models (e.g., Gemini 2.5 Flash) on datasets featuring cluttered backgrounds and variable illumination, showing that categorical and numerical accuracy remains above viable thresholds.

### Open Question 2
- **Question:** Can VLM inference be optimized through edge deployment or model compression to meet the sub-150 ms latency requirements of seamless prosthetic control?
- **Basis in paper:** [explicit] The paper notes that current latencies (2.6–9.4 s) are significantly higher than traditional systems (150 ms) and explicitly suggests that "optimization will be necessary... This could involve edge deployment, model compression, or the use of specialized, smaller VLMs."
- **Why unresolved:** The study utilized cloud-based APIs (OpenRouter), and while it identifies latency as a major bottleneck, it does not test local or compressed variants of these models to determine if the speed-accuracy trade-off can be improved.
- **What evidence would resolve it:** Comparative latency benchmarks of quantized or distilled VLMs running on embedded hardware (edge devices), demonstrating inference times under 300 ms while maintaining grasp parameter estimation within acceptable error margins.

### Open Question 3
- **Question:** Does domain-specific fine-tuning or the explicit incorporation of biomechanical constraints significantly reduce the high numerical errors observed in zero-shot grasp estimation?
- **Basis in paper:** [explicit] The authors note that all models were evaluated in a "zero-shot setting without task-specific fine-tuning" and suggest that "Domain-specific training... would likely improve performance significantly," specifically for reducing errors in hand rotation and aperture.
- **Why unresolved:** While zero-shot results showed high MAEs (up to 47.6° for rotation), it is unclear if the models lack the capacity for spatial reasoning or simply lack domain-specific context that could be provided via fine-tuning or refined prompting.
- **What evidence would resolve it:** A follow-up evaluation where a VLM is fine-tuned on a dataset of labeled prosthetic grasps, resulting in a statistically significant reduction in MAE for numerical parameters compared to the zero-shot baseline.

### Open Question 4
- **Question:** Can adaptive, closed-loop user feedback correct the systematic biases and inconsistencies in VLM grasp parameter estimation during dynamic use?
- **Basis in paper:** [explicit] The Discussion proposes that "Future work could explore adaptive, closed-loop learning via user feedback to improve model performance over time" to compensate for the observed variability in parameter estimation.
- **Why unresolved:** The current study assessed static snapshots in an open-loop fashion. It is unknown if the system can self-correct or if the user feedback mechanism would introduce cognitive loads that negate the benefits of semi-autonomous control.
- **What evidence would resolve it:** A human-in-the-loop study where user corrections to hand rotation or aperture are fed back into the model, demonstrating a measurable decrease in the frequency and magnitude of required corrections over multiple sessions.

## Limitations
- Controlled single-object dataset doesn't reflect real-world cluttered scenarios with occlusions and variable lighting
- Shared control paradigm assumes users can reliably detect and correct VLM errors without quantifying cognitive load
- Zero-shot evaluation without domain-specific fine-tuning leaves open whether performance could be substantially improved

## Confidence
- **High Confidence:** Object identification accuracy (>88%), shape classification (>97%), and object dimension estimation within 20mm are well-supported by consistent results across all eight VLMs
- **Medium Confidence:** Grasp type inference (44-91% accuracy) and hand rotation estimation (MAE 13-48°) show substantial variability between models and object types
- **Low Confidence:** Real-world deployment performance under variable conditions, user correction effectiveness, and long-term reliability remain speculative based on this controlled benchmark

## Next Checks
1. **Multi-object Clutter Testing:** Evaluate all eight VLMs on scenes containing 2-5 overlapping objects with partial occlusions to quantify performance degradation and identify failure modes not apparent in single-object scenarios.

2. **Closed-loop User Study:** Implement the shared control system with actual prosthetic users to measure: (a) time to successful grasp completion with vs. without user correction, (b) cognitive load using standardized scales, and (c) error detection rates across different user populations.

3. **Environmental Robustness Evaluation:** Test VLM performance across lighting conditions (0-500 lux), camera angles (±45° from optimal), and background variations (clean vs. cluttered) to establish operational boundaries and identify sensor requirements for reliable real-world deployment.