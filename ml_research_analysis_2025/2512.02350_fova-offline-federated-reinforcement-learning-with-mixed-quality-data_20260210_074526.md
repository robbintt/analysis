---
ver: rpa2
title: 'FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data'
arxiv_id: '2512.02350'
source_url: https://arxiv.org/abs/2512.02350
tags:
- policy
- local
- learning
- offline
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of performance degradation in
  offline Federated Reinforcement Learning (FRL) when data quality varies across clients.
  Existing methods struggle when some clients have low-quality offline data, which
  can "contaminate" the global policy during aggregation.
---

# FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data

## Quick Facts
- arXiv ID: 2512.02350
- Source URL: https://arxiv.org/abs/2512.02350
- Reference count: 40
- One-line primary result: FOVA achieves 20%-50% average score improvements and up to 56% performance gains in mixed-quality data scenarios compared to existing offline FRL methods

## Executive Summary
This paper addresses the challenge of performance degradation in offline Federated Reinforcement Learning (FRL) when clients possess data of varying quality. Existing methods struggle when low-quality data "contaminates" the global policy during aggregation. The proposed FOVA framework introduces a vote mechanism that selects the best action from local, global, and behavioral policies based on Q-values, and uses Advantage-Weighted Regression (AWR) to ensure consistent optimization objectives between local and global policies. These innovations enable FOVA to identify high-return actions and prevent low-quality behaviors from degrading performance.

## Method Summary
FOVA operates in a federated learning setting where multiple clients with heterogeneous, offline datasets collaborate to learn a global policy. Each client maintains a local Q-network and policy, trained using a novel Vote-based Conservative Q-Learning (VCQL) approach. The vote mechanism selects actions by maximizing Q-values among the local policy, global policy, and behavior policy for each state. Local policy updates employ AWR to align with a global surrogate objective derived from the aggregated server policy. The server performs linear averaging of parameters, and the process iterates. The framework is evaluated on D4RL benchmark tasks with varying data quality distributions across clients.

## Key Results
- FOVA achieves 20%-50% average score improvements over existing methods on standard benchmarks
- In mixed-quality data scenarios, FOVA maintains performance improvements of up to 56%
- Theoretical analysis proves strict policy improvement over behavioral policies under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Filtering low-quality behavior via a "Vote Policy" improves local Q-function estimation in mixed-quality settings.
- **Mechanism:** FOVA constructs a virtual "vote policy" that selects the action yielding maximum Q-value among three candidates: local policy, global policy, and behavior policy. This selected action is used to update the Q-target.
- **Core assumption:** At least one of the three available policies provides a high-value action for any given state.
- **Evidence anchors:** [abstract] "...exploits a vote mechanism to identify high-return actions..."; [section IV-A] Eq. (1) defines $\pi_v$ as $\arg \max_{\pi'} Q$ and Eq. (3) integrates this into the Bellman update.
- **Break condition:** If all three policies are suboptimal for a specific state, the "vote" merely selects the "best of the worst," potentially reinforcing incorrect value estimates.

### Mechanism 2
- **Claim:** Aligning local updates with a global surrogate objective resolves the server-client performance mismatch.
- **Mechanism:** FOVA uses AWR to derive a closed-form "ideal" global policy, and local training explicitly projects the local policy onto this ideal global manifold rather than just regularizing towards it.
- **Core assumption:** The advantage function accurately reflects the relative value of actions, and the KL constraint is sufficient to prevent distribution collapse.
- **Evidence anchors:** [abstract] "...construct consistent local and global training objectives..."; [section IV-B] Derivation from Eq. (10) to Eq. (17) shows the transition to the AWR loss.
- **Break condition:** If the temperature parameter $\beta$ is set too low, the algorithm becomes greedy and violates the trust region; if too high, it reduces to behavioral cloning and loses improvement ability.

### Mechanism 3
- **Claim:** Vote-based Conservative Q-Learning (VCQL) prevents overestimation of out-of-distribution actions.
- **Mechanism:** Standard CQL penalizes Q-values for actions not in the dataset. FOVA modifies this to penalize based on the divergence between the Vote Policy and the behavior policy.
- **Core assumption:** The regularization weight $\alpha$ is large enough to overcome sampling errors but small enough to allow learning.
- **Evidence anchors:** [abstract] "...theoretical analysis proves strict policy improvement and conservative Q-learning..."; [appendix A-A] Lemma 1 formally proves $\hat{V}(s) \leq V(s)$ under specific conditions for $\alpha$.
- **Break condition:** If the "Vote Policy" selects an out-of-distribution action that is not sufficiently penalized due to poor tuning of $\alpha$, the Q-function may overestimate, leading to policy divergence.

## Foundational Learning

- **Concept: Offline Reinforcement Learning (CQL)**
  - **Why needed here:** FOVA builds directly upon Conservative Q-Learning. Understanding the "pessimism" principle—penalizing unseen state-action pairs—is essential to grasp how FOVA prevents the vote mechanism from selecting dangerous, overestimated actions.
  - **Quick check question:** Why does standard Q-learning fail in offline settings, and how does adding a term like $\alpha \mathbb{E}[\log \sum \exp(Q)]$ help?

- **Concept: Advantage-Weighted Regression (AWR)**
  - **Why needed here:** The local policy improvement phase is an adaptation of AWR. You need to understand how AWR re-weights the log-probability of actions based on their advantage to see how FOVA derives the closed-form solution for the global policy.
  - **Quick check question:** In AWR, what does the exponential weight $\exp(\frac{1}{\beta}A(s,a))$ do to the probability of taking an action that is much worse than average?

- **Concept: Federated Heterogeneity**
  - **Why needed here:** The core problem is "mixed-quality data," a specific form of heterogeneity. Understanding that clients have non-IID data distributions is the motivation for the voting mechanism.
  - **Quick check question:** How does aggregating parameters from a client with "expert" data and a client with "random" data typically affect the performance of the global policy in standard FedAvg?

## Architecture Onboarding

- **Component map:**
  Client -> Vote Module -> Q-Network -> Policy Network -> Server (FedAvg)

- **Critical path:**
  1. Client downloads global policy and Q-network
  2. Vote Module evaluates Q-values for actions from local, global, and behavior policies
  3. Q-Network trained using max-Q action (Vote) and VCQL regularization
  4. Policy Network updated via AWR to align with global objective
  5. Updated parameters uploaded to server for aggregation

- **Design tradeoffs:**
  - Compute vs. Performance: Vote mechanism requires forward passes through Q-network for three policies per update step, increasing GPU memory and runtime by ~2% compared to simpler baselines.
  - Exploitation vs. Safety: Temperature parameter $\beta$ controls conservatism. Lower $\beta$ exploits high-advantage actions aggressively (risky); higher $\beta$ stays closer to behavior policy (safer but potentially stagnant).

- **Failure signatures:**
  - Server-Client Divergence: Global policy underperforms local policies (check $\lambda$ and $\beta$ values)
  - Performance Collapse in Mixed Quality: Adding "random" quality client causes global policy score to plummet (vote mechanism not filtering noise)
  - Q-Value Explosion: Unbounded Q-value growth indicates insufficient CQL penalty or OOD action selection

- **First 3 experiments:**
  1. Mixed-Quality Sanity Check: Replicate "Mixed Quality" scenario with 2 Expert and 2 Random clients to verify vote mechanism stability
  2. Ablation of Consistency: Disable AWR projection and measure gap between Server Return and Client Return
  3. Hyperparameter Sensitivity ($\beta$ & $\lambda$): Sweep temperature $\beta$ and weighting $\lambda$ on medium-quality dataset to find safe improvement region

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the vote mechanism be implemented with memory-efficient methods to handle high-dimensional state spaces on resource-constrained edge devices?
- Basis in paper: [explicit] Section VI identifies the vote mechanism's requirement for multiple Q-value evaluations as incurring "non-negligible GPU memory overhead," and suggests developing "memory-efficient vote through selective value estimation" as future work.
- Why unresolved: The current method evaluates Q-values for local, global, and behavioral policies simultaneously, which may be infeasible on typical edge hardware despite theoretical suitability.
- What evidence would resolve it: An implementation using selective value estimation that maintains the reported 20–50% performance gains while significantly lowering memory usage.

### Open Question 2
- Question: How can the critical hyperparameters $\alpha$, $\beta$, and $\lambda$ be adaptively tuned to handle dynamic variations in client data quality?
- Basis in paper: [explicit] Section VI lists "adaptive tuning capabilities" for hyperparameters as a limitation, noting that current values are static or derived from initial bounds.
- Why unresolved: The paper provides theoretical lower bounds, but manually setting these values may not be optimal for all clients in a federated system where data quality shifts over time.
- What evidence would resolve it: A study showing that an adaptive tuning method outperforms the fixed-parameter configuration in a non-stationary environment with shifting data quality distributions.

### Open Question 3
- Question: Can differential privacy (DP) or secure multi-party computation (SMPC) be integrated without destabilizing the vote mechanism's ability to distinguish policy quality?
- Basis in paper: [explicit] Section VI proposes enhancing privacy via DP and SMPC as a concrete future direction.
- Why unresolved: The vote mechanism relies on comparing precise Q-values. Adding DP noise or encryption could obscure differences between high and low-quality policies, potentially invalidating the "strict policy improvement" guarantees.
- What evidence would resolve it: Theoretical bounds proving VCQL conservatism remains valid under specific privacy budgets, or experiments showing vote mechanism robustness to DP noise.

## Limitations

- Proof Generality: Theoretical convergence guarantees assume bounded rewards and standard Lipschitz conditions; performance under unbounded or highly stochastic rewards is not discussed.
- Scalability to Large Action Spaces: Vote mechanism computational overhead may become prohibitive in high-dimensional action spaces like robotics with many joint angles.
- Behavioral Policy Estimation: The method requires estimating behavior policies for each client, but the paper doesn't specify how this is done or how estimation errors propagate.

## Confidence

- **High Confidence:** The empirical results showing 20%-50% average score improvements over baselines are well-supported by the experimental section and ablation studies.
- **Medium Confidence:** The theoretical analysis proving strict policy improvement is sound under stated assumptions, but the practical tightness of bounds and sensitivity to hyperparameters are not fully explored.
- **Low Confidence:** The claim that the Vote mechanism is "robust to all forms of heterogeneity" is overstated; the paper only tests heterogeneity in data quality, not in environment dynamics or reward structures.

## Next Checks

1. **Unbounded Reward Test:** Evaluate FOVA on a variant of D4RL tasks with artificially inflated or stochastic rewards to test robustness of theoretical convergence guarantees.
2. **High-Dimensional Action Space:** Implement FOVA on a benchmark with large action space (e.g., Humanoid from D4RL or custom robotics task) to measure computational overhead of Vote mechanism.
3. **Behavioral Policy Error Sensitivity:** Systematically degrade the quality of estimated behavior policies (e.g., by using fewer samples for BC) and measure impact on Vote mechanism's ability to filter low-quality data.