---
ver: rpa2
title: 'The Intercepted Self: How Generative AI Challenges the Dynamics of the Relational
  Self'
arxiv_id: '2509.13391'
source_url: https://arxiv.org/abs/2509.13391
tags:
- self
- will
- might
- generative
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores how generative AI challenges the dynamics
  of the relational self through three spheres: externalised output (things produced
  by humans), contextual sphere (framing of actions), and self-relating sphere (how
  we relate to ourselves). The authors argue that generative AI, through its predictive
  and anticipatory capacities, intercepts human initiatives across these spheres by
  standardizing outputs, reshaping contextual awareness through AI assistants, and
  influencing desire formation through companionship AI.'
---

# The Intercepted Self: How Generative AI Challenges the Dynamics of the Relational Self

## Quick Facts
- arXiv ID: 2509.13391
- Source URL: https://arxiv.org/abs/2509.13391
- Reference count: 10
- Primary result: Generative AI intercepts human selfhood across three spheres by standardizing outputs, reshaping contextual awareness, and influencing desire formation, risking essentialization and reduced agency.

## Executive Summary
This conceptual paper argues that generative AI challenges the "relational self" - understood as a dynamic process of becoming through relations - by intercepting human initiatives across three spheres: externalized output, contextual sphere, and self-relating sphere. The authors contend that AI's predictive capacities standardize outputs, reduce practical participation in tasks, and shape desire formation through engagement optimization, potentially leading to preference stabilization and diminished ownership of actions. The work calls for interdisciplinary research into how AI reshapes meaningful agency and the conditions of becoming.

## Method Summary
This is a conceptual philosophy paper proposing a theoretical framework rather than an empirical study. The authors employ philosophical argumentation and conceptual analysis to develop a three-sphere model examining how generative AI intercepts human initiatives. No datasets, algorithms, or training procedures are specified. The method involves mapping existing AI systems (ChatGPT, Apple Intelligence, Gemini, Replika, Claude) to theoretical constructs and calling for empirical validation of the framework's predictions.

## Key Results
- Generative AI outputs tend toward statistical averages that rarely mirror individual relational positionality, reducing ownership of creative work
- AI assistants that remove intermediary steps risk creating "sealed-off use objects" that diminish practical understanding of digital-physical processes
- Companionship AI optimized for engagement can shape desire formation, potentially creating an "essentialist self 2.0" through preference stabilization

## Why This Works (Mechanism)

### Mechanism 1
Generative AI disrupts the relational self by replacing deliberative human production with standardized, predictive outputs. Because GenAI generates content based on statistical averages rather than lived positionality, users shift from creators making deliberate choices to prompters selecting from homogenized outputs. This reduces ownership of the final result by averaging out the specific deliberative and automatic choices unique to an individual's history.

### Mechanism 2
AI assistants reduce human agency by collapsing intermediary steps required for task completion, diminishing practical participation in one's own context. By integrating device-wide data to execute multi-step actions, the AI reduces the uncertainty and cognitive load of tasks. This risks creating a "sealed-off use object" dynamic where users lose practical understanding of how digital and physical processes interlink.

### Mechanism 3
Companionship AI optimized for engagement intercepts desire formation by preemptively mirroring or eliciting preferences users would not develop independently. Because preferences are plastic and formed by behavior, AI systems that optimize for captivation can shape the user's self-narrative. This leads to an "essentialist self 2.0" where the AI stabilizes users into static preference sets to improve predictability.

## Foundational Learning

- **Relational Self (Process Ontology)**: The self as a dynamic process of becoming through relations rather than a static entity with fixed traits. Needed because the entire paper rests on defining selfhood as process rather than substance. Quick check: Can you explain why the authors argue that "preferences are not primary constitutive elements" of the self?

- **Contextual Awareness (in AI)**: Technical capability allowing AI to operate in the contextual sphere by aggregating device screen, email, calendar, and location data. Needed to understand the scope of data used for autonomous action. Quick check: What specific data types does the paper suggest future AI assistants will use to construct "personal context"?

- **Compatibilism & Identification**: Frankfurt's concept of "identification" (second-order mental states) defines ownership of action. Needed to frame how AI might separate users from "willing" ownership of choices. Quick check: According to the paper's application of Frankfurt, why might an AI-generated action plan fail to confer "ownership" even if the user accepts it?

## Architecture Onboarding

- **Component map**: Data Layer (aggregates user context: Device screen, Email, Calendar, Location) -> Modeling Layer (constructs predictive model of preferences) -> Orchestration Layer (plans and executes multi-step actions) -> Feedback Loop (optimizes for non-surprise or engagement)

- **Critical path**: The path from Context Aggregation -> Predictive Modeling -> Autonomous Execution. If prediction is accurate enough to avoid "surprising" the user, interception occurs without resistance.

- **Design tradeoffs**: Efficiency vs. Agency (reducing intermediary steps increases efficiency but risks atrophy of practical understanding); Predictability vs. Fluidity (optimizing for predictable behavior may "essentialize" the user); Engagement vs. Authentic Desire (optimizing for engagement metrics can override authentic desire formation).

- **Failure signatures**: The "Unwilling Addict" Pattern (users rely on AI for action plans but lack second-order endorsement); Preference Looping (AI constantly reinforces past versions, preventing evolution); Responsibility Gap (users feel no moral responsibility for AI-initiated actions gone wrong).

- **First 3 experiments**:
  1. Measure difference in "feeling of ownership" between users who manually complete a task vs. those who approve an AI-generated plan
  2. Track if users with highly predictive AI assistants show less variance in behavior over time compared to controls
  3. Test how often an AI assistant must act "out of line" with expectations to maintain critical engagement

## Open Questions the Paper Calls Out

### Open Question 1
Should AI assistants be aligned to preserve existing user preferences or to encourage legitimate preference alteration? The authors ask whether AI assistants should aim to preserve existing preferences or encourage legitimate preference alteration, noting that subjective preferences are plastic yet AI alignment typically aims for predictability. Empirical studies comparing user autonomy outcomes between stability-focused and fluidity-permitting AI alignment protocols would help resolve this.

### Open Question 2
Does reliance on AI-generated action plans undermine a user's capacity for ownership and moral responsibility? The paper questions whether users can fully own and assume responsibility for actions when the cognitive labor of planning is intercepted by AI, asking if "identification" with an action is possible when planning is externalized. Psychological experiments measuring agency attribution and blame assignment in users executing AI-suggested versus self-generated tasks could provide evidence.

### Open Question 3
How does engagement-optimized generative AI influence the formation of human desire? The paper notes that models optimized for engagement might "interfere in the desire formation of the self," calling for caution and further research. It remains undetermined whether AI "mirroring" supports self-discovery or exploits stochastic self-nature to maximize retention. Longitudinal analysis tracking desire stability and authenticity in users interacting with engagement-optimized versus neutral AI companions would help answer this.

## Limitations

- The conceptual framework relies heavily on philosophical distinctions that may not map cleanly to empirical validation
- The three-sphere model lacks clear operational definitions for "interception," "ownership," and "essentialization" that would allow systematic measurement
- Claims about preference stabilization and ownership erosion remain speculative without longitudinal studies tracking actual user behavior over time

## Confidence

- **Medium Confidence**: Claims about AI assistants reducing practical participation through removal of intermediary steps
- **Medium Confidence**: Arguments about companionship AI shaping desire formation through engagement optimization
- **Low Confidence**: Predictions about the timeline and severity of preference stabilization

## Next Checks

1. Conduct a controlled study comparing users' sense of ownership when manually completing vs. approving AI-generated task plans for identical outcomes
2. Track behavioral variance in AI assistant users versus non-users over 6-12 months, testing for preference stabilization
3. Determine through user testing the frequency of AI "surprises" needed to maintain active user supervision versus passive acceptance