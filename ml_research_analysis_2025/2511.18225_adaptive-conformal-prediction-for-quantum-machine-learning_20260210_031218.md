---
ver: rpa2
title: Adaptive Conformal Prediction for Quantum Machine Learning
arxiv_id: '2511.18225'
source_url: https://arxiv.org/abs/2511.18225
tags:
- quantum
- prediction
- score
- conformal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing reliable uncertainty
  quantification for quantum machine learning models in the presence of non-stationary
  hardware noise. The authors formalize how time-varying noise invalidates the exchangeability
  assumption required for standard conformal prediction, even when calibration and
  test data are exchangeable.
---

# Adaptive Conformal Prediction for Quantum Machine Learning

## Quick Facts
- **arXiv ID:** 2511.18225
- **Source URL:** https://arxiv.org/abs/2511.18225
- **Reference count:** 36
- **Primary result:** Adaptive Quantum Conformal Prediction (AQCP) maintains asymptotic coverage guarantees under non-stationary hardware noise by online recalibration of the miscoverage level.

## Executive Summary
This paper addresses the challenge of providing reliable uncertainty quantification for quantum machine learning models in the presence of non-stationary hardware noise. The authors formalize how time-varying noise invalidates the exchangeability assumption required for standard conformal prediction, even when calibration and test data are exchangeable. They introduce Adaptive Quantum Conformal Prediction (AQCP), which applies adaptive conformal inference to the quantum setting, maintaining asymptotic average coverage guarantees under arbitrary hardware noise conditions through online recalibration. Experiments on IBM quantum hardware demonstrate that AQCP achieves target coverage levels with greater stability than standard quantum conformal prediction. The work also evaluates multiple sample-based score functions for AQCP, showing that kernel density estimation and high-density region based scores achieve near-optimal prediction set sizes.

## Method Summary
The method introduces Adaptive Quantum Conformal Prediction (AQCP) that extends standard split conformal prediction to handle non-stationary quantum hardware noise. A quantum model consists of a learned angle encoder (neural network) and a hardware-efficient ansatz (5 qubits, 5 layers) trained on a noiseless simulator. For each test point, the model generates multiple quantum measurements (shots) that are used to construct prediction sets via score functions. AQCP adapts the miscoverage level α_i online using a feedback mechanism: α_{i+1} = α_i + γ(α - err_i), where err_i indicates coverage error. The approach is evaluated with multiple score functions including k-NN, Euclidean distance, kernel density estimation, and high-density region methods on both simulated and real IBM quantum hardware.

## Key Results
- AQCP maintains asymptotic average coverage guarantees under arbitrary hardware noise conditions through online recalibration
- On IBM quantum hardware, AQCP achieves target coverage levels with greater stability than standard quantum conformal prediction
- Kernel density estimation and high-density region based scores achieve near-optimal prediction set sizes as shot count increases
- Step size γ = 0.03 provides good balance between coverage stability and convergence speed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-stationary hardware noise breaks the exchangeability of conformity scores even when calibration and test data are exchangeable.
- **Mechanism:** The conformity score function $\hat{S}(x,y; A_{x,T})$ depends on the shot multiset $A_{x,T}$, which is drawn from a time-conditional distribution. When noise channels $\mathcal{E}_t$ vary with time $t$, scores become time-dependent, breaking exchangeability of the augmented observations $Z_i = (X_i, Y_i; A_{X_i,T_i})$.
- **Core assumption:** Noise processes in quantum hardware are genuinely non-stationary across calibration and test shots (sources include temperature fluctuations, control equipment oscillations, cosmic rays).
- **Evidence anchors:**
  - [abstract] "formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable"
  - [Section 3.2] Formal derivation showing $S_i = \hat{S}(X_i, Y_i; \{\hat{Y}_{X_i,t}\}_{t \in T_i})$ are not exchangeable when noise is time-dependent
  - [corpus] Related work "Noise-Adaptive Conformal Classification" addresses similar non-stationarity concerns in classical settings
- **Break condition:** If hardware noise were stationary or if scores could be constructed without dependence on shot outputs, exchangeability would be preserved.

### Mechanism 2
- **Claim:** Online recalibration via adaptive miscoverage level $\alpha_i$ maintains asymptotic average coverage guarantees under arbitrary noise.
- **Mechanism:** The update rule $\alpha_{i+1} = \alpha_i + \gamma(\alpha - \text{err}_i)$ creates negative feedback: when coverage errors exceed target ($\text{err}_i = 1$), $\alpha_i$ increases, expanding prediction sets. The bound $|\frac{1}{N}\sum_{i=1}^N \text{err}_i - \alpha| \leq \frac{\max\{\alpha_1, 1-\alpha_1\} + \gamma}{N\gamma}$ converges to zero as $N \to \infty$.
- **Core assumption:** The adaptive parameter $\alpha_i$ remains bounded in $[-\gamma, 1+\gamma]$; the process can adapt fast enough relative to noise drift rate.
- **Evidence anchors:**
  - [Section 4.1] Complete derivation of asymptotic guarantee $\lim_{N \to \infty} \frac{1}{N} \sum_{j=1}^N \text{err}_j \xrightarrow{a.s.} \alpha$
  - [Section 5.2, Figures 3-4] Empirical demonstration that $\gamma = 0.03$ maintains coverage near 90% target while $\gamma = 0$ (standard QCP) oscillates
  - [corpus] "Exploring the Noise Robustness of Online Conformal Prediction" examines similar adaptive mechanisms for distribution shifts
- **Break condition:** If step size $\gamma$ is too large (coverage volatility) or too small (inability to track rapid noise changes), the mechanism degrades.

### Mechanism 3
- **Claim:** Sample-based score functions that estimate conditional density achieve near-optimal prediction set sizes as shot count increases.
- **Mechanism:** Scores $s_{KDE}$ and $s_{HDR}$ approximate $-p(y|x)$ and $\int_{H_x(p(y|x))} p(\tilde{y}|x) d\tilde{y}$ respectively. By Theorem 3, scores that are strictly increasing transformations of $-p(y|x)$ are optimal for marginal coverage. As $M \to \infty$, KDE and HDR estimators converge to their population quantities.
- **Core assumption:** Kernel density estimation with appropriate bandwidth $h$ provides consistent density estimates; sample shots are drawn from the true (noisy) conditional distribution.
- **Evidence anchors:**
  - [Section 4.2 & Appendix B] Formal characterization: $\hat{S} \in \mathcal{S}_1$ iff $\hat{S}(x,y) = \phi(-p(y|x))$ for strictly increasing $\phi$
  - [Section 5.3, Figure 5a] $s_{HDR}$ achieves smallest average set size at $M = 1000$; $s_{Euc}$ produces sets 1.5-2x larger
  - [corpus] Limited direct corpus evidence on quantum-specific score function optimality; related work focuses on classical probabilistic CP
- **Break condition:** With too few shots ($M \leq 10$), density estimates are unreliable and all score functions perform similarly poorly.

## Foundational Learning

- **Concept: Split Conformal Prediction**
  - **Why needed here:** AQCP builds on split CP; understanding calibration scores, quantile computation, and the $(1-\alpha)$ coverage guarantee is prerequisite.
  - **Quick check question:** Given calibration scores $\{s_1, ..., s_n\}$ and miscoverage $\alpha = 0.1$, what quantile threshold $\lambda$ defines the prediction set?

- **Concept: Exchangeability vs. IID**
  - **Why needed here:** The paper's central insight is that non-stationary noise breaks exchangeability of scores even when data is IID; distinguishing these concepts is essential.
  - **Quick check question:** If $(Z_1, ..., Z_{n+1})$ are exchangeable and we apply a fixed score function to each, are the resulting scores exchangeable?

- **Concept: Parametrized Quantum Circuits and Measurement**
  - **Why needed here:** Understanding how PQCs produce bitstring outcomes, how angle encoding maps classical data, and why measurements are inherently probabilistic is necessary to grasp why scores depend on shot multisets.
  - **Quick check question:** A 5-qubit PQC measured in the computational basis produces outcomes in what space? How does the paper map these to the target space $\mathcal{Y}$?

## Architecture Onboarding

- **Component map:** Input $x$ -> Angle encoder (NN: 1→10→10→75) -> Rotation angles $\theta_W(x)$ -> 5-qubit HEA (5 layers: RZ→RY→RZ + linear CZ) -> Bitstring measurements -> Mapping function $f(b)$ -> Score computation with shot multiset $A_{x,T}$ -> Prediction set construction -> Adaptive recalibrator updates $\alpha_i$

- **Critical path:**
  1. Train PQC on noiseless simulator using cross-entropy loss (1000 samples, 100 epochs, lr=0.01)
  2. Collect calibration shots on quantum hardware (100 initial points × 100 shots each)
  3. For each test point: encode input → execute circuit → collect shots → compute scores for all candidate $y$ → construct prediction set → observe true label → update $\alpha_i$

- **Design tradeoffs:**
  - **Step size $\gamma$:** Higher values adapt faster but increase coverage volatility; paper uses $\gamma = 0.03$
  - **Shot count $M$:** More shots improve density estimation but increase quantum hardware costs; diminishing returns above $M \approx 100$
  - **Score function:** $s_{HDR}$ and $s_{KDE}$ optimal for efficiency; $s_{k-NN}$ simpler to implement; $s_{Euc}$ poor for multimodal distributions

- **Failure signatures:**
  - Coverage systematically below target: $\gamma$ too small for noise drift rate, or calibration set unrepresentative
  - Excessively large prediction sets: score function mismatched to output distribution (e.g., $s_{Euc}$ on multimodal data)
  - Coverage oscillates wildly: $\gamma$ too large

- **First 3 experiments:**
  1. **Baseline validation:** Implement AQCP with $s_{k-NN}$ on classical simulator (FakeQuitoV2) with $\gamma = 0.03$, verify coverage converges to $1 - \alpha$ as test points increase
  2. **Noise sensitivity:** Compare AQCP ($\gamma = 0.03$) vs. online QCP ($\gamma = 0$) on IBM hardware data; plot rolling coverage to quantify stability improvement
  3. **Score function ablation:** Run all five score functions ($s_{Euc}$, $s_{k-NN}$, $s_{KDE}$, $s_{HDR}$) across $M \in \{1, 10, 100, 1000\}$ shots; measure average set size and coverage to identify optimal choice for target distribution

## Open Questions the Paper Calls Out

- **Open Question 1:** Can dynamic step-size selection strategies for the adaptation parameter $\gamma$ improve the performance of Adaptive Quantum Conformal Prediction (AQCP) compared to the fixed step-size implementation used in this study?
  - **Basis in paper:** [explicit] The conclusion states that research on "optimal step-size selection in adaptive conformal prediction... could be explored in quantum settings."
  - **Why unresolved:** The authors utilized a fixed step size ($\gamma=0.03$), but optimal adaptation to noise drift likely requires a dynamic rate.
  - **What evidence would resolve it:** Experiments on quantum hardware comparing fixed vs. adaptive $\gamma$ strategies (e.g., Bhatnagar et al. 2023) regarding coverage stability and convergence speed.

- **Open Question 2:** Can the framework of weighted conformal prediction (Barber et al., 2023) yield valid finite-sample guarantees for quantum models by explicitly quantifying temporal noise drift?
  - **Basis in paper:** [explicit] The authors suggest that "if changes in noise over time could be quantified, the theory in Barber et al. (2023) might yield stronger finite-sample guarantees."
  - **Why unresolved:** The paper provides asymptotic guarantees (AQCP) and notes that existing finite-sample bounds rely on conditions (hidden Markov chains) unlikely to be met by quantum noise.
  - **What evidence would resolve it:** A theoretical derivation of total variation bounds for non-exchangeable quantum noise that satisfy the requirements of the weighted conformal prediction framework.

- **Open Question 3:** How does non-stationary hardware noise impact the efficiency (average prediction set size) of different score functions compared to the stationary simulations presented?
  - **Basis in paper:** [inferred] The efficiency analysis in Section 5.3 was performed exclusively on a classical simulator to "isolate the effect" of score functions, implying the combined effect of non-stationary noise and score choice on efficiency is unexplored.
  - **Why unresolved:** It is unclear if the efficiency rankings of score functions (e.g., HDR vs. Euclidean) remain consistent when subjected to the real, time-varying hardware noise used in the coverage experiments.
  - **What evidence would resolve it:** Efficiency data (average set sizes) collected from the IBM quantum processor under the same non-stationary conditions used for the coverage analysis.

## Limitations

- **Theoretical limitations:** Asymptotic guarantees require unbounded shot collections (M → ∞) and perfect density estimation, which are impractical for real quantum hardware with limited coherence times
- **Experimental scope:** Results are limited to a single noise realization on IBM hardware, with no systematic analysis of how different noise characteristics affect performance
- **Resource requirements:** Kernel density estimation and high-density region methods require substantially more shots than simple k-NN, potentially limiting practicality for resource-constrained quantum devices

## Confidence

- **Exchangeability breakdown under non-stationary noise:** High confidence - The mathematical derivation is rigorous and the mechanism is well-established in the conformal prediction literature
- **AQCP maintains asymptotic coverage under arbitrary noise:** Medium confidence - Theoretical proof exists but depends on idealized assumptions about step size and convergence
- **HDR and KDE scores achieve near-optimal set sizes:** Medium confidence - Empirical evidence shows improvement at high shot counts, but optimality claims are asymptotic
- **Online recalibration provides practical benefit:** High confidence - The experimental results on real quantum hardware are convincing and the effect is observable

## Next Checks

1. **Cross-distribution validation:** Test AQCP across diverse synthetic target distributions (unimodal, multimodal, heavy-tailed) to verify the robustness of HDR/KDE score performance claims beyond the symmetric bimodal case

2. **Noise regime sensitivity analysis:** Systematically vary the noise characteristics (drift rate, amplitude, correlation structure) on quantum simulators to quantify the relationship between step size γ and coverage stability across different hardware noise profiles

3. **Shot budget scalability test:** Evaluate prediction set efficiency and coverage accuracy across a wider range of shot counts (M ∈ {1, 5, 10, 20, 50, 100, 200, 500, 1000}) to establish practical shot count recommendations for different score functions and quantify diminishing returns