---
ver: rpa2
title: 'HCMRM: A High-Consistency Multimodal Relevance Model for Search Ads'
arxiv_id: '2502.05822'
source_url: https://arxiv.org/abs/2502.05822
tags:
- relevance
- video
- text
- hcmrm
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving query-to-video
  relevance matching in search advertising on short video platforms. The proposed
  High-Consistency Multimodal Relevance Model (HCMRM) enhances consistency between
  pre-training and downstream relevance tasks by introducing a pseudo-query-video
  matching mechanism.
---

# HCMRM: A High-Consistency Multimodal Relevance Model for Search Ads

## Quick Facts
- **arXiv ID**: 2502.05822
- **Source URL**: https://arxiv.org/abs/2502.05822
- **Reference count**: 40
- **Primary result**: Achieves 6.1% reduction in irrelevant ads and 1.4% revenue increase in Kuaishou's search advertising system

## Executive Summary
This paper addresses query-to-video relevance matching in search advertising on short video platforms. The proposed High-Consistency Multimodal Relevance Model (HCMRM) introduces a pseudo-query-video matching mechanism during pre-training and uses hierarchical softmax for ordinal relevance learning during fine-tuning. By extracting keywords from video text as pseudo-queries and learning ordinal relationships among four relevance labels (Bad, Less, Good, Excellent), HCMRM significantly improves both offline metrics (AUC, Spearman correlation) and online performance compared to existing approaches.

## Method Summary
HCMRM builds on ALBEF's dual-encoder architecture with vision encoder (ViT-B/32), text encoder (BERT-base-chinese), and fusion encoder. Pre-training uses four objectives: image-text contrastive learning, image-text matching, word-level masked language modeling, and pseudo-query-video matching (PQVM). PQVM generates synthetic query-video pairs by extracting keywords from video text via TF-IDF and NER. Fine-tuning employs a hierarchical softmax loss with three binary classifiers to learn ordinal relationships among four relevance levels. The model is pre-trained on 200M Chinese short videos and fine-tuned on 2.5M labeled query-video pairs.

## Key Results
- Achieves 13.8% improvement in Spearman correlation over binary cross-entropy baseline
- Reduces irrelevant ads by 6.1% and increases ad revenue by 1.4% in online A/B testing
- Outperforms both single-stream (ViLT) and dual-stream (ALBEF) baselines on offline metrics
- Hard negative sampling via ITC similarity improves ITM and PQVM discrimination

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Query-Video Matching (PQVM) for Pre-training Alignment
Synthetic query-video pairs derived from video text create pre-training objectives structurally consistent with downstream relevance tasks, improving transfer compared to standard image-text alignment. Keywords extracted from video text serve as pseudo-queries while remaining text acts as video context, learning to match (pseudo-query, remaining text, video frames) triplets using binary cross-entropy with hard negative sampling.

### Mechanism 2: Hierarchical Softmax for Ordinal Relevance Learning
Modeling four-level relevance labels as a two-layer symmetric binary tree enables learning both binary discrimination and ordinal ranking simultaneously. Three binary classifiers predict positive/negative, Good/Excellent within positives, and Bad/Less within negatives, with final probability computed as the product along the path.

### Mechanism 3: Hard Negative Sampling via Contrastive Similarity
Sampling hard negatives from in-batch similarity distributions improves ITM and PQVM discrimination more than random negative sampling. ITC produces an N×N similarity matrix, and negatives are drawn via multinomial sampling from similarity distributions, meaning semantically similar but non-matching pairs are more likely selected.

## Foundational Learning

- **Vision-Language Pre-training (ALBEF-style)**: Understanding ITC, ITM, and MLM pre-training objectives is prerequisite to grasping PQVM's incremental contribution. Quick check: Can you explain why ALBEF aligns visual and textual representations before fusion, and what role the momentum model plays in ITC?

- **Ordinal Classification and Regression**: The hierarchical softmax loss assumes ordinal label structure; distinguishing this from multi-class classification or pure regression is essential. Quick check: Why might binary cross-entropy fail to capture the difference between "Bad" and "Less" if they're both mapped to label 0?

- **Contrastive Learning and Hard Negative Mining**: ITC and hard negative sampling underpin both baseline ALBEF performance and PQVM improvements. Quick check: How does sampling negatives proportional to similarity (rather than uniformly) change the gradient signal during training?

## Architecture Onboarding

- **Component map**: Text Encoder (BERT-base-chinese first 6 layers) -> Vision Encoder (ViT-B/32 first 6 layers) -> Fusion Encoder (BERT-base-chinese last 6 layers) -> Pre-training heads (ITC, ITM, MLM, PQVM) -> Fine-tuning heads (3 binary classifiers for hierarchical softmax)

- **Critical path**: Input: Query string + video keyword sequence + video frame patches → Text and vision encoders produce unimodal embeddings → Fusion encoder produces joint [CLS] representation → During pre-training: four objectives with shared hard negative sampling → During fine-tuning: hierarchical softmax produces probabilities p0-p3; relevance score = p1 + 2p2 + 3p3

- **Design tradeoffs**: PQVM adds task-specific pre-training but requires no additional query data; trade-off is synthetic query quality vs. real click data noise. Hierarchical softmax improves ranking (Spearman) at slight AUC cost vs. binary cross-entropy. Dual-stream (ALBEF) vs. single-stream (ViLT): dual-stream aligns modalities better but adds complexity.

- **Failure signatures**: Low Spearman but high AUC: model discriminates positive/negative but fails ordinal ranking—check hierarchical softmax implementation. Pre-training fails to improve downstream: pseudo-queries may be unrepresentative; inspect keyword extraction quality. Hard negatives dominate loss: reduce sampling temperature or increase batch size.

- **First 3 experiments**: 1) Ablate PQVM: Pre-train ALBEF without PQVM, fine-tune with hierarchical softmax; measure gap in AUC/Spearman to quantify PQVM contribution. 2) Pseudo-query quality analysis: Manually inspect 100 random pseudo-queries vs. real user queries; measure lexical and semantic overlap. 3) Loss function comparison: Train HCMRM with binary cross-entropy, ordinal regression, and hierarchical softmax; compare offline metrics (AUC, Spearman) and online A/B test on small traffic slice.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can extensive domain-specific adaptation enable Multimodal Large Language Models (MLLMs) to outperform the specialized HCMRM architecture in relevance matching?
**Basis**: Section 4.7.2 notes MLLMs (e.g., Qwen-VL) currently underperform due to lack of domain-specific pre-training but have "immense potential for domain adaptation training."
**Why unresolved**: Only evaluated via fine-tuning on labeled data, without pre-training on the 200M video corpus used for HCMRM.
**Evidence needed**: Comparative experiments where MLLMs undergo continued pre-training on the 200M video material dataset before fine-tuning.

### Open Question 2
**Question**: How robust is the pseudo-query extraction mechanism when applied to videos with sparse, ambiguous, or low-quality textual metadata?
**Basis**: Methodology relies on assumption that video text contains keywords that "can effectively serve as queries," depending heavily on TF-IDF and NER quality.
**Why unresolved**: Paper does not analyze performance degradation on "long-tail" videos where OCR or ASR might fail or produce irrelevant text.
**Evidence needed**: Ablation studies on video subsets with low text density or high noise-to-signal ratios in the text modality.

### Open Question 3
**Question**: Can the pseudo-query pre-training approach be integrated with debiased user click data to achieve performance superior to either method alone?
**Basis**: Section 4.7.1 demonstrates raw click data is noisy and performs worse than pseudo-queries, but doesn't explore whether high-confidence, denoised behavioral data could complement the synthetic approach.
**Why unresolved**: Comparison pits PQVM against raw clicks, but doesn't explore hybrid pre-training objective using both synthetic queries and rigorously filtered user interactions.
**Evidence needed**: Experiments combining PQVM with click data filtered for high confidence (e.g., high dwell time, conversion) or corrected for position bias.

## Limitations

- **Data Representativeness**: Pseudo-query generation relies on keywords from video text that may not fully capture real user query distributions, particularly when video text contains promotional language
- **Label Hierarchy Assumptions**: Hierarchical softmax assumes balanced binary tree structure where intra-class distinctions are comparable in difficulty to positive/negative distinction, without ablation studies showing alternative structures
- **Offline-to-Online Generalization**: Results from Kuaishou's production system may not generalize to other platforms due to specific traffic conditions, ad inventory, and user behavior patterns

## Confidence

- **High Confidence**: Architectural design and implementation details are clearly specified, following standard practices with 5-fold cross-validation and A/B testing
- **Medium Confidence**: Mechanism explanations are logically coherent but lack direct empirical validation through ablation studies to isolate component contributions
- **Low Confidence**: Generalization to different platforms, query distributions, or ad systems remains uncertain due to proprietary datasets and short video advertising context

## Next Checks

1. **Ablation Study**: Implement controlled experiment comparing HCMRM against ALBEF with hierarchical softmax but without PQVM pre-training to isolate pseudo-query mechanism contribution

2. **Query Quality Analysis**: Conduct systematic comparison between pseudo-queries generated from video text and actual user queries from search logs, measuring lexical overlap, semantic similarity, and query intent alignment

3. **Cross-Platform Validation**: Deploy HCMRM on a different short video platform or advertising system with distinct query patterns and video content, comparing performance metrics to establish platform-specific vs. general improvements