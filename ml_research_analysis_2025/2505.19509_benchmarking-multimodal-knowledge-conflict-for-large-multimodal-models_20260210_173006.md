---
ver: rpa2
title: Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models
arxiv_id: '2505.19509'
source_url: https://arxiv.org/abs/2505.19509
tags:
- knowledge
- conflict
- conflicts
- entity
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MMKC-Bench, a benchmark designed to evaluate\
  \ multimodal knowledge conflicts in large multimodal models (LMMs). The benchmark\
  \ focuses on three types of conflicts\u2014entity recognition, entity knowledge,\
  \ and visual semantic\u2014across context-memory and inter-context scenarios."
---

# Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models

## Quick Facts
- **arXiv ID:** 2505.19509
- **Source URL:** https://arxiv.org/abs/2505.19509
- **Reference count:** 40
- **Primary result:** MMKC-Bench reveals LMMs favor internal parametric knowledge over external evidence in multimodal conflict scenarios, with larger models showing stronger internal knowledge reliance.

## Executive Summary
This paper introduces MMKC-Bench, a comprehensive benchmark designed to evaluate how large multimodal models handle knowledge conflicts across entity recognition, entity knowledge, and visual semantic dimensions. The benchmark covers 1,573 knowledge instances with 3,381 images spanning 23 categories, constructed through automated counterfactual editing with human verification. Experiments on nine LMMs from Qwen2.5-VL, InternVL3, and GPT-4o mini families reveal that models consistently favor internal parametric knowledge over external evidence, are more sensitive to factual knowledge conflicts than perceptual recognition conflicts, and show increasing internal knowledge reliance with model scale. These findings highlight critical limitations in current multimodal RAG systems and point to needed architectural and training innovations.

## Method Summary
MMKC-Bench evaluates multimodal knowledge conflicts through three conflict types (entity recognition, entity knowledge, visual semantic) across two scenarios (context-memory and inter-context conflicts). The benchmark uses counterfactual editing to generate synthetic conflicts, with each instance tested in both MCQ and open-ended formats. Performance is measured using Original Answer Ratio (OAR), Counter Answer Ratio (CAR), and Irrelevant Answer Ratio (IAR) for behavior analysis, plus binary classification accuracy for conflict detection. The evaluation covers nine LMMs across four sizes from Qwen2.5-VL, InternVL3, and GPT-4o mini families, using the VLMEvalKit library on NVIDIA L20/A100 GPUs.

## Key Results
- LMMs show consistent preference for internal parametric knowledge over external evidence across all conflict types and scenarios
- Larger models exhibit stronger internal knowledge reliance, with OAR increasing progressively from 3B to 70B parameters
- LMMs demonstrate higher sensitivity to factual knowledge conflicts compared to perceptual recognition conflicts
- Models can effectively detect conflicts at both coarse-grained (full evidence) and fine-grained (single-sentence) levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LMMs preferentially retrieve from internal parametric knowledge over external evidence in conflict scenarios.
- **Mechanism:** Internal knowledge exhibits higher activation strength during inference due to consolidated representations from pretraining, creating a bias toward parametric responses even when contradictory external context is provided.
- **Core assumption:** Training on isolated image-text pairs limits exposure to multi-source integration patterns that would otherwise strengthen context-following behavior.
- **Evidence anchors:**
  - [abstract] "they tend to favor internal parametric knowledge over external evidence"
  - [section 5.2] "under context-memory conflicts, the average OAR exceeds CAR in all cases (6 out of 6), indicating that LMMs tend to favor internal knowledge"
  - [corpus] "When Seeing Overrides Knowing" examines similar conflicts in VLMs between parametric and external knowledge

### Mechanism 2
- **Claim:** Sensitivity to conflicts varies by knowledge type—higher for factual attributes than perceptual recognition.
- **Mechanism:** Recognition tasks leverage robust vision-language alignment (perception) formed through extensive VQA/captioning training, while factual knowledge requires cognitive reasoning with weaker training coverage, creating differential conflict vulnerability.
- **Core assumption:** Perception and cognition pathways have different training data distributions and architectural reinforcement.
- **Evidence anchors:**
  - [section 5.2] "LMMs show lower OARs on knowledge-related conflicts than recognition-based conflicts, indicating greater sensitivity to factual inconsistencies"
  - [section 5.2] "entity recognition conflicts often show the highest OARs, suggesting LMMs more easily rely on internal knowledge for perception tasks"

### Mechanism 3
- **Claim:** Model scale amplifies internal knowledge reliance across all conflict types.
- **Mechanism:** Larger models encode more extensive parametric knowledge with stronger memorization, increasing retrieval confidence from internal representations and reducing relative influence of external context.
- **Core assumption:** Scale improves knowledge encoding density without proportional improvement in context integration mechanisms.
- **Evidence anchors:**
  - [section 5.2] "the Overall Agreement Rate (OAR) generally increases with model size within the Qwen2.5-VL series"
  - [figures 4,5] OAR improves progressively from 3B→7B→13B→70B across entity recognition, entity knowledge, and visual semantic conflicts

## Foundational Learning

- **Concept:** Context-memory vs. inter-context conflicts
  - **Why needed here:** MMKC-Bench distinguishes these two RAG-relevant conflict types; context-memory involves model parameters vs. external evidence, inter-context involves contradictions among external sources.
  - **Quick check question:** If a model receives two retrieved documents with contradictory facts about the same entity, which conflict type applies?

- **Concept:** Counterfactual editing for benchmark construction
  - **Why needed here:** The benchmark uses systematic entity/attribute substitution to create controlled conflict instances, enabling isolation of specific conflict dimensions.
  - **Quick check question:** How would you generate a counterfactual for "Elon Musk born 1971" while preserving semantic coherence?

- **Concept:** Coarse-grained vs. fine-grained conflict detection
  - **Why needed here:** The benchmark evaluates conflict awareness at two granularities—full evidence (coarse) and single-sentence subsets (fine)—revealing whether models detect conflicts locally or only holistically.
  - **Quick check question:** If a model detects conflict from full documents but fails on individual sentences, what limitation does this indicate?

## Architecture Onboarding

- **Component map:** Input: [Image_i, Text_description_i] × N instances → Conflict Generator (LLM-based counterfactual editing) → Evaluation Suite (MCQ + Open-ended QA) → Metrics Layer (OAR/CAR/IAR for behavior, Accuracy for detection)

- **Critical path:**
  1. Original knowledge collection (entity lists → images → descriptions)
  2. Conflict generation via counterfactual substitution (entity name/attribute/semantic meaning)
  3. Evaluation question generation with 4-option MCQ structure
  4. Human verification filtering (multiple rounds)
  5. Baseline evaluation to establish internal knowledge (non-conflict setting first)

- **Design tradeoffs:**
  - Synthetic conflicts via counterfactual editing vs. natural contradictions (paper acknowledges distribution gap with real-world conflicts)
  - Binary factual mismatches limit complexity vs. nuanced real-world contradictions
  - Multi-image capability requirement restricts model pool (excludes single-image LMMs)

- **Failure signatures:**
  - High IAR in open-ended format (>20%) suggests model confusion or hallucination under conflict
  - Detection accuracy higher in non-conflict than conflict scenarios indicates conservative conflict labeling
  - CAR stagnation despite inter-context evidence increase suggests fixed internal knowledge prioritization

- **First 3 experiments:**
  1. **Baseline establishment:** Run non-conflict QA to determine model's internal knowledge ground truth before conflict evaluation (enables OAR/CAR/IAR calculation).
  2. **Context-memory conflict sweep:** Test across all three conflict types (ER/EK/VS) with single external evidence, comparing MCQ vs. open-ended response patterns.
  3. **Scale ablation:** Compare OAR trends across model sizes (e.g., Qwen2.5-VL 3B/7B/32B/72B) to validate internal knowledge amplification hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the synthetic nature of counterfactual edits in MMKC-Bench affect the generalizability of conflict behavior findings compared to naturally occurring real-world conflicts?
- **Basis in paper:** [Explicit] The authors acknowledge in the Conclusion that "counterfactual editing... inevitably introduces a distribution gap" and explicitly call for future "real-world multimodal knowledge conflict benchmarks."
- **Why unresolved:** Current data is generated via LLM counterfactual editing, which may not capture the nuanced ambiguity or temporal drift found in organic knowledge conflicts.
- **What evidence would resolve it:** Evaluation results from a benchmark constructed from naturally occurring contradictions (e.g., updated Wikipedia entries or news corrections) showing correlation with MMKC-Bench scores.

### Open Question 2
- **Question:** What specific training data characteristics or architectural modifications can shift LMMs from relying on parametric memory to utilizing external contextual evidence in RAG settings?
- **Basis in paper:** [Explicit] The authors observe that LMMs favor internal knowledge unlike LLMs and state this "highlights the need... [for] innovations in training paradigms and model architecture" to incorporate external information.
- **Why unresolved:** The paper identifies the problem (internal bias) and hypothesizes the cause (isolated image-text pair training) but does not propose or test a solution.
- **What evidence would resolve it:** An ablation study training LMMs on multi-source interleaved data and measuring a significant increase in Counter Answer Ratio (CAR) on conflict benchmarks.

### Open Question 3
- **Question:** Why does scaling up model parameters correlate with a stronger reliance on internal parametric knowledge (higher OAR) rather than better conflict resolution?
- **Basis in paper:** [Inferred] The paper observes that "larger models exhibit a stronger promoting effect across all conflict types" (higher OAR), but leaves the underlying mechanism driving this increased rigidity unexplained.
- **Why unresolved:** While the trend is documented, it is unclear if this is due to stronger memorization, overfitting on parametric data, or a lack of scaling in context-integration mechanisms.
- **What evidence would resolve it:** A comparative analysis of internal attention mechanisms in large vs. small models when processing conflicting context, identifying where the suppression of external evidence occurs.

## Limitations
- Benchmark relies on synthetic conflicts generated through counterfactual editing, which may not capture real-world conflict complexity
- Limited evaluation to three model families (Qwen2.5-VL, InternVL3, GPT-4o mini) and specific sizes
- Open-ended format shows high IAR values (>20%), suggesting model confusion that could affect behavioral conclusions
- Paper acknowledges distribution gaps between synthetic conflicts and naturally occurring conflicts

## Confidence
- **High Confidence:** LMMs generally favor internal parametric knowledge over external evidence is well-supported with consistent OAR > CAR patterns across multiple models and conflict types
- **Medium Confidence:** Larger models exhibit stronger reliance on internal knowledge is supported by scale trends within Qwen2.5-VL series, but limited to single model family
- **Medium Confidence:** Differential sensitivity to knowledge-type conflicts is supported by OAR patterns, though underlying mechanism lacks direct empirical validation

## Next Checks
1. Test MMKC-Bench findings on naturally occurring conflicts from real-world multimodal RAG scenarios to assess synthetic vs. authentic conflict performance gaps
2. Extend scale ablation studies beyond Qwen2.5-VL to include diverse architectures to verify if internal knowledge amplification is a general trend
3. Implement conflict-aware training on a subset of models with cognitively demanding examples and measure OAR/CAR changes to validate perception-cognition pathway hypothesis