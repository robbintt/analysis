---
ver: rpa2
title: 'InteractComp: Evaluating Search Agents With Ambiguous Queries'
arxiv_id: '2510.24668'
source_url: https://arxiv.org/abs/2510.24668
tags:
- interaction
- search
- agents
- arxiv
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INTERACTCOMP evaluates search agents on ambiguous queries requiring
  interaction. We design a target-distractor methodology creating questions ambiguous
  without interaction, validated by expert annotators.
---

# InteractComp: Evaluating Search Agents With Ambiguous Queries

## Quick Facts
- **arXiv ID:** 2510.24668
- **Source URL:** https://arxiv.org/abs/2510.24668
- **Reference count:** 40
- **Primary result:** Interaction ability in search agents has stagnated while search performance improved seven-fold over 15 months

## Executive Summary
INTERACTCOMP introduces a benchmark for evaluating search agents on ambiguous queries requiring interaction to resolve. The framework uses a target-distractor methodology where questions are constructed using only shared attributes of a lesser-known target and popular distractor, creating genuine ambiguity that search alone cannot resolve. Expert annotators validate that these questions require interaction for correct answering. Across 210 questions spanning 9 domains and two languages, evaluation of 17 models reveals systematic overconfidence: best model achieves only 13.73% accuracy versus 71.50% with complete context. Forcing interaction doubles accuracy, exposing latent capability. Longitudinal analysis shows interaction abilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot in agent development.

## Method Summary
INTERACTCOMP evaluates search agents using a target-distractor methodology where questions are ambiguous without interaction but verifiable with context. Each instance includes an ambiguous question, hidden context, correct answer (1-2 words), and distractor entity. Agents operate under a ReAct framework with three configurations: answer-only, answer+search, and answer+search+interact, limited to 10 rounds. Actions include search(query), ask(question), and answer(response, confidence). A GPT-4o grader (temp=0.0) evaluates binary correctness comparing predicted vs. ground truth answers. Responder simulation uses GPT-4o (temp=1.0) providing yes/no/unknown responses based on hidden context.

## Key Results
- Interaction improves accuracy dramatically: 6.74-9.52% (search-only) vs 40.93-71.50% (with context)
- Best model achieves only 13.73% accuracy vs 71.50% with complete context, exposing systematic overconfidence
- Forcing interaction doubles accuracy (GPT-5: 20% → 40%), confirming latent capability blocked by strategy
- Interaction abilities stagnated over 15 months while search performance improved seven-fold

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Systematic overconfidence prevents models from engaging available interaction mechanisms, not capability deficits.
- **Mechanism:** Models prematurely conclude they have sufficient information and commit to answers without questioning, despite having access to interaction. This strategic failure blocks access to latent disambiguation capability.
- **Core assumption:** Models possess underlying reasoning capability but fail to recognize when information is incomplete.
- **Evidence anchors:**
  - [abstract] "best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits"
  - [section 4.5] "GPT-5 doubles its accuracy from 20% to 40% when compelled to ask 8 questions, confirming strong reasoning capabilities hindered by voluntary underuse of interaction"
  - [corpus] ECLAIR paper addresses clarification for ambiguous queries but focuses on enterprise assistants, not search agents specifically

### Mechanism 2
- **Claim:** Target-distractor pairing creates genuine ambiguity that search alone cannot resolve, validating interaction necessity.
- **Mechanism:** Questions constructed using only shared attributes of a lesser-known target and popular distractor create plausible alternative interpretations. Distinguishing attributes are hidden in context, accessible only through interaction.
- **Core assumption:** Web search results will surface the popular distractor, making direct retrieval unreliable.
- **Evidence anchors:**
  - [section 3.2] "questions use only shared attributes of a lesser-known target and a popular alternative, creating genuine ambiguity that search alone cannot resolve"
  - [section 4.3] Search-only accuracy (6.74-9.52%) vs with-context accuracy (40.93-71.50%) validates interaction as the bottleneck
  - [corpus] AmbiGraph-Eval similarly uses ambiguity in graph queries; Ask-to-Clarify addresses instruction ambiguity through dialogue

### Mechanism 3
- **Claim:** Interaction behavior correlates with calibration quality—models that question more develop better uncertainty awareness.
- **Mechanism:** The act of formulating clarifying questions forces models to confront knowledge gaps, improving confidence calibration even when questioning is not optimally targeted.
- **Core assumption:** Calibration improvements reflect genuine uncertainty awareness, not just behavioral patterns.
- **Evidence anchors:**
  - [section 4.2] "GPT-4o-mini's aggressive questioning strategy... results in dramatically better calibration (37.44 CE) compared to low-interaction models like Doubao-1.6 (84.35 CE)"
  - [section 4.2] "interaction, even when not optimally targeted, helps models develop more realistic confidence assessments"
  - [corpus] Corpus papers focus on ambiguity resolution mechanisms but do not address calibration-interaction relationship

## Foundational Learning

- **Concept: ReAct Framework (Reasoning + Acting)**
  - Why needed here: Base architecture for agents that interleave thinking with actions (search, interact, answer)
  - Quick check question: Can you explain why ReAct enables step-by-step information gathering rather than immediate answering?

- **Concept: Calibration Error**
  - Why needed here: Measures alignment between model confidence and actual accuracy; reveals overconfidence patterns
  - Quick check question: If a model reports 90% confidence but achieves 50% accuracy, what is the calibration problem?

- **Concept: Target-Distractor Methodology**
  - Why needed here: Core construction principle ensuring questions are ambiguous without interaction but verifiable with context
  - Quick check question: How does using shared vs. distinctive attributes create controlled ambiguity?

## Architecture Onboarding

- **Component map:** Question → Agent searches → Agent asks clarifying questions → Responder answers from context → Agent synthesizes → Final answer → Grader evaluates
- **Critical path:** Question → Agent searches → Agent asks clarifying questions → Responder answers from context → Agent synthesizes → Final answer → Grader evaluates
- **Design tradeoffs:**
  - Yes/no questions limit information gain per turn but ensure consistent responder behavior
  - 10-round limit balances evaluation cost with sufficient interaction opportunity
  - GPT-4o grader introduces potential evaluation variance but enables automated scaling
- **Failure signatures:**
  - High interaction rate with low accuracy: questioning without purpose (e.g., GPT-4o-mini: 73.95% IR, 7.13% accuracy)
  - Low interaction rate with high confidence: overconfident premature commitment
  - Accuracy improves under forced interaction but not voluntarily: latent capability blocked by strategy
- **First 3 experiments:**
  1. Run baseline evaluation comparing answer-only vs. search-only vs. full-interaction modes to replicate the 5× performance gap
  2. Test forced interaction thresholds (2, 4, 8 questions) to measure latent capability vs. strategic failure
  3. Analyze calibration error across interaction rates to validate the uncertainty-awareness mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reinforcement learning with verifiable rewards (RLVR) effectively train models to overcome systematic overconfidence and engage in proactive interaction?
- **Basis in paper:** [explicit] The conclusion states "the grounded nature of search provides clean reward signals for training, making INTERACTCOMP well-suited for reinforcement learning approaches to develop uncertainty-aware, actively interactive agents."
- **Why unresolved:** The paper demonstrates the problem (overconfidence preventing interaction despite latent capability) and suggests RLVR as a solution, but does not implement or evaluate any training methods.
- **What evidence would resolve it:** Training models using INTERACTCOMP's reward signals and measuring improvement in voluntary interaction rates and accuracy without forced interaction protocols.

### Open Question 2
- **Question:** What specific question-asking strategies distinguish effective from ineffective interaction?
- **Basis in paper:** [inferred] GPT-4o-mini asks questions in 73.95% of rounds yet achieves only 7.14% accuracy, while DeepSeek-R1 achieves 13.08% with 44.72% interaction rate, suggesting question quality—not quantity—determines success.
- **Why unresolved:** The paper reports interaction rates and accuracy correlations but does not analyze the content, sequencing, or information-theoretic value of questions asked.
- **What evidence would resolve it:** Qualitative analysis of question types across high/low-performing models, measuring information gain per question and strategic sequencing patterns.

### Open Question 3
- **Question:** Why do models fail to scale interaction usage despite having more opportunities, and what architectural or training changes would enable adaptive interaction?
- **Basis in paper:** [explicit] "Despite quadrupling round limits from 5 to 20, GPT-5 increases interactions from just 1.14 to 1.90, while Claude-Sonnet-4 barely reaches 0.78 interactions per instance."
- **Why unresolved:** The paper identifies systematic overconfidence as the bottleneck but does not investigate whether this stems from training objectives, reward modeling, or architectural constraints.
- **What evidence would resolve it:** Ablation studies varying training objectives (e.g., adding uncertainty penalties), comparing models fine-tuned on interaction vs. standard instruction tuning.

## Limitations

- Evaluation framework relies heavily on GPT-4o for both responder simulation and grading, introducing potential evaluation bias
- 210-question benchmark represents a relatively small sample across nine domains, raising questions about coverage and representativeness
- Several state-of-the-art models (GPT-5, Grok-4, Claude-Opus-4) were evaluated but may not be widely accessible for independent validation

## Confidence

- **High Confidence:** The core finding that interaction improves accuracy significantly (6.74-9.52% → 40.93-71.50%) is well-supported by systematic evaluation across 17 models. The performance gap between current capabilities and complete context accuracy is empirically robust.
- **Medium Confidence:** The interpretation that systematic overconfidence rather than capability deficits explains poor voluntary interaction performance is plausible but requires further investigation. Alternative explanations include architectural limitations or interaction mechanics.
- **Medium Confidence:** The claim that interaction abilities have stagnated while search performance improved seven-fold over 15 months is based on longitudinal comparisons but may be influenced by evolving model access and evaluation conditions.

## Next Checks

1. Replicate the forced interaction experiment (compelling models to ask 8 questions) across a broader model distribution to verify that latent capability consistently emerges under constraint.
2. Evaluate the same benchmark using multiple independent grader models to assess whether GPT-4o grader bias affects accuracy and calibration measurements.
3. Conduct ablation studies varying question ambiguity levels and shared attribute counts to test the boundary conditions of the target-distractor methodology's effectiveness.