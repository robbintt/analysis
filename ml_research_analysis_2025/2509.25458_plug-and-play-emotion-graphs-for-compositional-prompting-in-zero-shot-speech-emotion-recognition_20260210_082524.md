---
ver: rpa2
title: Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech
  Emotion Recognition
arxiv_id: '2509.25458'
source_url: https://arxiv.org/abs/2509.25458
tags:
- emotion
- speech
- reasoning
- acoustic
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of zero-shot speech emotion recognition
  (SER) with large audio-language models (LALMs), which struggle due to weak paralinguistic
  modeling and limited cross-modal reasoning. To overcome this, the authors propose
  Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a fine-tuning-free
  framework that leverages structured Emotion Graphs (EGs) to guide LALMs in emotion
  inference.
---

# Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2509.25458
- Source URL: https://arxiv.org/abs/2509.25458
- Reference count: 0
- The paper proposes CCoT-Emo, a fine-tuning-free framework that leverages structured Emotion Graphs to guide large audio-language models in zero-shot speech emotion recognition, achieving significant improvements over prior state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of zero-shot speech emotion recognition (SER) with large audio-language models (LALMs), which struggle due to weak paralinguistic modeling and limited cross-modal reasoning. To overcome this, the authors propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a fine-tuning-free framework that leverages structured Emotion Graphs (EGs) to guide LALMs in emotion inference. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations, providing interpretable and compositional representations. Experiments across five SER benchmarks show that CCoT-Emo consistently outperforms prior state-of-the-art methods and zero-shot baselines.

## Method Summary
The proposed CCoT-Emo framework is a fine-tuning-free approach for zero-shot speech emotion recognition that uses structured Emotion Graphs (EGs) to guide large audio-language models (LALMs). Each EG encodes seven acoustic features (pitch, speech rate, jitter, shimmer, etc.), textual sentiment, keywords, and cross-modal associations. The framework performs compositional reasoning by first extracting intermediate emotion-related features, then prompting the LALM to reason about these features in sequence, leading to more accurate emotion classification. The approach is evaluated across five SER benchmarks, showing consistent improvements over prior state-of-the-art methods and zero-shot baselines.

## Key Results
- Kimi-Audio with CCoT-Emo achieves an average accuracy of 67.7%, surpassing vanilla prompting by 7.2% and zero-shot Chain-of-Thought by 9.4%.
- On MERBench test1 and test2, gains of 8.5% and 7.5% are observed, respectively, demonstrating strong generalization to domain-shifted and long-form speech.
- The framework consistently outperforms prior state-of-the-art methods and zero-shot baselines across five SER benchmarks.

## Why This Works (Mechanism)
The proposed framework leverages structured Emotion Graphs (EGs) to encode seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. This structured representation guides large audio-language models (LALMs) in compositional reasoning by providing interpretable intermediate representations that bridge the gap between raw audio/text and emotion inference. The compositional Chain-of-Thought prompting approach allows LALMs to reason step-by-step through these intermediate representations, overcoming their limitations in paralinguistic modeling and cross-modal reasoning. This structured intermediate reasoning process enables more accurate emotion classification in zero-shot settings compared to traditional prompting approaches.

## Foundational Learning
- **Speech Emotion Recognition (SER)**: The task of automatically identifying emotions from speech signals. Needed because understanding human emotion from speech is crucial for human-computer interaction and requires models that can generalize across diverse emotional expressions and speaker characteristics.
- **Large Audio-Language Models (LALMs)**: Multimodal models that process both audio and text inputs. Needed because they offer the potential for zero-shot reasoning across modalities but struggle with paralinguistic features and cross-modal associations in emotion recognition.
- **Chain-of-Thought Prompting**: A prompting technique that guides models through intermediate reasoning steps. Needed because it helps LALMs overcome their limitations in complex reasoning tasks by breaking down the inference process into manageable steps.
- **Emotion Graphs (EGs)**: Structured representations encoding acoustic features, textual sentiment, keywords, and cross-modal associations. Needed because they provide interpretable intermediate representations that bridge the gap between raw inputs and emotion inference.
- **Compositional Reasoning**: The ability to combine multiple intermediate representations to reach a final conclusion. Needed because emotion recognition requires integrating multiple acoustic and textual cues in a structured manner.
- **Zero-shot Learning**: A learning paradigm where models make predictions on unseen classes without explicit training examples. Needed because it enables emotion recognition across diverse emotional categories without requiring labeled data for each emotion.

## Architecture Onboarding

**Component Map**: Emotion Graphs (acoustic features, textual sentiment, keywords, cross-modal associations) -> Compositional Chain-of-Thought Prompting -> Large Audio-Language Models (LALMs) -> Emotion Classification

**Critical Path**: Emotion Graph extraction -> Compositional reasoning prompt generation -> LALM inference -> Emotion classification

**Design Tradeoffs**: 
- Structured Emotion Graphs provide interpretability and compositional reasoning but require manual annotation, limiting scalability
- Fine-tuning-free approach avoids computational costs of model adaptation but depends on LALM capabilities
- Cross-modal associations based on predefined heuristics may not generalize to culturally diverse speech patterns

**Failure Signatures**: 
- Poor performance on highly accented or non-standard emotional expressions
- Degradation when Emotion Graph components are missing or noisy
- Limited generalization to emotions not well-represented in training Emotion Graphs

**Three First Experiments**:
1. Evaluate CCoT-Emo performance on a held-out test set with diverse speaker demographics and emotional expressions
2. Compare CCoT-Emo against traditional fine-tuned SER models on benchmark datasets
3. Test the framework's robustness to noisy or incomplete Emotion Graph inputs

## Open Questions the Paper Calls Out
None

## Limitations
- The framework depends on structured Emotion Graphs, which require manual annotation of acoustic and textual features, potentially limiting scalability and introducing annotation bias.
- Evaluation is primarily benchmark-focused, with limited real-world deployment or stress testing on highly diverse emotional expressions and speaker variability.
- Cross-modal associations in the Emotion Graphs are based on predefined linguistic heuristics, which may not generalize well to culturally diverse speech patterns.

## Confidence
- **High confidence**: The framework architecture (CCoT-Emo) and its core design principles (Emotion Graphs, compositional prompting) are well-described and reproducible. The reported performance improvements over baseline zero-shot methods are substantial and align with expectations for structured intermediate reasoning in LALMs.
- **Medium confidence**: The generalizability of results to out-of-domain emotional speech (e.g., highly accented or non-standard emotional expressions) is plausible but not fully validated. The impact of Emotion Graph granularity on downstream performance is inferred but not empirically tested.
- **Low confidence**: The long-term stability and robustness of the Emotion Graphs when applied to dynamic, real-time emotion inference tasks, and the extent to which improvements are due to the Emotion Graph structure versus Chain-of-Thought prompting in general.

## Next Checks
1. Conduct ablation studies removing individual Emotion Graph components (e.g., pitch, shimmer, textual sentiment) to isolate their contributions to performance gains.
2. Test the framework on a more diverse, out-of-distribution dataset with varied speaker demographics and cultural contexts to assess robustness.
3. Perform statistical significance tests (e.g., paired t-tests) across all benchmark comparisons to formally establish the reliability of reported improvements.