---
ver: rpa2
title: 'NarrativeTrack: Evaluating Video Language Models Beyond the Frame'
arxiv_id: '2601.01095'
source_url: https://arxiv.org/abs/2601.01095
tags:
- person
- action
- outfit
- scene
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NARRATIVETRACK is a benchmark that evaluates narrative understanding\
  \ in multimodal language models through entity-centric reasoning. It introduces\
  \ a compositional reasoning progression across three dimensions\u2014entity existence,\
  \ changes, and ambiguity\u2014to systematically assess how models track entities\
  \ over time."
---

# NarrativeTrack: Evaluating Video Language Models Beyond the Frame

## Quick Facts
- **arXiv ID:** 2601.01095
- **Source URL:** https://arxiv.org/abs/2601.01095
- **Reference count:** 40
- **Key outcome:** A benchmark that evaluates narrative understanding in multimodal language models through entity-centric reasoning, revealing a fundamental trade-off between perceptual grounding and temporal coherence.

## Executive Summary
NARRATIVETRACK is a benchmark that evaluates narrative understanding in multimodal language models through entity-centric reasoning. It introduces a compositional reasoning progression across three dimensions—entity existence, changes, and ambiguity—to systematically assess how models track entities over time. Using a fully automated pipeline, it extracts temporally grounded entity representations without manual annotation. Evaluation of state-of-the-art models reveals that current models fail to maintain coherent entity representations, with open-source general-purpose models excelling in perceptual grounding but lacking temporal coherence, while video-specific models capture temporal context but hallucinate under ambiguity. These findings expose a fundamental trade-off between perceptual grounding and temporal reasoning, showing that narrative understanding requires their integration.

## Method Summary
The benchmark uses an automated pipeline with three stages: (1) Entity Detection via Detectron2 + Owlv2 ensemble with IoU ≥ 0.5 fusion, (2) Entity Tracking via osnet_x1_0 ReID clustering and face recognition with Gemini majority voting, and (3) Contextual Recognition via Gemini-2.5-Pro. QA pairs are generated from templates aligned with the Compositional Reasoning Progression (CRP) framework and filtered using GPT-4o. The benchmark evaluates models across 1,006 QA pairs spanning binary, multiple-choice, and ordering formats, testing entity existence, changes (action/outfit/scene), and ambiguity across forward, backward, and agnostic temporal directions.

## Key Results
- Open-source general-purpose MLLMs excel at perceptual grounding but lack temporal coherence
- Video-specific MLLMs capture temporal context better but hallucinate under ambiguity
- Models show strong forward bias, performing up to 20% worse on backward reasoning tasks
- Performance peaks at 20 sampled frames and degrades beyond this density

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositional Reasoning Progression (CRP) systematically diagnoses where models fail in entity-centric narrative understanding.
- Mechanism: CRP decomposes narrative understanding into three escalating reasoning levels: entity existence (basic temporal continuity), entity changes (integrating perceptual grounding with temporal reasoning), and entity ambiguity (fine-grained disambiguation under visual uncertainty). This progression isolates specific failure modes rather than testing narrative comprehension as a monolithic capability.
- Core assumption: Narrative understanding is compositional, built from separable temporal and perceptual reasoning skills that can be independently evaluated.
- Evidence anchors:
  - [abstract] "a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions"
  - [section 3.2] "This progression transforms narrative reasoning from single-skill testing into a compositional diagnostic, enabling a systematic evaluation"
  - [corpus] Related work on temporal reasoning benchmarks confirms systematic evaluation is needed but doesn't address entity-centric focus specifically
- Break condition: If narrative understanding requires integrated temporal-perceptual reasoning that cannot be decomposed, CRP's incremental testing would miss interaction effects between levels.

### Mechanism 2
- Claim: The automated entity-centric pipeline enables scalable benchmark construction while maintaining annotation quality comparable to human labeling.
- Mechanism: The pipeline combines off-the-shelf detectors (Detectron2, Owlv2) with spatial consensus fusion, ReID-based clustering for main character identification, and MLLM-based contextual recognition. Ensemble verification with majority voting suppresses identity drift. Human evaluation shows 96.08% agreement with model-based voting, validating the unsupervised approach.
- Core assumption: Pre-trained detection, ReID, and MLLM models can be composed without cascading errors that would undermine entity representation quality.
- Evidence anchors:
  - [section 3.1] "ensemble detection improves recall from 0.780 (Detectron2 alone) to 0.848"
  - [section 3.1] "human-majority vote labels with predictions of our model-based majority voting method... agree on 96.08% of cases"
  - [corpus] Corpus shows related work on automated video annotation but limited evidence on entity-centric pipeline validation
- Break condition: If domain shift between pre-training data and benchmark videos causes systematic detector failures, entity representations would be unreliable.

### Mechanism 3
- Claim: The benchmark reveals a fundamental trade-off in current MLLMs between perceptual grounding and temporal coherence.
- Mechanism: Open-source general-purpose MLLMs (OGP-MLLMs) show strong perceptual grounding (performing best on "disappear" cases relying on static cues) but weak temporal coherence. Video-specific MLLMs (OVS-MLLMs) capture temporal continuity better (higher accuracy on "reappear" cases) but hallucinate entity contexts. This trade-off emerges from architectural and training differences between model families.
- Core assumption: The trade-off reflects fundamental architectural limitations rather than insufficient scale or training data.
- Evidence anchors:
  - [abstract] "open-source general-purpose models excelling in perceptual grounding but lacking temporal coherence, while video-specific models capture temporal context but hallucinate under ambiguity"
  - [section 4.2.2] "OGP-MLLMs perform best on disappear cases, suggesting reliance on static visual evidence... OVS-MLLMs achieve higher accuracy on reappear cases"
  - [corpus] Related work on video MLLMs shows temporal reasoning challenges but doesn't explicitly document this trade-off
- Break condition: If scaling model size or training data resolves this trade-off, it would indicate insufficient capability rather than architectural constraints.

## Foundational Learning

- Concept: Entity-centric reasoning
  - Why needed here: The entire benchmark is built around tracking entities (who/what/when/where) rather than scene-level semantics. Without understanding that entities serve as the basic units of narrative structure, the CRP framework won't make sense.
  - Quick check question: Can you explain why tracking "the person in the red jacket across three scene changes" requires different capabilities than summarizing "a video about a cooking show"?

- Concept: Temporal grounding vs. perceptual grounding
  - Why needed here: The paper's key finding is that these are competing capabilities in current models. Understanding this distinction is essential for interpreting results and designing improvements.
  - Quick check question: When a model correctly identifies an outfit but fails to recognize the person wearing it reappears later, which capability is failing?

- Concept: Forward bias in temporal reasoning
  - Why needed here: The benchmark reveals models perform 9-21% worse on backward reasoning (tracking from end to start). This directional bias parallels the "reversal curse" in LLMs and has architectural implications.
  - Quick check question: Why would a model trained with left-to-right decoding struggle to answer "What was the person doing before this scene?"

## Architecture Onboarding

- Component map: Automated Pipeline: Entity Detection -> Entity Tracking -> Contextual Recognition -> QA Generator -> Evaluation Framework

- Critical path: Start with Entity Detection ensemble (Detectron2 + Owlv2 with IoU≥0.5 fusion). This feeds Entity Tracking via ReID clustering and face recognition. Output trajectories go to Contextual Recognition (Gemini-2.5-Pro with highlighted crops). QA generation uses templates (Tables 4-8) with GPT-4o filtering. Always validate on AVA dataset subset first (recall check).

- Design tradeoffs: (1) Ensemble detection improves recall (+6.8%) but increases inference cost. (2) Using MLLM for contextual recognition leverages strong models but introduces dependency on proprietary systems. (3) 20-frame sampling optimizes accuracy but may miss fine-grained transitions in fast-paced videos. (4) Automated pipeline enables scalability but 30% of QA pairs require manual verification for distractor quality.

- Failure signatures: (1) If entity detection recall drops below 0.80, trajectories fragment → existence questions become unreliable. (2) If face recognition fails on occluded faces, identity drift increases → ambiguity questions become noisy. (3) If contextual recognition uses imprecise descriptions, outfit/scene change questions become ambiguous. (4) If synthetic distractors are too obvious, models solve via elimination rather than reasoning.

- First 3 experiments:
  1. Pipeline validation: Run detection ensemble on 10 AVA clips, compare recall against Detectron2-only baseline. Target: recall improvement ≥5% without excessive false positives.
  2. Entity tracking accuracy: Manually verify 50 tracked trajectories against ground truth. Calculate identity consistency rate. Target: ≥95% agreement with human majority vote.
  3. QA quality check: Sample 30 generated questions across all CRP levels, have 3 annotators rate validity. Target: Fleiss' κ ≥0.7 and ≥65% unanimous "keep" ratings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can bidirectional temporal modeling or contrastive reversal objectives effectively eliminate the forward directional bias in MLLMs' entity tracking?
- Basis in paper: [explicit] The paper states that models exhibit a strong temporal directional bias (up to 20.65% gap between forward and backward reasoning) and explicitly proposes that "addressing this limitation may require bidirectional temporal modeling or contrastive reversal objectives that explicitly enforce symmetry between forward and backward temporal reasoning over entity states."
- Why unresolved: Current MLLMs inherit left-to-right decoding from LLM backbones, binding entities to sequentially observed events without invertible mappings between earlier and later states.
- What evidence would resolve it: Training models with explicit reversal objectives and evaluating whether forward-backward accuracy gaps on NARRATIVETRACK disappear while maintaining overall performance.

### Open Question 2
- Question: How can entity-centric training objectives be designed to improve temporal coherence without sacrificing perceptual grounding?
- Basis in paper: [explicit] The authors repeatedly mention entity-centric training objectives as future work and conclude that "scaling trends further reveal that larger models generally perform better... though gains are inconsistent... highlighting the need for entity-centric learning."
- Why unresolved: The paper reveals a fundamental trade-off: OGP-MLLMs excel at perceptual grounding but lack temporal coherence, while OVS-MLLMs capture temporal context but hallucinate under ambiguity.
- What evidence would resolve it: Designing and testing novel training objectives that jointly optimize entity identity consistency across frames and fine-grained perceptual discrimination, then measuring improvements across all three CRP dimensions.

### Open Question 3
- Question: Would memory-augmented or recurrent architectures enable models to maintain coherent entity representations at higher frame densities beyond the observed 20-frame peak?
- Basis in paper: [inferred] The paper shows performance degrades beyond 20 frames and suggests in supplementary material that "memory-augmented or recurrent architectures may be required to mitigate weaknesses in reverse and bidirectional reasoning" while also noting "current MLLMs struggle to integrate dense temporal information effectively, lacking mechanisms for selective attention and long-term temporal coherence."
- Why unresolved: Simply increasing frame density adds redundant information without improving entity-centric performance, indicating architectural rather than input-scale limitations.
- What evidence would resolve it: Implementing memory-augmented architectures (e.g., external entity state banks) and testing whether performance scales monotonically with frame density on NARRATIVETRACK's ordering and ambiguity tasks.

## Limitations
- The automated pipeline depends on pre-trained models that may not generalize across diverse video domains
- The benchmark's focus on entity-centric reasoning may not fully capture broader narrative understanding
- Specific Gemini model variants for verification are not disclosed, creating reproducibility gaps

## Confidence
- **High Confidence**: The compositional progression framework and automated pipeline methodology are well-specified and validated through multiple experiments
- **Medium Confidence**: The trade-off between perceptual grounding and temporal coherence is demonstrated, but could be influenced by model scale or training data rather than fundamental architectural constraints
- **Low Confidence**: Generalization of findings beyond the tested datasets (AVA, Video-MME, LVBench) and model families (open-source vs. video-specific MLLMs)

## Next Checks
1. **Cross-Domain Generalization**: Test the benchmark on videos from domains outside the training distributions of the entity detection and tracking models (e.g., animated content, surveillance footage) to assess pipeline robustness.
2. **Architectural Intervention Study**: Evaluate whether architectural modifications (e.g., bidirectional attention, memory mechanisms) can reduce the forward bias in temporal reasoning and mitigate the perceptual-temporal trade-off.
3. **Fine-grained Ambiguity Analysis**: Conduct error analysis on the 20% of ambiguity questions where models fail, categorizing failure modes (hallucination vs. uncertainty handling vs. resolution) to inform targeted model improvements.