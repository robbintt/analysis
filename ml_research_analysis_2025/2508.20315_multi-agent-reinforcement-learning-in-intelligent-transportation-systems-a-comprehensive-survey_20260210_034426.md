---
ver: rpa2
title: 'Multi-Agent Reinforcement Learning in Intelligent Transportation Systems:
  A Comprehensive Survey'
arxiv_id: '2508.20315'
source_url: https://arxiv.org/abs/2508.20315
tags:
- learning
- reinforcement
- multi-agent
- marl
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of Multi-Agent Reinforcement
  Learning (MARL) applications in Intelligent Transportation Systems (ITS). It addresses
  the challenge of autonomous decision-making in dynamic, large-scale transportation
  environments by introducing a structured taxonomy categorizing MARL approaches based
  on coordination models and learning algorithms.
---

# Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey

## Quick Facts
- **arXiv ID:** 2508.20315
- **Source URL:** https://arxiv.org/abs/2508.20315
- **Reference count:** 40
- **Primary result:** Systematic survey categorizing MARL approaches for ITS with taxonomy, challenges, and future directions

## Executive Summary
This paper provides a comprehensive survey of Multi-Agent Reinforcement Learning (MARL) applications in Intelligent Transportation Systems (ITS). It addresses the challenge of autonomous decision-making in dynamic, large-scale transportation environments by introducing a structured taxonomy categorizing MARL approaches based on coordination models and learning algorithms. The survey covers applications in traffic signal control, connected and autonomous vehicle coordination, logistics optimization, and mobility-on-demand systems. It reviews widely used simulation platforms like SUMO, CARLA, and CityFlow for MARL experimentation and identifies core challenges including scalability, non-stationarity, credit assignment, communication constraints, and sim-to-real transfer gaps. The paper outlines future research opportunities emphasizing federated learning, safety-aware policy design, robust communication protocols, and integration with edge computing, providing a roadmap for advancing MARL toward practical ITS solutions.

## Method Summary
The paper conducts a systematic survey of MARL applications in ITS, reviewing coordination models (CTCE, CTDE, DTDE), algorithms (VDN, QMIX, MADDPG, MAPPO), and simulation platforms (SUMO, CARLA, CityFlow). It synthesizes challenges across scalability, non-stationarity, credit assignment, communication constraints, and sim-to-real transfer gaps. The methodology involves analyzing existing literature to construct a taxonomy, identifying common architectural patterns, and mapping algorithmic approaches to specific ITS domains. The survey structure follows a logical progression from problem formulation through mechanism analysis to future research directions, providing a comprehensive reference for researchers entering the field.

## Key Results
- CTDE coordination models dominate MARL in ITS due to their ability to stabilize learning while maintaining decentralized execution
- Value decomposition algorithms (VDN, QMIX) provide efficient credit assignment in cooperative traffic coordination tasks
- Communication-augmented learning can mitigate partial observability but assumes idealized communication conditions rarely met in real transportation networks
- Sim-to-real transfer gaps remain a fundamental barrier to real-world deployment despite advances in simulation platforms
- Scalability challenges persist as agent count increases, with exponential growth in joint state-action space making centralized control computationally infeasible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Centralized training with decentralized execution (CTDE) stabilizes learning in non-stationary environments by providing a stable global view during policy optimization.
- Mechanism: During training, a "centralized critic" observes the global state (s_t) and the joint actions of all agents (a_t). This transforms the non-stationary multi-agent problem (where other agents appear as a changing environment) into a stationary supervised learning problem for the critic. During execution, the "actor" uses only local observations (o_i) to act.
- Core assumption: The global state information is fully available and accurate during the training phase, and the local observations during execution are sufficient for the policy to select optimal actions.
- Evidence anchors:
  - [Section III-A-2] States that CTDE is dominant because centralized critics "help stabilize learning and improve credit assignment" while agents operate independently later.
  - [Section III-B-3] Describes MADDPG as using centralized critics to account for the influence of other agents, mitigating non-stationarity.
  - [Corpus] Paper "Robust and Efficient Communication..." notes that MARL approaches often assume ideal communication conditions, highlighting the reliance on data availability for this mechanism.
- Break condition: If the environment scale exceeds the capacity of the centralized critic to process global state vectors efficiently, or if the training environment differs significantly from execution (sim-to-real gap), this mechanism may fail to generalize.

### Mechanism 2
- Claim: Value decomposition enables efficient credit assignment in cooperative tasks by factorizing the global reward signal.
- Mechanism: Instead of learning a massive joint Q-function directly, algorithms like VDN or QMIX decompose the global value (Q_tot) into individual agent utilities (Q_i). By enforcing monotonicity (in QMIX) or additivity (in VDN), the mechanism ensures that maximizing an agent's local utility also maximizes the global team objective.
- Core assumption: The team objective can be accurately represented by the sum or monotonic mixing of individual value functions, implying that agents' contributions to the global reward are factorable.
- Evidence anchors:
  - [Section III-B-1] Explains that VDN approximates the joint Q-function as a sum of individual Q-values to drive "collective behavior."
  - [Section III-B-2] Notes QMIX extends this using a mixing network to ensure that selecting actions to maximize Q_i also maximizes Q_tot.
- Break condition: If the task requires complex non-monotonic coordination where an agent's best action depends counter-intuitively on suppressing the global reward in the short term, strict decomposition may limit optimal coordination.

### Mechanism 3
- Claim: Communication-augmented learning mitigates partial observability by creating shared representations of hidden states.
- Mechanism: In architectures like CommNet, agents transmit continuous vectors (hidden states) to a shared communication channel. By averaging or processing these vectors, agents gain insight into teammates' internal states or intentions, effectively widening the effective field of view without explicit state-space expansion.
- Core assumption: Agents have the bandwidth and low latency required to transmit these continuous vectors in real-time, and the "message" contains useful latent information about intent.
- Evidence anchors:
  - [Section III-B-8] Describes CommNet as enabling agents to "share information via continuous vectors" to select coordinated actions.
  - [Corpus] "Wireless Communication as an Information Sensor..." supports this by discussing V2X as a dynamic sensor, though it warns of communication constraints like limited bandwidth.
- Break condition: If communication latency exceeds the decision horizon or if bandwidth constraints force excessive quantization, the shared representations may become stale or noisy, degrading performance.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) & Partial Observability (POMDP)**
  - Why needed here: MARL in ITS operates on the assumption that the environment (traffic) has states (vehicle positions, signal phases). Understanding the difference between observing the full state vs. a partial local observation is critical for selecting CTDE vs. DTDE architectures.
  - Quick check question: Can your traffic signal agent see the queue lengths of all intersections (MDP/Global State) or only its own lanes (POMDP/Local Observation)?

- Concept: **The Credit Assignment Problem**
  - Why needed here: In a traffic network, if congestion drops, which of the 50 traffic lights gets the credit? Without solving this, agents may converge to lazy or conflicting behaviors. Value decomposition (Mechanism 2) is the primary solution discussed.
  - Quick check question: If a team of agents receives a single scalar reward (+1 for green wave), how does Agent 3 know if its specific action contributed to that +1?

- Concept: **Non-Stationarity**
  - Why needed here: In single-agent RL, the environment is static. In MARL, "the environment" includes other learning agents who change their policies constantly. This violates the Markov assumption and causes learning to diverge without specific mechanisms (like CTDE).
  - Quick check question: Why does Q-learning fail in a multi-agent intersection if every car is simultaneously learning to change lanes?

## Architecture Onboarding

- Component map:
  - Simulator (Environment) -> Agent Wrapper -> Policy Network (Actor) -> Action Selection -> Environment
  - Centralized Critic -> Agent Networks -> Replay Buffer -> Experience Collection

- Critical path:
  1. Define state/observation space (e.g., queue length, position)
  2. Define action space (e.g., discrete phase selection, continuous acceleration)
  3. Select Taxonomy: Choose CTDE for coordination (e.g., MAPPO/QMIX)
  4. Design Reward Function: Global throughput vs. local wait time
  5. Training Loop: Centralized critic updates -> Decentralized actor updates

- Design tradeoffs:
  - **Scalability vs. Stability:** CTCE (Centralized Training/Centralized Execution) offers stability but fails to scale; CTDE scales better but requires careful critic design
  - **Homogeneity vs. Heterogeneity:** Parameter sharing (PS-TRPO) speeds up training for identical agents (traffic lights) but fails for mixed agents (trucks vs. cars)

- Failure signatures:
  - **Relative Overgeneralization:** Agents converge to a policy that is robust to teammates' poor actions but suboptimal if teammates were smart (stuck in local optima)
  - **The "Lazy Agent" Problem:** In cooperative tasks with global rewards, some agents stop acting because the gradient implies they don't need to contribute (addressed by QMIX)
  - **Catastrophic Forgetting:** Policy degrades abruptly when traffic patterns shift (e.g., rush hour to night), requiring continual learning approaches

- First 3 experiments:
  1. **Baseline Single-Agent:** Run a standard DQN on a single intersection to establish a performance baseline for local control
  2. **Independent MARL (DTDE):** Deploy independent DQNs on a 4-intersection grid without shared critics. Observe the instability/non-stationarity as evidence of the need for coordination mechanisms
  3. **CTDE Implementation:** Implement QMIX or MAPPO on the same 4-intersection grid using the "Centralized Critic" pattern. Compare convergence speed and throughput against the Independent MARL baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MARL agents maintain safety guarantees and performance when transitioning from simulated environments to real-world transportation systems?
- Basis in paper: The paper identifies "the sim-to-real transfer gap" as a core challenge that "continues to hinder real-world deployment" (Abstract, Section V). Section VI states this gap "arises due to discrepancies in sensing accuracy, environment dynamics, and agent behaviors."
- Why unresolved: Simulators like SUMO and CARLA cannot fully capture real-world noise, sensor inaccuracies, or unpredictable human behaviors. The paper notes that domain randomization and real-world fine-tuning are partial solutions but not validated at scale.
- What evidence would resolve it: Successful real-world MARL deployments with documented performance metrics, or validated transfer protocols showing bounded performance degradation across diverse sim-to-real scenarios.

### Open Question 2
- Question: What communication protocols can enable effective multi-agent coordination under realistic bandwidth constraints, latency, and intermittent connectivity?
- Basis in paper: The paper identifies "communication constraints" as a core challenge (Abstract) and states that "real-world communication is constrained by limited bandwidth, transmission delays, packet loss, and unreliable connectivity" (Section V).
- Why unresolved: Existing approaches like CommNet assume idealized communication. The paper notes that designing "robust, efficient, and scalable" communication protocols "remains an active area of research."
- What evidence would resolve it: Communication-efficient MARL algorithms that achieve comparable coordination performance with significantly reduced message frequency, validated in environments with realistic channel models.

### Open Question 3
- Question: How can MARL systems scale to city-level deployments with hundreds or thousands of interacting agents without suffering from computational intractability or unstable learning dynamics?
- Basis in paper: The paper states "scalability" is a core challenge (Abstract) and that "as the number of agents increases, the joint state-action space grows exponentially, making centralized control or joint-policy learning computationally infeasible" (Section V).
- Why unresolved: The paper notes that factorized representations and decentralized learning help, but the fundamental trade-off between coordination quality and scalability remains unsolved for large-scale, city-wide ITS deployments.
- What evidence would resolve it: Demonstrated MARL deployments coordinating 100+ agents with stable convergence, or theoretical bounds on sample complexity as agent count increases.

## Limitations
- Many mechanisms assume ideal communication conditions rarely met in real transportation networks
- Scalability challenges are largely theoretical with limited discussion of computational bottlenecks in large-scale deployments
- The analysis of sim-to-real transfer gaps relies predominantly on simulation-based evidence without real-world validation
- Algorithmic mechanism descriptions lack empirical comparisons across different simulation platforms

## Confidence
- **High:** Taxonomy and challenge identification are well-supported by comprehensive literature review
- **Medium:** Algorithmic mechanism descriptions are accurate but lack empirical validation across platforms
- **Low:** Practical deployment feasibility claims are speculative given predominance of simulation evidence

## Next Checks
1. **Mechanism Validation:** Implement QMIX on a 4-intersection SUMO scenario to empirically test whether value decomposition improves credit assignment compared to independent DQNs

2. **Communication Constraints:** Modify CommNet to simulate bandwidth limits (e.g., message quantization, latency) and measure degradation in coordination performance under realistic ITS communication conditions

3. **Sim-to-Real Transfer:** Conduct domain randomization experiments in CityFlow across varying traffic demand patterns to quantify policy robustness before real-world deployment consideration