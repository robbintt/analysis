---
ver: rpa2
title: 'Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model'
arxiv_id: '2510.18573'
source_url: https://arxiv.org/abs/2510.18573
tags:
- video
- reference
- generation
- subject
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kaleido is a subject-to-video (S2V) generation framework that synthesizes
  videos conditioned on multiple reference images of target subjects. Existing S2V
  models struggle with multi-subject consistency and background disentanglement, resulting
  in lower reference fidelity and semantic drift.
---

# Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model

## Quick Facts
- **arXiv ID:** 2510.18573
- **Source URL:** https://arxiv.org/abs/2510.18573
- **Authors:** Zhenxing Zhang; Jiayan Teng; Zhuoyi Yang; Tiankun Cao; Cheng Wang; Xiaotao Gu; Jie Tang; Dan Guo; Meng Wang
- **Reference count:** 12
- **Key outcome:** Open-sourced S2V model achieving 0.723 S2V Consistency and 0.319 S2V Decoupling, significantly outperforming previous methods in multi-subject consistency and background disentanglement

## Executive Summary
Kaleido introduces a novel Subject-to-Video (S2V) generation framework that synthesizes videos conditioned on multiple reference images while maintaining subject consistency and background disentanglement. The framework addresses critical limitations in existing S2V models that struggle with multi-subject consistency and background leakage. By combining a dedicated data construction pipeline with Reference Rotary Positional Encoding (R-RoPE), Kaleido achieves superior performance in both consistency and fidelity metrics. The model approaches closed-source systems in subject consistency while excelling in background disentanglement and multi-subject scenarios, with both the data pipeline and pretrained S2V model being open-sourced for future research.

## Method Summary
Kaleido employs a two-stage training approach: pretraining on 2M pairs followed by SFT on 0.5M pairs. The framework fine-tunes Wan2.1-T2V-14B using Flow Matching loss with AdamW optimizer. Key innovations include a dedicated data construction pipeline using Grounding DINO + SAM for segmentation, cross-paired data synthesis with background inpainting via FLUX, and Reference Rotary Positional Encoding (R-RoPE) that shifts reference image positional vectors by (H_max, W_max). The method uses simple concatenation injection where reference images are concatenated with video noise along the sequence dimension, preserving the base model's architecture while enabling stable multi-image integration.

## Key Results
- Achieves S2V Consistency of 0.723 and S2V Decoupling of 0.319, significantly outperforming previous methods
- Approaches closed-source systems in subject consistency while excelling in background disentanglement
- Demonstrates superior performance in multi-subject scenarios with both quantitative metrics and qualitative comparisons
- Open-sources both the data pipeline and pretrained S2V model for community research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-paired data construction forces the model to learn disentangled subject representations rather than spurious background correlations
- **Mechanism:** By synthetically composing training pairs where subject and context are mismatched, the model cannot rely on simple co-occurrence statistics and must isolate intrinsic subject features
- **Core assumption:** The model learns conditional distributions based primarily on pixel-level consistency; entangled subject-background pairs would cause the model to memorize background as part of subject identity
- **Evidence anchors:** Abstract mentions cross-paired samples as key to overcoming semantic drift; Section 3.2 details background disentanglement strategies; CINEMA identifies multi-subject personalized generation as largely unexplored

### Mechanism 2
- **Claim:** Reference Rotary Positional Encoding (R-RoPE) prevents model from misinterpreting reference images as video frames by separating their positional manifolds
- **Mechanism:** R-RoPE shifts spatial coordinates of reference tokens to start after maximum video dimensions, ensuring attention operations treat reference tokens as distinct context rather than video continuation
- **Core assumption:** DiT's self-attention relies heavily on relative positional distances; overlapping positional IDs cause token disorder or temporal confusion
- **Evidence anchors:** Abstract introduces R-RoPE for stable multi-image integration; Section 4.2 formalizes position vector with shifted coordinates; Phantom utilizes latent concatenation but finds integration suboptimal

### Mechanism 3
- **Claim:** Direct sequence concatenation preserves generative priors of base T2V model better than adapter-based injection
- **Mechanism:** Reference image tokens are concatenated directly with noise tokens along sequence dimension, treating references as "prefix prompt" in latent space
- **Core assumption:** Pre-trained DiT backbone has sufficient capacity to relate text, video noise, and reference images if positional signals are unambiguous
- **Evidence anchors:** Section 4.2 states approach preserves original base model structure; Table 1 shows high aesthetic quality and motion smoothness scores; MV-S2V explores multi-view references implying architectural handling is critical

## Foundational Learning

- **Concept: Diffusion Transformers (DiT) & 3D RoPE**
  - **Why needed here:** Paper builds directly upon DiT architecture (Wan2.1) and modifies its positional encoding
  - **Quick check question:** How does rotary embedding represent relative distance differently than standard absolute sinusoidal embedding?

- **Concept: Subject-Background Disentanglement**
  - **Why needed here:** Primary failure mode Kaleido targets; data pipeline designed to break entanglement in "naive" datasets
  - **Quick check question:** Why does training on raw video frames cause model to copy backgrounds instead of just the subject?

- **Concept: Flow Matching / Rectified Flow**
  - **Why needed here:** Paper utilizes Flow Matching rather than standard DDPM noise scheduling
  - **Quick check question:** In Flow Matching, what does vector field v represent compared to standard noise prediction ε in DDPM?

## Architecture Onboarding

- **Component map:** Raw Video + Multi-Reference Images + Text Prompt -> 3D VAE (Video) + VAE (Image) + T5 (Text) -> DiT Backbone (Wan2.1) + R-RoPE Module -> Flow Matching Loss
- **Critical path:**
  1. Tokenization: Latent encoding of video noise and reference images
  2. Position Assignment: Video tokens receive standard temporal/spatial IDs; reference tokens receive shifted spatial IDs (R-RoPE) and fixed temporal ID 0
  3. Concatenation: Sequence = [I_1, I_2, ..., I_n, z]
  4. Attention: Joint self-attention over concatenated sequence
- **Design tradeoffs:**
  - Concatenation vs. Adapters: Kaleido chooses concatenation for efficiency and preservation of base priors, trading off explicit control that adapters provide
  - R-RoPE shift: Shifting by H_max ensures separation but requires handling out-of-distribution position IDs
- **Failure signatures:**
  - "Subject Copy-Paste": Model animates reference frame without following text prompt
  - "Background Leakage": Generated video contains objects from reference image
  - "Token Confusion": Attributes of Subject A appear on Subject B
- **First 3 experiments:**
  1. Sanity Check (R-RoPE): Run inference with standard RoPE vs. R-RoPE on 2-subject prompt
  2. Data Ablation: Train on "naive" frame pairs vs. "cross-paired" data and inspect background fidelity
  3. Latent Space Inspection: Visualize attention maps of video tokens attending to reference image tokens

## Open Questions the Paper Calls Out

- **Question:** Does R-RoPE strategy introduce attention degradation when processing extreme aspect ratios where spatial offset becomes significantly large?
- **Basis in paper:** Section 4.2 describes R-RoPE as shifting reference image positional vectors to start after maximum observed dimensions
- **Why unresolved:** Paper validates on standard benchmarks but doesn't analyze impact of large positional gaps on attention mechanism's ability to correlate reference details with distant video tokens
- **What evidence would resolve it:** Evaluations of subject fidelity and artifact frequency in generated videos with extreme resolutions compared to standard dimensions

- **Question:** To what extent does reliance on automated segmentation limit model's ability to generate subjects in contexts where they are naturally semi-transparent or visually entangled with background?
- **Basis in paper:** Section 3.2 states pipeline uses SAM for segmentation and FLUX inpainting to enforce strict subject-background disentanglement
- **Why unresolved:** Training exclusively on synthetic data where backgrounds are explicitly removed may teach model to ignore naturally occurring visual blending
- **What evidence would resolve it:** Comparative evaluation on test set featuring semi-transparent subjects against models trained on unmodified reference data

- **Question:** How does R-RoPE mechanism handle temporal consistency when reference images depict subject in drastically different poses compared to smooth motion required in generated video?
- **Basis in paper:** Section 4.2 notes R-RoPE assigns temporal positions starting from t=0 for each image
- **Why unresolved:** Paper demonstrates multi-subject consistency but doesn't isolate scenarios with temporally incoherent reference images
- **What evidence would resolve it:** Ablation study using temporally disjoint or highly pose-divergent reference sets to measure temporal smoothness and identity stability

## Limitations

- Training data and pretrained model not publicly available at time of writing, preventing independent verification
- Evaluation metrics (S2V Consistency, S2V Decoupling) are custom metrics that may not generalize to other benchmarks
- Focus primarily on multi-reference case with less attention to single-reference scenarios where simpler approaches might suffice
- Claims of performance improvements difficult to assess without access to baseline implementations or test data

## Confidence

**High Confidence:** R-RoPE positional encoding resolving reference-video token confusion is well-supported by theoretical formulation and ablation results showing degraded performance when removed

**Medium Confidence:** Cross-paired data construction significantly improving background disentanglement is supported by data, but extent of improvement over other strategies not fully explored

**Medium Confidence:** Overall performance improvements demonstrated on paper's test set, but without access to data or comparison to baseline implementations, difficult to assess if results represent true architectural improvements

## Next Checks

1. **R-RoPE Ablation Replication:** Implement and test standard RoPE vs. R-RoPE comparison on held-out multi-subject test set to verify subjects merge or attributes bleed without R-RoPE

2. **Data Pipeline Ablation:** Train two models—one on naive video frames and one on cross-paired data—using identical architectures and hyperparameters, then measure and compare background fidelity and subject consistency

3. **Cross-Model Consistency:** Apply Kaleido's R-RoPE and data pipeline to different base T2V model (e.g., HunyuanVideo or Luma) to test whether improvements are architecture-specific or generalizable across video generation frameworks