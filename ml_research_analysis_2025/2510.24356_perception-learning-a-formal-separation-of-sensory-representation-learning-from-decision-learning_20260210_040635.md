---
ver: rpa2
title: 'Perception Learning: A Formal Separation of Sensory Representation Learning
  from Decision Learning'
arxiv_id: '2510.24356'
source_url: https://arxiv.org/abs/2510.24356
tags:
- learning
- perception
- perceptual
- decision
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Perception Learning (PeL) addresses the problem of optimizing\
  \ sensory representations independently from downstream decision tasks. The core\
  \ method introduces a formal separation where the sensory encoder f\u03C6: X \u2192\
  \ Z is trained using task-agnostic signals (invariance, contrastive information,\
  \ diversity) without decision loss backpropagation."
---

# Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning

## Quick Facts
- **arXiv ID**: 2510.24356
- **Source URL**: https://arxiv.org/abs/2510.24356
- **Reference count**: 28
- **Key outcome**: PeL provides formal separation of sensory representation learning from decision tasks, proving orthogonality between perceptual updates and Bayes risk gradients under group invariance assumptions.

## Executive Summary
Perception Learning (PeL) introduces a formal separation between sensory representation learning and downstream decision tasks. The core insight is that perceptual properties can be optimized independently using task-agnostic signals (invariance, contrastive information, diversity) without backpropagation through decision loss. This separation enables modular training where the sensory encoder is trained to capture perceptual invariances and regularities, while decision heads are trained separately on frozen representations. The approach is theoretically grounded with proofs showing that perceptual updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients.

## Method Summary
PeL optimizes a sensory encoder f_φ: X → Z using task-agnostic signals while decision heads g_θ: Z → Y are trained separately without gradients flowing back into φ. The method combines three terms: an invariance loss enforcing stability under nuisance transformations, a symmetric InfoNCE contrastive term for discriminability, and variance/covariance floors to prevent collapse. Representations are evaluated using task-agnostic metrics including invariance curves, nuisance leakage probes, and geometric diagnostics. The theoretical foundation proves that perceptual updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients under group invariance and sufficiency assumptions.

## Key Results
- PeL updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients under group invariance assumptions
- Representations factor through sufficient invariants achieve Bayes-optimal decision performance regardless of injective parameterization
- Perceptual quality can be certified independently of downstream accuracy using representation-invariant functionals
- The approach enables reusable, robust perceptual codes with clear modular training and diagnostics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PeL updates that strengthen invariance while preserving sufficient statistics are orthogonal to Bayes task-risk gradients.
- Mechanism: When $f_\phi = h \circ T$ with $h$ injective on range(T), and T is a G-invariant sufficient statistic, changes to $\phi$ that maintain $\sigma(Z_\phi) = \sigma(T)$ leave the task posterior $\eta(z) = P(Y \in \cdot | Z = z)$ unchanged, making the Bayes risk constant along tangent directions that preserve orbit identity.
- Core assumption: A1-A5 hold, specifically that the task is G-invariant and T is sufficient with $Y \perp\!\!\!\perp X | T(X)$.
- Evidence anchors:
  - [abstract]: "proves that PeL updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients under assumptions of group invariance and sufficiency"
  - [section]: Theorem 1 proof shows $D_v F(\phi_0) = 0$ when $f_{\phi_0 + tv} = h_t \circ T$ with $h_t$ injective
  - [corpus]: SPRIG paper addresses perception-decision coordination via game dynamics but does not establish orthogonality
- Break condition: When A1 fails (label is not G-invariant) or when updates destroy orbit identity (h not injective on range(T)), merging distinct classes.

### Mechanism 2
- Claim: Representations that factor through a sufficient invariant T achieve Bayes-optimal decision performance regardless of the specific injective parameterization.
- Mechanism: The Markov condition $Y \perp\!\!\!\perp X | T(X)$ ensures $P(Y \in \cdot | Z) = E[\eta_T | \sigma(Z)]$; when $\sigma(Z) = \sigma(T)$, this equals $\eta_T$ almost surely, so $F(\phi) = E\mathcal{L}_\ell(\eta_T)$ is constant over the manifold $\mathcal{M} = \{\phi : f_\phi = h \circ T, h \text{ injective on range}(T)\}$.
- Core assumption: Existence of measurable orbit map $\pi: X \to T$ that is G-invariant and sufficient (A2), plus strict propriety of the task loss (A3).
- Evidence anchors:
  - [abstract]: "PeL targets perceptual properties through representation-invariant functionals"
  - [section]: Corollary 1 states $\inf_\theta R(\phi^\star, \theta) = \inf_\theta E\ell(g_\theta(Z), Y)$ with $Z = T(X)$ achieves Bayes risk
  - [corpus]: Weak corpus support; related papers focus on task-aware representations rather than sufficiency theory
- Break condition: Over-invariance that merges distinct orbits (e.g., making "6" and "9" indistinguishable under 180° rotation—Section 6.3 counterexample shows Bayes risk increases from 0 to 1/2).

### Mechanism 3
- Claim: Perceptual quality can be certified independently of downstream accuracy using representation-invariant functionals.
- Mechanism: Functionals $\Phi_P(f_\phi; P_X, G)$ measure properties like nuisance independence $I(Z; V)$, invariance $D(\alpha) = E\|f_\phi(x) - f_\phi(T_\alpha x)\|_2^2$, and geometric regularity $E\|\nabla_x f_\phi(x)\|_F^2$ directly in Z-space, without requiring task labels or downstream training.
- Core assumption: Perceptual properties are representation-invariant—if $f_\phi$ satisfies P, then $h \circ f_\phi$ also satisfies P for injective $h: Z \to Z'$ (Definition 2).
- Evidence anchors:
  - [abstract]: "suite of task-agnostic metrics including invariance curves, leakage probes, and geometric diagnostics"
  - [section]: Table 3 lists specific metrics: $\hat{I}(Z; V)/H(V)$ for leakage, D(α) curves for invariance, Fisher information trace for robustness
  - [corpus]: HyperTASR uses task-aware scene representations, contrasting with PeL's task-agnostic approach
- Break condition: Metrics that require label access or conflate with task-specific performance cease to be perception-evaluating.

## Foundational Learning

- Concept: **Group Actions and Orbit Spaces**
  - Why needed here: Assumptions A1-A2 require understanding how transformation groups G act on input spaces and what it means for statistics to be constant on G-orbits.
  - Quick check question: Given transformations G = {rotations by 0°, 180°}, what is the orbit of a "6" digit image, and is $P(Y=\text{"6"} | X)$ invariant on this orbit?

- Concept: **Sufficient Statistics and σ-algebras**
  - Why needed here: The orthogonality theorem hinges on $\sigma(Z_\phi) = \sigma(T)$—understanding when representations capture exactly the invariant sufficient information without coarsening or excess.
  - Quick check question: If T is sufficient for Y and $Z = h(T)$ for injective h, prove that Z is also sufficient. What happens if h is not injective?

- Concept: **Strictly Proper Scoring Rules**
  - Why needed here: Assumption A3 uses strictly proper losses (log-loss, Brier) to express Bayes risk as $F(\phi) = E\mathcal{L}_\ell(P(Y \in \cdot | Z_\phi))$, ensuring the minimizer is the true posterior.
  - Quick check question: Why does strict propriety guarantee that the Bayes act is $g_\theta^\star(z) = P(Y \in \cdot | Z = z)$?

## Architecture Onboarding

- Component map:
  - **Sensory encoder** $f_\phi: \mathcal{X} \to \mathcal{Z}$ — trained with PeL loss $\mathcal{L}_{perc}$ combining invariance, contrastive, and diversity terms
  - **Decision heads** $g_\theta: \mathcal{Z} \to \Delta(\mathcal{Y})$ — trained on frozen Z, no gradients backpropagate into $\phi$
  - **PeL objectives**: $L_{inv} = E\|f_\phi(X) - f_\phi(g \cdot X)\|_2^2$, InfoNCE contrastive term, variance/covariance floors
  - **Task-agnostic metrics**: Invariance curves D(α), leakage probes $\hat{I}(Z;V)/H(V)$, Jacobian smoothness, Fisher trace

- Critical path:
  1. Identify nuisance group G relevant to your domain (rotations, lighting, sensor ID)
  2. Implement $L_{inv}$ with augmentation sampling $T_\delta \sim G$
  3. Add symmetric InfoNCE for discriminability and variance regularization to prevent collapse
  4. Evaluate D(α) curves and leakage probes before any downstream training

- Design tradeoffs:
  - **Invariance strength vs. information preservation**: Stronger invariance may discard label-relevant signal (Section 6.3)
  - **Modularity vs. task-specific optimization**: Separation enables reuse and diagnostics but foregoes end-to-end tuning
  - **Metric sophistication vs. compute**: Fisher trace is theoretically grounded but expensive; variance floors are cheap proxies

- Failure signatures:
  - **Over-invariance**: Distinct classes mapped to same code (Bayes risk jumps to chance)
  - **Representation collapse**: Near-constant Z with $\sum_d \max(0, \gamma - \text{Var}(Z_d))$ large
  - **Nuisance leakage**: $\hat{I}(Z; V)/H(V) > 0.3$ indicates nuisance predictability
  - **Orbit merging**: Linear probe accuracy degrades on frozen Z compared to Z = X baseline

- First 3 experiments:
  1. **Invariance curve profiling**: Plot $D(\alpha)$ across transformation magnitudes α; verify AUC decreases during PeL training without downstream supervision.
  2. **Leakage probe audit**: Train adversary to predict nuisance V from Z; target probe AUC near 0.5 (random).
  3. **Sufficiency preservation check**: Fit $g_\theta$ on frozen PeL representations; compare accuracy to end-to-end baseline to confirm no catastrophic information loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "correct" invariance set G satisfying (A1) be identified in practice without prior knowledge of task labels?
- Basis in paper: [explicit] The counterexample in Section 6.3 demonstrates that enforcing invariance to the wrong group (e.g., 180° rotation when labels distinguish "6" vs "9") strictly increases Bayes risk, and the paper explicitly states "PeL should target nuisances, not causal factors."
- Why unresolved: The theoretical framework assumes G is given and satisfies (A1), but provides no mechanism for discovering which transforms are task-true invariances versus label-relevant variations in unlabeled settings.
- What evidence would resolve it: A systematic method or criterion for selecting G from data that provably avoids over-invariance, validated on benchmarks where ground-truth nuisance structure is known.

### Open Question 2
- Question: Do the proposed task-agnostic metrics (invariance curves, leakage probes, geometric diagnostics) empirically predict downstream transfer performance across diverse tasks?
- Basis in paper: [inferred] The paper introduces a "suite of task-agnostic evaluation metrics" but provides no experimental validation linking improvements in these metrics to downstream task accuracy or data efficiency.
- Why unresolved: Without empirical correlation studies, it remains unclear whether optimizing perceptual properties as defined actually yields practical benefits, or if the metrics capture orthogonal information to task performance.
- What evidence would resolve it: Large-scale experiments correlating metric scores with transfer performance across multiple domains (vision, audio, control), demonstrating that higher perceptual quality scores predict better downstream outcomes.

### Open Question 3
- Question: How can the injectivity-on-range(T) condition required by Theorem 1 be verified or enforced during training?
- Basis in paper: [explicit] The theorem requires f_φ = h ∘ T "with h injective on range(T)" and notes this "appears to prevent coarsening," but does not address practical enforcement.
- Why unresolved: Injectivity is a global property that cannot be directly optimized via gradient descent, and collapsing distinct orbits during training would violate the orthogonality guarantee without detection.
- What evidence would resolve it: A tractable proxy or regularizer that preserves orbit distinguishability, or empirical analysis showing whether standard PeL objectives implicitly maintain sufficient injectivity.

## Limitations
- Theoretical claims depend heavily on assumptions A1-A5 being met, particularly group-invariance of task labels and sufficiency of orbit maps
- The practical gap between formal separation and messy real data with correlated nuisances remains unquantified
- Relationship between perceptual quality metrics and downstream task performance is only sketched, not rigorously proven across diverse domains

## Confidence
- **High Confidence**: The orthogonality theorem proof (Mechanism 1) and sufficiency preservation result (Mechanism 2) appear mathematically sound under stated assumptions. The task-agnostic metrics framework (Mechanism 3) is clearly specified and implementable.
- **Medium Confidence**: The practical effectiveness of PeL depends on careful hyperparameter tuning (invariance strength, loss weights) that the paper doesn't fully specify. The claim that PeL "enables reusable, robust perceptual codes" needs empirical validation across multiple downstream tasks.
- **Low Confidence**: The theoretical separation between perceptual and decision learning may break down in practice when perceptual objectives inadvertently harm task-relevant information retention, as hinted in the "6" vs "9" counterexample.

## Next Checks
1. **Assumption Validation**: Systematically test A1-A5 across diverse datasets (digits, faces, speech) to quantify when group-invariance and sufficiency actually hold. Measure Bayes risk degradation when assumptions are violated.
2. **Metric Utility Validation**: Compare PeL representations against end-to-end trained ones across 5-10 downstream tasks to verify that good invariance/leakage metrics predict task performance. Identify cases where perceptual quality metrics mislead.
3. **Robustness Validation**: Test PeL under distribution shift (new nuisances, domain adaptation) to confirm that the perceptual separation provides actual robustness benefits versus monolithic training approaches.