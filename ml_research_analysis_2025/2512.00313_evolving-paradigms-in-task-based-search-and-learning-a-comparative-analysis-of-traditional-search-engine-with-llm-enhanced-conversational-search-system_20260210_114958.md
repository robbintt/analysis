---
ver: rpa2
title: 'Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis
  of Traditional Search Engine with LLM-Enhanced Conversational Search System'
arxiv_id: '2512.00313'
source_url: https://arxiv.org/abs/2512.00313
tags:
- search
- information
- learning
- copilot
- bing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared user search behaviors and learning outcomes
  between a traditional search engine (Bing) and an LLM-enhanced search system (Bing
  Copilot). A within-subjects experiment with 5 participants revealed that Copilot
  users initiated searches more efficiently, requiring less planning and experiencing
  reduced browsing effort.
---

# Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System

## Quick Facts
- arXiv ID: 2512.00313
- Source URL: https://arxiv.org/abs/2512.00313
- Authors: Zhitong Guan; Yi Wang
- Reference count: 40
- Primary result: Copilot users showed higher satisfaction and efficiency but potentially shallower learning due to reduced construction activities compared to Bing.

## Executive Summary
This study compares user search behaviors and learning outcomes between traditional search engine (Bing) and LLM-enhanced search system (Bing Copilot) through a within-subjects experiment with 5 participants. Results show Copilot users initiated searches more efficiently with reduced planning and browsing effort, while both systems followed Kuhlthau's ISP model. Copilot users reported higher satisfaction and perceived learning outcomes, though deeper learning was associated with more intensive construction activities during the search process. The study proposes a new information process model for LLM-powered search and recommends hybrid search features combining traditional indexing with LLM insights to better support learning and creativity.

## Method Summary
The study employed a within-subjects design with 5 participants (2 undergraduates, 3 graduate students) who had prior LLM experience and frequently used search engines for learning. Participants completed two open-ended comprehensive search tasks requiring critical and creative learning, using both Bing and Bing Copilot in counterbalanced order. Screen recordings captured behavioral metrics including query count, browsing depth, and copy-paste events. Participants completed pre/post questionnaires measuring perceived learning outcomes and creativity support on 11-point Likert scales, followed by semi-structured interviews. Mann-Whitney U tests analyzed questionnaire data, while content analysis examined learning notes and interview transcripts.

## Key Results
- Copilot users initiated searches more efficiently, requiring less planning and experiencing reduced browsing effort
- Both systems followed Kuhlthau's ISP model, but Copilot users engaged more deeply in exploration and formulation stages while Bing users spent more time in collection
- Copilot users reported higher satisfaction and perceived learning outcomes, though deeper learning was associated with more intensive construction activities during search

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Offloading during Exploration
- **Claim:** LLM-enhanced search reduces the cognitive barrier to initiating complex search tasks by synthesizing broad information, allowing users to bypass manual "sense-making" of multiple documents.
- **Mechanism:** In traditional search, users must manually aggregate data from scattered links. LLMs compress this by ingesting the query (even vague ones) and outputting a structured summary, shifting user effort from "gathering" to "evaluating."
- **Core assumption:** Users have limited cognitive bandwidth and prioritize reduced early-stage effort over comprehensive manual verification.
- **Evidence anchors:**
  - [abstract] "Copilot users initiated searches more efficiently, requiring less planning... reduced browsing effort."
  - [Page 9] "The Copilot response is very structured. I spent much less effort in understanding what's out there... in the search system (Bing) I need to open each link."
  - [corpus] Related work in Generative Search Engines (GSEs) confirms a shift from "ranked lists to synthesized answers," though it does not specifically verify the cognitive load reduction claims of this specific study.
- **Break condition:** Fails when the user requires highly specific, niche, or recent data not captured in the LLM's training window or top-k retrieval context.

### Mechanism 2: The Construction-Depth Trade-off
- **Claim:** While LLMs improve perceived efficiency, "deeper" learning is mechanistically tied to "construction activities" (manual grouping and labeling), which traditional search enforces but LLMs may circumvent.
- **Mechanism:** Learning is posited to occur through the active synthesis of information. Traditional search forces this synthesis (construction); LLMs provide pre-synthesized answers (selection), potentially bypassing the cognitive encoding process required for deep retention.
- **Core assumption:** Active cognitive construction is a prerequisite for deep learning, and "ease of use" can be inversely correlated with retention.
- **Evidence anchors:**
  - [Page 12] "Deeper learning was associated with more intensive construction activities... irrespective of the search system used."
  - [Page 10] "With Bing, participants actively grouped and labeled information... With Bing Copilot... typically involved highlighting... [which] may not encourage as thorough an engagement."
  - [corpus] No direct corpus evidence confirms the inverse relationship between LLM efficiency and retention depth; this is a specific finding of the primary paper.
- **Break condition:** Fails if the LLM interface is redesigned to force active user annotation or synthesis steps before revealing summaries.

### Mechanism 3: Affective Re-alignment of the Search Process
- **Claim:** LLMs improve user satisfaction by eliminating the "uncertainty dip" characteristic of the early Exploration stage in Kuhlthau's ISP model, though they shift frustration to the later Collection stage.
- **Mechanism:** Traditional search creates anxiety during exploration due to information overload. LLMs provide immediate structure, increasing early confidence. However, this shifts the "pain point" to later stages when the LLM fails to provide specific, detailed evidence.
- **Core assumption:** The user's emotional journey strictly follows the ISP model stages, and early success colors the overall perception of the system positively.
- **Evidence anchors:**
  - [Page 11] "With Bing, participants were in the dip of confidence during the exploration stage... With Bing Copilot, the formulation and collection stage was where negative emotions predominantly surfaced."
  - [Page 11] "Copilot received a significantly higher emotional rating in terms of satisfaction... despite negative emotions surfacing later."
  - [corpus] Weak support; corpus focuses on traffic acquisition and optimization, not user affective states.
- **Break condition:** Fails if the user's task is time-critical and the LLM "hallucinates" or provides generic advice during the initial query, causing immediate abandonment.

## Foundational Learning

- **Concept: Kuhlthau's Information Search Process (ISP) Model**
  - **Why needed here:** The paper uses ISP stages (Initiation, Selection, Exploration, Formulation, Collection, Presentation) as the primary lens to analyze where LLMs help vs. hinder. Without this, the distinction between "browsing" and "constructing" is opaque.
  - **Quick check question:** Can you identify which ISP stage is most prone to user anxiety in traditional search, and how Copilot alters that specific stage?

- **Concept: Construction Theory of Information Seeking**
  - **Why needed here:** This theory explains the counter-intuitive finding that "easier" search (Copilot) might lead to "shallower" learning. It posits that learning is an active process of building understanding, which passive consumption of summaries bypasses.
  - **Quick check question:** Why might a tool that "saves mental capacity" actually result in lower retention according to this theory?

- **Concept: Exploratory vs. Lookup Search**
  - **Why needed here:** The paper explicitly critiques traditional search for "lookup" efficiency at the expense of "exploratory" learning. Understanding this distinction is required to evaluate the proposed "Hybrid Search" solution.
  - **Quick check question:** Does the system architecture prioritize finding a specific fact (Lookup) or building a mental model of a new topic (Exploratory)?

## Architecture Onboarding

- **Component map:** Natural Language Task Prompts -> LLM Orchestrator -> Retrieval Augmented Generation (RAG) -> Synthesized Output -> User Workspace
- **Critical path:**
  1. **Query Intake:** System accepts verbose, unstructured task descriptions (unlike keyword engines)
  2. **RAG Retrieval:** System fetches top-K relevant web pages
  3. **Synthesis:** LLM generates a coherent narrative summary
  4. **Reference Attachment:** System links claims to specific URLs (critical for trust)
  5. **User Interaction:** User highlights/copies (Current bottleneck for learning)
- **Design tradeoffs:**
  - **Efficiency vs. Depth:** Optimizing for fast, structured answers (Copilot) reduces the "construction" effort that leads to deep memory encoding
  - **Breadth vs. Specificity:** LLMs excel at broad exploration but struggle with specific "Collection" details, frustrating users in later ISP stages
- **Failure signatures:**
  - **"Generic Fluff":** User becomes frustrated during the Collection stage because the LLM refuses to output specific data points, graphs, or raw data, defaulting to high-level summaries
  - **"Construction Bypass":** Users complete tasks with high satisfaction but low actual learning (measured via inability to recall information without the tool)
- **First 3 experiments:**
  1. **Hybrid UI Implementation:** Build an interface that combines LLM summaries (Exploration stage) with traditional deep-link results (Collection stage) to measure if "specificity" needs are better met
  2. **Construction-Forcing Interaction:** Modify the copy-paste workflow to require the user to tag or label the information before it enters their notes, testing if this restores "deep learning" outcomes
  3. **ISP Stage Timing Analysis:** Instrument the system to log time-spent-in-stage. Verify if users truly spend less time in Exploration and more time in Collection compared to legacy logs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed shift in the Information Search Process (ISP) model—specifically the tendency to skip planning and the altered engagement in exploration and formulation stages—persist in larger, more diverse populations?
- **Basis in paper:** [explicit] The authors state that as a "pilot investigation" constrained by a "limited sample size of five participants," future research "will necessitate broader participant inclusion... to substantiate and validate these initial observations."
- **Why unresolved:** The current sample size (N=5) is insufficient to generalize the proposed theoretical shifts in the ISP model or the specific hypotheses regarding stage-skipping.
- **What evidence would resolve it:** A replication of the study with a statistically powered sample size across different user demographics to confirm if the behavioral shifts are universal.

### Open Question 2
- **Question:** How does the reduction of manual "construction" activities (e.g., grouping/labeling information) in LLM-enhanced search impact long-term knowledge retention and depth of understanding?
- **Basis in paper:** [inferred] The paper notes that while perceived learning was higher with Copilot, "deeper learning was associated with more intensive construction activities" (common in traditional search). The authors observed that Copilot users engaged in less thorough synthesis, creating a tension between efficiency and depth.
- **Why unresolved:** The study relied on self-reported perceived learning and immediate task output rather than longitudinal retention metrics, leaving the long-term educational impact of reduced cognitive effort unknown.
- **What evidence would resolve it:** A follow-up experiment measuring delayed recall and transfer of knowledge (e.g., testing days or weeks after the search task) comparing the two systems.

### Open Question 3
- **Question:** Can a hybrid search interface effectively resolve the user frustration caused by the lack of specificity in LLM responses during the "collection" stage?
- **Basis in paper:** [explicit] The authors "advocate for the development of LLM search system designs" that implement a "hybrid search feature" combining traditional indexing with LLM insights to address users' expressed need for "greater specificity" and "niche information."
- **Why unresolved:** This design recommendation is proposed based on identified limitations in current tools (e.g., users falling back to traditional search for specific details) but has not yet been prototyped or tested by the authors.
- **What evidence would resolve it:** Usability studies of a prototype system that integrates traditional links with LLM summaries, specifically analyzing user success rates in retrieving specific, non-generic information.

## Limitations
- **Sample Size:** N=5 participants severely limits statistical generalizability despite within-subjects design
- **Single Task Domain:** Both tasks focused on "contemporary social topics" which may not generalize to technical or scientific domains
- **Selection Bias:** Participants specifically recruited for LLM experience may not represent average users

## Confidence
- **High Confidence:** Copilot reduces early-stage cognitive effort and browsing depth
- **Medium Confidence:** Copilot improves perceived satisfaction despite shifting negative affect to later stages
- **Medium Confidence:** Deeper learning correlates with construction activities
- **Low Confidence:** Proposed hybrid search architecture will resolve identified tensions

## Next Checks
1. **Replicate with N=30+** using identical protocol to establish statistical significance and validate the proposed information process model for LLM-powered search
2. **Domain Generalization Test** by replicating the study with technical/scientific tasks to assess whether the construction-depth trade-off holds across different information domains
3. **Construction-Forcing Intervention** by implementing the proposed modified copy-paste interface requiring information labeling before saving, then measuring whether this restores deep learning outcomes without sacrificing Copilot's efficiency benefits