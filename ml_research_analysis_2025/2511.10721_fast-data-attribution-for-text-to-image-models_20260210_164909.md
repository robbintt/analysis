---
ver: rpa2
title: Fast Data Attribution for Text-to-Image Models
arxiv_id: '2511.10721'
source_url: https://arxiv.org/abs/2511.10721
tags:
- attribution
- training
- data
- learning
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient data attribution
  for text-to-image models, which identifies the training images that most significantly
  influenced a generated output. Existing attribution methods are computationally
  expensive, making them impractical for real-world applications.
---

# Fast Data Attribution for Text-to-Image Models

## Quick Facts
- arXiv ID: 2511.10721
- Source URL: https://arxiv.org/abs/2511.10721
- Authors: Sheng-Yu Wang; Aaron Hertzmann; Alexei A Efros; Richard Zhang; Jun-Yan Zhu
- Reference count: 40
- Primary result: Achieves 2,500x to 400,000x speedup in data attribution for text-to-image models with better or competitive accuracy.

## Executive Summary
This paper addresses the computational bottleneck in data attribution for text-to-image models by distilling a slow, unlearning-based attribution method into a fast feature embedding space. The authors demonstrate that highly influential training images can be efficiently retrieved using cosine similarity in a learned embedding, supervised by the slow but accurate AbU+ method. This approach enables real-time attribution without sacrificing accuracy, making it practical for large-scale models like Stable Diffusion.

## Method Summary
The method distills a slow, unlearning-based attribution method (AbU+) into a feature embedding space for efficient retrieval of highly influential training images. It uses a learning-to-rank approach to train an MLP head on top of pretrained features (DINO + CLIP-Text), supervised by the AbU+ rankings. During deployment, combined with efficient indexing and search methods, the approach finds highly influential images without running expensive attribution algorithms. The method uses K-NN pruning to reduce data curation cost and cross-entropy loss with negative sampling to train the embedding model.

## Key Results
- Achieves 2,500x to 400,000x speedup compared to existing attribution methods
- Competitive or superior mean Average Precision (mAP) scores on both MSCOCO and LAION datasets
- DINO+CLIP-Text feature fusion outperforms individual features for Stable Diffusion attribution

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation from Slow Teacher to Fast Embedding
A slow but accurate attribution method can be distilled into a feature embedding space that enables fast similarity-based retrieval while preserving attribution accuracy. The method uses Attribution by Unlearning (AbU+) as a "teacher" that produces ground-truth attribution rankings, which supervise a learning-to-rank objective that trains an MLP head on top of pretrained features. At inference, attribution becomes cosine similarity search in the learned embedding space.

### Mechanism 2: Two-Stage Data Curation via K-NN Pruning
Computing attribution scores over all training data is unnecessary; a K-NN prefiltering stage preserves the influential images while making data collection tractable. For each synthesized query, first retrieve top-K nearest neighbors using off-the-shelf features, then run the expensive AbU+ method only on this subset.

### Mechanism 3: Cross-Entropy Learning-to-Rank with Negative Sampling
A pointwise cross-entropy loss on normalized ranks, combined with strategic sampling of non-neighbor negatives, effectively learns attribution rankings for large candidate sets. Normalize AbU+ scores into ranks, then train the embedding model via binary cross-entropy to predict these ranks, with occasional hard negatives to teach global discrimination.

## Foundational Learning

- **Concept: Influence Functions and Gradient-Based Attribution**
  - Why needed here: Understanding baseline methods (TRAK, D-TRAK, influence functions) clarifies why they are computationally prohibitive and what tradeoffs the proposed method avoids.
  - Quick check question: Can you explain why storing preconditioned gradients for every training image becomes infeasible for billion-parameter models?

- **Concept: Machine Unlearning and Certified Removal**
  - Why needed here: The teacher method (AbU+) is grounded in unlearning theory; understanding the Newton update with Fisher regularization clarifies how "forgetting" relates to influence.
  - Quick check question: How does the EK-FAC approximation improve over diagonal Fisher for estimating the unlearning update?

- **Concept: Learning-to-Rank Objectives (Pointwise vs. Pairwise vs. Listwise)**
  - Why needed here: The choice of cross-entropy over ordinal loss or MSE is non-obvious; understanding LTR taxonomy helps evaluate why this design supports fast inference.
  - Quick check question: Why does ordinal regression, despite competitive accuracy, fail to support the desired inference-time similarity search?

## Architecture Onboarding

- **Component map:** Pretrained encoders (DINO, CLIP-Text) -> Concatenated features -> 3-layer MLP head -> Attribution embedding space
- **Critical path:** Data curation (query → K-NN retrieval → AbU+ scoring → rank normalization) → Training (sample triples → forward pass → BCE loss → backprop) → Inference (precompute/index embeddings → embed query → cosine similarity search)
- **Design tradeoffs:**
  - Teacher accuracy vs. curation cost: AbU+ is more accurate but requires 2+ hours per query; K-NN pruning reduces this by 10x but risks missing edge-case influences
  - Feature fusion: DINO+CLIP-Text outperforms either alone but doubles embedding dimension and storage
  - Negative sampling rate: 10% is optimal; 0% loses global discrimination, >20% distracts from fine-grained ranking
- **Failure signatures:**
  - Degenerate rankings: Model outputs near-uniform scores for all candidates → check loss convergence, learning rate, or label normalization
  - Missing top-K neighbors: Retrieval fails to surface ground-truth influential images → increase K in curation or verify K-NN feature quality
  - Text-only bias (Stable Diffusion): Model ignores visual content → ensure image features are included and properly weighted
- **First 3 experiments:**
  1. Baseline feature comparison: Run attribution with untuned DINO, CLIP-Text, and DINO+CLIP-Text on a held-out query set; measure mAP against AbU+ ground truth to confirm fusion benefit.
  2. Loss function ablation: Train separate models with MSE, ordinal loss, and cross-entropy; compare mAP and inference latency to validate the chosen objective.
  3. K-NN pruning validation: For a subset of queries, run AbU+ on the full training set vs. K-NN subset only; measure recall of top-100 influential images to ensure pruning does not systematically exclude positives.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on computationally expensive AbU+ curation during training (2-4 hour query generation time)
- Assumption that K-NN prefiltering retains influential images may fail for semantically distant but functionally critical training samples
- Requires retraining or fine-tuning the MLP head for each new model or dataset, limiting immediate transferability

## Confidence
- **High**: Core distillation mechanism and retrieval speed claims (well-supported by methodology and runtime benchmarks)
- **Medium**: Attribution accuracy (competitive mAP scores but evaluation relies on single teacher method without cross-validation)
- **Medium**: Transferability across models/datasets (requires retraining MLP head for each new model)

## Next Checks
1. **K-NN Pruning Robustness**: Systematically measure recall of top-100 influential images when varying K (1k, 5k, 10k, 20k) on held-out query set, comparing full AbU+ rankings to pruned rankings.

2. **Teacher Method Cross-Validation**: Replace AbU+ with alternative attribution method (TRAK or influence functions) to generate ground-truth rankings and retrain embedding model; compare mAP and top-1 recall.

3. **Embedding Dimensionality Scaling**: Train and evaluate MLP head with reduced dimensions (384, 512) to quantify tradeoff between storage cost, retrieval speed, and attribution accuracy.