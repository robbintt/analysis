---
ver: rpa2
title: Neural Models and Language Model Prompting for the Multidimensional Evaluation
  of Open-Ended Conversations
arxiv_id: '2509.00841'
source_url: https://arxiv.org/abs/2509.00841
tags:
- score
- conversation
- chatbot
- evaluation
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multidimensional dialogue evaluation
  by developing small-scale neural models and large language model (LM) prompting
  methods to predict dialogue-level scores across ten dimensions. The proposed approaches
  include LM-based evaluators using various prompting strategies, regression models
  with a modern BERT encoder, and classification models with SBERT embeddings.
---

# Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations

## Quick Facts
- arXiv ID: 2509.00841
- Source URL: https://arxiv.org/abs/2509.00841
- Reference count: 12
- One-line primary result: LM prompting with under 13B parameters achieved highest average correlation on test set for dialogue evaluation, ranking second overall.

## Executive Summary
This paper addresses the challenge of multidimensional dialogue evaluation by developing neural models and LM prompting methods to predict dialogue-level scores across ten dimensions. The proposed approaches include LM-based evaluators using various prompting strategies, regression models with a modern BERT encoder, and classification models with SBERT embeddings. A hybrid method combines the best-performing techniques for each dimension. Experiments show that LM prompting achieves the highest average correlation on the test set, ranking second overall, while regression and classification models perform well on validation but overfit on test data. Notably, all approaches outperform the baseline on most individual dimensions, despite inconsistencies in score distributions between training and test sets. The work demonstrates competitive performance using models under 13 billion parameters, offering scalable solutions for automatic dialogue evaluation. Future work includes improving dataset quality and exploring domain adaptation.

## Method Summary
The paper proposes four approaches for dialogue evaluation: (1) LM Prompting using Deepseek Llama 8B, Deepseek Qwen 7B, or Qwen 2.5 7B Instruct 1M with basic/zero-shot/one-shot/few-shot/self-consistency prompts and varying dialogue context windows; (2) Regression with ModernBERT Large encoder (395M parameters, 8K context) + regression head trained with MSE loss on combined DSTC-12 + FED + ConTurE data with linear rescaling; (3) Classification using SBERT embeddings (all-MiniLM-L6-v2) + MLP per dimension with discretized classes rescaled to [0, 8]; (4) Hybrid combining regression (for Talent, Relevance, NonRepetition, Proactivity, Curiosity) and LM prompting (for Empathy, Trust, Skill, Overall, Capability). The study uses DSTC-12 (185 train, 120 test dialogues), FED (125 dialogues), and ConTurE (119 dialogues), with dimensions having different score ranges.

## Key Results
- LM prompting achieved the highest average correlation on the test set, ranking second overall despite modest absolute correlations (0.14-0.15)
- Regression and classification models showed high validation performance (0.68-0.79) but experienced drastic drops on test data
- All approaches outperformed the baseline on most individual dimensions, with classification excelling on Empathy
- The best model selection based on validation performance did not generalize to test data

## Why This Works (Mechanism)

### Mechanism 1: LM Prompting for Zero-Shot Dialogue Evaluation
- Claim: Language models with under 13B parameters can evaluate dialogues across multiple dimensions through carefully designed prompts, achieving modest but generalizable correlations with human judgments.
- Mechanism: The LM receives a dialogue context (full, partial, or summarized) along with a dimension definition and score range, then generates a numerical score and explanation. Different prompting strategies (zero-shot, few-shot, self-consistency) assign the LM roles like "expert evaluator" or "human evaluator" to frame the task.
- Core assumption: The pre-trained knowledge in LMs captures sufficient understanding of conversational qualities to enable evaluation without task-specific training.
- Evidence anchors:
  - [abstract] "LM prompting achieves the highest average correlation on the test set, ranking second overall"
  - [section 4.2] "we selected the best performing combination for each dimension, i.e., achieving the highest positive correlation values"
  - [corpus] Related work (Gunasekara et al., 2021; Kazi et al., 2024) suggests LMs-as-judges enable scalable evaluation, though corpus lacks direct replication of this specific prompting approach
- Break condition: Score distributions in test data differ significantly from training; LM prompts optimized for validation may misalign with test set score ranges (e.g., 0-100 training vs. 1-10 test for some dimensions).

### Mechanism 2: Regression with Long-Context Encoder for Score Prediction
- Claim: Encoder-based regression models can predict continuous dialogue quality scores but exhibit vulnerability to distribution shift between training and test sets.
- Mechanism: ModernBERT Large (395M parameters, 8K token context) encodes full dialogue context, with a regression layer predicting scores per dimension using MSE loss. Training combines DSTC-12 with mapped examples from FED and ConTurE datasets to reduce overfitting.
- Core assumption: The mapping between dimensions across datasets (e.g., "Inquisitive → Curiosity") preserves sufficient semantic similarity for transfer learning.
- Evidence anchors:
  - [abstract] "regression and classification models perform well on validation but overfit on test data"
  - [section 4.3] "ModernBERT has a context limit of 8K tokens allowing for encoding a larger dialogue context"
  - [section 5] "regression system shows drastic decreases for Relevance, NonRepetition, Proactivity, and Curiosity, despite achieving correlations of 0.68-0.79 on the validation set"
  - [corpus] Corpus lacks direct evidence for this specific encoder-based regression approach
- Break condition: Test set score ranges differ from training (e.g., Overall: 0-100 training vs. 1-5 test); model learns training distribution boundaries that don't transfer.

### Mechanism 3: Classification with Discretized Score Categories
- Claim: Treating evaluation scores as discrete classes enables MLP classification over SBERT embeddings, with dimension-specific hyperparameter tuning.
- Mechanism: Dialogue embeddings from SBERT feed into MLP classifiers trained to predict discretized score classes. Grid search identifies optimal class ranges ([0, 8] selected) and regularization per dimension. Predictions are rescaled back to original ranges.
- Core assumption: Evaluation scores represent ordinal categories rather than continuous values, and discrete prediction captures evaluator intent better than regression.
- Evidence anchors:
  - [section 1] "treating scores as classes, since they are integers that correspond to categories of evaluation"
  - [section 4.4] "we tuned Multi-Layer Perceptron (MLP) hyperparameters separately for each dimension"
  - [section 5] "classification approach... excels on six dimensions including Empathy, outperforming all other approaches in terms of number of winning dimensions"
  - [corpus] Corpus does not provide comparative evidence for classification vs. regression in dialogue evaluation
- Break condition: Rescaling from 0-8 classes back to 0-100 ranges introduces quantization errors; label smoothing with ModernBERT reduced performance.

## Foundational Learning

- **Concept: Spearman Correlation for Ordinal Evaluation**
  - Why needed here: Challenge uses mean absolute Spearman correlation as the primary metric; systems were tuned for positive correlation but ranked on absolute correlation.
  - Quick check question: If a system predicts scores that consistently rank-order dialogues opposite to human judgments (negative correlation), would absolute correlation capture this?

- **Concept: Distribution Shift in Evaluation Benchmarks**
  - Why needed here: Train/validation and test sets have different score distributions and ranges; this explains performance drops between validation (0.42-0.50) and test (0.14-0.15).
  - Quick check question: Your model trained on scores ranging 0-100 encounters test scores only in range 1-10. What happens to predictions outside the training distribution?

- **Concept: Dimension Mapping Across Datasets**
  - Why needed here: External datasets (FED, ConTurE) use different dimension names; mapping (e.g., "Likeable → Empathy") requires subjective judgment without access to original annotator instructions.
  - Quick check question: Two datasets measure related but distinct constructs. What validation would you perform before treating them as equivalent for training?

## Architecture Onboarding

- **Component map:**
  - LM Prompting Branch: Llama/Qwen/Deepseek models (7-8B) → Prompt templates (dimension-specific) → Dialogue context selection (full/partial/summarized) → Score generation
  - Regression Branch: ModernBERT-Large encoder (395M, 8K context) → Dense layer → MSE loss → Per-dimension models
  - Classification Branch: SBERT embeddings → MLP classifier → Discretized classes → Rescaling to original ranges
  - Hybrid: Dimension-level selection from {LM Prompting, Regression} based on validation performance

- **Critical path:** For new dimensions, start with LM prompting (fastest iteration), then train regression if validation correlation >0.3 and sufficient data exists. Classification requires additional hyperparameter search.

- **Design tradeoffs:**
  - LM Prompting: Best generalization, no training required, but requires prompt engineering per dimension; model choice matters less than dialogue context strategy (std=0.12 vs. 0.09)
  - Regression: Highest validation performance but poor test transfer; long context helps but doesn't solve distribution shift
  - Hybrid: Validation-optimized selection doesn't generalize; excluding classification may discard useful signals

- **Failure signatures:**
  - Negative correlations: Conceptual mismatch between LM/human interpretation of dimensions (e.g., Empathy: -0.08 test)
  - Sharp validation→test drop (>0.3): Overfitting to training score distribution; check test set distribution alignment
  - Inconsistent per-dimension performance: Dimension-specific prompt quality varies; some dimensions may need different context windows

- **First 3 experiments:**
  1. **Establish baseline distribution analysis:** Before training, compute score distribution statistics for each dimension across train/validation/test. Flag dimensions where test range differs from training by >50%.
  2. **Ablate dialogue context strategy:** For LM prompting, test identical prompts with different context windows (last 40%, first 20%+last 20%, summarized) on held-out data. Measure correlation variance.
  3. **Domain adaptation probe:** Train regression on combined DSTC-12 + FED + ConTurE, then evaluate on synthetic dialogues with known score distributions. Measure whether model extrapolates beyond training range or clamps to training bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do formal domain adaptation techniques compare to heuristic mapping when integrating heterogeneous dialogue evaluation datasets?
- Basis in paper: [explicit] The authors state future work includes "exploring domain adaptation techniques for training models on similar but larger datasets from distinct sources... to overcome data scarcity."
- Why unresolved: The current study relied on subjective, manual heuristics to map dimensions from FED and ConTurE to DSTC-12 dimensions, which may have introduced noise and limited model generalization.
- What evidence would resolve it: A comparative study measuring the correlation performance of regression/classification models trained using domain adaptation algorithms versus those trained on heuristically mapped combined datasets.

### Open Question 2
- Question: To what extent does a dedicated, high-quality annotation campaign mitigate the overfitting observed in neural evaluators?
- Basis in paper: [explicit] The authors propose "enhancing the quality of the dataset in a dedicated annotation campaign" to address inconsistencies in score ranges between train and test sets.
- Why unresolved: The regression and classification models showed high validation correlation but poor test performance, likely due to single-annotator noise and inconsistent score distributions in the original data.
- What evidence would resolve it: Training the proposed architectures on a re-annotated dataset with high inter-annotator agreement and consistent score ranges, then evaluating the gap between validation and test correlations.

### Open Question 3
- Question: Does incorporating specific human annotator instructions into prompts improve the alignment of LM-based evaluators with human judgments?
- Basis in paper: [inferred] The limitations section notes it would have been "beneficial to have the instructions provided to human annotators... to define the dimensions more accurately for the LM prompting method."
- Why unresolved: Current prompting strategies used general definitions (e.g., "Act like a human evaluator") rather than the exact guidelines provided to MTurk workers, potentially causing a conceptual mismatch in scoring criteria.
- What evidence would resolve it: An ablation study comparing the performance of LM prompting using general definitions versus prompts containing the exact annotator guidelines.

## Limitations

- Distribution mismatch between training/validation and test sets caused dramatic performance drops (0.42-0.50 validation to 0.14-0.15 test), particularly affecting regression and classification approaches.
- Critical implementation details remain unspecified, including MLP hyperparameters for classification and exact train/validation split ratios with random seeds.
- Dimension mapping across datasets relied on subjective heuristics without direct evidence of semantic equivalence, potentially introducing noise.

## Confidence

**High Confidence:** The core finding that LM prompting with under 13B parameters can achieve competitive dialogue evaluation performance, ranking second overall despite modest absolute correlations. The experimental methodology and comparison framework are sound.

**Medium Confidence:** The relative performance ordering of approaches (LM prompting > regression/classification on test set). While the paper reports these results, the distribution shift issue makes it difficult to determine whether this reflects true model superiority or differential sensitivity to test set characteristics.

**Low Confidence:** The claim that all approaches outperform the baseline on most individual dimensions. Given the baseline performance isn't specified and the validation-test correlation drop, it's unclear whether this superiority holds consistently across all dimensions when accounting for distribution differences.

## Next Checks

1. **Distribution Alignment Analysis:** Before any training, compute and visualize score distributions for each dimension across all datasets. Calculate statistical distance metrics (e.g., Wasserstein distance) between train/validation and test distributions. This will identify which dimensions are most vulnerable to distribution shift and guide data augmentation or domain adaptation strategies.

2. **Context Window Ablation Study:** Systematically test LM prompting performance across all four dialogue context strategies (last 40%, full, first 20% + last 20%, summarized) on a held-out validation set. Measure correlation variance and determine whether certain dimensions consistently benefit from specific context strategies, independent of score distribution effects.

3. **Distribution Robustness Test:** Train the regression model on synthetically generated dialogues with controlled score distributions that span the full expected range. Evaluate whether the model can extrapolate beyond its training distribution or whether it clamps predictions to training bounds, providing insight into whether distribution shift alone explains the performance drop.