---
ver: rpa2
title: Fine-tuning with RAG for Improving LLM Learning of New Skills
arxiv_id: '2510.01375'
source_url: https://arxiv.org/abs/2510.01375
tags:
- hints
- webshop
- cabinet
- alfworld
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method that converts retrieval-augmented
  generation into learned competence through distillation. The approach extracts reusable
  hints from agent failures, uses them to generate improved teacher trajectories via
  one-shot retrieval at episode start, and trains student models on these trajectories
  with hints removed to force internalization.
---

# Fine-tuning with RAG for Improving LLM Learning of New Skills

## Quick Facts
- arXiv ID: 2510.01375
- Source URL: https://arxiv.org/abs/2510.01375
- Reference count: 38
- Primary result: Distillation method achieves 91% success on ALFWorld vs 79% for baselines while using 10-60% fewer tokens

## Executive Summary
This paper presents a method that converts retrieval-augmented generation into learned competence through distillation. The approach extracts reusable hints from agent failures, uses them to generate improved teacher trajectories via one-shot retrieval at episode start, and trains student models on these trajectories with hints removed to force internalization. Across ALFWorld and WebShop benchmarks, distilled students consistently outperform baseline agents, achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens than retrieval-augmented teachers. The method generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.

## Method Summary
The method extracts imperative hints from failed agent trajectories using GPT-4o, deduplicates them via fuzzy matching, and partitions them by task category. These hints are retrieved once at episode start (k=3 default) and injected after task description. Teacher agents generate successful trajectories using this one-shot RAG approach, which are then used to train student models via LoRA fine-tuning with hints removed from the input to prevent copying. This forces the student to internalize the behavioral improvements rather than memorizing hint text. The pipeline works across different agent architectures (ReAct, StateAct) and scales (7B, 14B parameters), with different LoRA ranks for different domains (64 for ALFWorld, 16 for WebShop).

## Key Results
- Distilled students achieve 91% success rate on ALFWorld (vs 79% for baselines)
- WebShop performance improves from 61 to 72 with distillation
- Token efficiency improves by 10-60% compared to retrieval-augmented teachers
- Method generalizes across model scales (7B/14B) and architectures (ReAct/StateAct)
- k=3 hints optimal; k=6-9 shows diminishing returns or degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting imperative, generalizable hints from agent failures provides corrective guidance that improves subsequent trajectories
- Mechanism: Failed trajectories are analyzed by GPT-4o to generate 1-4 typed hints per failure; these are deduplicated via fuzzy matching (threshold 0.85) and partitioned by task/product category for targeted retrieval
- Core assumption: GPT-4o can correctly diagnose failure causes and propose rules that generalize beyond individual trajectories
- Evidence anchors:
  - [abstract]: "extracts compact, reusable hints from agent failures"
  - [section 3.3]: "hints must be imperative, use placeholders for generality, and emphasize preconditions and sequencing rather than surface descriptions"
  - [corpus]: Weak — neighbor papers address RAG applications but not failure-driven hint extraction specifically
- Break condition: If GPT-4o produces surface-level diagnostics or hints overfit to specific objects/IDs, retrieval quality degrades

### Mechanism 2
- Claim: Injecting hints once at episode start (one-shot retrieval) provides sufficient guidance while limiting token overhead
- Mechanism: Top-k hints (k=3 default) are retrieved via LLM re-ranking and injected after task description; no mid-episode retrieval occurs, forcing the agent to rely on static guidance
- Core assumption: Corrective advice can be determined from initial observation without mid-episode adaptation
- Evidence anchors:
  - [abstract]: "one-shot retrieval at episode start"
  - [section 3.4]: "No additional retrieval occurs during the episode, the agent must rely on this static set of hints for the remainder of the trajectory"
  - [corpus]: Weak — corpus does not address one-shot vs. continuous retrieval tradeoffs
- Break condition: In highly stochastic or long-horizon tasks requiring mid-episode adaptation, static hints may become stale or misleading

### Mechanism 3
- Claim: Training on successful teacher trajectories with hint strings removed forces behavioral internalization rather than text memorization
- Mechanism: Student models are fine-tuned (LoRA) on trajectories where the hint block is stripped; the objective approximates π_ϕ(a_t|o_≤t) ≈ π^RAG_θ(a_t|o_≤t, H_0) without hint dependency
- Core assumption: The teacher's improved action patterns can be learned without explicit access to the hint text
- Evidence anchors:
  - [abstract]: "trains student models on these trajectories with hints removed to force internalization"
  - [section 3.5]: "we remove the hint block from the input to prevent copying. This allows us to internalize the hints into the model weights"
  - [corpus]: Finetune-RAG (arXiv:2505.10792) similarly shows fine-tuning can reduce hallucination in RAG settings, supporting parameter-level learning
- Break condition: If teacher success heavily depends on explicitly reasoning about hint text rather than action pattern improvements, student may fail to internalize

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: The method trains student models to reproduce teacher behavior without runtime dependencies; understanding distillation objectives helps diagnose when students fail to internalize
  - Quick check question: Can you explain why training on teacher outputs without teacher inputs creates parameter-level compression?

- **RAG Architectures**
  - Why needed here: The pipeline converts RAG from runtime dependency to training supervision; you must understand retrieval-relevance tradeoffs to diagnose hint quality issues
  - Quick check question: What factors determine whether retrieved context improves vs. distracts model outputs?

- **Agent Action-Trajectory Representation**
  - Why needed here: Trajectories are serialized as single sequences with system/instruction/observation turns; understanding serialization is critical for debugging training data construction
  - Quick check question: If hint strings are accidentally left in student training data, what behavior would you expect at inference?

## Architecture Onboarding

- **Component map:**
  1. **Stage A**: Base agent rollouts → collect success/failure trajectories
  2. **Stage B**: GPT-4o hint extraction → typed, deduplicated hint banks
  3. **Stage C**: Teacher generation with one-shot RAG (k=3 via Qwen-2.5 7B re-ranking)
  4. **Stage D**: Student training (QLoRA, 4-bit backbone, 16-bit adapters) with hints removed

- **Critical path:** Failure collection → hint quality → teacher trajectory quality → student internalization. Garbage in at any stage propagates.

- **Design tradeoffs:**
  - k=3 hints vs. k=6/9: Paper shows k>6 inflates tokens without accuracy gains (k=9 drops to 76.87% on ALFWorld)
  - One-shot vs. continuous retrieval: Simpler but fails in stochastic environments requiring mid-episode adaptation
  - LoRA rank: 64 for ALFWorld (longer traces) vs. 16 for WebShop (shorter traces)

- **Failure signatures:**
  - Student matches SFT but not RAG performance → hints not properly internalized (check if hint block was removed)
  - RAG underperforms base → hint quality poor or retrieval mis-routed to wrong category
  - 7B WebShop fails with RAG → smaller models may over-constrain on hints (paper notes this explicitly)

- **First 3 experiments:**
  1. **Validate hint extraction quality:** Sample 20 failed trajectories, manually assess whether GPT-4o hints identify root causes vs. surface symptoms
  2. **Ablate retrieval depth:** Run k ∈ {1, 3, 6} on held-out tasks to reproduce paper's finding that k=3 is optimal for both domains
  3. **Sanity check internalization:** Train two students — one with hints included, one with hints removed. If both perform identically, hints weren't providing signal (or removal failed)

## Open Questions the Paper Calls Out
None

## Limitations
- One-shot retrieval approach may fail in highly dynamic or long-horizon environments requiring mid-episode adaptation
- Heavy reliance on GPT-4o for hint extraction with limited empirical validation of hint quality (only 20 sampled hints checked)
- Substantial failure data collection overhead required to build hint banks

## Confidence
- **High Confidence**: Token efficiency improvements, student model performance improvements on ALFWorld, general feasibility of converting RAG to internalized competence
- **Medium Confidence**: WebShop results showing generalization across domains, claim about hint deduplication quality
- **Low Confidence**: The mechanism by which hints improve teacher trajectories - results show improvement but lack ablation studies isolating hint quality

## Next Checks
1. **Hint Quality Audit**: Manually evaluate 100+ sampled hints across different failure types to quantify GPT-4o's diagnostic accuracy and identify systematic failure modes in hint generation
2. **Dynamic Retrieval Stress Test**: Design tasks with high mid-episode stochasticity to test whether one-shot retrieval breaks down compared to continuous retrieval approaches
3. **Data Efficiency Analysis**: Measure hint bank quality and student performance as a function of failure data volume to establish minimum viable dataset sizes and identify diminishing returns points