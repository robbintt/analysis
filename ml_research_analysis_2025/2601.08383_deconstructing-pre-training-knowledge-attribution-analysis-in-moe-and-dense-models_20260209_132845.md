---
ver: rpa2
title: 'Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense
  Models'
arxiv_id: '2601.08383'
source_url: https://arxiv.org/abs/2601.08383
tags:
- arxiv
- neurons
- dense
- importance
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Mixture-of-Experts (MoE) and dense
  Transformer architectures differ in their knowledge acquisition during pre-training.
  The authors introduce a gated neuron attribution metric (Gated-LPI) to measure how
  individual neurons influence log-probability increase across training steps.
---

# Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models

## Quick Facts
- arXiv ID: 2601.08383
- Source URL: https://arxiv.org/abs/2601.08383
- Reference count: 10
- Key outcome: MoE models develop a low-entropy computational backbone with early consolidation, while dense models exhibit continuous neuron turnover

## Executive Summary
This paper introduces a gated neuron attribution metric (Gated-LPI) to measure how individual neurons in Mixture-of-Experts (MoE) and dense Transformer architectures influence log-probability increase during pre-training. By tracking FFN and attention neurons in OLMoE-1B-7B and OLMo-7B models, the authors find that MoE architectures develop a stable low-entropy backbone where top neurons capture disproportionate positive updates, while dense models show continuous neuron turnover. The study reveals that MoE models achieve early layer-wise importance profile stabilization within 100K steps compared to dense model volatility, and demonstrate functional robustness through distributed knowledge storage that maintains performance even when high-importance components are ablated.

## Method Summary
The study uses Gated-LPI to compute neuron importance by measuring log-probability deltas when individual neuron contributions are added to the residual stream. The analysis tracks FFN neurons (columns in W_E^(2)) and attention neurons (columns in W_j^O) across checkpoints at 10K, 100K, 600K, and 1.2M steps. Stability metrics include Jaccard similarity (Jstab) for neuron set overlap, Positive-gain concentration (Rt) for importance distribution, Layer-distribution consistency (ρavg), and Cross-step coefficient of variation (σrel). Ablation experiments mask top-1% FFN neurons and top-10 attention heads to measure functional robustness via HIT@10 degradation on a relational facts dataset with 906 examples across 12 relations.

## Key Results
- Top ~1% of MoE neurons capture over 45% of positive updates versus 6% in dense models
- MoE models stabilize layer-wise importance profiles within <100K steps (ρavg 0.97) versus dense model volatility (ρavg 0.69)
- Masking top 1% FFN neurons reduces relational HIT@10 by ~35% in MoE versus ~96% in dense models
- Masking top 10 attention heads reduces HIT@10 by <10% in MoE versus >50% in dense models

## Why This Works (Mechanism)

### Mechanism 1: Sparsity-Induced Neuron Selection and Reinforcement
- **Claim:** MoE's sparse routing creates a low-entropy backbone where a small subset of neurons receives disproportionately many positive updates and remains stable throughout training.
- **Mechanism:** The Top-k gating mechanism selectively routes tokens to specific experts. Neurons within frequently-selected experts receive repeated gradient signals, causing them to specialize early. This creates positive feedback: important neurons get more reinforcement → become more important → get selected more.
- **Core assumption:** The gating scores correlate with neuron utility; routing decisions are not random noise.
- **Evidence anchors:** "The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline" and "OLMoE's Jaccard curve follows a three-phase explore–refine–consolidate trajectory... climbs monotonically to 44.2% by 1.2M steps" with "Rt ≈ 45–52%" concentration during consolidation.
- **Break condition:** If expert load-balancing losses force uniform expert selection, the concentration effect should diminish.

### Mechanism 2: Early Layer-wise Importance Profile Locking
- **Claim:** MoE architectures cause layer-level importance distributions to stabilize within the first ~100K training steps, whereas dense models remain volatile.
- **Mechanism:** Sparse activation distributes computation non-uniformly across layers and experts. Once routing patterns stabilize, each layer's contribution to the loss becomes predictable and locked-in. Dense models lack this routing constraint, allowing continuous reorganization.
- **Core assumption:** Layer importance is measurable via Gated-LPI and reflects genuine functional specialization.
- **Evidence anchors:** "The MoE model locks into a stable importance profile within <100K steps, whereas the dense model remains volatile throughout training" and "OLMoE achieves a near-perfect ρavg of 0.97 between 10K and 600K steps... σrel is an order of magnitude lower than OLMo's (0.37 vs. 5.01)"
- **Break condition:** If training data distribution shifts dramatically mid-training, early consolidation may become maladaptive.

### Mechanism 3: Distributed Knowledge Storage via Redundancy
- **Claim:** MoE's sparsity promotes distributed knowledge encoding, making the model robust to ablation of high-importance components.
- **Mechanism:** Knowledge is spread across multiple experts and heads rather than concentrated. When top components are masked, other neurons can partially compensate because (a) multiple experts learn overlapping functions, and (b) routing can adapt to activate alternatives.
- **Core assumption:** Distributed storage is a property of the architecture, not just training duration or data.
- **Evidence anchors:** "Masking the ten most important MoE attention heads reduces relational HIT@10 by <10%, compared with >50% for the dense model, showing that sparsity fosters distributed—rather than brittle—knowledge storage" and "OLMoE exhibits a moderate performance reduction of 35.5% [for top-1% FFN ablation]... OLMo nearly collapses, with a 96.1% mean drop."
- **Break condition:** If redundancy is primarily due to under-trained experts rather than architectural design, longer training could increase brittleness.

## Foundational Learning

- **Concept: Log-Probability Increase (LPI) Attribution**
  - **Why needed here:** The paper's core metric (Gated-LPI) extends LPI to MoE by measuring how individual neurons affect target token log-probability. Without understanding LPI, the attribution scores are uninterpretable.
  - **Quick check question:** If a neuron has I(v) = +0.5 for predicting "Paris" given "The capital of France is ___", what does this mean?

- **Concept: Top-k Routing in MoE**
  - **Why needed here:** The paper assumes Top-k routing (selecting k experts per token) as the gating mechanism. Understanding this is essential for interpreting why certain neurons get reinforced.
  - **Quick check question:** In Top-8 routing with 64 experts per layer, what fraction of expert parameters are active for any single token?

- **Concept: Jaccard Similarity for Set Stability**
  - **Why needed here:** The paper uses Jaccard overlap to quantify neuron stability across checkpoints. Understanding this metric is necessary to interpret the Jstab values reported.
  - **Quick check question:** If Jstab = 0.44 between consecutive checkpoints for top-1% neurons, what fraction of top neurons changed?

## Architecture Onboarding

- **Component map:** Input token → [Embedding] → [Attention Layer] → [MoE Layer] → Output → Gate Network → Top-k Expert Scores → [64 Expert FFNs] → Weighted Sum
- **Critical path:** Token embedding enters layer → Gate computes scores for all 64 experts → Top-8 experts selected, outputs weighted by gate scores → Expert outputs summed and added to residual stream → Gated-LPI traces which neurons contributed to log-prob increase
- **Design tradeoffs:**
  - **Capacity vs. Compute:** MoE decouples these; more experts = higher capacity without proportional FLOP increase
  - **Stability vs. Adaptability:** Early consolidation (MoE) improves robustness but may reduce plasticity for new knowledge
  - **Distributed vs. Concentrated:** Distributed storage improves ablation robustness but may reduce interpretability (harder to locate "knowledge neurons")
- **Failure signatures:**
  - **Expert collapse:** Few experts receive all routing weight (check expert utilization histogram)
  - **Routing instability:** High variance in which experts are selected for similar inputs across checkpoints
  - **Neuron churn (dense):** Low Jaccard overlap persisting late into training indicates failure to consolidate
- **First 3 experiments:**
  1. **Replicate Jstab analysis:** Extract top-1% neurons at consecutive checkpoints, compute Jaccard overlap. Compare MoE vs. dense on same token budget (not same step count).
  2. **Layer importance correlation:** Compute importance scores per layer at steps 50K, 100K, 600K. Calculate ρavg to verify early locking in MoE.
  3. **Targeted ablation test:** Mask top-1% FFN neurons and top-10 attention heads. Measure HIT@10 drop. Confirm MoE shows <10% drop for attention heads and <40% for FFN neurons.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the "early consolidation" observed in MoE models an artifact of the training data composition rather than the architectural sparsity?
- **Basis in paper:** [inferred] The authors acknowledge that OLMoE was trained on OLMoE-MIX (heavier on code/math) while OLMo used Dolma, noting this composition difference is a "potential confound."
- **Why unresolved:** The study compares two specific models with differing data regimes, making it difficult to isolate whether the rapid stability is due to the MoE architecture or the structural regularity of code/math data.
- **What evidence would resolve it:** A controlled ablation study pre-training both MoE and dense models on the exact same corpus and comparing the Jaccard stability ($J_{stab}$) trajectories.

### Open Question 2
- **Question:** Can "stability-aware expert pruning" or "explore-then-stabilize" routing strategies utilize the identified "low-entropy backbone" to improve training efficiency?
- **Basis in paper:** [explicit] The conclusion states these findings offer "insights for the design of stability-aware expert pruning and explore–then–stabilize routing strategies in future."
- **Why unresolved:** The current work is a diagnostic analysis (using Gated-LPI) of fully trained models; it does not implement or test dynamic training algorithms that act on this neuron-level stability data in real-time.
- **What evidence would resolve it:** Developing a routing mechanism that freezes or prunes experts based on the stabilization metrics ($\rho_{avg}$) defined in the paper, then measuring training speedup and final performance.

### Open Question 3
- **Question:** Do the functional robustness and "distributed" knowledge storage of MoE models hold for abstract reasoning or long-context tasks, or are they limited to relational fact retrieval?
- **Basis in paper:** [inferred] The analysis relies exclusively on a "relational facts dataset" (HIT@10) to measure knowledge attribution and robustness to ablation.
- **Why unresolved:** Relational facts (e.g., capitals, antonyms) may be stored differently than procedural knowledge or complex reasoning capabilities; the distributed nature might be task-specific.
- **What evidence would resolve it:** Repeating the neuron masking (ablation) experiments on benchmarks requiring multi-step reasoning (e.g., GSM8K) or long-context retrieval to see if the ~35% performance drop persists.

## Limitations
- Potential corpus difference confound between OLMoE (OLMoE-MIX) and OLMo (Dolma) training data affecting routing patterns
- Gated-LPI attribution assumes log-probability changes accurately reflect neuron importance, which may be context-dependent
- Stability metrics are sensitive to checkpoint frequency and may not capture fine-grained dynamics between checkpoints

## Confidence

**High Confidence:**
- MoE neurons capture disproportionately more positive updates than dense neurons (45% vs. 6% for top 1%)
- MoE achieves earlier layer-wise importance profile stabilization (ρavg 0.97 vs 0.69 at 600K steps)
- MoE ablation shows functional robustness (35.5% vs 96.1% HIT@10 drop for top-1% FFN neurons)

**Medium Confidence:**
- MoE develops a true "low-entropy computational backbone" rather than just having more training steps per active neuron
- The distributed knowledge storage is architecturally determined rather than a consequence of longer effective training time per expert
- Early consolidation is adaptive rather than potentially premature for future knowledge acquisition

**Low Confidence:**
- Claims about distributed vs. brittle knowledge storage mechanisms are primarily correlational
- The functional robustness extends to all knowledge types rather than being specific to the relational facts dataset
- Neuron churn differences are solely due to architectural constraints rather than optimization dynamics

## Next Checks
1. **Shared Corpus Control Experiment:** Retrain both OLMoE and OLMo on the same corpus (e.g., Dolma) for 600K steps and repeat the Gated-LPI analysis to isolate architectural effects from corpus-specific routing patterns.

2. **Intermediate Checkpoint Density Analysis:** Systematically vary checkpoint frequency (e.g., every 10K, 50K, 100K steps) and recompute stability metrics to determine if early consolidation is a true architectural feature or an artifact of coarse-grained observation.

3. **Alternative Attribution Method Validation:** Apply integrated gradients or attention rollout methods as alternative attribution techniques on the same checkpoints to test whether observed neuron importance patterns are method-specific or reflect genuine computational roles.