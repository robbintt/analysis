---
ver: rpa2
title: Enhancing Decision-Making of Large Language Models via Actor-Critic
arxiv_id: '2506.06376'
source_url: https://arxiv.org/abs/2506.06376
tags:
- steps
- action
- forward
- step
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in large language model (LLM) decision-making
  by introducing a novel Actor-Critic framework that integrates action evaluations
  with policy optimization. The method uses token logits associated with positive/negative
  outcomes, enhanced by future trajectory rollouts and reasoning, to estimate action
  values, and employs a gradient-free policy improvement mechanism.
---

# Enhancing Decision-Making of Large Language Models via Actor-Critic

## Quick Facts
- **arXiv ID**: 2506.06376
- **Source URL**: https://arxiv.org/abs/2506.06376
- **Reference count**: 40
- **One-line primary result**: Novel Actor-Critic framework using LLM-generated action values from sentiment token logits achieves competitive performance with 7B/8B models and surpasses GPT-4 baselines on complex decision-making tasks.

## Executive Summary
This paper introduces a novel Actor-Critic framework that enhances LLM decision-making by integrating action evaluations with policy optimization. The method extracts action values from token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning, and employs a gradient-free policy improvement mechanism. Experiments on ALFWorld, BabyAI-Text, and WebShop demonstrate consistent superiority over state-of-the-art methods, achieving competitive performance with 7B/8B parameter LLMs and even surpassing GPT-4-based baselines on complex tasks.

## Method Summary
The framework uses token logits associated with positive/negative outcomes to estimate action values. It prompts the LLM to evaluate actions after simulated rollouts by predicting "GOOD"/"BAD" outcome tokens, computing Q-values from the log-likelihood ratio. A gradient-free policy update mechanism re-weights the action distribution at inference time using KL-constrained optimization. The method employs a closed-form policy update balancing the LLM prior and critic evaluation, and enhances accuracy through future trajectory rollouts before evaluation.

## Key Results
- LAC consistently outperforms state-of-the-art methods across ALFWorld, BabyAI-Text, and WebShop benchmarks
- Achieves competitive performance with 7B/8B parameter LLMs while surpassing GPT-4-based baselines on complex tasks
- Demonstrates superior decision-making in long-horizon planning scenarios through ablation studies showing the importance of rollouts and reflection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Extracting action values ($Q_{LLM}$) from the log-likelihood ratio of positive-to-negative sentiment tokens provides a more robust evaluation signal than direct prompting.
- **Mechanism**: The framework prompts the LLM to evaluate an action (after a simulated rollout) by predicting a binary outcome token. It computes $Q_{LLM} = \log \frac{P(\text{GOOD})}{P(\text{BAD})}$. This utilizes the model's internal confidence (logits) rather than its generative text output, mapping the latent "belief" state to a scalar value.
- **Core assumption**: The log-probability distribution of specific sentiment tokens correlates linearly (or monotonically) with the true probability of task success, and the model has sufficient domain knowledge to distinguish states.
- **Evidence anchors**:
  - [abstract]: "...extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes..."
  - [section 4.1.2]: "We let the generated probabilities of 'GOOD' and 'BAD' represent $P(y_w|\dots)$ and $P(y_l|\dots)$... this formulation is simple and effective."
- **Break condition**: If the LLM lacks the domain knowledge to distinguish good states, the logits will reflect random guesswork, turning the critic into noise.

### Mechanism 2
- **Claim**: A gradient-free, closed-form policy update balancing the LLM prior and the Critic evaluation improves decision-making without the computational cost of backpropagation.
- **Mechanism**: Instead of updating weights, the method resamples the action distribution at inference time. It optimizes a KL-constrained objective to produce a new policy $\pi_{new} \propto \pi_{LLM} \cdot \exp(\alpha Q_{LLM})$. This re-weights the prior probabilities of candidate actions based on their evaluated Q-values.
- **Core assumption**: The LLM's prior policy $\pi_{LLM}$ is a strong baseline that must be preserved (via KL constraint) while allowing the critic to "nudge" the probability toward higher-value actions. It also assumes the set of sampled candidate actions contains the optimal action.
- **Evidence anchors**:
  - [abstract]: "...employs a gradient-free policy improvement mechanism."
  - [section 4.2]: "Equation (6) updates the action distribution... in a gradient-free way... lower computation burden compared to gradient-based methods."
- **Break condition**: If the candidate action set (sampled from the prior) is too small or low quality, the re-weighting mechanism cannot select the optimal action because it was never generated.

### Mechanism 3
- **Claim**: Simulating future trajectories (rollouts) before evaluation enhances the accuracy of the Critic by providing temporal context.
- **Mechanism**: Before calculating the Q-value, the LLM acts as a world model ($f_{LLM}$) to generate a hypothetical future trajectory ($u_t$) for a candidate action. The Critic evaluates the action conditioned on this future trajectory, allowing it to assess long-term consequences rather than immediate state only.
- **Core assumption**: The LLM possesses sufficient world-modeling capabilities to simulate realistic future states (imagination) that correlate with actual environment dynamics.
- **Evidence anchors**:
  - [abstract]: "...enhanced by future trajectory rollouts and reasoning..."
  - [section 4.1.2]: "Trajectory rollouts are especially important in tasks where the outcomes of actions may unfold over several steps."
- **Break condition**: If the environment is highly stochastic or the LLM hallucinates significantly during the rollout, the "imagined" future will diverge from reality, leading the Critic to optimize for a phantom future.

## Foundational Learning

- **Concept**: **KL Divergence Constraints in Optimization**
  - **Why needed here**: The policy update relies on minimizing KL divergence to prevent the new policy from straying too far from the stable pre-trained LLM prior. Without understanding this constraint, one might misinterpret the "gradient-free" update as arbitrary re-weighting.
  - **Quick check question**: If $\alpha \to \infty$ in Equation 6, does the policy rely entirely on the LLM prior or the Critic?

- **Concept**: **Logits vs. Softmax Probabilities**
  - **Why needed here**: The mechanism explicitly uses "token logits" (unnormalized scores) to derive Q-values via a log-ratio. Understanding that this reflects the model's raw confidence (internal belief) rather than just the output text is crucial.
  - **Quick check question**: Why is the log-ratio $\log(P_{pos}/P_{neg})$ preferred over just $P_{pos}$ for representing value?

- **Concept**: **Actor-Critic Architecture**
  - **Why needed here**: This paper adapts the classic RL paradigm where an Actor (policy) proposes actions and a Critic (value function) evaluates them. You must understand the separation of concerns to debug why LAC might fail (e.g., Actor generates bad candidates vs. Critic mis-evaluates them).
  - **Quick check question**: In LAC, does the Critic update the Actor's weights or its action distribution at inference time?

## Architecture Onboarding

- **Component map**: Prompt Manager -> Actor ($\pi_{LLM}$) -> World Model ($f_{LLM}$) -> Critic ($Q_{LLM}$) -> Optimizer
- **Critical path**: The **Critic Evaluation** (Section 4.1). The quality of the decision hinges on the "GOOD"/"BAD" token logits being a reliable proxy for success. If this correlation breaks, the re-weighting is random.
- **Design tradeoffs**:
  - **Latency vs. Accuracy**: Performing rollouts ($f_{LLM}$) and Critic evaluations adds significant latency per step compared to auto-regressive generation (ReAct).
  - **Prior vs. Critic**: The hyperparameter $\alpha$ controls trust. Low $\alpha$ trusts the LLM prior (safe but potentially sub-optimal); High $\alpha$ trusts the Critic (risky if the world model hallucinates).
- **Failure signatures**:
  - **Hallucination Cascades**: If the World Model predicts a false positive trajectory (e.g., "You pick up the key" when it's impossible), the Critic will assign a high Q-value to a doomed action.
  - **Conservative Loops**: If $\alpha$ is too low or the Critic is uncertain, the agent may repeat safe but useless actions (e.g., "checking the same drawer").
  - **Token Misalignment**: The Critic might output "GOOD" textually while the logit probability is low; implementation must strictly use logits, not parsed text.
- **First 3 experiments**:
  1. **Sanity Check (BabyAI-Text)**: Implement the "go to" task with a small LLM. Verify that the Q-values for "go forward" (toward goal) are higher than "turn left" (away) by plotting the logit distributions.
  2. **Ablation (No Rollout)**: Remove the World Model ($f_{LLM}$) component and run on ALFWorld. Confirm performance drops (as shown in Figure 4) to validate that immediate-state evaluation is insufficient for long-horizon tasks.
  3. **Hyperparameter Sensitivity**: Sweep $\alpha$ on a single task type (e.g., "pick up"). Determine if there is a stable region where performance is consistent, or if it varies wildly by LLM backbone (Table 12 suggests different optimal $\alpha$ for CodeLlama vs. Gemma).

## Open Questions the Paper Calls Out

- **Question**: Can the LAC framework's Q-value estimation be theoretically and empirically adapted to handle environments with continuous, non-sparse rewards rather than relying on binary success/failure outcomes?
- **Basis in paper**: [explicit] The Discussion section notes that while the current method handles continuous rewards empirically by treating the highest reward as "success," it is a "future direction to develop a more principal method to deal with such situations."
- **Why unresolved**: The current formulation of the critic (Eq. 3) relies on the log-odds of positive/negative tokens (e.g., "GOOD"/"BAD"), which maps naturally to binary outcomes but lacks a theoretical grounding for dense, scalar reward signals.
- **What evidence would resolve it**: A reformulation of the Q-function estimation that maps continuous reward magnitudes to LLM token probabilities, validated on benchmarks with dense reward structures (e.g., cumulative scoring environments).

- **Question**: How can LAC be modified to mitigate the "overthinking" tendency of specialized reasoning models (e.g., DeepSeek-R1) that fail to output direct environmental actions?
- **Basis in paper**: [explicit] Appendix A.13 reports that reasoning LLMs often generate detailed explanations but "avoid selecting the next action" despite explicit prompting, and suggests this requires "deeper exploration."
- **Why unresolved**: The current LAC framework relies on the base LLM's ability to switch between reasoning (critic/rollout) and acting (action generation), a balance that breaks when the model is optimized solely for internal reasoning chains.
- **What evidence would resolve it**: Experiments applying LAC to reasoning-specialized models using constrained decoding or specialized prompts that force the generation of actionable tokens after the reasoning phase.

- **Question**: Does applying reflection *after* action generation to verify predicted trajectories improve success rates compared to the current pre-generation reflection mechanism?
- **Basis in paper**: [explicit] The Discussion section states that reflection is currently only used before action generation and proposes applying it after generation to "re-sample candidate actions if the previous candidate actions all fail to complete the target task."
- **Why unresolved**: The current implementation uses reflection to guide the initial sampling, but it does not verify the validity of the selected action's predicted trajectory before execution, potentially allowing flawed plans to proceed.
- **What evidence would resolve it**: An ablation study comparing the current architecture against a variant where reflection acts as a post-hoc verifier to trigger re-sampling of actions with low Q-values.

## Limitations
- **Scalability to multi-turn, real-time environments**: The computational overhead of rollouts and candidate evaluations may be prohibitive in fast-paced environments requiring rapid decisions.
- **Dependence on high-quality training trajectories**: The method relies on annotated trajectories with explicit GOOD/BAD reflections, which may be scarce or costly to generate in many domains.
- **Implicit world model assumption**: The framework treats the LLM as a world model for rollouts, but this is untested for novel or highly complex environments where the model lacks pre-training coverage.

## Confidence
- **High confidence**: The gradient-free policy update mechanism (Mechanism 2) is well-specified and directly implementable from the equations provided.
- **Medium confidence**: The Q-value estimation via logit ratios (Mechanism 1) is plausible but relies on an untested assumption about the linear correlation between token logits and task success probabilities.
- **Medium confidence**: The value of trajectory rollouts (Mechanism 3) is supported by ablation results (Figure 4) but not independently verified across all benchmarks.

## Next Checks
1. **Zero-shot transfer test**: Apply LAC to a novel household task (e.g., "set the table") not in the ALFWorld training set. Measure whether the critic can still assign meaningful Q-values without prior exposure.
2. **Rollout ablation with stochasticity**: Re-run BabyAI-Text experiments with injected environmental randomness (e.g., slippery floors causing failed moves). Compare performance with/without rollouts to quantify robustness to uncertainty.
3. **Computational overhead profiling**: Instrument LAC to measure wall-clock time per decision across all three benchmarks. Compare against ReAct and standard auto-regressive baselines to confirm the claimed efficiency advantage.