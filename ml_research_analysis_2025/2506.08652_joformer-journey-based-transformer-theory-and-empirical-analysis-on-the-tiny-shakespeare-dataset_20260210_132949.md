---
ver: rpa2
title: 'JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the
  Tiny Shakespeare Dataset'
arxiv_id: '2506.08652'
source_url: https://arxiv.org/abs/2506.08652
tags:
- joformer
- positional
- position
- each
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JoFormer, a journey-based Transformer architecture
  that improves upon standard relative positional encodings by incorporating a learnable,
  non-commutative composition of positional transforms. Grounded in a theoretical
  framework for directional, non-commutative algebra, JoFormer represents the relative
  position between tokens as a product of learnable transforms along the sequence
  path, enabling richer, context-dependent positional biases.
---

# JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset

## Quick Facts
- arXiv ID: 2506.08652
- Source URL: https://arxiv.org/abs/2506.08652
- Authors: Mahesh Godavarti
- Reference count: 5
- Primary result: JoFormer improves relative positional encoding by modeling token positions as products of learnable, non-commutative transforms, showing lower perplexity and faster convergence on Tiny Shakespeare vs. RoFormer baseline.

## Executive Summary
This paper introduces JoFormer, a journey-based Transformer architecture that improves upon standard relative positional encodings by incorporating a learnable, non-commutative composition of positional transforms. Grounded in a theoretical framework for directional, non-commutative algebra, JoFormer represents the relative position between tokens as a product of learnable transforms along the sequence path, enabling richer, context-dependent positional biases. Evaluated on the Tiny Shakespeare character-level language modeling task, JoFormer variants consistently outperform a RoFormer baseline with rotary position embeddings, achieving lower perplexity and faster convergence—particularly in shallow models. The per-token variant, which learns token-specific rotations, shows the strongest gains, though the advantage diminishes with deeper architectures. JoFormer thus offers a principled, expressive alternative to fixed positional encodings, bridging the structured state evolution of SSMs with the flexible content-based attention of Transformers. The approach shows promise for enhancing positional modeling in sequence tasks while remaining computationally efficient.

## Method Summary
JoFormer introduces a journey-based relative positional encoding by representing the relative position between tokens as a product of learnable, non-commutative transforms along the sequence path. This approach is grounded in a theoretical framework for directional, non-commutative algebra, enabling richer, context-dependent positional biases. The architecture modifies standard Transformer self-attention by replacing fixed or learned relative positional encodings with a composition of journey-based transforms. These transforms are applied as part of the attention computation, allowing the model to capture the "journey" or path between tokens rather than just their absolute or fixed relative positions. The paper evaluates JoFormer on the Tiny Shakespeare character-level language modeling task, comparing its performance to a RoFormer baseline with rotary position embeddings. Several variants are explored, including per-token-specific rotations, with the per-token variant showing the strongest gains in perplexity and convergence speed, especially in shallower models.

## Key Results
- JoFormer consistently outperforms RoFormer with rotary position embeddings on Tiny Shakespeare, achieving lower perplexity and faster convergence.
- The per-token variant, which learns token-specific rotations, shows the strongest performance gains, particularly in shallow models.
- The advantage of JoFormer diminishes with deeper architectures, suggesting diminishing returns in deeper stacks.

## Why This Works (Mechanism)
JoFormer's journey-based positional encoding leverages non-commutative algebra to model the "journey" or path between tokens, rather than relying on fixed or absolute positional encodings. By representing relative positions as products of learnable transforms, the model can capture context-dependent positional biases, allowing for richer interactions between tokens. This compositional approach enables the model to encode not just the distance between tokens, but also the sequence of transformations along the path, which is particularly beneficial in shallow architectures where positional information is more critical. The use of learnable, non-commutative transforms also introduces flexibility, allowing the model to adapt positional representations to the specific characteristics of the input sequence.

## Foundational Learning
- **Non-commutative algebra**: Needed to model the order-dependent transformations between tokens; quick check: verify that transform composition order matters in experiments.
- **Relative positional encoding**: Needed to capture token relationships without relying on absolute positions; quick check: compare against absolute positional baselines.
- **Compositional positional modeling**: Needed to represent the "journey" between tokens as a sequence of transforms; quick check: ablate the compositional vs. fixed encoding components.
- **Journey-based path representation**: Needed to encode the sequence of transformations along the token path; quick check: visualize or analyze the learned journey transforms.
- **Learnable positional transforms**: Needed to adapt positional encodings to input-specific contexts; quick check: measure the impact of freezing vs. learning the transforms.

## Architecture Onboarding
- **Component map**: Input tokens → Learnable non-commutative transforms → Journey-based positional encoding → Modified self-attention → Output
- **Critical path**: Token embeddings → Positional transform composition → Attention computation → Residual connections → Feed-forward network
- **Design tradeoffs**: Flexibility vs. parameter efficiency (learnable transforms add parameters but capture richer positional information); depth sensitivity (advantage diminishes in deeper models).
- **Failure signatures**: Poor generalization to longer sequences or different tasks; overfitting in shallow models due to high parameter count; vanishing benefits in deeper stacks.
- **First experiments**: (1) Ablation study: remove compositional encoding, compare to fixed relative encodings; (2) Scalability test: evaluate on longer sequences; (3) Cross-task evaluation: test on non-character-level NLP tasks.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Evaluation is limited to a single, small-scale character-level language modeling dataset (Tiny Shakespeare), limiting generalizability.
- The paper does not address scalability concerns, such as computational overhead or performance in very long sequences.
- Ablation studies on architectural components are absent, leaving ambiguity about which design choices drive improvements.

## Confidence
- **High Confidence**: The experimental results on Tiny Shakespeare are internally consistent, and the architecture is clearly described and implementable. The theoretical motivation for non-commutative positional transforms is sound and well-grounded.
- **Medium Confidence**: The relative performance gains over RoFormer with rotary embeddings are plausible but may not generalize to other datasets or tasks. The diminishing advantage in deeper models is observed but not thoroughly explained.
- **Low Confidence**: Claims about bridging SSMs and Transformers or the broader applicability of JoFormer’s compositional approach are speculative without further empirical support.

## Next Checks
1. Evaluate JoFormer on multiple, larger-scale language modeling benchmarks (e.g., WikiText, PG-19) and diverse NLP tasks (e.g., summarization, translation) to assess generalizability.
2. Conduct ablation studies to isolate the impact of each component of the journey-based positional encoding and compare JoFormer to other recent positional encoding methods.
3. Analyze the computational overhead and memory usage of JoFormer, especially in deep or long-sequence settings, and test robustness to distributional shifts and out-of-distribution data.