---
ver: rpa2
title: Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training
arxiv_id: '2505.08971'
source_url: https://arxiv.org/abs/2505.08971
tags:
- arxiv
- prior
- wang
- tokens
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRIOR is a vision-language pre-training method that prioritizes
  image-related tokens to enhance learning. It addresses the issue where standard
  next-token prediction (NTP) models treat all caption tokens equally, leading to
  noise fitting and hallucination.
---

# Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training

## Quick Facts
- arXiv ID: 2505.08971
- Source URL: https://arxiv.org/abs/2505.08971
- Reference count: 40
- Key outcome: PRIOR improves VLPT by 19% (H-LVLM) and 8% (U-LVLM) on average benchmarks

## Executive Summary
PRIOR addresses the limitation in standard next-token prediction (NTP) for vision-language pre-training where all caption tokens are treated equally, leading to noise fitting and hallucination. The method introduces a text-only reference LLM that assigns importance scores to tokens based on their predictability from text alone—tokens harder to predict are deemed more image-related. These scores reweight the NTP loss during LVLM training, emphasizing visually grounded tokens. Implemented in both LVLM architectures with and without pre-trained visual encoders, PRIOR achieved significant improvements on multiple vision-language benchmarks while demonstrating better training stability and scaling behavior.

## Method Summary
PRIOR works by training a text-only reference LLM on caption data to compute token importance scores based on predictability from text alone. For each token, the score w_i = (1 - p_r(t_i | t_{<i}))^α inversely maps reference probability to importance, where low reference probability indicates high visual dependency. During LVLM training, the NTP loss is reweighted by normalized importance scores, creating a target distribution that emphasizes tokens requiring visual context. The method requires shared tokenizers between reference LLM and LVLM, and importance scores are computed offline before training begins.

## Key Results
- 19% average relative improvement on MME-Perception, MME-Reasoning, MMStar, POPE, MMBench, and SEEDBench for H-LVLM
- 8% average relative improvement for U-LVLM architecture
- Better training stability and more predictable scaling behavior compared to NTP
- Improved performance on both image-related and image-unrelated tokens compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
Tokens predictable from text alone carry minimal visual information, while tokens surprising to a text-only model are more likely image-related. A reference LLM trained on captions without images assigns low probability to image-dependent tokens (e.g., "house," "front yard") and high probability to syntactic/contextual tokens. The importance score w_i = (1 - p_r(t_i | t_{<i}))^α inversely maps reference probability to token importance. Core assumption: Text-only LLM predictive difficulty correlates with visual dependency. Evidence: Human annotation shows only 31.3% of caption words directly relate to images.

### Mechanism 2
Reweighting NTP loss via importance sampling shifts optimization focus toward visually grounded tokens while preserving language coherence. The loss L_PRIOR = -Σ(k · w_i / Σw_j) log p(t_i | v, t_{<i}) upweights tokens with high importance scores. Normalization ensures a proper distribution; scaling factor k preserves loss magnitude. Core assumption: Upweighting image-related tokens improves vision-language alignment without degrading general language modeling. Evidence: PRIOR achieves lower loss on both image-related and image-unrelated tokens compared to NTP.

### Mechanism 3
Prioritizing image-related tokens reduces overfitting to caption noise and mitigates hallucination. Standard NTP forces the model to predict all tokens, including those unrelated to the image. By downweighting text-predictable tokens (e.g., location, price in Fig. 1), PRIOR reduces pressure to hallucinate visual content for non-visual caption elements. Core assumption: Hallucinations partly stem from models attempting to ground non-visual caption content. Evidence: Only 31.3% of caption tokens are image-related; NTP treats all equally.

## Foundational Learning

- **Importance Sampling**: Why needed: PRIOR frames loss reweighting as sampling from a target distribution that emphasizes visually grounded tokens. Quick check: Can you explain why self-normalized importance weights reduce variance at the cost of bias?
- **Next-Token Prediction (NTP) in LVLMs**: Why needed: Understanding the baseline objective reveals why equal treatment of all tokens is suboptimal for vision-language alignment. Quick check: What happens when an LVLM optimizes NTP on captions with low image relevance?
- **Mutual Information I(V; T)**: Why needed: Appendix C justifies PRIOR via mutual information—tokens with larger log p(t|v) - log p(t) gaps benefit most from visual context. Quick check: How does maximizing I(v; t) relate to prioritizing tokens with low text-only predictability?

## Architecture Onboarding

- **Component map**: Reference LLM (text-only) -> Importance Score Computation (offline) -> LVLM Training Loop (reweighted NTP)
- **Critical path**: Prepare caption corpus → train reference LLM on text only → run inference to compute token-level importance scores → modify LVLM training with weighted loss
- **Design tradeoffs**: α=1 balances focus on image-related tokens vs. training stability; α>2 degrades performance. Scaling factor k maintains loss magnitude but adds hyperparameter. Reference LLM must share tokenizer with LVLM—limits dataset portability.
- **Failure signatures**: Performance plateaus early with α>2 (token weights too extreme); no improvement over NTP (check if reference LLM overfits or underfits); high variance in training loss (verify normalization).
- **First 3 experiments**: 1) Sanity check: train reference LLM, visualize importance scores on held-out examples; 2) Ablation α: train H-LVLM with α ∈ {0.5, 1, 2, 4} for 1000 steps; 3) Scaling comparison: train NTP vs. PRIOR on 7M, 21M, 70M tokens; fit scaling laws.

## Open Questions the Paper Calls Out

### Open Question 1
Can PRIOR be extended to enable cross-tokenizer transfer of importance scores, eliminating the requirement that LVLMs and reference LLMs share the same tokenizer? The current design depends on exact token alignment between reference model predictions and LVLM training, with no mechanism proposed for transferring importance scores across different vocabularies.

### Open Question 2
How does the size and capability of the reference LLM affect the quality of importance scores and downstream LVLM performance? Only one reference model configuration is tested, and the relationship between reference model capacity and importance score quality remains unexplored.

### Open Question 3
Does PRIOR's differential token weighting amplify or mitigate demographic and cultural biases present in web-scale caption datasets? The broader impacts section notes the differential weighting approach may unintentionally amplify existing biases in training data, but no empirical analysis of bias-related outcomes is conducted.

## Limitations
- PRIOR's effectiveness depends on caption statistics and may not transfer to domains with different caption-image relationships
- Importance scores rely on reference LLM quality; inaccurate reference models produce poor importance weights
- Tokenization artifacts may occur since single words may split into multiple tokens with different importance scores

## Confidence
**High Confidence**:
- PRIOR improves benchmark performance on tested architectures and datasets
- The reweighting mechanism (importance sampling formulation) is mathematically correct
- Token importance scores correlate with human judgments of image-relatedness

**Medium Confidence**:
- PRIOR reduces hallucination specifically through the proposed mechanism
- The method generalizes to other VLPT datasets and architectures
- The mutual information justification fully explains the empirical gains

**Low Confidence**:
- PRIOR eliminates hallucination completely
- The method scales optimally to billion-parameter models without modification
- The reference LLM importance scores are the optimal way to identify image-related tokens

## Next Checks
1. **Cross-Dataset Validation**: Apply PRIOR to a different VLPT dataset (e.g., LAION-5B) and evaluate whether the 19% average relative improvement holds.
2. **Hallucination-Specific Analysis**: Design an experiment isolating hallucination cases and measure whether PRIOR specifically reduces hallucination rates compared to NTP.
3. **Reference Model Ablation**: Train reference LLMs with different architectures (smaller/larger models, different training objectives) and evaluate how reference model quality affects PRIOR's performance.