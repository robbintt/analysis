---
ver: rpa2
title: 'Large Multimodal Models for Low-Resource Languages: A Survey'
arxiv_id: '2502.05568'
source_url: https://arxiv.org/abs/2502.05568
tags:
- languages
- language
- multimodal
- https
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically analyzes techniques for adapting large
  multimodal models to low-resource languages, examining 117 studies across 96 languages.
  It identifies three key research trends: multimodal data creation (from scratch
  or through dataset extension), synthetic data generation (via back-translation or
  image-based methods), and architectural innovations (including parameter-efficient
  adaptation and cross-lingual transfer).'
---

# Large Multimodal Models for Low-Resource Languages: A Survey

## Quick Facts
- **arXiv ID:** 2502.05568
- **Source URL:** https://arxiv.org/abs/2502.05568
- **Reference count:** 40
- **Primary result:** Systematic analysis of 117 studies across 96 low-resource languages, identifying key trends in multimodal data creation, synthetic generation, and architectural innovations

## Executive Summary
This survey systematically analyzes techniques for adapting large multimodal models to low-resource languages, examining 117 studies across 96 languages. It identifies three key research trends: multimodal data creation (from scratch or through dataset extension), synthetic data generation (via back-translation or image-based methods), and architectural innovations (including parameter-efficient adaptation and cross-lingual transfer). The study finds that while text-image combinations dominate (65% of research), significant gaps remain in audio and video modalities, which are crucial for predominantly oral languages. Performance varies by fusion strategy, with early fusion generally achieving highest accuracy but at greater computational cost. The survey highlights the need for culturally-grounded evaluation frameworks and community-centered approaches to address linguistic diversity and data sovereignty concerns.

## Method Summary
The survey conducted a systematic literature review of 117 papers published between 2018-2025, focusing on Large Multimodal Models (LMMs) for Low-Resource (LR) languages. The methodology involved searching specific digital libraries (ACL, arXiv, etc.) and categorizing approaches into three main research directions: multimodal data creation, synthetic data generation, and architectural innovations. The review includes comparative analysis of fusion strategies (early vs. late vs. architectural) and parameter-efficient adaptation methods, with experimental benchmarks cited from Multi30K, COCO, Flickr30k, and LR-specific datasets like HausaVG and DravidianMultiModality.

## Key Results
- Parameter-efficient methods achieve competitive performance with 3-5 orders of magnitude fewer trainable parameters (e.g., XtremeCLIP: 5-7K vs. 149M for full fine-tuning)
- Early fusion generally achieves highest accuracy but requires 2-3× more memory than late fusion
- Cross-modal transfer effectiveness varies significantly with linguistic distance between source and target languages
- Visual information serves as a semantic bridge for low-resource language processing when textual data is scarce

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual information serves as a semantic bridge for low-resource language processing when textual data is scarce.
- **Mechanism:** Visual features provide grounding context that disambiguates rare words and culturally-specific concepts, enabling models to leverage shared visual representations across languages where textual resources diverge significantly.
- **Core assumption:** Visual concepts have cross-lingual consistency; an image of a "market" carries similar semantic meaning regardless of whether the caption is in Hindi, Yoruba, or English.
- **Evidence anchors:** [abstract]: "visual information often serves as a crucial bridge for improving model performance in LR settings"; [Section 6]: Image-guided translation achieved +3 to +4.2 BLEU improvements over text-only baselines for Hindi, Bengali, and Hausa; visual features "help to resolve ambiguity and improve translation quality in LR scenarios"; [corpus]: LinguaMark benchmark paper confirms multilingual multimodal models exhibit "biased and unfair outputs across languages" due to limited linguistic coverage, but visual grounding provides a potential mitigation path.

### Mechanism 2
- **Claim:** Parameter-efficient adaptation methods achieve competitive performance while reducing trainable parameters by 3-5 orders of magnitude.
- **Mechanism:** Instead of full fine-tuning, these methods freeze pre-trained encoders and learn small adapter modules (5K-7K parameters for XtremeCLIP vs. 149M for full fine-tuning) that map visual-textual representations to task-specific outputs via prototype affinity matching or lightweight projection layers.
- **Core assumption:** The frozen backbone (typically trained on high-resource languages) contains sufficient representational capacity for low-resource adaptation through linear combinations or learned prototypes.
- **Evidence anchors:** [Section 8, Table 10]: XtremeCLIP achieves 62.06% accuracy on Visual Entailment with only 5-7K trainable parameters versus 51.10% for full fine-tuning (149M params)—a +21.4% improvement with 21,000× fewer parameters; [Section 8]: FEWVLM (740M params) outperforms Frozen (7B params, 31× larger) on VQAv2 by 33.8%; [corpus]: AdaDocVQA paper addresses low-resource document VQA through "adaptive framework" innovations, confirming parameter-efficient approaches are viable for resource-constrained settings.

### Mechanism 3
- **Claim:** Cross-modal transfer enables knowledge sharing from data-rich modalities or languages to resource-constrained targets.
- **Mechanism:** Pre-trained models leverage shared representation spaces across modalities (text, speech, vision) and languages. Transfer operates bidirectionally: (1) modality transfer (e.g., leveraging abundant text data to improve speech recognition) and (2) language transfer (adapting high-resource language representations to low-resource targets via adapters, code-switching, or initialization).
- **Core assumption:** Representational similarity exists between source and target domains—either typological similarity for language transfer (e.g., Spanish→Portuguese outperforms English→Tamil) or semantic consistency for modality transfer.
- **Evidence anchors:** [Section 7]: "transfer between closely related languages typically outperforms transfer between distant language families"; [Section 7]: Wang et al. adapted MDETR to new languages "using adapters and code-switching without relying on MT data"; Nortje et al. showed initialization from English speech-image models improved Yoruba few-shot word learning; [corpus]: Bhaasha/Bhasa/Zaban survey confirms LLMs for South Asian low-resource languages are "being overlooked," and transfer from English-centric models perpetuates bias—corroborating the survey's caution about language transfer effectiveness varying with linguistic distance.

## Foundational Learning

- **Concept:** Low-resource language taxonomy (Joshi et al. classification)
  - **Why needed here:** The survey references "Class 0" languages (88.4% of world's languages with zero NLP representation) and explains that resource scarcity is "institutionally constructed rather than inherent." Understanding this classification determines whether you should pursue dataset creation, synthetic generation, or transfer approaches.
  - **Quick check question:** Does your target language have any existing benchmark, pre-trained model, or parallel corpus? If no→Class 0 (dataset creation priority); if minimal but exists→dataset extension/synthetic augmentation viable.

- **Concept:** Multimodal fusion strategies (early vs. late vs. architectural)
  - **Why needed here:** Table 5 provides a decision guide: early fusion maximizes accuracy but requires 2-3× more memory; late fusion handles missing modalities gracefully; architectural fusion (attention-based) balances both at higher complexity. Your hardware constraints and task requirements determine the appropriate choice.
  - **Quick check question:** What happens if one modality (e.g., audio) is corrupted or missing during inference? If graceful degradation is required→late or architectural fusion with robustness mechanisms (e.g., MRF's attention-based modality selection).

- **Concept:** Catastrophic forgetting in continual/adaptation learning
  - **Why needed here:** The survey identifies this as a "significant technical hurdle" when adapting pre-trained models to new languages. Mechanisms like auxiliary CTC conditioning, LoRA-based adaptation, and progressive transfer strategies aim to mitigate this.
  - **Quick check question:** What baseline performance do you need to maintain on the source language(s) while adapting to your low-resource target? If >95% source performance is required→explore parameter-efficient methods (LoRA, adapters) rather than full fine-tuning.

## Architecture Onboarding

- **Component map:**
Input Modalities (Text, Image, Audio, Video)
    ↓
Modality-Specific Encoders
  - Text: mBERT, XLM-RoBERTa, mBART
  - Image: EfficientNet-B0, Swin-Tiny, ViT-Base
  - Audio: MFCC features, Whisper encoder
    ↓
Fusion Layer
  - Early: Feature concatenation → unified classifier
  - Late: Separate modality models → decision voting/weighted averaging
  - Architectural: Cross-attention, encoder-decoder bridges
    ↓
Task-Specific Head (VQA, translation, sentiment classification)
    ↓
Output

- **Critical path:**
  1. **Language assessment:** Determine resource class (existing datasets? parallel corpora? pre-trained models?)
  2. **Modality selection:** Text-image dominates (65% of surveyed work); add audio for oral languages, video for temporal/sequential tasks
  3. **Fusion strategy selection:** Use Table 5 decision guide based on memory constraints, robustness requirements, and accuracy needs
  4. **Parameter-efficient adaptation:** Start with frozen backbone + small adapter (XtremeCLIP-style prototype matching or LowCLIP lightweight encoders)
  5. **Evaluation design:** Multiple metrics + culturally-grounded human evaluation by native speakers

- **Design tradeoffs:**
| Constraint | Recommended Approach | Tradeoff |
|------------|---------------------|----------|
| <8GB VRAM | Late fusion with BiLSTM (~10M params) | Lower accuracy ceiling |
| Missing modalities | Late fusion with weighted averaging | Misses cross-modal interactions |
| Maximum accuracy | Early/intermediate fusion | 2-3× memory, strict alignment |
| Rapid prototyping | Late fusion or FEWVLM-style prompting | Suboptimal for complex tasks |
| Clinical/safety-critical | Architectural fusion with attention | High compute cost |

- **Failure signatures:**
  - **Translation noise propagation:** Back-translated synthetic data contains errors that compound through training; MRF mechanism reports failure "when all three modalities are simultaneously noisy"
  - **Cultural mismatch:** Models trained on Western images fail to recognize culturally-specific objects (traditional clothing, local foods, religious symbols); standard metrics (BLEU, accuracy) may mask this failure
  - **Catastrophic forgetting:** Source-language performance drops below acceptable threshold during adaptation
  - **Cold-start paralysis:** 42 languages have only one study each—no baselines, benchmarks, or community momentum for contextualization

- **First 3 experiments:**
  1. **Establish text-only baseline:** Train XLM-RoBERTa or mBERT on your task with available LR language data. This provides the floor for multimodal improvements.
  2. **Implement parameter-efficient multimodal extension:** Freeze text encoder, add EfficientNet-B0 image encoder (Table 9 shows best efficiency/performance ratio), and train only fusion layer with 500-2000 image-text pairs. Compare to text-only baseline.
  3. **Evaluate with community validation:** Recruit 3-5 native speakers to assess cultural appropriateness and semantic correctness of outputs, alongside automated metrics. Document cultural biases and failure modes in model card format.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can catastrophic forgetting mitigation strategies maintain over 95% source-language performance while achieving over 80% target-language performance for pairs with fewer than 1,000 parallel sentences?
- **Basis in paper:** [explicit] Section 10 (Future Work) explicitly sets these performance thresholds as a target for "short-term development."
- **Why unresolved:** Current cross-modal transfer methods struggle with catastrophic forgetting and inefficient knowledge transfer, particularly for structurally distant languages with extreme data scarcity.
- **What evidence would resolve it:** Reporting dual-performance metrics (source vs. target) on low-resource language pairs using the specified data constraints (<1k sentences).

**Open Question 2**
- **Question:** Can language-agnostic visual encoders pre-trained on culturally-diverse, non-Western image collections effectively reduce the bias observed in current Large Multimodal Models?
- **Basis in paper:** [explicit] Section 10 identifies the need to create visual encoders trained on non-Western contexts to address "documented Western bias in current visual representations."
- **Why unresolved:** Current vision-language models rely on visual features (e.g., from ImageNet) that encode Western-centric assumptions about objects, social roles, and salience.
- **What evidence would resolve it:** Evaluation of new encoders on culturally-specific object recognition tasks demonstrating superior performance in non-Western contexts compared to standard baselines.

**Open Question 3**
- **Question:** What is the optimal methodology for establishing standardized "LR-MMBench" evaluation suites that utilize culturally-adapted visual contexts validated by native speakers?
- **Basis in paper:** [explicit] Section 10 proposes the creation of "LR-MMBench" and Section 9 highlights that current metrics (BLEU, Accuracy) fail to capture cultural appropriateness.
- **Why unresolved:** Evaluation remains "underdeveloped," relying on translated Western benchmarks that fail to reflect authentic language use or visual grounding in target communities.
- **What evidence would resolve it:** The release of open-source benchmarks that document culturally-grounded annotation protocols and include non-Western visual contexts.

**Open Question 4**
- **Question:** Can explicit source-language selection guidelines based on typological similarity metrics (syntactic distance, shared writing systems) consistently outperform default transfer from English?
- **Basis in paper:** [explicit] Section 10 calls for establishing such guidelines to "maximize positive transfer for specific target languages."
- **Why unresolved:** Transfer effectiveness currently varies significantly by linguistic similarity, yet researchers often default to English as a pivot, ignoring potentially closer high-resource languages.
- **What evidence would resolve it:** Comparative studies showing that selecting source languages based on WALS features or syntactic distance yields higher performance gains than English-transfer baselines.

## Limitations
- **Survey Coverage Gaps:** Heavy focus on text-image combinations (65% of research) while only 2 studies address audio-video combinations, creating significant blind spot for predominantly oral languages
- **Data Quality and Cultural Bias:** Doesn't systematically evaluate quality of synthetic datasets or their cultural appropriateness; acknowledges exclusion from high-resource datasets but doesn't quantify impact
- **Evaluation Framework Limitations:** Identifies need for culturally-grounded evaluation frameworks but doesn't provide specific methodologies; most cited studies rely on standard metrics without community-centered validation

## Confidence
**High Confidence (Evidence Score >4.0):**
- Parameter-efficient adaptation methods achieve competitive performance with 3-5 orders of magnitude fewer trainable parameters
- Early fusion generally achieves highest accuracy but at greater computational cost
- Cross-modal transfer effectiveness varies with linguistic distance between source and target languages

**Medium Confidence (Evidence Score 2.5-4.0):**
- Visual information serves as a semantic bridge for low-resource language processing
- Catastrophic forgetting remains a significant technical hurdle in continual adaptation

**Low Confidence (Evidence Score <2.5):**
- Current multimodal approaches adequately serve predominantly oral low-resource languages
- Standard evaluation metrics capture cultural appropriateness and real-world usability

## Next Checks
**Check 1: Cultural Appropriateness Validation**
Replicate the LowCLIP architecture (mBERT + EfficientNet-B0) but add a cultural appropriateness validation layer using native speaker evaluation. Test on a low-resource language with strong visual culture (e.g., Yoruba or Tamil) using culturally-specific image-caption pairs. Document whether standard accuracy metrics correlate with human judgments of cultural relevance.

**Check 2: Audio-Modality Performance Gap**
Implement a minimal audio-text fusion system (Whisper encoder + mBERT) for a predominantly oral low-resource language (e.g., Hausa or Wolof). Compare performance against text-only baselines and text-image systems. Quantify the accuracy gap and document whether audio modality provides the semantic bridge that the survey claims visual information provides.

**Check 3: Catastrophic Forgetting Quantification**
Train a model using LoRA-based adaptation on a low-resource language while monitoring source-language performance degradation. Use a language pair with available pre-trained models (e.g., English-Spanish) and systematically vary adaptation parameters (adapter size, learning rate, training duration). Document the relationship between parameter efficiency and source language preservation.