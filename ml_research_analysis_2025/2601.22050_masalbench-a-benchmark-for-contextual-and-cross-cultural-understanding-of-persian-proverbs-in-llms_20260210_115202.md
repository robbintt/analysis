---
ver: rpa2
title: 'MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of
  Persian Proverbs in LLMs'
arxiv_id: '2601.22050'
source_url: https://arxiv.org/abs/2601.22050
tags:
- proverbs
- proverb
- persian
- llms
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs

## Quick Facts
- arXiv ID: 2601.22050
- Source URL: https://arxiv.org/abs/2601.22050
- Reference count: 3
- Primary result: Introduction of MasalBench, a benchmark for evaluating LLMs on Persian proverbs focusing on contextual and cross-cultural understanding

## Executive Summary
MasalBench is a newly introduced benchmark designed to evaluate large language models' ability to understand and interpret Persian proverbs within their cultural and contextual frameworks. The benchmark aims to address the gap in cross-cultural and low-resource language understanding by focusing specifically on the nuances of Persian idiomatic expressions. By creating task-specific challenges around proverbs, the authors seek to measure both linguistic comprehension and cultural sensitivity in LLM outputs.

## Method Summary
The paper presents MasalBench as a structured evaluation suite containing tasks centered on Persian proverbs, with an emphasis on contextual inference and cross-cultural interpretation. The methodology involves curating a dataset of proverbs paired with scenarios or questions that test understanding beyond literal translation. The evaluation leverages both automated scoring metrics and human judgment to assess model responses. However, the exact operationalization of "contextual" and "cross-cultural" understanding is not explicitly defined, which may affect reproducibility.

## Key Results
- MasalBench introduces a novel benchmark targeting Persian proverb comprehension in LLMs.
- The benchmark evaluates models on contextual and cross-cultural understanding tasks.
- Results suggest current LLMs face challenges in accurately interpreting culturally rich idiomatic expressions.

## Why This Works (Mechanism)
The benchmark leverages the inherent complexity of Persian proverbs, which often carry layered meanings dependent on cultural context. By designing tasks that require models to infer intent, recognize figurative language, and navigate cultural references, MasalBench exposes limitations in current LLMs' ability to generalize beyond surface-level language patterns. The use of both automated and human evaluation ensures a balanced assessment of model performance.

## Foundational Learning
- **Persian proverbs**: Idiomatic expressions unique to Persian culture, requiring cultural and historical knowledge for accurate interpretation. *Why needed*: Central to the benchmark's focus on cross-cultural understanding. *Quick check*: Verify dataset includes a diverse range of proverbs covering various themes and contexts.
- **Contextual inference**: The ability to derive meaning from surrounding text or situational cues. *Why needed*: Critical for understanding the intended message behind proverbs. *Quick check*: Ensure tasks include scenarios that necessitate contextual reasoning.
- **Cross-cultural understanding**: Recognizing and interpreting cultural nuances and references. *Why needed*: Evaluates models' sensitivity to cultural differences in language use. *Quick needed*: Assess whether evaluation criteria explicitly measure cultural awareness.

## Architecture Onboarding
- **Component map**: Proverb dataset -> Task generation -> Model evaluation -> Scoring
- **Critical path**: Dataset creation → Task design → LLM inference → Evaluation
- **Design tradeoffs**: Balancing cultural authenticity with task clarity; automated vs. human evaluation.
- **Failure signatures**: Misinterpretation of figurative language, over-reliance on literal translations, cultural bias in responses.
- **First experiments**:
  1. Test baseline LLM performance on literal translation tasks.
  2. Evaluate models on proverb interpretation with and without contextual cues.
  3. Compare automated vs. human evaluation consistency.

## Open Questions the Paper Calls Out
The paper does not explicitly list open questions but raises implicit concerns about the generalizability of the benchmark to other low-resource languages and the potential influence of cultural biases in the dataset and evaluation process.

## Limitations
- Lack of clear operational definitions for "contextual" and "cross-cultural" understanding.
- Absence of bias analysis in the dataset and evaluation methodology.
- Limited discussion on the benchmark's applicability to other low-resource languages.

## Confidence
- **Major claims**: Medium
  - The introduction of MasalBench is valuable but lacks detailed methodology and evaluation criteria, reducing confidence in the robustness of the benchmark.

## Next Checks
1. Define and operationalize "contextual" and "cross-cultural understanding" in the benchmark tasks to ensure clarity and reproducibility.
2. Provide detailed evaluation criteria and metrics for assessing LLM performance on the benchmark.
3. Conduct a bias analysis to identify and mitigate cultural biases in the dataset and evaluation process.