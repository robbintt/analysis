---
ver: rpa2
title: An Unlearning Framework for Continual Learning
arxiv_id: '2509.17530'
source_url: https://arxiv.org/abs/2509.17530
tags:
- unlearning
- learning
- tasks
- task
- uncle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UnCLe addresses the challenge of performing both continual learning
  and unlearning in a data-free setting, where conventional unlearning algorithms
  fail due to performance degradation on retained tasks and task relapse. The method
  uses a hypernetwork that generates task-specific network parameters conditioned
  on task embeddings, with unlearning achieved by aligning generated parameters with
  noise without requiring any data.
---

# An Unlearning Framework for Continual Learning

## Quick Facts
- arXiv ID: 2509.17530
- Source URL: https://arxiv.org/abs/2509.17530
- Authors: Sayanta Adhikari; Vishnuprasadh Kumaravelu; P. K. Srijith
- Reference count: 34
- Primary result: Data-free continual learning and unlearning framework using hypernetwork-generated parameters with task embeddings, achieving high retain-task accuracy while effectively unlearning forget-tasks.

## Executive Summary
UnCLe addresses the challenge of performing both continual learning and unlearning in a data-free setting, where conventional unlearning algorithms fail due to performance degradation on retained tasks and task relapse. The method uses a hypernetwork that generates task-specific network parameters conditioned on task embeddings, with unlearning achieved by aligning generated parameters with noise without requiring any data. Empirical evaluations on vision datasets (TinyImageNet, CIFAR-100, Permuted-MNIST, and 5-Tasks) demonstrate UnCLe's ability to maintain retain-task accuracy (e.g., 62.65% on CIFAR-100) while achieving near-random forget-task accuracy (10.00%), with minimal spill (0.64) and relapse (0.51) across tasks. The framework also mitigates model saturation by restoring plasticity through unlearning obsolete tasks.

## Method Summary
UnCLe employs a hypernetwork that learns to generate task-specific network parameters, using task embeddings. For learning, the hypernetwork is trained with knowledge distillation-inspired regularization to ensure consistent generation of previous task-specific parameters, preventing catastrophic forgetting. For unlearning, the framework aligns the generated parameters of the forget-task with noise samples, effectively randomizing them without accessing the original data. This approach allows selective unlearning while maintaining performance on retained tasks and preventing task relapse. The method was evaluated on multiple vision datasets, demonstrating its effectiveness in both continual learning and data-free unlearning scenarios.

## Key Results
- Maintains high retain-task accuracy (62.65% on CIFAR-100) while achieving near-random forget-task accuracy (10.00%)
- Demonstrates minimal spill (0.64) and relapse (0.51) across tasks
- Effectively mitigates model saturation by restoring plasticity through unlearning obsolete tasks
- Achieves data-free unlearning without requiring access to forget-task data

## Why This Works (Mechanism)

### Mechanism 1: Hypernetwork-Mediated Task Decomposition
- **Claim:** Conditional generation of task-specific parameters enables implicit task isolation without explicit parameter partitioning.
- **Mechanism:** The hypernetwork H(·;ϕ) maps learnable task embeddings e_t to main network parameters θ_t. This creates a shared knowledge repository (hypernetwork weights) with task-specific interfaces (embeddings), allowing knowledge transfer while maintaining functional separation.
- **Core assumption:** Task-specific optimal parameters lie in a learnable subspace reachable through conditioning; tasks are sufficiently distinct to benefit from separate embeddings.
- **Evidence anchors:** [abstract], [section - Methodology]
- **Break condition:** Embedding collision or hypernetwork capacity saturation.

### Mechanism 2: Noise Alignment for Data-Free Unlearning
- **Claim:** Optimizing generated parameters toward Gaussian noise produces outputs statistically indistinguishable from random initialization without accessing task data.
- **Mechanism:** The unlearning objective minimizes MSE between H(e_f;ϕ) and samples z_i ~ N(0,I_d). Averaging over multiple noise samples (n=10) prevents memorization of specific noise patterns while driving parameters toward high-entropy outputs.
- **Core assumption:** Randomly initialized networks produce maximally uncertain outputs; noise alignment approximates this state without destabilizing the hypernetwork.
- **Evidence anchors:** [abstract], [section - Unlearning]
- **Break condition:** Insufficient burn-in iterations or regularization weight misbalanced.

### Mechanism 3: Distillation Regularization for Stability-Plasticity Balance
- **Claim:** Constraining hypernetwork to generate similar parameters for previous tasks prevents catastrophic forgetting during both learning and unlearning operations.
- **Mechanism:** L_reg = (1/t-1)Σ||H(e_t';ϕ*) - H(e_t';ϕ)||²_2 anchors the hypernetwork to its frozen state (ϕ*) for all prior task embeddings. This functional consistency ensures retain-task performance preservation.
- **Core assumption:** Parameter similarity correlates with behavioral similarity; the Frobenius norm captures functionally relevant parameter changes.
- **Evidence anchors:** [section - Learning], [section - Unlearning]
- **Break condition:** Regularization weight too high (plasticity loss) or too low (forgetting/spill increases).

## Foundational Learning

- **Concept: Hypernetworks**
  - **Why needed here:** The entire architecture depends on understanding how one network can generate parameters for another. Without this, the conditional generation mechanism and its unlearning implications are opaque.
  - **Quick check question:** Can you explain why a hypernetwork conditioned on different embeddings can generate functionally distinct networks while sharing weights?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - **Why needed here:** The paper's motivation rests on the tension between learning new tasks and preserving old ones. Understanding why naive sequential training fails clarifies why regularization is necessary.
  - **Quick check question:** If you train a network on task A then task B without regularization, what happens to task A performance and why?

- **Concept: Machine Unlearning Objectives (Completeness, Specificity, Permanence)**
  - **Why needed here:** Evaluating UnCLe requires understanding what successful unlearning means beyond accuracy metrics. The paper introduces "spill" and "relapse" specifically to capture these properties.
  - **Quick check question:** What is the difference between a model that achieves 10% forget-task accuracy through unlearning vs. one that never learned the task? How would you detect the difference?

## Architecture Onboarding

- **Component map:** Task embeddings (dim=32) → Hypernetwork (3-layer MLP: 128→256→512) → Main network parameters (chunked into 200 per task) → Task-specific ResNet18/50

- **Critical path:**
  1. Initialize hypernetwork with Hyperfan method (ensures Kaiming He init for generated params)
  2. For learning: freeze ϕ*, optimize ϕ and e_t with L_task + β·L_reg
  3. For unlearning: optimize ϕ with γ·MSE(H(e_f;ϕ), z) + L_reg over E_u burn-in iterations
  4. Freeze chunk embeddings after first task to prevent drift

- **Design tradeoffs:**
  - Chunk count (200) vs. hypernetwork size: More chunks reduce single-head size but increase generation overhead
  - β value (0.001-0.1): Higher values prioritize stability but reduce plasticity and unlearning efficiency
  - Burn-in E_u (100→20 annealed): Longer burn-in ensures complete unlearning but increases latency
  - Noise samples n=10: Balances between memorization risk (n=1) and L2-norm collapse (n→∞)

- **Failure signatures:**
  - High spill (>5): γ too low or burn-in insufficient; retain-tasks affected during unlearning
  - High relapse (>2): Subsequent learning operations recover forgotten task performance; future-awareness failed
  - Low RA with good FA: Over-regularization during unlearning damaged shared knowledge
  - Hypernetwork parameter collapse: Using L2-norm directly instead of noise sampling drove ϕ→0

- **First 3 experiments:**
  1. **Baseline validation:** Run UnCLe on Permuted-MNIST with 5-task sequence (L0→L1→L2→L3→L4). Verify RA>95% and confirm β=0.1 produces stable learning without forgetting.
  2. **Unlearning stress test:** On CIFAR-100, execute sequence with consecutive unlearning operations (U3→U7→U0). Measure spill using Eq.4; target spill<1.0 to confirm specificity.
  3. **Relapse detection:** After unlearning task 0, continue learning 3+ new tasks. Track task 0 accuracy over subsequent operations; confirm no recovery beyond random (10%±1%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the UnCLe framework be extended to support class-incremental learning and unlearning, allowing for the selective removal of specific classes within a task rather than whole tasks?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the method "currently lacks the flexibility to individually learn and unlearn classes within each task" and suggest future work address "a class-incremental learning and unlearning setting."
- **Why unresolved:** The current architecture relies on distinct task embeddings to isolate and unlearn parameters, an approach that does not easily translate to the shared parameter spaces typical of class-incremental scenarios.
- **What evidence would resolve it:** A modification of the hypernetwork conditioning that successfully unlearns specific classes while maintaining accuracy on remaining classes of the same task, evaluated on standard class-incremental benchmarks.

### Open Question 2
- **Question:** Does the noise-alignment unlearning mechanism generalize effectively to non-vision domains, such as natural language processing or reinforcement learning?
- **Basis in paper:** [inferred] The empirical evaluation is strictly confined to computer vision datasets (TinyImageNet, CIFAR-100, MNIST), leaving the method's efficacy on discrete data types or sequential decision-making tasks unverified.
- **Why unresolved:** The unlearning objective minimizes the difference between generated parameters and Gaussian noise; it is unclear if parameter randomization via noise alignment effectively destroys semantic knowledge in non-vision architectures (e.g., Transformers).
- **What evidence would resolve it:** Successful application of UnCLe to text classification or RL benchmarks, demonstrating high retain-accuracy and low forget-accuracy metrics comparable to the vision results.

### Open Question 3
- **Question:** Are there theoretical guarantees regarding the trade-off between unlearning completeness (preventing relapse) and the stability of the hypernetwork over very long task sequences?
- **Basis in paper:** [inferred] While the paper empirically observes that unlearning alleviates saturation, the mechanism relies on balancing the noise alignment term $\gamma$ and regularization. It is not proven if this balance holds indefinitely or if "spill" accumulates non-linearly over hundreds of tasks.
- **Why unresolved:** The paper demonstrates empirical success on sequences of 15-30 requests, but lacks a theoretical bound on error accumulation or plasticity loss in lifelong learning scenarios.
- **What evidence would resolve it:** A theoretical analysis defining the conditions under which the hypernetwork remains stable, or empirical results on significantly longer task sequences (e.g., >100 tasks) showing no significant drift in retain-task accuracy.

## Limitations
- **Hyperparameter sensitivity:** Performance heavily depends on γ and β values, which were tuned per dataset without revealing the search space or sensitivity analysis.
- **Limited domain validation:** Empirical evaluation is strictly confined to computer vision datasets, leaving efficacy on non-vision domains unverified.
- **Theoretical gaps:** Lacks theoretical guarantees regarding the trade-off between unlearning completeness and hypernetwork stability over very long task sequences.

## Confidence

- **High confidence:** The core architectural claim that hypernetworks can generate task-specific parameters (based on established hypernetwork literature and clear methodology description)
- **Medium confidence:** The data-free unlearning mechanism's effectiveness, as spill/relapse metrics are well-defined but their absolute thresholds for "success" are dataset-dependent
- **Low confidence:** The claim that this approach prevents model saturation through unlearning, as the mechanism is described but empirical evidence is limited to accuracy metrics rather than capacity or plasticity measurements

## Next Checks

1. **Parameter collision test:** Systematically vary the number of task embeddings and measure performance degradation to determine the embedding collision threshold and validate the implicit task isolation claim.
2. **Burn-in sensitivity analysis:** Run unlearning with E_u values from 10 to 200 and measure spill/relapse curves to quantify the minimum effective burn-in and validate the trade-off between latency and unlearning completeness.
3. **Task embedding drift monitoring:** Track L2 distance between task embeddings before/after unlearning operations to empirically verify that embeddings drift toward noise distributions rather than collapsing to zero or other degenerate states.