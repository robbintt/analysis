---
ver: rpa2
title: Boosting Universal LLM Reward Design through Heuristic Reward Observation Space
  Evolution
arxiv_id: '2504.07596'
source_url: https://arxiv.org/abs/2504.07596
tags:
- reward
- design
- learning
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated reward design for
  reinforcement learning using large language models (LLMs). The authors propose a
  novel heuristic framework that enhances LLM-driven reward design by evolving the
  Reward Observation Space (ROS) through a table-based exploration caching mechanism
  and a text-code reconciliation strategy.
---

# Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution

## Quick Facts
- **arXiv ID**: 2504.07596
- **Source URL**: https://arxiv.org/abs/2504.07596
- **Reference count**: 34
- **Primary result**: Outperformed existing approaches in 9/20 bimanual manipulation tasks from the Bi-dexterous Manipulation benchmark.

## Executive Summary
This paper addresses automated reward design for reinforcement learning using large language models (LLMs) by introducing a novel heuristic framework that evolves the Reward Observation Space (ROS) through a table-based exploration caching mechanism and a text-code reconciliation strategy. The authors propose using a State Execution Table to track historical usage and success rates of environment states, breaking the Markovian constraint typical in LLM dialogues, and reconcile user-provided task descriptions with expert-defined success criteria using structured prompts. Evaluated on 20 bimanual manipulation tasks, the approach demonstrated superior performance and stability compared to existing methods.

## Method Summary
The framework uses GPT-4 to iteratively generate reward functions for robotic manipulation tasks, with each iteration producing K=16 reward samples. A State Execution Table (SET) tracks historical usage and success contributions of environment states, enabling non-Markovian memory across iterations. The ROS is disentangled into state selection (ROSst) and operation design (ROSop), alternating based on a success threshold τ. A separate LLM instance reconciles user task descriptions with expert success criteria to ensure alignment. The method was evaluated on IsaacGym with PPO training, using 5 iterations and 8× RTX 3090 GPUs.

## Key Results
- Outperformed existing approaches in 9 out of 20 bimanual manipulation tasks
- Matched performance in 5 additional tasks from the Bi-dexterous Manipulation benchmark
- Demonstrated improved exploration through Sampling State Disparity (SSD) metrics

## Why This Works (Mechanism)

### Mechanism 1: State Execution Table (SET) for Non-Markovian Memory
- Claim: A table-based caching mechanism improves reward exploration by maintaining cumulative statistics across all iterations, overcoming the context-length-limited "Markovian" memory constraint typical in LLM dialogues.
- Mechanism: SET maintains two columns per state: (1) historical usage frequency across all reward samples, and (2) accumulated success contribution. When generating new reward candidates, the LLM is prompted with SET to prioritize states with high success contributions but low usage—balancing exploitation and exploration.
- Core assumption: States that contributed to successful runs but remain under-explored are promising candidates for future reward configurations.
- Evidence anchors: [abstract] introduces SET to overcome Markovian constraints; [section IV-B] details the table structure with usage and success contribution columns; [corpus] CogMCTS paper conceptually aligns but uses different approach.

### Mechanism 2: Observation Space Disentanglement
- Claim: Decomposing reward design into alternating sub-problems (state selection vs. operation design) reduces the effective search space dimensionality and improves sample efficiency.
- Mechanism: The Reward Observation Space (ROS) is split into ROSst (which states to observe) and ROSop (what operations to compute on those states). At odd iterations or when success rate < τ, the LLM generates new state selections. At even iterations with success ≥ τ, it keeps the same states but proposes new operations.
- Core assumption: State selection and operation design can be meaningfully optimized separately without entangled dependencies that require joint optimization.
- Evidence anchors: [abstract] mentions evolving ROS through table-based caching and reconciliation; [section IV-B, Eq. 6] shows the disentanglement rule explicitly; [corpus] weak direct evidence, related work doesn't explicitly disentangle these sub-problems.

### Mechanism 3: User-Expert Mission Reconciliation
- Claim: Structured prompting can align potentially conflicting task descriptions from users with expert-defined success criteria, reducing ambiguity in reward design objectives.
- Mechanism: A separate LLM instance (LLMC) processes the user's natural language description U and the expert's success function Fsc through a structured template TU, producing a reconciled task specification DU(U). When no Fsc exists, LLMC generates one from scratch using few-shot prompting with examples from similar tasks.
- Core assumption: Misalignment between user intent and expert success definitions is a significant source of reward design failure, and LLMs can reliably detect and resolve such conflicts.
- Evidence anchors: [abstract] mentions reconciling user descriptions with expert criteria; [section IV-A] describes using LLMC to mitigate conflicts between U and Fsc; [corpus] no directly comparable mechanism found.

## Foundational Learning

- Concept: Reward Observation Space (ROS) construction
  - Why needed here: Understanding that a reward function is not monolithic code but a composition of (1) selected environment states and (2) operations on those states is essential for grasping the disentanglement strategy.
  - Quick check question: Given an environment with states [position, velocity, grip_force, distance_to_target], what is a valid ROSst subset and a corresponding ROSop that could produce a shaped reward?

- Concept: Markovian constraint in LLM dialogues
  - Why needed here: The paper frames LLM context limitations as a "Markovian" problem—each iteration only conditions on the immediately preceding context unless explicit memory mechanisms (like SET) are introduced.
  - Quick check question: If an LLM designs 16 reward samples per iteration for 5 iterations without SET, how many historical samples can it actually condition on by iteration 5?

- Concept: Evolutionary reward search with fitness functions
  - Why needed here: The framework treats reward design as an evolutionary optimization problem where Fsc (success rate) serves as a sparse fitness signal, and the LLM acts as a mutation/crossover operator.
  - Quick check question: Why might using Fsc directly as the fitness function be insufficient for guiding evolutionary search, and what alternative fitness signals could complement it?

## Architecture Onboarding

- Component map:
User Description (U) + Expert Success (Fsc) -> LLMC (Reconciliation) -> DU(U) + Fsc_reconciled -> LLMR (Reward Design Loop) -> K=16 Reward Samples -> PPO Training -> Evaluate with Fsc -> SET Update

- Critical path:
1. Reconciliation phase: U + Fsc -> LLMC -> DU(U), Fsc_reconciled (one-time preprocessing)
2. Iteration 1: LLMR generates K=16 rewards from DU(U) alone -> train -> evaluate -> initialize SET
3. Iterations 2-5: LLMR generates rewards using DR(previous best) + DA(performance) + DG(SET)
4. Final selection: Best reward across all N iterations by Fsc evaluation

- Design tradeoffs:
  - Threshold τ: Too high -> excessive state selection iterations (never switches to operation refinement); too low -> locks into suboptimal state configurations early. Paper uses task-specific τ derived from iteration-1 average success.
  - SET update strategy: Only successful runs update the table (noise reduction) vs. including failures (more exploration signal). Paper chooses success-only.
  - Disentanglement frequency: Alternating every iteration vs. adaptive based on performance gains. Paper uses adaptive with τ.

- Failure signatures:
  - SSD (Sampling State Disparity) remains low: indicates exploration collapse—LLM repeatedly selects the same states.
  - ESR stuck at 0.0 for multiple iterations: suggests reward code is syntactically valid but semantically misaligned with Fsc.
  - Large gap between iteration-best and final-best: suggests later iterations degrade quality—check if DR feedback is misguiding LLMR.

- First 3 experiments:
1. Reproduce BlockStack (ESR_avg=0.35) with full framework vs. ablated SET to verify caching contribution; log SSD metric to confirm exploration improvement.
2. Run DoorCloseOutward with τ=0.1 (fixed) vs. auto-τ to validate the threshold sensitivity finding from ablation; expect ESR improvement from 0.42 to ~1.0.
3. Test reconciliation on a novel task without predefined Fsc; have LLMC generate Fsc from scratch and compare final ESR against human-authored baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive threshold search algorithm improve the stability of the heuristic evolution process compared to the current fixed-threshold approach?
- Basis: [explicit] The authors state, "In the future, an adaptive threshold search algorithm could enable this mechanism to achieve better performance," noting that fixed thresholds caused underperformance in tasks like CatchAbreast.
- Why unresolved: The current method uses a static threshold based on initial iteration averages, which forces the LLM to focus too much on space member selection in tasks where this heuristic is suboptimal.
- Evidence: A comparison of task success rates (ESR) on the Bi-dexterous Manipulation benchmark using a dynamic threshold versus the static baseline.

### Open Question 2
- Question: How does the proposed framework perform when transferred to complex, real-world robotic scenarios outside of the IsaacGym simulation environment?
- Basis: [explicit] The conclusion states, "Future work will aim to... investigate the application of this framework in more complex, real-world scenarios."
- Why unresolved: The current evaluation is confined to simulation benchmarks, and real-world dynamics (e.g., noise, latency, partial observability) may disrupt the text-code reconciliation and reward execution.
- Evidence: Successful deployment of policies trained via this framework on physical robotic hardware in tasks with significant sim-to-real gaps.

### Open Question 3
- Question: How can the State Execution Table (SET) mechanism be adapted for environments with high-dimensional or unstructured state spaces that lack explicit semantic labels?
- Basis: [inferred] The SET mechanism relies on tracking the "historical usage and success rates of environment states" by name, assuming a structured observation space.
- Why unresolved: In environments defined by raw pixels or massive point clouds, enumerating specific states for the table is computationally intractable, potentially limiting the method's universality.
- Evidence: An evaluation of the framework's performance on vision-based RL tasks where state names must be derived or clustered from raw data.

## Limitations
- The framework relies on structured observation spaces with semantic state labels, limiting applicability to high-dimensional or unstructured environments.
- The reconciliation mechanism's effectiveness for severely underspecified or contradictory user descriptions remains unverified.
- The SET update mechanism is underspecified regarding how success contributions are distributed across states and what "truncated reward code" means line-by-line.

## Confidence
- **High Confidence**: The core methodology of using SET for non-Markovian memory and the overall evolutionary framework structure.
- **Medium Confidence**: The disentanglement strategy's general validity and the reconciliation mechanism's ability to resolve typical user-expert misalignments.
- **Low Confidence**: The exact prompt templates, SET update mechanics, and task-specific threshold values required for faithful reproduction.

## Next Checks
1. **SET Caching Validation**: Implement the full framework on BlockStack and compare performance with an ablated version lacking SET to quantify the caching contribution. Log the SSD metric across iterations to verify that exploration improves rather than collapses.
2. **Threshold Sensitivity Analysis**: Run DoorCloseOutward with both fixed τ=0.1 and the auto-τ method to validate the claim that proper threshold selection significantly impacts ESR. Document the performance gap.
3. **Reconciliation Robustness Test**: Create a novel task without predefined Fsc, have LLMC generate Fsc from scratch, and compare the final ESR against a human-authored Fsc baseline to evaluate the reconciliation mechanism's reliability.