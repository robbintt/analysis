---
ver: rpa2
title: 'TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning'
arxiv_id: '2510.00739'
source_url: https://arxiv.org/abs/2510.00739
tags:
- td-jepa
- learning
- representations
- zero-shot
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TD-JEPA addresses zero-shot reinforcement learning from offline,
  reward-free transitions by introducing a temporal-difference latent-predictive representation
  learning framework. The core idea is to train state and task encoders, along with
  a policy-conditioned multi-step predictor, using an off-policy TD loss that encourages
  representations predictive of long-term policy dynamics.
---

# TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.00739
- **Source URL**: https://arxiv.org/abs/2510.00739
- **Reference count**: 40
- **Key outcome**: TD-JEPA matches or outperforms state-of-the-art baselines on 13 datasets across locomotion, navigation, and manipulation tasks, particularly excelling in the challenging pixel-based setting.

## Executive Summary
TD-JEPA introduces a temporal-difference latent-predictive representation learning framework for zero-shot reinforcement learning from offline, reward-free transitions. The approach trains state and task encoders along with a policy-conditioned multi-step predictor using an off-policy TD loss that encourages representations predictive of long-term policy dynamics. This enables learning representations that approximate successor features in latent space, allowing zero-shot optimization of any reward function at test time. Empirically, TD-JEPA demonstrates strong performance across 13 datasets in both proprioceptive and pixel-based settings, while theoretically providing guarantees against representation collapse under proper conditions.

## Method Summary
TD-JEPA learns state encoder φ, task encoder ψ, and policy-conditioned predictor T_φ from offline transition data (s, a, s'). The predictor learns to forecast future task representations through a temporal-difference objective, effectively learning successor features in latent space. The key insight is that TD learning enables off-policy learning from reward-free data by bootstrapping latent predictions. At test time, any reward function can be mapped to a task embedding z_r via linear regression, and the corresponding policy π_{z_r} is retrieved directly without additional training. The method includes orthonormality regularization to prevent representation collapse and uses target networks with exponential moving average updates.

## Key Results
- Matches or outperforms state-of-the-art baselines on 13 datasets across locomotion, navigation, and manipulation tasks
- Excels particularly in pixel-based settings (64×64 RGB, 3-frame stacked) compared to contrastive approaches
- Enables fast downstream adaptation, often matching fine-tuning from scratch performance
- Demonstrates superior sample efficiency in zero-shot policy extraction through linear regression

## Why This Works (Mechanism)

### Mechanism 1: TD-based Latent-Predictive Learning
TD learning enables off-policy learning from offline, reward-free transitions by bootstrapping latent predictions. The predictor learns to predict future latent states through a TD objective where targets combine immediate next-state representations with bootstrapped predictions, avoiding on-policy sampling requirements. Core assumption: Predictors optimize faster than representations (prevents collapse). Evidence: Abstract states TD learning enables predictive representations across multiple policies from offline data; Theorem 2 shows if predictors train faster than representations, dynamics preserve covariance preventing collapse.

### Mechanism 2: Policy-Conditioned Successor Feature Approximation
The predictor approximates successor features for parameterized policies, enabling multi-step dynamics modeling. By conditioning the predictor on task embeddings z and training via TD, it learns F^π_z(s,a) = E[Σ γ^t ψ(s_t)] in latent space, capturing long-term policy dynamics. Core assumption: Symmetric transition kernels or appropriate covariance weighting. Evidence: Abstract mentions predictor recovers successor features in latent space; Proposition 1 shows predictor trained via L_MC-JEPA approximates successor features.

### Mechanism 3: Zero-Shot Policy Extraction via Linear Regression
Any reward function can be mapped to an optimal policy without additional training. Task encoder ψ defines linear reward space R_ψ. At test time, reward r is projected to task embedding z_r via linear regression z_r = argmin E[(r - ψ(s)^T z)^2], and policy π_{z_r} is retrieved directly. Core assumption: Reward function lies in (or near) span of ψ. Evidence: Abstract states this enables zero-shot optimization; Theorem 4 bounds policy evaluation error by successor measure approximation loss.

## Foundational Learning

- **Successor Features**: Why needed: Core to understanding how TD-JEPA's predictor captures long-term dynamics via F^π(s,a) = E[Σ γ^t ψ(s_t)]. Quick check: Can you explain why successor features enable zero-shot policy evaluation for linear rewards?
- **Self-Predictive Learning (BYOL-style)**: Why needed: TD-JEPA extends BYOL's bootstrap prediction to multi-step, policy-conditional setting with TD objectives. Quick check: How does stop-gradient on target representations prevent collapse in self-predictive learning?
- **Bellman Equation for Successor Measures**: Why needed: Enables off-policy TD learning (Eq. 7, 9) instead of requiring on-policy Monte Carlo sampling. Quick check: Why does the Bellman equation F^π(s,a) = E[ψ(s') + γF^π(s',a')] enable off-policy learning?

## Architecture Onboarding

- **Component map**: Offline transitions (s,a,s') → State encoder φ(s), task encoder ψ(s') → Predictor T_φ(φ(s),a,z) predicts ψ(s') + γT_φ(φ(s'),a',z) → Actor maximizes T_φ^T z
- **Critical path**: φ and ψ process observations; T_φ conditions on policy embeddings; actor optimizes expected return via successor features
- **Design tradeoffs**: Separate vs. shared encoders (separate shown better in 7/13 domains); latent-predictive vs. contrastive (latent-predictive scales better, outperforms on pixels); encoder depth varies by domain
- **Failure signatures**: Representation collapse (φ/ψ → 0 or constant); poor zero-shot transfer (behavioral dynamics vs policy-conditional); divergence (improper regularization)
- **First 3 experiments**: 1) Validate TD loss on small tabular MDP, verify predictor converges to successor features; 2) Train without orthonormality regularization, monitor φ^T φ covariance rank; 3) Sweep state/task encoder depths on antmaze-ln/antmaze-ls

## Open Questions the Paper Calls Out

1. Can learning objectives be designed for TD-JEPA that are compatible with asymmetric successor measures without relying on impractical backward-in-time sampling? The conclusion identifies this as an exciting direction to address reliance on symmetry assumption. Theoretical backward variant exists but is not easily optimized off-policy.

2. How does TD-JEPA's performance and stability scale when applied to large-scale, real-world robotic datasets? The conclusion suggests benchmarking on real robotic datasets to understand limitations. Current validation is restricted to simulated benchmarks which may not exhibit real-world noise and distributional shifts.

3. Is the explicit orthonormality regularization term strictly necessary to prevent representation collapse in deep implementations of TD-JEPA? Theorem 2 proves no collapse under continuous-time relaxation with optimal predictors, yet practical algorithm relies heavily on tuning regularization coefficient λ.

## Limitations
- Theoretical analysis assumes symmetric transition kernels or proper covariance weighting, which may not hold in real-world dynamics
- Practical effectiveness depends on proper initialization schemes that are not fully specified in the paper
- Zero-shot policy extraction assumes reward functions lie in span of task encoder representations, which may not hold for complex reward structures

## Confidence
- **High confidence**: Strong empirical performance across 13 datasets, particularly in pixel-based settings
- **Medium confidence**: Theoretical claims about preventing representation collapse and achieving low-rank factorization, depending on unspecified initialization and training rate assumptions
- **Low confidence**: Practical effectiveness of zero-shot policy extraction via linear regression for complex reward structures

## Next Checks
1. Train TD-JEPA without orthonormality regularization and monitor the rank of cov(φ) over time to verify theoretical claims about collapse prevention
2. Implement ablation that models behavioral policy dynamics instead of policy-conditional dynamics to validate importance of this distinction
3. Systematically vary network initialization schemes and measure impact on representation collapse and overall performance