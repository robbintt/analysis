---
ver: rpa2
title: Sanitizing Manufacturing Dataset Labels Using Vision-Language Models
arxiv_id: '2506.23465'
source_url: https://arxiv.org/abs/2506.23465
tags:
- labels
- label
- dataset
- image
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Vision-Language Sanitization and Refinement
  (VLSR) framework for cleaning and refining noisy labels in large-scale, multi-label
  manufacturing image datasets. The method leverages the CLIP vision-language model
  to embed both images and labels into a shared semantic space, enabling label quality
  assessment through cosine similarity comparisons.
---

# Sanitizing Manufacturing Dataset Labels Using Vision-Language Models

## Quick Facts
- **arXiv ID**: 2506.23465
- **Source URL**: https://arxiv.org/abs/2506.23465
- **Reference count**: 40
- **Primary result**: VLSR framework reduces 6,426 noisy manufacturing labels to 408 semantically coherent labels using CLIP embeddings and DBSCAN clustering

## Executive Summary
This paper presents VLSR (Vision-Language Sanitization and Refinement), a framework that leverages CLIP's vision-language embeddings to clean noisy labels in large-scale manufacturing image datasets. The method identifies irrelevant, misspelled, or semantically weak labels by comparing image-label cosine similarities and reduces label redundancy through DBSCAN clustering on text embeddings. Experiments on the Factorynet dataset demonstrate successful identification of labeling errors and significant vocabulary reduction from 6,426 to 408 distinct labels.

## Method Summary
VLSR embeds images and labels into a shared 768-dimensional CLIP space, then computes cosine similarities to assess label quality. The framework performs two main tasks: (1) identifying and replacing incorrect labels by comparing image-label similarity scores, and (2) clustering semantically similar labels using DBSCAN with cosine distance metric to reduce redundancy. The method processes the Factorynet dataset by generating CLIP embeddings for all images and labels, computing similarity matrices, applying DBSCAN clustering with ε=0.07 and min_samples=1, selecting the most frequent label per cluster as representative, and finally assigning each image its highest-similarity label.

## Key Results
- Successfully reduced Factorynet dataset label vocabulary from 6,426 to 408 distinct labels
- Identified and corrected mislabeled, misspelled, and semantically weak labels with minimal human intervention
- Demonstrated framework effectiveness through qualitative validation on over 10,000 manufacturing images

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Alignment via CLIP
- Claim: Cosine similarity between CLIP image and text embeddings correlates with label correctness
- Mechanism: CLIP's dual-encoder architecture maps images and labels to a shared 768-dimensional space where geometric proximity reflects semantic relatedness. Low similarity scores (closer to 0) indicate label-image mismatches; high scores (closer to 1) indicate correct assignments
- Core assumption: CLIP's pre-training on internet-scale image-text pairs transfers to manufacturing domain semantics without domain-specific fine-tuning
- Break condition: Manufacturing-specific jargon or novel equipment not well-represented in CLIP's pre-training data will produce unreliable similarity scores

### Mechanism 2: Density-Based Clustering for Semantic Consolidation
- Claim: DBSCAN on text embeddings groups semantically equivalent labels despite surface variations
- Mechanism: Label embeddings cluster by semantic meaning. DBSCAN with cosine distance metric (1 - cosine similarity) groups nearby points without pre-specifying cluster count, handling synonyms, misspellings, and casing variations
- Core assumption: Semantically similar labels have embeddings within ε distance (set to 0.07) in the CLIP space
- Break condition: ε values too high (e.g., 0.1) cause semantic drift—grouping unrelated labels like "Electronic locks" with "triangle files"

### Mechanism 3: Frequency-Weighted Representative Selection
- Claim: Selecting the most frequent label within a cluster as representative preserves dataset coherence
- Mechanism: After clustering, the label with highest occurrence becomes the cluster representative. This ensures the canonical label is both semantically valid and statistically dominant, reducing class imbalance
- Core assumption: Higher frequency correlates with correct/better label formulation
- Break condition: If the most frequent label is itself a common misspelling or jargon term, errors propagate across the cluster

## Foundational Learning

- **Concept**: CLIP Vision-Language Model
  - Why needed here: VLSR depends entirely on CLIP's ability to create aligned image-text embeddings; without understanding this foundation, the similarity comparisons lack interpretability
  - Quick check question: Can you explain why CLIP uses a dual-encoder architecture rather than a single joint encoder?

- **Concept**: Cosine Similarity vs. Euclidean Distance
  - Why needed here: DBSCAN defaults to Euclidean distance, but semantic similarity in embedding spaces is direction-driven; magnitude variations are irrelevant
  - Quick check question: For two embedding vectors [1,0,0] and [100,0,0], what is their cosine similarity? What is their Euclidean distance?

- **Concept**: DBSCAN Clustering
  - Why needed here: The choice of DBSCAN over K-Means is deliberate—cluster count is unknown, and noise points should remain unclustered rather than forced into groups
  - Quick check question: What happens to a point in DBSCAN if it has fewer neighbors than `min_samples` within distance `ε`?

## Architecture Onboarding

- **Component map**:
  Raw Dataset (images + noisy labels) -> CLIP Encoder (vit-large-patch14, 768-dim) -> Cosine Similarity Matrix -> DBSCAN Clustering (cosine distance, ε=0.07, min_samples=1) -> Small Cluster Merging -> Representative Label Selection (max frequency per cluster) -> Final Single-Label Assignment (highest similarity per image)

- **Critical path**: CLIP embedding quality -> ε parameter selection in DBSCAN -> representative label choice. Errors in embedding quality cannot be recovered downstream

- **Design tradeoffs**:
  - ε = 0.07 vs. higher values: Lower ε preserves semantic precision but may leave valid labels unclustered; higher ε risks semantic drift (paper shows ε=0.1 grouped unrelated items)
  - min_samples = 1 vs. higher: Setting to 1 allows singleton clusters (more coverage) but may retain noise; higher values filter noise but require denser label spaces
  - No label preprocessing: Preserves meaning for technical terms like "PPE" but allows casing/format variations to persist as separate labels

- **Failure signatures**:
  - Semantic drift in clusters: Check for unrelated items in same cluster; indicates ε too high
  - Singleton clusters dominating: Many clusters with count=1; indicates ε too low or sparse label space
  - Low similarity scores on clean labels: Values consistently <0.2 even for correct pairs; indicates CLIP doesn't transfer well to domain

- **First 3 experiments**:
  1. **Embedding sanity check**: Select 20 images with known correct labels; verify their cosine similarities are >0.25. If not, CLIP may not suit the domain
  2. **ε sensitivity analysis**: Run DBSCAN with ε ∈ {0.05, 0.07, 0.10} on a 500-label subset; manually inspect cluster coherence for each
  3. **Cluster quality validation**: For 10 random clusters, list all member labels and verify they are semantically equivalent; compute precision of grouping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive thresholding of cosine similarity scores enable fully unsupervised label sanitization?
- Basis in paper: The Conclusion states future work will explore "automating the sanitization process and enabling fully unsupervised identification and correction of noisy labels through adaptive thresholding of cosine similarity scores."
- Why unresolved: The current framework relies on fixed similarity thresholds and manual hyperparameter selection (specifically $\epsilon$ and minimum samples for DBSCAN), which requires human inspection to ensure semantic integrity
- What evidence would resolve it: An automated system that dynamically adjusts thresholds based on data distribution, demonstrating cluster quality comparable to human-verified baselines without manual intervention

### Open Question 2
- Question: Can foundation models effectively validate cluster coherence to reduce semantic drift?
- Basis in paper: The Conclusion proposes employing "foundation models... to validate the coherence of each cluster by verifying the semantic relatedness of the grouped labels."
- Why unresolved: The current clustering method is sensitive to hyperparameters; for example, setting $\epsilon$ too high results in "semantic drift," grouping conceptually unrelated labels (e.g., "Electronic locks" and "glass jars") into the same cluster
- What evidence would resolve it: A secondary validation model successfully rejecting incoherent clusters (low semantic relatedness) and triggering a re-clustering with adjusted parameters

### Open Question 3
- Question: Is the VLSR framework effective when extended to video or 3D data modalities?
- Basis in paper: The Conclusion suggests that future work could "explore extending the VLSR framework to other modalities, such as video or 3D data."
- Why unresolved: The current experimental validation is restricted to 2D images (Factorynet dataset), leaving the framework's applicability to temporal (video) or spatial (3D) manufacturing data unproven
- What evidence would resolve it: Successful application of the framework to video or 3D point-cloud datasets, showing similar reductions in label noise and vocabulary redundancy

## Limitations

- Domain transfer reliability: Assumes CLIP's pre-trained knowledge adequately covers manufacturing-specific semantics without domain-specific fine-tuning validation
- Parameter sensitivity: ε=0.07 parameter is presented as optimal without systematic sensitivity analysis across parameter space
- Validation methodology: Relies primarily on qualitative assessment through visual inspection without quantitative metrics for label identification precision/recall

## Confidence

- **High Confidence**: The fundamental mechanism of using CLIP embeddings for cross-modal similarity scoring is well-established and theoretically sound. The framework's architecture is clearly specified
- **Medium Confidence**: The 6,426→408 label reduction demonstrates effectiveness, but the lack of quantitative validation and limited domain testing reduces confidence in generalizability
- **Low Confidence**: Claims about minimal human intervention are unverified—the paper doesn't report human review effort required for validation or parameter tuning

## Next Checks

1. **Cross-Domain Transfer Test**: Apply VLSR to a non-manufacturing dataset (e.g., medical imaging or satellite imagery) with known label quality issues. Measure (a) label vocabulary reduction magnitude, (b) cluster semantic coherence, and (c) whether CLIP embeddings transfer without domain-specific fine-tuning

2. **Parameter Sensitivity Analysis**: Systematically vary ε ∈ {0.03, 0.05, 0.07, 0.09, 0.11} and min_samples ∈ {1, 2, 3} on Factorynet. For each configuration, compute: (a) number of clusters formed, (b) average cluster size, (c) manual semantic coherence score (1-5) for 20 random clusters, and (d) percentage of singletons

3. **Downstream Performance Validation**: Train an identical image classification model on Factorynet using: (a) original noisy labels, (b) VLSR-sanitized labels, and (c) human-curated clean labels. Compare test accuracy, precision@k, and confusion matrix patterns to quantify the practical benefit of automated sanitization