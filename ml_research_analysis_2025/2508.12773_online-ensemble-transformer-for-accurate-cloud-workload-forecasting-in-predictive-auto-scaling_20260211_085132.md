---
ver: rpa2
title: Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive
  Auto-Scaling
arxiv_id: '2508.12773'
source_url: https://arxiv.org/abs/2508.12773
tags:
- online
- forecasting
- workload
- data
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes E3Former, an online ensemble Transformer for
  workload forecasting in cloud auto-scaling systems. The model addresses the challenge
  of capturing complex multi-scale periodic patterns in volatile online workload streams
  by combining multi-resolution patching (Representer), Transformer-based sequence
  modeling, online adaptation (Adapter), and adaptive ensembling (Ensembler).
---

# Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling

## Quick Facts
- arXiv ID: 2508.12773
- Source URL: https://arxiv.org/abs/2508.12773
- Reference count: 40
- E3Former achieves 13.9% MSE, 11.7% MAE, and 19.1% WMAPE reduction over state-of-the-art baselines

## Executive Summary
This paper introduces E3Former, an online ensemble Transformer architecture for accurate cloud workload forecasting in predictive auto-scaling systems. The model addresses the challenge of capturing complex multi-scale periodic patterns in volatile online workload streams by combining multi-resolution patching, Transformer-based sequence modeling, online adaptation, and adaptive ensembling. Evaluated on real-world workload datasets, E3Former achieves significant forecasting accuracy improvements while maintaining low computational overhead, and has been deployed in ByteDance's IHPA platform supporting over 600,000 CPU cores.

## Method Summary
E3Former addresses the challenge of capturing complex multi-scale periodic patterns in volatile online workload streams through an online ensemble Transformer architecture. The method integrates four key components: Representer (multi-resolution patching to extract hierarchical representations), Transformer (sequence modeling to capture long-term dependencies), Adapter (online adaptation for concept drift), and Ensembler (adaptive weighting for prediction aggregation). The architecture uses shared parameters across multiple subnetworks to reduce computational overhead while maintaining ensemble benefits. The model processes time series data through multi-resolution patches, applies transformer encoders for sequence modeling, adapts to online concept drift through online learning, and combines predictions from multiple strategies using adaptive ensembling.

## Key Results
- Achieves 13.9% MSE reduction, 11.7% MAE reduction, and 19.1% WMAPE reduction compared to state-of-the-art baselines
- In Kubernetes auto-scaling tests, reduces average latency by 5.6% and maximum latency by 91.7%
- Reduces resource consumption by 7.3% (average) and 29.4% (maximum) versus Naïve HPA
- Successfully deployed in ByteDance's IHPA platform supporting >40% resource utilization reduction

## Why This Works (Mechanism)
The approach works by addressing the fundamental challenge of modeling multi-scale periodic patterns in volatile online workload streams. Multi-resolution patching extracts hierarchical representations that capture both short-term fluctuations and long-term trends. The Transformer architecture models long-term dependencies and complex temporal relationships that traditional statistical methods miss. Online adaptation enables the model to adjust to concept drift in real-time workloads, while adaptive ensembling combines multiple forecasting strategies to improve robustness and accuracy. The shared-parameter design reduces computational overhead while maintaining the benefits of ensemble learning.

## Foundational Learning
- Multi-resolution patching: Extracts hierarchical representations at different scales; needed to capture both short-term fluctuations and long-term trends; quick check: visualize feature maps at different resolutions
- Transformer-based sequence modeling: Captures long-term dependencies and complex temporal relationships; needed because traditional methods miss non-linear patterns; quick check: compare attention weights across time steps
- Online adaptation: Adjusts model parameters in response to concept drift; needed because workloads evolve over time; quick check: monitor prediction error drift over time
- Adaptive ensembling: Combines multiple forecasting strategies with dynamic weights; needed to balance accuracy and robustness; quick check: analyze ensemble weight stability during concept drift
- Shared-parameter architecture: Reduces computational overhead while maintaining ensemble benefits; needed for scalable deployment; quick check: measure parameter count vs independent ensembles

## Architecture Onboarding

Component map: Input time series -> Representer (multi-resolution patching) -> Transformer (sequence modeling) -> Adapter (online adaptation) -> Ensembler (adaptive aggregation) -> Output forecasts

Critical path: The core prediction pipeline follows: time series data → multi-resolution feature extraction → transformer encoding → online adaptation → ensemble combination → final forecast. The online adaptation module is critical as it directly impacts the model's ability to maintain accuracy under concept drift.

Design tradeoffs: The shared-parameter architecture reduces computational overhead but may limit individual model specialization compared to independent ensembles. Multi-resolution patching increases feature extraction capability but adds preprocessing complexity. Online adaptation provides robustness to concept drift but introduces additional computational overhead during inference.

Failure signatures: Performance degradation during rapid concept drift indicates insufficient online adaptation. High variance in ensemble weights suggests instability in the adaptation mechanism. Poor long-term forecasting accuracy may indicate insufficient temporal context in the representation learning stage.

First experiments:
1. Validate multi-resolution feature extraction by visualizing learned representations at different scales
2. Test transformer attention patterns to ensure meaningful long-term dependency capture
3. Measure online adaptation effectiveness by comparing performance with and without adaptation under concept drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can predictive auto-scaling architectures be redesigned to support the efficiency and accuracy requirements of even larger-scale computing and storage tasks beyond the current 600,000 CPU core capacity?
- Basis in paper: The conclusion states, "In the future, we plan to explore more efficient and accurate predictive auto-scaling systems to support the efficient and lean operation of increasingly large-scale computing and storage tasks."
- Why unresolved: The current E3Former implementation, while effective at the specified scale, may face computational bottlenecks or latency constraints as the number of monitored instances and metrics grows exponentially in future cloud environments.
- What evidence would resolve it: A scalability analysis demonstrating that the model's inference latency and memory footprint remain constant or grow sub-linearly as the number of concurrent time series increases by an order of magnitude.

### Open Question 2
- Question: Can a unified framework dynamically select between Online Scaling (OS) and Follow The Perturbed Leader (FTPL) strategies during inference to balance the trade-off between maximal accuracy and robustness against distribution shifts?
- Basis in paper: [inferred] The paper establishes a trade-off where E3Former-OS achieves lower error in stable online forecasting, while E3Former-FTPL offers better generalization in cold-start/transfer scenarios and higher throughput (inferred from Tables 4, 6, and Figure 8).
- Why unresolved: The current implementation requires selecting one strategy (OS or FTPL) a priori, forcing a choice between the superior accuracy of OS or the efficiency and adaptability of FTPL, without a mechanism to switch based on real-time conditions.
- What evidence would resolve it: An adaptive algorithm that monitors online distribution drift indicators and dynamically switches to FTPL during high-drift periods (cold-starts) and OS during stable periods, outperforming both static configurations.

### Open Question 3
- Question: How can the "look-back window" length be optimized adaptively rather than relying on a fixed static length (1440) when dealing with varying data availability in hot/cold storage tiers?
- Basis in paper: [inferred] The paper notes that hot storage (Redis) is limited in capacity compared to cold storage (ClickHouse), yet the implementation uses a fixed look-back length ($L=1440$) for all datasets (Section 5.1.3).
- Why unresolved: A fixed window may waste computational resources on low-information historical data in stable periods or miss critical long-term context in volatile periods if the window is too short for the specific workload's periodicity.
- What evidence would resolve it: A sensitivity analysis showing the impact of varying look-back lengths on the model's regret bound, or a mechanism that dynamically adjusts the input length based on the available "hot storage" data and forecast horizon.

## Limitations
- Results are primarily demonstrated in ByteDance's specific infrastructure configuration, limiting generalizability across different cloud providers
- The adaptive ensembling mechanism introduces additional complexity that could impact deployment in resource-constrained environments
- The evaluation relies on real-world datasets that may not fully represent the diversity of cloud workloads across different application types

## Confidence
- Core forecasting claims: High (significant quantitative improvements over baselines with real-world data)
- Auto-scaling performance claims: Medium (results demonstrated in specific deployment context rather than extensive cross-platform validation)
- Online adaptation effectiveness: Medium (paper lacks detailed analysis of adaptation latency and robustness to various drift types)

## Next Checks
1. Test E3Former across multiple cloud providers and workload types to assess generalizability beyond ByteDance's infrastructure
2. Conduct ablation studies to quantify the individual contributions of the Representer, Adapter, and Ensembler components to overall performance
3. Evaluate the model's behavior under extreme workload patterns and stress conditions to verify stability limits