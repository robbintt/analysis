---
ver: rpa2
title: Explainable Detection of Implicit Influential Patterns in Conversations via
  Data Augmentation
arxiv_id: '2506.14211'
source_url: https://arxiv.org/abs/2506.14211
tags:
- line
- influential
- implicit
- patterns
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework to detect implicit influential patterns
  in conversations using data augmentation with large language models. The method
  involves using reasoning models to identify influential segments in conversations,
  augmenting the data, and fine-tuning a base model in two phases using LoRA adapters.
---

# Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation

## Quick Facts
- arXiv ID: 2506.14211
- Source URL: https://arxiv.org/abs/2506.14211
- Reference count: 29
- Two-phase LoRA fine-tuning with data augmentation improves detection accuracy by 6% (binary) and 33-43% (multi-label) over baseline approaches

## Executive Summary
This paper presents a framework for detecting implicit influential patterns in conversations using data augmentation with large language models. The approach uses reasoning models to identify manipulative segments, augments the data, and fine-tunes a base model in two phases using LoRA adapters. The method achieves significant improvements in detection accuracy while enabling explainable identification of manipulative conversational segments. Experiments demonstrate that well-designed fine-tuning procedures outperform simply increasing model size.

## Method Summary
The framework processes conversations by first formatting them with line numbers, then using a distilled reasoning model (DeepSeek-R1-Distill-Llama-8B) to identify influential lines through 10 stochastic passes per conversation. These outputs are aggregated by a non-reasoning model (Llama-3.3-70B-Instruct) to create line-level labels. Two-phase LoRA fine-tuning follows: Phase 1 trains an adapter for instruction tuning to predict influential lines, while Phase 2 freezes these weights and trains a new adapter plus classification head for detection tasks. The approach was evaluated on MentalManipCon and MentalManipMaj datasets with binary and multi-label classification tasks.

## Key Results
- Improved binary classification accuracy by 6% (82.6% vs 76.8%) using Llama-3.2-3B compared to vanilla fine-tuning
- Achieved 33% improvement in detecting influence techniques and 43% improvement in victim vulnerability detection
- Demonstrated that well-designed fine-tuning pipelines outperform raw parameter scaling (3B parameters with framework outperformed 13B parameters with baseline fine-tuning)
- Enabled explainable detection by identifying specific manipulative lines within conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-model-based data augmentation creates precise training signals by identifying specific influential lines rather than labeling entire conversations
- Mechanism: A distilled reasoning model (DeepSeek-R1-Distill-Llama-8B) processes each conversation 10 times to identify line numbers containing implicit manipulation. A non-reasoning model (Llama-3.3-70B-Instruct) aggregates these stochastic outputs into consensus labels. This transforms binary conversation-level labels into granular line-level annotations.
- Core assumption: Reasoning models can reliably identify implicit manipulative patterns when given conversational context; aggregation across multiple runs reduces false positives from model stochasticity
- Evidence anchors:
  - [abstract] "the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models"
  - [section 3.1] "each conversation was prompted to the reasoning language model ten times, and the results from these analyses were summarized by another language model"
- Break condition: If reasoning models cannot consistently identify implicit patterns (evidenced by low inter-run agreement), aggregation will amplify noise rather than signal

### Mechanism 2
- Claim: Two-phase LoRA fine-tuning with frozen weight inheritance enables both explanatory localization and accurate classification
- Mechanism: Phase 1 attaches LoRA adapter (matrices A, B) for instruction fine-tuning on line-level labels, teaching the model to identify influential segment locations. Phase 2 freezes Phase 1 weights, adds a second LoRA adapter (matrices C, D) and classification head, training only new parameters for detection tasks. The combined forward pass becomes: h₂ = W₁y + ABy + CDy
- Core assumption: Knowledge from instruction fine-tuning (locating influential segments) transfers to classification without catastrophic interference when Phase 1 weights are frozen
- Evidence anchors:
  - [abstract] "fine-tuning a base model in two phases using LoRA adapters"
  - [section 3.2] "Following the initial instruction fine-tuning, the previous weights are frozen, and only the newly introduced parameters in the second adapter and the classification head are updated"
- Break condition: If Phase 1 instruction tuning overfits to augmentation artifacts or Phase 2 classification fails to leverage frozen representations, performance gains will not materialize

### Mechanism 3
- Claim: Well-designed fine-tuning pipelines outperform raw parameter scaling for implicit pattern detection
- Mechanism: The framework achieved 82.6% accuracy with Llama-3.2-3B versus 76.8% with Llama-2-13B (baseline fine-tuning)—a 6% improvement using 10 billion fewer parameters. Multi-label classification improved 33-43% over vanilla fine-tuning approaches
- Core assumption: Implicit influential patterns require task-specific training signals (line-level annotations) rather than general model capacity; larger models without appropriate fine-tuning data cannot compensate
- Evidence anchors:
  - [abstract] "experiments showed that well-designed fine-tuning procedures outperform simply increasing model size"
  - [section 6] "increasing the size of a language model does not significantly enhance detection performance"
- Break condition: If implicit patterns become more sophisticated or training data distribution shifts significantly, the relative advantage of fine-tuning over scale may diminish

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper freezes base model weights and trains only low-rank adapter matrices (A×B where rank r < min(d,k)), making two-phase fine-tuning computationally feasible without full parameter updates
  - Quick check question: Can you explain why freezing W₁ while training only ΔW₁ = AB preserves base knowledge while enabling task adaptation?

- Concept: **Instruction Fine-Tuning**
  - Why needed here: Phase 1 trains the model to output specific line numbers (e.g., "Line_1, Line_3, Line_5") rather than just classification labels, enabling explainable detection
  - Quick check question: How does instruction fine-tuning differ from standard supervised fine-tuning in terms of output format and task specification?

- Concept: **Reasoning Models with Thinking Tokens**
  - Why needed here: DeepSeek-R1-Distill supports intermediate reasoning steps before producing final answers, which the paper leverages for identifying subtle implicit patterns that require contextual analysis
  - Quick check question: What is the tradeoff between allowing a model to generate thinking tokens versus direct inference for detection tasks?

## Architecture Onboarding

- Component map:
  Data Preprocessing -> Augmentation Engine -> Phase 1 Model -> Phase 2 Model

- Critical path:
  1. Format conversations with line prefixes
  2. Run reasoning model 10× per conversation for line identification
  3. Aggregate outputs → create instruction-tuning dataset (input: conversation, output: "Line_X, Line_Y...")
  4. Phase 1: Train LoRA adapter on instruction task
  5. Freeze Phase 1, attach new adapter + classifier head
  6. Phase 2: Train classifier on original labels (binary/multi-label)

- Design tradeoffs:
  - **Model size vs. augmentation quality**: Paper shows Llama-3.2-1B with this framework outperforms Llama-2-13B with vanilla fine-tuning—prioritize pipeline design over scale
  - **Runs per conversation**: 10 runs chosen to handle stochasticity; fewer runs reduce compute but increase aggregation noise
  - **Reasoning model size**: 8B parameter distillation chosen for efficiency; larger reasoning models may improve augmentation but increase costs 10× (due to multiple runs)

- Failure signatures:
  - Low inter-run agreement in reasoning model outputs → augmentation quality degraded
  - Phase 1 overfits to augmentation artifacts → poor generalization to unseen conversations
  - Phase 2 classifier ignores frozen representations → similar performance to baseline fine-tuning
  - Model outputs inconsistent line formats → instruction fine-tuning fails to converge

- First 3 experiments:
  1. **Validate augmentation quality**: Manually sample 50-100 aggregated labels against human annotations; target >85% agreement before proceeding to fine-tuning
  2. **Ablate Phase 1**: Compare (Phase 1 + Phase 2) vs. (Phase 2 only with raw conversation labels) on held-out test set to isolate instruction-tuning contribution
  3. **Scaling test**: Run framework with Llama-3.2-1B, 3B, and 8B base models on MentalManipCon; verify 3B achieves paper-reported ~82.6% accuracy and identify if your implementation diverges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the framework maintain its detection performance when applied to natural, non-cinematic conversations such as real-world chat logs or social engineering logs?
- Basis in paper: [inferred] The paper utilizes the MentalManip dataset, which, as cited in the text, was constructed using dialogue collected from movies rather than organic human-computer interactions
- Why unresolved: Models trained on dramatic or scripted dialogue may overfit to linguistic cues that are exaggerated compared to subtle, real-world manipulation, a limitation not addressed by the current evaluation
- What evidence would resolve it: Evaluation results from training on the current dataset and testing on a held-out set of organic conversational logs (e.g., scam archives)

### Open Question 2
- Question: To what extent does the "reasoning" model used for data augmentation introduce systematic biases or hallucinations that affect the fine-tuned model's reliability?
- Basis in paper: [inferred] The methodology relies on a distilled reasoning model (DeepSeek-R1-Distill-Llama-8B) to generate ground-truth labels for training, with accuracy verified only by manual "sampling" rather than comprehensive validation
- Why unresolved: If the reasoning model consistently misidentifies specific types of nuance as influential, the student model will inherit and amplify these errors, a risk not quantified in the results
- What evidence would resolve it: An error analysis of the augmented data generation phase comparing the reasoning model's labels against a gold-standard human annotation set

### Open Question 3
- Question: Is the identification of specific conversational lines sufficient for end-user explainability, or does the lack of semantic rationale limit the utility of the system?
- Basis in paper: [inferred] The paper claims "explainable identification," but the framework is explicitly designed to output only the line numbers (e.g., "Line_1, Line_3") containing the influential patterns
- Why unresolved: While localization aids interpretability, it is unclear if users require a generated explanation of *why* a line is manipulative to trust or effectively use the system
- What evidence would resolve it: A user study measuring the helpfulness and trustworthiness of the model when providing only line localization versus providing natural language explanations

### Open Question 4
- Question: Would the application of this specific two-phase fine-tuning framework yield further improvements if applied to significantly larger base models?
- Basis in paper: [inferred] The paper concludes that a robust pipeline outperforms increasing model size, but the largest model tested was Llama-3.1-8B, while the comparison baseline was an older Llama-2-13B
- Why unresolved: The interaction between the proposed complex fine-tuning strategy and the emergent capabilities of much larger models (e.g., 70B+ parameters) remains untested
- What evidence would resolve it: Experimental results applying the same LoRA-based two-phase training to a 70B parameter model and comparing performance against the smaller variants

## Limitations

- Data augmentation reliability: The framework depends on reasoning models identifying implicit patterns that humans label only at the conversation level, without corpus validation showing reasoning model outputs match human annotations
- Two-phase transfer effectiveness: The claim that Phase 1 instruction tuning on line-level labels meaningfully improves Phase 2 classification performance lacks ablation evidence comparing against Phase 2 training directly on raw conversation labels
- Generalization beyond conversational datasets: All experiments use MentalManipCon and MentalManipMaj, which contain only two-party conversations with specific labeling conventions, limiting applicability to multi-party dialogues or different cultural contexts

## Confidence

- High confidence in: The core LoRA fine-tuning architecture (freezing base weights, training adapter matrices) is technically sound and well-established in the literature
- Medium confidence in: The specific 10-run aggregation strategy for reasoning model outputs and the exact instruction fine-tuning prompt formulation, which significantly impact performance but lack sensitivity analysis
- Low confidence in: The claim that the framework enables "explainable identification of manipulative conversational segments" beyond what simpler attention-based methods could provide, as the paper doesn't compare against baseline explainability approaches

## Next Checks

1. **Augmentation quality validation**: Manually annotate 100 conversations with line-level influence labels and compare against the aggregated reasoning model outputs. Calculate inter-rater reliability and augmentation accuracy before proceeding with fine-tuning experiments.

2. **Ablation of two-phase training**: Train three variants: (a) complete two-phase framework, (b) single-phase LoRA with raw conversation labels, (c) Phase 2 only starting from randomly initialized weights. Compare performance on held-out test sets to isolate the contribution of Phase 1 instruction tuning.

3. **Cross-dataset generalization test**: Apply the fine-tuned models to a different conversation dataset (e.g., real-world customer service interactions or political debates) without additional fine-tuning. Measure performance degradation and identify whether the approach generalizes beyond the training domain.