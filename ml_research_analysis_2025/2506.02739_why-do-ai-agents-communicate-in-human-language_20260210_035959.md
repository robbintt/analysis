---
ver: rpa2
title: Why do AI agents communicate in human language?
arxiv_id: '2506.02739'
source_url: https://arxiv.org/abs/2506.02739
tags:
- language
- communication
- multi-agent
- agent
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current multi-agent systems relying on natural
  language for communication suffer from structural misalignment between the high-dimensional
  semantic spaces of LLMs and the discrete token sequences of natural language. This
  misalignment leads to information loss, semantic drift, and behavioral inconsistencies
  during agent coordination.
---

# Why do AI agents communicate in human language?

## Quick Facts
- arXiv ID: 2506.02739
- Source URL: https://arxiv.org/abs/2506.02739
- Reference count: 40
- Key outcome: Natural language communication between AI agents suffers from structural misalignment with LLM internal representations, causing information loss, semantic drift, and coordination failures.

## Executive Summary
This paper identifies fundamental architectural limitations in current multi-agent systems that rely on natural language communication. The core issue stems from the mismatch between high-dimensional semantic spaces in which LLMs operate and the discrete token sequences of natural language, resulting in irreversible information loss during communication. This misalignment leads to cascading errors across interaction turns and creates gaps between expressed intent and actual execution. The authors propose a paradigm shift toward native multi-agent modeling with structured communication, explicit role persistence, and coordination graphs.

## Method Summary
The paper presents a theoretical analysis of why natural language communication fails in multi-agent LLM systems, identifying semantic space misalignment, cascading loss accumulation, and protocol-induced behavioral decoupling as core failure mechanisms. While no implementation details or experimental results are provided, the authors outline design requirements for improved architectures including role-bound semantic spaces, tensor-based communication with structural homomorphism guarantees, and dynamic coordination graphs. The work synthesizes evidence from related multi-agent systems and proposes a framework for addressing these limitations through explicit state synchronization and decoupled modules.

## Key Results
- Natural language creates many-to-one mapping from LLM internal states to tokens, causing irreversible information loss
- Multi-turn interactions compound reconstruction errors into systemic coordination failure
- LLMs lack native support for role persistence, task boundaries, and multi-agent dependencies

## Why This Works (Mechanism)

### Mechanism 1: Semantic Space Misalignment
- Claim: Natural language creates irreversible information loss when representing LLMs' high-dimensional internal states
- Mechanism: The projection f: H → L from continuous tensor space H to discrete token space L is many-to-one; distinct internal states map to identical linguistic expressions, preventing faithful reconstruction
- Core assumption: LLM internal representations encode task-relevant information in dense, continuous vectors that cannot be losslessly compressed to discrete tokens
- Evidence anchors:
  - [abstract]: "The semantic space of natural language is structurally misaligned with the high-dimensional vector spaces in which LLMs operate, resulting in information loss and behavioral drift"
  - [section III.A]: "As f is typically many-to-one, distinct internal semantic states may correspond to identical linguistic expressions, leading to semantic aliasing and non-invertibility"
  - [corpus]: Related MARL work (Foerster et al.) shows learned emergent protocols can outperform fixed languages, but corpus provides no direct empirical validation of this specific misalignment hypothesis
- Break condition: If models developed invertible semantic encodings where f⁻¹(f(h)) ≈ h, the compression-loss argument would not hold

### Mechanism 2: Cascading Semantic Loss Accumulation
- Claim: Multi-turn interactions compound reconstruction errors into systemic coordination failure
- Mechanism: Error δt = ∥h(j)t − f⁻¹(f(h(j)t))∥ accumulates across communication rounds; the total cascade loss Lcascade has non-zero lower bound under standard decoding
- Core assumption: Estimation errors are positively correlated across turns rather than self-correcting
- Evidence anchors:
  - [abstract]: "current LLM architectures, optimized for next-token prediction rather than multi-agent collaboration, lack native support for role persistence"
  - [section III.A]: "Lcascade cannot theoretically converge to zero without introducing additional structural priors or invertible semantic interfaces"
  - [corpus]: Laban et al. [14] document "lost-in-conversation" phenomenon where agents lose context in multi-turn planning—empirically consistent with cascade hypothesis
- Break condition: If agents could directly access environment state or shared memory (bypassing linguistic reconstruction), cascade would be interrupted

### Mechanism 3: Protocol-Induced Behavioral Decoupling
- Claim: Natural language protocols create systematic gap between expressed intent and executed action
- Mechanism: Language tokens lack direct binding to environment state space; agents generate plausible completion language without triggering actual execution (pseudo-execution)
- Core assumption: Models trained on text corpora learn linguistic coherence without grounding in executable action verification
- Evidence anchors:
  - [abstract]: "lack mechanisms for modeling role continuity, task boundaries, and multi-agent dependencies"
  - [section III.B]: "LLM-based agents frequently 'describe' the successful completion of a task without actually executing the required action"
  - [corpus]: Cemri et al. [18] empirically observed pseudo-execution in multi-agent LLM systems; consistent with proposed mechanism
- Break condition: If action execution were explicitly coupled to language generation via closed-loop verification, decoupling would be reduced

## Foundational Learning

- **Concept: Many-to-One Mapping (Non-Invertibility)**
  - Why needed here: The paper's core argument depends on understanding why f: H → L cannot be reversed—multiple high-dimensional states collapse to identical token sequences
  - Quick check question: Given a function mapping 100-dimensional vectors to single words, why can't you uniquely recover the original vector from its word output?

- **Concept: Semantic Drift**
  - Why needed here: The mechanism assumes meaning shifts across turns even when agents intend consistency; understanding this is essential for designing state synchronization
  - Quick check question: If Agent A transmits "optimize efficiency" and Agent B acts on it 10 turns later, what causes their interpretations to diverge?

- **Concept: State Synchronization vs. Message Passing**
  - Why needed here: The proposed paradigm shifts from exchanging human-readable messages to synchronizing internal tensor representations—a fundamental architectural change
  - Quick check question: What is the operational difference between "telling another agent what to do in English" and "sharing your task-representation tensor"?

## Architecture Onboarding

- **Component map:**
  - Role Space R = {r(1)...r(n)}: Explicit identifiers bound to agent states via h(i)t ∼ p(h|r(i))
  - Structured Communication (G, F): Tensor message generation G: H(i) → Mij; state update F preserving structural homomorphism (F ◦ G ≈ identity in collaborative subspace)
  - Coordination Graph Gt = (A, R, Et): Dynamic triplet tracking role relations via ψ(h(i)t, h(j)t)
  - Decoupled Modules: Perception-action loop (high-frequency), planning (long-horizon), coordination (consistency enforcement)

- **Critical path:**
  1. Implement role-binding mechanism that persists across turns (prevent role drift)
  2. Define tensor message format with structural homomorphism guarantees
  3. Build alignment verification: E[∥φ(h(i)t;s) − φ(h(j)t;s)∥²] ≤ ε (Eq. 9)
  4. Add coordination graph with edge weights encoding semantic coupling strength

- **Design tradeoffs:**
  - Interpretability ↔ Fidelity: Tensor communication preserves semantics but loses human-readability; hybrid approaches may be needed
  - Decoupling ↔ Integration: Separate modules reduce entanglement but increase system complexity
  - Structure ↔ Flexibility: Explicit coordination graphs prevent drift but constrain emergent coordination

- **Failure signatures:**
  - Redundant action generation (multiple agents executing identical subtasks—task decomposition failure)
  - Planning loops without execution (pseudo-execution; linguistic coherence without state change)
  - Role confusion (Agent A adopting Agent B's responsibilities mid-interaction)
  - Cascade propagation (one agent's error amplifying through coordination chain)

- **First 3 experiments:**
  1. **Baseline cascade measurement**: Run existing natural-language multi-agent system on coordinated task; use probe classifiers to measure semantic divergence between agents' internal states over turns
  2. **Role-binding ablation**: Compare agent consistency with explicit role-key embeddings vs. prompt-only role assignment; log role confusion event frequency
  3. **Communication medium comparison**: In minimal 2-agent coordination task, compare success rates across (a) natural language, (b) structured JSON state descriptors, (c) learned tensor communication via encoder-decoder

## Open Questions the Paper Calls Out
None

## Limitations
- No direct experimental validation of proposed mechanisms
- Missing quantitative metrics for measuring information loss or behavioral drift
- No implementation details for proposed structured communication paradigm
- Limited empirical evidence beyond references to existing multi-agent systems

## Confidence
- **High confidence**: Theoretical analysis of semantic space misalignment and observation that LLMs lack native multi-agent coordination mechanisms
- **Medium confidence**: Cascade accumulation hypothesis and protocol-induced behavioral decoupling mechanisms require empirical validation
- **Low confidence**: Proposed solutions (structured tensor communication, coordination graphs, role persistence) are untested and implementation complexity unknown

## Next Checks
1. **Cascade measurement experiment**: Instrument existing multi-agent LLM systems to track semantic divergence across communication turns using state probing techniques
2. **Communication medium ablation**: Implement controlled experiments comparing natural language vs. structured state descriptors vs. learned tensor communication in simple coordination tasks
3. **Role persistence evaluation**: Design experiments to measure agent consistency when using explicit role-key embeddings versus prompt-only role assignment