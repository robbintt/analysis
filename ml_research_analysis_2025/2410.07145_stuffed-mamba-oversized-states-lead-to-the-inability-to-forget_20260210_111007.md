---
ver: rpa2
title: 'Stuffed Mamba: Oversized States Lead to the Inability to Forget'
arxiv_id: '2410.07145'
source_url: https://arxiv.org/abs/2410.07145
tags:
- layer
- length
- state
- training
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Stuffed Mamba: Oversized States Lead to the Inability to Forget

## Quick Facts
- **arXiv ID:** 2410.07145
- **Source URL:** https://arxiv.org/abs/2410.07145
- **Reference count:** 40
- **Primary result:** Mamba-based models trained on contexts shorter than their state capacity fail to learn forgetting, leading to catastrophic degradation on longer sequences.

## Executive Summary
This paper identifies a critical failure mode in Mamba-based language models: when the training context length is shorter than the state size, models learn to retain all information rather than selectively forget, leading to poor length generalization. The authors demonstrate that this "inability to forget" causes memory interference that degrades performance on sequences longer than training. They establish a precise scaling law showing the minimum training length needed for forgetting scales linearly with state size, and propose interventions to artificially induce forgetting without retraining.

## Method Summary
The authors evaluate Mamba-2 checkpoints on synthetic passkey retrieval tasks and continue pre-training with varying context lengths (4K-256K). They analyze retention strength by tracking α values across positions, identify failure modes through state statistics and loss patterns, and test interventions like Reduced Memory Retention and Insertion (RRI) and Sliding Window methods. Experiments use truncated BPTT, WSD learning rate scheduling, and the RedPajama-V2 corpus with specific preprocessing.

## Key Results
- Models trained on 8K contexts fail catastrophically when evaluated on 32K+ contexts due to over-retention of early tokens
- Passkey retrieval accuracy drops to near-zero for all positions when context exceeds training length
- State mean/variance shows discontinuity at training length boundary, with outlier channels indicating memory saturation
- A linear relationship exists: T_forget ≈ 5.172 × N_S - 4.469 tokens needed for forgetting to emerge

## Why This Works (Mechanism)

### Mechanism 1: State Overparameterization Induces Forgetting Failure
- Claim: Models with states excessively large relative to training length never learn to forget because language modeling loss can be minimized without it.
- Mechanism: The recurrent state h_t accumulates information via α_t decay. When training length < state capacity, minimizing loss favors retention over selective forgetting. As training progresses, the model increasingly retains all context (an overfitting-like behavior).
- Core assumption: The optimization landscape biases toward retention when state capacity exceeds information density.
- Evidence anchors:
  - [abstract] "this issue stems from training on contexts that are too short for the state size, enabling the model to perform well without needing to learn how to forget"
  - [Section 4.1] Passkey retrieval accuracy for contexts longer than training length decreases with more training data, while short-context accuracy increases
  - [Section 5.2] Linear relationship T_forget = 5.172 × N_S - 4.469 with R² > 0.999
  - [corpus] Limited corpus support; related work "Characterizing Mamba's Selective Memory" discusses information loss thresholds but not the training-length interaction.
- Break condition: If training length exceeds the forget threshold, the model learns forgetting and length generalization improves substantially.

### Mechanism 2: Memory Interference from Over-Retention Degrades Retrieval
- Claim: Retaining all tokens causes retrieval errors through non-orthogonal key representations, damaging both early and recent token recall.
- Mechanism: State h_t = Σ α_i:t B_i x_i. When querying with C_t = B_s, retrieval returns α_s:t(C_t B_s)x_s plus interference Σ_{i≠s} α_i:t C_t B_i x_i. As context grows, B_i vectors cannot remain orthogonal, and cumulative interference exceeds tolerance.
- Core assumption: The interference term grows unboundedly with context length when decay is insufficient.
- Evidence anchors:
  - [Section 3.1] Eq. 7 formalizes retrieval error from non-orthogonal B_i
  - [Section 3.2.1] Three heads in layer 38 have first-token retention >0.997 at t=8K
  - [Section 2.2] Passkey retrieval fails on recent tokens when context exceeds threshold—not just early tokens
- Break condition: If α_t produces sufficient decay for early tokens, interference remains bounded.

### Mechanism 3: Artificial Forgetting Induction Restores Length Generalization
- Claim: Intervening on the update rule to force forgetting mitigates degradation without retraining.
- Mechanism: Two methods: (1) Reduced Memory Retention and Insertion (RRI) scales α_t by 0.9999 and B_t by 0.75; (2) Sliding Window computes h_t^(r) = h_t - α_{t-r+1:t} h_{t-r} to simulate fixed-width attention via state arithmetic.
- Core assumption: The learned α_t and B_t are causally responsible for over-retention, and scaling them proportionally reduces interference.
- Evidence anchors:
  - [Section 3.2.2] Figure 4 shows both RRI and Sliding Window reduce loss beyond training length vs. original model
  - [Section 3.2.2] LongMamba (dividing Δ_t by constant) also helps but compromises short-context performance
  - [corpus] "Forgetting Transformer" incorporates forget gates into attention; analogous mechanism but different architecture.
- Break condition: If scaling factors are too aggressive, short-context performance degrades from information loss.

## Foundational Learning

- **Recurrent State Dynamics and Memory Decay**
  - Why needed here: Mamba's update rule h_t = α_t h_{t-1} + B_t x_t depends critically on α_t behavior over long sequences.
  - Quick check question: If α_t = 0.999 for all t, what is h_{1000} approximately proportional to?

- **Information Interference in Compressed Representations**
  - Why needed here: Understanding why fixed-size states fail requires grasping how non-orthogonal representations cause retrieval noise.
  - Quick check question: In a d-dimensional space, how many approximately orthogonal vectors can exist?

- **Length Generalization vs. Extrapolation**
  - Why needed here: Distinguishing training distribution mismatch from architectural capacity limits.
  - Quick check question: Why might a model trained on 8K contexts fail at 16K even with no architectural constraints?

## Architecture Onboarding

- **Component map:**
  Input u_t → Conv(·) with kernel 4 → produces B_t, C_t, x_t, Δ_t → Δ_t → α_t = exp(-Δ_t exp(A)) controls decay → State update: h_t = α_t h_{t-1} + Δ_t B_t x_t → Output: y_t = C_t h_t + D ⊙ x_t → State size per layer: HPN = 256d (with P=64, N=128, H=2d/P)

- **Critical path:**
  1. Identify your target deployment context length L_deploy
  2. Compute required state size: N_S ≈ (L_deploy + 4.469) / 5.172
  3. Set training length T_train > T_forget for that state size
  4. Monitor first-token retention during training; should decay below 0.5 by T_train

- **Design tradeoffs:**
  - Larger state → higher recall capacity (exponential scaling T_recall) but requires longer training
  - Forgetting interventions (RRI, Sliding Window) help generalization but may harm short-context performance
  - Mamba-2's state is 8× larger than Mamba-1 (N=128 vs N=16), creating higher overparameterization risk

- **Failure signatures:**
  - Loss explodes sharply when token position exceeds training length (Figure 1)
  - State mean/variance shows discontinuity at training length boundary (Figure 5)
  - Passkey retrieval accuracy drops to near-zero for all positions, not just early ones
  - First-token retention remains >0.99 throughout training context

- **First 3 experiments:**
  1. **Retention profile audit:** Feed newline tokens to trained model; plot α_{1:t} for each head. If >0.9 at T_train, model hasn't learned forgetting.
  2. **Forget threshold calibration:** Train same architecture with varying T_train; identify where length generalization (loss at 2× T_train) stabilizes.
  3. **RRI ablation:** Apply α_t scaling factors {0.999, 0.9999, 0.99999} on held-out long contexts; plot loss vs. position to find optimal decay without short-context degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the linear relationship between state size and the forgetting training threshold, and the general "inability to forget" phenomenon, generalize to other recurrent architectures like RWKV or GLA?
- Basis in paper: [explicit] Section 2.1 states, "some conclusions/insights may apply to other architectures. We leave such exhaustive ablation studies for future work."
- Why unresolved: The study focuses almost exclusively on Mamba-2 due to the availability of checkpoints and its strong performance, noting only brief comparisons with RWKV-5/6 and Mamba-1 in appendices.
- What evidence would resolve it: Replicating the training length and state size scaling experiments detailed in Section 5.2 on architectures with different update rules (e.g., RWKV, GLA) to see if $T_{forget}$ scales linearly with state size $N_S$.

### Open Question 2
- Question: Can a mechanism or training objective be developed that forces a model to learn forgetting to improve length generalization without compromising performance on short-context tasks?
- Basis in paper: [inferred] Section 3.2.2 demonstrates that while artificially inducing forgetting (via RRI or Sliding Window methods) improves long-context performance, it "compromises short-context performance due to weaker memory insertion."
- Why unresolved: The authors identify a trade-off where the interventions necessary to fix long-context degradation inherently weaken the model's ability to utilize memory within its training window.
- What evidence would resolve it: A training technique that maintains the baseline perplexity of the model within the 8K training window while successfully achieving robust forgetting (loss stability) at 32K+ tokens.

### Open Question 3
- Question: Is the linear scaling of training length with state size strictly necessary, or can efficient regularization replace the need for expensive long-sequence training?
- Basis in paper: [inferred] Section 5.2 establishes the law $T_{forget} \approx 5.172 \cdot N_S$, implying that training large-state models requires prohibitively long context windows (e.g., >128K for large models) to learn forgetting.
- Why unresolved: The paper concludes that current training lengths are suboptimal, but the proposed solution (increasing training length linearly) implies massive computational overhead that future designs might avoid.
- What evidence would resolve it: A regularization loss or curriculum that successfully trains a model to exhibit the "forgetting" behavior (state capacity limit) on sequences significantly shorter than the theoretical $T_{forget}$ threshold.

## Limitations
- The paper focuses exclusively on Mamba-based architectures, leaving generalization to other recurrent models unexplored
- Artificial forgetting interventions (RRI, Sliding Window) improve long-context performance but compromise short-context performance
- The proposed solution requires training on contexts 5× longer than deployment context, creating significant computational overhead

## Confidence
- **High:** The linear scaling law T_forget ≈ 5.172 × N_S is empirically well-supported with R² > 0.999 across multiple model sizes
- **Medium:** The claim that overparameterization causes forgetting failure is strongly supported but relies on the assumption that optimization landscape biases toward retention
- **Medium:** The effectiveness of artificial forgetting interventions is demonstrated but shows trade-offs with short-context performance

## Next Checks
1. Replicate the passkey retrieval experiment on Mamba-2 370M checkpoint with context lengths 1K-32K to verify degradation patterns
2. Train a small Mamba model with varying T_train (8K, 16K, 32K) and plot loss at 2× training length to identify the forget threshold
3. Apply RRI with different scaling factors (0.999, 0.9999, 0.99999) on a held-out long context to find the optimal balance between forgetting and short-context performance