---
ver: rpa2
title: A Multi-Layered Large Language Model Framework for Disease Prediction
arxiv_id: '2502.00063'
source_url: https://arxiv.org/abs/2502.00063
tags:
- text
- language
- fine-tuning
- classification
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-layered framework combining LLM-based
  preprocessing with Arabic language model fine-tuning for disease prediction in social
  telehealth. The approach uses text refinement, summarization, and Named Entity Recognition
  to enhance Arabic language models CAMeL-BERT, AraBERT, and Asafaya-BERT for disease
  type classification and severity assessment.
---

# A Multi-Layered Large Language Model Framework for Disease Prediction

## Quick Facts
- arXiv ID: 2502.00063
- Source URL: https://arxiv.org/abs/2502.00063
- Reference count: 27
- One-line primary result: LLM-augmented preprocessing (NER) boosts Arabic BERT disease prediction accuracy to 83% (type) and 69% (severity).

## Executive Summary
This study introduces a two-stage pipeline that combines LLM-based preprocessing with Arabic language model fine-tuning for disease prediction in social telehealth. The approach uses text refinement, summarization, and Named Entity Recognition (NER) to enhance Arabic language models (CAMeL-BERT, AraBERT, Asafaya-BERT) for disease type classification and severity assessment. NER-augmented text yielded the highest performance, with CAMeL-BERT achieving 83% accuracy in type classification and 69% in severity assessment. Non-fine-tuned models performed poorly (13%-20% type, 40%-49% severity), demonstrating that integrating LLMs into preprocessing significantly improves diagnostic accuracy and supports effective telehealth applications.

## Method Summary
The framework uses a two-stage pipeline: (1) LLM preprocessing with LLAMA3 for text refinement, summarization, and medical NER, and (2) fine-tuning Arabic BERT models with LoRA. The preprocessing stage aims to clean and structure user-generated Arabic health posts, while the fine-tuning stage adapts the models for classification tasks. LoRA settings include rank=16, alpha scaling=8, and dropout=0.05. The models are trained with batch size=4, epochs=25, and custom loss functions (balanced and accuracy-weighted). Performance is evaluated using accuracy and balanced accuracy for type and severity tasks.

## Key Results
- NER-augmented preprocessing achieved 83% accuracy for disease type classification and 69% for severity assessment, outperforming other preprocessing methods.
- CAMeL-BERT outperformed AraBERT and Asafaya-BERT, likely due to broader pretraining domain coverage.
- Non-fine-tuned models performed poorly (13%-20% for type, 40%-49% for severity), highlighting the necessity of fine-tuning for preprocessing benefits to materialize.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NER-augmented preprocessing improves classification accuracy by extracting task-relevant medical entities before fine-tuning.
- Mechanism: LLAMA3 identifies and isolates medical entities (symptoms, conditions, drugs), reducing noise from conversational text and focusing the downstream classifier on clinically salient tokens. This signal amplification appears to improve the signal-to-noise ratio during gradient updates.
- Core assumption: The LLM's NER capability accurately extracts medically relevant entities from informal Arabic health posts without systematic omission or hallucination.
- Evidence anchors:
  - [abstract] "NER-augmented text achieved the highest performance: 83% accuracy for disease type classification and 69% for severity assessment, outperforming other preprocessing methods."
  - [section 5.3] "The best performing fine-tuned models came with the addition of NER preprocessing. For instance, bert-base-arabic-camelbert reached 83% Type accuracy and 69% Severity accuracy."
- Break condition: If NER extraction introduces systematic errors (e.g., missing negation markers, extracting irrelevant entities), downstream accuracy may degrade or plateau.

### Mechanism 2
- Claim: Fine-tuning is necessary for preprocessing benefits to materialize; preprocessing alone yields negligible improvements.
- Mechanism: Preprocessing creates higher-quality training data, but without domain-adaptive weight updates, the pretrained model cannot leverage the refined signal. LoRA enables efficient parameter updates that align representations with the medical classification objective.
- Core assumption: LoRA fine-tuning with rank 16 and alpha 32 sufficiently captures domain-specific features without overfitting on the relatively small dataset.
- Evidence anchors:
  - [abstract] "Non-fine-tuned models performed poorly (13%-20% for type, 40%-49% for severity)."
  - [section 5.3] "Without fine-tuning, models have seen very limited improvements with NER preprocessing. Type accuracy was in the range of 15%-20%."
- Break condition: If LoRA rank is too low or learning rate is misconfigured, preprocessing benefits may not transfer; if dataset is too small, overfitting may occur despite regularization.

### Mechanism 3
- Claim: Text summarization provides minimal improvement compared to refinement and NER for this classification task.
- Mechanism: Summarization reduces token count but may discard discriminative details (e.g., symptom duration, comorbidities) that inform severity and type classification. NER preserves these details explicitly.
- Core assumption: The summarization model does not systematically retain the most task-relevant information.
- Evidence anchors:
  - [section 5.4] "Summarization gives limited improvements over the baseline... Type accuracy was unchanged at 79% and Severity accuracy increased slightly to 64%."
  - [section 5.5] "NER-enhanced approach yielded the most notable improvements... implicates the use of NER as valuable in extracting important medical information."
- Break condition: If summarization is optimized for medical relevance (vs. general compression), performance gap may narrow.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tuning uses LoRA with rank 16, scaling factor 8, and 5% dropout. Understanding LoRA's decomposition of weight updates into low-rank matrices is essential for debugging and tuning.
  - Quick check question: What happens to gradient flow if LoRA rank is reduced from 16 to 4 on a small medical dataset?

- Concept: **Named Entity Recognition for Medical Text**
  - Why needed here: The framework's best-performing preprocessing relies on extracting symptoms, conditions, and drugs. NER errors propagate to classification.
  - Quick check question: How would you detect if the NER model systematically misses negated symptoms (e.g., "no fever")?

- Concept: **Multi-Label vs. Multi-Class Classification**
  - Why needed here: The task involves both type classification (multi-class) and severity assessment (potentially multi-label or ordinal). Loss function design matters.
  - Quick check question: What loss function adjustment is needed if severity labels are ordinal (mild < moderate < severe) rather than categorical?

## Architecture Onboarding

- Component map:
  Raw Arabic post → NER extraction (LLAMA3) → Augment text with extracted entities → Fine-tune CAMeL-BERT with LoRA → Classify type/severity

- Critical path:
  Raw Arabic post → NER extraction (LLAMA3) → Augment text with extracted entities → Fine-tune CAMeL-BERT with LoRA → Classify type/severity

- Design tradeoffs:
  - **Refinement vs. NER**: Refinement improves readability but may not preserve all medical tokens; NER explicitly preserves entities but loses context.
  - **Model selection**: CAMeL-BERT (mixed corpus) outperformed AraBERT (primarily news) and Asafaya-BERT—likely due to broader pretraining domain coverage.
  - **Preprocessing cost**: LLAMA3 inference adds latency; NER-only pipeline may be more efficient than full refinement+NER.

- Failure signatures:
  - Non-fine-tuned accuracy stuck at 13-20% → preprocessing not utilized without weight updates
  - Severity accuracy consistently lower than type (69% vs. 83%) → subjective labels harder to learn; may need ordinal regression
  - Summarization shows no gain → task-critical details being compressed away

- First 3 experiments:
  1. **Ablate preprocessing components**: Train three models with (a) NER only, (b) refinement only, (c) both; measure delta vs. baseline to confirm NER contribution.
  2. **LoRA rank sensitivity**: Test rank ∈ {4, 8, 16, 32} on a held-out validation set; monitor for overfitting and accuracy degradation.
  3. **Cross-model transfer**: Apply the best preprocessing pipeline (NER) to AraBERT and Asafaya-BERT; verify whether CAMeL-BERT's advantage persists or is a preprocessing artifact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this multi-layered LLM framework be effectively adapted to improve disease classification for non-Arabic languages or multilingual datasets?
- Basis in paper: [explicit] The conclusion states, "Future studies can extend this framework to other languages."
- Why unresolved: The current study exclusively evaluates Arabic language models (CAMeL-BERT, AraBERT, Asafaya-BERT) on Arabic user-generated content.
- What evidence would resolve it: Replicating the methodology on English or multilingual medical datasets and comparing performance against monolingual baselines.

### Open Question 2
- Question: How can the framework be optimized to close the performance gap between disease type classification and subjective severity assessment?
- Basis in paper: [inferred] Results show Type classification (83%) consistently outperforms Severity assessment (69%), yet the paper suggests this gap is inherent to the data's qualitative nature without offering a specific solution.
- Why unresolved: The methodology treats severity as a standard classification label, lacking specific mechanisms to handle the subjectivity the authors identify.
- What evidence would resolve it: Implementing ordinal regression or specialized attention mechanisms for severity tokens to determine if the gap can be narrowed.

### Open Question 3
- Question: Does LLM-based text refinement and summarization inadvertently remove clinically relevant context, limiting their utility compared to NER?
- Basis in paper: [inferred] Tables 3 and 4 show NER provided significant gains (83% accuracy) while summarization offered limited improvements, suggesting the summarization process may discard key diagnostic signals.
- Why unresolved: The study does not analyze what specific medical information was lost or retained during the LLAMA3 summarization and refinement phases.
- What evidence would resolve it: A qualitative error analysis comparing LLM-generated summaries against raw text to identify false negatives caused by text reduction.

## Limitations
- Data Dependence and Generalization: The framework's performance is tightly coupled to the specific Arabic medical dataset, which is not publicly available. Results may not generalize to other languages or domains.
- LLM Preprocessing Reliability: No error analysis of NER output or discussion of how the LLM handles negation, sarcasm, or ambiguous medical language common in social telehealth posts.
- Model Selection and Domain Fit: While CAMeL-BERT outperformed other models, the reasons are inferred rather than empirically tested; impact of LoRA settings on small datasets is not explored.

## Confidence
- **High Confidence**: The necessity of fine-tuning for preprocessing benefits to materialize (Mechanism 2). The poor performance of non-fine-tuned models (13-20% accuracy) is clearly demonstrated and aligns with established fine-tuning literature.
- **Medium Confidence**: The superiority of NER preprocessing over refinement and summarization (Mechanism 1 and 3). While the reported accuracy gains are clear, the underlying reasons (e.g., signal amplification, context preservation) are inferred and not experimentally isolated.
- **Low Confidence**: The claim that summarization provides minimal improvement due to loss of discriminative details. The paper reports a marginal gain (79% to 64%), but does not test whether medical-specific summarization could retain more task-relevant information.

## Next Checks
1. **Ablation Study on Preprocessing Components**: Train three models with (a) NER only, (b) refinement only, (c) both; measure delta vs. baseline to confirm NER contribution and isolate the impact of each preprocessing step.
2. **NER Output Quality Audit**: Manually inspect a sample of NER-extracted entities for accuracy, completeness, and handling of negation. Assess whether systematic errors (e.g., missing negated symptoms) could explain the severity task's lower accuracy (69% vs. 83%).
3. **Cross-Model Preprocessing Transfer**: Apply the best preprocessing pipeline (NER) to AraBERT and Asafaya-BERT; verify whether CAMeL-BERT's advantage persists or is a preprocessing artifact, and whether preprocessing can compensate for weaker base models.