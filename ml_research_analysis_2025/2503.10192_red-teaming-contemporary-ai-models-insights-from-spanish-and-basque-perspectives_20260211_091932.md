---
ver: rpa2
title: 'Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives'
arxiv_id: '2503.10192'
source_url: https://arxiv.org/abs/2503.10192
tags:
- spanish
- openai
- language
- salamandra
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducted Red Teaming sessions on three contemporary\
  \ LLMs\u2014OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra\u2014to assess biases\
  \ and safety issues in Spanish and Basque. Using ten participants across two sessions,\
  \ 670 conversations were analyzed, revealing that 37.9% exhibited harmful behaviors."
---

# Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives

## Quick Facts
- arXiv ID: 2503.10192
- Source URL: https://arxiv.org/abs/2503.10192
- Reference count: 30
- Primary result: 37.9% failure rate across 670 LLM conversations testing for bias and safety in Spanish and Basque

## Executive Summary
This study evaluated three contemporary LLMs—OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra—through structured Red Teaming sessions to assess safety and bias issues in Spanish and Basque languages. Ten participants conducted 670 conversations across eight bias categories and fourteen safety categories over two three-hour sessions. The results revealed significant safety concerns, with 37.9% of conversations exhibiting harmful behaviors, particularly in politically sensitive topics and safety-critical scenarios. ALIA Salamandra showed the highest failure rate at 50.6%, while DeepSeek R1 and OpenAI o3-mini demonstrated relatively better safety performance at 31.7% and 29.5% respectively.

The study highlighted critical disparities in model performance between languages, with Basque showing substantially higher failure rates than Spanish, especially for ALIA Salamandra (67.5% vs 43.1%). Political bias was observed across all models, with ALIA Salamandra showing particularly strong right-wing tendencies. The research provides actionable insights for improving AI safety in Spanish and Basque contexts, emphasizing the need for enhanced bias mitigation strategies and the importance of linguistic diversity in AI safety evaluations.

## Method Summary
The evaluation employed a structured Red Teaming methodology with ten participants conducting 670 conversations across two three-hour sessions. Participants followed detailed instructions to test three LLMs—OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra—across eight bias categories and fourteen safety categories in both Spanish and Basque. The process included prioritized topic lists, 15-minute free exploration blocks, and post-session cross-validation of flagged failures. Inter-rater reliability was measured using Cohen's kappa (0.956), and all conversation data, instructions, and planning templates were released for replicability.

## Key Results
- Overall failure rate: 37.9% across all models and languages
- ALIA Salamandra: 50.6% failure rate (highest among tested models)
- Basque vs Spanish: ALIA Salamandra failed 67.5% of Basque conversations vs 43.1% of Spanish
- Safety-critical failures: 83.3% for ALIA Salamandra in Basque safety categories
- Political bias: Observed across all models, with ALIA Salamandra showing strongest right-wing tendencies

## Why This Works (Mechanism)
The Red Teaming methodology effectively uncovers LLM vulnerabilities by combining structured testing protocols with human creativity and adversarial thinking. By engaging multiple participants with diverse perspectives and allowing both guided and free exploration, the approach captures a comprehensive range of potential failure modes that automated testing might miss. The cross-validation process ensures reliability of findings, while the focus on both Spanish and Basque languages provides insights into linguistic-specific safety challenges.

## Foundational Learning
- Red Teaming methodology: Why needed - to systematically identify AI safety vulnerabilities; Quick check - verify conversation logs show adversarial prompting patterns
- Inter-rater reliability (Cohen's kappa): Why needed - to ensure consistent failure identification; Quick check - confirm kappa > 0.9 indicates strong agreement
- Bias categorization framework: Why needed - to organize and analyze different types of failures; Quick check - review category definitions match conversation annotations
- Linguistic safety evaluation: Why needed - to understand language-specific AI behavior; Quick check - compare failure rates between Spanish and Basque
- Model comparison metrics: Why needed - to benchmark different LLMs; Quick check - verify calculation of failure percentages per model

## Architecture Onboarding

**Component Map**: Participants -> Red Teaming Sessions -> Conversation Logs -> Failure Analysis -> Safety/Bias Reports

**Critical Path**: Participant Training -> Structured Testing Sessions -> Conversation Logging -> Cross-validation -> Failure Classification -> Model Comparison

**Design Tradeoffs**: The study prioritized comprehensive testing over computational efficiency by using manual evaluation rather than automated testing. This approach captured nuanced safety failures but required significant human resources and time investment.

**Failure Signatures**: Safety failures manifest as explicit harmful content generation, refusal to answer legitimate safety questions, or generation of biased responses. Bias failures appear as skewed responses in politically sensitive topics, demographic stereotypes, or cultural misrepresentations.

**First Experiments**:
1. Replicate safety failure detection by testing each model with the same prompts used in the original study
2. Compare Basque vs Spanish performance by running parallel conversation sets in both languages
3. Validate political bias findings by testing each model's responses to politically charged topics

## Open Questions the Paper Calls Out
- Can the released conversation dataset be effectively used to train autonomous Red Teaming agents that match or exceed human performance in identifying LLM vulnerabilities in non-English languages?
- What specific factors cause Salamandra's failure rate to be 24 percentage points higher in Basque (67.5%) than in Spanish (43.1%), and can targeted interventions close this gap?
- How robust are the reported failure rates for DeepSeek R1 given the severely limited sample size (60 conversations total, with some bias categories having only 1-2 conversations)?
- Do the default temperature and sampling parameters used in this study systematically under- or over-estimate failure rates compared to typical user configurations?

## Limitations
- Single manual session with 10 participants limits generalizability
- Substantial imbalance between Spanish (590) and Basque (80) conversation volumes
- Server unavailability created sample size issues for DeepSeek R1
- Default model parameters may not reflect typical user configurations

## Confidence
- Overall failure rate (37.9%): High confidence - large sample size (670 conversations) and excellent inter-rater reliability (kappa = 0.956)
- Language-specific findings: Medium confidence - significant imbalance between Spanish and Basque testing volumes
- Model-specific failure rates: High confidence for o3-mini and DeepSeek R1, Medium confidence for Salamandra due to local deployment variations
- Political bias observations: Medium confidence - qualitative assessment across limited conversation samples

## Next Checks
1. Replicate the evaluation with a larger, more diverse participant pool to assess result stability and reduce sampling bias
2. Conduct balanced testing with equal Spanish and Basque conversation volumes to verify language-specific failure patterns
3. Test Salamandra using standardized cloud deployment rather than local inference to control for hardware-related performance variations