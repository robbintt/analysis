---
ver: rpa2
title: Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration
  Testing
arxiv_id: '2512.09882'
source_url: https://arxiv.org/abs/2512.09882
tags:
- agents
- cybersecurity
- artemis
- vulnerability
- penetration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive comparison of AI agents
  against human cybersecurity professionals in a live enterprise environment. The
  evaluation involved 10 cybersecurity professionals and six AI agents, including
  ARTEMIS, a novel multi-agent framework, on a university network with ~8,000 hosts.
---

# Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing

## Quick Facts
- arXiv ID: 2512.09882
- Source URL: https://arxiv.org/abs/2512.09882
- Authors: Justin W. Lin et al.
- Reference count: 40
- Primary result: ARTEMIS placed second overall, discovering 9 valid vulnerabilities with 82% valid submission rate, outperforming 9 of 10 human participants

## Executive Summary
This study presents the first comprehensive comparison of AI agents against human cybersecurity professionals in a live enterprise environment. The evaluation involved 10 cybersecurity professionals and six AI agents, including ARTEMIS, a novel multi-agent framework, on a university network with ~8,000 hosts. ARTEMIS placed second overall, demonstrating technical sophistication comparable to the strongest participants. While existing scaffolds underperformed, ARTEMIS showed advantages in systematic enumeration and parallel exploitation, with certain variants costing $18/hour versus $60/hour for professionals. However, AI agents exhibited higher false-positive rates and struggled with GUI-based tasks.

## Method Summary
The study deployed ARTEMIS, a multi-agent framework with supervisor-worker orchestration, on a university CS network (~8,000 hosts across 12 subnets). ARTEMIS uses dynamic prompt generation for task-specific sub-agent deployment, a three-phase triage module for validation, and context summarization for long-horizon sessions. Two configurations were tested: A1 (GPT-5 throughout) and A2 (ensemble supervisor + Claude Sonnet 4 sub-agents). The framework operated via 15 tools including spawn_codex, terminate_instance, and submit, with parallel sub-agent execution averaging 2.82 concurrent agents. Results were scored using a weighted combination of technical complexity and criticality across discovered vulnerabilities.

## Key Results
- ARTEMIS placed second overall in the penetration testing competition
- Discovered 9 valid vulnerabilities with 82% valid submission rate
- Outperformed 9 of 10 human participants on technical sophistication metrics
- Parallel sub-agent execution reached 8 concurrent agents, averaging 2.82 per iteration
- Certain ARTEMIS variants cost $18/hour versus $60/hour for professional services

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel sub-agent execution enables systematic enumeration at speeds humans cannot match.
- Mechanism: When the supervisor identifies multiple targets from scan results, it spawns independent sub-agents to probe each target concurrently. ARTEMIS reached 8 active sub-agents in parallel, averaging 2.82 concurrent sub-agents per supervisor iteration.
- Core assumption: Targets are sufficiently independent that parallel investigation does not create resource conflicts or missed dependencies.
- Evidence anchors:
  - [abstract] "AI agents offer advantages in systematic enumeration, parallel exploitation"
  - [section 4.2] "ARTEMIS reached a peak of 8 active sub-agents in parallel, averaging 2.82 concurrent sub-agents per supervisor iteration"
  - [corpus] VulnBot (arXiv:2501.13411) reports similar multi-agent collaboration benefits but does not quantify parallelism gains in production environments
- Break condition: Highly interconnected targets where probing one alters another's state; resource contention on target infrastructure causing service degradation.

### Mechanism 2
- Claim: Dynamic prompt generation reduces tool-selection errors and scope violations by injecting task-specific context.
- Mechanism: An external module generates custom system prompts for each sub-agent based on the delegated task, embedding relevant CLI tool suggestions and behavioral constraints. This separates prompt engineering from supervisor context.
- Core assumption: The prompt generation logic correctly anticipates required tools and constraints for each task type.
- Evidence anchors:
  - [section 3.3] "a custom prompt-generation module creates task-specific system prompts for sub-agents... helps avoid mistakes related to the use of incorrect tools or procedures"
  - [section A] "This step is critical in ensuring that ARTEMIS behaves in-bounds and does not go outside of the scope"
  - [corpus] PenTest++ (arXiv:2502.09484) uses templated prompts but does not isolate prompt generation as an external module; evidence for modular design is paper-specific
- Break condition: Novel vulnerability classes not covered by prompt templates; adversarial environments where tool suggestions are misleading.

### Mechanism 3
- Claim: Three-phase triage reduces false-positive submissions through automated reproduction and severity classification.
- Mechanism: The triager validates relevance (scope, non-duplicate), attempts reproduction, and classifies severity before submission. Failed validations route feedback to the supervisor for course correction.
- Core assumption: Automated reproduction is feasible without human judgment for the vulnerability types encountered.
- Evidence anchors:
  - [abstract] "82% valid submission rate" compared to 100% for top human participants
  - [section 3.3] "This triage process greatly reduces noise, as well as the risk of false positives"
  - [corpus] No direct corpus comparison; related work on agent triage is limited
- Break condition: Vulnerabilities requiring multi-step state changes or time-delayed effects; false positives from misinterpreting HTTP redirects as authentication success (observed in TinyPilot case).

## Foundational Learning

- Concept: MITRE ATT&CK framework (T1046, T1190, T1078, etc.)
  - Why needed here: The paper maps all participant and agent actions to ATT&CK techniques; understanding these identifiers is required to interpret attack patterns and compare human vs. agent behaviors.
  - Quick check question: What technique identifier describes network service enumeration?

- Concept: Multi-agent orchestration patterns (supervisor-worker)
  - Why needed here: ARTEMIS uses a hierarchical architecture with a supervisor spawning arbitrary sub-agents; understanding delegation, termination, and follow-up workflows is prerequisite to modifying the scaffold.
  - Quick check question: Which supervisor tool allows multi-turn conversation with a spawned sub-agent?

- Concept: Context window management for long-horizon tasks
  - Why needed here: ARTEMIS operates over 16-hour sessions by summarizing progress and clearing context; understanding summarization triggers and TODO persistence is critical for extending run duration.
  - Quick check question: What mechanism allows ARTEMIS to resume work after calling "finished"?

## Architecture Onboarding

- Component map:
  - Supervisor agent -> Sub-agents (forked Codex instances) -> Triage module -> Context management -> Session manager

- Critical path:
  1. User task → recursive TODO generation
  2. Supervisor reads TODOs, spawns sub-agents for enumeration
  3. Sub-agents return findings → supervisor submits to triager
  4. Triage validates, reproduces, classifies → routes feedback or final submission
  5. On "finished": summarize context, optionally switch models, continue new session

- Design tradeoffs:
  - Unlimited sub-agents vs. resource contention: More parallelism increases enumeration speed but risks overwhelming target infrastructure
  - Immediate submission vs. foothold deepening: ARTEMIS submits quickly (good for coverage) but may miss higher-severity chainable vulnerabilities (observed with TinyPilot CORS vs. RCE)
  - CLI-only vs. GUI interaction: CLI excels with legacy systems (IDRAC with outdated cipher suites) but fails on GUI-required tasks (TinyPilot console)

- Failure signatures:
  - High false-positive rate: HTTP 200 redirects misinterpreted as successful authentication
  - Early termination: Agent calls "finished" with remaining time; requires session management intervention
  - Scope drift: Without external prompt generation constraints, sub-agents may probe out-of-scope addresses
  - GUI deadlock: Agent stalls on interfaces requiring browser interaction; no fallback to headless alternatives

- First 3 experiments:
  1. Run A1 configuration (GPT-5 supervisor and sub-agents) on a single /24 subnet with 2-hour limit; measure valid submission rate and false positives against manual validation.
  2. Ablate the triage module by bypassing reproduction phase; compare false-positive rate to baseline to quantify triage contribution.
  3. Test dynamic prompt generation by removing tool suggestions from sub-agent prompts; measure increase in incorrect tool invocations or scope violations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would AI agents perform under authentic defensive conditions with active detection and interdiction, compared to the notified-IT scenario in this study?
- Basis in paper: [explicit] The authors state "authentic defensive conditions were absent: the IT team was aware of the test and manually approved flagged actions that would otherwise be interdicted."
- Why unresolved: This study intentionally cooperated with IT staff; no data exists on agent behavior under realistic adversarial defense.
- What evidence would resolve it: Run agents against hardened environments with active blue team monitoring, SIEM alerts, and automated response systems.

### Open Question 2
- Question: How does time compression affect both human and AI penetration testing effectiveness, given that most engagements span 1–2 weeks rather than 10 hours?
- Basis in paper: [explicit] The authors identify "the compressed time frame" as a key limitation, noting the disparity between their 10-hour engagement window and standard 1–2 week penetration tests.
- Why unresolved: The study design constrained all participants to the same short duration; performance scaling over longer periods remains unknown.
- What evidence would resolve it: Longitudinal studies comparing human and agent performance across varying engagement durations (e.g., 10 hours vs. 40 hours vs. 80 hours).

### Open Question 3
- Question: What architectural improvements would help AI agents identify vulnerability patterns rather than just executing technical exploitation steps?
- Basis in paper: [inferred] The elicitation trials showed ARTEMIS found vulnerabilities when given hints, suggesting "bottlenecks lie in identifying vulnerability patterns rather than technical execution."
- Why unresolved: The study demonstrates the gap exists but does not propose or evaluate mechanisms for improving vulnerability pattern recognition.
- What evidence would resolve it: Ablation studies testing different prompting strategies, knowledge bases, or reasoning modules targeting vulnerability pattern identification specifically.

### Open Question 4
- Question: To what extent would GUI-interaction capabilities close the performance gap between AI agents and humans on web-application vulnerabilities?
- Basis in paper: [explicit] The authors note ARTEMIS "struggled with GUI-based tasks" and that "advancements in computer-use agents should mitigate many of these bottlenecks."
- Why unresolved: The study used CLI-only agents; no comparison exists against agents with browser automation or multimodal GUI interaction.
- What evidence would resolve it: Compare ARTEMIS-style agents with and without computer-use capabilities on the same target environment, measuring vulnerability discovery rates for GUI-dependent attack vectors.

## Limitations

- Evaluation environment was a single university network with ~8,000 hosts, limiting generalizability to other enterprise settings
- Human participants included junior professionals, while ARTEMIS was tested with only two configurations
- Triage module's contribution to reducing false positives was not isolated in an ablation study
- Exact dynamic prompt templates and OpenAI Codex fork modifications were not disclosed

## Confidence

- High confidence: ARTEMIS placed second overall in the penetration testing competition, discovering 9 valid vulnerabilities with 82% valid submission rate
- Medium confidence: AI agents offer advantages in systematic enumeration and parallel exploitation, with certain ARTEMIS variants costing $18/hour versus $60/hour for professionals
- Low confidence: The triage module's three-phase process (relevance → reproduction → classification) significantly reduces false-positive submissions

## Next Checks

1. Replicate ARTEMIS in a different enterprise network to assess generalizability of the multi-agent approach and triage effectiveness
2. Ablate the triage module by running ARTEMIS with and without the three-phase triage process on the same target network
3. Compare enumeration speed to human baseline by measuring time taken to enumerate hosts and services versus a human professional on a controlled subset of the target network