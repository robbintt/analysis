---
ver: rpa2
title: Comment Staytime Prediction with LLM-enhanced Comment Understanding
arxiv_id: '2504.01602'
source_url: https://arxiv.org/abs/2504.01602
tags:
- comments
- staytime
- comment
- user
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task of predicting user staytime
  in the comments section of short-video platforms, addressing the limitations of
  existing watchtime prediction methods that overlook fine-grained comment interactions
  and interrelations. To tackle this, the authors propose LCU, a two-stage framework
  that leverages large language models (LLMs) to understand textual information in
  comments and incorporates user-agnostic and user-specific comment ranking tasks
  as auxiliary objectives.
---

# Comment Staytime Prediction with LLM-enhanced Comment Understanding

## Quick Facts
- arXiv ID: 2504.01602
- Source URL: https://arxiv.org/abs/2504.01602
- Reference count: 40
- Primary result: Novel LLM-enhanced framework improves comment staytime prediction and relevance ranking on real-world short-video platform data

## Executive Summary
This paper introduces a novel task of predicting user staytime in the comments section of short-video platforms, addressing the limitations of existing watchtime prediction methods that overlook fine-grained comment interactions and interrelations. To tackle this, the authors propose LCU, a two-stage framework that leverages large language models (LLMs) to understand textual information in comments and incorporates user-agnostic and user-specific comment ranking tasks as auxiliary objectives. LCU is evaluated on a newly released real-world dataset, KuaiComt, which contains rich user interaction data with videos and comments. Experimental results show significant improvements in both staytime prediction and relevance ranking tasks across multiple baseline models. Additionally, online A/B testing on the Kuaishou platform demonstrates practical benefits, with increases in user staytime and comment exposure. The work provides a foundation for enhancing user engagement modeling in recommendation systems.

## Method Summary
The LCU framework operates in two stages: (1) Fine-tune a pre-trained LLM (Qwen2-7b with LoRA) on domain-specific tasks including staytime bucketing, top comment prediction, and user-comment interaction, generating pre-trained embeddings for videos and comments; (2) Integrate these embeddings into a multi-task learning model that predicts staytime while jointly optimizing user-agnostic and user-specific comment ranking tasks. The model is trained on the KuaiComt dataset with a time-based 4:1:1 split, sampling 6 comments per interaction from top popular and user-interacted comments.

## Key Results
- LCU significantly outperforms baseline models on both staytime prediction (RMSE, MAE) and relevance ranking metrics (GAUC, NDCG, MRR)
- The model demonstrates strong cold-start performance, particularly for videos with no historical exposure
- Online A/B testing on Kuaishou platform shows 3.2% increase in user staytime and 0.2% increase in comment exposure
- Ablation studies confirm the importance of both auxiliary ranking tasks in improving overall performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Bridging via Domain-Specific Fine-Tuning
Fine-tuning LLMs on domain-specific prediction tasks creates embeddings that bridge the semantic gap between video content, comment text, and user engagement duration. The authors train Qwen2-7b to predict engagement outcomes (staytime bucketing, top comment, user-comment interaction) directly from text, producing embeddings conditioned to represent content in ways that correlate with user behavior rather than just semantic similarity.

### Mechanism 2: Fine-Grained Preference Regularization via Auxiliary Ranking
The framework employs multi-task learning with auxiliary ranking objectives that act as regularizers, forcing the representation layer to learn robust features of "engagement quality." By jointly optimizing staytime regression with user-agnostic (ListMLE) and user-specific (BCE) ranking losses, the model understands why users stay (e.g., because they found specific comments interesting).

### Mechanism 3: Content-Based Generalization for Cold-Start
LLM-based embeddings enable content-based inference for cold-start videos by relying on semantic content features rather than sparse ID-based collaborative filtering. The system decouples content understanding from the prediction model, allowing it to generate feature vectors for items with zero historical interaction logs and generalize patterns from popular to cold items.

## Foundational Learning

- **Learning to Rank (ListMLE vs. BCE)**: Different loss functions serve different goals—ListMLE optimizes entire comment popularity order, while BCE treats user interactions as binary classification. Quick check: Why choose ListMLE for general popularity ranking instead of point-wise regression on like counts?

- **Supervised Fine-Tuning (SFT) with LoRA**: Adapting massive pre-trained models (Qwen2-7b) to specific niches (video comments) efficiently requires LoRA to avoid retraining all weights. Quick check: What specific "domain-specific tasks" were constructed for the LLM fine-tuning data?

- **Multi-Task Learning (MTL) and Auxiliary Loss**: The model jointly predicts staytime and ranking, requiring careful balancing of L_Staytime, L_R1, and L_R2. Quick check: How does performance degrade if auxiliary ranking losses are removed, according to ablation studies?

## Architecture Onboarding

- **Component map**: LLM Fine-Tuner -> Pre-trained Embedding Tables (E_V, E_C) -> Online Predictor (User/Video IDs + Embeddings -> MHSA -> Three parallel heads)
- **Critical path**: Generation of Pre-trained Embedding Tables in Stage 1. The prediction model relies entirely on these embeddings—without them, Stage 2 cannot function as designed.
- **Design tradeoffs**: Offline embedding generation enables real-time serving but locks semantic understanding to snapshot time; comment sampling focuses on top popular comments, potentially missing niche engaging content.
- **Failure signatures**: Poor performance on videos where best comments aren't most popular; training instability if auxiliary loss weights are poorly scaled.
- **First 3 experiments**: 1) Validate LLM embedding quality on top comment prediction task; 2) Vary comment sampling size (3, 6, 10) to assess context window trade-offs; 3) Simulate cold-start by masking item IDs during training.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does explicit integration of video duration and watchtime features impact comment staytime prediction accuracy? The current framework reserves this for future exploration despite analyzing correlations in Appendix B.

- **Open Question 2**: How can the LLM-enhanced framework be deployed cost-effectively for real-time embedding updates across the full video corpus? Current deployment relies on offline fine-tuning for only 150,000 high-popularity videos due to computational constraints.

- **Open Question 3**: What is the optimal number of comments and sampling strategy for maximizing accuracy in highly engaged videos? The current 6-comment limit may miss signals from long-tail comments in videos with thousands of comments.

## Limitations

- The paper lacks detail on critical components like exact prompt templates for LLM input formatting, which could significantly impact embedding quality
- Ablation studies show auxiliary tasks help but don't explore optimal weighting or alternative auxiliary objectives systematically
- Cold-start evaluation is limited to videos with zero exposure—performance on videos with minimal (1-5) exposures is unexamined
- The proprietary KuaiComt dataset prevents independent verification of results
- Comment sampling strategy focusing on top popular comments may introduce bias and miss niche but engaging content

## Confidence

**High Confidence**: Core claim that LLM-enhanced embeddings improve staytime prediction accuracy compared to baselines is well-supported by experimental results and A/B testing showing 3.2% increase in user staytime.

**Medium Confidence**: Claim that two-stage framework is superior to end-to-end training is reasonable but lacks direct performance comparison with optimized end-to-end alternatives.

**Low Confidence**: Claim that specific auxiliary ranking tasks (user-agnostic vs user-specific) are optimal—ablation shows they help but doesn't explore alternatives or analyze potential gradient conflicts.

## Next Checks

1. **Prompt Template Validation**: Systematically vary prompt templates I_V(v) and I_C(c) with 3-5 different formats to determine impact on embedding quality and downstream prediction accuracy.

2. **Extended Cold-Start Analysis**: Expand evaluation to include videos with 1-5 historical exposures beyond the current "None" category to better understand performance on the realistic long-tail.

3. **Loss Weight Sensitivity Analysis**: Perform comprehensive grid search over λ1 and λ2 values with finer granularity and analyze stability of multi-task training objective, plotting Pareto frontier between staytime accuracy and ranking performance.