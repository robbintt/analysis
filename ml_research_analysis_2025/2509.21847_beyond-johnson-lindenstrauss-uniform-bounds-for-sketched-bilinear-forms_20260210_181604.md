---
ver: rpa2
title: 'Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms'
arxiv_id: '2509.21847'
source_url: https://arxiv.org/abs/2509.21847
tags:
- theorem
- have
- sketched
- where
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for analyzing uniform
  deviation bounds of sketched bilinear forms, extending classical results on random
  quadratic forms to the setting of cross-inner products over pairs of structured
  sets. The authors develop new chaining techniques for controlling suprema over product
  spaces and provide tight control in terms of the geometric complexity of the underlying
  sets.
---

# Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms

## Quick Facts
- arXiv ID: 2509.21847
- Source URL: https://arxiv.org/abs/2509.21847
- Reference count: 40
- Primary result: Unified framework for uniform deviation bounds of sketched bilinear forms, extending classical results on random quadratic forms to cross-inner products over structured sets.

## Executive Summary
This paper develops a unified framework for analyzing uniform deviation bounds of sketched bilinear forms, extending classical results on random quadratic forms to the setting of cross-inner products over pairs of structured sets. The authors introduce new chaining techniques for controlling suprema over product spaces and provide tight control in terms of the geometric complexity of the underlying sets. They develop a uniform bound for the sum of random quadratic forms with i.i.d. random vectors, showing that the deviation scales as √T; notably, such a bound does not exist even for the single-set case considered in prior works. The framework is applied to several machine learning problems, deriving improved bounds including improved finite time convergence guarantees for sketched Federated Learning algorithms and error bounds for linear regression that scale with the geometric complexity of the parameter and input sets rather than the ambient dimension.

## Method Summary
The paper's approach relies on generic chaining and introduces new techniques for handling suprema over pairs of sets. The key technical advance is extending generic chaining to product spaces to bound suprema of stochastic processes. The core technical advance is extending generic chaining to product spaces to bound suprema of stochastic processes. The authors introduce a "double chaining" technique that constructs separate admissible sequences for each set in the product space U × V. By chaining independently over each dimension, the supremum over the product space is bounded by the sum of the individual γ₂ complexities. For the sum of T i.i.d. sketched bilinear forms, the proof extends the generic chaining analysis to handle the supremum over a sum of independent chaos processes, with the √T scaling emerging from the sub-Gaussian tail of the summed process.

## Key Results
- New uniform deviation bounds for sketched bilinear forms scaling with γ₂(U,d) + γ₂(V,d)
- Extension to sums of T i.i.d. sketched bilinear forms with √T deviation scaling
- Improved finite time convergence guarantees for sketched Federated Learning algorithms
- Error bounds for linear regression scaling with geometric complexity rather than ambient dimension
- Sharper regret bounds for sketched linear bandit algorithms depending on geometric complexity of action and parameter sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uniform deviation bounds for sketched bilinear forms can be controlled by the sum of the γ₂ functionals of the two sets involved.
- Mechanism: The paper introduces a "double chaining" technique (Theorem 3.4) that constructs separate admissible sequences for each set in the product space U × V. By chaining independently over each dimension, the supremum over the product space is bounded by the sum of the individual γ₂ complexities, avoiding the potentially larger complexity of a joint set.
- Core assumption: The stochastic process indexed by (u, v) is Lipschitz in each coordinate separately (with high probability).
- Evidence anchors: [abstract], Theorem 3.4 and Remark 3.2, weak direct evidence from corpus.

### Mechanism 2
- Claim: For a sum of T i.i.d. sketched bilinear forms, the deviation bound scales by an extra √T factor.
- Mechanism: The proof extends the generic chaining analysis to handle the supremum over a sum of independent chaos processes (Theorem 5.1). The √T scaling emerges from the sub-Gaussian tail of the summed process.
- Core assumption: The sketching matrices {S_t} and random vectors {ξ_t} are i.i.d. across t.
- Evidence anchors: [abstract], Theorem 5.1 and Section F.2, weak evidence from corpus.

### Mechanism 3
- Claim: The regret of sketched linear bandit algorithms decomposes into a term for regret in the sketched space and a "restricted isometry" term that can be bounded using the new bilinear form analysis.
- Mechanism: The analysis (Theorem 5.2, Appendix G) explicitly decomposes the regret Reg_CB(T) into a term involving the sketched parameter and actions, plus a term involving the projection error θ_*^T (I - S_t^T S_t) a*. The second term is precisely a sum of sketched bilinear forms.
- Core assumption: The action and parameter sets have bounded geometric complexity.
- Evidence anchors: [abstract], Theorem 5.2 and discussion in Section 5.1, weak evidence from corpus.

## Foundational Learning

### Concept: Gaussian Width (ω(T))
- Why needed here: The paper's final uniform bounds are expressed in terms of Gaussian width, which measures the "size" of a set from a Gaussian perspective.
- Quick check question: Can you explain why the Gaussian width of the unit ball in ℝ^d is O(√d), and why it is smaller for structured sets like sparse vectors?

### Concept: Generic Chaining & γ₂ Functional
- Why needed here: The paper's core technical advance is extending generic chaining to product spaces to bound suprema of stochastic processes.
- Quick check question: How does the γ₂ functional differ from covering-number-based measures like Dudley's entropy integral, and why can it lead to sharper bounds on some sets?

### Concept: Restricted Isometry Property (RIP)
- Why needed here: The paper generalizes RIP-type guarantees from preserving norms to preserving inner products across two arbitrary sets.
- Quick check question: State the standard RIP condition (Equation 8) and contrast it with the bilinear preservation condition (Equation 9) that this paper analyzes.

## Architecture Onboarding

### Component map
Problem Formalizer -> Complexity Evaluator -> Chaining Analyzer -> Tail Binder -> Application Interface

### Critical path
Defining the sets M, N → Estimating their geometric complexities → Applying the generic chaining decomposition → Invoking concentration inequalities → Translating the final probabilistic bound into domain-specific metrics (convergence rate, regret).

### Design tradeoffs
1. **Sketching dimension (b) vs. accuracy**: Larger b yields tighter bounds but higher computational/communication cost.
2. **Analytical sharpness vs. generality**: Using Gaussian width (via γ₂) is sharper for structured sets but may be harder to estimate than ambient dimension d.
3. **Single vs. time-varying sketches**: Using one sketch matrix allows reusing Theorem 3.1 but may introduce dependence; using fresh sketches each round requires the more complex Theorem 5.1.

### Failure signatures
1. **Explosion of geometric complexity**: If ω(U) or ω(V) scale as √d, the derived sketching dimension b provides no compression benefit.
2. **Violation of sub-Gaussian assumption**: Heavy-tailed sketching matrices or data vectors break the concentration inequalities underlying the tail bounds.
3. **Incorrect set definition**: For applications, misidentifying the relevant sets leads to vacuous bounds.

### First 3 experiments
1. **Validate bilinear form bound**: Synthesize sets U, V with known Gaussian widths. Generate random sketching matrices S, empirically measure sup_{u∈U, v∈V} |u^T S^T S v - u^T u|, and compare to the bound from Proposition 4.2.
2. **Benchmark bandit algorithms**: Implement sk^-LinUCB and compare its regret to standard LinUCB on synthetic data where the parameter and action sets are structured. Vary the sketching dimension b and verify if the regret scales with √b and the geometric widths.
3. **Test federated learning convergence**: Run the sketch-DL algorithm on a standard federated learning task. Vary b and measure the convergence gap. Check if the error scales with the bound in Theorem 4.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the bilinear sketching framework be extended to adaptive sketches where the sketching matrix depends on previous iterations or observations?
- Basis in paper: [explicit] The conclusion explicitly mentions "extensions to adaptive sketches" as a future direction.
- Why unresolved: The entire theoretical development assumes fixed or independently redrawn sketching matrices, not ones that adapt to data or algorithm state.
- What evidence would resolve it: A theoretical extension of the chaining arguments to handle conditional sketch distributions, or empirical studies showing where adaptive sketches could improve over current guarantees.

### Open Question 2
- Question: Do the uniform deviation bounds for sketched bilinear forms remain valid under weaker assumptions on the random vectors or sketching matrices?
- Basis in paper: [inferred] The main results rely on Assumption 1, requiring independent, mean-zero, sub-Gaussian entries.
- Why unresolved: The proof techniques crucially use sub-Gaussian properties. It is unclear if similar bounds hold for heavy-tailed or dependent sketch constructions.
- What evidence would resolve it: Counterexamples showing failure under heavy tails, or new analyses extending the results to broader classes of random matrices.

### Open Question 3
- Question: Can the sum-of-quadratic-forms result (Theorem 5.1) be tightened with matching lower bounds on the deviation scaling?
- Basis in paper: [explicit] The authors note their bound for the sum case is novel, but do not discuss optimality or lower bounds.
- Why unresolved: While the √T scaling is shown via upper bounds, there is no proof that this rate cannot be improved, nor any comparison to information-theoretic limits.
- What evidence would resolve it: A minimax lower bound demonstrating that √T is unavoidable in the worst case, or refined upper bounds that improve the constants or dependence on geometric complexities.

### Open Question 4
- Question: How do the results transfer to non-linear predictors or non-convex optimization settings where bilinear forms arise naturally?
- Basis in paper: [explicit] The conclusion lists "analysis of non-linear predictors" as an open direction.
- Why unresolved: The generic chaining and bilinear form analysis rely on linear structure. Extending to neural networks or kernel methods would require handling non-linear mappings and their induced complexity.
- What evidence would resolve it: A theoretical framework bounding sketched inner products over function classes or empirical validation of sketched neural training with controlled deviations.

### Open Question 5
- Question: What are the optimal trade-offs between sketching dimension, geometric complexity, and communication/computation cost in distributed learning?
- Basis in paper: [inferred] The federated learning application provides bounds depending on Gaussian width of gradient and Hessian eigenvector sets.
- Why unresolved: The current analysis gives sufficient conditions but does not derive an optimal b that balances the sketching error against statistical estimation error under given communication budgets.
- What evidence would resolve it: A theoretical minimax analysis or an algorithm that adaptively selects b based on online estimates of the set complexities, coupled with regret or convergence bounds that explicitly minimize a cost function.

## Limitations

- The theoretical guarantees depend heavily on the sub-Gaussianity of sketching matrices and random vectors, which may not hold for all practical sketching constructions.
- The geometric complexity measures (γ₂, Gaussian width) can be difficult to compute or estimate for complex, real-world sets, limiting the practical applicability of the bounds.
- The extension to sums of i.i.d. sketched bilinear forms, while novel, only provides high-level intuition for the √T scaling rather than a complete, step-by-step proof.

## Confidence

**High Confidence**: The core chaining argument (Theorem 3.4) and its application to the single sketched bilinear form case (Theorem 3.1) are built on well-established generic chaining techniques.

**Medium Confidence**: The extension to sums of i.i.d. sketched forms (Theorem 5.1) follows a reasonable extension of the chaining logic, but the complete proof is in the appendix and relies on intricate concentration arguments.

**Low Confidence**: The practical implications of the bounds, such as the derived sketching dimensions for federated learning or the expected performance gains in bandit algorithms, depend on accurately estimating Gaussian widths and γ₂ functionals for complex, real-world sets.

## Next Checks

1. **Validate the √T scaling**: Synthesize a simple setting (e.g., U and V are sparse vectors with known sparsity patterns) and empirically measure the deviation of Σ_t u^T S_t^T S_t v for T independent sketching matrices. Compare the empirical scaling with the theoretical √T prediction from Theorem 5.1.

2. **Test sensitivity to sketching matrix distribution**: Implement the bandit algorithms (Theorem 5.2) using sketching matrices drawn from non-Gaussian distributions (e.g., Rademacher, sparse embedding matrices). Measure the regret and compare it to the Gaussian case to assess the robustness of the bounds to the sub-Gaussian assumption.

3. **Benchmark against structured data**: Apply the federated learning algorithm (Theorem 4.1) to a real-world federated dataset (e.g., FEMNIST) with known gradient and Hessian structure. Measure the convergence error and compare it to the bound involving w(G) and w(H) to validate the practical relevance of the geometric complexity measures.