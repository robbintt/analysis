---
ver: rpa2
title: 'DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer'
arxiv_id: '2505.15090'
source_url: https://arxiv.org/abs/2505.15090
tags:
- language
- deft-x
- task
- sparse
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer

## Quick Facts
- **arXiv ID:** 2505.15090
- **Source URL:** https://arxiv.org/abs/2505.15090
- **Reference count:** 40
- **Key outcome:** Achieves state-of-the-art zero-shot cross-lingual transfer on low-resource languages using denoised sparse fine-tuning

## Executive Summary
DeFT-X introduces a novel approach to zero-shot cross-lingual transfer by denoising weight matrices before magnitude pruning and sparse fine-tuning. The method applies SVD-based denoising to remove higher-order (low singular value) components, then uses magnitude-based pruning to identify trainable subnetworks. This improves composability between language-specific and task-specific sparse vectors while reducing destructive interference. Experiments on NusaX (5 Indonesian languages) and AmericasNLI (10 indigenous American languages) show state-of-the-art performance compared to LT-SFT, MAD-X, and TIES baselines.

## Method Summary
DeFT-X first performs full fine-tuning on task/language data to obtain ΔW, then applies SVD to decompose each weight matrix into lower-order (high singular value) components L and higher-order components. Higher-order components are magnitude-pruned (retaining 5%), then reconstructed as W ≈ L + S where S contains denoised higher-order components. Binary masks are generated using magnitude pruning (top-k selection), then sparse fine-tuning is performed with parameters reset to pretrained weights. For cross-lingual transfer, language-specific SFTs are learned via MLM on target language text, task-specific SFTs on source language labeled data, and final models are composed as θTL = θ(0) + φT + φL.

## Key Results
- Achieves 81.4 F1 on NusaX sentiment analysis, outperforming LT-SFT (79.7), MAD-X (79.2), and TIES (77.6)
- Maintains strong performance on AmericasNLI (10 indigenous American languages) despite extreme data scarcity
- Demonstrates robust performance even without source language initialization during task training
- Shows lower parameter overlap between language and task vectors compared to baselines, reducing destructive interference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SVD-based denoising of weight matrices before pruning yields more robust sparse fine-tuned vectors.
- **Mechanism**: The method decomposes each weight matrix ΔW using SVD into lower-order (high singular value) components L and higher-order (low singular value) components. Higher-order components are magnitude-pruned (retaining only 5%) while lower-order components are preserved intact, then reconstructed as W ≈ L + S where S contains denoised higher-order components.
- **Core assumption**: Higher-order components (lower singular values) predominantly capture uninformative or noisy artifacts, while lower-order components encode task-relevant signal.
- **Evidence anchors**:
  - [abstract] "denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs"
  - [section 3.1] "DEFT-X prunes the higher-order (lower singular value) components from the model weights, which are more likely to capture uninformative or noisy artefacts"
  - [section 5, Table 5] Ablation shows removing higher-order components entirely drops Avg. F1 from 81.4 to 80.6 on NusaX, indicating denoised (not removed) higher-order components contribute meaningfully.
  - [corpus] Weak direct corpus support; related work on LASER (Sharma et al., 2023) similarly prunes small singular components to improve reasoning without retraining.

### Mechanism 2
- **Claim**: Sparse fine-tuning with magnitude-based mask selection after denoising improves cross-lingual transfer quality compared to magnitude pruning alone.
- **Mechanism**: After denoising ΔW, construct binary mask μ where μ=1 for top-k entries with highest absolute values. Reset parameters to pretrained weights θ(0), then fine-tune only the masked subnetwork while freezing all other parameters, yielding sparse vector φ = θ(2) - θ(0).
- **Core assumption**: Magnitude of parameter changes after fine-tuning correlates with parameter importance for the task; sparsity prevents destructive interference during vector composition.
- **Evidence anchors**:
  - [abstract] "sparse masks for SFTs were identified using a simple magnitude-based pruning"
  - [section 3.1] "Sparsity is crucial to avoid destructive interference during model composition"
  - [section 5, Table 5] Removing sparse fine-tuning (pruning only) drops performance from 81.4 to 70.8 Avg. F1; removing both magnitude pruning and sparse fine-tuning drops to 79.2, demonstrating pruning+retraining synergy.
  - [corpus] No direct corpus validation for this specific two-phase approach; LT-SFT (Ansell et al., 2022) is the primary prior work establishing this template.

### Mechanism 3
- **Claim**: Denoising reduces parameter overlap between language and task sparse vectors, minimizing destructive interference during composition.
- **Mechanism**: Language-specific SFT φL is learned via MLM on target language text; task-specific SFT φT is learned on source language labeled data. Final model is composed as θTL = θ(0) + φT + φL. Lower overlap between φL and φT ensures distinct subnetworks capture distinct knowledge types.
- **Core assumption**: Language knowledge and task knowledge can be factorized into separate sparse subnetworks; their arithmetic combination preserves both without interference.
- **Evidence anchors**:
  - [section 3.2] "these language and task-specific vectors can be easily composed with the pretrained model to obtain the desired language-task model"
  - [section 5, Figure 2] "DEFT-X results in lower overlap across languages, indicating that denoising higher-order components helps remove redundancies while preserving language and task specific information"
  - [section 5, Figure 3] Language vector overlap is merely ~5%, suggesting each language learns a distinct subnetwork.
  - [corpus] Related work on TIES-merging (Yadav et al., 2023) addresses sign conflicts in model merging; no direct corpus validation of overlap-reduction mechanism for cross-lingual transfer.

## Foundational Learning

- **Concept: Lottery Ticket Hypothesis**
  - **Why needed here**: DeFT-X builds directly on LT-SFT which applies LTH to identify trainable sparse subnetworks. Understanding that subnetworks with original initialization can match full network performance is essential.
  - **Quick check question**: Can you explain why resetting pruned network parameters to their original initialization values (rather than random re-initialization) is critical for the "winning ticket" to maintain performance?

- **Concept: Singular Value Decomposition (SVD)**
  - **Why needed here**: Core to DeFT-X's denoising mechanism; requires understanding how SVD factorizes matrices into singular values/vectors and what lower-order vs. higher-order components represent.
  - **Quick check question**: Given a weight matrix W = UΣV^T, which singular values (largest or smallest) correspond to the principal directions of variance, and which are more likely to contain noise?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - **Why needed here**: The entire problem setting; understanding that this means using only labeled source language data and unlabeled target language text, with no labeled target language examples.
  - **Quick check question**: What two types of knowledge must be combined for zero-shot cross-lingual transfer, and which requires labeled vs. unlabeled data?

## Architecture Onboarding

- **Component map**:
  Pretrained base (XLM-RBASE/XLM-RLARGE) -> Full fine-tuning phase -> Delta computation -> SVD denoising module -> Magnitude pruning -> Sparse fine-tuning -> Vector composition -> Classifier head

- **Critical path**:
  1. Full fine-tune on task/language data → obtain θ(1)
  2. Compute δ = θ(1) - θ(0)
  3. For each weight matrix in δ: SVD → retain lower-order L → prune higher-order → reconstruct
  4. Apply magnitude pruning to denoised δ → generate mask μ
  5. Reset to θ(0), sparse fine-tune with mask μ → obtain φ
  6. For transfer: compose φL (target language) + φT (source task) + θ(0)

- **Design tradeoffs**:
  - **Rank selection**: 90% variance criterion vs. uniform rank (r=100/200/300); higher rank captures more information but may include noise; lower rank denoises more aggressively but may lose signal. Paper found 90% variance and uniform rank perform comparably, but very low-resource languages benefit from higher rank.
  - **Sparsity level (k)**: Language SFTs use 7.6M params (2.8% of XLM-RBASE); task SFTs use 14.2M params (5.2%). Matches MAD-X adapter parameter counts for fair comparison. Lower k increases efficiency but risks under-capacity.
  - **Higher-order retention**: Only 5% of higher-order components retained after magnitude pruning. Lower retention = more denoising but potential signal loss.
  - **Source language initialization**: Applying φs_L during task training helps LT-SFT but DeFT-X shows robustness without it (Table 4).

- **Failure signatures**:
  - **Large model degradation**: XLM-RLARGE underperforms XLM-RBASE on AmericasNLI (Table 3), possibly due to stronger high-resource language biases in pretraining.
  - **No retraining after pruning**: Performance collapses (81.4 → 70.8 F1 on NusaX per Table 5).
  - **Removing higher-order entirely**: Drops performance (81.4 → 80.6), indicating denoised higher-order components contribute signal.
  - **High language-task overlap**: Suggests denoising insufficient; should see ~5% overlap as in Figure 3.

- **First 3 experiments**:
  1. **Rank ablation on single language**: Train language SFT for one target language (e.g., Madurese with 0.84MB corpus) with r={50, 100, 200, 90% var} and measure MLM validation loss; identify optimal rank for your resource level.
  2. **Overlap diagnostic**: After training φL and φT, compute parameter overlap percentage; if >10%, investigate whether denoising threshold (currently 5% higher-order retention) needs adjustment or rank is too high.
  3. **Composition test without source language init**: Compare task SFT trained with vs. without φs_L initialization on a held-out validation set; if DeFT-X shows <0.5 F1 difference (as in Table 4), confirms denoising robustness; if larger gap, examine task complexity relative to language similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the denoising mechanism in DeFT-X improve zero-shot transfer performance when applied to decoder-only or encoder-decoder architectures?
- **Basis in paper**: [explicit] The authors state in the Limitations section: "we evaluate the method only on encoder-only architectures... leaving its effectiveness on decoder-only or encoder-decoder models unexplored."
- **Why unresolved**: The current study exclusively utilizes XLM-R (encoder-only). The interaction between SVD-based denoising and the generative components or attention mechanisms of decoder models remains unknown.
- **What evidence would resolve it**: Benchmark comparisons of DeFT-X against LT-SFT on generative models (e.g., mGPT, LLaMA) or encoder-decoder models (e.g., mT5) using cross-lingual generation tasks.

### Open Question 2
- **Question**: Can DeFT-X be effectively adapted for generative tasks such as machine translation or abstractive summarization?
- **Basis in paper**: [explicit] The authors note in the Limitations section: "our experiments are restricted to classification tasks (SA, NLI), and the applicability of DeFT-X to other tasks remains to be investigated."
- **Why unresolved**: Sparse fine-tuning and composition might behave differently when the output space is text sequences rather than class probabilities, and the pruning of "higher-order components" might remove nuances required for generation.
- **What evidence would resolve it**: Empirical results showing DeFT-X performance on standard generative benchmarks (e.g., FLORES, XSum) compared to dense and sparse fine-tuning baselines.

### Open Question 3
- **Question**: Is there an automated method to determine the optimal SVD rank for denoising that removes the need for task-specific or model-specific manual tuning?
- **Basis in paper**: [explicit] The authors identify in the Limitations section that "the choice of the optimal rank for denoising via SVD is currently model and task-specific and requires manual tuning."
- **Why unresolved**: The paper tests fixed uniform ranks (100, 200, 300) and a 90% variance threshold, but provides no theoretical or automated framework for selecting these hyperparameters a priori.
- **What evidence would resolve it**: A proposed algorithm (e.g., gradient-based or variance-analysis) that dynamically selects ranks per layer and achieves performance comparable to or better than the manually tuned baselines.

### Open Question 4
- **Question**: Does the composable denoising approach offer benefits in multi-modal learning or domain adaptation scenarios?
- **Basis in paper**: [explicit] The Conclusion states: "In future work, we plan to explore the broader applicability of DeFT-X beyond cross-lingual transfer, including its potential in multi-modal learning and domain adaptation."
- **Why unresolved**: While the method reduces interference in cross-lingual composition, it is unclear if the noise reduction properties of SVD denoising transfer effectively to the distinct feature spaces of multi-modal models (e.g., vision-language) or domain shifts.
- **What evidence would resolve it**: Application of DeFT-X to multi-modal models (e.g., LLaVA) or domain adaptation tasks (e.g., biomedical text) demonstrating improved composability over standard SFT.

## Limitations
- **Data scarcity scaling**: Method effectiveness at extremely low resource levels (<100KB) remains untested; rank selection heuristics may break down with poorly estimated matrix spectra.
- **Large model degradation**: XLM-RLARGE underperforms XLM-RBASE on AmericasNLI despite more parameters, contradicting scaling expectations; cause attributed to high-resource language bias without quantitative evidence.
- **Reproducibility gaps**: Key implementation details underspecified including exact per-layer ranks for 90% variance criterion and preprocessing pipelines for cited datasets; source language SFTs for XLM-RLARGE unavailable.

## Confidence

**High confidence**: The core denoising mechanism via SVD pruning of higher-order components is well-supported. Multiple ablation studies (Table 5) show removing denoising degrades performance from 81.4 to 80.6 F1, and the mechanism aligns with established work on low-rank approximation in LLMs (Sharma et al., 2023).

**Medium confidence**: The claim that sparse fine-tuning after denoising improves cross-lingual transfer quality is moderately supported, but the LT-SFT baseline comparison has a critical caveat—LT-SFT uses source language initialization during task training while DeFT-X doesn't, making the ablation less clean than presented.

**Low confidence**: The assertion that DeFT-X results in "lower overlap across languages" reducing destructive interference is primarily supported by a single figure (Figure 2) showing ~5% overlap, without comparison to baseline overlap values or theoretical justification for why denoising specifically reduces this overlap.

## Next Checks

1. **Rank sensitivity analysis**: Systematically vary rank r for both language and task SFTs on NusaX across the full range (r=50, 100, 200, 300, 90% variance) and measure performance degradation/gain. This would validate whether the 90% variance heuristic is optimal or if uniform ranks work better for specific resource levels.

2. **Overlap quantification with baselines**: Compute parameter overlap percentages between language and task vectors for LT-SFT, MAD-X, and TIES baselines on the same tasks. This would validate whether DeFT-X's ~5% overlap is genuinely lower than alternatives or if this is a universal property of sparse fine-tuning.

3. **Large model bias investigation**: Conduct an ablation where XLM-RLARGE is trained with reduced exposure to high-resource languages during pretraining or with explicit debiasing, then evaluate on AmericasNLI. This would test whether the observed performance drop is truly due to bias or other architectural factors in the larger model.