---
ver: rpa2
title: Large language models accurately predict public perceptions of support for
  climate action worldwide
arxiv_id: '2601.20141'
source_url: https://arxiv.org/abs/2601.20141
tags:
- climate
- willingness
- llms
- across
- countries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models can predict public misperceptions about\
  \ climate action with high accuracy (Claude MAE \u2248 5 p.p., r = .77), comparable\
  \ to traditional statistical models, by inferring second-order beliefs from first-order\
  \ willingness data. LLMs capture the core psychological mechanism\u2014social projection\
  \ with systematic underestimation\u2014rather than memorizing country-specific patterns."
---

# Large language models accurately predict public perceptions of support for climate action worldwide

## Quick Facts
- **arXiv ID:** 2601.20141
- **Source URL:** https://arxiv.org/abs/2601.20141
- **Reference count:** 0
- **Primary result:** LLMs predict public misperceptions about climate action with MAE ≈ 5 p.p. and r = .77, comparable to statistical models

## Executive Summary
Large language models (LLMs) can accurately predict public misperceptions about climate action support worldwide, with prediction errors of approximately 5 percentage points—comparable to traditional statistical models. The study demonstrates that LLMs infer perceived support by systematically applying a downward bias to actual willingness data, consistent with human social projection patterns. Performance varies by country development level, with higher accuracy in digitally connected, wealthier nations reflecting training data biases. The approach offers a scalable tool for identifying perception gaps where survey resources are scarce, though accuracy declines in less digitally connected settings.

## Method Summary
The study queried four LLMs (GPT-4o mini, Claude 3.5 Haiku, Gemini 2.5 Flash, Llama 4 Maverick) to predict second-order beliefs about climate action willingness across 125 countries using the Global Climate Change Survey (Gallup World Poll 2021/22, n=129,902). Models received staged prompts with country-level features including demographics, economics, climate data, and first-order willingness, with temperature=0 for deterministic decoding. Predictions were evaluated against ground truth using MAE, RMSE, and Pearson correlation, and benchmarked against OLS and Lasso regression models on 80:20 train-test splits. Ablation and counterfactual tests examined whether models used structured reasoning versus memorization.

## Key Results
- LLMs achieved MAE ≈ 5 p.p. and r = .77 in predicting public perceptions of climate action support
- Accuracy improved substantially when first-order willingness data was provided (MAE dropped from 7-8 p.p. to 4-5 p.p.)
- Performance correlated positively with internet penetration and GDP per capita (R² up to 0.27), declining in less digitally connected countries
- Best models showed feature integration consistent with social projection theory rather than memorization of country-specific patterns

## Why This Works (Mechanism)

### Mechanism 1: First-Order to Second-Order Social Projection
LLMs infer perceived support by applying a systematic downward bias to actual (first-order) willingness, mirroring human social projection. When first-order willingness data is provided, models apply an implicit discount factor (~25-30 p.p.) to estimate second-order beliefs. Feature ablation shows removing "own willingness" increases MAE by 1.5-2.2 p.p. for best models; flipping these values degrades predictions significantly (p < .001 for GPT, Gemini, Llama). The approach captures the core psychological mechanism—systematic underestimation of others' support—rather than memorizing country-specific patterns.

### Mechanism 2: Training Data Representation Bias
Prediction accuracy correlates with national internet penetration and GDP per capita, reflecting differential representation in training corpora. LLMs trained on web-scale text inherit geographic and economic skews in digital content production. Countries with more online content provide richer priors for inference. Performance declined in less digitally connected, lower-GDP countries, reflecting training data biases. Accuracy improves with both higher internet penetration and GDP per capita (all p < .001; R² ranges from 0.08 to 0.27).

### Mechanism 3: Compositional Reasoning Over Feature Integration
Best-performing LLMs combine input features flexibly rather than retrieving country-specific memorized values. Counterfactual tests show swapping country names or GDP values produces negligible MAE changes (<1 p.p.), while modifying substantive inputs (first-order willingness) shifts predictions substantially. Models weight features by relevance, not label association. Controlled tests show that LLMs rely on structured reasoning rather than memorized values.

## Foundational Learning

- **Concept: Pluralistic Ignorance / Perception Gaps**
  - Why needed here: The entire prediction task targets this phenomenon—the gap between actual support (69% globally willing to contribute) and perceived support (believed to be 43%).
  - Quick check question: If a population's actual climate action support is 70% but people believe only 40% of others support it, what is the perception gap magnitude and direction?

- **Concept: First-Order vs. Second-Order Beliefs**
  - Why needed here: LLMs must distinguish "what I believe" (first-order) from "what I think others believe" (second-order). The prompt explicitly instructs models to estimate the latter.
  - Quick check question: In the prompt "estimate what respondents in Argentina on average thought about how many other respondents are willing," which belief order is being requested?

- **Concept: Feature Ablation as Mechanistic Probe**
  - Why needed here: The study uses ablation (removing features one at a time) to determine which inputs drive predictions—essential methodology for validating inference vs. memorization claims.
  - Quick check question: If removing "own willingness" increases error substantially but removing "temperature" does not, what does this imply about the model's reasoning process?

## Architecture Onboarding

- **Component map:**
  - Gallup World Poll microdata -> Country-level feature vectors (8 features) -> LLM API calls (125 countries × 8 stages) -> Prediction outputs -> MAE/RMSE/r evaluation -> OLS/Lasso benchmarks

- **Critical path:**
  1. Construct country-level feature vectors from Gallup microdata + World Bank indicators
  2. Generate predictions across 8 information stages per country per model (125 × 8 = 1,000 calls/model)
  3. Run ablation (remove each feature block) and counterfactuals (flip values, swap labels)
  4. Benchmark against OLS/Lasso on 80:20 train-test splits (10 iterations)
  5. Regress MAE on internet penetration and GDP per capita to assess structural heterogeneity

- **Design tradeoffs:**
  - Deterministic decoding (temperature=0) ensures reproducibility but eliminates variance estimation from model stochasticity
  - Country-level aggregation (vs. individual-level) enables comprehensive geographic coverage but masks subnational variation
  - Post-release timing (models trained after survey publication) introduces memorization risk; counterfactual tests are the mitigation

- **Failure signatures:**
  - High MAE uniform across ablation conditions → model cannot use structured inputs (see Gemini)
  - Large MAE shifts on name-swap → reliance on memorized country associations
  - Negative correlation between predictions and ground truth → inverted reasoning direction
  - Performance uncorrelated with digital connectivity → representation bias mechanism absent

- **First 3 experiments:**
  1. Replicate stage-wise information addition with a new domain (e.g., vaccine willingness) to test generalization of projection mechanism
  2. Add subnational validation using US state-level data to assess whether country-level patterns hold at finer granularity
  3. Test few-shot prompting with 3-5 example countries to determine whether explicit demonstration of projection improves accuracy for low-visibility countries

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs accurately predict perception gaps at subnational levels, such as regions, cities, or specific demographic groups? The authors state that the "country-level analysis does not clarify whether these relationships hold at smaller scales," noting that national estimates may mask variation within larger countries. This remains unresolved because the current validation is restricted to the country level (N=125) due to data constraints, yet intervention strategies often require targeting specific local communities or demographics. Validation against regionally representative survey data would resolve this.

### Open Question 2
Does the use of LLM-based perception gap estimates improve the targeting and effectiveness of climate communication interventions compared to traditional heuristics? While the paper establishes the predictive validity of LLMs, it does not test the policy utility of these predictions in real-world campaigns. Randomized controlled trials (RCTs) where climate interventions are deployed based on LLM-predicted gaps versus control conditions would measure whether these predictions improve intervention targeting, message framing, or timing relative to expert judgment.

### Open Question 3
Can integrating lightweight digital signals (e.g., social media sentiment) or fine-tuning with local data correct the performance deficit in low-GDP, data-scarce countries? The study finds that accuracy declines systematically in less digitally connected, lower-GDP countries due to training data biases, but does not test the proposed remedies. Experiments augmenting LLM prompts with digital trace data (e.g., Google Trends, local social media) for low-GDP countries to measure reductions in Mean Absolute Error (MAE) would address this gap.

### Open Question 4
To what extent is the high performance of specific models attributable to the retrieval of memorized survey results rather than structural inference? While falsification tests (e.g., swapping country names) suggest reasoning, the possibility remains that models recognized the specific survey context from training data. Replicating the analysis using strictly novel survey data collected after the latest model training cutoffs, or using model versions with cutoffs strictly prior to the survey's publication, would resolve this uncertainty.

## Limitations

- Restricted geographic coverage: only 125 countries represented, excluding much of Sub-Saharan Africa and Central Asia where LLM accuracy is likely lowest
- Cross-sectional design prevents causal inference about how perception gaps affect actual climate action over time
- Country-level aggregation masks within-country heterogeneity, potentially obscuring urban-rural or demographic divides in climate perception
- Training data postdates the survey, creating residual memorization risk for frequently discussed countries

## Confidence

- **High confidence** in the core finding that LLMs predict second-order climate beliefs with MAE ≈ 5 p.p. and r = .77, comparable to statistical models, based on robust benchmarking against ground-truth survey data
- **Medium confidence** in the projection mechanism claim—supported by ablation tests but requiring external validation in different domains to confirm generalizability beyond climate attitudes
- **Medium confidence** in the training data representation hypothesis—the correlation with internet penetration and GDP is statistically significant but does not establish causality or rule out confounding factors like education or media freedom

## Next Checks

1. **External domain validation**: Replicate the prediction task for vaccine willingness or pandemic behavior in a different global survey to test whether LLMs consistently apply downward projection from first-order to second-order beliefs across contexts

2. **Subnational granularity test**: Apply the method to US states or EU regions where individual-level data exists to determine if the country-level pattern holds at finer geographic scales and to identify within-country perception heterogeneity

3. **Temporal dynamics assessment**: Track perception gaps over multiple years using repeated cross-sections to establish whether LLM predictions capture not just static snapshots but evolving social belief patterns that correlate with actual behavior change