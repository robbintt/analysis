---
ver: rpa2
title: Latent Chain-of-Thought for Visual Reasoning
arxiv_id: '2510.23925'
source_url: https://arxiv.org/abs/2510.23925
tags:
- reasoning
- reward
- visual
- inference
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a latent chain-of-thought (CoT) training
  method for visual reasoning in large vision-language models (LVLMs). The core idea
  is to treat reasoning as posterior inference over latent rationales using amortized
  variational inference.
---

# Latent Chain-of-Thought for Visual Reasoning

## Quick Facts
- arXiv ID: 2510.23925
- Source URL: https://arxiv.org/abs/2510.23925
- Reference count: 40
- Qwen2.5-VL-7B achieves 68.4% accuracy on MathVista, a 6.6% improvement over the base model and 10.6% over GRPO

## Executive Summary
This paper introduces a latent chain-of-thought (CoT) training method for visual reasoning in large vision-language models (LVLMs). The core idea is to treat reasoning as posterior inference over latent rationales using amortized variational inference. A reference-guided policy exploration method and token-level reward approximation enable efficient training with diverse trajectory coverage. A Bayesian inference-scaling strategy efficiently ranks optimal rationales and answers without external reward models. Empirically, the method improves state-of-the-art LVLMs on seven reasoning benchmarks, with Qwen2.5-VL-7B achieving 68.4% accuracy on MathVista (6.6% over base) and outperforming GRPO by 10.6%. The approach enhances effectiveness, generalization, and interpretability while mitigating reward hacking and reducing reliance on biased reward models.

## Method Summary
The method reformulates visual chain-of-thought reasoning as posterior inference over latent rationales using amortized variational inference. It employs a reference-guided policy exploration method and token-level reward approximation to enable efficient training with diverse trajectory coverage. A Bayesian inference-scaling strategy is used to rank optimal rationales and answers without external reward models. The approach is evaluated on seven reasoning benchmarks, demonstrating state-of-the-art performance improvements.

## Key Results
- Qwen2.5-VL-7B achieves 68.4% accuracy on MathVista, a 6.6% improvement over the base model and 10.6% over GRPO
- The method improves state-of-the-art LVLMs on seven reasoning benchmarks
- The approach enhances effectiveness, generalization, and interpretability while mitigating reward hacking and reducing reliance on biased reward models

## Why This Works (Mechanism)
The method treats reasoning as posterior inference over latent rationales using amortized variational inference. This allows for efficient training with diverse trajectory coverage through reference-guided policy exploration and token-level reward approximation. The Bayesian inference-scaling strategy efficiently ranks optimal rationales and answers without external reward models, leading to improved performance and interpretability.

## Foundational Learning
- **Amortized Variational Inference**: Why needed: Enables efficient training by approximating the posterior distribution of latent rationales. Quick check: Verify that the learned policy q_θ approximates the true posterior well.
- **Reference-Guided Policy Exploration**: Why needed: Guides the exploration of diverse reasoning trajectories. Quick check: Ensure that the reference rationales are diverse and relevant to the task.
- **Token-Level Reward Approximation**: Why needed: Provides fine-grained feedback for training. Quick check: Verify that the reward approximation is accurate and stable.
- **Bayesian Inference-Scaling**: Why needed: Efficiently ranks optimal rationales and answers without external reward models. Quick check: Ensure that the scaling strategy is effective and does not introduce bias.
- **Latent CoT**: Why needed: Enables the model to generate and evaluate multiple reasoning paths. Quick check: Verify that the latent rationales are diverse and lead to correct answers.

## Architecture Onboarding

### Component Map
- **Input**: Visual and textual inputs
- **Base Model**: Qwen2.5-VL-7B (or 3B)
- **SFT**: Fine-tunes the base model on reasoning data to obtain π_Φ
- **RGFN**: Trains the policy q_θ using reference-guided exploration
- **BiN**: Performs inference by sampling rationales and selecting the best one
- **Output**: Reasoning answer

### Critical Path
1. Fine-tune base model on reasoning data to obtain π_Φ
2. Initialize policy q_θ from π_Φ
3. Train q_θ using RGFN with reference-guided exploration
4. Perform inference using BiN to select the best rationale and answer

### Design Tradeoffs
- **Reference Quality vs. Exploration**: High-quality references may limit exploration, while low-quality references may lead to suboptimal solutions.
- **Token-Level Rewards vs. Sequence-Level Rewards**: Token-level rewards provide finer feedback but may be noisier, while sequence-level rewards are more stable but less informative.
- **Model Size vs. Efficiency**: Larger models may achieve better performance but require more resources, while smaller models are more efficient but may have limited capacity.

### Failure Signatures
- **Catastrophic Forgetting**: High-likelihood, low-reward outputs indicate that the model is forgetting previously learned knowledge.
- **Training Instability**: Mode collapse or high variance in rewards indicates that the training process is unstable.
- **Reward Hacking**: The model may learn to exploit the reward function rather than genuinely reasoning about the task.

### First Experiments
1. **Ablation of Reference-Guided Exploration**: Run RGFN without reference filtering to quantify the contribution of this component to the overall improvement.
2. **Teacher Model Sensitivity**: Repeat the RGFN pipeline using different teacher models to isolate the effects of reference quality on performance.
3. **Generalization to Unseen Tasks**: Evaluate the fine-tuned model on a held-out reasoning dataset to test robustness beyond the reported benchmarks.

## Open Questions the Paper Calls Out
- **Scaling to Larger Models**: Does the performance and computational efficiency of the LaCoT framework scale effectively to LVLMs with significantly larger parameter counts (e.g., >70B)?
- **Mitigating Visual Hallucinations**: Can the variational inference framework be adapted to explicitly detect and mitigate visual hallucinations, rather than solely optimizing for reasoning consistency?
- **Knowledge Distillation and Synthetic Data Generation**: Can the latent rationales generated by LaCoT be effectively utilized for knowledge distillation or synthetic data generation to train smaller student models?
- **Exploration Efficiency with Long Sequences**: How does the Reference-Guided policy exploration efficiency degrade as the required reasoning chain length increases significantly (e.g., beyond the ~1k tokens mentioned)?

## Limitations
- The method is only applied to models up to 7B parameters due to resource constraints.
- The approach does not address issues such as hallucination, which are closely related to internal knowledge.
- The computational cost of RGFN fine-tuning is resource-intensive, which may limit practical deployment for some users.

## Confidence
- **High Confidence**: The mathematical framing of reasoning as amortized variational inference over latent rationales is sound and aligns with established VAE/GFlowNet theory. The core BiN inference algorithm (Eq. 9) is clearly specified.
- **Medium Confidence**: Empirical improvements are well-documented on reported benchmarks. However, ablation studies are limited, and the exact ablation of key design choices is not fully explored.
- **Low Confidence**: The precise impact of teacher model selection and reference rationale quality on downstream performance is not quantified. Similarly, the sensitivity to hyperparameters like λ, τ_max, or the number of candidates m is unclear.

## Next Checks
1. **Ablation of Reference-Guided Exploration**: Run RGFN without reference filtering to quantify the contribution of this component to the 10.6% GRPO improvement.
2. **Teacher Model Sensitivity**: Repeat the RGFN pipeline using only GPT-4o rationales, then only DeepSeek-R1, and compare final accuracy on MathVista to isolate teacher model effects.
3. **Generalization to Unseen Tasks**: Evaluate the fine-tuned 7B model on a held-out reasoning dataset to test robustness beyond the seven reported benchmarks.