---
ver: rpa2
title: 'HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated
  Multimodal Classification'
arxiv_id: '2512.10701'
source_url: https://arxiv.org/abs/2512.10701
tags:
- learning
- federated
- multimodal
- data
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces HybridVFL, a framework that combines client-side
  feature disentanglement with server-side cross-modal transformer fusion to address
  performance bottlenecks in vertical federated learning for multimodal medical data.
  HybridVFL achieves a balanced accuracy of 0.9235 on the HAM10000 skin lesion dataset,
  outperforming standard federated baselines by 14.69 percentage points and approaching
  the performance of centralized multimodal models.
---

# HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification

## Quick Facts
- **arXiv ID:** 2512.10701
- **Source URL:** https://arxiv.org/abs/2512.10701
- **Reference count:** 36
- **Primary result:** HybridVFL achieves 0.9235 balanced accuracy on HAM10000, outperforming VFL concatenation baseline by 14.69 percentage points

## Executive Summary
This study introduces HybridVFL, a framework that combines client-side feature disentanglement with server-side cross-modal transformer fusion to address performance bottlenecks in vertical federated learning for multimodal medical data. HybridVFL achieves a balanced accuracy of 0.9235 on the HAM10000 skin lesion dataset, outperforming standard federated baselines by 14.69 percentage points and approaching the performance of centralized multimodal models. The framework effectively integrates image and tabular data while preserving privacy, demonstrating the superiority of semantically aware fusion over simple concatenation in privacy-preserving edge environments.

## Method Summary
HybridVFL operates in a vertical federated learning setting where two clients (image and tabular) each possess different features for the same patient samples. The framework employs dual-output encoders at each client that decompose raw inputs into invariant (shared semantic) and modality-specific embeddings. These embeddings are transmitted to a central server, which aligns invariant features using cosine consistency loss before fusing all four embeddings through a transformer encoder with self-attention. The fused representation is then classified to predict skin lesion types, achieving high accuracy while keeping raw data localized.

## Key Results
- **Balanced Accuracy:** 0.9235 on HAM10000 skin lesion classification
- **Performance Gain:** 14.69 percentage points improvement over VFL concatenation baseline (0.7766)
- **Privacy Preservation:** Raw data remains at client sites; only 1600-dimensional embeddings transmitted

## Why This Works (Mechanism)

### Mechanism 1: Client-Side Feature Disentanglement
- Decomposing raw inputs into invariant and modality-specific embeddings enables richer semantic representation without exposing raw data
- Each client employs a dual-output encoder (CNN for images, MLP for tabular) that separates features into z_inv (shared semantics) and z_spec (modality-unique information)
- Core assumption: Classification benefits from explicitly separating shared versus modality-specific information
- Evidence: Framework description in abstract and Section 3.2; theoretical grounding from Higgins et al. [9] on disentangled representations

### Mechanism 2: Invariant Alignment via Cosine Consistency Loss
- Enforcing consistency between invariant embeddings from different clients improves cross-modal generalization
- Server minimizes L_cons = 1 - cos(z_I_inv, z_T_inv) before fusion, encouraging shared latent space
- Core assumption: Invariant features from both modalities should occupy similar regions in embedding space
- Evidence: Section 3.3 describes alignment mechanism; MOON contrastive learning [13] provides theoretical support

### Mechanism 3: Cross-Modal Transformer Contextual Fusion
- Self-attention over concatenated disentangled embeddings captures inter-modality dependencies better than simple concatenation
- Four embeddings form a token sequence S = [z_I_inv, z_I_spec, z_T_inv, z_T_spec] processed by transformer encoder
- Core assumption: Interactions between modalities are context-dependent and benefit from learned attention weights
- Evidence: Table 1 shows VFL Baseline (0.7766) vs. HybridVFL (0.9235); transformer-based fusion established in multimodal literature

## Foundational Learning

- **Concept: Vertical Federated Learning (VFL) vs. Horizontal FL**
  - Why needed here: HybridVFL operates where clients hold different features for same samples, not different samples with same features
  - Quick check question: Can you explain why embedding exchange replaces gradient sharing in this architecture?

- **Concept: Disentangled Representations (Invariant vs. Specific)**
  - Why needed here: The core innovation relies on separating features that should align across modalities from those that shouldn't
  - Quick check question: If you removed the disentanglement and used single embeddings per client, what component of the loss function would become meaningless?

- **Concept: Transformer Self-Attention for Multimodal Fusion**
  - Why needed here: Understanding why attention outperforms concatenation requires grasping how Q-K-V mechanisms weight token relationships
  - Quick check question: In the attention equation, what would happen if d_k (key dimension) were set too small relative to embedding dimension?

## Architecture Onboarding

- **Component map:** Client C_I (Image) -> CNN encoder E_I -> outputs (z_I_inv, z_I_spec) → Client C_T (Tabular) -> MLP encoder E_T -> outputs (z_T_inv, z_T_spec) → Server S: L_cons alignment → Transformer T_φ fusion → classifier h_ψ → predictions
- **Critical path:** Client encoding → embedding transmission → invariant alignment (L_cons) → transformer fusion → classification loss → gradient backprop to client encoders
- **Design tradeoffs:** Embedding dimension (larger preserves information but increases communication cost); λ_cons coefficient (controls alignment strength); Transformer depth (more layers capture complex interactions but increase server compute)
- **Failure signatures:** L_cons not decreasing (invariant features may lack shared semantics); accuracy stuck near baseline (transformer may be under-capacity); communication bottleneck (embedding dimensions exceed network capacity)
- **First 3 experiments:** 1) Ablate disentanglement: Replace dual-output encoders with single-output, compare accuracy; 2) Vary λ_cons: Run sensitivity analysis (0.0, 0.1, 0.5, 1.0, 2.0) to find optimal alignment strength; 3) Fuse only invariant embeddings: Remove z_spec tokens from transformer input

## Open Questions the Paper Calls Out
- Can HybridVFL maintain its performance advantages when generalized to diverse clinical benchmarks and extended to additional data modalities such as genomics?
- What are the precise computational latency and energy consumption footprints of the client-side disentanglement encoders on resource-constrained edge hardware?
- Can HybridVFL be augmented with differential privacy or secure multi-party computation (MPC) to defend against embedding inversion attacks without significantly degrading classification accuracy?
- How does the cross-modal transformer fusion mechanism scale with respect to communication overhead and attention complexity as the number of vertical clients increases beyond two?

## Limitations
- **Missing implementation details:** Encoder architectures, exact embedding dimensions, and disentanglement mechanism specifications are not provided
- **Hyperparameter uncertainty:** Training parameters (optimizer, learning rate, batch size, λ_cons value) are unspecified
- **Class imbalance concerns:** HAM10000 severe imbalance not addressed in methodology, raising questions about balanced accuracy interpretation

## Confidence
- **High:** Conceptual framework of client-side disentanglement + server-side transformer fusion is technically sound
- **Medium:** 14.69 percentage point improvement over concatenation is plausible but lacks ablation studies isolating disentanglement's contribution
- **Low:** Performance comparisons to centralized baselines assume identical model architectures, which isn't verified

## Next Checks
1. **Ablation study:** Remove disentanglement (single embeddings per client) and compare accuracy to quantify disentanglement's contribution
2. **Communication cost analysis:** Measure actual bandwidth usage under realistic edge network conditions
3. **Robustness testing:** Evaluate performance degradation under varying λ_cons values (0.0 to 2.0) to identify alignment strength break conditions