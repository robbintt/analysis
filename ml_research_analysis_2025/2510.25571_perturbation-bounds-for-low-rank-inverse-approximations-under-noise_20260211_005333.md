---
ver: rpa2
title: Perturbation Bounds for Low-Rank Inverse Approximations under Noise
arxiv_id: '2510.25571'
source_url: https://arxiv.org/abs/2510.25571
tags:
- bound
- matrix
- theorem
- low-rank
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of spectral-norm robustness for
  low-rank pseudoinverses under general additive noise. While classical perturbation
  bounds for full inverses are well-established, they fail to account for truncation
  and spectral structure, leading to overly pessimistic estimates when the noise interacts
  with low-curvature subspaces.
---

# Perturbation Bounds for Low-Rank Inverse Approximations under Noise

## Quick Facts
- **arXiv ID:** 2510.25571
- **Source URL:** https://arxiv.org/abs/2510.25571
- **Reference count:** 40
- **Primary result:** Sharp spectral-norm perturbation bounds for low-rank pseudoinverses under noise, improving over classical estimates by up to a factor of √n in favorable regimes.

## Executive Summary
This paper addresses the problem of spectral-norm robustness for low-rank pseudoinverses under general additive noise. While classical perturbation bounds for full inverses are well-established, they fail to account for truncation and spectral structure, leading to overly pessimistic estimates when the noise interacts with low-curvature subspaces. The authors introduce a novel application of contour integral techniques to the non-entire function f(z) = 1/z, enabling localized resolvent expansions around small eigenvalues. Their main result provides sharp, spectrum-aware bounds on the perturbation error ‖(Â⁻¹)ₚ − A⁻¹ₚ‖ under a mild gap condition 4‖E‖ ≤ min{λₙ, δₙ₋ₚ}. In favorable regimes, this bound improves upon classical Eckart-Young-Mirsky-Neumann estimates by up to a factor of √n. Empirically, the bound closely tracks the true error across real and synthetic matrices, while classical estimates overpredict by one to two orders of magnitude.

## Method Summary
The authors derive perturbation bounds for low-rank approximations of matrix inverses using contour integral techniques. The method involves representing the low-rank pseudoinverse via a contour integral around the p smallest eigenvalues, then bounding the perturbation error through resolvent analysis. The key innovation is applying contour bootstrapping to the non-entire function f(z) = 1/z, which allows localized analysis of the spectral gap. The bound depends on both the noise magnitude and the eigengap δₙ₋ₚ, with a gap condition 4‖E‖ ≤ min{λₙ, δₙ₋ₚ} ensuring validity. Empirical validation uses real-world covariance and stiffness matrices, as well as synthetic quantum harmonic oscillator systems, comparing the proposed bound against classical Neumann-expansion estimates.

## Key Results
- Introduces novel contour bootstrapping technique for f(z) = 1/z, enabling localized resolvent analysis
- Provides sharp bound improving over classical EYM-N estimates by up to √n factor in favorable regimes
- Demonstrates practical applicability with gap condition satisfied for typical noise levels in differential privacy and numerical applications
- Empirical validation shows proposed bound tracks true error within factor of 2-5 across real and synthetic matrices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The perturbation error decomposes into two interpretable components: classical inverse sensitivity (‖E‖/λ²ₙ) and subspace projection sensitivity (‖E‖/(λₙ₋ₚδₙ₋ₚ)).
- **Mechanism**: Contour bootstrapping isolates the first-order perturbation term F₁ from the full contour integral F. Under the gap condition, higher-order terms (F₂, F₃, ...) are bounded by F₁ itself, yielding F ≤ 2F₁ (Lemma 3.1). The tailored rectangular contour Γ with vertices at λₙ/2 and λₙ₋ₚ₊₁ + δₙ₋ₚ/2 makes the integrals tractable.
- **Core assumption**: 4‖E‖ ≤ min{λₙ, δₙ₋ₚ} ensures max_{z∈Γ} ‖(zI−A)⁻¹E‖ ≤ 1/2, which is required for the bootstrapping argument to close.
- **Evidence anchors**:
  - [Section 3, Lemma 3.1]: "If 4‖E‖ ≤ min{λₙ, δₙ₋ₚ}, then F ≤ 2F₁"
  - [Section 2, Theorem 2.1]: Explicit two-term bound
  - [Corpus]: Related work on spectral perturbation (arXiv:2510.25670) confirms eigengap dependence is a recurring pattern in low-rank analysis, but does not replicate the contour bootstrapping technique for f(z)=1/z
- **Break condition**: If δₙ₋ₚ < 4‖E‖, the contour Γ cannot simultaneously enclose the p smallest eigenvalues of both A and Ã while maintaining sufficient distance from the spectrum. Eigenvalue reordering may occur (see Appendix F.1).

### Mechanism 2
- **Claim**: The bound improves over classical EYM-N estimates by up to √n when ‖E‖ ≪ λ²ₙ/λₙ₋ₚ and δₙ₋ₚ is well-separated.
- **Mechanism**: Classical bounds use Neumann expansion + Eckart-Young truncation, yielding O(‖E‖/λ²ₙ + 1/λₙ₋ₚ). The contour method exploits spectral locality: the integral over Γ₃ (right vertical segment) contributes O(‖E‖/(λₙ₋ₚδₙ₋ₚ)) rather than O(1/λₙ₋ₚ). When δₙ₋ₚ ≫ ‖E‖, this term can be much smaller.
- **Core assumption**: Assumption: The eigengap δₙ₋ₚ must be estimated accurately. The paper states robustness to moderate misestimation (within ‖E‖), but this is not formally proven for all regimes.
- **Evidence anchors**:
  - [Section 2]: "In favorable cases, our result yields up to a √n-factor improvement"
  - [Section 7.2, Tables 2-9]: Empirical ratios show EYM-N bound exceeds our bound by 1.5-1334× across test cases
  - [Corpus]: No direct corpus comparison available; neighboring papers focus on different norm settings (Frobenius, Schatten-p)
- **Break condition**: When λ²ₙ/λₙ₋ₚ ≪ ‖E‖, the two bounds coincide in order magnitude (Section 2 comparison).

### Mechanism 3
- **Claim**: The gap condition 4‖E‖ ≤ min{λₙ, δₙ₋ₚ} is practically achievable on real-world matrices with noise levels common in differential privacy and numerical applications.
- **Mechanism**: For sub-Gaussian noise with variance proxy Δ², ‖E‖ ≈ 2Δ√n with high probability. The maximal allowable variance Δ_max = min{λₙ, δₙ₋ₚ}/(8√n). On US Census covariance (n=69) and BCSSTK09 stiffness (n=1083), Δ_max ranges 0.38-4.78×10¹, exceeding typical DP noise scales.
- **Core assumption**: Assumption: The smallest eigenvalue λₙ and gap δₙ₋ₚ are computable or estimable. For black-box matrices, this may require iterative eigensolvers.
- **Evidence anchors**:
  - [Section 7.1, Table 1]: Explicit Δ_max values for p∈[1,17] on Census and p∈[1,8] on BCSSTK09
  - [Section 7.1]: "For differential privacy, ε-DP corresponds to b=1/ε, Theorem 2.1 applies as long as ε>0.03"
  - [Corpus]: Private covariance estimation papers (arXiv:2510.26717, arXiv:2502.07657) operate in similar noise regimes but do not provide spectral-norm guarantees for inverse approximations
- **Break condition**: Matrices with extremely small eigengaps (e.g., BCSSTK09 for p∈{2,7} has δₙ₋ₚ < 10⁻⁹) have undefined or unreliable Δ_max. The bound should not be applied.

## Foundational Learning

- **Concept: Eigendecomposition and spectral gaps**
  - Why needed here: The entire bound depends on eigenvalues λᵢ and gaps δᵢ = λᵢ − λᵢ₊₁. Without understanding how these quantities control subspace stability under perturbation, the mechanism is opaque.
  - Quick check question: Given a 4×4 matrix with eigenvalues [10, 8, 3, 1], what is δ₂ and which subspace does it separate?

- **Concept: Contour integrals for matrix functions**
  - Why needed here: The proof represents A⁻¹ₚ = (1/2πi)∫_Γ z⁻¹(zI−A)⁻¹dz. Understanding why this works (Cauchy's integral formula extended to matrices via functional calculus) is essential to follow Section 3.
  - Quick check question: Why must the contour Γ enclose the p smallest eigenvalues but exclude z=0 for f(z)=1/z?

- **Concept: Classical perturbation theory (Weyl, Neumann)**
  - Why needed here: The paper positions its results against the EYM-N baseline. Understanding ‖Ã⁻¹−A⁻¹‖ ≤ ‖E‖/(λ²ₙ(1−‖E‖/λₙ)) clarifies what the new bound improves upon.
  - Quick check question: Under what condition does the Neumann series for (A+E)⁻¹ converge, and what does this imply about the maximum allowable ‖E‖?

## Architecture Onboarding

- **Component map**: Spectrum estimator -> Noise estimator -> Gap condition checker -> Bound evaluator -> Rank selector

- **Critical path**:
  1. Estimate tail spectrum (λₙ, λₙ₋ₚ, δₙ₋ₚ) — most expensive step, O(k·nnz(A)) for k Lanczos iterations
  2. Characterize noise ‖E‖ — typically O(1) if noise model known, O(n²) if empirical
  3. Check gap condition — O(1) comparison
  4. Evaluate bound — O(1) arithmetic

- **Design tradeoffs**:
  - **Gap strictness vs. applicability**: Smaller p → larger δₙ₋ₚ → easier condition satisfaction, but cruder approximation
  - **Exact vs. estimated spectrum**: Computing exact λₙ, δₙ₋ₚ is costly; estimates within ‖E‖ suffice (per Section 3 discussion) but require careful error propagation
  - **Conservatism vs. tightness**: Constant factors (4, 5) can be tightened with more complex analysis (Theorem 5.2), but at cost of additional spectral parameters (doubling distance r, interaction term x)

- **Failure signatures**:
  - **Bound vastly exceeds empirical error**: Check if δₙ₋ₚ ≫ ‖E‖; bound may be loose in this regime. Consider refined bound (Theorem 5.2).
  - **Assumption check fails**: δₙ₋ₚ < 4‖E‖ indicates potential eigenvalue reordering. Bound is not applicable. Consider reducing p or accepting cruder EYM-N estimate.
  - **Bound underestimates empirical error**: Verify that (Ã⁻¹)ₚ is computed correctly (best rank-p approximation of inverse, not inverse of best rank-p approximation). These differ substantially.

- **First 3 experiments**:
  1. **Assumption validation on target domain**: For your matrix class, compute the distribution of (4‖E‖)/min{λₙ, δₙ₋ₚ} across typical noise levels. Flag what fraction of instances satisfy the condition.
  2. **Bound tightness benchmark**: Compare ‖(Ã⁻¹)ₚ − A⁻¹ₚ‖, our bound, and EYM-N bound across 10-20 noise scales. Plot ratio (our bound)/(empirical error) to assess conservatism.
  3. **Rank selection sweep**: For fixed error tolerance ε, find minimum p such that bound ≤ ε. Compare with p selected by λ⁻¹ₙ₋ₚ < ε (naive threshold). Quantify rank savings.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can sharp perturbation bounds for low-rank inverse approximations be derived for other structured metrics, specifically Schatten-$p$ or Ky Fan norms?
  - **Basis in paper**: [explicit] Section 8 lists "obtaining the sharp perturbation bounds of low-rank inverse approximation for other structured metrics such as Schatten-p norm or the Ky Fan norm" as a future direction.
  - **Why unresolved**: The current analysis relies on spectral-norm properties and contour integrals that do not immediately generalize to other matrix norms without new technical tools.
  - **What evidence would resolve it**: A derivation of error bounds analogous to Theorem 2.1 for Schatten-$p$ norms, validated empirically against the spectral norm results.

- **Open Question 2**: How do these perturbation bounds extend to adaptive or iterative settings where the matrix $A$ evolves over time or is subject to structured noise?
  - **Basis in paper**: [explicit] Section 8 states the results "do not directly extend to adaptive or iterative settings where the matrix evolves over time" and suggests "analyzing structured or time-varying noise."
  - **Why unresolved**: The proof assumes a static matrix with general additive noise; dynamic or structured perturbations would require recursive or time-dependent stability analysis.
  - **What evidence would resolve it**: A theoretical guarantee bounding the error accumulation over a sequence of matrices $(A_t)$ or under noise correlated with the spectrum.

- **Open Question 3**: Can the contour bootstrapping technique be generalized to other non-entire matrix functions, such as matrix roots or resolvents?
  - **Basis in paper**: [explicit] Section 8 proposes "extending contour techniques to other non-entire matrix functions such as resolvents or matrix roots."
  - **Why unresolved**: The method is specifically tailored for $f(z)=1/z$; applying it to functions with different singularities (e.g., $z^{1/2}$) requires new contour geometries and integral estimations.
  - **What evidence would resolve it**: Derivation of low-rank perturbation bounds for $A^{1/2}$ or $\log(A)$ using the contour integral framework introduced in Section 3.

## Limitations
- The gap condition 4‖E‖ ≤ min{λₙ, δₙ₋ₚ} may be violated in matrices with clustered spectra, limiting applicability.
- Numerical instability in matrix inversion (κ > 10¹⁰) introduces floating-point errors that could dominate the perturbation analysis.
- Results are validated on symmetric matrices with additive noise; extension to asymmetric or multiplicative noise is not addressed.

## Confidence
- **High confidence**: The contour integral technique is novel and mathematically rigorous. The proof structure (bootstrapping, resolvent bounds) is sound.
- **Medium confidence**: Empirical validation on real-world matrices supports the theoretical claims, but numerical precision issues may affect the BCSSTK09 results.
- **Low confidence**: The claim that the bound is "tight" (ratio approaching 1) is based on synthetic data with carefully controlled noise scaling. Real-world applicability depends on the gap condition being satisfied, which is not universally true.

## Next Checks
1. **Gap condition prevalence**: Analyze the fraction of matrices in your target domain (e.g., covariance matrices, stiffness matrices) that satisfy 4‖E‖ ≤ min{λₙ, δₙ₋ₚ} for typical noise levels. This quantifies the bound's practical scope.
2. **Numerical precision audit**: For ill-conditioned matrices (κ > 10¹⁰), compare results using high-precision arithmetic (e.g., mpmath) vs. standard floating-point. Quantify the impact of solver tolerance on the empirical error.
3. **Asymmetric extension**: Test the bound on rectangular matrices (e.g., data matrices, Jacobians) by applying it to the symmetric Gram matrix AᵀA. Validate whether the spectral-gap condition remains the controlling factor.