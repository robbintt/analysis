---
ver: rpa2
title: Exploring Design Choices for Autoregressive Deep Learning Climate Models
arxiv_id: '2505.02506'
source_url: https://arxiv.org/abs/2505.02506
tags:
- rmse
- steps
- layers
- config
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the long-term stability of three deep learning\
  \ architectures for autoregressive climate modeling: FourCastNet, SFNO, and ClimaX.\
  \ Models are trained on ERA5 reanalysis data at 5.625\xB0 resolution using two sets\
  \ of prognostic variables, with systematic variation in autoregressive training\
  \ steps, model capacity, and hyperparameters."
---

# Exploring Design Choices for Autoregressive Deep Learning Climate Models

## Quick Facts
- arXiv ID: 2505.02506
- Source URL: https://arxiv.org/abs/2505.02506
- Authors: Florian Gallusser; Simon Hentschel; Anna Krause; Andreas Hotho
- Reference count: 40
- Primary result: SFNO achieves most robust stability across hyperparameter choices for 10-year climate rollouts

## Executive Summary
This paper systematically evaluates three deep learning architectures (FourCastNet, SFNO, ClimaX) for autoregressive climate modeling on ERA5 data at 5.625° resolution. The study varies autoregressive training steps, model capacity, and hyperparameters to assess long-term stability over 10-year rollouts. Results show SFNO provides the most hyperparameter-robust stability, while FourCastNet and ClimaX require careful capacity tuning to avoid polar overfitting. All architectures exhibit seed-dependent instability that cannot be predicted from single-step accuracy. Multi-step training consistently improves stability across architectures.

## Method Summary
The study uses Weatherbench1 ERA5 data at 5.625° resolution with two prognostic variable sets (8 or 33 variables). Models are trained on 1979-2007 data with 6-hourly timesteps, using TISR forcing and constant fields (land-sea mask, orography). A systematic grid search varies autoregressive steps (1, 2, 4), layers (4, 6, 8), and hidden dimensions (128, 256, 512) across 10 random seeds per configuration. The objective is weighted MSE loss summed over M autoregressive steps, with 20 epochs using Adam optimizer and early stopping. Stability is assessed via 10-year rollouts from 2009-2018, measuring area-weighted normalized RMSE against climatology.

## Key Results
- Multi-step autoregressive training (M=4) consistently improves long-term stability and reduces drift across all architectures
- SFNO exhibits greatest hyperparameter robustness, tolerating higher capacity without polar overfitting
- All architectures show seed-dependent instability that cannot be predicted from single-step accuracy
- ClimaX tends to stabilize with larger variable set (33 vars) while SFNO/FCN become unstable with more variables

## Why This Works (Mechanism)

### Mechanism 1: Multi-step autoregressive training
Multi-step training objectives improve long-term stability by exposing models to their own autoregressive errors during optimization. This encourages learning of error-correcting dynamics rather than error-amplifying ones, preventing exponential divergence over time. The approach assumes that composed prediction errors during training induce stable learning dynamics in distribution over long horizons.

### Mechanism 2: Spherical Fourier Neural Operators
SFNO provides superior hyperparameter robustness through implicit spherical geometry constraints via Spherical Harmonics transforms. These constraints prevent the model from learning spurious correlations in high-latitude regions where regular grids distort spatial relationships. The hypothesis assumes that FCN and ClimaX overfit to noisy signals near poles when model capacity increases.

### Mechanism 3: Variable set sensitivity
Stability depends on the specific prognostic variable set due to interactions between architecture mixing strategies and state space complexity. ClimaX (Transformer-based) tends to stabilize with larger variable sets due to its tokenization scheme handling information diversity better, while FCN/SFNO struggle with increased noise or dimensionality relative to their capacity.

## Foundational Learning

- **Autoregressive Error Accumulation**: Errors compound when predicting X_{t+1} from X_t, explaining why medium-range models fail at climate timescales. Quick check: If a model has bias of +0.01 per step, what's total bias after 100 steps?

- **Spherical Geometry & Spectral Transforms**: SFNO uses Spherical Harmonics to respect sphere geometry, handling "pole noise" better than grid-based methods. Quick check: Why does standard 2D Fourier Transform struggle with data on sphere near poles?

- **Prognostic vs. Forcing Variables**: Model predicts prognostic variables (X_t) while forcings (F_t like TISR) act as physical tethers. Quick check: In loss function L_t, which variable types are predicted versus provided as constants?

## Architecture Onboarding

- **Component map**: Inputs (X_t, C, F_t) → Backbone (SFNO/FourCastNet/ClimaX) → Training Objective (weighted MSE over M steps) → Output (X_{t+Δt})

- **Critical path**:
  1. Select variable set: Start with 8 vars for FCN/SFNO; consider 33 for ClimaX
  2. Configure multi-step loss: Set M=4 immediately (M=1 is proven unstable)
  3. Tune capacity: For FCN/ClimaX, constrain D≤256; SFNO can handle higher D with lower LR

- **Design tradeoffs**:
  - SFNO: High stability, requires spherical grid handling and lower learning rates
  - FCN/ClimaX: Can match SFNO but brittle; require capacity limiting to avoid pole-overfitting
  - Variable count: More variables provide physical info but increase optimization difficulty for CNN/FNO

- **Failure signatures**:
  - Pole overfitting: Increasing D in FCN/ClimaX leads to exploding RMSE
  - Seed instability: Even correct configs fail on specific seeds
  - Drift vs. explosion: Models either drift to climatology (acceptable) or values explode to NaN

- **First 3 experiments**:
  1. Train SFNO (L=4, D=256, M=4) on 8-variable set; verify 10-year stability
  2. Train ClimaX/FCN on 33 vars with D=512 vs D=128; observe overfitting
  3. Take best config from Exp 1, run 10 different seeds; quantify RMSE variance

## Open Questions the Paper Calls Out

1. Do stability findings generalize to higher spatial resolutions beyond 5.625°?
2. What mechanisms cause seed-dependent instability despite good single-step accuracy?
3. How does integrating time-varying external forcings and Earth system coupling impact stability?
4. To what extent do optimization algorithms and regularization techniques influence long-term stability?

## Limitations
- Results obtained at coarse 5.625° resolution; generalization to higher resolutions unclear
- Seed-dependent instability suggests fundamental sensitivity not captured by 10-seed experiments
- Stability of FCN/ClimaX highly sensitive to hyperparameter choices with unclear theoretical guidance
- Focus on free-running atmosphere without ocean coupling or time-varying climate forcings

## Confidence

- **High Confidence**: Multi-step autoregressive training consistently improves long-term stability across architectures
- **Medium Confidence**: SFNO's hyperparameter robustness demonstrated, but geometric mechanism remains theoretical
- **Low Confidence**: Architecture-specific variable set sensitivity claims based on limited experimental combinations

## Next Checks

1. **Resolution Scaling Study**: Replicate stability experiments at 1.4° and 2.8° to test SFNO's geometric advantages at higher resolutions

2. **Architecture Ablation with Variable Subsets**: Systematically vary prognostic variable sets by removing subsets rather than using full 8 or 33 variable sets

3. **Stability Basins Characterization**: For top-performing configs, perform systematic hyperparameter sweeps around stability threshold to map hyperparameter space "stability basins"