---
ver: rpa2
title: 'rQdia: Regularizing Q-Value Distributions With Image Augmentation'
arxiv_id: '2506.21367'
source_url: https://arxiv.org/abs/2506.21367
tags:
- learning
- rqdia
- state
- arxiv
- q-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: rQdia is a simple regularization method that improves deep reinforcement
  learning from pixels by regularizing Q-value distributions across augmented images.
  The key idea is to add an auxiliary loss that minimizes the mean squared error between
  Q-value distributions for original and augmented images, thereby encouraging visual
  invariance.
---

# rQdia: Regularizing Q-Value Distributions With Image Augmentation

## Quick Facts
- arXiv ID: 2506.21367
- Source URL: https://arxiv.org/abs/2506.21367
- Reference count: 20
- Key outcome: Improves pixel-based RL by regularizing Q-value distributions across augmented images, achieving SOTA on MuJoCo and Atari

## Executive Summary
rQdia is a simple regularization method that improves deep reinforcement learning from pixels by regularizing Q-value distributions across augmented images. The key idea is to add an auxiliary loss that minimizes the mean squared error between Q-value distributions for original and augmented images, thereby encouraging visual invariance. When applied to DrQ and SAC on MuJoCo continuous control tasks, rQdia improves performance on 9/12 and 10/12 tasks respectively, both in sample efficiency and final rewards. Remarkably, with rQdia, pixel-based continuous control finally surpasses the state encoding baseline. On the Atari benchmark, rQdia improves Data-Efficient Rainbow on 18/26 environments compared to 10/26 for CURL. The method is orthogonal to existing techniques and can be easily integrated into current frameworks, providing consistent gains across diverse RL settings.

## Method Summary
rQdia adds an auxiliary loss to regularize Q-value distributions across augmented images. For each state in a mini-batch, the method computes Q-values for all actions in that mini-batch, both for the original and augmented versions of the state. An MSE loss is then applied between these two Q-value distributions. The augmentation used is a 4-pixel pad followed by random crop. This loss is added to the standard RL loss during training. The method works with both continuous control (DrQ, SAC) and discrete control (Rainbow) by adjusting how the Q-value distributions are computed and compared.

## Key Results
- Improves DrQ on 9/12 MuJoCo tasks and SAC on 10/12 tasks
- rQdia is the first method to surpass state encoding baselines in pixel-based continuous control
- Improves Data-Efficient Rainbow on 18/26 Atari environments (vs 10/26 for CURL)
- Provides consistent gains across diverse RL settings and is orthogonal to existing techniques

## Why This Works (Mechanism)

### Mechanism 1: Q-Value Distribution Invariance via Cross-Augmentation Consistency
Enforcing consistency between Q-value distributions across augmented views of the same state induces useful visual invariances in the encoder. An auxiliary MSE loss minimizes the squared difference between Q(s, a) and Q(aug(s), a) for all actions in a mini-batch, pushing the encoder toward representations where such perturbations are irrelevant.

### Mechanism 2: Distribution-Level Regularization Captures Action-Relational Structure
Regularizing the full Q-value distribution provides a richer invariance signal than regularizing only the Q-value of the action taken. Unlike DrQ, which averages Q-predictions for the selected action across augmentations, rQdia enforces consistency across all actions sampled in the mini-batch, preserving the relative preferability of actions.

### Mechanism 3: Historical Action Sampling as Feasible State-Action Pairs
Using actions sampled from the replay buffer provides a practical way to approximate Q-value distributions without requiring explicit action sampling strategies. For each state s in the mini-batch, rQdia computes Q(s, a_i) for all actions a_i from the same mini-batch, using "historical" actions taken in potentially different states.

## Foundational Learning

- **Q-Functions and the Bellman Equation**: Understanding that Q-values estimate future value and are updated via temporal difference learning is essential to grasp what is being regularized.
  - Quick check question: If Q(s, a) = 10 and γ = 0.99, what does this represent about future rewards?

- **Data Augmentation in RL**: Understanding why augmentations should not change the optimal action is crucial.
  - Quick check question: Why might a random horizontal flip be a poor augmentation for a side-scrolling game where moving right is always rewarded?

- **Distributional RL (C51 / Rainbow)**: For Atari experiments, rQdia is applied to Rainbow, which outputs a distribution over returns rather than a scalar Q-value.
  - Quick check question: In distributional RL, instead of predicting E[Σγᵗr_t], what does the network output?

## Architecture Onboarding

- **Component map**: Pixel input -> Encoder (CNN) -> Latent representation -> Q-Network(s) -> Q-values -> rQdia Loss Module -> Total loss -> Backpropagation
- **Critical path**: 1) Sample mini-batch of transitions from replay buffer, 2) Apply augmentation to each state, 3) Compute Q-values for all (s, a_i) and (aug(s), a_i) pairs, 4) Compute L_rQdia = MSE between distributions, 5) Add to standard RL loss, 6) Backpropagate and update jointly
- **Design tradeoffs**: MSE on logits vs. KL-divergence for Rainbow (MSE works better empirically), batch size affects action diversity vs. memory efficiency, conservative augmentation choice may limit potential gains
- **Failure signatures**: Performance drop on finger_spin with DrQ (over-regularization), no gain when combined with CURL (conflicting invariances), KL-divergence underperforms MSE for Rainbow
- **First 3 experiments**:
  1. Minimal reproduction on cheetah_run with DrQ: Implement rQdia and compare final reward and sample efficiency to baseline
  2. Ablation of action sampling: Compare using full mini-batch actions vs. random subset vs. only selected action, measuring performance and stability
  3. Augmentation sensitivity test: Test with different augmentations on Frostbite (helps) vs. Pong (may not help), analyzing which augmentations are effective

## Open Questions the Paper Calls Out

- Does directly regularizing action distributions (via log probabilities) improve performance in continuous control settings compared to regularizing Q-value distributions?
- Can sampling actions based on state similarity rather than random mini-batch availability improve the efficacy of rQdia?
- Why does rQdia synergize with DrQ but fail to provide gains when combined with contrastive learning methods like CURL?

## Limitations
- No direct empirical evidence for why MSE on logits outperforms KL-divergence for distributional Q-values
- Claim of orthogonality to CURL is based only on ablation results, not theoretical or mechanism-level analysis
- Mini-batch action sampling as proxy for Q-value distributions is practical but not rigorously justified

## Confidence
- High: rQdia improves sample efficiency and final performance on MuJoCo and Atari tasks when applied to DrQ and SAC
- Medium: The claim that rQdia surpasses state encoding baselines in pixel-based continuous control is novel and well-supported
- Low: The assertion that the method is universally orthogonal to other techniques (like CURL) is not robustly validated

## Next Checks
1. **Mechanism validation**: Add an ablation measuring the effect of using random actions instead of mini-batch actions on the auxiliary loss, comparing Q-value distribution consistency and final performance
2. **Distributional RL ablation**: For Rainbow, directly compare MSE on logits vs. KL-divergence on full return distributions across multiple Atari games, reporting both stability and performance
3. **Augmentation sensitivity**: Systematically vary augmentation strength (translation range, addition of color jitter) and measure impact on tasks where rQdia helps (cheetah_run) vs. hurts (finger_spin)