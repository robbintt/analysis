---
ver: rpa2
title: Leveraging large language models for structured information extraction from
  pathology reports
arxiv_id: '2502.12183'
source_url: https://arxiv.org/abs/2502.12183
tags:
- data
- reports
- information
- extraction
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the accuracy of large language models (LLMs)\
  \ for extracting structured pathology data from breast cancer histopathology reports,\
  \ comparing their performance to a human annotator. Using a gold standard dataset\
  \ derived from 111 reports, the research tests five LLMs\u2014including GPT-4o and\
  \ Llama 3.1 variants\u2014on 51 pathology features specified in the study's data\
  \ dictionary."
---

# Leveraging large language models for structured information extraction from pathology reports

## Quick Facts
- **arXiv ID:** 2502.12183
- **Source URL:** https://arxiv.org/abs/2502.12183
- **Reference count:** 2
- **Primary result:** GPT-4o and Llama 3.1 405B achieve accuracy comparable to human annotators (96.1% and 94.7% vs 95.4%) on structured pathology data extraction

## Executive Summary
This study evaluates large language models for extracting structured pathology data from breast cancer histopathology reports. Using a gold standard dataset of 111 reports and testing 51 pathology features, the research demonstrates that GPT-4o and Llama 3.1 405B achieve extraction accuracy comparable to human annotators through zero-shot prompting without requiring labeled training data. The study also introduces an open-source web application enabling non-programmers to customize structured information extraction using natural language instructions.

## Method Summary
The researchers created a gold standard dataset by manually extracting 51 pathology features from 111 breast cancer histopathology reports. Five LLMs (GPT-4o, Llama 3.1 405B, Llama 3.1 70B, Llama 3.1 8B, and Gemma 2 27B) were evaluated on 50 test reports using zero-shot prompting with JSON Schema-formatted metadata. The study compared model performance against a human annotator and examined performance across different model sizes. An open-source web application was developed to enable customization of extraction tasks through natural language instructions.

## Key Results
- GPT-4o achieved 96.1% accuracy, matching human annotator performance (95.4%)
- Llama 3.1 405B achieved 94.7% accuracy, also matching human performance
- Llama 3.1 70B achieved 91.6% accuracy, slightly below human but viable for self-hosting
- Llama 3.1 8B achieved only 75% accuracy, showing notable inconsistency
- JSON Schema formatting significantly improved metadata processing compared to CSV

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-accuracy structured extraction is achievable via zero-shot prompting without task-specific weight updates.
- **Mechanism:** Instruction-tuned LLMs map natural language definitions and unstructured input text to a structured output schema using pre-existing semantic knowledge of medical terminology rather than learned task-specific weights.
- **Core assumption:** Pathology reports contain linguistic patterns and terminology within the model's pre-training corpus distribution.
- **Evidence anchors:** Abstract states LLMs achieve efficient extraction through zero-shot prompting without labeled data; section 4.3 confirms performance matching expert annotators demonstrates strong generalization capability.
- **Break condition:** Reports use highly idiosyncratic shorthand, non-standard acronyms, or formatting that falls outside the model's semantic resolution.

### Mechanism 2
- **Claim:** JSON Schema formatting significantly improves complex metadata processing compared to CSV formats.
- **Mechanism:** JSON Schema's hierarchical structure disentangles technical metadata from descriptive metadata, reducing attention diffusion and preventing feature definition confusion when extracting large numbers of fields.
- **Core assumption:** LLMs possess sufficient capacity to parse and maintain schema constraint adherence throughout generation.
- **Evidence anchors:** Section 2.1.2 states JSON Schema significantly improved LLM ability to process metadata information simultaneously for large feature sets.
- **Break condition:** Schema complexity exceeds the model's context window or attention resolution, causing truncated or hallucinated fields.

### Mechanism 3
- **Claim:** Performance scales with model parameter count, but viable self-hosted alternatives exist at lower tiers.
- **Mechanism:** Larger models exhibit emergent reasoning capabilities that resolve ambiguities better than smaller models, which struggle with consistency.
- **Core assumption:** Inference hardware can support the specific parameter count required for desired accuracy.
- **Evidence anchors:** Section 3 shows larger models demonstrated increased performance; Llama 405B matched human performance while Llama 8B was notably inconsistent.
- **Break condition:** Computational cost of larger models outweighs marginal accuracy gains, or latency requirements force smallest model that fails accuracy thresholds.

## Foundational Learning

- **Concept: Zero-shot Prompting vs. Fine-tuning**
  - **Why needed here:** This is the core operational definition of the study's approach.
  - **Quick check question:** If you feed the model 5 example reports with answers before asking it to extract the 6th, is this still zero-shot? (Answer: No, that is "few-shot.")

- **Concept: JSON Schema**
  - **Why needed here:** The paper identifies schema format as a critical performance lever.
  - **Quick check question:** Can a JSON Schema enforce that a "Tumor Size" value must be a number and not a string? (Answer: Yes, via the "type": "number" field.)

- **Concept: OCR Pipeline & De-identification**
  - **Why needed here:** The study notes LLMs require machine-readable text, necessitating pre-processing.
  - **Quick check question:** Why might a human annotator outperform an LLM on a scanned document with "illegible text"? (Answer: Humans can use visual context; LLMs rely on OCR transcript which may be garbage.)

## Architecture Onboarding

- **Component map:** Plaintext pathology report -> JSON Schema configuration -> API-compatible endpoint (OpenAI or self-hosted) -> Structured JSON + JSON-LD output

- **Critical path:** Data Dictionary Definition -> JSON Schema Conversion is the most sensitive step. Poor schema formatting causes model confusion.

- **Design tradeoffs:**
  - GPT-4o (Proprietary) vs. Llama 405B (Self-hosted): Ease of Use/State-of-the-Art accuracy vs. Data Privacy/Governance
  - Llama 70B vs. Llama 8B: High Cost/Hardware Requirements vs. Portability but with significant accuracy loss (75%)

- **Failure signatures:**
  - "Conflicting Sections" Ambiguity: Model extracts from Macroscopic section conflicting with Summary section
  - OCR Drift: Model extracts "Smm" instead of "5mm" due to OCR failure
  - Over-constraining: Model assigns "absent" to features not mentioned, confusing "no finding" with "negative finding"

- **First 3 experiments:**
  1. Baseline Validation: Run open-source tool on 10 sample reports using GPT-4o to verify ~96% accuracy
  2. Schema Stress Test: Compare CSV vs JSON Schema versions on same report to observe accuracy drop
  3. Model Tier Comparison: Run extraction batch against Llama 3.1 70B vs GPT-4o to quantify privacy/cost vs accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can large multimodal models (LMMs) accurately extract structured data directly from scanned pathology report images, eliminating the need for separate OCR and layout reassembly steps?
- **Basis in paper:** [Explicit] The paper states exploring LMMs presents an opportunity to streamline structured information extraction by potentially eliminating separate OCR and layout alignment steps.
- **Why unresolved:** Current study relied on DocTR and Tesseract for OCR before LLM processing. Authors note LMMs still fall short of text recognition models' performance.
- **What evidence would resolve it:** Comparative evaluation of LMMs versus current OCR+LLM pipeline on same dataset measuring extraction accuracy and processing time.

### Open Question 2
- **Question:** Can LLMs accurately assign extracted features to correct specimen in pathology reports containing multiple tumor specimens?
- **Basis in paper:** [Explicit] Authors identify evaluating LLMs' ability to handle reports with multiple specimen descriptions as important future study, noting current research focused solely on single-specimen reports.
- **Why unresolved:** Multiple specimens introduce complex ambiguity regarding which specific specimen features belong to, explicitly excluded from current evaluation.
- **What evidence would resolve it:** Performance metrics from LLMs tested on multi-specimen report dataset with ground truth linking features to specific specimens.

### Open Question 3
- **Question:** Can the prompt engineering process be automated by LLMs seeded only with a data dictionary to achieve accuracy comparable to human-refined prompts?
- **Basis in paper:** [Explicit] Paper suggests future studies could explore automating this step through LLMs seeded only with the data dictionary, referring to manual iterative prompt development.
- **Why unresolved:** Current study relied on manual human effort to refine JSON Schema and instructions, which is resource-intensive and potentially subjective.
- **What evidence would resolve it:** Study comparing extraction accuracy of autonomously generated LLM prompts against human-engineered prompts used in original study.

## Limitations

- **Generalization Boundaries:** Study validates performance on breast cancer pathology reports from single institution; cross-institutional performance remains unverified.
- **Semantic Resolution Limits:** Approach relies on model's pre-training corpus containing relevant terminology; reports with highly idiosyncratic shorthand may degrade performance.
- **Schema Complexity Constraints:** Study doesn't test schema complexity limits; very large feature sets or deeply nested schemas may exceed model's attention capacity.

## Confidence

- **High Confidence:** Core finding that GPT-4o and Llama 3.1 405B achieve accuracy comparable to human annotators (96.1% and 94.7% vs 95.4%) is well-supported by gold standard evaluation.
- **Medium Confidence:** Claim that performance scales predictably with parameter count is supported by data but only tests three model sizes.
- **Medium Confidence:** Assertion that zero-shot prompting works effectively is supported, but study doesn't compare against few-shot or fine-tuned alternatives.

## Next Checks

1. **Cross-Institutional Validation:** Evaluate same extraction pipeline on pathology reports from 2-3 different institutions with varying reporting styles to quantify generalization limits.

2. **OCR Error Impact Assessment:** Systematically inject controlled OCR errors into clean reports and measure corresponding degradation in extraction accuracy to establish error propagation bounds.

3. **Schema Complexity Scaling Test:** Create progressively larger data dictionaries (25, 50, 75, 100 features) and measure extraction accuracy to identify schema complexity threshold where performance degrades significantly.