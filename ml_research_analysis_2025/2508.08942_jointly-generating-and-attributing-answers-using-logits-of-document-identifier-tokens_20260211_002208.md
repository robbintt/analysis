---
ver: rpa2
title: Jointly Generating and Attributing Answers using Logits of Document-Identifier
  Tokens
arxiv_id: '2508.08942'
source_url: https://arxiv.org/abs/2508.08942
tags:
- answer
- attribution
- documents
- generation
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoDIT, a method for jointly generating and
  faithfully attributing answers in retrieval-augmented generation (RAG) by leveraging
  the logits of document-identifier tokens. LoDIT addresses the challenge of hallucination
  in large language models by explicitly linking generated answers to their supporting
  documents using internal model signals.
---

# Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens

## Quick Facts
- arXiv ID: 2508.08942
- Source URL: https://arxiv.org/abs/2508.08942
- Reference count: 40
- One-line primary result: LoDIT improves RAG faithfulness by up to 14% in TRUST score over state-of-the-art models.

## Executive Summary
This paper introduces LoDIT, a method for jointly generating and faithfully attributing answers in retrieval-augmented generation (RAG) by leveraging the logits of document-identifier tokens. LoDIT addresses the challenge of hallucination in large language models by explicitly linking generated answers to their supporting documents using internal model signals. The approach involves marking retrieved documents with specific token identifiers, then using the logits of these tokens during generation to estimate each document's contribution to the answer. These contributions are then aggregated to produce faithful document attributions. Experiments on the Trust-Align benchmark demonstrate that LoDIT significantly outperforms state-of-the-art models in terms of trustworthiness, achieving up to 14% improvement in the TRUST score compared to existing methods. The approach also shows robustness to variations in context length and document order, while maintaining low latency.

## Method Summary
LoDIT jointly fine-tunes a decoder LLM (e.g., Llama 3.1) on answer generation and document attribution by marking retrieved documents with unique identifier tokens (e.g., `<AA>`, `<BB>`). During training, it minimizes a combined loss: standard cross-entropy for answer tokens and MSE between predicted identifier logits and empirically set target values (4 for gold, 2 for context, 0 for random, -2 for out-of-context). At inference, it monitors identifier token logits for each generated token and aggregates these signals using a threshold-based pooling function to attribute each statement in the answer to its supporting documents.

## Key Results
- LoDIT achieves up to 14% improvement in TRUST score over state-of-the-art RAG models on the Trust-Align benchmark.
- The method maintains low latency compared to multi-pass ablation approaches like MIRAGE.
- LoDIT shows robustness to context length variations and document ordering when properly fine-tuned.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prefixing documents with special identifier tokens (e.g., `<AA>`) creates a causal proxy where the logit of the identifier token reflects the model's dependence on that document for generating the current answer token.
- **Mechanism:** During the forward pass, the attention mechanism distributes focus across the context. If document $d_k$ (marked with `<AA>`) provides the evidence for the next token $s$, the model's internal state increases the likelihood of the identifier token `<AA>` appearing in the output distribution. By monitoring the logits of these identifier tokens—rather than the generated answer tokens—the model signals which source is driving the generation.
- **Core assumption:** The model learns to associate the identifier token with the semantic content of the document it wraps, effectively treating the ID as a "summary feature" of that document.
- **Evidence anchors:**
  - [Abstract] "It consists of two steps: (1) marking the documents with specific token identifiers and then leveraging the logits of these tokens to estimate the contribution..."
  - [Section 4.1] "...we postulate that the logits of label tokens could bridge between source documents in the input context and their representative labels in the LLM output."
  - [Corpus] Limited direct corpus evidence for this specific mechanism; relying on paper text.
- **Break condition:** If the context length exceeds the model's effective attention window, or if the identifier tokens are too generic (low perplexity) and appear frequently in pre-training data, the signal-to-noise ratio in the logits drops, decoupling the ID logit from the document content.

### Mechanism 2
- **Claim:** Joint fine-tuning with an MSE loss on identifier logits acts as a debiasing mechanism, forcing the model to break spurious correlations (like document order) and explicitly encode attribution.
- **Mechanism:** Raw LLM logits suffer from selection bias (favoring earlier documents) and token bias (favoring common tokens). By calculating $L_{att}$ (MSE) between the predicted identifier logits and target labels (e.g., 4 for correct doc, 2 for context but incorrect, -2 for out of context), the training process shapes the logit distribution. This forces the model to "think" about the attribution explicitly during the generation of every answer token.
- **Core assumption:** The gradient updates from the attribution loss ($L_{att}$) do not catastrophically interfere with the language modeling head's ability to generate fluent text ($L_{ans}$).
- **Evidence anchors:**
  - [Section 4.2.2] "To mitigate the issues mentioned above, we propose fine-tuning the backbone LLM M jointly on the tasks of answer generation and attribution..."
  - [Section 4.2.2] "The attribution loss is an MSE loss... computed solely for the document identifier tokens."
  - [Corpus] Paper: "ProFit" discusses leveraging probability signals in SFT, supporting the idea of using internal probabilities for alignment, though not identical to this MSE approach.
- **Break condition:** If the hyperparameter $\alpha$ (balancing answer vs. attribution loss) is set too high, the model may prioritize outputting high-logit identifiers over generating the correct answer tokens, leading to correct citations but incoherent answers.

### Mechanism 3
- **Claim:** Top-k pooling aggregation converts noisy token-level signals into robust statement-level attributions.
- **Mechanism:** Token-level logits are volatile. A document might influence a single word but not the whole sentence. The aggregation function $\Psi$ checks if a document's contribution exceeds a threshold $\phi_{prop}$ for a specific proportion $\lambda$ of tokens in a statement. This filters out brief, accidental correlations (noise) in favor of sustained influence.
- **Core assumption:** Faithful attribution requires sustained contribution over a sequence of tokens, rather than isolated token matches.
- **Evidence anchors:**
  - [Section 4.3] "The intuition behind this aggregation operator is that documents should be attributed to the answer statement if they contribute highly to at least a thresholded percentage of the associated tokens."
  - [Section 6.2] Ablation study shows "max" and "avg" pooling strategies generally underperform or are dataset-dependent compared to the proposed threshold-based pooling.
  - [Corpus] Not applicable (specific implementation detail).
- **Break condition:** If the answer statement is very short (e.g., "Yes"), the proportion threshold $\lambda$ may be too restrictive to trigger any attribution, resulting in false refusal or unattributed answers.

## Foundational Learning

- **Concept: Logits vs. Probabilities**
  - **Why needed here:** LoDIT operates on logits (pre-softmax scores) rather than final probabilities. The authors argue that the raw magnitude of the logit for an identifier token carries the causal signal, and they use MSE on these raw values for training.
  - **Quick check question:** Why would an MSE loss on logits be preferred over Cross-Entropy on probabilities when training a model to output specific scores for non-generated tokens?

- **Concept: Selection Bias in LLMs**
  - **Why needed here:** The paper explicitly combats the tendency of LLMs to favor documents at the start of the context (position bias) or specific token patterns (token bias). Understanding this is key to understanding why "Zero-Shot" marking fails and fine-tuning is required.
  - **Quick check question:** In a standard RAG prompt with 5 documents, if you ask an uncalibrated LLM which document is most relevant based solely on its internal confidence, which position (1-5) is it most likely to pick regardless of content?

- **Concept: Faithfulness vs. Correctness**
  - **Why needed here:** This is the central evaluation distinction. A model can be *correct* (right answer) but unfaithful (used parametric memory, not the context). LoDIT optimizes for faithfulness (the link between context and generation).
  - **Quick check question:** If a model answers "Paris" to "What is the capital of France?" but cites a document about French cheese that mentions Paris in passing, is the attribution faithful? Is it correct?

## Architecture Onboarding

- **Component map:** Marking Layer -> LoDIT LLM (Fine-tuned) -> Logit Monitor -> Aggregator
- **Critical path:** The training loop. Specifically, the construction of the target labels $\hat{l}_k$ for the MSE loss. If these labels are not constructed correctly (e.g., assigning negative values to distractor documents vs. zero for random documents), the debiasing mechanism fails.
- **Design tradeoffs:**
  - **Marking Strategy:** The paper finds `marking_BA` (wrapping the whole document) superior to `marking_BAS` (wrapping every sentence) for fine-tuning, suggesting that too much marker noise degrades performance.
  - **Latency vs. Accuracy:** LoDIT adds minimal latency (just reading specific logits) compared to ablation-based methods (like MIRAGE) which require multiple forward passes, but requires a specialized fine-tuned checkpoint.
- **Failure signatures:**
  - **Refusal Loop:** If logits for all identifiers are low, the model triggers the "refusal" mechanism. If the threshold $\lambda$ is too high, the model may refuse to answer answerable questions.
  - **Token Bleed:** If the model starts generating the identifier tokens (e.g., printing "AA") as part of the answer text, it indicates a failure in separating the generation task from the attribution task.
- **First 3 experiments:**
  1. **Baseline Bias Check:** Run the base Llama model on the marked prompt (Zero-Shot). Plot the average logit of `<AA>` (first document) vs. `<JJ>` (last document) on a random batch to visualize position bias.
  2. **Overfitting Test:** Train LoDIT with $\alpha=0$ (standard SFT) vs. $\alpha=0.25$ (LoDIT). Compare TRUST scores to ensure the attribution loss isn't degrading the answer correctness (F1AC).
  3. **Threshold Sensitivity:** Sweep $\lambda$ (proportion threshold) on a validation set. Verify the paper's claim that performance is robust to this hyperparameter (Table 5).

## Open Questions the Paper Calls Out
- **Question:** Can the token-level contribution estimation and statement-level aggregation be integrated into a fully end-to-end learnable framework rather than relying on heuristic pooling functions?
- **Question:** Can the logit-based approach used for attribution be extended to directly guide answer token selection to mitigate inherent model biases (e.g., selection or token bias) during generation?
- **Question:** How can LoDIT be adapted to maintain performance robustness when the number of retrieved context documents varies significantly from the fixed training configuration?
- **Question:** Is the empirical setting of logit target values (4, 2, 0, -2) theoretically optimal, or can the contribution signal be learned without fixed scalar labels?

## Limitations
- **Fine-tuning Dependency:** LoDIT requires computationally expensive joint fine-tuning of the LLM, creating a dependency on proprietary model weights.
- **Marker Token Sensitivity:** The approach is sensitive to the choice and encoding of identifier tokens; if these tokens appear in pre-training data or are split by the tokenizer, the attribution signal degrades.
- **Limited Generalization Testing:** The paper does not test LoDIT on out-of-domain questions or with different retriever types, which could affect the reliability of the attribution signal.

## Confidence
- **High Confidence:** The core technical innovation of using document-identifier token logits for attribution is well-specified and the experimental methodology is clear. The reported TRUST score improvements (up to 14%) are verifiable against the Trust-Align benchmark.
- **Medium Confidence:** The ablation studies supporting the choice of marking strategy (`marking_BA`) and aggregation method (top-k pooling) are limited in scope. The paper does not explore alternative identifier token schemes or more complex attribution aggregation functions.
- **Low Confidence:** The exact mechanism by which the model learns to associate identifier tokens with document content (Mechanism 1) is not empirically validated beyond the stated logit patterns. The paper assumes this association without providing direct evidence from attention weights or intermediate representations.

## Next Checks
1. **Attention Mechanism Validation:** To verify Mechanism 1, analyze the attention weights during generation. For a sample of correctly attributed answers, extract the attention patterns between the identifier token `<AA>` and the document text tokens. Verify that the attention mass is concentrated on the document content, not on the marker itself or other parts of the context.
2. **Cross-Model Generalization:** Fine-tune a different decoder-only model (e.g., Mistral 7B or Qwen 7B) using the same LoDIT procedure. Compare the TRUST score improvement to the reported 14% for Llama 3.1. This tests whether the approach is model-agnostic or overfits to Llama's architecture.
3. **Tokenization Robustness Test:** Intentionally modify the marking strategy to use a high-frequency token as an identifier (e.g., `<SEP>`). Run a small fine-tuning experiment and measure the degradation in TRUST score and attribution accuracy. This would quantify the importance of using rare, dedicated identifier tokens.