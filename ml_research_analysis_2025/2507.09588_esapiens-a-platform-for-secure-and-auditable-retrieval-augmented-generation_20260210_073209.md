---
ver: rpa2
title: 'eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation'
arxiv_id: '2507.09588'
source_url: https://arxiv.org/abs/2507.09588
tags:
- data
- esapiens
- agent
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: eSapiens is an enterprise-grade AI platform designed to securely
  bridge large language models with proprietary business data and workflows. It integrates
  hybrid vector retrieval, structured document ingestion, and no-code orchestration
  via LangChain, supporting major LLMs including OpenAI, Claude, Gemini, and DeepSeek.
---

# eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2507.09588
- **Source URL:** https://arxiv.org/abs/2507.09588
- **Reference count:** 11
- **Primary result:** Achieves 91.3% top-3 retrieval accuracy on legal corpora and up to 23% improvement in factual alignment compared to baseline RAG systems.

## Executive Summary
eSapiens is an enterprise-grade AI platform designed to securely bridge large language models with proprietary business data and workflows. It integrates hybrid vector retrieval, structured document ingestion, and no-code orchestration via LangChain, supporting major LLMs including OpenAI, Claude, Gemini, and DeepSeek. The system features the DEREK engine for citation-aware retrieval-augmented generation and the THOR agent for SQL-style queries over enterprise databases. Two key experiments validate its performance: (1) a retrieval benchmark on legal corpora shows chunk size of 512 tokens achieves the highest precision with 91.3% top-3 accuracy; (2) generation quality tests using TRACe metrics across five LLMs demonstrate up to 23% improvement in factual alignment and context consistency compared to baseline approaches. These results confirm eSapiens' effectiveness in delivering trustworthy, auditable AI workflows for high-stakes domains like legal and finance.

## Method Summary
The platform implements a hybrid retrieval-augmented generation architecture using LangChain for orchestration and Elasticsearch 8.x with dense_vector indexing for document storage. The DEREK engine combines BM25 keyword matching with semantic vector similarity to retrieve relevant document snippets, while the THOR agent provides SQL-style querying capabilities with self-correction loops for failed queries. The system uses a multi-agent coordination framework where a Supervisor Agent routes queries to specialized agents based on intent classification. Evaluation was conducted on LegalBench subsets (PrivacyQA, CUAD, MAUD, ContractNLI) for retrieval performance and RAGtruth dataset for generation quality, measuring metrics including Recall@k, Precision@k, and TRACe scores for hallucination detection and factual alignment.

## Key Results
- Achieved 91.3% top-3 accuracy on legal corpus retrieval with 512-token chunk size
- Demonstrated up to 23% improvement in factual alignment using TRACe metrics compared to baseline approaches
- Successfully implemented hybrid retrieval combining BM25 and vector similarity for improved context relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval (BM25 + vector similarity) improves context relevance over pure vector search.
- Mechanism: Keyword-based BM25 captures exact term matches while dense vector retrieval handles semantic similarity; combining both broadens relevant passage coverage before LLM generation.
- Core assumption: Documents contain both terminology-heavy passages (BM25-favorable) and conceptually related content (vector-favorable) that neither method retrieves completely alone.
- Evidence anchors:
  - [Section 5.6] "Hybrid Retrieval Strategy: Combines keyword-based and semantic similarity searches, returning the top 50 most relevant document snippets efficiently via Elasticsearch."
  - [Section 6] "A hybrid search that mixes BM25 and vector similarity improves recall on long legal text."
  - [corpus] Related work on RAG systems (Lewis et al., 2020) validates hybrid retrieval benefits, though specific BM25+vector combination benchmarks are not directly cited here.
- Break condition: When queries are purely semantic (no keyword overlap) or purely lexical (no semantic depth), hybrid offers minimal gain over single-method retrieval.

### Mechanism 2
- Claim: Multi-agent orchestration with specialized routing improves task-specific output quality.
- Mechanism: A Supervisor Agent classifies query intent (document QA vs. structured data analysis) and routes to specialized agents (Knowledge Base Agent or Data Analyze Agent), each with purpose-built pipelines and context handling.
- Core assumption: Query intent is classifiable with sufficient accuracy at routing time, and specialized agents outperform a single monolithic RAG pipeline.
- Evidence anchors:
  - [Section 7.2] "The overall system is built on a multi-agent coordination framework."
  - [Section 7.3.1] "Supervisor Agent identifies the query as 'document QA' and forwards to DEREK Agent."
  - [Section 7.4.2] "Supervisor Agent determines whether it requires structured query processing and dispatches it to the SQL Generation Agent."
  - [corpus] Gorilla and Toolformer demonstrate autonomous agent routing, but eSapiens does not provide comparative benchmarks against single-agent baselines.
- Break condition: Ambiguous or multi-intent queries cause misrouting, requiring fallback to parallel execution or user clarification.

### Mechanism 3
- Claim: Self-correction loops with LLM-guided SQL regeneration improve structured query success rates.
- Mechanism: When SQL execution fails or returns empty results, the Self-Correction Module invokes the LLM to analyze the failure, regenerate the query with corrected schema references or logic, and retry execution.
- Core assumption: LLMs can diagnose SQL failures from error messages and schema context sufficiently to produce corrected queries.
- Evidence anchors:
  - [Section 7.4.2] "If execution fails (e.g., due to schema mismatch), the Self-Correction Module is triggered to analyze the failure, regenerate the query using LLM feedback and retry."
  - [Appendix C] SQL examples show eSapiens THOR generating more robust queries with fuzzy matching (`LIKE '%hip hop%' OR '%hip-hop%'`) compared to baseline products.
  - [corpus] No external benchmarks for text-to-SQL self-correction are cited; mechanism effectiveness is inferred from example outputs only.
- Break condition: Failures due to fundamental schema misunderstanding or missing data cannot be resolved by query rewrites alone.

## Foundational Learning

- **Concept: Chunking strategies and their retrieval tradeoffs**
  - Why needed here: The paper experiments on chunk sizes (500 vs. 1000 tokens) and their impact on recall/precision; understanding this is essential for configuring DEREK pipelines.
  - Quick check question: Given a document corpus with fragmented sections, would smaller or larger chunk sizes improve recall at low k values?

- **Concept: RAG architecture (retrieve-then-generate pattern)**
  - Why needed here: DEREK and THOR both follow RAG patterns; engineers must understand how retrieval context is injected into LLM prompts.
  - Quick check question: What is the failure mode if retrieved context is irrelevant but the LLM still generates an answer?

- **Concept: Multi-agent orchestration patterns (LangGraph/LangChain)**
  - Why needed here: The system uses LangGraph for agent coordination and LangChain for tool calling; understanding stateless service design and execution graphs is required for debugging.
  - Quick check question: How does a Supervisor Agent decide which downstream agent receives a query?

## Architecture Onboarding

- **Component map:**
  Frontend/UI -> API Gateway -> Application Logic Layer -> Knowledge Adaptation Layer -> Agents -> LLM Backbone -> Storage

- **Critical path:**
  1. User submits natural language query via UI
  2. API Gateway authenticates and forwards to Application Logic Layer
  3. Supervisor Agent classifies intent and routes to appropriate agent
  4. For document QA: Retriever performs hybrid search → LLM generates answer with citations
  5. For structured queries: SQL Generation Agent constructs query → Self-Correction Module retries on failure → Result Interpretation Agent generates natural language summary
  6. Response returned with source citations and audit log entry

- **Design tradeoffs:**
  - **Chunk size (500 vs. 1000 tokens):** Smaller chunks improve precision for fragmented text; larger chunks improve recall for semantic context. Paper appendix recommends 1000 for production workloads.
  - **FAISS vs. Elasticsearch hybrid:** FAISS baseline yields lower hallucination rates but less natural outputs; eSapiens hybrid retrieval improves context relevance with tradeoff in factual strictness.
  - **Model agnosticism vs. optimization:** Supporting multiple LLMs increases flexibility but prevents model-specific optimizations.

- **Failure signatures:**
  - **High hallucination rate:** Retrieved context is insufficient or model generates beyond context; mitigate by adjusting retrieval k or tightening prompt constraints.
  - **SQL execution failures:** Schema mismatch or ambiguous field names; Self-Correction Module should retry, but persistent failures indicate schema documentation gaps.
  - **Low retrieval recall:** Chunk size or embedding model misconfigured for domain; re-evaluate chunking strategy and embedding quality.

- **First 3 experiments:**
  1. **Chunk size benchmark:** Run retrieval on domain-specific corpus with chunk sizes 256, 512, 1000 tokens; measure Recall@k and Precision@k to identify optimal configuration.
  2. **Hybrid vs. pure retrieval comparison:** Disable BM25 or vector component in DEREK; measure context relevance and answer quality using TRACe metrics.
  3. **Self-correction loop stress test:** Submit ambiguous or schema-misaligned SQL queries to THOR; track retry success rate and identify failure patterns requiring manual intervention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive chunking strategy be developed to dynamically select window sizes (e.g., 512 vs. 1024 tokens) based on document structure to optimize retrieval across heterogeneous corpora?
- Basis in paper: [inferred] from Appendix A.4, which notes that chunk=500 yields higher recall for fragmented text (PrivacyQA) while chunk=1000 is superior for semantically dense contracts (CUAD, MAUD).
- Why unresolved: The current system relies on a static chunk size, forcing a trade-off between precision on fragmented documents and recall on continuous texts.
- What evidence would resolve it: A benchmark demonstrating that an adaptive algorithm matches or exceeds the best-in-class recall of both static configurations across the LegalBench subsets.

### Open Question 2
- Question: How can the DEREK engine reduce the elevated hallucination rates (14%–27%) observed in eSapiens to match the strict factual grounding of the FAISS baseline without losing output naturalness?
- Basis in paper: [explicit] in Appendix B.3, which states that while eSapiens improves relevance, it suffers from higher hallucination rates than the baseline because "final-stage model generation... may hallucinate based on general LLM priors."
- Why unresolved: The paper identifies the trade-off but does not propose a mechanism to suppress priors while maintaining the "naturalness" that distinguishes the eSapiens output.
- What evidence would resolve it: Experimentation showing a reduction in "pc hallucinated" tokens to baseline levels (<10%) while preserving high human-rated Accuracy scores (approx. 4.0/5).

### Open Question 3
- Question: What is the quantitative impact of the THOR agent's self-correction loop on SQL execution success rates and latency compared to single-shot generation methods?
- Basis in paper: [inferred] from Section 7.4.3, which describes a self-correction module that triggers upon query failure or low-quality ratings, though the paper provides no specific success rate metrics or retry statistics for this loop.
- Why unresolved: While the architecture is described, the efficacy (how often self-correction saves a failed query) and efficiency (latency cost of retries) remain unquantified.
- What evidence would resolve it: Ablation studies reporting the success rate of initial SQL generation versus post-retry execution, alongside average latency measurements for the retry loop.

## Limitations

- The hybrid retrieval mechanism lacks specific implementation details for query fusion weighting between BM25 and vector similarity components.
- The self-correction loop's effectiveness is demonstrated only through example outputs without quantitative success rate metrics or error type classification.
- The multi-agent routing benefits are inferred from architecture rather than comparative benchmarks against single-agent baselines, creating uncertainty about routing accuracy for ambiguous queries.

## Confidence

- **High confidence:** The retrieval benchmark methodology and TRACe evaluation framework are well-specified and reproducible. The multi-agent architecture pattern aligns with established LangGraph practices.
- **Medium confidence:** The reported 91.3% top-3 accuracy and 23% improvement in factual alignment are plausible given the hybrid retrieval approach, but depend on undisclosed implementation details.
- **Low confidence:** The self-correction loop's reliability cannot be fully assessed without success rate statistics or error type classification.

## Next Checks

1. Implement the hybrid retrieval fusion algorithm with controlled weight parameters and measure precision-recall tradeoffs across different weightings.
2. Conduct a controlled experiment comparing single-agent vs. multi-agent routing on ambiguous query sets to quantify routing accuracy.
3. Stress-test the SQL self-correction mechanism with systematically corrupted queries to measure retry success rates and identify failure patterns.