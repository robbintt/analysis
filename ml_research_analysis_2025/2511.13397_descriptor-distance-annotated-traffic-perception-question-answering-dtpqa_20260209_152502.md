---
ver: rpa2
title: 'Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)'
arxiv_id: '2511.13397'
source_url: https://arxiv.org/abs/2511.13397
tags:
- traffic
- samples
- distance
- question
- dtpqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTPQA is a distance-annotated traffic perception benchmark for
  evaluating VLMs' perception capabilities in isolation from reasoning or world knowledge.
  It consists of 19,149 samples split into synthetic (CARLA) and real-world (nuScenes)
  parts, covering six and four categories respectively.
---

# Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)

## Quick Facts
- **arXiv ID:** 2511.13397
- **Source URL:** https://arxiv.org/abs/2511.13397
- **Reference count:** 15
- **Primary result:** DTPQA is a distance-annotated traffic perception benchmark evaluating VLMs' perception capabilities in isolation from reasoning or world knowledge, revealing significant performance gaps compared to humans, especially for spatial perception tasks.

## Executive Summary
DTPQA is a novel benchmark designed to evaluate the perception capabilities of vision-language models (VLMs) in traffic scenarios, with a specific focus on how performance degrades with increasing object distance. The benchmark consists of 19,149 samples split between synthetic (CARLA-based) and real-world (nuScenes-based) data, covering six and four categories respectively. Each sample includes an image, question, answer, and distance annotation, enabling fine-grained analysis of perception degradation as a function of range. The benchmark uses a multiple-choice format with balanced answers per distance to prevent language bias and isolate visual perception from reasoning capabilities.

## Method Summary
DTPQA was created using CARLA 0.9.15 for synthetic data and the nuScenes dataset for real-world data. The pipeline involves spawning ego-vehicles and objects at controlled distances (5m-50m), capturing images, and validating them through manual inspection. The benchmark includes six synthetic categories (pedestrian crossing, direction, multiple pedestrians, truck blinker, traffic lights, traffic signs) and four real categories (person presence, direction, count, location). The data is structured as multiple-choice questions with balanced answer frequencies per distance bin to mitigate language bias. The complete dataset, including evaluation scripts, is available via GitHub and Mendeley Data.

## Key Results
- DTPQA enables fine-grained analysis of VLM perception degradation with increasing distance, revealing performance gaps that single-distance benchmarks would miss.
- Nine state-of-the-art small VLMs were evaluated, showing significant performance gaps compared to human baselines, especially for spatial perception tasks.
- The benchmark successfully isolates visual perception from reasoning capabilities through trivial, perception-only questions in multiple-choice format.

## Why This Works (Mechanism)

### Mechanism 1: Distance-Calibrated Perception Degradation Analysis
DTPQA enables fine-grained analysis of VLM perception capability as a function of object distance, revealing performance gaps that single-distance benchmarks would miss. Each sample includes both a perception question AND a distance annotation (5m–50m bins plus precise distance). By aggregating accuracy within distance strata, evaluators can isolate how visual perception degrades with range—a critical failure mode in driving scenarios.

### Mechanism 2: Perception–Reasoning Decoupling via Trivial Questions
Using trivial, perception-only questions in a multiple-choice format isolates visual perception from reasoning and world knowledge. Questions like "Is there a pedestrian crossing the road?" require only visual detection, not inference. Multiple-choice eliminates language-generation evaluation metrics that conflate perception quality with linguistic fluency.

### Mechanism 3: Language-Bias Mitigation via Answer Balancing
Enforcing equal answer frequencies per distance bin prevents models from exploiting statistical priors (e.g., always guessing "right" for pedestrian direction). For each category/distance combination, samples are divided equally among possible answers, forcing models to rely on visual evidence rather than dataset-level answer frequencies.

## Foundational Learning

- **Visual Question Answering (VQA) Evaluation**: Understanding how VQA metrics work (accuracy, chance-corrected accuracy) is prerequisite to interpreting results. Quick check: Given a 3-choice balanced VQA dataset, what chance-level accuracy would you expect?

- **Vision-Language Model Architecture Basics**: Knowing the typical components (vision encoder, projection, language decoder) helps diagnose where perception failures originate. Quick check: Which component in a VLM is primarily responsible for extracting visual features from an image?

- **Sim-to-Real Domain Shift**: Understanding performance gaps between CARLA renders and real nuScenes camera images is crucial for interpreting results. Quick check: Name two visual artifacts that might differ between CARLA renders and real nuScenes camera images.

## Architecture Onboarding

- **Component map**: DTPQA Benchmark -> DTP-Synthetic (CARLA-based) with images and annotations, DTP-Real (nuScenes-based) with separate downloads, Evaluation Scripts (GitHub repo)

- **Critical path**: 1) Download DTP-Synthetic images and annotations.json from Mendeley Data, 2) Download nuScenes dataset separately for DTP-Real images, 3) Load annotations, filter by category/distance as needed, 4) Run VLM inference with multiple-choice prompting, 5) Compute accuracy per category, then aggregate by distance bin

- **Design tradeoffs**: Synthetic vs. Real (CARLA enables controlled distance variation and perfect annotations; nuScenes provides real-world noise and edge cases but requires distance binning), Multiple-choice vs. Open-ended (eliminates language metric noise but constrains evaluation), Category breadth vs. Depth (six synthetic + four real categories cover diverse perception tasks but may undersample rare scenarios)

- **Failure signatures**: Uniformly low accuracy across distances → fundamental perception failure, Sharp drop at specific distance (e.g., 30m+) → resolution/scale threshold crossed, Asymmetric errors in balanced categories → residual language bias or vision encoder asymmetry, Large synth-to-real gap without corresponding human gap → domain shift artifact

- **First 3 experiments**: 1) Baseline evaluation: Run your target VLM on all DTPQA categories, compute overall accuracy and chance-corrected accuracy, compare to human baseline (~90%+ on most categories), 2) Distance-stratified analysis: For one category (e.g., Cat.2-Synth - pedestrian direction), plot accuracy vs. distance bin, identify the distance threshold where performance drops below acceptable levels, 3) Ablation on prompt phrasing: Rerun Cat.1-Real with two different question phrasings and measure variance

## Open Questions the Paper Calls Out

### Open Question 1
How do different weather conditions affect VLM perception accuracy at increasing distances in traffic scenes? The authors collected weather metadata but did not conduct any weather-stratified analysis of model performance.

### Open Question 2
How sensitive are VLMs to variations in question phrasing and prompt structure on simple traffic perception tasks? The preliminary investigation mentioned was not comprehensive; systematic prompt engineering analysis remains unexplored.

### Open Question 3
Can fine-tuning on DTPQA improve VLM perception capabilities without degrading general performance? The authors only used DTPQA for evaluation, not for training or fine-tuning experiments.

### Open Question 4
What is the sim-to-real gap when models are evaluated on DTP-Synthetic versus DTP-Real? No cross-domain correlation analysis was reported; the two dataset parts were treated separately.

## Limitations

- The validity of the perception-reasoning decoupling assumption lacks direct empirical validation through controlled experiments.
- The assumption that distance is the primary driver of perception degradation may overlook other confounding factors like occlusion, lighting, or object size.
- The language-bias mitigation through per-distance answer balancing is theoretically sound but lacks empirical validation against models that might develop distance-dependent biases.

## Confidence

- **High confidence**: The benchmark's structural design (distance annotations, multiple-choice format, balanced answers) and its implementation in evaluating nine SOTA VLMs are well-documented and reproducible.
- **Medium confidence**: The claim that DTPQA effectively isolates perception from reasoning requires validation through controlled experiments.
- **Low confidence**: The assumption that distance is the primary driver of perception degradation without systematic control for other visual factors.

## Next Checks

1. **Decoupling validation**: Run the same VLM on DTPQA and a reasoning-requiring VQA benchmark (e.g., VQA-v2) to quantify the gap between perception-only and reasoning+perception performance.

2. **Confounding factor analysis**: For a subset of DTPQA samples, systematically vary factors like occlusion or lighting while holding distance constant to isolate their impact on perception accuracy.

3. **Distance-bias test**: Evaluate a language-model-only baseline (no visual input) on DTPQA's multiple-choice questions to quantify residual language bias after the per-distance balancing.