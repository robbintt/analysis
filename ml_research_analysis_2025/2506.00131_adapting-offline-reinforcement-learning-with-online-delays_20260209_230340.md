---
ver: rpa2
title: Adapting Offline Reinforcement Learning with Online Delays
arxiv_id: '2506.00131'
source_url: https://arxiv.org/abs/2506.00131
tags:
- policy
- offline
- learning
- belief
- delayed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DT-CORL, an offline reinforcement learning
  framework designed to handle online delays when only delay-free data is available.
  The core idea is to use a transformer-based belief predictor to estimate delay-compensated
  states, then perform policy optimization in the original state space while enforcing
  behavior regularization to avoid out-of-distribution actions.
---

# Adapting Offline Reinforcement Learning with Online Delays

## Quick Facts
- arXiv ID: 2506.00131
- Source URL: https://arxiv.org/abs/2506.00131
- Reference count: 40
- This paper introduces DT-CORL, an offline reinforcement learning framework designed to handle online delays when only delay-free data is available.

## Executive Summary
This paper introduces DT-CORL, an offline reinforcement learning framework designed to handle online delays when only delay-free data is available. The core idea is to use a transformer-based belief predictor to estimate delay-compensated states, then perform policy optimization in the original state space while enforcing behavior regularization to avoid out-of-distribution actions. This approach avoids the state-space explosion of naive augmentation and the compounding prediction errors of frozen belief pipelines. Experiments on D4RL locomotion tasks with deterministic and stochastic delays show DT-CORL consistently outperforms both augmented-state and vanilla belief-based baselines, achieving higher returns and better sample efficiency across multiple delay settings. The method effectively bridges the sim-to-real gap while preserving data efficiency.

## Method Summary
DT-CORL combines transformer-based belief prediction with behavior regularization to enable offline RL under online delays. The method trains a belief predictor on delay-free offline data to estimate current states from delayed observations. During online execution, this predictor compensates for system delays without requiring augmentation of the state space. The policy is then trained on the original state space but includes behavior regularization to prevent out-of-distribution actions that could arise from prediction errors. This design avoids both the state-space explosion of naive augmentation approaches and the compounding prediction errors that occur when using a frozen belief pipeline for online inference.

## Key Results
- DT-CORL consistently outperforms both augmented-state and vanilla belief-based baselines on D4RL locomotion tasks
- The method achieves higher returns and better sample efficiency across multiple delay settings (deterministic and stochastic)
- DT-CORL effectively bridges the sim-to-real gap while preserving data efficiency

## Why This Works (Mechanism)
The transformer-based belief predictor can capture complex temporal dependencies in the delay-free offline data, learning to reconstruct current states from delayed observations. By training this predictor end-to-end with the policy optimization, the system avoids the compounding errors that occur when using a separately trained, frozen belief model. The behavior regularization ensures the policy remains within the distribution of states seen during offline training, providing robustness against any remaining prediction errors. This combination allows DT-CORL to effectively handle delays without requiring access to delay-corrupted data during training.

## Foundational Learning
- **Transformer-based belief prediction**: Needed to capture complex temporal dependencies in state estimation; quick check: verify attention patterns show meaningful temporal relationships
- **Behavior regularization in offline RL**: Required to prevent out-of-distribution actions that could cause failures; quick check: monitor KL divergence between policy and behavior policy
- **State-space augmentation trade-offs**: Naive augmentation causes exponential growth in state space; quick check: compare computational complexity and sample efficiency with/without augmentation
- **Offline-to-online transfer**: Critical for sim-to-real applications where delays exist only in deployment; quick check: test performance degradation when moving from offline to online settings

## Architecture Onboarding

Component map:
Delay-free offline data -> Transformer belief predictor -> Delay-compensated state estimates -> Policy optimization (with behavior regularization) -> Action selection

Critical path:
The critical path runs through the transformer belief predictor and policy optimization loop. The predictor must accurately estimate current states from delayed observations, while the policy must learn to act optimally in these predicted states while staying within the behavior distribution.

Design tradeoffs:
- State-space augmentation vs. belief prediction: Augmentation provides exact solutions but explodes state space; belief prediction is compact but introduces approximation errors
- Frozen vs. joint training: Frozen belief models are simpler but suffer from compounding errors; joint training is more complex but maintains accuracy
- Behavior regularization strength: Stronger regularization improves safety but may limit performance; weaker regularization enables higher returns but increases risk

Failure signatures:
- Poor belief prediction manifests as systematic state estimation errors correlated with delay magnitude
- Insufficient behavior regularization shows as sudden performance drops when encountering out-of-distribution states
- Suboptimal delay compensation appears as degraded performance proportional to delay length

First experiments:
1. Evaluate belief prediction accuracy across different delay magnitudes using held-out delay-free data
2. Test policy performance with ground-truth states vs. predicted states to isolate prediction errors
3. Perform ablation study removing behavior regularization to quantify its contribution to robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to MuJoCo locomotion tasks in D4RL benchmark suite; real-world deployment scenarios unexplored
- Claims of consistent performance across "multiple delay settings" lack validation for extremely long delays or non-stationary delay distributions
- Scalability to high-dimensional observations (e.g., vision-based inputs) is unverified
- Ablation studies on relative importance of transformer belief prediction versus behavior regularization are absent

## Confidence
- **High**: DT-CORL improves performance over baselines on tested D4RL tasks with deterministic and stochastic delays
- **Medium**: Generalization to unseen delay patterns and longer delays
- **Low**: Scalability to high-dimensional or multimodal observations

## Next Checks
1. Evaluate DT-CORL on a real-robot platform (e.g., legged locomotion) with measured actuation and sensing delays
2. Test robustness to delay distributions not seen during offline training, including non-stationary and multi-modal delays
3. Perform an ablation study isolating the contributions of transformer belief prediction versus behavior regularization under varying delay magnitudes