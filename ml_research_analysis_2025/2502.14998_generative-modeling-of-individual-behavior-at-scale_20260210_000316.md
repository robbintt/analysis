---
ver: rpa2
title: Generative Modeling of Individual Behavior at Scale
arxiv_id: '2502.14998'
source_url: https://arxiv.org/abs/2502.14998
tags:
- player
- style
- players
- each
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable, generative approach to modeling
  individual human behavior, focusing on behavioral stylometry and per-player generative
  modeling. The key idea is to frame the problem as a multi-task learning setting,
  where each task corresponds to modeling a specific player's behavior.
---

# Generative Modeling of Individual Behavior at Scale

## Quick Facts
- arXiv ID: 2502.14998
- Source URL: https://arxiv.org/abs/2502.14998
- Authors: Nabil Omi; Lucas Caccia; Anurag Sarkar; Jordan T. Ash; Siddhartha Sen
- Reference count: 27
- One-line primary result: MHR adapter routing learns compact style vectors for 47,864 chess players with 94.4% stylometry accuracy, enabling few-shot learning, generative modeling, and interpretable style manipulation.

## Executive Summary
This paper introduces a scalable, generative approach to modeling individual human behavior using parameter-efficient fine-tuning (PEFT) and Multi-Head Adapter Routing (MHR). By framing each player as a task and learning an explicit style vector, the method achieves high stylometry accuracy on chess and Rocket League, supports few-shot learning for new players, and enables interpretable style manipulation via attribute steering. The approach demonstrates generality across domains, including image generation for celebrities.

## Method Summary
The method leverages MHR to learn an explicit style vector for each player by routing over a shared inventory of LoRA adapters. During training, a routing matrix Z assigns each player a distribution over skill modules, enabling selective activation of latent behaviors. After training, style vectors support stylometry (via cosine similarity), generative modeling (move generation), and style manipulation (via attribute deltas). Few-shot learning is achieved by optimizing only the routing vector for new players while freezing adapters.

## Key Results
- 94.4% top-1 stylometry accuracy on 47,864 chess players with 100 query games
- 86.7% top-1 stylometry accuracy on 2,000 Rocket League players
- Style vectors enable interpretable attribute steering and interpolation between players

## Why This Works (Mechanism)

### Mechanism 1: Style Vector Learning via Soft Adapter Routing
- Claim: A compact style vector can encode individual behavioral patterns by learning a distribution over shared skill modules.
- Mechanism: Each player is assigned a row in a routing matrix Z (their "style vector") that specifies softmax weights over an inventory of LoRA adapters. During forward passes, adapters are combined proportionally to these weights, so the style vector selectively activates latent skills (e.g., aggressive play, defensive patterns). The base model remains frozen; only adapters and routing are trained.
- Core assumption: Individual behaviors decompose into composable latent skills that can be shared across a population, and soft routing can capture person-specific skill combinations without negative interference.
- Evidence anchors:
  - [abstract] "use parameter-efficient fine-tuning (PEFT) methods to learn an explicit style vector for each person. Style vectors are generative: they selectively activate shared 'skill' parameters to generate actions in the style of each person."
  - [section 2.2] "the τ-th row of the routing matrix Z is effectively selecting which adapter modules to include in the linear combination... Z[τ] specifies which latent skills are activated for user τ; we call this their style vector."
  - [corpus] Weak direct evidence; neighbor papers focus on behavior modeling but not the MHR routing mechanism specifically.
- Break condition: If the number of latent skills (adapter inventory size) is too small relative to behavioral diversity, or if player behaviors are not well-approximated by linear combinations of shared skills, style vectors may fail to capture individuality.

### Mechanism 2: Few-Shot Learning via Routing-Only Fine-Tuning
- Claim: New players can be embedded into the style space with minimal data by freezing adapters and learning only their routing vector.
- Mechanism: After initial MHR training, the LoRA adapter inventory M is frozen. For a new player, a randomly initialized row is added to Z and optimized on a reference set (e.g., 100 games). This yields a style vector that can be compared to existing vectors via cosine similarity for stylometry, or used for move generation.
- Core assumption: The adapter inventory has already captured the space of latent skills; new players are novel combinations, not novel skills.
- Evidence anchors:
  - [abstract] "enables few-shot learning, stylometry, and interpretable style manipulation"
  - [section 2.2] "Caccia et al. [2023] proposed routing-only finetuning, where after an initial phase of pretraining, the adapter modules are fixed, and only the routing parameters Z are learned for a new task."
  - [corpus] PANTHER paper discusses generative pretraining for sequential user behavior but does not validate routing-only fine-tuning directly.
- Break condition: If a new player exhibits genuinely novel skills not represented in the adapter inventory, routing-only fine-tuning will produce poor embeddings.

### Mechanism 3: Style Steering via Delta Vectors
- Claim: Human-interpretable attributes can be amplified or suppressed by computing a "style delta" from high-attribute players and adding it to any style vector.
- Mechanism: Identify players scoring high on an attribute (e.g., king danger). Compute the mean of their style vectors, subtract the population mean, yielding a direction in style space. Adding this delta to any player's vector shifts their behavior toward that attribute while preserving other aspects of their style.
- Core assumption: Attributes correspond to directions in the learned style space that generalize across players, and style vectors are approximately additive.
- Evidence anchors:
  - [abstract] "style vectors can be combined, interpolated, and steered toward human-interpretable attributes"
  - [section 4] "We extract the common direction among these players, by averaging their style vectors and subtracting the population average. This yields a style delta vector that can be added to any player's style vector."
  - [corpus] No direct validation in neighbor papers; related work on task vectors (Ilharco et al.) mentioned but not in corpus.
- Break condition: If attributes are not linearly decodable from style vectors, or if adding deltas causes unintended entanglement with other attributes, steering will fail or produce incoherent behavior.

## Foundational Learning

- Concept: Multi-Task Learning (MTL)
  - Why needed here: The paper frames each player as a separate task; understanding MTL is essential to grasp how knowledge transfers across players and why shared adapters help.
  - Quick check question: Can you explain why training separate models for each player causes negative interference, while shared adapters with soft routing might avoid it?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: The adapter inventory uses LoRA to parameterize skill modules efficiently; without this, scaling to 47,864 players would be computationally infeasible.
  - Quick check question: Given a weight matrix W ∈ R^(d×d), how many parameters does a rank-r LoRA add? Why is this more efficient than full fine-tuning?

- Concept: Behavioral Stylometry
  - Why needed here: The target application is identifying individuals from their actions; understanding this framing clarifies why move-matching accuracy and cosine similarity are the key metrics.
  - Quick check question: If two players have identical win rates but different move distributions, would a stylometry system based on embeddings distinguish them? Why or why not?

## Architecture Onboarding

- Component map:
  1. Base model (frozen): S&E ResNet for chess (15.7M params) or GPT-2 for Rocket League (87.7M params)
  2. MHR adapter layers: LoRA modules (rank 16) attached to linear transformations
  3. Adapter inventory M: m shared skill modules (32 for chess, 16 for Rocket League)
  4. Routing matrix Z: |T| × m × h tensor (players × inventory × heads)
  5. Style vector: Z[τ, :, :] for player τ, a distribution over skills per head

- Critical path:
  1. Train base model on aggregated player data
  2. Freeze base, train MHR layers + routing on fine-tuning player set using two-speed learning rate (higher for Z)
  3. Freeze adapters, perform routing-only fine-tuning for few-shot players
  4. Compute style deltas by aggregating high-attribute players

- Design tradeoffs:
  - Inventory size vs. expressivity: Larger m captures more skills but increases routing parameters and may fragment representations.
  - Number of heads h: More heads enable finer-grained routing but linearly increase Z's size (though still negligible vs. LoRA params).
  - Routing-only vs. full fine-tuning: Routing-only is 100× cheaper but assumes adapter inventory is sufficiently expressive.

- Failure signatures:
  - Stylometry accuracy near random baseline: Adapter inventory may be too small, or training data per player insufficient.
  - Style steering has no effect or changes unrelated attributes: Delta direction may not isolate the intended attribute; re-examine player selection criteria.
  - Move-matching accuracy degrades after steering: Delta magnitude may be too large; scale down or use interpolation.

- First 3 experiments:
  1. Sanity check: Train MHR-Maia on a 100-player subset; verify per-player move-matching accuracy is within 1% of individual fine-tuning.
  2. Stylometry scaling: On the full 47,864-player set, confirm top-1 accuracy >90% with 100 query games; plot accuracy vs. query set size.
  3. Style steering validation: For 3 attributes (e.g., king danger, bishop pair, mobility), compute deltas, apply to held-out players, and verify attribute scores change significantly (Δ > 0.5 std) while other attributes remain stable (Δ < 0.2 std).

## Open Questions the Paper Calls Out
None

## Limitations
- Skill compositionality assumption may fail if player behaviors require novel, non-combinatorial patterns
- Few-shot learning depends on adapter inventory expressiveness; novel skills may not be representable
- Style steering relies on linear additivity of deltas, which may not hold due to high-dimensional entanglement

## Confidence
- **High confidence**: Stylometry accuracy (>90% on chess, >86% on Rocket League) and move generation quality, as these are directly validated with held-out data and standard metrics.
- **Medium confidence**: Few-shot learning performance and style steering effectiveness, as these depend on stronger assumptions about the structure of the style space and are less extensively validated.
- **Low confidence**: Generalization to other domains (e.g., image generation) and the scalability of the approach to much larger populations or more complex behaviors, as these are only briefly demonstrated.

## Next Checks
1. **Robustness to Adapter Inventory Size**: Systematically vary the number of LoRA adapters (m) and evaluate stylometry accuracy and move generation quality. Identify the minimum m required for saturation and test whether stylometry degrades gracefully if m is too small.

2. **Few-Shot Learning on Novel Behaviors**: Create a test set of players whose behaviors are qualitatively different from the training set (e.g., extreme strategies in chess). Measure stylometry accuracy and move generation after routing-only fine-tuning, and compare to random and average-style baselines.

3. **Style Steering Interpretability and Stability**: For each attribute, apply the computed delta to multiple held-out players and measure: (a) the change in the target attribute score, (b) changes in other attributes, and (c) qualitative coherence of generated behavior. Test whether deltas remain stable under different random seeds and player selection criteria.