---
ver: rpa2
title: 'RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits'
arxiv_id: '2508.19272'
source_url: https://arxiv.org/abs/2508.19272
tags:
- conversation
- user
- conversations
- platform
- aphene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAGAPHENE is a chat-based annotation platform designed to enable
  the creation of high-quality, multi-turn conversational benchmarks for evaluating
  retrieval-augmented generation (RAG) systems. Unlike existing annotation tools,
  RAGAPHENE supports real-time agent response generation and allows annotators to
  edit both retrieved passages and generated responses, simulating real-world conversational
  scenarios.
---

# RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits

## Quick Facts
- arXiv ID: 2508.19272
- Source URL: https://arxiv.org/abs/2508.19272
- Reference count: 5
- RAGAPHENE is a chat-based annotation platform enabling real-time editing of passages and responses to build high-quality multi-turn RAG conversation benchmarks.

## Executive Summary
RAGAPHENE is a web-based annotation platform designed to create high-quality, multi-turn conversational benchmarks for evaluating retrieval-augmented generation (RAG) systems. Unlike existing tools, it allows annotators to edit both retrieved passages and generated responses in real-time, simulating real-world dialogue scenarios. The platform includes three modes—Create, Review, and Experiment—and has been used to generate over 5,000 conversations with input from 40 annotators. A user study with 31 professional annotators confirmed that features like response editing and lexical overlap highlighting significantly improve conversation quality.

## Method Summary
RAGAPHENE is a React/NextJS 14 frontend with a Python backend, using Carbon Design System for UI. It integrates ElasticSearch, MongoDB Atlas, and IBM Cloudant for retrieval, and WatsonX.AI and OpenAI for generation. The platform supports three modes: Create (build conversations with customizable retrievers and generators), Review (ensure quality via accept/edit/reject), and Experiment (run small-scale evaluations). Conversations are exported as structured JSON and can be analyzed with InspectorRAGet. The tool is lightweight, privacy-focused, and emphasizes human-in-the-loop editing for high-quality ground truth.

## Key Results
- RAGAPHENE enables creation of 110 high-quality multi-turn RAG conversations with over 5,000 created and 1,000+ reviewed by 30+ annotators.
- User study (n=31) shows response editing and lexical overlap highlighting most improve perceived data quality.
- Platform supports integration with InspectorRAGet for deeper analysis and is designed for due diligence in domain-specific RAG model selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enabling annotators to edit both retrieved passages and generated responses improves benchmark quality compared to annotation-only approaches.
- Mechanism: The platform surfaces retriever and generator failures to the annotator at each turn, allowing real-time correction. This creates ground truth that explicitly encodes what the system *should* have retrieved and generated, not just what it did.
- Core assumption: Annotators can accurately identify when retrieval or generation has failed and can produce correct alternatives.
- Evidence anchors:
  - [abstract] "It enables annotators to simulate real-world dialogues by integrating customizable retrievers and generators, allowing real-time editing of passages and agent responses."
  - [section 5, Table 1] User study (n=31) shows "Editing the agent responses" (μ=4.26) and "Highlights for overlapping text" (μ=4.13) ranked highest for impact on data quality (5-point scale: 1="No decrease" to 5="Extreme decrease" if removed).
  - [corpus] Related work (MTRAG, RankArena) focuses on evaluation but does not emphasize co-editing passages and responses as a quality mechanism.
- Break condition: If annotators lack domain expertise, edits may introduce errors rather than corrections. The paper notes annotators reported "advanced beginner-level of RAG understanding" (μ=2.61/5), suggesting this is a real constraint.

### Mechanism 2
- Claim: Highlighting lexical overlap between response and passages helps annotators verify faithfulness more efficiently.
- Mechanism: Visual highlighting reduces the cognitive load of cross-referencing the response against multiple passages, making it easier to spot hallucinated content that lacks grounding.
- Core assumption: Lexical overlap correlates with factual grounding; absence of overlap signals potential hallucination.
- Evidence anchors:
  - [section 3.1] "Finally, to help users check if the response is faithful to the passages, the platform highlights the lexical overlap between the response and the passages."
  - [section 5, Table 1] "Highlights for overlapping text in the context and response" ranked second (μ=4.13) in quality impact.
  - [corpus] No direct corpus comparison available; this appears to be a platform-specific contribution.
- Break condition: Faithful paraphrases with low lexical overlap would be flagged incorrectly; hallucinations with high lexical overlap (e.g., copied but misattributed) would be missed.

### Mechanism 3
- Claim: A separate review workflow with accept/edit/reject decisions produces higher-quality benchmarks than single-pass creation alone.
- Mechanism: Decoupling creation from review allows a second annotator to catch errors, ensure consistency, and enforce standards without the cognitive burden of simultaneous creation and quality checking.
- Core assumption: Reviewers can make consistent quality judgments and repairs without changing the conversational intent.
- Evidence anchors:
  - [section 3.2] "In this mode a reviewer will receive a batch of conversations. The reviewer reads through each conversation and can accept the conversation as is, accept it but edit the conversation, or reject it."
  - [section 4.1] "We have successfully created 110 high quality Multi-Turn RAG conversations using the creation and review workflows... over 5,000 conversations created and over 1,000 conversations reviewed by over 30 annotators."
  - [corpus] Related platforms (Label Studio, First-Aid) support annotation but not this specific multi-stage review pipeline.
- Break condition: If reviewer guidelines are ambiguous or inter-annotator agreement is low, review adds noise rather than quality. The paper does not report agreement metrics.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire platform is built around evaluating and improving RAG systems where an LLM generates responses conditioned on retrieved passages.
  - Quick check question: Can you explain why a RAG system might produce a plausible-sounding but factually incorrect answer?

- Concept: **Multi-turn dialogue context**
  - Why needed here: Benchmarks must evaluate whether systems maintain coherence across follow-up questions, clarifications, and topic shifts.
  - Quick check question: How does resolving a follow-up question ("What about their Q3 revenue?") differ from answering a standalone query?

- Concept: **Ground truth construction via human repair**
  - Why needed here: The platform's approach assumes the "correct" system behavior is defined by human-edited passages and responses, not raw model outputs.
  - Quick check question: If a retriever returns 5 passages but only 2 are relevant, how should the ground truth be represented?

## Architecture Onboarding

- Component map: Frontend (React/NextJS 14) -> Backend (Python ≥3.10) -> Retrieval engines (ElasticSearch, MongoDB Atlas, IBM Cloudant) -> Generation engines (WatsonX.AI, OpenAI) -> Data export (JSON) -> Analysis (InspectorRAGet)

- Critical path:
  1. Configure retriever (corpus, k passages, query formulation) and generator (model, prompt, decoding)
  2. Create conversation: write question → receive response → edit passages → edit response → add enrichments
  3. Export as JSON
  4. Review: load batch → accept/edit/reject → add comments → export
  5. Experiment: upload conversations → select split strategy → choose models/metrics → run (max 100 tasks) → export for InspectorRAGet

- Design tradeoffs:
  - Stateless architecture prioritizes privacy but requires users to manage JSON exports manually
  - Experiment mode is intentionally limited to 100 tasks (designed for rapid prototyping, not large-scale eval)
  - Lexical overlap highlighting is fast but imperfect for semantic faithfulness
  - Reviewers cannot change questions (preserves intent) but may limit repair options

- Failure signatures:
  - High edit rates on responses may indicate generator misconfiguration or prompt issues
  - Frequent passage searches suggest retriever is missing relevant docs
  - High rejection rates in review mode may indicate unclear creation guidelines
  - Annotators taking >30 min/conversation (as reported) signals task complexity

- First 3 experiments:
  1. **Baseline characterization**: Run existing conversations through Experiment mode with default retriever/generator; measure ROUGE, Recall, and LLM-as-Judge scores to establish performance baseline.
  2. **Ablation on retrieval**: Keep generator fixed, swap retriever (e.g., BM25 vs. dense embeddings); compare passage relevance annotations and response quality metrics.
  3. **Prompt sensitivity**: Keep retriever fixed, test 2-3 prompt variants for the same generator; measure impact on response faithfulness (using overlap highlighting and human review).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the human-in-the-loop "repair" of passages and responses lead to datasets that improve downstream RAG model performance more effectively than synthetic data or unedited annotations?
- Basis in paper: [inferred] The user study quantifies *perceived* feature importance (Table 1) but does not measure the objective impact of the "repaired" data on downstream model training or evaluation accuracy.
- Why unresolved: The paper validates the tool's usability and user preference, but the causal link between these specific annotation interventions and improved model performance remains assumed.
- What evidence would resolve it: An experiment comparing RAG systems fine-tuned or evaluated on RAGAPHENE-repaired data against systems using raw LLM outputs or standard annotations.

### Open Question 2
- Question: To what extent does the annotator's level of RAG expertise influence the quality and correctness of the "repaired" responses and passage relevance judgments?
- Basis in paper: [inferred] The user study population self-reported an "advanced beginner" level of RAG understanding (µ=2.61/5), raising questions about the validity of their corrections for complex retrieval errors.
- Why unresolved: The study does not correlate annotator expertise with the frequency or accuracy of their interventions, nor does it specify if "advanced beginner" knowledge is sufficient for identifying subtle hallucinations.
- What evidence would resolve it: A comparative analysis of annotation quality and error detection rates between expert RAG engineers and novice annotators using the platform.

### Open Question 3
- Question: Can the annotation workflow be automated or streamlined to reduce the reported 30+ minutes required per conversation without sacrificing data quality?
- Basis in paper: [inferred] The authors note that creating high-quality conversations is "hard and time-consuming," with annotators reporting an average of over 30 minutes per session.
- Why unresolved: The paper introduces a manual interface for repair but does not explore methods to accelerate the "search" or "edit" phases, which may limit the scalability of dataset creation.
- What evidence would resolve it: A study measuring time-to-completion and resulting data quality when integrating automated repair suggestions or heuristics into the tool.

## Limitations
- The paper does not report inter-annotator agreement or quality metrics for reviewed conversations, making reliability of the human-enhanced ground truth difficult to assess.
- The user study sample size (n=31) is modest, and platform performance with less experienced annotators or on different domains is untested.
- The lexical overlap highlighting mechanism is limited to surface-level features and may miss semantic hallucinations or flag valid paraphrases.

## Confidence
- **High confidence**: The platform's core architecture and feature set are well-documented and demonstrably functional (110 conversations created, 1,000+ reviewed). The user study provides strong evidence that key features (response editing, overlap highlighting, review workflow) improve perceived data quality.
- **Medium confidence**: The effectiveness of the human-enhanced ground truth approach relies on annotator expertise, which is only partially characterized (self-reported "advanced beginner" RAG knowledge). The paper does not quantify the impact of edits on downstream RAG system performance.
- **Low confidence**: Claims about the platform's utility for "due diligence in selecting models for domain-specific applications" are aspirational; the paper does not provide evidence of successful model selection or domain transfer.

## Next Checks
1. **Inter-annotator agreement**: Run a study with multiple annotators creating and reviewing the same conversations; compute Cohen's kappa or Krippendorff's alpha for edit decisions and review judgments.
2. **Downstream impact**: Evaluate whether conversations with human-enhanced passages/responses lead to better RAG system performance (e.g., higher faithfulness scores) compared to raw system outputs or annotation-only datasets.
3. **Feature ablation**: Create a subset of conversations without response editing or overlap highlighting; compare quality metrics and annotator time per conversation to quantify the marginal value of each feature.