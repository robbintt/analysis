---
ver: rpa2
title: Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an
  LLM Agent
arxiv_id: '2509.25593'
source_url: https://arxiv.org/abs/2509.25593
tags:
- edge
- latent
- text
- agent
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel explainable AI method using large
  language models (LLMs) to perform identity mapping between feedback fuzzy cognitive
  maps (FCMs) and text representations. The approach mimics autoencoder behavior through
  multi-prompting, converting FCMs to detailed but unnatural text (latent I), then
  optionally refining to natural-sounding text (latent II) at the cost of detail.
---

# Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent

## Quick Facts
- arXiv ID: 2509.25593
- Source URL: https://arxiv.org/abs/2509.25593
- Authors: Akash Kumar Panda; Olaoluwa Adigun; Bart Kosko
- Reference count: 24
- Primary result: Novel LLM-based method autoencodes FCMs to/from text with interpretable latent representations, preserving strong causal edges while losing weak ones

## Executive Summary
This paper introduces a novel explainable AI method that uses large language models (LLMs) to approximate autoencoder behavior for feedback fuzzy cognitive maps (FCMs). The approach performs identity mapping between FCMs and text representations through multi-prompting, converting FCMs to detailed but unnatural text (Latent I), then optionally refining to natural-sounding text (Latent II) at the cost of detail. Experiments with Google's Gemini 2.5 Pro on three FCMs demonstrate that reconstruction preserves strong causal edges while losing weak ones, achieving interpretable latent representations unlike black-box neural networks.

## Method Summary
The method uses a single-agent multi-prompting approach with Google's Gemini 2.5 Pro to approximate autoencoder behavior for FCMs. The LLM agent performs sequential decomposition: first encoding the FCM into detailed text (Latent I), optionally editing for naturalness (Latent II), then decoding through three subtasks—noun detection, node detection with magnitude filtering, and edge extraction with textual justification. Unlike neural autoencoders, this creates human-readable latent text representations that preserve strong causal edges while losing weak ones. The system achieves reconstruction with l1, l2, and l∞ norm errors while providing interpretable text-based justifications for decisions.

## Key Results
- Reconstruction preserves strong causal edges (|w| > 0.5) while losing weak edges, as shown in edge matrix comparisons across three FCMs
- Text-based latent representations enable human interpretation of FCM structure, unlike black-box neural networks
- Node polarity issues occur during content editing, where concepts like "loss of appetite" become "appetite," inverting connected edge signs
- l1 reconstruction errors: 14.56 (Latent I) vs 78.40 (Latent II) vs 41.20 (adjusted for polarity flips)

## Why This Works (Mechanism)

### Mechanism 1: Sequential Multi-Prompt Decomposition
Breaking FCM↔text translation into discrete subtasks (encoding, editing, noun detection, node detection, edge extraction) improves reconstruction fidelity. The LLM agent receives successive system instructions that constrain each subtask's output space. Core assumption: LLMs maintain sufficient context across prompt sequence to preserve causal structure without explicit error signals between stages.

### Mechanism 2: Causal Salience Filtering via Natural Language Bottleneck
The text representation acts as lossy compression that preferentially preserves high-magnitude edges while discarding weak edges. Strong causal edges translate to explicit linguistic markers ("strongly causes"), while weak edges receive diluted or absent encoding. Core assumption: LLM's natural language generation preferentially verbalizes salient relationships.

### Mechanism 3: Node Polarity Sensitivity in Natural Language Refinement
Content editing for naturalness can flip node polarity (e.g., "loss of appetite" → "appetite"), inverting downstream edge signs. Latent I prioritizes accuracy with unnatural phrasing; Latent II optimizes fluency, potentially dropping negation markers. Core assumption: Naturalness-fluency tradeoff is inherent to current LLMs.

## Foundational Learning

- **Fuzzy Cognitive Maps (FCMs)**: Core data structure being autoencoded. FCMs are directed weighted graphs where nodes = causal variables, edges w_ij ∈ [-1,1] = causal strength/sign. State evolves via C(t+1) = φ(C(t)·E).
  - Quick check: Given a 3-node FCM with edge weights w_12=0.8, w_23=-0.5, w_31=0.3, what does the negative weight signify about the C2→C3 relationship?

- **Autoencoder Identity Mapping**: This paper approximates autoencoder behavior (input→latent→reconstruction) using LLM prompts instead of neural networks. Key difference: latent space is human-readable text, not hidden vectors.
  - Quick check: In a standard neural autoencoder, what role does the bottleneck layer play, and what corresponds to it in this LLM-based system?

- **Named Entity Recognition (NER) for Causal Extraction**: Decoder's first subtask extracts noun phrases as node candidates. NER capability determines whether "psychomotor retardation" is recognized as a single concept vs. two unrelated words.
  - Quick check: Why might an LLM struggle to distinguish between "fatigue" (a causal variable) and "fatigue" (mentioned incidentally in explanatory text)?

## Architecture Onboarding

- **Component map**:
Input FCM (edge matrix E, node list)
    ↓
[Encoder Prompt] → Latent I (detailed, unnatural text)
    ↓
[Content Editor Prompt] → Latent II (natural, lossier text) [OPTIONAL]
    ↓
[Decoder Prompts]:
  1. Noun Detection (NER) → candidate node list
  2. Node Detection (magnitude filtering) → final node list
  3. Edge Extraction (pairwise inference) → reconstructed edge matrix E'
    ↓
Reconstructed FCM

- **Critical path**:
  1. Encoding prompt design—must specify edge-weight→language mapping (e.g., "strongly" for |w|>0.7)
  2. Latent text quality—determines reconstruction ceiling
  3. Node detection—errors propagate to all downstream edges; flipped nodes invert sign on all connected edges

- **Design tradeoffs**:
  - Latent I vs. Latent II: Accuracy vs. readability. Table II shows l1 error: 14.56 (Latent I) vs. 78.40 (Latent II) vs. 41.20 (adjusted for flips)
  - Temperature=0: Used for reproducibility; higher temperatures may increase creativity but reduce consistency
  - Node count: Paper tests 6-14 node FCMs; scaling behavior unknown

- **Failure signatures**:
  - Missing edges: Non-zero edges in E become zero in E' (weak edge dropout)
  - Sign flips: Negative edges appearing where positives expected (node polarity inversion)
  - Node fragmentation: Single concept split into multiple nodes (NER failure)
  - Hallucinated edges: Edges appear in E' without textual basis (decoder over-inference)

- **First 3 experiments**:
  1. **Baseline reconstruction test**: Take 3 FCMs from paper (depression, depression subset, celiac), reproduce Latent I→E' pipeline, verify l1/l2/l∞ errors match Table II within 10%.
  2. **Ablation on prompt decomposition**: Compare 3-subtask decoding vs. single-prompt decoding on same Latent I text; measure edge preservation rate for |w|>0.5 edges.
  3. **Polarity preservation test**: Construct a 5-node FCM with 3 negated concepts (e.g., "lack of X," "absence of Y"); run full pipeline and measure node-name accuracy in reconstruction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the node-flipping problem (where concept polarity inverts, e.g., "loss of appetite" → "appetite") be systematically prevented during text-to-FCM reconstruction?
- Basis in paper: The authors observe that "the 4th, 9th, and 10th reconstructed nodes from the latent II summary represented the opposite of the corresponding causal variable from the target FCM," causing edge sign inversions.
- Why unresolved: The paper documents this phenomenon and manually adjusts for it in error analysis, but provides no mechanism to detect or prevent polarity flips during encoding or decoding.
- What evidence would resolve it: A modified prompting strategy or post-processing step that correctly preserves node polarity across the FCM→text→FCM cycle, validated across multiple FCM domains.

### Open Question 2
- Question: Does this autoencoding approach generalize across different LLM architectures and parameter scales beyond Gemini 2.5 Pro?
- Basis in paper: All experiments use a single LLM (Gemini 2.5 Pro) with fixed hyperparameters (temperature=0, top-p=0.95), limiting claims about the method's robustness.
- Why unresolved: The paper does not test whether the multi-prompting strategy and reconstruction fidelity hold for other models (e.g., GPT-4, Claude, open-source LLMs) or different temperature/top-p settings.
- What evidence would resolve it: Systematic experiments showing reconstruction error norms (l1, l2, l∞) across at least 3-5 different LLMs and varied sampling parameters on the same FCM benchmark.

### Open Question 3
- Question: What techniques could improve preservation of weak causal edges (|w_ij| < 0.3) during the lossy text-mediated reconstruction?
- Basis in paper: The paper states: "The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges" and "many non-zero edge weights in E changed to zero" in reconstructed FCMs.
- Why unresolved: The current content-editing and decoding prompts implicitly filter weak edges as linguistically less salient; no mechanism counteracts this information loss.
- What evidence would resolve it: A modified encoding scheme (e.g., explicit weak-edge markers) or decoding prompt that yields statistically significant improvement in weak-edge recall without degrading strong-edge preservation.

## Limitations

- Prompt Specification Gap: The paper describes functional requirements but does not provide exact prompt wording or structured output formats, creating ambiguity in reproducing results.
- Node Polarity Preservation: Content editing for naturalness can flip node polarity, causing sign inversions in connected edges without a systematic solution.
- Edge-Weight-to-Language Mapping: The encoding scheme that translates numeric edge weights to natural language descriptions is not explicitly defined.

## Confidence

- **High Confidence**: The core mechanism of sequential multi-prompt decomposition is well-supported by experimental results; preservation of strong edges while losing weak edges is consistently demonstrated.
- **Medium Confidence**: Claims about explainable AI benefits are supported by text-based justifications, but evaluation focuses on reconstruction accuracy rather than interpretability benefits to end users.
- **Low Confidence**: Scalability claims beyond tested 6-14 node FCMs are speculative; paper does not address context window limitations or performance degradation with larger FCMs.

## Next Checks

1. **Prompt Fidelity Test**: Reconstruct the exact prompt sequences from functional descriptions and verify that the same linguistic patterns emerge for equivalent edge weights across different FCMs.

2. **Polarity Preservation Benchmark**: Create a test suite of 10 FCMs with varying densities of negated concepts and measure the node polarity error rate when using Latent II text versus Latent I text.

3. **Weak Edge Preservation Analysis**: Systematically vary the minimum edge weight threshold for preservation and plot the trade-off curve between reconstruction completeness and error metrics to determine the optimal threshold for different application domains.