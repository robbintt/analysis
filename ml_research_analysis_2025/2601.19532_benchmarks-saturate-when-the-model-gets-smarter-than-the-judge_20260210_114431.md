---
ver: rpa2
title: Benchmarks Saturate When The Model Gets Smarter Than The Judge
arxiv_id: '2601.19532'
source_url: https://arxiv.org/abs/2601.19532
tags:
- answer
- judge
- problem
- gpt-5
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how evaluation noise in LLM benchmarks
  is driven not just by dataset errors, but increasingly by judge errors as model
  performance improves. The authors introduce Omni-MATH-2, a manually cleaned version
  of the Omni-MATH dataset with exact-answer subsets (n=4181) and tagged non-standard
  subsets (n=247).
---

# Benchmarks Saturate When The Model Gets Smarter Than The Judge

## Quick Facts
- arXiv ID: 2601.19532
- Source URL: https://arxiv.org/abs/2601.19532
- Reference count: 40
- Key finding: Judge competence gaps, not just dataset errors, drive evaluation noise as model performance improves

## Executive Summary
This paper demonstrates that LLM benchmark saturation results from the interaction between dataset quality, model ability, and judge competence, not solely from model properties. The authors introduce Omni-MATH-2, a manually cleaned math dataset with exact-answer subsets (n=4181) and tagged non-standard subsets (n=247). They benchmark five state-of-the-art models and compare evaluations from Omni-Judge and GPT-5 mini, finding that judge choice significantly alters both absolute accuracy and model rankings. Expert auditing reveals that Omni-Judge is wrong in 96.4% of judge disagreements on clean questions, mainly due to failure to assess answer equivalence.

## Method Summary
The authors manually cleaned the Omni-MATH dataset to create Omni-MATH-2, removing 14.6% of problems and tagging 5.6% as non-standard (missing images, proofs, estimates, or degenerate problems). They evaluated five state-of-the-art models on the exact-answer subset (n=4181) and compared results from two judges: Omni-Judge and GPT-5 mini. Expert auditing of 100 sampled disagreements revealed judge error patterns, and they analyzed how disagreement rates varied by problem domain and difficulty tier.

## Key Results
- Omni-Judge is wrong in 96.4% of disagreements on clean exact-answer problems, primarily failing to recognize mathematically equivalent expressions
- Judge disagreement increases with problem difficulty, with the largest deltas in Tier 4 problems
- Model rankings change significantly depending on judge choice (Gemini ranks 1st with GPT-5 mini vs 5th with Omni-Judge)
- Neither judge reliably handles tagged problems (missing images, proofs, estimates), marking correct responses as wrong

## Why This Works (Mechanism)

### Mechanism 1: Judge Competence Gap Dominates Evaluation at Frontier
- Claim: When model accuracy approaches judge capability limits, evaluation noise from judge errors dominates measured performance differences between models
- Mechanism: LLM-based judges fail to recognize mathematically equivalent expressions in different forms (e.g., simplified vs. unsimplified fractions, closed-form vs. summation notation)
- Core assumption: Judge errors are systematic and favor certain answer formulations over mathematically equivalent alternatives
- Evidence anchors: 96.4% audit error rate on disagreements, 75/100 cases failed equivalence assessment

### Mechanism 2: Dataset Error Types Propagate Through Evaluation Pipeline
- Claim: Dataset quality issues cascade through extraction, equivalence checking, and scoring—causing systematic misclassification unrelated to model ability
- Mechanism: Four error categories (missing images, proof requests, estimation tasks, degenerate problems) create structural incompatibility with exact-answer verification
- Core assumption: The evaluation pipeline does not implement task-type-aware verification logic
- Evidence anchors: 14.6% edited problems, 5.6% tagged as non-standard, neither judge identifies failure modes

### Mechanism 3: Difficulty-Dependent Judge Disagreement
- Claim: Judge disagreement increases with problem difficulty, making judge competence more impactful for frontier/hard problems
- Mechanism: Harder problems produce more complex answers with more valid reformulations
- Core assumption: Problem difficulty correlates with answer complexity and alternative-formulation space
- Evidence anchors: Largest accuracy deltas in Tier 4 problems, domain-specific disagreement patterns

## Foundational Learning

- **Concept: Answer Equivalence vs. String Matching**
  - Why needed here: The primary judge failure mode is confusing surface-form differences with semantic non-equivalence
  - Quick check question: Given reference answer "Σᵢ₌₁ⁿ min(n+1−i, 2i−1)" and model answer "⌈n(n+1)/3⌉", what verification steps would you need?

- **Concept: Evaluation as (Dataset, Model, Judge) Triplet**
  - Why needed here: Benchmark scores are not a model property alone—they're emergent from the interaction
  - Quick check question: If you swap judges and model rankings change, what does that tell you about the benchmark's saturation state?

- **Concept: Judge Calibration via Disagreement Audit**
  - Why needed here: Without expert annotation of disagreements, you cannot know which judge is wrong
  - Quick check question: You have two judges with 5% disagreement rate on your benchmark. What's the minimum audit sample size needed to estimate which judge is more accurate with 95% confidence?

## Architecture Onboarding

- **Component map**: Dataset layer (Omni-MATH-2-Filtered + Omni-MATH-2-Tagged) -> Model layer (5 SOTA models) -> Judge layer (Omni-Judge vs GPT-5 mini) -> Audit layer (expert + LLM council)
- **Critical path**: 1. Clean/tag dataset -> 2. Filter to exact-answer subset -> 3. Run all models -> 4. Evaluate with multiple judges -> 5. Sample disagreements for human audit -> 6. Report confidence intervals
- **Design tradeoffs**: Filtering tagged problems reduces noise but loses coverage; multi-judge evaluation increases cost but reveals hidden disagreement; human audit is expensive but essential for calibration
- **Failure signatures**: Judge disagreement clustered in specific domains (Calculus: -8.9 to +3.2 accuracy delta) and difficulty tiers (Tier 4: -12.0 to +3.9 delta); models correctly identifying missing information marked incorrect; estimates scored against exact reference answers
- **First 3 experiments**:
  1. Judge comparison on held-out sample: Run your judge and a stronger reference judge on 500 problems; measure disagreement rate and domain/difficulty distribution
  2. Disagreement audit: Manually review 50-100 disagreements with expert annotators; compute judge error rates by error category
  3. Tagged-problem handling test: Evaluate models on estimation problems both with and without problem-stated scoring rules; measure gap between judge score and true contest score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed patterns of judge-induced noise and bias generalize across a diverse array of LLM-based judges and prompt configurations?
- Basis in paper: The authors state they "focus on two judges under specific prompts and settings" and "cannot fully characterize the space of judge behaviors"
- Why unresolved: The study only compares Omni-Judge and GPT-5 mini, leaving the reliability of other evaluator models untested
- What evidence would resolve it: A broad evaluation study using a multi-model judge ensemble to assess the same model generations across varying prompt templates

### Open Question 2
- Question: To what degree do incorrect or incomplete reference answers (ground truth) drive the disagreements observed between LLM judges?
- Basis in paper: The authors acknowledge they "did not revise the solutions and reference answers" and that expert auditing revealed wrong reference answers in a subset of disagreements
- Why unresolved: The study isolates dataset errors but does not systematically separate the specific impact of wrong ground truth on judge calibration
- What evidence would resolve it: An ablation study evaluating judges on a dataset with manually verified and corrected reference solutions

### Open Question 3
- Question: Can a quantifiable "evaluator margin" be established to predict when judge incompetence will mask model improvements?
- Basis in paper: The authors discuss the "interaction phenomenon" where judge errors mask differences, suggesting the need for an "evaluator margin"
- Why unresolved: While the paper demonstrates the phenomenon qualitatively, it does not define a specific threshold or metric for the required competence gap
- What evidence would resolve it: A theoretical model correlating judge accuracy with model score variance to define the minimum judge capability required for reliable ranking

## Limitations
- The audit sample size (n=100 disagreements) may not generalize to non-standard problem types
- The study's focus on math benchmarks limits external validity to other domains
- Judge calibration depends on having an expert reference, but no absolute ground truth exists for many mathematical equivalences

## Confidence
- High: Judge competence dominates evaluation noise at frontier performance (96.4% audit result)
- Medium: Difficulty-dependent disagreement pattern (based on 5-model tier analysis)
- Low: Extrapolation to non-math domains without domain-specific validation

## Next Checks
1. **Domain transfer test**: Apply multi-judge evaluation to coding benchmarks (e.g., HumanEval) and compare disagreement patterns with math results
2. **Judge capability scaling**: Measure judge-accuracy vs. model-accuracy trajectories to identify when judge errors become dominant
3. **Automated disagreement detection**: Develop statistical methods to flag high-risk problems for audit without requiring full expert review of all disagreements