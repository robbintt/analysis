---
ver: rpa2
title: 'HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning'
arxiv_id: '2511.15355'
source_url: https://arxiv.org/abs/2511.15355
tags:
- questions
- spanish
- arxiv
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HEAD-QA v2 is an expanded multilingual benchmark for healthcare
  reasoning, growing from 6,765 to 12,751 questions across ten years of Spanish medical
  exams. It introduces new English, Italian, Galician, and Russian versions, and evaluates
  multiple open-source LLMs using prompting, retrieval-augmented generation, and probability-based
  answer selection.
---

# HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning

## Quick Facts
- arXiv ID: 2511.15355
- Source URL: https://arxiv.org/abs/2511.15355
- Reference count: 0
- Expanded multilingual benchmark for healthcare reasoning from 6,765 to 12,751 questions across ten years of Spanish medical exams

## Executive Summary
HEAD-QA v2 is a significantly expanded multilingual healthcare reasoning benchmark that grows from 6,765 to 12,751 questions across ten years of Spanish medical exams. The benchmark introduces new English, Italian, Galician, and Russian versions and evaluates multiple open-source LLMs using prompting, retrieval-augmented generation, and probability-based answer selection. The results demonstrate that model scale strongly drives performance, with Llama-3.1-70B achieving the highest accuracy (~84%) in English, while smaller models perform notably worse. Complex inference strategies such as Chain-of-Thought and RAG provide limited or no improvement over simpler approaches.

## Method Summary
The authors expanded HEAD-QA from 6,765 to 12,751 questions by incorporating a full decade of Spanish MIR exams (2009-2018). They created multilingual versions through machine translation and evaluated several open-source LLMs including Llama-3.1 models and Llama-3.2 variants. The evaluation employed multiple strategies: zero-shot and few-shot prompting, Chain-of-Thought reasoning, retrieval-augmented generation with external knowledge sources, and probability-based answer selection. Models were assessed using both question-level accuracy and exam-level scoring based on the Spanish MIR grading system.

## Key Results
- Model scale strongly correlates with performance, with Llama-3.1-70B achieving ~84% accuracy in English
- Complex inference strategies (Chain-of-Thought, RAG) provide minimal improvement over simpler approaches
- Translation quality is strong across all language versions, with back-translation scores indicating reliable multilingual outputs
- Smaller models perform significantly worse than larger ones, highlighting the importance of model capacity for healthcare reasoning

## Why This Works (Mechanism)
The strong performance correlation with model scale suggests that healthcare reasoning benefits from larger parameter counts and more extensive training data coverage. The limited improvement from complex inference strategies indicates that the benchmark questions can be solved effectively through direct pattern matching and knowledge retrieval, without requiring sophisticated reasoning chains. The multilingual success demonstrates that machine translation can preserve medical knowledge across languages when combined with sufficient model capacity.

## Foundational Learning
None

## Architecture Onboarding
None

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would frontier proprietary LLMs (GPT-4, Claude, Gemini) perform on HEAD-QA v2 compared to open-source models?
- Basis in paper: [explicit] The authors state in the Limitations: "This study did not include evaluations with frontier proprietary LLMs such as GPT-4, Claude, or Gemini, primarily due to funding resources to access APIs."
- Why unresolved: Financial constraints prevented API access; the paper only benchmarks open-access models up to 70B parameters.
- What evidence would resolve it: Running the same evaluation protocols (zero-shot, few-shot, CoT, RAG) on proprietary models and comparing accuracy and exam scores.

### Open Question 2
- Question: What is the impact of in-context example selection strategy on healthcare QA performance?
- Basis in paper: [explicit] The authors note: "Exploring the impact of example selection could be an interesting direction for future work" (Page 5), after observing that few-shot provided limited benefit over zero-shot.
- Why unresolved: Only one fixed set of three examples was tested; systematic variation of example choice, relevance, and diversity was not conducted.
- What evidence would resolve it: A controlled study varying example selection methods (random vs. semantic similarity vs. domain-matched) and measuring performance changes across models.

### Open Question 3
- Question: How do models perform on the Italian, Galician, and Russian translations of HEAD-QA v2?
- Basis in paper: [explicit] The authors released these language variants but state: "model evaluation was not conducted on these versions" and they "will be released...to serve as a foundation for future research on cross-lingual and multilingual evaluation" (Page 4).
- Why unresolved: Resource constraints prevented human validation and model evaluation on non-English translations; only back-translation quality metrics were computed.
- What evidence would resolve it: Running the full benchmark evaluation (all prompting strategies) on Italian, Galician, and Russian test sets and comparing cross-lingual performance gaps.

## Limitations
- The benchmark relies on Spanish medical exam questions, which may not generalize well to other healthcare domains or languages beyond the tested ones
- The study only evaluates a limited set of open-source models, leaving potential performance gaps for other architectures or proprietary systems
- The claim that Chain-of-Thought and RAG provide limited improvement is based on specific implementations and may not hold for alternative reasoning strategies or larger model families

## Confidence

Model scale and performance: High - The strong correlation between model size and accuracy is well-supported by the empirical results, particularly for Llama-3.1-70B.

Effectiveness of inference strategies: Medium - While results show limited gains from CoT and RAG, the analysis is constrained to specific configurations and may not capture all potential benefits.

Translation quality: Medium - Back-translation scores indicate reliability, but the evaluation does not fully address domain-specific accuracy or clinical validity.

## Next Checks
1. Test additional open-source and proprietary models, including larger parameter counts and newer architectures, to confirm the observed performance trends.

2. Conduct human evaluations of translation quality, focusing on medical terminology accuracy and contextual appropriateness across all supported languages.

3. Explore alternative reasoning strategies and prompt engineering techniques, including domain-specific fine-tuning, to assess potential improvements beyond model scale.