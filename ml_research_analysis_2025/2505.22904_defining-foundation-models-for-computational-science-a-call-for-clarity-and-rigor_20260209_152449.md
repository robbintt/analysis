---
ver: rpa2
title: 'Defining Foundation Models for Computational Science: A Call for Clarity and
  Rigor'
arxiv_id: '2505.22904'
source_url: https://arxiv.org/abs/2505.22904
tags:
- foundation
- dd-fem
- computational
- science
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of a rigorous, widely accepted definition
  of foundation models in computational science, a term increasingly used without
  clear criteria. The authors propose a formal definition centered on generality,
  reusability, and scalability, inspired by traditional foundational methods like
  FEM and FVM.
---

# Defining Foundation Models for Computational Science: A Call for Clarity and Rigor

## Quick Facts
- arXiv ID: 2505.22904
- Source URL: https://arxiv.org/abs/2505.22904
- Reference count: 40
- Primary result: Proposes formal definition of foundation models in computational science based on generality, reusability, and scalability; introduces DD-FEM framework demonstrating key characteristics

## Executive Summary
This paper addresses the lack of a rigorous, widely accepted definition of foundation models in computational science, a term increasingly used without clear criteria. The authors propose a formal definition centered on generality, reusability, and scalability, inspired by traditional foundational methods like FEM and FVM. They outline essential and desirable characteristics for such models, including wide generalization across problems, computational domains, and tasks without requiring retraining. The paper introduces the Data-Driven Finite Element Method (DD-FEM), a framework that integrates the modular structure of classical FEM with data-driven learning, demonstrating its potential to meet the proposed criteria through numerical examples showing spatial extrapolation, generalization across PDE types, and robustness in different physical regimes. This work provides a rigorous foundation for evaluating and developing future foundation models in computational science.

## Method Summary
The paper proposes a formal definition of foundation models in computational science based on three core characteristics: generality (wide applicability across problems), reusability (effectiveness without retraining), and scalability (independence of training data scale from global problem size). To demonstrate these principles, the authors introduce DD-FEM, which fuses classical FEM's modular structure with data-driven learning. The framework decouples data generation from global domain scale by training basis functions on small, computationally inexpensive subdomain simulations, then assembles these locally trained components for new geometries. This approach enables spatial extrapolation and generalization across different PDE types while maintaining physics consistency through residual minimization during global assembly.

## Key Results
- DD-FEM successfully extrapolates from 2x2 training configurations to 16x16 global domains while maintaining accuracy
- A single nonlinear manifold learned from both Poisson and Burgers equations generalizes across different PDE types
- The framework demonstrates spatial extrapolation and generalization across PDE classes without retraining

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Data Scale via Local Learning
- **Claim:** Training data-driven basis functions on small, localized subdomains enables the model to generalize to arbitrarily large global domains without retraining.
- **Mechanism:** The framework breaks the dependence between training data size and global problem size. Instead of learning a monolithic mapping for a specific geometry, the model learns the "physics" of a generic subdomain (element). These reusable components are assembled for new geometries, similar to how finite elements work.
- **Core assumption:** The solution manifold of the local subdomain is rich enough to represent the solution behavior in any valid global configuration.
- **Evidence anchors:**
  - [Section 4.1] "DD-FEM... decouple data generation from the scale of the global domain... training data can be generated from small, computationally inexpensive subdomain simulations."
  - [Section 5.2.1] Demonstrates spatial extrapolation from 2x2 training configurations to 16x16 or 10x10 global domains.
- **Break condition:** If local boundary conditions vary too drastically or interact non-locally (e.g., long-range forces not captured in the subdomain), the local basis will fail to represent the global solution.

### Mechanism 2: Physics Consistency via Governing Equation Assembly
- **Claim:** Enforcing governing equations (PDEs) during the global assembly phase ensures scientific consistency (e.g., conservation laws), distinguishing the framework from purely black-box surrogates.
- **Mechanism:** Unlike standard neural operators that approximate the input-output map directly, this approach uses data-driven bases to approximate the *trial space*. The final solution is obtained by solving the weak form of the PDE (e.g., via residual minimization) over the assembled domain.
- **Core assumption:** The data-driven bases span the solution space sufficiently for the numerical solver to converge to the correct physical solution.
- **Evidence anchors:**
  - [Abstract] "...fuses the modular structure of classical FEM... [demonstrating] physics consistency."
  - [Section 4.1] "DD-FEM promotes scientific consistency by numerically solving the governing equations... just as FEM does."
- **Break condition:** If the optimization/solver strategy (e.g., Static Condensation) is mismatched to the non-linearity of the problem, the enforcement of physics may fail or yield non-unique solutions.

### Mechanism 3: Manifold Generalization across PDE Classes
- **Claim:** A shared latent representation (nonlinear manifold) learned from diverse PDE data can generalize across distinct physical regimes (e.g., elliptic vs. hyperbolic).
- **Mechanism:** By training a single autoencoder or compression model on snapshots from different PDE types (Poisson + Burgers), the model learns a "universal" basis capable of representing distinct dynamical behaviors.
- **Core assumption:** Disparate physical phenomena share latent structural similarities that can be captured by a single neural decoder.
- **Evidence anchors:**
  - [Section 5.2.3] "A neural decoder network was trained using a combination of 4,000 snapshots from 2D Poisson... and 101,000 snapshots from... Burgers."
  - [Section 5.2.3] "Demonstrates the capability... to generalize across different types of PDEs."
- **Break condition:** If the dominant frequencies or characteristic speeds of test PDEs lie outside the spectral content of the training PDEs, the shared basis will lack the expressivity to represent the new physics.

## Foundational Learning

- **Concept:** **Variational Formulation (Weak Form)**
  - **Why needed here:** DD-FEM does not simply predict outputs; it minimizes residuals. Understanding how to derive the weak form of a PDE is required to implement the "Global Assembly" and "Solve" steps.
  - **Quick check question:** Can you convert the strong form of the Poisson equation ($-\nabla^2 u = f$) into its weak integral form?

- **Concept:** **Dimensionality Reduction (POD/Autoencoders)**
  - **Why needed here:** The "Local Learning" phase relies on compressing high-fidelity snapshots into lower-dimensional bases. You must understand SVD (linear) or Autoencoders (nonlinear) to construct the "data-driven elements."
  - **Quick check question:** What is the trade-off between using a linear subspace (SVD) vs. a nonlinear manifold (Autoencoder) for representing convective flow features?

- **Concept:** **Domain Decomposition**
  - **Why needed here:** The entire scalability argument rests on partitioning a global domain into independent subdomains. You need to understand how interface conditions (continuity) are handled between subdomains.
  - **Quick check question:** How does a Discontinuous Galerkin (DG) method enforce continuity between elements differently than a Continuous Galerkin (CG) method?

## Architecture Onboarding

- **Component map:** Data Generator -> Local Basis Trainer -> Global Assembler -> Physics Solver
- **Critical path:** Generating a diverse enough local snapshot database → Training a robust local basis → Implementing the global assembly interface logic
- **Design tradeoffs:**
  - **Linear vs. Nonlinear Bases:** Linear (SVD) is faster and mathematically rigorous but may fail for advection-dominated problems. Nonlinear (Neural) is more expressive but harder to train and less stable.
  - **Assembly Strategy:** Static Condensation (Null-Space) is efficient but rigid; Residual Minimization (Full-Space) is flexible but computationally heavier per degree of freedom.
- **Failure signatures:**
  - **Homogenization artifacts:** The model predicts smooth/averaged solutions that satisfy the PDE but miss localized shocks or boundary layers (underfitting in local bases).
  - **Assembly divergence:** The global solver fails to converge because the data-driven basis creates an ill-conditioned global system.
- **First 3 experiments:**
  1. **Local Basis Fidelity:** Train a basis on a 1D/2D element with varying boundary conditions; test reconstruction error on unseen boundary conditions.
  2. **Spatial Extrapolation:** Assemble a 10x10 global domain from a basis trained only on a 2x2 domain for a steady-state diffusion problem.
  3. **Source Generalization:** Train on sinusoidal source terms; test the assembled model on a spiral or point-source (delta-like) RHS (replicating Section 5.2.2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a domain-specific model (e.g., limited to fluid mechanics) qualify as a foundation model if it exhibits sufficient generalization within that domain, or must it span multiple physical domains?
- Basis in paper: [explicit] Section 10 asks, "Can a domain-specific model... still qualify as a foundation model if it exhibits sufficient generalization and reusability within that domain?"
- Why unresolved: The proposed definition requires "broad distribution of scientific application types," but the boundary between intra-domain breadth and cross-domain breadth remains undefined.
- What evidence would resolve it: Community consensus on a taxonomy distinguishing "domain-specific" from "general" foundation models in science.

### Open Question 2
- Question: Can classical Finite Element Method (FEM) and domain decomposition theory be extended to provide rigorous convergence and stability guarantees for Data-Driven Finite Element Methods (DD-FEM)?
- Basis in paper: [explicit] Section 11 states, "Can we extend classical FEM and domain decomposition theory to rigorously characterize convergence, stability, and error propagation in DD-FEM?"
- Why unresolved: Replacing polynomial bases with data-driven bases introduces new approximation errors and sources of instability not covered by standard FEM theory.
- What evidence would resolve it: Mathematical proofs of well-posedness, stability, and a priori error estimates for systems utilizing data-driven bases.

### Open Question 3
- Question: Should strict adherence to scientific consistency (e.g., conservation laws, symmetries) be a mandatory requirement for a model to be considered a foundation model in computational science?
- Basis in paper: [explicit] Section 10 asks, "Should scientific consistency... be a strict requirement, or can foundation models be effective even when violating known physics?"
- Why unresolved: It is currently unclear if empirical generalization capabilities can substitute for strict physical validity in high-stakes scientific applications.
- What evidence would resolve it: Comparative studies showing whether models lacking explicit physical constraints fail to generalize or produce unphysical results in safety-critical extrapolation tasks.

## Limitations
- The framework's scalability claims hinge on the assumption that local subdomain physics are representative of global behavior, which may break down for problems with long-range interactions or strong non-local coupling
- Numerical examples focus on relatively simple PDE types; performance on complex multiphysics or transient problems remains unverified
- Computational cost of generating diverse local snapshots and training the global solver is not fully characterized

## Confidence
- **High Confidence:** The conceptual framework for defining foundation models in computational science and the modular DD-FEM architecture are well-articulated and internally consistent
- **Medium Confidence:** The mechanism for decoupling data scale via local learning is logically sound, but its universal applicability across all PDE types is not yet proven
- **Medium Confidence:** The physics consistency mechanism is valid for enforcing governing equations, but the practical limitations of the chosen solver (e.g., Static Condensation) are not fully explored
- **Low Confidence:** The claim of manifold generalization across disparate PDE classes (e.g., elliptic to hyperbolic) is demonstrated on a very limited dataset and requires significantly more rigorous testing

## Next Checks
1. **Robustness Test:** Evaluate DD-FEM on a problem with strong non-local coupling (e.g., advection-dominated flow or wave propagation) where local subdomain physics are insufficient to capture global behavior
2. **Scalability Audit:** Characterize the full computational pipeline cost (local data generation + basis training + global assembly + solve) for increasing global domain sizes and compare it to a traditional FEM solver
3. **Basis Expressivity Test:** Systematically compare the reconstruction error and prediction accuracy of linear (SVD) vs. nonlinear (Autoencoder) bases across a spectrum of PDE problems, from diffusion-dominated to convection-dominated, to identify failure modes of each approach