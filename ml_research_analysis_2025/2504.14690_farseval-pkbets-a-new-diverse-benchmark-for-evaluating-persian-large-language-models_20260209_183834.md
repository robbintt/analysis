---
ver: rpa2
title: 'FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language
  models'
arxiv_id: '2504.14690'
source_url: https://arxiv.org/abs/2504.14690
tags:
- questions
- language
- persian
- benchmark
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FarsEval-PKBETS, a Persian language benchmark
  designed to evaluate large language models (LLMs) across diverse domains and tasks.
  It includes 4,000 questions and answers in multiple formats (multiple-choice, short-answer,
  and descriptive) covering medicine, law, religion, Persian language, encyclopedic
  knowledge, human preferences, social knowledge, ethics, bias, text generation, and
  cultural rights.
---

# FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models

## Quick Facts
- arXiv ID: 2504.14690
- Source URL: https://arxiv.org/abs/2504.14690
- Reference count: 40
- Models tested achieved <50% average accuracy, demonstrating benchmark difficulty

## Executive Summary
FarsEval-PKBETS is a comprehensive Persian language benchmark designed to evaluate large language models across diverse domains and tasks. The benchmark includes 4,000 questions and answers in multiple formats (multiple-choice, short-answer, and descriptive) covering medicine, law, religion, Persian language, encyclopedic knowledge, human preferences, social knowledge, ethics, bias, text generation, and cultural rights. Developed with domain experts, it incorporates Persian linguistic and cultural nuances to create a challenging evaluation platform. Three Persian LLMs (Llama3-70B, PersianMind, and Dorna) were tested, achieving an average accuracy below 50%, demonstrating the benchmark's difficulty and the current limitations of Persian language models.

## Method Summary
The benchmark was developed using human expert annotators across multiple domains, with questions created and reviewed through a custom platform called Saba. The dataset contains 4,000 records in three formats: 2,300 multiple-choice questions, 500 short-answer questions, and 1,200 descriptive questions. Models were evaluated via API integration with Saba, and responses were assessed using both automated metrics and human evaluation. Accuracy was measured using three labels: Correct, Semi-correct, and Wrong, with dual scoring for strict accuracy (correct only) and lenient accuracy (correct + semi-correct).

## Key Results
- Three Persian LLMs (Llama3-70B, PersianMind, Dorna) achieved average accuracy below 50%
- Models selected correct multiple-choice options in 43% of cases but failed to provide fully correct justifications
- Benchmark covers 12+ domains with varying sample sizes (50-1,200 questions per category)
- Descriptive questions revealed deeper understanding gaps not captured by multiple-choice formats

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating culturally specific linguistic ambiguities and local knowledge exposes gaps in general-purpose LLMs that standard translated benchmarks miss.
- **Mechanism:** By designing questions around Iranian laws, religious nuances (Shia Islam), and Persian-specific wordplay (e.g., "foots" meaning death vs. birthday), the benchmark forces the model to rely on deep semantic understanding rather than pattern matching from English-centric training data.
- **Core assumption:** The model's pre-training distribution lacks sufficient density of high-quality, culturally localized Persian text, causing failures in reasoning tasks.
- **Evidence anchors:** [abstract] Mentions the benchmark "incorporates linguistics, cultural, and local considerations relevant to the Persian language and Iran." [section] Page 5 cites an example where the model must distinguish "The grandfather has passed away" from "The grandfather blew out his birthday cake" (both using "foot" in Persian) to solve an ethics question.

### Mechanism 2
- **Claim:** Heterogeneous evaluation formats (Multiple-Choice, Short-Answer, Descriptive) prevent models from inflating scores via test-taking heuristics.
- **Mechanism:** Moving beyond Multiple-Choice Questions (MCQ) to descriptive answers eliminates the ability to guess correctly (25% baseline) or select an option while providing contradictory reasoning.
- **Core assumption:** Accurate answer selection in MCQs does not correlate perfectly with accurate generative capability or reasoning validity.
- **Evidence anchors:** [abstract] Highlights "various formats, including multiple choice, short answer and descriptive responses." [section] Page 6 notes that in an experiment, "in 43% of cases, the model failed to provide a fully correct justification for their accurate choice."

### Mechanism 3
- **Claim:** Expert-driven data validation ensures the "ground truth" is culturally and factually accurate, minimizing false negatives caused by ambiguous or erroneous reference answers.
- **Mechanism:** Domain experts (doctors, lawyers) review questions to ensure difficulty stems from complexity, not ambiguity. This filters out noise that might otherwise penalize a model unfairly.
- **Core assumption:** General annotators or synthetic data pipelines cannot reliably generate high-validity questions for specialized domains like "Complementary & Alternative Medicine" or "Constitution of IRI."
- **Evidence anchors:** [abstract] States the benchmark "was developed with human experts." [section] Page 5 describes "Utilizing expert annotators... In the medical domain, a team of medical students and professors supervised the question generation."

## Foundational Learning

- **Concept: Persian Morphology & Ambiguity**
  - **Why needed here:** The benchmark specifically exploits Persian language features like homographs (e.g., "foot" meaning death vs. blow) and complex verb conjugation to test true understanding.
  - **Quick check question:** Can you explain why a sub-word tokenizer might struggle with the Persian word "پدربزرگ" (grandfather) combined with "فوت" (death/blow) compared to a space-delimited language?

- **Concept: Evaluation Bias in MCQs**
  - **Why needed here:** The paper explicitly demonstrates that models often select the right option for the wrong reason, requiring an understanding of why MCQ accuracy is a "lower bound" for capability.
  - **Quick check question:** If a model achieves 60% accuracy on a 4-option MCQ test, how would you determine if this is due to knowledge or random guessing, and why does adding a "descriptive" task solve this?

- **Concept: Cultural Alignment vs. Translation**
  - **Why needed here:** Many Persian benchmarks are translated from English; this paper argues for *native* data creation to capture local laws, religion, and social norms.
  - **Quick check question:** Why would a question about "Constitutional Law" fail if translated directly from a US benchmark to a Persian context, regardless of translation quality?

## Architecture Onboarding

- **Component map:** Expert annotators (Medicine, Law, etc.) & Standardized Exams -> Saba (custom annotation platform for data submission, review, and revision) -> 4,000 Records (Question, Reference Answer, Metadata, Category) -> Llama3-70B, PersianMind, Dorna (Evaluated via API in Saba) -> Human review + Automated accuracy metrics (Correct vs. Semi-correct vs. Wrong)

- **Critical path:** 1. Question Creation (Expert) -> 2. Expert Review (Rejection/Revision loop in *Saba*) -> 3. Model Inference (API call) -> 4. Evaluation (Human/Auto comparison)

- **Design tradeoffs:**
  - **Breadth vs. Depth:** 4,000 samples across 20+ categories provides broad coverage but limits statistical power per category (e.g., Emergency Medicine has only 50 samples)
  - **Static vs. Dynamic:** The benchmark is fixed (to prevent data leakage), implying it will eventually "expire" as models improve, unlike an open-ended "arena" style evaluation

- **Failure signatures:**
  - **High MCQ / Low Descriptive:** Indicates surface-level pattern matching without reasoning (the "43% justification failure" cited in the paper)
  - **Domain Collapse:** Scores near 0% in "Poems & Lyrics" or "Lexical Semantics" suggest the model's tokenizer or training data failed to capture Persian stylistic nuances

- **First 3 experiments:**
  1. **Baseline vs. RAG:** Test Llama3-70B on the "Law" and "Medicine" categories with and without a Retrieval-Augmented Generation (RAG) system hooked to Iranian legal/medical codes to measure knowledge retrieval gaps
  2. **Justification Consistency Check:** For the MCQ subset, force the model to generate a reasoning chain *before* selecting an option to see if the 43% inconsistency rate improves (Chain-of-Thought prompting)
  3. **Cross-Category Transfer:** Fine-tune a smaller model (e.g., Dorna) on the "Encyclopedic Knowledge" subset and evaluate performance on "Social Knowledge" to test if general cultural knowledge transfers to social reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can automated evaluation effectively assess the reasoning consistency of Persian LLMs beyond simply checking the selected answer option?
- **Basis in paper:** [Explicit] The authors found that in 43% of cases, models selected the correct multiple-choice option but failed to provide a fully correct justification for their choice.
- **Why unresolved:** The paper introduces descriptive questions to mitigate this but relies on human review via the "Saba" platform, lacking a proposed scalable automated solution for validating reasoning.
- **What evidence would resolve it:** A new metric or evaluation protocol that correlates strongly with human judgment of reasoning quality on the descriptive subset of FarsEval-PKBETS.

### Open Question 2
- **Question:** Does high performance on Persian linguistic tasks (e.g., grammar) correlate with competence in Iranian cultural and local contexts?
- **Basis in paper:** [Inferred] The authors note that current models "struggle with responding to questions within Iranian cultural and local contexts," yet the benchmark separates "Persian Language" from cultural categories like "Social Knowledge."
- **Why unresolved:** The paper reports low average scores overall but does not provide an analysis of the correlation between linguistic proficiency and cultural understanding.
- **What evidence would resolve it:** A correlation analysis of model scores on the "Persian Language" category versus scores on "Social Knowledge," "Religion," and "Encyclopedic Knowledge."

### Open Question 3
- **Question:** Can the "Semi-correct" label introduced in this benchmark serve as a robust intermediate metric for tracking model improvements?
- **Basis in paper:** [Explicit] The technical validation introduces a "Semi-correct" classification for responses with minor errors (e.g., correct text but wrong option identifier) and reports dual accuracy scores.
- **Why unresolved:** The definition relies on specific error types, and it is unclear if this intermediate state effectively guides model fine-tuning or merely adds noise to the evaluation.
- **What evidence would resolve it:** Demonstration that models consistently transition from "semi-correct" to "correct" through targeted fine-tuning, validating the metric's utility for optimization.

## Limitations
- Evaluation relies heavily on human judgment for descriptive answers, creating potential consistency issues across evaluators
- Uneven question distribution across domains (ranging from 50 to 1,200 questions per category) limits statistical power for domain-specific conclusions
- Fixed benchmark nature means it may become saturated as Persian LLMs improve, requiring periodic updates to maintain difficulty

## Confidence
- **High Confidence (8/10):** The claim that FarsEval-PKBETS provides a challenging evaluation platform for Persian LLMs is well-supported by the reported sub-50% average accuracy across three diverse models
- **Medium Confidence (6/10):** The assertion that the benchmark uniquely captures Persian cultural nuances and exposes LLM limitations is plausible but relies on comparison to unspecified existing benchmarks
- **Low Confidence (4/10):** The paper's claim about "comprehensive" coverage across 12+ domains is difficult to verify given the uneven distribution of questions

## Next Checks
1. **Inter-annotator Reliability Analysis:** Conduct a statistical analysis of human evaluator agreement rates for descriptive answers across multiple domains to quantify the consistency and potential bias in the "semi-correct" grading system

2. **Cross-Benchmark Comparison Study:** Systematically compare model performance on FarsEval-PKBETS versus other Persian benchmarks (PersianMedQA, PARSE, PBBQ) to empirically validate claims about unique cultural and linguistic coverage

3. **Domain-Specific Power Analysis:** Perform statistical power calculations for each domain to determine whether the current sample sizes (particularly in smaller categories like Emergency Medicine with 50 questions) provide sufficient statistical power to detect meaningful performance differences between models