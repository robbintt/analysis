---
ver: rpa2
title: Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit
  Schemes
arxiv_id: '2504.08074'
source_url: https://arxiv.org/abs/2504.08074
tags:
- policy
- time
- tolling
- toll
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dynamic tolling in tradable
  credit schemes (TCS) for traffic management. The core method involves formulating
  day-to-day dynamic tolling as a Markov Decision Process and solving it using reinforcement
  learning algorithms, specifically Proximal Policy Optimization (PPO).
---

# Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit Schemes

## Quick Facts
- **arXiv ID:** 2504.08074
- **Source URL:** https://arxiv.org/abs/2504.08074
- **Reference count:** 9
- **One-line primary result:** RL approach achieves travel times and social welfare comparable to Bayesian optimization benchmarks while demonstrating generalization across different capacity and demand scenarios.

## Executive Summary
This paper addresses dynamic tolling in tradable credit schemes for traffic management by formulating day-to-day tolling as a Markov Decision Process and solving it using reinforcement learning, specifically Proximal Policy Optimization (PPO). The method achieves travel time and social welfare comparable to Bayesian optimization benchmarks while demonstrating generalization across different capacity and demand scenarios. The approach uses Gaussian toll profile parameterization and introduces CAPS regularization to address action oscillation issues in continuous control, producing smoother and more practical tolling strategies.

## Method Summary
The paper formulates day-to-day dynamic tolling as a finite-horizon MDP with a 60-day horizon, where the agent learns to adjust toll profile parameters (amplitude M, peak time μ, spread σ) rather than setting absolute toll values. The RL agent uses PPO with MLP policy and critic networks, augmented with CAPS regularization to smooth actions across consecutive states and nearby spatial regions. The environment simulates a tradable credit scheme with 7,500 travelers, using a Macroscopic Fundamental Diagram (MFD) model for traffic supply and logit-mixture models for demand. The reward balances average individual travel time reduction with public transit mode share incentives.

## Key Results
- RL approach achieves travel times and social welfare comparable to Bayesian optimization benchmarks
- Generalizes across different capacity (90-110%) and demand scenarios
- CAPS regularization effectively mitigates action oscillation, producing smoother toll strategies
- Transfer learning works well for 90-110% scenarios but may require more tuning for edge cases

## Why This Works (Mechanism)

### Mechanism 1: MDP Formulation for Sequential Toll Adjustment
The day-to-day tolling problem is formulated as a finite-horizon MDP where the agent adjusts toll parameters based on current traffic state. The state vector captures departure flows, token prices, and current toll parameters, while actions are adjustments to toll profile parameters. This sequential decision-making approach adapts to evolving traffic states rather than optimizing for a fixed equilibrium.

### Mechanism 2: Gaussian Toll Profile Parameterization
Toll profiles are parameterized as Gaussian distributions (T(h|M, μ, σ) = M·e^(-(t_h-μ)²/2σ²)) rather than setting tolls for each time interval independently. This reduces the action space dimensionality while maintaining temporal structure, constraining solutions to smooth, bell-shaped profiles aligned with peak congestion.

### Mechanism 3: CAPS Regularization for Policy Smoothness
CAPS regularization adds temporal and spatial smoothness penalties to the PPO objective to mitigate action oscillation. The modified objective penalizes large action differences between consecutive states and for nearby states, with L1-norm temporal and L2-norm spatial regularization yielding the best results for practical tolling strategies.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The entire framework treats tolling as sequential decision-making; understanding states, actions, transitions, rewards, and discount factors is essential. Quick check: Can you explain why the discount factor γ is set to 1 in this paper, and what that implies about the optimization objective?

- **Proximal Policy Optimization (PPO)**: PPO is the core RL algorithm; understanding the clipped surrogate objective, advantage estimation, and policy/value network roles is necessary for implementation and debugging. Quick check: What does the clip range ε=0.2 constrain, and why might a larger value cause training instability?

- **Day-to-Day Traffic Dynamics**: The environment simulates how travelers learn and adapt over days; understanding Cantarella-Cascetta learning, MFD supply models, and TCS market mechanics is required to interpret state transitions. Quick check: How does the toll level affect traveler perceptions of travel time in the extended learning model (Equation 15)?

## Architecture Onboarding

- **Component map:**
```
Environment (5 modules):
├── Demand (logit-mixture mode/departure choice)
├── Supply (trip-based MFD model)
├── Market (token trading)
├── Regulator (token allocation, toll profile, price adjustment)
└── Day-to-day learning (perceived travel time updates)

RL Agent:
├── Policy network: [146] → Tanh[8] → Tanh[8] → Linear[3] (outputs action deltas)
├── Critic network: [146] → Tanh[8] → Tanh[8] → Linear[1] (value estimation)
└── PPO optimizer with CAPS regularization
```

- **Critical path:**
1. Initialize toll parameters M, μ, σ and token price p
2. For each day d: simulate demand → supply → market → compute reward
3. Collect rollouts (60 days × 10-32 parallel environments)
4. Update policy/value networks using PPO with clipped objective + CAPS penalties
5. Repeat until convergence (~100-200 episodes)

- **Design tradeoffs:**
  - 1D vs 3D action space: 1D (amplitude only) is more stable but less flexible; 3D enables full profile control but requires more tuning and regularization
  - Batch size: 960 (half episode) works best; 480 causes oscillatory convergence, 1920 slows training
  - Epoch number: 16 epochs stable; 32 epochs with moderate batch size causes oscillation
  - Regularization type: L1 temporal + L2 spatial smoothness yields higher rewards; different combinations produce qualitatively different toll strategies

- **Failure signatures:**
  - Oscillating tolls: Toll amplitude swings between 3.32-3.80 daily without convergence → insufficient regularization or too many epochs
  - Suboptimal convergence to high tolls: Policy locks into high amplitude (~5+) with low reward → batch size too small
  - Slow convergence with high variance: Wide shaded regions in AITT/reward plots → batch size too large
  - Token price instability: Price fluctuates outside $1.15-$1.35 range → market threshold K̄ may need adjustment

- **First 3 experiments:**
  1. Replicate 1D baseline: Train PPO with amplitude-only action, batch_size=2400, n_epochs=10, no regularization. Compare AITT and social welfare against BO benchmark in Table 3.
  2. Ablate regularization: In 3D action space with batch_size=480 (oscillation-prone setting), test each CAPS variant (TL1, TL2, SL1, SL2) individually with λ_T=λ_S=10⁻⁴. Measure reduction in toll variance and reward improvement.
  3. Transfer learning stress test: Train on baseline capacity/demand, then evaluate on 80% and 120% scenarios (beyond the 90-110% tested). Identify the breaking point where transferred policy degrades significantly vs. learn-from-scratch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed deep reinforcement learning framework effectively scale to large, realistic urban networks while maintaining computational efficiency?
- **Basis in paper:** The authors explicitly identify "scaling to large networks" as a potential challenge and suggest future research should address "environments with more realistic modeling of travel behavior, public transportation, and network topology."
- **Why unresolved:** The current study utilizes a simplified single-reservoir Macroscopic Fundamental Diagram (MFD) model rather than a complex, large-scale network topology.
- **What evidence would resolve it:** Successful application and convergence of the RL agent in simulations using city-scale, multi-modal transportation networks.

### Open Question 2
- **Question:** Why do "learn-from-scratch" policies occasionally yield lower rewards than transferred policies in specific capacity and demand scenarios (e.g., 110% capacity)?
- **Basis in paper:** The results section describes the performance of the learn-from-scratch policy in the 110% capacity scenario as "counterintuitive," noting it "could indicate that more hyperparameter tuning is required."
- **Why unresolved:** The paper observes the anomaly but does not determine if the cause is local optima, insufficient exploration, or specific hyperparameter sensitivity in lower-congestion regimes.
- **What evidence would resolve it:** An ablation study analyzing the learning dynamics and hyperparameter sensitivity of agents trained from scratch specifically in these counterintuitive scenarios.

### Open Question 3
- **Question:** How does the choice of regularization norm (L1 vs. L2) systematically impact policy smoothness and convergence stability across different traffic dynamics?
- **Basis in paper:** The authors note that different regularization techniques produce "qualitatively different" strategies and that their "performance may be context-specific," implying the optimal choice is not yet understood.
- **Why unresolved:** The paper observes distinct behaviors (e.g., L1 temporal smoothness vs. L2 spatial smoothness) but lacks a theoretical explanation for why one might be superior for specific traffic management goals.
- **What evidence would resolve it:** A comparative analysis of policy stability and oscillation frequency using L1 versus L2 regularization across a wider variety of stochastic demand profiles.

## Limitations
- Gaussian toll profile parameterization may not capture optimal strategies requiring multi-modal or asymmetric profiles
- Generalization claims not tested at extreme conditions (beyond 90-110% capacity/demand)
- Real-world applicability depends on assumptions about market stability and computational feasibility that are not empirically validated
- MDP formulation assumes Markovian transitions that may not hold under market volatility or strategic behavior

## Confidence

- **High Confidence:** MDP formulation and PPO implementation details are well-specified and reproducible; CAPS regularization mechanism and its effects are clearly demonstrated
- **Medium Confidence:** Generalization across capacity/demand scenarios is supported but lacks stress-testing at extreme conditions; Gaussian parameterization is plausible but not rigorously validated
- **Low Confidence:** Real-world applicability depends heavily on assumptions about market stability, traveler behavior consistency, and computational feasibility that are not empirically validated beyond the synthetic environment

## Next Checks

1. **Stress Test Generalization Boundaries:** Train the RL policy on the baseline scenario, then systematically evaluate performance at 80%, 90%, 100%, 110%, and 120% of baseline capacity and demand. Measure AITT degradation, reward collapse points, and compare against policies trained specifically for each scenario.

2. **Validate Gaussian Parameterization Sufficiency:** Implement a more flexible toll profile representation (e.g., piecewise linear or multi-modal Gaussian mixture) and train RL policies using the same CAPS framework. Compare AITT, reward, and convergence speed against the single-Gaussian approach.

3. **Market Stability Sensitivity Analysis:** Systematically vary the market threshold K̄ and token price bounds to test policy robustness to market volatility. Introduce artificial market shocks and measure policy adaptation speed, AITT recovery time, and market price stability.