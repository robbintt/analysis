---
ver: rpa2
title: 'LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language
  Model Hallucinations with External Knowledge'
arxiv_id: '2504.21132'
source_url: https://arxiv.org/abs/2504.21132
tags:
- data
- tools
- have
- agent
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the LLM-ENHANCER system, a merged approach
  using vector embeddings to reduce hallucinations in large language models by integrating
  external knowledge from sources like Google, Wikipedia, and DuckDuckGo. The system
  operates in parallel, combining data from multiple sources, splitting it into chunks,
  and using vector embeddings to identify the most relevant information for the LLM.
---

# LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge

## Quick Facts
- **arXiv ID**: 2504.21132
- **Source URL**: https://arxiv.org/abs/2504.21132
- **Reference count**: 22
- **Primary result**: 5.2% F1 improvement over sequential methods, 32.4% over GPT-3.5 Turbo on recent data

## Executive Summary
This paper introduces LLM-ENHANCER, a system that reduces hallucinations in large language models by integrating external knowledge from multiple web sources in parallel. The approach queries Google, Wikipedia, and DuckDuckGo simultaneously, merges the results, chunks the text, and uses vector embeddings to identify the most relevant information for the LLM. The system achieves significant improvements in factual accuracy while preserving response naturalness, particularly for open-source models like Mistral 7B without requiring extensive retraining.

## Method Summary
LLM-ENHANCER operates by querying multiple search APIs (Google via SerpAPI, DuckDuckGo, and Wikipedia) in parallel rather than sequentially. The retrieved text is merged into a single corpus, split into chunks (400 characters with 100 overlap), and embedded using all-MiniLM-L6-v2. The top-10 most relevant chunks are retrieved via ChromaDB and injected into the LLM prompt. The system uses Mistral 7B through GPT4All with LangChain agents, and includes an automated Answer Comparator using LLM and sentiment analysis for evaluation. The approach was tested on WikiQA (369 questions) and a custom 2023-2024 dataset (500 questions).

## Key Results
- 5.2% improvement in F1 score over sequential tool usage methods
- 32.4% improvement over GPT-3.5 Turbo on recent data (2023-2024)
- Accuracy improvement from 0.58 to 0.77 F1 score on WikiQA dataset (19% gain)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel multi-source retrieval with merged vector selection improves factual accuracy over sequential tool selection.
- Mechanism: The system queries Google, Wikipedia, and DuckDuckGo simultaneously rather than letting an agent choose one tool sequentially. Results are combined, chunked, embedded, and the top-k most relevant chunks are retrieved via ChromaDB to ground the LLM's answer.
- Core assumption: Merging multiple web sources reduces single-source bias and retrieval errors; vector similarity reliably surfaces the most factual chunks.

### Mechanism 2
- Claim: Grounding generation in retrieved chunks reduces hallucination risk compared to parametric-only generation.
- Mechanism: Retrieved chunks are injected into the prompt so the LLM conditions on external evidence. An automated Answer Comparator evaluates whether generated answers contain the reference answer.
- Core assumption: The LLM will attend to and correctly synthesize provided evidence; the comparator reliably detects semantic containment.

### Mechanism 3
- Claim: Parallel execution with merged retrieval yields lower latency than sequential tool-calling for comparable coverage.
- Mechanism: Sequential agents invoke tools one-by-one, each step consuming reasoning tokens and I/O time. Parallel calls + embedding retrieval reduce orchestration overhead and reasoning steps.
- Core assumption: Embedding overhead is lower than sequential agent reasoning/decision overhead.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The LLM-ENHANCER is a RAG variant that fetches live web data and injects top chunks into the prompt.
  - Quick check question: Can you explain how RAG differs from fine-tuning for knowledge updates?

- **Concept: Vector embeddings + semantic search**
  - Why needed here: Chunks are embedded and retrieved via ChromaDB; understanding similarity search is essential.
  - Quick check question: What does it mean for two texts to be "close" in embedding space?

- **Concept: Agent tool orchestration**
  - Why needed here: The system replaces sequential agent decisions with parallel tool calls; understanding agents clarifies what's being optimized.
  - Quick check question: How does a ReAct agent decide which tool to call next?

## Architecture Onboarding

- **Component map**: User query → ZeroShot ReAct Agent → Parallel Merged Tool (Google, Wikipedia, DuckDuckGo) → RecursiveCharacterTextSplitter → all-MiniLM-L6-v2 embeddings → ChromaDB → Top-10 chunks → Mistral 7B prompt → Answer → Answer Comparator

- **Critical path**: Parallel tool calls → merge text → chunk → embed → ChromaDB query → prompt injection → generation. Embedding and retrieval occur per query, creating latency.

- **Design tradeoffs**:
  - Parallel merging vs. sequential agents: Faster, more comprehensive, but higher token usage and embedding overhead
  - Chunk parameters (400/100): Smaller chunks increase precision but may split context; larger chunks increase token load
  - Open-source LLM (Mistral 7B) vs. proprietary: Cost/privacy benefits, but lower raw capability than GPT-3.5/4

- **Failure signatures**:
  - Slow responses (>2 min) → embedding bottleneck or large merged text; optimize chunk count or cache embeddings
  - Wrong answers despite retrieval → chunks may not contain answer; inspect top-k chunks for relevance
  - Random tool selection (legacy) → ensure merged tool class wraps all sources; disable sequential agent fallback

- **First 3 experiments**:
  1. Ablate single sources: Run Google-only, Wikipedia-only, DuckDuckGo-only, and merged; compare F1 and latency on 100-sample subset
  2. Tune chunking: Test (size=400, overlap=100) vs (size=800, overlap=200); measure retrieval relevance and F1
  3. Swap embedding model: Compare all-MiniLM-L6-v2 vs a larger encoder on retrieval quality and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal chunk size and overlap configurations for maximizing F1-score in the LLM-ENHANCER retrieval pipeline?
- Basis: Future work states chunk size and overlapping can be configured to check performance metrics
- Why unresolved: Only fixed parameters (chunk_size=400, chunk_overlap=100) were tested without systematic ablation
- Evidence needed: Grid search or Bayesian optimization over chunk sizes (100-1000 tokens) and overlap ratios, evaluated on both datasets

### Open Question 2
- Question: Can alternative embedding models reduce vectorization latency while maintaining or improving retrieval relevance?
- Basis: Future work states better accuracy can be achieved with more advanced vector embedding models
- Why unresolved: Only one embedding model was tested; no comparison of embedding quality vs. speed trade-offs
- Evidence needed: Benchmarking 3-5 embedding models on retrieval precision@k and embedding latency per document

### Open Question 3
- Question: Does the LLM-based Answer Comparator with sentiment analysis produce evaluation scores consistent with human expert judgments?
- Basis: The evaluation methodology relies on automated pipeline; prior work noted lack of human evaluation as limitation
- Why unresolved: No human annotation baseline was established to validate the automated comparator's reliability
- Evidence needed: Correlation analysis between automated comparator outputs and human expert ratings on 100-200 QA pairs

## Limitations
- Higher token usage and processing time due to vector embedding operations (170+ tokens vs 20 for sequential)
- Implementation details missing (exact prompt templates and threshold logic for sentiment analysis)
- Dataset specificity with small sample sizes (369 and 500 questions) limiting generalizability

## Confidence

- **High confidence**: Parallel retrieval mechanism and basic architecture are technically sound; reported latency improvements are reliable
- **Medium confidence**: F1-score improvements are supported by described methodology but need external validation; Answer Comparator effectiveness is plausible but untested
- **Low confidence**: Generalizability to other domains, robustness to noisy content, and scalability to larger knowledge bases remain uncertain

## Next Checks

1. **Ablation study across multiple domains**: Test on at least three additional diverse datasets (medical, technical, creative) with 500+ questions each to assess generalizability

2. **Human evaluation of hallucination reduction**: Conduct blinded human evaluations comparing LLM-ENHANCER outputs against sequential retrieval and no-retrieval baselines on 100+ sample questions

3. **Scaling analysis**: Evaluate performance (accuracy, latency, token usage) as corpus size scales from 1K to 100K documents, testing impact of varying chunk sizes (200, 400, 800 characters) and retrieved chunk counts (5, 10, 20)