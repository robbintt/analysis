---
ver: rpa2
title: 'CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large
  Language Models'
arxiv_id: '2502.15932'
source_url: https://arxiv.org/abs/2502.15932
tags:
- vulnerability
- dataset
- comment
- evaluation
- cybersecurity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CVE-LLM, a large language model system for
  automating vulnerability evaluation in medical device cybersecurity. The system
  uses historical evaluation data from Siemens Healthineers to train an LLM that can
  assess the impact of new vulnerabilities on medical devices, performing tasks like
  generating internal and customer comments, determining affected status, and calculating
  CVSS vectors.
---

# CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models

## Quick Facts
- arXiv ID: 2502.15932
- Source URL: https://arxiv.org/abs/2502.15932
- Reference count: 14
- Primary result: Large language model system achieving 94-96% accuracy in automating medical device vulnerability assessment

## Executive Summary
CVE-LLM is a large language model system designed to automate vulnerability evaluation for medical devices. The system processes vulnerability notifications and generates required outputs including internal and customer-facing comments, affected status determinations, and CVSS environmental vectors. By leveraging historical evaluation data from Siemens Healthineers, CVE-LLM performs these tasks with high accuracy while operating 50-100 times faster than human experts. The approach combines domain-adaptive pretraining on vulnerability descriptions with instruction tuning that incorporates cybersecurity ontology knowledge, specifically CAPEC prerequisites, severity levels, and mitigations from the SEPSES knowledge graph.

## Method Summary
The CVE-LLM system employs a two-stage training approach: domain-adaptive pretraining (DAPT) followed by supervised finetuning (SFT). DAPT is performed on a corpus of 350K vulnerability-related documents including NVD CVE descriptions and internal Siemens documents. The base MPT-7B model is adapted with 539 domain-specific tokens. SFT uses 750K instruction-response pairs derived from 208K historical expert evaluations across 1.7K medical devices, enriched with ontology knowledge. The model is fine-tuned using the Lion optimizer and inference is performed using vLLM with beam search optimization. The system handles tasks including VEX category classification, justification generation, comment generation, and CVSS vector computation through both classification and text generation approaches.

## Key Results
- Achieves 94% accuracy for VEX category classification and 96% accuracy for CVSS vector generation
- Operates 50-100 times faster than human experts in vulnerability assessment tasks
- Successfully deployed in production to assist cybersecurity experts in managing large volumes of vulnerability assessments

## Why This Works (Mechanism)
The system's effectiveness stems from combining domain-specific pretraining with ontology-enriched instruction tuning. By pretraining on 350K vulnerability-related documents, the model develops specialized language understanding for cybersecurity terminology and medical device contexts. The instruction tuning phase leverages 208K historical expert evaluations to learn specific assessment patterns and decision-making processes. The ontology enrichment with CAPEC knowledge provides structured cybersecurity concepts that enhance the model's reasoning capabilities for vulnerability assessment tasks.

## Foundational Learning
- **Domain-Adaptive Pretraining (DAPT)**: Required to build specialized language understanding for vulnerability descriptions and medical device terminology. Quick check: Vocabulary expansion should include 539 domain-specific tokens for medical device components.
- **Ontology Integration**: Uses CAPEC entries (prerequisites, severity, mitigations) to enrich instructions and provide structured cybersecurity knowledge. Quick check: SEPSES knowledge graph mapping should be implemented for ontology enrichment.
- **Instruction Tuning**: Fine-tuning on expert-generated evaluations teaches the model specific assessment patterns and decision-making processes. Quick check: Alpaca format should be used for instruction-response pairs.
- **Multi-Task Learning**: Simultaneously handles classification (VEX categories, CVSS) and generation (comments) tasks. Quick check: Classification tasks should use micro-F1 metrics, generation tasks should use ROUGE-L.
- **Inference Optimization**: Uses vLLM with beam search (size 7) and adaptive sequence length handling. Quick check: Beam size 7 should be validated for optimal performance.

## Architecture Onboarding

**Component Map**: NVD Data + Internal Docs -> DAPT -> Enriched Instructions -> SFT -> vLLM Inference -> VEX Categories, Comments, CVSS Vectors

**Critical Path**: The critical path is the SFT phase where the model learns to map vulnerability notifications to specific assessment outputs using historical expert data enriched with ontology knowledge.

**Design Tradeoffs**: The system prioritizes speed (50-100x faster than humans) over perfect accuracy, accepting some hallucination and templated responses (85% of customer comments) to achieve practical deployment. The use of beam search optimization balances computational efficiency with output quality.

**Failure Signatures**: The model exhibits frequent hallucination of software versions and component names in generated comments. Performance degrades significantly on multi-vulnerability notifications exceeding 920 tokens. Customer comments show high template repetition (85%), indicating limited variability in responses.

**First Experiments**:
1. Validate tokenization by checking that 539 domain-specific tokens are correctly added to MPT-7B tokenizer
2. Test ontology enrichment by verifying CAPEC prerequisites, severity, and mitigations are correctly mapped to CVE entries
3. Benchmark inference speed using vLLM with beam_size=7 on single-vulnerability notifications to confirm 50-100x speed improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary data (208K historical evaluations and 102K internal documents) prevents independent verification and limits generalizability
- Model shows performance degradation on multi-vulnerability notifications exceeding 920 tokens
- Generated customer comments are highly templated (85%), raising concerns about handling novel vulnerability scenarios
- Tendency to hallucinate software versions and component names in generated comments

## Confidence

| Claim | Confidence |
|-------|------------|
| Speed improvements (50-100x faster) | High |
| 94% accuracy for VEX categories and 96% for CVSS vectors | Medium |
| Ability to handle novel vulnerabilities and complex scenarios | Low |

## Next Checks

1. Implement systematic hallucination testing by comparing generated comments against ground truth component lists to quantify hallucination rates, particularly for software versions and component names

2. Create benchmark tests using multi-vulnerability notifications exceeding 920 tokens to measure performance degradation and validate the proposed splitting approach

3. Train a model version using only publicly available NVD data and test on an independent medical device vendor's vulnerability assessment dataset to assess domain transferability