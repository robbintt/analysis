---
ver: rpa2
title: 'SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning
  on LLM'
arxiv_id: '2504.14286'
source_url: https://arxiv.org/abs/2504.14286
tags:
- training
- reasoning
- arxiv
- code
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes SRPO, a reinforcement learning framework that
  achieves state-of-the-art performance on both math and coding tasks using only 10%
  of the training steps compared to DeepSeek-R1-Zero. The approach introduces a two-stage
  training paradigm that first develops reasoning abilities through math data, then
  integrates coding data, and employs history resampling to improve sample efficiency
  by filtering out consistently correct answers.
---

# SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM

## Quick Facts
- arXiv ID: 2504.14286
- Source URL: https://arxiv.org/abs/2504.14286
- Reference count: 27
- Primary result: Achieves 50.0 pass@1 on AIME24 and 41.6 pass@1 on LiveCodeBench with only 10% of DeepSeek-R1-Zero's training steps

## Executive Summary
SRPO introduces a reinforcement learning framework that achieves state-of-the-art performance on both mathematical and coding tasks by employing a novel two-stage training paradigm. The approach first develops reasoning abilities through math-focused reinforcement learning, then integrates coding data to create a unified reasoning system. Using Qwen2.5-32B as the base model, SRPO demonstrates emergent reasoning behaviors including self-correction and numerical substitution while achieving significant performance gains with reduced computational resources.

## Method Summary
SRPO implements a two-stage reinforcement learning framework that alternates between mathematical and coding reasoning tasks. The method employs history resampling to improve sample efficiency by filtering out consistently correct answers from the training pool. The framework uses a group relative policy optimization approach where the policy network processes reasoning trajectories and learns to generate step-by-step solutions. Training proceeds in three phases: initial math-focused RL, history resampling implementation, and coding data integration, with the model developing cross-domain reasoning capabilities through this progressive training schedule.

## Key Results
- Achieves 50.0 pass@1 on AIME24 competition mathematics problems
- Achieves 41.6 pass@1 on LiveCodeBench coding evaluation
- Requires only 10% of training steps compared to DeepSeek-R1-Zero
- Demonstrates emergent reasoning behaviors including self-correction and verification patterns

## Why This Works (Mechanism)
The effectiveness of SRPO stems from its strategic two-stage training approach that first builds strong foundational reasoning capabilities in mathematics before expanding to coding tasks. The history resampling mechanism significantly improves sample efficiency by focusing training on challenging examples rather than repeatedly reinforcing already-mastered solutions. The group relative policy optimization allows the model to learn relative improvements rather than absolute score maximization, leading to more stable training dynamics and better generalization across domains.

## Foundational Learning
- **Reinforcement Learning Basics**: Understanding reward maximization and policy optimization - needed to grasp how the model improves through trial and error; quick check: can you explain the difference between on-policy and off-policy RL?
- **Group Relative Policy Optimization**: A technique for stable RL training - needed to understand the specific optimization method used; quick check: can you describe how GRPO differs from standard PPO?
- **Cross-Domain Generalization**: How models transfer knowledge between domains - needed to understand the two-stage training paradigm; quick check: can you explain why math reasoning might transfer to coding reasoning?
- **Sample Efficiency Techniques**: Methods to reduce training data requirements - needed to understand history resampling; quick check: can you describe why filtering out consistently correct answers improves learning?
- **Emergent Behavior in LLMs**: How complex reasoning patterns arise from training - needed to interpret the observed reasoning behaviors; quick check: can you list three examples of emergent reasoning patterns observed in the paper?

## Architecture Onboarding

Component Map: Base Model (Qwen2.5-32B) -> RL Policy Network -> History Resampling Module -> Reward Function -> Training Loop

Critical Path: The critical training path follows the two-stage paradigm where mathematical reasoning development must complete before coding integration begins, with history resampling operating continuously throughout both stages to maintain sample efficiency.

Design Tradeoffs: The framework trades potential immediate performance gains from mixed-domain training for more stable, generalizable reasoning capabilities through sequential domain specialization. This approach requires more careful hyperparameter tuning but results in better emergent reasoning behaviors.

Failure Signatures: Potential failures include catastrophic forgetting when switching between domains, instability in the reward function leading to mode collapse, and inefficient sampling if the history resampling thresholds are poorly calibrated.

First Experiments:
1. Validate the history resampling mechanism independently by testing sample efficiency improvements on a fixed math task
2. Test the two-stage training paradigm by comparing sequential versus parallel domain training
3. Evaluate the impact of different reward function designs on emergent reasoning behaviors

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to a single base model (Qwen2.5-32B), restricting generalizability across model families
- Comparison with DeepSeek-R1-Zero relies on reported rather than reproduced results
- Qualitative observations of emergent behaviors lack quantitative measurement or systematic characterization

## Confidence

**Major Claims and Confidence Labels:**
- **Performance Claims**: High confidence - State-of-the-art results on both AIME24 and LiveCodeBench with clear baseline comparisons
- **Sample Efficiency Claims**: Medium confidence - 10% training step reduction supported, but comparison methodology lacks full transparency
- **Emergent Behaviors**: Medium confidence - Compelling qualitative observations but no quantitative measurement framework

**Major Uncertainties:**
- History resampling effectiveness depends on specific reward function and threshold selection without thorough sensitivity analysis
- Two-stage training benefits not isolated from other architectural choices in the framework
- Scalability to larger models or different domains remains untested

## Next Checks
1. Reproduce SRPO performance on at least two additional base models (including non-Qwen architecture) to establish cross-model generalizability
2. Conduct ablation studies to isolate the contribution of history resampling versus other components in the training pipeline
3. Extend evaluation to include non-mathematical, non-coding reasoning tasks (e.g., logical reasoning, commonsense reasoning) to test true cross-domain generalization