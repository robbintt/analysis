---
ver: rpa2
title: A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online
  Dual Variable Guidance
arxiv_id: '2505.04494'
source_url: https://arxiv.org/abs/2505.04494
tags:
- policy
- regularized
- proof
- stochastic
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PGDA-RL, a novel primal-dual projected gradient
  descent-ascent algorithm for solving regularized Markov Decision Processes (MDPs).
  The method combines experience replay-based gradient estimation with a two-timescale
  decomposition of the underlying optimization problem, operating asynchronously on
  single-trajectory data with on-policy exploration.
---

# A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance

## Quick Facts
- **arXiv ID:** 2505.04494
- **Source URL:** https://arxiv.org/abs/2505.04494
- **Reference count:** 40
- **Primary result:** PGDA-RL achieves $O(k^{-2/3})$ mean-square convergence for dual iterates in regularized MDPs using single-trajectory data.

## Executive Summary
This paper introduces PGDA-RL, a novel primal-dual projected gradient descent-ascent algorithm for solving regularized Markov Decision Processes (MDPs). The method combines experience replay-based gradient estimation with a two-timescale decomposition of the underlying optimization problem, operating asynchronously on single-trajectory data with on-policy exploration. Under a strengthened ergodicity assumption on the Markov chain, the authors establish a last-iterate finite-time guarantee with $O(k^{-2/3})$ mean-square convergence for the dual iterates. The algorithm converges almost surely to the optimal value function and policy of the regularized MDP without requiring a simulator or fixed behavioral policy. Numerical experiments on the FrozenLake environment demonstrate convergence of the learned policy toward the optimal regularized policy, validating the theoretical findings.

## Method Summary
PGDA-RL solves entropy-regularized MDPs via a linear programming formulation using a primal-dual projected gradient descent-ascent approach. The algorithm operates asynchronously on single-trajectory data, maintaining a structured experience replay buffer that stores transitions separately for each state-action pair. The key innovation is the two-timescale decomposition where primal updates (value function) use faster diminishing stepsizes ($\alpha_k \approx k^{-2/3}$) while dual updates (occupancy measures) use slower stepsizes ($\beta_k \approx k^{-1}$). This separation allows the value function to track the slowly evolving dual variable. The method employs local stepsizes based on visitation counts, projected gradient updates, and $\epsilon$-greedy exploration with linear decay. Regularization (quadratic penalty on primal and entropy on dual) ensures strong convexity-concavity, enabling finite-time convergence guarantees.

## Key Results
- Achieves $O(k^{-2/3})$ mean-square convergence rate for dual iterates under strengthened ergodicity assumption
- Converges almost surely to optimal value function and policy of regularized MDP
- Operates on single-trajectory data without requiring simulator or fixed behavioral policy
- Demonstrates convergence on FrozenLake environment with KL divergence decreasing toward optimal policy

## Why This Works (Mechanism)

### Mechanism 1: Two-Timescale Decomposition
The algorithm achieves convergence by treating value function learning as a "fast transient" tracking the "slowly evolving" dual variable (occupancy measure), which effectively decouples the nested min-max optimization loop. PGDA-RL employs diminishing stepsizes where primal updates $\alpha_k \approx k^{-2/3}$ operate faster than dual updates $\beta_k \approx k^{-1}$. This separation allows the value function $V$ to equilibrate to the best response $\lambda(\rho)$ for the current policy iterate $\rho$ before the policy significantly shifts, ensuring the dual ascent direction is valid. If stepsizes decay too rapidly (e.g., $\alpha_k \sim k^{-1}$) or are equal, the "quasi-static" assumption fails, potentially causing oscillation or divergence.

### Mechanism 2: Structured Experience Replay with Fast Mixing
Structured experience replay provides asymptotically unbiased gradient estimates in asynchronous settings, provided the induced Markov chain mixes sufficiently fast to ensure linear growth of visitation counts. The buffer $D_k$ stores transitions. The gradient bias $E_k$ depends on the L1 distance between the empirical kernel $P_{D_k}$ and true kernel $P$. Under Uniform Geometric Ergodicity (UGE), the minimum visitation count $\nu_k$ grows linearly ($\geq p^*k/2$), forcing the empirical kernel error to decay as $O(\sqrt{\frac{\log k}{k}})$. If the environment lacks a uniformly ergodic exploration policy (e.g., disconnected state spaces or traps), $\nu_k$ may grow sublinearly, causing bias to persist indefinitely.

### Mechanism 3: Regularization for Strong Convexity-Concavity
Regularization transforms the saddle-point problem from a linear program into a strongly convex-concave game, guaranteeing unique equilibria and Lipschitz gradients required for finite-time convergence. A quadratic penalty $\frac{\eta_V}{2}\|V\|_2^2$ on the primal and conditional entropy $\eta_\rho g(\rho)$ on the dual are added. This ensures $\eta_V$-strong convexity in $V$ and $\mu_{opt}$-strong concavity in $\rho$, preventing the degeneracy common in unregularized LP formulations. If regularization parameters $\eta_V, \eta_\rho \to 0$, the gradients lose Lipschitz properties and the optimal dual may not be unique, breaking the convergence rate proofs.

## Foundational Learning

### Concept: Stochastic Approximation (SA) & ODE Method
Why needed here: The paper analyzes the asynchronous algorithm as a noisy discretization of a differential inclusion. Understanding SA is required to see why the "fast" variable tracks an ODE while the "slow" variable sees only an averaged equilibrium.
Quick check question: Can you explain why a martingale difference sequence with bounded second moments acts like "zero-mean noise" in the limit of decreasing stepsizes?

### Concept: Occupancy Measures & LP formulation of MDPs
Why needed here: The algorithm optimizes $\rho$ (the dual variable), which represents the state-action visitation distribution. The policy is derived *from* $\rho$ rather than optimized directly.
Quick check question: If $\rho^*(s,a)$ is the optimal occupancy measure, how do you recover the optimal policy $\pi^*(a|s)$? (Hint: Equation 3).

### Concept: Uniform Geometric Ergodicity (UGE)
Why needed here: The finite-time rate relies on "mixing." UGE ensures that regardless of the starting state, the Markov chain reaches its stationary distribution exponentially fast with uniform constants across all policies in the set $H$.
Quick check question: Why is "uniform" mixing (constants $C_X, \varrho$ valid for all $\rho \in H$) necessary for analyzing an algorithm where the policy changes at every step?

## Architecture Onboarding

### Component map:
Primal $V$ (Fast) -> Dual $\rho$ (Slow) -> Buffer $D_k$ -> Counters $\nu_k(s,a)$

### Critical path:
Observe $(s_k, a_k, s_{k+1})$ -> Update Buffer -> Draw sample $\xi_k$ -> Compute Gradient $\hat{g}_k, \hat{h}_k$ -> Update $V_k$ (local stepsize) -> Update $\rho_k$ (local stepsize + Projection) -> Update behavior policy $\pi_k$

### Design tradeoffs:
- **Exploration vs. Bias:** Larger $\epsilon$-greedy (or higher entropy weight $\eta_\rho$) ensures faster mixing (better bias decay) but may slow convergence to the deterministic optimal policy
- **Primal Regularization $\eta_V$:** Higher $\eta_V$ improves stability (contraction constant) but biases the value estimation away from the true $V^*$

### Failure signatures:
- **Stagnation:** If $\nu_k(s,a)$ stops growing for some pair, the effective stepsize for that component freezes
- **Projection Clipping:** If constants $C_L, C_U$ are misestimated, iterates may hit the boundary of $H$ repeatedly, preventing convergence to the interior optimum
- **Oscillation:** If $\beta_k$ decays too slowly relative to $\alpha_k$, the "quasi-static" assumption breaks, causing the value function to chase a moving target

### First 3 experiments:
1. **Reproduce FrozenLake Trajectory:** Run Algorithm 2 with specified hyperparameters ($\eta_\rho=0.1, \eta_V=0.1$). Plot the KL divergence $D(\pi_r^* \| \pi_{\rho_k})$ to verify the $O(k^{-2/3})$ trend implied by Theorem 4.2
2. **Ablation on Local Stepsize:** Compare the standard local clock update ($\beta(\nu_k)$) against a global decay ($\beta(k)$). Expectation: Local clocks stabilize learning on rarely visited states; global decay may fail to converge if visitation is skewed
3. **Mixing Time Sensitivity:** Test on a "RiverSwim" or grid-world with long corridors (high mixing time $\tau_k$). Verify if the convergence rate degrades as predicted by the $\tau_k^2$ term in the error bound (Proposition 4.3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PGDA-RL framework be extended to handle linear function approximation while retaining finite-time guarantees?
- **Basis in paper:** [explicit] The authors state that extending the method to function approximation remains a "pressing challenge," specifically noting the need to accommodate approximation error and stability issues
- **Why unresolved:** The current theoretical analysis relies on the tabular setting to manage the stability of the value iterates and the specific bias introduced by the replay buffer. The asynchronous update scheme would require significant reformulation to handle approximation errors inherent in function approximation
- **What evidence would resolve it:** A convergence proof for PGDA-RL within a linear MDP setting that explicitly bounds the approximation error under Markovian single-trajectory data

### Open Question 2
- **Question:** Can last-iterate finite-time convergence rates be derived under milder ergodicity and mixing conditions?
- **Basis in paper:** [explicit] The authors identify the relaxation of the strengthened ergodicity assumption (Assumption 3) and the high-probability visitation event $G_\delta$ as a "natural future direction"
- **Why unresolved:** The current $O(k^{-2/3})$ rate relies on the existence of a deterministic uniformly geometrically ergodic policy and a burn-in period to ensure linear growth of visitation counts. Weakening these requires new concentration tools for inhomogeneous Markov chains
- **What evidence would resolve it:** A finite-time guarantee that holds under the weaker Assumption 1 (general ergodicity) without requiring the high-probability visitation floor or the existence of a specific deterministic ergodic policy

### Open Question 3
- **Question:** Can acceleration techniques for convexified regularized MDPs be integrated into the asynchronous, Markovian setting?
- **Basis in paper:** [explicit] The authors note it is an "interesting open question" to combine acceleration ideas (previously shown under full model knowledge) with the present replay-based setting
- **Why unresolved:** Current acceleration methods for this problem class (e.g., by Li et al. [2024]) typically assume full model knowledge or deterministic updates. Integrating these into a stochastic approximation framework with biased gradients and asynchronous updates is non-trivial
- **What evidence would resolve it:** An accelerated variant of PGDA-RL that achieves a faster convergence rate (e.g., $O(k^{-1})$) while operating under the same Markovian data constraints

## Limitations
- Analysis relies heavily on a strengthened ergodicity assumption that may not hold in general MDPs with sparse rewards or disconnected state spaces
- Convergence guarantees are asymptotic and the $O(k^{-2/3})$ rate is specific to the quadratic regularization scheme - removing regularization would likely break the finite-time analysis
- Numerical experiments are limited to a single tabular environment (FrozenLake), leaving open questions about performance in larger or continuous state spaces

## Confidence

**High:** The two-timescale decomposition mechanism and its role in decoupling the nested optimization (Mechanism 1) - supported by explicit ODE analysis and timescale separation lemmas

**Medium:** The bias analysis for structured experience replay (Mechanism 2) - relies on a specific uniform ergodicity assumption that may not generalize

**Low:** The numerical results showing convergence to the optimal regularized policy - only one environment tested, and the comparison against "optimal" baselines lacks methodological detail

## Next Checks

1. **Break Assumption 3:** Test the algorithm on an MDP with poor mixing (e.g., a RiverSwim environment). Measure whether the visitation lower bound $\nu_k \geq p^*k/2$ still holds and if convergence degrades as predicted by the mixing time term

2. **Remove Regularization:** Run the algorithm with $\eta_V = \eta_\rho = 0$ (unregularized). Check if the iterates remain bounded and whether the convergence rate analysis still applies, or if the algorithm diverges/stagnates

3. **Compare Local vs. Global Stepsizes:** Implement a variant using global stepsizes $\alpha_k \propto k^{-2/3}, \beta_k \propto k^{-1}$ for all state-action pairs, regardless of visitation. Compare convergence speed and stability against the local clock mechanism, particularly for states with very different visitation frequencies