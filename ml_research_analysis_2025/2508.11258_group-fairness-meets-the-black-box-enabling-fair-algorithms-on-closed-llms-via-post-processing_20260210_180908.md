---
ver: rpa2
title: 'Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs
  via Post-Processing'
arxiv_id: '2508.11258'
source_url: https://arxiv.org/abs/2508.11258
tags:
- fairness
- fair
- predictions
- framework
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enforcing group fairness
  in classifiers built using closed-weight large language models (LLMs) like GPT-4,
  Gemini, or Claude, which do not expose model weights or internal representations.
  Traditional fairness algorithms that rely on model fine-tuning or access to embeddings
  are inapplicable in such settings.
---

# Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing

## Quick Facts
- **arXiv ID**: 2508.11258
- **Source URL**: https://arxiv.org/abs/2508.11258
- **Reference count**: 40
- **Primary result**: Post-processing framework achieves strong accuracy-fairness tradeoffs for closed-weight LLMs by eliciting sufficient statistics from probabilistic predictions

## Executive Summary
This paper addresses the challenge of enforcing group fairness in classifiers built using closed-weight large language models (LLMs) like GPT-4, Gemini, or Claude, which do not expose model weights or internal representations. Traditional fairness algorithms that rely on model fine-tuning or access to embeddings are inapplicable in such settings. The proposed solution is a post-processing framework that treats the LLM as a feature extractor, eliciting informative features from its probabilistic predictions via strategically designed prompts tailored to the specified fairness criterion. These features, which ideally contain sufficient statistics for fair classification, are then used by a lightweight fair algorithm to train a fair classifier. Experiments on five datasets, including three tabular ones, demonstrate that this approach achieves strong accuracy-fairness tradeoffs for both open-weight and closed-weight LLMs. Notably, the framework is data-efficient and outperforms fair classifiers trained on LLM embeddings or from scratch on raw tabular features, especially in low-data regimes, complementing prior work showing LLMs' competitiveness in few-shot classification.

## Method Summary
The framework treats closed-weight LLMs as feature extractors by eliciting probabilistic predictions through strategically designed prompts. For a given fairness criterion, the method constructs multiple-choice question answering (MCQA) prompts to query the LLM for conditional distributions P(Y|X) and P(A,B|X), where A is the sensitive attribute and B is the label. The LLM's log probabilities for answer choices are extracted and aggregated into a low-dimensional feature vector (G×K dimensions, where G is the number of groups and K is the number of labels). These features are calibrated via logistic regression and used by standard fair algorithms (Reductions, MinDiff, or LinearPost) to train fair classifiers. The approach is specifically designed for the attribute-blind setting where sensitive attributes are not available at test time.

## Key Results
- The framework achieves strong Area Under Tradeoff Curve (AUTC) performance across five datasets and three fairness criteria (SP, TPR, FPR, EO)
- In low-data regimes (<1000 examples), prediction-based features outperform both LLM embeddings and raw tabular features due to their low dimensionality
- Explicitly prompting for group attributes significantly improves fairness performance in most cases, though exceptions exist when LLM predictions for A are poor or when Y predictions already encode group information
- The approach works effectively for both open-weight and closed-weight LLMs, with the latter being the primary target application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting LLMs for joint predictions of labels and group attributes yields sufficient statistics for fair classification.
- Mechanism: Bayes-optimal fair classifiers can be expressed as a (potentially linear) function of P(Y|X) and P(A,B|X), where (A,B) are fairness-conditioning variables (e.g., sensitive attribute and label). By querying the LLM for these conditionals via strategically designed prompts, the extracted log probabilities encode the information needed for optimal fair decisions.
- Core assumption: The LLM produces approximately Bayes-optimal predictions for the queried conditionals.
- Evidence anchors:
  - [abstract] "features are elicited from its probabilistic predictions... to obtain sufficient statistics for fair classification"
  - [Page 5, Section 4] "recent work on post-processing algorithms... shows that the Bayes-optimal fair classifier... can be expressed as a (potentially randomized) function of the conditional distribution of the task labels P(Y|X) and the joint P(A,B|X)"
  - [corpus] Related work (Zeng et al.) on Bayes-optimal fair classification supports this theoretical basis, though not explicitly tested here.
- Break condition: If the LLM's predictions are systematically miscalibrated or fail to capture P(A|Y,X) accurately (e.g., balanced accuracy of only 0.656 for predicting A on COMPAS), the extracted features may not be sufficient, degrading performance.

### Mechanism 2
- Claim: Low-dimensional LLM prediction features reduce sample complexity for learning fair classifiers.
- Mechanism: The extracted features (G×K dimensions, e.g., 4 on Adult) are dramatically lower-dimensional than LLM embeddings (4096+ dimensions) or raw tabular features (97–810 dimensions). This reduces overfitting risk when training data is scarce, enabling effective fairness mitigation in low-data regimes.
- Core assumption: The dimensionality reduction preserves task-relevant information for fair classification.
- Evidence anchors:
  - [Page 7, Section 5] "features extracted in our framework are very low-dimensional—of K×G dimensions, e.g., 4 on Adult"
  - [Page 10, Section 6.2] "in the very low-data regime (fewer than 1000 examples), our framework outperforms the others due to its low dimensionality"
  - [corpus] Weak direct corpus support; Hegselmann et al. (cited in paper) showed similar data-efficiency for LLM tabular classification, but not specifically for fairness.
- Break condition: As training data increases, the information bottleneck from low-dimensional features causes performance to plateau, while embeddings/tabloid features continue improving (observed in Figure 4).

### Mechanism 3
- Claim: Prompting for group attributes explicitly (not just labels) is necessary for effective fairness mitigation.
- Mechanism: The fair algorithm requires estimates of P(A,Y|X) to adjust decisions across groups. Prompting only for Y yields features that may implicitly encode A, but this is unreliable. Explicitly prompting for A (conditionally on Y when needed) provides the necessary group information.
- Core assumption: The LLM can predict A with reasonable accuracy from the input.
- Evidence anchors:
  - [Page 11, Section 6.3] "in nearly all cases, excluding group information... degrades the performance"
  - [Page 11-12] Exceptions occur when LLM poorly predicts A (COMPAS: 0.656 balanced accuracy) or when Y predictions already encode A (BiasBios: 0.8768 from Y alone vs. 0.9961 from full joint).
  - [corpus] No direct corpus evidence on this specific ablation.
- Break condition: If A cannot be reliably inferred from X (LLM accuracy near chance), prompting for A adds noise rather than signal.

## Foundational Learning

- Concept: **Group fairness criteria (SP, TPR, EO)**
  - Why needed here: The framework must be instantiated with a specific fairness criterion, which determines which conditional distributions to prompt for.
  - Quick check question: For Equalized Odds, what must be equalized across groups? (Answer: P(Ŷ=k|A=a,Y=j) for all groups a,a′ and labels j,k.)

- Concept: **LLM log probability extraction via MCQA prompting**
  - Why needed here: The framework relies on extracting calibrated probabilistic predictions, not just argmax outputs.
  - Quick check question: Why use token log probabilities over the answer choices rather than sampling completions? (Answer: More efficient; sampling is a fallback when logits are unavailable.)

- Concept: **Attribute-aware vs. attribute-blind settings**
  - Why needed here: Determines whether sensitive attributes are available at test time; the paper focuses on the harder attribute-blind setting.
  - Quick check question: In attribute-blind Equalized Odds, what additional information must be prompted for compared to the attribute-aware case? (Answer: P(A|Y,X) to estimate group membership without observing A at test time.)

## Architecture Onboarding

- Component map:
  Prompt designer -> LLM query layer -> Feature constructor -> Calibrator -> Fair algorithm

- Critical path: Prompt design → LLM querying → Feature construction → Calibration → Fair algorithm training. Errors in early stages (bad prompts, missing logits) propagate and cannot be recovered downstream.

- Design tradeoffs:
  - Joint prompt (G×K choices) vs. decomposed prompts (1+K queries): Joint is simpler but may overwhelm LLM; decomposed adds query cost but simplifies each prediction task.
  - Zero-shot vs. few-shot prompting: Zero-shot is simpler; few-shot may improve prediction accuracy but requires more data.
  - Feature dimension vs. information preservation: Lower dimensions improve sample efficiency but risk information bottleneck as data scales.

- Failure signatures:
  - High fairness violation despite mitigation: Check if LLM predictions for A are poor (balanced accuracy <0.7); may need better prompts or task-appropriate LLM.
  - Missing logits in API response: Assign large negative value (e.g., -50) to missing choices; if many missing, consider sampling fallback.
  - Performance plateaus as data increases: Information bottleneck in low-dimensional features; consider switching to embeddings for high-data regimes.

- First 3 experiments:
  1. **Sanity check on simple dataset**: Run framework on Adult with Llama 3.1 8B, LinearPost, SP fairness. Verify AUTC > 0.3 and tradeoff curve improves over no-mitigation baseline.
  2. **Ablation on group prompting**: Compare full framework (prompting for A and Y) vs. Y-only variant on ACSIncome. Expect degradation when A is excluded.
  3. **Data scaling test**: Vary training size (100–20000) on Adult; identify crossover points where embeddings/tabloid features outperform prediction-based features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can additional information be elicited from LLMs (beyond log probabilities of $A$ and $Y$) to overcome the performance plateau observed in the proposed framework as training data increases?
- Basis in paper: [explicit] The Conclusion states, "There could be additional information to elicit from the LLM that may improve the performance... particularly given the performance plateau observed in Fig. 4, in contrast to the continued gains from training on LLM embeddings."
- Why unresolved: The paper demonstrates that the proposed low-dimensional features ($KG$ dimensions) create a sample complexity advantage in low-data regimes but suffer an information bottleneck compared to high-dimensional embeddings (e.g., 4096 dims) as data increases.
- What evidence would resolve it: A study identifying and extracting new feature types from LLM outputs (e.g., attention scores if available, or uncertainty metrics) that improve the Area Under the Tradeoff Curve (AUTC) in high-data regimes without sacrificing the fairness properties.

### Open Question 2
- Question: How effective is the proposed post-processing framework when combined with advanced prompting strategies like Chain-of-Thought (CoT) reasoning or prompt-based fairness mitigation?
- Basis in paper: [explicit] The Conclusion notes, "While our experiments focused on zero-shot prompting, the framework naturally supports... chain-of-thought prompting... Prompt-based fairness mitigation strategies could also be incorporated, with the fair algorithm complementing these methods."
- Why unresolved: The experiments were strictly zero-shot. It is unknown if the intermediate tokens generated in CoT or the modified distributions from fairness-mitigating prompts would provide "sufficient statistics" that are easier or harder for the post-processing algorithm to correct.
- What evidence would resolve it: Empirical results comparing the accuracy-fairness tradeoffs of the current zero-shot implementation against versions utilizing CoT or fair-prompting techniques across the same datasets.

### Open Question 3
- Question: How robust is the theoretical guarantee of "sufficient statistics" when the LLM's predictions deviate significantly from Bayes-optimality?
- Basis in paper: [explicit] The Conclusion lists a limitation: "It relies on the condition that the elicited predictions... contain sufficient information... This condition holds only if the LLM produces Bayes-optimal predictions, which is unlikely in practice."
- Why unresolved: The framework assumes the elicited $P(A,Y|X)$ is a sufficient statistic for the Bayes-optimal fair classifier. Real-world LLMs suffer from miscalibration and hallucinations, potentially violating this assumption in ways not fully captured by the logistic regression calibration step.
- What evidence would resolve it: Theoretical analysis or empirical stress-testing on datasets where LLMs are known to be poorly calibrated or biased, showing whether the post-processing algorithm fails to converge to the optimal fair classifier.

### Open Question 4
- Question: How does the framework's performance degrade when restricted to the top-$k$ log probabilities typically provided by closed-weight model APIs (e.g., $k=20$ in GPT-4o)?
- Basis in paper: [inferred] The paper notes on Page 5 that for GPT-4o, "only the top-k logits are accessible... we assign a large negative value to the missing logits." It does not analyze if this heuristic for missing classes impacts the accuracy of the joint distribution estimation $P(A,Y|X)$.
- Why unresolved: Assigning a fixed negative value (e.g., -50) to non-top-$k$ tokens is a heuristic that treats all missing probabilities as near-zero, which might distort the feature space for datasets with many classes or subtle group distinctions.
- What evidence would resolve it: An ablation study on an open-weight model simulating the top-$k$ restriction, comparing performance when using the full probability vector versus the heuristic-filled truncated vector.

## Limitations

- The framework's effectiveness critically depends on the LLM's ability to accurately predict sensitive attributes from input features, with poor performance when LLM predictions for A fall below ~0.7 balanced accuracy
- The low-dimensional nature of extracted features creates an information bottleneck that causes performance to plateau as training data increases substantially
- The approach assumes the LLM produces approximately Bayes-optimal predictions, which is unlikely in practice given real-world calibration issues and hallucinations

## Confidence

**High confidence**: The core mechanism of using LLM probabilistic predictions as sufficient statistics for fair classification is theoretically sound, supported by prior work on Bayes-optimal fair classification. The experimental results showing strong AUTC performance across multiple datasets and LLMs are well-documented and reproducible.

**Medium confidence**: The data-efficiency advantage in low-data regimes is demonstrated but relies on assumptions about the information content of low-dimensional features. The ablation studies on group prompting provide clear evidence for the necessity of explicit group information, though exceptions exist for specific dataset-LLM combinations.

**Low confidence**: The long-term scalability of the framework as training data increases dramatically is uncertain, with the paper acknowledging the information bottleneck limitation but not providing extensive validation at very large data scales.

## Next Checks

1. **LLM prediction accuracy validation**: Measure the LLM's balanced accuracy for predicting sensitive attributes on each dataset before applying the framework. If accuracy falls below 0.7, investigate alternative prompting strategies or different LLMs for those specific tasks.

2. **Data scaling crossover point**: Conduct experiments varying training size from 100 to 100,000 examples on Adult and COMPAS to precisely identify where embeddings or raw tabular features outperform the low-dimensional prediction features, validating the information bottleneck claim.

3. **Alternative feature dimensionality test**: Experiment with intermediate-dimensional features (e.g., 50-500 dimensions) using LLM embeddings with dimensionality reduction to assess whether there's an optimal dimensionality that balances sample efficiency and information preservation.