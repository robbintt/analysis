---
ver: rpa2
title: Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World
  Settings
arxiv_id: '2512.12492'
source_url: https://arxiv.org/abs/2512.12492
tags:
- clinical
- detection
- conditions
- recall
- polyp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust polyp detection in
  real-world endoscopy, where adverse imaging conditions like motion blur, illumination
  changes, and occlusions degrade detection performance. The authors propose ADAPTIVEDETECTOR,
  a two-stage detector-verifier framework combining a YOLOv11 detector with a vision-language
  model (VLM) verifier.
---

# Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings

## Quick Facts
- **arXiv ID:** 2512.12492
- **Source URL:** https://arxiv.org/abs/2512.12492
- **Reference count:** 0
- **Primary result:** Two-stage detector-verifier framework improves polyp detection recall by 14-22pp under adverse imaging conditions while maintaining precision within ±1.7pp of baseline.

## Executive Summary
This paper addresses the challenge of robust polyp detection in real-world endoscopy, where adverse imaging conditions like motion blur, illumination changes, and occlusions degrade detection performance. The authors propose ADAPTIVEDETECTOR, a two-stage detector-verifier framework combining a YOLOv11 detector with a vision-language model (VLM) verifier. The system adaptively adjusts detection thresholds based on VLM-guided image quality assessment and employs a cost-sensitive reinforcement learning approach (GRPO) to optimize the verifier for clinical priorities, emphasizing recall over precision. To enable rigorous evaluation, the authors construct a synthetic testbed by systematically degrading clean polyp datasets with realistic adverse conditions.

## Method Summary
ADAPTIVEDETECTOR employs a two-stage pipeline: YOLOv11 generates candidate bounding boxes at adaptive confidence thresholds (τ_high=0.5 for clean images, τ_low=0.2 for degraded), then a Qwen-VL verifier fine-tuned via supervised learning and GRPO with asymmetric rewards (λ_FN=2.0) performs binary classification on candidate crops. The VLM quality assessor determines per-frame degradation levels to trigger threshold adaptation. The framework is trained only on clean data but evaluated on 500 synthetically degraded images generated from CVC-ClinicDB and Kvasir-SEG datasets.

## Key Results
- ADAPTIVEDETECTOR achieves 14-22pp higher recall than YOLOv11 baseline on synthetically degraded images
- Precision remains within 0.7pp below to 1.7pp above YOLO baseline across all degradation types
- VLM-guided adaptive thresholding contributes 6-8pp recall improvement over fixed-threshold baseline
- GRPO fine-tuning with asymmetric rewards improves recall by 4-6pp compared to supervised fine-tuning alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive threshold selection based on image quality improves recall under degraded conditions while preserving precision on clean images.
- **Mechanism:** The VLM performs a global quality assessment (illumination, clarity, artifacts) on each frame. When adverse conditions are detected, the system switches to τ_low (0.2) to generate more candidates; otherwise, it uses τ_high (0.5) to suppress spurious detections. This dynamic adjustment allows the detector to cast a wider net precisely when polyps are harder to see.
- **Core assumption:** The VLM can reliably distinguish clean from degraded frames, and degraded frames genuinely require lower thresholds to capture missed lesions.
- **Evidence anchors:**
  - [abstract] "The detector adaptively adjusts per-frame confidence thresholds under VLM guidance"
  - [Section 3.1.1] "τ_low favors recall by generating more candidates under challenging conditions, while τ_high maintains precision in clean images"
  - [corpus] Weak direct support; related work on test-time adaptation (Noisy Test-Time Adaptation in VLMs) addresses distribution shifts but not threshold adaptation specifically.
- **Break condition:** If VLM quality assessment fails (e.g., misclassifies a clean frame as degraded), thresholds invert, potentially flooding the verifier with candidates or suppressing legitimate detections.

### Mechanism 2
- **Claim:** Cascaded detector-verifier architecture recovers false negatives that a standalone detector misses under adverse imaging conditions.
- **Mechanism:** YOLOv11 generates candidate bounding boxes at the adaptive threshold. Each candidate crop is then semantically verified by the Qwen-VL verifier using binary classification prompts. Only candidates passing both stages are retained. The verifier adds a second "opinion" that can rescue low-confidence detections YOLO would otherwise discard.
- **Core assumption:** The VLM verifier has semantic understanding superior to the detector's pattern matching for borderline cases, and the two errors are not perfectly correlated.
- **Evidence anchors:**
  - [abstract] "two-stage detector–verifier framework comprising a YOLOv11 detector with a vision–language model (VLM) verifier"
  - [Section 3.1.2] "Only candidates that receive positive decisions from both the YOLO detector and VLM verifier are retained"
  - [corpus] Endo-CLIP demonstrates VLM pre-training on colonoscopy data improves downstream tasks, supporting the verifier's semantic capacity.
- **Break condition:** If the initial detector fails to generate any candidate for a polyp (e.g., under extreme occlusion), the verifier has nothing to rescue—recall gains are bounded by Stage 1 coverage.

### Mechanism 3
- **Claim:** GRPO with asymmetric cost-sensitive rewards aligns the verifier with clinical priorities (recall over precision).
- **Mechanism:** After supervised fine-tuning, the VLM verifier undergoes Group Relative Policy Optimization. The reward function applies λ_FN > 1 penalty for false negatives, explicitly encoding that missed polyps are costlier than false alarms. GRPO compares multiple candidate responses per input and reinforces relatively superior predictions.
- **Core assumption:** The asymmetric reward structure correctly captures clinical utility, and GRPO can optimize this objective without destabilizing the pre-trained VLM.
- **Evidence anchors:**
  - [abstract] "fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections"
  - [Section 3.4] "R_conf = 1/N Σ r_ci − λ_FN · FN_penalty" and "λ_FN > 1 reflects the clinical reality that missed polyps have more severe consequences"
  - [corpus] No direct corpus evidence for GRPO in medical imaging; this appears novel.
- **Break condition:** If the reward function over-penalizes false negatives relative to false positives, the verifier may become over-permissive, degrading precision beyond clinical acceptability.

## Foundational Learning

- **Concept: Precision-Recall Trade-off in Medical Detection**
  - **Why needed here:** The entire framework hinges on the clinical asymmetry between missed polyps (false negatives) and unnecessary follow-up (false positives). Understanding this trade-off is essential to interpret the GRPO reward design.
  - **Quick check question:** Given a polyp detector with 80% recall and 90% precision, what happens to precision if you lower the detection threshold to achieve 95% recall?

- **Concept: Vision-Language Model Prompting**
  - **Why needed here:** The VLM verifier and quality assessor both rely on structured prompts (Table 1). Poor prompt design can cause parsing failures or inconsistent outputs, directly breaking the pipeline.
  - **Quick check question:** What output format must the VLM return for the verifier's binary decision to be parsed correctly?

- **Concept: Policy Optimization Basics (GRPO/RLHF)**
  - **Why needed here:** GRPO fine-tunes the verifier via comparative reward signals rather than pure supervised loss. Engineers need to grasp how reward shaping influences model behavior.
  - **Quick check question:** In GRPO, how does the advantage function A(x, y) determine which responses are reinforced?

## Architecture Onboarding

- **Component map:** Input Frame → VLM Quality Assessment → Adaptive Threshold Controller → YOLOv11 Detector → Candidate Bounding Boxes → For each box: Crop → VLM Verifier → Consensus Filter → Final Detections

- **Critical path:** VLM quality assessment → threshold selection → YOLO candidate generation → VLM verification. If any stage fails (e.g., VLM times out, YOLO produces zero candidates), downstream stages cannot compensate.

- **Design tradeoffs:**
  - **Recall vs. Precision:** Lowering τ_low increases recall at precision cost; GRPO's λ_FN tunes this balance.
  - **Latency vs. Accuracy:** Each VLM call adds ~50–80ms; more candidates = more verification overhead.
  - **Zero-shot generalization vs. Training complexity:** Training only on clean data enables deployment flexibility but requires robust synthetic degradation simulation for evaluation.

- **Failure signatures:**
  - **Zero candidates generated:** YOLO threshold too aggressive for severely degraded frames; check τ_low setting and VLM quality assessment accuracy.
  - **High false positive rate:** VLM verifier over-accepting; may need increased τ_conf or re-examined GRPO reward weights.
  - **Parsing errors in VLM output:** Prompt format not followed; verify JSON schema compliance and add R_format reward weight.

- **First 3 experiments:**
  1. **Baseline ablation:** Run YOLOv11 alone on clean vs. degraded test sets to quantify the recall gap (expected: ~15–20 point drop on degraded).
  2. **Threshold sweep:** Fix VLM verification, vary τ from 0.1 to 0.6 on degraded images; plot recall-precision curve to validate τ_low = 0.2 choice.
  3. **GRPO reward sensitivity:** Train verifier variants with λ_FN ∈ {1.0, 1.5, 2.0, 3.0}; compare recall/precision trade-offs to identify optimal clinical balance.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of ADAPTIVEDETECTOR on synthetically degraded images correlate with detection accuracy on real-world clinical data containing natural artifacts?
  - **Basis in paper:** The authors note "a synthetic-to-real domain gap remains between synthetic artifacts and real-world endoscopic variations" and suggest "validation on clinical data with natural degradations would strengthen the findings" (Section 4.8).
  - **Why unresolved:** The study relies exclusively on a synthetic testbed generated by Qwen-Image-Edit to simulate open-world conditions, lacking evaluation on non-synthetic, clinically degraded datasets.
  - **What evidence would resolve it:** A comparative study measuring the model's recall and precision on a held-out set of real colonoscopy frames labeled with specific degradation types.

- **Open Question 2:** Can integrating temporal reasoning or video context mitigate the failure mode where the initial YOLOv11 detector generates no candidates under severe occlusion?
  - **Basis in paper:** The authors state the framework "processes individual frames independently" and that under severe degradation, "the initial detector may fail to generate any candidates," suggesting "temporal aggregation approaches" as future work (Section 4.8).
  - **Why unresolved:** The current architecture is strictly frame-based; if the first stage produces zero bounding boxes due to extreme conditions, the second-stage verifier has no input to process, resulting in an unavoidable miss.
  - **What evidence would resolve it:** An ablation study where the detector incorporates information from adjacent frames to recover missed candidates in single-frame occlusion scenarios.

- **Open Question 3:** What specific types of "hard negative" anatomical structures cause the most false positives for the VLM verifier, and can targeted augmentation alleviate this?
  - **Basis in paper:** The paper identifies "common sources" of false positives, including "prominent mucosal folds" and "specular highlights," suggesting "enhanced training with more diverse hard negatives" (Section 4.8).
  - **Why unresolved:** While the current reward function penalizes false positives, the model still struggles to distinguish visually similar non-polyp structures from true positives in the verification stage.
  - **What evidence would resolve it:** Detailed failure analysis showing the reduction rate of false positives on mucosal folds after adding these specific structures as negative examples in the GRPO fine-tuning data.

## Limitations

- **Synthetic testbed dependency:** Evaluation relies entirely on synthetically degraded images, which may not capture the full complexity and variability of real-world adverse conditions.
- **Limited clinical grounding:** The cost-sensitive reward design assumes missed polyps are clinically worse than false positives, but this assumption lacks physician-in-the-loop validation.
- **GRPO novelty without ablation:** The use of GRPO for verifier fine-tuning is presented without ablation studies comparing it to simpler alternatives.

## Confidence

- **High confidence:** The core mechanism of adaptive threshold selection based on image quality is well-supported by experimental results showing consistent recall improvements across degradation types.
- **Medium confidence:** The GRPO fine-tuning approach and its impact on clinical utility metrics are supported but could benefit from additional ablation studies and clearer clinical validation.
- **Low confidence:** The VLM quality assessment method lacks sufficient detail for independent verification, and clinical assumptions about asymmetric costs remain untested with medical experts.

## Next Checks

1. **Real-world clinical validation:** Test the system on endoscopic videos from actual procedures containing naturally occurring adverse conditions rather than synthetic degradations to assess practical performance.
2. **GRPO ablation study:** Compare the full ADAPTIVEDETECTOR system against variants using only supervised fine-tuning or alternative reward structures to isolate the contribution of the GRPO component.
3. **Physician utility assessment:** Conduct formal evaluation with gastroenterologists to validate the clinical assumptions underlying the asymmetric cost function and assess the system's impact on diagnostic decision-making.