---
ver: rpa2
title: 'WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with
  Model Merging'
arxiv_id: '2510.26546'
source_url: https://arxiv.org/abs/2510.26546
tags:
- domain
- target
- merging
- performance
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WeaveRec, a novel framework addressing performance
  degradation in LLM-based cross-domain sequential recommendation when using model
  merging. The core insight is that source-domain models often perform poorly on target
  domains, causing merged models to underperform.
---

# WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging

## Quick Facts
- **arXiv ID**: 2510.26546
- **Source URL**: https://arxiv.org/abs/2510.26546
- **Reference count**: 40
- **One-line primary result**: WeaveRec effectively mitigates performance degradation in LLM-based cross-domain sequential recommendation when using model merging by training hybrid LoRA modules on mixed source-target data.

## Executive Summary
This paper addresses a critical challenge in cross-domain sequential recommendation (CDSR): performance degradation when using model merging techniques. The core insight is that naive model merging fails because source-domain models perform poorly on target domains, causing merged models to underperform. WeaveRec solves this by training hybrid LoRA modules using mixed source-target domain data, then merging them with target-domain LoRA modules in a weaving fashion. This approach provably reduces generalization error upper bounds while maintaining inference efficiency without additional memory or latency costs. Extensive experiments across single-source, multi-source, and cross-platform scenarios demonstrate WeaveRec consistently outperforms baselines.

## Method Summary
WeaveRec is a plug-and-play framework that uses LoRA modules for efficient model merging in cross-domain sequential recommendation. The method trains a target-domain LoRA on target data alone, and for each source domain, trains a hybrid LoRA using a mix of that source's data and the target domain's data. These LoRAs are then merged via weight averaging to create a single merged model. The framework uses Qwen2-7B as the LLM backbone with LoRA rank=16, alpha=32, and dropout=0.05. Training uses a learning rate of 2e-4 and batch size of 128, with 40,000 samples from each domain for hybrid LoRA training.

## Key Results
- WeaveRec consistently outperforms naive model merging baselines across single-source, multi-source, and cross-platform CDSR scenarios
- The framework maintains inference efficiency without additional latency or memory costs
- Hybrid LoRA training on mixed source-target data provably reduces generalization error upper bounds compared to source-only training
- WeaveRec demonstrates effective plug-and-play modularity, allowing scalable addition/removal of source domains

## Why This Works (Mechanism)

### Mechanism 1: Hybrid LoRA Training Mitigates Source-Domain Knowledge Conflicts
Training LoRA modules on mixed source-target domain data creates models that are more effective when merged with target-domain models. Naive model merging degrades performance because source-domain models perform poorly on the target domain, dragging the merged model into a suboptimal compromise. WeaveRec's hybrid approach learns source patterns while being adapted to the target domain's distribution, making it a competent peer for the target-domain model during merging.

### Mechanism 2: Theoretical Reduction of Generalization Error
The paper provides a theoretical guarantee that training on mixed source and target data reduces the upper bound of generalization error on the target domain compared to training on source data alone. Using domain adaptation theory (H-divergence), a model trained on mixed distribution has smaller distributional distance to the target domain, leading to a tighter error bound.

### Mechanism 3: Plug-and-Play Modularity for Efficient Knowledge Transfer
WeaveRec's design allows scalable and efficient addition or removal of source domains without retraining the entire system while maintaining constant inference cost. Hybrid LoRAs are trained independently for each source domain, making them modular components. Merging produces a single LoRA, keeping inference as fast as single-model inference.

## Foundational Learning

- **Concept: Cross-Domain Sequential Recommendation (CDSR)** - Why needed: This is the core problem WeaveRec addresses. Understanding this transfer problem is essential to appreciating why naive merging fails and why WeaveRec is proposed. Quick check: Given user interaction sequences in domains A and B, how would a CDSR model predict the next item in domain B for a user? (Answer: By learning transferable patterns from sequences in A and applying them to B).

- **Concept: Model Merging (Task Arithmetic/Weight Averaging)** - Why needed: WeaveRec uses model merging as its core technique to combine knowledge. Understanding how model parameters can be merged is key to understanding the mechanism. Quick check: If you have a model M_A trained on task A and M_B on task B, what is a simple way to create a single model that can perform both tasks? (Answer: Create a new model M_C whose parameters are a weighted average of M_A and M_B's parameters).

- **Concept: LoRA (Low-Rank Adaptation)** - Why needed: WeaveRec applies model merging to LoRA modules, making the approach computationally feasible. Understanding that LoRA modules are small, trainable parameter sets is key to understanding the efficiency. Quick check: Instead of updating all 7 billion parameters of an LLM for a recommendation task, what does LoRA do? (Answer: It adds and trains small, low-rank matrices while keeping the main LLM weights frozen).

## Architecture Onboarding

- **Component map**: LLM Backbone (Qwen2-7B) -> LoRA Modules (Target-Domain + Hybrid Source LoRAs) -> Merger (Weight Averaging) -> Merged LoRA

- **Critical path**:
  1. Data Preparation: Convert interaction data from all domains into text-based instruction datasets
  2. LoRA Training: Train target-domain LoRA on target data only; for each source domain, train hybrid LoRA on mixed source-target data
  3. Model Merging: Combine parameters of Target-Domain LoRA and all Hybrid Source LoRAs using weight averaging
  4. Inference: Load single merged LoRA into frozen LLM backbone and generate recommendations

- **Design tradeoffs**: Hybrid training increases training data per source module but improves cross-domain alignment. The paper finds simple Weight Averaging is surprisingly effective compared to complex methods like Ties-Merging.

- **Failure signatures**: Performance degradation after merging indicates the merging process has failed. Poor performance of source-only models on target domain confirms the root cause of naive merging failure.

- **First 3 experiments**:
  1. Reproduce the degradation: Train source and target LoRAs separately, merge them, and show performance drops on target domain
  2. Validate hybrid training: Train a single hybrid LoRA on mixed source-target data and compare its target domain performance against source-only and target-only models
  3. Full WeaveRec vs. baselines: Implement complete WeaveRec pipeline with multiple source domains and compare final performance against target-only and naive merging baselines

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal number and specific subset of source domains be automatically determined for a given target domain to maximize performance gains? The authors note that the optimal number of source domains varies across different target domains, reflecting inherent heterogeneity among them. This requires manual selection and experimental trial-and-error currently.

### Open Question 2
Can a specialized model merging algorithm be developed that surpasses simple Weight Averaging for LLM-based recommendation without losing semantic knowledge? The paper observes that sophisticated methods like Ties-Merging and LoRA-LEGO fail in this context, hypothesizing they manipulate parameters too aggressively.

### Open Question 3
Does the effectiveness of WeaveRec's weaving strategy and its theoretical generalization bounds scale consistently to significantly larger LLMs (70B+ parameters) or alternative architectures? The experimental validation is confined to the Qwen2-7B model, and the theoretical analysis relies on approximations that may shift as model capacity increases.

## Limitations
- The empirical validation is limited to relatively homogeneous domains (Amazon product categories) and does not explore edge cases where source-target domain distributions are extremely different
- The theoretical analysis provides an upper bound guarantee but does not quantify the actual gap between the bound and empirical performance
- Scalability to many more domains or significantly different domain pairs remains unproven beyond experiments with up to 3 source domains

## Confidence
- **High Confidence**: The empirical demonstration that naive model merging causes performance degradation, and that WeaveRec effectively mitigates this issue across multiple benchmarks
- **Medium Confidence**: The theoretical guarantee that hybrid training reduces generalization error bounds, depending on the tightness of the bound and convergence assumptions
- **Medium Confidence**: The plug-and-play modularity claim, validated with up to 3 source domains but unproven at larger scales

## Next Checks
1. **Stress Test Hybrid Training**: Systematically vary the mixing ratio of source-to-target data in hybrid LoRA training (e.g., 90-10, 50-50, 10-90) and measure the impact on target domain performance
2. **Bound Tightness Analysis**: Measure the actual generalization error gap between source-only, hybrid, and target-only models, and compare this to the theoretical bound to assess its practical tightness
3. **Cross-Platform Scalability**: Extend experiments to include more than 2 platforms (e.g., MovieLens, BookCrossing, and Amazon) to evaluate WeaveRec's performance when scaling to multiple, potentially more heterogeneous data sources