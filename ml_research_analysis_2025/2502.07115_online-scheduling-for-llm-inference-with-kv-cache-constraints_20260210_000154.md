---
ver: rpa2
title: Online Scheduling for LLM Inference with KV Cache Constraints
arxiv_id: '2502.07115'
source_url: https://arxiv.org/abs/2502.07115
tags:
- memory
- requests
- scheduling
- inference
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of minimizing end-to-end latency
  in large language model inference by jointly batching and scheduling incoming prompts
  while managing the KV cache memory constraints. The authors introduce a hindsight
  optimal benchmark via integer programming and prove that no deterministic online
  algorithm can achieve a constant competitive ratio under arbitrary arrivals.
---

# Online Scheduling for LLM Inference with KV Cache Constraints

## Quick Facts
- arXiv ID: 2502.07115
- Source URL: https://arxiv.org/abs/2502.07115
- Reference count: 40
- One-line primary result: MC-SF achieves near-optimal latency with provable guarantees under structured conditions

## Executive Summary
This paper addresses the challenge of minimizing end-to-end latency in large language model inference by jointly batching and scheduling incoming prompts while managing KV cache memory constraints. The authors introduce a polynomial-time Memory Constrained Shortest First (MC-SF) algorithm that prioritizes partially completed requests and selects new ones to maximize batch size under memory constraints. Theoretical analysis shows that under conditions of identical prompt sizes, simultaneous arrivals, and bounded prediction errors on output lengths, MC-SF achieves a constant competitive ratio against the hindsight-optimal offline solution.

## Method Summary
The MC-SF algorithm operates by maintaining two queues: S(t) for partially completed requests and R(t) for waiting requests. At each decision point, it prioritizes all requests in S(t) before greedily selecting new requests from R(t) sorted by predicted output length. For each candidate request, it validates a predictive memory constraint at all future completion times within the batch's horizon, ensuring the total memory usage never exceeds the capacity M. The algorithm achieves polynomial-time complexity by using a greedy approach rather than solving the NP-hard integer programming benchmark.

## Key Results
- MC-SF matches hindsight-optimal performance with average latency ratio of ~1.005 in no-information-gap case
- Algorithm significantly outperforms benchmark scheduling policies in latency and memory efficiency
- Under structured conditions (identical prompt sizes, simultaneous arrivals, bounded prediction errors), MC-SF achieves O(1) competitive ratio
- Empirically validates effectiveness on both synthetic data and real-world conversation dataset

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Prioritizing partially completed requests and proactively checking future memory constraints prevents stalls caused by KV cache growth.
- Mechanism: MC-SF processes S(t) first, then validates predictive memory constraint (Eq. 5) at all future completion times for candidate batches from R(t). This ensures memory usage never exceeds M by accounting for both initial prompt sizes and incremental KV growth.
- Core assumption: Predictor provides ō_i that upper-bounds true output length (ō_i ≥ o_i), enabling conservative memory planning.
- Evidence anchors: Abstract describes MC-SF's prioritization and constraint checking; Section 4 details the feasibility check in Eq. (5).

### Mechanism 2
- Claim: Greedily selecting new requests with shortest predicted output lengths improves batch packing efficiency and reduces average latency.
- Mechanism: After serving S(t), MC-SF sorts R(t) by ō_i and greedily adds requests if they satisfy predictive memory constraint. Shorter jobs reduce peak memory contribution and time-to-completion, enabling more efficient packing.
- Core assumption: Shorter jobs have lower memory footprint/latency impact, making them more efficient to batch.
- Evidence anchors: Algorithm 1 explicitly sorts by ō_i; section 4 explains how smaller ō_i values reduce peak memory consumption.

### Mechanism 3
- Claim: Under structured arrivals and bounded prediction errors, greedy online algorithm achieves constant competitive ratio against hindsight-optimal offline solution.
- Mechanism: Proves no deterministic online algorithm has constant competitive ratio under arbitrary arrivals, then shows MC-SF is O(1)-competitive under specific conditions through LP relaxation bounds.
- Core assumption: Request stream has sufficient structure (identical prompt sizes, simultaneous arrivals) for greedy heuristic to exploit.
- Evidence anchors: Abstract states the theorem conditions; Section 4 provides formal statement and proof sketch.

## Foundational Learning
- Concept: KV Cache Dynamics in Transformer Decoding
  - Why needed here: Core constraint is linear growth of KV cache memory during token generation; this dynamic profile is the central challenge.
  - Quick check question: How does memory footprint of single request change from prefill to final token generation?

- Concept: Competitive Analysis
  - Why needed here: Paper evaluates online algorithm against offline "hindsight optimal" benchmark; understanding competitive ratios is key to interpreting theoretical guarantees.
  - Quick check question: What does it mean for online algorithm to be c-competitive against offline optimal solution?

- Concept: Integer Programming for Scheduling
  - Why needed here: "Gold standard" benchmark defined by Integer Program; grasping how constraints capture non-preemptive, memory-constrained scheduling is crucial.
  - Quick check question: How do IP decision variables and constraints encode non-preemptive execution and time-varying memory usage?

## Architecture Onboarding
- Component map: Scheduler (MC-SF) -> Request Queues (S(t), R(t)) -> Memory Model (KV cache with capacity M) -> Predictor Module (provides ō_i) -> Batch Constructor (builds U(t)) -> Benchmark Solver (IP solver for hindsight optimal)
- Critical path:
  1. Request arrival: Request i arrives with s_i; Predictor provides ō_i
  2. Batch decision: Scheduler prioritizes all requests in S(t)
  3. Greedy selection: Batch Constructor sorts R(t) by ō_i, iteratively adds to U(t)
  4. Feasibility check: Validates predictive memory constraint (Eq. 5) at all relevant future times
  5. Execution: Process batch S(t) ∪ U(t); KV cache updates (new token KV added, completed requests cleared)
- Design tradeoffs:
  - Latency vs. Throughput: MC-SF optimizes for latency; greedy shortest-first may not maximize throughput in heavy-tailed workloads
  - Conservatism vs. Efficiency: Loose prediction (large α) reduces memory utilization to prevent overflow; tight bound increases risk
  - Complexity vs. Optimality: MC-SF is O(M²) per step, far more scalable than NP-hard IP benchmark
- Failure signatures:
  - Memory Overflow: Underestimating o_i causes actual memory to exceed M, triggering eviction/clearing events
  - Head-of-Line Blocking: Single long request in S(t) can delay new short requests in R(t)
- First 3 experiments:
  1. Baseline Comparison: Compare MC-SF against FCFS scheduler on synthetic dataset with Poisson rate 1.0, measuring average end-to-end latency
  2. Prediction Sensitivity: On real-world dataset, introduce prediction error by sampling ō_i ~ Uniform[(1-ε)o_i, (1+ε)o_i], measure latency degradation as ε increases from 0.0 to 0.8
  3. Scalability Analysis: Measure decision loop execution time per step as concurrent requests increase from 10 to 1000, comparing against IP benchmark solve time

## Open Questions the Paper Calls Out
- Question: Can joint design of output-length predictors and batching-scheduling policies achieve better competitive ratios than using predictions as independent inputs?
  - Basis in paper: Paper suggests jointly designing prediction mechanisms and batching-scheduling policies that operate simultaneously during inference
  - Why unresolved: Current work assumes predictions are provided; interaction between predictor design and scheduler is unexplored
  - What evidence would resolve it: Algorithm co-optimizing both components with competitive ratio analysis under predictor-scheduler coupling

- Question: How can single-worker MC-SF framework be extended to heterogeneous multi-GPU settings with load balancing?
  - Basis in paper: Paper identifies extending to multiple computational workers operating in parallel as important direction
  - Why unresolved: Multiple workers introduce matching decisions between requests and workers absent in single-worker analysis
  - What evidence would resolve it: Polynomial-time algorithm for multi-worker scheduling with provable competitive ratio or approximation guarantee

- Question: What competitive ratio is achievable under hybrid stochastic-adversarial arrivals where most requests follow unknown distribution but small fraction are extreme outliers?
  - Basis in paper: Paper identifies studying arrival models with most requests from unknown stochastic process but small fraction of extreme outliers as important direction
  - Why unresolved: Neither purely adversarial analysis nor fully stochastic models capture this realistic workload heterogeneity
  - What evidence would resolve it: Competitive ratio bounds parameterized by fraction of adversarial outliers or heavy-tail properties

## Limitations
- Theoretical guarantees limited to highly structured scenarios (identical prompt sizes, simultaneous arrivals, bounded prediction errors)
- Performance depends entirely on empirical validation in realistic settings under arbitrary online arrivals
- Memory constraint validation mechanism effectiveness depends on prediction quality, which is not fully specified
- Vidur simulator integration details for Llama2-70B on A100 GPUs are not fully specified

## Confidence
- **High Confidence**: Mechanism of prioritizing partially completed requests and greedily selecting shortest-output-length requests is clearly described and implemented in Algorithm 1; empirical results showing MC-SF outperforming FCFS are directly supported
- **Medium Confidence**: Theoretical competitive ratio analysis is sound within stated assumptions, but these assumptions (identical prompt sizes, simultaneous arrivals) are rarely met in practice
- **Medium Confidence**: Memory constraint validation through Equation (5) is well-specified, but effectiveness depends on prediction quality

## Next Checks
1. **Prediction Sensitivity Analysis**: Systematically vary prediction error margin ε and measure resulting latency degradation and memory overflow frequency to quantify practical impact of imperfect predictions
2. **Arrival Pattern Robustness**: Test MC-SF under bursty arrivals (high λ) versus steady arrivals (low λ) to validate whether algorithm maintains latency advantage across different traffic patterns
3. **Alternative Predictor Integration**: Implement and compare MC-SF with different prediction strategies (statistical estimators, learned models) to assess how predictor quality affects overall system performance and whether theoretical bounds remain meaningful in practice