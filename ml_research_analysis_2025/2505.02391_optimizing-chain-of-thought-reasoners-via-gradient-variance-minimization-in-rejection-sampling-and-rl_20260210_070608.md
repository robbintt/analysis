---
ver: rpa2
title: Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in
  Rejection Sampling and RL
arxiv_id: '2505.02391'
source_url: https://arxiv.org/abs/2505.02391
tags:
- sample
- sampling
- arxiv
- which
- units
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training large language models
  for chain-of-thought (CoT) reasoning by addressing the inefficiency in gradient
  estimation caused by uniform sampling strategies in existing methods like RAFT.
  The authors propose GVM-RAFT, a dynamic sample allocation strategy that minimizes
  stochastic gradient variance by adaptively distributing computational resources
  based on prompt-specific acceptance rates and gradient norms.
---

# Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL

## Quick Facts
- **arXiv ID:** 2505.02391
- **Source URL:** https://arxiv.org/abs/2505.02391
- **Reference count:** 40
- **Primary result:** GVM-RAFT achieves 2-4× speedup in convergence with improved accuracy over vanilla RAFT for CoT reasoning tasks

## Executive Summary
This paper addresses the inefficiency in training large language models for chain-of-thought reasoning by tackling the high variance in gradient estimation caused by uniform sampling in existing methods like RAFT. The authors propose GVM-RAFT, a theoretically grounded dynamic sample allocation strategy that minimizes stochastic gradient variance by adaptively distributing computational resources based on prompt-specific acceptance rates and gradient norms. The method achieves significant improvements in sample efficiency and convergence speed while maintaining or improving accuracy. The framework also generalizes to reinforcement learning algorithms like GRPO, demonstrating broad applicability to reasoning-oriented training paradigms.

## Method Summary
The authors introduce GVM-RAFT, which optimizes the allocation of computational resources in rejection sampling by minimizing the variance of stochastic gradients. Instead of uniformly sampling multiple CoT paths per prompt, GVM-RAFT dynamically allocates samples based on prompt-specific characteristics: acceptance rates and gradient norms. The method formulates an optimization problem that minimizes the trace of the covariance matrix of the stochastic gradient estimator, subject to a computational budget constraint. This leads to an adaptive sampling strategy where prompts with higher uncertainty or more complex reasoning paths receive more computational resources. The approach is theoretically justified through analysis of gradient variance and is implemented efficiently using moving averages to track acceptance rates and gradient norms during training.

## Key Results
- Achieves 2-4× speedup in convergence compared to vanilla RAFT on mathematical reasoning tasks
- Maintains or improves accuracy while using fewer computational resources
- Generalizes to reinforcement learning algorithms like GRPO with similar efficiency gains
- Effective across different model scales (Qwen2.5-3B, Qwen2.5-7B) and reasoning datasets (GSM8K, MATH, AIME)

## Why This Works (Mechanism)
The method works by addressing the fundamental inefficiency in rejection sampling where all prompts receive equal computational resources regardless of their difficulty or the variance in their gradient estimates. By minimizing gradient variance through adaptive sample allocation, GVM-RAFT ensures that computational resources are focused where they have the greatest impact on learning. The theoretical foundation shows that the optimal allocation depends on the ratio of acceptance rates to gradient norms across prompts, which captures both the likelihood of a sample being accepted and the magnitude of the gradient it produces.

## Foundational Learning
- **Chain-of-Thought reasoning:** Sequential reasoning steps generated by LLMs to solve complex problems - needed to understand the task domain and why intermediate reasoning steps matter for training
- **Rejection sampling:** Technique for generating samples from a target distribution using an acceptance-rejection mechanism - needed to understand the baseline RAFT method and its limitations
- **Gradient variance minimization:** Optimization principle where reducing variance in gradient estimates leads to faster and more stable convergence - needed to grasp the core theoretical contribution
- **Sample complexity:** Relationship between the number of samples needed and the accuracy of parameter estimation - needed to understand the efficiency gains claimed by the method
- **Stochastic optimization:** Framework for optimizing objectives with noisy gradient estimates - needed to place the work in the broader context of training large models

## Architecture Onboarding
- **Component map:** Prompts → Acceptance Rate Estimator & Gradient Norm Tracker → Dynamic Allocator → RAFT Sampling → Model Update
- **Critical path:** Input prompt → generate multiple reasoning paths → evaluate correctness → compute acceptance rate → estimate gradient norm → allocate samples → update model parameters
- **Design tradeoffs:** Adaptive sampling vs. computational overhead for tracking acceptance rates and gradient norms; theoretical optimality vs. practical implementation simplicity
- **Failure signatures:** Degraded performance when acceptance rate estimates become stale; inefficient allocation when gradient norm estimates are inaccurate; potential overfitting to prompt-specific patterns
- **First experiments:** 1) Compare convergence curves of GVM-RAFT vs. vanilla RAFT on GSM8K with fixed computational budget, 2) Analyze sample allocation patterns across different difficulty levels of math problems, 3) Test sensitivity to hyperparameters controlling the trade-off between exploration and exploitation in sample allocation

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Additional computational overhead for computing acceptance rates and gradient norms is not explicitly quantified in wall-clock time
- Theoretical analysis relies on assumptions about acceptance rate and gradient norm distributions that may not hold uniformly across all tasks
- Experiments are limited to mathematical reasoning tasks using Qwen models, limiting generalizability to other domains
- Claims about generalization to RL algorithms like GRPO are supported by limited experimentation

## Confidence
- **High:** Core theoretical contribution regarding gradient variance minimization is well-established in optimization literature
- **Medium:** Empirical results showing 2-4× speedup are convincing within tested mathematical reasoning tasks
- **Low:** Generalization to RL algorithms is supported by limited experimentation

## Next Checks
1. Conduct ablation studies comparing wall-clock training time between vanilla RAFT and GVM-RAFT implementations on identical hardware
2. Test the method across diverse reasoning domains including code generation, commonsense reasoning, and multimodal tasks
3. Perform sensitivity analysis on hyperparameters governing the dynamic allocation strategy across different model scales and task difficulties