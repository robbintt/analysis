---
ver: rpa2
title: 'When Language Overrules: Revealing Text Dominance in Multimodal Large Language
  Models'
arxiv_id: '2508.10552'
source_url: https://arxiv.org/abs/2508.10552
tags:
- modality
- text
- attention
- dominance
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of text dominance in multimodal\
  \ large language models, where models over-rely on textual inputs while underutilizing\
  \ other modalities like images, video, audio, time-series, and graphs. The authors\
  \ propose two metrics\u2014Modality Dominance Index (MDI) and Attention Efficiency\
  \ Index (AEI)\u2014to quantify this imbalance across different modalities and model\
  \ architectures."
---

# When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2508.10552
- **Source URL**: https://arxiv.org/abs/2508.10552
- **Reference count**: 6
- **Primary result**: Proposes MDI and AEI metrics to quantify text dominance; reduces MDI from 10.23 to 0.86 in LLaVA-7B using CLS-guided token compression

## Executive Summary
This paper addresses a critical issue in multimodal large language models where text tokens disproportionately dominate model attention compared to other modalities. The authors systematically analyze six leading multimodal models across different modalities (vision, audio, video, time-series, graphs) and identify three root causes of this imbalance: token redundancy in non-textual inputs, suboptimal fusion architectures, and task formulations that over-rely on text. To quantify this phenomenon, they introduce two novel metrics—Modality Dominance Index (MDI) and Attention Efficiency Index (AEI)—that measure the extent of modality attention imbalance and efficiency. Through extensive experiments, they demonstrate that text tokens consistently receive significantly higher attention weights than tokens from other modalities across diverse model architectures and tasks.

The paper proposes a practical token compression method guided by CLS token attention to mitigate text dominance. By reducing redundant tokens in non-textual modalities, this approach effectively rebalances cross-modal attention. When applied to LLaVA-7B, the method dramatically reduces the MDI from 10.23 to 0.86, demonstrating significant improvement in multimodal integration. The authors position their work as foundational for developing more equitable multimodal models that can truly leverage the complementary strengths of different data modalities rather than defaulting to text-based reasoning.

## Method Summary
The authors develop two novel metrics to quantify text dominance: the Modality Dominance Index (MDI) measures the extent to which one modality dominates attention relative to others, while the Attention Efficiency Index (AEI) evaluates the overall efficiency of cross-modal attention distribution. To mitigate identified text dominance, they propose a token compression method guided by CLS token attention that reduces redundant tokens in non-textual modalities. The method works by analyzing the attention weights of the CLS token and selectively removing less informative tokens from vision encoders. This approach is applied to LLaVA-7B, where the frozen visual encoder processes compressed token sequences while maintaining the original language model architecture. The compressed tokens retain the most salient visual information while eliminating redundancy that previously diluted their attention share.

## Key Results
- Analysis of six leading multimodal models reveals text tokens consistently dominate attention across all modalities
- MDI and AEI metrics successfully quantify text dominance, with MDI values ranging from 0.86 to 10.23 across different models
- Token compression guided by CLS attention reduces MDI from 10.23 to 0.86 in LLaVA-7B
- Three root causes identified: token redundancy in non-textual modalities, fusion architecture design, and text-favoring task formulations
- The compression method effectively rebalances cross-modal attention without requiring model retraining

## Why This Works (Mechanism)
The token compression method works by leveraging the attention distribution of the CLS token, which captures global context across all input tokens. By analyzing which visual tokens receive the least attention from the CLS token, the method identifies and removes redundant or less informative visual features. This selective pruning increases the relative attention share of remaining visual tokens without degrading their representational capacity. The approach exploits the observation that many visual tokens in standard vision encoders contain redundant information, and that the attention mechanism naturally assigns higher weights to more informative tokens. By reducing token count while preserving the most salient visual features, the method creates a more balanced attention landscape where non-textual modalities can effectively contribute to the multimodal reasoning process.

## Foundational Learning

**Multimodal Large Language Models**: AI systems that process and integrate multiple data types (text, images, audio, etc.) through unified architectures. Why needed: Understanding the baseline framework for analyzing cross-modal interactions. Quick check: Verify models use separate encoders with fusion mechanisms rather than monolithic architectures.

**Attention Mechanisms**: Mathematical operations that weight input tokens based on their relevance to each other, enabling selective focus on important information. Why needed: Core to understanding how models prioritize different modalities. Quick check: Confirm attention scores sum to 1 across tokens for each query position.

**Vision Transformers (ViTs)**: Transformer-based architectures that process images as sequences of patches rather than convolutional features. Why needed: Many multimodal models use ViTs as vision encoders with CLS tokens for global context. Quick check: Verify ViTs use patch embedding followed by positional encoding.

**Modality Fusion**: The architectural process of combining information from different modalities, which can occur at various depths (early, middle, or late fusion). Why needed: Fusion strategy significantly impacts how modalities influence each other. Quick check: Identify whether fusion occurs before or after individual modality encoding.

**Token Redundancy**: The phenomenon where multiple tokens encode similar or overlapping information, reducing overall efficiency. Why needed: Explains why visual tokens may have diluted attention impact. Quick check: Measure cosine similarity between adjacent visual token embeddings.

## Architecture Onboarding

**Component Map**: Visual Encoder (ViT) -> [CLS token generation] -> Fusion Layer -> Language Model Backbone -> Output Generator

**Critical Path**: Input tokens → Individual modality encoding → Cross-modal attention → Fusion → Final prediction

**Design Tradeoffs**: 
- Early fusion enables better cross-modal interaction but requires careful modality alignment
- Late fusion preserves modality-specific features but may limit interaction depth
- Token compression reduces computational cost but requires preserving critical information

**Failure Signatures**:
- High MDI values (>5) indicate severe text dominance
- Low AEI values suggest inefficient attention distribution
- Performance degradation after compression indicates over-aggressive token removal

**Three First Experiments**:
1. Compute MDI and AEI metrics on baseline LLaVA-7B to establish text dominance baseline
2. Apply token compression at varying compression rates (10%, 25%, 50%) to identify optimal balance
3. Compare attention distribution heatmaps before and after compression to visualize rebalancing effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural redesign or task reformulation mitigate text dominance as effectively as token compression, and what are the potential trade-offs?
- Basis in paper: The Conclusion explicitly states that future work will explore "architectural redesign to foster more integrated modality fusion and task reformulation to reduce over-reliance on textual prompts" to evaluate their synergy with token compression.
- Why unresolved: The current work primarily validates token compression as a mitigation strategy, leaving these alternative approaches untested.
- What evidence would resolve it: Comparative studies showing MDI and AEI scores for models specifically trained with modified fusion architectures or reformulated tasks against the token compression baseline.

### Open Question 2
- Question: Can the proposed CLS-guided token compression method be effectively adapted for non-visual modalities like audio, time-series, and graphs which may lack an explicit [CLS] token?
- Basis in paper: The token compression method (Section 5) is implemented specifically for LLaVA-1.5 using a "frozen visual encoder" and [CLS] tokens. The paper analyzes text dominance in Audio and Time-series models (Table 1) but only demonstrates the solution for the image modality.
- Why unresolved: The reliance on a specific Vision Transformer component ([CLS] token) suggests the current solution may not directly transfer to other modalities that use different tokenization or encoding schemes.
- What evidence would resolve it: Experiments applying equivalent token reduction techniques to Audio (e.g., Qwen-Audio) and Time-series (e.g., ChatTS) models, demonstrating similar reductions in MDI.

### Open Question 3
- Question: Does a lower, more balanced Modality Dominance Index (MDI ≈ 1) correlate with improved accuracy on downstream tasks, or does it simply reflect a statistical redistribution of attention?
- Basis in paper: The paper claims the method "effectively rebalances model attention" and offers a foundation for "more equitable" models, but the experimental results focus heavily on the reduction of the MDI metric itself rather than a comprehensive analysis of task performance accuracy improvements.
- Why unresolved: While attention is rebalanced, it remains unclear if this "equitable integration" leads to better reasoning capabilities or correct answers in complex multimodal tasks.
- What evidence would resolve it: A correlation analysis plotting task accuracy (e.g., VQA scores) against the achieved MDI values for various models and compression rates.

## Limitations
- Evaluation primarily focuses on vision-language tasks, with limited exploration of how text dominance manifests in other modalities like audio, video, or time-series data
- Proposed token compression method may not generalize equally well to modalities with different structural characteristics
- MDI and AEI metrics require further validation across diverse model architectures and task types to establish their robustness

## Confidence
- **High**: Core finding that text dominance is a prevalent issue across multimodal models
- **Medium**: Effectiveness of the token compression method, as results are demonstrated primarily on one model variant (LLaVA-7B)
- **Low**: Generalizability of the three identified causes (token redundancy, fusion architecture, task formulation) to all multimodal model families and modalities

## Next Checks
1. Evaluate the token compression method across diverse multimodal models including audio-language and video-language architectures to test cross-modal applicability.
2. Validate the MDI and AEI metrics on models with different fusion strategies (early fusion vs late fusion) to confirm their sensitivity to architectural variations.
3. Conduct ablation studies on task formulations to systematically isolate the impact of different prompting strategies on modality attention distribution.