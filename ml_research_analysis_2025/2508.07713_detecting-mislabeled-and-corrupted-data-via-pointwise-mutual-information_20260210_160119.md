---
ver: rpa2
title: Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information
arxiv_id: '2508.07713'
source_url: https://arxiv.org/abs/2508.07713
tags:
- noise
- data
- labels
- information
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mutual information-based method for detecting
  and filtering mislabeled and corrupted data in supervised learning. The approach
  leverages pointwise mutual information (PMI) to quantify the statistical dependency
  between inputs and labels, identifying samples with low PMI scores as likely noisy
  or mislabeled instances.
---

# Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information

## Quick Facts
- arXiv ID: 2508.07713
- Source URL: https://arxiv.org/abs/2508.07713
- Reference count: 40
- Primary result: MI-based filtering improves MNIST classification accuracy by up to 15% under 80% label corruption

## Executive Summary
This paper introduces a mutual information-based method for detecting and filtering mislabeled and corrupted data in supervised learning. The approach leverages pointwise mutual information (PMI) to quantify the statistical dependency between inputs and labels, identifying samples with low PMI scores as likely noisy or mislabeled instances. Using the Kraskov-Stögbauer-Grassberger (KSG) estimator, the method computes local MI contributions for each sample, enabling fine-grained data quality assessment without requiring labeled validation data or model retraining.

## Method Summary
The method compresses MNIST images to latent space using a VAE, then computes per-sample PMI scores via the KSG estimator: I(xi; yi) = ψ(k) + ψ(N) − ψ(nx(i)+1) − ψ(ny(i)+1). Samples are ranked by MI (global or class-wise) and high-MI subsets are selected for training a logistic regression classifier. Experiments on MNIST with synthetic label noise (0.2, 0.5, 0.8 corruption rates) demonstrate accuracy improvements up to 15% compared to random sampling baselines.

## Key Results
- MI-based selection improves MNIST classification accuracy by up to 15% under 80% label corruption
- Mislabeled samples consistently exhibit negative PMI values, effectively separating them from correctly labeled samples
- Method shows robustness to benign input modifications (Gaussian noise, mild augmentations) while flagging semantically corrupted inputs

## Why This Works (Mechanism)

### Mechanism 1: PMI-based Noise Detection via Statistical Dependency
- Claim: Samples with low pointwise mutual information between input and label are more likely to be mislabeled or corrupted
- Core assumption: True underlying data distribution has meaningful input-label dependencies that persist locally; corruption disrupts these dependencies in detectable ways
- Evidence: Mislabeled points exhibit negative MI values in experiments; limited direct validation from corpus
- Break condition: Systematic noise patterns that preserve statistical dependencies within wrong classes

### Mechanism 2: KSG Estimator for Local MI Attribution Without Distribution Knowledge
- Claim: KSG enables per-sample MI computation directly from data without requiring true probability distributions
- Core assumption: k-NN structure in latent space reflects meaningful statistical dependencies; VAE latent representation preserves input-label relationships
- Evidence: KSG formula provided and used in experiments; standard estimator choice not challenged in corpus
- Break condition: High-dimensional spaces or insufficient samples causing k-NN distances to become uninformative

### Mechanism 3: Selective Robustness to Benign vs. Semantic-Altering Perturbations
- Claim: MI-based filtering preserves benign visual noise while rejecting semantically corrupted inputs
- Core assumption: Semantic content maps to consistent regions in latent space; VAE representation separates semantic from non-semantic variation
- Evidence: Gaussian noise maintains stable MI scores while strong geometric transforms produce low MI scores
- Break condition: Augmentations that intentionally alter semantics (mixup, CutMix) may be incorrectly flagged as noisy

## Foundational Learning

- **Concept: Mutual Information as Statistical Dependency**
  - Why needed here: Method hinges on understanding MI as "reduction in uncertainty about one variable when the other is known"
  - Quick check: If I(X;Y) = 0, what does that imply about their relationship? (Answer: They are statistically independent)

- **Concept: Pointwise Mutual Information vs. Global MI**
  - Why needed here: Innovation decomposes global MI into per-sample contributions to identify individual noisy points
  - Quick check: If PMI(x,y) < 0 for a specific sample, does that mean the dataset has negative global MI? (Answer: No)

- **Concept: k-Nearest Neighbor Density Estimation**
  - Why needed here: KSG estimator relies on comparing neighbor counts in joint vs. marginal spaces
  - Quick check: In KSG, what does it mean if a point has many joint-space neighbors but few marginal-space neighbors? (Answer: High local MI—strong coupling)

## Architecture Onboarding

- **Component map:** Data preprocessing -> VAE encoder -> latent representations -> KSG estimator -> per-sample PMI scores -> score ranking -> threshold-based selection -> filtered dataset -> downstream classifier training

- **Critical path:** VAE quality directly affects MI estimation—if latent space doesn't preserve input-label relationships, KSG scores will be meaningless. Validate latent representations maintain class separability before computing PMI.

- **Design tradeoffs:**
  - Global vs. class-wise selection: Class-wise preserves balance but requires sufficient samples per class; global is simpler but may disproportionately filter minority classes
  - k (neighbor count): Lower k captures finer local structure but increases variance; higher k smooths estimates but may miss fine-grained noise
  - Retention ratio: Aggressive filtering removes more noise but risks discarding clean samples near decision boundaries

- **Failure signatures:**
  - High false positive rate on difficult but clean samples (ambiguous handwriting)
  - Systematic class imbalance after filtering if certain classes naturally have lower PMI distributions
  - No improvement over random sampling when noise rate is low

- **First 3 experiments:**
  1. Baseline validation on clean data: Compute PMI distribution on uncorrupted MNIST; verify scores are generally positive and well-distributed across classes
  2. Controlled noise injection test: Inject 20% random label noise, compute PMI, measure AUC for detecting corrupted samples
  3. Threshold sensitivity analysis: Vary retention ratio (30%, 50%, 70%, 90%) and compare classifier accuracy under 50% label noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MI-based selection framework maintain its efficacy when applied to complex, high-dimensional benchmarks like CIFAR-10 or TinyImageNet?
- Basis: Authors explicitly identify limitation to MNIST dataset and state "Future work will explore the extension of this framework to more complex models and benchmarks"
- Why unresolved: Current validation restricted to simple MNIST dataset using logistic regression
- What evidence would resolve it: Empirical results demonstrating consistent accuracy improvements on CIFAR-10 or TinyImageNet

### Open Question 2
- Question: How does the performance of this model-agnostic method compare to state-of-the-art noise-robust training techniques?
- Basis: Authors note in conclusion they "will also include comparisons with stronger baselines" in future iterations
- Why unresolved: Current experiments primarily compare against random sampling and internal variations
- What evidence would resolve it: Comparative analysis showing accuracy and noise detection rates against contemporary methods under identical noise conditions

### Open Question 3
- Question: Is the method sensitive to the quality of the dimensionality reduction step, and can it remain robust if feature extractor is trained on noisy data?
- Basis: Method claims to be "model-agnostic" yet experimental setup relies on VAE for projection before MI calculation
- Why unresolved: Paper does not clarify if VAE was trained on clean or noisy data
- What evidence would resolve it: Ablation study analyzing MI score distributions when feature extractor is trained on datasets with varying corruption levels

### Open Question 4
- Question: Can the method distinguish between "hard" (clean but low-probability) examples and "noisy" examples in instance-dependent noise scenarios?
- Basis: Paper tests synthetic random label noise but identifies instance-dependent noise as difficult real-world issue
- Why unresolved: Hard examples may naturally exhibit low PMI due to class ambiguity
- What evidence would resolve it: Evaluation on datasets with instance-dependent noise patterns to measure false positive rate for difficult clean samples

## Limitations
- Current validation limited to MNIST dataset with logistic regression classifier
- Method effectiveness on complex high-dimensional datasets like CIFAR-10 remains unproven
- Performance comparison to state-of-the-art noise-robust training techniques not established

## Confidence
- VAE representation quality affecting MI estimation: Medium
- KSG estimator behavior in high-dimensional latent spaces: Medium
- 15% accuracy improvement claim vs. random sampling: Medium
- Robustness to benign vs. semantic corruption distinction: Medium

## Next Checks
1. VAE representation validation: Verify latent representations maintain class separability and semantic structure through t-SNE visualization and classification accuracy tests in the latent space
2. KSG estimator sensitivity analysis: Systematically vary k (neighbor count) and dimensionality to identify break points where estimation quality degrades
3. Noise type ablation study: Test method's effectiveness across different noise patterns (random flips, confusion between similar classes, Gaussian input corruption) to quantify robustness and identify failure scenarios