---
ver: rpa2
title: Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning
arxiv_id: '2509.23285'
source_url: https://arxiv.org/abs/2509.23285
tags:
- arxiv
- reasoning
- wang
- tool
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tool-Light, a framework designed to improve
  the effectiveness of tool-integrated reasoning (TIR) in large language models. The
  method addresses suboptimal tool usage behaviors by analyzing TIR from the perspective
  of information entropy, showing that tool calls cause distinct changes in entropy
  distribution.
---

# Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning

## Quick Facts
- arXiv ID: 2509.23285
- Source URL: https://arxiv.org/abs/2509.23285
- Reference count: 40
- Primary result: Entropy-guided sampling + two-stage DPO training significantly improves tool-integrated reasoning efficiency and accuracy across 10 challenging datasets

## Executive Summary
This paper introduces Tool-Light, a framework designed to improve the effectiveness of tool-integrated reasoning (TIR) in large language models. The method addresses suboptimal tool usage behaviors by analyzing TIR from the perspective of information entropy, showing that tool calls cause distinct changes in entropy distribution. The proposed framework includes entropy-guided sampling for dataset construction and a two-stage training pipeline: supervised fine-tuning followed by self-evolved direct preference optimization. Experiments on 10 challenging datasets demonstrate significant improvements in TIR efficiency and accuracy compared to baselines. The method effectively reduces overthinking and unnecessary tool calls while maintaining reasoning performance.

## Method Summary
Tool-Light employs a two-stage training pipeline for TIR optimization. First, supervised fine-tuning (SFT) trains basic tool-use competency. Second, a self-evolved direct preference optimization (DPO) stage iteratively refines tool usage efficiency. The framework introduces entropy-guided sampling that branches reasoning paths at high-entropy token positions, generating diverse tool-use trajectories with lower computational cost than exhaustive rollout sampling. A novel difficulty-adaptive pair-selection mechanism classifies samples as hard or easy based on correct-trajectory percentage, then applies different positive/negative criteria per difficulty level. The training uses LoRA (rank=8) with 2 DPO loops optimal, mixing vanilla and entropy-guided sampling at a 13:7 ratio.

## Key Results
- Entropy-guided sampling produces more diverse tool-use trajectories with lower computational cost than vanilla rollout sampling
- Two-stage training (SFT + Self-Evolved DPO) prevents models from conflating correct reasoning with excessive tool use
- Dynamic difficulty-adaptive pair selection prevents overfitting to simple patterns while ensuring efficiency gains on mastered problems
- Optimal performance achieved with 2 DPO loops; additional loops cause overfitting and performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Guided Sampling for Efficient Exploration
The method generates a main reasoning chain, computes average entropy at token positions (first 10, 20, 30, 40, 50 tokens per step), identifies top-k highest-entropy positions, then branches multiple paths from these decision points. High-entropy positions correlate with reasoning uncertainty where tool-call decisions are most impactful. This strategy produces more diverse tool-use trajectories with lower computational cost than exhaustive rollout sampling.

### Mechanism 2: Two-Stage DPO Separates Competency Acquisition from Efficiency Optimization
Stage 1 (SFT) teaches basic tool-use syntax and reasoning patterns. Stage 2A (Pre-Aligned DPO) uses preference pairs where positive = correct + minimal tools + low entropy, negative = incorrect + more tools, teaching the model to reduce unnecessary calls. Stage 2B (Self-Evolved DPO) iteratively samples from the current model, dynamically adjusting difficulty—hard samples get longer positive chains, easy samples enforce minimal-tool efficiency.

### Mechanism 3: Difficulty-Adaptive Positive-Negative Pair Selection
Samples are classified by correct-trajectory percentage (hard: ≤50%, easy: ≥50%). For hard samples, positive = longest correct chain (encouraging tool use when needed), negative = shortest incorrect chain. For easy samples, positive = fewest tools + lowest entropy, negative = most tools + incorrect. This adapts training signal to model proficiency.

## Foundational Learning

- **Concept: Information Entropy in Autoregressive Generation**
  - Why needed here: The entire sampling strategy depends on computing and interpreting entropy distributions across token positions in reasoning chains
  - Quick check question: Can you explain why high entropy at a token position indicates model uncertainty, and how this relates to branching search strategies?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The core training innovation applies DPO with specialized pair-selection criteria rather than standard RL reward signals
  - Quick check question: How does DPO's loss function (maximizing log-ratio difference between preferred and dispreferred outputs) differ from PPO's clipped objective?

- **Concept: Tool-Integrated Reasoning Paradigm**
  - Why needed here: Understanding what TIR enables (external computation, retrieval) and what failure modes it introduces (overuse, underuse, overthinking) is prerequisite to appreciating the solution
  - Quick check question: What are the three suboptimal behaviors this paper identifies in TIR models, and why can't standard SFT alone fix them?

## Architecture Onboarding

- **Component map**: Source Data Filter → Entropy Calculator → Hybrid Sampler → Difficulty Classifier → Pair Selector → Two-Stage Trainer

- **Critical path**: Source data construction → M_sft inference → Entropy-guided + vanilla sampling → Pair selection → Pre-Aligned DPO → Self-Evolved DPO (2 loops optimal per Table 2)

- **Design tradeoffs**:
  - Entropy-guided vs. vanilla sampling ratio: Paper finds 13:7 optimal. Higher vanilla ratio increases tool use but reduces efficiency metric
  - DPO loop count: 2 loops optimal. More loops cause overfitting, fewer leave performance on table
  - Hard/easy threshold: 50% correct-trajectory boundary. Adjust based on base model capability

- **Failure signatures**:
  - Overthinking persists: Check if negative examples actually have more tool calls than positives—pair selection may be inverted
  - Accuracy degrades: DPO β hyperparameter may be too high, over-penalizing reasonable exploration
  - Tool underuse spikes: Self-Evolved DPO may be over-pruning on easy samples—relax positive criteria to allow necessary tools
  - Entropy branching yields low diversity: Top-k positions may be too late in reasoning chain—earlier branching captures more meaningful divergences

- **First 3 experiments**:
  1. **Entropy baseline**: Train with vanilla sampling only (no entropy-guided), compare Efficiency/Necessity metrics against hybrid approach
  2. **Loop ablation**: Run Self-Evolved DPO for 1, 2, 3, 4, 5 loops on held-out validation set
  3. **Pair-selection sanity check**: Randomize positive/negative selection and measure performance drop

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance degradation observed in self-evolved DPO loops beyond the second iteration be mitigated to allow for continuous improvement? Table 2 and Section 5.3 show that performance, efficiency, and necessity metrics peak at 2 loops and decline at 3, 4, and 5 loops, attributed to overfitting. The framework lacks mechanisms to filter low-quality pairs generated in later loops or early-stopping criteria based on data utility.

### Open Question 2
Does the strict selection of low-entropy trajectories as positive examples inadvertently prune complex but correct reasoning paths? Section 4.2 dictates that positive examples must have the "lowest entropy," potentially discarding high-entropy "exploratory" paths that might be robust for generalization. The paper doesn't ablate the impact of entropy-based selection on diversity of learned reasoning strategies.

### Open Question 3
Can the correlation between information entropy and tool usage efficiency be generalized to multimodal reasoning domains? Section 3 analyzes entropy distribution using text-based QA datasets, but entropy dynamics of visual or audio tokens during tool calls are unknown and may not exhibit the same patterns observed in text.

## Limitations
- Entropy-guided sampling lacks direct experimental validation against simpler alternatives like random branching
- Adaptive pair-selection mechanism's effectiveness depends heavily on accurate difficulty classification with an arbitrary 50% threshold
- Self-evolved DPO framework introduces significant complexity with multiple interacting hyperparameters
- Limited ablation studies showing sensitivity to key parameters like difficulty thresholds

## Confidence
- **High confidence**: Two-stage training pipeline improves TIR efficiency metrics compared to SFT-only baselines
- **Medium confidence**: Entropy-guided sampling produces more diverse tool-use trajectories than vanilla sampling
- **Low confidence**: Dynamic difficulty-adaptive pair selection is necessary for optimal performance

## Next Checks
1. **Entropy-guided vs. random branching ablation**: Run the full pipeline with entropy-guided sampling disabled (branch at random positions instead of high-entropy positions) and compare Efficiency/Necessity metrics

2. **Pair-selection criteria ablation**: Implement two simplified variants: (a) fixed pair-selection regardless of difficulty, and (b) random positive/negative assignment. Compare against the full adaptive approach

3. **Loop count sensitivity analysis**: Extend the DPO loop ablation beyond 2 loops to 1, 3, 4, and 5 iterations on a held-out validation set. Plot Efficiency, Necessity, and accuracy metrics across all loop counts