---
ver: rpa2
title: Contributions to the Decision Theoretic Foundations of Machine Learning and
  Robust Statistics under Weakly Structured Information
arxiv_id: '2501.10195'
source_url: https://arxiv.org/abs/2501.10195
tags:
- decision
- contribution
- data
- preference
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores decision-theoretic foundations for machine
  learning and robust statistics under weakly structured information. It addresses
  the challenge of making decisions under uncertainty when preference and uncertainty
  information is incomplete or imprecise.
---

# Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information

## Quick Facts
- arXiv ID: 2501.10195
- Source URL: https://arxiv.org/abs/2501.10195
- Reference count: 40
- Primary result: Decision-theoretic framework for robust comparison of classifiers under weakly structured information using preference systems and credal sets.

## Executive Summary
This thesis develops a decision-theoretic framework for comparing machine learning models and statistical methods under conditions of incomplete or imprecise information. The core challenge addressed is how to make robust decisions when preference information is partially ordinal and partially cardinal, and when uncertainty is represented by imprecise probabilities (credal sets). The framework generalizes stochastic dominance to handle this weakly structured information, enabling robust comparisons of algorithms even when performance metrics have mixed scales or when uncertainty about data distributions exists.

## Method Summary
The method centers on constructing preference systems that encode both ordinal dominance relations (e.g., one algorithm is at least as good as another on all metrics) and partial cardinal information (e.g., the intensity of preference between specific outcomes). These systems are combined with credal sets representing imprecise probabilities over datasets. Dominance between random variables (representing algorithm performances) is operationalized through linear programming: checking whether one random variable has greater expected utility than another under all compatible utility functions and all probability distributions in the credal set. Statistical significance is assessed through permutation-based tests that randomize the association between algorithms and observed performances.

## Key Results
- Generalized Stochastic Dominance (GSD) provides a principled framework for comparing algorithms under mixed ordinal-cardinal information and imprecise probabilities
- The GSD relation can be checked via linear programming, making it computationally tractable
- Permutation-based statistical tests enable robust significance testing for GSD relations without distributional assumptions
- Applications demonstrate practical utility in multi-dimensional poverty measurement, medical diagnosis, and credit risk assessment
- Empirical decision theory offers a protocol-based approach to estimate choice functions without requiring Closed World assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference systems allow for the principled integration of heterogeneous data scales (ordinal and partial cardinal) into a unified comparison framework.
- Mechanism: The system models data not by a single arbitrary scale, but by a set $U_A$ of all compatible utility functions consistent with the ordinal relation $R_1$ and the intensity relation $R_2$. If one random variable dominates another under *all* admissible utility functions in this set, the dominance is robust to the specific scaling choice.
- Core assumption: The relations $R_1$ and $R_2$ are consistent (simultaneously representable by a utility function) as per Definition 2.
- Evidence anchors:
  - [abstract]: Mentions "combining ordinal and partial cardinal information" and "scale-robust ordering principle."
  - [Section A.1.3]: Defines the consistency criterion required for the preference system to be valid.
  - [corpus]: Weak direct support; neighbor papers focus on specific ML applications rather than the underlying preference structure logic.
- Break condition: If the preference system is inconsistent (Definition 2 violated), the set of representations $U_A$ is empty, and the mechanism fails to produce a comparison.

### Mechanism 2
- Claim: Regularization via the granularity parameter $\delta$ enables the balancing of discriminative power against robustness in statistical comparisons.
- Mechanism: Instead of requiring dominance over all possible compatible scales, the framework allows a "granularity" threshold $\delta$. This restricts the set of admissible utility functions $N^\delta_A$ to those where utility differences are "noticeable" (above $\delta$). This prevents small, potentially noisy differences in ordinal metrics from blocking dominance relations.
- Core assumption: A small $\delta$ exists such that $N^\delta_A$ is non-empty, meaning the system is $\delta$-consistent.
- Evidence anchors:
  - [Section A.1.3]: Defines $\delta$-consistency and the normalized representation set $N^\delta_A$.
  - [Section B.1.2]: Discusses regularization as a technique to mitigate sampling noise in empirical preference systems.
  - [corpus]: No direct corpus support for this specific regularization technique in related papers.
- Break condition: If $\delta$ is set too high relative to the preference intensities, $N^\delta_A$ becomes empty, and no acts can be compared.

### Mechanism 3
- Claim: Permutation-based testing provides a non-parametric method to verify dominance relations (GSD) without relying on distributional assumptions.
- Mechanism: The mechanism operationalizes statistical testing by computing a test statistic (based on the dominance relation) on observed samples. It then simulates the null distribution by randomly permuting the labels of the observations (e.g., which classifier produced which result) and recomputing the statistic. Significance is determined by the extremity of the observed statistic relative to this permutation distribution.
- Core assumption: Under the null hypothesis, the samples are exchangeable (i.i.d.).
- Evidence anchors:
  - [abstract]: Mentions "permutation-based statistical tests" enabling "consistent estimation and robust statistical comparison."
  - [Section C.1.2]: Describes the use of permutation tests for checking GSD relations from i.i.d. samples.
  - [corpus]: "Robust Statistics vs. Machine Learning..." discusses robustness but focuses on GNSS outliers rather than permutation testing for dominance.
- Break condition: If the i.i.d. assumption is severely violated (e.g., strong temporal correlation), the permutation distribution does not reflect the true null distribution, invalidating the p-value.

## Foundational Learning

- Concept: **Partial Orders and Preorders**
  - Why needed here: The core data structure, the Preference System, relies on a preorder $R_1$ (ordinal info) and a preorder on pairs $R_2$ (intensity). Understanding that $R_1$ allows for incomparability (unlike total orders) is essential for interpreting "weak views" of choice functions.
  - Quick check question: If $a$ is preferred to $b$ and $b$ is preferred to $c$, is $c$ necessarily comparable to $a$ in a preorder? (Answer: Yes, via transitivity, but strict preference is not guaranteed without antisymmetry; actually, strictly speaking, preorders are reflexive and transitive. If $a \ge b$ and $b \ge c$, then $a \ge c$. The point is distinguishing strict preference $P_R$ from indifference $I_R$ and incomparability).

- Concept: **Expected Utility (EU) and Stochastic Dominance (SD)**
  - Why needed here: GSD is explicitly defined as a generalization of First-Order Stochastic Dominance (FSD). Section A.1.2 contrasts the "strong" view of EU with the "weak" view of FSD. Without this baseline, the "Generalized" aspect is meaningless.
  - Quick check question: Why does First-Order Stochastic Dominance often result in "incomparable" outcomes when Expected Utility usually picks a single winner?

- Concept: **Credal Sets**
  - Why needed here: The framework models uncertainty not with a single probability measure $\pi$, but with a set of measures $M$ (a credal set). This models "weakly structured information" about the uncertainty itself.
  - Quick check question: How does a credal set differ from a Bayesian confidence interval? (Clue: Credal sets represent a state of belief, not a frequency of estimation error).

## Architecture Onboarding

- Component map: Consequence Space $\mathcal{A}$ -> Preference System $\mathcal{A}=[A, R_1, R_2]$ -> Credal Set $\mathcal{M}$ -> Linear Programming Solver -> GSD Relation / Choice Set

- Critical path: The accurate definition of $R_2$ (intensity relation). If $R_2$ is empty, the system collapses to standard First-Order Stochastic Dominance. If $R_2$ is too strict, $U_A$ may become empty or highly constrained, leading to false positives (spurious dominance) if not carefully regularized.

- Design tradeoffs:
  - **Robustness vs. Decisiveness:** Using a larger credal set $\mathcal{M}$ (more uncertainty) increases robustness but makes finding dominance harder (more "incomparable" results).
  - **Complexity vs. Information:** Including partial cardinal info ($R_2$) increases complexity (requires LP solving vs. simple sort) but allows for finer distinctions than pure ordinal comparison.

- Failure signatures:
  - **Empty Choice Set:** Indicates constraints are too tight (possibly inconsistent $R_1, R_2$).
  - **Universal Indeterminacy:** All algorithms are incomparable. Suggests $\mathcal{M}$ is too wide or the regularization $\delta$ is too low (allowing too many conflicting utility functions).

- First 3 experiments:
  1. **Sanity Check (LP formulation):** Create a toy dataset with 2 metrics (Accuracy, Speed) and 2 algorithms. Define $R_1$ as component-wise dominance. Manually calculate the dominance condition and verify the LP solver outputs the same "non-dominated" set.
  2. **Scale Sensitivity:** Run a benchmark comparing classifiers. First, treat all metrics as purely ordinal ($R_2$ empty). Then, add partial cardinal info ($R_2$ constraints) on Accuracy. Observe if the "GSD-front" (set of non-dominated classifiers) shrinks.
  3. **Robustness to Noise:** Perturb the dataset probabilities (create a small credal set $\mathcal{M}$ around the empirical distribution). Check if the dominance relations identified in Experiment 2 persist across the credal set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can empirical choice functions derived from act-consequence protocols serve as consistent estimators for theoretical choice functions without relying on the Closed World or Small World Assumptions?
- Basis in paper: [explicit] Section A.3 proposes "empirical decision theory" using protocols of act-consequence pairs to avoid specifying states a priori.
- Why unresolved: While initial results suggest feasibility, formal desiderata beyond consistency and proofs for general choice functions are lacking.
- What evidence would resolve it: Proofs of strong consistency for empirical GSD-based choice functions and a formal set of axioms for empirical choice functions.

### Open Question 2
- Question: How can statistical inference be developed for partial order-valued data using the proposed depth-based statistical models?
- Basis in paper: [explicit] Section C.3 identifies "estimation of model parameters," "one- and two-sample tests," and "regression analysis" as necessary next steps.
- Why unresolved: Current work covers descriptive analysis and modeling (Contribution 9), but formal inferential methods for this non-standard data type are missing.
- What evidence would resolve it: The formulation of consistent estimators for depth-based model parameters and valid statistical tests for comparing samples of partial orders.

### Open Question 3
- Question: Can the characterization of utility representations for spaces with a single cardinal dimension be exploited to reduce the computational complexity of Generalized Stochastic Dominance (GSD) linear programs?
- Basis in paper: [explicit] Section C.3 notes that Proposition 8 suggests a potential drastic reduction in computational complexity for the single cardinal dimension case.
- Why unresolved: The theoretical link is established, but a specific efficient algorithm or reformulation of the linear programs has not yet been developed.
- What evidence would resolve it: An algorithm for the single-cardinal case with lower time complexity than the general LP-based approach.

## Limitations

- The specific implementation details of the linear programming formulation for GSD dominance checks, particularly the encoding of intensity relations ($R_2$) and regularization parameters, are not fully specified in the available summary.
- The validity of the permutation testing approach depends heavily on the i.i.d. assumption, and the framework's performance under dependent data structures remains unclear.
- While the framework claims to handle weakly structured information, the conditions under which the preference system becomes inconsistent or the credal set becomes too wide (leading to universal indeterminacy) are not quantified.

## Confidence

- **High Confidence:** The theoretical foundation linking preference systems to GSD relations and the core mechanism of using linear programming for dominance checks are well-established within the decision theory literature.
- **Medium Confidence:** The practical utility of the framework for real-world problems (poverty measurement, medical diagnosis) is supported by the thesis, but the specific empirical results and their generalizability require further validation.
- **Low Confidence:** The robustness of the permutation-based statistical tests under violated assumptions (e.g., non-i.i.d. data) and the precise tuning of regularization parameters for optimal performance are not fully addressed.

## Next Checks

1. **Implementation Verification:** Implement the LP formulation for GSD dominance using a simple synthetic dataset. Compare the results with manual calculations to ensure the constraints and objective function are correctly encoded.

2. **Robustness to Dependency:** Apply the permutation testing framework to a dataset with known temporal correlation. Compare the p-values obtained with those from a bootstrap approach to assess the impact of violated i.i.d. assumptions.

3. **Parameter Sensitivity Analysis:** Conduct a grid search over the regularization parameter $\delta$ for a benchmark comparison problem. Plot the size of the GSD-Front as a function of $\delta$ to identify the range where the framework provides informative, yet robust, results.