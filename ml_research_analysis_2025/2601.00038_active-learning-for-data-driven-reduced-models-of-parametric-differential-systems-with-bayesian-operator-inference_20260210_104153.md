---
ver: rpa2
title: Active learning for data-driven reduced models of parametric differential systems
  with Bayesian operator inference
arxiv_id: '2601.00038'
source_url: https://arxiv.org/abs/2601.00038
tags:
- parameter
- training
- sampling
- parametric
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a Bayesian framework for data-driven reduced-order
  modeling of parametric dynamical systems, addressing the challenge that model quality
  depends critically on training data selection. The approach combines operator inference
  with Bayesian regression to construct probabilistic reduced-order models that provide
  prediction uncertainties.
---

# Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference

## Quick Facts
- arXiv ID: 2601.00038
- Source URL: https://arxiv.org/abs/2601.00038
- Authors: Shane A. McQuarrie; Mengwu Guo; Anirban Chaudhuri
- Reference count: 40
- Key outcome: Bayesian operator inference with active learning achieves >10× accuracy improvement and superior stability with half the training data compared to random sampling

## Executive Summary
This work addresses the challenge of constructing accurate and stable reduced-order models (ROMs) for parametric dynamical systems when training data is limited. The authors propose a Bayesian operator inference framework that provides probabilistic ROMs with uncertainty quantification, enabling an active learning strategy for optimal parameter selection. By adaptively choosing training parameters based on prediction uncertainties, the method significantly improves global accuracy and stability while reducing the required training data.

The approach was validated on a 1D diffusion-reaction equation and a 2D viscous Burgers' equation, demonstrating substantial improvements over random sampling. With adaptive sampling, ROM error decreased from over 10% to below 1%, and the variance in error was reduced by three orders of magnitude. These results highlight the effectiveness of uncertainty-aware active learning in producing efficient and robust ROMs for data-limited applications.

## Method Summary
The proposed method combines Bayesian operator inference with an active learning strategy to construct probabilistic reduced-order models for parametric differential systems. The Bayesian framework provides prediction uncertainties that guide the selection of new training parameters, focusing on regions where the model is least certain. This adaptive sampling approach aims to improve global accuracy and stability with fewer training samples compared to random sampling.

The method was tested on two model problems: a one-dimensional diffusion-reaction equation and a two-dimensional viscous Burgers' equation. In both cases, the adaptive sampling strategy outperformed random sampling, achieving comparable or better accuracy with approximately half the training data. The ROM error was significantly reduced, and the variance in error was dramatically decreased, demonstrating the effectiveness of the uncertainty-aware active learning approach.

## Key Results
- ROM error decreased from over 10% to below 1% with adaptive sampling versus random sampling
- Variance in ROM error reduced by three orders of magnitude with adaptive sampling
- Achieved comparable accuracy and superior stability with only about half the training data

## Why This Works (Mechanism)
The Bayesian operator inference framework provides probabilistic ROMs that quantify prediction uncertainties. These uncertainties guide the active learning strategy, directing new training data collection to regions where the model is least certain. This targeted approach improves global accuracy and stability more efficiently than random sampling.

The adaptive sampling strategy effectively balances exploration and exploitation, focusing on parameters that are likely to yield the greatest improvement in model quality. By iteratively refining the ROM based on uncertainty estimates, the method converges to a more accurate and stable model with fewer training samples, addressing the challenge of data-limited scenarios.

## Foundational Learning
- **Bayesian regression**: Needed for uncertainty quantification in ROMs; check by verifying posterior distributions are well-calibrated and capture true parameter uncertainties.
- **Operator inference**: Required for data-driven ROM construction; check by confirming ROMs accurately reproduce system dynamics for trained parameters.
- **Active learning**: Essential for efficient parameter selection; check by demonstrating improved accuracy with fewer training samples compared to random sampling.
- **Reduced-order modeling**: Fundamental for computational efficiency; check by verifying significant dimensionality reduction while maintaining accuracy.
- **Parametric differential systems**: Core application domain; check by ensuring ROMs generalize across parameter variations.

## Architecture Onboarding

**Component Map**: Data → Bayesian Operator Inference → Uncertainty Quantification → Active Learning → New Training Parameters → Updated ROM

**Critical Path**: The critical path flows from data collection through Bayesian operator inference to generate probabilistic ROMs, then uses uncertainty quantification to guide active learning for parameter selection, ultimately updating the ROM with new training data.

**Design Tradeoffs**: The Bayesian approach provides valuable uncertainty quantification but introduces computational overhead compared to deterministic methods. The active learning strategy balances exploration and exploitation but may require careful tuning to avoid overfitting or missing important parameter regions.

**Failure Signatures**: If the ROM exhibits high variance or instability, it may indicate insufficient training data or poor parameter selection. Overly confident predictions with large errors suggest the uncertainty quantification is not well-calibrated. Computational bottlenecks may arise from the increased complexity of the Bayesian framework.

**3 First Experiments**:
1. Test the method on a simple linear parametric system to validate the basic functionality and uncertainty quantification.
2. Compare the adaptive sampling strategy against random sampling on a nonlinear parametric problem to demonstrate efficiency gains.
3. Evaluate the stability of the ROMs over long-time integration to assess the effectiveness of the active learning approach in maintaining accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow validation scope: Only tested on 1D diffusion-reaction and 2D viscous Burgers' equations, limiting generalizability to other parametric systems.
- Computational overhead: The Bayesian approach introduces increased complexity and computational cost compared to deterministic methods, which may be prohibitive for large-scale problems.
- Medium confidence in major claims: While improvements are demonstrated, the limited test cases and lack of comparisons with alternative strategies reduce confidence in the universality of the findings.

## Confidence
- Accuracy improvements with adaptive sampling: Medium
- Stability improvements with adaptive sampling: Medium
- Computational efficiency gains: Low
- Generalizability to other parametric systems: Low

## Next Checks
1. Test the Bayesian operator inference with active learning on a broader range of parametric PDE systems, including higher-dimensional problems and those with different physical characteristics (e.g., advection-dominated, multi-physics coupling).

2. Conduct a systematic study comparing the computational cost and accuracy trade-offs between the Bayesian approach and traditional deterministic operator inference with active learning, quantifying the overhead and identifying scenarios where the benefits justify the increased complexity.

3. Validate the stability improvements on long-time integration problems, extending beyond the initial transient period to assess whether the adaptive sampling consistently maintains stability for prolonged simulations.