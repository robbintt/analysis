---
ver: rpa2
title: We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent
  Systems
arxiv_id: '2506.13666'
source_url: https://arxiv.org/abs/2506.13666
tags:
- safety
- arxiv
- agent
- systems
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and advocates for research on third-party
  safety risks in Model Context Protocol (MCP)-powered agent systems. MCP, the de
  facto standard for LLM-agent interactions, introduces new safety vulnerabilities
  by enabling malicious third-party services to exploit agent workflows.
---

# We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems

## Quick Facts
- arXiv ID: 2506.13666
- Source URL: https://arxiv.org/abs/2506.13666
- Authors: Junfeng Fang; Zijun Yao; Ruipeng Wang; Haokai Ma; Xiang Wang; Tat-Seng Chua
- Reference count: 40
- Primary result: All tested MCP agents are vulnerable to third-party prompt injection attacks, with significant safety breaches and performance degradation.

## Executive Summary
This paper identifies and advocates for research on third-party safety risks in Model Context Protocol (MCP)-powered agent systems. MCP, the de facto standard for LLM-agent interactions, introduces new safety vulnerabilities by enabling malicious third-party services to exploit agent workflows. Through SAFE MCP, a controlled framework for evaluating these risks, the authors demonstrate that all tested MCP agents are susceptible to prompt injection attacks, with significant performance degradation and safety breaches. Passive detection methods (e.g., service whitelisting) fail to reliably identify advanced attacks, while active defenses (e.g., LLM-based content filtering) mitigate harm but may reduce task accuracy. The paper proposes six research directions: red teaming, MCP-safe LLM development, safety evaluation, data accumulation, MCP service safeguards, and safe ecosystem construction. SAFE MCP is publicly available at https://github.com/littlelittlenine/SafeMCP.git.

## Method Summary
The paper evaluates safety risks in MCP-powered agent systems through SAFE MCP, a controlled framework testing vulnerability to malicious third-party services. The methodology involves implementing MCP services as tools, injecting various attack strategies (Direct, AutoDAN, CodeChameleon, DeepInception, CipherChat, ReNeLLM) into service descriptions or responses, and measuring performance and safety impacts. The evaluation uses 8 backbone LLMs across 10 AgentGym scenarios, running 240 trials total. Metrics include Relative Accuracy Loss (RAL), Attack Success Rate (ASR), Harm Rate (HR), and Detection Ratio (DR) via OpenAI-moderation API and LLaMA-Guard. Active defense employs a safeguard agent (GPT-4o-mini) to filter responses, while passive defense uses whitelisting and moderation APIs.

## Key Results
- All tested MCP agents (GPT-4o, GPT-4o-mini, OpenAI-o1, OpenAI-o3-mini, Qwen2.5-32B, Qwen3-14B, Qwen3-32B, Doubao-1.5-Pro) are vulnerable to direct prompt injection attacks
- Advanced obfuscation attacks (CodeChameleon, DeepInception) achieve low detection rates while maintaining high attack success rates
- Active defense reduces attack success and harm rates but causes measurable performance degradation (increased RAL)
- Passive detection methods fail to reliably identify advanced attacks, with OpenAI-moderation and LLaMA-Guard showing inconsistent performance

## Why This Works (Mechanism)

### Mechanism 1: Third-Party Prompt Injection via Service Surfaces
- Claim: If a third-party MCP service includes adversarial prompts in its metadata or responses, it may manipulate the host agent into executing unintended actions.
- Mechanism: MCP allows external services to provide "service descriptions" and "returned responses." When the backbone LLM processes this third-party text as context, instruction-following capabilities may trigger the hidden commands, overriding prior user instructions. This transforms the service from a data provider into an active attacker.
- Core assumption: The backbone LLM cannot reliably distinguish between trusted system instructions and untrusted external data context.
- Evidence anchors:
  - [abstract]: "MCP introduces third-party services... provider[s] are potentially malicious and... exploit vulnerabilities to sabotage user-agent interactions."
  - [Section 3.1]: "Direct Attack... cause significant safety issues... attack success rate (ASR) and harm rate (HR) are also significantly increased."
  - [corpus]: Related work on "IntentMiner" confirms tool call analysis can reveal hidden intents in MCP environments.
- Break condition: The mechanism fails if the agent treats all external service text as immutable data (quarantined context) rather than executable instructions.

### Mechanism 2: Obfuscation-Based Bypass of Passive Detection
- Claim: Advanced attack strategies using encryption or semantic hiding can evade passive detection systems (e.g., moderation APIs) while remaining executable by the agent.
- Mechanism: Passive defenses scan for known malicious patterns. Attacks like CodeChameleon (encryption) or DeepInception (nested personas) obfuscate the payload. The detection model sees benign-looking text, but the backbone LLM "decodes" or interprets the hidden intent during reasoning, leading to a false negative in detection but a successful attack.
- Core assumption: The semantic understanding of the backbone agent is deeper or differs structurally from the fixed-pattern matching of the safety detector.
- Evidence anchors:
  - [Section 3.2]: "For the other advanced attacks, there exist attacks that cannot be appropriately detected... OpenAI-moderation even fall short of LLaMA-Guard."
  - [Figure 2]: Shows 100% detection for Direct Attacks but significantly lower rates for obfuscated attacks like ReNeLLM.
  - [corpus]: "MCP-SafetyBench" highlights that multi-server workflows complicate simple filtering.
- Break condition: The mechanism fails if the detector possesses the same generative capabilities as the target agent to "unfold" the attack before classification.

### Mechanism 3: The Safety-Utility Trade-off in Active Defense
- Claim: Actively filtering or rewriting service responses reduces harm rates but creates a measurable drop in task accuracy.
- Mechanism: Active defense employs a "safeguard agent" to paraphrase or extract useful information from responses. This sanitization inadvertently removes critical nuance or data required for the specific task (e.g., precise numerical values or semantic details), causing the agent to fail the original user objective.
- Core assumption: The safeguard agent is imperfect at separating "malicious instructions" from "complex necessary context."
- Evidence anchors:
  - [Section 3.3]: "Active defense generally helps to reduce both ASR and HR... [but] RAL decreases... caused by the fact that the active defense sometimes failed to extract the useful information."
  - [abstract]: "Active defense... reduces attack success but can degrade task performance."
- Break condition: The mechanism fails if the extraction model is semantically perfect or if the downstream task is tolerant of information loss.

## Foundational Learning

- Concept: **Model Context Protocol (MCP)**
  - Why needed here: MCP is the standardized interface layer identified as the attack vector. Understanding it as a bridge between an LLM and external tools/services is required to grasp how "service descriptions" become injection points.
  - Quick check question: Can you distinguish between an MCP "service description" and a standard API JSON schema?

- Concept: **Instruction Hierarchy & Context Shifting**
  - Why needed here: The attack relies on the agent prioritizing the "latest" or "immediate" context (the malicious service output) over the "original" user goal.
  - Quick check question: Does an LLM inherently assign higher priority to a user prompt or a system-level tool output?

- Concept: **Agent Utility vs. Safety Alignment**
  - Why needed here: The paper highlights that aggressive safety measures (Active Defense) hurt helpfulness (RAL).
  - Quick check question: In a safety filter, what is the consequence of a "False Positive" on a benign but complex instruction?

## Architecture Onboarding

- Component map: User -> Backbone LLM -> MCP Interface -> Third-Party Service -> Response -> (Safeguard Agent) -> Action
- Critical path:
  1. User sends a task to the Agent.
  2. Agent queries the MCP Service (Attacker).
  3. Service returns a Response containing an adversarial payload.
  4. (Optional) Safeguard Agent attempts to filter the Response.
  5. Agent processes Response and generates a harmful action or fails the task.
- Design tradeoffs:
  - **Passive vs. Active Defense**: Passive (blacklists/detection) is low-latency but easily bypassed by obfuscation. Active (rewriting) is robust but computationally expensive and degrades accuracy (RAL).
  - **Open Ecosystem vs. Walled Garden**: MCP allows any third-party service (high capability, high risk) vs. a pre-vetted whitelist (low risk, low flexibility).
- Failure signatures:
  - **High RAL (Relative Accuracy Loss)**: Agent completes the task incorrectly or fails entirely.
  - **High ASR (Attack Success Rate)**: Agent generates restricted/harmful content.
  - **Low Detection Rate**: Safeguard fails to flag obfuscated attacks (e.g., CodeChameleon).
- First 3 experiments:
  1. **Baseline Injection**: Implement a "Direct Attack" in a service description to establish the upper bound of vulnerability (Expected: High ASR, High Detection).
  2. **Obfuscation Stress Test**: Deploy "CodeChameleon" or "DeepInception" attacks to measure the drop in Detection Rate (Expected: Low Detection, High ASR).
  3. **Utility Cost Measurement**: Enable Active Defense (rewriting) on benign tasks to measure the induced Relative Accuracy Loss (RAL).

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on proprietary API calls (OpenAI-moderation, LLaMA-Guard) for safety metrics may introduce vendor-specific biases
- Framework assumes MCP service descriptions are fully controllable by an attacker, ignoring real-world security controls like cryptographic signatures
- "Prompt fusion" technique for advanced attacks is not fully specified, leaving ambiguity about the auxiliary LLM's role
- Active defense using GPT-4o-mini may introduce its own biases and hallucinations, compounding the safety-utility trade-off

## Confidence
- **High Confidence**: The fundamental vulnerability of MCP agents to third-party prompt injection (Section 3.1 results showing high ASR across all tested models)
- **Medium Confidence**: The effectiveness of advanced attack obfuscation techniques (Detection Ratio varies significantly across attack types, but methodology details are limited)
- **Medium Confidence**: The safety-utility trade-off in active defense (RAL measurements are reported, but the exact sanitization criteria are not specified)
- **Low Confidence**: The generalizability of results to production MCP ecosystems, given the simplified testbed and absence of real-world security controls

## Next Checks
1. **Independent Detection Validation**: Reproduce ASR/HR measurements using open-source safety classifiers (e.g., GPT4AllGuard, LlamaGuard) to verify vendor-independent detection rates
2. **Attack Template Dissection**: Implement and test the "prompt fusion" technique separately to isolate its contribution to obfuscation effectiveness
3. **Defense Mechanism Ablation**: Run active defense with different safeguard agent prompts and compare RAL across configurations to identify optimal safety-utility balance