---
ver: rpa2
title: 'Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance'
arxiv_id: '2511.04172'
source_url: https://arxiv.org/abs/2511.04172
tags:
- chatbot
- data
- system
- retrieval
- university
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an AI-powered chatbot designed to serve as
  a mentor for BRAC University students, addressing the challenge of providing personalized
  on-demand guidance at scale. The system employs a hybrid retrieval mechanism combining
  BM25 lexical ranking with ChromaDB semantic retrieval, using LLaMA-3.3-70B for response
  generation.
---

# Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance

## Quick Facts
- arXiv ID: 2511.04172
- Source URL: https://arxiv.org/abs/2511.04172
- Reference count: 34
- Primary result: AI chatbot using hybrid retrieval (BM25 + semantic) achieves BERTScore 0.831 and METEOR 0.809 for university guidance

## Executive Summary
This paper presents an AI-powered chatbot designed to serve as a mentor for BRAC University students, addressing the challenge of providing personalized on-demand guidance at scale. The system employs a hybrid retrieval mechanism combining BM25 lexical ranking with ChromaDB semantic retrieval, using LLaMA-3.3-70B for response generation. The data ingestion pipeline efficiently processes updates (106.82 seconds) compared to new data (368.62 seconds). The chatbot achieved strong performance metrics: BERTScore of 0.831 and METEOR score of 0.809, indicating high semantic relevance in generated responses. The system aims to help students better understand university life and plan academic routines in the open-credit university environment.

## Method Summary
The chatbot uses a hybrid retrieval approach combining BM25 lexical ranking with ChromaDB semantic retrieval, with LLaMA-3.3-70B via GROQ API for response generation. Data is ingested from CSV files, webpages, and Facebook groups, processed through a timestamp-based incremental pipeline using RecursiveCharacterTextSplitter (1000 chars, 200 overlap) and all-MiniLM-L6-v2 embeddings. Retrieved documents are formatted with references and sent to the LLM with system prompts and conversation history. The system is built with Streamlit interface and SQLite for structured storage, achieving strong semantic relevance metrics while maintaining efficient data processing.

## Key Results
- Achieved BERTScore of 0.831 and METEOR score of 0.809, indicating high semantic relevance
- Data ingestion pipeline processes updates in 106.82 seconds vs. 368.62 seconds for new data
- Successfully handles multi-turn conversations with session state management
- Addresses entity resolution challenges for course codes and faculty names

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining BM25 lexical ranking with ChromaDB semantic retrieval improves document relevance over either method alone.
- Mechanism: BM25 scores exact term matches; ChromaDB retrieves semantically similar vectors. Scores are normalized and combined with weighted sum (λ·BM25 + (1−λ)·Similarity). Equal weights (0.5 each) balance lexical precision with semantic recall.
- Core assumption: Users may query with either exact terminology or conceptual language; both retrieval signals are complementary, not redundant.
- Evidence anchors:
  - [abstract] "hybrid approach, combining BM25 lexical ranking with ChromaDB semantic retrieval"
  - [section IV.A] "By balancing the BM25 and ChromaDB scores, the hybrid retriever capitalizes on the strengths of each method"
  - [corpus] Related work (CyberBOT, arXiv:2504.00389) uses ontology-grounded RAG for domain accuracy, supporting hybrid approaches in education—but no direct comparison to this specific BM25+ChromaDB design.
- Break condition: If queries are predominantly single-word entities (e.g., "CSE220"), semantic retrieval adds noise; if queries are highly abstract, BM25 returns low scores. Monitor per-query BM25 vs. ChromaDB score distributions.

### Mechanism 2
- Claim: Timestamp-based incremental ingestion reduces data pipeline processing time by ~70% for updates versus full re-ingestion.
- Mechanism: SQLite stores records with ingestion timestamps. On re-run, the pipeline compares timestamps against last consumption threshold; only newer records are chunked and embedded. Content hashing (SHA-256) for webpages prevents duplicate processing of unchanged content.
- Core assumption: Most data changes are incremental (schedule updates, new FAQs), not wholesale replacement.
- Evidence anchors:
  - [abstract] "taking 106.82 seconds for updates, compared to 368.62 seconds for new data"
  - [section III.C] "The pipeline takes timestamp updates to limit its data consumption only to new or modified data"
  - [corpus] No direct corpus evidence on incremental ingestion efficiency; this is a system design claim specific to this implementation.
- Break condition: If upstream data changes lack timestamp updates, or if schema changes frequently, incremental logic misses records. Validate timestamp monotonicity and hash collision rates.

### Mechanism 3
- Claim: Context-augmented generation with conversation history and source metadata improves response relevance and trustworthiness.
- Mechanism: Top-k retrieved documents are formatted as context with references. LLaMA-3.3-70B (via GROQ) receives system message defining behavior, user query, retrieved context, and source references. Session history maintains dialogue coherence across turns.
- Core assumption: LLM can synthesize retrieved context into coherent, accurate responses; source references enhance user verification.
- Evidence anchors:
  - [section IV.B.3] "The constructed response uses a combination of user query and hybrid retriever context alongside corresponding references"
  - [section V] BERTScore 0.831 and METEOR 0.809 indicate high semantic relevance
  - [corpus] SCRIPT chatbot study (arXiv:2507.17258) shows structured guidance improves novice interactions, supporting context framing—but evaluates a different LLM and domain.
- Break condition: If retrieved context exceeds LLM context window or contains conflicting information, generation quality degrades. Monitor context length and contradiction frequency.

## Foundational Learning

- Concept: **Sparse vs. Dense Retrieval**
  - Why needed here: The hybrid mechanism requires understanding why BM25 (sparse, term-based) and vector search (dense, semantic) return different results for the same query.
  - Quick check question: For query "thesis deadline extension," would BM25 or ChromaDB better match a document about "dissertation submission deadline policy"?

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: The system injects retrieved documents into the LLM prompt; understanding context window limits, chunking strategies, and citation grounding is essential.
  - Quick check question: If your top-5 retrieved chunks total 6000 tokens but your LLM has 4096 token context, what must happen before generation?

- Concept: **Text Chunking with Overlap**
  - Why needed here: RecursiveCharacterTextSplitter creates 1000-character chunks with 200-character overlap to preserve cross-chunk context.
  - Quick check question: Why might a 200-character overlap help retrieve information about a course prerequisite that spans two chunks?

## Architecture Onboarding

- Component map:
  - Data sources (CSV, webpages, Facebook groups) -> SQLite (structured) -> ChromaDB (vector)
  - Retrieval: BM25 + ChromaDB hybrid with weighted scoring (λ=0.5)
  - Generation: LLaMA-3.3-70B via GROQ API with system prompt and context
  - Interface: Streamlit with session state for conversation history

- Critical path:
  1. Ingest: Validate source -> Store in SQLite with timestamp -> Chunk -> Embed -> Store in ChromaDB
  2. Query: Tokenize input -> Parallel BM25 + ChromaDB retrieval -> Normalize and combine scores -> Top-k selection
  3. Generate: Format context + references -> Send to LLM with system prompt -> Return response with sources

- Design tradeoffs:
  - Equal weights (0.5/0.5) for BM25 vs. semantic: May not suit all query types; tune λ per domain.
  - 1000-char chunks with 200 overlap: Balances granularity vs. context; may split tables or lists awkwardly.
  - Single embedding model: all-MiniLM-L6-v2 is efficient but may underperform on domain-specific terminology.

- Failure signatures:
  - Entity resolution errors: "CSE220" vs. "CSE 220" match differently; aliases cause inconsistent answers.
  - Empty or stale retrieval: Timestamp logic failure -> no new records indexed.
  - API failures: GROQ outages return generic error messages.

- First 3 experiments:
  1. **Ablation on retrieval weights**: Vary λ ∈ {0.3, 0.5, 0.7} and measure BERTScore/METEOR on held-out queries to find optimal balance for this domain.
  2. **Chunk size sensitivity**: Compare 500-char, 1000-char, and 1500-char chunks on retrieval precision for multi-part questions (e.g., prerequisites chains).
  3. **Entity normalization layer**: Implement canonical name mapping for course codes and faculty initials; measure reduction in entity resolution failures via manual review of 50 queries.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can knowledge graph integration measurably improve entity resolution accuracy, particularly for matching faculty full names with short forms or initials?
  - Basis: The authors state entity resolution problems exist in the chatbot system by causing an inability to match full names with short forms thus producing different answer patterns. In Future Work, they propose a knowledge graph to boost entity identification through effective linking of faculty names, initials, and email information.
  - Why unresolved: The current system lacks structured entity relationships, leading to inconsistent retrieval when users reference entities differently (e.g., "Dr. Sadeque" vs. "Farig Yousuf Sadeque").
  - What evidence would resolve it: A comparative evaluation showing improved F1 scores on entity-heavy queries after knowledge graph integration.

- **Open Question 2**: Would integrating Agentic RAG or reinforcement learning approaches significantly improve contextual consistency in multi-turn conversations compared to the current hybrid retrieval system?
  - Basis: Future Work states contextual consistency can improve through integration of retrieval methods such as Agentic RAG together with reinforcement learning.
  - Why unresolved: The paper does not evaluate multi-turn conversation coherence or test alternative retrieval architectures beyond the BM25-ChromaDB hybrid.
  - What evidence would resolve it: Human evaluation or automated metrics (e.g., conversational coherence scores) comparing the current system against Agentic RAG implementations.

- **Open Question 3**: Can the chatbot achieve comparable BERTScore and METEOR performance when extended to multilingual queries in Bangla or Banglish?
  - Basis: Limitations note that students who either need communication in Bangla or Banglish face difficulties accessing the chatbot because it lacks support for multiple languages. Future Work proposes integrating multilingual LLMs mBERT and Multilingual T5 to make the chatbot more inclusive.
  - Why unresolved: The current dataset and embedding models are English-only; no experiments have been conducted on multilingual capabilities.
  - What evidence would resolve it: BERTScore/METEOR evaluations on a Bangla/Banglish query dataset after multilingual model integration.

## Limitations
- Entity resolution failures for course codes and faculty names due to inconsistent naming conventions
- System evaluation relies on automated metrics without human judgment of answer quality
- Limited to English language queries, excluding Bangla or Banglish speakers
- GROQ API dependency introduces potential availability and cost concerns

## Confidence
- **High confidence**: The data ingestion pipeline design (timestamp-based incremental updates) and the hybrid retrieval architecture (combining BM25 with semantic search) are well-specified and technically sound based on the described implementation.
- **Medium confidence**: The performance metrics (BERTScore 0.831, METEOR 0.809) indicate good semantic relevance, but without knowing the evaluation dataset composition or human judgment, the practical quality remains uncertain.
- **Low confidence**: The claim that the chatbot will help students "better understand university life and plan academic routines" lacks empirical validation through user studies or satisfaction measurements.

## Next Checks
1. **Query-type sensitivity test**: Systematically vary the hybrid retrieval weight (λ) across 0.3, 0.5, and 0.7 for different query types (exact terms vs. conceptual questions) and measure impact on BERTScore and METEOR to determine optimal weighting for this domain.

2. **Entity resolution audit**: Manually evaluate 50 randomly selected queries involving course codes, faculty names, and other domain-specific entities to quantify entity resolution failures and implement canonical name mapping if failures exceed 15%.

3. **User experience validation**: Conduct a small-scale user study with 20-30 BRAC University students testing the chatbot on common guidance tasks, measuring both task completion rates and subjective satisfaction scores to validate the claimed practical utility.