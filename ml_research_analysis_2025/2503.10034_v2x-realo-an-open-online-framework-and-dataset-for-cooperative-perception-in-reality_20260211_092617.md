---
ver: rpa2
title: 'V2X-ReaLO: An Open Online Framework and Dataset for Cooperative Perception
  in Reality'
arxiv_id: '2503.10034'
source_url: https://arxiv.org/abs/2503.10034
tags:
- fusion
- perception
- cooperative
- online
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V2X-ReaLO introduces an open online framework and dataset for real-world
  Vehicle-to-Everything (V2X) cooperative perception, addressing the gap between simulation-based
  research and real-world deployment. The framework integrates early, late, and intermediate
  fusion methods within a unified pipeline and provides the first practical demonstration
  of online intermediate fusion's feasibility and performance under genuine real-world
  constraints.
---

# V2X-ReaLO: An Open Online Framework and Dataset for Cooperative Perception in Reality

## Quick Facts
- arXiv ID: 2503.10034
- Source URL: https://arxiv.org/abs/2503.10034
- Authors: Hao Xiang; Zhaoliang Zheng; Xin Xia; Seth Z. Zhao; Letian Gao; Zewei Zhou; Tianhui Cai; Yun Zhang; Jiaqi Ma
- Reference count: 40
- Primary result: First open online framework demonstrating real-world V2X cooperative perception with intermediate fusion

## Executive Summary
V2X-ReaLO introduces an open online framework and dataset for real-world Vehicle-to-Everything (V2X) cooperative perception, addressing the gap between simulation-based research and real-world deployment. The framework integrates early, late, and intermediate fusion methods within a unified pipeline and provides the first practical demonstration of online intermediate fusion's feasibility and performance under genuine real-world constraints. A key contribution is the extension of the V2X-Real dataset to dynamic, synchronized ROS bags with 25,028 test frames (6,850 annotated), enabling real-time evaluations of perception accuracy and communication latency in complex urban scenarios. Experiments show that intermediate fusion methods like AttFuse and V2X-ViT outperform other strategies in V2I and I2I modes, though performance degrades for smaller objects and highly mobile targets under real-world asynchronous conditions. The work highlights the importance of optimizing both visual reasoning and computational efficiency for real-time V2X systems.

## Method Summary
The framework implements a unified V2X cooperative perception pipeline supporting V2V, V2I, and I2I modes. It uses PointPillar for local perception, generating BEV features compressed 32× via a lightweight convolutional network for transmission. The system employs ROS-based communication with GPS synchronization and a Feature Bank for temporal alignment. Three fusion strategies are implemented: early fusion (raw point clouds), late fusion (detection boxes), and intermediate fusion (compressed BEV features). The V2X-ReaLO dataset extends V2X-Real with synchronized ROS bags containing 25,028 test frames with 6,850 annotated for online evaluation. Training uses focal loss and smooth L1 on the V2X-Real dataset, with evaluation measuring AP@IoU=0.3/0.5 across vehicle, pedestrian, and truck classes while monitoring transmission latency and inference speed.

## Key Results
- Intermediate fusion methods (AttFuse, V2X-ViT) outperform early and late fusion in V2I and I2I modes
- Online performance gap: V2X-ViT drops from 39.8% offline to 26.6% online mAP@IoU=0.5 due to latency and asynchrony
- 32× compression achieves 840KB message size with 41.7ms latency, balancing efficiency and accuracy
- Smaller objects (pedestrians) show significant performance degradation in V2V mode under real-world conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressing intermediate BEV features enables real-time V2X cooperation under bandwidth constraints.
- Mechanism: The framework uses a lightweight convolutional compression network to reduce BEV feature channel dimensions, transfers data from GPU to CPU, serializes it, and transmits it via ROS messages over Wi-Fi. The receiver deserializes, decompresses, and reconstructs features in GPU memory for fusion.
- Core assumption: A 32× compression ratio sufficiently preserves feature fidelity for accurate perception while meeting real-time latency requirements.
- Evidence anchors:
  - [abstract] "framework integrates early, late, and intermediate fusion methods within a unified pipeline"
  - [section] Tab. II shows 32× compression reduces message size to 840KB with 41.7ms latency, providing a "reasonable balance."
  - [corpus] Corpus evidence on this specific compression technique is weak; related papers focus on fusion architectures, not transmission modules.
- Break condition: Aggressive compression (e.g., >64×) may lose critical spatial details, degrading detection of small objects like pedestrians.

### Mechanism 2
- Claim: Intermediate fusion outperforms early and late fusion in V2I/I2I modes by balancing information richness and communication efficiency.
- Mechanism: Intermediate fusion transmits compact neural BEV features that retain more spatial context than detection boxes (late fusion) but require less bandwidth than raw point clouds (early fusion). This is particularly effective when one agent (infrastructure) is stationary, reducing temporal misalignment.
- Core assumption: The communication medium (Wi-Fi) has sufficient bandwidth for 840KB messages with ~42ms latency, and feature alignment errors are manageable.
- Evidence anchors:
  - [abstract] "Experiments show that intermediate fusion methods like AttFuse and V2X-ViT outperform other strategies in V2I and I2I modes"
  - [section] Tab. VI shows AttFuse achieving 53.3% mAP@IoU=0.5 in V2I vs 47.1% for Late Fusion.
  - [corpus] "Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception" survey notes limited communication as a key constraint, supporting the need for bandwidth-aware strategies.
- Break condition: In highly mobile V2V scenarios, latency-induced feature misalignment can negate benefits, causing performance to drop below late fusion for small objects.

### Mechanism 3
- Claim: Real-world online evaluation exposes a performance gap compared to offline evaluation, primarily due to asynchronous message arrival and computational delays.
- Mechanism: Offline datasets assume synchronized, perfect data. Online deployment introduces delays from processing, serialization (e.g., 19ms GPU-host transfer), and network latency, causing temporal misalignment between collaborator and ego features.
- Core assumption: The observed online-offline gap is primarily caused by temporal-spatial misalignment from system latency, not fundamental model architecture flaws.
- Evidence anchors:
  - [abstract] "performance degrades for smaller objects and highly mobile targets under real-world asynchronous conditions"
  - [section] Tab. VII shows V2X-ViT dropping from 39.8% offline mAP@IoU=0.5 to 26.6% online.
  - [corpus] "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?" highlights software system characteristics but lacks specific data on this gap.
- Break condition: If system latency can be reduced to near-zero or models become robust to large temporal offsets, this gap would theoretically close.

## Foundational Learning

- Concept: Bird's Eye View (BEV) Representation
  - Why needed here: Intermediate fusion relies on projecting 3D LiDAR points into a 2D BEV feature map (F ∈ R^{H×W×C}) for efficient processing and fusion. Understanding this spatial projection is critical for debugging alignment issues.
  - Quick check question: How does a rotation in the vehicle's local frame affect the BEV feature map coordinates?

- Concept: Robot Operating System (ROS) & Message Passing
  - Why needed here: The entire framework is built on ROS nodes communicating via topics. Knowledge of ROS messages, serialization, and `rosbag` replay is essential for using the provided dataset and framework.
  - Quick check question: What is the difference between a ROS topic and a ROS service, and which is used for streaming LiDAR data?

- Concept: Temporal Alignment & Latency Compensation
  - Why needed here: The paper highlights that asynchrony is a major performance degrader. Understanding how timestamps, GPS synchronization, and buffering (Feature Bank) work to align features is key to the system's operation.
  - Quick check question: If a collaborator's message arrives 50ms late, how should the ego agent's Feature Bank query logic handle it to maintain temporal consistency?

## Architecture Onboarding

- Component map: LiDAR input -> PointPillar -> BEV features -> Transmission Encoder (compress) -> ROS Message -> Wi-Fi -> ROS Message -> Transmission Decoder (decompress) -> Feature Bank -> Fusion Module -> Detection Head
- Critical path: The loop from LiDAR input -> PointPillar -> Transmission Encoder -> [Network Latency] -> Transmission Decoder -> Fusion -> Detection must complete within the sensor frame rate (100ms for 10Hz) to be real-time. The GPU-host data transfer (19.19ms) is a identified major bottleneck.
- Design tradeoffs:
  - Compression vs. Accuracy: Higher compression (64x) reduces latency (14.3ms) but risks losing information. 32x is chosen as a balance.
  - Model Complexity vs. Inference Speed: V2X-ViT is more accurate offline but slower (118.2ms), causing larger online drops. Simpler models (AttFuse, 68.6ms) are more robust to real-time constraints.
  - Message Size vs. Bandwidth: Early Fusion (3.7MB) provides complete data but high latency (144.2ms). Intermediate fusion (840KB, 41.7ms) is a compromise.
- Failure signatures:
  - High latency/dropped frames: If inference time >100ms, the system will skip frames, leading to empty Feature Bank queries and degraded fusion.
  - Pose estimation errors: Misaligned poses cause BEV features to be fused at incorrect spatial locations, leading to ghost objects or missed detections.
  - Small object disappearance: Pedestrians (small features) are most susceptible to compression loss and temporal misalignment, showing the largest AP drop online.
- First 3 experiments:
  1. Baseline Latency Profiling: Run the provided ROS bag in a no-fusion setting. Measure end-to-end latency for PointPillar only to establish a lower bound for system performance.
  2. Fusion Mode Comparison (V2I): Deploy Late, Early, and Intermediate fusion (AttFuse) in a V2I scenario using the dataset. Record mAP, inference speed, and network latency. Verify the paper's claim that Intermediate fusion outperforms others in this mode.
  3. Ablation on Compression Ratio: Using a single Intermediate Fusion model, sweep compression ratios (8x, 16x, 32x, 64x). Plot mAP vs. total transmission latency to empirically find the optimal tradeoff point for your specific network hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cooperative perception systems be optimized for small objects and highly mobile targets under real-world asynchronous conditions?
- Basis in paper: [explicit] "intermediate fusion methods exhibit lower accuracy for detecting smaller objects (e.g., pedestrians) in V2V, suggesting that highly mobile targets are particularly susceptible to compounded spatial and temporal shifts"
- Why unresolved: The paper identifies this degradation pattern but does not propose or evaluate mechanisms specifically designed to handle the compounded temporal-spatial misalignments affecting small, fast-moving objects.
- What evidence would resolve it: Novel fusion architectures or temporal prediction modules evaluated on pedestrian and motorcycle detection in V2V scenarios, demonstrating improved AP@IoU=0.5 under asynchronous online conditions.

### Open Question 2
- Question: How can transformer-based intermediate fusion models be jointly optimized for both perception accuracy and computational efficiency in real-time deployment?
- Basis in paper: [explicit] "This result underscores the importance of jointly optimizing a cooperative perception model's visual reasoning capabilities and computational efficiency to better accommodate real-time deployment challenges"
- Why unresolved: V2X-ViT achieves the highest offline accuracy but suffers the largest online performance drop (−13.2 mAP@IoU=0.5) due to its 118.2ms inference time, while lighter models like AttFuse maintain better online stability.
- What evidence would resolve it: Architectural modifications, knowledge distillation, or latency-aware training strategies that reduce inference time below 100ms while preserving transformer-based accuracy gains.

### Open Question 3
- Question: What communication protocols and compression strategies are optimal for intermediate fusion across diverse V2X infrastructure types?
- Basis in paper: [inferred] The framework uses only Wi-Fi with 32× compression, stating it is "readily extendable to other communication infrastructures," but provides no evaluation across C-V2X, DSRC, or 5G which have different latency-bandwidth tradeoffs.
- Why unresolved: The optimal balance between compression rate, feature fidelity, and transmission latency likely varies by communication standard and scenario complexity, but only one configuration is benchmarked.
- What evidence would resolve it: Comparative studies across multiple communication standards and adaptive compression mechanisms that dynamically adjust based on network conditions and object priorities.

## Limitations
- Reproducibility depends on framework code and dataset release, which were pending at publication
- Limited evaluation of communication protocols beyond Wi-Fi, despite claims of extensibility
- Performance degradation for small objects and V2V scenarios suggests limitations in handling highly mobile agents
- Missing specific training hyperparameters and compression network architecture details

## Confidence

**High Confidence:** The fundamental mechanism of using compressed BEV features for intermediate fusion is sound and well-supported by the experimental results. The performance gap between online and offline evaluation is clearly demonstrated and explained through temporal misalignment and system latency.

**Medium Confidence:** The specific performance numbers (AP scores, latency measurements) depend heavily on the exact implementation details and hardware setup. The choice of 32× compression ratio as optimal is based on empirical testing within the paper's specific configuration.

**Low Confidence:** The generalizability of results to different hardware setups, network conditions, or urban environments beyond the tested scenarios remains uncertain without broader validation.

## Next Checks

1. **Framework Latency Profiling:** Reproduce the basic PointPillar pipeline and measure end-to-end latency components (GPU inference, host transfer, serialization, network transmission) to establish baseline performance metrics.

2. **Compression Ratio Tradeoff Analysis:** Implement the 32× compression module and conduct ablation studies across different compression ratios (8×, 16×, 32×, 64×) to empirically verify the claimed balance between transmission efficiency and feature fidelity.

3. **Temporal Alignment Robustness Test:** Design experiments that inject controlled delays into the ROS message pipeline to quantify the impact of temporal misalignment on fusion performance, particularly for V2V scenarios and small object detection.