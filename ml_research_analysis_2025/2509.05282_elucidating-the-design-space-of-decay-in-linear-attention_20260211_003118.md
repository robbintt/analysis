---
ver: rpa2
title: Elucidating the Design Space of Decay in Linear Attention
arxiv_id: '2509.05282'
source_url: https://arxiv.org/abs/2509.05282
tags:
- decay
- mamba2
- linear
- median
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically explores the design space of decay mechanisms
  in linear attention models, identifying four key dimensions: parameterization strategy,
  parameter sharing, decay granularity, and compatibility with positional encodings.
  Through extensive experiments on language modeling benchmarks, the authors find
  that effective decay values typically cluster around 0.8, parameter sharing can
  lead to extreme decay values that degrade performance, vector decay generally outperforms
  scalar decay under identical parameterization, and relative positional encodings
  like RoPE typically provide negligible benefits.'
---

# Elucidating the Design Space of Decay in Linear Attention

## Quick Facts
- arXiv ID: 2509.05282
- Source URL: https://arxiv.org/abs/2509.05282
- Reference count: 40
- Primary result: Systematically explores decay mechanisms in linear attention, finding effective decay values cluster around 0.8 and proposing Simple Decay for competitive performance with reduced complexity.

## Executive Summary
This paper systematically explores the design space of decay mechanisms in linear attention models across four key dimensions: parameterization strategy, parameter sharing, decay granularity, and compatibility with positional encodings. Through extensive experiments on language modeling benchmarks, the authors find that effective decay values typically cluster around 0.8, parameter sharing can lead to extreme decay values that degrade performance, vector decay generally outperforms scalar decay under identical parameterization, and relative positional encodings like RoPE typically provide negligible benefits. Based on these insights, they propose Simple Decay, a streamlined parameterization scheme that achieves competitive performance with reduced complexity. The study provides actionable guidance for designing efficient linear attention mechanisms and highlights the critical role of properly configured decay in sequence modeling.

## Method Summary
The study evaluates linear attention models with decay across three model sizes (160M, 410M, 1.45B parameters) using fineweb-edu-10B dataset with sequence length 2048. The Decay Linear Transformer architecture uses GLU channel mixers, silu kernel, RMSNorm, and low-rank sigmoid output gate. Four decay variants are tested: Mamba2 decay, GLA decay, Hgrn2 decay, and Simple Decay. Simple Decay parameterizes λ_t = sigmoid(f_t + ∆_t) with ∆_t initialized via argsigmoid(p) where p ∈ {0.8, 0.9, 0.95, 0.99}. Experiments systematically vary parameterization strategies, parameter sharing configurations, scalar vs vector decay granularity, and positional encoding compatibility. Models are trained for 20,000 steps using AdamW optimizer with batch size 256 across 8x A100 GPUs.

## Key Results
- Effective decay values cluster around 0.8, with performance degrading when values approach 0 or 1
- Parameter sharing between decay and key computation can cause decay values to drift toward extremes, degrading performance in some architectures but not others
- Vector decay generally outperforms scalar decay under identical parameterization, but scalar decay with better parameterization can surpass vector decay with worse parameterization
- RoPE typically provides negligible benefits when combined with linear attention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Effective decay values in linear attention cluster around 0.8, with performance degrading when values approach 0 or 1.
- Mechanism: Decay values near 1 cause "attention dilution" (historical signals accumulate without sufficient suppression), while values near 0 overly restrict the receptive window, losing long-range dependencies. The 0.8 median represents a learned balance between preserving relevant history and filtering noise.
- Core assumption: The optimal decay range transfers across model scales; the 0.8 finding from 160M–1.45B models generalizes to larger architectures.
- Evidence anchors:
  - [abstract] "effective configurations are typically confined to a specific range of parameters"
  - [Section 5.1] "Mamba2's median decay values cluster around 0.8, consistently above 0.6... LightNet's median decay values are nearly 1, akin to linear attention without decay, causing attention dilution"
  - [corpus] Related work on structured state-space duality (arXiv:2510.04944) establishes SSM–attention equivalence but does not contradict decay range findings.
- Break condition: Tasks requiring extreme memory horizons (very long-range recall) may benefit from decay values closer to 1; tasks requiring rapid context switching may benefit from values closer to 0.5.

### Mechanism 2
- Claim: Vector decay outperforms scalar decay under identical parameterization, but scalar decay with better parameterization can surpass vector decay with worse parameterization.
- Mechanism: Vector decay provides dimension-specific forgetting rates, allowing fine-grained control over which feature channels retain history. However, the *range* of decay values matters more than granularity—scalar decay with median ~0.8 outperforms vector decay with suboptimal median values.
- Core assumption: The low-rank mapping for vector decay (reducing parameter overhead) does not significantly constrain expressivity compared to full-rank alternatives.
- Evidence anchors:
  - [abstract] "under identical parameterization strategies, scalar decay generally underperforms compared to its vector-based counterpart. However, in certain scenarios with alternative parameterization strategies, scalar decay may unexpectedly surpass vector decay"
  - [Section 5.3] "Scalar decay variants with better performance (compared to vector decay) typically have higher median values. For example, Mamba2 scalar decay has a higher median than Hgrn2 vector decay"
  - [corpus] No direct corpus evidence on vector vs. scalar comparison; related papers focus on SSM structures, not decay granularity.
- Break condition: When computational budget severely constrains parameter count, scalar decay may be preferred despite theoretical suboptimality.

### Mechanism 3
- Claim: Parameter sharing between decay and key computation can cause decay values to drift toward extremes (0 or 1), degrading performance in some architectures but not others.
- Mechanism: When decay is computed as k_t = 1 − λ_t (sharing parameters with key), the coupling can push learned decay toward boundaries depending on how key gradients interact with decay gradients. GLA and LightNet show degraded performance; Mamba2 and Hgrn2 are more robust.
- Core assumption: The observed degradation is due to decay value drift, not merely reduced parameter count; the analysis attributes performance drops to median shifts rather than capacity limits.
- Evidence anchors:
  - [abstract] "parameter sharing cannot be used arbitrarily, as it may cause decay values to be too large or too small"
  - [Section 5.2] "For LightNet... average decay value increased from 0.97 to 0.99, making it closer to having no decay at all"
  - [corpus] No corpus evidence directly addresses parameter sharing in decay; this appears to be a novel contribution.
- Break condition: Architectures with inherently stable decay dynamics (e.g., Mamba2's discretization-based scheme) may tolerate sharing; those with sensitive decay computation should use independent parameters.

## Foundational Learning

- **Linear Attention Recurrence (s_t = diag(λ_t)s_{t-1} + k_t(v_t)^⊤)**
  - Why needed here: All decay mechanisms operate within this unified framework; understanding the state update equation is prerequisite to interpreting decay effects.
  - Quick check question: Can you explain why this formulation achieves O(n) complexity compared to softmax attention's O(n²)?

- **Decay as Exponential Memory Kernel**
  - Why needed here: The decay value λ_t determines how past contributions are weighted; values < 1 create exponentially decaying memory, λ = 1 creates perfect memory (subject to numerical issues), λ ≈ 0 creates near-sighted models.
  - Quick check question: If λ = 0.8, what is the effective receptive window size (in steps) before a historical contribution drops below 10%?

- **RoPE–Vector Decay Incompatibility**
  - Why needed here: The paper uses scalar decay when testing RoPE because naive vector decay + RoPE fails to preserve relative position information (Appendix A.3).
  - Quick check question: Why does RoPE require the decay vector to have paired elements (γ_{t,2i} = γ_{t,2i+1}) for compatibility?

## Architecture Onboarding

- **Component map**:
  - Token Mixer: Linear Attention with decay → computes s_t via recurrence, outputs O = Norm(concat([O^1,...,O^h]) ⊙ U)
  - Decay computation: X → linear projection → activation f_t → parameterization function f(·) → λ_t
  - Channel Mixer: GLU with RMSNorm
  - Key components: Query/Key/Value projections (using silu kernel), low-rank output gate U

- **Critical path**:
  1. Input X (sequence length n, hidden dim d)
  2. Decay activation F computation (varies by parameterization strategy)
  3. Decay λ_t = sigmoid(f_t + ∆_t) [Simple Decay] or alternative parameterization
  4. State update s_t = diag(λ_t)s_{t-1} + k_t v_t^T
  5. Output (o_t)^⊤ = (q_t)^⊤ s_t
  6. Gated output with normalization

- **Design tradeoffs**:
  - Vector vs. Scalar decay: +expressivity vs. +parameters/compute
  - Parameter sharing: -parameters vs. risk of decay drift
  - RoPE integration: +positional awareness (theoretical) vs. -compatibility constraints / negligible empirical gain
  - Simple Decay initialization p: Higher p (e.g., 0.99) → better performance in experiments, but may anneal during training

- **Failure signatures**:
  - Perplexity high + median decay ≈ 1 → attention dilution; increase decay strength or check parameterization
  - Performance drops after enabling parameter sharing → decay drift; inspect median decay per layer
  - RoPE + vector decay → relative position encoding breaks; use scalar decay or structured vector decay with paired elements
  - Median decay varies wildly across layers → parameterization instability; consider simplifying to Simple Decay

- **First 3 experiments**:
  1. **Baseline decay sweep**: Implement Simple Decay with p ∈ {0.8, 0.9, 0.95, 0.99} on a 160M model; log median decay per layer and final perplexity. Expect p=0.99 to match or exceed Mamba2 baseline.
  2. **Parameter sharing ablation**: Enable k_t = 1 − λ_t sharing for Mamba2 and GLA; compare median decay shift and performance delta. Expect Mamba2 stable, GLA degraded.
  3. **RoPE compatibility test**: Add RoPE to scalar-decay models (Mamba2, TNL) and measure perplexity change. Expect negligible impact per Section 5.4 findings.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the optimal decay parameterizations (specifically Simple Decay) maintain their performance advantages when scaled to models significantly larger than 1.45B parameters?
  - Basis in paper: [explicit] The Conclusion explicitly states, "Future research could investigate the applicability of these insights to larger models."
  - Why unresolved: The experimental scope was limited to model sizes of 160M, 410M, and 1.45B parameters.
  - What evidence would resolve it: Training runs on 7B+ parameter models showing that Simple Decay matches or exceeds Mamba2 performance without instability.

- **Open Question 2**: Do the findings regarding decay granularity and parameterization generalize to diverse downstream tasks outside of standard language modeling benchmarks?
  - Basis in paper: [explicit] The Conclusion suggests investigating "diverse downstream tasks" as a specific avenue for future work.
  - Why unresolved: The study focused on language modeling perplexity and standard zero-shot classification benchmarks (e.g., PIQA, HellaSwag).
  - What evidence would resolve it: Evaluating transfer learning performance on tasks requiring distinct reasoning or modalities (e.g., long-context retrieval, code generation).

- **Open Question 3**: Does increasing training data volume eliminate the zero-shot accuracy fluctuations observed in DPLR (Diagonal Plus Low-Rank) models using Simple Decay?
  - Basis in paper: [inferred] In Section 5.6, the authors "hypothesize that this fluctuation may be attributed to the limited number of training tokens."
  - Why unresolved: The DPLR experiments showed lower loss but inconsistent accuracy compared to diagonal models, and this hypothesis was not tested.
  - What evidence would resolve it: Extending training duration for DPLR models beyond the 20,000 steps used in the paper to observe if accuracy stabilizes.

## Limitations

- **Architecture dependence**: Parameter sharing degradation appears architecture-specific (severe in GLA/LightNet, minimal in Mamba2/Hgrn2), suggesting the findings may not transfer to architectures outside the studied subset.
- **Scale limitations**: All experiments use models between 160M-1.45B parameters; optimal decay values and mechanism effectiveness at scale (e.g., >10B parameters) remain unknown.
- **Dataset specificity**: Experiments use only fineweb-edu-10B and standard benchmarks; performance characteristics on other domains (code, multilingual, multimodal) are unverified.

## Confidence

- **High Confidence**:
  - Effective decay values cluster around 0.8, with performance degrading at extremes (0 or 1)
  - Parameter sharing can cause decay drift toward extremes, degrading performance in some architectures
  - Simple Decay achieves competitive performance with reduced complexity

- **Medium Confidence**:
  - Vector decay generally outperforms scalar decay under identical parameterization
  - RoPE typically provides negligible benefits when combined with linear attention

- **Low Confidence**:
  - The specific 0.8 decay value is universally optimal across all tasks and scales
  - Parameter sharing failure modes apply uniformly across all architectures
  - RoPE incompatibility with vector decay is fundamental rather than implementation-specific

## Next Checks

1. **Scale Transfer Validation**: Train the Simple Decay architecture at 10B+ parameters on the same benchmarks. Measure whether the 0.8 decay median and performance advantages persist. Document any shift in optimal decay values or mechanism effectiveness at scale.

2. **Architecture Generalization Test**: Apply the decay mechanism design principles to a non-SSM architecture (e.g., standard transformer with linear attention replacement). Specifically test whether parameter sharing degradation patterns observed in GLA/LightNet extend to vanilla transformers, and whether Simple Decay initialization remains effective.

3. **Domain Transfer Evaluation**: Evaluate the decay mechanisms on code modeling (using CodeParrot or similar) and multilingual modeling (using multilingual C4). Compare whether the same decay value distributions (median ~0.8) and mechanism preferences (vector > scalar) hold, or if domain-specific decay characteristics emerge.