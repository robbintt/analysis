---
ver: rpa2
title: Machine Unlearning Fails to Remove Data Poisoning Attacks
arxiv_id: '2406.17216'
source_url: https://arxiv.org/abs/2406.17216
tags:
- unlearning
- data
- poisoning
- training
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates eight state-of-the-art machine unlearning
  algorithms across image and language models using three types of data poisoning
  attacks: targeted, indiscriminate, and Gaussian. Despite being granted a relatively
  large compute budget (up to 10% of original training), none of the unlearning methods
  successfully removed the effects of poisoned data.'
---

# Machine Unlearning Fails to Remove Data Poisoning Attacks

## Quick Facts
- arXiv ID: 2406.17216
- Source URL: https://arxiv.org/abs/2406.17216
- Reference count: 40
- The study finds that eight state-of-the-art machine unlearning algorithms fail to remove effects of data poisoning attacks, even with compute budgets up to 10% of original training.

## Executive Summary
This study systematically evaluates eight state-of-the-art machine unlearning algorithms against three types of data poisoning attacks across image and language models. Despite being granted relatively large compute budgets (up to 10% of original training), none of the unlearning methods successfully removed the effects of poisoned data. The authors introduce Gaussian data poisoning as a new evaluation metric, which measures unlearning efficacy through normalized inner products between model gradients and added Gaussian noise. This method is computationally efficient and applicable across all data domains. The study concludes that current approximate unlearning methods are insufficient for removing poisoned data and advocates for more rigorous evaluations or provable guarantees.

## Method Summary
The researchers injected poisoned data into training sets (1.5-2.5% of training data) using targeted, indiscriminate, and Gaussian poisoning attacks. They then applied eight unlearning algorithms (GD, NGD, GA, EUk, CFk, SCRUB, NegGrad+, SSD) with compute budgets up to 10% of original training. Evaluation used three metrics: test accuracy retention, poison success rate for targeted attacks, and TPR@FPR=0.01 via Gaussian inner products (GUS metric) to detect residual poison influence. The study tested ResNet-18 on CIFAR-10 and GPT-2 on IMDb across multiple random seeds.

## Key Results
- None of the eight unlearning algorithms successfully removed poisoned data effects across all three attack types
- Even with generous 10% compute budgets, unlearning methods failed to match retraining-from-scratch performance
- The Gaussian poisoning metric revealed residual poison influence undetectable by standard membership inference attacks
- Poison-induced model shifts were found to lie in subspaces orthogonal to clean training data gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoned samples cause larger parameter shifts than clean samples, making standard compute budgets insufficient for unlearning.
- Mechanism: Data poisoning attacks optimize perturbations to maximize influence on model parameters, shifting the trained model further from the desired unlearned objective than removing equivalent random clean samples would require.
- Core assumption: The ℓ1 distance between a poisoned model and the desired unlearned model correlates with computational difficulty of recovery.
- Evidence anchors: [section 6]: "Figure 5 shows the ℓ1 norm of the model shift introduced by unlearning data poisons and random clean training data, demonstrating that data poisons introduce much larger model shifts"; [section 5.2]: "none of the unlearning methods performs on par with retraining from scratch"
- Break condition: If compute budgets were scaled proportionally to poison-induced shift magnitude, unlearning may succeed—but this undermines the efficiency rationale.

### Mechanism 2
- Claim: Poison-induced model shifts occupy subspaces orthogonal to clean data gradients, preventing gradient-based unlearning from reversing them.
- Mechanism: Gradient-based unlearning computes updates from clean retain data. If poison influence lies in a subspace orthogonal to these gradients, the update directions cannot reach the poisoned parameters.
- Core assumption: Cosine similarity between clean gradients and desired unlearning direction predicts unlearning success.
- Evidence anchors: [section 6]: "Figure 14 shows that the desired unlearning direction for data poisons is orthogonal to the update direction from gradient descent as the cosine similarity between the update directions is small"; [section 6]: "training with poison samples not only shifts the model by a larger amount, but the resultant shift lies in a subspace orthogonal to the span of clean training samples"
- Break condition: Unlearning algorithms that explicitly use poison sample gradients (not just retain data) could overcome this—assuming poison samples are known.

### Mechanism 3
- Claim: Gaussian poisoning provides a computationally efficient, domain-agnostic metric for detecting residual poison influence via gradient-noise correlation.
- Mechanism: Add Gaussian noise ξ to training samples. If the model retains poison influence, the inner product ⟨∇xℓ(θ, x_base), ξ⟩ will deviate from N(0,1). After successful unlearning, this distribution should match the independence baseline.
- Core assumption: The normalized inner product follows a Gaussian distribution under independence, and deviations indicate memorization of poison structure.
- Evidence anchors: [section 4.3]: "we expect that a trained model θ_initial has a non-zero correlation with the added Gaussian perturbation vectors"; [appendix B.3]: Equation (3) derives TPR(FPR) = 1 - Φ(Φ^(-1)(1-FPR) - μ) with μ = √(d/2), predicting higher detection power in higher dimensions
- Break condition: If unlearning is exact, the test statistic should return to N(0,1); approximate methods leave residual signal detectable at low FPR.

## Foundational Learning

- Concept: **Membership Inference Attacks (MIAs)**
  - Why needed here: MIAs are the standard evaluation for unlearning efficacy. The paper critiques them as insufficient—low TPR at low FPR makes them unreliable for detecting residual poison influence.
  - Quick check question: Given a model and a data point, can you distinguish whether the point was in the training set using only the model's loss? If TPR ≈ FPR, what does that imply?

- Concept: **Gradient-Based Optimization Landscapes**
  - Why needed here: Unlearning methods (GD, NGD, SCRUB) rely on gradient descent to shift model parameters. Understanding orthogonality and subspaces is critical for diagnosing failure.
  - Quick check question: If your loss gradient is orthogonal to the direction needed to reach the optimum, what happens to convergence? What geometric constraint does this impose?

- Concept: **Data Poisoning Attack Taxonomy**
  - Why needed here: The paper tests three attack types with different objectives—targeted (misclassify specific points), indiscriminate (degrade overall accuracy), Gaussian (invisible signal injection). Each stresses unlearning differently.
  - Quick check question: If a poison attack's goal is to remain invisible on test performance, what evaluation metric would still detect it?

## Architecture Onboarding

- Component map: Clean dataset -> Poison injection -> Corrupted model training -> Unlearning module -> Evaluation layer (accuracy, poison success, TPR@FPR=0.01)
- Critical path: 1) Inject poisons into training set (1.5-2.5% of data); 2) Train model to convergence on corrupted data; 3) Identify forget set (all poison samples assumed known); 4) Apply unlearning algorithm within compute budget; 5) Evaluate: does TPR@FPR=0.01 drop to random baseline? Does test accuracy recover?
- Design tradeoffs:
  - Compute budget vs. unlearning quality: 10% budget is generous yet insufficient; practical systems need <1%
  - Algorithm choice vs. task dependency: NGD works for CIFAR-10 Gaussian poisons but fails on GPT-2; SCRUB partially works for GPT-2 Gaussian but fails on targeted attacks
  - Evaluation rigor vs. computational cost: Standard MIAs are cheap but low-power; Gaussian poisoning requires one gradient pass per sample (still O(n) but with high statistical power)
- Failure signatures:
  - TPR@FPR=0.01 remains elevated (>>0.01) after unlearning → residual poison influence
  - Test accuracy recovers but targeted attack success remains high → unlearning touched retain set behavior but not poison-specific subspaces
  - EUk causes severe text generation degradation on LLMs → reinitializing last k layers destroys learned representations
- First 3 experiments:
  1. Baseline Gaussian poisoning on ResNet-18/CIFAR-10: Inject 750 poisoned samples (1.5%), train for 100 epochs, apply NGD with σ²=1e-7 for 10 epochs. Measure TPR@FPR=0.01 before and after. Expect: minimal reduction from baseline.
  2. Cross-task transfer test: Take best-performing unlearning method from vision (NGD) and apply to GPT-2/IMDb with same hyperparameters. Measure both test accuracy drop and poison success rate. Expect: method does not transfer; accuracy may drop 10%.
  3. Compute budget scaling: For a fixed poison attack (indiscriminate on CIFAR-10), vary unlearning compute budget from 4% → 10% for GD and EUk. Plot test accuracy recovery curve. Expect: diminishing returns; no method reaches retrain baseline even at 10%.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can unlearning algorithms be designed to specifically mitigate poisoning effects that lie in subspaces orthogonal to clean training data?
- **Basis in paper:** [Explicit] The authors hypothesize in Section 6 that "poison samples shift the model in a subspace orthogonal to clean training samples," causing standard gradient-based updates to fail.
- **Why unresolved:** Current methods rely on gradients from clean data or ascent on the forget set, neither of which effectively navigates the orthogonal subspace containing the poison influence.
- **What evidence would resolve it:** A novel algorithm that successfully removes the specific "model shift" caused by poisons without requiring the compute budget of full retraining.

### Open Question 2
- **Question:** Can provable unlearning methods (exact or approximate) effectively remove data poisoning effects where heuristic methods have failed?
- **Basis in paper:** [Explicit] The conclusion advocates for "provable guarantees for machine unlearning algorithms as the way forward" after demonstrating the failure of existing heuristic methods.
- **Why unresolved:** This study focused on empirical, approximate algorithms; it did not evaluate certified unlearning mechanisms which might offer different guarantees.
- **What evidence would resolve it:** Evaluation of certified unlearning algorithms using the proposed Gaussian poisoning metric to verify if they achieve TPR = FPR.

### Open Question 3
- **Question:** How can transferable unlearning methods be developed to maintain efficacy across different tasks and data modalities?
- **Basis in paper:** [Explicit] Section 5.2 notes that "the success of an approximate unlearning method over one task may not transfer to other tasks," warranting further research.
- **Why unresolved:** Methods like EUk succeeded in vision tasks but failed or degraded performance significantly in language tasks (GPT-2).
- **What evidence would resolve it:** An algorithm that demonstrates consistent unlearning efficacy (e.g., high accuracy, low TPR) on both CIFAR-10 and IMDb benchmarks simultaneously.

## Limitations
- The study assumes poison samples are known during unlearning, which is unrealistic in practice
- Limited scope to two model architectures (ResNet-18, GPT-2) and two datasets
- The Gaussian poisoning metric, while innovative, lacks established baselines for comparison in the broader literature

## Confidence
- **High Confidence**: Empirical observation that unlearning fails across all tested methods and attack types (Figures 1-3, 5-6)
- **Medium Confidence**: Hypotheses about orthogonal subspaces and poison-induced parameter shifts (geometric claims need more rigorous proof)
- **Medium Confidence**: Gaussian poisoning as evaluation metric (novel but requires validation on more datasets and attacks)

## Next Checks
1. Test unlearning with unknown poison samples (using heuristic detection or random forgetting) to assess practical feasibility
2. Evaluate on additional architectures (Vision Transformer, smaller LLMs) and datasets (ImageNet subsets, other text corpora)
3. Conduct ablation studies varying poison injection strength (beyond 1.5-2.5%) and unlearning compute budgets (down to practical 1% range)