---
ver: rpa2
title: 'MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models
  on Italian Medical Entrance Examinations'
arxiv_id: '2509.07135'
source_url: https://arxiv.org/abs/2509.07135
tags:
- italian
- medbench-it
- arxiv
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedBench-IT is the first comprehensive benchmark for evaluating
  large language models on Italian medical university entrance examinations. It contains
  17,410 expert-written multiple-choice questions across six subjects and three difficulty
  levels, curated from a leading Italian publisher.
---

# MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations

## Quick Facts
- **arXiv ID:** 2509.07135
- **Source URL:** https://arxiv.org/abs/2509.07135
- **Reference count:** 40
- **Primary result:** First comprehensive benchmark for evaluating LLMs on Italian medical university entrance exams with 17,410 expert-written questions

## Executive Summary
MedBench-IT is the first comprehensive benchmark for evaluating large language models on Italian medical university entrance examinations. It contains 17,410 expert-written multiple-choice questions across six subjects and three difficulty levels, curated from a leading Italian publisher. The benchmark was used to evaluate both proprietary models (including GPT-4o, Claude, and DeepSeek variants) and resource-efficient open-source alternatives (<30B parameters) using standard and reasoning-eliciting prompts. Key results include: top proprietary models achieved around 90% accuracy, while the best open-source models exceeded 70%; Logic and Mathematics were consistently the most challenging subjects; reproducibility tests showed 88.86% response consistency for GPT-4o, with lower consistency in reasoning-intensive subjects; minimal ordering bias was observed; and explicit reasoning prompts showed little benefit for top models. MedBench-IT provides a crucial resource for Italian NLP research, educational technology development, and standardized evaluation of LLMs in specialized domains.

## Method Summary
The benchmark evaluates zero-shot multiple-choice question answering on Italian medical entrance exam questions using two prompt formats: standard (direct answer) and reasoning-eliciting (Chain-of-Thought style). Models are evaluated across 17,410 expert-curated MCQs spanning six subjects with varying difficulty levels. Proprietary models are accessed via API (Dec 2024-Jan 2025), while open-source models (<30B parameters) are deployed locally using vLLM with <40GB VRAM. Performance is measured by accuracy, with additional analyses for reproducibility (response consistency across runs), ordering bias, and readability correlation. The dataset is proprietary and requires author access.

## Key Results
- Top proprietary models achieved around 90% accuracy, while the best open-source models exceeded 70%
- Logic and Mathematics were consistently the most challenging subjects across all models
- GPT-4o showed 88.86% response consistency overall, with lower consistency (73.6%) in reasoning-intensive subjects
- Explicit reasoning prompts showed little benefit for top models, sometimes slightly degrading performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proprietary models with larger parameter counts and specialized reasoning training demonstrate superior performance on complex medical and logical reasoning tasks.
- Mechanism: Models like DeepSeek-R1 (671B parameters) and o1-preview, which are specifically trained with reinforcement learning for reasoning, utilize vast internal knowledge bases and multi-step reasoning pathways. This allows them to navigate abstract problems more effectively than smaller models that may rely more on pattern matching.
- Core assumption: Performance on a static benchmark translates to reliable, real-world reasoning capabilities, and the superior scores of proprietary models reflect genuine understanding rather than potential data contamination.
- Evidence anchors:
  - [abstract] "Key results include: top proprietary models achieved around 90% accuracy, while the best open-source models exceeded 70%".
  - [section 5.1] "DeepSeek-R1 [achieved] 91.9 [accuracy]... o1-preview... 89.1".
  - [corpus] Corpus neighbor 'Challenging the Abilities of Large Language Models in Italian: a Community Initiative' discusses evaluating LLMs for Italian, but does not directly comment on the proprietary vs. open-source performance mechanism for this benchmark.
- Break condition: If evidence emerges of significant data contamination in the training sets of top models (which the paper acknowledges as a limitation in Section 7), or if deployed models fail to show similar reasoning capabilities on novel, out-of-distribution medical problems.

### Mechanism 2
- Claim: Performance variability is significantly higher in tasks requiring abstract reasoning compared to factual knowledge recall.
- Mechanism: Factual questions (Biology, Chemistry) allow models to leverage strong, statistically-frequent associations in their training data. In contrast, Logic and Mathematics problems require manipulating abstract concepts and following multi-step procedures. This process is less deterministic and more prone to stochastic variation, leading to higher inconsistency between runs.
- Core assumption: The consistency observed in a reproducibility test (two runs) is a reliable proxy for the model's inherent stability on a given subject type.
- Evidence anchors:
  - [abstract] "reproducibility tests showed 88.86% response consistency for GPT-4o, with lower consistency in reasoning-intensive subjects".
  - [section 5.2] "Higher consistency was observed in knowledge-based subjects like Biology (96.8%)... while lower consistency was found in subjects requiring complex reasoning: Mathematics (79.8%) and Logic (73.6%)."
  - [corpus] N/A (No direct corpus evidence found for this specific consistency/difficulty relationship).
- Break condition: If future testing with different prompting strategies or larger sample sizes shows that reasoning consistency improves to match factual consistency, invalidating the link between reasoning complexity and variability.

### Mechanism 3
- Claim: Explicitly prompting for reasoning provides minimal or negative benefit for top-performing models on this benchmark.
- Mechanism: Advanced models may already employ implicit, efficient internal reasoning pathways ("System 1-like" intuition) to solve these multiple-choice questions. Forcing a "System 2-like" explicit Chain-of-Thought (CoT) can introduce unnecessary complexity, verbosity, and potential for cascading errors in the generated text before the final answer, thereby degrading performance.
- Core assumption: The observed lack of improvement with reasoning prompts is due to efficient internal processing, not a failure of the benchmark to elicit true reasoning or a flaw in the specific prompt design.
- Evidence anchors:
  - [abstract] "explicit reasoning prompts showed little benefit for top models".
  - [section 5.4] "Top models often performed slightly worse with reasoning prompts, suggesting they employ efficient internal pathways..." and "Analysis showed models tend to produce shorter explanations when correct compared to incorrect answers...".
  - [corpus] N/A (No direct corpus evidence found regarding the efficacy of CoT on this specific benchmark).
- Break condition: If qualitative analysis of the reasoning chains reveals they are consistently flawed or hallucinated even for correct answers, suggesting the models are not actually reasoning effectively, regardless of the prompt style.

## Foundational Learning

- Concept: Zero-Shot Evaluation
  - Why needed here: The entire benchmark methodology relies on presenting questions to models without any prior examples or fine-tuning on the dataset. This tests the model's inherent, pre-trained capabilities.
  - Quick check question: What is the primary advantage of zero-shot evaluation over fine-tuning a model on the benchmark?

- Concept: Ordering Bias
  - Why needed here: A critical robustness test. Multiple-choice LLMs can be sensitive to the position of the correct answer. This benchmark explicitly tested for this to ensure scores reflect knowledge, not positional artifacts.
  - Quick check question: Why is shuffling the answer options a necessary step to validate the robustness of an LLM's performance on a multiple-choice test?

- Concept: Reproducibility vs. Accuracy
  - Why needed here: A high accuracy score is meaningless if the model cannot consistently arrive at the same answer. The paper's detailed reproducibility analysis (e.g., 73.6% consistency in Logic for GPT-4o) is as important as the raw accuracy numbers.
  - Quick check question: If a model has 90% accuracy but only 60% response consistency, what does this suggest about its reliability for deployment in a high-stakes domain?

## Architecture Onboarding

- Component map: The system centers on the **MedBench-IT dataset** (17,410 MCQs). This is fed into various **LLM Inference Engines** (API-based for proprietary, local deployments using frameworks like vLLM for open-source). Prompts are constructed by a **Prompting Module** (standard vs. reasoning-eliciting). **Evaluation Scripts** process model outputs to compute accuracy, and specialized modules test for reproducibility, ordering bias, and readability correlation.

- Critical path:
    1. Load a question from the MedBench-IT dataset.
    2. The Prompting Module formats it using either the standard or reasoning-eliciting template.
    3. Send the prompt to the target model via its respective interface (API or local).
    4. The Evaluation Script parses the model's response (extracting the numerical answer).
    5. Compare the extracted answer against the ground truth key.
    6. Aggregate results and run specialized tests (reproducibility, bias) on subsets of questions.

- Design tradeoffs:
    - **Open Source vs. Proprietary APIs**: The paper evaluates both. Proprietary APIs offer higher performance but at financial cost and without local control. Open-source models allow local deployment (addressing privacy) but require significant hardware (e.g., <40GB VRAM for <30B parameter models) and currently yield lower accuracy.
    - **Prompting Strategy**: The tradeoff is between simplicity (standard prompt) and attempting to elicit better reasoning (CoT). The paper's evidence suggests the simpler approach is often more effective for top models.

- Failure signatures:
    - **Format adherence failure**: The model fails to output "Risposta: [numero]" or "Ragionamento: [spiegazione]\nRisposta: [numero]", making automated parsing impossible.
    - **Low reproducibility**: High variance between identical runs on reasoning-intensive subjects indicates the model's decision boundary is unstable for those concepts.
    - **Ordering bias sensitivity**: A significant performance drop when answer choices are shuffled indicates the model is relying on positional heuristics rather than content knowledge.

- First 3 experiments:
  1. **Baseline Performance Run**: Select a target open-source model (e.g., Qwen 2.5 7B Instruct) and run the entire MedBench-IT dataset using the standard zero-shot prompt. Record overall and per-subject accuracy to establish a baseline.
  2. **Reproducibility Stress Test**: For the same model, run the Logic and Mathematics subsets three times each with temperature > 0. Calculate the answer consistency across runs to quantify variability in reasoning-heavy tasks.
  3. **Reasoning Prompt Ablation**: For a high-performing proprietary model (e.g., GPT-4o) and the open-source baseline, compare results using the standard prompt vs. the reasoning-eliciting prompt. Analyze if the CoT approach improves or degrades performance, particularly in the model's weakest subject areas.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would few-shot prompting with complete Chain-of-Thought traces improve accuracy on MedBench-IT, particularly for smaller open-source models (<30B parameters)?
- Basis in paper: [explicit] The authors explicitly state: "no few-shot evaluation was conducted. This is an interesting extension, particularly for the reasoning approach, where providing complete CoT traces can improve model performance, especially for smaller models."
- Why unresolved: Only zero-shot standard and reasoning-eliciting prompts were tested; the potential benefit of few-shot examples remains unexplored despite CoT literature suggesting gains for smaller models.
- What evidence would resolve it: Re-running evaluation on MedBench-IT using few-shot prompts with exemplar CoT traces, comparing accuracy deltas between few-shot and zero-shot conditions across model sizes.

### Open Question 2
- Question: How would Retrieval-Augmented Generation (RAG) architectures perform on Italian medical entrance questions if context documents were provided?
- Basis in paper: [explicit] The authors note the dataset "does not include context documents, limiting its use for evaluating Retrieval-Augmented Generation (RAG) architectures, which can significantly improve performance."
- Why unresolved: MedBench-IT contains only standalone questions without supporting passages; RAG evaluation cannot be conducted with current benchmark design.
- What evidence would resolve it: Augmenting MedBench-IT questions with relevant textbook passages or medical reference documents, then evaluating RAG-pipeline performance against baseline LLM-only accuracy.

### Open Question 3
- Question: What specific error patterns and failure modes characterize LLM performance on Logic and Mathematics questions, given these subjects consistently yield lowest accuracy?
- Basis in paper: [inferred] Results show Logic and Mathematics as "major bottlenecks across all models" with 15-25 percentage point gaps versus Biology/Chemistry. The authors list "conducting deeper qualitative error analysis" as future work but provide only accuracy metrics.
- Why unresolved: The paper reports accuracy differences but does not analyze whether errors stem from misreading questions, flawed reasoning chains, calculation errors, or conceptual misunderstandings.
- What evidence would resolve it: Manual or automated categorization of incorrect responses on Logic/Math questions, identifying whether errors cluster around specific reasoning types, question structures, or knowledge gaps.

## Limitations
- The benchmark dataset is proprietary (Edizioni Simone), limiting independent verification and broad community adoption
- Potential data contamination risk between benchmark content and LLM training data could inflate accuracy scores
- Results are specific to Italian medical entrance exams and may not generalize to other domains or real-world clinical reasoning

## Confidence
- **High Confidence:** The benchmark construction methodology (17,410 expert-curated questions, standardized zero-shot evaluation, comprehensive subject/difficulty coverage) is robust and well-documented. The observed performance gap between proprietary and open-source models is consistent with broader LLM literature.
- **Medium Confidence:** The reproducibility and ordering bias analyses are methodologically sound, but the specific consistency percentages (e.g., 73.6% for Logic) may vary with different prompt engineering or model versions. The minimal benefit of explicit reasoning prompts is plausible but requires further validation across different prompt designs.
- **Low Confidence:** Claims about the specific mechanisms underlying performance differences (e.g., "efficient internal pathways" vs. pattern matching) are speculative without direct analysis of model internals or reasoning chain quality.

## Next Checks
1. **Independent Dataset Evaluation:** Request access to the MedBench-IT dataset and conduct an independent evaluation using the exact methodology to verify reported accuracy and consistency figures across multiple model versions and prompt strategies.
2. **Reasoning Chain Analysis:** For a subset of incorrect and correct answers from top models, conduct qualitative analysis of the generated reasoning chains (when prompted) to determine if they represent genuine reasoning or post-hoc rationalization, and assess their correlation with final answer correctness.
3. **Cross-Domain Transfer Test:** Evaluate the same models on a different Italian-language medical reasoning benchmark (e.g., IMB or OSCE-style questions) to assess whether performance patterns (subject difficulty, consistency) generalize beyond university entrance exams.