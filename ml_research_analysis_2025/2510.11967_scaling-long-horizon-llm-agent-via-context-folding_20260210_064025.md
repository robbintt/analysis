---
ver: rpa2
title: Scaling Long-Horizon LLM Agent via Context-Folding
arxiv_id: '2510.11967'
source_url: https://arxiv.org/abs/2510.11967
tags:
- context
- agent
- folding
- zhang
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Context folding addresses the fundamental constraint of context
  length on long-horizon LLM agent tasks by introducing an active context management
  mechanism. The method enables agents to branch into temporary sub-trajectories for
  localized subtasks and fold them upon completion, retaining only concise summaries.
---

# Scaling Long-Horizon LLM Agent via Context-Folding

## Quick Facts
- **arXiv ID**: 2510.11967
- **Source URL**: https://arxiv.org/abs/2510.11967
- **Reference count**: 40
- **Primary result**: FoldGRPO agent achieves 62.0% pass@1 on BrowseComp-Plus and 58.0% on SWE-Bench Verified using 10× smaller active context (32K vs 327K tokens) while matching or outperforming ReAct baselines.

## Executive Summary
Context folding addresses the fundamental constraint of context length on long-horizon LLM agent tasks by introducing an active context management mechanism. The method enables agents to branch into temporary sub-trajectories for localized subtasks and fold them upon completion, retaining only concise summaries. An end-to-end reinforcement learning framework called FoldGRPO was developed with dynamic folded contexts and dense process rewards that encourage effective task decomposition and context management. On complex long-horizon benchmarks including BrowseComp-Plus and SWE-Bench Verified, the folding agent achieved pass@1 scores of 62.0% and 58.0% respectively, matching or outperforming ReAct baselines while using 10× smaller active context (32K vs 327K tokens). RL training provided absolute improvements of 20.0% on BrowseComp-Plus and 8.8% on SWE-Bench. The agent learned to generate longer outputs and invoke more tool calls, demonstrating improved exploration and problem-solving capabilities. Results indicate that active context management through folding is a principled path toward scalable long-horizon agency.

## Method Summary
FoldGRPO extends the ReAct framework by introducing branch/return tools that enable hierarchical context compression. When an agent encounters a subtask, it calls `branch(description, prompt)` to create a temporary sub-trajectory, then `return(message)` to fold the intermediate steps while retaining only a summary. A context manager tracks branch depth and filters history at each generation step. During training, dense process rewards guide context management: Unfolded Token Penalty (-1) discourages token-heavy operations in main context, Out-of-Scope Penalty (-0.2) keeps branches focused on assigned subtasks, and Failure Penalty (-1) penalizes failed tool calls. The KV-cache rolls back to branch points on return calls for inference efficiency. FoldGRPO modifies GRPO by incorporating these dynamic folded contexts and process rewards into the policy optimization.

## Key Results
- **Performance**: FoldGRPO achieves 62.0% pass@1 on BrowseComp-Plus and 58.0% on SWE-Bench Verified
- **Efficiency**: Uses 32K active context vs 327K for ReAct baselines while maintaining performance
- **RL Impact**: Absolute improvements of 20.0% on BrowseComp-Plus and 8.8% on SWE-Bench from RL training
- **Context Management**: Reduces main trajectory tokens from 22,285 to 7,752 with 93.5% finish rate vs 73.8% baseline

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Context Compression via Structured Subtask Boundaries
- Claim: Structured branch/return operations create natural compression points that preserve task-relevant information while discarding intermediate steps.
- Mechanism: The agent explicitly delineates subtask boundaries using `branch(description, prompt)` and `return(message)` calls. The context manager F folds all action-observation pairs between these calls, retaining only the return message as a summary. This converts linear history growth into hierarchical chunks.
- Core assumption: Subtask boundaries align with information utility—intermediate reasoning within a branch is less valuable than the outcome summary for downstream decisions.
- Evidence anchors:
  - [abstract]: "An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome."
  - [Section 2.2]: Shows formal folding operation: F(a1,o1,a2,o2,[a3,o3,a4,o4],o4,a5,o5,[a6,o6,a7,o7,a8,o8],a9...) → (a1,o1,a2,o4,a5,o8,a9,o9,a10,o10)
  - [Table 2]: FoldGRPO achieves 90%+ context compression (7,752 main tokens from 100K+ total) with 93.5% finish rate vs 73.8% for GRPO baseline.
- Break condition: If subtasks are poorly decomposed (too granular or overlapping), folding may discard critical information or create redundant branches, reducing compression efficiency.

### Mechanism 2: Process Rewards Guide Emergent Context Management Behavior
- Claim: Dense, token-level process rewards enable RL to learn effective branching strategies that sparse outcome rewards cannot teach.
- Mechanism: Three penalty signals shape behavior: (1) Unfolded Token Penalty (-1) discourages token-heavy operations in main context when >50% full; (2) Out-of-Scope Penalty (-0.2) keeps branches focused on assigned subtasks; (3) Failure Penalty (-1) for failed tool calls. These dense signals provide immediate feedback on context management quality.
- Core assumption: Effective context management decomposes into learnable atomic behaviors (when to branch, what to include in branches, how to summarize returns) that process rewards can shape.
- Evidence anchors:
  - [Section 2.3.2]: "We empirically observe that this sparse reward signal is insufficient for learning effective context folding. Specifically, two critical failure modes emerge: (i) The agent fails to plan strategically, leaving token-intensive operations unfolded... (ii) The agent struggles with proper branch management."
  - [Table 2]: Standard GRPO produces 22,285-token main trajectories with 76.2% scope accuracy; FoldGRPO achieves 7,752 tokens with 89.5% accuracy.
  - [corpus]: COMPASS paper confirms "context management as the central bottleneck" for long-horizon reasoning, though no direct comparison of process reward designs is available in neighbors.
- Break condition: If penalty magnitudes are mis calibrated relative to outcome rewards, agents may over-optimize for context compression at the expense of task success, or vice versa.

### Mechanism 3: KV-Cache Rollback for Inference Efficiency
- Claim: Folding enables inference efficiency by rolling back the KV-cache to branch points rather than regenerating from scratch.
- Mechanism: When `return(message)` is called, the system rolls back the KV-cache to the position before the corresponding `branch()` call. The return message is then appended, matching the context prefix that existed at branch creation time. This avoids quadratic attention cost on folded content.
- Core assumption: Folded content (intermediate branch steps) is sufficiently independent that removing it doesn't invalidate subsequent computations.
- Evidence anchors:
  - [Section 2.2]: "During inference, the agent manages a context KV-cache: when return action is called, it rolls back the KV-cache to the corresponding branch position, where the context prefix matches that before calling the branch action."
  - [Figure 8]: Shows 1.43x faster training step time and 1.52x faster rollout time compared to 327K ReAct agent.
  - [corpus]: No direct corpus comparison of KV-cache rollback vs alternative compression methods found.
- Break condition: If branches modify shared state (e.g., file system changes, global variables) that subsequent reasoning depends on, rolling back context without regenerating may cause coherence failures.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: FoldGRPO extends GRPO by adding process rewards and folded contexts. Understanding the base algorithm (group-based advantage estimation, importance sampling ratios) is prerequisite to modifying it.
  - Quick check question: Can you explain why GRPO uses group-relative advantages rather than absolute value estimates, and how this affects variance?

- **Concept: Process vs Outcome Rewards in RL**
  - Why needed here: The paper's core contribution is showing that sparse outcome rewards fail for context management learning, requiring dense process rewards. Understanding this distinction is critical.
  - Quick check question: Given a long-horizon task with binary success/failure feedback, what are two failure modes when using only outcome rewards?

- **Concept: Multi-turn LLM Agent Architecture (ReAct pattern)**
  - Why needed here: Context folding modifies the standard ReAct loop by introducing branch/return actions. You need to understand the baseline to evaluate the modification.
  - Quick check question: In a ReAct agent, what is appended to context at each step, and how does this differ from the context-folding formulation?

## Architecture Onboarding

- **Component map**: Context Manager (F) -> Tool Scaffold (branch/return) -> FoldGRPO Trainer -> KV-Cache Controller -> Process Reward Module

- **Critical path**:
  1. Implement branch/return tools in agent scaffold
  2. Build context manager that filters τ<i based on branch-return pairs before each πθ call
  3. Implement KV-cache rollback for inference efficiency
  4. Add process reward computation to training loop
  5. Integrate folded context handling into GRPO's importance sampling ratio computation

- **Design tradeoffs**:
  - **Max branches vs context limit**: Paper uses 10 branches with 32K context each (327K total theoretical). More branches increase capacity but require more sophisticated branch management learning.
  - **Penalty magnitudes**: Unfolded (-1), Out-of-Scope (-0.2), Failure (-1). These are tuned relative to binary outcome reward {0,1}. Adjusting requires careful calibration.
  - **GPT-5-nano for out-of-scope detection**: Adds inference overhead but enables focused branch behavior. Alternative: rule-based heuristics or learned classifier.

- **Failure signatures**:
  - **Runaway main context**: Agent doesn't branch when needed → check Unfolded Token Penalty threshold (50%) and ensure penalty is applied correctly
  - **Branch scope drift**: Agent performs unrelated actions in branch → verify out-of-scope penalty is computed and scaled appropriately
  - **Missing return calls**: Agent continues in branch indefinitely → check return tool availability and whether failure penalty is triggering
  - **Information loss**: Folding discards critical details → inspect return message quality; may need to adjust training or add return-quality reward

- **First 3 experiments**:
  1. **Ablate process rewards**: Train with FoldGRPO but disable each penalty type individually to measure contribution. Paper shows GRPO without process rewards degrades to 73.8% finish rate vs 93.5%.
  2. **Vary branch budget**: Test with 0, 5, 10, 15 branches on BrowseComp-Plus to characterize scaling behavior. Paper's Figure 5 (left) suggests plateau around 320K tokens.
  3. **Test out-of-distribution complexity**: Combine multiple easy questions (Section 4.4.2) to evaluate length generalization. Agent trained on ≤10 branches adapted to 32.6 average branches on 50-question tasks—replicate this finding.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can context folding agents effectively utilize parallel branching to improve performance on breadth-first tasks?
  - Basis in paper: [explicit] Section 4.5.3 states, "Whether the folding agent can benefit from parallel branching... remains an open question," noting that current tasks may be too "depth-first" in nature.
  - Why unresolved: Initial experiments showed parallel branching matched but did not exceed single-branch performance on BrowseComp-Plus.
  - Evidence: Evaluating the agent on benchmarks explicitly designed for breadth-first information gathering (e.g., WideSearch) to observe performance deltas.

- **Open Question 2**: Is it feasible to develop hierarchical folding strategies (multi-layer context folding) for deeper compression?
  - Basis in paper: [explicit] The Conclusion lists "multi-layer context folding, which develops hierarchical folding strategies where folds themselves can be further folded," as a future direction.
  - Why unresolved: The current implementation uses a flat branch-return structure and explicitly disables nesting within execution states to prevent complexity.
  - Evidence: A demonstration of a recursive folding architecture successfully handling tasks requiring ultra-long horizons without context overflow.

- **Open Question 3**: Can the "out-of-scope" penalty be learned without reliance on external, high-capability LLM judges?
  - Basis in paper: [inferred] Section 2.3.2 describes the Out-scope penalty relying on GPT-5-nano to judge sub-task relevance, creating a dependency on a specific external model.
  - Why unresolved: Using an external judge introduces latency and potential error propagation; a fully self-contained RL agent is preferable for scalability.
  - Evidence: Successful training runs where the process reward for scope adherence is generated by the agent itself or a significantly smaller heuristic model.

## Limitations
- The method relies on effective subtask decomposition, which is learned rather than engineered, creating potential for information loss if decomposition is poor
- Evaluation uses proprietary or partially released datasets, particularly BrowseComp-Plus, limiting reproducibility and broader benchmarking
- The comparison against ReAct baselines uses a 10× larger context (327K vs 32K), which may not reflect practical deployment scenarios with constrained context windows

## Confidence

- **High Confidence**: The core mechanism of hierarchical context compression through branch/return operations is clearly specified and demonstrated with measurable context reduction (7,752 vs 22,285 main tokens) and improved efficiency (1.43× training speedup). The process reward framework for guiding context management behavior is well-defined with explicit penalty signals.

- **Medium Confidence**: The claimed improvements in task performance (62.0% pass@1 on BrowseComp-Plus, 58.0% on SWE-Bench) are supported by the presented results, but the evaluation setup has limitations. The absence of published BrowseComp-Plus data and the specific difficulty filtering of SWE-Bench Verified reduce external validation opportunities.

- **Low Confidence**: The scalability claims for longer horizons beyond tested benchmarks are not empirically validated. The paper shows adaptation to combined tasks but doesn't test significantly longer sequences or more complex branching patterns than the 10-branch limit.

## Next Checks

1. **Process Reward Ablation Study**: Systematically disable each process reward component (Unfolded Token, Out-of-Scope, Failure penalties) to quantify their individual contributions to both context management quality and task success rates. This would validate whether the dense rewards are essential or if some components could be simplified.

2. **Branch Budget Scaling Analysis**: Test the agent with varying branch limits (0, 5, 10, 15, 20) on BrowseComp-Plus to identify the point of diminishing returns and characterize how context compression efficiency scales with branch capacity. This would clarify whether the 10-branch limit is optimal or could be extended.

3. **Out-of-Distribution Complexity Testing**: Design tasks that combine multiple BrowseComp-Plus questions or create progressively longer software engineering problems to evaluate whether the learned context management strategies generalize to substantially longer horizons than seen during training.