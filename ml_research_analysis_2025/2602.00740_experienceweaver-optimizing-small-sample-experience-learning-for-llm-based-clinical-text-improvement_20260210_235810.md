---
ver: rpa2
title: 'ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based
  Clinical Text Improvement'
arxiv_id: '2602.00740'
source_url: https://arxiv.org/abs/2602.00740
tags:
- experience
- text
- error
- score
- tips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExperienceWeaver introduces a hierarchical framework that transforms
  sparse, noisy clinical feedback into structured, actionable experience through two-stage
  distillation. The method extracts multi-dimensional feedback, distills it into error-specific
  Tips and functional Strategies, and injects this layered experience into an agentic
  pipeline for clinical text improvement.
---

# ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement

## Quick Facts
- arXiv ID: 2602.00740
- Source URL: https://arxiv.org/abs/2602.00740
- Reference count: 40
- Primary result: Outperforms strong baselines including Gemini-3 Pro (+5.93%) and GPT-5.1 (+6.35%) in clinical text improvement

## Executive Summary
ExperienceWeaver introduces a hierarchical framework that transforms sparse, noisy clinical feedback into structured, actionable experience through two-stage distillation. The method extracts multi-dimensional feedback, distills it into error-specific Tips and functional Strategies, and injects this layered experience into an agentic pipeline for clinical text improvement. Evaluations across four clinical datasets show consistent performance gains, especially in small-sample settings.

## Method Summary
ExperienceWeaver implements a two-stage hierarchical distillation process for clinical text improvement. Stage 1 abstracts raw multi-dimensional feedback (correctness, formatting, meaningfulness, readability) into condensed experiences via LLM-based abstraction and hierarchical combination with group size N_G. Stage 2 re-weaves these into error-specific Tips (τ_e threshold) and functional Strategies per pipeline phase (Detection, Revision, Self-Critique). The framework uses a three-agent orchestrator with a retriever that injects relevant Tips and Strategies based on the current phase and detected error types.

## Key Results
- Consistent performance gains across four clinical datasets
- Outperforms Gemini-3 Pro by 5.93% and GPT-5.1 by 6.35% in clinical text improvement tasks
- Shows particular effectiveness in small-sample settings
- Demonstrates superior error detection and text revision capabilities
- Ablation studies confirm importance of experience granularity and weaving group size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical distillation converts sparse, noisy clinical feedback into structured, transferable experience.
- **Mechanism:** Two-stage process first abstracts raw feedback per evaluation metric via LLM-based Experience Abstractor, then hierarchically combines overlapping experiences using tree-based Combination step (group size N_G). This yields condensed experience pool (~10 experiences per metric, 100-300 words each) that retains concrete details while removing redundancy.
- **Core assumption:** Raw feedback contains latent, generalizable patterns that can be unified without critical information loss.
- **Evidence anchors:** [abstract] "ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge." [section 3.1.1] Defines LLM_abstract for abstraction and LLM_combine for hierarchical merging.
- **Break condition:** If feedback is too sparse (<~5 samples per metric) or contradictory, abstraction may produce vague or conflicting experiences.

### Mechanism 2
- **Claim:** Layered experience injection (error-specific Tips + functional Strategies) guides phase-specific reasoning in agentic pipelines.
- **Mechanism:** Stage 2 re-weaves distilled experiences by error type into concrete Tips, and across tips into high-level Strategies per pipeline phase. Retriever injects relevant layer based on current phase and detected error type, controlled by tip count τ_t.
- **Core assumption:** Breaking experience into two granularity levels aligns with how agents decompose the revision task (error identification → correction → self-evaluation).
- **Evidence anchors:** [abstract] "distills it into error-specific Tips and functional Strategies... learns 'how to revise' rather than just 'what to revise'." [section 3.1.2-3.1.3] Formally defines Tp_e for tips and S_p for strategies.
- **Break condition:** If error types are misclassified or τ_t is set too high, injected context may become noisy or exceed effective context length.

### Mechanism 3
- **Claim:** Multi-dimensional feedback (correctness, formatting, meaningfulness, readability) as a teacher enables comprehensive, balanced improvement.
- **Mechanism:** Evaluation agent generates feedback across four dimensions per revision. This multi-dimensional signal drives experience abstraction (one pool per dimension) and is used to compute pairwise improvement scores D_i,j. Framework optimizes across all dimensions simultaneously.
- **Core assumption:** Clinical text quality is multi-faceted, and improvements in one dimension should not come at the cost of others.
- **Evidence anchors:** [section 3.2.2] Defines four feedback dimensions and states aim is "multi-dimensional improvements rather than one-dimensional gains." [section 5.2] Table 1 reports gains across different data types.
- **Break condition:** If feedback dimensions are highly correlated or LLM evaluator shows bias, distilled experience may overfit to a single implicit dimension.

## Foundational Learning

- **In-Context Learning (ICL)**
  - **Why needed here:** ExperienceWeaver is training-free framework that injects distilled experience into prompts. Understanding ICL is essential to grasp how structured tips and strategies guide model without weight updates.
  - **Quick check question:** How does ICL leverage context window to adapt model's behavior, and what are limitations with unstructured or lengthy context?

- **Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Paper positions ExperienceWeaver as paradigm shift from standard RAG (which retrieves raw cases) to experience retrieval. Contrasting the two clarifies the innovation.
  - **Quick check question:** In standard RAG, why might retrieving similar cases fail to convey reasoning needed for revision in small-sample settings?

- **Agentic Systems & ReAct Paradigm**
  - **Why needed here:** Framework uses multi-agent orchestrator (Detection, Revision, Self-Critique) following ReAct (Reason+Act) pattern. This background explains task decomposition and coordination.
  - **Quick check question:** In ReAct-style agent, how does orchestrator decide which specialized agent to invoke next?

## Architecture Onboarding

- **Component map:** Memory Storage (raw records) -> Stage 1 (Experience Abstractor -> Combiner with group size N_G) -> Experience Pool -> Stage 2 (Error-Specific Tip Distiller with threshold τ_e -> Functional Strategy Distiller) -> Tips Pool + Strategies Pool -> Experience Retriever (max tips τ_t) -> Agents (Detection, Revision, Self-Critique)
- **Critical path:** 1. Collect multi-dimensional feedback on training samples. 2. Run Stage 1 to generate distilled experience pool. 3. Run Stage 2 to re-weave into Tips and Strategies. 4. At inference, retriever injects relevant layers into active agent's prompt.
- **Design tradeoffs:** Weaving group size N_G: smaller preserves detail but increases processing time; larger improves efficiency but may lose concreteness. Minimum error frequency τ_e: higher focuses on common errors, improving precision; lower broadens coverage but risks noisy tips. Max retrieved tips τ_t: balances informativeness vs. context length; paper uses τ_t=5.
- **Failure signatures:** Over-abstraction (tips become generic without actionable examples). Error type explosion (too many low-frequency error types lead to fragmented, sparse tips). Context overload (exceeding effective context length with too many tips/strategies).
- **First 3 experiments:** 1. Ablation on N_G: Run Stage 1 with N_G = 2, 4, 8 on fixed training set; measure downstream task performance and time cost. 2. Threshold sweep for τ_e: Vary τ_e from 2 to 10; evaluate precision/recall of error detection and quality of distilled tips. 3. In-context length study for τ_t: Inject 1, 3, 5, 7 tips; monitor revision quality scores and LLM attention on injected context.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does ExperienceWeaver's performance and computational efficiency scale when applied to datasets significantly larger than the ~200 samples used in this study?
- **Basis in paper:** [explicit] Section 7 (Limitations) states experiments relied on approximately 200 samples which "limits generalizability," and suggests future work should explore "scalability with larger datasets to confirm robustness."
- **Why unresolved:** Current validation is restricted to small-sample scenarios designed for in-context learning, leaving system's behavior on large-scale clinical corpora unconfirmed.
- **What evidence would resolve it:** Benchmarking results on datasets containing thousands to millions of clinical records, analyzing both performance metrics and resource consumption.

### Open Question 2
- **Question:** Can inherent biases and "residual variation" of LLM-as-a-Judge evaluators be fully eliminated to ensure robust automated evaluation?
- **Basis in paper:** [explicit] Section 7 (Limitations) notes that while models with stable patterns were selected, "residual variation remains" due to distinct evaluation behaviors across different base models.
- **Why unresolved:** Study mitigates but does not solve fundamental instability and bias of generative models when used as evaluators.
- **What evidence would resolve it:** Development of calibration method that aligns LLM evaluators with human ground truth across diverse metrics, eliminating statistical significance in variance between runs.

### Open Question 3
- **Question:** How can system safeguard against "over-correction" where automated revisions inadvertently alter physician's specific stylistic intent or nuanced meaning?
- **Basis in paper:** [explicit] Section 7 (Potential risks) identifies "over-correction" as risk where model might "alter the physician's specific stylistic nuances or intent in favor of standardized phrasing."
- **Why unresolved:** Current feedback loop optimizes for standardization and error correction, which may inherently conflict with preserving individual author intent.
- **What evidence would resolve it:** Human-subject study where physicians rate "intent preservation" of revisions alongside standard quality metrics.

## Limitations
- Exact error type taxonomy (20-30 types mentioned) and complete orchestrator logic remain unspecified
- Evaluation relies heavily on LLM-as-Judge methodology which may introduce bias or inconsistency
- Limited sensitivity analysis for optimal parameter settings (N_G, τ_e, τ_t)

## Confidence
- **High confidence:** Core hierarchical distillation mechanism and observed performance gains over strong baselines
- **Medium confidence:** Generalizability of error-specific Tips and functional Strategies approach across clinical domains
- **Low confidence:** Optimal parameter settings due to limited sensitivity analysis

## Next Checks
1. Conduct error type taxonomy validation with domain experts to verify 20-30 error types are comprehensive and clinically relevant
2. Implement ablation studies varying N_G, τ_e, and τ_t to identify sensitivity and robustness boundaries
3. Test framework's transferability by applying it to different clinical domain (e.g., pathology reports) using same distilled experiences