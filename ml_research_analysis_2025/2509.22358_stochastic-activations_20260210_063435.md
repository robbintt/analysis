---
ver: rpa2
title: Stochastic activations
arxiv_id: '2509.22358'
source_url: https://arxiv.org/abs/2509.22358
tags:
- relu
- silu
- activation
- stocha
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stochastic activations introduce a novel strategy for large language
  models that randomly selects between non-linear functions like SILU and RELU during
  the feed-forward layer. This approach addresses the optimization challenges of RELU,
  particularly its constant shape for negative inputs that prevents gradient flow.
---

# Stochastic activations

## Quick Facts
- arXiv ID: 2509.22358
- Source URL: https://arxiv.org/abs/2509.22358
- Reference count: 28
- Introduces method achieving 65% CPU speedup and 1.5× GPU speedup while maintaining performance comparable to SILU baseline

## Executive Summary
Stochastic activations introduce a novel strategy for large language models that randomly selects between non-linear functions like SILU and RELU during the feed-forward layer. This approach addresses the optimization challenges of RELU, particularly its constant shape for negative inputs that prevents gradient flow. The method is applied in two ways: (1) pre-training with SILU followed by fine-tuning with RELU to achieve sparse activations at inference time, resulting in significant CPU and GPU speedups (65% and 1.5× respectively) while maintaining performance comparable to SILU; (2) using stochastic activations at inference time to generate diverse text sequences, providing an alternative to temperature sampling.

## Method Summary
The method uses stochastic mixing of SILU and RELU during training to maintain gradient flow while exposing the network to RELU's sparsity pattern. For negative inputs, the model samples SILU with probability p and RELU with probability 1-p during training. The approach is applied in two ways: pre-training with SILU followed by fine-tuning with RELU (Swi+FT) to achieve sparse activations at inference time, and using stochastic activations at inference time to generate diverse text sequences. The technique is implemented in decoder-only transformer architectures (LM1.5B, LM3B) using RMSNorm, RoPE, and Grouped-Query Attention.

## Key Results
- Achieved 65% CPU speedup and 1.5× GPU speedup at inference time while maintaining performance comparable to SILU baseline
- StochA inference with p=0.7 and temperature=0 outperforms SILU with temperature∈{0.1, 0.2, 0.5} on NQ task when selecting best of N generations
- Swi+FT alone underperforms, but StochA + Swi+FT combination achieves validation loss competitive with SILU baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic mixing of SILU and RELU during training maintains gradient flow while exposing the network to RELU's sparsity pattern
- Mechanism: For negative inputs, SILU provides non-zero gradients (x·σ(x) where σ is sigmoid), while RELU provides zero gradients. By sampling SILU with probability p and RELU with probability 1-p during training, the network receives gradient signal on a portion of negative activations, mitigating the "dying RELU" problem where neurons become permanently inactive
- Core assumption: The gradient signal from SILU on negative inputs is sufficient to keep weights updating meaningfully, even when RELU is selected
- Evidence anchors: [abstract] "This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow." [section 3.2] Defines Ψ_p(x) = (1-ω)×RELU(x) + ω×SILU(x) for x<0, where ω∼Bernoulli(p)

### Mechanism 2
- Claim: Fine-tuning with RELU after SILU pre-training (Swi+FT) transfers learned representations while adapting to sparse inference
- Mechanism: Pre-training with SILU establishes useful weight configurations without dead neurons. When switching to RELU for the final α% of training steps (typically 5-10%), the loss spikes temporarily but recovers as the model adapts to RELU's sparsity pattern while retaining most of its learned knowledge
- Core assumption: The SILU and RELU activations are sufficiently similar (same asymptotes at ±∞, same value at 0) that switching doesn't catastrophically disrupt learned representations
- Evidence anchors: [section 3.1] "We observe a spike in the loss at the time we change the activation, see Figure 2. However, the optimization rapidly recovers." [section 4.2, Table 1] Swi+FT alone underperforms; StochA + Swi+FT combination achieves validation loss competitive with SILU baseline

### Mechanism 3
- Claim: Stochastic activations at inference time provide diversity through activation-level randomness, an alternative to temperature sampling
- Mechanism: At inference, each token generation uses independent Bernoulli draws to select between SILU and RELU for negative inputs. This creates variation in the hidden states across generations, producing diverse outputs from the same prompt without modifying the softmax temperature
- Core assumption: Activation-level stochasticity propagates meaningfully to output distribution variation, not just noise
- Evidence anchors: [section 4.4, Figure 6] StochA with p=0.7 and temperature=0 outperforms SILU with temperature∈{0.1, 0.2, 0.5} on NQ task when selecting best of N generations by normalized log-likelihood

## Foundational Learning

- Concept: **Dying RELU problem**
  - Why needed here: The entire motivation for stochastic activations rests on understanding why RELU fails during training—neurons with consistently negative pre-activations receive zero gradient, causing their weights to stop updating permanently
  - Quick check question: If a neuron's pre-activation input is always negative, what is the gradient through RELU, and what happens to that neuron's weights during backpropagation?

- Concept: **Sparsity in feed-forward networks**
  - Why needed here: The paper exploits RELU's property of producing exact zeros for negative inputs, enabling computational savings by skipping operations on inactive neurons. Understanding matrix-sparse vector multiplication is essential
  - Quick check question: In a two-layer FFN with RELU activation, if 90% of activations are zero, what fraction of the second layer's matrix multiplication FLOPs can theoretically be skipped?

- Concept: **Quantization-aware training analogy**
  - Why needed here: The paper draws explicit comparison to QuantNoise (Fan et al., 2020)—the idea of exposing a model to inference-time conditions during training to reduce train-test discrepancy. This frames StochA as conceptually similar
  - Quick check question: In quantization-aware training, why is it beneficial to simulate quantization during training rather than applying it only at inference?

## Architecture Onboarding

- Component map:
  Input → Linear(W1) → Stochastic Activation → Gate(W3) ⊙ → Linear(W2) → Output
                              ↓
                    Bernoulli(p) selects:
                    - RELU with prob 1-p
                    - SILU with prob p

- Critical path:
  1. **Training from scratch**: Enable StochA with p∈{0.3, 0.5} during pre-training. For final 5-10% of steps, switch to RELU-only (Swi+FT)
  2. **Continuous pre-training**: Starting from SILU checkpoint, fine-tune with Swi+FT alone (switch to RELU) works better than StochA for this scenario (Table 6)
  3. **Inference**: Use RELU for sparse, fast inference; use StochA for diverse generation

- Design tradeoffs:
  - Higher p → better gradient flow but lower sparsity at inference
  - Higher α → better RELU adaptation but potential forgetting
  - StochA at inference → more diversity but non-deterministic outputs (unsuitable when reproducibility required)
  - Sparsity exploitation requires custom kernels; standard PyTorch won't automatically accelerate sparse matrix-vector products

- Failure signatures:
  - Loss spike when switching to RELU is normal, but if it doesn't recover within ~500 steps, α may be too small or learning rate too low
  - Dead neurons manifest as near-zero row norms in W1 (Figure 7, right); monitor this during training
  - If validation loss with RELU inference is >> SILU baseline, Swi+FT adaptation is insufficient—combine with StochA during pre-training

- First 3 experiments:
  1. **Baseline comparison**: Train identical small models (e.g., 100M params) with SILU-only, RELU-only, and StochA(p=0.5)+Swi+FT(α=0.05). Compare validation perplexity and sparsity rate. Expect: RELU-only worst, StochA+Swi+FT approaching SILU performance with >80% sparsity
  2. **Sparsity-speedup validation**: Take a trained StochA+Swi+FT model, measure per-token latency on CPU with and without sparse FFN implementation. Target: 1.5x+ speedup at 85%+ sparsity
  3. **Diversity ablation**: Generate 20 responses per prompt using StochA inference (p=0.7, temp=0) vs SILU (temp=0.5). Compute TTR and best-of-N accuracy. Expect: StochA shows higher TTR; quality depends on task

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not transfer to larger models (>10B parameters) or different architectures like encoder-decoder or vision transformers
- The optimal fine-tuning fraction (α=0.05-0.10) appears somewhat arbitrary without sensitivity analysis across broader range of values
- Diversity claims rely heavily on specific NQ task and may not generalize to other generation tasks

## Confidence

**High Confidence:**
- The fundamental mechanism of gradient flow preservation through stochastic mixing of SILU and RELU is theoretically sound and empirically validated
- The Swi+FT approach achieves competitive validation loss compared to SILU baseline (within ~0.1 perplexity points)
- CPU and GPU speedup measurements from sparse implementations are reproducible given the same sparsity rates

**Medium Confidence:**
- The diversity benefits at inference time generalize beyond the tested NQ task
- The 65% CPU and 1.5× GPU speedups maintain their relative advantage at larger model scales
- The optimal fine-tuning fraction (α=0.05-0.10) remains optimal across different model sizes

**Low Confidence:**
- The sparsity patterns observed in LM1.5B/3B models will scale predictably to models 10× larger
- The diversity mechanism works equivalently well for non-English languages and code generation
- The train-test activation switching doesn't introduce subtle long-term stability issues

## Next Checks

1. **Scale sensitivity analysis**: Reproduce the StochA + Swi+FT approach on a 10B parameter model variant, measuring not just final performance but also the loss spike magnitude during activation switching and the sparsity-rate evolution throughout training. Compare the optimal fine-tuning fraction α at this scale versus LM1.5B/3B.

2. **Diversity generalization benchmark**: Test StochA inference-time diversity on multiple generation tasks including summarization (CNN/DailyMail), code generation (MBPP, HumanEval), and open-ended generation (story completion). Measure not just lexical diversity (TTR) but also semantic diversity using embedding-based metrics, and evaluate quality trade-offs using human evaluation on a subset.

3. **Architecture transfer validation**: Implement StochA in an encoder-decoder architecture (e.g., T5-style) and a vision transformer (e.g., ViT), measuring whether the sparsity-speedup benefits transfer. For vision tasks, evaluate whether the stochastic activation helps with gradient flow in convolutional layers where negative activations might be more prevalent than in LLMs.