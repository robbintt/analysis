---
ver: rpa2
title: Clinical semantics for lung cancer prediction
arxiv_id: '2508.14627'
source_url: https://arxiv.org/abs/2508.14627
tags:
- clinical
- embeddings
- poincar
- prediction
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the problem of improving lung cancer onset\
  \ prediction by incorporating semantic relationships from clinical terminologies\
  \ into predictive models. The core method involves mapping SNOMED clinical term\
  \ hierarchies into a low-dimensional hyperbolic space using Poincar\xE9 embeddings,\
  \ which preserve the tree-like structure of clinical taxonomies."
---

# Clinical semantics for lung cancer prediction

## Quick Facts
- arXiv ID: 2508.14627
- Source URL: https://arxiv.org/abs/2508.14627
- Reference count: 0
- Primary result: Poincaré embeddings improved lung cancer prediction AUROC by 0.02-0.03 over baseline models

## Executive Summary
This study demonstrates that incorporating semantic relationships from SNOMED clinical term hierarchies into predictive models can improve lung cancer onset prediction. The authors map SNOMED hierarchies into hyperbolic space using Poincaré embeddings, then integrate these pre-trained embeddings into deep learning architectures. Results show modest but consistent improvements in discrimination performance, with ResNet models achieving up to 0.72 AUROC compared to 0.70 for baseline models. The approach preserves hierarchical relationships more efficiently than standard Euclidean representations, particularly for rare concepts that would be lost in low-dimensional Euclidean space.

## Method Summary
The methodology involves extracting SNOMED clinical findings hierarchy, training Poincaré embeddings using Riemannian stochastic gradient descent to preserve hierarchical structure, and integrating these embeddings into ResNet and Transformer architectures. The Poincaré ball model allows deep hierarchies to fit in low-dimensional space while maintaining semantic relationships. A logarithmic map projects hyperbolic vectors onto Euclidean tangent planes for integration with standard neural networks. The hybrid architecture uses Poincaré embeddings for SNOMED concepts while maintaining standard embeddings for drugs and other non-hierarchical covariates.

## Key Results
- ResNet models with Poincaré embeddings achieved 0.72 AUROC compared to 0.70 for baseline
- Transformer models improved from 0.67 to 0.70 AUROC with Poincaré embeddings
- Best calibration observed in ResNet models using 10-dimensional Poincaré embeddings
- Mean rank evaluation showed consistent improvement across embedding dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperbolic embeddings preserve hierarchical relationships in clinical taxonomies more efficiently than Euclidean representations.
- Mechanism: Poincaré ball model's exponential volume growth mirrors taxonomic branching, allowing deep hierarchies to fit in low-dimensional space while preserving parent-child distances.
- Core assumption: SNOMED hierarchy captures clinically meaningful relationships predictive of lung cancer risk.
- Evidence anchors: Abstract mentions mapping SNOMED to hyperbolic space; section 2.2 explains volume growth; corpus shows weak direct evidence for hyperbolic specifically.
- Break condition: If SNOMED hierarchy doesn't capture meaningful relationships for the outcome, geometric preservation provides no benefit.

### Mechanism 2
- Claim: Log-map projection enables integration with standard neural networks while retaining hierarchical structure.
- Mechanism: Logarithmic map projects Poincaré vectors onto Euclidean tangent plane, preserving relationships; Poincaré layer is frozen with trainable additive offset.
- Core assumption: Log-map projection sufficiently preserves semantic relationships for downstream learning.
- Evidence anchors: Section 2.4 describes log-map projection and frozen Poincaré layer with trainable offset.
- Break condition: If log-map introduces distortion degrading relationships for rare concepts near Poincaré ball boundary.

### Mechanism 3
- Claim: Hybrid embedding architecture allows semantic priors for SNOMED while keeping other concepts learnable.
- Mechanism: SNOMED concepts (38,351 nodes) use pre-trained Poincaré embeddings; non-SNOMED concepts use randomly initialized Euclidean embeddings.
- Core assumption: Drug prescriptions and other non-SNOMED concepts benefit less from hierarchical priors.
- Evidence anchors: Section 2.4 explains SNOMED vs non-SNOMED embedding strategy; results provide concept coverage statistics.
- Break condition: If predictive signals come from drug concepts whose relationships would benefit from hierarchical encoding.

## Foundational Learning

- Concept: **Poincaré Ball Model**
  - Why needed here: Core geometric representation; understanding non-linear distance increases toward boundary is essential for interpreting embedding quality.
  - Quick check question: Can you explain why a point near the Poincaré ball boundary represents a more specific (deeper) concept than one near the origin?

- Concept: **Riemannian Stochastic Gradient Descent (RSGD)**
  - Why needed here: Training embeddings on hyperbolic manifolds requires gradient updates respecting curvature; standard SGD would leave valid embedding space.
  - Quick check question: Why can't you apply standard Euclidean gradient updates directly to coordinates in the Poincaré ball?

- Concept: **Logarithmic/Exponential Maps for Manifolds**
  - Why needed here: Required to convert between hyperbolic embeddings and Euclidean neural network layers.
  - Quick check question: What information might be lost when projecting a hyperbolic embedding to a Euclidean tangent plane?

## Architecture Onboarding

- Component map: SNOMED Subgraph Extraction -> Poincaré Embedding Pre-training -> Log-Map Projection -> Hybrid Embedding Layer -> Backbone Model

- Critical path: Extract SNOMED ancestral subtree for cohort's observed concepts → Pre-train Poincaré embeddings (optimize mean rank) → Apply log-map projection → Train ResNet/Transformer with frozen Poincaré + trainable offset

- Design tradeoffs:
  - Lower dimensions (3-10): Better regularization, may underfit; ResNet showed best calibration at 10-dim
  - Higher dimensions (30-100): Better hierarchy representation, risk of overfitting; marginally improved mean rank
  - Directed vs undirected taxonomy: Paper found consistent results; directed preserves original SNOMED semantics
  - Freezing vs fine-tuning Poincaré: Freezing preserves pre-trained structure; offset layer allows task adaptation

- Failure signatures:
  - Mean rank not improving during embedding pre-training → check negative sampling rate, burn-in epochs
  - AUROC worse than baseline → verify log-map implementation, check concept coverage
  - Calibration degradation → may indicate embedding dimension mismatch with model capacity

- First 3 experiments:
  1. Replicate embedding pre-training on your SNOMED subset; verify mean rank decreases with dimensions (compare 10 vs 100)
  2. Ablation: Compare Poincaré vs randomly initialized Euclidean embeddings on same architecture; expect ~0.02 AUROC difference
  3. Test transportability: Train on one site, evaluate on another; hypothesize Poincaré may reduce heterogeneity gap

## Open Questions the Paper Calls Out

- Question: Can performance improvements generalize to clinical prediction tasks other than lung cancer onset?
  - Basis: Authors state "Modest improvement was found for a single use case and further evaluation on other databases and outcomes is needed."
  - Why unresolved: Study restricted to single outcome and dataset
  - What evidence would resolve: Replicating methodology on diverse clinical outcomes

- Question: Do pre-trained Poincaré embeddings improve transportability across databases with heterogeneous coding practices?
  - Basis: Authors hypothesize embeddings "could address some degree of database heterogeneity"
  - Why unresolved: Single EHR source used; hypothesis not tested
  - What evidence would resolve: External validation comparing performance gaps between standard and Poincaré-enhanced models

- Question: Would graph neural networks utilizing these embeddings outperform current architectures?
  - Basis: Authors suggest "more appropriate models may exist in the form of a graph neural network"
  - Why unresolved: Current architecture requires mapping to Euclidean tangent plane, potentially losing geometric information
  - What evidence would resolve: Comparative study evaluating hyperbolic GNNs against ResNet and Transformer models

## Limitations

- Limited to single outcome (lung cancer) and single dataset (Optum EHR), restricting generalizability
- Modest performance gains (0.02-0.03 AUROC improvement) suggest domain-specific or dataset-dependent benefits
- Lack of detailed architectural specifications for baseline ResNet and Transformer models

## Confidence

- Mechanism 1 (Hyperbolic preservation): Medium - theoretically sound but limited clinical validation
- Mechanism 2 (Log-map projection): Medium - implementation details underspecified
- Mechanism 3 (Hybrid embedding architecture): Low - no ablation studies on non-SNOMED concepts

## Next Checks

1. Replicate hyperparameter sweep for Poincaré embeddings on a different clinical dataset to verify mean rank improvements scale consistently with dimension
2. Conduct ablation study comparing Poincaré embeddings against random Euclidean embeddings on non-SNOMED concepts to assess whether hierarchical benefit is SNOMED-specific
3. Test model robustness by training on one healthcare system and evaluating on another to assess transportability and potential bias reduction claims