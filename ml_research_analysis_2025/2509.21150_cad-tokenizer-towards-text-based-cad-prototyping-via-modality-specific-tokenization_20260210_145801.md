---
ver: rpa2
title: 'CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization'
arxiv_id: '2509.21150'
source_url: https://arxiv.org/abs/2509.21150
tags:
- cad-tokenizer
- primitive
- tokens
- sequence
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAD-Tokenizer, the first framework for unified
  text-based CAD prototyping that combines Text-to-CAD generation and CAD editing.
  The key innovation is a modality-specific tokenization strategy using a sequence-based
  VQ-VAE with primitive-level pooling, which produces compact, primitive-aware tokens
  aligned with CAD's structural nature.
---

# CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization

## Quick Facts
- **arXiv ID**: 2509.21150
- **Source URL**: https://arxiv.org/abs/2509.21150
- **Reference count**: 40
- **Key outcome**: Introduces a unified text-based CAD prototyping framework that combines Text-to-CAD generation and CAD editing, achieving better performance than general-purpose LLMs and task-specific baselines through modality-specific tokenization and FSA-guided decoding.

## Executive Summary
This paper introduces CAD-Tokenizer, the first framework for unified text-based CAD prototyping that combines Text-to-CAD generation and CAD editing. The key innovation is a modality-specific tokenization strategy using a sequence-based VQ-VAE with primitive-level pooling, which produces compact, primitive-aware tokens aligned with CAD's structural nature. This approach enables LLMs to directly model inter-primitive relations and predict the next operation rather than the next word piece. CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance over both general-purpose LLMs and task-specific baselines. For example, on CAD editing, it improves F1 scores by around 10 points each compared to the task-specific CAD-Editor model.

## Method Summary
CAD-Tokenizer addresses the challenge of text-based CAD prototyping by developing a modality-specific tokenization strategy. The core component is a sequence-based VQ-VAE with primitive-level pooling that decomposes CAD construction sequences into sketch-extrusion pairs (primitives) and generates discrete tokens for each primitive. These tokens are aligned with LLM embeddings using bidirectional adapters that map between the VQ-VAE latent space and the frozen LLM embedding space. The model is fine-tuned on CAD tasks using LoRA adapters, and inference employs a Finite-State Automaton to enforce CAD grammar constraints and improve generation validity.

## Key Results
- CAD-Tokenizer significantly improves instruction following and generation quality compared to both general-purpose LLMs and task-specific baselines
- Achieves approximately 10-point F1 score improvements in CAD editing compared to the task-specific CAD-Editor model
- VQ-VAE with primitive-level pooling reduces token count from 800+ to 60 while maintaining semantic integrity
- Finite-State Automaton reduces Invalidity Ratio from 4.1% to 0.4% by enforcing CAD grammar constraints

## Why This Works (Mechanism)

### Mechanism 1: Primitive-Level Tokenization via VQ-VAE with Primitive Pooling
- **Claim:** Tokenizing CAD sequences into primitive-level discrete tokens instead of natural language word pieces improves the LLM's ability to model inter-primitive relations and predict the next operation.
- **Mechanism:** A sequence-based VQ-VAE encoder processes the CAD construction sequence. A primitive-specific pooling layer replaces standard global pooling, generating distinct latent representations for each sketch-extrusion pair (primitive). These latent vectors are quantized via a codebook lookup (VQ). The resulting discrete tokens represent compact, semantically meaningful CAD primitives. This allows the LLM to predict the next primitive token (operation), rather than next word piece, reducing fragmentation.
- **Core assumption:** Sketch-extrusion pairs form a natural semantic unit ("primitive") within a CAD construction sequence that captures sufficient geometric intent for reconstruction and generation, and that a shared codebook can represent these diverse units.
- **Evidence anchors:**
  - [abstract] "We propose CAD-Tokenizer... using a sequence-based VQ-VAE with primitive-level pooling... produces compact, primitive-aware tokens... LLMs to directly model inter-primitive relations and predict the next operation rather than the next word piece."
  - [Section 3.2] "Each input sequence C needs to be decomposed into sketch–extrusion pairs... encoded into latent vectors... by transformer encoders. Unlike standard VQ-VAEs that pool the entire sequence into a single latent vector, we introduce a primitive-specific pooling layer, producing multiple discrete representations for each sketch–extrusion pair."
  - [corpus] Neighboring papers like "Draw it like Euclid" (arXiv:2601.09428) also explore generating CAD via construction steps, supporting the intuition that CAD has an inherent sequential, constructive structure amenable to specialized tokenization.
- **Break condition:** If the primitive definition (sketch-extrusion pair) is too coarse, losing critical internal detail (e.g., individual curve parameters), or too fine, failing to capture the holistic primitive intent, reconstruction and generation quality will degrade. If the VQ codebook capacity is insufficient for the diversity of primitives, representation collapse or poor reconstruction will occur.

### Mechanism 2: Cross-Modal Alignment via Bidirectional Adapters
- **Claim:** Aligning the learned CAD token embeddings with the pre-trained LLM's embedding space using lightweight adapters enables seamless integration and efficient fine-tuning.
- **Mechanism:** The VQ-VAE produces discrete tokens corresponding to indices in its codebook. These tokens, represented by their codebook vectors, exist in a $d_{vq}$-dimensional space. To use them with an LLM with embedding dimension $d_{tok}$, two linear adapter layers are trained. One adapter maps from $d_{vq}$ to $d_{tok}$, projecting the CAD token into the LLM's input space. A second adapter maps from $d_{tok}$ back to $d_{vq}$, trained with a reconstruction loss to ensure the mapped token retains fidelity to the original VQ representation. This bridges the modality gap without modifying the frozen LLM backbone or the pre-trained VQ-VAE.
- **Core assumption:** The pre-trained LLM's embedding space is sufficiently expressive and "universal" that CAD primitive semantics can be linearly projected into it without catastrophic forgetting or distortion, and that a simple linear adapter is sufficient for this alignment.
- **Evidence anchors:**
  - [Section 3.2] "We introduce a novel use of adapters that map VQ-VAE outputs and LLM embeddings bi-directionally... leverage the frozen embedding and logit layers of the pretrained LLM. Adapters are trained with a vector reconstruction loss..."
  - [Section 3.2] "This alignment produces native LLM-recognizable primitive IDs, enabling seamless integration without modifying the backbone."
  - [corpus] Weak/missing. The specific technique of bidirectional adapter training for VQ-VAE to LLM alignment is a novel contribution of this paper. General adapter techniques (like LoRA, cited in the paper) are common, but this specific application is unique.
- **Break condition:** If the CAD primitive semantic space and the LLM text embedding space are fundamentally misaligned (e.g., highly non-linear relationship), a simple linear adapter will fail to create a meaningful mapping, leading to poor instruction following and generation. If the adapter adds too much capacity, it could overfit during the alignment phase and not generalize.

### Mechanism 3: Grammar-Constrained Decoding via Finite-State Automaton (FSA)
- **Claim:** Enforcing CAD grammar constraints during inference via a Finite-State Automaton (FSA) reduces syntactic errors and improves the validity of generated CAD sequences.
- **Mechanism:** CAD construction sequences follow a formal grammar (e.g., a sketch must precede an extrusion, curves must be closed loops). An FSA is designed to encode these valid state transitions. During autoregressive decoding, at each step, the FSA provides a binary mask over the vocabulary, restricting the LLM's output logits to only grammar-compliant tokens. The next token is sampled from this restricted distribution. The FSA state updates based on the chosen token.
- **Core assumption:** The vast majority of invalid or low-quality CAD generations from the fine-tuned LLM are due to syntactic violations (grammar errors) rather than semantic errors that still conform to the grammar. The FSA can adequately capture the necessary syntactic rules.
- **Evidence anchors:**
  - [abstract] "Our contributions are... we leverage the formal grammar of CAD by introducing a finite-state automaton–based sampling strategy, which enforces valid syntax in generation and improves quality."
  - [Section 3.4] "Unlike natural language, CAD sequences are formal languages... we can improve the sample quality by designing a finite-state automaton (FSA) that formalizes valid CAD construction rules... At each step, the FSA provides a series of masks restricting the candidate logits to grammar-compliant tokens, ensuring syntactic validity."
  - [corpus] Weak/missing. The use of FSA for constrained decoding in generative models exists (e.g., in structured data generation), but its specific application to CAD sequence generation with LLMs is a key contribution of this work.
- **Break condition:** If the FSA is too restrictive (e.g., cannot represent certain valid but complex CAD constructs), it will stifle creativity and limit the model's capability. If the FSA is too lenient, it will fail to prevent syntactic errors. If the LLM's learned distribution places probability mass on syntactically invalid tokens that are semantically "close" to valid ones, the FSA might force a valid but nonsensical token, leading to semantic errors despite syntactic correctness.

## Foundational Learning

- **Concept: Vector Quantized Variational Autoencoder (VQ-VAE)**
  - **Why needed here:** The core of the CAD-Tokenizer is a VQ-VAE. Understanding how it compresses continuous data into a discrete latent space (codebook) is essential to grasp how CAD sequences become tokens.
  - **Quick check question:** Can you explain the role of the codebook in a VQ-VAE and how the "straight-through estimator" helps in training?

- **Concept: Large Language Model (LLM) Tokenization and Embeddings**
  - **Why needed here:** The paper critiques standard LLM tokenizers (BPE) and aligns new CAD tokens with LLM embeddings. You need to know what tokenizers do and what the embedding space represents.
  - **Quick check question:** What is the fundamental difference between a subword tokenizer (like BPE) and the tokenizer proposed in this paper for CAD data? Why does this matter for an LLM?

- **Concept: Finite-State Automaton (FSA)**
  - **Why needed here:** The paper uses an FSA for constrained decoding. You need to understand what an FSA is (states, transitions) and how it can enforce a formal grammar.
  - **Quick check question:** How can a state machine be used to restrict the possible next tokens generated by an LLM to ensure the output follows a specific format?

## Architecture Onboarding

- **Component map:**
  1. CAD Sequence: Raw input (sketch/extrusion commands)
  2. VQ-VAE Encoder: Transformer-based, processes the sequence
  3. Primitive Pooling: Custom layer that segments the encoder's output into primitive-level vectors
  4. Vector Quantization (VQ) Layer: Looks up primitive vectors in a learned codebook, outputting discrete token indices
  5. Adapter Layers (x2): Linear layers. Adapter 1 maps VQ vectors to LLM embedding space. Adapter 2 maps LLM embeddings back to VQ space (for alignment training)
  6. LLM Backbone (e.g., LLaMA): The frozen pre-trained model. Instruction text is tokenized normally. CAD sequences are tokenized by the VQ-VAE and Adapter 1
  7. VQ-VAE Decoder: Transformer-based, reconstructs the CAD sequence from discrete tokens (used in VQ-VAE pre-training)
  8. Finite-State Automaton (FSA): A logical module (not a neural network) that runs during inference, masking LLM logits to enforce CAD grammar

- **Critical path:**
  1. Pre-training: Train the VQ-VAE (Encoder + Pooling + VQ + Decoder) on CAD sequences to achieve good reconstruction
  2. Alignment: Freeze the VQ-VAE and LLM. Train the two Adapter layers using the vector reconstruction loss
  3. Fine-tuning: Freeze the LLM backbone and fine-tune LoRA adapters (a different kind of adapter, low-rank) on the unified text-to-CAD and CAD editing task. The input is (instruction, encoded CAD). The target is encoded CAD
  4. Inference: Use the fine-tuned LLM. Pass the instruction. For CAD output, the LLM generates token IDs, which are masked by the FSA. The chosen tokens are decoded back to a CAD sequence by the VQ-VAE Decoder

- **Design tradeoffs:**
  - Primitive Granularity: Pooling at the curve level vs. loop level vs. sketch level. The paper finds pooling at the loop/primitive level is optimal. A finer level creates too many tokens for the LLM; a coarser level compresses too much information for the VQ-VAE
  - Tokenizer Alignment: Training adapters vs. co-training the VQ-VAE with the LLM. Adapters are more compute-efficient but assume a linear relationship between embedding spaces
  - Decoding Strategy: FSA-guided vs. top-p vs. beam search. FSA improves validity and quality but requires manually defining the automaton rules, which might not cover all edge cases

- **Failure signatures:**
  - Tokenizer Collapse: The VQ-VAE reconstruction loss plateaus high. Generated CAD sequences are nonsensical. Check codebook usage; if only a few codebook entries are used, the capacity is wasted
  - Misalignment: The LLM ignores the CAD tokens or treats them as noise. Instruction following is poor. The adapter reconstruction loss may be high
  - Over-constrained Decoding: The model gets stuck in a loop or generates trivially simple shapes because the FSA is too restrictive or the LLM places no probability mass on valid complex tokens
  - Vanilla-LLaMA style failure: Generated CAD sequences are fragments and syntactically invalid, proving the default tokenizer is unsuitable (this is a baseline failure, not a failure of the proposed method)

- **First 3 experiments:**
  1. Tokenizer Reconstruction Quality: Train the primitive-pooling VQ-VAE on the CAD dataset. Measure reconstruction F1-score and Chamfer Distance. Compare against a baseline without primitive pooling (single latent vector). This validates the core representation
  2. Ablation on Pooling Granularity: Train and fine-tune the full CAD-Tokenizer pipeline using different pooling variants (e.g., 'curve', 'loop', 'single'). Compare downstream performance on Text-to-CAD generation to find the optimal granularity
  3. FSA vs. Standard Decoding: Take the fine-tuned CAD-Tokenizer model. Generate CAD sequences using FSA-guided sampling, top-p sampling, and beam search. Compare the Invalidity Ratio (IR) and other quality metrics to demonstrate the FSA's effectiveness

## Open Questions the Paper Calls Out

- **Question:** How can evaluation metrics be redesigned to accurately capture the "intent to keep the original shape" in CAD editing tasks, bridging the gap between distributional scores and semantic correctness?
- **Basis in paper:** [explicit] Section 5 (Limitations) explicitly states there is a "gap between the distributional metrics and the actual performance in CAD Editing, which stems from the failure to evaluate the 'intent to keep the original shape.'"
- **Why unresolved:** Current distributional metrics like COV and JSD give high scores to models that reproduce the original sequence without editing, failing to penalize the disregard of editing instructions
- **What evidence would resolve it:** The proposal of a new metric or evaluation protocol that penalizes failure to modify the input sequence when an edit is requested, showing high correlation with human judgment of instruction following

## Limitations
- The bidirectional adapter alignment assumes a linear relationship between CAD primitive embeddings and LLM text embeddings, which may not hold for complex semantic relationships
- The FSA relies on manually defined grammar rules that may not cover all valid CAD constructs, potentially limiting model capability or allowing invalid sequences through
- The model's effectiveness on complex industrial CAD shapes is unproven due to the quality gap between open-source and private-sector CAD datasets

## Confidence
- **High Confidence**: VQ-VAE with primitive pooling effectively creates compact, semantically meaningful tokens for CAD sequences
- **Medium Confidence**: CAD-Tokenizer significantly improves instruction following and generation quality compared to baselines
- **Low Confidence**: Bidirectional adapter alignment is the most efficient and effective way to bridge VQ-VAE and LLM embedding spaces

## Next Checks
1. **FSA Robustness Test**: Design test sequences with nested operations and conditional logic to evaluate FSA completeness and its impact on generation quality
2. **Adapter Alignment Ablation**: Compare linear adapter alignment to joint training and non-linear alignment methods to validate the optimal bridging approach
3. **Generalization to Diverse CAD Domains**: Evaluate performance on a different CAD dataset with distinct style or application to assess cross-domain generalization