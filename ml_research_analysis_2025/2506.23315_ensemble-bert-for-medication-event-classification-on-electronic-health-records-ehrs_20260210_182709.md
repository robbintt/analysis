---
ver: rpa2
title: Ensemble BERT for Medication Event Classification on Electronic Health Records
  (EHRs)
arxiv_id: '2506.23315'
source_url: https://arxiv.org/abs/2506.23315
tags:
- bert
- medication
- data
- ensemble
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ensemble BERT model for medication event
  classification on Electronic Health Records (EHRs) in the n2c2 2022 challenge. The
  approach pretrains multiple BERT models on general (Wikipedia), biomedical (PubMed),
  and clinical (MIMIC) data, fine-tunes them on the CMED training set, and integrates
  their predictions using majority voting.
---

# Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)

## Quick Facts
- arXiv ID: 2506.23315
- Source URL: https://arxiv.org/abs/2506.23315
- Authors: Shouvon Sarker; Xishuang Dong; Lijun Qian
- Reference count: 24
- Primary result: Ensemble BERT improves strict Micro-F1 by ~5% and strict Macro-F1 by ~6% compared to individual pretrained models

## Executive Summary
This paper presents an ensemble BERT approach for medication event classification in clinical text, achieving state-of-the-art performance on the n2c2 2022 medication classification challenge. The method combines multiple BERT models pretrained on different corpora (Wikipedia, PubMed, MIMIC) through majority voting after fine-tuning on the CMED dataset. The ensemble achieves significant improvements in both strict Micro-F1 (0.8401) and Macro-F1 (0.7744) metrics, demonstrating the value of domain-diversified pretraining for clinical NLP tasks.

## Method Summary
The approach pretrains multiple BERT models on general (Wikipedia), biomedical (PubMed), and clinical (MIMIC) data, then fine-tunes each on the CMED training set containing 7,230 medication mentions across 400 notes. After fine-tuning, the models generate probability distributions over three classes (Disposition, NoDisposition, Undetermined) for each token in test notes. These predictions are integrated using majority soft voting, where class probabilities are summed across all models and the highest-scoring class is selected. The method also performs medication identification as a binary drug/non-drug extraction task.

## Key Results
- Ensemble BERT achieves strict Micro-F1 of 0.8401 and strict Macro-F1 of 0.7744
- Improves over best single model (Robertalarge) by ~5% in strict Micro-F1 and ~6% in strict Macro-F1
- Shows precision improvements of up to 6% for medication identification task
- Majority voting outperforms weighted voting based on Expected Calibration Error

## Why This Works (Mechanism)

### Mechanism 1: Domain-Diversified Pretraining Transfer
Combining BERT models pretrained on different corpora (general, biomedical, clinical) captures complementary linguistic patterns. Wikipedia-trained models learn general syntax, PubMed models capture biomedical terminology, and MIMIC models learn clinical shorthand and abbreviations. When fine-tuned on the same task, each model makes different errors that ensemble voting can reconcile.

### Mechanism 2: Majority Soft Voting for Classification Robustness
Soft voting across ensemble members improves classification by summing probability distributions before selecting the final class. This reduces variance in boundary cases where individual models are uncertain, as each model's confidence scores are aggregated rather than selecting the single most confident prediction.

### Mechanism 3: Task-Specific Fine-Tuning for Context Learning
Fine-tuning pretrained models on CMED training data adapts general/biomedical representations to the specific medication event classification task. The 400-note training set contains 7,230 annotated medication mentions that help models recognize patterns like "no longer taking metronidazole" → B-Disposition.

## Foundational Learning

- Concept: BERT Architecture & Tokenization
  - Why needed here: Understanding that BERT uses 512-token sequences with [CLS], [SEP], and [MASK] tokens is necessary to debug preprocessing failures and interpret model outputs.
  - Quick check question: What does the [CLS] token represent in BERT's classification pipeline?

- Concept: Transfer Learning Paradigm
  - Why needed here: This entire approach relies on pretraining → fine-tuning. You must understand which weights are frozen vs. updated at each stage.
  - Quick check question: During fine-tuning on CMED, are all BERT layers updated or only the classification head?

- Concept: Evaluation Metrics for Imbalanced Classification
  - Why needed here: The CMED dataset is imbalanced (Undetermined is only 7%). Micro-F1 and Macro-F1 measure different things; choosing the wrong metric can mask poor performance on minority classes.
  - Quick check question: If a model achieves high Micro-F1 but low Macro-F1, what does this indicate about its per-class performance?

## Architecture Onboarding

- Component map: Pretraining stage (BERTbase, BioBERT variants, ClinicalBERT, RoBERTa variants) → Fine-tuning stage (each pretrained model → CMED training) → Inference stage (CMED test note → each fine-tuned model → probability distributions) → Ensemble stage (all probability distributions → majority soft voting → final prediction)

- Critical path: Pretrained model selection → fine-tuning hyperparameters → voting strategy. The paper shows majority voting outperforms weighted voting (which used Expected Calibration Error), suggesting ensemble strategy choice materially affects outcomes.

- Design tradeoffs: More ensemble members increase inference cost linearly but may yield diminishing accuracy gains; weighted voting based on ECE underperformed majority voting in this study—the paper does not fully explain why; strict vs. lenient evaluation: strict requires exact offset match; lenient requires only overlap.

- Failure signatures: Low recall on Undetermined class (7% of data): models may systematically default to Disposition/NoDisposition; correlated errors across ensemble members: if all BioBERT variants fail on the same inputs, voting provides no benefit; mismatch between training and inference tokenization: BERT's WordPiece tokenizer must be applied consistently.

- First 3 experiments:
  1. Replicate single-model baseline: Fine-tune ClinicalBERT on CMED training split; report strict/lenient Micro-F1 and Macro-F1. Compare to paper's 0.7322 strict Micro-F1.
  2. Ablate ensemble size: Start with 2-model ensemble (ClinicalBERT + BioBERT pubmed), then add Robertalarge. Measure whether gains are subadditive (suggesting correlated errors).
  3. Test voting strategy: Compare soft majority voting vs. hard voting vs. ECE-weighted voting on a held-out validation set. The paper reports weighted voting underperformed—investigate whether this holds with different calibration windows.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can weighted ensemble strategies be optimized to outperform majority voting for medication event classification?
**Basis in paper:** The conclusion explicitly states that future work will "perform more investigation on how to enhance weighted ensemble models regarding their performance."
**Why unresolved:** The current weighted ensemble (based on Expected Calibration Error) underperformed compared to the majority voting baseline, leaving the potential of weighted integration untapped.
**What evidence would resolve it:** A new weighting heuristic that achieves strict Micro-F1 or Macro-F1 scores higher than the reported majority voting baseline (0.8401 and 0.7744 respectively).

### Open Question 2
**Question:** Which alternative metrics or calibration methods can correct the "inappropriate weights" assigned during the weighted voting process?
**Basis in paper:** The results section notes that the weighted ensemble "cannot enhance performance due to inappropriate weights assigned," suggesting the chosen Expected Calibration Error (ECE) metric was insufficient.
**Why unresolved:** The paper does not explore why ECE failed or what features of the validation data should determine model influence in the ensemble.
**What evidence would resolve it:** Demonstrating that a different metric (e.g., validation F1-loss or Shannon entropy) produces weights that correlate with improved test-time accuracy.

### Open Question 3
**Question:** To what extent does the severe class imbalance (only 7% "Undetermined" class) limit the Macro-F1 performance, and can targeted data augmentation mitigate this?
**Basis in paper:** The methodology section highlights that the dataset is imbalanced with a 7% representation for the "Undetermined" class, yet no specific balancing techniques (e.g., oversampling) are integrated into the model training.
**Why unresolved:** It remains unclear if the ensemble's improvement is derived primarily from better handling of majority classes ("Disposition") or if it generalizes to the minority class.
**What evidence would resolve it:** Ablation studies showing per-class F1-scores for the "Undetermined" class with and without the use of class weights or data augmentation.

## Limitations
- Unknown fine-tuning hyperparameters (learning rate, batch size, epochs) make faithful reproduction difficult
- Class imbalance (Undetermined at only 7% of data) may cause systematic bias in model predictions
- Limited external validation raises questions about generalizability to other clinical datasets
- 400-note training set size may be insufficient for robust generalization to diverse clinical contexts

## Confidence
- **High confidence**: The core claim that ensemble BERT models improve medication event classification over individual models is well-supported by the reported metrics (Micro-F1 +5%, Macro-F1 +6%).
- **Medium confidence**: The superiority of majority voting over weighted voting is demonstrated, but the underlying reasons (why ECE weighting failed) are not thoroughly investigated.
- **Low confidence**: The generalizability of results to other clinical datasets or medication event tasks is uncertain due to the specific characteristics of the CMED dataset and the limited external validation.

## Next Checks
1. **Ablate ensemble size systematically**: Test whether adding more models yields diminishing returns by measuring performance gains as ensemble size increases from 2 to 11 members. This will reveal if models make correlated errors that limit ensemble benefits.

2. **Characterize class-wise performance**: Analyze per-class precision, recall, and F1 scores for all three labels (Disposition, NoDisposition, Undetermined). Focus on understanding why Undetermined (7% class) may be systematically under-predicted, and test class weighting or oversampling strategies to address imbalance.

3. **Validate voting strategy robustness**: Compare majority voting against weighted voting (using multiple calibration windows) and hard voting on a held-out validation set. Investigate whether the poor performance of ECE-weighted voting persists with different calibration approaches or if it was due to implementation specifics.