---
ver: rpa2
title: Task-Level Insights from Eigenvalues across Sequence Models
arxiv_id: '2510.09379'
source_url: https://arxiv.org/abs/2510.09379
tags:
- layer
- attention
- eigenvalue
- norm
- mamba-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a dynamical systems framework to analyze
  and compare the eigenvalue spectra of different sequence models, including softmax
  attention, linear attention, and state space models. The key finding is that eigenvalues
  are strongly correlated with task requirements: tasks needing long-term memory show
  eigenvalues clustered near one, while tasks requiring selective forgetting exhibit
  peaks near zero.'
---

# Task-Level Insights from Eigenvalues across Sequence Models

## Quick Facts
- **arXiv ID:** 2510.09379
- **Source URL:** https://arxiv.org/abs/2510.09379
- **Reference count:** 40
- **Primary result:** Eigenvalue spectra of sequence models (attention, SSMs) correlate with task memory requirements, offering a principled way to analyze and guide architectural design.

## Executive Summary
This work introduces a dynamical systems framework to analyze and compare the eigenvalue spectra of different sequence models, including softmax attention, linear attention, and state space models. The key finding is that eigenvalues are strongly correlated with task requirements: tasks needing long-term memory show eigenvalues clustered near one, while tasks requiring selective forgetting exhibit peaks near zero. These spectral signatures persist across model classes and align with task performance, revealing a principled way to understand and guide architectural choices. The study also shows that architectural modifications, such as gating and convolution, are reflected in the eigenvalue spectra, offering a tool for interpreting model behavior and improve design.

## Method Summary
The study uses the Dynamical Systems Framework (DSF) to extract eigenvalue distributions from sequence models trained on long-range dependency tasks. Six model architectures (S4, LRU, Mamba-2, softmax/linear/norm attention) are trained on the LRA benchmark (ListOps, IMDb, CIFAR-10), MQAR, and WikiText-103. Eigenvalues are computed from the state transition matrix Λ for each model, with special handling for LPV models (Mamba-2, attention) via DSF reformulation. Performance metrics (accuracy, perplexity) are correlated with eigenvalue distributions binned in ranges [0.0-0.1, 0.1-0.5, 0.5-0.9, 0.9-1, 1-10, 10-100, >100]. Models are trained with AdamW, linear warmup, and cosine annealing, using GPT-2 style backbones with task-specific configurations.

## Key Results
- Eigenvalue distributions align with task requirements: ListOps (long memory) → eigenvalues near 1; MQAR (selective memory) → eigenvalues near 0.
- Architectural modifications (gating, convolution) shift eigenvalue spectra in predictable ways, correlating with improved performance.
- Attention models develop task-appropriate eigenvalue spectra despite lacking explicit spectral constraints during training.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Eigenvalue distributions of the state transition matrix Λ reveal memory retention characteristics that correlate with task requirements.
- **Mechanism:** The state transition Λ governs information decay. Eigenvalues near 1 preserve information across timesteps (slow decay), while eigenvalues near 0 induce rapid forgetting. This maps directly to task demands: ListOps requires preserving all tokens (eigenvalues concentrated near 1), while MQAR needs selective retrieval (eigenvalues near 0).
- **Core assumption:** Assumption: The DSF representation (Equation 1) faithfully captures the essential dynamics of both SSMs and attention mechanisms, enabling cross-architectural comparison via eigenvalues.
- **Evidence anchors:**
  - [abstract] "revealing spectral signatures that align with task requirements... high concentration near one for long-memory tasks and near zero for selective memory tasks"
  - [Section 4.2] "When long-term memory is critical, we consistently observe a strong concentration of eigenvalues near one, whereas tasks that require selective forgetting exhibit peaks near zero"
  - [corpus] Weak corpus evidence; neighboring papers discuss attention alternatives but do not directly validate eigenvalue-task correlations.
- **Break condition:** If tasks require different memory patterns than simple binary long/selective (e.g., medium-term dependencies), the eigenvalue-to-performance mapping may not hold linearly.

### Mechanism 2
- **Claim:** Architectural modifications shift eigenvalue distributions in predictable ways, reallocate computational responsibilities between components.
- **Mechanism:** Explicit gating handles selectivity externally, freeing Λ from needing eigenvalues near 0, shifting distributions toward 1. Convolution provides local context directly, reducing need for Λ to maintain long-term memory, shifting distributions toward 0.
- **Core assumption:** Assumption: The observed spectral shifts causally improve performance rather than correlating with other architectural benefits.
- **Evidence anchors:**
  - [Section 5.1] "introducing an explicit gating mechanism alleviates the need for the dynamical system to implement gating implicitly... this shift reflects a redistribution of eigenvalues away from the selectivity regime (near zero) toward the memory regime (near one)"
  - [Section 5.2] "Introduction of convolution causes a shift in the eigenvalue spectrum, with eigenvalues occurring more frequently near zero"
  - [corpus] Related work (Grazzi et al. 2025, cited in paper) discusses eigenvalue placement in SSMs but does not validate gating/convolution spectral effects.
- **Break condition:** If adding gating or convolution introduces optimization difficulties or conflicts with other inductive biases, the spectral shift may not translate to performance gains.

### Mechanism 3
- **Claim:** Attention models develop eigenvalue spectra despite lacking explicit spectral constraints during training, suggesting implicit pressure toward task-appropriate dynamics.
- **Mechanism:** While SSMs explicitly constrain eigenvalues (e.g., to unit circle for stability), attention models' eigenvalues emerge from learned W_Q, W_K weights through the DSF reformulation. On ListOps, even attention avoids near-zero eigenvalues, suggesting training dynamics implicitly shape spectra.
- **Core assumption:** Assumption: The DSF-computed eigenvalues capture meaningful properties of attention rather than artifacts of the reformulation.
- **Evidence anchors:**
  - [Section 4.2] "It is interesting that this behavior extends to attention models, despite the fact that their eigenvalues are not explicitly designed but instead emerge from learned weight matrices"
  - [Section 3.1] Derivation showing Λ_i = η(q_{i-1}, k_{i-1}) / η(q_i, k_i) for attention mechanisms
  - [corpus] "Design Principles for Sequence Models via Coefficient Dynamics" (corpus neighbor) develops unified frameworks but does not address implicit spectral emergence.
- **Break condition:** If the DSF normalization η introduces artifacts or if eigenvalues above 1 (observed in attention) cause instability in practice, the interpretability of attention spectra degrades.

## Foundational Learning

- **Concept: Linear time-invariant (LTI) vs. linear parameter-varying (LPV) systems**
  - **Why needed here:** SSMs like S4 are LTI (fixed Λ), while attention and Mamba-2 are LPV (input-dependent Λ). This distinction affects how eigenvalues are computed and interpreted.
  - **Quick check question:** Given h_i = Λ_i h_{i-1} + B_i u_i, if Λ_i depends on u_i, can you compute eigenvalues at initialization before seeing data?

- **Concept: Eigenvalue interpretation in discrete-time systems**
  - **Why needed here:** The paper's core claim rests on |λ| < 1 for stability, |λ| ≈ 1 for memory, |λ| ≈ 0 for forgetting. Understanding why requires grasping that h_t = Λ^t h_0.
  - **Quick check question:** If Λ has an eigenvalue of 0.9, approximately how many timesteps until the corresponding mode decays to <1% of its initial value?

- **Concept: State space model discretization**
  - **Why needed here:** SSMs operate in continuous time and require discretization (controlled by Δ) to become Λ. Mamba-2 makes Δ input-dependent; the pseudo-LTI variant fixes it.
  - **Quick check question:** If continuous-time eigenvalue λ_c = -1 and discretization step Δ = 0.1, what is the discrete eigenvalue under Euler discretization?

## Architecture Onboarding

- **Component map:** h_i = Λ_i h_{i-1} + B_i u_i, y_i = C_i h_i + D_i u_i -> DSF reformulation for attention -> eigenvalue computation via eigendecomposition -> task performance correlation

- **Critical path:**
  1. Identify model class (attention vs. SSM, LTI vs. LPV)
  2. Extract Λ appropriately (batch-average for LPV/input-dependent)
  3. Compute eigenvalues, aggregate across heads/layers
  4. Map to task memory requirements using the spectral signatures framework

- **Design tradeoffs:**
  - LTI (S4, LRU): Strong long-memory performance, poor selectivity (few eigenvalues near 0)
  - LPV (Mamba-2): Balanced memory/selectivity, higher variance in eigenvalues across inputs
  - Attention: Unconstrained spectra (can exceed 1, potential instability), emergent task-dependent patterns

- **Failure signatures:**
  - Eigenvalues > 1 (observed in attention on CIFAR-10): may indicate unstable dynamics, correlates with poor LRA performance
  - Excessive gating (eigenvalues near 0) on tasks requiring full-sequence memory (e.g., CIFAR-10 performance drops with gating)
  - High variance across inputs (Mamba-2 on ListOps): suggests input-dependency may conflict with uniform memory requirements

- **First 3 experiments:**
  1. **Baseline spectral profiling:** Train S4, Mamba-2, and softmax attention on a single LRA task (e.g., ListOps). Compute eigenvalue distributions at initialization and after training. Verify that well-performing models concentrate eigenvalues near 1.
  2. **Ablation on architectural modifications:** Take linear attention on MQAR. Add (a) explicit gating, (b) 1D convolution, (c) both. Measure performance and eigenvalue shifts. Expect gating to reduce need for near-0 eigenvalues, convolution to reduce need for near-1 eigenvalues.
  3. **Task transfer analysis:** Train a model on MQAR (selective memory), then fine-tune on ListOps (full memory). Track eigenvalue distribution shift during fine-tuning to verify task-appropriate spectral adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does introducing explicit gating to attention models on the ListOps task induce low eigenvalue spectra (selective forgetting) when the task theoretically requires preserving all input information?
- Basis in paper: [explicit] Section 5.1 states: "An exception arises on ListOps... causes explicit gating to interact with the dynamics in an atypical way... The precise mechanism behind this effect remains unclear."
- Why unresolved: The authors observe the counter-intuitive spectral shift and performance variance but lack a theoretical explanation for why the gating mechanism forces this behavior on a memory-preserving task.
- What evidence would resolve it: An analysis isolating the interaction between the SiLU activation and attention normalization terms specifically on hierarchical reasoning tasks, or ablation studies showing if the effect is an artifact of optimization dynamics.

### Open Question 2
- Question: Can task-specific initialization schemes be designed to enforce optimal eigenvalue distributions, particularly for architectures like Linear Attention which tend to retain their initial spectral shape during training?
- Basis in paper: [explicit] Section 4.2 notes that Linear Attention retains its initialization shape, "raising the question of the potential importance of a task-dependent initialization."
- Why unresolved: While the paper establishes that final eigenvalue distributions correlate with task performance, it does not validate whether pre-setting these distributions via initialization is sufficient or necessary for success.
- What evidence would resolve it: Experiments initializing Linear Attention layers with eigenvalues concentrated near one for long-memory tasks, demonstrating improved convergence or accuracy compared to standard initializations.

### Open Question 3
- Question: How do positional embeddings influence the eigenvalue spectra of attention mechanisms when represented within the Dynamical Systems Framework?
- Basis in paper: [explicit] Section 6 (Limitations) states: "Extending these analyses... to other choices, such as positional embeddings, represents an important direction for future research."
- Why unresolved: The study compares SSMs (which lack positional embeddings) and attention models (which use them), but does not isolate the specific spectral contribution or distortion caused by the embeddings themselves.
- What evidence would resolve it: A comparative spectral analysis of attention models trained with and without positional embeddings to quantify the shift in eigenvalue distributions attributable to positional information.

## Limitations

- The framework's generalizability is limited by the small set of tasks (LRA suite plus MQAR) with binary memory patterns.
- The DSF reformulation's ability to faithfully capture attention dynamics remains untested beyond this work.
- Eigenvalues > 1 in attention models raise stability questions not addressed here.

## Confidence

- **High confidence:** SSM eigenvalue-task correlations (well-established in prior SSM literature, validated here)
- **Medium confidence:** Attention eigenvalue emergence and task alignment (DSF reformulation novel, limited cross-task validation)
- **Medium confidence:** Architectural modification effects on spectra (mechanism plausible but sample size limited to single tasks)

## Next Checks

1. **Cross-task generalization:** Apply the spectral profiling framework to a diverse set of tasks including those with mixed memory requirements (e.g., parity, copy, or sequential decision tasks) to test whether eigenvalue distributions adapt appropriately beyond the binary long/selective framework.

2. **Ablation on reformulation artifacts:** Design a controlled experiment comparing eigenvalues computed via DSF against direct eigendecomposition of learned attention weight matrices (where feasible) to isolate whether observed patterns reflect true model dynamics or DSF-specific effects.

3. **Stability verification for attention:** Implement a gradient-based attack or noise injection test to measure the practical impact of eigenvalues > 1 in attention models, quantifying whether these values correlate with training instability or performance degradation.