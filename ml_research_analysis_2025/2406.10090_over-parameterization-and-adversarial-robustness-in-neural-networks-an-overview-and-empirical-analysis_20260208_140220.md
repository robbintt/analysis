---
ver: rpa2
title: 'Over-parameterization and Adversarial Robustness in Neural Networks: An Overview
  and Empirical Analysis'
arxiv_id: '2406.10090'
source_url: https://arxiv.org/abs/2406.10090
tags:
- networks
- robustness
- adversarial
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the robustness of over-parameterized neural
  networks against adversarial attacks. The authors empirically compare the adversarial
  robustness of neural networks with varying parameter sizes using the Projected Gradient
  Descent (PGD) attack and a comprehensive ensemble attack called AutoAttack.
---

# Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis

## Quick Facts
- arXiv ID: 2406.10090
- Source URL: https://arxiv.org/abs/2406.10090
- Reference count: 40
- Key outcome: Over-parameterized neural networks show significantly higher adversarial robustness than under-parameterized counterparts

## Executive Summary
This study investigates the robustness of over-parameterized neural networks against adversarial attacks. The authors empirically compare the adversarial robustness of neural networks with varying parameter sizes using the Projected Gradient Descent (PGD) attack and AutoAttack ensemble. Experiments conducted on three neural network architectures (CNN, FC-ReLU, and ResNet) trained on MNIST and CIFAR10 datasets demonstrate that over-parameterized networks are significantly more robust to adversarial attacks than their under-parameterized counterparts, contradicting classical overfitting expectations.

## Method Summary
The study employs three neural network architectures scaled by width parameter z: CNN (2 conv layers, z ∈ [1,30]), FC-ReLU (2 hidden layers with z and 8z neurons, z ∈ [4,40]), and ResNet (6 residual blocks). Models were trained using Adam optimizer (lr=0.001) on subsampled datasets (MNIST: 3k train/1k test; CIFAR10: 20k train/1k test). Adversarial robustness was evaluated using L2-PGD (100 iterations, step size 1e-3) across multiple ε values, with results verified using AutoAttack ensemble and Indicators of Attack Failure (IoAF) to ensure attack effectiveness.

## Key Results
- Over-parameterized networks consistently show lower error rates under adversarial attacks across all tested architectures
- Security Evaluation Curves demonstrate robustness improvements beyond the interpolation point
- IoAF verification confirmed no attack failures, validating the reliability of robustness results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterized networks exhibit higher adversarial robustness (lower error rates under attack) compared to under-parameterized counterparts.
- Mechanism: Empirical results demonstrate that as model capacity increases beyond the interpolation point, error rates under PGD and AutoAttack consistently decrease across CNN, FC-ReLU, and ResNet architectures on MNIST and CIFAR10.
- Core assumption: The observed robustness improvement is intrinsic to the larger parameter space and not an artifact of attack failure.
- Evidence anchors:
  - [abstract] "Our results show that over-parameterized networks are robust against adversarial attacks as opposed to their under-parameterized counterparts."
  - [section 4.2.2] Security evaluation curves (Fig. 3) show error rates for over-parameterized models are significantly lower than smaller models under PGD attacks across MNIST-CNN, MNIST-FC-ReLU, and CIFAR10-ResNet.
  - [corpus] Related work on low-capacity networks (arxiv 2507.16278) suggests robustness correlates with capacity, supporting the trend but not confirming causality.

### Mechanism 2
- Claim: Reliable robustness evaluation requires verifying attack effectiveness to avoid false security conclusions.
- Mechanism: The paper addresses potential attack failure (gradient masking, suboptimal optimization) by: (1) using AutoAttack ensemble to diversify attack strategies, and (2) applying Indicators of Attack Failures (IoAF) to detect optimization issues.
- Core assumption: Current attack failure detection tools are comprehensive enough to catch common failure modes.
- Evidence anchors:
  - [abstract] "However, unlike the previous works, we also evaluate the considered attack's reliability to support the results' veracity."
  - [section 3.4] "We verified the results obtained with the PGD attack using Indicators of Attack Failures (IoAF)... we have observed no triggers for attack failures for any of the six indicators."

### Mechanism 3
- Claim: Increasing parameters beyond the zero-training-error interpolation point benefits robustness, contradicting classical overfitting expectations.
- Mechanism: Empirical observations show that as parameter count increases (e.g., MNIST-CNN: 760 → 38,170; CIFAR10-ResNet: 25k → 11M), error rates on adversarial examples decrease.
- Core assumption: The observed trend generalizes beyond the tested architectures and datasets.
- Evidence anchors:
  - [abstract] "...provides empirical evidence that increasing the number of parameters beyond the point of zero training error improves the network's robustness..."
  - [section 4.2.1] Base performance figures show test error continues to decrease with more parameters, contrary to classical overfitting.

## Foundational Learning

- **Concept: Projected Gradient Descent (PGD) Attack**
  - Why needed here: Core method for generating adversarial examples and evaluating robustness. Understanding how PGD iteratively maximizes loss within perturbation bounds is essential to interpret security evaluation curves.
  - Quick check question: Can you explain how PGD's projection step keeps perturbations within the allowed ε-ball?

- **Concept: Double-Descent Curve**
  - Why needed here: Frames why over-parameterized models might generalize (and potentially robustify) better than classical bias-variance tradeoff predicts. The paper explicitly references Belkin et al.'s theory.
  - Quick check question: What happens to variance and total error as model capacity moves beyond the interpolation point?

- **Concept: Gradient Masking and Attack Failure Detection**
  - Why needed here: Critical for evaluating whether robustness claims are real or artifacts of broken attacks. The paper uses IoAF and AutoAttack to address this.
  - Quick check question: Why might a gradient-based attack fail to find an adversarial example even when one exists?

## Architecture Onboarding

- **Component map:**
  - Dataset subsampling -> Model training (width scaling) -> PGD attack generation -> IoAF verification -> AutoAttack validation -> Security Evaluation Curves

- **Critical path:**
  1. Train model architectures across parameter spectrum (10 sizes per architecture)
  2. Evaluate base performance on clean test data
  3. Generate adversarial examples with PGD at multiple ε levels
  4. Run IoAF to verify attack effectiveness
  5. Cross-validate with AutoAttack ensemble
  6. Plot SECs and compare robustness across parameter scales

- **Design tradeoffs:**
  - Width vs. depth scaling: Paper focuses on width-wise expansion; depth effects on robustness are not explored
  - Dataset simplicity: MNIST and CIFAR10 are relatively simple; results may not transfer to high-resolution or non-image domains
  - Attack norm: L2 norm used; L∞ or L1 norms might yield different robustness trends
  - Training regime: Standard (non-adversarial) training; adversarial training is explicitly out of scope

- **Failure signatures:**
  - False robustness: SECs show low error but IoAF triggers (e.g., gradient vanishing, convergence failure) → attack likely failed
  - No robustness gain: Over-parameterized model SECs overlap or exceed under-parameterized ones → robustness benefit absent or dataset/architecture dependent
  - Base performance collapse: Test error rises with parameters → potential training issues or data mismatch

- **First 3 experiments:**
  1. Replicate MNIST-CNN robustness trend: Train 10 CNN widths, run PGD at ε ∈ {0.3, 0.98, 1.65, 2.33, 3.0}, plot SECs, verify with IoAF. Expected: larger models show lower error curves.
  2. Test AutoAttack consistency: On a subset of models (smallest, mid, largest), run full AutoAttack ensemble. Compare SECs to PGD-only. Expected: similar trends with potentially higher error rates (stricter evaluation).
  3. Probe break condition: Deliberately use suboptimal PGD hyperparameters (e.g., 10 iterations instead of 100) on one model. Verify IoAF triggers to confirm failure detection works.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical mechanisms explain why over-parameterized networks exhibit higher adversarial robustness?
- Basis in paper: [explicit] Section 6 states the empirical evidence provided is a "starting point for theoretical works explaining why over-parametrized networks are more robust."
- Why unresolved: The current work relies solely on empirical observations and does not provide a formal theoretical explanation for the phenomenon.
- What evidence would resolve it: A formal proof or framework connecting model capacity and the interpolation threshold to adversarial robustness bounds.

### Open Question 2
- Question: Can smaller networks be engineered to achieve accuracy and robustness comparable to over-parameterized networks?
- Basis in paper: [explicit] Section 5 suggests investigating if it is possible to obtain smaller networks with comparable performance "to avoid computational burden."
- Why unresolved: Over-parameterization offers robustness benefits but incurs high computational costs; it is unknown if these benefits can be distilled into efficient architectures.
- What evidence would resolve it: Experiments demonstrating that pruning or distillation techniques can maintain robustness levels while significantly reducing parameter count.

### Open Question 3
- Question: How does over-parameterization impact a model's vulnerability to privacy attacks?
- Basis in paper: [explicit] Section 5 lists "evaluating over-parameterized models against privacy attacks" as a future direction.
- Why unresolved: The study focuses exclusively on evasion attacks (integrity violations) and does not address confidentiality or privacy threats.
- What evidence would resolve it: An evaluation of over-parameterized models against privacy attacks, such as membership inference or model inversion.

## Limitations
- Results are confined to MNIST and CIFAR10 datasets, which may not generalize to more complex or real-world scenarios
- The study focuses on width-scaling rather than depth-scaling, leaving questions about architectural dimensions unanswered
- Theoretical mechanisms underlying the observed robustness improvements remain unexplained

## Confidence
- **High confidence**: Empirical observation that over-parameterized networks show lower error rates under adversarial attacks
- **Medium confidence**: The reliability of attack evaluation through IoAF and AutoAttack verification
- **Medium confidence**: The claim that robustness improves beyond the interpolation point, based on the tested parameter ranges

## Next Checks
1. Replicate experiments on additional datasets (e.g., SVHN, CIFAR100) to test generalizability
2. Investigate depth-scaling effects by comparing shallow vs. deep architectures at similar parameter counts
3. Conduct ablation studies varying optimization hyperparameters (learning rate schedules, batch sizes) to assess robustness stability