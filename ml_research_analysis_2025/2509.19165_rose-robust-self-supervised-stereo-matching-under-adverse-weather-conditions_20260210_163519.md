---
ver: rpa2
title: 'RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions'
arxiv_id: '2509.19165'
source_url: https://arxiv.org/abs/2509.19165
tags:
- stereo
- weather
- adverse
- feature
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of self-supervised stereo matching
  under adverse weather conditions such as night, rain, and fog, where standard methods
  fail due to degraded feature extraction and ineffective photometric consistency
  supervision. The authors propose RoSe, which integrates robust priors from Vision
  Foundation Models (VFMs) into CNN-based Feature Pyramid Networks (FPNs) to enhance
  feature representation under challenging conditions.
---

# RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions

## Quick Facts
- arXiv ID: 2509.19165
- Source URL: https://arxiv.org/abs/2509.19165
- Reference count: 40
- The method integrates Vision Foundation Models into CNN-based FPNs with an Anti-Adverse Feature Enhancement Module to significantly improve self-supervised stereo matching under adverse weather conditions.

## Executive Summary
RoSe addresses the challenge of self-supervised stereo matching under adverse weather conditions (night, rain, fog) where standard photometric consistency methods fail due to degraded feature extraction and ineffective supervision. The authors propose a hybrid approach that integrates Vision Foundation Models (VFMs) with CNN-based Feature Pyramid Networks (FPNs), enhanced by an Anti-Adverse Feature Enhancement Module (AFEM) that suppresses weather-related degradation using frequency domain filtering. The method employs a two-step training pipeline: first establishing scene correspondence consistency between synthetic clear and adverse pairs, then using distillation with pseudo-labels from the clear-weather model. Evaluated on synthetic datasets with various adverse conditions, RoSe significantly outperforms state-of-the-art self-supervised methods.

## Method Summary
RoSe uses a hybrid feature extractor combining Vision Foundation Models (VFMs like DAMv2 or SAM) with CNN-based FPNs, integrated with an Anti-Adverse Feature Enhancement Module (AFEM) that suppresses weather degradation through frequency domain filtering. The training follows a two-step pipeline: Step 1 trains on synthetic clear/adverse pairs using feature and disparity consistency losses; Step 2 uses distillation, where the Step 1 model generates pseudo-labels for clear images that guide training on mixed data. The method relies on synthetic adverse datasets generated from clear pairs using CycleGAN-Turbo, maintaining geometric correspondence while introducing weather effects.

## Key Results
- Achieves 35.7% reduction in EPE and 33.4% reduction in Bad 3.0 error rate compared to previous self-supervised methods on DrivingStereo under adverse conditions
- Maintains 90% of clean-weather performance under adverse conditions while other methods drop significantly
- Outperforms all self-supervised baselines by notable margins on both synthetic and real-world benchmarks (KITTI)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating Vision Foundation Models (VFMs) into a CNN-based Feature Pyramid Network (FPN) creates robust, degradation-invariant feature representations.
- **Mechanism:** VFMs provide high-level semantic priors robust to domain shifts. These are fused with FPN features to recover spatial details. AFEM suppresses weather-specific "style" using frequency domain filtering (FFT) and normalization (IN/BN), preserving content.
- **Core assumption:** VFM features contain semantic information that is conserved across weather domains, while weather degradation is primarily a style or frequency-specific artifact separable from content.
- **Evidence anchors:** [abstract] "injecting robust priors derived from the visual foundation model into the CNN-based feature extractor to improve feature representation..." [section III-B] "AFEM operates across the spatial, channel, and frequency domains, effectively disentangling degradation-related noise..."
- **Break condition:** The mechanism likely fails if the weather degradation is so severe it destroys the semantic structure the VFM relies on (e.g., whiteout conditions), or if the frequency overlap between "content" and "degradation" is high.

### Mechanism 2
- **Claim:** Scene correspondence consistency between synthetic adverse and clear pairs forces the model to learn a unified latent space insensitive to weather.
- **Mechanism:** The method uses synthetic pairs (Clear, Adverse) with identical geometry. It minimizes a Feature Consistency Loss ($L_{fc}$) and Disparity Consistency Loss ($L_{dc}$). This constrains the network to produce identical feature embeddings and disparity outputs for the same scene regardless of weather artifacts.
- **Core assumption:** The synthetic adverse images generated by CycleGAN-Turbo maintain precise geometric alignment with the clear images, and the "domain gap" is purely visual, not geometric.
- **Evidence anchors:** [abstract] "create synthetic stereo datasets... preserving the scene correspondence property... align underlying scene results from clean and adverse image pairs." [section III-C] "This process aligns the learned features from both clear and adverse pairs in the latent space."
- **Break condition:** Breaks if the image-to-image translation model introduces geometric distortions (e.g., warping objects), causing the consistency loss to penalize correct geometric features that appear shifted in synthetic data.

### Mechanism 3
- **Claim:** Distillation using "clear-weather" pseudo-labels decouples training from the photometric consistency assumption, which fails in adverse conditions.
- **Mechanism:** Step 2 uses the model trained in Step 1 as a frozen "Teacher" to generate high-quality pseudo-disparity maps for clear images. The "Student" is trained on mixed/adverse data to match these outputs via $L_{kd}$. This bypasses the need to compute photometric loss on noisy adverse images.
- **Core assumption:** The Teacher model (from Step 1) is sufficiently robust to provide accurate ground truth for the student, and the "clear" predictions are transferable to "adverse" visual domains.
- **Evidence anchors:** [abstract] "adverse weather distillation... align underlying scene results... improving model disparity estimation." [section III-D] "We enhance our RoSe... by using high-quality pseudo-supervised loss to replace the photometric consistency loss."
- **Break condition:** If the student encounters a weather pattern distinct from the synthetic data used in Step 1, or if the Teacher's "clear" domain features do not overlap with the Student's "adverse" input features.

## Foundational Learning

- **Concept: Photometric Consistency (Self-Supervised Stereo)**
  - **Why needed here:** This is the baseline mechanism the paper argues *against* using in adverse weather. Understanding that standard methods warp the right view to match the left (using pixel intensity) explains why rain/fog breaks the supervision signal.
  - **Quick check question:** If you reconstruct a left image using a warped right image during a rainstorm, will the SSIM loss be high or low, and why does that matter for training?

- **Concept: Feature Pyramid Networks (FPN) vs. Vision Transformers (ViT)**
  - **Why needed here:** The architecture fuses these two. You must understand that FPNs are good at local, multi-scale detail (texture), while ViTs (VFMs) are good at global context (semantics), and RoSe uses AFEM to force them to play nice together.
  - **Quick check question:** Why does a standard CNN struggle with "textureless" or "reflective" regions compared to a Vision Foundation Model?

- **Concept: Knowledge Distillation**
  - **Why needed here:** Step 2 of the pipeline relies entirely on this. You need to distinguish between the "Teacher" (frozen, robust) and "Student" (learning, adaptable) to implement the training loop correctly.
  - **Quick check question:** In the distillation step, why must the Teacher weights be frozen, and what data should the Teacher see vs. the Student?

## Architecture Onboarding

- **Component map:** Feature Extractor (ViT + CNN FPN) -> Anti-Adverse Feature Enhancement Module (AFEM) -> RAFT-Stereo Cost Aggregation (CVA + GRU) -> Disparity Refinement
- **Critical path:**
  1. **Data Prep:** Generate synthetic adverse pairs (clear â†’ rain/fog/night) using CycleGAN-Turbo
  2. **Step 1 (Correspondence):** Train dual-branch network (Clear + Adverse) enforcing consistency. *Output: Teacher weights*
  3. **Step 2 (Distillation):** Freeze Teacher. Train Student on mixed data using Teacher's predictions as GT
- **Design tradeoffs:**
  - **Synthetic Data Quality:** The paper acknowledges reliance on CycleGAN-Turbo. Imperfect translations (geometric warping) act as noise. *Tradeoff:* Robustness to weather vs. potential overfitting to translation artifacts
  - **VFM Capacity:** Using ViT-Large vs. ViT-Base. *Tradeoff:* Accuracy vs. Latency (Table VIII shows Large is better but slower)
- **Failure signatures:**
  - **Training Collapse (Step 1):** If $L_{fc}$ and $L_{dc}$ are used without $L_{photo}$, features collapse to zero. The paper notes $L_{photo}$ is still needed as a weak anchor
  - **Artifacts in Sky/Textureless areas:** While better than baselines, the paper notes supervised methods still sometimes handle artifacts better (Table V/Fig 5 context)
  - **Severe Degradation:** Fig 11 shows failure cases where severe visual loss leads to unreliable disparity
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the feature extractor with *only* VFM vs. *only* FPN on a small adverse subset to verify the performance gap (Table VII, rows 2-3)
  2. **Module Validation:** Activate AFEM and observe the drop in EPE (End-Point Error) specifically on the "Rainy" validation set (Table VII, row 6)
  3. **Pipeline Validation:** Compare Step 1 output (Self-Supervised) vs. Step 2 output (Distilled) to verify that distillation actually tightens the error margins on the KITTI benchmark

## Open Questions the Paper Calls Out

- **Question:** How can self-supervised stereo matching frameworks be stabilized to prevent failure under severe weather degradation where visibility is critically compromised?
  - **Basis in paper:** [explicit] The authors explicitly identify a limitation in Section V, stating, "Under severe image degradation, RoSe may produce unreliable disparity estimates."
  - **Why unresolved:** The current method relies on feature extractors and photometric assumptions that break down when visual information is largely obscured by extreme weather.
  - **What evidence would resolve it:** Evaluation results on a dataset featuring extreme visibility conditions (e.g., heavy blizzards or dense fog) showing maintained disparity accuracy.

- **Question:** Can the reliance on synthetic adverse weather pairs be reduced to avoid introducing noise from imperfect image-to-image translation models?
  - **Basis in paper:** [explicit] The Limitations section states, "RoSe relies heavily on the realism and consistency of synthetic adverse weather pairs... Inaccuracies... may introduce noise into the supervisory signals."
  - **Why unresolved:** The framework currently uses CycleGAN-Turbo to generate training data, and geometric distortions in these synthetic images can degrade the quality of the self-supervision.
  - **What evidence would resolve it:** A method performing comparably using strictly real-world unpaired adverse data, or a technique that filters translation artifacts before the consistency loss is applied.

## Limitations
- Reliance on synthetic data generation via CycleGAN-Turbo introduces potential geometric distortions that could affect the consistency loss mechanism
- Computational overhead of hybrid VFM+FPN architecture (particularly with ViT-Large) may limit practical deployment on resource-constrained systems
- Real-world validation on actual adverse weather data remains limited, with current evaluation focused on synthetic datasets

## Confidence
- **High Confidence:** The general framework of using VFMs for robust feature extraction under adverse conditions, supported by ablation studies showing performance degradation when removed (Table VII)
- **Medium Confidence:** The effectiveness of the two-step training pipeline, particularly the distillation phase, as the paper demonstrates improvements but doesn't extensively validate against alternative training strategies
- **Medium Confidence:** The specific frequency domain filtering approach in AFEM, as the paper shows quantitative improvements but limited qualitative analysis of what exactly is being suppressed

## Next Checks
1. **Real-World Validation:** Test RoSe on actual adverse weather datasets (e.g., RainCityscapes, fog datasets) rather than relying solely on synthetic data to verify generalization
2. **Geometric Distortion Analysis:** Conduct experiments measuring geometric alignment quality between synthetic adverse and clear pairs to quantify potential translation artifacts affecting consistency loss
3. **Ablation of VFM Capacity:** Compare ViT-Base vs. ViT-Large performance across the full pipeline to determine if the computational overhead of larger models is justified by the accuracy gains