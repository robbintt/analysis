---
ver: rpa2
title: 'Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients
  by f-SoftArgmax Parameterization with Coupled Regularization'
arxiv_id: '2601.12604'
source_url: https://arxiv.org/abs/2601.12604
tags:
- policy
- lemma
- regularization
- convergence
- parameterization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel family of policy parameterizations
  based on f-softargmax operators, which generalizes the standard softmax parameterization
  used in policy gradient methods. By coupling these parameterizations with their
  corresponding f-divergence regularizers, the authors demonstrate that the resulting
  regularized objective satisfies a Polyak-Lojasiewicz inequality, enabling improved
  convergence guarantees for stochastic policy gradient methods.
---

# Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization

## Quick Facts
- arXiv ID: 2601.12604
- Source URL: https://arxiv.org/abs/2601.12604
- Reference count: 40
- This paper introduces f-softargmax policy parameterizations that achieve polynomial sample complexity versus exponential for softmax, with Tsallis PPO outperforming PPO in noisy exploration settings.

## Executive Summary
This paper addresses the fundamental limitation of softmax policy parameterization in reinforcement learning: the exponentially slow convergence caused by ill-conditioned optimization landscapes near deterministic policies. The authors propose a novel family of f-softargmax parameterizations coupled with their corresponding f-divergence regularizers, demonstrating that this combination satisfies a Polyak-Łojasiewicz inequality enabling improved convergence guarantees. For Tsallis divergence generators with α ∈ (0,1), the method achieves polynomial sample complexity in contrast to the exponential complexity of standard softmax, with empirical validation showing better performance in exploration-heavy and noisy environments.

## Method Summary
The method replaces standard softmax parameterization with f-softargmax operators derived from f-divergence generators, coupling this with a regularizer induced by the same f-divergence. The f-softargmax operator produces policy weights that depend on the curvature of the divergence generator, avoiding the flat regions associated with near-deterministic policies. By maintaining this coupling, the resulting regularized objective satisfies a uniform Polyak-Łojasiewicz inequality on a well-defined parameter region, enabling non-asymptotic last-iterate convergence guarantees. The approach is implemented through f-PG for tabular settings and extended to deep RL via α-Tsallis PPO, where the policy network outputs use Tsallis-softargmax and rewards are regularized with Tsallis divergence.

## Key Results
- Tsallis divergence generators with α ∈ (0,1) achieve polynomial sample complexity versus exponential for standard softmax-KL coupling
- α-Tsallis PPO outperforms standard PPO in Noisy CartPole environments with varying reward noise levels
- The coupled parameterization-regularization setup satisfies a uniform Polyak-Łojasiewicz inequality enabling improved convergence guarantees
- Landscape visualization confirms Tsallis-Tsallis coupling removes flat regions while softmax-entropy retains them

## Why This Works (Mechanism)

### Mechanism 1
Replacing softmax with f-softargmax parameterization fundamentally improves the conditioning of the policy gradient optimization landscape by avoiding flat regions associated with near-deterministic policies. The f-softargmax operator produces policy weights that depend on the curvature (second derivative) of the divergence generator f, creating a parameterization where gradient information remains informative even as policies approach deterministic behavior, unlike softmax where gradients can vanish exponentially. Core assumption: The divergence generator f satisfies regularity conditions including strict convexity, bounded derivatives, and lim(u↓0+) f'(u) = -∞ (Assumption Af), ensuring the parameterization is well-behaved across the simplex.

### Mechanism 2
Coupling the parameterization with a regularizer induced by the same f-divergence creates an objective that satisfies a uniform Polyak-Łojasiewicz inequality on a well-defined region, enabling non-asymptotic last-iterate convergence guarantees without preconditioning. The coupled setup ensures that the optimal f-regularized policy can be represented exactly by the f-softargmax parameterization (θ* = q*/λ). This alignment allows the suboptimality gap to be bounded by the gradient norm through a non-uniform Łojasiewicz inequality, which becomes uniform when restricted to parameters encoding "non-degenerate" policies via a projection operator. Core assumption: Both the reference policy πref has full support (Assumption P) and the initial state distribution has non-zero mass everywhere (Assumption Aρ), ensuring exploration of all states during optimization.

### Mechanism 3
For α-Tsallis divergence generators with α ∈ (0,1), the coupled approach achieves polynomial sample complexity for the unregularized objective, in contrast to the exponential complexity inherent to standard softmax-entropy parameterization. The second derivative of the convex conjugate (f*)'' grows polynomially for Tsallis generators, yielding a Polyak-Łojasiewicz constant with polynomial dependence on problem parameters. This translates to polynomial sample complexity bounds through the explicit rates derived in the convergence analysis, whereas KL divergence's (f*)'' grows exponentially, leading to exponential complexity. Core assumption: The target precision ε is sufficiently small relative to problem parameters and the regularization temperature λ is appropriately tuned as λ = Θ((1-γ)ε/α^2) to balance regularization bias against convergence rate.

## Foundational Learning

- **Policy gradient methods in tabular reinforcement learning**: The entire framework builds on standard policy gradient algorithms, extending them through alternative parameterizations. Understanding how REINFORCE-style updates work with softmax is essential to appreciate why alternative parameterizations matter. *Quick check*: Can you derive the gradient of the expected return with respect to policy parameters using the log-derivative trick, and explain why this leads to high variance?

- **f-divergences and their role in optimization**: The paper's core innovation uses f-divergence generators to define both parameterizations and regularizers. Understanding that KL divergence is just one member of this family (with f(u) = u log u) is crucial for seeing the generalization. *Quick check*: For the Tsallis divergence generator f(u) = (u^α - αu + α - 1)/(α(α-1)), what are its first and second derivatives, and how do they differ from those of the KL generator?

- **Polyak-Łojasiewicz inequality and its implications for convergence**: The main theoretical contribution shows the coupled objective satisfies this inequality, which guarantees linear convergence rates. Understanding that PL condition provides a "gradient domination" property bridging local information to global optimality is key. *Quick check*: If a function satisfies ||∇f(x)||² ≥ μ(f* - f(x)) for all x, what does this imply about the convergence of gradient ascent with step size η ≤ 1/L, and how does μ affect the convergence rate?

## Architecture Onboarding

- **Component map**: Parameterization layer (f-softargmax via root-finding) -> Regularized objective (adds λ·Df(π||πref) to rewards) -> Projection operator Tτ (ensures min probability threshold) -> Gradient estimator (REINFORCE-style with ∂log π/∂θ)

- **Critical path**: Implementing f-softargmax efficiently is the bottleneck. For Tsallis, this requires solving for μ in Σ πref(a)·[1 + (α-1)(x(a)-μ)]^(1/(α-1)) = 1 via bisection (O(|A|) per state). The projection operator adds negligible overhead as it only adjusts extreme weights.

- **Design tradeoffs**:
  - α selection: Lower α increases regularization strength and exploration but may slow convergence to deterministic optimal policies; α ≈ 0.5-0.7 works well empirically across environments
  - Temperature λ: Must scale with target precision; too large introduces bias, too small may cause numerical issues with the root-finding
  - Reference policy πref: Uniform over actions is simplest; non-uniform choices can encode prior knowledge but complicate analysis

- **Failure signatures**:
  1. Degenerate policies: If λ is too small or τλ projection disabled, policies may collapse to near-deterministic with vanishing gradients (recovering softmax's exponential slowdown)
  2. Numerical overflow: For Tsallis with α close to 1, the exponent 1/(α-1) becomes large; use log-space arithmetic for stability
  3. Inconsistent coupling: Using different generators for parameterization and regularization breaks the theoretical guarantees and may produce worse conditioning than standard softmax-entropy

- **First 3 experiments**:
  1. Landscape visualization: Replicate Figure 1 for a 2-action bandit, comparing softmax-entropy vs. Tsallis-Tsallis landscapes; confirm Tsallis coupling removes flat regions while softmax-entropy retains them
  2. Tabular ablation: Implement f-PG on NChain (size 15-20) and DeepSea (size 20-30), sweeping α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and λ ∈ {0.01, 0.1, 1.0}; expect α < 1 to outperform softmax-entropy baseline
  3. Scale test with α-Tsallis PPO: Modify PPO implementation to use Tsallis-softargmax policy head and Tsallis regularization bonus, evaluate on Noisy CartPole with increasing reward noise (σ² = 0.5, 2.0, 10.0)

## Open Questions the Paper Calls Out

- **Can the convergence guarantees and polynomial sample complexity of f-PG be extended to the adversarial MDP setting?**: An important direction for future work is to extend these guarantees for adversarial MDPs, where Tsallis regularization has already proven effective in the bandit setting. The current theoretical analysis relies on the stationarity of the MDP transition dynamics and reward functions, which do not hold in adversarial settings where the environment changes arbitrarily.

- **Can the theoretical framework be modified to support Tsallis divergences with α > 1, which induce sparse policies?**: Tsallis divergences with α > 1 violate condition (ii): since f'(0) is finite, the induced policies are sparse... We leave the extension to this setting for future work. The current analysis relies on Assumption Af, which requires lim_{u ↓ 0+} f'(u) = -∞ to ensure the policy is smooth and non-sparse at the boundary.

- **Do the global last-iterate convergence guarantees and Polyak-Łojasiewicz (PL) inequality persist under function approximation (e.g., deep neural networks)?**: The Abstract states the guarantees are for "stochastic policy gradient methods for finite MDPs," and Section 1 specifies the analysis is established "in the tabular setting." The experiments apply the method to deep RL (PPO), but the theory is restricted to the tabular case.

- **Can the choice of the generator f (or the parameter α) be adapted online based on the environment structure to achieve uniformly optimal performance?**: The experiments show that "no single choice of α is uniformly optimal," with different tasks favoring different couplings. The Conclusion suggests "the need for a tunable parameterization-regularization pair."

## Limitations

- The analysis critically depends on Assumption Af holding for the divergence generator f, which excludes important cases like KL divergence for achieving polynomial complexity
- The empirical validation focuses on relatively small-scale problems (tabular NChain/DeepSea and a single continuous control task), leaving questions about scalability
- The coupling mechanism requires careful tuning of both α and λ parameters, with the theoretical analysis suggesting α ≈ 11/(2 log(1/ε)) for optimal polynomial bounds

## Confidence

- **High confidence**: The theoretical framework is internally consistent - the derivation of the PL inequality for coupled f-softargmax and f-regularization follows standard techniques, and the exponential vs polynomial complexity separation is mathematically rigorous given the assumptions. The landscape visualization in Figure 1 provides clear empirical evidence that softmax-entropy parameterization creates flat regions that Tsallis-Tsallis coupling resolves.

- **Medium confidence**: The practical performance gains demonstrated on Noisy CartPole are promising but limited to one deep RL environment. The tabular experiments show improved sample efficiency, but the effect size and generalizability across different MDP structures remains to be established.

- **Low confidence**: The optimal parameter selection guidance (α ≈ 11/(2 log(1/ε))) is derived from asymptotic analysis and may not translate well to finite-horizon or finite-sample regimes. The paper doesn't address how the method performs when the reference policy π_ref is non-uniform or when state-action spaces are continuous rather than discrete.

## Next Checks

1. **Scale-up test**: Evaluate α-Tsallis PPO on high-dimensional continuous control tasks (HalfCheetah, Walker2d) with varying reward noise levels to verify the robustness claims extend beyond CartPole and assess computational overhead of the f-softargmax operator.

2. **Parameter sensitivity analysis**: Systematically sweep α and λ across multiple environment families (bandits, gridworlds, control tasks) to characterize the stability of the coupling mechanism and identify regimes where performance degrades or improves unexpectedly.

3. **Convergence trajectory analysis**: Track not just final performance but the entire learning curves to verify the predicted convergence rate improvements - specifically, measure how quickly policies escape near-deterministic regions and whether the PL inequality manifests as linear convergence in practice.