---
ver: rpa2
title: 'TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi'
arxiv_id: '2502.04245'
source_url: https://arxiv.org/abs/2502.04245
tags:
- entity
- languages
- named
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TriNER, a multilingual Named Entity Recognition
  (NER) model for Hindi, Bengali, and Marathi. The authors combine existing datasets
  (HiNER, MahaNER, B-NER) into a unified corpus with 6 entity classes and fine-tune
  multilingual transformer models (XLM-RoBERTa, mBERT, MuRIL).
---

# TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi

## Quick Facts
- **arXiv ID**: 2502.04245
- **Source URL**: https://arxiv.org/abs/2502.04245
- **Reference count**: 31
- **Primary result**: Fine-tuned XLM-RoBERTa achieves state-of-the-art NER F1 score of 92.11 on Hindi, Bengali, and Marathi

## Executive Summary
This paper introduces TriNER, a multilingual Named Entity Recognition (NER) model for Hindi, Bengali, and Marathi. The authors create a unified corpus by combining HiNER, MahaNER, and B-NER datasets with a standardized 6-class tag schema (PERSON, LOCATION, ORGANIZATION, NUMEX, TIMEX, MISC) and fine-tune multilingual transformer models (XLM-RoBERTa, mBERT, MuRIL). The XLM-R model achieves an F1 score of 92.11, demonstrating that training on a combined multilingual corpus enhances NER performance and reduces inconsistencies in entity groups and tag naming conventions across these Indian regional languages.

## Method Summary
The authors combine three existing NER datasets (HiNER for Hindi, MahaNER for Marathi, and B-NER for Bengali) into a unified corpus with 6 entity classes using the IOB2 tagging scheme. They map disparate tag sets from each dataset to this common schema, automatically annotate NUMEX tags for B-NER using a float-conversion script, and create training/validation splits. The unified corpus is then used to fine-tune three multilingual transformer models: XLM-RoBERTa, mBERT, and MuRIL, with the XLM-R model achieving the best performance at F1 92.11 using hyperparameters of learning rate 2e-5, batch size 16, and 5 epochs.

## Key Results
- Fine-tuned XLM-RoBERTa achieves state-of-the-art F1 score of 92.11 on the multilingual NER task
- Multilingual training on combined corpus outperforms monolingual approaches for most entity classes
- The unified 6-class taxonomy successfully reduces inconsistencies in entity groups and tag names across the three languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unified multilingual training across Hindi, Bengali, and Marathi improves NER performance compared to monolingual models.
- **Mechanism**: Combining datasets exposes the model to a broader range of linguistic patterns and entity contexts. If the languages share structural similarities or loanwords, the model can leverage cross-lingual transfer, improving generalization, particularly for lower-resource classes.
- **Core assumption**: Hindi, Bengali, and Marathi share sufficient linguistic features or entity contexts for cross-lingual transfer to be beneficial.
- **Evidence anchors**:
  - [abstract]: "This paper details our work to build a multilingual NER model... achieving an F1 Score of 92.11... significantly reduce the inconsistencies..."
  - [section 4.2]: "training a model on a combined multilingual corpus enhances NER performance for most, especially low resource languages."
  - [corpus]: Neighbor paper "Tokenization Matters: Improving Zero-Shot NER for Indic Languages" supports the focus on multilingual Indic NLP, but TriNER's specific cross-lingual transfer mechanism is only claimed, not deeply analyzed in the provided text.
- **Break condition**: If the languages have mutually exclusive entity representations or tagging conventions that cannot be normalized, combining datasets introduces noise and degrades performance.

### Mechanism 2
- **Claim**: Fine-tuning large multilingual transformers (specifically XLM-RoBERTa) provides a superior initialization for NER in these languages compared to training a custom transformer from scratch.
- **Mechanism**: Pre-trained models like XLM-R have already learned rich, contextual representations of these languages from massive corpora. Fine-tuning adapts these general representations to the specific task of NER, a form of transfer learning that requires less task-specific data to achieve high performance.
- **Core assumption**: The pre-training data and objectives of XLM-RoBERTa are sufficiently aligned with the NER task for these specific Indian languages.
- **Evidence anchors**:
  - [abstract]: "fine-tune multilingual transformer models (XLM-RoBERTa, mBERT, MuRIL). The fine-tuned XLM-R model achieves state-of-the-art performance..."
  - [section 4.2, Table 2]: Shows XLM-R Fine-Tuned (F1 92.11) vastly outperforming the Custom Decoder Only Model (F1 79.27).
  - [corpus]: "Positional Attention for Efficient BERT-Based Named Entity Recognition" confirms BERT-based fine-tuning is a dominant paradigm for NER.
- **Break condition**: If the pre-trained model's tokenizer handles these languages poorly (e.g., excessive fragmentation of words), it could create a bottleneck that limits performance gains from fine-tuning.

### Mechanism 3
- **Claim**: Standardizing entity taxonomies to 6 classes and mapping disparate datasets to this schema reduces training noise and improves model consistency.
- **Mechanism**: Disparate tag sets (e.g., NEP, NEL, NEO vs. PER, LOC, ORG) are mapped to a common schema (PERSON, LOCATION, etc.). This unification simplifies the learning objective, preventing the model from having to learn different labels for the same underlying concept and increasing the effective training examples per unified class.
- **Core assumption**: The mapping from source tags to the 6 unified classes is accurate and does not introduce significant label noise.
- **Evidence anchors**:
  - [abstract]: "...significantly reduce the inconsistencies in entity groups and tag names, across the three languages."
  - [section 3.1, Table 1]: Details the specific mapping from source dataset tags to the unified 6 tags.
  - [corpus]: Weak or missing direct corpus evidence on the impact of taxonomy unification; this is an internal mechanism of the TriNER methodology.
- **Break condition**: If the mapping is lossy or erroneous (e.g., mapping a nuanced tag incorrectly to 'MISC'), it introduces label noise that harms model precision for that class.

## Foundational Learning

- **Concept: Named Entity Recognition (NER)**
  - **Why needed here**: This is the core task. You must understand that it is a token classification problem where the goal is to assign a category (e.g., Person, Location) to each word in a sequence.
  - **Quick check question**: Given the sentence "सलमान खान ने मुंबई में फिल्म की शूटिंग की" (Salman Khan shot the film in Mumbai), which tokens should be classified as LOCATION?

- **Concept: Transformer Fine-Tuning**
  - **Why needed here**: The paper's best results come from fine-tuning, not training from scratch. You need to grasp that this means taking a pre-trained model (already knowledgeable about language) and continuing its training on a smaller, task-specific dataset.
  - **Quick check question**: Why would a model pre-trained on a massive multilingual corpus likely perform better than a transformer trained only on the combined HiNER, MahaNER, and B-NER datasets?

- **Concept: Multilingual Cross-Transfer**
  - **Why needed here**: The central hypothesis is that combining languages helps. This relies on the idea that models can transfer learned patterns (e.g., recognizing names) from one language to another.
  - **Quick check question**: If the model learns to recognize locations in Hindi text, how might that knowledge help it recognize locations in Bengali text, even if the words are different?

## Architecture Onboarding

- **Component map**:
  - Unified Data Corpus -> Tokenizer -> Transformer Backbone -> Classification Head -> SeqEval Metric

- **Critical path**:
  1. Data Curation: Align tags from three different datasets into a single schema
  2. Tokenization: Convert text to model-compatible input
  3. Fine-Tuning: Run backpropagation on the unified corpus to adjust weights
  4. Evaluation: Use seqeval to measure entity-level performance

- **Design tradeoffs**:
  - Unified Taxonomy vs. Granularity: The paper reduces tag sets (e.g., dropping literature/religion tags from HiNER into MISC) for consistency, potentially losing fine-grained information
  - Single Multilingual Model vs. Specialized Models: Using one model (TriNER) lowers inference cost and complexity but may sacrifice peak performance on any single language compared to a dedicated model
  - XLM-R vs. MuRIL: The paper notes XLM-R has a slightly higher F1, but MuRIL handles out-of-vocabulary terms better, suggesting a choice between raw benchmark performance and real-world robustness

- **Failure signatures**:
  - Low Recall on Specific Entities: If the model fails to identify entities common in one language but not others, it indicates poor cross-lingual transfer
  - High 'MISC' Confusion: The MISC class aggregates disparate tags; if precision is low, the model may be learning a noisy, undefined pattern
  - Tokenization Artifacts: If entity boundaries consistently align with sub-word tokens incorrectly, it signals a mismatch between the tokenizer and the language structure

- **First 3 experiments**:
  1. Establish a Baseline: Replicate the single best model result by fine-tuning `xlm-roberta-base` on the unified corpus and comparing your F1 score against the reported 92.11
  2. Ablation by Language: Train three separate models, each on only one language's data (Hindi, Bengali, Marathi), and compare their performance on the shared validation set to quantify the benefit of multilingual training
  3. Tag Granularity Analysis: Evaluate the model's performance specifically on the 'MISC' and 'NUMEX' classes (which were automatically annotated or heavily mapped) to understand the impact of the label unification strategy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the TriNER unified training framework be effectively scaled to include additional low-resource Indian languages without suffering from catastrophic forgetting or performance degradation?
- **Basis in paper**: [explicit] The Conclusion explicitly states that "Future work could focus on expanding the model to include additional Indian languages."
- **Why unresolved**: The current study is limited to Hindi, Bengali, and Marathi; it is unknown if the model's architecture and the "common set of 6 entity classes" scale effectively to languages with different morphological structures or smaller datasets.
- **What evidence would resolve it**: Evaluation results (F1 scores) showing the model maintaining high performance (>90 F1) when trained on a corpus including languages such as Tamil, Telugu, or Gujarati.

### Open Question 2
- **Question**: How does increasing the granularity of the taxonomy beyond the unified 6-class schema impact the model's ability to resolve the semantic ambiguities currently absorbed by the "MISC" tag?
- **Basis in paper**: [explicit] The Conclusion identifies "introducing more entity groups for more precise classifications" as a direction for future work.
- **Why unresolved**: The authors currently map diverse tags (e.g., LITERATURE, RELIGION, GPE) into a broad "MISC" or simplified categories to ensure consistency, which may obscure specific entity types.
- **What evidence would resolve it**: A comparative study showing F1 scores for a fine-grained model (e.g., 10+ classes) versus the current coarse-grained model, specifically analyzing the precision drop or gain in the MISC category.

### Open Question 3
- **Question**: To what degree does the heuristic-based automatic annotation of the NUMEX class for the B-NER dataset introduce label noise that limits the model's numerical entity recognition performance?
- **Basis in paper**: [inferred] Section 3.1 notes that for the B-NER dataset, the authors "automatically annotated the dataset using a python script" for NUMEX because the tag was absent, unlike the manually annotated HiNER and MahaNER datasets.
- **Why unresolved**: While Table 3 shows high NUMEX scores (98.72 F1), the use of a simple script (checking float conversion) for training data may fail to capture context-specific numerical entities or differentiate them from other quantities, a limitation not isolated in the current error analysis.
- **What evidence would resolve it**: An ablation study comparing model performance when the B-NER NUMEX subset is excluded versus included, or an error analysis of false positives specifically derived from the B-NER portion of the validation set.

## Limitations
- The core hypothesis of cross-lingual transfer is asserted but not proven due to lack of ablation studies comparing multilingual vs. monolingual models
- Heavy aggregation of diverse tags into the MISC class may reduce model precision for fine-grained entity types
- The automatic NUMEX annotation for B-NER via float-conversion may introduce label noise by missing valid numeric entities or including false positives
- "State-of-the-art" claim lacks comparison to other recent multilingual NER models on the same datasets

## Confidence
- **High Confidence**: The technical implementation of fine-tuning a pre-trained XLM-RoBERTa model on the combined dataset is sound and the reported F1 score of 92.11 is a verifiable result. The methodology for creating a unified corpus and the use of standard evaluation metrics (seqeval, IOB2) are well-established practices.
- **Medium Confidence**: The claim that the unified 6-class schema "significantly reduces inconsistencies" is supported by the design but not empirically validated. The paper does not provide a comparison of model performance before and after the schema unification, nor does it analyze the potential information loss from tag aggregation.
- **Low Confidence**: The core hypothesis that multilingual training provides a *transfer learning benefit* over monolingual training is asserted but not proven. The paper does not include the critical ablation experiments (training separate models per language) needed to isolate the effect of cross-lingual transfer from the effect of increased dataset size.

## Next Checks
1. **Ablation by Language**: Train three separate models, each on only one language's data (Hindi, Bengali, Marathi), using the same amount of training data as allocated to that language in the combined corpus. Compare their performance on the shared validation set to the multilingual model's results. This will directly test whether the improvement comes from cross-lingual transfer or simply from more data.

2. **Tag Granularity Impact**: Evaluate the model's performance specifically on the 'MISC' and 'NUMEX' classes. Analyze whether the high aggregation of diverse tags into 'MISC' is harming precision for any of the original, more specific entity types (e.g., HiNER's literature/religion tags). Consider training a second model with a more granular tag set for a subset of the data to measure the tradeoff between consistency and fine-grained accuracy.

3. **NUMEX Annotation Validation**: Manually inspect a random sample of the B-NER dataset's NUMEX annotations. Verify that the float-conversion heuristic is correctly identifying numeric entities (dates, quantities, identifiers) and not creating false positives. If significant errors are found, implement a more robust annotation rule (e.g., regex patterns for dates, phone numbers) and retrain the model to measure the impact on NUMEX and overall F1 score.