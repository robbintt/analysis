---
ver: rpa2
title: 'Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning
  for LLM with The Cognitive Rotation Space'
arxiv_id: '2509.25743'
source_url: https://arxiv.org/abs/2509.25743
tags:
- unlearning
- rotation
- dataset
- arxiv
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continuous unlearning in large
  language models, where existing methods suffer from cumulative catastrophic utility
  loss and require a retained dataset. The proposed method, Rotation Control Unlearning
  (RCU), transforms LoRA updates into rotational operations in a cognitive rotation
  space, using rotational angles to quantify unlearning degree.
---

# Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space

## Quick Facts
- **arXiv ID:** 2509.25743
- **Source URL:** https://arxiv.org/abs/2509.25743
- **Reference count:** 16
- **Primary result:** Rotation-based continuous unlearning method achieving state-of-the-art performance without retained dataset

## Executive Summary
This paper addresses continuous unlearning for LLMs, where existing methods suffer from catastrophic utility loss and require retained datasets. The proposed Rotation Control Unlearning (RCU) transforms LoRA updates into rotational operations in a cognitive rotation space, using rotation angles to quantify unlearning degree. The method introduces orthogonal rotation axes regularization to minimize interference between consecutive unlearning requests. RCU achieves state-of-the-art performance on multiple datasets without requiring a retained dataset, reducing sample-level and distribution-level unlearning metrics while maintaining model utility with 58% less storage overhead.

## Method Summary
RCU uses LoRA with antisymmetric constraint (LSk) to enforce rotation-like parameter updates, orthogonal rotation axes regularization (Lo) to prevent interference between requests, and cross-entropy loss on unlearning data. An OOD detector trained concurrently outputs rotational salience weight β that controls unlearning intensity at inference. The method operates on LLaMA2-7b with LoRA rank=8 on attention layers, using preference optimization with refusal/random labels for unlearning targets. Hyperparameters are tuned per dataset (ScienceQA: λ1=0.1, λ2=0.1, λ3=1; TOFU: λ1=0.01, λ2=0.5, λ3=1).

## Key Results
- Achieves 58% less storage overhead compared to previous best method
- Significantly reduces sample-level and distribution-level unlearning metrics
- Maintains model utility across multiple continuous unlearning requests
- Eliminates need for retained dataset required by competing methods

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Rotation Space Construction via Skew Symmetric Constraint
- **Claim:** Constraining LoRA updates to approximate rotation matrices enables precise quantification of unlearning degree through rotation angle changes.
- **Mechanism:** The skew symmetric loss L_sk = ||(BA)^T + BA||²_F forces BA to approximate an antisymmetric matrix C. Since R = exp(C) ≈ I + C when C << I, and rotations preserve geometric structure, unlearning becomes controllable through rotation angle θ proportional to ||BA||.
- **Core assumption:** BA remains sufficiently small (BA << I) for first-order Taylor approximation I + BA ≈ exp(BA) to remain valid.
- **Break condition:** If ||BA|| grows beyond approximately 0.1, rotation approximation degrades and unlearning becomes unpredictable.

### Mechanism 2: Orthogonal Rotation Axes Regularization for Interference Minimization
- **Claim:** Enforcing perpendicularity between consecutive unlearning rotations prevents parameter drift in previously learned unlearning requests.
- **Mechanism:** The orthogonal rotation axes loss L_o = ||(W_t - W_{t-1}) · W_{t-1}||²_F ensures each new unlearning rotation operates in a subspace perpendicular to previous rotations. Theorem 2 proves antisymmetric matrices A ⊥ A' correspond to perpendicular rotation axes.
- **Core assumption:** Rotation subspaces can be adequately captured through BA parameter outer product, with perpendicularity at parameter level translating to functional independence.
- **Break condition:** As unlearning requests exceed effective rank capacity of LoRA matrices, finding truly perpendicular subspaces becomes mathematically constrained.

### Mechanism 3: Rotational Salience Weight via OOD Detection and Distributional Compensation
- **Claim:** An OOD detector combined with distributional shift compensator generates scalar weight β that precisely controls rotation (unlearning) application during inference.
- **Mechanism:** OOD detector produces combined score γ_t using Mahalanobis distance and cosine similarity. Distributional shift compensator maps this to rotational salience weight β through piecewise function with dataset-specific thresholds. At inference, effective update is W^t_x = (I + β·BA)W where β scales rotation angle proportionally.
- **Core assumption:** Relationship between OOD score γ_t and optimal rotation angle β follows consistent monotonic mapping captured by piecewise linear/threshold function.
- **Break condition:** If test samples fall outside OOD detector's training distribution, γ_t scores become unreliable, leading to incorrect β values and either over-unlearning (utility loss) or under-unlearning (retention of harmful content).

## Foundational Learning

- **Concept: Lie Groups and Lie Algebra (SO(n) and so(n))**
  - **Why needed here:** Constructs unlearning as rotation operations using relationship between Lie group SO(n) (special orthogonal matrices) and Lie algebra so(n) (antisymmetric matrices). Exponential map exp: so(n) → SO(n) converts constrained parameter updates into valid rotations.
  - **Quick check question:** Given antisymmetric matrix C where C^T = -C, what properties does R = exp(C) have, and why does ||C|| << 1 matter for approximation R ≈ I + C?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Entire RCU method operates on LoRA parameters {A, B} rather than full model weights. Understanding W' = W + BA decomposes high-dimensional update into low-rank factors (B ∈ R^{U×K}, A ∈ R^{K×V}, K << min(U,V)) is essential for grasping how rotational constraints can be efficiently applied.
  - **Quick check question:** If W ∈ R^{4096×4096} and LoRA rank K = 8, how many trainable parameters does LoRA introduce, and why does this enable efficient unlearning?

- **Concept: Out-of-Distribution (OOD) Detection**
  - **Why needed here:** RCU converts unlearning control problem into OOD detection task where unlearned samples are treated as in-distribution for detector. Detector's output scores determine rotational salience weight β that controls unlearning intensity at inference.
  - **Quick check question:** Why would treating unlearned data as "in-distribution" for a detector help identify which inputs require unlearning during inference?

## Architecture Onboarding

**Component map:**
Training Pipeline: Unlearning Request D_U,t → Frozen LLM W → LoRA (A, B) → Combined Loss L_overall = λ₁L_Sk + λ₂L_o + λ₃L_CE → OOD Detector + L_OOD = L_CEL + L_MLM + L_Ua
Inference Pipeline: Input x → OOD Detector → Score γ_t → Compensator → β → (I + β·BA)W → Output

**Critical path:**
1. Initialize LoRA parameters {A, B} with small random values (ensuring BA << I)
2. For each unlearning request t: Compute L_Sk to constrain BA as antisymmetric, compute L_o against previous request's BA product for orthogonality, compute L_CE with refusal/random labels on unlearning data, update only LoRA parameters (frozen backbone W)
3. Train OOD detector concurrently with L_CEL + L_MLM + L_Ua
4. At inference: compute γ_t from detector, map to β, apply scaled rotation

**Design tradeoffs:**
- **λ₁ (skew symmetric weight):** Higher values enforce stricter rotation structure but may slow convergence. Paper uses λ₁=0.1 (ScienceQA) and λ₁=0.01 (TOFU)
- **λ₂ (orthogonal weight):** Critical for continuous unlearning; too low causes cumulative utility loss, too high prevents learning. Paper uses λ₂=0.1 (ScienceQA) and λ₂=0.5 (TOFU)
- **LoRA rank:** Paper uses rank 8. Higher ranks allow more unlearning requests before perpendicular subspace exhaustion but increase storage (paper achieves 16MB vs. 39MB for baseline)
- **Threshold values Γ₁, Γ₂:** Dataset-specific; require calibration on validation data. ScienceQA uses Γ₁=1e-80, Γ₂=0.1; TOFU uses Γ₁=0.2, Γ₂=1.0

**Failure signatures:**
- **D.U. metric increases across requests:** Orthogonal constraint failing; check λ₂ value and verify L_o is being computed correctly against previous iteration's parameters
- **C.QA./O.QA. accuracy drops sharply:** Over-unlearning due to β values too high; verify Γ₂ threshold and check if BA magnitude has grown beyond ~0.1
- **S.U. remains near 100%:** Under-unlearning; skew symmetric constraint may be too strong (λ₁ too high) or learning rate too low
- **Erratic β values across similar inputs:** OOD detector not properly aligned; check L_Ua is being computed and detector is trained on correct unlearning data split

**First 3 experiments:**
1. **Single-request ablation on ScienceQA subset:** Train with only L_CE, then add L_Sk, then add L_o to isolate each component's contribution. Verify BA stays at ~10^-5 magnitude throughout
2. **Continuous unlearning stress test:** Run 5 consecutive unlearning requests on ScienceQA domains (biology→physics→chemistry→economics→earth-science) with and without L_o. Plot S.U., D.U., and utility metrics (C.QA., O.QA.) across requests to visualize cumulative catastrophic utility loss mitigation
3. **β sensitivity analysis:** Fix a trained model and manually sweep β from 0 to 1 on held-out test samples. Plot relationship between β and S.U./D.U. to verify concentrated response range matches Figure 2 (should see sharp transition around β=0.15-0.45 for ScienceQA)

## Open Questions the Paper Calls Out
- How does RCU performance scale with number of continuous unlearning requests beyond the 5 tested in experiments?
- Can the threshold parameters (Γ₁, Γ₂) and mapping function M(γt) be learned adaptively rather than manually tuned per dataset?
- How sensitive is the cognitive rotation space approximation (I + BA ≈ exp(BA)) to violations of BA << I assumption under different learning rates or rank configurations?

## Limitations
- Approximation validity relies on BA << I for first-order Taylor approximation; no analysis of degradation when ||BA|| > 0.1
- Subspace exhaustion may occur when unlearning requests exceed LoRA rank capacity; no analysis of when this limit manifests
- OOD detector reliability depends entirely on ability to distinguish unlearned samples; no robustness analysis for distribution shifts or adversarial inputs

## Confidence
- **High confidence:** Orthogonal rotation axes regularization is well-defined mathematically with clear theoretical grounding in Theorem 2; experimental results showing reduced cumulative utility loss are reproducible
- **Medium confidence:** Cognitive rotation space construction works within stated bounds (BA ~ 10^-5 to 10^-6), but lacks analysis of failure modes when approximations break down; storage overhead claim is directly supported
- **Low confidence:** Distributional shift compensator's piecewise linear mapping from γ_t to β appears dataset-specific without clear generalization principles; threshold values are presented as empirical constants without derivation

## Next Checks
1. **Approximation bound analysis:** Systematically vary learning rates and training epochs to find exact threshold where ||BA|| > 0.1, then measure how S.U., D.U., and utility metrics degrade beyond this point
2. **Subspace capacity test:** Run continuous unlearning beyond 8 requests on ScienceQA with LoRA rank=8, then repeat with rank=16. Plot utility metrics to identify when orthogonal constraint fails and quantify relationship between rank and request capacity
3. **OOD detector stress test:** Create adversarial test sets with controlled distribution shifts (e.g., paraphrasing, synonym replacement, topic drift) and measure β value stability and unlearning effectiveness across these perturbations