---
ver: rpa2
title: 'Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?'
arxiv_id: '2503.18018'
source_url: https://arxiv.org/abs/2503.18018
tags:
- cultural
- reasoning
- mathematical
- gsm8k
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether LLMs maintain mathematical reasoning
  performance when cultural contexts shift. To test this, the authors created six
  culturally adapted datasets from GSM8K, replacing names, food items, and cultural
  references while preserving mathematical logic.
---

# Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?

## Quick Facts
- **arXiv ID**: 2503.18018
- **Source URL**: https://arxiv.org/abs/2503.18018
- **Reference count**: 40
- **Primary result**: LLMs generally perform worse on culturally adapted math problems, with smaller models showing larger accuracy drops

## Executive Summary
This study investigates whether large language models maintain mathematical reasoning performance when cultural contexts shift. The authors created six culturally adapted datasets from GSM8K by replacing Western cultural references (names, food items, places) with those from underrepresented cultures while preserving mathematical logic. Across 14 LLMs, results show consistent accuracy drops on culturally adapted problems, with smaller models exhibiting more pronounced declines. While larger models like Claude 3.5 Sonnet and GPT-4o generalize better, even they show measurable performance reductions, suggesting cultural unfamiliarity impacts mathematical reasoning.

## Method Summary
The researchers adapted 1,198 GSM8K questions using cultural dictionaries for six underrepresented regions (Pakistan, Moldova, Somalia, Haiti, Suriname, Solomon Islands). They identified cultural entities, created symbolic placeholders, built mapping dictionaries via web search, and systematically replaced entities using strict indexing rules. Each of 14 LLMs was evaluated three times per question using identical prompts across all datasets, with strict accuracy (all three responses correct) as the primary metric. Statistical significance was assessed using McNemar's test with 95% Wilson score confidence intervals.

## Key Results
- Models show measurable accuracy drops when cultural contexts shift from Western defaults to underrepresented cultures
- Smaller models (e.g., LLaMA 8B) exhibit greater performance drops compared to larger models (e.g., Claude 3.5 Sonnet)
- Even large models show measurable declines, though they generalize better than smaller models
- Models struggle with numerical reasoning when using less familiar currency units, treating decimal values as whole numbers

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Fidelity and Context Window Efficiency
Models may allocate attention resources inefficiently when cultural adaptation replaces familiar tokens with rare or complex sub-word sequences. The study found token count increases when adapting names (e.g., 104→109 tokens), suggesting different linguistic structures alter processing complexity and potentially degrade reasoning performance.

### Mechanism 2: Semantic Prior Interference (Reasoning via Association)
Models rely on strong semantic priors—associations between cultural entities and their attributes—as heuristic shortcuts. When these associations break (e.g., "0.1 HTG" treated as whole number due to lack of decimal semantic prior), the reasoning chain fails not from math incompetence but from missing cultural context in training data.

### Mechanism 3: Scale-Dependent Contextual Generalization
Larger parameter counts appear to dilute reliance on specific surface-form statistics, allowing better generalization to unseen cultural contexts. Smaller models struggle significantly more than larger models, suggesting reasoning capability scales with parameter count and data diversity, creating more abstract representations less brittle to input shifts.

## Foundational Learning

- **Distribution Shift / Out-of-Distribution (OOD) Generalization**: Essential for understanding how models fail when test data diverges from training data. Quick check: Does the model fail because it can't do math, or because it hasn't seen certain cultural references before?

- **Semantic Priors vs. Formal Logic**: Explains why changing cultural entities breaks reasoning chains. Quick check: If I swap "apples" for "xylophones" in a counting problem, does accuracy drop? If so, the model relies on semantic priors rather than formal logic.

- **Synthetic Data Generation (Perturbation)**: Key technique for isolating specific model capabilities. Quick check: How do you ensure swapping a "name" doesn't accidentally change the difficulty of the math problem?

## Architecture Onboarding

- **Component map**: GSM8K Question + Cultural Dictionary -> Adapter Script -> 14 LLMs -> Evaluator (Strict Accuracy)
- **Critical path**: Dictionary Creation and Mapping Rules are most critical. If replacement is random rather than indexed, logical consistency breaks (e.g., one person becomes two different names in same sentence)
- **Design tradeoffs**: Synthetic vs. organic cultural adaptations (controls math logic but may miss deep nuance); Strict vs. loose accuracy (reduces noise but may penalize 90% consistent models)
- **Failure signatures**: Hallucinated constraints (assumes rare currency must be integer); Entity confusion (interprets cultural family term as singular when it should imply pair)
- **First 3 experiments**:
  1. Token Ablation: Measure accuracy drops on questions where token count increases significantly vs. those where it stays same
  2. Semantic Definition Injection: Rerun failed currency problems with explicit system prompt defining currency properties
  3. Cross-Continental Transfer: Fine-tune small model on one cultural variant and test performance improvement on others

## Open Questions the Paper Calls Out

- To what extent is the performance drop caused by tokenizer inefficiency versus semantic failure to understand cultural entities? The study identifies both factors but provides no ablation study to quantify their relative impact.

- Does the observed fragility in mathematical reasoning generalize to other high-stakes logical domains like legal or medical reasoning? The empirical evaluation is strictly limited to GSM8K mathematical benchmark.

- Can explicit instruction-tuning on cultural entities effectively close the performance gap for smaller models, or is massive parameter scale required? The paper suggests diverse training data is needed but doesn't determine if post-training fine-tuning suffices.

## Limitations

- The study uses synthetic cultural adaptations rather than natively written problems, potentially missing deeper cultural nuances
- Entity replacement process, while controlled, cannot guarantee cultural substitutions don't inadvertently alter problem difficulty
- Evaluation focuses on Western-centric GSM8K as baseline, limiting generalizability to other mathematical reasoning datasets

## Confidence

- **High Confidence**: Core finding of measurable accuracy drops when cultural contexts shift; performance gradients between model sizes; currency decimal handling errors
- **Medium Confidence**: Mechanism explanations (tokenization, semantic priors, scale effects) are plausible but not definitively proven
- **Low Confidence**: Generalizability beyond six tested cultures; extent to which results hold for different mathematical tasks or complex cultural contexts

## Next Checks

1. **Semantic Definition Injection Test**: Re-run failed currency problems with explicit system prompts defining currency properties. If accuracy improves significantly (expected 20-40% increase), validates semantic prior interference mechanism.

2. **Cross-Model Tokenizer Analysis**: Compare tokenization patterns across models for same cultural adaptations. Measure token count variance, subword segmentation differences, and attention pattern shifts to validate or refute tokenization fidelity hypothesis.

3. **Native vs. Synthetic Problem Comparison**: Evaluate models on synthetic adaptations and natively written math problems from target cultures (if available). Larger performance gap on native problems would suggest synthetic approach underestimates true cultural adaptation challenge.