---
ver: rpa2
title: The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning
arxiv_id: '2505.15134'
source_url: https://arxiv.org/abs/2505.15134
tags:
- entropy
- arxiv
- reasoning
- math
- em-inf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study shows that entropy minimization (EM), a technique that
  encourages models to focus on their most confident outputs, can significantly improve
  large language models'' performance on math, physics, and coding tasks without any
  labeled data. Three approaches were explored: unsupervised finetuning by minimizing
  token-level entropy (EM-FT), reinforcement learning with negative entropy as the
  only reward (EM-RL), and inference-time logit adjustment to reduce entropy (EM-INF).'
---

# The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning

## Quick Facts
- arXiv ID: 2505.15134
- Source URL: https://arxiv.org/abs/2505.15134
- Reference count: 40
- Primary result: Entropy minimization significantly improves LLM reasoning on math, physics, and coding tasks without labeled data

## Executive Summary
This paper demonstrates that entropy minimization (EM), a technique that encourages models to focus on their most confident outputs, can significantly improve large language models' performance on reasoning tasks without any labeled data. Three approaches were explored: unsupervised finetuning by minimizing token-level entropy (EM-FT), reinforcement learning with negative entropy as the only reward (EM-RL), and inference-time logit adjustment to reduce entropy (EM-INF). The findings reveal that pretrained LLMs possess underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, with EM-INF achieving 3x efficiency gains over self-consistency while outperforming proprietary models like GPT-4o on challenging benchmarks.

## Method Summary
The paper explores three entropy minimization approaches. EM-FT adds an entropy minimization loss to unsupervised finetuning, encouraging the model to concentrate probability mass on high-confidence tokens. EM-RL uses negative entropy as the sole reward in reinforcement learning, training the model to exploit existing knowledge rather than explore. EM-INF performs logit optimization at inference time, adjusting output logits to reduce entropy without parameter updates. All methods share the core principle of leveraging model confidence as a proxy for correctness, operating under the assumption that capable models are more likely to be correct when confident.

## Key Results
- EM-FT achieved competitive results to supervised methods on math and coding tasks
- EM-RL, trained without labeled data, matched or outperformed strong RL baselines like GRPO and RLOO on most tasks
- EM-INF enabled a 32B parameter model to outperform GPT-4o and Claude 3 Opus on SciCode while being 3x more efficient than self-consistency

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Correctness Correlation via Latent Capability Elicitation
The paper posits that pretrained LLMs have high prior probability for correct reasoning paths. By minimizing entropy, the policy learns to suppress exploration of low-probability (likely incorrect) paths and reinforces the most confident, pre-existing high-probability trajectories. This assumes the base model is already "capable" and its confidence correlates with correctness. Evidence shows EM fails on Llama-3.1-8B for math, supporting the assumption that base capability is a prerequisite. The method effectively amplifies existing capabilities but can degrade performance if the model lacks requisite capability or has misaligned priors.

### Mechanism 2: Inference-Time Decision Boundary Sharpening (EM-INF)
EM-INF treats output logits as free parameters and performs gradient descent on the entropy of the resulting softmax distribution. Crucially, in high-uncertainty settings, this optimization can change the relative order of non-top logits (unlike temperature scaling), potentially steering the model toward a more deterministic reasoning path. The method assumes model uncertainty is "epistemic" rather than "aleatoric" and can be resolved by local optimization. Performance collapses if the entropy threshold δ is set too low, pushing the model toward a greedy, deterministic output that might be confidently wrong.

### Mechanism 3: Exploitation vs. Exploration Trade-off Inversion
Using negative entropy as a standalone reward in RL inverts the standard RL objective to favor exploitation of existing knowledge over exploration. Standard entropy-regularized RL maximizes entropy to encourage exploration, while EM-RL minimizes entropy, forcing the model to commit to specific reasoning chains early. This acts as a filter, retaining only the most robust pretraining-derived logic. The method assumes pretraining data has already exposed the model to necessary reasoning patterns. If a task requires out-of-distribution reasoning not seen during pretraining, EM-RL will fail to discover new solutions.

## Foundational Learning

**Shannon Entropy (Token vs. Trajectory)**
- Why needed here: The paper distinguishes between minimizing entropy at token level (EM-FT/EM-INF) vs. trajectory level (EM-RL-sequence)
- Quick check question: Does minimizing trajectory entropy guarantee low token-level entropy at every step? (Answer: No, it only ensures the sequence probability is high)

**REINFORCE / Policy Gradient**
- Why needed here: EM-RL uses policy gradients to optimize the negative entropy reward
- Quick check question: In EM-RL, what serves as the "reward" signal r(y)? (Answer: The negative sum of token-level entropy)

**Self-Consistency vs. Logit Adjustment**
- Why needed here: The paper positions EM-INF as a more efficient alternative to Self-Consistency
- Quick check question: Why is EM-INF 3x more efficient than Self-Consistency? (Answer: SC requires O(Nn) forward passes for N samples; EM-INF requires only O(n) passes)

## Architecture Onboarding

**Component map:**
1. Base Model: Qwen2.5-Math-7B / Llama-3.1 (Frozen for EM-INF, Trainable for EM-FT/EM-RL)
2. Entropy Estimator: Calculates H(π_θ) (Eqn 1 & 2)
3. Logit Optimizer (EM-INF only): Gradient descent loop on logits z_t to minimize L_EM-INF (Eqn 5)
4. RL Trainer (EM-RL only): PPO/RLOO setup with reward r = -H(π_θ)

**Critical path:**
1. Forward pass generates logits z_t
2. (EM-INF) Optimize z_t to satisfy entropy threshold δ via GD
3. Sample next token y_t
4. (EM-FT/EM-RL) Accumulate entropy loss/reward over full trajectory
5. Backpropagate to update model weights (only for EM-FT/EM-RL)

**Design tradeoffs:**
- EM-FT vs. EM-RL: EM-FT is computationally cheaper but less expressive than EM-RL which can shape distribution via reward baselines
- Threshold δ (EM-INF): Setting δ too low leads to "greedy" collapse; too high provides no benefit. Range 0.1 < δ < 0.5 is cited as optimal

**Failure signatures:**
- Collapse: Output becomes repetitive or empty (solve by increasing δ or adding KL-regularizer to base model)
- Negative Gain on Llama: EM worsens performance on less capable models (verify base model benchmark scores before applying EM)
- Value Misalignment: EM fails on subjective tasks (e.g., IndVal) where confidence does not equal correctness

**First 3 experiments:**
1. EM-INF Ablation: Implement EM-INF on Qwen-7B for math task (e.g., AMC). Compare against greedy decoding and adaptive temperature scaling to verify the "logit order change" benefit
2. Capability Dependence Test: Run EM-FT on Qwen-2.5 (strong base) vs. Llama-3.1 (weaker base) on same math dataset to reproduce "capability prerequisite" finding
3. Compute Efficiency Benchmark: Compare EM-INF (N=1) vs. Self-Consistency (N=4) on SciCode, measuring accuracy vs. FLOPs to validate 3x efficiency claim

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do specific pretraining data mixtures or intrinsic model capabilities dictate whether entropy minimization improves or degrades performance? The paper establishes correlation between base model capability and EM success but does not isolate specific pretraining factors required to make a model receptive to entropy minimization. Evidence would require comparative analysis of models trained with varying ratios of reasoning data, measuring correlation between specific pre-training benchmarks and delta gained from EM-FT/EM-RL.

### Open Question 2
Can entropy minimization be modified to work for tasks where model confidence is a poor proxy for correctness, such as subjective value alignment? The current methodology assumes confidence implies correctness and lacks mechanism to handle domains where model may be confidently wrong or where multiple diverse answers are valid. Evidence would require study introducing diversity-promoting regularizer to EM objective on alignment tasks, testing if this prevents model from collapsing to confident but incorrect pretraining biases.

### Open Question 3
Can entropy minimization serve as an effective regularizer for supervised or verified reinforcement learning rather than just standalone objective? The paper strictly contrasts unsupervised EM against supervised baselines but does not explore if combining EM with labeled data could bridge performance gap on harder datasets. Evidence would require experiments applying EM-RL as auxiliary loss alongside standard reward modeling to see if it stabilizes training or improves sample efficiency.

## Limitations
- Effectiveness critically depends on pretrained model already possessing latent reasoning capabilities
- EM-INF requires careful tuning of entropy threshold δ with no systematic sensitivity analysis provided
- All variants appear vulnerable to distribution shift, potentially amplifying incorrect high-confidence predictions

## Confidence

**High Confidence Claims:**
- EM-INF improves inference efficiency over self-consistency (3x efficiency gain verified through FLOPs analysis)
- EM-FT achieves competitive performance to supervised methods on math/coding tasks
- EM-RL matches or exceeds GRPO/RLOO baselines on most tasks when base capability exists

**Medium Confidence Claims:**
- Entropy minimization reveals "previously underappreciated reasoning capabilities" - supported by ablation studies but lacks mechanistic explanation
- EM-INF logit adjustment can change non-top logit order - theoretically sound but empirical demonstration limited to specific task examples

**Low Confidence Claims:**
- EM-INF outperforms GPT-4o/Claude 3 Opus on SciCode - single benchmark result requiring external validation
- EM is universally applicable across task types - contradicted by IndVal failure case

## Next Checks

1. **Capability Prerequisite Mapping**: Systematically test EM-FT across model capability spectrum (Qwen2.5-1.5B, 7B, 32B vs Llama-3.1-8B, 70B) on identical math datasets to establish precise capability thresholds where EM transitions from beneficial to harmful.

2. **Distribution Shift Robustness**: Evaluate EM-INF on progressively out-of-distribution math problems while tracking confidence calibration metrics to quantify degradation patterns.

3. **Threshold Optimization Study**: Conduct grid search over δ values (0.05 to 1.0) across 5 task families to produce task-specific δ recommendations and identify any universal optimal ranges.