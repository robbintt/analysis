---
ver: rpa2
title: 'Graph-Eq: Discovering Mathematical Equations using Graph Generative Models'
arxiv_id: '2503.23617'
source_url: https://arxiv.org/abs/2503.23617
tags:
- equations
- latent
- equation
- space
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graph-Eq, a deep graph generative model for
  discovering mathematical equations from data. The core innovation is representing
  equations as directed acyclic graphs and using a conditional variational autoencoder
  (CVAE) to learn a continuous latent space of equations.
---

# Graph-Eq: Discovering Mathematical Equations using Graph Generative Models

## Quick Facts
- arXiv ID: 2503.23617
- Source URL: https://arxiv.org/abs/2503.23617
- Reference count: 28
- Primary result: Recovers ground-truth equations in 11 of 20 test datasets, outperforming baseline genetic programming approaches

## Executive Summary
This paper introduces Graph-Eq, a deep graph generative model that discovers mathematical equations from data by representing equations as directed acyclic graphs (DAGs) and learning a continuous latent space through a conditional variational autoencoder (CVAE). Unlike traditional genetic programming methods, Graph-Eq trains on randomly generated equations to learn structured representations that capture both structural and functional similarities between equations. The method uses dataset embeddings as conditions to guide the latent space organization, ensuring equations producing similar outputs are mapped closely together. Bayesian optimization then efficiently explores this learned latent space to find equations that best fit given datasets, achieving a 55% solution rate on 20 test cases compared to 40% for baseline models.

## Method Summary
Graph-Eq represents mathematical equations as DAGs where input features are source nodes, operators are intermediate nodes, and the output is a sink node. A CVAE is trained on a large corpus of randomly generated equations, with a graph neural network encoder mapping DAGs to latent vectors and a GRU-based decoder reconstructing them. Dataset embeddings (generated by polynomial fitting or a pre-trained Set Transformer) condition the CVAE to ensure functional similarity in the latent space. During discovery, Bayesian optimization explores the continuous latent space to find equations minimizing prediction error on new datasets, with SymPy used for equation simplification.

## Key Results
- Recovers ground-truth equation in 11 of 20 test datasets with known solutions
- Outperforms baseline genetic programming approach that recovers 8 of 20 equations
- Achieves 55% solution rate compared to 40% for unconditioned baseline
- Demonstrates that conditioning on dataset embeddings significantly improves discovery performance

## Why This Works (Mechanism)

### Mechanism 1: DAG Representation for Equations
The paper maps equations to DAGs where input features are source nodes, mathematical operators are intermediate nodes, and the output is the sink node. This structure encodes the computational flow of the equation, allowing graph neural networks to process equation semantics directly. The core assumption is that mathematical equations can be fully and unambiguously represented as DAGs while preserving their semantic and computational properties.

### Mechanism 2: Continuous Latent Space Learning with CVAE
The CVAE learns a continuous and structured latent representation of the equation space by training on randomly generated equations. The encoder maps an equation's DAG into a latent vector z, while the decoder learns to reconstruct the DAG from z. A KL divergence loss regularizes this space, making it smooth and continuous. The core assumption is that the space of valid mathematical equations is amenable to being captured by a continuous probability distribution where similar equations are located near each other.

### Mechanism 3: Conditioning for Functional Similarity
Conditioning the CVAE on dataset embeddings ensures the latent space organizes equations by functional output, not just structural similarity. For each training equation, a small dataset of input-output pairs is generated and encoded into a condition vector c, which is fed to both encoder and decoder. This forces the model to place equations producing similar outputs close together in the latent space. The core assumption is that the dataset embedding captures essential functional characteristics and effectively guides latent space organization.

## Foundational Learning

- **Variational Autoencoders (VAEs) and Conditional VAEs**: Understanding the encoder-decoder structure, reparameterization trick, and KL divergence loss is essential since Graph-Eq's core is a CVAE. Why needed: The KL divergence creates the continuous latent space crucial for Bayesian optimization. Quick check: Can you explain why a VAE is better suited for this task than a standard autoencoder, and what the "condition" in a Conditional VAE changes?

- **Bayesian Optimization (BO)**: Understanding how BO builds probabilistic surrogate models and uses acquisition functions to explore the latent space is crucial for the discovery phase. Why needed: BO efficiently searches the continuous latent space without exhaustive sampling. Quick check: Why is Bayesian Optimization particularly useful for black-box functions that are "expensive to evaluate," and how does it balance exploration vs. exploitation?

- **Graph Neural Networks (GNNs) and DAGs**: Understanding how GNNs process graph-structured data is essential for grasping the encoder and decoder mechanics. Why needed: The input and output are graphs (DAGs representing equations), not images or text. Quick check: How does a Graph Neural Network process a Directed Acyclic Graph differently from a standard Recurrent Neural Network processing a linear sequence?

## Architecture Onboarding

- **Component map**: Equation Generator -> DAG Converter -> Dataset Embedder -> CVAE Encoder -> Latent Space -> CVAE Decoder -> Equation Candidate -> Evaluator -> Bayesian Optimizer (feedback loop)

- **Critical path**:
  1. Training: Random Equation -> DAG -> Dataset -> Embedding c -> Graph Encoder -> Latent z -> Graph Decoder -> Reconstructed DAG
  2. Discovery: New Dataset -> Embedding c -> Bayesian Optimizer samples z -> Graph Decoder -> Candidate Equation -> Evaluation -> BO proposes next z

- **Design tradeoffs**: The paper shows a tradeoff between dataset embedding strength and novelty. A simple polynomial embedding may be weak but leads to higher novelty (generalization), while a powerful NeSymRes embedding is more descriptive but can lead to lower novelty due to overfitting to the training distribution.

- **Failure signatures**: Invalid decoded equations (e.g., sin with two inputs) indicate the decoder is failing to learn the grammar of equation DAGs. Low novelty suggests overfitting to training equations. The architecture cannot discover numerical constants, only structural relationships.

- **First 3 experiments**:
  1. Ablation on Conditioning: Compare CVAE with and without dataset conditioning on the same test set, showing improvement from 40% to 55% solution rate.
  2. Latent Space Visualization: Generate 2D heatmap of latent space for a target dataset by decoding and evaluating equations at each point to validate smoothness assumption.
  3. Scalability Test: Train on 20K vs. 120K equations and measure reconstruction accuracy, validity, uniqueness, and novelty to test the need for large training corpora.

## Open Questions the Paper Calls Out

The paper explicitly identifies several limitations and open questions:

1. **Discovery of Numerical Constants**: The current DAG structure cannot represent numerical constants (e.g., 4.5*x), limiting the method to structural relationships only.

2. **Impact of Invalid Equations on Sample Efficiency**: The paper doesn't analyze the computational overhead from evaluating invalid candidates during Bayesian optimization, despite validity rates ranging from 67% to 87%.

3. **Optimizing the Validity-Novelty Tradeoff**: The paper experiments with fixed embedding strategies but doesn't propose methods to optimize the tradeoff between high validity and high novelty in dataset embeddings.

## Limitations

- Cannot discover numerical constants, only structural relationships between variables
- Dependent on quality of dataset embeddings, which may not capture complex functional characteristics
- Requires large training corpus (20K-120K equations) to learn meaningful representations
- Unknown architectural details (latent dimensions, hidden sizes) that could affect reproducibility

## Confidence

- **DAG Representation**: High - well-established approach for equation encoding
- **CVAE Framework**: Medium-High - standard architecture with proven effectiveness for continuous latent spaces
- **Conditioning Mechanism**: Medium - innovative but highly dependent on dataset embedding quality
- **Bayesian Optimization**: High - proven black-box optimization method
- **Overall Method**: Medium - demonstrated effectiveness but with acknowledged limitations and reproducibility challenges

## Next Checks

1. **Ablation Study on Dataset Embedding**: Systematically compare polynomial embedding against NeSymRes embedding across multiple test sets to quantify the tradeoff between functional accuracy and novelty.

2. **Latent Space Smoothness Validation**: Generate 2D heatmap of latent space for a target dataset by decoding and evaluating equations at each point to empirically verify the smoothness assumption required for Bayesian optimization.

3. **Generalization Stress Test**: Evaluate Graph-Eq on equations structurally similar to training corpus versus distinctly different ones (e.g., higher-degree polynomials, multiple trigonometric terms) to assess true discovery capability beyond memorization.