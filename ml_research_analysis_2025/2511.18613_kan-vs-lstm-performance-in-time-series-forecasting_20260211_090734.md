---
ver: rpa2
title: KAN vs LSTM Performance in Time Series Forecasting
arxiv_id: '2511.18613'
source_url: https://arxiv.org/abs/2511.18613
tags:
- lstm
- forecasting
- performance
- networks
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term
  Memory networks (LSTM) for time series forecasting of stock prices, evaluating their
  performance across different prediction horizons. The study demonstrates that LSTM
  models are substantially superior to KAN, achieving 7-10 times better performance
  with test RMSE values as low as 0.039 for 1-day predictions compared to KAN's 0.390.
---

# KAN vs LSTM Performance in Time Series Forecasting

## Quick Facts
- arXiv ID: 2511.18613
- Source URL: https://arxiv.org/abs/2511.18613
- Reference count: 39
- LSTM achieves 7-10x better accuracy than KAN for stock price prediction (RMSE 0.039 vs 0.390)

## Executive Summary
This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for time series forecasting of stock prices. The study demonstrates that LSTM models are substantially superior to KAN, achieving 7-10 times better performance with test RMSE values as low as 0.039 for 1-day predictions compared to KAN's 0.390. While KAN shows computational efficiency advantages (2.1x faster training) and theoretical interpretability through the Kolmogorov-Arnold representation theorem, these benefits are outweighed by LSTM's superior predictive accuracy. The research finds that simpler LSTM configurations (2-layer, 10-unit) outperform deeper networks due to vanishing gradient issues.

## Method Summary
The study evaluates KAN and LSTM architectures on stock price time series forecasting using historical market data from yfinance API. Both models use a 20-day look-back window for next-day prediction. LSTM implementations use TensorFlow with varying depths (2-6 layers), units (10-100), and activation functions (tanh, linear). KAN uses PyKAN library with LBFGS optimizer and B-spline activations varying grid size (3-7) and k-values (2-6). Performance is measured via RMSE across normal, volatile, and trending market conditions.

## Key Results
- LSTM achieves test RMSE as low as 0.039 for 1-day predictions, compared to KAN's 0.390
- KAN trains 2.1x faster than LSTM but with significantly worse accuracy
- 2-layer LSTM with 10 units and tanh activation outperforms deeper networks due to vanishing gradient issues
- KAN maintains some capability for extended horizon forecasting (up to 200 days) where LSTM struggles computationally, but with significantly higher error rates

## Why This Works (Mechanism)

### Mechanism 1: LSTM Gated Memory Preserves Temporal Dependencies
- LSTM's gating mechanisms enable superior sequential pattern retention compared to KAN's static function decomposition approach
- Input, forget, and output gates regulate information flow through the cell state $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$, allowing selective retention of long-term dependencies while filtering noise
- Core assumption: The advantage stems from architectural design for sequential data, not merely hyperparameter tuning
- Break condition: If prediction horizons extend beyond ~100 days or computational resources are severely constrained, LSTM's advantage diminishes

### Mechanism 2: KAN Spline Optimization Struggles with Non-Convex Sequential Loss Landscapes
- KAN's theoretical interpretability comes at a practical cost—free knot polynomial spline optimization is fundamentally ill-suited for non-deterministic time series
- KAN approximates activations via learnable B-splines with grid size and k-value parameters, but optimization landscape becomes "non-smooth and non-convex" for stock data
- Core assumption: The performance gap reflects optimization difficulty rather than insufficient model capacity
- Break condition: Progress in free knot polynomial spline optimization mathematics could narrow the gap

### Mechanism 3: Shallow LSTM Architectures Avoid Gradient Pathologies in Financial Noise
- Counter-intuitively, simpler LSTM configurations outperform deeper ones for noisy financial time series due to gradient flow and overfitting dynamics
- The 2-layer, 10-unit tanh configuration achieved optimal test RMSE (0.0745) while 6-layer models degraded (0.107-0.108)
- Core assumption: Financial time series contain limited exploitable pattern depth; additional layers model noise rather than signal
- Break condition: For cleaner, more deterministic time series, deeper architectures may provide benefits not observed in this financial domain

## Foundational Learning

- **Vanishing/Exploding Gradients in Recurrent Networks**
  - Why needed here: Understanding why 2-layer LSTMs outperform 6-layer configurations requires grasping how repeated multiplication through time steps attenuates or amplifies error signals
  - Quick check question: Given a 20-day look-back window with 6 stacked LSTM layers, approximately how many matrix multiplications does a gradient traverse? (Answer: 120 multiplications)

- **B-Spline Approximation and Knot Placement**
  - Why needed here: KAN's core innovation replaces fixed activations with learnable splines; understanding grid size and k-degree tradeoffs explains why Config 5 (k=2, grid=3) performed optimally
  - Quick check question: If grid size controls "granularity" and k controls "smoothness," which parameter would you increase to capture rapid stock price jumps? (Answer: Increase grid for finer local detail)

- **Kolmogorov-Arnold Representation Theorem vs. Universal Approximation Theorem**
  - Why needed here: The paper contrasts KAN's theoretical basis (function decomposition into univariate compositions) with standard neural networks' basis (layered transformations with fixed activations)
  - Quick check question: Does the Kolmogorov-Arnold theorem guarantee anything about modeling temporal dependencies? (Answer: No—it's a pure function decomposition result for $f: [0,1]^n \to \mathbb{R}$, with no temporal structure in its formulation)

## Architecture Onboarding

- Component map: Input (20-day lookback window) -> LSTM Path -> Stacked LSTM layers (optimal: 2) -> Hidden units per layer (optimal: 10) -> Activation: tanh -> Dense output
- Component map: Input (20-day lookback window) -> KAN Path -> KAN hidden layer(s) -> Grid size (3-7 range tested) -> B-spline degree k (2-6 tested) -> Neurons: len(train_data) // 10 -> LBFGS optimizer -> single output

- Critical path: Start with LSTM. The 2-layer, 10-unit, tanh configuration provides the best accuracy-speed balance. Only explore KAN if interpretability or extreme computational constraints outweigh 7-10x accuracy loss.

- Design tradeoffs:
  - Accuracy vs. Speed: LSTM (~75s training, RMSE 0.039-0.085) vs. KAN (~35s training, RMSE 0.15-0.67)
  - Accuracy vs. Interpretability: LSTM (black-box) vs. KAN (symbolification possible via spline inspection)
  - Short vs. Long Horizon: LSTM dominant 1-100 days; KAN only viable option 200+ days but with high error

- Failure signatures:
  - LSTM test RMSE >> train RMSE: Overfitting—reduce units or add regularization
  - LSTM train RMSE high with deep stacks: Gradient flow issue—reduce layers to 2-3
  - KAN test RMSE varies wildly across market conditions: Spline overfitting—reduce grid size or increase k
  - KAN training diverges: LBFGS sensitivity—reduce learning rate or switch to Adam

- First 3 experiments:
  1. **Baseline LSTM**: 2-layer, 10-unit, tanh activation, 20-day lookback, 1-day prediction horizon. Expected: RMSE ~0.04-0.08.
  2. **LSTM Depth Ablation**: Repeat with 4-layer and 6-layer configurations. Expected: Performance degrades (RMSE 0.08-0.11).
  3. **KAN Efficiency Benchmark**: Grid=3, k=2, neurons=len(train)//10. Expected: RMSE ~0.15-0.39, training ~35s.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advancements in solving non-smooth, non-convex optimization problems for free knot polynomial spline construction significantly improve KAN predictive accuracy?
- Basis in paper: The authors state that the lack of efficient computational methods for this approximation is a main explanation for poor performance and identify it as a specific future research direction
- Why unresolved: Current optimisation techniques struggle with the non-smooth nature of the problem, leading to suboptimal function approximation in standard KAN implementations
- What evidence would resolve it: Development of new KAN optimizers that achieve test RMSE competitive with LSTM (e.g., < 0.05) on non-deterministic financial data

### Open Question 2
- Question: Can specialized KAN architectures, such as Time-Frequency KAN (TFKAN), close the predictive accuracy gap with LSTM models?
- Basis in paper: The authors mention specialized variants like TFKAN as potential improvements for long-term forecasting but note that standard KAN implementations currently lack competitiveness
- Why unresolved: The study focused on standard KAN implementations, leaving the performance of domain-specific variants against LSTMs empirically unverified in this context
- What evidence would resolve it: Comparative benchmarks showing specialized KAN variants reducing the performance gap (currently 7-10x) against LSTMs on identical time series tasks

### Open Question 3
- Question: Can hybrid architectures combining KAN interpretability with LSTM temporal processing outperform standalone models?
- Basis in paper: The conclusion suggests KAN may benefit from "hybrid approaches" combining its strengths with robust predictive capabilities
- Why unresolved: The paper only evaluated standalone models; it did not test whether combining KAN's spline-based interpretability with LSTM's gating mechanisms is technically feasible or beneficial
- What evidence would resolve it: A proposed hybrid KAN-LSTM model demonstrating lower RMSE than standalone LSTM while retaining KAN's symbolic formula extraction capabilities

## Limitations

- Findings are constrained to financial time series domain and may not generalize to other time series applications
- Performance gap could reflect optimization challenges with KAN's spline-based approach rather than fundamental architectural limitations
- Computational efficiency advantage measured only for short horizons, extended horizon performance remains poorly characterized

## Confidence

- **High confidence**: LSTM's superior accuracy (7-10x better RMSE) and optimal 2-layer architecture performance
- **Medium confidence**: KAN's computational efficiency advantage and interpretability claims
- **Low confidence**: KAN's viability for extended horizons (200+ days) due to computational constraints and high error rates

## Next Checks

1. **Cross-domain validation**: Test both architectures on non-financial time series (weather, energy consumption, IoT sensor data) to determine if LSTM's advantage generalizes beyond stock markets

2. **Hybrid architecture exploration**: Implement and evaluate temporal extensions to KAN (TKAN/AR-KAN approaches) to assess whether specialized modifications can close the performance gap while maintaining interpretability

3. **Optimization methodology comparison**: Conduct ablation studies varying optimizers (Adam vs LBFGS), learning rates, and initialization strategies for KAN to determine if performance improvements stem from architectural changes versus optimization parameter tuning