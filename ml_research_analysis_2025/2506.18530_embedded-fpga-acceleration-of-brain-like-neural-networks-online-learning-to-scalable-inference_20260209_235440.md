---
ver: rpa2
title: 'Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning
  to Scalable Inference'
arxiv_id: '2506.18530'
source_url: https://arxiv.org/abs/2506.18530
tags:
- fpga
- kernel
- precision
- learning
- bcpnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first embedded FPGA accelerator for Brain-Like
  Neural Networks (BLNNs), specifically the Bayesian Confidence Propagation Neural
  Network (BCPNN), targeting low-power edge devices. The authors implement both online
  learning and inference-only kernels using High-Level Synthesis on a Zynq UltraScale+
  SoC, supporting variable and mixed precision.
---

# Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference

## Quick Facts
- arXiv ID: 2506.18530
- Source URL: https://arxiv.org/abs/2506.18530
- Reference count: 19
- This paper presents the first embedded FPGA accelerator for Brain-Like Neural Networks (BLNNs), specifically the Bayesian Confidence Propagation Neural Network (BCPNN), targeting low-power edge devices.

## Executive Summary
This paper presents the first embedded FPGA accelerator for Brain-Like Neural Networks (BLNNs), specifically the Bayesian Confidence Propagation Neural Network (BCPNN), targeting low-power edge devices. The authors implement both online learning and inference-only kernels using High-Level Synthesis on a Zynq UltraScale+ SoC, supporting variable and mixed precision. They evaluate their design on MNIST, Pneumonia, and Breast Cancer datasets, achieving up to 17.5× latency improvement and 94% energy savings compared to ARM baselines while maintaining accuracy. The work demonstrates that FP16 precision provides optimal performance for edge deployment, with inference-only kernels being significantly more efficient than full learning kernels.

## Method Summary
The authors implement BCPNN kernels in C++ using Vitis HLS on a Zynq UltraScale+ ZCU104 FPGA, with host code running on ARM Cortex-A53 cores under Ubuntu 22.04. The architecture uses a stream-based dataflow with AXI-Stream FIFOs connecting sub-kernels, enabling parallel execution. Data is fetched from DDR via 256-bit AXI4 bursts and processed through sparse weight lookups, bias computations, and soft-winner-take-all (soft-WTA) mechanisms. The design supports FP32, FP16, and mixed FXP16/FP16 precision, with model parameters (weights, biases, sparse indices) loaded from binary files. RTL co-simulation validates FIFO depths and AXI traffic patterns to prevent deadlocks.

## Key Results
- Up to 17.5× latency improvement and 94% energy savings compared to ARM Cortex-A53 baseline
- FP16 precision achieves optimal performance for edge deployment with minimal accuracy loss
- Inference-only kernels are 2-2.6× faster than full online-learning kernels due to reduced sub-kernel complexity and higher parallelization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stream-based dataflow architecture enables parallel sub-kernel execution and sustained throughput under memory bandwidth constraints.
- Mechanism: Sub-kernels are interconnected via AXI-Stream FIFO channels, allowing independent pipeline stages to operate concurrently. The host initiates kernels via AXI control; kernels fetch data from DDR via 256-bit AXI4 Memory-Mapped bursts (8 float or 16 half-precision values per cycle), converting bursts to streams. Loop-unroll pragmas set internal parallel factors within sub-kernels. This decouples memory access from compute, hiding latency through data-driven execution.
- Core assumption: Sufficient FIFO depth and deadlock-free AXI-Stream traffic are achievable via RTL co-simulation validation; burst widths match kernel parallelism factors.
- Evidence anchors:
  - [abstract] "We implement both online learning and inference-only kernels with support for variable and mixed precision."
  - [Section III-A] "Sub-kernels are interconnected via AXI-stream FIFO channels, enabling dataflow and parallel execution with a data-driven process. Within the sub-kernel, the loop-unroll pragma sets the internal parallel factor."
  - [Section V-A] "Resource usage (LUT, FF, DSP, BRAM) remains nearly constant due to the dataflow architecture maintaining fixed parallelization and FIFO depths."
- Break condition: If FIFO depths are underestimated or AXI-Stream routing causes backpressure deadlock, throughput collapses; also, if burst width exceeds kernel consumption rate, buffers overflow.

### Mechanism 2
- Claim: Reducing numerical precision from FP32 to FP16 increases parallelism factor and reduces energy per inference with minimal accuracy loss on tested datasets.
- Mechanism: FP16 halves data width, enabling 16 values per 256-bit burst vs. 8 for FP32. This doubles the parallelism factor under identical memory bandwidth. FP16's numeric range (~±65,504, min increment 1/1024) is sufficient for BLNN inference weights and activations. Lower precision reduces DSP utilization and on-chip storage, lowering dynamic power. Mixed precision (FXP16 storage + FP16 accumulation) further reduces storage but adds conversion overhead and can degrade accuracy on complex datasets.
- Core assumption: BCPNN inference computations do not require FP32 dynamic range for the evaluated datasets; fixed-point Q3.12 format adequately represents trained parameters for simpler tasks.
- Evidence anchors:
  - [Section III-C] "The FP16 variant doubles the parallel data fetch to sixteen values per cycle. So, it can increase the parallelism factor to 16."
  - [Section V-B] "Both half and mixed precision significantly reduce latency and energy across datasets... Accuracy is stable when precision reduces from float to half for all datasets (e.g., MNIST remains at 94.6%). However, mixed precision introduces minor accuracy loss for more complex datasets (Pneumonia drops to 84.1%, Breast notably to 78.8%)."
  - [corpus] Related work (FeNN, SteganoSNN) suggests FPGAs benefit from low-precision arithmetic for SNN acceleration, but direct evidence for BCPNN-specific precision tolerance is limited to this paper's empirical results.
- Break condition: If model weights or activations exceed FP16 range, or if accumulation overflow occurs in mixed precision without proper FP16 guards, accuracy degrades unacceptably (as observed with Breast dataset in mixed precision).

### Mechanism 3
- Claim: BCPNN's local Hebbian learning and sparse connectivity reduce computational intensity and enable online learning without backpropagation.
- Mechanism: BCPNN uses synaptic traces (z-, p-traces) encoding exponential moving averages of neural activation events. Learning computes biases and weights directly from probability traces via log ratios (Eq. 1-2), avoiding global gradient computation. Sparse connectivity (controlled by nact/nsil) limits active synapses per hidden HCU, reducing multiply-accumulate operations. Structural plasticity rewires connections based on activity, maintaining sparsity while adapting representations. This local, correlation-based learning maps efficiently to parallel hardware with minimal inter-unit communication.
- Core assumption: Local learning rules suffice for the target classification tasks without error backpropagation; sparsity levels preserve representational capacity.
- Evidence anchors:
  - [Section II-A] "BLNNs... rely on sparse connectivity and use correlation-based Hebbian types of learning instead of any form of error backpropagation."
  - [Section II-A] "The Bayesian-Hebbian learning rule of BCPNN computes biases and connection weights directly from these probability traces... wij = log(pij / (pi · pj))"
  - [Section V-C] "Reductions in HCU strongly correlate with latency improvements... aggressive sparsification of connectivity (nact/nsil=80/24) significantly reduces accuracy to 64.1%."
  - [corpus] Word2Spike and related neuromorphic work similarly leverage sparse, local learning, but corpus lacks direct comparative evidence for BCPNN-specific sparsity-accuracy tradeoffs.
- Break condition: If sparsity is too aggressive (nact/nsil ratio too low), representational capacity collapses and accuracy degrades sharply; if learning rate α or trace time constants τp are misconfigured, trace estimates become unstable.

## Foundational Learning

- Concept: **High-Level Synthesis (HLS) for FPGA acceleration**
  - Why needed here: The entire BCPNN accelerator is implemented in C++ and synthesized to RTL using Vitis HLS, not hand-coded Verilog/VHDL. Understanding HLS pragmas (loop unroll, dataflow, pipeline) and interface mapping (AXI4, AXI-Stream) is essential to replicate or extend this work.
  - Quick check question: Can you explain how `#pragma HLS DATAFLOW` enables concurrent sub-kernel execution, and what constraints it imposes on variable sharing?

- Concept: **BCPNN model fundamentals (Hypercolumns, Minicolumns, Synaptic Traces)**
  - Why needed here: The accelerator implements a three-layer BCPNN with HCU/MCU modular structure and probability-based synaptic traces. Without understanding the computational primitives (bias = log(pj), weight = log(pij/(pi·pj))), you cannot validate correctness or debug numerical issues.
  - Quick check question: Given pre-synaptic activation probability pi=0.1, post-synaptic pj=0.2, and joint probability pij=0.05, compute the synaptic weight wij.

- Concept: **Fixed-point and floating-point numeric representations**
  - Why needed here: The paper explores FP32, FP16, and mixed FXP16/FP16 precision. Understanding dynamic range, precision, and overflow behavior is critical for selecting the right format and diagnosing accuracy loss.
  - Quick check question: What is the representable range and minimum positive increment of Q3.12 fixed-point format? Would it safely store a weight value of 5.75?

## Architecture Onboarding

- Component map:
  - **Processing System (PS)**: ARM Cortex-A53 cores running Ubuntu, host C++ code, OpenCL runtime, dataset loading from SD card to DDR.
  - **Programmable Logic (PL)**: FPGA fabric containing BCPNN kernels (full online-learning or inference-only), synthesized via Vitis HLS.
  - **AXI Infrastructure**: 256-bit AXI4 Memory-Mapped interfaces for DDR burst access; AXI-Stream FIFOs for inter-sub-kernel dataflow; AXI-Lite for kernel control signals.
  - **Memory Hierarchy**: Off-chip DDR (2GB on ZCU104) stores dataset and parameters; on-chip BRAM stores FIFO buffers and local kernel variables.
  - **Power Monitoring**: INA226 IC on ZCU104 for board-level and execution power measurement.

- Critical path:
  1. Host initializes FPGA bitstream via `xlnx-config`.
  2. Host loads dataset from SD card to DDR, prepares parameter buffer (weights, biases, sparse indices from training).
  3. Host launches kernel via OpenCL, passing buffer pointers and control parameters.
  4. Kernel fetches input batches via 256-bit AXI4 bursts, converts to streams.
  5. Sub-kernels execute in dataflow: input stream → hidden layer computation (sparse weight lookup, bias addition, soft-WTA) → output layer computation → result stream.
  6. Results written back to DDR; host reads and evaluates accuracy.
  7. For online-learning kernel: synaptic trace updates and weight/bias recomputation occur within the pipeline.

- Design tradeoffs:
  - **Full kernel vs. inference-only**: Full kernel supports online learning but consumes 76% BRAM (limited to parallelism factor 4); inference-only uses <52% BRAM, supports parallelism factor 8 (FP32) or 16 (FP16), achieving 11-17× speedup.
  - **Precision selection**: FP16 offers best latency/energy/accuracy balance; mixed precision adds conversion overhead and risks accuracy loss on complex data; FP32 is safest but slowest.
  - **Model scaling**: Reducing HCU improves latency most; reducing MCU helps moderately; aggressive sparsity (nact/nsil) risks accuracy collapse. Resource usage remains stable due to fixed dataflow structure.
  - **Clock frequency**: FP32 kernels run at 120 MHz; FP16/mixed at 130 MHz (tuned to avoid timing violations on critical paths).

- Failure signatures:
  - **Deadlock**: Kernel hangs during execution; caused by insufficient FIFO depth or AXI-Stream backpressure cycles. Detect via RTL co-simulation; increase FIFO depths or restructure dataflow.
  - **Accuracy drop on precision change**: If FP16→mixed precision causes >2% accuracy loss, dataset may require higher dynamic range; revert to FP16 or adjust Q-format.
  - **Timing violation**: Bitstream fails place-and-route; caused by overly aggressive parallelism or high clock target. Reduce unroll factor or lower clock frequency.
  - **Power budget exceeded**: Board power exceeds 30W envelope (unlikely on ZCU104 but possible on larger designs); reduce parallelism or enable clock gating.

- First 3 experiments:
  1. **Baseline replication**: Implement inference-only FP32 kernel for MNIST with HCU=32, MCU=128, parallelism factor 8. Measure latency and accuracy against ARM baseline. Validate 10×+ speedup and <0.5% accuracy variance.
  2. **Precision sweep**: Run inference-only kernel on Pneumonia dataset across FP32, FP16, and mixed precision. Plot latency vs. energy vs. accuracy. Confirm FP16 as optimal; document accuracy loss threshold for mixed precision.
  3. **Sparsity sensitivity**: On Breast Cancer dataset, vary nact/nsil from 676/156 (baseline) to 160/40 and 80/24. Measure accuracy degradation and latency improvement. Identify the sparsity level where accuracy drops below acceptable threshold (e.g., <80%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What accuracy-efficiency trade-offs emerge when deploying BLNN accelerators on larger-scale, higher-resolution real-world edge datasets (e.g., ImageNet-scale, video streams)?
- Basis in paper: [explicit] The conclusion states "future work aimed at broader application exploration"; experiments were limited to small datasets (28×28 to 128×128 inputs).
- Why unresolved: Only MNIST, Pneumonia, and Breast Cancer datasets were tested; generalization to larger, more diverse workloads remains unknown.
- What evidence would resolve it: Benchmark results on datasets with higher resolution, more classes, and temporal data, reporting latency, energy, and accuracy.

### Open Question 2
- Question: Can the online-learning kernel achieve performance comparable to inference-only through architectural innovations, given its 2–2.6× speedup versus 11–17× for inference?
- Basis in paper: [inferred] The full kernel shows limited speed-up "primarily due to it requiring more subkernels to run and limited parallelization (factor of 4), compared to a factor of 8 and fewer subkernels to run in the inference-only kernel."
- Why unresolved: The BRAM-heavy synaptic trace storage and lower parallelism cap fundamental limits; no alternative memory/compute strategies were explored.
- What evidence would resolve it: A modified kernel design with higher parallelism or memory optimization that narrows the speed-up gap while preserving online learning.

### Open Question 3
- Question: How can mixed-precision configurations be tuned to retain accuracy on complex datasets, where FXP16/FP16 dropped Breast Cancer accuracy from 84% to 78.8%?
- Basis in paper: [inferred] Mixed precision "introduces non-trivial accuracy loss in more complex datasets, limiting its practical applicability."
- Why unresolved: The paper does not investigate alternative fixed-point formats, dynamic rescaling, or layer-wise precision assignment to mitigate loss.
- What evidence would resolve it: Ablation study with varied Q-formats, per-layer precision, or adaptive precision, showing restored accuracy on complex datasets.

## Limitations

- **Major Uncertainties**: The paper does not provide the exact HLS source code, making faithful reproduction challenging without reverse-engineering the described dataflow architecture. The binary parameter format and exact AXI interface bindings are unspecified, requiring custom implementation.
- **Low Confidence Areas**: The exact numerical ranges where FP16 fails for complex datasets aren't quantified, and the mixed-precision accuracy degradation mechanism isn't fully explained. The model scaling benefits assume fixed dataflow architecture without exploring alternative parallelization strategies.

## Confidence

- **High Confidence**: The FPGA implementation methodology using Vitis HLS and AXI-based interfaces is well-established and reproducible. The accuracy and latency improvements over ARM baseline are directly measured and reported with clear statistical context.
- **Medium Confidence**: The precision-performance tradeoffs (FP16 vs mixed vs FP32) are empirically demonstrated on specific datasets, but generalizability to other domains requires validation. The sparsity-accuracy relationships are well-characterized for the tested datasets but may not extend to all BLNN applications.
- **Low Confidence**: The exact numerical ranges where FP16 fails for complex datasets aren't quantified, and the mixed-precision accuracy degradation mechanism isn't fully explained. The model scaling benefits assume fixed dataflow architecture without exploring alternative parallelization strategies.

## Next Checks

1. **RTL Co-simulation Validation**: Implement the AXI-Stream dataflow with FIFO depths determined through systematic RTL co-simulation to identify deadlock conditions and verify sustained throughput under various burst patterns.
2. **Precision Robustness Testing**: Extend the precision sweep beyond the three tested datasets to include high-dynamic-range applications (e.g., medical imaging with wide intensity distributions) to identify failure modes for FP16 and mixed precision.
3. **Sparsity Scaling Analysis**: Perform a systematic study of sparsity levels (nact/nsil ratios) across multiple dataset complexities to map the accuracy-latency tradeoff curve and identify optimal sparsity thresholds for different application domains.