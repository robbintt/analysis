---
ver: rpa2
title: Regularized Gradient Temporal-Difference Learning
arxiv_id: '2601.20599'
source_url: https://arxiv.org/abs/2601.20599
tags:
- gtd2
- r-gtd
- rgtd
- solution
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Regularized Gradient Temporal-Difference Learning
  (R-GTD), a method that addresses instability in off-policy policy evaluation when
  the feature interaction matrix (FIM) is singular or ill-conditioned. By reformulating
  the mean-square projected Bellman error (MSPBE) minimization with regularization,
  R-GTD ensures convergence to a unique solution even when the FIM is singular, overcoming
  a key limitation of standard GTD2 algorithms.
---

# Regularized Gradient Temporal-Difference Learning

## Quick Facts
- arXiv ID: 2601.20599
- Source URL: https://arxiv.org/abs/2601.20599
- Reference count: 40
- Key outcome: R-GTD enables stable convergence to a unique solution even when the feature interaction matrix (FIM) is singular, overcoming a key limitation of standard GTD2 algorithms.

## Executive Summary
This paper addresses instability in off-policy policy evaluation when the feature interaction matrix (FIM) is singular or ill-conditioned. The authors propose Regularized Gradient Temporal-Difference Learning (R-GTD), which reformulates the mean-square projected Bellman error (MSPBE) minimization with regularization. This ensures convergence to a unique solution even when the FIM is singular, overcoming a key limitation of standard GTD2 algorithms. Theoretical analysis establishes convergence guarantees via primal-dual gradient dynamics and explicit error bounds comparing R-GTD to the true projected solution.

## Method Summary
R-GTD reformulates MSPBE minimization by adding a quadratic regularization term and introducing an auxiliary slack variable with penalty. The augmented constraint matrix becomes block form [FIM, Φ⊤DβΦ], which is always full row rank due to the positive definite Φ⊤DβΦ component. This enables convergence guarantees from primal-dual gradient dynamics theory regardless of FIM singularity. The method uses stochastic updates with three parameters (θ, w, λ) and step-size schedule αk = 1/(k + 30), with regularization coefficient c controlling the trade-off between stability and bias.

## Key Results
- R-GTD converges stably in settings where GTD2 exhibits instability, with reduced variance across runs
- In nonsingular cases, R-GTD retains the same asymptotic performance as GTD2 while providing improved stability in singular cases
- Theoretical error bounds quantify the O(1/c) bias introduced by regularization

## Why This Works (Mechanism)

### Mechanism 1: Regularized Constrained Optimization Reformulation
Adding regularization to the MSPBE minimization guarantees a unique solution even when the FIM is singular. The quadratic term selects a particular solution from the affine solution set by penalizing the null-space component.

### Mechanism 2: Full Row Rank Constraint Matrix via Auxiliary Variable
The augmented constraint matrix [FIM, Φ⊤DβΦ] is always full row rank, enabling convergence guarantees from PDGD theory. Since Φ⊤DβΦ ≻ 0, the combined matrix satisfies the full row rank condition regardless of FIM singularity.

### Mechanism 3: Asymptotic Recovery with Bounded Bias
As regularization parameter c → ∞, θRGTD converges to the minimum-norm GTD2 solution. The regularized solution admits expansion θRGTD = (θGTD2 - ΠN(G)(θGTD2)) + O(1/c) in the singular case.

## Foundational Learning

- **Saddle-point optimization and primal-dual gradient dynamics**: Why needed - R-GTD is derived as solving a min-max saddle-point problem; Quick check - Given Lagrangian L(x,λ) = f(x) + λ⊤(Ax-b) with f strongly convex and A full row rank, does PDGD converge?

- **Projected Bellman equation and MSPBE**: Why needed - The paper reformulates MSPBE minimization; Quick check - Why does the standard Bellman equation Φθ = Rπ + γPπΦθ typically have no solution under linear function approximation?

- **Feature interaction matrix (FIM) and its role in GTD convergence**: Why needed - The core problem is FIM singularity causing GTD2 instability; Quick check - What happens to the GTD2 solution θGTD2 = -(FIM)⁻¹Φ⊤DβRπ when FIM is singular?

## Architecture Onboarding

- **Component map**: θ (value parameter) → w (auxiliary/slack variable) → λ (dual/Lagrange multiplier) → c (regularization coefficient)

- **Critical path**: Initialize (θ₀, w₀, λ₀) → Sample trajectory (sk, ak, s'k) from behavior policy β → Compute TD error δk and importance ratio ρk → Update θ, w, λ via stochastic PDGD → Iterate until convergence

- **Design tradeoffs**: Small c provides strong regularization and stability but introduces O(1/c) bias; Large c approaches GTD2 behavior but may become unstable near singularity; Step-size αk follows Robbins-Monro conditions

- **Failure signatures**: θ diverges or oscillates (check step-size or feature normalization); High variance across runs (may indicate c too small or insufficient exploration); Convergence to wrong value (verify importance sampling ratios)

- **First 3 experiments**: 
  1. Reproduce Figure 4: Construct MDP with singular FIM (smallest singular value ~10⁻¹³), compare GTD2 vs R-GTD with c ∈ {0.2, 0.4, 1}
  2. Validate asymptotic recovery: In nonsingular setting, verify θRGTD → θGTD2 as c increases
  3. Baird's counterexample test: Run on Baird's star-shaped MDP, verify TD(0) diverges while both GTD2 and R-GTD converge

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical convergence guarantees of R-GTD be extended to nonlinear function approximation settings? The Conclusion states, "Future work includes extensions to nonlinear function approximation..."

### Open Question 2
What are the finite-time convergence rates (sample complexity bounds) for the R-GTD algorithm? The Conclusion identifies "finite-time convergence analysis" as a direction for future work.

### Open Question 3
How can the regularization parameter c be adapted or selected to optimally balance stability against the introduced bias? The error bounds show a solution bias of O(1/c), while the stability relies on c being finite.

## Limitations
- Core mechanism relies on Φ⊤DβΦ being positive definite; if features are linearly dependent under stationary distribution, theoretical guarantees weaken
- Empirical validation limited to single synthetic MDP with 100 states and 10 actions, leaving scalability to larger problems untested
- Assumes behavior policy has full support for importance sampling ratios to remain bounded

## Confidence

- **High Confidence**: Convergence guarantees under PDGD framework when FIM is singular (supported by explicit error bounds and asymptotic expansions)
- **Medium Confidence**: Stability improvements over GTD2 in singular cases (supported by 30-run experiments but limited to one synthetic setting)
- **Low Confidence**: Asymptotic recovery claims (θRGTD → θGTD2 as c → ∞) in nonsingular cases (limited theoretical analysis and no empirical validation)

## Next Checks

1. **Scalability Test**: Apply R-GTD to a larger MDP (e.g., 1000 states, 50 actions) and verify that stability gains persist without computational degradation

2. **Feature Dependency Analysis**: Construct a test case where Φ⊤DβΦ is nearly singular and measure how regularization parameter c affects both stability and bias

3. **Off-Policy Robustness**: Evaluate R-GTD on Baird's counterexample with varying behavior policies (not just full-support cases) to test importance sampling robustness