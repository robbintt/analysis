---
ver: rpa2
title: Alleviating Attention Hacking in Discriminative Reward Modeling through Interaction
  Distillation
arxiv_id: '2508.02618'
source_url: https://arxiv.org/abs/2508.02618
tags:
- uni00000013
- uni00000011
- attention
- reward
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies attention hacking as a fundamental limitation
  in discriminative reward modeling, where the unidirectional causal attention mechanism
  and independent Siamese-encoding paradigm lead to inadequate token-level interaction,
  making judgment signals vulnerable to misallocated attention. To address this, the
  authors propose Interaction Distillation, a novel training framework that introduces
  an interaction-based natural language understanding model as a teacher to provide
  comprehensive attention patterns, then guides the reward model to simulate these
  patterns through an attentional alignment objective.
---

# Alleviating Attention Hacking in Discriminative Reward Modeling through Interaction Distillation

## Quick Facts
- **arXiv ID**: 2508.02618
- **Source URL**: https://arxiv.org/abs/2508.02618
- **Reference count**: 12
- **Primary result**: Interaction Distillation achieves 68.25% average accuracy on Reward Bench and win rates >0.5 against all baselines in RLHF tasks

## Executive Summary
This paper addresses a fundamental limitation in discriminative reward modeling called "attention hacking," where unidirectional causal attention and independent Siamese-encoding lead to inadequate token-level interactions between chosen and rejected responses. The authors propose Interaction Distillation, a novel training framework that uses a bidirectional NLU teacher model to provide comprehensive attention patterns, which are then distilled into the reward model through an attentional alignment objective. Experimental results demonstrate that this approach delivers more stable and generalizable reward signals compared to state-of-the-art methods, achieving superior performance in both reinforcement learning from human feedback tasks and out-of-distribution preference perception tasks.

## Method Summary
The method employs a DeBERTa-large model fine-tuned on SNLI as a frozen teacher to provide attention patterns for both intra-sequence (chosen-to-chosen, rejected-to-rejected) and inter-sequence (chosen-to-rejected, rejected-to-chosen) interactions. A decoder-only LLaMA3-8B reward model computes simulated attention maps from its query and key matrices and aligns them with the teacher's attention maps using L2 loss. The total loss combines the standard Bradley-Terry preference loss with the attention distillation loss, weighted by hyperparameter η. The approach introduces no additional inference costs while improving both in-distribution RLHF performance and out-of-distribution generalization on the Reward Bench benchmark.

## Key Results
- ID-RM achieves 68.25% average accuracy on Reward Bench benchmark
- Win rates exceed 0.5 against all baselines in RLHF tasks
- SNLI fine-tuning is critical: un-tuned teacher causes >5 point win rate drop
- Best results with K=1 (last block only) for attention alignment
- η in range [0.4, 1.0] provides stable performance

## Why This Works (Mechanism)

### Mechanism 1: Forward-Decaying Attention
Unidirectional causal attention in decoder-only RMs causes forward-decaying attention, making models over-weight later tokens and lose prompt semantics. Causal masking creates a lower-triangular attention pattern where position i cannot attend to positions j > i. This induces a recency bias, causing the RM to base decisions disproportionately on response-end tokens rather than prompt-response relationships.

### Mechanism 2: Siamese Encoding Deficiency
The independent Siamese-encoding paradigm prevents token-level inter-sequence attention, causing deficient cross-sequence semantic comparison. Chosen and rejected responses are encoded separately, yielding representations H^c and H^r without any attention computation A^{c→r} or A^{r→c} at training time. The RM must distinguish preference solely from pooled representations, lacking fine-grained token-level contrast.

### Mechanism 3: Attention-Level Distillation
Distilling comprehensive attention patterns from a bidirectional NLU teacher guides the student RM to simulate missing interactions, improving preference modeling without inference overhead. The teacher model produces full attention maps across all blocks, and the student RM aligns its simulated attention maps through L2 loss, learning what interactions it should have captured.

## Foundational Learning

- **Causal vs. Bidirectional Attention**
  - Why needed here: Understanding why decoder-only models cannot attend backward is essential to grasping the intra-sequence attention deficiency.
  - Quick check question: Can a token at position 10 in a causal Transformer attend to position 15? (No)

- **Siamese Encoding**
  - Why needed here: The preference modeling paradigm encodes chosen/rejected responses independently; understanding this clarifies the inter-sequence attention gap.
  - Quick check question: In a Siamese RM, does the encoding of the chosen response depend on the rejected response? (No)

- **Attention-Level Knowledge Distillation**
  - Why needed here: The method transfers knowledge via attention map alignment, not output logits; this differs from standard response-based distillation.
  - Quick check question: What is being minimized in attention-level distillation vs. soft-label distillation? (Attention map distances vs. output probabilities)

## Architecture Onboarding

- **Component map**: 
  - Teacher Model (DeBERTa-SNLI) -> Attention Maps (A^c→c, A^c→r, A^r→c, A^r→r)
  - Student RM (LLaMA3-8B) -> Simulated Attention Maps
  - Alignment Loss -> Total Loss (Bradley-Terry + η·L_ID)

- **Critical path**:
  1. Forward pass through teacher with concatenated sequences → extract attention maps from top K blocks
  2. Forward pass through RM with separate sequences → compute simulated inter-sequence attention using Q/K matrices
  3. Compute L_ID alignment loss + L_PM preference loss
  4. Backprop only through RM (teacher frozen)

- **Design tradeoffs**:
  - Teacher selection: DeBERTa vs. BERT shows minimal difference, but SNLI fine-tuning is critical (Table 3: -5.41 win rate drop without tuning)
  - Block alignment K: Best results at K=1 (last block only); deeper alignment degrades performance due to non-equivalent processing across architectures
  - Weight η: Stable in range [0.4, 1.0]; higher values prioritize interaction distillation over preference modeling

- **Failure signatures**:
  - Forward-decaying attention: Check cumulative attention distribution across sequence stages; failed cases show late-stage spikes
  - Missing inter-sequence contrast: Visualize A^{Sim}_{c→r}; near-uniform values indicate inadequate cross-sequence learning
  - Teacher mismatch: Using un-tuned teacher causes >5 point win rate drop (Table 3)

- **First 3 experiments**:
  1. Train standard BT-RM on HH-RLHF, plot cumulative attention across sequence stages to confirm forward-decaying pattern
  2. Compare DeBERTa-SNLI vs. DeBERTa-untuned vs. BERT-SNLI on held-out preference subset to validate teacher sensitivity
  3. Run small-scale sweep on η ∈ {0.4, 0.6, 0.8, 1.0} and K ∈ {1, 2, 4} with 10% of training data

## Open Questions the Paper Calls Out
None

## Limitations

- **Architectural Dependency**: The solution is inherently tied to decoder-only architectures with causal attention and Siamese encoding; cross-encoder or bidirectional approaches would require fundamental redesign.
- **Teacher Bias Transfer**: The method relies on attention patterns from a DeBERTa model fine-tuned on SNLI, but there's no analysis of whether NLI-specific attention biases could negatively transfer to preference modeling tasks.
- **Inference Overhead Trade-off**: While claiming "no additional inference costs," this assumes teacher attention patterns remain valid across domains; task-specific fine-tuning would introduce significant overhead.

## Confidence

**High Confidence**: The identification of forward-decaying attention as a causal factor in RM failure (Mechanism 1) is well-supported by experimental evidence showing attention spikes in failed cases and accuracy drops with masking.

**Medium Confidence**: The claim that inter-sequence attention distillation improves OOD generalization is supported by Reward Bench results (68.25% average accuracy), though the paper doesn't analyze which specific out-of-distribution types benefit most.

**Low Confidence**: The assertion that the method is "compatible with other optimization algorithms" is weakly supported, as experiments primarily compare against Bradley-Terry baselines and PPO-based RLHF with limited exploration of alternative frameworks.

## Next Checks

1. **Attention Pattern Transferability**: Test the distilled RM on a preference task where NLI and preference semantics diverge significantly (e.g., stylistic preferences vs. factual correctness) to quantify domain shift effects.

2. **Architectural Generalization**: Implement a minimal cross-encoder variant of the RM and measure whether the inter-sequence distillation component still provides benefits to validate whether the Siamese attention gap is truly fundamental.

3. **Teacher Selection Sensitivity**: Systematically compare attention distillation performance using teachers from different NLU tasks (MNLI, QQP, SQuAD) and different model families (BERT, RoBERTa, DeBERTa) to establish optimal teacher configurations.