---
ver: rpa2
title: 'Generative Models for Long Time Series: Approximately Equivariant Recurrent
  Network Structures for an Adjusted Training Scheme'
arxiv_id: '2505.05020'
source_url: https://arxiv.org/abs/2505.05020
tags:
- sequence
- time
- dataset
- data
- rvae-st
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a recurrent variational autoencoder with subsequent
  training (RVAE-ST) for generating long time series. The key idea is to use a time-shift
  equivariant architecture with a progressively increasing sequence length training
  scheme to improve performance on long, approximately stationary time series.
---

# Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme

## Quick Facts
- **arXiv ID:** 2505.05020
- **Source URL:** https://arxiv.org/abs/2505.05020
- **Reference count:** 40
- **Key outcome:** RVAE-ST outperforms state-of-the-art on highly stationary datasets using equivariant recurrent architecture with progressive sequence length training.

## Executive Summary
This paper introduces RVAE-ST, a recurrent variational autoencoder with a time-shift equivariant architecture and a progressively increasing sequence length training scheme for generating long time series. The key innovation is leveraging the inductive bias of approximate time-shift equivariance to capture long-range dependencies more effectively than existing methods. The model maintains a constant number of parameters regardless of sequence length, making it scalable for very long sequences. Experiments on five benchmark datasets demonstrate superior performance on highly stationary data while remaining competitive on less stationary datasets.

## Method Summary
The method combines a recurrent VAE architecture with a curriculum learning approach for sequence length. The encoder is a 4-layer LSTM stack that processes input time series and produces latent distribution parameters. The decoder uses a repeat-vector operation to apply a fixed latent code across the entire sequence, followed by another 4-layer LSTM stack and a time-distributed linear output layer. The key innovation is the subsequent training scheme: starting with short sequences (length 100) and progressively increasing to longer sequences (up to 1000), allowing the model to learn long-range dependencies incrementally rather than being overwhelmed from the start.

## Key Results
- RVAE-ST outperforms state-of-the-art generative models on highly stationary datasets (Electric Motor, ECG, Sine) across FID, discriminative scores, and ELBO metrics
- The model maintains competitive performance on less stationary datasets (ETT, MetroPT3) despite its equivariance bias
- Performance degrades gracefully when generating sequences beyond the trained length, showing stable but repetitive patterns
- The architecture achieves constant parameter count regardless of sequence length due to the repeat-vector decoder design

## Why This Works (Mechanism)
The success of RVAE-ST stems from its approximate time-shift equivariance, which allows the model to learn translation-invariant patterns in stationary time series. The subsequent training scheme addresses the fundamental challenge of training recurrent models on very long sequences by gradually building up temporal context. This curriculum approach prevents the vanishing/exploding gradient problems that typically plague long-sequence training while maintaining the model's ability to capture long-range dependencies. The combination of equivariant architecture and progressive training creates a powerful inductive bias specifically suited for quasi-periodic, stationary time series.

## Foundational Learning
- **Concept: Approximate Equivariance**
  - Why needed here: This is the core inductive bias of the model. You must understand that it's not a perfect mathematical property in this implementation but a useful structural tendency that the architecture encourages, especially for long sequences.
  - Quick check question: Explain why a standard LSTM is only *approximately* time-shift equivariant, especially at the start of a sequence. What component of the RVAE-ST decoder makes it *perfectly* equivariant?

- **Concept: Curriculum Learning for Sequence Length**
  - Why needed here: The model's success on long sequences is not just due to the architecture, but critically depends on the training strategy. Understanding the motivation and execution of this scheme is essential for reproducing the results.
  - Quick check question: Why might training a recurrent model directly on a very long sequence (e.g., l=1000) fail? How does the proposed subsequent training scheme address this?

- **Concept: Evidence Lower Bound (ELBO) and Variational Autoencoders**
  - Why needed here: The model is a VAE, and its loss function and performance evaluation are framed in terms of the ELBO. A grasp of the reconstruction vs. regularization trade-off (controlled by α and β) is necessary for tuning and interpretation.
  - Quick check question: What are the two competing terms in the VAE loss function used in this paper? How does the paper's normalization of the ELBO (`ELBOnorm`) allow for fairer comparison across different datasets and sequence lengths?

## Architecture Onboarding
- **Component map:** Encoder LSTM stack -> Latent distribution parameters (µ, log(σ)) -> Sampled latent vector z -> Repeat vector operation -> Decoder LSTM stack -> Time-distributed linear layer
- **Critical path:** Sampled Latent Vector (z) → Repeat Vector Operation → Stacked LSTM Decoder → Time-Distributed Linear Layer
- **Design tradeoffs:**
    - Stationary vs. Non-Stationary Data: The equivariance bias makes the model excellent for quasi-periodic data (Electric Motor, ECG) but less competitive on datasets with strong irregular patterns or trends (ETT, MetroPT3)
    - Fixed Latent Code vs. Dynamic VAE: Using a single, fixed latent vector `z` for the entire sequence captures global properties efficiently but may be insufficient for sequences with highly complex, evolving dynamics
    - Complexity vs. Scalability: The model sacrifices the powerful, non-local pattern matching of Transformers for the scalable, sequential processing of LSTMs, avoiding the quadratic memory cost

- **Failure signatures:**
    - Training Instability/Divergence: If trained conventionally on very long sequences from the start, the model may fail to converge
    - Posterior Collapse: The latent space may become uninformative, resulting in smooth, featureless outputs
    - Repetitive/Flatline Generation: For non-stationary or highly dynamic data, especially beyond the trained sequence length, the model may fall into a stable state and generate repetitive, low-variance patterns

- **First 3 experiments:**
    1. Ablate the Training Scheme: Train the RVAE-ST model conventionally on a long sequence (e.g., l=1000) and compare its performance (ELBO, FID) against the full subsequent training approach to verify the critical contribution of the curriculum
    2. Test on Non-Stationary Data: Evaluate the trained model on a dataset with strong non-stationary characteristics (e.g., ETT) and compare its performance against a baseline like TimeVAE or Diffusion-TS to confirm the limitations imposed by the equivariance bias
    3. Generate Beyond Training Length: Using weights trained up to l=1000, generate sequences of length l=5000. Visually inspect the output for the emergence of stable, repetitive states, as described in Appendix A.1, to understand the model's out-of-distribution behavior

## Open Questions the Paper Calls Out
- How can the stepwise sequence length training schedule be formally optimized? The authors state the increment schedule was chosen in a "relatively ad-hoc manner" and suggest that optimizing this scheme could enhance performance.
- Can the approximate time-shift equivariance and training methodology be successfully adapted to diffusion-based generative models? The discussion explicitly proposes extending the methodology to other model classes, specifically diffusion-based models.
- How can the model be modified to prevent degradation when generating non-stationary sequences significantly longer than the training horizon? Appendix A.1 shows that for less stationary datasets (ETT, MetroPT3), the model produces repetitive, flatline patterns when generating sequences of length 5000 (trained up to 1000).

## Limitations
- The model's performance is highly dependent on the stationarity assumption, which is not rigorously quantified in the paper
- The evaluation relies heavily on FID scores via TS2Vec embeddings, which may not fully capture temporal structure or long-range dependencies
- The subsequent training scheme requires careful tuning of sequence length progression and early stopping criteria, which are not fully specified

## Confidence
- **High Confidence:** The architectural design (LSTMs with repeat-vector decoder) and the subsequent training scheme are clearly described and reproducible
- **Medium Confidence:** The claim that the model maintains a constant number of parameters regardless of sequence length is valid for the architecture but depends on implementation details
- **Low Confidence:** The paper's assertion that the model "significantly outperforms" state-of-the-art methods is context-dependent and may not hold for datasets with strong non-stationary characteristics

## Next Checks
1. **Quantify Stationarity:** Apply a stationarity test (e.g., Augmented Dickey-Fuller) to all benchmark datasets and correlate the results with the model's performance to validate the assumption that equivariance benefits stationary data
2. **Test Beyond Trained Length:** Generate sequences longer than the maximum trained length (e.g., l=5000 when trained up to l=1000) and analyze the emergence of stable, repetitive states as described in Appendix A.1
3. **Hybrid Architecture Exploration:** Modify the decoder to use a dynamic latent sequence (rather than a fixed `z`) and evaluate its performance on non-stationary datasets to assess whether the model can be extended beyond its current limitations