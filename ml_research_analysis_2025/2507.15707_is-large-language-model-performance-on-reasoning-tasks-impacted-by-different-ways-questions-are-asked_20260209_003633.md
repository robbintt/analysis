---
ver: rpa2
title: Is Large Language Model Performance on Reasoning Tasks Impacted by Different
  Ways Questions Are Asked?
arxiv_id: '2507.15707'
source_url: https://arxiv.org/abs/2507.15707
tags:
- reasoning
- question
- accuracy
- answer
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how different question types impact large\
  \ language model (LLM) performance on reasoning tasks. The authors evaluate five\
  \ LLMs\u2014two closed-source (GPT-4o, GPT-3.5-turbo) and three open-source (Llama-8B,\
  \ Llama-1B, Gemma-7B)\u2014on quantitative reasoning (GSM8K300) and deductive reasoning\
  \ (bAbI300) tasks using short-answer questions (SAQs), multiple-choice questions\
  \ (MCQs), and true/false questions (TFQs)."
---

# Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?
## Quick Facts
- arXiv ID: 2507.15707
- Source URL: https://arxiv.org/abs/2507.15707
- Authors: Seok Hwan Song; Mohna Chakraborty; Qi Li; Wallapak Tavanapong
- Reference count: 37
- Large language models perform differently across question formats (SAQ, MCQ, TFQ), with reasoning accuracy not always matching final selection accuracy.

## Executive Summary
This study investigates how different question types impact large language model (LLM) performance on reasoning tasks. The authors evaluate five LLMs—two closed-source (GPT-4o, GPT-3.5-turbo) and three open-source (Llama-8B, Llama-1B, Gemma-7B)—on quantitative reasoning (GSM8K300) and deductive reasoning (bAbI300) tasks using short-answer questions (SAQs), multiple-choice questions (MCQs), and true/false questions (TFQs). Performance is measured by final selection accuracy (correct final answer) and reasoning accuracy (correct reasoning steps). Key findings include: (1) significant differences in LLM performance across question types, with SAQs generally outperforming MCQs and TFQs in reasoning accuracy; (2) reasoning accuracy does not always correlate with final selection accuracy, indicating that LLMs may guess correctly despite flawed reasoning; (3) the position of correct answers in MCQs, the number of options, and the choice of words (e.g., "True or False" vs. "Yes or No") significantly influence performance. These insights highlight the need to consider question type design in LLM benchmarking and development.

## Method Summary
The study evaluates five LLMs on GSM8K300 and bAbI300 datasets using three question formats: SAQs, MCQs, and TFQs. Performance is measured using two metrics: final selection accuracy (correct final answer) and reasoning accuracy (correct reasoning steps). The experiments use zero-shot greedy decoding with temperature=0, and reasoning accuracy is assessed through manual human annotation. Statistical significance is determined using Wilcoxon signed-rank tests.

## Key Results
- SAQs generally outperform MCQs and TFQs in reasoning accuracy across all models and tasks
- Reasoning accuracy does not necessarily correlate with final selection accuracy, revealing decoupling between reasoning and selection
- LLM performance is significantly influenced by answer position in MCQs, number of options, and wording choices in TFQs

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Selection Decoupling in Structured Outputs
- Claim: LLMs can achieve high final selection accuracy without corresponding reasoning accuracy when question formats provide scaffolded options.
- Mechanism: When presented with MCQs or TFQs, LLMs may pattern-match to plausible options or guess correctly before reasoning, bypassing actual computation. Conversely, correct reasoning can be negated by selection failures (e.g., choosing the wrong option despite correct calculation).
- Core assumption: Assumption: Final selection accuracy reflects reasoning capability; this paper demonstrates they are separable.
- Evidence anchors:
  - [abstract] "Reasoning accuracy does not necessarily correlate with the final selection accuracy."
  - [Section 6.1, Pattern A] "Good with guessing: LLMs make incorrect reasoning steps but guess the final answer correctly... occurs in approximately 96% of incorrect outputs with Gemma on TFQ."
  - [corpus] No direct corpus support; neighbor papers focus on reasoning enhancement, not format-induced decoupling.
- Break condition: When question formats require open-ended generation without anchors (SAQs), reasoning and selection accuracy converge.

### Mechanism 2: Token and Position Bias in Selection Tasks
- Claim: LLM performance on MCQs is sensitive to non-semantic factors including option position, token labels, and the presence of catch-all options.
- Mechanism: LLMs exhibit selection biases—preference for specific positions or tokens—rather than purely content-based reasoning. The "Something else" option creates additional decision complexity; models often select numerically proximate options instead.
- Core assumption: Assumption: Position effects are artifacts of training data distributions or attention patterns, not reasoning.
- Evidence anchors:
  - [Section 7.1] "Using only SEOs as the correct answers (oSEO) results in a significant performance drop on MCQs. The models... struggled to select SEOs when their final reasoning answer did not match one of the provided options."
  - [Section 4.1] "SAQ wins over MCQ (o-SEO) in 80% of the cases (4/5)."
  - [corpus] Weak support; neighbor papers address reasoning architectures, not selection bias.
- Break condition: When correct answers are uniformly distributed across positions and SEO is excluded, position bias effects diminish but do not disappear.

### Mechanism 3: Wording Sensitivity in Binary Classification
- Claim: Surface-level lexical choices ("True/False" vs "Yes/No", question vs statement format) cause measurable performance differences in TFQs.
- Mechanism: Different lexical framings may activate different reasoning pathways or priors; "True" correct answers outperform "False" correct answers, suggesting response bias toward affirmation.
- Core assumption: Assumption: Models have learned implicit priors favoring affirmative responses from training corpora.
- Evidence anchors:
  - [Section 8.3, Table 6] "GPT-3.5's FS accuracy... drops with statistical significance when 'Yes or No' instead of 'True or False' in all scenarios except when... guiding instruction to solve the problem first (+st)."
  - [Section 8.2] "LLMs mostly reason better with TFQs with 'True' as correct answers."
  - [corpus] No corpus support; neighbor papers do not examine wording effects.
- Break condition: When explicit instructions to "solve first" are added, wording sensitivity partially attenuates.

## Foundational Learning

- Concept: **Paired statistical testing (Wilcoxon signed-rank)**
  - Why needed here: The paper uses paired tests to isolate question-type effects within the same problem instances, controlling for problem difficulty variance.
  - Quick check question: Why would paired tests be preferred over independent samples when comparing SAQ vs MCQ performance on the same underlying problems?

- Concept: **Reasoning accuracy vs final selection accuracy**
  - Why needed here: These are distinct metrics; conflating them masks cases where models guess correctly or select incorrectly despite valid reasoning.
  - Quick check question: If a model has 90% final selection accuracy but 60% reasoning accuracy on MCQs, what can you infer about its true reasoning capability?

- Concept: **Selection bias in LLM outputs**
  - Why needed here: Prior work (Zheng et al., 2023, cited in paper) establishes that LLMs favor certain token positions; this paper extends findings to SEO options and option count effects.
  - Quick check question: What experimental control would isolate whether performance differences are due to reasoning vs position bias?

## Architecture Onboarding

- Component map:
  - Problem sample -> Prompt formatter -> LLM inference -> String matching -> Human evaluation

- Critical path:
  1. Sample problem from GSM8K300 or bAbI300
  2. Apply prompt template (SAQ/MCQ/TFQ with specific configuration)
  3. Query LLM once per problem-configuration pair
  4. Extract final selection via string matching
  5. Route output to human evaluator for reasoning accuracy annotation

- Design tradeoffs:
  - 5-option vs 11-option MCQs: Higher option counts increase reasoning burden but reduce guessing probability; paper shows 11-option performance drops for some models.
  - SEO inclusion: Provides SAQ-like fallback but introduces selection complexity; models struggle to use it appropriately.
  - Automated vs manual evaluation: Final selection is automatable; reasoning accuracy requires costly human annotation.

- Failure signatures:
  - **Correct reasoning, wrong selection** (Pattern D): Valid calculation but incorrect option choice due to unit/decimal errors or premature guessing.
  - **Wrong reasoning, correct selection** (Pattern A): Guessing before reasoning; high in TFQ-False conditions.
  - **SEO avoidance** (Pattern E): Models re-calculate to match numerical options rather than selecting SEO.
  - **Early stopping** (Pattern H): Reasoning terminates mid-process, missing final selection entirely.

- First 3 experiments:
  1. Replicate SAQ vs MCQ (U vs U-SEO vs oSEO) comparison on a 50-problem subset; verify reasoning-selection decoupling pattern holds.
  2. Test position bias by shuffling correct answer positions across 5-option MCQs; measure if accuracy varies by position bucket.
  3. Ablate wording sensitivity by comparing "True/False" vs "Yes/No" with and without "solve first" instruction on TFQs; quantify instruction attenuation effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do prompting strategies (e.g., few-shot learning or Chain-of-Thought) mitigate or exacerbate the performance discrepancies observed across different question types?
- Basis in paper: [explicit] Section 10 (Limitations) states that the experiments "do not involve prompting methods or few-shot learning strategies," limiting the scope to zero-shot greedy decoding.
- Why unresolved: The current study isolates the impact of question format under zero-shot conditions; it remains unknown if providing examples or explicit reasoning prompts neutralizes the selection biases found in MCQs and TFQs.
- What evidence would resolve it: Re-evaluating the GSM8K300 and bAbI300 benchmarks using few-shot and CoT prompting to measure if the accuracy gap between SAQs and MCQs/TFQs narrows.

### Open Question 2
- Question: How does the complexity or length of the reasoning chain affect the divergence between final selection accuracy and reasoning accuracy?
- Basis in paper: [explicit] Section 10 (Limitations) notes that the proposed measure "does not account for... how the number of reasoning steps affects performance."
- Why unresolved: The study treats reasoning as a binary (correct/incorrect) but observes that models often guess correctly with flawed reasoning; the correlation between task complexity and this "guessing" behavior is unmeasured.
- What evidence would resolve it: A fine-grained analysis correlating the number of steps in the ground-truth solution with the likelihood of a "Correct Final Selection but Wrong Reasoning" error pattern.

### Open Question 3
- Question: Can fine-tuning on specific question formats eliminate the observed selection biases, such as the preference for "True" over "False" or the difficulty with "Something else" options?
- Basis in paper: [inferred] Section 11 (Ethical Considerations) suggests results "may not generalize... if fine-tuned," and Section 9 highlights the need to improve accuracy in selecting final answers.
- Why unresolved: The study evaluates pre-trained/instruct models but does not determine if these biases are permanent artifacts of the architecture or if they can be corrected via targeted training data.
- What evidence would resolve it: Fine-tuning models on a balanced dataset of TFQs (equal True/False distribution) and MCQs (with "Something else" as a frequent correct answer) to observe if the statistical performance gaps disappear.

## Limitations
- Human annotation dependency introduces potential inter-rater variability without reported agreement statistics
- Controlled synthetic datasets (GSM8K300, bAbI300) may not generalize to real-world scenarios
- Single inference per configuration prevents measuring consistency and confounds stochastic effects

## Confidence
- **High confidence**: Question type effects on final selection accuracy are robust and well-documented. The decoupling of reasoning and selection accuracy is consistently observed across multiple models and task types.
- **Medium confidence**: Specific mechanism claims (position bias, wording sensitivity) are supported by the data but require additional experimental controls to rule out alternative explanations.
- **Low confidence**: Generalization claims to broader LLM benchmarking practices and real-world applications lack supporting evidence from diverse datasets or evaluation protocols.

## Next Checks
1. Compute Cohen's kappa or similar statistics for reasoning accuracy annotations across multiple evaluators on a subset of outputs to quantify measurement uncertainty.
2. Run each problem-configuration pair through each LLM multiple times (3-5 samples) to measure consistency and distinguish systematic biases from stochastic variation.
3. Evaluate the same question-type effects on a non-synthetic dataset (e.g., MultiArith or actual educational assessment questions) to assess whether controlled experimental findings hold in naturalistic settings.