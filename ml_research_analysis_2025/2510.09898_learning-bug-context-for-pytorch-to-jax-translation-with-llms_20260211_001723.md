---
ver: rpa2
title: Learning Bug Context for PyTorch-to-JAX Translation with LLMs
arxiv_id: '2510.09898'
source_url: https://arxiv.org/abs/2510.09898
tags:
- code
- pytorch
- translation
- evaluation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating PyTorch code
  to JAX using large language models (LLMs), which is difficult due to differences
  in design and limited JAX representation in training data. The proposed T2J framework
  uses a fixed-bug dataset created by professional developers who correct initial
  JAX translations from GPT-4o-mini, then augments prompts with these fixes to guide
  lightweight LLMs.
---

# Learning Bug Context for PyTorch-to-JAX Translation with LLMs

## Quick Facts
- arXiv ID: 2510.09898
- Source URL: https://arxiv.org/abs/2510.09898
- Reference count: 38
- This paper proposes T2J, a framework that improves PyTorch-to-JAX translation by injecting bug-fix context into prompts, achieving up to 10% CodeBLEU improvement and 50% reduction in FixCost.

## Executive Summary
This paper addresses the challenge of translating PyTorch code to JAX using large language models (LLMs), which is difficult due to differences in design and limited JAX representation in training data. The proposed T2J framework uses a fixed-bug dataset created by professional developers who correct initial JAX translations from GPT-4o-mini, then augments prompts with these fixes to guide lightweight LLMs. T2J introduces three tailored evaluation metrics—CodeTrans, FixCost, and Comparison Scores—to assess translation quality. Empirically, T2J improves GPT-4o-mini performance by up to 10% on CodeBLEU, 50% on FixCost Score, 1.33 points on CodeTrans Score, and 100% on Comparison Score, with generated code running up to 2.5× faster than the baseline.

## Method Summary
T2J creates a fixed-bug dataset by having professional developers debug JAX translations generated by a cheap LLM (GPT-4o-mini) from PyTorch code, resulting in 163 error/solution pairs from 20 samples. For new translations, T2J augments prompts with relevant bug-fix examples from this dataset in JSON format, using cross-validation to exclude the current sample. The framework evaluates translations using four metrics: CodeBLEU for syntactic similarity, T2J CodeTrans Score (LLM-as-judge for functional correctness), T2J FixCost Score (human fix steps), and T2J Comparison Score (head-to-head ranking).

## Key Results
- T2J improves GPT-4o-mini performance by up to 10% on CodeBLEU and 50% on FixCost Score
- CodeTrans Score improves by 1.33 points and Comparison Score shows 100% improvement
- Generated JAX code runs up to 2.5× faster than baseline translations
- All metrics exhibit weak correlation (below 0.3) with fixing cost, indicating they capture different quality aspects

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning via Structured Bug-Fix Patterns
Providing LLMs with concrete error-to-fix mappings from a curated dataset improves translation quality by steering generation away from recurrent failure modes. The fixed-bug dataset (163 error/solution pairs) is injected into prompts as JSON context, where each entry contains: erroneous code, error message, fix description, and corrected code. The LLM uses these examples to recognize and avoid similar patterns during translation. Core assumption: lightweight LLMs can generalize from bug-fix patterns across different code samples to prevent similar errors in new translations (cross-sample transfer). Evidence: augmented prompt specification in Section 4.1.3; weak corpus support—no prior work specifically on PyTorch-to-JAX bug pattern transfer found. Break condition: if error patterns are too sparse or code-specific, generalization fails.

### Mechanism 2: Human-Fixed Ground Truth Reduces Distribution Shift
Professional developer verification creates higher-quality reference data than relying on costly LLM outputs alone. Two developers with 5+ years of Python experience iteratively debug JAX drafts until functional equivalence with PyTorch source. This process captures realistic error patterns that LLMs actually make, not hypothetical ones. Core assumption: human-verified fixes represent ground truth that is both correct and representative of real translation challenges. Evidence: Section 4.2 describes the human bug-fixing process; Table 5 shows human verification adds value even for strong models. Break condition: human developers may introduce subjective biases or miss subtle semantic differences; verification depends on test case coverage.

### Mechanism 3: Multi-Metric Evaluation Captures Different Quality Dimensions
Combining syntactic (CodeBLEU), semantic (CodeTrans), and effort-based (FixCost) metrics provides a more complete picture of translation quality than any single metric. CodeTrans uses LLM-as-judge for usefulness and functional correctness (0-4 scale). FixCost counts human fix steps. Comparison Score directly ranks two translations. Weak correlation between metrics (< 0.3) suggests they capture distinct aspects. Core assumption: FixCost Score approximates real development effort; LLM-as-judge correlates with human judgment of correctness. Evidence: Section 5.2.2 shows weak correlation between metrics and fixing cost; Section 4.3 explains CodeBLEU insufficiency for semantic equivalence. Break condition: LLM-as-judge may inherit model biases; discrete fix-step counting doesn't capture fix difficulty variation.

## Foundational Learning

- **In-Context Learning (ICL):**
  - Why needed here: T2J avoids fine-tuning costs by using demonstration examples directly in prompts.
  - Quick check question: Can you explain why providing 19 bug-fix examples in a prompt might help with the 20th translation, even without model weight updates?

- **PyTorch vs. JAX Execution Models:**
  - Why needed here: PyTorch uses eager execution with mutable state; JAX uses functional programming with immutable arrays and explicit PRNG keys. This mismatch causes translation errors.
  - Quick check question: In Figure 1, why does the JAX version require an `init_params` function while PyTorch handles initialization in `__init__`?

- **LLM-as-Judge Evaluation Paradigm:**
  - Why needed here: Traditional metrics like BLEU fail for code semantic equivalence. LLM-as-judge enables reference-free or reference-augmented quality assessment.
  - Quick check question: What are two failure modes when using an LLM to evaluate code correctness?

## Architecture Onboarding

- **Component map:** Human Bug Fixing -> Fixed-Bug Dataset -> In-Context Learning -> Translation Evaluation
- **Critical path:** 1) Curate PyTorch samples from TorchLeet (20) + CodeParrot (100) 2) Generate initial JAX translations with cheap LLM 3) Human verification creates fixed-bug dataset (163 pairs from 20 samples) 4) For new translation: construct augmented prompt with relevant bug-fix examples 5) Evaluate with all four metrics
- **Design tradeoffs:** Intrinsic (20 samples with human ground truth) vs. Extrinsic (100 GitHub samples with costly LLM ground truth): Human verification is expensive but more reliable; GitHub samples are realistic but lack test cases. Cross-validation (leave-one-out) vs. held-out test: Cross-validation maximizes limited data but may overfit to the 20-sample set. Discrete fix-step counting vs. weighted effort: Current approach treats all fixes equally; complex fixes may require more effort.
- **Failure signatures:** CodeBLEU improves but functional correctness doesn't → syntactic similarity without semantic preservation. FixCost varies widely across samples (1-32 steps) → some patterns not covered by bug-fix dataset. Extrinsic evaluation shows baseline outperforming T2J on CodeBLEU (0.41 vs 0.38) → metric limitations or distribution shift.
- **First 3 experiments:** 1) Reproduce intrinsic evaluation: Run 20-sample cross-validation with standard vs. augmented prompts; verify CodeTrans Score improvement (target: +1.33 points). 2) Ablate bug-fix context: Test with partial bug-fix datasets (e.g., only training-loop errors) to identify which error categories contribute most to improvement. 3) Extend to open-source LLMs: Apply T2J prompt augmentation to Llama or Mistral; compare against GPT-4o-mini baseline to test generalization beyond OpenAI models.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does T2J perform when applied to open-source LLMs (e.g., CodeLLaMA, DeepSeek-Coder)? Basis: T2J has not yet been applied to improving open LLMs, due to budget constraints that limit our ability to hire software professionals for the human bug-fixing process on these models. Why unresolved: Budget constraints prevented hiring professionals to create the fixed-bug dataset for open LLMs. What evidence would resolve it: Results from applying T2J's prompt augmentation to open-source LLMs, using the existing fixed-bug dataset or a newly curated one.

- **Open Question 2:** How can the relative effort of each bug-fixing step be estimated beyond simple step counting? Basis: our measure of fixing cost is currently based only on counting fixing steps, whereas in practice, each fix may vary in difficulty. There is a need of an algorithm that estimates the relative effort of each bug-fixing step. Why unresolved: Current T2J FixCost Score treats all fixes equally, ignoring complexity differences. What evidence would resolve it: A weighted fixing cost algorithm validated against developer time/effort data.

- **Open Question 3:** Why do all evaluation metrics show weak correlation (below 0.3) with human fixing cost? Basis: Table 3 shows Pearson/Spearman correlations between metrics and T2J FixCost Score all below 0.3; authors note One possible reason is that other metrics are continuous, whereas fixing cost is measured as discrete steps. Why unresolved: The discrepancy suggests either metrics capture different quality aspects or the fixing cost measure is too coarse. What evidence would resolve it: Analysis with finer-grained fixing effort measures (e.g., time per step) or correlation with functional correctness on larger datasets.

- **Open Question 4:** Does T2J's prompt augmentation effectively transfer to code domains beyond problem-solving snippets (e.g., production ML pipelines)? Basis: we conducted the human bug-fixing process only on the problem-solving code dataset, which we want to extend this process for other domains. Why unresolved: The fixed-bug dataset was built only from TorchLeet problems with test cases; generalization to complex, repository-level code remains untested. What evidence would resolve it: Evaluation on diverse PyTorch codebases (e.g., HuggingFace Transformers) with human-verified JAX translations.

## Limitations

- Fixed-bug dataset size (163 pairs from 20 samples) raises concerns about coverage and generalizability to unseen code patterns
- Human-verified ground truth is resource-intensive and may introduce subjective biases
- Evaluation framework relies heavily on LLM-as-judge metrics (CodeTrans, Comparison Score) which lack human benchmark validation
- Weak correlation between evaluation metrics suggests they capture different aspects but also raises questions about metric reliability
- Intrinsic evaluation uses cross-validation on a small dataset, which may overestimate performance on real-world translations

## Confidence

- **High Confidence:** CodeBLEU improvements (10% gain) and FixCost reduction (50% improvement) are empirically measured with clear methodology
- **Medium Confidence:** T2J CodeTrans Score improvements (+1.33 points) rely on LLM-as-judge which has known consistency issues
- **Low Confidence:** Real-world performance on extrinsic GitHub samples, where baseline outperforms T2J on CodeBLEU (0.41 vs 0.38), suggesting evaluation limitations or distribution shift

## Next Checks

1. **Coverage Analysis:** Systematically categorize error types in the fixed-bug dataset and measure which categories contribute most to translation failures; test whether T2J performance degrades predictably when specific error categories are omitted.

2. **Human Evaluation Benchmark:** Conduct blind human evaluation comparing T2J vs baseline translations on 10 samples, rating functional correctness and usefulness on 0-4 scale to validate LLM-as-judge scores.

3. **Generalization Test:** Apply T2J prompt augmentation to open-source LLMs (Llama-3, Mistral) on the 100 extrinsic samples; measure whether bug-fix context transfer generalizes beyond GPT-4o-mini.