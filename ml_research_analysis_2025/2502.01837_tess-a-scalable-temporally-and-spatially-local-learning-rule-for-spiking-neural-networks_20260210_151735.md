---
ver: rpa2
title: 'TESS: A Scalable Temporally and Spatially Local Learning Rule for Spiking
  Neural Networks'
arxiv_id: '2502.01837'
source_url: https://arxiv.org/abs/2502.01837
tags:
- learning
- tess
- time
- memory
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training spiking neural networks
  (SNNs) efficiently on resource-constrained edge devices, where conventional error
  backpropagation methods are computationally and memory intensive due to their reliance
  on global information flow across layers and time. To overcome these limitations,
  the authors propose TESS, a temporally and spatially local learning rule inspired
  by biological mechanisms such as eligibility traces, spike-timing-dependent plasticity
  (STDP), and neural activity synchronization.
---

# TESS: A Scalable Temporally and Spatially Local Learning Rule for Spiking Neural Networks

## Quick Facts
- **arXiv ID**: 2502.01837
- **Source URL**: https://arxiv.org/abs/2502.01837
- **Reference count**: 35
- **Primary result**: Achieves backpropagation-comparable accuracy on CIFAR10, CIFAR100, DVS Gesture, and CIFAR10-DVS while using 205-661× fewer MACs and 3-10× less memory

## Executive Summary
TESS addresses the challenge of training spiking neural networks (SNNs) efficiently on resource-constrained edge devices, where conventional error backpropagation methods are computationally and memory intensive due to their reliance on global information flow across layers and time. To overcome these limitations, the authors propose TESS, a temporally and spatially local learning rule inspired by biological mechanisms such as eligibility traces, spike-timing-dependent plasticity (STDP), and neural activity synchronization. TESS operates entirely within the local context of each neuron, leveraging eligibility traces for temporal credit assignment and a locally generated learning signal based on fixed basis vectors for spatial credit assignment. This design eliminates the need for backpropagation across layers, significantly reducing both computational and memory overhead.

The primary results demonstrate that TESS achieves performance comparable to backpropagation through time (BPTT) on several challenging datasets, including IBM DVS Gesture, CIFAR10, CIFAR100, and CIFAR10-DVS. Specifically, TESS matches BPTT's accuracy on most datasets while incurring only a ~1.4% accuracy drop on CIFAR10-DVS. Additionally, TESS achieves 205–661× fewer multiply-accumulate (MAC) operations and 3–10× lower memory usage compared to BPTT. These findings highlight TESS's potential for enabling scalable, energy-efficient on-device learning in SNNs, making it well-suited for real-time applications on low-power edge devices.

## Method Summary
TESS is a three-factor learning rule for SNNs that decouples temporal and spatial credit assignment. It uses decoupled pre- and post-synaptic eligibility traces for temporal locality, eliminating the need for storing time-step matrices by setting β=0. For spatial locality, it employs a fixed binary projection matrix B to generate local learning signals without weight transposition. The weight update rule combines these traces with a surrogate gradient of the membrane potential. The method uses a VGG-9 architecture, Adam optimizer (lr=0.001), and trains for 200 epochs with specific LIF parameters (γ=0.5, vth=0.6) and eligibility trace constants (λpre=0.5, λpost=0.2).

## Key Results
- Matches BPTT accuracy on CIFAR10, CIFAR100, and DVS Gesture datasets
- Achieves 205-661× fewer MAC operations compared to BPTT
- Uses 3-10× less memory than BPTT across all tested datasets
- Only 1.4% accuracy drop on CIFAR10-DVS compared to BPTT

## Why This Works (Mechanism)

### Mechanism 1: Temporal Locality via Decoupled Eligibility Traces
- **Claim**: If eligibility traces are formulated as decoupled pre- and post-synaptic filters rather than synaptic matrices, memory complexity decouples from the sequence length T, enabling constant memory accumulation over time.
- **Mechanism**: TESS maintains two recurrent variables per layer: q^(l)[t] (a low-pass filter of input spikes) and h^(l)[t] (a trace of post-synaptic membrane potentials). By setting the inter-trace decay β=0, the system avoids storing a history matrix O(n^2). Instead, the weight update ΔW is computed instantaneously at every time step using the current trace states.
- **Core assumption**: Assuming that the temporal credit assignment usually solved by unrolling time in BPTT can be approximated by modulating instantaneous traces with a learning signal.
- **Evidence anchors**:
  - [Section III, a)]: "we restrict the formulation to instantaneous eligibility traces by setting β=0... reduces the memory complexity to O(n) by independently tracking pre- and post-synaptic activity traces."
  - [Section III-B1]: "MemTESS = 2 ∑ n(l)... independent of the number of time steps."
  - [Corpus]: Neighbor paper "Traces Propagation" supports the viability of forward-only propagation for memory efficiency, though specific implementation details differ.
- **Break condition**: If the task requires credit assignment across time horizons significantly longer than the decay constants λpre or λpost (e.g., thousands of steps), the trace decays may erase gradient information before it can be utilized.

### Mechanism 2: Spatial Locality via Fixed Random Projections
- **Claim**: If hidden layers receive error signals projected via fixed, random binary matrices rather than learned weight transposes, spatial credit assignment can occur without global backpropagation.
- **Mechanism**: A Learning Signal Generation (LSG) block projects local output spikes o^(l)[t] into a C-dimensional subspace using a fixed matrix B^(l) (square waves). It computes a local error against the global target y* and projects this error back to the neuron dimension. This simulates a top-down modulatory signal without requiring the weights W to be transposed or propagated backwards.
- **Core assumption**: Assumption: Neurons can align their activity to minimize a global loss even if the feedback path is a fixed, random projection (hypothesis analogous to Feedback Alignment).
- **Evidence anchors**:
  - [Section III, b)]: "TESS introduces a local mechanism... eliminating the need for global error propagation... unlocking scalability."
  - [Section IV-B2]: TESS outperforms S-TLLR on DVS Gesture and CIFAR100, suggesting the fixed feedback effectively drives learning.
  - [Corpus]: Corpus evidence for fixed projections in SNNs is supported by "Traces Propagation" and "Stochastic Quantum Spiking," but validation on deep architectures (VGG-9) is primarily limited to this paper's results.
- **Break condition**: If the network depth increases significantly beyond the VGG-9 architecture tested, the fixed random projections may fail to provide sufficiently informative gradients to early layers (the "diminishing signal" problem).

### Mechanism 3: Surrogate Gradient Interpolation via Membrane Potential
- **Claim**: If the spike activation function is approximated by a differentiable surrogate (triangular function) during the backward pass, local three-factor learning rules can approximate gradient descent.
- **Mechanism**: The term Ψ(u^(l)[t]) acts as a proxy for the derivative of the spike threshold. When combined with the local learning signal m^(l)[t] and pre-synaptic trace q^(l)[t], it forms a local update: ΔW ≈ m · Ψ(u) · q.
- **Core assumption**: Assuming the triangular approximation of the gradient around the threshold is sufficiently wide to capture membrane potential fluctuations but narrow enough to exclude noise.
- **Evidence anchors**:
  - [Section IV-A2]: "The secondary activation function was chosen to be a triangular function, defined as Ψ(u) = 0.3 · max(1 - |u - vth|, 0)."
  - [Section III, c)]: Defines the weight update interaction.
- **Break condition**: If the membrane potential operates far from the threshold vth (outside the 0.3 range), the surrogate gradient vanishes, and learning stops for those neurons.

## Foundational Learning

- **Concept: Leaky Integrate-and-Fire (LIF) Dynamics**
  - **Why needed here**: TESS relies on the specific state variables of LIF neurons (u[t] for membrane potential, o[t] for spikes) to construct the eligibility traces. The decay factor γ and reset mechanism directly influence the trace calculations.
  - **Quick check question**: Can you explain how the reset mechanism u[t] - vth·o[t] affects the derivative of the spike output?

- **Concept: Credit Assignment Problem (Spatial vs. Temporal)**
  - **Why needed here**: The core contribution of TESS is solving these two problems locally. You must distinguish between assigning blame across layers (spatial) versus assigning blame across time steps (temporal) to understand why TESS separates the LSG (spatial) from traces (temporal).
  - **Quick check question**: In a video classification task, does TESS solve the "which frame caused the error" problem using the LSG or the eligibility traces?

- **Concept: Three-Factor Learning Rules**
  - **Why needed here**: TESS is explicitly a three-factor rule. Understanding that weight updates require (1) Pre-synaptic activity, (2) Post-synaptic state, and (3) a Modulatory signal is essential for debugging the update logic.
  - **Quick check question**: Identify the three factors in the TESS update equation (Eq. 10). Which one carries the target information?

## Architecture Onboarding

- **Component map**: Input x -> LIF layers -> Trace Accumulator (q, h) -> LSG Block (B) -> Update Logic (ΔW) -> Output o

- **Critical path**: The trace update q^(l)[t] = λpre·q^(l)[t-1] + o^(l-1)[t] must complete within the time step t to enable immediate weight updates. The LSG computation (B · o) must also be low-latency as it occurs at every step where t ≥ tl.

- **Design tradeoffs**:
  - **Non-causal term (αpost)**: Including αpost (setting it to ±1) improves accuracy on CIFAR10/100 (+1.8% gain) but doubles the local memory requirement for trace storage (from 1 vector to 2 per neuron type). Set to 0 for strict memory constraints.
  - **Basis Matrix (B)**: Using square waves is hardware-efficient but theoretically less expressive than learned projections.

- **Failure signatures**:
  - **Zero gradients**: If the threshold vth is set too high or low, causing membrane potentials to sit outside the surrogate gradient window (width 0.3).
  - **Divergence**: If the LSG projection B is not quasi-orthogonal (e.g., random unstructured noise), neurons may receive conflicting error signals.

- **First 3 experiments**:
  1. **Sanity Check (MNIST/DVS Gesture)**: Implement TESS on a shallow network to verify the LSG block produces valid learning signals. Compare accuracy against a standard BPTT baseline.
  2. **Ablation on αpost**: Run CIFAR10 with αpost ∈ {-1, 0, +1}. Confirm the memory-vs-accuracy tradeoff reported in Table II (expect ~2x memory reduction when αpost=0).
  3. **Memory Scaling Test**: Profile memory usage while increasing time steps T on a sequence task. Verify memory usage remains flat (O(Ln)) unlike BPTT (O(TLn)).

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond the general limitations of local learning rules and the need for further validation on diverse architectures and tasks.

## Limitations
- **Fixed Projection Matrix Construction**: The exact method for constructing the quasi-orthogonal square wave basis matrix B is not specified in detail, which could affect reproducibility.
- **Architecture Specificity**: Results are primarily validated on VGG-9 architecture, with no experiments showing TESS performance on other network architectures or different neuron models.
- **Temporal Credit Assignment Horizon**: TESS's effectiveness for tasks requiring long-term temporal dependencies (beyond the 10-20 timesteps tested) is unknown.

## Confidence
- **High Confidence**: Memory efficiency claims (205-661× fewer MACs, 3-10× lower memory usage vs BPTT) - these are straightforward calculations based on the algorithm's design and clearly demonstrated.
- **Medium Confidence**: Accuracy claims on CIFAR10, CIFAR100, and DVS Gesture - results are presented with statistical evidence, but the fixed projection matrix construction uncertainty affects reproducibility.
- **Medium Confidence**: The spatial locality mechanism (fixed random projections) - supported by comparisons to S-TLLR and related work, but the fixed projection assumption hasn't been tested across diverse architectures.
- **Low Confidence**: Performance on long-sequence tasks (beyond 20 timesteps) - not tested in the paper, and theoretical limitations exist due to trace decay.

## Next Checks
1. **Projection Matrix Sensitivity Analysis**: Systematically vary the quasi-orthogonality properties of matrix B (different binary patterns, orthogonality levels) and measure impact on CIFAR10/100 accuracy to determine robustness requirements.
2. **Architecture Generalization Test**: Implement TESS on a ResNet-18 architecture and compare accuracy/memory efficiency against the VGG-9 results to assess architecture independence.
3. **Temporal Horizon Extension**: Evaluate TESS on a sequence classification task requiring 50-100 timesteps (e.g., extended DVS sequences or audio data) to identify the practical limits of temporal credit assignment.