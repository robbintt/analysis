---
ver: rpa2
title: 'On the Origins of Sampling Bias: Implications on Fairness Measurement and
  Mitigation'
arxiv_id: '2503.17956'
source_url: https://arxiv.org/abs/2503.17956
tags:
- training
- size
- group
- protected
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the issue of sampling bias in machine learning
  fairness measurement and mitigation. The authors introduce two variants of sampling
  bias: sample size bias (SSB) and underrepresentation bias (URB).'
---

# On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation

## Quick Facts
- **arXiv ID:** 2503.17956
- **Source URL:** https://arxiv.org/abs/2503.17956
- **Reference count:** 40
- **Key outcome:** This paper demonstrates that sampling bias significantly impacts fairness measurement and mitigation, showing that metrics combining sensitivity and specificity (AUC, ZOL) are more resilient to limited or imbalanced data, and that collecting more samples for underrepresented groups typically amplifies rather than reduces discrimination.

## Executive Summary
This paper systematically investigates how sampling bias affects fairness measurement and mitigation in machine learning. The authors identify two key types of sampling bias: Sample Size Bias (SSB) where training data is limited, and Underrepresentation Bias (URB) where one group is proportionally smaller. Through extensive experiments on benchmark datasets using Logistic Regression and other algorithms, they demonstrate that certain fairness metrics are more robust than others under these conditions. The study reveals that discrimination is often underestimated in small datasets and that naive data augmentation can actually worsen disparities. Based on these findings, the paper provides actionable recommendations for practitioners working with fairness-sensitive applications.

## Method Summary
The study employs three benchmark tabular datasets (Adult, COMPAS, Dutch Census) and systematically manipulates them to create controlled bias conditions. For SSB experiments, models are trained on increasing dataset sizes (10-2000 samples) with 30 random subsets per size. For URB experiments, total training size is fixed at 1000 samples while varying the ratio of sensitive groups from extreme (0.01) to balanced. The primary algorithm used is Logistic Regression, with Decision Trees and Random Forests as baselines. The authors evaluate discrimination using multiple metrics (FPR, Equal Opportunity, AUC, ZOL) and test mitigation techniques including Reweighing and GerryFairClassifier. They also compare random vs. selective data augmentation strategies for underrepresented groups.

## Key Results
- Metrics combining sensitivity and specificity (AUC, ZOL) are more resilient to limited size or imbalanced training sets than single-threshold metrics.
- Discrimination is often underestimated in small datasets and revealed as data grows, rather than being amplified.
- Collecting more random samples for underrepresented groups typically amplifies discrimination rather than reducing it.
- Balancing data with respect to outcomes (not just representation) is a more effective solution for reducing discrimination.
- The efficiency of pre-processing and in-processing mitigation techniques is diluted by limited or imbalanced training data.

## Why This Works (Mechanism)

### Mechanism 1: Metric Robustness via Precision-Recall Trade-offs
- **Claim:** Metrics combining sensitivity and specificity (AUC, Zero-One Loss) provide more reliable discrimination estimates than single-threshold metrics (FPR, Equal Opportunity) when training data is limited or imbalanced.
- **Mechanism:** Single metrics are prone to skewness in small datasets where outcomes may cluster disproportionately. Metrics like AUC calculate performance across all classification thresholds, effectively averaging out the variance caused by specific sample distributions.
- **Core assumption:** The discrimination signal is consistent across thresholds.
- **Evidence anchors:** [abstract] "metrics combining sensitivity and specificity (AUC, ZOL) are more resilient to limited size or imbalanced training sets."
- **Break condition:** If the model fails to learn any signal (random guessing), AUC remains at 0.5 regardless of bias, masking disparity.

### Mechanism 2: Discrimination Attenuation via Feature Suppression
- **Claim:** Training on small datasets causes models to underestimate true discrimination rather than amplify it, provided the population is initially biased.
- **Mechanism:** In limited data regimes, standard learners act as regularizers; they may fail to assign significant weight to sensitive attributes because the signal-to-noise ratio is too low. As data increases, the model's confidence grows, allowing it to leverage the sensitive attribute more heavily.
- **Core assumption:** The underlying population data contains a strong, biased correlation between the sensitive attribute and the outcome.
- **Evidence anchors:** [section 5.1, Obs3] "for small training data samples, the contribution of the sensitive attribute to the model is not significant... As the training data sample is getting larger... discrimination is higher."
- **Break condition:** If the learning algorithm is forced to overfit (low regularization), it may latch onto noise, potentially destabilizing this attenuation effect.

### Mechanism 3: Amplification via Distributional Reinforcement
- **Claim:** Randomly collecting more samples for an underrepresented group typically amplifies discrimination rather than reducing it.
- **Mechanism:** If the underrepresented group's data reflects a biased distribution, increasing the sample count merely reinforces the statistical link between the sensitive attribute and the negative outcome.
- **Core assumption:** The new data follows the same distribution as the existing underrepresented data.
- **Evidence anchors:** [abstract] "collecting more data samples for underrepresented groups typically amplifies discrimination rather than reduces it."
- **Break condition:** Selective augmentation (sampling specifically to balance positive/negative outcomes within the group) breaks this mechanism.

## Foundational Learning

- **Concept: Sensitivity vs. Specificity Trade-off**
  - **Why needed here:** To understand why AUC (which captures both) is stable while metrics like FPR (specificity-adjacent) or TPR (sensitivity) fluctuate wildly in small data.
  - **Quick check question:** If a dataset has only 10 samples and 9 are negative, why might Accuracy be misleading but AUC remain informative?

- **Concept: Feature Importance/Attribution (e.g., SHAP)**
  - **Why needed here:** The paper uses feature importance to explain why discrimination appears "hidden" in small models; the model literally ignores the sensitive feature until enough data validates its use.
  - **Quick check question:** Does a drop in a model's accuracy imply the sensitive attribute became more or less important to the prediction?

- **Concept: Representation vs. Outcome Balancing**
  - **Why needed here:** Crucial for the data augmentation strategy. Understanding the difference between "50% Group A / 50% Group B" (Representation) vs "50% Positive / 50% Negative within Group B" (Outcome).
  - **Quick check question:** If you balance the number of men and women in a dataset, but men have a 90% success rate and women 10%, have you solved the disparity?

## Architecture Onboarding

- **Component map:** Data Layer -> Sampling Engine -> Model Layer -> Evaluation Layer
- **Critical path:** 1. Generate biased sample (SSB or URB). 2. Train model. 3. Measure discrimination gap (Disc). 4. Compare Disc against the "Population" reference.
- **Design tradeoffs:**
  - Metric Stability vs. Granularity: AUC/ZOL are stable but may obscure where the error lies compared to Equalized Odds.
  - Reweighing vs. Data Efficiency: Reweighing fixes URB but struggles with SSB; GerryFairClassifier can fail catastrophically under extreme URB.
- **Failure signatures:**
  - Inverse Scaling: Discrimination increasing as data size grows (indicates initial attenuation).
  - Mitigation Dampening: Bias mitigation techniques having near-zero effect on very small datasets (<100 samples).
- **First 3 experiments:**
  1. SSB Stress Test: Train on n=20 vs n=2000. Plot Discrimination (Disc_FPR) vs Size. Observe if the line slopes up (attenuation) or down (amplification).
  2. Metric Robustness Check: Using the URB setup (99% vs 1% split), compare the variance of AUC vs FPR across 30 runs. Verify AUC variance is lower.
  3. Augmentation A/B Test: Fix URB. Run two augmentation arms: (A) Random oversampling of minority, (B) Selective oversampling (positive outcomes only). Compare resulting Disc_SD.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed resilience of ranking-based metrics (AUC, ZOL) and the attenuation effects of sampling bias hold for deep learning architectures in non-tabular domains?
- Basis in paper: The authors focus exclusively on tabular benchmark datasets and mainstream classifiers while acknowledging that related work in computer vision handles bias differently but does not test those architectures here.
- Why unresolved: The behavior of deep neural networks regarding feature importance and sampling bias may differ significantly from the convex or tree-based models tested in this study.
- What evidence would resolve it: Replication of the SSB and URB experiments using deep neural networks on high-dimensional datasets (e.g., CelebA or image corpora) to observe if AUC/ZOL remain robust.

### Open Question 2
- Question: Can a theoretical or empirical lower bound on sample size be established to ensure that bias mitigation techniques are effective and not "diluted" by variance?
- Basis in paper: The authors conclude that the efficiency of pre-processing and in-processing mitigation techniques is "diluted" by limited or imbalanced training data, suggesting a dependency on m that is characterized graphically but not formally bounded.
- Why unresolved: The paper demonstrates that dilution occurs but does not define the specific conditions or thresholds (e.g., m > X) required for these techniques to reliably reduce discrimination.
- What evidence would resolve it: A theoretical analysis or extended empirical sweep identifying the "break-even" point where the variance of the mitigation algorithm decreases sufficiently to outperform the biased baseline.

### Open Question 3
- Question: How does the selective data augmentation strategy impact the predictive accuracy and calibration of the model for the protected group?
- Basis in paper: The paper recommends balancing data with respect to outcomes rather than random oversampling to reduce discrimination, but it focuses primarily on the gap (discrimination) rather than the absolute performance or potential overfitting effects on the protected class.
- Why unresolved: While reducing the gap is a fairness goal, it is unclear if this specific augmentation strategy compromises the model's fidelity or reliability for the underrepresented group.
- What evidence would resolve it: Experiments measuring precision, recall, and Brier scores for the protected group specifically under the selective augmentation regime compared to the random augmentation regime.

## Limitations

- The study focuses exclusively on Logistic Regression as the primary algorithm, with all major conclusions drawn from this single model family, raising questions about generalizability to other architectures.
- The experimental setup uses artificially created extreme imbalances (90% vs 10% positive rates) which may not reflect real-world distributions where discrimination manifests differently.
- The paper does not explore the interaction between sampling bias and model architecture choices (depth, regularization strength) beyond basic algorithm selection.

## Confidence

- **High Confidence:** The core finding that AUC and ZOL are more stable metrics than FPR/Equal Opportunity under data scarcity - this follows directly from the mathematical properties of these metrics.
- **Medium Confidence:** The claim that discrimination is underestimated in small datasets and revealed as data grows - while the experiments support this, the mechanism's dependency on Logistic Regression's specific regularization behavior warrants further testing.
- **Medium Confidence:** The amplification of discrimination through random data collection - this is well-supported but assumes the added data follows the same biased distribution as existing data.

## Next Checks

1. **Algorithm Generalization Test:** Repeat the SSB experiments using a decision tree with no regularization and a highly regularized model to test whether the attenuation effect depends on the learning algorithm's capacity constraints.

2. **Distribution Shift Validation:** Test whether the amplification mechanism holds when the augmented data comes from a different distribution (e.g., sampling from the full population rather than the biased minority subset).

3. **Metric Sensitivity Analysis:** Systematically vary the decision threshold for FPR/Equal Opportunity metrics to determine if the observed instability is due to threshold selection rather than fundamental metric properties.