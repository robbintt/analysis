---
ver: rpa2
title: A Novel Metric for Detecting Memorization in Generative Models for Brain MRI
  Synthesis
arxiv_id: '2509.16582'
source_url: https://arxiv.org/abs/2509.16582
tags:
- memorization
- deepssim
- similarity
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepSSIM, a novel self-supervised metric
  designed to detect memorization in generative models for brain MRI synthesis. The
  method projects images into a learned embedding space where cosine similarity matches
  ground-truth SSIM scores, enabling efficient and scalable similarity assessment
  without requiring precise spatial alignment.
---

# A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis

## Quick Facts
- arXiv ID: 2509.16582
- Source URL: https://arxiv.org/abs/2509.16582
- Reference count: 31
- Primary result: DeepSSIM improves macro F1 scores by +52.03% over best competing memorization metric for brain MRI generative models

## Executive Summary
This paper introduces DeepSSIM, a self-supervised metric for detecting memorization in generative models synthesizing brain MRI scans. The method learns a 256-dimensional embedding space where cosine similarity approximates ground-truth SSIM scores, enabling efficient similarity assessment without pixel-level alignment. By incorporating structure-preserving augmentations, DeepSSIM captures fine-grained anatomical features critical for evaluating memorization. The approach is evaluated on brain MRI and chest X-ray datasets, demonstrating state-of-the-art performance and strong generalization to cross-modality data.

## Method Summary
DeepSSIM trains a ConvNeXt Base backbone to map image pairs into an embedding space where cosine similarity matches ground-truth SSIM scores computed on registered images. The model uses structure-preserving augmentations (flips, rotations, contrast shifts) during training to enable spatial robustness at inference. After training on 144M+ real-synthetic pairs, DeepSSIM embeds all training and generated images, uses FAISS for efficient nearest-neighbor search, and classifies pairs into duplicate/similar/different categories using two thresholds (α=0.6, β=0.85). The memorization score is the percentage of training images with at least one duplicate in the generated set.

## Key Results
- DeepSSIM achieves +52.03% improvement in macro F1 score over best competing memorization metric
- Outperforms state-of-the-art methods (SSCD, CLIP) by significant margins on brain MRI dataset
- Demonstrates strong cross-modality generalization on chest X-ray data with minimal performance drop
- Offers computational efficiency through embedding-based similarity search versus pixel-level SSIM computation

## Why This Works (Mechanism)

### Mechanism 1
- Learning an embedding space where cosine similarity correlates with SSIM enables scalable memorization detection without pixel-level alignment
- Core assumption: SSIM computed on registered images provides valid ground-truth proxy for anatomical similarity
- Evidence: Equation (4) shows MSE loss between cosine similarity and SSIM; weak corpus support from neighbor papers on generative synthesis approaches
- Break condition: If SSIM fails to capture clinically meaningful anatomical similarities in brain MRI

### Mechanism 2
- Structure-preserving augmentations enable robust similarity estimation under spatial misalignment
- Core assumption: Selected augmentations preserve anatomical identity of brain MRI scans
- Evidence: Abstract states augmentations allow similarity estimation without precise spatial alignment
- Break condition: Aggressive augmentations that alter anatomical semantics would corrupt the similarity signal

### Mechanism 3
- Threshold-based triage (different/similar/duplicate) enables granular memorization quantification beyond binary detection
- Core assumption: Threshold values generalize across datasets and generative model configurations
- Evidence: Equation (5) defines classification function; threshold sensitivity analysis shows gradual F1 degradation with noise
- Break condition: If similarity score distributions shift significantly across modalities or architectures, fixed thresholds may misclassify pairs

## Foundational Learning

- **Structural Similarity Index (SSIM)**
  - Why needed: DeepSSIM's training objective directly regresses to SSIM scores; understanding SSIM's components clarifies what "similarity" the embedding space learns
  - Quick check: Can you explain why SSIM remains relatively stable under certain image degradations, and why this might be both a limitation and an advantage for memorization detection?

- **Self-Supervised Learning with Proxy Tasks**
  - Why needed: DeepSSIM uses SSIM as self-supervisory signal rather than manual labels; enables training at scale without annotation bottlenecks
  - Quick check: How does using SSIM as proxy task differ from contrastive learning approaches like SimCLR or CLIP-style training?

- **Cosine Similarity in Embedding Spaces**
  - Why needed: Embedding space explicitly trained so cosine similarity ≈ SSIM; enables efficient nearest-neighbor search via FAISS
  - Quick check: Why is cosine similarity preferred over Euclidean distance for high-dimensional semantic matching, and what normalization assumptions does this require?

## Architecture Onboarding

- **Component map:** ConvNeXt Base backbone (pretrained, first layers frozen, classification head replaced with identity) → 256-dim embedding → FAISS nearest-neighbor search → threshold-based classification

- **Critical path:** Backbone selection (ConvNeXt reduces MAE by 43% vs. best ResNet) → augmentation design (must preserve anatomy) → threshold calibration (directly impacts F1 and memorization rate estimates)

- **Design tradeoffs:** SSIM ground-truth requires image registration during training; inference does not. Three-class granularity vs. simplicity requires two thresholds to tune. Domain-specific training improves performance but requires training data access.

- **Failure signatures:** Embedding collapse (monitor MAE and Silhouette Score). Threshold drift (re-run grid search or validate threshold sensitivity). Augmentation leakage (audit augmentation pipeline with expert review).

- **First 3 experiments:**
  1. Train DeepSSIM with ResNet-18/34/50/101 and ConvNeXt Base on same data; compare MAE between predicted and ground-truth SSIM to confirm ConvNeXt superiority
  2. Apply anatomy-preserving transformations to test pairs and compare DeepSSIM vs. SSIM performance degradation (Table 1B shows SSIM drops to 30.32% F1 while DeepSSIM maintains 81.00%)
  3. Train DeepSSIM on brain MRI, evaluate on chest X-ray (or vice versa) to assess transferability; re-training on target domain expected to improve performance

## Open Questions the Paper Calls Out

- Can DeepSSIM be effectively extended to process full 3D volumetric data rather than 2D slices?
- Can DeepSSIM be utilized as an active regularization term during training to mitigate memorization?
- Does training on SSIM cause DeepSSIM to miss "semantic" memorization where structural similarity is low but sensitive content is preserved?

## Limitations

- Reliance on SSIM as ground-truth similarity measure may inherit SSIM's limitations in capturing clinically meaningful anatomical variations
- Fixed threshold values (α=0.6, β=0.85) may not generalize across different generative models or data distributions without recalibration
- Domain-specific training requirement limits applicability when training data is unavailable

## Confidence

- **High confidence:** The embedding space learning mechanism (cosine similarity trained to match SSIM) is technically sound and well-specified
- **Medium confidence:** Cross-modality generalization claims are supported by the chest X-ray experiment but would benefit from additional validation on diverse imaging modalities
- **Low confidence:** The memorization detection threshold calibration's robustness across different LDM architectures and training configurations remains untested

## Next Checks

1. **Clinical validity audit:** Have neuroradiologists validate that "duplicate" classifications correspond to clinically significant memorization rather than acceptable anatomical variation
2. **Threshold generalization study:** Systematically vary α and β across multiple generative model architectures to quantify sensitivity to threshold drift
3. **Computational efficiency benchmarking:** Compare DeepSSIM's inference time and memory requirements against SSIM baseline on datasets of increasing scale (10K, 100K, 1M pairs)