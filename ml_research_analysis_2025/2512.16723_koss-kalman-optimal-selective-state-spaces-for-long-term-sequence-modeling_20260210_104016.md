---
ver: rpa2
title: 'KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling'
arxiv_id: '2512.16723'
source_url: https://arxiv.org/abs/2512.16723
tags:
- koss
- state
- input
- kalman
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KOSS, a Kalman-optimal Selective State Space
  model that formulates selection as latent state uncertainty minimization using Kalman
  filtering principles. KOSS employs a continuous-time latent update modulated by
  a Kalman gain that dynamically filters information based on both content and context,
  addressing limitations of input-only selection mechanisms.
---

# KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling

## Quick Facts
- arXiv ID: 2512.16723
- Source URL: https://arxiv.org/abs/2512.16723
- Reference count: 22
- Primary result: KOSS reduces MSE by 2.92-36.23% across nine long-term forecasting benchmarks while achieving 79% accuracy on selective copying with distractors

## Executive Summary
KOSS introduces a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization using Kalman filtering principles. Unlike input-only selective mechanisms, KOSS employs a continuous-time latent update modulated by a Kalman gain that dynamically filters information based on both content and context. The model uses global spectral differentiation for stable frequency-domain derivative estimation and a segment-wise scan for efficient computation. KOSS achieves state-of-the-art performance on long-term time series forecasting tasks while demonstrating superior robustness to distractors in selective copying tasks.

## Method Summary
KOSS extends selective state space models by deriving the selection mechanism from Kalman filtering theory, where the Kalman gain minimizes latent state uncertainty. The model computes an innovation term (prediction error) that serves as a proxy for optimal gain, enabling context-aware selectivity. A Spectral Differentiation Unit estimates input derivatives via FFT for stable continuous-time updates. To handle state-dependent dynamics that prevent standard parallel scans, KOSS employs a segment-wise approach that balances expressiveness and efficiency. The architecture integrates these components into a unified framework that processes sequences through modulated state transitions.

## Key Results
- Reduces MSE by 2.92-36.23% across nine long-term forecasting benchmarks
- Achieves over 79% accuracy on selective copying with distractors (baselines below 20%)
- Outperforms state-of-the-art models in both accuracy and stability
- Real-world SSR case study confirms robustness under irregular intervals and noisy conditions

## Why This Works (Mechanism)

### Mechanism 1: Innovation-Driven Context-Aware Selectivity
The model computes innovation (prediction error) as a proxy for optimal Kalman gain, enabling selection based on both input content and latent state context. This distinguishes relevant signals from correlated distractors through state-aware filtering.

### Mechanism 2: Spectral Differentiation for Stable Derivative Estimation
Global frequency-domain differentiation via FFT provides more stable gradient estimates than local finite differences, reducing noise sensitivity in continuous-time SSM updates.

### Mechanism 3: Segment-wise Parallel Scan for Recurrence Decoupling
Sequence segmentation enables hardware-efficient parallel training while preserving state-dependent dynamics through recursive inter-segment updates, balancing modeling fidelity and computational efficiency.

## Foundational Learning

- **Concept: Kalman Filtering (Innovation & Gain)** - Understanding that Kalman Gain balances prediction vs. measurement trust, and Innovation quantifies observation error, is essential for grasping context-aware selectivity. *Quick check*: If Innovation is consistently near zero, the state relies mostly on prediction.
- **Concept: Spectral Differentiation (Fourier Domain)** - The SDU module requires understanding why multiplying by jω in frequency domain equals differentiation in time domain. *Quick check*: Why does FFT differentiation amplify high-frequency noise?
- **Concept: Linear Time-Varying (LTV) Systems** - KOSS has time-varying matrices unlike standard SSMs, requiring understanding of LTV vs. LTI differences. *Quick check*: Why can't simple global convolution process LTV systems?

## Architecture Onboarding

- **Component map**: Input Layer -> SDU (derivative estimation) -> Nonlinear Module (Innovation processing) -> Parameter Modulation (A_K, B_K computation) -> Segment-wise Scan (recurrence execution) -> Output Layer
- **Critical path**: Innovation calculation (x - C·h_prev) is the most critical divergence from standard SSMs; errors here reduce the model to input-only selection
- **Design tradeoffs**: Segment length S balances theoretical fidelity (small S) vs. training speed (large S); spectral cutoff in SDU balances stability vs. signal fidelity
- **Failure signatures**: Training instability from exploding Innovation terms; performance collapse on distractors indicating degenerated input-only selection
- **First 3 experiments**: 1) Replace SDU with finite differences to verify derivative stability improvement, 2) Ablate context feedback in IDS to confirm performance drop on selective copying, 3) Profile memory/throughput while sweeping segment length S

## Open Questions the Paper Calls Out

### Open Question 1
Can KOSS generalize to non-time-series modalities like speech, genomics, and multimodal reasoning? The paper plans to extend to these areas where context-aware integration is pivotal, but current evaluation is limited to LTSF and radar tracking.

### Open Question 2
Does the steady-state Kalman gain approximation limit tracking of rapidly changing dynamics compared to full Riccati solutions? While convergence is proven for linear systems, the non-linear context learning error remains unquantified.

### Open Question 3
How does segment-wise scan computational efficiency scale to billion-parameter LLM scales? Current benchmarks use 0.2M parameters, much smaller than typical Transformer or Mamba scales.

## Limitations

- Nonlinear mapping φ(Innov) → K lacks architectural detail, complicating exact replication
- Fixed segment length S=16 may not generalize across datasets with varying optimal values
- SDU frequency truncation parameter ω_cut is unspecified, potentially affecting stability across different noise profiles

## Confidence

- **High**: MSE reduction claims (2.92-36.23%) on nine LTSF benchmarks with clear numerical improvements
- **Medium**: Innovation-driven selectivity mechanism - theoretically sound but lacks ablation showing degradation when disabled
- **Medium**: Spectral differentiation stability claims - novel integration with limited comparative evidence
- **Low**: Real-world SSR case study - single application without statistical validation

## Next Checks

1. **Ablation of Innovation-Driven Selectivity**: Disable context feedback in IDS module (set Innovation=0) and verify performance collapse on selective copying task
2. **SDU Frequency Response Analysis**: Test SDU on signals with known high-frequency components to quantify information loss from spectral truncation across different ω_cut values
3. **Segment Length Sensitivity**: Systematically vary S from 1 to full sequence length on representative LTSF dataset to map computational efficiency vs. modeling accuracy trade-off