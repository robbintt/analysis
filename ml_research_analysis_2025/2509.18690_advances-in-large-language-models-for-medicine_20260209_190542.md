---
ver: rpa2
title: Advances in Large Language Models for Medicine
arxiv_id: '2509.18690'
source_url: https://arxiv.org/abs/2509.18690
tags:
- medical
- llms
- language
- large
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the current state of large language
  models (LLMs) in medicine, analyzing training methodologies, healthcare applications,
  evaluation approaches, and future research directions. The authors classify medical
  LLMs into three types based on training methods (pre-training, fine-tuning, prompting)
  and evaluation approaches into machine-based and human-centered methods.
---

# Advances in Large Language Models for Medicine

## Quick Facts
- arXiv ID: 2509.18690
- Source URL: https://arxiv.org/abs/2509.18690
- Authors: Zhiyu Kan; Wensheng Gan; Zhenlian Qi; Philip S. Yu
- Reference count: 40
- Primary result: Systematic survey classifying medical LLMs by training methods and evaluation approaches, identifying key challenges and future research directions.

## Executive Summary
This survey provides a comprehensive analysis of large language models in medicine, examining their training methodologies, healthcare applications, and evaluation approaches. The authors propose a novel three-category taxonomy based on training methods (pre-training, fine-tuning, prompting) and evaluation approaches (machine-based, human-centered). They identify critical challenges including hallucinations, lack of CRUD operations, insufficient unified evaluation benchmarks, and ethical concerns around privacy. The review highlights significant applications across clinical decision support, personalized treatment, medical education, drug discovery, and medical imaging.

## Method Summary
The authors conducted a systematic literature review using Web of Science, DBLP, IEEE Xplore, and Google Scholar databases with the keywords "LLM medicine" and "medical LLM" (2020-2025 priority). They applied inclusion criteria requiring peer-reviewed English publications with innovative methods or comprehensive coverage of LLM training-to-deployment, while excluding non-peer-reviewed work and compilations without original insight. The final corpus consists of 40 papers, from which they developed their classification taxonomy and analyzed trends in publication counts.

## Key Results
- Medical LLMs can be classified into three types based on training methods: pre-training (domain-specific corpora), fine-tuning (supervised/instruction), and prompting (zero/few-shot with chain-of-thought)
- Two evaluation approaches exist: machine-based (accuracy, F1, BLEU, ROUGE) and human-centered (expert review dimensions)
- Major challenges include hallucinations, inability to perform CRUD operations, lack of unified evaluation benchmarks, and privacy/ethical concerns
- Key applications span clinical decision support, personalized treatment, medical education, drug discovery, and medical imaging
- Future directions include enhancing medical terminology understanding, developing multimodal LLMs, fostering interdisciplinary collaboration, creating new evaluation benchmarks, and improving legal frameworks

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific fine-tuning of general-purpose LLMs improves medical task performance compared to unadapted models. Medical corpora contain specialized terminology and reasoning patterns that, when used for supervised or instruction fine-tuning, teach models domain-specific token distributions while preserving general capabilities. This works because the pre-trained model has sufficient capacity to encode medical knowledge without catastrophic forgetting.

### Mechanism 2
Prompt engineering alone can match or exceed fine-tuned medical model performance without parameter updates. Chain-of-thought prompting and in-context learning leverage the model's frozen weights by structuring input to activate relevant reasoning pathways. The model retrieves and applies learned medical knowledge through contextual activation rather than weight modification, working when the base model's pre-training data contained sufficient medical knowledge.

### Mechanism 3
Retrieval-augmented generation reduces hallucinations and improves factual accuracy in medical applications. RAG dynamically retrieves relevant documents from authoritative medical databases and conditions generation on this context, grounding outputs in verifiable sources. This addresses hallucination generation, opaque reasoning patterns, and dependence on obsolete data through real-time knowledge base integration.

## Foundational Learning

- **Concept: Transformer Self-Attention Architecture**
  - Why needed here: All medical LLMs discussed inherit from Transformer-based architectures. Understanding attention mechanisms explains how models capture long-range dependencies in clinical narratives.
  - Quick check question: Can you explain why self-attention handles long medical documents better than RNNs?

- **Concept: Self-Supervised Learning Objectives (MLM, NSP, NTP)**
  - Why needed here: Medical pre-training uses these objectives on domain corpora. Understanding them clarifies what knowledge gets encoded during pre-training versus fine-tuning.
  - Quick check question: What is the difference between masked language modeling and next token prediction, and which does GPT use?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA, Adapters)**
  - Why needed here: Medical LLM development costs are prohibitive (paper notes GPT-4 training exceeded $100M). PEFT methods like LoRA enable practical medical adaptation.
  - Quick check question: How does LoRA reduce trainable parameters while preserving model capacity?

## Architecture Onboarding

- **Component map:**
Medical LLM Pipeline -> Pre-training Phase (Medical corpora with MLM/NTP objectives) -> Adaptation Phase (Fine-tuning with SFT/IFT/PEFT or Prompting with ICL/CoT/RAG) -> Evaluation Phase (Machine metrics or Human expert review)

- **Critical path:** For resource-constrained teams: Start with prompting (zero cost) → Add few-shot examples → Implement RAG if hallucinations persist → Consider LoRA fine-tuning only if prompting insufficient.

- **Design tradeoffs:**
| Approach | Compute Cost | Data Required | Update Flexibility | Best For |
|----------|--------------|---------------|-------------------|----------|
| Full pre-training | Very High | Massive | Low | Organizations with $10M+ budgets |
| Fine-tuning (SFT/IFT) | Moderate | Medium | Low | Specialized tasks with labeled data |
| PEFT (LoRA) | Low | Small | Medium | Budget-constrained specialization |
| Prompting | Near-zero | None | High | Prototyping, fast iteration |
| RAG | Low | None (uses external KB) | High | Knowledge-intensive, current data |

- **Failure signatures:**
  - Hallucination rate >15%: Add RAG or improve prompt grounding
  - Poor medical terminology: Increase domain corpus in fine-tuning
  - Inconsistent outputs across similar queries: Review few-shot example diversity
  - Model refuses medical questions: Safety filters may be overactive; adjust system prompts

- **First 3 experiments:**
  1. **Baseline prompting test:** Use Med-PaLM-style few-shot prompts on 50 medical QA pairs. Measure accuracy against ground truth. Expected: 60-80% depending on base model.
  2. **CoT comparison:** Add chain-of-thought prompting to same task. Assess whether reasoning transparency improves accuracy or merely increases token cost.
  3. **RAG integration pilot:** Connect to PubMed or clinical guidelines API for a constrained domain (e.g., diabetes management). Compare hallucination rates with and without retrieval for 20 queries requiring recent guidelines.

## Open Questions the Paper Calls Out

### Open Question 1
How can new evaluation benchmarks be designed to assess LLMs' adaptability and clinical utility beyond static medical exam accuracy? Current benchmarks rely on static datasets that fail to measure adaptability, scalability, or safety in dynamic clinical environments.

### Open Question 2
What architectures are required for Medical LLMs to effectively integrate and interpret time-series physiological data such as ECGs and PPGs? Most Multimodal LLMs currently prioritize vision and language modalities, leaving a gap in methods to process temporal physiological signals alongside text.

### Open Question 3
Can model architectures be modified to support direct Create, Read, Update, and Delete (CRUD) operations for specific medical knowledge without full retraining? Current unified architectures prevent inspecting or modifying specific memories, forcing time-consuming full retraining for updates.

## Limitations
- **Temporal Scope Limitation**: The review covers 2020-2025, potentially missing foundational work and rapid post-2025 developments.
- **Publication Bias**: Prioritizes peer-reviewed English publications, potentially underrepresenting preprints, non-English work, and industrial applications.
- **Evaluation Benchmark Fragmentation**: Acknowledges lack of unified standards but doesn't systematically compare existing benchmarks' validity or clinical relevance.

## Confidence
- **High Confidence**: Classification of training methodologies and their documented performance differences. Domain-specific fine-tuning improving performance is well-established.
- **Medium Confidence**: The three identified future research directions (terminology understanding, multimodal integration, interdisciplinary collaboration) are logical but their practical feasibility remains uncertain.
- **Low Confidence**: Claims about specific performance thresholds (e.g., "surpassing human clinicians") due to inconsistent evaluation methodologies and publication bias toward positive results.

## Next Checks
1. **Reproduce Taxonomy Classification**: Independently classify 10 randomly selected models from included papers using the proposed training method categories. Calculate inter-rater agreement to validate taxonomy clarity.
2. **Benchmark Consistency Analysis**: Extract evaluation metrics from at least 5 studies covering the same medical task. Compare whether performance differences are statistically significant or reflect different benchmark choices.
3. **Temporal Validation**: Search for literature from the 6 months following the review's cutoff date. Document whether identified trends (multimodal focus, RAG adoption) continue or if new paradigms emerged.