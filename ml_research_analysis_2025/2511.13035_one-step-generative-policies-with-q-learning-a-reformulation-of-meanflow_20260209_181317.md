---
ver: rpa2
title: 'One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow'
arxiv_id: '2511.13035'
source_url: https://arxiv.org/abs/2511.13035
tags:
- uni00000013
- uni00000011
- learning
- uni00000048
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a one-step generative policy for offline reinforcement
  learning that reformulates MeanFlow to enable direct noise-to-action generation
  without requiring multi-step ODE integration or two-stage distillation. The key
  idea is to integrate the velocity field and noise-to-action transformation into
  a single policy network, using a residual formulation that ensures efficient and
  stable Q-learning.
---

# One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow

## Quick Facts
- **arXiv ID:** 2511.13035
- **Source URL:** https://arxiv.org/abs/2511.13035
- **Reference count:** 27
- **Primary result:** Introduces one-step generative policy for offline RL using residual reformulation of MeanFlow, achieving state-of-the-art results across 73 tasks

## Executive Summary
This paper introduces a one-step generative policy for offline reinforcement learning that reformulates MeanFlow to enable direct noise-to-action generation without requiring multi-step ODE integration or two-stage distillation. The key idea is to integrate the velocity field and noise-to-action transformation into a single policy network, using a residual formulation that ensures efficient and stable Q-learning. This approach captures complex multimodal action distributions while maintaining inference efficiency. Experiments on 73 tasks across OGBench and D4RL benchmarks show strong performance in both offline and offline-to-online settings, achieving state-of-the-art results in most tasks and robust adaptation during online fine-tuning.

## Method Summary
The method reformulates MeanFlow by introducing a residual function g(at, b, t) = at - u(at, b, t) that directly maps interpolated noisy actions to target actions in one step. The policy network takes state, noisy action, and time parameters as input, outputting the residual. Training combines MeanFlow Identity loss for behavior cloning with Q-learning loss for policy improvement, using an adaptive behavior cloning coefficient. Value-guided rejection sampling selects the highest-Q action from K candidates to stabilize critic training. The approach avoids the computational complexity of multi-step ODE integration while maintaining the expressivity of flow-based models.

## Key Results
- Achieves state-of-the-art performance across 73 tasks spanning OGBench and D4RL benchmarks
- Shows robust adaptation during online fine-tuning after offline training
- Demonstrates superior performance-efficiency tradeoff compared to Gaussian policies and two-stage flow approaches
- Effectively captures multimodal action distributions while maintaining inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
The residual reformulation g(at, b, t) = at - u(at, b, t) enables stable, single-stage, one-step action generation for Q-learning. By defining the policy output as a residual around the interpolated noisy action at, the network learns the full noise-to-action mapping directly. This avoids the decoupled velocity-estimation-then-integration pipeline of naive MeanFlow, which causes out-of-bound actions and Q-target mismatch during early training. The MeanFlow Identity holds, and a neural network can approximate the required residual function under the conditions of the Universal Approximation Theorem.

### Mechanism 2
Value-guided rejection sampling (selecting the highest-Q action from K candidates) stabilizes critic training and improves policy performance. Generating K actions in parallel and selecting the best according to the current Q-function reduces the variance of the target action in the Bellman update. This acts as an in-silico form of policy improvement within the critic update step. The Q-function provides a sufficiently reliable signal to rank candidate actions, and the overhead of generating K actions is acceptable when mitigated by parallel sampling.

### Mechanism 3
Adaptive behavior cloning coefficient (α) balances imitation and value maximization, leading to more robust convergence. A PID-like controller adjusts α based on the moving average of the BC loss. If the BC loss spikes, α is increased to keep the policy near the data; if it drops too low, α is decreased to allow more Q-driven optimization. The historical BC loss is a good proxy for policy-data divergence, and the fixed scaling factors are broadly applicable across tasks.

## Foundational Learning

- **Concept: Offline Reinforcement Learning & Extrapolation Error**
  - Why needed here: The entire method is designed for the offline RL setting where an agent learns from a fixed dataset. Understanding the core challenge—extrapolation error from evaluating out-of-distribution actions with a Q-function—is essential to grasp why behavior cloning regularization is a non-optional component.
  - Quick check question: Why can't we simply train a Q-function and a policy to maximize it using only data from a fixed dataset, without any regularization?

- **Concept: Flow Matching & MeanFlow**
  - Why needed here: The policy architecture is a reformulation of MeanFlow, which itself builds on Flow Matching. Grasping the core idea—learning a velocity field to transform noise to data—is crucial for understanding the paper's central technical contribution: changing what the network directly outputs (from velocity to a residual action).
  - Quick check question: What is the key difference between traditional Flow Matching and MeanFlow that enables one-step generation?

- **Concept: Q-Learning with Generative Policies**
  - Why needed here: The paper's goal is to make a generative (flow-based) policy compatible with standard Q-learning actor-critic frameworks. One must understand the typical incompatibility (backpropagation through time for multi-step policies) to appreciate the solution's value.
  - Quick check question: What is the primary computational/optimization challenge when trying to train a multi-step diffusion or flow policy using the Q-function's gradient?

## Architecture Onboarding

- **Component map:** State, noisy action, time parameters -> Policy Network (DiT) -> Residual output -> Action selection -> Q-function evaluation
- **Critical path:** 1) Sample data (s,a), noise ε, and times (b,t) 2) Construct at = (1-t)a + tε 3) Compute policy output gθ(s, at, b, t) 4) Compute LMFI using MeanFlow Identity target (requires JVP) 5) Compute LQ = -Qϕ(s, aπ) where aπ = gθ(s, ε) 6) Update policy with LQ + α*LMFI 7) Update critic via TD error using value-guided targets 8) Periodically update α based on BC loss history
- **Design tradeoffs:** Time step sampling (uniform continuous vs discrete), initialization (zero final projection), architecture depth (shallow DiT vs deeper), candidate size K for rejection sampling
- **Failure signatures:** Action clipping/bound loss (naive formulation), mode collapse (underfitting), critic divergence (insufficient BC regularization)
- **First 3 experiments:** 1) Toy dataset validation on multimodal 2D data 2) Ablation on α and K on single OGBench task 3) Baseline comparison against Gaussian policy and two-stage flow policy on 2-3 diverse tasks

## Open Questions the Paper Calls Out

- **Can multi-objective optimization techniques (e.g., Jacobian Descent) replace the current adaptive coefficient mechanism to fully automate the trade-off between behavior cloning and Q-learning?** The authors state future work could explore multi-objective optimization techniques to better balance competing objectives. The current adaptive mechanism relies on sensitive initialization and per-task tuning.

- **How can the computational overhead of Jacobian-vector products (JVPs) be reduced to improve training speed in large-scale settings?** The method relies on computing JVPs, which introduces additional computational overhead compared to simpler Gaussian policies. Algorithmic modifications or approximations could lower wall-clock time per step.

- **Can a universal strategy for sampling time steps (b, t) be derived that prevents training collapse without manual adjustment for task complexity?** The authors currently treat time step configuration as a task-dependent hyperparameter, recommending different settings based on environment. A single fixed configuration that yields consistent performance across tasks would be valuable.

## Limitations

- The method relies on computing Jacobian-vector products, introducing computational overhead compared to simpler policies
- The adaptive behavior cloning coefficient mechanism is heuristic-based without theoretical convergence guarantees
- Time step sampling configuration remains task-dependent, requiring manual adjustment for optimal performance
- Theoretical analysis of convergence guarantees and approximation error bounds is lacking

## Confidence

- **High Confidence**: The empirical performance claims on OGBench and D4RL benchmarks - consistent state-of-the-art results across 73 tasks with clear evaluation methodology
- **Medium Confidence**: The mechanism claims for the residual reformulation - validated against naive baseline but lacks theoretical grounding
- **Low Confidence**: The adaptive BC coefficient mechanism - introduced as heuristic without theoretical analysis, effectiveness may be task-dependent

## Next Checks

1. **Convergence Analysis**: Conduct theoretical analysis of the MeanFlow Identity's implications for the residual function g's convergence properties, including bounds on approximation error as a function of network capacity and dataset size.

2. **BC Coefficient Ablation**: Systematically vary the adaptive BC coefficient's parameters (moving average window, scaling factors, update frequency) across tasks to identify which components are essential versus task-dependent heuristics.

3. **Sparse Reward Transfer**: Test the method on tasks with significantly sparser rewards than the OGBench/D4RL benchmarks (e.g., Atari games from pixels) to evaluate whether the adaptive BC mechanism and value-guided sampling remain effective when Q-function quality degrades.