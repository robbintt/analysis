---
ver: rpa2
title: Improving Industrial Injection Molding Processes with Explainable AI for Quality
  Classification
arxiv_id: '2511.08108'
source_url: https://arxiv.org/abs/2511.08108
tags:
- data
- features
- quality
- feature
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study aimed to improve the interpretability and efficiency
  of machine learning models for quality classification in industrial injection molding.
  A key challenge was the lack of interpretability in ML models and the complexity
  of data acquisition from older machines with limited sensor capabilities.
---

# Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification

## Quick Facts
- arXiv ID: 2511.08108
- Source URL: https://arxiv.org/abs/2511.08108
- Authors: Georg Rottenwalter; Marcel Tilly; Victor Owolabi
- Reference count: 18
- Primary result: Feature reduction from 19 to 9 sensors improved LSTM model accuracy from 86% to 91% while reducing inference time by 7%

## Executive Summary
This study addresses the challenge of interpretability in machine learning models for quality classification in industrial injection molding. Using Explainable AI techniques (SHAP, LIME, Grad-CAM), the researchers identified the most important features from 19 input parameters in an LSTM model. By reducing the feature set to 9 and then to 6 sensors, they evaluated the trade-off between model performance, inference speed, and interpretability. The results demonstrate that targeted feature reduction can enhance model generalization and efficiency, making AI-driven quality control more feasible for industrial settings with limited sensor capabilities.

## Method Summary
The study used time-series sensor data from 1,171 injection molding cycles, augmented to 3,138 training samples with a 67/33 train/validation split. A 3-layer LSTM classifier (300→100→100 units) served as the base model, trained with Adam optimizer (lr=0.001325) for 350 epochs. Three XAI methods (SHAP GradientExplainer, LIME TabularExplainer, and custom Grad-CAM) were applied to rank feature importance across multiple runs. The top 9 and top 6 features were selected based on aggregated importance scores, and the model was retrained with reduced input dimensions to evaluate performance trade-offs.

## Key Results
- Reducing features from 19 to 9 improved validation accuracy from 86% to 91% and F1 score from 89% to 92%
- The 9-feature model reduced inference time by 7% with minimal VRAM impact
- Reducing to 6 features maintained 84% accuracy and 86% F1 score while achieving 13% faster inference
- The 9-feature model showed better generalization than the full 19-feature model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted feature reduction based on post-hoc explainability can improve model generalization by filtering noisy or redundant sensor inputs.
- **Mechanism:** XAI techniques rank features by contribution to decision boundaries. Removing low-ranking features regularizes the model, reducing overfitting to non-informative sensor noise.
- **Core assumption:** Removed features contain more noise or redundant covariance than unique signal relevant to quality classification.
- **Evidence anchors:**
  - After reducing to 9 features, mean validation accuracy increased to 91%, suggesting improved generalization.
  - 19-feature model had lower accuracy (86%) compared to 9-feature model (91%), implying full feature set introduced variance.
  - Neighbor paper supports general viability of XAI in this domain.

### Mechanism 2
- **Claim:** Inference efficiency gains are achieved primarily through input dimensionality reduction rather than model architectural changes.
- **Mechanism:** Reducing input variables streamlines matrix operations in initial LSTM layers. Since architecture remains constant, inference speedup derives from reduced data preprocessing and smaller input tensors.
- **Core assumption:** Bottleneck for inference speed is partially determined by input data volume/processing rather than just network depth.
- **Evidence anchors:**
  - 9-feature model has 7% faster inference time while VRAM consumption is only minimally reduced.
  - Reducing to 6 features maintains 84% accuracy with small increase in inference speed.
  - VRAM consumption is mainly determined by model architecture, not input dimension.

### Mechanism 3
- **Claim:** Consensus across multiple XAI methods stabilizes feature selection by mitigating individual method biases.
- **Mechanism:** The study aggregates importance rankings from SHAP, LIME, and Grad-CAM. Averaging the "Total" frequency count ensures selected features are robust across different interpretability mathematical foundations.
- **Core assumption:** Features identified as important by multiple distinct explanation algorithms are objectively more relevant than those identified by a single algorithm.
- **Evidence anchors:**
  - SHAP and LIME identified largely similar parameters while Grad-CAM prioritized them differently.
  - "Total" column represents combined frequency across methods.
  - Visualizes differing temporal relevance maps, justifying need for aggregation.

## Foundational Learning

- **Concept: Long Short-Term Memory (LSTM) Networks**
  - **Why needed here:** The study uses a 3-layer LSTM as the base classifier because injection molding is a time-series process where sensor readings over time (1800 steps), not just single snapshots, determine quality.
  - **Quick check question:** Can you explain why a standard Feed-Forward Neural Network might fail to capture the "sequence" of pressure changes during an injection cycle?

- **Concept: Post-hoc Explainability (SHAP/LIME)**
  - **Why needed here:** The core contribution relies on using these "after-the-fact" tools to open the "black box" of the LSTM to determine which of the 19 sensors actually matter.
  - **Quick check question:** What is the difference between a local explanation (why *this specific* part was labeled bad) and the global explanation needed to reduce the sensor count for the whole system?

- **Concept: The Bias-Variance Tradeoff**
  - **Why needed here:** The counter-intuitive result that *fewer* features lead to *higher* accuracy (86% -> 91%) is a classic case of reducing variance (overfitting) by removing noisy inputs.
  - **Quick check question:** Why might adding more sensors (features) make a model perform worse on new, unseen validation data?

## Architecture Onboarding

- **Component map:** Data Source -> Preprocessing -> Base Model -> Analysis Layer -> Optimization Engine -> Deployed Model
- **Critical path:** The transition from the Full Model to the Ranked Feature List. If XAI analysis incorrectly ranks a critical sensor as low importance, all subsequent reduced models will fail.
- **Design tradeoffs:**
  - 9 Features: Optimizes for Accuracy (91%) and Interpretability. Best for high-value parts where quality detection is paramount.
  - 6 Features: Optimizes for Inference Speed (13% faster) and Hardware Frugality (older machines). Best for high-speed production where 84% accuracy is acceptable.
  - Memory vs. Speed: Reducing features speeds up computation but saves negligible VRAM (Architecture determines VRAM).
- **Failure signatures:**
  - High Variance in Retraining: 6-feature model showed instability (Std Dev rises), indicating the model is struggling to converge with limited data.
  - Disagreement in XAI: If SHAP and Grad-CAM correlation is near zero, the feature ranking becomes arbitrary.
- **First 3 experiments:**
  1. **Baseline Verification:** Train the LSTM with all 19 features on the provided dataset to reproduce the ~86% accuracy benchmark.
  2. **XAI Consensus Check:** Run SHAP and LIME on the baseline model; verify that "Injection Pressure" and "Actual Clamping Force" appear in the top ranks as per the paper's Table II.
  3. **Ablation Study:** Retrain the model using *only* the bottom 3 features (the ones the paper discarded). Confirm that accuracy crashes, proving the XAI successfully distinguished signal from noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the identified subset of 9 or 6 features effectively train machine learning models using only synthetic data, removing the need for real-world data collection?
- **Basis in paper:** The authors state that "it was investigated whether this process could be simplified through targeted feature reduction" for synthetic data, noting in the conclusion that this establishes a foundation for "more effective generation of synthetic data" which needs further validation.
- **Why unresolved:** While the paper demonstrates that reduced features improve real-data classification, it does not provide experimental results validating that these specific features are sufficient for training models purely on synthetic simulations.
- **What evidence would resolve it:** A comparative study showing that models trained on synthetic data (generated using only the reduced feature set) achieve comparable performance to models trained on real sensor data.

### Open Question 2
- **Question:** How does the ranking of feature importance and the resulting model performance change when applied to injection molding processes with different product geometries or materials?
- **Basis in paper:** The conclusion notes that the "variability observed across training runs underscores the necessity of... employing diverse datasets to ensure the robustness of the system."
- **Why unresolved:** The study utilized a single dataset involving a specific polypropylene box; it is unclear if the 9 identified features (e.g., Injection Pressure, Clamping Force) remain the most critical parameters for different molds or polymers.
- **What evidence would resolve it:** Cross-validation results from experiments conducted on varying mold shapes (e.g., thin-walled parts) and different materials (e.g., ABS or PET) using the same XAI-driven feature reduction pipeline.

### Open Question 3
- **Question:** What specific factors drive the "less stable" performance of the 6-feature model compared to the 9-feature model, and can stability be recovered without adding features?
- **Basis in paper:** The results show the 6-feature model was "less stable" with a standard deviation of 9.24% compared to 5.79% for the 9-feature model, despite the hypothesis that feature reduction generally improves generalization.
- **Why unresolved:** The paper reports the instability (lower accuracy and higher variance) but does not investigate whether this is due to the loss of critical non-linear interactions or the specific exclusion of certain sensor types.
- **What evidence would resolve it:** An ablation study analyzing the variance contributions of the removed features or the application of regularization techniques to the 6-feature model to see if stability matches the 9-feature baseline.

## Limitations
- Complete list of all 19 input feature names remains unknown, limiting full reproducibility
- Data preprocessing details (normalization, handling of missing values) are unspecified
- Custom Grad-CAM implementation for LSTM is not provided
- Quality labeling criteria are not documented

## Confidence
- **High confidence:** General mechanism of XAI-driven feature reduction improving generalization
- **Medium confidence:** Specific accuracy improvements (91%→86%) due to lack of full data specification
- **Low confidence:** Generalizability across different injection molding setups without additional validation

## Next Checks
1. Verify that retraining with only the bottom 3 features (as identified by XAI) causes accuracy to drop significantly, confirming the ranking method's validity
2. Test the 9-feature model on a held-out test set from a different time period to confirm generalization beyond validation data
3. Implement the same feature reduction pipeline on a publicly available time-series dataset (e.g., UCI repository) to test methodology transferability