---
ver: rpa2
title: A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks
arxiv_id: '2511.18269'
source_url: https://arxiv.org/abs/2511.18269
tags:
- fairness
- resource
- schedulers
- each
- substitution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hybrid operations research (OR) and machine
  learning (ML) framework for fair resource substitution in large-scale logistics
  networks. The framework combines an OR model that minimizes network imbalance while
  incorporating fairness metrics (minimax and Gini index) with an ML component that
  learns historical scheduler preferences to reduce the decision space.
---

# A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks

## Quick Facts
- arXiv ID: 2511.18269
- Source URL: https://arxiv.org/abs/2511.18269
- Reference count: 27
- Primary result: Hybrid OR-ML framework achieves 80% model size reduction and 90% execution time decrease while preserving fairness in resource substitution for large-scale logistics networks

## Executive Summary
This paper introduces a hybrid operations research (OR) and machine learning (ML) framework for fair resource substitution in large-scale logistics networks. The framework combines an OR model that minimizes network imbalance while incorporating fairness metrics (minimax and Gini index) with an ML component that learns historical scheduler preferences to reduce the decision space. A dynamic top-κ mechanism assigns arc-specific κ values based on betweenness centrality, replacing static approaches. Applied to a major package delivery company's network, the framework achieved significant computational improvements while preserving optimality and improving fairness metrics.

## Method Summary
The framework operates in two stages: Stage 1 computes the minimal achievable network imbalance I* using a mixed-integer programming (MIP) formulation, while Stage 2 fixes I* as a constraint and optimizes substitution decisions under fairness metrics. An ML component, specifically a deep neural network (DNN) with two hidden layers, predicts resource-arc compatibility based on historical solutions, enabling search space reduction through a top-κ candidate selection mechanism. Dynamic κ assignment based on betweenness centrality allows arcs with higher structural importance to have broader substitution options. The approach generates a Pareto frontier of solutions balancing efficiency and fairness through varying weights on the primary objective.

## Key Results
- Achieved 80% reduction in model size and 90% decrease in execution time compared to baseline MIP approaches
- Redistributed workload more equitably among schedulers, reducing maximum burden by up to 42% with only modest increases in total changes
- Demonstrated that fairness can be achieved without sacrificing global efficiency, though at the cost of higher total disruptions and increased computation time in larger instances
- The approach is broadly applicable to resource rebalancing problems in transportation and logistics

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Decoupled Optimization with Fairness Constraints
Separating imbalance minimization from fairness-constrained substitution enables tractable solutions while preserving optimality on the primary objective. Stage 1 computes minimal achievable imbalance I* using standard MIP. Stage 2 fixes I* as a constraint and optimizes substitution decisions under fairness metrics (minimax or Gini), preventing fairness-optimality tradeoffs on the primary goal.

### Mechanism 2: ML-Guided Search Space Reduction via Top-κ Candidate Selection
A DNN trained on historical solutions can predict resource-arc compatibility, enabling an 80% reduction in binary variables while preserving optimality. The DNN outputs probability vectors over resources for each arc. Selecting top-κ candidates per arc reduces decision variables from |A|×|R| to |A|×κ. Since training data contains only compatible assignments, feasibility is preserved by construction.

### Mechanism 3: Dynamic Top-κ Assignment via Betweenness Centrality
Adapting κ per arc based on network structural importance achieves optimality with lower average κ than static approaches. Arcs with high betweenness centrality (more shortest paths passing through) receive larger κ values, allowing broader substitution exploration. Low-betweenness arcs get smaller κ, prioritizing efficiency where local changes have limited network impact.

## Foundational Learning

- Concept: Mixed-Integer Programming (MIP) Formulation
  - Why needed here: The OR component is a binary MIP; understanding constraint construction, objective linearization (e.g., minimax via auxiliary variable Z), and solver interfaces is essential.
  - Quick check question: Can you linearize `min max_s Σ_a(1 - xₐ)` using an auxiliary variable and inequality constraints?

- Concept: Betweenness Centrality in Directed Networks
  - Why needed here: The dynamic κ mechanism relies on computing arc betweenness; understanding sampling-based approximations for large networks is practical.
  - Quick check question: For a directed graph, does betweenness count paths that traverse an edge in either direction, or only in the edge's direction?

- Concept: Supervised Learning for Structured Prediction
  - Why needed here: The DNN predicts probability distributions over discrete resources; understanding softmax outputs, label encoding from frequency data, and top-κ extraction is required.
  - Quick check question: If training labels are derived from frequency across multiple reference solutions, how should you handle arcs that appear in only one reference solution?

## Architecture Onboarding

- Component map: Historical reference solutions → feature extraction → DNN prediction → top-κ extraction → dynamic κ assignment → reduced MIP → Stage 1 MIP → I* → Stage 2 MIP → solution portfolio
- Critical path: 1) Compute betweenness centrality for all arcs (sampling-based for scalability), 2) Train DNN on historical solutions (one-time, ~2 hours; retraining ~minutes), 3) For each new instance: predict top-κ per arc using dynamic κₐ, 4) Construct reduced MIP with only candidate resources, 5) Solve Stage 1 → obtain I*, 6) Solve Stage 2 with fairness objective → obtain substitution plan, 7) Generate portfolio by varying α (efficiency weight) from 0 to 1
- Design tradeoffs: κ size (smaller → faster but may miss optimal; larger → slower but more complete), fairness metric (minimax minimizes worst-case burden but may increase total changes significantly; Gini promotes overall equality but is computationally more expensive), portfolio depth (more α values → richer tradeoffs but more solver calls)
- Failure signatures: ML underfitting (TOPₖ accuracy <70% at κ=3), infeasible Stage 2 (I* cannot be achieved with reduced candidates), solver timeout on large instances, unfair solutions under minimax (Z* does not decrease)
- First 3 experiments: 1) Validate ML accuracy (train on Classes 1-6, test on Class 7; target TOPₖ >95% at κ=3), 2) Static vs. dynamic κ comparison (Class 6 instance; measure optimality gap, arc reduction, runtime), 3) Efficiency-fairness Pareto frontier (Class 4 instance; plot total changes vs. max burden for α ∈ {0, 0.25, 0.5, 0.75, 1})

## Open Questions the Paper Calls Out
- How can the framework be extended to incorporate stochastic demand and supply uncertainties while preserving fairness guarantees?
- Can adaptive online learning replace the current offline training approach to enable real-time adaptation to shifting scheduler preferences?
- How should the betweenness thresholds (τ₁, τ₂) and the mapping from centrality classes to κ values be systematically optimized rather than empirically tuned?
- Can the framework achieve computational efficiency comparable to the centralized implementation when distributed across schedulers with limited information sharing?

## Limitations
- Dynamic κ thresholds (τ₁, τ₂) and arc-specific κ values are stated as "empirically determined" without quantitative specification, limiting exact reproduction
- Betweenness centrality-based κ assignment assumes high-betweenness arcs are more substitution-critical, but this correlation is not empirically validated within the paper
- While the ML reduction achieves strong results on the tested package delivery network, the generalizability to other logistics domains is not demonstrated

## Confidence
- High Confidence: Two-stage MIP framework for minimizing imbalance then optimizing under fairness constraints
- Medium Confidence: ML-guided search space reduction via DNN prediction and the 80% reduction claim
- Medium Confidence: Dynamic κ assignment via betweenness centrality and the runtime improvement claim

## Next Checks
1. **ML Robustness Testing**: Train the DNN on Classes 1-6 and evaluate TOPκ accuracy on Class 7. Verify that TOPκ >95% at κ=3 is consistently achieved across all arcs.
2. **Dynamic κ Sensitivity Analysis**: Systematically vary τ₁ and τ₂ thresholds on a Class 6 instance and measure impacts on arc reduction, optimality gap, and runtime. Compare to multiple static κ values.
3. **Fairness Metric Comparison**: Solve the Stage 2 minimax and Gini models on a Class 4 instance across α ∈ {0, 0.25, 0.5, 0.75, 1}. Quantify and compare the "price of fairness" (Δ_FAIR - Δ*) and solver runtime.