---
ver: rpa2
title: 'U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT'
arxiv_id: '2509.12069'
source_url: https://arxiv.org/abs/2509.12069
tags:
- u-mamba2
- segmentation
- cbct
- dice
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents U-Mamba2, a neural network architecture for
  multi-anatomy CBCT segmentation that integrates Mamba2 state space models into the
  U-Net architecture. The method leverages structured state space duality (SSD) framework
  to enforce stronger structural constraints, achieving higher efficiency without
  compromising performance compared to previous approaches.
---

# U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT

## Quick Facts
- arXiv ID: 2509.12069
- Source URL: https://arxiv.org/abs/2509.12069
- Reference count: 35
- First place in both tasks of ToothFairy3 challenge with mean Dice 0.84/0.87, HD95 38.17/2.15

## Executive Summary
This paper presents U-Mamba2, a neural network architecture for multi-anatomy CBCT segmentation that integrates Mamba2 state space models into the U-Net architecture. The method leverages structured state space duality (SSD) framework to enforce stronger structural constraints, achieving higher efficiency without compromising performance compared to previous approaches. U-Mamba2 achieved first place in both tasks of the ToothFairy3 challenge, demonstrating state-of-the-art performance on dental anatomy segmentation with mean Dice scores of 0.84 and 0.87 for the two tasks.

## Method Summary
U-Mamba2 integrates Mamba2's structured state space duality framework into a U-Net architecture, replacing the bottleneck with SSD blocks that enable linear-time global dependency modeling. The model incorporates interactive click prompts with cross-attention blocks, pre-training using self-supervised learning on unlabeled CBCT scans, and dental domain knowledge including label smoothing for related anatomies, weighted loss for tiny structures, and left-right mirroring augmentation. The architecture processes 3D CBCT volumes through a 7-stage encoder, Mamba2 bottleneck, and 7-stage decoder with skip connections, achieving first place in both tasks of the ToothFairy3 challenge.

## Key Results
- Task 1: Mean Dice 0.84, HD95 38.17, inference time 40.58s on held-out test data
- Task 2: Mean Dice 0.87, HD95 2.15 on held-out test data
- Achieved first place in both tasks of ToothFairy3 challenge
- Label smoothing alone improved overall Dice from 0.867 to 0.872

## Why This Works (Mechanism)

### Mechanism 1
Integrating Mamba2's structured state space duality (SSD) framework into the U-Net bottleneck enables linear-time global dependency modeling with improved efficiency over selective scan approaches. The SSD framework constrains the internal recurrent structure through stronger structural constraints on hidden space, replacing the selective scan operation with matrix multiplication. This enables tensor and sequence parallelism. Features are reshaped from (B, C, H, W, D) to (B, T, C) where T=H×W×D, processed through Mamba2, then reshaped back.

### Mechanism 2
Label smoothing across anatomically related classes (left-right counterparts, neighboring teeth, nerve pairs) improves segmentation by encoding structural priors into the loss landscape. For each voxel with ground-truth class k, instead of one-hot encoding, assign probability 0.9 to class k and distribute 0.1 evenly across related classes in set Sr. This soft supervision signals anatomical relationships without penalizing confident predictions on genuinely ambiguous boundaries.

### Mechanism 3
Left-right mirroring augmentation with symmetric label swapping expands effective training data while preserving spatial orientation learning. During augmentation, when mirroring across the sagittal plane, swap class labels for bilateral structures (e.g., "Upper left canine" ↔ "Upper right canine"). During test-time augmentation, swap predicted logits correspondingly before ensembling. This increases mirroring axis combinations from 3 to 7.

## Foundational Learning

- **State Space Models (SSMs) and Selective Scan**: Mamba2 builds on SSM theory; understanding the difference between selective scan (Mamba1) and SSD's matrix multiplication (Mamba2) is necessary to debug and extend the architecture. Can you explain why matrix multiplication enables better parallelism than recurrent selective scan on modern GPUs?

- **Cross-Attention Mechanics**: Task 2's interactive segmentation relies on fusing click prompt embeddings with image features via two-way cross-attention. In cross-attention, if query dimension differs from key/value dimension, what projection must occur before attention computation?

- **Label Smoothing for Structured Outputs**: The paper applies non-uniform smoothing based on anatomical relationships, not standard uniform smoothing. How does distributing 0.1 probability across related classes differ from standard label smoothing (uniform distribution across all classes)?

## Architecture Onboarding

- **Component map**: Input CBCT (H×W×D) → Encoder: 7 stages × [ResBlock×2 → Strided Conv] → Bottleneck: U-Mamba2 Block → Decoder: 7 stages × [ResBlock → Transposed Conv] → Softmax → Segmentation (46 classes)

- **Critical path**: 
  1. Ensure input patch size (128×256×256 or 160×288×288) matches training configuration; mismatched dimensions will break the reshape operations in the SSD block.
  2. Verify pre-trained weights (from DAE SSL) are loaded for all layers except final segmentation head and interactive branch.
  3. During inference, sliding window tile size and TTA mirror axes must be configured identically to trained settings for reproducible benchmark results.

- **Design tradeoffs**:
  - SSD at bottleneck only: Paper reports empirical best for 3D CT; tradeoff is limited global context at finer resolutions vs. memory efficiency.
  - Tile size 0.9 vs 0.5: Larger tile reduces inference time by 12.9% with only 0.002 Dice drop.
  - Mirror axes '1,2' (anterior/posterior, left/right): Best Dice-time tradeoff for TTA; excludes superior/inferior axis.

- **Failure signatures**:
  - Confusion between teeth and crowns/implants: Occurs with metallic artifacts; no architectural fix proposed.
  - False positives at image edges: Observed in worst-case predictions; post-processing via connected component volume thresholding mitigates.
  - Left/right class swaps: If label-swapping augmentation is misconfigured, model will predict mirror-image errors on bilateral structures.

- **First 3 experiments**:
  1. Ablate SSD location: Run SSD block at encoder stage 4, decoder stage 4, and bottleneck only; measure Dice and inference time to validate bottleneck-only design choice.
  2. Stress test tiny structures: Evaluate ILN Dice specifically with/without weighted loss (×10 weight) to isolate contribution; paper shows improvement from 0.617 to 0.635.
  3. TTA configuration sweep: Systematically test all 7 mirroring axis combinations with tile sizes [0.5, 0.7, 0.9] to reproduce and extend Figure 3 findings on held-out validation split.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparative evidence for SSD framework advantages over selective scan in dental segmentation literature
- No validation on other anatomical regions or imaging modalities beyond CBCT dental segmentation
- Anatomical relationships for label smoothing lack explicit mapping for all 46 classes

## Confidence
- **High Confidence**: Task 1 and Task 2 benchmark results (mean Dice, HD95, inference time) - directly measured on held-out test data from ToothFairy3 challenge
- **Medium Confidence**: Architectural contributions (SSD integration, label smoothing, cross-attention for clicks) - theoretical mechanisms are sound but limited comparative evidence exists in dental segmentation literature
- **Low Confidence**: Generalization beyond CBCT dental segmentation - no validation on other anatomical regions or imaging modalities is provided

## Next Checks
1. Ablate SSD location: Run SSD block at encoder stage 4, decoder stage 4, and bottleneck only; measure Dice and inference time to validate bottleneck-only design choice.
2. Stress test tiny structures: Evaluate ILN Dice specifically with/without weighted loss (×10 weight) to isolate contribution; paper shows improvement from 0.617 to 0.635.
3. TTA configuration sweep: Systematically test all 7 mirroring axis combinations with tile sizes [0.5, 0.7, 0.9] to reproduce and extend Figure 3 findings on held-out validation split.