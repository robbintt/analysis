---
ver: rpa2
title: 'CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large
  Vision-Language Models'
arxiv_id: '2602.00247'
source_url: https://arxiv.org/abs/2602.00247
tags:
- visual
- tokens
- attention
- layers
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of large vision-language
  models (LVLMs) by tackling two key sources of redundancy: (1) visual tokens with
  high attention scores but low actual contribution to the residual stream, and (2)
  redundant Feed-Forward Network (FFN) computations for visual tokens in intermediate
  layers. The authors propose CAPA, a dual-strategy framework that prunes visual tokens
  based on attention contribution (weighted by value vector magnitude) rather than
  raw attention scores, and approximates redundant FFNs for visual tokens using lightweight
  element-wise Hadamard products.'
---

# CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models

## Quick Facts
- arXiv ID: 2602.00247
- Source URL: https://arxiv.org/abs/2602.00247
- Authors: Samyak Jha; Junho Kim
- Reference count: 19
- Primary result: Maintains near-vanilla performance at 75% pruning while reducing FLOPs by 78%

## Executive Summary
This paper addresses the inefficiency of large vision-language models (LVLMs) by tackling two key sources of redundancy: (1) visual tokens with high attention scores but low actual contribution to the residual stream, and (2) redundant Feed-Forward Network (FFN) computations for visual tokens in intermediate layers. The authors propose CAPA, a dual-strategy framework that prunes visual tokens based on attention contribution (weighted by value vector magnitude) rather than raw attention scores, and approximates redundant FFNs for visual tokens using lightweight element-wise Hadamard products. Experiments across three LVLM backbones (LLaVA-1.5, Qwen2.5-VL, and InternVL3) on six benchmarks show CAPA achieves strong efficiency-performance trade-offs, maintaining near-Vanilla performance even at 75% pruning while reducing FLOPs by 78%. The method is particularly effective at transition layers and outperforms baselines like FastV and Feather, especially for tasks requiring fine-grained visual perception.

## Method Summary
CAPA is a dual-strategy framework that accelerates LVLM inference through contribution-aware pruning and FFN approximation. The method first computes attention contribution scores for visual tokens using weighted attention values, then dynamically prunes tokens based on these scores rather than raw attention. It identifies redundant FFN layers through cosine similarity analysis and approximates their computations using learned Hadamard scaling vectors. The approach operates at every generation step, retaining a subset of visual tokens in the KV cache while maintaining model accuracy. A calibration phase using 500 samples from a dataset identifies redundant FFN layers and computes optimal scaling vectors for approximation.

## Key Results
- Achieves 78% FLOPs reduction while maintaining near-vanilla accuracy at 75% pruning ratio
- Outperforms baselines FastV and Feather on multiple benchmarks, especially for fine-grained visual perception tasks
- Shows effectiveness across three different LVLM backbones: LLaVA-1.5, Qwen2.5-VL, and InternVL3
- Particularly effective at transition layers where visual tokens shift from structural anchors to probability dumps

## Why This Works (Mechanism)
CAPA works by addressing two distinct sources of computational redundancy in LVLMs. First, it recognizes that attention scores alone are poor indicators of token importance, instead computing a contribution score that measures actual impact on the residual stream using value vector magnitudes. Second, it identifies that visual tokens in intermediate layers undergo highly redundant FFN computations that can be approximated with simple linear transformations. By combining these approaches—pruning low-contribution tokens and approximating redundant FFNs—CAPA achieves significant computational savings without sacrificing model performance.

## Foundational Learning

**Concept: Residual Stream and Token Contribution**
- Why needed here: To understand Mechanism 1. The paper's central argument is that token importance should be measured by its contribution to the *residual stream*, not just its attention score.
- Quick check question: In a Transformer layer, how does the output of the attention and FFN sub-layers get combined to form the final layer output?

**Concept: Feed-Forward Networks (FFNs) in Transformers**
- Why needed here: To understand Mechanism 2. The paper targets FFNs for approximation, so one must know their structure (typically two linear projections with a non-linearity) and their role in processing token representations.
- Quick check question: What is the computational complexity of a standard FFN layer with respect to the model's hidden dimension $d$?

**Concept: KV Cache and Generative Inference**
- Why needed here: To understand the practical application of CAPA. The method operates "at every generation step" and retains a subset of visual tokens in the key-value cache, a core component of autoregressive LLM inference.
- Quick check question: In autoregressive decoding, what is stored in the KV cache to avoid redundant computations for past tokens?

## Architecture Onboarding

- **Component map:** Visual tokens -> Attention Contribution Module -> Top-k Pruning -> KV Cache Update -> FFN Approximation Module (in redundant layers) -> Final Output

- **Critical path:**
  1. **Calibration Phase:** Run 500 samples from a calibration dataset (e.g., COCO train) to identify redundant FFN layers (via cosine similarity) and compute the optimal $\alpha$ vectors.
  2. **Inference Phase - Forward Pass:** Visual tokens are processed by the model.
  3. **Inference Phase - Pruning:** At a specified "transition layer," compute attention contribution $C_i$ for all visual tokens, keep the top-$k$, and update the KV cache for all subsequent layers.
  4. **Inference Phase - FFN Approximation:** For layers marked as redundant, apply the learned $\alpha$ vector via an element-wise Hadamard product ($x \odot \alpha$) instead of the full FFN, but only for visual tokens.

- **Design tradeoffs:**
  - **Pruning Ratio ($k$):** Higher ratios improve speed but risk accuracy. The paper suggests a 75% pruning ratio.
  - **Layer Selection:** The choice of which layers to prune and which to approximate is critical. Pruning at "transition" layers and approximating in "intermediate" layers is the recommended strategy. Applying approximation to the wrong layers (e.g., final output layers) could cause significant degradation.
  - **Calibration Data:** The $\alpha$ vectors are computed on a specific dataset. A mismatch between calibration and deployment data could reduce approximation quality.

- **Failure signatures:**
  - **Pruning Collapse:** Sudden, catastrophic drop in performance, likely caused by pruning "Structural Anchors" (high contribution tokens). This would indicate the attention contribution metric is failing for that specific input type.
  - **Approximation Drift:** Gradual performance decline on tasks requiring fine-grained visual detail, suggesting the linear approximation is not capturing some necessary non-linearity.
  - **Instability at High Sparsity:** The paper notes that baselines fail beyond 50% pruning. If CAPA exhibits similar failure, it may indicate the contribution metric is not robust enough for extreme sparsity.

- **First 3 experiments:**
  1. **Ablation Study (Pruning):** Run the vanilla model and the pruned model (CAPA) on a benchmark. Measure the Hellinger distance between their output distributions (logits) as described in Section 5.3. This validates that attention contribution is a superior metric to attention score.
  2. **Ablation Study (FFN Approximation):** Implement three variants on a target model: (a) Vanilla, (b) Vanilla with FFN layers skipped entirely, and (c) Vanilla with FFN layers replaced by the Hadamard product. Compare accuracy on a benchmark like VQAv2 to confirm the approximation recovers lost performance (Table 2).
  3. **FLOPs vs. Accuracy Trade-off Analysis:** Sweep the pruning ratio (e.g., 50%, 60%, 70%, 80%) and measure both the average accuracy across benchmarks (Fig. 7) and the theoretical FLOPs reduction (Fig. 8). This establishes the method's efficiency frontier.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can learned controllers that adaptively decide, per-input and per-layer, how many visual tokens to retain achieve finer-grained efficiency-accuracy trade-offs than CAPA's static pruning heuristics?
- Basis in paper: [explicit] The Limitations section states: "A promising direction for future work is the design of learned controllers that adaptively decide, for each input and at each layer, how many visual tokens to retain (or how much computation to allocate). Such layer-wise and data-dependent policies could enable finer-grained trade-offs between computation and accuracy than static pruning heuristics."
- Why unresolved: CAPA uses fixed thresholds (e.g., retaining 25% of tokens) and static layer-selection criteria; the paper does not explore dynamic, input-adaptive policies.
- What evidence would resolve it: Experiments comparing CAPA against a trained controller that predicts optimal pruning ratios per layer and per input, measuring FLOPs-accuracy Pareto curves.

**Open Question 2**
- Question: Would jointly training token-retention policies with efficient FFN alternatives (e.g., low-rank or MoE-style blocks) yield models that are both faster and more robust than post-hoc CAPA modifications?
- Basis in paper: [explicit] The Limitations section explicitly calls for "architectural" future work: "developing vision–language backbones in which the standard FFN is replaced or reparameterized by more efficient modules that are specifically tailored to visual tokens... Jointly optimizing token-retention policies and such efficient FFN alternatives during training may yield models that are both faster and more robust than what post-hoc modifications can achieve."
- Why unresolved: CAPA operates on fixed, pretrained backbones; no experiments explore end-to-end training of the proposed components.
- What evidence would resolve it: Training new LVLMs from scratch with Hadamard-like or low-rank FFNs for visual tokens, then comparing inference efficiency and task performance against CAPA-applied pretrained models.

**Open Question 3**
- Question: Does the "Read-Only Manifold Hypothesis"—that visual tokens in intermediate layers must remain representationally stable to serve as static context for text tokens—hold across diverse architectures and tasks, or is it an artifact of the specific models and benchmarks studied?
- Basis in paper: [inferred] The appendix proposes this theoretical hypothesis to explain observed FFN linearity for visual tokens, but validation is limited to three architectures (LLaVA, QwenVL, InternVL) and MSCOCO-based analyses. The hypothesis is not rigorously tested against alternative explanations (e.g., training dynamics, tokenization schemes).
- Why unresolved: The hypothesis is offered as a post-hoc explanation; no controlled experiments manipulate the proposed mechanism or test it under distribution shift.
- What evidence would resolve it: Probing experiments across more architectures, tasks (e.g., video, multi-image), and training regimes, plus ablations that intentionally destabilize visual token representations to observe downstream effects.

## Limitations
- Reliance on attention contribution metric quality, which may not be robust across diverse visual domains and tasks
- FFN approximation assumes linear redundancy that may not hold for all architectural variants or specialized fine-tuning scenarios
- Phase transition layer identification lacks clear automated detection mechanism for new architectures

## Confidence

**High Confidence:** The core mathematical framework for contribution-aware pruning is internally consistent and the Hadamard approximation for FFNs is straightforward to implement. The reported FLOPs reduction of 78% at 75% pruning appears credible given the clear computational savings from both mechanisms.

**Medium Confidence:** The superiority claims over baselines (FastV, Feather) are supported by benchmark results, but the relative performance differences are modest in some cases. The ablation study on contribution metrics is convincing, but the evaluation of FFN approximation effectiveness relies primarily on accuracy retention rather than direct validation of the linear assumption.

**Low Confidence:** The generalizability of the learned α vectors across different visual domains is unclear. The paper does not report sensitivity analyses showing how performance degrades when calibration data differs from deployment data.

## Next Checks

1. **Contribution Metric Robustness Test:** Generate synthetic visual sequences where attention scores are artificially manipulated (e.g., boosting attention to irrelevant tokens). Verify that CAPA's contribution metric correctly identifies and preserves structurally important tokens while pruning probability dumps, and that raw attention score methods fail this test.

2. **FFN Approximation Linearity Validation:** For each FFN layer targeted by CAPA, compute the actual cosine similarity between x and x+FFN(x) on the validation set. Compare this to the η=0.96 threshold used during calibration to verify that only truly redundant layers are approximated, and measure accuracy drop when forcing approximation on non-redundant layers.

3. **Cross-Domain Generalization Study:** Calibrate CAPA's α vectors on COCO training data, then evaluate performance degradation when tested exclusively on domain-shifted data (e.g., medical images, satellite imagery, or fine-grained object detection datasets). Quantify whether re-calibration is necessary for domain adaptation.