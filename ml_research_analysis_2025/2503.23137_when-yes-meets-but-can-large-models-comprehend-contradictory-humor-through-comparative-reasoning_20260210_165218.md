---
ver: rpa2
title: 'When ''YES'' Meets ''BUT'': Can Large Models Comprehend Contradictory Humor
  Through Comparative Reasoning?'
arxiv_id: '2503.23137'
source_url: https://arxiv.org/abs/2503.23137
tags:
- comic
- reasoning
- humor
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding humor in comics
  that require comparative reasoning across juxtaposed panels. The authors introduce
  YESBUT (V2), a novel benchmark of 1,262 comics annotated with literal descriptions,
  explicit contradictions, underlying symbolism, titles, and social knowledge.
---

# When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?

## Quick Facts
- **arXiv ID**: 2503.23137
- **Source URL**: https://arxiv.org/abs/2503.23137
- **Reference count**: 40
- **Primary result**: VLMs significantly underperform humans on understanding humor in two-panel comics requiring comparative reasoning, with common failures in visual perception, key element identification, and hallucinations.

## Executive Summary
This paper introduces YESBUT (V2), a benchmark of 1,262 two-panel comics requiring comparative reasoning to comprehend humor based on contradictions between juxtaposed panels. The authors evaluate vision-language models and large language models across four tasks: literal description writing, contradiction generation, symbolism selection, and title matching. Experiments reveal that even state-of-the-art models struggle with basic visual perception, cross-panel correspondence, and social knowledge integration. The paper proposes two effective approaches: text-only training with synthesized narrative data and explicit social knowledge augmentation. This work highlights critical gaps in VLMs' ability to grasp nuanced cultural and narrative contexts.

## Method Summary
The authors construct YESBUT (V2) benchmark with 1,262 comics annotated with literal descriptions, explicit contradictions, underlying symbolism (correct + 3 distractors), titles (correct + 3 distractors), social knowledge, and linguistic context. Four tasks evaluate VLMs and LLMs: (1) Literal Description Writing - generate descriptions of both panels, (2) Contradiction Generation - explain the humorous contradiction, (3) Underlying Symbolism Selection - choose correct meaning from 4 options, and (4) Title Matching - match correct title from 4 options. Zero-shot evaluation is performed across general-purpose VLMs, multi-image VLMs, and reasoning-enhanced VLMs. Text-only training uses GPT-4o to synthesize 20,000 contradictory comic scene descriptions for fine-tuning language components via LoRA. Social knowledge augmentation involves injecting human-annotated norms and cultural references into prompts.

## Key Results
- VLMs achieve significantly lower accuracy than humans on all four tasks, with GPT-4o performing best but still showing substantial gaps
- Text-only training improves performance across all models, with LLaVA-Next-7B gaining 8.56 percentage points in symbolism accuracy
- Social knowledge augmentation consistently improves or maintains performance across all evaluated VLMs
- Visual perception errors, key element omission, and hallucinations are the most common failure modes
- Treating panels as separate images decreases performance despite multi-image training, suggesting cross-panel relationship modeling is challenging

## Why This Works (Mechanism)

### Mechanism 1
Comparative reasoning across juxtaposed panels enables detection of narrative contradictions that create humor. Models must (1) accurately perceive visual elements in each panel, (2) identify corresponding elements across panels, (3) compute the semantic conflict between them, and (4) map this to humorous intent through social knowledge. The core assumption is that contradiction detection is the primary cognitive operation underlying humor comprehension in two-panel comics.

### Mechanism 2
Text-only training with synthesized narrative data improves VLM deep reasoning capabilities without requiring image-text pairs. The approach uses GPT-4o to generate 20,000 contradictory comic scene descriptions with associated reasoning questions, then fine-tunes only the language model component via LoRA while freezing visual perception modules. The core assumption is that language reasoning ability generalizes from textual narratives to multimodal inputs when visual perception remains unchanged.

### Mechanism 3
Explicit social knowledge augmentation improves humor comprehension by providing missing cultural and normative context. This mechanism injects human-annotated social knowledge (norms, cultural references, linguistic context) into prompts before reasoning tasks. The core assumption is that models possess reasoning capacity but lack activation of relevant social knowledge during inference.

## Foundational Learning

- **Juxtaposition-based narrative structure**: Why needed - Unlike single-image humor, two-panel comics encode meaning through the relationship between panels, not within individual panels. Quick check - Given two panels showing the same character in different contexts, can you identify which visual elements must correspond vs. contrast?

- **Autoregressive limitations for bidirectional reasoning**: Why needed - The paper explicitly attributes model limitations to "constraints of the autoregressive paradigm, which hinders bidirectional reasoning." Quick check - How would left-to-right token generation constrain a model's ability to verify that panel 1 elements match panel 2 elements before generating a contradiction explanation?

- **Error cascading in multi-stage reasoning**: Why needed - Section 6.1.2 shows that LLMs using VLM-generated descriptions inherit errors, and Section 6.2.1 shows task decomposition with self-generated descriptions can worsen performance. Quick check - If a VLM misidentifies "flower petals" as "pink beans," how does this propagate through subsequent symbolism selection?

## Architecture Onboarding

- **Component map**: Input processor (handles single composite image or split panel images) -> Visual encoder (frozen during text-only fine-tuning) -> Language model (fine-tuned via LoRA) -> Knowledge augmentation module (optional injection)

- **Critical path**: 1. Visual perception of key elements in each panel (failure cascades - Section 6.4), 2. Cross-panel correspondence mapping, 3. Contradiction detection via comparative reasoning, 4. Social/cultural knowledge integration, 5. Abstract symbolism or title generation

- **Design tradeoffs**: End-to-end vs. decomposed (decomposed approaches with self-generated descriptions degrade performance - Figure 7), Single vs. multi-image input (single composite images perform better despite multi-image training - Table 4), Training data modality (text-only training avoids costly image-text pairs but assumes language reasoning transfers)

- **Failure signatures**: Visual perception errors (misidentifying objects - petals â†’ "pink beans" or "ham slices"), Key element omission (missing critical visual punchlines - "half clothes" in 50% off sale), Hallucination/incorrect association (fabricating narratives - tutorial context for toilet paper comic), Cross-panel confusion (attributing elements to wrong panel)

- **First 3 experiments**: 1. Establish baseline on literal description task - run VLM on 50 samples, manually score Correctness/Completeness/Faithfulness using 5-point scale, 2. Test social knowledge injection on held-out set - compare performance with/without explicit knowledge augmentation on 30 comics requiring social norms, 3. Validate text-only fine-tuning transfer - fine-tune language component on 5,000 synthesized samples, evaluate on held-out comics, compare against unfrozen baseline

## Open Questions the Paper Calls Out

1. **Can VLMs be improved to generate descriptions accurate enough that task decomposition becomes beneficial for deep reasoning?** The paper shows task decomposition with model-generated descriptions fails while oracle descriptions help, suggesting description quality is the bottleneck.

2. **What architectural or training approaches could enable VLMs to effectively capture cross-panel narrative relationships?** Despite multi-image training, splitting panels decreases performance, indicating current architectures fail at cross-panel relationship modeling.

3. **How can social knowledge be natively integrated into VLMs rather than injected via prompts?** Prompt-based augmentation is a workaround, not a fundamental solution to the knowledge gap.

4. **How can the three identified error patterns (visual perception errors, key element omission, and hallucinations) be systematically reduced in VLMs for narrative understanding?** The paper diagnoses failure modes but doesn't propose architecture- or training-level remedies for each error type.

## Limitations

- The benchmark construction methodology is well-specified, but dataset download format and annotation schema details are not fully provided
- The text-only training transfer mechanism shows improvements but lacks strong external validation and ablation studies
- Social knowledge augmentation effects are demonstrated but the knowledge injection methodology isn't fully specified
- Error analysis identifies failure modes but doesn't measure their relative frequency or systematic impact

## Confidence

- **High Confidence**: The benchmark construction methodology (YESBUT V2) is reproducible, the four-task framework is clearly specified, and the general observation that VLMs underperform humans on these tasks appears well-supported
- **Medium Confidence**: The text-only training improvement claims are reasonable given reported results, but lack strong external validation. Social knowledge augmentation effects are demonstrated but methodology isn't fully specified
- **Low Confidence**: The assertion that comparative reasoning is the "primary cognitive operation" underlying humor comprehension - this may be one of several mechanisms

## Next Checks

1. **Error propagation analysis**: Systematically measure how visual perception errors cascade through the four-task pipeline by comparing oracle vs. model-generated descriptions across all tasks

2. **Cross-cultural generalizability**: Test whether the social knowledge augmentation approach works across different cultural contexts by evaluating the benchmark with models trained on non-Western data distributions

3. **Alternative reasoning mechanisms**: Design experiments to isolate whether humor comprehension relies primarily on comparative reasoning vs. pattern recognition by creating variants of the comics that maintain visual contradictions but remove temporal/spatial juxtaposition cues