---
ver: rpa2
title: 'ZiGong 1.0: A Large Language Model for Financial Credit'
arxiv_id: '2502.16159'
source_url: https://arxiv.org/abs/2502.16159
tags:
- data
- financial
- credit
- training
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models for financial credit assessment tasks, which require specialized expertise
  and suffer from issues like hallucination and knowledge forgetting. The authors
  propose ZiGong, a Mistral-based model enhanced through multi-task supervised fine-tuning
  and a novel data pruning methodology.
---

# ZiGong 1.0: A Large Language Model for Financial Credit

## Quick Facts
- arXiv ID: 2502.16159
- Source URL: https://arxiv.org/abs/2502.16159
- Authors: Yu Lei; Zixuan Wang; Chu Liu; Tongyao Wang
- Reference count: 2
- Key outcome: ZiGong achieves superior performance across multiple financial credit tasks compared to other models, with notable improvements on datasets like Australia and Credit Card Fraud

## Executive Summary
This paper addresses the challenge of improving large language models for financial credit assessment tasks, which require specialized expertise and suffer from issues like hallucination and knowledge forgetting. The authors propose ZiGong, a Mistral-based model enhanced through multi-task supervised fine-tuning and a novel data pruning methodology. Their approach employs an agent model to score training samples and select top-k influential data points, combined with TracSeq, an improved data distillation method that incorporates temporal dependencies in financial data. This hybrid training strategy effectively mitigates hallucinations while maintaining reliability in downstream applications. Experimental results show that ZiGong achieves superior performance across multiple financial credit tasks compared to other models, with notable improvements on datasets like Australia and Credit Card Fraud.

## Method Summary
ZiGong combines multi-task supervised fine-tuning with a novel data pruning methodology. The approach uses a lightweight agent model to compute influence scores for training samples via TracSeq, which extends gradient-based influence estimation by incorporating temporal decay factors. The method selects Top-k high-influence samples and combines them with 30% randomly sampled data to maintain diversity. The pruned dataset is then used to fine-tune Mistral 7B with LoRA adapters (rank=8, alpha=16) across diverse financial credit tasks including credit scoring, fraud detection, claim analysis, sentiment analysis, and QA. The training employs instruction templates to unify discriminative and generative tasks in a multi-task learning framework.

## Key Results
- ZiGong achieves superior performance across multiple financial credit tasks compared to other models
- Notable improvements on datasets like Australia and Credit Card Fraud with higher accuracy and F1 scores
- Data pruning methodology shows that using only 50% high-influence samples can match performance of full dataset
- Successfully deployed in Behavior Card service demonstrating practical efficacy in real-world loan process applications

## Why This Works (Mechanism)

### Mechanism 1: TracSeq Temporal Influence Scoring
- Claim: Incorporating temporal decay into gradient-based influence estimation improves identification of high-value training samples for financial behavior data.
- Mechanism: Extends TracInCP by adding a time decay factor γ^(T-ti) that weights recent user behaviors more heavily than distant ones when computing gradient similarity between training and test samples.
- Core assumption: Financial behavior exhibits temporal dependency where recent actions are more predictive of future outcomes than older actions.
- Evidence anchors: [abstract] mentions "incorporates temporal dependencies"; [section 3.1] shows formula with γ^(T-ti) decay factor; corpus papers support credit scoring benchmarks but not temporal influence methods specifically.

### Mechanism 2: Hybrid Data Pruning with Agent-Based Selection
- Claim: Selecting Top-K high-influence samples via a lightweight agent model, then mixing with random samples, reduces hallucination while maintaining task performance.
- Mechanism: A domain-specific lightweight agent model scores all training samples by influence; Top-K are selected and combined with 70% randomly sampled data to filter noisy/low-value samples while preserving diversity.
- Core assumption: Hallucination rates correlate with low-quality or mislabeled training examples that exert disproportionate gradient influence.
- Evidence anchors: [abstract] describes agent-based scoring and sample integration; [section 3.1] states "Top-k most influential training samples" are selected; corpus supports selective fine-tuning principles but not this specific approach.

### Mechanism 3: Multi-Task LoRA Fine-Tuning on Mistral 7B
- Claim: Joint training across diverse financial credit tasks with low-rank adaptation produces a generalist model that outperforms task-specific baselines.
- Mechanism: Freezes Mistral 7B backbone, applies LoRA adapters (rank=8, alpha=16) to query/key/value projections. Multi-task training spans discriminative and generative financial tasks unified via shared instruction templates.
- Core assumption: Financial credit tasks share latent representations that transfer through joint training.
- Evidence anchors: [abstract] mentions "multi-task supervised fine-tuning"; [section 3.2 + Table 1] lists task templates and LoRA configuration; corpus supports multi-task financial LLM evaluation.

## Foundational Learning

- Concept: **Gradient-based influence functions**
  - Why needed here: TracSeq builds on TracInCP, which uses gradient dot products to estimate how much a training sample affects test loss.
  - Quick check question: Can you explain why gradient similarity between a training sample and a test sample indicates influence on model predictions?

- Concept: **Temporal/sequential modeling in financial data**
  - Why needed here: The paper's core innovation assumes financial behaviors are time-dependent.
  - Quick check question: Why might a credit card transaction 30 days ago be less predictive of fraud than one from yesterday?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: All experiments use LoRA for parameter-efficient fine-tuning.
  - Quick check question: What are the tradeoffs of LoRA rank=8 vs. rank=32 in terms of expressiveness and overfitting risk?

## Architecture Onboarding

- Component map: Raw Financial Data -> Agent Model (lightweight scorer) -> TracSeq Influence Scores -> Top-K Selection + Random Sampling (30%/70%) -> Instruction Template Application -> Mistral 7B + LoRA Adapters (rank=8, q/k/v) -> Multi-Task SFT -> ZiGong Model

- Critical path: The data pruning pipeline (Agent -> TracSeq -> Top-K selection) is the novel contribution. Errors here cascade to all downstream tasks. Verify influence score distributions before training.

- Design tradeoffs:
  - 30% pruned + 70% random: Balances quality (pruned) vs. diversity (random). Paper shows 50% high-influence matches full dataset, suggesting this ratio is conservative.
  - LoRA rank=8: Low-rank limits adaptation capacity but reduces overfitting risk on small financial datasets.
  - Time decay γ: Not explicitly tuned in paper. Default γ∈(0,1] requires calibration per dataset.

- Failure signatures:
  - KS scores degrade vs. baseline -> check for data leakage between train/test splits in temporal sequences
  - Hallucination increases -> agent model may be mis-scoring; visualize score distribution per class
  - Missing rate (Miss) spikes -> LoRA adapters may be underfitting; increase rank or learning rate

- First 3 experiments:
  1. Baseline reproduction: Train Mistral 7B + LoRA on full dataset without pruning. Measure Acc/F1/KS on Australia and Credit Card Fraud to establish baseline.
  2. Ablate temporal decay: Compare TracSeq (with γ decay) vs. standard TracInCP (no decay) on same Top-K selection. Expect degraded performance without temporal weighting.
  3. Vary prune ratio: Test 10%/30%/50%/70% pruned data ratios while holding total samples constant. Identify saturation point where more pruning hurts diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TracSeq data pruning methodology amplify or mitigate algorithmic bias and fairness issues across different demographic groups in credit scoring?
- Basis in paper: Section 2.2 explicitly identifies "biases inherent in training data" and "ethical AI" as open challenges in financial LLMs, yet the experimental evaluation of ZiGong focuses solely on performance metrics without measuring fairness.
- Why unresolved: The paper demonstrates improved accuracy but does not analyze whether selecting "high-influence" samples disproportionately favors or penalizes specific sub-populations.
- What evidence would resolve it: A comparative fairness audit (e.g., using Equalized Odds or Demographic Parity) on the German or Australia datasets before and after applying the TracSeq pruning.

### Open Question 2
- Question: Is the fixed 70/30 ratio of random-to-high-influence data optimal for all financial tasks, or does the ideal mixture ratio vary by task complexity?
- Basis in paper: Section 3.2 states the training process uses a fixed split where "70% of the samples are randomly selected... while the remaining 30% are high-influence," without providing ablation studies on this specific hyperparameter.
- Why unresolved: It is unclear if this ratio represents a task-specific hack or a generalizable principle for financial instruction tuning.
- What evidence would resolve it: A sensitivity analysis comparing model performance across various mixing ratios (e.g., 50/50, 30/70) on the diverse set of tasks.

### Open Question 3
- Question: Does the reported reduction in "Miss" rates directly correlate with a measurable reduction in factual hallucination on open-ended generative tasks?
- Basis in paper: The abstract and introduction claim the method "mitigates hallucinations," but Section 5 reports results using Accuracy, F1, and "Miss" metrics, which evaluate classification correctness rather than factual consistency in generation.
- Why unresolved: A lower classification error rate does not necessarily guarantee the absence of hallucinated reasoning or fabricated financial attributes in generative outputs.
- What evidence would resolve it: Evaluation using a dedicated hallucination metric or human evaluation on the generative "Financial Auditing" or "QA" tasks mentioned in Table 1.

## Limitations
- Agent model architecture for influence scoring is unspecified, making exact reproduction challenging
- Critical hyperparameters (time decay γ, checkpoint count k, Top-k threshold) are missing from the paper
- Real-world deployment success is claimed but detailed validation metrics are not provided
- Temporal dependency assumption is not empirically validated despite being core to the method

## Confidence
- High confidence: LoRA fine-tuning methodology and task template structure (well-documented in related literature)
- Medium confidence: TracSeq temporal influence scoring concept (logical extension of existing methods but not fully validated)
- Medium confidence: Data pruning effectiveness (supported by ablation showing 50% high-influence matches full dataset, but agent model details missing)
- Low confidence: Temporal dependency assumption and its implementation details (no empirical validation of γ decay effectiveness)

## Next Checks
1. Implement ablation study comparing TracSeq (with temporal decay) vs. standard TracInCP on the same Top-K selection to quantify temporal decay contribution
2. Test multiple time decay factors (γ=0.9, 0.99, 0.999) to identify optimal temporal weighting for different financial datasets
3. Evaluate class-specific performance impact of data pruning to ensure minority class samples are not systematically excluded by influence scoring