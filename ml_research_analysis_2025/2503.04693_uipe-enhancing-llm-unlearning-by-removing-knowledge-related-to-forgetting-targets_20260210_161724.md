---
ver: rpa2
title: 'UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting
  Targets'
arxiv_id: '2503.04693'
source_url: https://arxiv.org/abs/2503.04693
tags:
- unlearning
- knowledge
- forget
- related
- uipe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation in existing large language
  model (LLM) unlearning methods: models can reconstruct forgotten knowledge by leveraging
  logically related knowledge, leading to suboptimal unlearning performance. To address
  this, the authors propose Unlearning Improvement via Parameter Extrapolation (UIPE),
  which amplifies parameter updates during unlearning to better eliminate related
  knowledge without requiring additional training data.'
---

# UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets

## Quick Facts
- **arXiv ID:** 2503.04693
- **Source URL:** https://arxiv.org/abs/2503.04693
- **Reference count:** 25
- **Key outcome:** UIPE significantly enhances forgetting performance of various unlearning methods while maintaining model utility, especially for smaller target forget sets (1% and 5% of data).

## Executive Summary
This paper identifies a critical limitation in existing LLM unlearning methods: models can reconstruct forgotten knowledge by leveraging logically related knowledge, leading to suboptimal unlearning performance. The authors propose Unlearning Improvement via Parameter Extrapolation (UIPE), which amplifies parameter updates during unlearning to better eliminate related knowledge without requiring additional training data. Experiments on the TOFU benchmark demonstrate that UIPE significantly enhances the forgetting performance of various mainstream unlearning methods while maintaining model utility, particularly for smaller target forget sets.

## Method Summary
UIPE addresses the problem of knowledge reconstruction in LLM unlearning by amplifying parameter updates through extrapolation. The method takes an unlearned model checkpoint and computes the parameter difference vector from the initial model, then extrapolates this update by a coefficient α to amplify forgetting of both target and related knowledge. This approach works with any base unlearning method (GA, Grad. Diff., KL Min., NPO) and requires no additional training data or model modifications.

## Key Results
- UIPE improves forget quality across all tested base methods on Forget01 (1% data) and Forget05 (5% data) benchmarks
- The method shows minimal improvement on Forget10 (10% data), confirming scalability limitations
- Different base methods require different optimal α values, with NPO showing relative insensitivity to α magnitude
- UIPE maintains model utility better than simply increasing base unlearning iterations

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Reconstruction via Related Knowledge
LLMs can reconstruct forgotten target knowledge by reasoning with logically related knowledge that was not included in the forget set. When only the direct fact (e.g., "John has diabetes") is unlearned, the model retains correlated facts (e.g., "John uses insulin" + "insulin treats diabetes") and can inferentially reconstruct the target through multi-hop reasoning during inference. This mechanism is validated by comparing models trained with/without related knowledge exposure, showing that unlearning related knowledge improves target forgetting.

### Mechanism 2: Gradient Similarity Induces Correlated Parameter Updates
Related knowledge exhibits similar gradient directions in parameter space, creating correlated update patterns during both learning and unlearning. For related knowledge pairs, their gradients show high cosine similarity because they share underlying conceptual structure and parameter dependencies. This gradient similarity serves as a reliable proxy for knowledge relatedness and storage locality in the model, enabling targeted manipulation of related knowledge through parameter updates.

### Mechanism 3: Projection-Based Unlearning Cascade Amplified via Extrapolation
Gradient ascent on target knowledge creates a projected update component along related knowledge gradients; amplifying this projection through parameter extrapolation enhances related knowledge forgetting. The parameter update vector from gradient ascent on target knowledge projects onto related knowledge gradient direction, and UIPE amplifies this projection by scaling the update vector. This achieves stronger forgetting of related knowledge without requiring it in the training data.

## Foundational Learning

- **Gradient Ascent for Unlearning**
  - Why needed here: UIPE builds on GA-based methods as the base unlearning approach; understanding that unlearning reverses the optimization direction by maximizing loss on forget set is essential.
  - Quick check question: Can you explain why maximizing LGA(θ) = EDf[log(Pθ(y|x))] leads to forgetting, and what catastrophic collapse risk it introduces?

- **Parameter Update Geometry and Projection**
  - Why needed here: UIPE's core insight relies on understanding how a parameter update vector in one direction creates projected components in other directions based on cosine similarity.
  - Quick check question: Given two gradient vectors g1 and g2 with cosine similarity 0.8, if you apply update Δθ = g1, what is the magnitude of the projected component along g2's direction?

- **Knowledge-Parameter Storage in LLMs**
  - Why needed here: The method assumes facts are stored in distributed parameter patterns where related knowledge shares similar storage distributions, making gradient-based manipulation possible.
  - Quick check question: Why would "John uses insulin" and "John has diabetes" have similar gradient patterns in a language model's parameter space?

## Architecture Onboarding

- **Component map:**
  Initial Model (Pθini) -> Base Unlearning Method: GA/Grad.Diff./KL Min./NPO -> Unlearned Model (Pθun) -> Update Vector Computation: v = θun - θini -> UIPE Extrapolation: θuipe = θun + α·v -> Final Unlearned Model (Pθuipe)

- **Critical path:**
  1. Select appropriate checkpoint from base unlearning (balance forget quality vs. model utility—avoid models with good forgetting but collapsed utility)
  2. Compute clean parameter difference vector v (ensure same architecture, no intermediate modifications)
  3. Tune α coefficient (paper shows optimal α varies by method: larger α for Grad.Diff./KL Min., careful tuning for GA to avoid over-forgetting)

- **Design tradeoffs:**
  - **α magnitude:** Higher α improves forget quality but risks utility degradation; paper shows α sensitivity varies by base method (GA shows quality degradation at high α, NPO relatively insensitive)
  - **Base epoch selection:** Earlier epochs maintain better utility but weaker forget quality for extrapolation base; later epochs have better forgetting but may have degraded utility
  - **Forget set size:** UIPE effectiveness decreases as forget set scales (Forget10 shows minimal improvement vs. Forget01/Forget05); paper attributes this to "low-quality parameter update vectors v" at larger scales

- **Failure signatures:**
  - Over-amplification (α too high): Model utility collapse, especially with GA base method
  - Poor base selection: If Pθun already has utility collapse, extrapolation amplifies the damage
  - Large forget sets (10%+): UIPE shows minimal improvement—likely due to degraded gradient quality from multi-target interference
  - Corrupted projection relationship: If target knowledge fully forgotten before computing v, Rθ(ki, k'i) becomes meaningless

- **First 3 experiments:**
  1. **Baseline validation on synthetic data:** Replicate Pθ1/Pθ2/Pθ3 comparison from Section 4.2 to confirm related knowledge enables reconstruction and that unlearning related knowledge improves target forgetting. Use ROUGE-L on TruthfulQA for utility, ROUGE-L on forget/related sets for forget quality.
  
  2. **UIPE coefficient sweep:** For a single base method (recommend KL Min. or Grad.Diff. as they show consistent improvement with larger α), test α ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} on Forget01 dataset. Plot forget quality vs. model utility trade-off curve. Expect: monotonic forget quality improvement with α, moderate utility decline.
  
  3. **Cross-method comparison at fixed α:** Apply UIPE with α=0.5 to all four base methods (GA, Grad.Diff., KL Min., NPO) on Forget05 dataset. Compare improvement magnitude relative to each baseline's optimal checkpoint. Expect: NPO shows smallest relative improvement (already strong baseline), GA/KL Min. show largest absolute gains but with utility trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the amplify coefficient $\alpha$ be determined automatically to eliminate the need for manual tuning across different baseline methods?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "The optimal amplify coefficient $\alpha$ requires manual selection across different baseline methods, necessitating further research to establish automated selection strategies."
- **Why unresolved:** Currently, $\alpha$ is a hyperparameter that varies depending on the unlearning method used (e.g., GA vs. NPO), requiring manual search to balance utility and forgetting.
- **What evidence would resolve it:** A proposed heuristic or adaptive algorithm that sets $\alpha$ dynamically based on model gradients or loss metrics, demonstrating performance comparable to manual tuning.

### Open Question 2
- **Question:** Does UIPE retain its effectiveness on significantly larger models (e.g., 70B parameters) where knowledge storage is more complex and distributed?
- **Basis in paper:** [explicit] The paper notes the study focuses on LLaMA2-7B and that "larger parameter scales model (e.g., 70B) typically contain richer and more complex knowledge representations" requiring further assessment.
- **Why unresolved:** The relationship between parameter extrapolation and model scale is unknown; vector amplification might behave differently in higher-dimensional parameter spaces.
- **What evidence would resolve it:** Experimental results applying UIPE to 70B models, showing that extrapolating update vectors still effectively removes related knowledge without degrading model utility.

### Open Question 3
- **Question:** Can the method be modified to maintain robustness when applied to larger target forget sets (e.g., 10% of data) where current performance degrades?
- **Basis in paper:** [inferred] Section 6.2 notes that "As the scale of forgetting data increases, UIPE’s improvement effects show a weakening trend" because baseline methods produce low-quality update vectors for large forget sets.
- **Why unresolved:** UIPE relies on the quality of the initial update vector; as the forget set grows, this vector appears to become less effective for extrapolation.
- **What evidence would resolve it:** A modification to the UIPE mechanism or the base unlearning process that sustains forget quality improvements on the Forget10 benchmark.

## Limitations
- UIPE effectiveness decreases with larger forget sets (>5%) due to degraded update vector quality
- The method requires manual tuning of the α coefficient across different base methods
- No theoretical guarantees for the extrapolation mechanism; empirical success depends on linear relationship assumptions
- Limited validation on larger models (70B+) where knowledge storage patterns may differ significantly

## Confidence
- **High Confidence:** Mechanism 1 (knowledge reconstruction via related knowledge) - well-validated with Pθ1/Pθ2 comparison showing statistically significant differences
- **Medium Confidence:** Mechanism 2 (gradient similarity induces correlated updates) - theoretically sound but limited corpus corroboration beyond the paper itself
- **Medium Confidence:** Mechanism 3 (projection amplification via extrapolation) - empirically validated but sensitivity to α tuning and break conditions under-documented

## Next Checks
1. **Break Condition Validation:** Systematically test UIPE performance when related knowledge has decreasing logical connection to target knowledge (e.g., "John has diabetes" vs. "John likes apples"). Measure forget quality degradation to identify the relationship threshold below which reconstruction fails.

2. **α Sensitivity Boundary Testing:** For each base method, identify the α value where utility begins catastrophic collapse (expect GA to show sharp threshold around α=0.8-1.0, NPO more gradual). This establishes safe operational bounds for practical deployment.

3. **Knowledge Density Impact Study:** Vary the number of logically related facts per target (e.g., 1, 3, 5, 10 related facts) to test whether UIPE's effectiveness scales with knowledge density or if there's an optimal density beyond which reconstruction becomes too complex for gradient-based amplification.