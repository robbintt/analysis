---
ver: rpa2
title: Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers
  and Convolutional Networks Alike
arxiv_id: '2601.11022'
source_url: https://arxiv.org/abs/2601.11022
tags:
- quantile
- data
- loss
- distribution
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses test-time adaptation (TTA) for image classification,
  where the test data distribution differs from training. Most existing TTA methods
  are architecture-specific and rely on batch-norm layers or model retraining.
---

# Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers and Convolutional Networks Alike

## Quick Facts
- arXiv ID: 2601.11022
- Source URL: https://arxiv.org/abs/2601.11022
- Authors: Sravan Danda; Aditya Challa; Shlok Mehendale; Snehanshu Saha
- Reference count: 25
- Primary result: Architecture-agnostic TTA method using quantile loss achieves 25% accuracy gain on CIFAR10-C, 11% on CIFAR100-C, and 4.6% on TinyImageNet-C

## Executive Summary
This paper addresses test-time adaptation (TTA) for image classification when test data distribution differs from training. Most existing TTA methods are architecture-specific and rely on batch-norm layers or model retraining. The authors propose an architecture-agnostic approach by learning a decorruption operator that maps test images to the training distribution using quantile loss. The method is validated on CIFAR10-C, CIFAR100-C, and TinyImageNet-C using both CNN (ResNet18) and transformer (CCT, CVT, ViT-Lite) architectures. Results show significant improvements over baselines without requiring paired clean-corrupt images.

## Method Summary
The method learns a decorruptor network T_θ that transforms test images to match the training distribution in feature space. The key innovation is using geometric quantile matching as the loss function, which measures the discrepancy between high-dimensional feature distributions without requiring paired clean-corrupt images. The quantile loss is optimized using a memory bank to reduce variance, enabling mini-batch training. The decorruptor is initialized as identity and trained while keeping the pre-trained classifier frozen. The approach scales well with small batch sizes and works across different architectures including both convolutional networks and transformers.

## Key Results
- 25% accuracy improvement on CIFAR10-C over baseline (no adaptation)
- 11% accuracy improvement on CIFAR100-C over baseline
- 4.6% accuracy improvement on TinyImageNet-C over baseline
- Robust to small batch sizes (128-512) with minimal performance degradation
- Works across CNN (ResNet18) and transformer (CCT, CVT, ViT-Lite) architectures

## Why This Works (Mechanism)

### Mechanism 1: Geometric Quantile Matching for Distribution Alignment
Minimizing quantile loss between source and de-corrupted test features aligns marginal distributions without requiring paired clean-corrupt training data. The quantile index function U_F(z) = E_{X~F}[z-X/||z-X||] maps any vector z to a bounded unit ball representation capturing directional averages from all support points. Matching these indices across distributions forces their full geometry to align, not just moments (mean/variance). The inverse map theorem (Theorem 1) ensures every data point is a quantile, enabling complete distribution characterization.

### Mechanism 2: Implicit Class-Conditional Alignment via Local Identifiability
Under good initialization, minimizing marginal quantile loss implicitly aligns class-conditionals without label access. The decorruptor T_θ is initialized as identity. Since the pre-trained classifier retains some accuracy on corrupted data, the local neighborhood around θ₀ satisfies local identifiability—decorruption parameters that match marginal distributions locally correspond to the true inverse transform. This prevents convergence to "flipped" solutions where marginals align but class-conditionals are scrambled.

### Mechanism 3: Variance-Reduced Gradient Estimation via Memory Bank
Composite-separable loss functions can be optimized with mini-batches using a memory bank that stores snapshot-averaged h_r functions. The memory bank stores (1/n)Σ_i h_r(x_i;θ_snap), enabling control-variate estimator that reduces variance by factor (1-ρ²) where ρ≈1 when θ_t≈θ_snap. This allows efficient mini-batch training despite the non-sample-separable nature of the quantile loss.

## Foundational Learning

- **Geometric Quantiles in High Dimensions**
  - Why needed: Univariate quantiles don't generalize to R^k; Chaudhuri's formulation defines quantiles via loss minimization using directional parameters
  - Quick check: Given a 2D dataset with points at (0,0), (1,0), (0,1), what is the geometric median (quantile at u=0)?

- **Covariate Shift vs. Label Shift**
  - Why needed: TTA-C benchmarks assume covariate shift (P(X) changes, P(Y|X) unchanged), justifying learning T: X_test→X_train rather than recalibrating class probabilities
  - Quick check: If test data has same images but different class proportions than training, would quantile matching help? Why or why not?

- **Control Variates and Variance Reduction**
  - Why needed: Quantile loss requires computing expectations over all samples—O(n) per gradient step; control-variate estimator exploits correlation between θ_t and θ_snap to reduce variance without bias
  - Quick check: If correlation ρ=0.9 between current and snapshot batch means, what's the variance reduction factor?

## Architecture Onboarding

- **Component map:**
  Test Image X+ → Decorruptor T_θ → Classifier f (frozen) → Features f(T_θ(X+))
                                              ↓
  Source Features {f(x_i)} (pre-computed) → Quantile Index Computation
                                              ↓
                                      Quantile Loss L_quant
                                              ↓
                                    Gradient Update on θ only
  Memory Bank: stores source features + snapshot h_r averages

- **Critical path:**
  1. Pre-compute source features {f(x_i)} and their quantile indices for sampled reference points
  2. Initialize T_θ as identity (convolutional network with residual connections)
  3. For each epoch: forward pass test batch through T_θ and f; compute quantile indices of reference points w.r.t. de-corrupted features; compute MSE between source and de-corrupted quantile indices; backprop to θ only
  4. Periodically update memory bank snapshot averages

- **Design tradeoffs:**
  - Number of reference quantiles: ~10 per class sufficient; more quantiles → better coverage but higher memory
  - Batch size: Algorithm robust to small batches (128-512); smaller batches → more frequent updates but higher noise
  - Decorruptor architecture: 6.2M parameter conv net with multi-scale spatial convolutions + deconv; overhead scales with corruption complexity

- **Failure signatures:**
  - Accuracy drops below baseline: Check quantile index computation numerical stability
  - Loss plateaus early: May indicate bad initialization—verify classifier accuracy on corrupted data
  - Class-conditional flip: Increase learning rate warmup or add regularization to penalize large initial deviations

- **First 3 experiments:**
  1. Sanity check on 2D toy data (six blobs or two moons): Train linear decorruptor, verify it learns inverse transform by visualizing de-corrupted vs. source points
  2. CIFAR10-C single corruption (Gaussian noise level 5): Train decorruptor with ResNet18 frozen; track quantile loss, MSE, and accuracy; verify correlation between losses
  3. Small-batch scaling test on CIFAR100-C with CCT: Compare batch sizes 128, 256, 512 with fixed quantile count; verify robustness within 1-2% accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can the "quantile basis" of a probability distribution be formally characterized to design algorithms that bypass the need for a memory bank? The current method relies on composite-separability and a memory bank to manage the non-sample-separable nature of the loss; characterizing the quantile basis could enable more efficient algorithms without cached statistics.

### Open Question 2
How sensitive is the decorruptor to "bad" initializations that fail to satisfy the "local identifiability" condition required for class-conditional alignment? While the paper argues that the identity map is usually a good initializer, it does not quantify the margin of error or how often optimization falls into "flipped" local minima in high-dimensional spaces.

### Open Question 3
Can this architecture-agnostic approach be extended to dynamic, non-stationary test distributions where the corruption type changes over time? The algorithm assumes a fixed test distribution to accumulate feature statistics; it's unclear if the decorruptor can adapt continuously without catastrophically forgetting previous adaptations.

## Limitations
- Decorruptor architecture details remain underspecified, making exact reproduction difficult
- Training hyperparameters (learning rate, epochs, optimizer) are not provided, requiring significant tuning
- Regularization terms for batch statistics are mentioned but lack specification of weights or implementation details

## Confidence
- **High Confidence**: Empirical results on CIFAR10-C/CIFAR100-C show consistent accuracy improvements (25%/11%) over baselines; architecture-agnostic nature validated across CNNs and transformers
- **Medium Confidence**: Theoretical claims about quantile loss equivalence to pairwise MSE and Wasserstein distances under assumptions (A1)-(A3); Figure 3 and 10-11 demonstrate initialization sensitivity
- **Low Confidence**: Generalizability beyond CIFAR-style benchmarks to real-world scenarios; scalability to much larger datasets or higher-resolution images

## Next Checks
1. **Initialization Sensitivity**: Reproduce Figure 10-11 by training decorruptors with intentionally poor initializations (e.g., 180° rotation or scaling) to verify class-conditional flip phenomenon
2. **Memory Bank Impact**: Compare quantile loss convergence with and without memory bank variance reduction across batch sizes 64-512 to quantify overhead-benefit tradeoff
3. **Quantile Coverage Analysis**: Systematically vary reference quantile count per class (1, 5, 10, 20, 50) on CIFAR100-C to verify the claimed "~10 quantiles per class sufficient" finding from Table 3