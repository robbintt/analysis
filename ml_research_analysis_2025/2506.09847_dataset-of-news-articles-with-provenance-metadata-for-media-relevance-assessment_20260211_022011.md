---
ver: rpa2
title: Dataset of News Articles with Provenance Metadata for Media Relevance Assessment
arxiv_id: '2506.09847'
source_url: https://arxiv.org/abs/2506.09847
tags:
- news
- arxiv
- provenance
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the News Media Provenance Dataset, the first
  dataset of news articles with provenance-tagged images for assessing media relevance.
  The dataset contains 637 articles with C2PA metadata embedding relevant (human-annotated)
  and irrelevant (AI-generated) location and date information.
---

# Dataset of News Articles with Provenance Metadata for Media Relevance Assessment

## Quick Facts
- arXiv ID: 2506.09847
- Source URL: https://arxiv.org/abs/2506.09847
- Authors: Tomas Peterka; Matyas Bohacek
- Reference count: 17
- Primary result: First dataset for media provenance relevance assessment with 637 news articles containing C2PA metadata; LOR accuracy 64-81%, DTOR accuracy 42-58%

## Executive Summary
This paper introduces the News Media Provenance Dataset, the first dataset of news articles with provenance-tagged images for assessing media relevance. The dataset contains 637 articles with C2PA metadata embedding relevant (human-annotated) and irrelevant (AI-generated) location and date information. Two tasks are proposed: Location of Origin Relevance (LOR) and Date and Time of Origin Relevance (DTOR). Baseline evaluations on six LLMs show strong LOR performance (64-81% accuracy) but significantly weaker DTOR results (42-58% accuracy), indicating models struggle with temporal reasoning. Qualitative analysis reveals models can reason about location but fail to handle date and timeline inferences. The work highlights the need for specialized architectures and improved temporal reasoning capabilities in media provenance assessment.

## Method Summary
The dataset was constructed by scraping 637 news articles from Webz.io using Newspaper4k, extracting title, body, image, and caption. Human annotators labeled relevant locations and dates using Argilla, while ChatGPT-4o generated irrelevant metadata alternatives. C2PA metadata was embedded into images using the C2PA library. Six LLMs (ChatGPT-4o, DeepSeek V3, Gemma 2 27B Instruct, Llama 3.1 8B Instruct, Mistral 7B Instruct v0.3, Phi 3.5 Vision Instruct) were evaluated zero-shot on binary classification tasks using standardized prompts. Accuracy was computed per task and aggregated at the article level.

## Key Results
- Location of Origin Relevance (LOR) accuracy ranges from 64-81% across six LLMs
- Date and Time of Origin Relevance (DTOR) accuracy ranges from 42-58%, significantly lower than LOR
- Article-level accuracy (both tasks correct) peaks at 47% for ChatGPT-4o
- Models show strong entity extraction capabilities but struggle with temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can successfully assess Location of Origin Relevance (LOR) by performing entity extraction and alignment between article text and provenance metadata.
- Mechanism: The model extracts named entities (cities, countries) from the article's title and body, then compares these against the "location of origin" field in the C2PA provenance metadata. A direct match yields a "relevant" judgment.
- Core assumption: The news article explicitly names the event's location, and the provenance metadata accurately reflects where the image was captured.
- Evidence anchors:
  - [section 5.2]: The qualitative evaluation shows ChatGPT-4o correctly reasoned: "The image was taken in Albany, USA, which is directly relevant to the news story. The incident occurred in Albany..."
  - [abstract]: Notes "strong LOR performance (64-81% accuracy)" across baseline models.
  - [corpus]: Related work (Peterka & Bohacek 2025) hypothesizes provenance metadata could answer relevance questions, but direct corpus validation of this specific entity-matching mechanism is weak.
- Break condition: This mechanism fails when article locations are vague (e.g., "a local restaurant"), when locations span broad regions, or when provenance metadata has been stripped or spoofed.

### Mechanism 2
- Claim: Assessing Date and Time of Origin Relevance (DTOR) requires multi-step temporal reasoning that current LLMs cannot reliably perform.
- Mechanism: The model must parse multiple temporal references in an article (event date, publication date, follow-up dates), construct an internal timeline, and determine if the image's capture date falls within a relevant window relative to the core event.
- Core assumption: Models can perform temporal arithmetic and distinguish between an event's timeline and the article's reporting perspective—a capability that appears lacking.
- Evidence anchors:
  - [section 5.2]: The model incorrectly rejected a May 2024 image for a story about a May 29 attack, conflating the event with later June recovery updates: "Since it is unclear whether the image was taken before or after the attack... the timing is not conclusively relevant."
  - [abstract]: DTOR accuracy ranges from 42-58%, indicating models "struggle with temporal reasoning."
  - [corpus]: Corpus evidence for this specific temporal reasoning failure mode is weak/missing.
- Break condition: Breaks for articles with complex or ambiguous timelines, for events without clear temporal boundaries (e.g., natural disasters), or for recent events outside the model's training data.

### Mechanism 3
- Claim: Synthetic negative sampling via LLMs creates a balanced dataset for training and evaluating provenance-based relevance classifiers.
- Mechanism: ChatGPT-4o generates plausible but irrelevant location/date metadata pairs for each article, creating a binary classification task that forces models to reject semantic plausibility in favor of factual-consistency checking.
- Core assumption: The generated negative samples are sufficiently realistic and diverse to represent real-world misattempts, and don't introduce exploitable artifacts.
- Evidence anchors:
  - [section 3.2]: "ChatGPT-4o was used to simulate additional provenance metadata that were not relevant to the article... two data points where one of the provenance metadata fields is not relevant... and one data point where both provenance metadata fields are not relevant."
  - [section 1]: The dataset contains "relevant (human-annotated) and irrelevant (AI-generated) location and date information."
  - [corpus]: Corpus evidence for this specific synthetic data generation approach is weak/missing.
- Break condition: Fails if negative samples are trivially obvious, lack diversity, or introduce spurious correlations that models exploit instead of learning genuine relevance assessment.

## Foundational Learning

- Concept: Provenance Metadata (C2PA Standard)
  - Why needed here: The entire task framework depends on having trustworthy, structured metadata about where and when media was created. Without this, relevance assessment reverts to semantic analysis only.
  - Quick check question: Can you explain why cryptographic signing in C2PA prevents metadata tampering, and what happens if an image is re-saved or edited without C2PA-aware software?

- Concept: Named Entity Recognition (NER) for Locations and Temporal Expressions
  - Why needed here: Models must reliably extract locations (cities, countries) and temporal expressions (dates, months, years) from unstructured news text before comparing them to structured metadata fields.
  - Quick check question: Given the sentence "The attack occurred outside Shogun Sushi in upstate New York last month," what entities would a robust NER system need to extract for LOR and DTOR tasks?

- Concept: Temporal Reasoning and Timeline Construction
  - Why needed here: DTOR requires understanding sequences of events, not just isolated dates. Models must build implicit timelines from narratives and reason about event ordering.
  - Quick check question: If an article describes a May 29 attack, a June 10 hospital discharge, and was published June 13, what date range should be considered "relevant" for an image of the victim?

## Architecture Onboarding

- Component map: Article scraping -> Human annotation -> Synthetic negative generation -> C2PA embedding -> LLM prompting -> Accuracy evaluation
- Critical path: Article scraping → Human annotation → Synthetic negative generation → C2PA embedding → LLM prompting → Accuracy evaluation
- Design tradeoffs:
  - **Dataset size (637 articles) vs. annotation quality**: Smaller dataset with multi-annotator validation (80% location, 56% date agreement on shared samples) vs. larger-scale automated labeling
  - **Zero-shot baseline vs. specialized architectures**: Current evaluation uses off-the-shelf LLMs for baseline; paper explicitly notes "room for specialized architectures" for DTOR
  - **Simulated vs. real provenance**: Dataset simulates C2PA metadata because real adoption is limited; tradeoff between realism and research feasibility
- Failure signatures:
  - **Temporal conflation**: Models confuse event dates with publication dates or follow-up dates (observed in qualitative analysis)
  - **Recent event blind spots**: Models struggle with articles about events post-dating their training data (hypothesized in Section 6)
  - **Ambiguity handling**: Annotators marked N/A for unclear cases; models may forced-choice on ambiguous inputs
- First 3 experiments:
  1. **Prompt engineering for temporal reasoning**: Design prompts that explicitly instruct models to construct event timelines before making DTOR judgments; compare against baseline prompts
  2. **Fine-tuning on synthetic negatives**: Train a smaller model (e.g., Llama 3.1 8B) on the balanced dataset with hard negatives to see if DTOR performance improves beyond zero-shot baselines
  3. **Error analysis by article recency**: Segment test set by article publication date relative to model training cutoffs to quantify the "recent event" degradation effect hypothesized in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized architectures be developed to close the performance gap between Location of Origin Relevance (LOR) and Date and Time of Origin Relevance (DTOR) tasks?
- Basis in paper: [explicit] The authors state that the "low baseline performance [on DTOR] reinforces the need for developing new architectures tailored to the LOR and DTOR tasks" to improve temporal reasoning.
- Why unresolved: Current LLMs struggle to reason about timelines, often conflating the article's publication date with the event date and the image's capture date.
- What evidence would resolve it: A model architecture that achieves comparable accuracy (e.g., >75%) on DTOR as on LOR using the provided dataset.

### Open Question 2
- Question: Can automated methods effectively distinguish between evidentiary images and illustrative images where strict provenance alignment is unnecessary?
- Basis in paper: [explicit] The limitations section notes that "future work could explore automatic methods for detecting when precise alignment is necessary" to handle cases like generic portraits or historical illustrations.
- Why unresolved: The current task formulation treats all images uniformly, potentially flagging legitimate illustrative media as irrelevant due to mismatched provenance metadata.
- What evidence would resolve it: A classification mechanism that successfully identifies and filters illustrative imagery before the relevance assessment is applied.

### Open Question 3
- Question: How does provenance assessment performance generalize to non-Western news contexts and low-resource languages?
- Basis in paper: [explicit] The discussion identifies that "expanding the dataset to include non-Western news contexts and additional languages will be essential to ensure inclusive support."
- Why unresolved: The current dataset is derived primarily from Western domains (e.g., Yahoo, CBS, Fox), leaving the efficacy of these methods on global news content unknown.
- What evidence would resolve it: Evaluation results from a replicated dataset constructed from diverse linguistic and cultural sources.

## Limitations

- Synthetic negative sampling lacks external validation for realism and diversity
- Temporal reasoning challenges are identified but not fully characterized across all failure modes
- Dataset size and annotation agreement limitations constrain reliability for robust model training

## Confidence

- **High Confidence**: LOR task mechanism (entity matching between article text and provenance metadata) is well-supported by quantitative results (64-81% accuracy) and qualitative examples
- **Medium Confidence**: DTOR task challenges and model failures are well-documented but underlying causes require further investigation
- **Low Confidence**: Synthetic negative sampling effectiveness lacks external validation compared to real-world provenance mismatches

## Next Checks

1. **Prompt Engineering Experiment**: Design and test prompts that explicitly instruct models to construct event timelines before making DTOR judgments. Compare accuracy against baseline prompts to quantify the impact of temporal reasoning scaffolding.

2. **Negative Sample Realism Analysis**: Conduct human evaluation of the ChatGPT-4o generated negative samples to assess their realism and diversity. Compare model performance on synthetic vs. human-generated negatives to validate the synthetic approach.

3. **Temporal Reasoning Ablation Study**: Create controlled test cases with varying temporal complexity (simple date matching, event sequences, publication date vs. event date) to systematically identify which aspects of temporal reasoning most challenge current models.