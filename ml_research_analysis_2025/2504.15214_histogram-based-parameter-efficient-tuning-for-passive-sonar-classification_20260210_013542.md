---
ver: rpa2
title: Histogram-based Parameter-efficient Tuning for Passive Sonar Classification
arxiv_id: '2504.15214'
source_url: https://arxiv.org/abs/2504.15214
tags:
- vtuad
- histogram
- adapters
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel histogram-based parameter-efficient
  tuning (HPT) method that captures feature distributions through histogram layers
  to improve transfer learning in passive sonar classification. The approach outperforms
  conventional adapters on three underwater acoustic datasets, achieving up to 91.8%
  accuracy on VTUAD versus 89.8% for adapters, while using fewer parameters and converging
  faster.
---

# Histogram-based Parameter-efficient Tuning for Passive Sonar Classification

## Quick Facts
- arXiv ID: 2504.15214
- Source URL: https://arxiv.org/abs/2504.15214
- Reference count: 28
- Primary result: Histogram-based parameter-efficient tuning (HPT) achieves 91.8% accuracy on VTUAD dataset versus 89.8% for adapters while using fewer parameters and faster convergence

## Executive Summary
This paper introduces Histogram-based Parameter-efficient Tuning (HPT), a novel PETL method that captures feature distributions through histogram layers to improve transfer learning in passive sonar classification. HPT integrates histogram layers in parallel with multi-head self-attention in transformer models, using learnable radial basis function binning to summarize distributional statistics. The method outperforms conventional adapters on three underwater acoustic datasets, achieving up to 91.8% accuracy on VTUAD versus 89.8% for adapters, while using fewer parameters and converging faster. HPT provides a distribution-aware alternative to existing PETL methods, balancing parameter savings with performance gains.

## Method Summary
HPT captures target domain statistics through learnable histogram binning, providing distribution-aware adaptation that conventional adapters lack. The method uses two successive 1×1 convolutions to project features into B bins with learnable centers and widths, computes soft bin assignments via radial basis functions, and aggregates normalized responses through adaptive average pooling. These statistical summaries are broadcast to augment all patches with the same distributional context. HPT modules are integrated in parallel with multi-head self-attention sublayers, allowing attention mechanisms to operate on features already augmented with global statistical context. The approach uses Kaiming initialization for faster convergence compared to zero-initialized adapters.

## Key Results
- HPT achieves 91.8% accuracy on VTUAD dataset versus 89.8% for adapters with similar parameter counts
- HPT converges faster than adapters on larger datasets, attributed partly to Kaiming initialization
- Feature similarity analysis shows HPT maintains better alignment with fully fine-tuned models at mid-layers compared to adapters
- Parameter efficiency: 4-16 bins yield 7.9K-18K shared parameters versus adapter reduction rates of 256/128/64

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aware Feature Modulation via Histogram Layers
HPT captures target domain statistics through learnable histogram binning, providing distribution-aware adaptation that conventional adapters lack. Two successive 1×1 convolutions project features into B bins with learnable centers and widths. A radial basis function computes soft bin assignments, normalized responses are aggregated via adaptive average pooling, then broadcast to augment all patches with statistical context. This addresses distributional shifts between pre-training and target domains that benefit from explicit statistical summarization rather than purely additive transformations.

### Mechanism 2: Parallel Integration with Multi-Head Self-Attention
HPT positions histogram layers parallel to MHSA (rather than sequentially) to inform attention computation with distributional context. Given layer-normalized features, the output combines residual connections: Z = X + MHSA(XLN) + H(XLN). This allows attention mechanisms to operate on features already augmented with global statistical context, potentially providing richer representations than sequential processing.

### Mechanism 3: Kaiming Initialization Enables Faster Convergence
HPT uses default Kaiming initialization for histogram layers, maintaining variance better than zero-initialized adapters and accelerating early training. This preserves forward/backward variance across layers, enabling gradient flow from iteration one. Zero-initialization creates a "cold start" where adapters must first learn to produce non-trivial outputs before useful adaptation begins.

## Foundational Learning

- **Parameter-Efficient Transfer Learning (PETL) taxonomy**: HPT is positioned as an "additive" PETL method; understanding where it fits among selective (BitFit), reparameterized (LoRA, SSF), and hybrid (UniPELT) approaches clarifies its design rationale. Quick check: Can you explain why HPT is classified as additive rather than reparameterized, given it introduces new parameters rather than modifying existing weights?

- **Distribution shift in transfer learning**: The paper's core hypothesis is that conventional adapters "struggle to capture distributional shifts in intermediate feature embeddings"—understanding what distribution shift means (covariate, label, or feature-level) is prerequisite. Quick check: When pre-training on ImageNet/AudioSet and fine-tuning on passive sonar, what types of distribution shift would you expect in intermediate feature embeddings?

- **Radial basis functions for soft binning**: The histogram layer uses RBFs with learnable centers and widths to compute soft bin assignments rather than hard thresholds. Quick check: Why might soft bin assignment be preferable to hard histogram binning for gradient-based learning?

## Architecture Onboarding

- **Component map**: Spectrogram (128-dim log Mel) → AST patch embedding → transformer blocks → HPT module (1×1 conv → RBF binning → adaptive pooling → broadcast) → parallel MHSA integration → classification head

- **Critical path**: Spectrogram → AST patch embedding → transformer blocks → At each designated block: XLN → H(XLN) computes histogram features → H(XLN) added to residual alongside MHSA output → Final CLS token → classifier

- **Design tradeoffs**: Bins vs parameters (4/8/16 bins yield 7.9K–18K shared parameters; more bins = more expressive but diminishing returns), Shared vs non-shared weights (shared reduces footprint; non-shared allows layer-specific distributional alignment), Placement (MHSA-parallel is default; FFN placement not empirically tested)

- **Failure signatures**: Saturating accuracy with adapters but not HPT (indicates distributional shift not captured by linear bottleneck), High variance in small datasets (ShipsEar shows σ=3.1 for HPT-16 vs σ=0.6 for adapters—suggests histogram instability with limited data), Feature similarity divergence at mid-layers (adapters deviate strongly from fully fine-tuned representations; HPT maintains closer alignment)

- **First 3 experiments**:
  1. Baseline comparison: Run linear probe, adapter (r=256/128/64), and HPT (4/8/16 bins) with shared weights on your target sonar dataset. Report accuracy, parameters, and training time.
  2. Convergence analysis: Plot validation loss curves for adapter-64 vs HPT-16. Measure epochs to 90% of final accuracy—expect faster convergence with HPT on larger datasets.
  3. Feature similarity diagnostic: Compute layer-wise CKA or cosine similarity between your HPT/adapter representations and a fully fine-tuned baseline. Verify that HPT maintains higher mid-layer alignment.

## Open Questions the Paper Calls Out

- **Can HPT be jointly optimized with reparametrized methods like LoRA to leverage complementary adaptation benefits?**: The conclusion states, "Another promising direction lies in combining HPT with other PETL techniques (e.g., LoRA), by joint optimization to leverage complementary benefits." The paper evaluates HPT strictly against other methods in isolation, not in combination.

- **Do adaptive binning strategies improve HPT robustness on downstream tasks with extremely sparse data?**: The discussion notes, "HPT's reliance on distributional modeling could become less beneficial for tasks with extremely sparse data... Addressing this would likely involve adaptive binning strategies." The current method uses fixed bins learned via convolution, which may fail to adequately capture distributions when training samples are few.

- **Do alternative binning functions provide a richer representation of feature distributions than the employed Radial Basis Function (RBF)?**: The conclusion explicitly suggests, "Future work could explore different binning functions to capture a richer distribution." The implementation relies specifically on RBFs to compute bin assignments; other function classes remain untested.

- **Can HPT be effectively integrated into diffusion-based generative models for scalable training?**: The conclusion proposes, "integrating HPT with diffusion-based approaches could lead to a more scalable framework for efficient training of generative models." The current work validates HPT only on discriminative passive sonar classification tasks.

## Limitations
- The distribution shift hypothesis is weakly supported by direct evidence; while HPT outperforms adapters on sonar tasks, there is limited ablation or diagnostic analysis proving that distributional modeling is the causal mechanism.
- Dataset-specific performance variations suggest the method may not generalize uniformly; HPT-16 shows higher variance on the smaller ShipsEar dataset (σ=3.1) compared to adapters (σ=0.6), raising concerns about stability with limited data.
- The comparative advantage of parallel MHSA integration over alternative placements (e.g., after FFN) remains untested, leaving open whether the parallel configuration is optimal or simply sufficient.

## Confidence
- **High confidence**: HPT achieves superior accuracy and parameter efficiency on the tested sonar datasets compared to conventional adapters; the architectural specification and experimental setup are clearly described.
- **Medium confidence**: HPT's faster convergence and better mid-layer feature similarity to fully fine-tuned models are well-documented, though the relative contribution of initialization versus architecture is not fully disentangled.
- **Medium confidence**: The distribution-aware adaptation mechanism is plausible given the empirical results, but direct evidence (e.g., distributional shift quantification or ablation studies) is limited.

## Next Checks
1. Conduct an ablation study comparing HPT with adapters initialized using Kaiming versus zero initialization to isolate the effect of initialization on convergence speed and performance.
2. Test alternative histogram layer placements (e.g., after FFN) to determine if parallel MHSA integration is optimal or if other configurations yield similar or better results.
3. Perform a feature distribution analysis (e.g., using Earth Mover's Distance or KL divergence) on target datasets to quantify distributional shifts and assess whether HPT's gains correlate with greater domain shift.