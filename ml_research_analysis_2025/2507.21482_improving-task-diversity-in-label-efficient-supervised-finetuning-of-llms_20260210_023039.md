---
ver: rpa2
title: Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs
arxiv_id: '2507.21482'
source_url: https://arxiv.org/abs/2507.21482
tags:
- task
- tasks
- data
- examples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a task-diversity-based approach for label-efficient
  supervised fine-tuning of large language models, addressing the challenge of reducing
  expensive human annotation while maintaining model performance. The method allocates
  annotation budgets across tasks using an inverse confidence weighting strategy,
  leveraging the fact that pre-trained models exhibit varying confidence levels across
  different tasks.
---

# Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs

## Quick Facts
- arXiv ID: 2507.21482
- Source URL: https://arxiv.org/abs/2507.21482
- Authors: Abhinav Arabelly; Jagrut Nemade; Robert D Nowak; Jifan Zhang
- Reference count: 22
- Primary result: Weighted Task Diversity achieves 4% higher MMLU scores and 53-54% win rates vs full-dataset baseline using 50-80% fewer annotations

## Executive Summary
This paper introduces a task-diversity-based approach for label-efficient supervised fine-tuning of large language models. The method addresses the challenge of reducing expensive human annotation while maintaining model performance by allocating annotation budgets across tasks using an inverse confidence weighting strategy. Experiments on FLAN V2 and Dolly datasets demonstrate that the Weighted Task Diversity method achieves performance comparable to or better than training on the full dataset while using 50-80% fewer annotations, with a 4% increase in MMLU score and 53-54% win rates in AlpacaEval comparisons against the full-dataset baseline.

## Method Summary
The approach consists of two algorithms: Task Diversity (uniform allocation across tasks) and Weighted Task Diversity (confidence-informed allocation). The method operates in a one-batch active learning setting where the base model generates responses for all unlabeled prompts, confidence scores are computed as the product of token-level probabilities, and budget allocation is determined by inverse confidence weighting with per-task clamping. Selected prompts are then annotated and used for SFT training with LoRA. The approach is simpler and more computationally efficient than existing diversity-based methods while producing superior or equivalent results.

## Key Results
- Weighted Task Diversity achieves 4% higher MMLU scores compared to full-dataset baseline
- 53-54% win rates in AlpacaEval comparisons against full-dataset baseline
- 50-80% reduction in annotation budget while maintaining or improving performance
- Outperforms K-Center and Random sampling methods across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Allocating annotation budget inversely to model confidence concentrates labels on tasks where the model is most uncertain, potentially yielding larger learning gains per labeled example. The base model generates responses for unlabeled prompts; confidence for each example is the product of token-level probabilities. Task-level average confidence is computed, and budget allocation per task is set proportional to the inverse of this confidence (with a minimum base allocation). This upweights tasks where the model's predictive distribution is less peaked, where error-correcting gradients are expected to be larger.

### Mechanism 2
Enforcing a minimum per-task allocation via clamping and round-robin sampling prevents task collapse and maintains broad coverage. After inverse-confidence weighting, a clamping operation ensures each task receives at least a base budget (e.g., 5 examples). A round-robin algorithm iterates across tasks, selecting examples until the total budget is exhausted. This guarantees no task is entirely ignored even if its confidence is high.

### Mechanism 3
Operating in a one-batch active learning setting avoids expensive iterative retraining while still capturing most of the learning value from strategic data selection. Selection uses only base model inference (no gradient computations or multi-round feedback). Prompts are selected once, annotated, and then SFT proceeds. This reduces logistical complexity and compute compared to iterative active learning.

## Foundational Learning

- **Concept**: Supervised Finetuning (SFT) of LLMs
  - Why needed here: The entire method operates within SFT; understanding how prompt-response pairs shape model behavior is essential to grasp why data selection matters.
  - Quick check question: Can you explain how SFT differs from pretraining and RLHF in terms of data and training objective?

- **Concept**: Uncertainty Quantification for Active Learning
  - Why needed here: The method relies on confidence-based uncertainty to guide selection; distinguishing epistemic vs aleatoric uncertainty and their proxies is critical.
  - Quick check question: Given a model's token probabilities for two prompts, how would you compute and compare their confidence scores using the paper's definition?

- **Concept**: Prompt-Diversity vs Task-Diversity
  - Why needed here: The paper explicitly contrasts embedding-space prompt-diversity methods with task-level diversity; understanding this clarifies the contribution.
  - Quick check question: For a dataset where prompts cluster in embedding space but belong to only two task categories, would K-Center and Task Diversity select similar subsets? Why or why not?

## Architecture Onboarding

- **Component map**: Base Model Inference Engine -> Task Confidence Aggregator -> Budget Allocator -> Round-Robin Sampler -> SFT Trainer

- **Critical path**: 1) Run inference on all unlabeled prompts; store per-token probabilities. 2) Compute per-example confidence, aggregate to per-task average confidence. 3) Allocate budget across tasks using inverse confidence with minimum allocation. 4) Execute round-robin sampling to select exact prompts per task. 5) Annotate selected prompts and run SFT.

- **Design tradeoffs**: Minimum per-task allocation (higher values increase coverage but reduce focus on uncertain tasks); task granularity (coarse tasks simplify allocation but may hide within-task diversity); confidence metric (product of probabilities is length-sensitive, alternatives may be more robust).

- **Failure signatures**: High allocation to low-confidence tasks with many redundant examples wastes budget; tasks with very few examples hit availability constraints, distorting intended proportions; confidence estimates dominated by pretraining exposure cause oversampling of familiar domains; random sampling outperforms proposed method, indicating misaligned task labels or confidence signal.

- **First 3 experiments**: 1) Replicate on a small open dataset (e.g., Dolly subset) with LLaMA-2 7B; verify Weighted Task Diversity matches random at full budget and exceeds it at 50% budget. 2) Ablate minimum per-task allocation (0 vs 5 vs 10 examples) to measure impact on coverage and performance. 3) Compare confidence metrics (product of probabilities vs entropy vs margin) to test robustness of the uncertainty signal.

## Open Questions the Paper Calls Out

- **Question**: Does the inverse confidence weighting strategy generalize to model architectures beyond LLaMA-2 7B?
- **Basis in paper**: [explicit] The authors state in the Limitations section: "Our experiments primarily focus on the LLaMA-2 7B architecture, and extending these findings to other model architectures represents a promising direction for future work."
- **Why unresolved**: All experiments use a single architecture, leaving unclear whether the approach works for models with different pre-training distributions, tokenizer designs, or scale.
- **What evidence would resolve it**: Experiments applying the same task diversity methods to other architectures (e.g., Mistral, Gemma, or larger LLaMA variants) showing comparable annotation cost reductions and performance gains.

- **Question**: Can task categorizations be automatically inferred for datasets lacking explicit task labels while maintaining selection effectiveness?
- **Basis in paper**: [explicit] The Limitations section notes: "There may be specific domains where such categorizations are less defined. In these cases, automated methods for identifying task definitions and categorizations could be beneficial."
- **Why unresolved**: The method relies on pre-existing task labels from curated datasets, but real-world scenarios may lack such annotations.
- **What evidence would resolve it**: A study comparing automatic task clustering (using embedding-based or semantic methods) against ground-truth task labels, measuring performance degradation when using inferred vs. true categorizations.

- **Question**: How does the choice of base allocation (5 examples per task) affect performance across different budget regimes and dataset compositions?
- **Basis in paper**: [inferred] The method sets "a small base budget (e.g., 5 examples)" but this hyperparameter is not systematically varied or analyzed across experiments.
- **Why unresolved**: The sensitivity of results to this threshold remains unknown, particularly for datasets with highly uneven task distributions or when budgets are extremely constrained.
- **What evidence would resolve it**: Ablation experiments varying the base allocation from 0 to higher values across multiple budgets and datasets, analyzing how threshold changes impact final model performance.

## Limitations
- Assumes task labels are available and meaningful for allocation, which is not guaranteed in all datasets
- Confidence metric (product of token probabilities) may be length-sensitive and dominated by pretraining exposure
- One-batch selection strategy lacks validation against iterative methods that could potentially yield better results
- Performance depends on task granularity - coarse tasks simplify allocation but may hide within-task diversity

## Confidence
- **High confidence**: Weighted Task Diversity achieves comparable or better performance than full-dataset training with 50-80% fewer annotations
- **Medium confidence**: Inverse confidence weighting effectively identifies high-value examples for annotation
- **Low confidence**: One-batch active learning is sufficient for capturing most learning value from strategic data selection

## Next Checks
1. Compare one-batch vs iterative selection: Implement a 2-3 round active learning baseline using the same confidence metric and compare performance to the one-batch approach to quantify the efficiency-accuracy tradeoff.

2. Test confidence metric robustness: Replace the product-of-probabilities confidence metric with alternatives (entropy, margin-based) and evaluate whether the selection quality and performance gains are maintained across different uncertainty measures.

3. Evaluate task granularity impact: Create synthetic datasets with varying task granularity (e.g., merge/split existing tasks) and test whether the method's performance is sensitive to task definition, identifying optimal granularity for this approach.