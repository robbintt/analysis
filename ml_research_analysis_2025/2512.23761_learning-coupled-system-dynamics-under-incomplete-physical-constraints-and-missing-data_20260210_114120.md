---
ver: rpa2
title: Learning Coupled System Dynamics under Incomplete Physical Constraints and
  Missing Data
arxiv_id: '2512.23761'
source_url: https://arxiv.org/abs/2512.23761
tags:
- data
- error
- system
- sparsity
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a sparse multitask neural network framework
  for learning coupled system dynamics when only partial physical constraints and
  data are available. The approach integrates physics-informed loss terms with data-driven
  learning, employing sparsity regularization and mesh-free sampling to improve computational
  efficiency and model compression.
---

# Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data

## Quick Facts
- arXiv ID: 2512.23761
- Source URL: https://arxiv.org/abs/2512.23761
- Reference count: 40
- Primary result: Sparse multitask neural network recovers coupled system solutions with 90% parameter reduction and <10% relative ℓ₂ error when physics and data are mutually exclusive across variables.

## Executive Summary
This paper introduces MUSIC, a sparse multitask neural network framework that learns full-dimensional solutions of coupled PDE systems when physical constraints and observational data are incomplete and mutually exclusive. The approach uses a shared neural network to simultaneously predict all system variables, with data variables trained via supervised loss and equation variables constrained by PDE residuals computed through automatic differentiation. Hard-concrete regularization induces structured sparsity, achieving up to 90% parameter reduction while maintaining accuracy below 10% relative ℓ₂ error across four benchmark systems.

## Method Summary
MUSIC employs a multitask fully-connected neural network with shared hidden layers that outputs predictions for all system variables simultaneously. For variables with available data but unknown physics ("data variables"), supervised loss penalizes prediction-data mismatch. For variables with known governing equations but no data ("equation variables"), physics-informed loss penalizes PDE residual violations computed via automatic differentiation. The framework uses mesh-free random sampling of spatiotemporal points, eliminating the need for uniform collocation grids. Hard-concrete regularization induces structured ℓ₀ sparsity, with iterative weight thresholding as an alternative for systems dominated by zero values. All inputs and outputs are normalized to [0,1] using min-max scaling.

## Key Results
- Relative ℓ₂ errors consistently below 10% across all four test systems (SWE, FN, λ-ω, wildfire) under varying noise levels and data scarcity
- Model sparsity reduces parameter counts by up to 90% without sacrificing accuracy, outperforming non-sparse baselines
- MUSIC handles discontinuous solutions and outperforms existing approaches in both accuracy and computational efficiency
- Structured sparsity (7.93% nonzero) achieves 3.98% error vs. unstructured sparsity (8% nonzero) achieving 73.83% error on SWE system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask neural networks can recover full-dimensional coupled system solutions when data and physics constraints are mutually exclusive across variables.
- Mechanism: A shared-weight neural network simultaneously outputs predictions for all variables. For "data variables" (where observations exist but no governing equation), a supervised loss penalizes prediction-data mismatch. For "equation variables" (where governing PDEs are known but no data exists), automatic differentiation computes PDE residuals at sampled spatiotemporal points, penalizing physics violation. The coupling emerges because the physics loss for equation variables requires predicted values of data variables as inputs, forcing the network to learn consistent joint representations.
- Core assumption: The coupled system's variables share learnable structure that can be captured through shared hidden layers; the known physics for one variable provides sufficient inductive bias to constrain the full system.
- Evidence anchors:
  - [abstract]: "MUSIC, a sparsity induced multitask neural network framework that integrates partial physical constraints with data-driven learning to recover full-dimensional solutions of coupled systems when physics-constrained and data-informed variables are mutually exclusive."
  - [Section 2, Eq. 3-4]: Loss combines data fitting terms ∥uᵢ − ûᵢ∥² for data variables with PDE residual terms ∥∂ûⱼ/∂t − Fⱼ(û₁,...,ûₙ)∥² for equation variables.
  - [corpus]: Neighbor paper "Learning Neural Operators from Partial Observations" addresses related partial observation challenges but assumes different structure; MUSIC's mutual exclusivity assumption appears novel.
- Break condition: If data and equation variables are not truly coupled (weak cross-dependencies in F), physics loss provides minimal constraint on data variable learning, leading to unconstrained solutions.

### Mechanism 2
- Claim: Structured ℓ₀ sparsity regularization via hard-concrete distribution improves generalization under data scarcity while achieving up to 90% parameter reduction.
- Mechanism: Binary "gates" zⱼ ∈ {0,1} multiply each neuron output, with gate probabilities πⱼ learned jointly with weights. The hard-concrete distribution provides a continuous relaxation enabling gradient-based optimization while inducing near-binary gates at convergence. Gates that remain "off" after training are pruned, yielding compressed subnetworks. Sparsity acts as implicit regularization, reducing overfitting capacity when training data is limited.
- Core assumption: The true solution mapping can be represented by a sparse subnetwork within the overparameterized architecture; gradient descent can discover this subnetwork.
- Evidence anchors:
  - [abstract]: "model sparsity reducing parameter counts by up to 90% without sacrificing accuracy"
  - [Section 2, Eq. 5]: Loss includes Λ∥Θ∥₀ penalty; Section 2 describes hard-concrete approximation zⱼ = min(1, max(0, sⱼ)) with stretch parameters.
  - [Table 9]: Structured sparsity (7.93% nonzero) achieves 3.98% error vs. unstructured sparsity (8% nonzero) achieving 73.83% error on SWE system.
  - [corpus]: Weak corpus signal on L₀ specifically for PDE solving; most physics-informed ML uses L₂ regularization.
- Break condition: For systems dominated by constant/zero values (e.g., wildfire β variable), structured sparsity can cause "model collapse" where all gates turn off; unstructured thresholding becomes necessary.

### Mechanism 3
- Claim: Mesh-free random sampling eliminates the need for uniform collocation grids and separate collocation points, using the same randomly sampled spatiotemporal points for both data fitting and physics regularization.
- Mechanism: Rather than requiring uniformly gridded training data (often impractical for real sensors), the method samples Nₛ spatial and Nₜ temporal points uniformly at random from the domain. These same points serve dual purpose: where ground-truth data exists, they contribute to data loss; at all points, they contribute to physics loss via automatic differentiation. Initial conditions are explicitly sampled since t=0 is unlikely to be randomly selected.
- Core assumption: Random sampling provides sufficient coverage of the solution manifold; the physics residual evaluated at sampled points generalizes to unsampled regions.
- Evidence anchors:
  - [abstract]: "MUSIC employs mesh-free (random) sampling of training data"
  - [Section 2]: "The multitask DNN uses the training data for enforcing both data fitting as well as physical constraints, leading to the omission of separate collocation points."
  - [Section 4.1]: "In our proposed model we do not include any loss terms with respect to the boundary conditions... mesh free sampling of data... also selects points from the spatial boundary" for dimensions ≥ 2.
  - [corpus]: "Incomplete Data, Complete Dynamics" addresses irregularly sampled data but uses diffusion models rather than PINN-style approaches.
- Break condition: For 1D spatial domains, boundary points may never be sampled, causing incorrect solutions at domain edges unless boundary conditions are explicitly enforced.

## Foundational Learning

- **Physics-Informed Neural Networks (PINNs)**
  - Why needed here: MUSIC extends PINN methodology to incomplete-physics scenarios; without understanding PINN loss formulation (data + physics residuals + boundary/initial conditions), the mutual exclusivity formulation won't make sense.
  - Quick check question: Can you write the standard PINN loss for solving ∂u/∂t = F(u) with data {(xᵢ, tᵢ, uᵢ)}?

- **Automatic Differentiation for PDE Residuals**
  - Why needed here: Computing ∂û/∂t and spatial derivatives ∇û from neural network outputs is fundamental to evaluating physics loss; numerical differentiation would introduce errors incompatible with the framework.
  - Quick check question: Given a neural network û(x,t; Θ), how would you compute ∂²û/∂x² using PyTorch or JAX?

- **Sparse Regularization (L₀/L₁) and Pruning**
  - Why needed here: The hard-concrete distribution and weight thresholding mechanisms require understanding the tradeoff between sparsity (compression, regularization) and expressiveness; structured vs. unstructured sparsity have different failure modes.
  - Quick check question: Why does L₀ regularization require continuous relaxation (e.g., hard-concrete, concrete dropout) for gradient-based optimization?

## Architecture Onboarding

- **Component map**:
  - Input layer: (d+1) dimensions (spatial coordinates + time) → H hidden units
  - Hidden layers: L-1 layers of H × H with activation ϕ (ReLU for discontinuous/positive solutions; sinusoidal for wave-like patterns)
  - Output layer: H → n units (one per system variable)
  - Sparsity module: Hard-concrete gates per neuron (structured) OR iterative magnitude thresholding (unstructured)
  - Loss aggregator: Weighted sum of data terms, physics residual terms, initial condition terms, and sparsity penalty

- **Critical path**:
  1. Normalize all inputs/outputs to [0,1] via min-max scaling (essential for stable training)
  2. Initialize dense network; train 10k epochs without sparsity to prevent premature collapse
  3. Enable sparsity regularization; train with ADAM (lr 10⁻² to 10⁻⁴)
  4. Validate on held-out 20%; tune λ₀ (sparsity weight) via validation error
  5. Post-training: prune "off" gates (structured) or zero sub-threshold weights (unstructured)

- **Design tradeoffs**:
  - Structured (neuron) sparsity: Better compression (removes entire neurons), works for most systems, but can collapse on zero-dominated data
  - Unstructured (weight) sparsity: More stable on sparse/zero-dominated data, but less compression (scattered weights keep all neurons active)
  - More layers/neurons: Lower error but risk overfitting with scarce data; Table 1 shows 6-layer/50-neuron worse than 4-layer/20-neuron for SWE
  - Spatial vs. temporal sampling: Increasing Nₛ helps more than increasing Nₜ (Figure 6); prioritize spatial coverage

- **Failure signatures**:
  - High error at domain boundaries (1D systems): Mesh-free sampling missed boundary points → explicitly add boundary condition loss
  - Model predicts constant zero: Structured sparsity collapsed on zero-dominated data → switch to unstructured thresholding
  - Error grows exponentially in forecast domain: Training interval too short → ensure training covers sufficient dynamics evolution before forecasting
  - Divergent training loss: Learning rate too high or normalization missing → verify all data in [0,1], reduce lr to 10⁻⁴

- **First 3 experiments**:
  1. Reproduce SWE dam-break results with 4 layers, 20 neurons, Nₛ=100, Nₜ=800; verify ~4% error and ~8% nonzero parameters. This validates basic implementation.
  2. Ablate sparsity: Compare MUSIC vs. PIML_inc (no L₀) on FN system with Nₛ=500, noise=10%. Confirm regularization benefit emerges under noise/scarcity.
  3. Test domain shift: Train on t∈[0,0.5], predict on t∈(0.5,1] for SWE. Measure error growth to understand temporal generalization limits for your application.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the accuracy of the recovered solution degrade when the provided physical equation for the "equation variable" contains model form errors or approximated coefficients rather than exact analytical forms?
- **Basis in paper:** [explicit] Section 4.6 (Limitations) states, "If any term in the equation is unknown or inaccurately modeled, the accuracy of the learned solutions could be compromised."
- **Why unresolved:** The current study assumes perfect knowledge of the governing physics for the equation variable ($F_{k+1}, \dots, F_n$) during training; sensitivity to perturbations or structural errors in these equations was not quantified.
- **What evidence would resolve it:** A sensitivity analysis measuring the relative $\ell_2$ error of the full-field solution against increasing perturbations in the coefficients or structure of the known governing equations.

### Open Question 2
- **Question:** Can the MUSIC framework be extended to simultaneously discover the governing equations for the data variable while solving for the full-field solution?
- **Basis in paper:** [inferred] The introduction reviews methods for discovering equations (SINDy, PINN-SR) but notes they fail when priors are disjoint. The current method solves the system but does not identify the unknown physics of the data variable.
- **Why unresolved:** The current loss function treats the physical constraints for the equation variable as fixed priors and does not include a mechanism for sparse regression or symbolic regression on the data variable side.
- **What evidence would resolve it:** A modified framework that outputs both the system solution and an identifiable symbolic representation of the data variable's dynamics, validated on a benchmark system where the "unknown" dynamics are withheld during training.

### Open Question 3
- **Question:** Can the computational latency of iterative hard thresholding be reduced for systems with constant-dominated regimes without causing the model collapse associated with structured sparsity?
- **Basis in paper:** [explicit] Section 4.6 notes that for systems like the wildfire model, "structured sparsity can lead to model collapse," forcing the use of unstructured sparsity where "thresholding algorithms can make the model training slower."
- **Why unresolved:** There is currently a trade-off where the stable method (unstructured sparsity/thresholding) is computationally slower, while the faster method (structured/neuron sparsity) risks learning trivial zero-solutions for sparse data.
- **What evidence would resolve it:** An optimization algorithm or regularization schedule that achieves training speeds comparable to structured sparsity while maintaining stability on datasets dominated by zero-valued regions.

## Limitations
- The framework assumes perfect knowledge of governing equations for the equation variable; inaccuracies in these equations compromise solution accuracy.
- Structured sparsity can cause model collapse for systems with constant or zero-dominated data, requiring less efficient unstructured thresholding alternatives.
- Mesh-free sampling may miss boundary points in 1D systems, leading to incorrect solutions at domain edges without explicit boundary condition enforcement.

## Confidence
- **High confidence**: The mutual exclusivity formulation (data variables + equation variables) is well-defined and consistently implemented across experiments; error reduction claims (10% relative ℓ₂) are reproducible with specified architectures.
- **Medium confidence**: Sparsity regularization benefits (90% parameter reduction) are demonstrated but depend critically on unknown hard-concrete hyperparameters; unstructured vs. structured sparsity choice requires system-specific assessment.
- **Low confidence**: Generalization to higher-dimensional systems (3D+) is untested; the claim that mesh-free sampling "eliminates" collocation grids needs validation for irregular domains.

## Next Checks
1. **Hyperparameter sensitivity**: Sweep hard-concrete β values (0.5-5) on FN system; verify sparsity patterns converge to similar subnetwork structures.
2. **Boundary condition ablation**: Compare 1D SWE results with and without explicit boundary loss terms; quantify error reduction at x=0 and x=L.
3. **Temporal generalization stress test**: Train on t∈[0,0.5] for SWE, predict t∈(0.5,1]; measure error growth rate to identify forecasting limitations.