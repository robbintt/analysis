---
ver: rpa2
title: Extracting Disaster Impacts and Impact Related Locations in Social Media Posts
  Using Large Language Models
arxiv_id: '2511.21753'
source_url: https://arxiv.org/abs/2511.21753
tags:
- location
- locations
- disaster
- social
- impacted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses Large Language Models (LLMs) to identify disaster
  impacts and impacted locations in social media posts. A novel corpus (DILC) is developed
  to annotate impacts and impacted locations, addressing the gap in distinguishing
  affected from non-affected locations.
---

# Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models

## Quick Facts
- arXiv ID: 2511.21753
- Source URL: https://arxiv.org/abs/2511.21753
- Reference count: 40
- This study uses Large Language Models (LLMs) to identify disaster impacts and impacted locations in social media posts, achieving F1-scores of 0.69 for impact and 0.74 for impacted location extraction.

## Executive Summary
This study addresses the challenge of extracting disaster impacts and impacted locations from social media posts using Large Language Models (LLMs). The researchers developed a novel corpus (DILC) annotated with both impact types and impacted locations, addressing a gap in distinguishing affected from non-affected locations. Through fine-tuning Llama 3.2 3B with domain-specific data and implementing post-processing to reduce hallucinations, the approach demonstrates significant improvements over pre-trained models and traditional NER tools. The findings show LLMs' potential for actionable situational awareness in disaster response, with disaster-specific fine-tuning yielding better impact extraction and cross-disaster training improving impacted location recognition.

## Method Summary
The researchers fine-tuned Llama 3.2 3B using LoRA with DILC corpus data, employing hyperparameters including learning rate of 2e-4, batch size of 8, and LoRA rank of 16. They used persona-based prompts with 6-shot examples and implemented post-processing validation to reduce hallucinations by checking predictions against source text. The evaluation employed token-level precision/recall/F1 with exact match scoring. For comparison, they also tested Llama 3.3 70B in zero-shot configuration with and without post-processing.

## Key Results
- Fine-tuned Llama 3.2 3B achieved F1 scores of 0.69 for impact extraction and 0.74 for impacted location extraction
- Post-processing validation improved all-locations extraction from F1 0.77 to 0.85 with Llama 3.3 70B
- Disaster-specific fine-tuning yielded F1 0.86 for earthquake impacts versus 0.83 for cross-disaster training
- Cross-disaster fine-tuning outperformed disaster-specific training for impacted location extraction (F1 0.77 vs 0.65 for wildfire data)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning improves impact type extraction more than general disaster fine-tuning
- Mechanism: Disaster-specific training data contains linguistic patterns and terminology unique to each disaster type, enabling the model to learn disaster-conditional impact vocabulary that general disaster corpora dilute.
- Core assumption: Impact types exhibit stronger lexical patterns within disaster types than across them.
- Evidence anchors: Section 6 shows earthquake-specific fine-tuning achieved F1 0.86 versus 0.83 for all-disaster fine-tuning on Pakistan Earthquake dataset.
- Break condition: If impact types share near-identical linguistic patterns across disaster types, disaster-specific fine-tuning would show no advantage.

### Mechanism 2
- Claim: Post-processing against source text reduces hallucinated locations and corrects frequency miscounts
- Mechanism: LLMs generate outputs autoregressively without inherent verification; a deterministic string-matching step validates each predicted location against the original tweet, removing non-existent locations and adjusting frequency counts.
- Core assumption: Hallucinations manifest as outputs not verbatim present in the source text.
- Evidence anchors: Section 6, Table 7 shows Llama 3.3 70B Basic zero-shot improves from F1 0.77 to 0.85 for all locations with post-processing.
- Break condition: If hallucinations take the form of semantically correct but lexically different location names, simple string matching would incorrectly reject valid predictions.

### Mechanism 3
- Claim: Cross-disaster fine-tuning improves impacted location extraction better than disaster-specific fine-tuning
- Mechanism: Impacted location recognition requires distinguishing impacted from non-impacted locations within context; exposure to diverse disaster scenarios trains the model on the general semantic cue of "location + proximal impact description" rather than disaster-specific vocabulary.
- Core assumption: The syntactic/semantic relationship between impact descriptions and locations generalizes across disaster types.
- Evidence anchors: Section 6 shows all-disaster fine-tuning achieves F1 0.77 for Greece Wildfire impacted locations versus 0.65 for wildfire-specific fine-tuning.
- Break condition: If location-impact relationships are highly disaster-specific, cross-disaster training would introduce noise rather than generalization.

## Foundational Learning

- Concept: Named Entity Recognition (NER) as sequence labeling vs. generative extraction
  - Why needed here: Traditional NER assigns token-level labels; LLMs generate text spans. The shift to generative extraction enables extracting "impacted locations" as a relational concept rather than just "locations."
  - Quick check question: Can you explain why a model might extract "Mati" as a location but fail to determine whether it was impacted?

- Concept: Fine-tuning with LoRA (Low-Rank Adaptation)
  - Why needed here: The paper fine-tunes Llama 3.2 3B using LoRA to adapt the model without full parameter updates, making disaster-specific adaptation computationally feasible.
  - Quick check question: What does freezing most model weights and only training low-rank matrices achieve compared to full fine-tuning?

- Concept: Prompt engineering strategies (Zero-shot, Few-shot, Persona, Chain-of-Thought)
  - Why needed here: The paper tests Basic, Persona, and CoT prompts with 0-6 examples. Understanding how prompt framing affects extraction quality is essential for reproducing results.
  - Quick check question: Why might a persona prompt ("Act as an emergency responder") improve extraction of actionable information compared to a basic prompt?

## Architecture Onboarding

- Component map: DILC corpus (1,461 tweets) -> Llama 3.2 3B (fine-tuned) -> Prompt templates -> LLM generation -> Post-processing validation -> Token-level evaluation
- Critical path: Corpus annotation (BRAT tool) -> Fine-tuning (LoRA, rank=16, α=16, lr=2e-4) -> Inference (temperature=0, top_p=0.9) -> Post-processing (string validation) -> Evaluation (exact match scoring)
- Design tradeoffs:
  - Larger model (70B) vs. fine-tuned smaller model (3B): 70B better for all-locations task; fine-tuned 3B outperforms pre-trained 70B on impact extraction (F1 0.69 vs. 0.60)
  - Disaster-specific vs. cross-disaster training: Specific better for impacts; cross-disaster better for impacted locations
  - Post-processing string matching: Reduces hallucinations but risks rejecting valid paraphrases
- Failure signatures:
  - Span variation: Model predicts "Jatlaan" when gold standard is "Jatlaan Canal"
  - Granularity mismatch: Model selects country-level location when city-level impact is described
  - Frequency hallucination: Model reports location appearing once when it appears twice
  - Impact-location confusion: Model outputs impact word as location (e.g., "flood" as location)
- First 3 experiments:
  1. Replicate all-locations extraction with Llama 3.3 70B + post-processing on a held-out subset of DILC; verify F1 improvement from ~0.77 to ~0.85
  2. Fine-tune Llama 3.2 3B on earthquake-only subset of DILC; compare impact F1 against all-disaster fine-tuning to validate disaster-specific advantage
  3. Ablate post-processing: Run inference with and without string validation on same test set; quantify hallucination reduction rate and false rejection rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fine-tuned LLM framework be extended to extract granular impact information, specifically regarding severity levels and affected entities (e.g., buildings, roads, people)?
- Basis in paper: Section 8 states the aim to develop "granular impact information on severity and affected entities (e.g., buildings, roads, people)" to elevate practical utility.
- Why unresolved: The current DILC corpus and models focus on classifying "types of impact" and identifying "impacted locations," but do not capture the intensity of the impact or the specific objects/victims involved.
- What evidence would resolve it: A modified annotation schema labeling severity and entities, followed by experiments showing high F1-scores for these new extraction tasks.

### Open Question 2
- Question: To what extent does incorporating multilingual datasets improve the adaptability and robustness of the proposed models across diverse linguistic and cultural contexts?
- Basis in paper: Section 8 explicitly lists extending the framework by "incorporating multilingual datasets" to improve adaptability as a primary future aim.
- Why unresolved: The current study is restricted to English tweets, and the authors note that performance varies even between native and non-native English-speaking regions.
- What evidence would resolve it: Successful fine-tuning results on a multilingual disaster corpus demonstrating stable performance across different languages.

### Open Question 3
- Question: Do span-based evaluation metrics, such as Soft Jaccard, provide a more accurate assessment of model performance than exact match F1-scores given the prevalence of partial matches?
- Basis in paper: Section 7 notes that "span variation" often leads to partial matches and suggests employing "overlap-based measures such as Soft Jaccard."
- Why unresolved: The current evaluation uses strict exact-match classification, which penalizes models for partial span errors common in natural language generation.
- What evidence would resolve it: A comparative evaluation showing that span-based metrics correlate better with human judgment of utility in disaster scenarios.

### Open Question 4
- Question: How does the integration of geocoding with the extracted impacted locations influence the precision of resource targeting in operational disaster response?
- Basis in paper: Section 8 identifies "geocoding extracted locations for precise resource targeting" as a necessary step to advance the system into an end-to-end disaster management tool.
- Why unresolved: The current research stops at text extraction; it does not map the extracted "impacted locations" to geographic coordinates to verify spatial accuracy.
- What evidence would resolve it: A pipeline that successfully converts extracted location text into map coordinates with high spatial resolution relative to ground truth data.

## Limitations

- The study relies on a single corpus (DILC) of 1,461 tweets across 19 disasters, limiting generalizability to other disaster types or regions
- The post-processing approach uses simple string matching that may incorrectly reject valid paraphrases or handle cultural variations in location naming
- The absence of temporal validation means the model's performance on emerging, real-time disaster events remains untested
- The evaluation framework's exact span matching penalizes partial predictions, potentially underestimating model performance in practical scenarios

## Confidence

- **High**: The mechanism of post-processing reducing hallucinations (verified by F1 improvement from 0.77 to 0.85) and the cross-disaster advantage for impacted location extraction are well-supported by ablation results
- **Medium**: Disaster-specific fine-tuning advantages for impact extraction have mixed support—while the Pakistan Earthquake dataset shows clear improvement, the mechanism's generalizability across all disaster types needs broader validation
- **Low**: The superiority claims over traditional NER tools are based on single-dataset comparisons without ablation studies to isolate whether improvements stem from model architecture or task formulation differences

## Next Checks

1. Test the fine-tuned model on an external disaster dataset (e.g., CrisisBench) to evaluate cross-dataset generalization and verify whether disaster-specific advantages persist beyond DILC
2. Implement fuzzy string matching in post-processing to quantify trade-offs between hallucination reduction and valid paraphrase acceptance, comparing results against the current exact-match approach
3. Conduct temporal validation by simulating real-time deployment—train on historical disasters and test on recent events (post-2023) to assess performance decay and adaptation needs for emerging disaster scenarios