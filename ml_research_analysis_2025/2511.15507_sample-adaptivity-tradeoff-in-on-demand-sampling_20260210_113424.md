---
ver: rpa2
title: Sample-Adaptivity Tradeoff in On-Demand Sampling
arxiv_id: '2511.15507'
source_url: https://arxiv.org/abs/2511.15507
tags:
- algorithm
- sample
- complexity
- round
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work formalizes the tradeoff between sample complexity and
  round complexity in on-demand sampling for multi-distribution learning. The core
  method involves introducing a new framework called Optimization via On-Demand Sampling
  (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing
  MDL algorithms.
---

# Sample-Adaptivity Tradeoff in On-Demand Sampling

## Quick Facts
- arXiv ID: 2511.15507
- Source URL: https://arxiv.org/abs/2511.15507
- Reference count: 40
- Primary result: Introduces OODS framework showing sample complexity scales as dkΘ(1/r)/ε for r-round algorithms in MDL

## Executive Summary
This paper formalizes the fundamental tradeoff between sample complexity and round complexity in on-demand sampling for multi-distribution learning. The authors introduce the Optimization via On-Demand Sampling (OODS) framework, which abstracts the sample-adaptivity tradeoff and captures most existing multi-distribution learning algorithms. The work establishes that for realizable multi-distribution learning, the optimal sample complexity scales as dkΘ(1/r)/ε for an r-round algorithm, demonstrating that even a small constant number of rounds can significantly improve upon fully non-adaptive approaches.

For agnostic multi-distribution learning, the paper presents an algorithm achieving near-optimal sample complexity of Õ((d+k)/ε²) within Õ(√k) rounds. The analysis leverages LazyHedge techniques adapted from multi-distribution learning algorithms. The paper also establishes nearly tight lower bounds for OODS, showing that achieving sub-polynomial round complexity would require fundamentally new techniques, providing insight into the inherent hardness of the problem.

## Method Summary
The paper introduces the Optimization via On-Demand Sampling (OODS) framework as a unifying abstraction for studying the sample-adaptivity tradeoff in multi-distribution learning. OODS formalizes how algorithms can adaptively request samples from multiple distributions across rounds, balancing the benefits of adaptivity against the costs in sample complexity. The framework captures existing multi-distribution learning algorithms and enables systematic analysis of the round-sample complexity relationship. For realizable MDL, the analysis shows optimal sample complexity scales as dkΘ(1/r)/ε, while for agnostic MDL, the approach achieves near-optimal Õ((d+k)/ε²) sample complexity within Õ(√k) rounds using techniques adapted from LazyHedge algorithms.

## Key Results
- OODS framework captures most existing MDL algorithms and formalizes sample-adaptivity tradeoff
- Realizable MDL achieves optimal sample complexity dkΘ(1/r)/ε with r rounds, showing small constant rounds significantly improve over non-adaptive approaches
- Agnostic MDL achieves near-optimal Õ((d+k)/ε²) sample complexity within Õ(√k) rounds
- Nearly tight lower bounds established, indicating sub-polynomial round complexity requires fundamentally new techniques

## Why This Works (Mechanism)
The mechanism works by providing a formal framework that captures the fundamental tension between adaptivity and sample efficiency in multi-distribution learning. By allowing algorithms to request samples adaptively across rounds, OODS enables more efficient information gathering about multiple distributions compared to non-adaptive approaches. The LazyHedge techniques adapted for agnostic MDL provide a way to balance exploration across distributions while maintaining theoretical guarantees. The lower bound analysis reveals the inherent hardness of reducing round complexity without sacrificing sample efficiency, establishing fundamental limits on how much adaptivity can improve performance.

## Foundational Learning
- **Multi-Distribution Learning (MDL)**: Why needed - The problem setting where algorithms must learn from multiple distributions simultaneously. Quick check - Can you identify scenarios where learning from multiple related distributions is beneficial versus learning each separately?

- **Sample-Adaptivity Tradeoff**: Why needed - Understanding the balance between how many rounds of adaptation are allowed versus total samples needed. Quick check - Can you explain why more rounds of adaptivity might reduce total sample requirements in certain learning problems?

- **LazyHedge Techniques**: Why needed - Algorithmic approach for maintaining multiple experts/strategies with provable regret guarantees. Quick check - Can you describe how LazyHedge differs from standard Hedge/Multiplicative Weights algorithms in terms of update frequency?

- **On-Demand Sampling**: Why needed - The model where algorithms can request specific samples rather than receiving them passively. Quick check - Can you identify practical scenarios where on-demand sampling would be more efficient than passive sampling?

## Architecture Onboarding
**Component Map**: OODS framework -> Sample complexity analysis -> Lower bound establishment -> Algorithm design
**Critical Path**: Framework definition → Realizable MDL analysis → Agnostic MDL algorithm → Lower bound proof
**Design Tradeoffs**: Adaptivity vs sample complexity vs computational efficiency
**Failure Signatures**: Infeasible sample requirements, computational intractability, failure to achieve desired accuracy
**First Experiments**:
1. Empirical validation of OODS framework on synthetic MDL datasets with varying numbers of distributions
2. Comparison of sample complexity between fully adaptive, partially adaptive, and non-adaptive approaches
3. Testing agnostic MDL algorithm performance on real-world multi-distribution learning problems

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Analysis relies heavily on idealized assumptions about MDL problem structure, particularly the realizability condition
- Lower bound results depend on specific problem formulations that may not capture all real-world scenarios
- Focus on worst-case bounds provides theoretical guarantees but may not translate directly to empirical performance across diverse datasets

## Confidence
- High confidence in the OODS framework's ability to capture existing MDL algorithms
- Medium confidence in the sample complexity bounds for realizable MDL due to strong assumptions
- Medium confidence in the agnostic MDL results given the empirical success of LazyHedge techniques
- Low confidence in the practical implications of the lower bounds without empirical validation

## Next Checks
1. Empirical validation of the OODS framework on benchmark MDL datasets to verify theoretical guarantees hold in practice
2. Extension of the analysis to more realistic non-realizable scenarios with noisy or corrupted data
3. Investigation of alternative algorithmic approaches that could potentially break the established lower bounds while maintaining practical efficiency