---
ver: rpa2
title: 'WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous
  Affective Computing'
arxiv_id: '2510.15221'
source_url: https://arxiv.org/abs/2510.15221
tags:
- emotional
- emotion
- data
- dataset
- affective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WELD presents a groundbreaking 733,651-record longitudinal dataset
  of workplace emotions collected over 30.5 months from 38 employees in a natural
  office setting. The dataset uniquely captures facial expression data during the
  COVID-19 pandemic, including responses to major events like the Shanghai lockdown.
---

# WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing

## Quick Facts
- arXiv ID: 2510.15221
- Source URL: https://arxiv.org/abs/2510.15221
- Reference count: 32
- Primary result: 733,651-record longitudinal workplace emotion dataset collected over 30.5 months from 38 employees

## Executive Summary
WELD presents a groundbreaking 733,651-record longitudinal dataset of workplace emotions collected over 30.5 months from 38 employees in a natural office setting. The dataset uniquely captures facial expression data during the COVID-19 pandemic, including responses to major events like the Shanghai lockdown. Each record contains seven emotion probabilities plus rich metadata including job roles, employment outcomes, and personality traits. The dataset introduces 32 extended emotional metrics derived from affective science frameworks. Technical validation demonstrates exceptional data quality through replication of psychological patterns (weekend effect: +192% valence improvement), flawless turnover prediction (AUC=1.0), and strong construct validity (κ=0.72 for personality-emotion alignment). This is the largest and longest-running longitudinal workplace emotion dataset publicly available, enabling advances in emotion recognition, affective dynamics modeling, emotional contagion analysis, and emotion-aware system design.

## Method Summary
The dataset was collected through unobtrusive facial expression capture using commercial facial recognition APIs to extract seven emotion probabilities (neutral, happy, sad, surprised, fear, disgusted, angry) every 10 seconds from 38 employees over 30.5 months. Extended emotional metrics were computed including valence/arousal using circumplex formulas, volatility (rolling standard deviation), inertia (autocorrelation), and 29 other features. Three prediction tasks were evaluated: 7-class emotion classification, continuous valence prediction, and binary turnover prediction. Models included Random Forest, SVM, and LSTM with participant-level cross-validation to prevent leakage.

## Key Results
- Replicated weekend effect with +192% valence improvement (Cohen's d=1.83, p<0.001)
- Achieved 91.2% emotion classification accuracy using LSTM with 50-record sequences
- Perfect turnover prediction (AUC=1.0) using 32 extended emotional metrics
- Strong construct validity with personality traits (κ=0.72)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extended emotional metrics computed from longitudinal facial expression data can predict employee departure with high accuracy.
- **Mechanism:** Emotional distress patterns—specifically elevated fear ratio (12.2% feature importance), anger ratio (10.7%), reduced happiness (8.7%), and higher volatility (7.3%)—reflect cumulative psychological strain that precedes turnover decisions. These metrics aggregate thousands of momentary observations into stable individual signatures.
- **Core assumption:** Emotional trajectories measured via facial expressions meaningfully reflect internal psychological states that drive behavioral outcomes.
- **Evidence anchors:**
  - [abstract] "flawless turnover prediction (AUC=1.0)"
  - [section IV.D] "prediction accuracy surpassed 90% using data from three months prior to departure"
  - [corpus] Weak direct evidence; corpus focuses on emotion recognition, not workplace outcomes
- **Break condition:** Small sample (N=38) limits generalizability; leave-one-out validation yields 0.98±0.03 AUC, not perfect. Single organization and culture may not transfer.

### Mechanism 2
- **Claim:** Temporal sequence modeling outperforms static feature approaches for emotion recognition in naturalistic settings.
- **Mechanism:** LSTM captures autocorrelation in emotion sequences—emotional inertia (mean ρ₁=0.42) means current state predicts near-future states. Static models miss transition dynamics where 67% of errors occur within 30 seconds of emotional shifts.
- **Core assumption:** Emotions exhibit lawful temporal dependencies exploitable by sequence models.
- **Evidence anchors:**
  - [section V.A] LSTM achieves 91.2% vs. RF 89.3% vs. SVM 85.7% (McNemar's test p<0.001)
  - [section VI.A] Performance plateaus at sequence length 50 (~8-10 minutes); shorter contexts underperform
  - [corpus] "Modelling Emotions in Face-to-Face Setting" supports temporal dynamics integration
- **Break condition:** Performance degrades 3.8% under strict cross-participant splits (87.4%), suggesting individual expression patterns vary. High-volatility individuals show 2x error rates (14.3% vs. 7.1%).

### Mechanism 3
- **Claim:** Seven discrete emotion probabilities can be transformed into continuous valence-arousal space while preserving psychological validity.
- **Mechanism:** Linear combination via circumplex model: valence = happy − sad − 0.5·angry − 0.5·fear; arousal = surprised + fear + angry − sad. This dimensionality reduction enables detection of aggregate patterns (weekend effect: +192% valence, p<0.001).
- **Core assumption:** Basic emotions have consistent valence-arousal signatures; weighted linear combination approximates true affective experience.
- **Evidence anchors:**
  - [section III.E] Explicit formulas with coefficient values
  - [section IV.C] Weekend effect replication (+192%, Cohen's d=1.83) validates valence metric against known psychological phenomenon
  - [corpus] EVA-MED dataset "enhances precision of Valence-Arousal Model," supporting this dimensional approach
- **Break condition:** 88.2% system-human agreement (κ=0.84) means ~12% of raw probabilities contain error. Errors compound in derived metrics.

## Foundational Learning

- **Concept: Circumplex Model of Affect**
  - Why needed here: Understanding why valence/arousal formulas work requires grasping that emotions exist in 2D space (pleasant-unpleasant × activated-deactivated), not just discrete categories.
  - Quick check question: If someone shows high surprise probability with low other emotions, would valence be near zero but arousal high?

- **Concept: Emotional Inertia and Volatility**
  - Why needed here: The 32 extended metrics include autocorrelation-based measures (inertia) and rolling standard deviation (volatility). Without understanding these as individual difference variables, their predictive power is opaque.
  - Quick check question: A person with inertia ρ=0.75 vs. ρ=0.25—which would show more predictable emotion trajectories?

- **Concept: Construct Validity via Known Effects**
  - Why needed here: The paper validates its emotion measures not by ground truth labels (impossible in naturalistic settings) but by replicating established findings (weekend effect, diurnal rhythm). This is the validation strategy.
  - Quick check question: If valence showed no weekend effect, would that challenge the measurement validity or the psychological theory?

## Architecture Onboarding

- **Component map:** Raw facial probabilities → valence/arousal formulas → 32 extended metrics → sequence construction (50 records) → LSTM → ensemble with RF
- **Critical path:** Raw probabilities → validate sum=1.0 within ε<10⁻⁶ (data integrity check) → compute valence/arousal using circumplex formulas → aggregate to daily level for extended metrics (volatility, inertia require ≥30 observations) → construct sequences with 50-record context window → LSTM inference → ensemble with RF for robustness
- **Design tradeoffs:** Edge processing (privacy-preserving, 1.5% accuracy drop) vs. centralized training; sequence length: 50 optimal; longer adds compute without gain, shorter loses temporal context; excluding low-volume participants (<60 days) prevents re-identification but reduces sample
- **Failure signatures:** Error clustering at emotional transitions (67% within 30 seconds of state changes); 2.3× higher error rates outside standard work hours (sparse training data); volatile individuals (σ>0.35) show 14.3% error vs. 7.1% for stable individuals—consider per-person calibration; confusion patterns: neutral↔sad (23%), fear↔surprise (18%), angry↔disgust (15%)
- **First 3 experiments:**
  1. Replicate valence computation on sample records, validate weekend effect exists (weekday mean ≈-0.21, weekend ≈+0.19)
  2. Train RF classifier on 32 extended metrics with leave-one-out CV; expect AUC 0.95-1.0 but note small-sample variance
  3. Ablate temporal context: train LSTM with sequence lengths [10, 25, 50, 100] to verify plateau at 50; expect 87.4%→91.2%→91.3%

## Open Questions the Paper Calls Out
- **Question:** Do the high-amplitude emotional patterns observed (e.g., the 192% weekend effect and AUC=1.0 turnover prediction) generalize across different cultural contexts, industries, and organizational scales?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the dataset is limited to a single Chinese software company and explicitly call for future work to "gather comparable datasets across diverse cultures (e.g., Western vs. Eastern), industries... and organizational scales."
- **Question:** Is the relationship between negative emotional metrics (high fear/anger) and employee turnover causal, or are they merely correlates of a third variable (e.g., external life stress)?
- **Basis in paper:** [explicit] The Discussion notes that while predictive validity is high (AUC=1.0), "our analyses remain correlational" and explicitly states "Causal inference studies are needed to clarify the relationships."
- **Question:** Can integrating multi-modal signals (e.g., voice, physiological data) resolve the 12% facial recognition error rate and capture nuanced states like "engagement" or "boredom"?
- **Basis in paper:** [explicit] The authors acknowledge that "Facial expressions provide an imperfect window" and that the seven basic emotions miss workplace-relevant feelings. They explicitly suggest "Incorporating multi-modal sensing... could enhance measurement reliability."

## Limitations
- Small sample size (N=38) from single organization limits generalizability
- Perfect turnover prediction (AUC=1.0) likely overfits to idiosyncratic patterns
- Undisclosed facial expression API and LSTM hyperparameters prevent complete replication
- Data access URL marked "to be assigned" blocks immediate reproduction

## Confidence
- **High confidence:** Weekend effect replication (+192% valence improvement), construct validity with personality traits (κ=0.72), baseline emotion classification accuracy (91.2% LSTM vs. 89.3% RF)
- **Medium confidence:** Turnover prediction performance (AUC=1.0 with leave-one-out but 0.98±0.03 with cross-participant), extended metric predictive power, emotional contagion analysis
- **Low confidence:** Generalization to different organizational contexts, applicability to diverse populations, robustness of 32 extended metrics in independent samples

## Next Checks
1. **Temporal validation:** Test whether the +192% weekend effect replicates in a held-out subset of participants or external dataset, using the same valence computation formula and statistical tests (t-test, Cohen's d)
2. **Model robustness:** Train the turnover prediction model using only cross-participant splits (no leave-one-out) and evaluate AUC on truly unseen participants to assess overfit risk
3. **API transparency:** Replicate emotion extraction using multiple facial expression APIs (e.g., Microsoft Face API, Google Vision, AWS Rekognition) to verify consistency of the 7 emotion probabilities across systems