---
ver: rpa2
title: A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational
  Environments
arxiv_id: '2505.01794'
source_url: https://arxiv.org/abs/2505.01794
tags:
- skills
- soft
- students
- video
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing soft skills like
  decision-making, communication, and creativity in undergraduate students. It proposes
  a fuzzy logic-based framework that integrates the Granular Linguistic Model of Phenomena
  (GLMP) with multimodal analysis, combining video, audio, and text data to evaluate
  these skills.
---

# A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments

## Quick Facts
- arXiv ID: 2505.01794
- Source URL: https://arxiv.org/abs/2505.01794
- Reference count: 40
- Primary result: Fuzzy logic-based framework combining video, audio, and text achieves 0.51–0.94 correlation with instructor ratings for soft skills assessment

## Executive Summary
This paper presents a framework for evaluating soft skills—decision-making, communication, and creativity—in undergraduate students using multimodal data. The system integrates video, audio, and text analysis with fuzzy logic (GLMP) to produce interpretable linguistic labels and explainable reports via LLM. Tested with 49 students across two universities, the approach showed strong correlations (0.51–0.94) between system scores and instructor ratings, with high teacher satisfaction for transparency and fairness.

## Method Summary
The framework extracts 16 behavioral measures from video (facial expressions, gaze, gestures), audio (prosody, speech speed), and text (vocabulary, connectors) using pre-trained models (DeepFace, Whisper, dlib, etc.). These measures flow through a hierarchical GLMP structure with four levels: measures → attributes → dimensions → soft skills. Fuzzy rules (≤3 inputs) or weighted averages (>3 inputs) aggregate evidence at each level, producing five-level linguistic labels. An LLM generates natural language reports from structured GLMP outputs. The system was validated against instructor ratings and teacher satisfaction questionnaires.

## Key Results
- Strong positive correlations (0.51–0.94) between system-generated scores and instructor ratings across all three soft skills
- High teacher satisfaction (4.0–5.0/5.0) for transparency, fairness, and feedback quality
- Text-based tasks better capture decision-making expressions; video tasks better reveal creativity and non-verbal communication cues

## Why This Works (Mechanism)

### Mechanism 1
Multimodal data integration captures more nuanced behavioral expressions than single-modality approaches. Video reveals non-verbal cues (body language, tone) that text-based assessment misses, while text better captures certain decision-making expressions. Behavioral manifestations of soft skills are distributed across verbal and non-verbal channels, and extracting signals from multiple channels improves coverage of the construct space.

### Mechanism 2
Fuzzy logic with hierarchical aggregation (GLMP) handles uncertainty in behavioral assessment and produces interpretable linguistic outputs. The GLMP structures assessment as computational perceptions at multiple granularity levels, producing five-level linguistic labels rather than single numerical scores. Soft skill behaviors are inherently imprecise and context-dependent; linguistic labels approximate human judgment better than crisp numerical thresholds.

### Mechanism 3
Combining GLMP outputs with LLM-generated reports produces explainable assessments aligned with educator reasoning. GLMP generates structured linguistic labels, which the LLM transforms into readable feedback highlighting strengths and areas for improvement. Stakeholders understand soft skill assessments better when presented as natural language narratives rather than raw scores or rule traces.

## Foundational Learning

- Concept: Fuzzy Logic Membership Functions
  - Why needed here: GLMP uses membership functions to convert raw measurements into validity degrees for linguistic labels (Low/Medium/High)
  - Quick check question: Given a speech speed of 4.5 syllables/second, how would you determine its membership in "Low," "Medium," and "High" categories if thresholds overlap?

- Concept: Hierarchical Feature Aggregation
  - Why needed here: The framework aggregates 16 measures → attributes → 3 dimensions → 1 soft skill per target
  - Quick check question: If the "Speed" attribute aggregates "Reaction time," "Fluency," and "Speech speed" measures, and each measure has different validity degrees, how does a change in one measure propagate to the final soft skill score?

- Concept: Multimodal Signal Alignment
  - Why needed here: Video, audio, and text modalities operate at different temporal resolutions (frames, audio samples, transcribed words)
  - Quick check question: If a student pauses frequently (audio cue) but maintains strong gaze (video cue), how should these potentially conflicting signals be interpreted for the "Communication" dimension?

## Architecture Onboarding

- Component map:
  - Video file (MP4/MOV, 1280×720) -> FFmpeg extracts audio (WAV) + frames (images)
  - Video: OpenCV (Haar cascade for face/eyes/smile), DeepFace (7 emotion classes), dlib (68-point landmarks for gaze/blink)
  - Audio: MyProsody (pitch, intensity, rhythm)
  - Text: Whisper (transcription), spaCy (POS tagging), custom dictionaries (connectors, crutches, vocabulary)
  - 16 measures -> fuzzy membership functions -> 5-level linguistic labels with validity degrees
  - GLMP with PMs—fuzzy rules (≤3 inputs) or weighted averages (>3 inputs)—across 4 hierarchy levels
  - Pre-report (structured linguistic labels) -> LLM (LLaMA) with template -> natural language report (PDF)

- Critical path:
  1. Video upload and format validation
  2. Parallel extraction of audio/text/video signals (highest latency step)
  3. Measure computation from extracted signals
  4. Hierarchical fuzzy aggregation (measures → attributes → dimensions → soft skills)
  5. Pre-report generation
  6. LLM report synthesis
  7. Display and PDF export

- Design tradeoffs:
  - Fuzzy rules vs. weighted averages: Fuzzy rules preserve interpretability but become combinatorially complex with >3 inputs; weighted averages scale but lose rule-level transparency
  - Video-only vs. hybrid assessment: Video captures non-verbal cues but introduces variability from student comfort and technical quality; text provides stability but misses behavioral signals
  - Pre-trained vs. fine-tuned models: Using off-the-shelf DeepFace/Whisper enables rapid deployment but may underperform on domain-specific vocabulary (e.g., regional Spanish variants)

- Failure signatures:
  - Low correlation with instructor ratings on specific skills: May indicate misaligned fuzzy rules or weights for that skill's dimensions/attributes
  - Inconsistent scores across tasks for the student: May reflect emotional state variability or task format effects rather than skill changes
  - LLM reports generating claims not in pre-report: Prompt engineering insufficient; hallucination risk realized
  - Video processing failures on specific formats/lighting conditions: Haar cascade limitations; poor pre-processing robustness

- First 3 experiments:
  1. Single-modality ablation: Run assessment using only video, only audio, or only text for the same student recordings. Compare correlation with instructor ratings to quantify each modality's contribution and identify which skills depend on which modalities.
  2. Fuzzy rule sensitivity analysis: Perturb relevance weights (R) and membership function boundaries for one soft skill (e.g., decision-making) and measure score stability. Identify which parameters most affect outcomes and whether expert-defined weights are optimal.
  3. Cross-institutional validation: Apply the same GLMP model to a different student population (different language/cultural context) without modification. Measure correlation with local instructor ratings to assess generalization and identify needed adaptations.

## Open Questions the Paper Calls Out

- **How can the framework be refined to account for cultural and linguistic diversity in non-verbal behaviour interpretation?**
  - The authors note that models may require further refinement to address cultural and linguistic differences, particularly in international contexts, citing specific challenges with Maya-influenced Spanish. Cultural variability in behavioural expressions requires ongoing evaluation to ensure equitable use.

- **To what extent does the integration of physiological signals (e.g., EEG) enhance the assessment of emotional states compared to the current video-audio-text approach?**
  - The current framework relies solely on external cues and has not yet been tested with internal physiological data, which could provide a richer, more objective understanding of student affect. A comparative study measuring correlation accuracy with and without physiological data inputs would resolve this.

- **What is the optimal weighting strategy for hybrid assessments that combine video-based and text-based tasks to maximize the reliability of soft skill evaluation?**
  - While the paper identifies that decision-making often fares better in text and creativity in video, it does not define a methodology for combining these modalities into a single, comprehensive competency score. Experimental results defining an aggregation function would resolve this.

## Limitations
- Cultural generalizability is low—the model is tuned for specific Spanish dialects and may not transfer without adaptation
- Key specifications (fuzzy rule bases, weight assignments) are incompletely documented, blocking faithful reproduction
- Temporal synchronization across modalities remains unvalidated, creating potential misalignment in behavioral interpretation

## Confidence
- Decision-making assessment: High confidence (0.75–0.94 correlation)
- Creativity assessment: High confidence (0.75–0.94 correlation)  
- Communication assessment: Medium confidence (0.51 correlation)
- Cultural generalizability: Low confidence (tuned for specific Spanish dialects)
- Technical reproducibility: Low confidence (incomplete specification of fuzzy rules and weights)

## Next Checks
1. Conduct modality ablation testing: Assess the same students using only video, only audio, and only text inputs to quantify each modality's unique contribution and identify which skills depend on which channels.
2. Test cross-cultural transferability: Apply the exact GLMP configuration to a different linguistic/cultural population without modification, then measure correlation decay and identify necessary adaptations.
3. Verify temporal alignment robustness: Introduce controlled time offsets between extracted signals and measure how correlation with expert ratings degrades, establishing acceptable synchronization tolerances.