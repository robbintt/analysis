---
ver: rpa2
title: 'Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate
  Syntactic Structure Representations in Large Language Models and the Human Brain'
arxiv_id: '2510.13255'
source_url: https://arxiv.org/abs/2510.13255
tags:
- brain
- syntactic
- sentence
- phrase
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HFTP detects neuron-wise syntactic processing in both LLMs and\
  \ human brain regions using frequency-domain analysis, revealing hierarchical structure\
  \ encoding. Six models\u2014GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, and GLM-4\u2014\
  show distinct patterns of sentence and phrase neuron distribution, with newer versions\
  \ like Gemma 2 exhibiting stronger brain alignment than Gemma, while Llama 3.1 shows\
  \ weaker alignment than Llama 2."
---

# Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain

## Quick Facts
- arXiv ID: 2510.13255
- Source URL: https://arxiv.org/abs/2510.13255
- Reference count: 40
- One-line primary result: HFTP detects neuron-wise syntactic processing in both LLMs and human brain regions, revealing hierarchical structure encoding and model-brain alignment.

## Executive Summary
The Hierarchical Frequency Tagging Probe (HFTP) is a novel method that uses frequency-domain analysis to identify computational units in large language models (LLMs) and brain regions encoding hierarchical syntactic structures. By presenting linguistic stimuli rhythmically (phrases at 2 Hz, sentences at 1 Hz) and analyzing spectral peaks in neural activations, HFTP isolates neurons and brain regions specifically processing syntactic boundaries. The method reveals that different LLMs have distinct distributions of sentence and phrase neurons, with newer models like Gemma 2 showing stronger alignment with human brain representations than older versions. Brain regions involved in syntactic processing, particularly in the left hemisphere, align more closely with LLM representations than right-hemisphere regions.

## Method Summary
HFTP applies frequency-domain analysis to identify neuron-wise components in LLMs and cortical regions encoding syntactic structures. The method presents concatenated linguistic stimuli where monosyllabic words occur at 4 Hz, phrases at 2 Hz, and sentences at 1 Hz. Time-domain activations from LLM MLP neurons or sEEG channels are transformed via FFT to extract frequency-domain information. Statistical thresholds identify significant neurons/channels, and representational similarity analysis (RSA) correlates structure representations between models and brains. The approach enables cross-modal comparisons and works with both artificial and naturalistic text.

## Key Results
- Six LLMs (GPT-2, Gemma, Gemma 2, Llama 2, Llama 3.1, GLM-4) contain distinct distributions of sentence and phrase neurons, with newer models like Gemma 2 showing stronger brain alignment than Gemma
- Left hemisphere brain regions (A1, STG, MTG, IFG) align more closely with LLM representations than right-hemisphere regions
- Llama 3.1 shows weaker brain alignment than Llama 2, suggesting model improvements don't uniformly enhance human-like syntactic processing
- HFTP successfully extends to naturalistic text while maintaining ability to detect hierarchical structure encoding

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Isolation of Syntactic Hierarchies
Hierarchical syntactic structures can be isolated as distinct spectral signatures using frequency-domain analysis. The HFTP method presents linguistic stimuli rhythmically (phrases at 2 Hz, sentences at 1 Hz) and applies FFT to identify units with significant peaks at these frequencies, isolating neurons/brain regions responding to syntactic boundaries rather than general processing.

### Mechanism 2: Representational Alignment via Structure Similarity
Similarity between LLMs and human brains in encoding syntactic structure is quantified by comparing their Structure Representational Dissimilarity Matrices (SRDMs). High correlation between SRDMs implies similar encoding of syntactic structures, measured through RSA.

### Mechanism 3: Neuron-Wise Specialization for Syntactic Levels
Specific computational units in both LLMs and human brains can be identified that preferentially process particular levels of syntactic hierarchy. Statistical thresholds classify units as sentence or phrase processors based on frequency amplitude and z-score deviation from control conditions.

## Foundational Learning

- **Fast Fourier Transform (FFT) and Spectral Analysis**: Core mathematical tool converting time-series activations to frequency-domain representations to find syntactic peaks. Quick check: If an input signal repeats every 2 seconds, at what frequency (in Hz) would you expect to see a peak?

- **Representational Similarity Analysis (RSA)**: Method comparing "syntactic structure representations" between LLMs and brains by correlating internal representational geometries. Quick check: If two brain regions have highly correlated RDMs, what does that imply about their function?

- **Syntactic Hierarchy (Phrases, Sentences)**: The paper focuses on how specific levels of linguistic structure are processed. Understanding the difference between phrases and sentences is necessary to interpret the 1 Hz and 2 Hz findings. Quick check: In HFTP, which frequency is associated with phrase-level processing and which with sentence-level processing?

## Architecture Onboarding

- **Component map**: Input Layer (syntactic corpus) -> LLM Layers (Transformer layers, MLP activations) -> HFTP Probe (Activation Extraction, Frequency Transformer, Statistical Classifier) -> Alignment Module (SRDM Calculator, RSA Engine, Summary Metrics)

- **Critical path**: The accuracy of the entire pipeline depends on the Frequency Transformer. If FFT is applied to noisy, non-periodic, or improperly windowed activations, the identified "sentence" and "phrase" peaks will be spurious, invalidating all downstream alignment analysis.

- **Design tradeoffs**: 
  - Stimulus Control vs. Generalizability: Artificial rhythmic corpora provide clean spectral peaks but may not generalize to natural language
  - Granularity vs. Noise: Individual neuron analysis provides specificity but is susceptible to noise
  - Choice of LLM Sub-component: Targeting MLP neurons based on prior work, but attention heads might yield different results

- **Failure signatures**: 
  - No peaks in frequency domain: Indicates non-rhythmic processing or misconfigured sampling
  - Peaks in random control condition: Suggests lexical or tokenization artifacts, not syntactic structure
  - Near-zero RSA scores: Indicates no alignment between model and brain representations
  - Uniform neuron distribution: Suggests overly permissive statistical thresholding

- **First 3 experiments**:
  1. Baseline Run on Known Model: Apply HFTP to a simple model (like a small RNN) using the syntactic corpus to confirm no significant peaks appear
  2. Ablation of Control Condition: Run probe on GPT-2 without randomized control and compare results to quantify control's role
  3. Layer-wise Analysis: For a single model, plot density of "sentence neurons" vs. "phrase neurons" across layers to replicate key results

## Open Questions the Paper Calls Out

- **What specific architectural modifications or training data characteristics drive divergent brain-alignment trends in model updates?** The study identifies correlation but doesn't isolate causal variables responsible for brain-similarity improvement or degradation.

- **Does reliance on synthetic data and code-heavy corpora inevitably shift LLM syntactic mechanisms away from human-like neural representations?** This hypothesis about Llama 3.1's lower alignment needs empirical testing of the link between data modality and "human-likeness."

- **Do hierarchical frequency tagging patterns and model-brain alignment hold universally across typologically distinct languages?** The method is validated on Chinese and English, but generalization to languages with different syntactic structures remains unconfirmed.

## Limitations
- Control for Non-Syntactic Regularities: Unclear whether HFTP fully controls for confounding effects from lexical frequency, co-occurrence statistics, or tokenization artifacts
- Generalizability to Naturalistic Language: Primary experiments use highly artificial stimuli; strength for everyday language processing remains to be fully established
- Distribution vs. Localization Debate: Assumes localization of syntactic processing, but alternative views posit it as a highly distributed, emergent property

## Confidence

- **High Confidence**: The general framework of frequency-domain analysis to identify periodic neural responses is well-established; finding that certain LLM layers show better alignment with human brain data than others is likely robust
- **Medium Confidence**: The specific claim that HFTP can isolate "sentence" and "phrase" neurons with high specificity and that these correspond to genuine syntactic processing is plausible but requires further validation
- **Low Confidence**: The precise ranking of model-brain alignment and claims that newer model versions don't uniformly improve brain-alignment are interesting but could be sensitive to specific models, datasets, or analytical choices

## Next Checks
1. **Ablation of Control Conditions**: Run HFTP probe on a trained LLM without randomized control condition and compare results to quantify the essential role of control in isolating true syntactic processing

2. **Validation on Out-of-Distribution Language**: Apply HFTP to sentences with complex or rare syntactic structures not well-represented in training corpora to determine if method captures frequency-based memorization rather than abstract syntactic rules

3. **Statistical Power Analysis**: Perform post-hoc power analysis to determine if reported effect sizes (e.g., differences in RSA scores between model versions) are detectable given sample size of neurons/channels and trials used