---
ver: rpa2
title: Offline Reinforcement Learning with Wasserstein Regularization via Optimal
  Transport Maps
arxiv_id: '2507.10843'
source_url: https://arxiv.org/abs/2507.10843
tags:
- learning
- distance
- wasserstein
- offline
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the distributional shift problem in offline
  reinforcement learning by introducing a Wasserstein regularization method that avoids
  adversarial training. The key innovation is using input-convex neural networks (ICNNs)
  to model optimal transport maps, enabling Wasserstein distance computation without
  a discriminator.
---

# Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps

## Quick Facts
- arXiv ID: 2507.10843
- Source URL: https://arxiv.org/abs/2507.10843
- Reference count: 24
- Primary result: Achieves comparable or superior performance to baselines on D4RL benchmark, with notable improvements on hopper-medium-v2 (+10.4) and kitchen-partial-v0 (+21.5)

## Executive Summary
This paper addresses the distributional shift problem in offline reinforcement learning by introducing a Wasserstein regularization method that avoids adversarial training. The key innovation is using input-convex neural networks (ICNNs) to model optimal transport maps, enabling Wasserstein distance computation without a discriminator. This approach is more stable than adversarial methods while capturing action similarity in continuous spaces. Experiments on the D4RL benchmark show that the proposed method achieves comparable or superior performance to widely used baselines, with notable improvements on specific tasks.

## Method Summary
The paper introduces a novel offline RL approach using Wasserstein regularization via optimal transport maps. The method leverages input-convex neural networks (ICNNs) to learn transport maps between actions in the dataset and potential actions, effectively constraining the policy to remain close to the dataset distribution. Unlike adversarial methods that use discriminators to estimate Wasserstein distances, this approach directly computes the Wasserstein-1 distance using ICNNs, which guarantees convexity in the action space. The transport map is integrated into the Q-learning framework, where the regularization term penalizes deviations from the dataset distribution while still allowing necessary exploration within the support of observed actions.

## Key Results
- Achieves comparable or superior performance to widely used baselines on D4RL benchmark
- Notable improvements on hopper-medium-v2 (+10.4) and kitchen-partial-v0 (+21.5)
- Demonstrates more stable learning compared to adversarial Wasserstein regularization approaches
- Avoids discriminator instability while maintaining computational tractability

## Why This Works (Mechanism)
The method works by leveraging the mathematical properties of Wasserstein distance and ICNNs. The Wasserstein-1 distance measures the minimal cost of transporting one probability distribution to another, which naturally captures action similarity in continuous spaces. By using ICNNs to model the transport map, the method ensures convexity in the action space, making the optimization tractable while avoiding the instability of adversarial training. The transport map effectively regularizes the policy by penalizing actions that would require large "transport costs" from the dataset distribution, thus mitigating distributional shift.

## Foundational Learning

**Wasserstein Distance** - A metric between probability distributions that measures the minimal cost of transforming one distribution into another
*Why needed:* Provides a principled way to measure distributional shift between the learned policy and dataset
*Quick check:* Verify that the transport cost matrix is symmetric and satisfies triangle inequality

**Input-Convex Neural Networks (ICNNs)** - Neural networks where the output is a convex function of the input
*Why needed:* Guarantees tractable optimization of the transport map while avoiding adversarial training instability
*Quick check:* Confirm that all non-linearities are convex and weight matrices maintain convexity constraints

**Distributional Shift** - The mismatch between the state-action visitation distribution of the learned policy and the dataset
*Why needed:* Primary challenge in offline RL that causes overestimation and poor generalization
*Quick check:* Measure the KL divergence between dataset and policy distributions during training

**Optimal Transport Maps** - Functions that transform one probability distribution into another with minimal total cost
*Why needed:* Provides the mathematical foundation for regularizing policy actions based on dataset similarity
*Quick check:* Verify that the transport map is bijective and preserves probability mass

## Architecture Onboarding

**Component Map:** Dataset -> ICNN Transport Map -> Regularized Q-function -> Policy

**Critical Path:** The transport map must be learned efficiently to provide meaningful regularization without bottlenecking Q-function updates. The ICNN architecture should balance expressiveness with computational tractability.

**Design Tradeoffs:** ICNNs provide stability but may struggle with highly non-convex transport maps. The method trades some representational power for guaranteed convexity and training stability.

**Failure Signatures:** If the transport map converges to identity mapping, regularization becomes ineffective. Discriminator instability manifests as training oscillations and degraded performance.

**First Experiments:** 1) Train ICNN on synthetic transport tasks to verify convexity properties. 2) Evaluate regularization strength on a simple continuous control task. 3) Compare transport distances for in-distribution vs out-of-distribution actions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Q-DOT framework be effectively combined with trajectory stitching techniques to improve performance on sparse reward tasks where regularization alone is insufficient?
- Basis in paper: [explicit] The authors note in Section 4.2 that for the Antmaze domain, "regularization alone does not substantially enhance stitching ability," suggesting that "further performance improvements would require incorporating additional techniques beyond regularization."
- Why unresolved: The current method focuses on constraining the policy to the dataset distribution but lacks a mechanism to compose optimal sub-trajectories (stitching) from different parts of the offline data, which is necessary for solving sparse reward mazes.
- Evidence: Demonstration of improved normalized returns on Antmaze tasks (specifically large-diverse and large-play) that exceed the current Q-DOT baselines by successfully combining high-reward segments from distinct trajectories.

### Open Question 2
- Question: How can the value function be improved to better guide the transport map in transforming low-quality trajectories, particularly in environments like HalfCheetah?
- Basis in paper: [explicit] In the Supplementary Materials (Section 8), the authors observe that in HalfCheetah, the transport map fails to identify advantageous transformations, concluding that "learning a value function capable of effectively transforming low-quality trajectories remains a challenge for future research."
- Why unresolved: The current expectile regression approach (IQL) may not provide sufficiently distinct advantage estimates to drive the transport map away from the identity mapping for low-quality data in specific locomotion tasks.
- Evidence: A stronger negative correlation between trajectory reward and average transport distance in HalfCheetah tasks, indicating that the model successfully modifies low-reward trajectories more significantly than high-reward ones.

### Open Question 3
- Question: Can the discriminator-free Wasserstein regularization via ICNNs be effectively adapted for online or off-policy reinforcement learning settings?
- Basis in paper: [explicit] The conclusion states that "This method has the potential for further development beyond offline RL, extending to other RL settings."
- Why unresolved: The stability and sample efficiency of learning optimal transport maps via ICNNs have only been validated on static datasets; it is unknown if the method remains stable or beneficial when the target distribution shifts dynamically during online interaction.
- Evidence: Stable convergence curves and competitive final returns when applying the ICNN-based regularization to standard online RL benchmarks, without the instability usually associated with adversarial discriminator methods.

## Limitations
- The method's computational complexity may become prohibitive for high-dimensional action spaces
- ICNNs may struggle with highly non-convex transport maps, limiting representational power
- The offline setting's assumption of static datasets may not reflect real-world scenarios where data collection continues during learning

## Confidence

**Method feasibility and core innovation:** High
**Experimental results on D4RL:** Medium (limited scope)
**Theoretical guarantees:** Medium (requires additional assumptions)
**Scalability to complex environments:** Low

## Next Checks
1. Test the method on continuous control tasks with higher-dimensional action spaces (e.g., humanoid) to assess scalability.
2. Conduct ablation studies isolating the contribution of ICNNs from other architectural choices.
3. Evaluate performance when the dataset contains significant distributional shift or contains only suboptimal demonstrations.