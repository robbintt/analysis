---
ver: rpa2
title: How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought
  Reasoning with Sparse Autoencoding
arxiv_id: '2507.22928'
source_url: https://arxiv.org/abs/2507.22928
tags:
- features
- patching
- nocot
- feature
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether chain-of-thought (CoT) prompting
  leads to more faithful internal reasoning in large language models. Using sparse
  autoencoders (SAEs) to extract interpretable features and activation patching to
  perform causal interventions, the authors analyze the GSM8K math dataset with Pythia-70M
  and Pythia-2.8B models.
---

# How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding

## Quick Facts
- **arXiv ID:** 2507.22928
- **Source URL:** https://arxiv.org/abs/2507.22928
- **Authors:** Xi Chen; Aske Plaat; Niki van Stein
- **Reference count:** 18
- **Key outcome:** CoT prompting induces more interpretable and causally effective internal structures in high-capacity LLMs, validating its role in structured reasoning.

## Executive Summary
This paper investigates whether chain-of-thought (CoT) prompting leads to more faithful internal reasoning in large language models. Using sparse autoencoders (SAEs) to extract interpretable features and activation patching to perform causal interventions, the authors analyze the GSM8K math dataset with Pythia-70M and Pythia-2.8B models. CoT features were swapped into noCoT reasoning runs to measure causal impact on answer log-probabilities. In the 2.8B model, CoT features consistently improved output confidence (e.g., from 1.2 to 4.3), while in the 70M model, no reliable gains were observed. CoT also induced significantly higher activation sparsity and more interpretable features in the larger model, indicating more modular internal computation. Interestingly, randomly selected CoT features often outperformed top-ranked ones in the 2.8B model, suggesting useful information is distributed across features rather than concentrated.

## Method Summary
The study uses GSM8K math problems with Pythia-70M and 2.8B models, comparing CoT (3-shot prompt with step-by-step reasoning) versus noCoT (question only) conditions. For each model and condition, the authors train sparse autoencoders (SAE) on residual stream activations from layer 2 at the final token position. They extract features using dictionary ratios of 4 and 8, then perform activation patching: swapping feature activations from CoT runs into noCoT runs to measure changes in answer log-probability. They test both Top-K and Random-K patching strategies, varying K from 2 to 128. The analysis measures ∆logP (change in answer confidence), feature interpretability scores, and activation sparsity levels.

## Key Results
- In the 2.8B model, CoT features consistently improved output confidence (e.g., from 1.2 to 4.3), while in the 70M model, no reliable gains were observed.
- CoT also induced significantly higher activation sparsity and more interpretable features in the larger model, indicating more modular internal computation.
- Randomly selected CoT features often outperformed top-ranked ones in the 2.8B model, suggesting useful information is distributed across features rather than concentrated.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chain-of-thought (CoT) prompting induces causally effective internal representations that enhance reasoning fidelity, but this capacity is strictly conditional on model scale.
- **Mechanism:** In sufficiently large models (e.g., Pythia-2.8B), CoT prompts elicit sparse, monosemantic features in the residual stream. When these specific features are patched into a "no CoT" forward pass, they causally increase the log-probability of the correct answer. This effect fails to materialize in smaller models (e.g., Pythia-70M), suggesting a threshold for structural reorganization exists.
- **Core assumption:** The Sparse Autoencoder (SAE) successfully isolates reasoning-relevant features from noise, and the increase in answer log-probability serves as a valid proxy for "faithful" reasoning.
- **Evidence anchors:**
  - [abstract]: "...Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M..."
  - [section]: "In the 2.8B model, CoT features consistently improved output confidence (e.g., from 1.2 to 4.3), while in the 70M model, no reliable gains were observed."
  - [corpus]: Related work ("CoT is Not True Reasoning...") challenges the "reasoning" label, suggesting CoT acts as a constraint. This paper refines that view by showing CoT creates causal structure *only* at sufficient scale.
- **Break condition:** This mechanism likely fails or reverses if the model capacity is insufficient to resolve polysemanticity, resulting in no performance gain from feature patching.

### Mechanism 2
- **Claim:** Useful reasoning information during CoT is distributed across the feature space rather than concentrated in the highest-activation neurons ("distributed structured sparsity").
- **Mechanism:** Counter-intuitively, randomly selected CoT features often outperform top-ranked features when patched. Top-K selection may overfit to local activation peaks, missing "support" features. CoT creates a structure where information is spread across many moderately active features.
- **Core assumption:** Random sampling better approximates the true information manifold than max-activation selection in this specific context.
- **Evidence anchors:**
  - [abstract]: "...randomly selected CoT features often outperformed top-ranked ones in the 2.8B model, suggesting useful information is distributed across features rather than concentrated."
  - [section]: "This suggests that useful information from CoT prompts is widely distributed among moderately activated features... The Top-K strategy may overfit to local peaks..."
  - [corpus]: "How Chain-of-Thought Works?" traces information flow; this paper adds that the flow is likely distributed and sparse rather than channel-specific.
- **Break condition:** If the SAE dictionary size is too small or the L1 penalty too high, distributed features may be crushed, causing random patching to fail.

### Mechanism 3
- **Claim:** CoT prompting induces a state of "structured sparsity" that disentangles internal representations.
- **Mechanism:** CoT significantly suppresses global activations (most neurons near zero) while increasing the variance of neuron counts per feature. This separation (disentanglement) makes features more interpretable and monosemantic, reducing superposition effects common in standard prompting.
- **Core assumption:** Higher activation sparsity correlates directly with improved modularity and semantic coherence.
- **Evidence anchors:**
  - [abstract]: "CoT also induced significantly higher activation sparsity and more interpretable features in the larger model, indicating more modular internal computation."
  - [section]: "This 'structured sparsity' means that the useful information activated by CoT prompts is not concentrated in a few highly activated features, but is more widely spread..."
  - [corpus]: Sparse Autoencoders are noted in the paper as addressing superposition; this mechanism extends that utility to prompting strategies.
- **Break condition:** If the prompt is too complex or the model too small, the sparsity may degrade into mere inactivity rather than structured computation.

## Foundational Learning

- **Concept:** Sparse Autoencoders (SAE)
  - **Why needed here:** SAEs are the core tool used to decompose the "black box" activations into interpretable features. Without understanding how L1 regularization enforces sparsity to resolve polysemanticity, the intervention results are opaque.
  - **Quick check question:** How does the L1 penalty in the SAE loss function ($\lambda\|h\|_1$) help create monosemantic features from dense residual stream activations?

- **Concept:** Activation Patching (Causal Intervention)
  - **Why needed here:** The paper moves beyond correlation (what features fire?) to causation (what features *make* the answer correct?). Understanding patching is essential to interpret the $\Delta \log P$ results.
  - **Quick check question:** If patching a feature from a CoT run into a NoCoT run increases the answer log-probability, what does that strictly imply about that feature's role?

- **Concept:** Polysemanticity & Superposition
  - **Why needed here:** The paper frames CoT as a solution to the "superposition" problem where neurons represent multiple concepts. Grasping this concept explains *why* interpretability is hard in the first place.
  - **Quick check question:** Why is a "polysemantic" neuron difficult to interpret, and how does the paper claim CoT alleviates this?

## Architecture Onboarding

- **Component map:** Input (GSM8K problems) -> CoT Prompt (3-shot) vs. NoCoT -> Pythia-70M / 2.8B (Residual Stream Layer 2) -> SAE (Dictionary Ratio 4/8) -> Sparse Feature Vectors -> Activation Patching (Top-K / Random-K) -> Metric (ΔlogP & Interpretability Score)

- **Critical path:** The extraction of the residual stream at the **final token position** and the specific **dictionary ratio** of the SAE are the most sensitive hyperparameters. If the SAE is under-trained or the ratio wrong, features remain polysemantic, and patching yields noise.

- **Design tradeoffs:**
  - **Dictionary Ratio (4 vs 8):** Higher ratio (8) = sparser features but potentially lost information; Lower ratio = denser, potentially more confused features.
  - **Patching Strategy:** Top-K is more deterministic but risks missing distributed context (Random-K is noisier but proves distribution).

- **Failure signatures:**
  - **Scale Failure:** Symmetric distributions in patching (CoT → NoCoT looks like NoCoT → CoT), as seen in Pythia-70M.
  - **Interpretability Illusion:** High explanation scores that do not correlate with causal patching success (the paper argues they correlate in 2.8B but not 70M).

- **First 3 experiments:**
  1. **Baseline Replication:** Train an SAE on Pythia-2.8B Layer 2 residuals with a dictionary ratio of 4. Run Top-20 patching on a held-out GSM8K sample to confirm the positive $\Delta \log P$ shift.
  2. **Random vs. Top-K Ablation:** Repeat the patching experiment using Random-20 features. Verify if random selection outperforms Top-K, confirming the distributed information hypothesis.
  3. **Scale Thresholding:** Run the same SAE/Patching pipeline on an intermediate model (e.g., Pythia-410M) to identify the parameter count where CoT features begin to exhibit causal efficacy.

## Open Questions the Paper Calls Out

- **Open Question 1:** At what exact model scale does CoT-induced feature causality emerge?
  - **Basis in paper:** [explicit] Authors observe a "clear scale threshold" between 70M (no reliable effect) and 2.8B (strong positive transfer), but do not test intermediate sizes.
  - **Why unresolved:** Only two model sizes were evaluated, leaving the minimum capacity requirement for causally effective CoT features undefined.
  - **What evidence would resolve it:** Running SAE+patching experiments on intermediate models (e.g., 410M, 1B, 1.4B) to identify when consistent positive log-probability shifts emerge.

- **Open Question 2:** Do CoT features causally influence reasoning at intermediate steps, or only at the final token position?
  - **Basis in paper:** [explicit] Authors state: "our activation patching targets only the residual activation of the final token and does not trace causal effects through the reasoning process" and propose "token-level and path-based causal analysis" as future work.
  - **Why unresolved:** The static SAE snapshot approach cannot capture dynamic reasoning trajectories across the full chain-of-thought.
  - **What evidence would resolve it:** Stepwise activation patching across all tokens in the reasoning chain, combined with path-patching to trace feature influence through computational circuits.

- **Open Question 3:** Do these findings generalize to larger models and different architectures beyond Pythia?
  - **Basis in paper:** [explicit] Authors acknowledge: "experiments are restricted to Pythia-2.8B and smaller variants; we did not include larger models such as LLaMA-7B, and our findings may not generalize."
  - **Why unresolved:** Different architectures may organize internal representations differently, and SAE effectiveness may vary with scale beyond 2.8B.
  - **What evidence would resolve it:** Applying the same methodology to LLaMA-7B+, Mistral, or other model families, comparing sparsity patterns and causal transfer effects.

- **Open Question 4:** What causal information is missed by SAE decomposition due to distributed or entangled representations?
  - **Basis in paper:** [explicit] Authors note: "SAE-based feature analysis introduces biases and may miss distributed or entangled representations. Not all interpretable SAE features have causal effects."
  - **Why unresolved:** SAEs impose sparsity constraints that may artificially separate features that are genuinely entangled in the model's true computation.
  - **What evidence would resolve it:** Comparing SAE feature interventions against non-sparse decomposition methods and measuring whether distributed representations carry complementary causal signals.

## Limitations

- **Model Scale Dependency:** The stark contrast between Pythia-70M and 2.8B results raises questions about the minimum scale required for CoT reasoning efficacy, which was not explored.
- **SAE Interpretability:** The claim that CoT induces "more interpretable features" relies on a specific interpretation score, but the robustness of this metric across different SAE hyperparameters is unclear.
- **Causal Attribution:** The activation patching method assumes that changes in log-probability directly reflect the causal role of patched features, potentially missing interactions with other model components.

## Confidence

- **High:** The observation that CoT features improve answer log-probability in the 2.8B model (e.g., from 1.2 to 4.3) is well-supported by the data and the mechanistic explanation of distributed structured sparsity is plausible.
- **Medium:** The claim that CoT induces higher activation sparsity and more interpretable features is supported, but the direct causal link between sparsity and improved reasoning fidelity is not fully established.
- **Low:** The assertion that CoT creates "faithful internal reasoning" is a strong claim that goes beyond the evidence. The paper shows that CoT features are causally effective, but it does not prove that the model is "reasoning" in a human-like sense rather than following a learned pattern.

## Next Checks

1. **Intermediate Scale Experiment:** Test the Pythia-410M model to identify the precise parameter threshold where CoT features begin to exhibit reliable causal efficacy.
2. **Cross-Architecture Replication:** Apply the same SAE and patching pipeline to a different model family (e.g., LLaMA-7B) to test the generalizability of the findings.
3. **Feature Interaction Analysis:** Investigate whether the effectiveness of CoT features depends on the presence of other features by performing ablation studies where multiple features are patched simultaneously or in sequence.