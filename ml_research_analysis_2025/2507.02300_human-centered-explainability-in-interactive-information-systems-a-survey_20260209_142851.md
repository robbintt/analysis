---
ver: rpa2
title: 'Human-Centered Explainability in Interactive Information Systems: A Survey'
arxiv_id: '2507.02300'
source_url: https://arxiv.org/abs/2507.02300
tags:
- explainability
- explanations
- explanation
- user
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review examines how explainability has been conceptualized,
  designed, and evaluated in interactive information systems. Following PRISMA guidelines,
  100 relevant articles were identified and analyzed.
---

# Human-Centered Explainability in Interactive Information Systems: A Survey

## Quick Facts
- arXiv ID: 2507.02300
- Source URL: https://arxiv.org/abs/2507.02300
- Reference count: 40
- Primary result: Systematic review of 100 articles on conceptualization, design, and evaluation of explainability in interactive information systems

## Executive Summary
This systematic review examines how explainability has been conceptualized, designed, and evaluated in interactive information systems. Following PRISMA guidelines, 100 relevant articles were identified and analyzed. Key findings include: (1) five dimensions used to conceptualize explainabilityâ€”where, why, whom, what, and how; (2) a classification scheme of explanation designs by interactivity, modality, and interface type; and (3) six user-centered categories for measuring explainability. The review highlights a disconnect between conceptual definitions and practical implementation, emphasizing the need for standardized frameworks that align explainability goals with design and evaluation strategies. Future research should focus on improving methodological rigor in user studies, developing validated measurement tools, and exploring audience-centered approaches for emerging AI applications.

## Method Summary
The review followed PRISMA guidelines with a two-stage search across 8 academic databases, initially yielding 1,391 papers and subsequently 100 articles after screening. Three researchers independently coded one-third of the papers each, using a structured taxonomy to analyze definitions (5 dimensions), interface characteristics (interactivity, modality, interface type), and measurement constructs (6 categories). The coding scheme was cross-checked for consistency, and structural encoding was applied for textual analysis of definitions.

## Key Results
- Five dimensions emerged for conceptualizing explainability: where, why, whom, what, and how
- Classification scheme for explanation designs based on interactivity, modality, and interface type
- Six user-centered categories for measuring explainability: Intrinsic, Format, Usability, Experiential, Ethics, and Interaction
- Disconnect identified between conceptual definitions and practical implementation of explainability
- Static interfaces dominate the literature despite growing interest in interactive explanations

## Why This Works (Mechanism)

### Mechanism 1: The Definition-Design Alignment Loop
- **Claim:** If explainability definitions are decoupled from interface design strategies, user trust and task performance may degrade due to misaligned expectations
- **Mechanism:** The paper identifies a disconnect between conceptual definitions and practical implementation. The proposed mechanism is conceptual alignment: explicitly mapping definition dimensions (Where, Why, Whom, What, How) to design choices
- **Core assumption:** A clear definition acts as a functional requirement that constrains the design space, reducing the semantic gap between user needs and system outputs
- **Evidence anchors:** Section 5.1.1 highlights conceptual gaps; Section 5.2.2 emphasizes that definitions should serve as foundation for both design and evaluation
- **Break condition:** This mechanism fails if the target "Whom" (user) is heterogeneous or undefined, causing a single definition/design pair to satisfy no one

### Mechanism 2: Interactive Affordances for Scrutability
- **Claim:** Transitioning from static to interactive interfaces likely increases Scrutability and Usefulness by allowing users to filter, focus, or simulate outcomes
- **Mechanism:** The paper categorizes designs by interactivity. Interactive interfaces support actions like clarify, simulate, and compare, moving from one-way transmission to dialogue
- **Core assumption:** Users desire and are capable of engaging in two-way interaction to refine their understanding
- **Evidence anchors:** Section 2.2 discusses growing effort to explore interactive explanation designs; Section 4.3 shows 44 of 85 articles utilized interactive interfaces
- **Break condition:** This mechanism breaks if interaction requires high technical expertise or if system latency disrupts cognitive flow

### Mechanism 3: Multi-Dimensional Evaluation Validity
- **Claim:** Relying solely on Experiential metrics to evaluate explainability risks validating ineffective systems
- **Mechanism:** The review finds Experiential measures are most frequent (23%), while Intrinsic properties like fidelity are measured less consistently. The mechanism is triangulated validation
- **Core assumption:** User satisfaction is an imperfect proxy for actual system understanding or explanation truthfulness
- **Evidence anchors:** Section 4.4.1 shows Experiential is most measured category; Section 5.1.3 notes lack of clear operationalization of attributes
- **Break condition:** This fails if Intrinsic metrics are computationally defined but semantically meaningless to the human user

## Foundational Learning

- **Concept: The "Where, Why, Whom, What, How" Framework**
  - Why needed here: Critiques vague use of "Explainability" and provides core tool for deconstructing requirements before writing code
  - Quick check question: Can you define the "Why" of your system's explanation differently than the "How"?

- **Concept: Modality-Interface Coupling**
  - Why needed here: Designers often treat interface and explanation modality as independent, but they are coupled
  - Quick check question: If you switch your interface from Dashboard to Chatbot, how must your explanation modality change to maintain readability?

- **Concept: The "Satisfaction-Fidelity" Gap**
  - Why needed here: Engineers often optimize for what users say they like, but an explanation can be satisfying but wrong
  - Quick check question: Does your current evaluation metric measure if the user liked the explanation, or if it actually reflects the model's logic?

## Architecture Onboarding

- **Component map:** Definition Layer (5 Dimensions) -> Presentation Layer (Interface Type + Modality) -> Interaction Layer (Static vs. Interactive) -> Evaluation Layer (6 Categories)
- **Critical path:**
  1. Define Context: Use 5 Dimensions to lock down Why and Whom
  2. Select Interface/Modality: Choose based on Where
  3. Implement Interactivity: Add Clarify or Simulate if task requires high user agency
  4. Triangulate Evaluation: Measure at least one metric from Intrinsic, Usability, and Experiential categories
- **Design tradeoffs:**
  - Static vs. Interactive: Static is easier to build/test but offers lower scrutability; Interactive builds better mental models but introduces higher engineering complexity
  - Fidelity vs. Readability: High fidelity often reduces readability; must balance What against Whom
- **Failure signatures:**
  - "Black Box" placebo: High satisfaction on explanation with zero correlation to actual model logic
  - Modality Clash: Complex graphical visualizations on voice-only or chat-based interface
  - "Generic User" trap: Designing for "The User" without specifying Expert vs. Novice
- **First 3 experiments:**
  1. Dimension Audit: Map existing explanation features to 5 Dimensions to find undefined gaps
  2. Modality A/B Test: Deploy same content in two modalities and measure Usability and Time Spent
  3. Fidelity Check: Correlate Intrinsic metric (accuracy relative to model) with Experiential metric (User Satisfaction)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does adoption of standardized reporting guidelines affect replicability and generalizability of results in XAI user studies?
- **Basis in paper:** Section 5.2.1 states future research should adopt standardized reporting guidelines to ensure sufficient disclosure of details about user studies
- **Why unresolved:** Current studies frequently omit key data (e.g., age, AI familiarity), creating methodological gap that hinders systematic comparison
- **What evidence would resolve it:** Validated reporting checklists for XAI user studies, followed by longitudinal analyses showing improved cross-study consistency

### Open Question 2
- **Question:** To what extent does aligning the "Five Dimensions" with design and evaluation strategies improve user task performance?
- **Basis in paper:** Section 5.2.2 argues definition-driven approach holds great potential to address disconnect between conceptual and practical work
- **Why unresolved:** Lack of integrated frameworks that systematically map definition dimensions to specific interface designs and evaluation metrics
- **What evidence would resolve it:** Empirical studies comparing user performance in systems designed using five-dimension framework versus ad-hoc strategies

### Open Question 3
- **Question:** How can audience-centered explainability approaches be adapted to support dynamic sense-making and trust calibration in LLMs and Generative AI?
- **Basis in paper:** Section 5.2.3 suggests future research should move beyond system-focused computational methods and prioritize audience-centered approaches for GenAI systems
- **Why unresolved:** GenAI systems are open-ended, interactive, and context-sensitive, making traditional static explanations inadequate
- **What evidence would resolve it:** Development and validation of new evaluation measures for GenAI that capture dynamic user needs and situated trust

## Limitations
- Medium confidence for claims about prevalence of static vs. interactive interfaces due to small sample size and potential selection bias
- Low confidence for broader claim about pervasive disconnect between conceptual and practical work, primarily drawn from synthesis of 100 articles
- Unknown inter-rater reliability scores and specific threshold for resolving coding disagreements

## Confidence

- **High**: Taxonomy of definition dimensions (Where, Why, Whom, What, How) and their application to coding; interface design classification scheme; measurement category framework
- **Medium**: Assertion that static interfaces dominate literature; need for standardized frameworks aligning definitions with design and evaluation
- **Low**: Broader claim about disconnect requiring urgent action; effectiveness of proposed mechanisms without further empirical testing

## Next Checks
1. External Corpus Validation: Replicate coding scheme on 50-100 articles from databases not included in original search (e.g., arXiv, SpringerLink)
2. Practitioner Survey: Survey 30-50 AI system designers about disconnect between conceptual definitions and practical implementation
3. Measurement Instrument Validation: Develop and pilot standardized survey instrument based on 6 measurement categories to assess explainability in real-world AI system