---
ver: rpa2
title: How Tech Workers Contend with Hazards of Humanlikeness in Generative AI
arxiv_id: '2512.19832'
source_url: https://arxiv.org/abs/2512.19832
tags:
- genai
- workers
- humanlike
- humanlikeness
- hazards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tech workers navigate humanlike generative AI amid unclear guidance
  and competing conceptualizations of humanlikeness. Using focus groups with 30 professionals
  across six roles, the study found workers draw on positional knowledge to assess
  risks despite limited policies.
---

# How Tech Workers Contend with Hazards of Humanlikeness in Generative AI

## Quick Facts
- arXiv ID: 2512.19832
- Source URL: https://arxiv.org/abs/2512.19832
- Authors: Mark Díaz; Renee Shelby; Eric Corbett; Andrew Smart
- Reference count: 40
- Tech workers navigate humanlike generative AI amid unclear guidance and competing conceptualizations of humanlikeness

## Executive Summary
This qualitative study investigates how tech workers across six professional roles understand and respond to "humanlikeness" in generative AI systems. Through focus groups with 30 professionals, the research reveals that workers rely on positional knowledge to assess risks when organizational guidance is absent. The study develops a conceptual map distinguishing humanlikeness features from anthropomorphism and identifies key hazards including overtrust, workload burden, and believability concerns. Findings emphasize the need for clearer operational definitions and cross-functional coordination to guide responsible development and mitigate potential harms.

## Method Summary
The study employed seven remote focus groups with 30 U.S.-based tech workers across six roles (ML Engineering, Product Policy, UX, PM, Tech Writing, Communications). Using a hybrid codebook thematic analysis approach with ATLAS.ti software, two researchers coded transcripts following a five-stage process combining deductive (literature-based) and inductive (emergent) coding methods. The analysis produced a conceptual map articulating relationships between humanlikeness features, anthropomorphism, and identified hazards.

## Key Results
- Workers draw on positional knowledge to assess humanlikeness risks despite limited organizational policies
- Humanlikeness encompasses interaction dynamics, conversational input, and task sophistication
- The study calls for clearer operational definitions of humanlikeness and cross-functional support
- Key hazards identified include overtrust, workload burden, believability concerns, and disparate AI literacy impacts

## Why This Works (Mechanism)

### Mechanism 1
Workers' positional knowledge shapes which humanlikeness hazards they identify, leading to fragmented mitigation when coordination is absent. Role-specific expertise creates divergent hazard salience without cross-functional coordination.

### Mechanism 2
Conflating "humanlikeness" (design features) with "anthropomorphism" (user perception) obscures causal pathways from specific features to specific hazards, preventing targeted mitigation.

### Mechanism 3
Adoption pressure combined with missing guidance forces workers into ad hoc risk assessment, increasing cognitive burden and producing inconsistent mitigation practices.

## Foundational Learning

- **Sociotechnical hazards**: Hazards emerge from interactions between technical features and social context, not from features alone; risk assessment requires situating features in use contexts.
  - Quick check: Can you explain why the same humanlikeness feature (e.g., conversational input) might be beneficial in one context (accessibility) and hazardous in another (fraud detection)?

- **Positionality and situated knowledge**: Different workers perceive different hazards based on their role-specific experiences; comprehensive hazard mapping requires cross-functional input.
  - Quick check: If you're building a hazard taxonomy for a genAI product, whose perspectives must you include beyond engineers?

- **Hazard vs. harm distinction**: Hazards are conditions with potential for harm; harms are realized negative outcomes. Mitigation targets hazards to prevent harm cascade.
  - Quick check: Is "anthropomorphism" a hazard or a harm? (Answer: a hazard—it can lead to harms like overtrust, but isn't inherently harmful.)

## Architecture Onboarding

- **Component map**: Humanlikeness feature categories (visual cues, linguistic cues, behavioral cues, task sophistication, interaction dynamics) -> Hazard categories (additional labor, miscalibrated trust, believability, worsened UX, disparate AI literacy, worker replacement) -> Worker role lenses (ML engineering, policy, UX, communications, product management) -> Organizational decision points (model selection, interface design, output formatting, policy development, user communication)

- **Critical path**: Start with humanlikeness feature inventory -> map features to potential hazards per context -> identify which roles perceive which hazards -> establish cross-functional review to aggregate positional knowledge -> develop context-specific mitigation thresholds

- **Design tradeoffs**: Humanlike conversational input improves accessibility and lowers barriers for low-technical-literacy users, but simultaneously enables more convincing fraud and reduces friction that might prompt critical evaluation. Calibrate based on use context and user vulnerability.

- **Failure signatures**:
  - Mitigation whack-a-mole: Addressing hazards in isolation without understanding feature-hazard interactions leads to new unintended harms
  - Guidance obsolescence: Policies written for one model version become irrelevant after updates
  - Role silos: If only engineers evaluate humanlikeness, UX/social interaction hazards go undetected

- **First 3 experiments**:
  1. Feature-to-hazard mapping validation: Catalog humanlikeness features and systematically elicit hazard perceptions from workers across ≥4 roles to identify blind spots
  2. Anthropomorphism measurement pilot: Test whether anthropomorphism scales correlate with specific hazard indicators for one humanlikeness feature category
  3. Cross-functional hazard review simulation: Run mock product review where each role presents top 3 perceived humanlikeness hazards and synthesize into prioritized mitigation backlog

## Open Questions the Paper Calls Out

### Open Question 1
Which specific humanlikeness features correlate most strongly with distinct hazards such as miscalibrated trust or additional labor? This requires empirical studies mapping specific design features to hazard frequency and severity across diverse genAI interfaces.

### Open Question 2
What measurement thresholds for humanlike features optimize the balance between minimizing hazards and preserving desired user experiences? Experimental studies testing varying feature intensities against metrics for trust calibration and task success are needed.

### Open Question 3
How can organizational structures facilitate information flow regarding humanlikeness risks between upstream foundation model developers and downstream product teams? Case studies of cross-functional coordination systems demonstrating improved risk communication are required.

### Open Question 4
Does the relationship between worker positionality and hazard perception vary significantly across non-tech industries compared to the tech sector? Comparative qualitative studies with knowledge workers in healthcare, education, and finance would test generalizability.

## Limitations

- Small sample (n=30) from U.S. tech workers limits generalizability to other regions, contexts, or non-tech sectors
- Findings based on self-reported perceptions may not capture actual behavioral outcomes
- Conceptual map requires empirical validation across different product contexts and user populations

## Confidence

- **High Confidence**: Workers lack clear guidance and draw on positional knowledge to assess risks (supported by direct quotes and multiple role perspectives)
- **Medium Confidence**: Conceptual map's feature-hazard relationships (theoretically plausible but requiring empirical testing)
- **Medium Confidence**: Conflation mechanism between humanlikeness and anthropomorphism as obstacle to targeted mitigation (logically sound but weakly supported by corpus evidence)

## Next Checks

1. **Cross-sector replication**: Conduct similar focus groups with workers in healthcare, education, and financial services to test whether positional knowledge patterns hold across domains with different regulatory and user contexts.

2. **Feature-hazard mapping validation**: For a specific product context, systematically test whether identified humanlikeness features actually produce the predicted hazards using mixed methods (surveys measuring perceived hazards plus observational studies of actual user behavior).

3. **Guidance protocol effectiveness trial**: Compare hazard assessment outcomes between teams using ad hoc approaches versus teams provided with structured, role-specific guidance protocols to quantify cognitive burden reduction and consistency improvements.