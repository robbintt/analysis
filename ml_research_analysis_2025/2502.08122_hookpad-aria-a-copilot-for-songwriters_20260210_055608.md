---
ver: rpa2
title: 'Hookpad Aria: A Copilot for Songwriters'
arxiv_id: '2502.08122'
source_url: https://arxiv.org/abs/2502.08122
tags:
- music
- hookpad
- aria
- users
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hookpad Aria is a generative AI system integrated into Hookpad,
  a web-based editor for composing Western pop songs as lead sheets. The system supports
  non-sequential composition workflows including left-to-right generation, fill-in-the-middle,
  and bidirectional melody-harmony generation.
---

# Hookpad Aria: A Copilot for Songwriters

## Quick Facts
- arXiv ID: 2502.08122
- Source URL: https://arxiv.org/abs/2502.08122
- Authors: Chris Donahue; Shih-Lun Wu; Yewon Kim; Dave Carlton; Ryan Miyakawa; John Thickstun
- Reference count: 0
- Primary result: 318k suggestions generated for 3k users since March 2024, with 74k (23%) accepted

## Executive Summary
Hookpad Aria is a generative AI system integrated into Hookpad, a web-based editor for composing Western pop songs as lead sheets. The system supports non-sequential composition workflows including left-to-right generation, fill-in-the-middle, and bidirectional melody-harmony generation. Built on the Anticipatory Music Transformer fine-tuned on 50k lead sheets from TheoryTab, Aria offers users control over selected regions while conditioning on global song attributes like meter, key, and tempo. Since its March 2024 release, Aria has generated 318k suggestions for 3k users, with 74k suggestions accepted into songs. User interviews revealed that Aria facilitates creative ideation, preserves user agency through short reusable suggestions and seamless integration, and identified needs for additional control like genre and emotional tone. The system creates a scalable data flywheel for future research in model alignment, personalization, and human-AI music co-creation evaluation.

## Method Summary
Hookpad Aria builds on the Anticipatory Music Transformer, a 360M parameter model fine-tuned on 50k lead sheets from TheoryTab. The system partitions MIDI notes into events and controls, shifting controls forward by 5 seconds and interleaving them with events into a single sequence modeled by a standard Transformer. Random infilling examples are constructed by sampling time spans and capability types (left-to-right, fill-in-middle, melody-to-harmony, harmony-to-melody) and partitioning melody, harmony, and click track accordingly. A click track with one note per beat encodes beat positions in absolute time. The system generates short, locally-scoped suggestions for user-selected measure spans, logging accept/ignore actions as implicit feedback.

## Key Results
- 318k suggestions generated for 3k users since March 2024 launch
- 74k suggestions accepted (23% acceptance rate)
- User interviews with 8 participants revealed system facilitates creative ideation while preserving agency
- Identified user needs for genre, emotional tone, and structural element controls

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anticipatory conditioning enables bidirectional infilling within a standard autoregressive framework
- Mechanism: The model partitions MIDI notes into events (e) and controls (c), shifting controls forward by 5 seconds and interleaving them with events. By training on Pθ(e | c), the model learns to generate current events while "anticipating" future control tokens—enabling fill-in-the-middle without architectural changes to the Transformer
- Core assumption: The 5-second anticipation window is sufficient for capturing relevant musical context spans in Western pop songs
- Evidence anchors: [section] "Controls are shifted forward in time by 5 seconds and interleaved with events into a single sequence which is then modeled with a standard Transformer LM, representing Pθ(e | c)." [section] "By partitioning training examples such that notes in subsequent context are assigned to controls, the Anticipatory Music Transformer learns to fill-in-the-middle." [corpus] Weak direct corpus support; neighbor papers focus on harmonization and emotion datasets rather than anticipatory architectures
- Break condition: If musical phrases frequently span >5 seconds or require longer-range structural dependencies, anticipation may fail to capture critical context

### Mechanism 2
- Claim: Capability-specific fine-tuning via random partitioning yields compositional control modes from a single model
- Mechanism: Each training example randomly selects a time span [ts, te] and a capability (left-to-right, fill-in-middle, melody-to-harmony, harmony-to-melody), then partitions melody M, harmony H, and click track C into events/controls accordingly. The model learns all four capabilities through exposure to varied partitioning schemes
- Core assumption: Random sampling across capabilities during fine-tuning produces roughly balanced competence across all generation modes
- Evidence anchors: [section] Table 1 defines partitioning rules: e.g., Fill-in-middle uses M<te ∪ H<te as events and M≥te ∪ H≥te ∪ C as controls. [section] "Our backbone is the result of fine tuning on many of these random examples per song." [corpus] No corpus validation of multi-capability training; neighbor papers address single-task harmonization
- Break condition: If certain capabilities (e.g., melody-to-harmony) are underrepresented in random sampling, model quality may degrade asymmetrically

### Mechanism 3
- Claim: Short, locally-scoped suggestions preserve creative agency by keeping humans in compositional control
- Mechanism: Users select specific measure spans and receive infinite scrollable suggestions for just that region, conditioned on local context. Unlike text-to-music systems that generate complete tracks, Aria's suggestions serve as "creative sparks" that users can accept, modify, or discard—maintaining ownership of the overall composition
- Core assumption: Users prefer granular control over end-to-end generation for songwriting tasks
- Evidence anchors: [section] "Participants viewed Hookpad Aria as a fellow songwriter, turning to it when they encountered creative blocks." [section] "Users remarked that three aspects of Hookpad Aria conveyed a sense of agency: short, reusable suggestions... seamless integration... non-sequential generation." [corpus] Limited corpus support; no comparative studies on agency in music co-creation found in neighbors
- Break condition: If users expect genre/emotion control or longer-form generation, the current localized approach may feel restrictive

## Foundational Learning

- Concept: Autoregressive language modeling for symbolic music
  - Why needed here: Aria builds on a Transformer LM that predicts token sequences; understanding next-token prediction is essential for grasping how anticipation modifies standard autoregression
  - Quick check question: How does interleaving future "control" tokens with current "event" tokens change what the model conditions on during inference?

- Concept: Functional harmony representation
  - Why needed here: Hookpad uses a proprietary functional harmony encoding (e.g., Roman numeral analysis) rather than raw MIDI; the system must translate between these representations for model input/output
  - Quick check question: What information might be lost when converting functional harmony symbols to MIDI note events?

- Concept: Click track as temporal anchor
  - Why needed here: Hookpad indexes time in beats, not absolute time; adding a click track "instrument" with one note per beat allows the model to learn beat-aligned generation
  - Quick check question: Why can't the model infer beat positions from note onsets alone?

## Architecture Onboarding

- Component map: Hookpad editor -> functional harmony + melody encoding -> MIDI with click track -> partition into events/controls -> Anticipatory Music Transformer -> decode to functional format -> display in UI
- Critical path: 1) User selects measure span and generation mode 2) System retrieves local context + global attributes 3) Encoder constructs events/controls partition per Table 1 4) Model samples from Pθ(e | c) autoregressively 5) Decoder converts output back to Hookpad's functional format 6) User browses, modifies, or accepts suggestion -> feedback logged
- Design tradeoffs: 5-second anticipation window vs. longer context (shorter windows reduce compute but may miss long-range structure); functional harmony encoding vs. raw MIDI (functional representation captures musical semantics but requires lossy translation); implicit feedback only vs. explicit ratings (accept/ignore signals are noisy but scale well; explicit ratings would improve alignment but reduce engagement)
- Failure signatures: Harmonic incoherence (generated harmony clashes with surrounding context -> may indicate insufficient local context window or training data gaps); rhythmic drift (generated melody misaligns with beat grid -> click track encoding failure or insufficient beat anticipation); mode confusion (harmony-to-melody generation produces chord-like outputs -> capability partitioning learned incorrectly)
- First 3 experiments: 1) Capability ablation: Evaluate generation quality for each of four modes separately to identify asymmetric weaknesses in fine-tuning approach 2) Anticipation window sweep: Test 2s, 5s, 10s windows on fill-in-the-middle tasks to validate 5-second design choice against longer musical phrases 3) Feedback signal analysis: Correlate implicit acceptance rates with user-reported satisfaction to assess whether accept/ignore is reliable proxy for quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can symbolic music models be conditioned to provide high-level semantic control over genre, emotional tone, and structural elements (e.g., verse vs. chorus)?
- Basis in paper: [explicit] Section 3 (Findings) states that while users appreciated the existing controls, they "expressed a desire for even more control" specifically over "genre, emotional tone, intended instruments, and structural elements."
- Why unresolved: The current system conditions primarily on local context and global attributes, but lacks mechanisms to interpret or enforce semantic descriptors or functional song structure
- What evidence would resolve it: A modified interface allowing users to tag requests with emotion or section labels, resulting in generated output that statistically matches or user-studies confirm matches those intent descriptions

### Open Question 2
- Question: Can implicit feedback signals (suggestion acceptance vs. ignore) be effectively used to align music generation models with user preferences?
- Basis in paper: [explicit] Section 4 (Research Opportunities) explicitly lists "aligning models to user feedback" as a potential avenue for future research enabled by the "scalable data flywheel"
- Why unresolved: While the infrastructure collects this data, the paper does not describe methods for utilizing this binary implicit signal to fine-tune the model's weights or objective function
- What evidence would resolve it: A study demonstrating that a model fine-tuned on accepted suggestions achieves a higher acceptance rate or user satisfaction score in A/B testing compared to the baseline model

### Open Question 3
- Question: How do various generative architectures compare in "in-the-wild" user satisfaction when evaluated via large-scale A/B testing?
- Basis in paper: [explicit] Section 4 highlights the opportunity for "A/B testing different generative models for in-the-wild evaluation" as a specific research goal
- Why unresolved: The paper introduces a single system and reports aggregate usage stats, but it does not compare different backbone architectures or decoding strategies within the live environment
- What evidence would resolve it: Deployment logs showing statistically significant differences in acceptance rates or usage duration between distinct model configurations served to randomized user groups

## Limitations
- Evaluation relies entirely on qualitative interviews with 8 users rather than quantitative performance metrics or larger-scale user studies
- 23% acceptance rate provides only coarse signal of user satisfaction without understanding distribution across capabilities or user skill levels
- Anticipatory conditioning mechanism's effectiveness depends critically on 5-second window assumption, which lacks empirical validation against longer musical phrases

## Confidence

- **High confidence**: The basic architecture (Anticipatory Music Transformer fine-tuned on TheoryTab) is technically sound and the integration into Hookpad's functional harmony representation is well-specified
- **Medium confidence**: The four-capability fine-tuning approach and the 5-second anticipation window are theoretically justified but lack corpus validation or ablation studies
- **Low confidence**: Claims about the system's impact on creative agency and its effectiveness as a "copilot" are based on a small sample of user interviews without comparative analysis

## Next Checks

1. **Capability balance evaluation**: Systematically evaluate generation quality (via human ratings) for each of the four capabilities (left-to-right, fill-in-middle, melody-to-harmony, harmony-to-melody) to identify asymmetric weaknesses in the fine-tuning approach

2. **Anticipation window ablation**: Conduct controlled experiments testing 2-second, 5-second, and 10-second anticipation windows on fill-in-the-middle tasks with professional musicians to empirically validate the 5-second design choice

3. **Feedback signal validation**: Correlate implicit acceptance rates with explicit user satisfaction ratings from a follow-up survey (n=50+) to determine whether accept/ignore signals reliably proxy for generation quality