---
ver: rpa2
title: Predicting Social Media Engagement from Emotional and Temporal Features
arxiv_id: '2508.21650'
source_url: https://arxiv.org/abs/2508.21650
tags:
- engagement
- likes
- emotional
- comments
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning approach for predicting
  social media engagement (comments and likes) from emotional and temporal features.
  The dataset contains 600 songs with annotations for valence, arousal, and related
  sentiment metrics.
---

# Predicting Social Media Engagement from Emotional and Temporal Features

## Quick Facts
- arXiv ID: 2508.21650
- Source URL: https://arxiv.org/abs/2508.21650
- Reference count: 11
- Primary result: Machine learning model predicts likes (R²=0.98) but not comments (R²=0.41) from emotional and temporal features

## Executive Summary
This paper presents a machine learning approach for predicting social media engagement (comments and likes) from emotional and temporal features. The dataset contains 600 songs with annotations for valence, arousal, and related sentiment metrics. A multi-target regression model based on HistGradientBoostingRegressor is trained on log-transformed engagement ratios to address skewed targets. Performance is evaluated with both a custom order of magnitude accuracy and standard regression metrics, including the coefficient of determination (R²). Results show that emotional and temporal metadata, together with existing view counts, predict future engagement effectively. The model attains R² = 0.98 for likes but only R² = 0.41 for comments. This gap indicates that likes are largely driven by readily captured affective and exposure signals, whereas comments depend on additional factors not represented in the current feature set.

## Method Summary
The study uses a multi-target regression approach with HistGradientBoostingRegressor to predict engagement ratios (comments-per-view and likes-per-view) from emotional and temporal features. The dataset contains 600 songs with 10 emotional features (valence, arousal, tension, atmospheric, happy, dark, sad, angry, sensual, sentimental), view counts, engagement metrics, and upload dates. The preprocessing pipeline involves filtering zero/negative values, deriving temporal features (age in days, upload month/day of week), computing engagement ratios, clipping top 1% of ratios, and applying log(1+x) transforms to targets and views. The model is trained using MultiOutputRegressor wrapper with hyperparameter tuning via HalvingRandomSearchCV, optimizing negative mean absolute error. Evaluation uses custom order-of-magnitude accuracy plus standard metrics (MAE, RMSE, R²).

## Key Results
- The model achieves R² = 0.98 for predicting likes but only R² = 0.41 for comments
- Log-transforming engagement ratios stabilizes variance and improves model performance
- Temporal features (upload timing, content age) contribute to prediction accuracy
- The large performance gap between likes and comments suggests comments require unobserved semantic context

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low-effort engagement (likes) is predictable via affective and exposure signals, whereas high-effort engagement (comments) requires unobserved semantic or community context.
- **Mechanism:** Likes function as a reflexive signal of immediate content reception, correlating strongly with valence, arousal, and view counts. Comments represent deliberative discourse, introducing variance from external factors (e.g., controversy, specific topics) not present in the feature set.
- **Core assumption:** The emotional annotations (valence, arousal) provided in the dataset accurately reflect the emotional response of the general user population.
- **Evidence anchors:** [abstract] "The model attains R² = 0.98 for likes but only R² = 0.41 for comments... likes are largely driven by readily captured affective and exposure signals." [section 3, page 5] "Comments require more cognitive effort and are often influenced by deeper, more nuanced factors... not present in our feature set."
- **Break condition:** If the emotional features used are too generic to capture niche "discourse triggers" (e.g., specific lyrical controversies), the high R² for likes may hold, but comment prediction will fail to improve without semantic features.

### Mechanism 2
- **Claim:** Log-transforming engagement targets and view counts stabilizes variance, allowing the regressor to learn relative quality rather than absolute virality.
- **Mechanism:** Social media data is heavy-tailed; raw counts create error surfaces dominated by outliers. Applying `log(1+x)` normalizes the distribution, preventing the model from prioritizing viral outliers at the expense of the mid-tail.
- **Core assumption:** The relationship between features and the log-transformed target is approximately linear or learnable by tree ensembles, and the error distribution is stabilized by the transform.
- **Evidence anchors:** [section 2.1, page 2] "The raw Views count... was log-transformed... to normalize the distribution... target ratios... were then log-transformed... to stabilize their variance."
- **Break condition:** If the data contains a significant number of zero-values (handled here by filtering) or if the "viral" nature is the *only* signal of interest, log transforms would suppress the primary signal.

### Mechanism 3
- **Claim:** Normalizing engagement by views (ratios) decouples content quality from exposure bias.
- **Mechanism:** Raw counts correlate simply with time/algorithmic push. Ratios (e.g., `Likes/Views`) approximate the probability of engagement given an impression, acting as a more stable target for the regressor to assess content intrinsic appeal.
- **Core assumption:** View counts act as a sufficient proxy for exposure, and the ratio represents a stationary relationship between exposure and engagement.
- **Evidence anchors:** [section 2.1, page 2] "These ratios are a more stable representation of engagement quality than raw counts, which are heavily dependent on the sheer number of views."
- **Break condition:** If view counts are artificially inflated (e.g., bots, autoplay) without corresponding engagement potential, the ratio targets become noisy or misleading.

## Foundational Learning

- **Concept: HistGradientBoostingRegressor**
  - **Why needed here:** It is the core estimator chosen for its efficiency with mid-scale data (600 rows) and native handling of issues (missing values) that often plague real-world social media datasets.
  - **Quick check question:** Why is a histogram-based gradient booster preferred over standard Random Forest for datasets with high-cardinality features or missing values?

- **Concept: Target Transformation (Log-Transform)**
  - **Why needed here:** Social media engagement follows a power law. Without this, the model's Mean Absolute Error (MAE) would be dominated by a few viral songs, rendering predictions useless for average content.
  - **Quick check question:** If you back-transform a prediction from log-space to real-space, how does the error distribution change? (Hint: it's not symmetric).

- **Concept: Order-of-Magnitude Accuracy**
  - **Why needed here:** Standard R² or MAE can be misleading in skewed distributions. This custom metric aligns with the practical business need: distinguishing between "hundreds" vs "thousands" of likes is more critical than being off by exactly 50 likes on a scale of 100,000.
  - **Quick check question:** Why is exact RMSE a poor proxy for "success" when predicting viral content?

## Architecture Onboarding

- **Component map:** Input data -> Preprocessing (cleaning, feature engineering, log transforms) -> MultiOutputRegressor(HistGradientBoostingRegressor) -> Evaluation (custom accuracy + standard metrics)
- **Critical path:** The preprocessing of **engagement ratios** and subsequent **log-transformation**. If this step is missed or the clipping of the top 1% is not applied, the model will overfit to the skewed tail and fail to generalize.
- **Design tradeoffs:**
  - **Interpretability vs. Performance:** Used a custom accuracy metric for practical interpretability vs. standard R² for statistical rigor.
  - **Target Selection:** Using ratios (engagement quality) vs. raw counts (virality). The paper chooses quality/stability, sacrificing the ability to predict "breakout" viral scale directly.
- **Failure signatures:**
  - **The "Comment Gap":** Do not expect the model to explain comment variance (R² stuck ~0.41). This is not a bug, but a data limitation (comments need text/semantic features).
  - **Overfitting on small data:** With only 600 samples, aggressive hyperparameter tuning on `HistGradientBoosting` without cross-validation could lead to memorization.
- **First 3 experiments:**
  1. **Ablation Study:** Retrain the model removing `log_views` to determine if the high R² for likes is driven primarily by emotional features or just raw exposure.
  2. **Residual Analysis on Comments:** Analyze the specific songs where comment prediction fails most severely (high residuals) to identify missing features (e.g., genre, lyrical sentiment).
  3. **Threshold Sensitivity:** Test the Order-of-Magnitude Accuracy metric against standard MAE on different sub-slices of the data (e.g., niche vs. popular songs) to ensure the custom metric isn't hiding systematic bias.

## Open Questions the Paper Calls Out

- **Question:** To what extent does the inclusion of textual content features (lyrics, titles) close the performance gap between likes and comments prediction?
  - **Basis in paper:** [explicit] The authors state future work should "explore the inclusion of new features derived from the content itself" to address the low $R^2$ for comments.
  - **Why unresolved:** The model explains 98% of variance for likes but only 41% for comments, indicating missing drivers for conversational engagement.
  - **What evidence would resolve it:** A comparative study showing increased $R^2$ scores for comments when text-derived embeddings are added to the feature set.

- **Question:** How do emotional and temporal feature sets independently contribute to the overall predictive power of the model?
  - **Basis in paper:** [explicit] Section 3.1 explicitly notes that an "ablation study is necessary to quantify the individual contribution of each feature set."
  - **Why unresolved:** The current study aggregates features, leaving the specific marginal value of emotional metadata versus temporal metadata unknown.
  - **What evidence would resolve it:** Reporting model performance metrics after training exclusively on isolated feature subsets (e.g., only temporal vs. only emotional).

- **Question:** Which specific emotional dimensions are the most significant predictors of user engagement?
  - **Basis in paper:** [explicit] The authors list "feature importance analysis, possibly using methods like SHAP values" as a key area for future work.
  - **Why unresolved:** While the aggregate feature set is effective, the paper does not detail which of the 10 emotional metrics (e.g., Valence, Arousal) drive predictions.
  - **What evidence would resolve it:** SHAP (SHapley Additive exPlanations) values or similar interpretability scores ranking features by importance.

## Limitations

- The model's high R² for likes (0.98) versus comments (0.41) reveals a fundamental limitation: emotional and temporal features capture reflexive engagement well but miss the semantic or community-specific triggers that drive comments.
- The dataset size (600 songs) limits generalizability—results may not hold for broader music or content domains.
- The approach cannot predict deliberative engagement without richer features like content text or user demographics.

## Confidence

- **High Confidence:** Log-transforming targets stabilizes variance and prevents viral outliers from dominating error metrics. The preprocessing pipeline (ratios, clipping, log transforms) is sound for skewed social media data.
- **Medium Confidence:** Emotional features (valence, arousal) are valid proxies for user response to music, but the claim that they drive 98% of like variance needs external validation beyond this dataset.
- **Low Confidence:** The assertion that comments require "unobserved semantic context" is plausible but untested—residual analysis could reveal whether missing features (e.g., lyrical sentiment, genre) would improve predictions.

## Next Checks

1. **Ablation Study on Views:** Retrain the model without `log_views` to quantify whether the 0.98 R² for likes is driven by emotional features or just exposure. If R² drops significantly, the model overfits to popularity rather than content quality.

2. **Residual Analysis on Comments:** Identify songs with highest comment prediction errors. Check if these share traits (e.g., controversial genres, viral moments) not captured by current features. This tests whether semantic or community context is the true missing link.

3. **Cross-Domain Generalization:** Apply the model to a different content type (e.g., videos or articles) with similar emotional annotations. If performance drops sharply, the approach is domain-specific; if stable, it generalizes to low-effort engagement prediction.