---
ver: rpa2
title: "G\xF6del Test: Can Large Language Models Solve Easy Conjectures?"
arxiv_id: '2509.18383'
source_url: https://arxiv.org/abs/2509.18383
tags:
- gpt-5
- submodular
- algorithm
- proof
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We propose the G\xF6del Test: evaluating whether large language\
  \ models can produce correct proofs for very simple, previously unsolved conjectures.\
  \ To explore this challenge, we study GPT-5's performance on five conjectures in\
  \ combinatorial optimization, each supported by one or two source papers."
---

# Gödel Test: Can Large Language Models Solve Easy Conjectures?

## Quick Facts
- **arXiv ID**: 2509.18383
- **Source URL**: https://arxiv.org/abs/2509.18383
- **Reference count**: 13
- **Primary result**: GPT-5 produced nearly correct proofs for three easier problems and refuted one conjecture, but struggled with cross-paper synthesis

## Executive Summary
This paper introduces the Gödel Test, which evaluates whether large language models can produce correct proofs for simple, previously unsolved conjectures. The authors tested GPT-5 on five combinatorial optimization problems, each supported by one or two source papers. Results showed meaningful progress: GPT-5 succeeded on three easier problems and even refuted one conjecture while deriving a valid solution for another. However, it failed on problems requiring synthesis across multiple sources and provided flawed analysis for harder open-ended problems. The findings suggest GPT-5 has improved baseline mathematical competence and occasional originality, but struggles with connecting proof techniques across sources.

## Method Summary
The authors selected five unsolved conjectures in combinatorial optimization, each with supporting source papers. They prompted GPT-5 to generate proofs for these problems, evaluating the outputs based on correctness and completeness. The evaluation involved human assessment of proof quality, with particular attention to whether solutions were "nearly correct" or contained fundamental errors. The study focused on problems of varying difficulty, including both straightforward proofs and those requiring synthesis of techniques from multiple sources.

## Key Results
- GPT-5 produced nearly correct proofs for three easier problems
- The model successfully refuted one conjecture and derived a valid solution for another
- GPT-5 failed on problems requiring cross-paper synthesis and provided flawed analysis for harder open-ended problems

## Why This Works (Mechanism)
The study demonstrates that large language models can leverage pattern recognition and existing mathematical knowledge to generate proofs for well-defined problems. GPT-5's success on simpler problems suggests it can effectively retrieve and apply proof techniques from its training data. The model's ability to refute a conjecture indicates it can engage in critical mathematical reasoning rather than merely reproducing known results. However, failures on synthesis tasks reveal limitations in connecting disparate mathematical concepts across sources.

## Foundational Learning
- **Combinatorial optimization**: Understanding of optimization problems with discrete structures is essential for grasping the problem domain
  - *Why needed*: The conjectures tested are specifically in this field
  - *Quick check*: Can identify basic optimization problems like shortest path or maximum flow

- **Mathematical proof techniques**: Familiarity with common proof methods (induction, contradiction, construction) enables evaluation of generated proofs
  - *Why needed*: Proofs require specific logical structures and reasoning patterns
  - *Quick check*: Can distinguish between valid and invalid proof steps

- **Cross-source synthesis**: Ability to integrate concepts from multiple papers is crucial for complex problems
  - *Why needed*: Some conjectures require combining techniques from different sources
  - *Quick check*: Can identify connections between concepts from different mathematical papers

## Architecture Onboarding
**Component map**: Problem description -> GPT-5 processing -> Proof generation -> Human evaluation
**Critical path**: Source paper analysis → Problem understanding → Proof technique selection → Solution verification
**Design tradeoffs**: The study prioritizes problem accessibility over diversity, focusing on combinatorial optimization problems that are simple enough for GPT-5 but still unsolved
**Failure signatures**: Inability to synthesize across sources, flawed analysis on open-ended problems, incorrect application of proof techniques
**First experiments**: 1) Test GPT-5 on problems requiring only single-source solutions 2) Evaluate model's ability to refute conjectures 3) Assess performance on problems with varying degrees of difficulty

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of five conjectures limits generalizability
- Problems are limited to combinatorial optimization, not representing full mathematical spectrum
- Evaluation criteria for "nearly correct" proofs lack precise formalization
- Study lacks formal verification of generated proofs, relying on human evaluation

## Confidence
- **High confidence**: GPT-5 demonstrates improved baseline mathematical competence compared to previous models
- **Medium confidence**: The model shows occasional originality in proof generation and refutation
- **Low confidence**: The model's ability to synthesize techniques across multiple source papers

## Next Checks
1. **Expand problem diversity**: Test GPT-5 on a larger set of 20-30 conjectures spanning multiple mathematical domains (number theory, algebra, geometry) to better assess generalization.

2. **Implement automated proof verification**: Develop formal verification protocols using theorem provers to objectively assess proof correctness, reducing human bias in evaluation.

3. **Cross-model comparison**: Compare GPT-5's performance against other contemporary models (Claude, Gemini, specialized math models) on the same conjecture set to establish relative performance benchmarks.