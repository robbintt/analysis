---
ver: rpa2
title: A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under
  Prior Mismatch
arxiv_id: '2601.09831'
source_url: https://arxiv.org/abs/2601.09831
tags:
- have
- then
- assumption
- denoiser
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes the first convergence theory for plug-and-play
  proximal gradient descent (PnP-PGD) when the denoiser is trained on a mismatched
  prior distribution. The analysis removes restrictive assumptions from prior work,
  allowing nonconvex data fidelity terms, nonconvex regularizers, and expansive denoisers.
---

# A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch

## Quick Facts
- arXiv ID: 2601.09831
- Source URL: https://arxiv.org/abs/2601.09831
- Reference count: 5
- Primary result: First convergence theory for PnP-PGD with mismatched priors, allowing nonconvex terms and expansive denoisers, with EPnP variant reducing error variance

## Executive Summary
This work establishes convergence theory for Plug-and-Play Proximal Gradient Descent (PnP-PGD) when the denoiser is trained on a mismatched prior distribution. The analysis removes restrictive assumptions from prior work, allowing nonconvex data fidelity terms, nonconvex regularizers, and expansive denoisers. The main result shows that PnP-PGD converges to a stationary point under a mismatch error bound condition, with explicit rates dependent on the mismatch magnitude. The paper also introduces equivariant PnP (EPnP), proving it reduces error variance and tightens convergence bounds compared to standard PnP-PGD when the bias is anisotropic.

## Method Summary
The method analyzes PnP-PGD by treating the mismatched denoiser as an $\epsilon$-optimal inexact proximal operator rather than the exact solution. This allows convergence analysis under prior mismatch by separating the convergence rate ($O(1/t)$) from accumulated mismatch error. The Gradient-Step denoiser parameterization enables handling expansive denoisers and nonconvex regularizers. EPnP is implemented by averaging denoiser outputs over a symmetry group to reduce structural bias variance.

## Key Results
- Proves PnP-PGD convergence under prior mismatch with gradient norms decreasing as $O(1/t)$ plus error term
- Allows nonconvex data fidelity, nonconvex regularizers, and expansive denoisers
- Introduces EPnP that reduces error variance and tightens convergence bounds
- Convergence depends on mismatch error being summable (decreasing as $O(1/k^{1+\delta})$)

## Why This Works (Mechanism)

### Mechanism 1
PnP-PGD maintains convergence under prior mismatch if the mismatched denoiser acts as an $\epsilon$-optimal inexact proximal operator. The method models the mismatched denoiser not as the exact proximal operator but as an approximate solver. By defining the approximation error $\epsilon_k$ and leveraging the strong convexity of the proximal sub-problem, the analysis separates the convergence rate ($O(1/t)$) from the accumulated mismatch error. The gradient norm decreases as long as the error sequence $\{\epsilon_k\}$ is summable or decreases sufficiently fast.

### Mechanism 2
Convergence is provable for expansive denoisers and nonconvex regularizers via the Gradient-Step denoiser parameterization. Standard PnP theory often requires non-expansive denoisers (Lipschitz constant $L \leq 1$). This work leverages the GS denoiser form $D_\sigma = \nabla h_\sigma$. By requiring the residual $Id - D_\sigma$ to be contractive, the authors ensure the potential $h_\sigma$ is strongly convex. This allows the denoiser to be expansive (Lipschitz $> 1$) while still guaranteeing the associated regularizer $\phi_\sigma$ is well-defined and smooth.

### Mechanism 3
Equivariant PnP (EPnP) tightens the convergence bound by reducing structural bias variance. Structural prior mismatch introduces an error bias $E$ that is typically anisotropic. EPnP averages the denoiser output over a symmetry group $G$. Since the target denoiser is equivariant, the bias of the averaged estimator has strictly lower squared norm than the original bias by a factor of the variance of the bias over the group.

## Foundational Learning

- **Proximal Gradient Descent & Proximal Operator**: Understanding that $prox$ solves a local quadratic minimization is essential to grasp why "inexact proximal solvers" (mismatched denoisers) introduce bounded error terms.
- **Strong Convexity & Smoothness**: The core theoretical lever is proving the proximal sub-problem is strongly convex (via the contractive residual assumption). This property allows bounding the distance to the solution using the function value gap.
- **Equivariance and Group Theory in Vision**: To understand EPnP, one must know what it means for a function (denoiser) to be equivariant to a group (e.g., rotations). The theory relies on averaging over this group to cancel out directional biases.

## Architecture Onboarding

- **Component map**: PnP Loop -> Mismatched Denoiser ($\hat{D}_\sigma$) -> EPnP Wrapper
- **Critical path**: Validating the contractive residual condition. An engineer must verify that $||\nabla g_\sigma(x) - \nabla g_\sigma(y)|| \leq L ||x-y||$ for $L < 1$, or determine the relaxation parameter $\alpha$ required to enforce it.
- **Design tradeoffs**: 
  - Step size ($\lambda$) vs. Contraction: The theorem requires $\lambda L_f < 1$, capping regularization strength
  - Relaxation ($\alpha$): Using relaxed denoiser $D^\alpha_\sigma = \alpha D_\sigma + (1-\alpha)Id$ helps meet theoretical assumptions but may dampen denoising power
- **Failure signatures**:
  - Exploding Iterates: If $L_f$ is underestimated, violating $\lambda L_f < 1$, loss may increase
  - Stagnation: If the mismatch error $\epsilon_k$ is large and non-decreasing, gradient norm will plateau
- **First 3 experiments**:
  1. Verify Residual Contractivity: Calculate Jacobian of denoiser residual and check spectral norm
  2. Baseline PnP vs. Theory: Run PnP-PGD on mismatched task and plot gradient norm vs iterations
  3. EPnP Ablation: Implement EPnP with rotation/flip averaging and compare error floor

## Open Questions the Paper Calls Out

### Open Question 1
Is the assumption that mismatched and target denoisers share the exact same range ($Im(D_\sigma) = Im(\hat{D}_\sigma)$) valid for practical deep architectures with differing inductive biases? This assumption is described as "mild" but may be restrictive if structural mismatch fundamentally alters the manifold of denoised outputs.

### Open Question 2
Does EPnP fail to tighten the convergence bound if the bias generated by the mismatched prior is isotropic rather than anisotropic? Lemma 2.2 proves variance reduction only under anisotropic bias conditions. If structural error is uniformly distributed, the variance term could vanish, negating theoretical gain.

### Open Question 3
Do practical mismatched denoisers satisfy the summable error condition ($\sum \epsilon_k < \infty$) required to guarantee gradient norm convergence to zero? Convergence requires $\epsilon_k$ to decrease as $O(1/k^{1+\delta})$. It's unverified if standard deep learning denoisers exhibit this specific decay rate during optimization.

### Open Question 4
How does the step-size constraint $\lambda L_f < 1$ limit PnP-PGD performance in ill-conditioned inverse problems? The theorem caps regularization parameter $\lambda$ based on data fidelity Lipschitz constant, potentially preventing optimal regularization strengths for high-quality recovery.

## Limitations
- Theoretical framework relies on gradient-step denoiser structure which may not hold for arbitrary pretrained deep denoisers
- Lipschitz constant requirements (L < 1 for residual, λL_f < 1) could be restrictive in practice
- EPnP improvement assumes structural bias is anisotropic and transformation group aligns with data invariances
- Explicit rates O(1/t) + error term may be dominated by error floor in severe mismatch cases

## Confidence

- **High confidence**: Core theoretical mechanism showing PnP-PGD convergence under prior mismatch via inexact proximal operator analysis; contractive residual condition for expansive denoisers
- **Medium confidence**: Practical significance of EPnP's variance reduction; assumptions about structural bias anisotropy may not hold universally
- **Low confidence**: Explicit rates O(1/t) + error term are meaningful when mismatch is severe, as error floor may dominate

## Next Checks

1. **Lipschitz Constant Verification**: Implement spectral norm estimation of denoiser Jacobian (I - J_D) across multiple pretrained models. Test whether relaxation parameter α is necessary for convergence and quantify trade-off with denoising performance.

2. **Prior Mismatch Severity Study**: Systematically vary mismatch between training and test distributions (faces → MRI → natural images) and measure error term in Theorem 2.1. Verify whether error floor scales as predicted and if it dominates for severe mismatches.

3. **EPnP Ablation Under Different Biases**: Implement EPnP with various transformation groups (rotations, flips, scales) on architectures with known structural biases (CNNs vs Transformers). Measure bias variance reduction and verify Lemma 2.2's predictions across different bias types.