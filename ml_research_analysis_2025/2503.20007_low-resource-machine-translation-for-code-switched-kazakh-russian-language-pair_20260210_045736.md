---
ver: rpa2
title: Low-resource Machine Translation for Code-switched Kazakh-Russian Language
  Pair
arxiv_id: '2503.20007'
source_url: https://arxiv.org/abs/2503.20007
tags:
- data
- language
- translation
- computational
- kazakh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for training machine translation
  systems on the Kazakh-Russian language pair without labeled data, using synthetic
  data generation to address the lack of code-switched parallel corpora. The approach
  employs five types of data augmentation, with SimAlign-based alignment (cs-5) yielding
  the best results.
---

# Low-resource Machine Translation for Code-switched Kazakh-Russian Language Pair

## Quick Facts
- **arXiv ID**: 2503.20007
- **Source URL**: https://arxiv.org/abs/2503.20007
- **Reference count**: 26
- **Primary result**: Best model achieves 16.48 BLEU, nearly matching commercial system performance

## Executive Summary
This paper addresses the challenge of machine translation for Kazakh-Russian code-switched text in a low-resource setting without labeled parallel data. The authors develop a synthetic data generation approach using five types of data augmentation, with SimAlign-based word alignment (cs-5) producing the most linguistically natural code-switched data. The newly introduced Kazakh-Russian code-switching dataset contains 618 parallel sentences. The best model (NLLB-3.3B with PiSSA fine-tuning) achieves 16.48 BLEU, approaching commercial system performance and outperforming it in human evaluation.

## Method Summary
The method employs synthetic data generation through five augmentation types to create code-switched training data without labeled parallel corpora. The best-performing approach (cs-5) uses SimAlign for contextual word alignment with 15% token replacement. The model training pipeline combines augmented data with domain adaptation from translated Russian tweet corpus (RTC) using NLLB-200-distilled-600M. The final model fine-tunes NLLB-3.3B using PiSSA parameter-efficient approach on the combined corpus, achieving 16.48 BLEU on the KRCS evaluation set.

## Key Results
- NLLB-3.3B achieves 16.48 BLEU, approaching commercial system performance
- SimAlign-based augmentation (cs-5) outperforms other methods, improving baseline by ~2 BLEU
- Human evaluation contradicts BLEU rankings, with NLLB-3.3B scoring 3.09 vs Yandex MT at 2.80
- Domain adaptation via RTC corpus contributes ~0.6 BLEU improvement

## Why This Works (Mechanism)

### Mechanism 1: SimAlign produces linguistically natural code-switched data
- Claim: SimAlign-based word alignment creates more natural code-switched synthetic data than dictionary or statistical methods
- Evidence: Only cs-5 augmentation improved baseline results; others degraded performance
- Break condition: Poor alignment quality for morphologically rich languages

### Mechanism 2: Domain adaptation improves colloquial translation
- Claim: Translated social media corpora improve model performance on informal code-switched evaluation data
- Evidence: LaBSE embeddings show RTC closer to KRCS than other training sources
- Break condition: Translation quality too poor or domain characteristics not preserved

### Mechanism 3: Multilingual pre-training provides strong foundation
- Claim: NLLB-3.3B pre-trained representations enable competitive performance after synthetic fine-tuning
- Evidence: Model achieves BLEU score close to commercial system; human evaluation favors it
- Break condition: Pre-training lacks sufficient representation of target language pair

## Foundational Learning

- **Code-switching vs. code-mixing terminology**: Paper uses these terms somewhat interchangeably; understanding the distinction affects augmentation design interpretation. Quick check: Which augmentation type involves morphological adaptation versus simple lexical insertion?

- **Word alignment in statistical vs neural MT**: Understanding fastalign versus SimAlign clarifies cs-5 superiority. Quick check: Why would contextual embeddings produce better alignments for code-switched data than co-occurrence statistics?

- **BLEU limitations for code-switched evaluation**: Human evaluation contradicted BLEU rankings. Quick check: What aspects of code-switched translation might BLEU miss that human evaluators would catch?

## Architecture Onboarding

- **Component map**: Raw Kazakh corpora → preprocessing → augmentation (cs-1 through cs-5) → combined with domain adaptation data (RTC) → fine-tune NLLB-3.3B with PiSSA
- **Critical path**: 1) Prepare training data with M2M100 filtering; 2) Generate cs-5 augmented data using SimAlign; 3) Translate RTC corpus via NLLB-200-distilled-600M; 4) Combine all data with deduplication; 5) Fine-tune NLLB-3.3B with PiSSA
- **Design tradeoffs**: cs-5 best for real code-switching but other methods may help on synthetic test sets; model size vs efficiency requires PiSSA rather than full fine-tuning; RTC alone insufficient for domain adaptation
- **Failure signatures**: BLEU degradation on original test sets with cs-1 through cs-4 augmentations; large gap between automatic metrics and human evaluation; identity baseline achieving 7.55 BLEU indicates substantial Russian tokens in code-switched sentences
- **First 3 experiments**:
  1. Reproduce augmentation comparison: Train transformer-600 on each augmentation type, evaluate on KRCS to verify cs-5 superiority
  2. Ablate domain adaptation: Train with/without RTC corpus, measure delta on KRCS
  3. Validate human evaluation discrepancy: Sample 100 KRCS sentences, compare NLLB-3.3B vs Yandex MT outputs using blinded human Likert scoring

## Open Questions the Paper Calls Out
None

## Limitations
- Data availability and access remains significant limitation due to restricted datasets
- Metric inadequacy for code-switching demonstrated by BLEU vs human evaluation gap
- Generalization concerns from cs-1 through cs-4 degrading performance suggest potential overfitting

## Confidence

| Assessment | Confidence Level | Justification |
|------------|------------------|---------------|
| Core methodology | High | Well-specified approach with reproducible results |
| Mechanism explanations | Medium | Demonstrated effectiveness but lacks detailed linguistic analysis |
| Cross-lingual generalization | Low | Results specific to Kazakh-Russian without validation on other language pairs |

## Next Checks
1. **Replicate augmentation ablation study**: Train transformer-600 from scratch on each augmentation type using specified hyperparameters; evaluate all models on KRCS to verify cs-5 superiority and confirm degradation patterns of other methods.

2. **Domain adaptation quantification**: Train two NLLB-3.3B models with PiSSA—one with RTC domain adaptation data, one without; measure performance delta on KRCS and conduct ablation on RTC filtering thresholds.

3. **Metric validation study**: Sample 200 KRCS sentences and conduct blinded human evaluation comparing NLLB-3.3B outputs versus Yandex MT system outputs using Likert scale; calculate correlation between automatic metrics and human scores to quantify metric inadequacy.