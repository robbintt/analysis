---
ver: rpa2
title: Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report
  Retrieval
arxiv_id: '2501.09134'
source_url: https://arxiv.org/abs/2501.09134
tags:
- retrieval
- learning
- medical
- performance
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks the robustness of four contrastive learning
  models (CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP) for medical image-report retrieval
  tasks. The authors introduce an occlusion retrieval task to evaluate model performance
  under varying levels of image corruption.
---

# Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval

## Quick Facts
- **arXiv ID:** 2501.09134
- **Source URL:** https://arxiv.org/abs/2501.09134
- **Reference count:** 40
- **Primary result:** CXR-CLIP consistently achieves best performance across most occlusion ratios and recall thresholds in medical image-report retrieval benchmarking

## Executive Summary
This paper benchmarks four contrastive learning models (CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP) for medical image-report retrieval tasks using an occlusion-based robustness evaluation. The authors introduce an occlusion retrieval task to evaluate model performance under varying levels of image corruption on the MIMIC-CXR dataset. Results show that while CXR-CLIP consistently outperforms other models across most occlusion ratios and recall thresholds, all models exhibit sensitivity to out-of-distribution data with performance decreasing proportionally to occlusion levels. The findings highlight the need for improved robustness in these models and emphasize the importance of domain-specific training data for medical image-report retrieval tasks.

## Method Summary
The study evaluates pre-trained contrastive learning models on the MIMIC-CXR validation set, filtered to include only studies with "Findings" or "Impression" sections (994 studies, 1,770 X-rays). Four models are benchmarked: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. An occlusion function randomly blocks p% of pixels in input images across 8 occlusion levels (0% to 81%). Models generate embeddings for occluded images and text reports, and Recall@k metrics (k ∈ {5, 10, 20, 30, 50, 100}) are calculated to measure retrieval performance. The evaluation is purely inference-based using pre-trained weights without additional training.

## Key Results
- CXR-CLIP consistently achieves the best performance across most occlusion ratios and recall thresholds
- CXR-RePaiR ranks second in overall performance
- MedCLIP shows potential robustness to occlusion but weaker overall retrieval accuracy
- All models exhibit proportional performance degradation as occlusion levels increase
- General-purpose CLIP performs worst, aligning with random performance expectations

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Semantic Alignment
General-purpose models like CLIP encode natural image features and lay language, which do not map to the specific visual textures and clinical terminology found in chest X-rays. Domain-specific models like CXR-CLIP align these specific modalities through targeted contrastive loss on medical pairs, creating the observed performance gap between general and medical-specific models.

### Mechanism 2: Unpaired Training as Regularization
Training with unpaired data and rule-based labelers may increase robustness to image corruption while reducing absolute retrieval accuracy. Decoupling strict image-text pairs prevents the model from overfitting to specific pixel-text correlations, acting as regularization that allows better generalization when partial information is missing.

### Mechanism 3: Global Feature Reliance
Contrastive retrieval performance degrades proportionally to the loss of informative pixels, indicating models rely on global statistics rather than local pathological features. As occlusion increases and visual information deviates from training distribution, models fail to infer missing parts, leading to linear drops in alignment scores between occluded image embeddings and report embeddings.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE/Triplet Loss)**
  - **Why needed here:** The paper evaluates models based on their ability to pull matching image-report pairs closer in embedding space than non-matching pairs. Understanding the loss function is required to interpret why certain pairs fail.
  - **Quick check question:** Can you explain why increasing the batch size generally improves contrastive learning performance?

- **Concept: Recall@K (R@K)**
  - **Why needed here:** This is the sole metric used for benchmarking. It measures if the correct report appears in the top k retrieved candidates.
  - **Quick check question:** If a model has high Recall@5 but low Recall@100, what does that imply about the embedding space distribution?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The paper introduces an "occlusion retrieval task" specifically to test OOD robustness.
  - **Quick check question:** How does random occlusion differ from adversarial occlusion in terms of evaluating model robustness?

## Architecture Onboarding

- **Component map:** Image Encoder -> Projection -> Cosine Similarity -> Text Encoder -> Projection
- **Critical path:** Load pre-trained weights → Apply occlusion function to input images → Generate embeddings for occluded images and all candidate reports → Compute similarity matrix and sort to find top-k matches
- **Design tradeoffs:** CXR-CLIP vs. MedCLIP represents the trade-off between absolute accuracy (paired data + LLM descriptions) versus data efficiency/robustness (unpaired data). Higher k in Recall@k yields higher scores but is less clinically useful than lower k.
- **Failure signatures:** Proportional drop in Recall as occlusion increases indicates lack of robustness; scores clustering around 1/N (random baseline) suggests broken embedding space or domain mismatch; low performance even at 0% occlusion suggests training strategy failure.
- **First 3 experiments:** 1) Run CLIP vs. CXR-CLIP on MIMIC-CXR validation set (0% occlusion) to reproduce domain gap shown in Table 1; 2) Plot Recall@10 vs. Occlusion Ratio for CXR-CLIP to verify proportional decrease claim; 3) Replace random occlusion with center-crop occlusion to test if models rely more on lung fields vs. edges.

## Open Questions the Paper Calls Out

### Open Question 1
What specific data augmentation strategies or architectural modifications can effectively mitigate the proportional performance drop in contrastive learning models under high occlusion levels? The conclusion explicitly states that investigating the impact of different types of data augmentation and architectural modifications on model performance is crucial to address the identified limitations.

### Open Question 2
Can the trade-off between retrieval performance and robustness—observed in MedCLIP due to unpaired training—be optimized to achieve both high accuracy and resistance to out-of-distribution data? It remains unclear if the observed robustness is an inherent byproduct of lower-fidelity unpaired training or a tunable characteristic.

### Open Question 3
Does the sensitivity to occlusion observed in chest X-ray models generalize to other medical imaging modalities or corruption types? The study is limited to random pixel occlusion on MIMIC-CXR, but the introduction references adversarial attacks and general vulnerability, raising questions about performance under structured occlusion, motion blur, or different imaging physics.

## Limitations
- The paper does not specify the exact occlusion masking strategy (contiguous vs. random scattered pixels, fill value)
- Limited ablation studies on model architecture differences beyond training data strategy
- No analysis of which specific image regions are most critical for retrieval performance

## Confidence
- **High Confidence:** Domain-specific models outperform general-purpose models for medical retrieval
- **Medium Confidence:** CXR-CLIP's superior performance is primarily due to LLM-enhanced report descriptions
- **Medium Confidence:** MedCLIP's relative robustness stems from unpaired training strategy

## Next Checks
1. Replicate the occlusion masking procedure with different strategies (contiguous vs. scattered) to verify the proportional degradation claim is robust to implementation details
2. Conduct controlled experiments isolating the impact of LLM-enhanced descriptions versus domain-specific image features in CXR-CLIP
3. Test center-crop occlusion versus random occlusion to determine if models rely more on specific anatomical regions or global image statistics