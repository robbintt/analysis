---
ver: rpa2
title: Exploring NLP Benchmarks in an Extremely Low-Resource Setting
arxiv_id: '2509.03962'
source_url: https://arxiv.org/abs/2509.03962
tags:
- ladin
- italian
- translation
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first publicly available datasets for sentiment
  analysis and multiple-choice question answering in Ladin, an endangered Romance
  language. The authors create synthetic parallel Ladin-Italian data by translating
  monolingual Italian data and applying rigorous filtering and back-translation procedures
  to ensure quality.
---

# Exploring NLP Benchmarks in an Extremely Low-Resource Setting

## Quick Facts
- **arXiv ID**: 2509.03962
- **Source URL**: https://arxiv.org/abs/2509.03962
- **Reference count**: 22
- **Primary result**: First public datasets for sentiment analysis and MCQA in Ladin, achieving BLEU scores up to 18.30 using synthetic data augmentation

## Executive Summary
This paper addresses the challenge of developing NLP benchmarks for Ladin, an endangered Romance language with extremely limited resources. The authors create synthetic parallel Ladin-Italian data by translating monolingual Italian data and applying rigorous filtering and back-translation procedures to ensure quality. They demonstrate that incorporating these synthetic datasets into machine translation training substantially improves performance over existing Italian-Ladin translation baselines. Using an NLLB-based model fine-tuned on both authentic and synthetic data, they achieve BLEU scores up to 18.30 for Italian-to-Ladin translation, outperforming previous benchmarks. The synthetic data also supports sentiment analysis and multiple-choice question answering tasks, with few-shot learning approaches achieving accuracy scores up to 93.99% on sentiment analysis and 44.20% on question answering in Ladin.

## Method Summary
The authors create synthetic parallel data for Ladin by translating monolingual Italian data (DIta) to Ladin (DLad) using the best-performing MT model, then applying two-stage filtering: (1) LaBSE cosine similarity threshold (≥0.68) between source and translation to ensure semantic alignment, and (2) back-translation with BLEU/METEOR thresholds to verify round-trip fidelity. This produces synthetic paired data (SDIta_Lad) that augments the authentic corpus. They fine-tune NLLB-200-1.3B on both authentic and synthetic data, achieving BLEU scores up to 18.30 for Italian-to-Ladin translation. The approach extends to sentiment analysis and multiple-choice question answering tasks, with few-shot learning achieving accuracy scores up to 93.99% on sentiment analysis and 44.20% on question answering in Ladin.

## Key Results
- NLLB-based fine-tuning achieves BLEU 18.30 (Italian→Ladin), outperforming LLM baselines (BLEU 6.95)
- Synthetic data incorporation improves chrF++ from 44.60 to 44.62
- Few-shot learning achieves sentiment analysis accuracy up to 93.99% in Ladin
- Multiple-choice question answering reaches 44.20% accuracy, though all models perform below 45% even on Italian data

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Data Quality Filtering
- **Claim**: Synthetic parallel data created via translation-filtering pipelines can improve MT performance for extremely low-resource languages when authentic data is scarce.
- **Mechanism**: The method translates monolingual Italian data to Ladin, then applies two-stage filtering using LaBSE cosine similarity (≥0.68) and back-translation with BLEU/METEOR thresholds to ensure semantic alignment and round-trip fidelity.
- **Core assumption**: Filtering thresholds effectively remove low-quality translations while preserving semantically faithful examples, and semantic similarity scores correlate with translation utility for downstream training.
- **Evidence anchors**: Synthetic SA dataset reduced from 30,477 to 12,511 entries after filtering; MCQA reduced from 4,651 to 764. Final synthetic data shows LaBSE cosine similarity of 86.61 with manually-translated gold data.

### Mechanism 2: Multilingual Transfer Learning
- **Claim**: Fine-tuned encoder-decoder models (specifically NLLB) outperform LLM-based few-shot and fine-tuning approaches for translation into extremely low-resource languages.
- **Mechanism**: NLLB-200-1.3B, trained on 200+ languages including related Raeto-Romance languages (Friulian), generalizes to Ladin despite being an "unseen" language through cross-lingual representations that transfer to linguistically proximate languages.
- **Core assumption**: Linguistic similarity between Ladin and Friulian enables transfer despite no explicit Ladin training data, and tokenizer segmentation quality for Ladin is adequate for meaningful representation learning.
- **Evidence anchors**: FT-NLLB achieves BLEU 17.76 (Ita→Lad) vs. FT-LLaMA-70B at 6.95. Tokenizer analysis shows average tokens-per-word ratio of 1.50 for Ladin, compared to 1.39 for Italian.

### Mechanism 3: Domain-Aware Data Augmentation
- **Claim**: Synthetic data augmentation provides consistent MT improvements across evaluation metrics and test subsets, even when authentic training data exists.
- **Mechanism**: Combining synthetic data with authentic data increases training diversity and exposes the model to broader vocabulary and syntactic patterns, with synthetic data covering sentiment analysis reviews and MCQA topics that complement the authentic data's focus on basic/concise examples.
- **Core assumption**: Domain expansion through synthetic data does not introduce harmful distribution shift, and quality of synthetic data is sufficient that benefits outweigh noise.
- **Evidence anchors**: Incorporating synthetic data consistently improves performance across all evaluation metrics. Authentic data averages 5.02 words/sentence while synthetic SA averages 70 words/entry, demonstrating substantial domain difference.

## Foundational Learning

- **Concept: Back-translation for Quality Filtering**
  - Why needed here: Used as quality verification step—retranslating Ladin→Italian and comparing against original Italian to identify semantic drift, not for data augmentation
  - Quick check question: Can you explain why back-translation quality (BLEU/METEOR scores) serves as a proxy for forward-translation reliability?

- **Concept: LaBSE (Language-Agnostic BERT Sentence Embeddings)**
  - Why needed here: Used for cross-lingual semantic similarity scoring between Italian source and Ladin translation when no gold reference exists; critical for Filtering I stage
  - Quick check question: Why might a multilingual sentence embedding model be more reliable for quality filtering than surface-level metrics like BLEU?

- **Concept: Language Tags for Unseen Languages in Multilingual Models**
  - Why needed here: The paper introduces a "Ladin-specific language tag in the tokenizer" for NLLB/mBART, a standard technique for extending multilingual models to new languages
  - Quick check question: What role does a language tag play in an encoder-decoder model, and where does it appear in the input/output sequence?

## Architecture Onboarding

- **Component map**: ADIta_Lad (authentic parallel, 18,139 pairs) -> FT-NLLB-1.3B (baseline MT) -> DIta_SA/MCQA (Italian monolingual) -> Translation Engine -> Filtering I (LaBSE ≥0.68) -> Back-translation -> Filtering II (BLEU/METEOR thresholds) -> SDLad_SA (12,511 entries) and SDLad_MCQA (764 entries) -> Retraining with combined data

- **Critical path**: 1. Establish baseline MT using ADIta_Lad → select FT-NLLB as best model (BLEU 17.76) 2. Translate DIta_SA and DIta_MCQA to Ladin using FT-NLLB 3. Apply Filtering I (LaBSE threshold 0.68) 4. Back-translate Ladin→Italian 5. Apply Filtering II (SA: BLEU≥33.63, METEOR≥0.58; MCQA: BLEU≥36.58, METEOR≥0.62) 6. Combine synthetic + authentic data 7. Retrain MT and evaluate on test sets T={t1, t2, t3}

- **Design tradeoffs**:
  - Aggressive filtering vs. dataset size: Final MCQA dataset is only 764 entries—may be insufficient for robust training
  - GPT-4 grammar correction: Applied to Italian SA data before translation; adds cost and potential semantic drift but improves source quality
  - Model selection: NLLB-1.3B chosen over larger LLMs (70B) despite availability; suggests parameter efficiency matters more than scale for ELRLs
  - Single-dialect focus: Val Badia variant only; may not generalize to other Ladin dialects with different orthographic conventions

- **Failure signatures**:
  - MCQA low performance: All models achieve <45% accuracy even on Italian MCQA—task is inherently harder than SA
  - Dialect mismatch: If synthetic data contains Val Badia conventions but evaluation uses mixed dialectal features, performance degrades
  - Length distribution shift: Authentic data avg 5 words/sentence; synthetic SA avg 70 words/entry—models may struggle with longer sequences
  - Tokenization overhead: 1.50 tokens/word for Ladin vs. 1.39 for Italian suggests vocabulary coverage gaps

- **First 3 experiments**:
  1. Replicate baseline comparison: Train FT-NLLB and FT-LLaMA-8B on ADIta_Lad only; verify BLEU gap (~17.76 vs. ~10.71) holds
  2. Ablate filtering thresholds: Test LaBSE thresholds at 0.60, 0.68 (paper), and 0.75 to quantify sensitivity; measure final dataset size and downstream MT BLEU
  3. Cross-dialect validation: If possible, obtain even small samples of other Ladin variants (Gherdëina, Fascia); evaluate zero-shot transfer to understand dialect generalization limits

## Open Questions the Paper Calls Out

- **Knowledge Distillation**: Can knowledge distillation techniques effectively transfer capabilities from high-resource language models to extremely low-resource languages like Ladin? The paper notes this as future work, suggesting it as an alternative to current synthetic data augmentation approaches.

- **Monolingual Data Exploitation**: How can the underutilized monolingual Ladin data from *La Usc di Ladins* (archived since 2012) be better exploited to improve NLP model performance? The paper identifies this substantial existing Ladin monolingual corpus as an untapped resource.

- **Generalization to Complex Texts**: To what extent do models trained on the short, simple sentences in ADIta_Lad generalize to longer, more complex Ladin texts? The paper notes that training data averages only 5.02 words per Ladin sentence, raising questions about performance on longer texts.

- **MCQA Task Difficulty**: Why does MCQA performance remain low across all evaluated models, even on Italian data? The paper suggests limited dataset size and broad topical coverage as potential causes but does not systematically investigate these factors.

## Limitations

- Dataset size constraints: The synthetic MCQA dataset contains only 764 entries after filtering, which may be insufficient for robust training
- Domain and dialect mismatch: Substantial distribution shift between synthetic SA data (avg 70 words) and authentic parallel data (avg 5.02 words/sentence)
- Limited evaluation scope: Focuses primarily on Italian→Ladin translation with limited analysis of Ladin→Italian performance and no human evaluation

## Confidence

- **High Confidence**: NLLB-based models outperform LLM-based approaches; synthetic data incorporation consistently improves MT performance; synthetic data creation pipeline produces high-quality data
- **Medium Confidence**: Filtering thresholds effectively balance quality and quantity; NLLB advantage due to linguistic similarity with Friulian; synthetic data quality sufficient for downstream training
- **Low Confidence**: MCQA performance results reliability given small dataset size; approach generalizes to other ELRLs without related languages in pretraining; synthetic data does not introduce harmful distribution shift

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary LaBSE (0.60-0.75) and back-translation (BLEU/METEOR) thresholds to quantify quality-quantity tradeoff; measure final dataset size, downstream MT BLEU scores, and synthetic data semantic similarity

2. **Cross-Dialect Transfer Evaluation**: Obtain small samples of other Ladin dialects (Gherdëina, Fascia) and evaluate zero-shot transfer from Val Badia-trained model to validate generalization beyond single dialect

3. **Human Evaluation of Synthetic Data Quality**: Conduct human assessment of stratified sample of synthetic translations to validate automatic metrics (LaBSE, BLEU, METEOR) correlate with human judgments of translation quality and task suitability