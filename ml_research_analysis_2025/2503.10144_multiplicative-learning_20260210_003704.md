---
ver: rpa2
title: Multiplicative Learning
arxiv_id: '2503.10144'
source_url: https://arxiv.org/abs/2503.10144
tags:
- learning
- weight
- error
- update
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Expectation Reflection (ER), a novel neural
  network training algorithm that updates weights multiplicatively based on the ratio
  of observed to predicted outputs. Unlike backpropagation (BP), ER does not require
  predefined loss functions or learning rate hyperparameters.
---

# Multiplicative Learning

## Quick Facts
- arXiv ID: 2503.10144
- Source URL: https://arxiv.org/abs/2503.10144
- Reference count: 6
- One-line primary result: ER achieves near-optimal performance in a single iteration with a 2.88% error rate on MNIST and 56.65% on CIFAR-10.

## Executive Summary
This paper presents Expectation Reflection (ER), a novel neural network training algorithm that updates weights multiplicatively based on the ratio of observed to predicted outputs. Unlike backpropagation, ER does not require predefined loss functions or learning rate hyperparameters. The authors analytically demonstrate that ER is closely related to backpropagation with modifications that enable faster learning, and show its connection to target propagation. They extend ER to multilayer networks and evaluate its performance on image classification tasks (MNIST and CIFAR-10), achieving results comparable to backpropagation but with significantly fewer updates.

## Method Summary
ER is a neural network training algorithm that uses multiplicative weight updates based on the ratio of observed to predicted outputs, eliminating the need for predefined loss functions or learning rate hyperparameters. The method updates weights using pseudo-inverses instead of transposes, treating the weight update as a linear regression problem that allows for larger, more optimal step sizes. The authors extend ER to multilayer networks and demonstrate its connection to target propagation through the use of inverse activation function approximations.

## Key Results
- ER achieves optimal weight updates in a single iteration
- 2.88% error rate on MNIST
- 56.65% error rate on CIFAR-10
- Performance comparable to backpropagation with significantly fewer updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiplicative updates based on the ratio of observed to predicted outputs accelerate convergence compared to additive gradient descent.
- Mechanism: ER uses the update rule $H_{new} = \frac{\hat{Y}}{Y} \odot H$ (element-wise). This ratio acts as a direct scaling factor. If $\hat{Y} > Y$, $H$ is amplified. If $\hat{Y}$ has the opposite sign to $Y$, the sign of $H$ is corrected.
- Core assumption: The multiplicative factor $\frac{\hat{Y}}{Y}$ provides a more direct and potent error signal correction than the additive gradient derived from a loss function.
- Evidence anchors:
  - [abstract] "updates weights multiplicatively based on the ratio of observed to predicted outputs."
  - [Page 4] "ER employs a multiplicative parameter update, enabling faster and more efficient convergence."
- Break condition: The mechanism fails if $Y$ becomes zero, causing division by zero. The paper states this is avoided due to the properties of the tanh function.

### Mechanism 2
- Claim: Using the pseudo-inverse $X^+$ instead of the transpose $X^T$ for weight updates allows for larger, more optimal step sizes, effectively treating the weight update as a linear regression problem.
- Mechanism: In gradient descent, $\Delta W = X^T \Delta H$. In ER, $\Delta W = X^+ \Delta H = (X^T X)^{-1} X^T \Delta H$. The authors frame this as the difference between two extremes of ridge regression.
- Core assumption: The "optimal" update direction can be found by inverting the input correlation structure ($X^T X$), assuming this inversion is stable.
- Evidence anchors:
  - [Page 12] "It is noteworthy that as α increases, the resulting ∆W approaches the GD solution... Conversely, when α = 0, we obtain ∆W = (X^T X)−1X^T ∆H = X +∆H, which corresponds to the linear regression solution used in the ER algorithm."
- Break condition: Instability arises if $X^T X$ is singular or ill-conditioned, especially with small mini-batches.

### Mechanism 3
- Claim: ER approximates the inverse activation function $\sigma^{-1}(Y)$'s gradient rather than the forward activation function $\sigma(H)$'s gradient, linking it to Target Propagation.
- Mechanism: The pre-activation update uses $\Delta H = \frac{H}{Y} \odot \Delta Y$. The authors show $\frac{H}{Y} \approx \frac{dH}{dY} = (\sigma^{-1}(Y))'$. This is functionally equivalent to propagating targets backward through an approximated inverse function.
- Core assumption: The ratio $H/Y$ is a sufficient and stable estimator for the inverse gradient, maintaining error signal strength even when the forward gradient $\sigma'(H)$ would vanish.
- Evidence anchors:
  - [Page 11] "employing H/Y instead of σ′(H) in the pre-activation update offers advantages of enhanced error signal propagation."
  - [Page 14] "Thus, ER leverages an approximation of the inverse function to update the weight, reflecting the core idea of TP."
- Break condition: The mechanism relies on the specific properties of the activation function (e.g., tanh) and its inverse.

## Foundational Learning

- Concept: **Moore-Penrose Pseudo-inverse ($X^+$)**
  - Why needed here: It is the core mathematical operation replacing the transpose in backpropagation for computing weight updates, enabling the "one-shot" regression-like update.
  - Quick check question: Given a matrix $X$ (inputs), what is the pseudo-inverse $X^+$ and how does it differ from a simple transpose $X^T$?

- Concept: **Target Propagation (TP)**
  - Why needed here: The paper interprets its mechanism as a form of TP, using an inverse function approximation to propagate targets backward instead of gradients, which is crucial for understanding how ER extends to multilayer networks.
  - Quick check question: In Target Propagation, how are targets $T_{l-1}$ generated from targets $T_l$, and what problem does this solve compared to backpropagation?

- Concept: **Internal Covariate Shift**
  - Why needed here: The paper explicitly addresses this as a major challenge for ER because of its large weight updates, necessitating the specific "Enhanced Algorithm" which updates weights sequentially while re-computing activations.
  - Quick check question: Why does a large weight update in layer $l$ pose a problem for the inputs of layer $l+1$, and how does batch normalization or sequential updating address it?

## Architecture Onboarding

- Component map: Input -> Expectation Reflection Layer (computes $\Delta H_l = \frac{H_l}{Z_l} \odot \Delta Z_l$) -> Pseudo-Inverse Propagator (computes $\Delta Z_{l-1} = \Delta H_l W_l^+$) -> Regression Updater (computes $\Delta W_l = Z_{l-1}^+ \Delta H_l$)

- Critical path: The critical path is the backward pass. Unlike BP, which is purely vector-matrix multiplication, this path involves computing matrix pseudo-inverses ($W_l^+, Z_{l-1}^+$), which is computationally more expensive and complex.

- Design tradeoffs:
  - **Speed vs. Stability:** ER achieves near-optimal performance in a single iteration (speed) but is prone to instability, especially with mini-batches, requiring careful regularization (tradeoff).
  - **No Hyperparameters (Full-batch) vs. Hyperparameters (Mini-batch):** The ideal ER has no learning rate, but mini-batch training forces the introduction of a learning rate $\eta$ and regularization $\alpha$ to remain stable.
  - **Pseudo-inverse vs. Transpose:** Using the pseudo-inverse allows for larger, more direct updates but requires inverting covariance matrices, which has a higher computational cost ($O(N^3)$) than the transpose ($O(1)$) and can be numerically unstable.

- Failure signatures:
  - **Singular Matrix Error:** Crash during pseudo-inverse computation, especially with small or correlated mini-batches.
  - **Divergence/Loss Explosion:** Unstable training if the learning rate/interpolation parameter $\eta$ is too high or if regularization $\alpha$ is insufficient.
  - **Poor Performance with Original Algorithm:** Using the naive multilayer extension (Alg 1) will fail due to internal covariate shift.

- First 3 experiments:
  1.  **Reproduce Single-Layer ER:** Implement the core ER update ($H_{new} = \frac{\hat{Y}}{Y} H$, $W_{new} = X^+ H_{new}$) on a simple logistic regression task (e.g., subset of MNIST binary classification) to verify the "one-shot" learning claim.
  2.  **Test Regularization Strategies:** Compare full-batch performance on MNIST using the "Enhanced Multilayer ER" (Alg 2) against a version with naive matrix inversion. Introduce a ridge term ($\alpha$) and measure impact on stability.
  3.  **Mini-batch Ablation:** Apply the modified mini-batch ER (with $\eta$ and $\alpha$) to CIFAR-10. Vary the interpolation parameter $\eta$ to find the balance between retaining the "one-shot" property and ensuring stable convergence across batches.

## Open Questions the Paper Calls Out
None

## Limitations
- The core multiplicative update mechanism's superiority is demonstrated primarily on small-scale datasets (MNIST, CIFAR-10).
- The algorithm is prone to instability, particularly in mini-batch settings, requiring the introduction of hyperparameters that were absent in the ideal full-batch version.
- The computational cost of the pseudo-inverse, especially for large networks, is a significant practical limitation not fully addressed.

## Confidence
- **High Confidence:** The mathematical derivation of the ER update rules from the constraint optimization problem and the demonstration of the relationship between the pseudo-inverse update and ridge regression are well-founded and clearly explained.
- **Medium Confidence:** The performance claims on MNIST and CIFAR-10 are supported by the results, but the comparison is primarily against backpropagation and does not include a broader range of modern optimizers on these datasets.
- **Low Confidence:** The claim that ER "achieves optimal weight updates in a single iteration" is an idealization for the full-batch case. The practical implementation, especially with mini-batches and the "Enhanced Algorithm," requires multiple iterations and careful tuning.

## Next Checks
1. **Robustness to Activation Functions:** Test the ER algorithm with activation functions other than tanh (e.g., ReLU, sigmoid) to verify if the inverse gradient approximation remains stable and effective.
2. **Scalability Assessment:** Evaluate the computational cost and memory requirements of the pseudo-inverse operation for larger, deeper networks (e.g., VGG, ResNet architectures) on standard benchmarks to understand the practical limitations.
3. **Hyperparameter Sensitivity Analysis:** Conduct a systematic study of the learning rate (η) and regularization (α) parameters in the mini-batch ER algorithm to characterize their impact on convergence speed, stability, and final performance across different dataset sizes and network depths.