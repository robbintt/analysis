---
ver: rpa2
title: Unraveling the cognitive patterns of Large Language Models through module communities
arxiv_id: '2508.18192'
source_url: https://arxiv.org/abs/2508.18192
tags:
- attn
- modules
- skills
- cognitive
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a network-based framework to analyze cognitive
  skill distribution in Large Language Models (LLMs) by integrating cognitive science,
  neuroscience, and machine learning principles. The authors construct multipartite
  networks linking cognitive skills, datasets, and LLM modules, and employ community
  detection techniques to uncover skill-associated module clusters.
---

# Unraveling the cognitive patterns of Large Language Models through module communities

## Quick Facts
- arXiv ID: 2508.18192
- Source URL: https://arxiv.org/abs/2508.18192
- Authors: Kushal Raj Bhandari; Pin-Yu Chen; Jianxi Gao
- Reference count: 40
- Key outcome: Network-based framework reveals LLMs exhibit distributed yet interconnected cognitive organization with stronger cross-regional interactions than biological brains, and community-based fine-tuning shows high weight changes but fails to outperform random module selection.

## Executive Summary
This paper introduces a network-based framework to analyze cognitive skill distribution in Large Language Models (LLMs) by integrating cognitive science, neuroscience, and machine learning principles. The authors construct multipartite networks linking cognitive skills, datasets, and LLM modules, and employ community detection techniques to uncover skill-associated module clusters. Results reveal that LLMs exhibit distributed yet interconnected cognitive organization similar to avian and small mammalian brains, but with stronger cross-regional interactions and neural plasticity compared to biological systems. Network analysis shows distinct community structures with unique skill distributions, but low alignment with predefined cognitive functions. Community-based fine-tuning induces significant weight changes but does not outperform random module selection in accuracy, highlighting the distributed nature of knowledge encoding in LLMs.

## Method Summary
The framework constructs bipartite networks linking 53 cognitive skills to 174 MCQ datasets (via GPT-3.5 labeling) and datasets to LLM modules (via gradient-based pruning importance). These are projected into a module connectivity network where community detection reveals skill-associated clusters. The Louvain algorithm identifies communities, which are then tested through fine-tuning experiments comparing community-specific vs random module selection. Network analysis reveals distinct community structures with unique skill distributions, but low alignment with predefined cognitive functions. Community-based fine-tuning induces significant weight changes but fails to outperform random module selection in accuracy, highlighting the distributed nature of knowledge encoding in LLMs.

## Key Results
- LLMs exhibit distributed yet interconnected cognitive organization similar to avian and small mammalian brains
- Detected communities have distinct skill distributions but low alignment with predefined cognitive functions
- Community-based fine-tuning shows high weight changes but no accuracy improvement over random module selection
- Channel-based pruning leads to higher cross-community integration but lower functional distinctiveness compared to block-based pruning

## Why This Works (Mechanism)

### Mechanism 1: Skill-Module Association via Bipartite Network Projection
The framework constructs two bipartite networks: $B_{SD}$ (Skills-to-Datasets, labeled by GPT-3.5) and $B_{DM}$ (Datasets-to-Modules, derived from gradient-based pruning importance). It projects these into a single Skills-Modules network ($B_{SM}$) and subsequently a Module-Module connectivity network ($P_M$). The Louvain algorithm then detects clusters (communities) of tightly coupled modules. The core assumption is that module importance (preservation of accuracy after pruning) is a valid proxy for functional role in cognitive skills.

### Mechanism 2: Weak-Localization (Distributed Encoding)
LLMs organize cognitive skills via a "weak-localization" architecture where distinct communities exist but rely heavily on cross-regional interaction. The paper compares fine-tuning only community modules against random modules. While targeted modules show high plasticity (large weight changes), they fail to outperform random subsets in accuracy, implying knowledge is distributed rather than encapsulated within communities.

### Mechanism 3: Misalignment of Structural and Semantic Taxonomies
The structural communities that emerge from network analysis are statistically distinct but semantically misaligned with predefined human cognitive categories. The paper uses Adjusted Rand Index (ARI) and Chi-squared tests to compare detected communities against ground-truth cognitive labels, finding that while communities have unique skill distributions, alignment with specific cognitive function labels is near random.

## Foundational Learning

- **Concept: Bipartite Network Projection**
  - **Why needed here:** The entire methodology rests on collapsing a 3-part system (Skills ↔ Datasets ↔ Modules) into a 2-part projection (Modules ↔ Modules) to analyze connectivity.
  - **Quick check question:** If you have edges connecting Users to Posts and Posts to Hashtags, how would you calculate the similarity between two Users based on shared Hashtags?

- **Concept: Gradient-based Pruning (Taylor Expansion)**
  - **Why needed here:** To quantify the strength of the "edge" between a Dataset and a Module, the paper uses Taylor expansion to estimate the importance of weights without retraining.
  - **Quick check question:** Why might a first-order Taylor approximation ($|\text{gradient} \times \text{weight}|$) be preferred over calculating the exact loss change for every parameter in a 7B model?

- **Concept: Adjusted Rand Index (ARI)**
  - **Why needed here:** ARI is the key metric used to prove that the "communities" found mathematically do not map neatly onto the "cognitive functions" defined by psychologists.
  - **Quick check question:** If an ARI score is 0, does it mean the clustering algorithm failed, or simply that it found a valid structure that is orthogonal to the provided labels?

## Architecture Onboarding

- **Component map:** 174 MCQ datasets → GPT-3.5 skill labeling → LLM-Pruner importance scores → Bipartite network projection → Louvain community detection → Ward's hierarchical clustering → Fine-tuning validation

- **Critical path:** The validity of the Skills-to-Dataset mapping ($B_{SD}$). The paper automates this using GPT-3.5 (detailed in Supplementary Figure 7). If this mapping is low-quality, the subsequent projection creates a network of hallucinated relationships.

- **Design tradeoffs:**
  - Block vs. Channel Pruning: Block-based pruning creates clearer "bimodal" community structures, while Channel-based pruning leads to higher cross-community integration but lower functional distinctiveness
  - Network Density: The authors use spectral sparsification to handle density of projected network, which preserves spectral properties but adds stochasticity to community detection

- **Failure signatures:**
  - "Sensitivity without Specialization": High L2 weight changes in specific modules during fine-tuning, but accuracy identical to random selection
  - Near-Zero ARI: Indicates model's internal organization is mathematically valid but functionally alien compared to human cognitive definitions

- **First 3 experiments:**
  1. Validate the Labeling: Reproduce the $B_{SD}$ matrix manually for a subset of 5 datasets to verify if GPT-3.5 skill assignments match human expert intuition
  2. Ablation on Pruning Strategy: Run community detection using only Block-based pruning vs only Channel-based pruning on a smaller model to visualize difference in community "crispness"
  3. Sanity Check Fine-tuning: Perform "Random vs. Community" fine-tuning experiment. If community-based fine-tuning does outperform random, it suggests the "weak-localization" hypothesis might not hold for that specific architecture/dataset pair

## Open Questions the Paper Calls Out

### Open Question 1
Does the weak-localization architecture and distributed cognitive organization observed in 7B parameter models persist or fundamentally shift when analyzing significantly larger LLM architectures or different architectural paradigms (e.g., Mixture of Experts)? The study is restricted to 7 billion parameter variants; it is unknown if these network properties are universal or scale-dependent.

### Open Question 2
Can adaptive optimization strategies that explicitly leverage network-wide dependencies and inter-layer connectivity succeed where rigid community-based fine-tuning failed? The authors conclude that future work should explore network-wide dependencies, inter-layer connectivity, and adaptive optimization strategies because community-based fine-tuning did not outperform random selection.

### Open Question 3
To what extent does the granularity and accuracy of the cognitive skill taxonomy influence the alignment between detected module communities and functional cognitive categories? The discussion notes that the abstract cognitive skills defined here could be further refined to capture a more nuanced spectrum of human cognition, potentially deepening skill-module associations.

## Limitations

- Core findings hinge on quality of automated skill labeling via GPT-3.5 and validity of pruning-based module importance as functional proxy
- Low alignment between communities and cognitive functions could reflect methodological artifacts rather than fundamental architectural differences
- Community detection results may be sensitive to pruning strategy choices and network projection parameters not fully specified

## Confidence

- **High confidence:** Framework for constructing multipartite networks and applying community detection is methodologically sound and reproducible
- **Medium confidence:** Weak-localization hypothesis (distributed knowledge encoding) is well-supported by fine-tuning experiments, though optimization artifacts cannot be fully ruled out
- **Low confidence:** Interpretation of low cognitive function alignment as evidence of fundamentally different LLM organization, given uncertainties in ground-truth skill mapping quality

## Next Checks

1. Manual validation of skill labeling: Have human experts label skills for 10-15 randomly selected questions from diverse datasets and compare against GPT-3.5 outputs to quantify labeling noise
2. Ablation on fine-tuning protocols: Repeat community vs. random fine-tuning experiment with carefully tuned learning rates for smaller parameter spaces to distinguish structural properties from optimization artifacts
3. Alternative community detection validation: Apply a second community detection algorithm (e.g., Infomap) to the module network and verify whether the same weak-localization patterns and cognitive misalignment emerge independently