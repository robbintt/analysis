---
ver: rpa2
title: On-Policy Optimization with Group Equivalent Preference for Multi-Programming
  Language Understanding
arxiv_id: '2505.12723'
source_url: https://arxiv.org/abs/2505.12723
tags:
- code
- llms
- preference
- languages
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OORL, a reinforcement learning framework
  for enhancing multilingual code generation in large language models. It integrates
  on-policy RL with rule-based rewards for code translation accuracy and Group Equivalent
  Preference Optimization (GEPO), which uses groups of intermediate representations
  (IRs) to capture functional equivalence.
---

# On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding

## Quick Facts
- **arXiv ID**: 2505.12723
- **Source URL**: https://arxiv.org/abs/2505.12723
- **Reference count**: 30
- **Primary result**: OORL framework improves multilingual code generation via on-policy RL with rule-based rewards and GEPO using IR groups, achieving significant gains on MultiPL-E and CrossPLEval benchmarks including low-resource languages.

## Executive Summary
This paper introduces OORL, a reinforcement learning framework for enhancing multilingual code generation in large language models. It integrates on-policy RL with rule-based rewards for code translation accuracy and Group Equivalent Preference Optimization (GEPO), which uses groups of intermediate representations (IRs) to capture functional equivalence. By training models to recognize equivalent IRs, GEPO guides nuanced understanding of code functionality across languages. Experiments show that OORL significantly improves performance on code generation benchmarks like MultiPL-E and a new multilingual translation benchmark, CrossPLEval, including on low-resource languages. The framework addresses the performance gap in non-popular programming languages by leveraging code translation tasks and preference optimization with IRs.

## Method Summary
OORL combines on-policy reinforcement learning with Group Equivalent Preference Optimization (GEPO) to improve multilingual code generation. The on-policy RL component uses REINFORCE++ with binary rewards derived from unit test success (1 if code compiles and passes tests, 0 otherwise) for code translation tasks. GEPO processes groups of functionally equivalent IRs (winners) and non-equivalent IRs (losers) to teach semantic equivalence across implementations. The framework trains on translation data from high-resource languages (Python, C++) to transfer proficiency to lower-resource languages, with Qwen3-8B as the base model. Training uses a combined objective with on-policy RL loss and GEPO loss, leveraging DeepSpeed-Zero Stage 2 for distributed training.

## Key Results
- OORL achieves significant improvements on MultiPL-E benchmark compared to baseline models
- CrossPLEval benchmark demonstrates strong performance on multilingual code translation tasks
- Model generalizes to unseen languages (Scala, Go, TypeScript, C#, Haskell) despite not being in training data
- Low-resource languages show notable relative improvements compared to high-resource counterparts

## Why This Works (Mechanism)

### Mechanism 1: Binary Rule-Based Reward Signal for Code Translation
- Claim: On-policy RL with unit-test-derived rewards improves multi-language code generation by enforcing functional correctness during translation training.
- Mechanism: The model samples translation trajectories, receives binary reward (1 if code compiles and passes unit tests, 0 otherwise), and optimizes via REINFORCE++ with a clipped objective. This creates a sparse but unambiguous correctness signal that directly shapes the policy toward functionally valid translations.
- Core assumption: Unit tests provide sufficient coverage to distinguish correct from incorrect translations, and the binary signal is informative enough despite lacking process-level granularity.
- Evidence anchors:
  - [abstract]: "on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests"
  - [section 3.1]: "R_rule(q, o) = 1 if o represents a successful code translation from q, 0 otherwise"
  - [corpus]: Weak direct evidence; neighbor papers focus on multilingual code generation but not specifically on rule-based RL rewards for translation.
- Break condition: If unit tests have poor coverage or fail to capture semantic equivalence, the reward signal becomes noisy and optimization degrades.

### Mechanism 2: Group Equivalent Preference Optimization (GEPO) with IRs
- Claim: Preference optimization over groups of functionally equivalent IRs teaches models to recognize semantic equivalence across diverse code implementations.
- Mechanism: GEPO constructs winner groups (Y_w: n equivalent IRs from different LLVM optimization levels) and loser groups (Y_l: m non-equivalent IRs augmented from winners). The loss combines: (1) a preference term comparing average winner vs. loser rewards, and (2) a variance constraint enforcing similar rewards within the winner group. This extends DPO from pairwise to group-wise comparisons while explicitly modeling equivalence.
- Core assumption: IRs expose functional semantics more accessibly than high-level code, and different optimization levels produce semantically equivalent but syntactically distinct representations the model can learn from.
- Evidence anchors:
  - [abstract]: "GEPO trains the LLM using intermediate representations (IRs) groups... while also utilizing signals about the mutual equivalence between IRs within the group"
  - [section 3.2]: "GEPO compares the average reward of Y_w to the average reward of Y_l... it explicitly enforces equivalence within Y_w by constraining the variance of their rewards"
  - [corpus]: Weak evidence; neighbor "SPPD" uses process preference learning but not IR-based group equivalence.
- Break condition: If IRs are too complex for the model to distinguish equivalence, or if augmented non-equivalent IRs are not meaningfully different, the preference signal weakens.

### Mechanism 3: Cross-Language Transfer via Translation Training
- Claim: Training on code translation tasks transfers proficiency from high-resource languages (Python, C++) to lower-resource languages.
- Mechanism: Translation requires understanding source semantics and mapping to target syntax. By training on Python/C++ → other languages, the model learns inter-language correspondences and generalizes to unseen languages not in training data (Scala, Go, TypeScript, C#, Haskell showed gains despite being excluded from training).
- Core assumption: Functional understanding is partially language-agnostic; skills learned through translation transfer to generation tasks in the target language.
- Evidence anchors:
  - [abstract]: "facilitating the transfer of coding proficiency across diverse programming languages"
  - [section 4.3]: "Scala, Go, TypeScript, C#, and Haskell... the latter five languages are not included in the training dataset... Qwen3-8B-OORL exhibits significant relative improvements"
  - [corpus]: Related work "MultiPL-MoE" addresses multi-programming-lingual extension but via MoE architecture, not translation training.
- Break condition: If source and target languages have fundamentally different paradigms (e.g., procedural → functional), transfer may be limited.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: GEPO extends DPO's pairwise preference framework to group-wise comparisons. Understanding DPO's implicit reward derivation is prerequisite to understanding how GEPO adds variance constraints.
  - Quick check question: Can you explain how DPO converts RLHF's reward optimization into a direct policy loss without training a separate reward model?

- Concept: **Compiler Intermediate Representations (IRs)**
  - Why needed here: The method relies on LLVM IRs as language-agnostic representations. Understanding that IRs expose low-level operations (registers, control flow) independent of source syntax is essential.
  - Quick check question: Why would LLVM IR from -O0 and -O3 optimization levels be semantically equivalent but syntactically different?

- Concept: **On-Policy vs. Off-Policy RL**
  - Why needed here: OORL integrates both—on-policy RL for real-time trajectory sampling with rule rewards, off-policy GEPO for static preference data. The distinction matters for understanding sample efficiency and stability tradeoffs.
  - Quick check question: In on-policy RL, why must trajectories be sampled from the current policy rather than a replay buffer?

## Architecture Onboarding

- Component map:
  - On-Policy RL Branch: Policy model π_θ → Sample translations → Unit test execution → Binary reward R_rule → REINFORCE++ loss L_OnP
  - Off-Policy GEPO Branch: Static IR preference dataset D_gpref → Winner group Y_w (equivalent IRs) + Loser group Y_l (non-equivalent IRs) → GEPO loss L_GEPO (preference + variance terms)
  - Combined Objective: L = w_rl · L_OnP + w_gepo · L_GEPO (weights: 1.0 and 0.01 respectively)

- Critical path:
  1. Collect/construct IR groups: Compile C code with multiple LLVM optimization levels (-Oz, -O3) for Y_w; augment for Y_l
  2. Prepare translation training data with unit tests (2400 problems from SYNTHETIC-1, APPS)
  3. Initialize policy from SFT reference model (Qwen3-8B)
  4. Iterate: Sample on-policy translation trajectories → compute binary rewards → combine with GEPO loss on static IR data → update policy

- Design tradeoffs:
  - Memory: GEPO processes grouped responses, increasing GPU memory vs. pairwise DPO. Serialization can reduce memory but increases latency.
  - Reward granularity: Binary rewards are coarse but unambiguous; GEPO adds fine-grained equivalence signals but requires IR construction overhead.
  - Generalization vs. training coverage: Model generalizes to unseen languages (Scala, Haskell) but absolute performance on low-resource languages remains lower than high-resource ones.

- Failure signatures:
  - Unit tests pass but translation is semantically incorrect (insufficient test coverage)
  - GEPO variance constraint too tight → all winner rewards collapse, reducing preference discrimination
  - Memory OOM with large IR groups → consider serialization or smaller group sizes

- First 3 experiments:
  1. **Ablate on-policy RL only**: Train with REINFORCE++ without GEPO on translation data. Expect: MultiPL-E ~73.6 (Table 4 shows this). Validates binary reward contribution.
  2. **Compare GEPO vs. DPO**: Train with REINFORCE++ + DPO vs. REINFORCE++ + GEPO. Expect: GEPO outperforms on CrossPLEval (63.15 vs. 58.70 average). Validates group equivalence modeling.
  3. **Test zero-shot language generalization**: Evaluate on languages excluded from training (Haskell, Scala). Expect: Significant relative improvement but lower absolute scores than trained languages. Validates cross-language transfer claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the memory overhead of GEPO's group-based optimization be reduced without sacrificing training efficiency or model performance?
- Basis in paper: [explicit] The authors note in Section C (Limitations) that "GEPO manages larger groups of preference data, necessitating more GPU memory resources" and suggest serialization as a mitigation, but acknowledge it "introduces an increase in training latency."
- Why unresolved: The trade-off between memory efficiency and training speed remains unquantified, and no empirical results compare serialization vs. parallel processing.
- What evidence would resolve it: Ablation experiments comparing memory usage, training time, and final benchmark scores between parallel and serialized GEPO implementations across different group sizes.

### Open Question 2
- Question: Does the OORL framework scale effectively to larger LLMs (e.g., 70B+ parameters) with similar relative performance gains?
- Basis in paper: [inferred] All experiments are conducted solely on Qwen3-8B, and no analysis is provided on whether the improvements transfer to models with different capacities or architectures.
- Why unresolved: Larger models may have different inductive biases and may not benefit equally from IR-based preference optimization, especially if they already capture functional equivalence more effectively.
- What evidence would resolve it: Evaluation of OORL-trained variants of larger models (e.g., Qwen2.5-Coder-32B, DeepSeek-Coder-V2-Base) on MultiPL-E and CrossPLEval benchmarks.

### Open Question 3
- Question: Would training GEPO with multiple source languages (beyond C) improve cross-lingual transfer and low-resource language performance?
- Basis in paper: [inferred] GEPO is trained exclusively on "9600 curated C-to-IR groups" while the on-policy RL component uses Python and C++ sources. The paper does not ablate whether multilingual IR training provides additional benefits.
- Why unresolved: It is unclear whether the observed improvements stem from C-specific IR knowledge or generalize to IRs derived from other languages.
- What evidence would resolve it: Ablation studies training GEPO with IRs from Python, C++, and Java sources, then evaluating on the full CrossPLEval benchmark including low-resource languages like Haskell and Scala.

## Limitations
- Memory overhead from GEPO's group-based optimization requires more GPU resources and may benefit from serialization despite increased latency
- Critical implementation details for constructing IR preference groups (augmentation strategy, group sizes, variance thresholds) are unspecified
- Performance on low-resource languages remains lower than high-resource languages despite relative improvements

## Confidence

- **Mechanism 1 (Binary Rule-Based Reward)**: High confidence - The binary reward signal from unit tests is clearly specified and directly supported by the described REINFORCE++ implementation.
- **Mechanism 2 (GEPO with IR Groups)**: Medium confidence - While the theoretical framework is sound, critical implementation details for constructing preference groups are missing, creating uncertainty about practical effectiveness.
- **Mechanism 3 (Cross-Language Transfer)**: Medium confidence - The transfer claim is supported by observed improvements on unseen languages, but the mechanism assumes functional understanding transfers more readily than empirical evidence shows.

## Next Checks

1. **Validate GEPO preference signal construction**: Implement multiple IR augmentation strategies (semantic-preserving mutations, syntactic variations, corruption techniques) and measure how each affects GEPO loss stability and training convergence.

2. **Test hyperparameter sensitivity**: Systematically vary group sizes (n=2-10, m=3-20), β scaling (0.1-10), and variance thresholds (ϵ=0.01-0.1) to identify stable operating ranges for GEPO performance.

3. **Benchmark transfer effectiveness**: Evaluate zero-shot performance on languages with different paradigms (e.g., functional → imperative) to test the limits of cross-language transfer and identify which language pairs benefit most from translation training.