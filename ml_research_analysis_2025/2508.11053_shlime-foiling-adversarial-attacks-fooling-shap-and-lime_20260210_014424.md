---
ver: rpa2
title: 'SHLIME: Foiling adversarial attacks fooling SHAP and LIME'
arxiv_id: '2508.11053'
source_url: https://arxiv.org/abs/2508.11053
tags:
- lime
- shap
- adversarial
- classifier
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the vulnerability of post-hoc explanation
  methods like LIME and SHAP to adversarial manipulation, where biased models can
  deceive these tools into providing misleading interpretations. Building on Slack
  et al.'s framework, we replicated their findings showing that adversarial classifiers
  can successfully fool both methods into deflecting feature importance away from
  sensitive attributes.
---

# SHLIME: Foiling adversarial attacks fooling SHAP and LIME

## Quick Facts
- arXiv ID: 2508.11053
- Source URL: https://arxiv.org/abs/2508.11053
- Reference count: 2
- Primary result: SHLIME, an ensemble of LIME and SHAP, shows improved robustness against adversarial attacks that fool individual post-hoc explanation methods.

## Executive Summary
This research addresses the vulnerability of post-hoc explanation methods like LIME and SHAP to adversarial manipulation, where biased models can deceive these tools into providing misleading interpretations. Building on Slack et al.'s framework, we replicated their findings showing that adversarial classifiers can successfully fool both methods into deflecting feature importance away from sensitive attributes. We then introduced SHLIME, a novel ensemble approach combining LIME and SHAP by multiplying their outputs. Our experimental results demonstrate that SHLIME substantially improves robustness against adversarial attacks compared to either method alone. While maintaining high explanation accuracy across a wider range of Out-of-Distribution classifier accuracies (F1 scores), SHLIME shows more gradual degradation in performance as classifier accuracy increases, making it more reliable in detecting bias even when adversarial models become more sophisticated.

## Method Summary
The research constructs an adversarial classifier framework where an Out-of-Distribution (OOD) detector routes between a biased classifier and an unbiased surrogate based on whether inputs are real or perturbed. The proposed defense, SHLIME, computes feature importance by multiplying LIME and SHAP values element-wise, using SHAP as a scaling factor on LIME outputs. Experiments were conducted on the COMPAS dataset, evaluating how well the sensitive attribute appears in top-ranked features across varying OOD classifier F1 scores. The ensemble approach aims to maintain robust explanation accuracy (above 0.8) across a wider range of adversarial strengths compared to individual methods.

## Key Results
- SHLIME maintains explanation accuracy above 0.8 for a wider range of OOD classifier F1 scores (0.4-0.75) compared to LIME or SHAP individually
- SHAP begins being fooled at lower OOD classifier accuracies (F1 ≈ 0.45-0.5) with gradual degradation, while LIME remains robust until F1 ≈ 0.7-0.75 then fails abruptly beyond F1 > 0.8
- SHLIME shows more gradual degradation in performance as OOD classifier accuracy increases, rather than the sharp drops seen in individual methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial classifiers fool post-hoc explanations by detecting and responding differently to perturbed (out-of-distribution) inputs versus real data.
- Mechanism: Explanation methods like LIME and SHAP probe models by generating perturbed instances around a data point. These perturbations typically lie off the data manifold and are detectable. An adversarial classifier e(x) routes behavior: it applies the biased classifier f(x) on real distribution data X_dist, but applies an unbiased surrogate ψ(x) on perturbed inputs. Explanation methods thus receive misleading signals from the unbiased path.
- Core assumption: The OOD classifier can reliably distinguish perturbed from real instances; perturbations remain visually or statistically separable from the original distribution.
- Evidence anchors:
  - [abstract] "these methods are vulnerable to adversarial manipulation, potentially concealing harmful biases"
  - [section: Proposed Framework] "e(x) = f(x) if x ∈ X_dist, ψ(x) otherwise"
  - [corpus] Neighbor papers (e.g., "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation") corroborate XAI vulnerability to adversarial manipulation, though not this specific routing mechanism.
- Break condition: If perturbations become indistinguishable from real data (OOD classifier F1 drops), the adversarial classifier cannot reliably switch behaviors and loses its ability to deceive.

### Mechanism 2
- Claim: LIME and SHAP exhibit complementary vulnerability curves to adversarial OOD classifiers with different F1 accuracies.
- Mechanism: SHAP begins being fooled at lower OOD classifier accuracies (F1 ≈ 0.45-0.5) with gradual degradation, while LIME remains robust until a higher threshold (F1 ≈ 0.7-0.75) then fails abruptly beyond F1 > 0.8. This creates a "coverage gap" where each method fails in different regimes.
- Core assumption: The different sampling strategies (LIME's noise-based perturbation vs SHAP's feature-subset sampling) and weighting kernels produce systematically different OOD characteristics.
- Evidence anchors:
  - [section: Extensions] "SHAP is partially susceptible to attacks from less accurate OOD classifiers (F1 ≈ 0.45)... while LIME remains resilient until reaching higher accuracies (F1 ≈ 0.7)"
  - [section: Results] Figure 3 and 4 show replicated sensitivity analyses with divergent curves
  - [corpus] Weak direct corroboration; neighbor papers discuss XAI vulnerabilities broadly but not complementary failure patterns between methods.
- Break condition: If a single OOD classifier is optimized to fool both methods simultaneously (rather than independently), complementary patterns may converge.

### Mechanism 3
- Claim: Multiplying LIME and SHAP values (SHLIME) yields an ensemble that inherits extended resilience from LIME and gradual decay from SHAP.
- Mechanism: SHAP values are bounded [0,1]; LIME values can be unbounded and negative. Multiplication uses SHAP as a scaling factor on LIME output. When either method produces low/zero attribution for a biased feature (indicating successful deception), the product approaches zero regardless of the other's value. The ensemble thus requires both methods to be simultaneously fooled for the attack to succeed.
- Core assumption: Deception of LIME and SHAP is not perfectly correlated across F1 scores; there exist regimes where one remains reliable while the other is compromised.
- Evidence anchors:
  - [section: Extensions] "the most straightforward approach to doing so was simply multiplying the two values together, using the SHAP values as a scaling factor"
  - [section: Conclusions] "BASIC SHLIME maintains strong explanation accuracy (consistently above 0.8) for a wider range of F1 scores"
  - [corpus] No direct external validation of multiplication-based ensemble for this use case found in neighbor papers.
- Break condition: If adversaries develop SHLIME-specific OOD classifiers or joint perturbation strategies, the ensemble advantage may diminish.

## Foundational Learning

- Concept: Additive Feature Attribution Methods (LIME/SHAP)
  - Why needed here: Understanding how LIME and SHAP produce local explanations via linear surrogate models is essential to grasp why perturbation-based probing creates attack surfaces.
  - Quick check question: Can you explain why LIME uses distance-weighted sampling while SHAP uses Shapley-value-consistent kernel weighting?

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: The adversarial framework hinges on distinguishing real from perturbed data; understanding OOD classification (and its failure modes) explains attack feasibility.
  - Quick check question: What happens to an adversarial classifier's deception capability if the OOD detector has F1 = 0.5?

- Concept: Explanation Fidelity vs. Robustness Trade-offs
  - Why needed here: SHLIME optimizes for robustness against adversarial concealment; practitioners must understand when this may reduce fidelity on non-adversarial models.
  - Quick check question: If SHLIME multiplies LIME and SHAP values, what happens to attribution when SHAP ≈ 0 but LIME is large?

## Architecture Onboarding

- Component map:
  - Biased Classifier f -> OOD Detector is_OOD -> Adversarial Classifier e
  - Adversarial Classifier e -> LIME Explainer -> SHLIME Product
  - Adversarial Classifier e -> SHAP Explainer -> SHLIME Product

- Critical path:
  1. Train biased classifier f on dataset with sensitive attribute
  2. Generate perturbations (LIME-style noise, SHAP-style subsets) to train is_OOD classifier
  3. Construct adversarial e(x) using is_OOD to route between f and ψ
  4. Run LIME and SHAP on e(x); compute SHLIME product
  5. Evaluate whether sensitive attribute appears in top-k features across explanation methods

- Design tradeoffs:
  - Multiplication vs. weighted average: Multiplication preserves information when methods disagree (product → 0 if either is fooled), but loses magnitude calibration; averaging may dilute robustness
  - OOD classifier F1 threshold: Higher F1 improves attack success but is harder to achieve; SHLIME targets the mid-range (F1 0.5–0.75) where individual methods fail differentially
  - Assumption: SHLIME has not yet been tested with SHLIME-specific OOD classifiers; current results use LIME or SHAP OOD detectors

- Failure signatures:
  - SHLIME accuracy drops sharply if OOD classifier F1 exceeds ~0.75 (both component methods fooled)
  - Negative LIME values with positive SHAP values may produce misleading signed products
  - Computational cost approximately doubles (running both explainers)

- First 3 experiments:
  1. Replicate Slack et al. COMPAS experiment: Verify that adversarial classifiers fool LIME/SHAP at varying OOD F1 scores; establish baseline curves (Figures 3-4 in paper)
  2. Ablation on combination strategy: Compare SHLIME (multiplication) vs. weighted average vs. max-pooling of LIME/SHAP attributions; measure robustness across F1 range
  3. Dataset transferability: Test SHLIME on Communities and Crime and German Credit datasets to assess generalization beyond COMPAS; verify whether complementary vulnerability patterns hold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a specialized Out-of-Distribution (OOD) classifier be constructed to successfully deceive the SHLIME ensemble?
- Basis in paper: [explicit] The authors state the need to "create a proper OOD classifier" to validate robustness on a "fair scale."
- Why unresolved: The study utilized existing LIME and SHAP OOD classifiers; a unified classifier tailored to SHLIME was beyond the project's scope.
- What evidence would resolve it: Development of a SHLIME-specific OOD classifier that demonstrates successful bias concealment against the ensemble method.

### Open Question 2
- Question: Does SHLIME maintain standard interpretability performance compared to individual methods on benign datasets?
- Basis in paper: [explicit] The authors note it is "important to consider how well it performs in its intended interpretability task," acknowledging a potential performance decrease.
- Why unresolved: The research focused exclusively on robustness against adversarial attacks rather than baseline utility or fidelity.
- What evidence would resolve it: Benchmarking results comparing SHLIME's fidelity and explanation quality against LIME and SHAP on non-adversarial datasets.

### Open Question 3
- Question: Would a "mixture of experts" approach yield superior robustness compared to the multiplicative method?
- Basis in paper: [explicit] The paper lists exploring alternative ensemble methods, specifically a "mixture of experts," as an avenue for future work.
- Why unresolved: Only the straightforward multiplication of values was implemented and tested in this study.
- What evidence would resolve it: Comparative analysis of different ensemble architectures (e.g., mixture of experts vs. multiplication) under adversarial conditions.

## Limitations
- The multiplication-based ensemble assumes LIME and SHAP vulnerabilities are uncorrelated, which may not hold against sophisticated, SHLIME-specific attacks
- Handling of negative LIME values during multiplication remains unclear, potentially affecting attribution accuracy
- Experiments were conducted primarily on the COMPAS dataset; generalization to other domains requires validation

## Confidence
- High confidence: The foundational observation that LIME and SHAP have complementary vulnerability patterns to adversarial OOD classifiers
- Medium confidence: SHLIME's improved robustness range and the mechanism by which multiplication provides ensemble protection
- Low confidence: SHLIME's performance against adaptive adversaries who optimize specifically for the multiplication strategy

## Next Checks
1. **OOD Detector Saturation Test**: Systematically evaluate SHLIME performance as OOD classifier F1 approaches 1.0 to identify the precise threshold where both component methods fail simultaneously
2. **Alternative Ensemble Strategies**: Compare multiplication against weighted averaging and max-pooling approaches to verify if the multiplicative property is essential to SHLIME's advantage
3. **Cross-Dataset Transferability**: Validate SHLIME's complementary vulnerability patterns on German Credit and Communities and Crime datasets to assess generalization beyond COMPAS