---
ver: rpa2
title: 'DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine
  MRI-guided radiotherapy'
arxiv_id: '2508.10260'
source_url: https://arxiv.org/abs/2508.10260
tags:
- dinomotion
- motion
- tracking
- registration
- radiotherapy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DINOMotion, a deep learning-based framework
  for robust tissue motion tracking in 2D-Cine MRI-guided radiotherapy. The method
  leverages the DINOv2 foundation model with Low-Rank Adaptation (LoRA) to extract
  corresponding landmarks between moving and template scans, which are then used to
  compute optimal image registration transformations.
---

# DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy

## Quick Facts
- arXiv ID: 2508.10260
- Source URL: https://arxiv.org/abs/2508.10260
- Reference count: 40
- Primary result: DINOMotion achieved Dice scores of 92.07% (kidney), 90.90% (liver), and 95.23% (lung) on 2D-Cine MRI registration, outperforming state-of-the-art methods.

## Executive Summary
This paper introduces DINOMotion, a deep learning framework for robust tissue motion tracking in 2D-Cine MRI-guided radiotherapy. The method leverages the DINOv2 foundation model with Low-Rank Adaptation (LoRA) to extract corresponding landmarks between moving and template scans, which are then used to compute optimal image registration transformations. Evaluated on volunteer and patient datasets, DINOMotion achieved high Dice scores across kidney, liver, and lung organs while consistently outperforming baseline methods, particularly in handling large misalignments. The landmark-based approach also provides interpretability, enabling clinicians to visualize anatomical correspondences and improve trust in the motion tracking process.

## Method Summary
DINOMotion is a landmark-based registration framework that uses DINOv2 ViT backbone with LoRA layers to predict 64 corresponding landmarks between moving and template 2D-Cine MRI scans. The model outputs (x,y) coordinates for each landmark, which are then used to compute either rigid, affine, or Thin-Plate Spline (TPS) transformations. The framework was trained on 190K pairs from 17 volunteer subjects using MSE loss between the transformed moving image and template. The model supports both linear and nonlinear transformations while providing interpretability through explicit visual correspondences between anatomical structures.

## Key Results
- Achieved Dice scores of 92.07% (kidney), 90.90% (liver), and 95.23% (lung)
- Corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm
- Processing time of approximately 30 ms per scan, demonstrating real-time capability
- Consistently outperformed state-of-the-art methods, particularly in handling large misalignments
- Landmark-based approach provides interpretability through explicit visual correspondences

## Why This Works (Mechanism)

### Mechanism 1: ViT Global Receptive Field for Large Misalignment Handling
DINOMotion handles large initial misalignments better than CNN-based methods because its DINOv2 ViT backbone captures global context via self-attention. Unlike CNNs which concatenate inputs at early layers with limited receptive fields, the Vision Transformer attends to all patches globally from the first layer, allowing the feature extractor to establish correspondence between anatomically similar regions even when they are spatially distant in the image grid.

### Mechanism 2: Landmark-Based Transformation as an Interpretable Bottle-neck
The explicit prediction of corresponding landmarks provides an interpretable interface and guarantees diffeomorphic properties for certain transformations (like TPS), avoiding implausible deformations common in direct dense-field regression. The model outputs a fixed number (N=64) of (x,y) coordinate pairs for each image, and a differentiable transformation solver computes the optimal mapping based only on these sparse correspondences.

### Mechanism 3: LoRA for Efficient Domain Adaptation
Low-Rank Adaptation (LoRA) allows efficient fine-tuning of the large DINOv2 model on a relatively small medical dataset without catastrophic forgetting or prohibitive computational cost. LoRA injects small, trainable rank-decomposition matrices into the Transformer's attention layers (rank=4), keeping the pre-trained weights frozen and constraining updates to a low-rank subspace.

## Foundational Learning

- **Vision Transformers (ViT) and Self-Attention**: The core backbone of DINOMotion processes images as sequences of patches using self-attention to weigh the importance of all other patches when processing one, providing the global context critical for large misalignment handling. *Quick check: If a patch in the top-left corner needs to reference a patch in the bottom-right corner to find a match, which mechanism in a ViT makes this possible immediately?*

- **Thin-Plate Spline (TPS) Interpolation**: Used for non-linear registration mode, TPS is a spline-based technique that models a smooth surface passing exactly through a set of control points (landmarks). Understanding TPS is crucial for interpreting how sparse landmarks dictate the dense deformation field and why it ensures smoothness. *Quick check: What physical analogy is the TPS energy term derived from?*

- **Center-of-Mass (CoM) Layer for Landmark Regression**: The paper uses a CoM layer that converts a heatmap (probability/activation map) into a single (x,y) coordinate by calculating the centroid of the activations. This is differentiable and avoids quantization issues from simple argmax. *Quick check: Why is a soft-argmax or Center-of-Mass layer generally preferred over a hard argmax operation?*

## Architecture Onboarding

- **Component map**: Input Pre-processor (normalize, resize 224x224, replicate to 3 channels) -> DINOv2 ViT-Base + LoRA (rank=4) -> CNN Decoder (3 blocks: 512→256→64) -> Center-of-Mass (CoM) layer -> Landmarks (64x2) -> Transformation Solver (Rigid/Affine/TPS) -> Spatial Transformer -> Registered Image -> MSE Loss

- **Critical path**: Moving Image -> Pre-processor -> DINOv2 + LoRA -> CNN Decoder -> CoM Layer -> **Landmarks** -> Transformation Solver -> Spatial Transformer -> Registered Image -> MSE Loss

- **Design tradeoffs**: Speed vs. Accuracy (DINOMotion is ~7x slower than VoxelMorph on GPU at 30ms vs 4ms), Interpretability vs. Flexibility (sparse landmark bottleneck enforces global coherence but may miss fine local details), LoRA Rank (4 balances fine-tuning capacity with memory/speed)

- **Failure signatures**: Uniformative Landmarks (clustering in corners/non-informative areas), TPS "Folding" (negative Jacobian determinant from conflicting landmark positions), Occlusion/Through-Plane (partial structural correspondence failures)

- **First 3 experiments**:
  1. Run inference with pre-trained DINOMotion model on sample data and visualize detected landmarks on both template and moving images
  2. Identify LoRA matrix definitions in code and train a version with LoRA disabled to quantify performance gain
  3. Create synthetic dataset with extreme affine transformations (>30° rotation, >20% translation) and compare landmark consistency against baseline detectors

## Open Questions the Paper Calls Out

- **Model Compression**: Can model distillation or pruning compress the DINOv2 backbone to reduce inference time below VoxelMorph (approx. 4ms) without sacrificing registration accuracy? The authors plan to explore this to address the current speed limitation.

- **3D and Temporal Contexts**: Does incorporating volumetric (3D) and temporal contexts into DINOMotion improve the handling of complex deformations, such as through-plane motion? The current framework does not directly leverage these contexts.

- **Multi-institutional Generalization**: Does DINOMotion maintain robust performance across multi-institutional datasets with inter-scanner variability? The model was trained and validated using data from a single institution.

- **Organ-Specific Landmark Strategies**: Does adopting organ-specific landmark prediction or weighting strategies improve localization accuracy for anatomically complex organs? The current method weights all 64 landmarks equally.

## Limitations

- The framework processes individual 2D slices independently, limiting its ability to resolve out-of-plane motion
- Reliance on a single scanner type (1.5 T MR-Linac) leaves adaptability to different imaging protocols unproven
- The use of 64 landmarks may be insufficient for capturing complex, nonlinear deformations in some anatomical regions

## Confidence

- **High Confidence**: The framework's ability to produce interpretable landmark correspondences and its computational efficiency for real-time applications
- **Medium Confidence**: The reported Dice and Hausdorff scores, given the relatively small patient cohort (n=23) and lack of external validation
- **Low Confidence**: The claim of superiority over all state-of-the-art methods, as the comparison lacks ablation studies isolating the contributions of individual components

## Next Checks

1. **Cross-Modality Generalization**: Test DINOMotion on 2D-Cine MRI data from a different MR-Linac vendor or imaging protocol to assess robustness to domain shifts

2. **Landmark Sufficiency Analysis**: Experiment with varying the number of landmarks (e.g., 32, 128) to determine the optimal trade-off between computational efficiency and registration accuracy

3. **Failure Mode Analysis**: Systematically evaluate performance on synthetic data with known failure modes (e.g., occlusions, through-plane motion) to identify and quantify the framework's limitations