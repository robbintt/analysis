---
ver: rpa2
title: 'OFMU: Optimization-Driven Framework for Machine Unlearning'
arxiv_id: '2509.22483'
source_url: https://arxiv.org/abs/2509.22483
tags:
- unlearning
- ofmu
- forgetting
- inner
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OFMU, a penalty-based bi-level optimization
  framework for machine unlearning. OFMU explicitly prioritizes forgetting (removing
  the influence of targeted data) over retention (maintaining performance on remaining
  data) by formulating unlearning as an inner maximization problem and utility restoration
  as an outer minimization problem.
---

# OFMU: Optimization-Driven Framework for Machine Unlearning

## Quick Facts
- arXiv ID: 2509.22483
- Source URL: https://arxiv.org/abs/2509.22483
- Reference count: 40
- Key outcome: Introduces OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting over retention, demonstrating superior performance on language and vision benchmarks while improving robustness against membership inference attacks.

## Executive Summary
This paper presents OFMU (Optimization-Driven Framework for Machine Unlearning), a novel penalty-based bi-level optimization approach to machine unlearning that explicitly prioritizes the removal of targeted data influence over maintaining performance on remaining data. The framework formulates unlearning as an inner maximization problem and utility restoration as an outer minimization problem, using a similarity-aware penalty to decorrelate forget and retain gradients. Theoretical analysis provides convergence guarantees under both convex and non-convex regimes. Extensive experiments on TOFU, WMDP, CIFAR-10, and CIFAR-100 benchmarks demonstrate that OFMU consistently outperforms existing methods in balancing forget efficacy with retained utility while improving robustness against membership inference attacks.

## Method Summary
OFMU is a penalty-based bi-level optimization framework that formulates machine unlearning as a bilevel problem where the inner maximization explicitly prioritizes forgetting the targeted data while the outer minimization restores utility on the retained dataset. The key innovation is a similarity-aware penalty that decorrelates the gradients of forgetting and retention objectives, avoiding the need to solve the inner problem to completion by enforcing stationarity through a soft constraint. The method uses Hessian-vector products for efficient gradient computation and employs a cosine similarity-based penalty to maintain gradient decorrelation throughout optimization. Theoretical analysis establishes convergence guarantees under both convex and non-convex regimes, with the framework showing particular effectiveness in handling "hard-to-unlearn" samples through adaptive gradient management.

## Key Results
- OFMU achieves superior forgetting efficacy compared to existing methods while maintaining competitive retention performance on TOFU, WMDP, CIFAR-10, and CIFAR-100 benchmarks
- The framework demonstrates improved robustness against membership inference attacks compared to baseline unlearning methods
- Theoretical convergence guarantees are established under both convex and non-convex regimes, validating the optimization approach
- The similarity-aware penalty effectively decorrelates forget and retain gradients, enabling efficient optimization without solving the inner problem to completion

## Why This Works (Mechanism)
OFMU works by explicitly formulating unlearning as a bi-level optimization problem where forgetting is prioritized through inner maximization, while retention is handled through outer minimization with a similarity-aware penalty. The key mechanism is the decorrelation of forget and retain gradients using cosine similarity, which prevents conflicting updates and allows the model to simultaneously unlearn targeted data while maintaining performance on retained data. By enforcing stationarity through a soft constraint rather than solving the inner problem completely, OFMU achieves computational efficiency while maintaining effectiveness. The framework's prioritization of forgetting over retention through the bilevel structure ensures that privacy requirements are met even at the cost of some utility loss, which is often acceptable in practical unlearning scenarios.

## Foundational Learning

### Bi-level Optimization
- **Why needed**: Enables the explicit prioritization of forgetting over retention through nested optimization problems
- **Quick check**: Verify the inner problem solves the forgetting objective while the outer problem optimizes retained utility

### Gradient Similarity Measures
- **Why needed**: Cosine similarity decorrelates conflicting gradients from forget and retain objectives
- **Quick check**: Monitor gradient cosine similarity throughout training to ensure effective decorrelation

### Hessian-Vector Products
- **Why needed**: Enables efficient computation of second-order information without full Hessian matrices
- **Quick check**: Confirm computational efficiency gains compared to full second-order methods

### Soft Constraint Enforcement
- **Why needed**: Allows approximate stationarity without fully solving the inner maximization problem
- **Quick check**: Verify that stationarity constraints are satisfied sufficiently for convergence

## Architecture Onboarding

### Component Map
Data → Inner Maximization (Forget) → Outer Minimization (Retain) → Similarity-Aware Penalty → Model Update

### Critical Path
The critical path involves computing gradients for both forgetting and retention, applying the similarity-aware penalty to decorrelate them, and updating model parameters through the bi-level optimization structure.

### Design Tradeoffs
Prioritizes forgetting over retention by explicit formulation rather than balancing through weighted sums, sacrificing some retained utility for stronger privacy guarantees. Uses soft constraints for computational efficiency rather than exact solutions.

### Failure Signatures
If gradients remain correlated despite penalty application, forgetting efficacy will degrade. If stationarity enforcement is too weak, the inner problem may not adequately prioritize forgetting.

### First 3 Experiments to Run
1. Evaluate forgetting efficacy on a small forget set (e.g., forget01) while measuring retention drop on retained data
2. Test gradient decorrelation effectiveness by monitoring cosine similarity throughout training
3. Compare computational overhead with exact inner problem solutions versus the soft constraint approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can OFMU be effectively extended to continual unlearning scenarios where multiple unlearning requests arrive sequentially over time?
- **Basis in paper**: The Conclusion section explicitly identifies "extending OFMU to continual unlearning scenarios, where multiple requests arrive sequentially" as a direction to enhance applicability.
- **Why unresolved**: The current framework and experiments evaluate static unlearning on fixed forget sets, but do not analyze the cumulative impact of sequential updates on model stability or the preservation of previously unlearned states.
- **What evidence would resolve it**: An evaluation of OFMU on a streaming data setting, measuring the degradation of retention accuracy and forgetting efficacy across successive unlearning requests compared to a retrained baseline.

### Open Question 2
- **Question**: Do adaptive penalty schedules or alternative gradient similarity measures offer improved robustness and convergence compared to the fixed schedule and cosine similarity used in the current study?
- **Basis in paper**: The Conclusion notes that "investigating adaptive penalty schedules and alternative gradient similarity measures could further improve robustness."
- **Why unresolved**: While the paper demonstrates the efficacy of a specific penalty schedule and cosine similarity, it does not explore dynamic adjustments based on optimization progress or other metrics for gradient decorrelation.
- **What evidence would resolve it**: Ablation studies testing various adaptive penalty functions and similarity metrics on the TOFU and CIFAR benchmarks, specifically analyzing convergence rates and performance on "hard-to-unlearn" samples.

### Open Question 3
- **Question**: Does OFMU scale effectively to foundation models significantly larger than 7B parameters and to diverse data modalities such as speech or multimodal inputs?
- **Basis in paper**: The Conclusion lists "applying OFMU to even larger foundation models and diverse modalities such as speech and multimodal learning" as a promising avenue for future research.
- **Why unresolved**: The empirical validation is limited to LLaMA-2-7B and ResNet-style vision models; it remains untested whether the memory requirements for Hessian-vector products and the bi-level optimization dynamics remain tractable for significantly larger or multimodal architectures.
- **What evidence would resolve it**: Experimental results applying OFMU to models with 70B+ parameters or multimodal benchmarks, reporting computational overhead and unlearning performance metrics.

## Limitations
- The fixed weighting between forgetting and retention priorities may not generalize to all real-world scenarios where data privacy and model utility must be balanced more carefully
- The soft constraint on stationarity in the inner problem lacks detailed analysis of how the choice of this constraint affects convergence across different model architectures and datasets
- The similarity-aware penalty's effectiveness depends on the quality of gradient similarity measures, which may not capture complex dependencies in high-dimensional neural network representations

## Confidence

- **High confidence**: Theoretical convergence guarantees under convex and non-convex regimes, as these follow established optimization principles and are presented with appropriate mathematical rigor
- **Medium confidence**: Empirical superiority claims, as the experiments show consistent improvement but comparisons are limited to a specific set of baselines and benchmarks
- **Medium confidence**: Robustness against membership inference attacks, as this claim relies on a single attack method and limited threat model

## Next Checks

1. Test OFMU's performance when the forget-retention priority ratio is varied dynamically based on downstream application requirements rather than using a fixed weighting scheme
2. Evaluate the framework's effectiveness when the similarity-aware penalty uses alternative gradient similarity metrics (e.g., cosine similarity, Euclidean distance) to assess robustness to similarity measure choice
3. Benchmark against additional unlearning methods beyond the current baselines, particularly those that use different approaches (e.g., data augmentation, differential privacy) to ensure comprehensive comparison