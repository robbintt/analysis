---
ver: rpa2
title: Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st
  Prize Solution
arxiv_id: '2510.16443'
source_url: https://arxiv.org/abs/2510.16443
tags:
- data
- training
- robust
- adversarial
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The challenge focused on creating a deep neural network that remains
  robust when classifying particle jets as "two top-jets" or "two W-boson-jets" under
  adversarial attacks, specifically using the Random Distribution Shuffle Attack (RDSA).
  The primary obstacle was the absence of direct access to similarly generated adversarial
  examples for training.
---

# Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution

## Quick Facts
- arXiv ID: 2510.16443
- Source URL: https://arxiv.org/abs/2510.16443
- Authors: Dimitris Stefanopoulos; Andreas Voskou
- Reference count: 8
- Primary result: 1st place in ECML-PKDD 2025 Model Robustness Competition with 80% mixed accuracy (88% clean, 72% adversarial)

## Executive Summary
This work presents the winning solution to the ECML-PKDD 2025 Model Robustness Competition, focusing on binary classification of particle jets under adversarial attacks. The key challenge was the absence of direct access to similarly generated adversarial examples for training. To overcome this, the team implemented an "antiRDSA" data generation method that produced 15 million synthetic training samples by perturbing existing data in the same manner as the Random Distribution Shuffle Attack (RDSA), but preserving true labels. The resulting model achieved a mixed accuracy score of 80%, outperforming the second-place solution by two percentage points.

## Method Summary
The solution combines a custom data augmentation technique called "antiRDSA" with a specialized neural network architecture. The antiRDSA method generates 15 million synthetic training samples by applying the same shuffling logic as RDSA but retaining true labels, effectively creating labeled adversarial-like data. The architecture employs shared-weight embeddings for each of the three feature types (pT, φ, η), followed by a dense fusion tail using a multi-layer perceptron. The final ensemble consists of four classifiers with varying input dropout rates and training datasets, trained for a single epoch on the combined clean and synthetic data.

## Key Results
- Achieved 80% mixed accuracy score, winning 1st place in the competition
- 88% accuracy on clean samples and 72% on adversarial data
- Outperformed second-place solution by two percentage points
- Demonstrated effectiveness of antiRDSA data augmentation and shared-weight embedding architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on "antiRDSA" synthetic data effectively closes the distribution gap between clean training inputs and adversarial test inputs.
- **Mechanism:** By generating 15 million samples using the same shuffling logic as the attack (RDSA) but retaining true labels, the model learns decision boundaries that are invariant to the specific perturbations introduced by the attack.
- **Core assumption:** The model generalizes better when exposed to the specific statistical structure of the attack noise during training.
- **Evidence anchors:** Abstract mentions custom methodology derived from RDSA; section 3 describes antiRDSA excluding the model query step.
- **Break condition:** If the adversarial attack at inference time changes its perturbation distribution significantly, the specific "antiRDSA" invariance may fail to transfer.

### Mechanism 2
- **Claim:** Shared-weight embeddings based on physical units (pT, φ, η) act as an effective inductive bias for tabular particle physics data.
- **Mechanism:** Instead of treating all 87 features identically, the architecture groups features by physical type, allowing the model to learn specialized transformations for "angles" vs. "momenta."
- **Core assumption:** Features of the same physical type share underlying mathematical properties that benefit from identical non-linear projections.
- **Evidence anchors:** Abstract describes feature embedding block with shared weights for three feature types; section 4 explains grouping by data types.
- **Break condition:** If specific features within the same physical group have vastly different scales or semantic meanings, strict weight sharing could degrade performance.

### Mechanism 3
- **Claim:** Ensembling diverse input noise and data augmentation strategies reduces variance in adversarial accuracy.
- **Mechanism:** The ensemble averages predictions from models trained with different input dropout rates (7.5% vs 12.5%) and data generation depths (n_vars 5 vs 10).
- **Core assumption:** The hidden test set contains a mix of adversarial difficulties that cannot be fully covered by a single training configuration.
- **Evidence anchors:** Section 4 describes the averaging ensemble of 2+2 networks with different dropout rates and training datasets.
- **Break condition:** If the attack is specifically optimized to fool the ensemble, this diversity might offer diminishing returns compared to a single stronger model.

## Foundational Learning

- **Concept: Adversarial Robustness via Data Augmentation**
  - **Why needed here:** The core problem is a distribution shift between train and test. You cannot solve this with architecture alone; you must understand how to synthesize "worst-case" data to inoculate the model.
  - **Quick check question:** Can you explain why standard Gaussian noise augmentation might fail against a structured attack like RDSA?

- **Concept: Feature Embeddings for Tabular Data**
  - **Why needed here:** The architecture rejects raw scalar inputs in favor of vector embeddings. You must understand why mapping a scalar x → R^8 allows the MLP to better separate complex non-linear boundaries.
  - **Quick check question:** How does sharing weights across features (e.g., all φ features) differ mathematically from having a single "scalar-to-vector" lookup table?

- **Concept: Ensemble Diversity**
  - **Why needed here:** The winning solution relied on averaging models with different input dropouts. You need to distinguish between "bagging" (data resampling) and this approach of "input corruption variance."
  - **Quick check question:** Why would averaging two models trained on the same data with different dropout rates improve robustness, compared to just using the lower dropout rate?

## Architecture Onboarding

- **Component map:**
  Input (87 scalar features) → Router (splits into 3 groups: pT, φ, η) → Embedding Block (three distinct 2-layer ReLU MLPs, shared weights per type) → Fusion (concatenation of 87×8D vectors) → Dense Tail (MLP with 256 hidden units, tanh activation)

- **Critical path:**
  The Data Generation Phase is the most critical and resource-intensive step. The model architecture is simple; the performance relies almost entirely on the successful generation of the 15M+ "antiRDSA" samples.

- **Design tradeoffs:**
  - Periodic vs ReLU Embeddings: The authors wanted periodic embeddings (better for angles) but were blocked by competition format constraints, settling for ReLU.
  - One Epoch Training: Training on 15M samples for 1 epoch is extremely efficient but offers no chance for early stopping or validation-based model selection.

- **Failure signatures:**
  - Clean High / Adv Low: Model learned clean features but failed to generalize to perturbations (Data generation mismatch).
  - Low Training Accuracy: Embedding weights may have diverged; check learning rate relative to the large batch sizes implied by 15M samples.

- **First 3 experiments:**
  1. Baseline Validation: Train the exact architecture on clean data only. Verify that accuracy drops to near-random on the RDSA-perturbed validation set to confirm the vulnerability.
  2. Ablation on "AntiRDSA": Generate a small subset (e.g., 100k) of "antiRDSA" data. Train and compare robustness vs. generic Gaussian noise augmentation.
  3. Embedding Variance: Implement the shared-weight embedding logic. Try collapsing all 87 features to a single shared embedding vs. the 3-type split to quantify the domain-specific benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent would substituting the simplified ReLU embeddings with periodic or transformer-based architectures improve the robustness score?
- **Basis in paper:** The authors explicitly suggest in the Discussion that integrating modern architectures like periodic embedding variants or transformers could improve performance by 0.02–0.03 units.
- **Why unresolved:** Competition constraints prohibited these custom layers, leaving the theoretical gain unverified on this specific RDSA dataset.
- **What evidence would resolve it:** Benchmarking the antiRDSA data on a Transformer-based tabular model (e.g., TabTransformer) within the same evaluation framework.

### Open Question 2
- **Question:** Can stochastic or Bayesian deep learning methods provide superior robustness against RDSA compared to the deterministic ensemble approach used?
- **Basis in paper:** The Discussion posits that stochastic methodologies, including Bayesian variational techniques, may enhance robustness to distributional shifts.
- **Why unresolved:** The solution relied on ensembling deterministic MLPs; stochastic layers were not implemented or tested.
- **What evidence would resolve it:** A comparative study measuring the Mixed Accuracy score of a Bayesian Neural Network trained on the generated antiRDSA dataset.

### Open Question 3
- **Question:** Does the robustness gained via antiRDSA training transfer effectively to other adversarial attack types, or is it overfitted to the specific mechanics of RDSA?
- **Basis in paper:** The method is described as "method-aware," specifically designed to counter RDSA by mimicking its perturbation mechanics.
- **Why unresolved:** The paper does not evaluate the model against standard adversarial attacks (e.g., PGD, FGSM), only the competition's specific RDSA hidden set.
- **What evidence would resolve it:** Evaluating the final ensemble model against standard gradient-based adversarial attacks on the clean dataset.

## Limitations

- The success critically depends on the "antiRDSA" augmentation matching the test-time attack distribution, which cannot be verified without access to the test set.
- The single-epoch training approach leaves no room for early stopping or validation-based model selection, creating high-variance outcomes.
- The paper omits precise training hyperparameters (optimizer type, learning rate, batch size, loss function), making exact reproduction difficult.

## Confidence

- **High confidence:** The fundamental mechanism of using label-preserving adversarial-like data augmentation (antiRDSA) to improve robustness is well-supported by the reported results.
- **Medium confidence:** The effectiveness of shared-weight embeddings for physics features is plausible given the domain-specific grouping, but lacks explicit ablation studies.
- **Medium confidence:** The ensemble strategy combining different dropout rates and data augmentation depths is logically sound for variance reduction, but the specific configuration is not rigorously justified beyond empirical success.

## Next Checks

1. **Ablation on antiRDSA effectiveness:** Generate a small validation set using RDSA and compare model performance trained on clean data only versus data augmented with antiRDSA. Verify that antiRDSA provides statistically significant improvement over generic noise augmentation.

2. **Embedding architecture ablation:** Implement three variants of the embedding block: (a) single shared embedding for all 87 features, (b) three-type shared embedding (current), (c) independent embeddings per feature. Measure impact on both clean and adversarial accuracy.

3. **Ensemble diversity analysis:** Train individual ensemble members separately and analyze their failure patterns on adversarial examples. Compute pairwise correlation of predictions and error rates to verify that diversity is contributing to robustness gains.