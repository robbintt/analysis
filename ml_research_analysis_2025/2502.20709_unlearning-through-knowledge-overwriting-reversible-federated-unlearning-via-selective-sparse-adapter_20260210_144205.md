---
ver: rpa2
title: 'Unlearning through Knowledge Overwriting: Reversible Federated Unlearning
  via Selective Sparse Adapter'
arxiv_id: '2502.20709'
source_url: https://arxiv.org/abs/2502.20709
tags:
- unlearning
- knowledge
- data
- client
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FUSED addresses the problem of federated unlearning, enabling
  models to forget sensitive or harmful knowledge while maintaining performance on
  remaining data. It introduces a two-stage process: first, identifying critical layers
  sensitive to knowledge changes through layer-wise analysis; second, constructing
  sparse unlearning adapters for these layers and training them independently without
  altering original model parameters.'
---

# Unlearning through Knowledge Overwriting: Reversible Federated Unlearning via Selective Sparse Adapter

## Quick Facts
- arXiv ID: 2502.20709
- Source URL: https://arxiv.org/abs/2502.20709
- Reference count: 40
- Key result: Achieves unlearning effectiveness comparable to retraining while reducing communication costs by 40-98% and maintaining accuracy >0.99 on remaining data

## Executive Summary
FUSED introduces a two-stage federated unlearning approach that identifies critical model layers through sensitivity analysis and constructs sparse adapters to overwrite forgotten knowledge without modifying base parameters. The method enables reversible unlearning by storing knowledge modifications in modular adapters that can be instantly removed. Experiments across client, class, and sample unlearning scenarios demonstrate FUSED achieves comparable forgetting performance to full retraining while significantly reducing computational and communication costs.

## Method Summary
FUSED operates in two stages: First, Critical Layer Identification (CLI) analyzes parameter sensitivity across federated clients to identify layers storing the most knowledge. Second, sparse unlearning adapters are constructed for these critical layers and trained independently on remaining data while base model parameters remain frozen. The adapters overwrite forgotten knowledge when task gradients oppose each other (cosine similarity < 0), and knowledge can be instantly restored by removing adapters.

## Key Results
- Achieves unlearning effectiveness comparable to full retraining across client, class, and sample unlearning scenarios
- Reduces communication costs by 40-98% (42.73M to 0.98M params on CIFAR-10, 177K to 11K on FashionMNIST)
- Maintains accuracy >0.99 on remaining data while effectively forgetting target knowledge
- Enables instant reversibility by simply discarding adapters

## Why This Works (Mechanism)

### Mechanism 1: Critical Layer Identification via Parameter Sensitivity Analysis
Layers with largest parameter changes across clients are most critical for knowledge storage. After one federated round, compute Manhattan distance between client layer parameters and global model, aggregate distances weighted by data volume to rank sensitivity. Top-K sensitive layers are selected for adapter placement. Core assumption: Parameter magnitude change correlates with knowledge importance. Evidence: CLI consistently identifies final layers as most sensitive across architectures.

### Mechanism 2: Gradient Opposition Enables Knowledge Overwriting
Training adapters on remaining data (D_r) overwrites forgotten knowledge (D_u) when task gradients are sufficiently opposed (cosine similarity < 0). Performance degradation on forgotten task ΔL_T1 ≈ -η·||∇L_T1||·||∇L_T2||·Φ, where Φ is cosine similarity. When Φ < 0 (gradients > 90° apart), learning T2 degrades T1 performance. Core assumption: Loss landscape permits first-order Taylor approximation and gradient directions have negative cosine similarity. Evidence: Theoretical analysis shows overwriting occurs when angle between gradients exceeds 90 degrees.

### Mechanism 3: Modular Sparse Adapters Enable Reversibility and Cost Reduction
Storing knowledge modifications in sparse, independent adapters enables instant reversal and reduces communication by 40-98%. Adapters are sparse binary masks applied to critical layers. During inference: p_new = p_Af + p_Lf. To reverse unlearning, discard A_f and restore original model. Only adapters (11K-0.98M params) are transmitted vs. full model (177K-42M). Core assumption: Knowledge to be forgotten is sufficiently localized to K critical layers. Evidence: Communication reduction demonstrated across multiple datasets and architectures.

## Foundational Learning

- **Federated Averaging (FedAvg)**: FUSED uses FedAvg for adapter aggregation. Understanding how local updates combine into global parameters is essential for debugging convergence. Quick check: Can you explain why FedAvg requires multiple communication rounds to converge, and how non-IID data affects aggregation?

- **First-Order Taylor Expansion of Loss Landscapes**: The theoretical justification for knowledge overwriting relies on linear approximation of loss near optimal parameters. Quick check: Under what conditions does a first-order Taylor approximation break down for neural network loss functions?

- **Parameter-Efficient Fine-Tuning (Adapters/LoRA)**: FUSED's sparse adapters follow the same design philosophy as PEFT methods—modifying behavior without changing base weights. Quick check: Why do adapter-based methods typically freeze base model parameters, and what are the tradeoffs vs. full fine-tuning?

## Architecture Onboarding

- **Component map**: Server -> Global model M_r, CLI analysis, adapter distribution; Clients -> Frozen M_r + sparse A_f, train A_f on D_r, upload A_f; Unlearning layers L_f -> Top-K sensitive layers; Frozen layers L_r -> All non-critical layers; Sparse adapters A_f -> Random dropout masks

- **Critical path**: 1) Run 1 federated iteration → collect client models → compute Dif f_l for all layers → generate ranked list L_S; 2) Select top-K layers → create sparse masks → distribute A_f to clients with D_r; 3) For I iterations: clients freeze M_r, merge A_f with L_f locally, train on D_r, upload A_f only; 4) Server aggregates A_f via FedAvg; 5) Final unlearning model M_f = M_r + A_f

- **Design tradeoffs**: K (number of critical layers) - higher K → better unlearning, higher cost; Sparsity level p - higher p → more adapter capacity, higher communication; Training iterations I - more iterations → better overwriting, higher computational cost

- **Failure signatures**: Indiscriminate unlearning - RA drops significantly → CLI selected wrong layers or gradient opposition insufficient; Incomplete forgetting - FA remains high → K too low or I insufficient; No reversibility - ReA stays high after adapter removal → adapters modified frozen layers; MIA remains high - privacy leakage persists → unlearning wasn't effective

- **First 3 experiments**: 1) CLI ablation - compare FUSED with CLI vs. random layer selection, verify CLI improves RA by 5-15%; 2) Layer sensitivity profiling - on your dataset, run CLI after 1 federated round, plot Dif f values, confirm later layers show highest sensitivity; 3) Communication-accuracy tradeoff curve - vary sparsity (90%, 95%, 99%) and K (2, 4, 6), plot RA/FA vs. communication cost

## Open Questions the Paper Calls Out

- Can FUSED maintain unlearning effectiveness when the remaining dataset (D_r) is extremely limited or sparse, without relying on the current assumption of abundant residual data? The paper notes it still requires a great number of remaining data to train the adapters and suggests data compression techniques might solve this problem.

- Can the unlearning process be decoupled from the active participation of all remaining clients to reduce the high coordination overhead required by the current design? The authors note FUSED requires participation of all clients containing residual knowledge, demanding higher client engagement.

- How does the Critical Layer Identification (CLI) method perform in highly complex models where knowledge sensitivity is distributed across many layers rather than localized in the final layers? The evaluation is limited to ResNet18, LeNet, and a small Transformer.

## Limitations

- Requires abundant remaining data for adapter training, with no exploration of low-data regimes
- Demands participation of all clients containing residual knowledge, creating high coordination overhead
- Assumes knowledge is localized in final layers, which may not hold for deeper or more complex architectures

## Confidence

- **High Confidence**: Modular adapter architecture enabling reversibility and cost reduction - well-established in PEFT literature
- **Medium Confidence**: Layer sensitivity analysis (CLI) method - theoretically sound but depends on unknown sparsity and iteration parameters
- **Medium Confidence**: Gradient opposition mechanism for knowledge overwriting - theoretically justified but requires specific conditions (Φ < 0)

## Next Checks

1. **Parameter Sensitivity Analysis**: Reproduce CLI ablation study by comparing FUSED with CLI vs. random layer selection on your target dataset, measuring RA improvement and verifying that later layers show highest sensitivity.

2. **Communication-Accuracy Tradeoff**: Systematically vary adapter sparsity (90%, 95%, 99% dropout) and number of critical layers (K=2, 4, 6), plotting RA/FA vs. communication costs to identify optimal deployment parameters.

3. **Gradient Direction Validation**: For your specific forgotten/remaining task pairs, measure actual cosine similarity between gradients to confirm whether Φ < 0, validating the theoretical foundation for knowledge overwriting.