---
ver: rpa2
title: Sign-Symmetry Learning Rules are Robust Fine-Tuners
arxiv_id: '2502.05925'
source_url: https://arxiv.org/abs/2502.05925
tags:
- learning
- methods
- sign-symmetry
- attacks
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a hybrid training approach that combines standard
  backpropagation (BP) for pre-training with fine-tuning using sign-symmetry learning
  rules (uSF, frSF, brSF). These rules approximate gradient updates by using only
  the sign of weight matrices, reducing reliance on exact gradient computations.
---

# Sign-Symmetry Learning Rules are Robust Fine-Tuners

## Quick Facts
- arXiv ID: 2502.05925
- Source URL: https://arxiv.org/abs/2502.05925
- Authors: Aymene Berriche; Mehdi Zakaria Adjal; Riyadh Baghdadi
- Reference count: 40
- Primary result: Sign-symmetry fine-tuning achieves competitive accuracy while significantly enhancing robustness to white-box adversarial attacks

## Executive Summary
This work introduces a hybrid training approach that combines standard backpropagation (BP) for pre-training with fine-tuning using sign-symmetry learning rules (uSF, frSF, brSF). These rules approximate gradient updates by using only the sign of weight matrices, reducing reliance on exact gradient computations. The method is evaluated on image classification and hashing-based retrieval tasks using multiple backbones (AlexNet, VGG-16, ResNet-18) and datasets (CIFAR-10, MS-COCO, NUS-WIDE, ImageNet). Across all settings, sign-symmetry fine-tuning achieves accuracy and mean average precision close to BP, often matching or exceeding it. Crucially, the approach significantly enhances robustness to gradient-based adversarial attacks (FGSM, PGD, HAG, SDHA), with accuracy and retrieval performance degrading much more slowly under perturbations compared to BP-only fine-tuning. Black-box attacks show no specific disadvantage for sign-symmetry, confirming the robustness improvement is tied to gradient obfuscation. The findings demonstrate that integrating biologically inspired learning rules into standard training pipelines can yield both competitive performance and improved adversarial resilience.

## Method Summary
The method uses standard backpropagation to pre-train backbones on ImageNet, then fine-tunes on downstream tasks using sign-symmetry learning rules that replace exact gradient computations with approximate feedback based on sign(W^T). Three variants are explored: uSF (uses only sign of weights), frSF (adds fixed random magnitude), and brSF (redraws random magnitude per batch). The approach is evaluated on image classification and retrieval tasks, comparing against standard backpropagation fine-tuning.

## Key Results
- Sign-symmetry fine-tuning matches or exceeds BP performance on CIFAR-10 classification (ResNet-18) and retrieval tasks across multiple datasets
- Sign-symmetry methods show dramatically improved robustness to white-box adversarial attacks (FGSM, PGD, HAG, SDHA), with accuracy dropping far more slowly than BP under increasing perturbation strength
- Black-box attacks (Boundary, HSJA) show no significant robustness advantage for sign-symmetry, supporting the gradient-specificity of the defense
- frSF variant consistently outperforms uSF and brSF, providing the best balance of stability and performance

## Why This Works (Mechanism)

### Mechanism 1: Gradient Approximation Obfuscates White-Box Attacks
- **Claim:** Sign-symmetry fine-tuned models resist gradient-based attacks because the training process uses approximate, not exact, gradients.
- **Mechanism:** White-box attacks (FGSM, PGD, HAG, SDHA) compute adversarial perturbations by exploiting the model's exact gradient. Sign-symmetry methods compute error signals using `V = sign(W^T)` (uSF), `V = M ◦ sign(W^T)` with fixed random M (frSF), or batch-redrawn M (brSF). This decouples the backward-pass gradient structure from the forward weights, creating a mismatch between what the attacker computes and what the model "expects."
- **Core assumption:** The robustness stems from gradient misalignment, not from fundamentally different decision boundaries. Assumption: the attack's optimization trajectory is disrupted by non-symmetric feedback.
- **Evidence anchors:**
  - [abstract]: "sign-symmetry methods reduce weight transport and introduce approximate gradients, enhancing robustness"
  - [section 6.2.1]: "BP accuracy decreases drastically compared to Sign-Symmetry methods... BP's accuracy drops to 0 rapidly at ε = 0.1" while sign-symmetry retains substantial accuracy
  - [corpus]: Weak—corpus papers focus on biological plausibility and credit assignment, not adversarial robustness specifically. No direct validation of the gradient-mismatch hypothesis.
- **Break condition:** If black-box attacks showed similar robustness gaps, the mechanism would be incorrect. The paper reports no significant difference in black-box attacks (Tables 3-4), which supports—but does not prove—the gradient obfuscation hypothesis.

### Mechanism 2: BP Pre-training + Sign-Symmetry Fine-tuning Separation
- **Claim:** Combining BP for representation learning with sign-symmetry for task adaptation preserves accuracy while adding robustness.
- **Mechanism:** BP excels at learning rich feature representations during pre-training on large datasets (ImageNet). Sign-symmetry, which underperforms BP when training from scratch, is sufficient for task adaptation (transfer learning) while introducing robustness-inducing gradient noise.
- **Core assumption:** Sign-symmetry can match BP performance during fine-tuning because the backbone representations are already well-formed. Assumption: the robustness benefit does not require re-learning features from scratch.
- **Evidence anchors:**
  - [section 1]: "BP demonstrates superior capability in representation learning during the pre-training phase... both BP and Sign-Symmetry methods achieve comparable results in task adaptation during fine-tuning"
  - [section 6.1, Table 1-2]: Sign-symmetry methods perform comparably or outperform BP in 6/12 retrieval benchmarks and multiple classification settings
  - [corpus]: Moderate—Bartunov et al. (2018) is cited showing BP outperforms FA from scratch; this paper leverages that insight to avoid training from scratch.
- **Break condition:** If sign-symmetry fine-tuning degraded backbone features significantly, accuracy would drop below BP baselines. Results show comparable performance, suggesting this doesn't occur.

### Mechanism 3: Weight Transport Reduction Breaks Gradient Symmetry
- **Claim:** Reducing weight transport (using only sign or sign+random magnitude for feedback) disrupts the precise gradient structure attackers rely on.
- **Mechanism:** Backpropagation uses `δ_l = (W^T_{l+1} δ_{l+1}) ◦ f'(a_l)`—full weight symmetry. Sign-symmetry replaces `W^T` with `sign(W^T)` or `M ◦ sign(W^T)`. This reduces "precision of synaptic change in reducing error" (Figure 1), making gradients less informative for attackers while remaining sufficient for learning.
- **Core assumption:** Gradient-based attacks are sensitive to the precision of gradient information; approximate gradients provide less exploitable signal.
- **Evidence anchors:**
  - [section 3.1]: Formal definition shows uSF uses only sign(W^T), frSF adds fixed random magnitude, brSF redraws magnitude per batch
  - [section 6.2.2, Tables 3-4]: Black-box attacks (Boundary, HSJA) show no consistent robustness advantage, supporting that the benefit is gradient-specific
  - [corpus]: Weak—no corpus papers directly test weight transport vs adversarial robustness
- **Break condition:** If black-box attacks showed significant robustness differences, the mechanism would extend beyond gradient obfuscation. The paper finds no such difference.

## Foundational Learning

- **Concept: Backpropagation and the Weight Transport Problem**
  - Why needed here: The paper's intervention directly targets BP's biological implausibility—using identical weights for forward and backward passes. Understanding this clarifies why sign-symmetry is both plausible and robust.
  - Quick check question: Why is using `W^T` for the backward pass considered biologically implausible, and how does sign-symmetry approximate it?

- **Concept: White-Box vs Black-Box Adversarial Attacks**
  - Why needed here: The robustness claim is specific to gradient-based (white-box) attacks. Distinguishing FGSM/PGD (gradient-dependent) from Boundary/HSJA (query-based) is essential to interpret the results correctly.
  - Quick check question: If a model resists PGD but not Boundary Attack, what does that suggest about the source of robustness?

- **Concept: Credit Assignment and Feedback Alignment**
  - Why needed here: Sign-symmetry is positioned between full backpropagation and feedback alignment. Understanding where it sits on the "precision of synaptic change" spectrum (Figure 1) explains the tradeoff between learning efficiency and robustness.
  - Quick check question: What information does frSF preserve that pure Feedback Alignment (FA) discards, and why might this matter for fine-tuning?

## Architecture Onboarding

- **Component map:**
  - Pre-trained backbone (AlexNet / VGG-16 / ResNet-18) -> Task-specific head (classification or hashing) -> Sign-symmetry feedback mechanism -> Loss function

- **Critical path:**
  1. Load ImageNet-pretrained backbone (torchvision weights)
  2. Replace final layer with task-appropriate head
  3. Configure optimizer: Adam (β₁=0.9, β₂=0.999), weight decay 0.0005
  4. Set learning rates: 10⁻⁵ for backbone, 10⁻⁴ for task head
  5. **Replace backward pass**: Compute `δ_l` using sign-symmetry feedback matrix instead of `W^T`
  6. Fine-tune 10-20 epochs (don't freeze backbone)
  7. Evaluate on clean data + adversarial attacks (FGSM, PGD with ε ∈ [0, 0.5])

- **Design tradeoffs:**
  - **uSF vs frSF vs brSF**: Paper recommends frSF as most stable and performant. brSF adds more randomness but may be less stable. uSF is simplest but less performant in some settings.
  - **Learning rate split**: Backbone uses 10× lower LR than task head. If backbone LR is too high, features may degrade; too low, and fine-tuning may be ineffective.
  - **Fine-tuning epochs**: 10 epochs for ImageNet/NUS-WIDE, 20 for others. Longer may not improve robustness further.
  - **Assumption**: The robustness benefit requires fine-tuning all layers, not just the head.

- **Failure signatures:**
  - **Accuracy significantly below BP baseline**: Learning rate mismatch or unstable sign-symmetry variant. Try frSF, verify LR scaling.
  - **Robustness unchanged from BP**: Confirm sign-symmetry backward pass is actually being used (not accidentally using standard autograd).
  - **Training divergence**: brSF may be too unstable; switch to frSF.
  - **Black-box attack shows robustness gap**: This would contradict paper's mechanism—verify attack implementation.

- **First 3 experiments:**
  1. **Replicate CIFAR-10 classification (ResNet-18)**: Fine-tune with BP vs frSF. Test PGD attack (ε=0.1, 5 iterations). Expected: frSF maintains ~50%+ accuracy, BP drops near 0.
  2. **Ablation: gradient-computation attack**: Fine-tune with frSF, but attack using BP-computed gradients (not sign-symmetry). If robustness disappears, confirms gradient mismatch is the mechanism.
  3. **Generalization test**: Apply to a modern backbone (e.g., EfficientNet or ConvNeXt) on a different dataset (e.g., CIFAR-100). Verify robustness gains transfer beyond AlexNet/VGG/ResNet.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact mechanism by which sign-symmetry gradients resist white-box attacks remains inferred, not proven
- Generalization to larger, more diverse architectures (e.g., ConvNeXt, ViT) is untested
- Whether robustness transfers to other attack types (reprogramming, universal perturbations) is unknown

## Confidence
- Robustness to white-box attacks: Medium (strong empirical evidence, plausible mechanism)
- Competitive clean accuracy: High (directly verified across multiple settings)
- Biological plausibility as a side benefit: Low (not the paper's focus, minimal validation)

## Next Checks
1. Ablation: Fine-tune with frSF, attack using BP-computed gradients. If robustness disappears, confirms gradient mismatch is the mechanism.
2. Generalization test: Apply to a modern backbone (e.g., EfficientNet or ConvNeXt) on CIFAR-100. Verify robustness gains transfer beyond AlexNet/VGG/ResNet.
3. Cross-task robustness: Fine-tune for object detection or semantic segmentation. Test if sign-symmetry fine-tuning provides robustness benefits beyond image classification and retrieval.