---
ver: rpa2
title: 'HEP-JEPA: A foundation model for collider physics using joint embedding predictive
  architecture'
arxiv_id: '2502.03933'
source_url: https://arxiv.org/abs/2502.03933
tags:
- hep-jepa
- physics
- learning
- dataset
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HEP-JEPA, a transformer-based foundation model
  for collider physics tasks, particularly jet classification at the Large Hadron
  Collider. The model uses a Joint Embedding Predictive Architecture (JEPA) paradigm
  to self-supervised pre-train on the JetClass dataset containing 100 million jet
  samples.
---

# HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture

## Quick Facts
- arXiv ID: 2502.03933
- Source URL: https://arxiv.org/abs/2502.03933
- Authors: Jai Bardhan; Radhikesh Agrawal; Abhiram Tilak; Cyrin Neeraj; Subhadip Mitra
- Reference count: 23
- Primary result: Transformer-based JEPA foundation model achieves 4-6% better few-shot accuracy than training from scratch on collider physics tasks

## Executive Summary
This paper introduces HEP-JEPA, a transformer-based foundation model for collider physics that uses Joint Embedding Predictive Architecture (JEPA) for self-supervised pre-training on jet data. The model learns transferable representations by predicting embeddings of unseen jet constituents from context constituents, eliminating the need for decoder-based reconstruction. HEP-JEPA demonstrates strong performance on few-shot learning tasks, achieving 4-6% better accuracy than models trained from scratch when using 0.05-0.5% of available labels, and shows competitive performance on downstream tasks including top tagging and quark-gluon jet discrimination.

## Method Summary
HEP-JEPA uses a transformer architecture with 12 layers and 2.5M parameters, pre-trained on the JetClass dataset containing 100 million jet samples using a JEPA paradigm. The model employs contiguous masking in the (η, ϕ) plane to sample target and context jet constituents, predicting target embeddings from context embeddings using a predictor MLP. A physics bias matrix encoding angular distances and momentum correlations is injected into transformer attention. The model is pre-trained for 4 epochs using smooth L1 loss in embedding space, then fine-tuned on downstream tasks like top tagging and quark-gluon discrimination with minimal task-specific training data.

## Key Results
- HEP-JEPA achieves 4-6% better accuracy than training from scratch in few-shot learning with 0.05-0.5% labels
- The model shows competitive performance on downstream tasks: top tagging accuracy 0.929 and quark-gluon jet discrimination accuracy 0.821
- Contiguous masking outperforms random masking for jet structure learning
- Physics bias matrix and register tokens provide approximately 2% accuracy improvement each according to ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Predicting in latent embedding space rather than reconstructing raw inputs yields representations that transfer better across downstream tasks with limited labels.
- **Mechanism:** JEPA operates entirely in representation space—context encoder processes visible jet constituent patches, predictor forecasts target patch embeddings directly, and smooth L1 loss compares predictions against EMA-teacher-encoded targets. This bypasses decoder-induced biases toward input-space minutiae.
- **Core assumption:** Semantic regularities in jet physics are better captured as abstract embeddings than as pixel/point-level reconstructions.
- **Evidence anchors:** [abstract] "the model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen target constituents"; [section 1] "JEPA learns predictive representations by modelling missing or unseen embeddings directly in the latent space without a decoder or full input reconstruction"

### Mechanism 2
- **Claim:** Physics-derived attention biases encode relational priors (angular distances, momentum correlations) that accelerate learning of physically meaningful representations.
- **Mechanism:** Bias terms—∆R (angular separation), kT (transverse momentum × distance), z (momentum fraction), m² (invariant mass squared)—are computed from summed four-vectors of particle groups and injected into transformer attention logits before softmax.
- **Core assumption:** Jet constituent relationships follow Lorentz-invariant dynamics that are not automatically learned by generic attention from limited data.
- **Evidence anchors:** [section 3.2.2] "We also introduce a physics bias matrix from Ref. (Qu et al., 2022a); however, our implementation differs since we calculate pairwise interactions between tokens (groups) instead of particles"; [section 6.2, Table 6] Ablation shows ≈2% accuracy gain with physics bias enabled (0.570 vs 0.557)

### Mechanism 3
- **Claim:** Contiguous, spatially coherent masking in the (η, ϕ) plane forces the model to learn global jet structure rather than memorizing local particle patterns.
- **Mechanism:** Greedy sequencer orders tokens by proximity; contiguous blocks are sampled as targets (scale 0.15–0.2) and context (scale 0.4–0.75); overlapping regions removed to prevent trivial prediction.
- **Core assumption:** Jet substructure exhibits spatial coherence—particle groups carry correlated physics information.
- **Evidence anchors:** [section 3.2.1] "we also utilise a greedy sequencing algorithm to ensure spatial coherence when performing the contiguous masking"; [section 6.1, Table 4] Contiguous masking (accuracy 0.581 with 1 target) outperforms random masking (0.579)

## Foundational Learning

- **Concept: Joint Embedding Predictive Architecture (JEPA)**
  - **Why needed here:** JEPA replaces reconstruction-based SSL with embedding prediction, reducing computational overhead and focusing on semantic features. Understanding this distinction is prerequisite to grasping why HEP-JEPA avoids decoders entirely.
  - **Quick check question:** Can you explain why predicting embeddings might fail if the encoder collapses to constant outputs, and how an EMA teacher mitigates this?

- **Concept: Transformer Attention with Physics Priors**
  - **Why needed here:** The model injects physics-derived bias terms into attention, assuming students understand standard scaled dot-product attention first.
  - **Quick check question:** Given standard attention Attention(Q, K, V) = softmax(QK^T/√d)V, where would you add a bias matrix B encoding ∆R distances?

- **Concept: Jet Physics Fundamentals (pT, η, ϕ, Anti-kT Clustering)**
  - **Why needed here:** Input features include pseudorapidity (η), azimuth (ϕ), transverse momentum (pT), and jet clustering uses anti-kT algorithm. Without this, the tokenization and bias terms are opaque.
  - **Quick check question:** Why might ∆R = √((η₁-η₂)² + (ϕ₁-ϕ₂)²) be a sensible distance metric for particle jets in a collider detector?

## Architecture Onboarding

- **Component map:** Input jet (n particles × 7 features) → Tokenizer → c tokens → Sequencer → Mask into context/target blocks → Encode context → Predict target embeddings → Compare with EMA-encoded targets → Backprop through context encoder + predictor only

- **Critical path:** Input jet (n particles × 7 features) → Tokenizer → c centers → k-NN groups → PointNet encoder → c tokens → Sequencer → Mask into context/target blocks → Encode context → Predict target embeddings → Compare with EMA-encoded targets → Backprop through context encoder + predictor only

- **Design tradeoffs:**
  - **Token count (c):** More tokens capture finer structure but increase compute; FPS samples adaptively based on jet size
  - **Context/target ratio:** Paper finds context [0.4, 0.75] and target [0.15, 0.2] scales work best; too much context → trivial prediction, too little → insufficient signal
  - **Physics bias vs. registers:** Both add ≈2% accuracy independently (ablation Tables 6–7); combined effect not explicitly reported

- **Failure signatures:**
  - **Representation collapse:** All outputs converge to constant vectors; detect by monitoring embedding variance across batches
  - **Trivial prediction:** Loss drops too fast with high context/target overlap; verify disjoint masking enforced
  - **Overfitting to local features:** Random masking outperforms contiguous; check sequencer implementation

- **First 3 experiments:**
  1. **Ablate physics bias:** Train with and without bias matrix; expect ~2% accuracy gap per Table 6; confirm bias terms correctly computed from group four-vectors
  2. **Vary target count:** Test 1, 4, 8 targets per sample per Table 4; validate that performance is insensitive (paper finds no significant difference)
  3. **Few-shot frozen vs. fine-tuned:** Using 0.5% labels, compare frozen backbone (only head trained) vs. full fine-tuning; expect fine-tuned to win by 3–4% per Table 1

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can HEP-JEPA be effectively extended to generative tasks and full collision event classification?
- **Basis in paper:** [explicit] The conclusion states that "extending HEP-JEPA to generative tasks and event classification is the next step to make it a truly cross-task FM for collider physics."
- **Why unresolved:** The current architecture predicts embeddings in latent space for discriminative tagging tasks, lacking the decoder necessary for generating physical jet samples or processing entire collision events.
- **What evidence would resolve it:** Demonstration that HEP-JEPA representations can successfully condition a decoder to generate high-fidelity jet samples or classify full events comparably to specialized models.

### Open Question 2
- **Question:** Does HEP-JEPA generalize to unsupervised tasks like detector unfolding and weakly supervised anomaly detection?
- **Basis in paper:** [explicit] The authors explicitly suggest that "Evaluating HEP-JEPA on other tasks, such as unfolding detector measurements... and anomaly detection using weakly supervised methods, could further validate its generalisability."
- **Why unresolved:** The paper currently evaluates only supervised classification (tagging); the model's ability to reconstruct truth-level kinematics from detector data or identify rare signals without labels is unknown.
- **What evidence would resolve it:** Benchmarking the pre-trained model on standard unfolding datasets (e.g., OmniFold) and the LHCOlympics anomaly detection dataset.

### Open Question 3
- **Question:** Can HEP-JEPA close the absolute performance gap with state-of-the-art supervised baselines like Particle Transformer (ParT)?
- **Basis in paper:** [inferred] The conclusion acknowledges "scopes for improvement compared to other state-of-the-art task-specific methods like ParT," as Tables 2 and 3 show ParT outperforms HEP-JEPA on top and quark-gluon tagging.
- **Why unresolved:** It is unclear if the abstract representations learned via self-supervision inherently lack the fine-grained detail needed to surpass fully supervised models on specific tasks, even with scaling.
- **What evidence would resolve it:** Experiments scaling model size and pre-training duration to see if the performance difference vanishes or persists, indicating a fundamental limit of the JEPA objective in this domain.

## Limitations
- Dataset and generalization: Results based on simulation (DELPHES) rather than real LHC collision data
- Architecture specification gaps: Key hyperparameters like embedding dimension and optimizer settings not fully specified
- Comparative analysis limitations: Lacks direct comparison to established SSL methods like contrastive learning

## Confidence

**High Confidence:**
- JEPA architecture can be successfully applied to jet physics problems
- Contiguous masking performs better than random masking for jet structure learning
- Physics bias terms provide measurable improvements to classification accuracy
- Few-shot learning performance exceeds training from scratch across tested label fractions

**Medium Confidence:**
- JEPA outperforms traditional SSL methods in this domain (limited comparative evidence)
- The specific masking scales ([0.15, 0.2] for targets, [0.4, 0.75] for context) are optimal (based on single ablation study)
- The 4-6% improvement in few-shot learning is consistent across different downstream tasks (tested on limited task set)

**Low Confidence:**
- JEPA will maintain advantages when scaled to even larger datasets (beyond 100M jets)
- The model will generalize robustly to real LHC data without further adaptation
- The current architecture represents the optimal JEPA design for collider physics (no comprehensive architecture search reported)

## Next Checks

1. **Real Data Validation:** Test HEP-JEPA on a real LHC dataset (such as public CMS or ATLAS open data) to verify that simulation-trained representations transfer to actual collision data.

2. **Direct SSL Comparison:** Implement and compare against established self-supervised learning methods (such as contrastive learning, masked autoencoders, or momentum contrast) on identical JetClass data splits and downstream tasks.

3. **Bias Term Ablation:** Conduct a systematic ablation study isolating each physics bias term (∆R, kT, z, m²) individually to determine which components drive the observed performance improvements.