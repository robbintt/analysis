---
ver: rpa2
title: Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability
  in Financial Time Series
arxiv_id: '2509.04921'
source_url: https://arxiv.org/abs/2509.04921
tags:
- time
- series
- chaotic
- training
- predictive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for forecasting financial time series
  by generating artificial chaotic time series through resampling and using these
  as training data for large-scale pre-training. The approach employs the Lorenz model
  to simulate price, volume, and order flow dynamics, with predictive horizons extended
  by increasing the resampling interval.
---

# Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series

## Quick Facts
- **arXiv ID:** 2509.04921
- **Source URL:** https://arxiv.org/abs/2509.04921
- **Authors:** Yuki Takemoto
- **Reference count:** 18
- **Primary result:** A scaling law-like phenomenon where predictive performance at extended horizons can be achieved by exponentially increasing training samples.

## Executive Summary
This paper presents a novel approach to financial time series forecasting by generating synthetic chaotic time series through resampling the Lorenz model and using these as training data for large-scale pre-training. The method demonstrates that models trained on artificial chaotic dynamics can achieve zero-shot transfer to real Bitcoin trade data, outperforming traditional autocorrelation models. A key finding is the observation of a scaling law: maintaining predictive performance at longer forecasting horizons requires exponentially more training samples. The work suggests that near-future events in chaotic systems might be predictable with sufficient computational resources, potentially opening new avenues for financial forecasting.

## Method Summary
The method generates synthetic chaotic time series by numerically integrating the Lorenz equations with perturbed parameters (σ∈[9,11], ρ∈[26,30], β∈[2.3,3.1]) at fine intervals (Δt=0.01) and resampling at longer intervals to simulate financial observations. These synthetic series are used to pre-train decoder-only Transformer models (1M-10M parameters) with a single epoch over 10 billion samples per horizon. The trained models are then evaluated zero-shot on Bitcoin trade data aggregated to various timeframes (5-60 seconds), with performance measured by excess returns against an autocorrelation baseline. The approach relies on an assumed structural correspondence between Lorenz variables and financial indicators (order flow, price change rate, volume).

## Key Results
- Models pre-trained on synthetic chaotic time series achieved superior performance to autocorrelation models when predicting Bitcoin trade data across multiple timeframes (5-60 seconds)
- A scaling law-like phenomenon was observed: predictive performance at extended horizons can be maintained by exponentially increasing training samples
- The model successfully reconstructed Lorenz attractor shapes in its predictions, validating learning of chaotic dynamics
- Zero-shot transfer performance was best at the 15-second timeframe for Bitcoin data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Resampling chaotic time series at longer intervals simulates financial time series while preserving underlying attractor dynamics.
- **Mechanism:** The Lorenz model generates continuous chaotic trajectories at fine integration time (Δt=0.01). Resampling every N intervals creates discrete observations where autocorrelation decreases with interval length, mirroring how real financial series arise from unobserved microstructure dynamics.
- **Core assumption:** Financial time series possess chaotic properties at microstructure time scales that are "discretized out" during observation.
- **Evidence anchors:**
  - [abstract] "generating artificial chaotic time series through resampling and using these as training data"
  - [Section 1, page 3] "We propose that chaotic properties exist at the microstructure level... and that financial time series are formed through discrete observations"
  - [corpus] Panda (arXiv:2505.13755) similarly pre-trains on chaotic dynamics but uses evolutionary algorithm-generated equations rather than resampling.
- **Break condition:** If financial series are fundamentally stochastic (not discretized chaos), the synthetic-to-real transfer degrades.

### Mechanism 2
- **Claim:** Predictive performance at extended horizons can be maintained by exponentially increasing training samples—a scaling law-like phenomenon.
- **Mechanism:** Longer resampling intervals reduce autocorrelation, making prediction harder. The model compensates by seeing exponentially more diverse contexts, gradually reconstructing the attractor in prediction space.
- **Core assumption:** The Lorenz attractor's structure is learnable given sufficient coverage of state space through random parameter perturbations.
- **Evidence anchors:**
  - [abstract] "predictive performance at a certain level for extended horizons can be achieved by exponentially increasing training samples"
  - [Section 3.2, page 14] Figure 9 shows training samples required to reach correlation 0.1 grows exponentially with predictive horizon
  - [corpus] No direct corpus precedent for this horizon-specific scaling law; LLM scaling laws focus on model size/data volume, not temporal horizon.
- **Break condition:** If the relationship is not exponential (e.g., polynomial or saturating), extrapolation fails at longer horizons.

### Mechanism 3
- **Claim:** Zero-shot transfer to financial series works because the Lorenz variables (x, y, z) structurally correspond to order flow, price change rate, and volume.
- **Mechanism:** Economic interpretation maps x→order flow (sentiment), y→price change rate (trading signal), z→volume (overheating). The model learns cross-variable dynamics that generalize despite financial noise.
- **Core assumption:** The Lorenz equations capture sufficient structural similarity to market microstructure dynamics.
- **Evidence anchors:**
  - [Section 2.4, page 9-10] Explicit mapping of Lorenz equations to economic interpretations with parameter meanings (σ=reaction speed, ρ=sentiment amplification, β=volume decay)
  - [Section 3.3, page 15] Zero-shot predictions on Bitcoin data outperform autocorrelation model, with best results at 15-second timeframe
  - [corpus] Weak direct evidence; Kronos and DELPHYNE papers address financial TSFMs but use real financial data, not chaotic pre-training.
- **Break condition:** If the economic mapping is spurious, transfer performance should not exceed simple autocorrelation baselines.

## Foundational Learning

- **Concept: Chaotic dynamics and Lyapunov exponents**
  - Why needed here: Understanding why prediction difficulty increases with horizon—chaotic systems have sensitive dependence on initial conditions, meaning small errors grow exponentially.
  - Quick check question: Can you explain why a system with positive Lyapunov exponent becomes harder to predict further into the future?

- **Concept: Attractor reconstruction in state space**
  - Why needed here: The paper evaluates learning progress by observing whether the model reconstructs the Lorenz attractor shape in its predictions.
  - Quick check question: What does it mean visually if a model's predictions gradually "fill in" an attractor shape?

- **Concept: Zero-shot transfer learning**
  - Why needed here: The core claim is that pre-training on synthetic chaos transfers to real financial data without retraining.
  - Quick check question: Why is zero-shot transfer harder for time series than for language domains?

## Architecture Onboarding

- **Component map:** Lorenz parameter sampling -> Numerical integration -> Resampling at interval N -> Synthetic financial series -> Model forward pass with causal attention -> MSE loss summed across x,y,z

- **Critical path:**
  1. Lorenz parameter sampling (σ, ρ, β perturbations)
  2. Numerical integration → raw chaotic trajectory
  3. Resampling at interval N → synthetic financial series
  4. Model forward pass with causal attention
  5. MSE loss summed across 3 dimensions

- **Design tradeoffs:**
  - Model size: 1M vs 10M shows minimal loss difference (suggests 3D state space is information-bottleneck, not model capacity)
  - Training samples vs horizon: Exponential sample scaling makes long horizons computationally expensive
  - Single epoch prevents overfitting but requires massive data generation throughput

- **Failure signatures:**
  - Predictions collapse to near-zero values (attractor not forming)
  - Correlation coefficient plateaus below target (insufficient samples for horizon)
  - Zero-shot transfer underperforms autocorrelation (economic mapping invalid)

- **First 3 experiments:**
  1. **Baseline validation:** Train Model 1M on horizon 100 with 1B samples; verify attractor reconstruction and correlation >0.1 on held-out Lorenz data.
  2. **Scaling curve estimation:** Train horizons [100, 300, 500] with sample counts [100M, 1B, 10B]; fit exponential relationship to confirm scaling law.
  3. **Transfer sanity check:** Evaluate zero-shot on Bitcoin 5-second data; compare excess returns vs autocorrelation. If negative, revisit variable mapping.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed scaling law (exponential increase in training samples maintaining performance for extended horizons) generalize to chaotic systems other than the Lorenz model?
- **Basis in paper:** [explicit] The conclusion states that "validity of this law across multiple chaotic models" is a key verification point, noting that reference [4] confirmed scaling with diversity rather than sample count.
- **Why unresolved:** The current study exclusively utilized the Lorenz model to simulate price, volume, and order flow dynamics.
- **What evidence would resolve it:** Replicating the pre-training experiments using other chaotic generators (e.g., Henon map, Mackey-Glass) to observe if the exponential sample-horizon relationship persists.

### Open Question 2
- **Question:** Can predictive horizons beyond 1000 unit times achieve acceptable accuracy (e.g., correlation > 0.1) by scaling training samples to 100 billion or more?
- **Basis in paper:** [explicit] The author notes that for the 1000-unit horizon, the correlation did not reach 0.1 at 10 billion samples, but "no convergence was observed," suggesting 100-200 billion samples might succeed.
- **Why unresolved:** Computational constraints limited the current study to 10 billion samples per horizon.
- **What evidence would resolve it:** Completing the ongoing training runs with 100 billion samples and evaluating correlation coefficients for longer predictive horizons.

### Open Question 3
- **Question:** Why does model performance appear to plateau with increased parameters (around 1M) rather than scaling with model size, as is typical in LLMs?
- **Basis in paper:** [inferred] The results show nearly identical loss curves for 1M and 10M parameter models, contrasting with the clear scaling law observed for training sample counts.
- **Why unresolved:** The paper speculates that the low dimensionality (3 variables) of the chaotic system may suffice for smaller embedding spaces, but this is not verified.
- **What evidence would resolve it:** A systematic ablation study varying model dimensionality against the intrinsic dimensionality of various target chaotic systems.

## Limitations

- The assumption that financial time series are discretized observations of underlying chaotic dynamics at microstructure levels remains empirically unverified
- The computational expense of the approach is prohibitive, requiring billions of training samples for extended horizons
- The economic interpretation mapping between Lorenz variables and financial indicators (order flow, price change, volume) is intuitively motivated but lacks rigorous validation
- Testing is limited to one asset (Bitcoin) over a narrow timeframe, raising questions about generalizability to other markets and economic conditions

## Confidence

**High Confidence (80-100%):** The observation that Lorenz time series generated through resampling at different intervals can be used to train models for time series prediction, and that performance on the Lorenz system itself follows a predictable pattern with respect to training samples and horizon length.

**Medium Confidence (50-80%):** The specific economic interpretation mapping (x→order flow, y→price change rate, z→volume) and the resulting zero-shot transfer performance on Bitcoin data, given the limited scope of empirical validation and potential overfitting to this particular asset and market structure.

**Low Confidence (0-50%):** The generalization of the observed scaling law from synthetic Lorenz data to real financial markets, and the claim that this approach represents a broadly applicable methodology for financial forecasting rather than a narrow special case.

## Next Checks

1. **Scaling Law Verification:** Reproduce the exponential relationship between training samples and predictive horizon using a simplified implementation with horizon values [100, 300, 500] and sample counts [100M, 1B, 10B]. Fit the exponential curve and compare the exponent to the paper's reported value. This validates whether the scaling law is robust to implementation details.

2. **Economic Mapping Stress Test:** Generate synthetic Lorenz data with systematically perturbed parameters (σ, ρ, β) beyond the stated ranges and measure zero-shot transfer performance degradation on Bitcoin data. If performance drops sharply with parameter drift, the economic mapping is fragile and overfit to a narrow chaotic regime.

3. **Alternative Chaotic Generator Comparison:** Replace the Lorenz model with a different chaotic system (e.g., Rössler or Mackey-Glass) while keeping all other aspects constant. Compare zero-shot transfer performance to Bitcoin data. Significant performance differences would indicate the Lorenz model's structural similarity to markets is coincidental rather than fundamental.