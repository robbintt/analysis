---
ver: rpa2
title: Recovering Individual-Level Activity Sequences from Location-Based Service
  Data Using a Novel Transformer-Based Model
arxiv_id: '2508.02734'
source_url: https://arxiv.org/abs/2508.02734
tags:
- activity
- data
- activities
- sequence
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces VSNIT, a transformer-based model that integrates
  the Insertion Transformer's flexible sequence construction with the Variable Selection
  Network's dynamic covariate handling capability, to recover missing activity segments
  in incomplete individual-level sequences while preserving existing data. The model
  outperforms the baseline by inserting more diverse, realistic activity patterns,
  achieving an average of 4.24 daily activities (vs.
---

# Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model

## Quick Facts
- arXiv ID: 2508.02734
- Source URL: https://arxiv.org/abs/2508.02734
- Reference count: 0
- This study introduces VSNIT, a transformer-based model that integrates the Insertion Transformer's flexible sequence construction with the Variable Selection Network's dynamic covariate handling capability, to recover missing activity segments in incomplete individual-level sequences while preserving existing data.

## Executive Summary
This study addresses the challenge of recovering missing activities in individual-level daily activity sequences derived from sparse Location-Based Service (LBS) data. The proposed VSNIT model combines an Insertion Transformer architecture with a Variable Selection Network to insert missing activities at correct positions while preserving existing data. The model leverages time-dependent and static covariates through a gated residual network-based feature selection mechanism, achieving superior performance in recovering realistic and diverse activity patterns compared to baseline approaches.

## Method Summary
The VSNIT model processes incomplete activity sequences by first using a Variable Selection Network (VSN) to dynamically weight relevant covariates per sequence position. The VSN employs Gated Residual Networks (GRNs) with ELU activation, GLU gating, and LayerNorm to select instance-wise variables. These selected features are then combined with activity sequence embeddings and processed through a multi-head self-attention encoder. The insertion decoder predicts (position, activity) pairs iteratively until the sequence is complete. The model was trained on 3,437 samples (0.3% of available data) created by strategically removing activities from complete sequences, with evaluation against a baseline vanilla Insertion Transformer.

## Key Results
- Achieved 4.24 average daily activities (vs. 3.43 for baseline)
- 0.398 average correct insertion position accuracy (vs. 0.253 for baseline)
- Recovered 1,809 correct activities with F1 score of 0.340 (vs. 0.235 for baseline)
- Demonstrated superior accuracy and diversity in activity sequence recovery tasks

## Why This Works (Mechanism)

### Mechanism 1: Non-Autoregressive Insertion for Position-Activity Joint Modeling
The Insertion Transformer backbone enables simultaneous prediction of both *what* activity to insert and *where* to insert it, rather than generating sequences sequentially. At each decoding step, the model evaluates all position pairs in the current sequence and predicts a distribution over (position, token) combinations. This allows insertion at any gap while preserving already-identified activities as anchoring points.

### Mechanism 2: Dynamic Covariate Selection via Gated Residual Networks
The Variable Selection Network adaptively weights static and time-dependent covariates per-sequence-position, filtering noise and amplifying task-relevant context. For each position t, VSN computes softmax selection weights over all covariates using a GRN that takes flattened covariate vectors as input and static covariates as context.

### Mechanism 3: Self-Attention for Long-Range Activity Dependencies
Multi-head self-attention captures relationships between distant activities (e.g., morning home departure informing evening return), which sequential models may lose. Scaled dot-product attention computes pairwise relevance scores across all sequence positions simultaneously, weighted by learnable query/key projections.

## Foundational Learning

- **Concept: Insertion Transformer (Non-Autoregressive Generation)**
  - Why needed here: Unlike standard seq2seq models that generate left-to-right, insertion-based models can fill gaps at arbitrary positions—essential when missing segments occur mid-sequence.
  - Quick check question: Given sequence [Home, _, Home] with one missing activity, would an autoregressive model know *where* to generate the missing token without explicit position encoding?

- **Concept: Variable Selection Network (Feature Gating)**
  - Why needed here: The model receives 10+ covariates (time, mode, demographics); VSN learns which matter for each prediction rather than requiring manual feature engineering.
  - Quick check question: Why might "weekday" be more predictive for "WorkForPay" insertion than "GoShopping"? How would VSN capture this adaptively?

- **Concept: Gated Residual Networks (GLU-based Non-linearity)**
  - Why needed here: GRNs form VSN's building blocks; the GLU (Gated Linear Unit) allows the network to suppress irrelevant signals by learning a gate that can output near-zero values.
  - Quick check question: If a covariate is noise for a particular prediction, how does GLU help the model ignore it?

## Architecture Onboarding

- **Component map:**
  Input Layer: Incomplete activity sequence + static covariates + time-dependent covariates
  ↓
  [VSN Module] → GRN(context=static) → Softmax weights over covariates → Per-covariate GRN → Weighted aggregation
  ↓
  [Encoder] Multi-head self-attention over sequence + covariate embeddings
  ↓
  [Insertion Decoder] Predicts (position, activity) pairs
  ↓
  Output: Completed activity sequence

- **Critical path:**
  1. Covariates → VSN (selects relevant features per position)
  2. Selected features + activity sequence → Encoder attention
  3. Decoder attends to encoder output → predicts insertion distribution
  4. Iterative insertion until convergence or max iterations

- **Design tradeoffs:**
  - **Precision vs. Diversity:** Proposed model shows lower precision (0.165 vs. 0.218) but higher recall (0.140 vs. 0.080), indicating it inserts more aggressively—useful for recovering diverse patterns but risking false positives.
  - **Order-dependent vs. order-independent evaluation:** Position-accurate insertion is harder (F1=0.152) than just predicting correct activities regardless of position (F1=0.340); choose metric based on application needs.
  - **Training data scale:** Only 0.3% of available data (3,437 samples) was used for training; paper claims strong results but generalizability to full dataset is unproven.

- **Failure signatures:**
  - **Over-insertion:** High total inserted activities but low precision → model hallucinates activities.
  - **Mode collapse:** Top-20 insertion distribution dominated by few patterns (see baseline Figure 5b) → covariate handling failure or insufficient training.
  - **Transition disruption:** Inserted activities create implausible transitions (e.g., Home→Home without intermediate activity) → attention failing to capture sequential coherence.

- **First 3 experiments:**
  1. **Ablation study:** Remove VSN (use vanilla Insertion Transformer) to isolate covariate-handling contribution; compare precision/recall/F1.
  2. **Missing-rate sensitivity:** Systematically vary % of activities removed (10%, 30%, 50%, 70%) to characterize recovery degradation curve.
  3. **Covariate importance analysis:** Extract VSN attention weights to identify which covariates drive specific activity predictions; validate against domain knowledge (e.g., weekday → WorkForPay).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the VSNIT model maintain its superior performance and accuracy when applied to regions with different urban structures or mobility cultures outside of Philadelphia County?
- Basis in paper: [explicit] The authors state in the Conclusion that future research should enable the model to be "widely applied in different regions to replicate near-real human activities."
- Why unresolved: The current study validates the model exclusively on data from Philadelphia County, leaving regional generalizability untested.
- What evidence would resolve it: Successful application and validation of VSNIT on LBS datasets from geographically or culturally distinct cities.

### Open Question 2
- Question: To what extent does integrating non-traditional external data sources, such as real-time weather or social media activity, enhance the model's context-awareness and recovery accuracy?
- Basis in paper: [explicit] The Conclusion suggests that "integrating more data sources (e.g., weather or social media)" is a necessary step to further enhance capabilities.
- Why unresolved: The current implementation relies on standard covariates (time, mode, distance, census demographics) and does not incorporate environmental or dynamic social factors.
- What evidence would resolve it: An ablation study showing performance metrics (F1 score, insertion accuracy) improving with the addition of weather or social media covariates.

### Open Question 3
- Question: Does scaling the training dataset from the current 0.3% subset to the full available dataset significantly improve the model's ability to handle edge cases and rare activity patterns?
- Basis in paper: [explicit] The Discussion notes that "scaling up training with larger datasets could boost generalizability" and handle edge cases more effectively.
- Why unresolved: The study utilized a randomly selected subset (3,437 samples) for training, leaving the benefits of full-scale training unexplored due to computational or methodological constraints in this phase.
- What evidence would resolve it: A comparison of model performance on rare activity classes when trained on the full 1.1 million sample dataset versus the subset.

### Open Question 4
- Question: How can the feature engineering be refined to better capture individual-level heterogeneity that the current static and time-dependent covariates fail to capture?
- Basis in paper: [explicit] The Discussion identifies a limitation where "the level of individual-level heterogeneity is still insufficient" and suggests further feature engineering is needed.
- Why unresolved: The current model uses census-tract level demographics as proxies for individual attributes, which may dilute personal nuances.
- What evidence would resolve it: Development of new individual-specific features (e.g., historical frequency) that yield a statistically significant increase in sequence recovery metrics.

## Limitations

- **Dataset representativeness**: The model was trained on only 3,437 samples (0.3% of available data), raising concerns about generalizability.
- **Hyperparameter transparency**: Critical training details including learning rate, batch size, number of layers, attention heads, and optimization strategy are not specified.
- **Activity inference methodology**: The paper states HomeActivity and WorkForPay are inferred but doesn't specify how without NAICS codes.

## Confidence

**VSNIT architecture superiority (overall performance)**: High confidence in the reported F1 improvements and diversity gains, as these are directly measurable from the results presented.

**Dynamic covariate handling importance**: Medium confidence. While the architecture claims VSN provides adaptive feature selection, the paper doesn't present ablation studies comparing VSNIT to Insertion Transformer without VSN.

**Long-range dependency capture**: Medium confidence. Self-attention is well-established, but specific validation that it improves activity sequence recovery over simpler approaches is limited.

**Generalizability and scalability**: Low confidence. The small training sample size and lack of cross-validation or external dataset testing limit claims about broader applicability.

## Next Checks

1. **Ablation study on VSN contribution**: Remove the Variable Selection Network component and retrain the model to isolate its impact on performance. Compare precision, recall, and F1 scores to determine if the complexity is justified.

2. **Sensitivity analysis to training data volume**: Train models using 1%, 5%, 10%, and 50% of the available training data to establish the relationship between sample size and performance, testing the claim that good results can be achieved with minimal data.

3. **External validation on different datasets**: Test the trained model on LBS data from different cities, time periods, or demographic contexts to evaluate robustness and identify potential overfitting to the Philadelphia January 2020 dataset.