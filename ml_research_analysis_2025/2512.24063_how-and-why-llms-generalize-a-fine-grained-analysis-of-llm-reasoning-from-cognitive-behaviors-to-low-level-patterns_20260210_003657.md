---
ver: rpa2
title: 'How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from
  Cognitive Behaviors to Low-Level Patterns'
arxiv_id: '2512.24063'
source_url: https://arxiv.org/abs/2512.24063
tags:
- reasoning
- arxiv
- diagnostic
- retrieval
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a fine-grained benchmark that decomposes\
  \ reasoning into five core cognitive behaviors\u2014calculation, enumeration, simulation,\
  \ fact retrieval, and diagnostic checking\u2014across four domains: mathematics,\
  \ scientific reasoning, coding, and non-reasoning tasks. By tracking these skills\
  \ across supervised fine-tuning (SFT) and reinforcement learning (RL) tuning, the\
  \ study reveals that RL tends to preserve balanced skill profiles and robust generalization,\
  \ whereas SFT often over-specializes, yielding uneven skill gains with spikes in\
  \ narrow areas."
---

# How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns

## Quick Facts
- arXiv ID: 2512.24063
- Source URL: https://arxiv.org/abs/2512.24063
- Reference count: 40
- Key outcome: RL preserves balanced reasoning skills while SFT causes over-specialization; differences linked to training objectives rather than parameter magnitude

## Executive Summary
This work introduces a fine-grained benchmark that decomposes reasoning into five core cognitive behaviors—calculation, enumeration, simulation, fact retrieval, and diagnostic checking—across four domains: mathematics, scientific reasoning, coding, and non-reasoning tasks. By tracking these skills across supervised fine-tuning (SFT) and reinforcement learning (RL) tuning, the study reveals that RL tends to preserve balanced skill profiles and robust generalization, whereas SFT often over-specializes, yielding uneven skill gains with spikes in narrow areas. The differences are linked to training objectives rather than parameter magnitude, as both methods update a similar proportion of model weights. The findings highlight the importance of fostering broad, transferable reasoning skills and point toward behavior-aware training strategies to improve model robustness and interpretability.

## Method Summary
The study fine-tunes Qwen3 models (1.7B, 4B, 14B variants) using both SFT and RL (GRPO) on 47K curated math problems with CoT traces from Qwen3-32B-Instruct. A custom benchmark evaluates five cognitive behaviors across four domains, using embedding-based item selection followed by manual verification. SFT uses LLaMA-Factory with short/long CoT traces; RL uses Verl with answer-correctness rewards. Sparse Autoencoders analyze latent representations. Models are evaluated with temp=0.6, top-p=0.95, and accuracy per behavior is measured.

## Key Results
- RL-tuned models maintain more stable, balanced behavioral profiles across all five cognitive skills, while SFT models exhibit pronounced spikes in single skills (often diagnostic) with dips below baseline in others
- Long CoT supervision partially rebalances SFT skill profiles toward more systematic reasoning by strengthening simulation and diagnostic checking
- Both SFT and RL update approximately 98% of parameters with similar magnitudes, yet produce different behavioral outcomes due to optimization objective differences
- Neither SFT nor RL shows strong cross-domain transfer, though RL preserves better generalization within domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL tuning preserves balanced cognitive skill profiles while SFT induces over-specialization
- Mechanism: RL optimizes for outcome correctness via reward signals, allowing multiple valid reasoning paths to be reinforced. SFT forces the model to mimic specific teacher traces, causing it to latch onto surface heuristics present in those traces rather than learning transferable reasoning primitives.
- Core assumption: The diversity of reasoning paths that lead to correct rewards is broader than the specific traces in SFT data, creating implicit regularization
- Evidence anchors: [abstract] "RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns"; [Page 6] "SFT-tuned models consistently exhibit asymmetric profiles, often showing pronounced spikes in a single skill (most commonly diagnostic) while dipping well below baseline in others"

### Mechanism 2
- Claim: Long chain-of-thought (CoT) supervision during SFT partially rebalances skill profiles toward more systematic reasoning
- Mechanism: Long CoT traces provide richer supervision that explicitly demonstrates simulation, diagnostic checking, and enumeration steps. Short CoT traces tend to emphasize calculation and final answers.
- Core assumption: The content and structure of supervision traces directly shape which cognitive skills are strengthened during fine-tuning
- Evidence anchors: [Page 6-7] "The no-think profile (blue) spikes on calculation while lagging on simulation, diagnostic checking, and enumeration. In contrast, the think models trained with long CoT (orange) are more rounded"

### Mechanism 3
- Claim: Behavioral divergence between SFT and RL arises from optimization objectives, not parameter update magnitude
- Mechanism: Both SFT and RL update approximately 98% of model parameters with comparable magnitudes, yet RL's reward-driven optimization nudges the decision boundary toward correct outputs while allowing internal representations flexibility, whereas SFT's likelihood maximization forces specific token-level predictions.
- Core assumption: The direction of gradient updates matters more than their magnitude for generalization outcomes
- Evidence anchors: [Page 8, Table 2] Shows SFT and RL both change ~97.8% of parameters with similar magnitudes, yet produce different behavioral profiles

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The paper assumes familiarity with CoT as the training signal for both SFT and RL. Understanding that CoT traces are sequences of intermediate reasoning steps is essential to interpret the five cognitive behaviors.
  - Quick check question: Can you explain why a model trained on short CoT traces might develop unbalanced skill profiles compared to one trained on long CoT?

- Concept: Reinforcement Learning from Verifiable Rewards
  - Why needed here: The RL variant uses GRPO (Group Relative Policy Optimization) with answer-correctness rewards. Understanding that rewards are sparse (only final answer) but still shape reasoning paths is crucial.
  - Quick check question: If the reward signal only depends on the final answer correctness, how might RL still influence intermediate reasoning behaviors?

- Concept: Skill Decomposition for Reasoning
  - Why needed here: The paper's central contribution is decomposing "reasoning" into five atomic behaviors. Without this framing, the radar plots and skill balance analysis would be opaque.
  - Quick check question: Why might aggregate accuracy mask weaknesses in specific cognitive skills like enumeration or simulation?

## Architecture Onboarding

- Component map: Qwen3 Base Model -> SFT (LLaMA-Factory) or RL (Verl + GRPO) -> Checkpoint Extraction -> Behavior-Domain Evaluation -> SAE Analysis
- Critical path: 1. Base model selection → 2. SFT or RL fine-tuning on 47K math problems → 3. Checkpoint extraction at multiple training steps → 4. Evaluation on behavior-domain grid → 5. SAE analysis of latent representations
- Design tradeoffs: SFT is faster to converge but risks over-specialization; RL requires more compute (16 rollouts per prompt) but preserves broader skills; long CoT traces improve skill balance but increase training sequence length
- Failure signatures: SFT over-specialization shows jagged radar profiles with narrow spikes and dips below baseline; RL collapse would show unstable training curves or collapsed skills; cross-domain transfer failure shows limited performance on non-training domains
- First 3 experiments: 1. Train Qwen3-4B with both SFT (short CoT) and RL, verify SFT produces jagged profiles while RL produces rounded ones; 2. Ablate CoT length: train SFT variants with short vs long CoT traces and quantify skill profile shifts; 3. Layer-wise SAE probe: train SAEs on layer 16 for base, SFT, and RL models, score SAE features against five cognitive behaviors

## Open Questions the Paper Calls Out

- Can behavior-aware training objectives be explicitly designed to prevent skill collapse during supervised fine-tuning?
- Can lightweight interpretability methods effectively connect low-level statistics to hidden representations without the computational cost of Sparse Autoencoders (SAEs)?
- How do curriculum strategies reinforcing both domain-specific and transferable skills mitigate the cross-domain generalization gaps identified in reasoning models?

## Limitations

- The behavioral benchmark covers only four domains with limited examples per behavior-domain cell (50-200 items), potentially missing edge cases
- Findings are based exclusively on Qwen3 models, raising questions about generalizability to other model families or architectures
- The study uses only one RL variant (GRPO with answer-correctness rewards), so conclusions may not extend to other RL algorithms or reward structures
- The paper does not explore long-term stability of skill profiles beyond the training trajectories shown

## Confidence

- **High confidence**: SFT produces more jagged, over-specialized skill profiles while RL maintains more balanced profiles is well-supported by radar plots and quantitative metrics across multiple model sizes
- **Medium confidence**: The mechanism linking training objectives to skill profile differences is plausible but not definitively proven; alternative explanations like gradient noise differences remain unexplored
- **Low confidence**: Claims about specific "behavior-aware training strategies" for improving model robustness are more speculative; the paper demonstrates what happens but provides limited guidance on systematic design

## Next Checks

1. **Cross-model family validation**: Replicate the SFT vs RL skill profile comparison using a different base model family (e.g., Llama or Mistral) to determine whether observed patterns are universal phenomena or specific to Qwen3 architecture

2. **Ablation of CoT quality**: Systematically vary the quality and diversity of CoT traces used for SFT training while keeping base model and training procedure constant to isolate impact of supervision quality on skill profile formation

3. **Reward structure sensitivity analysis**: Train RL variants with different reward granularities (e.g., step-level rewards for intermediate reasoning correctness vs only final answer rewards) to determine whether preservation of balanced skills depends critically on reward signal design or optimization objective itself