---
ver: rpa2
title: '\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic
  Modulation Recognition'
arxiv_id: '2509.08300'
source_url: https://arxiv.org/abs/2509.08300
tags:
- coreset
- selection
- training
- samples
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FoQuS introduces a forgetting-quality coreset selection framework
  for AMR tasks, addressing the inefficiency of training deep learning models on large
  datasets. The method records prediction trajectories during full-dataset training
  and constructs three complementary importance metrics: Forgetting Score (locating
  decision-boundary samples), Persistent Error Score (capturing hard-to-fit samples),
  and Quality Score (measuring learning value).'
---

# *FoQuS*: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition

## Quick Facts
- arXiv ID: 2509.08300
- Source URL: https://arxiv.org/abs/2509.08300
- Reference count: 21
- Primary result: Achieves 74.56% accuracy on RML2016.10a with only 1% of data, outperforming 10 baselines

## Executive Summary
FoQuS introduces a forgetting-quality coreset selection framework for AMR tasks, addressing the inefficiency of training deep learning models on large datasets. The method records prediction trajectories during full-dataset training and constructs three complementary importance metrics: Forgetting Score (locating decision-boundary samples), Persistent Error Score (capturing hard-to-fit samples), and Quality Score (measuring learning value). These metrics are normalized and combined to create the FoQuS score, which is used to select diverse coresets through tiered sampling. Experiments on three AMR datasets (RML2016.10a, Sig2019-12, RML2018.01a) show that FoQuS outperforms 10 baselines, achieving 74.56% accuracy on RML2016.10a with just 1% of data, and maintains cross-architecture generalization.

## Method Summary
FoQuS operates in two phases: selection and training. In the selection phase, a proxy model is trained on the full dataset for T epochs while recording per-sample prediction trajectories (correctness and loss at each epoch). From these trajectories, three scores are computed: Forgetting Score (transitions from correct to incorrect), Persistent Error Score (consecutive incorrect predictions), and Quality Score (accumulated loss/correctness balance with β=0.1). These scores are normalized and summed into a single FoQuS score per sample. The dataset is then partitioned into three tiers based on score ranges, and samples are selected proportionally from each tier to form the coreset. The training phase uses this coreset to train target models, achieving comparable accuracy to models trained on the full dataset.

## Key Results
- Achieves 74.56% accuracy on RML2016.10a with only 1% of data, outperforming 10 baselines
- Maintains cross-architecture generalization: coresets selected with CNN1D work well for CNN2D training
- Ablation study shows complementary contribution: single-metric scores achieve 35–54% accuracy at 1% sampling, while full FoQuS achieves 54.1%
- Tiered sampling preserves SNR diversity compared to single-tier selection methods

## Why This Works (Mechanism)

### Mechanism 1: Training Dynamics Capture
Recording per-sample prediction trajectories during full-dataset training reveals information about each sample's learning difficulty and importance that static metrics cannot capture. The method trains a model for T epochs on the full dataset and records the predicted label and correctness at each epoch. From this trajectory, it derives three scores: forgetting score (transitions from correct to incorrect), persistent error score (consecutive incorrect predictions), and quality score (balance of cumulative loss and correctness). These are normalized and summed into a single FoQuS score per sample. The core assumption is that training dynamics observed during a single full training run are representative of how future models will interact with these samples.

### Mechanism 2: Complementary Metric Fusion
Combining three distinct importance metrics captures complementary aspects of sample value, improving coreset quality compared to single-metric approaches. Forgetting score identifies boundary samples that flip predictions; persistent error score captures hard-to-fit samples; quality score weights samples that provide useful gradients despite being partially learned. These are normalized to comparable scales and summed, rather than selecting based on any single criterion. The core assumption is that the three metrics provide non-redundant information about sample importance.

### Mechanism 3: Tiered Proportional Selection
Selecting samples from multiple score tiers rather than simply taking the top-k highest-scoring samples improves coreset diversity and preserves coverage of different SNR levels and modulation types. After computing FoQuS scores for all samples, the dataset is partitioned into three tiers based on score ranges. A predetermined proportion is drawn from each tier to form the final coreset, rather than selecting only the globally highest-scoring samples. The core assumption is that high-scoring samples alone do not sufficiently represent the full data distribution.

## Foundational Learning

- **Concept: Coreset Selection**
  - Why needed: The entire method is built on the coreset premise—that a small, carefully chosen subset can approximate full-dataset performance. Understanding the formal objective is essential to interpret why FoQuS uses trajectory-based metrics.
  - Quick check: Can you explain how Equation 1 formalizes the coreset objective in terms of loss approximation?

- **Concept: Training Dynamics and Forgetting Events**
  - Why needed: The forgetting score is derived from "forgetting events" (correct→incorrect transitions). Understanding why these events correlate with boundary proximity and sample importance is necessary to interpret the metric.
  - Quick check: Given a sample that is correctly classified at epoch 10 but incorrect at epoch 11, what does this imply about its position relative to the decision boundary?

- **Concept: Signal-to-Noise Ratio (SNR) in AMR**
  - Why needed: The paper explicitly argues that image-based coreset methods fail on AMR tasks because they ignore SNR structure. Understanding SNR's role in modulation recognition helps explain why tiered selection matters.
  - Quick check: Why would a coreset that oversamples high-SNR samples perform poorly at test time in realistic (noisy) conditions?

## Architecture Onboarding

- **Component map:**
  1. Full training pass: Train model f_θ on full dataset D for T epochs
  2. Trajectory logger: Record ŷᵢ⁽ᵗ⁾ and cᵢ⁽ᵗ⁾ for each sample at each epoch
  3. Score computer: Calculate S_forget, S_persist_err, and S_quality per sample
  4. Score normalizer: Normalize each metric to [0, 1] range
  5. FoQuS aggregator: Sum normalized scores
  6. Tier partitioner: Divide samples into 3 tiers by score
  7. Proportional sampler: Extract predetermined fraction from each tier to form coreset S

- **Critical path:** The trajectory logging is the bottleneck—it requires storing N × T predictions and computing per-sample statistics across all epochs. Memory scales with dataset size and training duration.

- **Design tradeoffs:**
  - Initial overhead vs. downstream savings: Must pay full training cost once; only beneficial if coreset is reused multiple times
  - Number of epochs T: Longer training provides richer trajectory data but increases upfront cost
  - Tier proportions: The paper does not specify exact proportions; tuning these per-dataset may improve results but adds complexity

- **Failure signatures:**
  - Coreset underperforms at low SNR: Likely means tiered selection did not preserve enough low-SNR samples
  - Cross-architecture transfer fails: May indicate the selection model was too dissimilar from target models
  - Single-metric dominance: If one score dominates the normalized sum, recheck scaling factors

- **First 3 experiments:**
  1. Reproduce ablation (Table V): Train on RML2016.10a with 1% coreset; compare single-metric vs. full FoQuS to verify complementary contribution
  2. Cross-architecture validation: Select coreset with CNN1D, train CNN2D on that coreset, and compare against CNN2D-selected coreset
  3. SNR stratification check: After selection, histogram the SNR distribution of the coreset vs. original dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several limitations are implicit in the methodology and experimental design.

## Limitations
- Requires full training pass upfront to collect trajectory data, creating significant initial computational overhead
- Critical hyperparameters (T, tier boundaries, sampling proportions) are not specified and may require per-dataset tuning
- Empirical validation focuses on high-SNR data; performance on low-SNR samples and SNR distribution preservation needs further investigation

## Confidence
- **High confidence:** The core mechanism of combining three complementary metrics (forgetting, persistent error, quality) is well-defined and supported by the ablation study
- **Medium confidence:** The claim of cross-architecture generalization is supported by Table III but only tested across two architectures
- **Medium confidence:** The claim that the approach works "efficiently" needs clarification—while it reduces training data size, it requires a full training pass upfront

## Next Checks
1. **Tier composition analysis:** After coreset selection, compute and compare the SNR distribution of each tier versus the original dataset to verify that low-SNR samples are adequately represented across all tiers.
2. **Proxy model sensitivity test:** Repeat the selection process with different proxy model architectures (e.g., shallower vs. deeper CNNs) and evaluate how coreset quality varies, measuring the stability of FoQuS scores across proxy choices.
3. **Resource cost accounting:** Calculate the total computational cost of the full selection phase (proxy training + trajectory logging) versus the savings from reduced target model training, and determine the breakeven point in terms of number of retraining iterations.