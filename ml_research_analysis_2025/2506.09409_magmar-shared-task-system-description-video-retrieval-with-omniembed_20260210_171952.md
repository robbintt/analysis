---
ver: rpa2
title: 'MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed'
arxiv_id: '2506.09409'
source_url: https://arxiv.org/abs/2506.09409
tags:
- video
- retrieval
- text
- audio
- multivent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OmniEmbed-Multivent, a unified multimodal retrieval
  framework that leverages OmniEmbed from the Tevatron 2.0 toolkit to address the
  challenge of video retrieval. Traditional methods rely heavily on textual metadata,
  but real-world videos contain complex visual, auditory, and textual information
  that is difficult to integrate.
---

# MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed

## Quick Facts
- arXiv ID: 2506.09409
- Source URL: https://arxiv.org/abs/2506.09409
- Reference count: 9
- Primary result: nDCG@10 of 0.753 on MAGMaR shared task test set

## Executive Summary
This paper presents OmniEmbed-Multivent, a unified multimodal retrieval framework that leverages OmniEmbed from the Tevatron 2.0 toolkit to address the challenge of video retrieval. Traditional methods rely heavily on textual metadata, but real-world videos contain complex visual, auditory, and textual information that is difficult to integrate. The authors fine-tune OmniEmbed on the MultiVENT 2.0 dataset, which includes 217,000 videos with aligned features such as frames, audio, and transcripts. Their approach demonstrates that non-text modalities alone can achieve effectiveness comparable to text-only retrieval, and combining all modalities yields further improvements.

## Method Summary
The authors fine-tune OmniEmbed-v0.1 (Qwen2.5-Omni-7B Thinker module) using LoRA on MultiVENT 2.0 data with 24 uniformly sampled frames, audio tracks, and concatenated text metadata (title, caption, description, ASR transcripts). They employ InfoNCE loss with hard negatives mined via DRAMA-1B retriever on ASR transcripts (top-50 retrieval, 3 negatives per query). The model is trained to produce unified embeddings for multimodal inputs, enabling ANN search across modalities. At inference, they encode queries and corpus documents using the fine-tuned model and retrieve via Tevatron toolkit.

## Key Results
- Achieved nDCG@10 of 0.753 on MAGMaR shared task test set, the highest among public submissions
- Non-text modalities alone can achieve effectiveness comparable to text-only retrieval
- Model demonstrates robustness to modality mismatch between training and inference
- Heavy text dependence observed for "Raw" video type (nDCG@10 drops from 0.391 to 0.205 when excluding text)

## Why This Works (Mechanism)

### Mechanism 1
Joint multimodal training on combined inputs (text+video+audio) enables emergent cross-modal alignment that pairwise training cannot achieve. OmniEmbed-v0 was originally trained on pairwise modality data (text–text, text–video, text–audio separately). Fine-tuning on MultiVENT 2.0 with all three modalities presented together allows the model to learn unified relevance patterns that transfer across modalities simultaneously. Core assumption: The Thinker module of Qwen2.5-Omni-7B has sufficient capacity to integrate compound multimodal signals without catastrophic forgetting.

### Mechanism 2
Semantically-aligned hard negatives from dense retrieval improve contrastive training in multilingual settings where lexical overlap fails. Instead of BM25 mining, the authors use DRAMA-1B (a multilingual dense retriever) to retrieve top-50 documents via ASR transcripts; non-positive documents in this set serve as hard negatives. This yields linguistically-aligned negatives that share semantic content but differ in relevance. Core assumption: DRAMA-1B's multilingual representations are sufficiently aligned with OmniEmbed's embedding space for meaningful negative selection.

### Mechanism 3
Inference-time modality configuration determines effectiveness patterns more than training-time modality exposure, suggesting robust cross-modal transfer. Experiments show that models with similar inference configurations exhibit similar effectiveness patterns regardless of whether they were trained on full or partial modalities. This indicates the model generalizes across modality mismatches. Core assumption: The unified embedding space preserves modality-invariant semantic structure.

## Foundational Learning

- **InfoNCE Loss (Contrastive Learning)**
  - Why needed here: The training objective uses InfoNCE to learn embeddings where positive query-document pairs are closer than negatives in the shared space
  - Quick check question: Can you explain why increasing the number of hard negatives improves representation quality, and when it might cause training instability?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Continual fine-tuning of a 7B parameter model is done via LoRA, enabling efficient adaptation without full model retraining
  - Quick check question: What determines the rank size in LoRA, and how does it affect the tradeoff between adaptation capacity and catastrophic forgetting?

- **Dense Retrieval with Unified Embeddings**
  - Why needed here: OmniEmbed produces single vectors for multimodal inputs, enabling ANN search across modalities rather than separate retrieval pipelines
  - Quick check question: How would you handle approximate nearest neighbor search when embeddings span multiple modalities with potentially different intra-class variance?

## Architecture Onboarding

- Component map: Input → Frame Sampler (24 frames) → Qwen2.5-Omni-7B Thinker → Unified Embedding ↘ Audio features ↗ ↘ Text (title+caption+ASR) ↗ ↓ InfoNCE Loss (query, positive, 3 hard negatives) ↓ DRAMA-1B (hard negative mining, offline)

- Critical path: 1) Data preparation: Extract ASR via Whisper, sample 24 frames per video, concatenate text metadata 2) Hard negative mining: Use DRAMA-1B to retrieve top-50 per query; filter to 3 negatives 3) LoRA fine-tuning: Initialize from Tevatron/OmniEmbed-v0.1, train on MultiVENT 2.0 4) Inference: Encode queries and corpus documents, perform ANN retrieval via Tevatron toolkit

- Design tradeoffs:
  - Frame count (24): Higher counts capture more temporal coverage but increase compute; paper does not ablate this
  - Text concatenation strategy: Combining title+caption+description+ASR may introduce noise; consider attention masking
  - Hard negative count (3): More negatives improve discrimination but slow training; paper does not report ablations

- Failure signatures:
  - Low scores on "Raw" video type (0.391 vs 0.753 overall) indicates audio/video noise overwhelms signal
  - Poor performance on text-only inference when model trained on full modalities may indicate modality imbalance
  - Cross-lingual degradation (e.g., Korean 0.355, Chinese 0.326 vs Spanish 0.823) suggests uneven multilingual capacity

- First 3 experiments:
  1. Modality ablation: Run inference with text-only, audio-only, video-only to establish per-modality baselines and confirm Table 2 patterns reproduce
  2. Frame count sensitivity: Test 12, 24, 48 frames on a subset to find compute/accuracy inflection point
  3. Hard negative source comparison: Compare DRAMA-1B vs random negatives vs BM25 (for monolingual subset) to isolate multilingual benefit

## Open Questions the Paper Calls Out

- How does retrieval effectiveness vary across fine-grained, query-level semantic categories? The authors state, "It would be ideal to analyze the query-level categorization, but unfortunately, it is not yet available to the public," and express a desire to perform this analysis later.

- Can the model's robustness be improved for "Raw" videos where non-text modalities currently underperform due to noise? The authors note that for "Raw" videos, effectiveness drops significantly when excluding text, hypothesizing that "video and audio are very noisy under this category."

- Does employing multimodal-aware hard negative mining yield better contrastive training signals than text-only mining? The authors utilize DRAMA-1B, a text-based retriever, to mine hard negatives from ASR transcripts because BM25 is ineffective cross-lingually.

## Limitations

- The hard negative mining approach assumes DRAMA-1B embeddings are semantically aligned with OmniEmbed, but no cross-encoder alignment or re-ranking is performed, which could limit negative quality for low-resource languages.

- Heavy text dependence observed for "Raw" video type indicates the model struggles with noisy audio/video signals, limiting real-world applicability for unedited content.

- Cross-lingual performance varies significantly (Korean 0.355, Chinese 0.326 vs Spanish 0.823), suggesting uneven multilingual capacity that may affect global deployment.

## Confidence

- **High confidence**: Claims about overall effectiveness (nDCG@10 = 0.753) and the benefit of multimodal training over text-only
- **Medium confidence**: Claims about unified embedding space properties and the specific contribution of each modality
- **Low confidence**: Claims about robustness to training-inference modality mismatch and the universal applicability of hard negative mining across all languages

## Next Checks

1. **Modality transfer robustness test**: Train separate models on text-only, audio-only, and video-only MultiVENT 2.0 data, then evaluate each on all three inference modalities to quantify cross-modal generalization gaps.

2. **Hard negative quality audit**: For a stratified sample of queries across languages, manually evaluate the top-5 DRAMA-1B retrieved documents for each to verify they are truly semantically similar but non-relevant, and measure how negative quality correlates with final retrieval effectiveness.

3. **Frame count sensitivity analysis**: Systematically vary the number of sampled frames (12, 24, 48, 96) on a held-out validation set to identify the point of diminishing returns and establish the optimal tradeoff between computational cost and retrieval quality.