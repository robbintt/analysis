---
ver: rpa2
title: 'OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+
  Tasks'
arxiv_id: '2505.18775'
source_url: https://arxiv.org/abs/2505.18775
tags:
- image
- generation
- task
- generate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniGenBench, a comprehensive benchmark with
  57 tasks designed to evaluate the instruction-following capabilities of large multimodal
  models across perception-centric and cognition-centric dimensions. Tasks are derived
  from MegaBench and curated through human filtering to ensure diversity and difficulty.
---

# OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks

## Quick Facts
- **arXiv ID:** 2505.18775
- **Source URL:** https://arxiv.org/abs/2505.18775
- **Reference count:** 40
- **Primary result:** Introduces a comprehensive benchmark with 57 tasks to evaluate instruction-following capabilities of large multimodal models across perception and cognition dimensions

## Executive Summary
This paper introduces OmniGenBench, a comprehensive benchmark with 57 tasks designed to evaluate the instruction-following capabilities of large multimodal models across perception-centric and cognition-centric dimensions. Tasks are derived from MegaBench and curated through human filtering to ensure diversity and difficulty. Evaluation employs a dual-mode protocol: automated visual parsing for perception tasks and LLM-as-a-judge for cognition tasks. Testing mainstream models (e.g., GPT-4o-Native, Gemini-2.0-Flash, Seedream) shows GPT-4o-Native achieving state-of-the-art performance across all dimensions, particularly in complex reasoning and world knowledge tasks, while open-source models lag notably behind.

## Method Summary
OmniGenBench derives 57 tasks from MegaBench by identifying reversible tasks that can be transformed from visual understanding queries into generation prompts. Human annotators filter and validate these tasks. The benchmark employs a dual-mode evaluation protocol: perception-centric tasks use off-the-shelf visual parsers for automated evaluation, while cognition-centric tasks use LLM-as-a-Judge (Gemini-2.5-Pro) with task-specific evaluation criteria. The final OmniScore combines consistency (0.8), realism (0.1), and aesthetics (0.1) to prioritize instruction-following while maintaining image quality.

## Key Results
- GPT-4o-Native achieves state-of-the-art performance across all task dimensions, especially in complex reasoning and world knowledge tasks
- Open-source models (SDXL, SD1.5) notably lag behind proprietary systems, suggesting gaps in parameter scale and training data
- High agreement rates (0.82-0.96) between OmniScore and human evaluation validate the benchmark's reliability
- Models show distinct limitations in tasks requiring professional knowledge (STEM diagrams, complex reasoning)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual-mode evaluation protocol enables reliable assessment across fundamentally different task types that no single metric can adequately cover.
- Mechanism: Perception-centric tasks (e.g., object counting, spatial positioning) are evaluated using off-the-shelf visual parsing tools that extract structured object information, while cognition-centric tasks (e.g., reasoning, world knowledge) use MLLM-as-a-Judge with task-specific evaluation criteria auto-generated by Gemini-2.5-Pro. This separation acknowledges that verifying "3 apples present" requires different machinery than assessing whether a generated image correctly depicts "the Titanic's state in 2018."
- Core assumption: Visual parsers remain sufficiently reliable for object-level verification, and the judge MLLM's evaluation criteria align with human judgment.
- Evidence anchors:
  - [section 3.3]: "for perception-centric tasks...we directly follow the evaluation protocols proposed in previous work [40, 13]...for cognition-centric tasks...we adopt the MLLM-as-a-Judge paradigm"
  - [section 4.2]: Agreement rates between OmniScore and human evaluation range from 0.82–0.96 across task categories
  - [corpus]: Related work (MOAT, Chart2Code) similarly employs task-specific evaluation strategies, suggesting domain consensus
- Break condition: If visual parsers fail on complex compositions (occluded objects, unusual viewpoints) or judge MLLMs exhibit systematic biases toward certain visual styles, the dual-mode protocol's reliability degrades.

### Mechanism 2
- Claim: OmniScore's weighted composition (0.8 consistency + 0.1 realism + 0.1 aesthetics) prioritizes instruction-following while maintaining image quality as a secondary constraint.
- Mechanism: The heavy weighting on consistency ensures models are rewarded primarily for accurately reflecting the user's intent rather than generating visually impressive but semantically incorrect images. Realism and aesthetics serve as regularizers to prevent degenerate solutions (e.g., correct object count but incomprehensible rendering).
- Core assumption: Consistency, realism, and aesthetics are largely independent dimensions that can be meaningfully separated and weighted.
- Evidence anchors:
  - [section 3.3]: "OmniScore = 0.8 × Consistency + 0.1 × Realism + 0.1 × Aesthetics Quality...This weighting scheme prioritizes adherence to the instruction"
  - [section 4.2]: High agreement rates with human evaluators validate the weighting scheme empirically
  - [corpus]: No corpus papers explicitly critique or validate this specific weighting; alternative schemes remain unexplored
- Break condition: If high consistency scores can be achieved through trivial or adversarial generations that satisfy parsed criteria without genuine instruction comprehension, the metric becomes gameable.

### Mechanism 3
- Claim: The reversible task filtering from MegaBench creates a benchmark where generation difficulty is anchored in verifiable understanding tasks.
- Mechanism: By identifying tasks where visual understanding queries can be "reversed" into generation prompts (e.g., "identify the cheapest flight" → "generate a flight board showing DL 5819 as cheapest"), the benchmark ensures generation tasks have objectively assessable criteria rather than purely subjective quality judgments.
- Core assumption: The reverse-engineering process preserves task complexity and doesn't inadvertently create easier proxy tasks.
- Evidence anchors:
  - [section 3.2]: "we instruct human annotators to construct image generation requests for each reversible task by combining the rephrased VQA question with its corresponding answer"
  - [section 1]: "these auto-generated queries are reviewed and filtered by human annotators to ensure correctness and, more importantly, to retain a high level of challenge"
  - [corpus]: MegaBench's own validation as a "widely acknowledged all-around benchmark" supports the source material's quality
- Break condition: If the reversal process systematically loses nuance present in original understanding tasks, or if human filtering introduces bias toward certain prompt types, benchmark validity is compromised.

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: Understanding that automated evaluation of open-ended generation requires a meta-model to assess quality, as no ground-truth reference exists for most cognition-centric tasks.
  - Quick check question: Can you explain why a simple pixel-level comparison would fail for evaluating "commonsense reasoning in image generation"?

- Concept: **Perception vs. cognition task taxonomy**
  - Why needed here: The benchmark's core organizational principle—tasks requiring only visual fidelity (perception) versus those requiring reasoning from world knowledge or context (cognition).
  - Quick check question: Would "generate an image of a dog in 2020 given its 2015 photo" be perception-centric or cognition-centric, and why?

- Concept: **Compositional complexity in text-to-image generation**
  - Why needed here: The paper identifies high object density and attribute binding as key differentiators between models (GPT-4o succeeds where others fail).
  - Quick check question: What makes generating "6 horses doing different actions" harder than generating "6 horses"?

## Architecture Onboarding

- Component map:
  - **Task Curation Layer**: MegaBench → reversible task identification → GPT-4o prompt generation → 3-annotator human filtering
  - **Evaluation Layer**: Dual routing—perception tasks → visual parsers (object detection/segmentation); cognition tasks → criteria generator (Gemini-2.5-Pro) → judge MLLM
  - **Scoring Layer**: Individual dimension scores (consistency, realism, aesthetics) → weighted OmniScore aggregation

- Critical path: Task taxonomy design → evaluation prompt engineering (task-specific criteria) → judge calibration against human labels → score aggregation. The evaluation prompt generation (Figure 4) is the highest-leverage component.

- Design tradeoffs:
  - Automated vs. human evaluation: Fully automated for scalability, but introduces parser/judge biases
  - Breadth (57 tasks) vs. depth per task: Prioritizes comprehensive coverage; may under-sample specific failure modes
  - Unified OmniScore vs. per-dimension reporting: Single metric enables ranking but obscures specific capability gaps

- Failure signatures:
  - Low agreement rates with human judgment (below 0.8) would indicate judge calibration failure
  - Systematic score inflation for specific models suggests evaluation prompt leakage or bias
  - High variance across annotators in filtering stage indicates ill-defined task boundaries

- First 3 experiments:
  1. Run baseline evaluation on a subset of 10 tasks across all 6 categories using open-source models (SDXL, SD1.5) to establish floor performance and validate evaluation pipeline execution.
  2. Ablate the OmniScore weights (e.g., 0.5/0.25/0.25) to test whether ranking stability holds under alternative weighting schemes.
  3. Manually inspect 20 "hard cases" where GPT-4o succeeds but other models fail to identify specific capability gaps (text rendering, ID preservation, structured knowledge representation) for targeted improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative models be improved to accurately visualize content requiring deep, specialized professional knowledge (e.g., complex STEM diagrams) where current SOTA models fail?
- Basis in paper: [explicit] The paper notes in Section 4.3 that despite GPT-4o's leadership, it and other leading models exhibit distinct limitations, specifically failing to "generate elements requiring strong professional knowledge."
- Why unresolved: The paper identifies this failure mode through qualitative comparison (e.g., Lewis structures, mathematical reasoning) but does not propose a methodology to fix the lack of domain-specific visual knowledge.
- What evidence would resolve it: Successful generation of complex STEM visualizations (like correct chemical structures or geometric proofs) by a model on the benchmark, or an ablation study showing that incorporating specific domain training data resolves these errors.

### Open Question 2
- Question: What specific architectural or training data interventions are necessary to close the performance gap between open-source models and proprietary systems like GPT-4o-Native?
- Basis in paper: [explicit] In Section 4.1, the paper highlights that open-source models (e.g., SD1.5, SDXL) perform substantially lower, suggesting the gap may stem from "larger parameter scales and more extensive training datasets," but leaves the exact causes unconfirmed.
- Why unresolved: The paper quantifies the gap but does not isolate whether the failure in open-source models is due to architecture, data quality, or simply model scale.
- What evidence would resolve it: An ablation study varying scale and data composition on open-source architectures to achieve parity with GPT-4o on the OmniGenBench metrics.

### Open Question 3
- Question: Does the "LLM-as-a-Judge" evaluation paradigm fail to penalize specific types of semantic hallucinations in complex, cognition-centric tasks?
- Basis in paper: [inferred] The methodology relies on Gemini-2.5-Pro to evaluate cognition-centric tasks (Section 3.3). While the paper claims high agreement with humans, using a model to judge another model risks "blind spots" where the judge model fails to detect subtle reasoning errors it would also make.
- Why unresolved: The paper validates general agreement rates but does not analyze failure cases where the automated judge might rate a hallucinated reasoning step as correct.
- What evidence would resolve it: A "failure analysis" of the evaluation protocol showing instances where the automated judge assigned a high score to an image that human experts flagged for containing semantic reasoning errors.

## Limitations
- The evaluation protocol's heavy reliance on Gemini-2.5-Pro for both task-specific criteria generation and final scoring introduces potential bias, though high agreement rates with human judgment mitigate this concern
- The dual-mode protocol's effectiveness depends critically on the reliability of off-the-shelf visual parsers, which may struggle with complex compositions, occluded objects, or unusual viewpoints
- The benchmark's coverage, while broad at 57 tasks, may under-sample specific failure modes or edge cases in instruction-following

## Confidence
- **High Confidence:** The benchmark architecture (task derivation from MegaBench, dual-mode evaluation, weighted OmniScore) is well-specified and methodologically sound
- **Medium Confidence:** The empirical results showing GPT-4o-Native's state-of-the-art performance are reliable, though the relative performance gaps between models may be sensitive to evaluation prompt quality and judge calibration
- **Medium Confidence:** The claim that open-source models notably lag behind frontier models is supported by the data, though the specific magnitude of these gaps warrants further validation across diverse use cases

## Next Checks
1. **Judge Calibration Validation:** Hold out 20% of cognition tasks with human annotations and compute agreement rates between Gemini-2.5-Pro and human judges across all models to quantify potential judge bias
2. **Parser Robustness Testing:** Systematically test visual parsers on adversarial cases (occluded objects, unusual viewpoints, complex compositions) to establish failure thresholds and identify when human evaluation becomes necessary
3. **Weight Sensitivity Analysis:** Perform ablation studies across different OmniScore weightings (e.g., 0.5/0.25/0.25, 0.9/0.05/0.05) to test ranking stability and determine if current weights meaningfully reflect instruction-following quality versus aesthetics