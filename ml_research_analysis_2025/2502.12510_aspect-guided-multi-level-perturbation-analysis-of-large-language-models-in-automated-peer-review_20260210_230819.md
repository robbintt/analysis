---
ver: rpa2
title: Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models in
  Automated Peer Review
arxiv_id: '2502.12510'
source_url: https://arxiv.org/abs/2502.12510
tags:
- review
- perturbation
- score
- presentation
- before
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an aspect-guided, multi-level perturbation\
  \ framework to assess the robustness of Large Language Models (LLMs) in automated\
  \ peer review. The framework applies targeted perturbations to three components\u2014\
  papers, reviews, and rebuttals\u2014across five aspects: contribution, soundness,\
  \ presentation, tone, and completeness."
---

# Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models in Automated Peer Review

## Quick Facts
- arXiv ID: 2502.12510
- Source URL: https://arxiv.org/abs/2502.12510
- Reference count: 40
- Single-target perturbations to peer review components produce statistically significant biases in LLM decision-making

## Executive Summary
This paper introduces an aspect-guided, multi-level perturbation framework to assess the robustness of Large Language Models (LLMs) in automated peer review. The framework applies targeted perturbations to three components—papers, reviews, and rebuttals—across five aspects: contribution, soundness, presentation, tone, and completeness. By evaluating how these perturbations influence LLM-as-Reviewer and LLM-as-Meta-Reviewer outputs, the study uncovers significant vulnerabilities: (1) review conclusions recommending "strong reject" disproportionately sway meta-reviews; (2) hostile or factually flawed reviews may be misinterpreted as thorough, increasing acceptance rates; (3) incomplete or aggressive rebuttals can paradoxically lead to higher acceptance; (4) missing methodological details are sometimes misattributed to presentation issues rather than soundness. These biases persist across Chain-of-Thought prompting strategies, highlighting the need for stronger oversight and safeguards in automated peer review systems.

## Method Summary
The study employs a three-component perturbation framework (paper, review, rebuttal) with nine aspect-mode combinations, using GPT-4o for LLM-based perturbations plus rule-based string edits. The method tests 508 accepted ICLR 2024 papers with zero-shot prompting across three Chain-of-Thought variants (None, Dimension, Template). Statistical validation uses Wilcoxon Signed-Rank Test for directional effects and Two One-Sided Tests (TOST) for equivalence. The framework isolates causal effects through single-target perturbations while measuring cross-dimensional attribution errors and conclusion anchoring biases in meta-review decisions.

## Key Results
- Meta-reviewer over-relies on review conclusions, particularly "strong reject" labels, causing average -1.65 overall score drops
- Missing methodological details trigger presentation score penalties rather than soundness score drops (violating expected invariance)
- Hostile tone or incomplete rebuttals paradoxically increase acceptance rates
- Conclusion anchoring persists across all three Chain-of-Thought prompting strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-target perturbations isolate causal effects of each peer-review component on LLM decision-making.
- Mechanism: The framework modifies exactly one input (paper, reviews, or rebuttal) while holding others constant, then compares LLM outputs between perturbed and baseline conditions using directional tests (Wilcoxon Signed-Rank) and invariance tests (TOST). This isolates which component manipulations produce significant score/decision shifts.
- Core assumption: LLM behavior changes can be meaningfully attributed to individual input modifications rather than emergent multi-component interactions.
- Evidence anchors:
  - [abstract] "we investigate how aspect-based manipulations... can introduce significant biases in the review process"
  - [section 2.2] "We define three perturbation functions... Applying any of these functions, Fα(T), produces a perturbed input, T'. We then compare the LLM outputs for the perturbed input, opert, with the unperturbed baseline outputs, obase."
  - [corpus] Related work on adversarial attacks (Liu et al., 2023; Zhang et al., 2023) supports single-component perturbation validity, though multi-component interaction effects remain underexplored.
- Break condition: If perturbations to multiple components produce non-additive effects (e.g., paper soundness + review tone interact synergistically), single-target analysis would miss higher-order vulnerabilities.

### Mechanism 2
- Claim: Aspect-guided taxonomy exposes dimension-specific misattribution errors in LLM evaluation.
- Mechanism: By perturbing specific quality dimensions (e.g., removing methodological details for soundness) and measuring which scores change, the framework detects when LLMs penalize the wrong dimension. The key finding: missing methods trigger presentation score drops rather than soundness score drops, violating expected invariance.
- Core assumption: Quality dimensions are conceptually separable and LLMs should maintain cross-dimensional invariance (e.g., presentation flaws should not affect soundness judgments).
- Evidence anchors:
  - [section 4.2] "Critically, while the Soundness score itself shows no significant difference... the Presentation score shows a statistically significant decrease (↓). This violates the expected invariance: a flaw in soundness is being misattributed to an issue in presentation."
  - [section 4.3] "Misinterpretation of Missing Information... LLM sometimes penalizes abstract methodological details without explanation... as presentation problems, rather than recognizing them as core soundness issues"
  - [corpus] PaperAudit-Bench and FLAWS benchmark similarly address error detection but focus on localization rather than cross-dimensional attribution.
- Break condition: If human reviewers also misattribute flaws across dimensions (presentation/soundness confusion is common), the "vulnerability" may reflect inherent task ambiguity rather than LLM-specific failure.

### Mechanism 3
- Claim: Meta-reviewer over-relies on review conclusions (especially "strong reject") while underweighting tone and completeness signals.
- Mechanism: Flipping review conclusions to "strong reject" produces the largest effect size (-1.65 overall score drop in Template CoT), while hostile tone or incomplete rebuttals paradoxically increase acceptance rates. The LLM appears to treat negative conclusions as authoritative while failing to discount manipulative framing.
- Core assumption: Meta-reviewers should weight content quality over surface signals (tone, conclusion labels) and detect adversarial reviews.
- Evidence anchors:
  - [section 5.3] "Over-reliance on Overall Ratings... meta-reviewer is heavily influenced by the reviewers' overall numerical ratings, particularly strong rejections, often neglecting the detailed content within the reviews"
  - [Table 3] Conclusion perturbation produces -1.65 average overall score change; all other review perturbations increase scores.
  - [corpus] ReviewScore addresses misinformed review detection but focuses on incorrect premises rather than conclusion anchoring bias specifically.
- Break condition: If "strong reject" recommendations legitimately correlate with higher-quality critique (signal vs. noise), the observed sensitivity may be rational rather than a vulnerability.

## Foundational Learning

- Concept: Wilcoxon Signed-Rank Test (non-parametric paired difference testing)
  - Why needed here: Determines whether perturbations produce statistically significant directional changes in scores without assuming normality. Used to validate that "strong reject" conclusions significantly decrease meta-review scores.
  - Quick check question: Why use Wilcoxon instead of paired t-test for 1-10 rating scales with potential ordinal interpretation?

- Concept: Two One-Sided Tests (TOST) for equivalence
  - Why needed here: Distinguishes "no significant difference" (p > 0.05) from "meaningful equivalence" (effect within practical bounds). Critical for invariance tests—showing presentation perturbations truly don't affect contribution scores.
  - Quick check question: If Wilcoxon shows p=0.12, does that prove the groups are equivalent? What additional step does TOST require?

- Concept: Chain-of-Thought (CoT) prompting variants
  - Why needed here: The paper tests three CoT configurations (None, Dimension, Template) to assess whether explicit reasoning reduces perturbation susceptibility. Results show CoT style affects vulnerability magnitude but biases persist.
  - Quick check question: Why might None-CoT show different susceptibility patterns than Template-CoT for the same perturbation?

## Architecture Onboarding

- Component map:
  - Perturbation Engine (GPT-4o + rule-based) -> LLM-as-Reviewer (Mrev) -> LLM-as-Meta-Reviewer (Mmeta) -> Statistical Testing Layer

- Critical path:
  1. Collect baseline outputs from Mrev and Mmeta on unperturbed ICLR 2024 papers
  2. Apply single-target perturbation to one component (paper/review/rebuttal)
  3. Generate perturbed outputs from same models
  4. Compute score differences and run statistical tests
  5. Flag violations: expected-directional failures (should decrease but doesn't) and invariance failures (shouldn't change but does)

- Design tradeoffs:
  - Accepted papers only: Ensures high-quality baseline but limits generalization to borderline/rejected papers
  - Zero-shot prompting: Avoids context-length issues but may underutilize in-context learning potential
  - GPT-4o as both perturber and evaluator: Risk of self-preference bias (author notes this limitation)
  - Single-target vs. multi-target: Cleaner attribution but misses interaction effects

- Failure signatures:
  - Score increases after quality-degrading perturbations (e.g., incomplete rebuttals → higher acceptance)
  - Cross-dimensional misattribution (soundness removal → presentation score drop)
  - Conclusion anchoring (single "strong reject" overrides multiple positive reviews)
  - CoT inconsistency (different CoT styles produce contradictory patterns)

- First 3 experiments:
  1. Reproduce directional test on review conclusion perturbation: Apply string-based conclusion flip to 50 papers, verify -1.0+ overall score drop using Wilcoxon with p<0.05 threshold.
  2. Validate invariance test on paper soundness: Remove methodological details from 30 papers, run TOST to confirm presentation score equivalence (margin ±0.5) while checking for unexpected soundness score stability.
  3. Ablate CoT sensitivity: Run identical perturbations through all three CoT variants, quantify variance in effect sizes to determine if Template-CoT provides partial defense.

## Open Questions the Paper Calls Out
None

## Limitations
- GPT-4o used for both perturbation generation and evaluation introduces potential self-preference bias
- Zero-shot prompting may underrepresent in-context learning capabilities that could mitigate identified vulnerabilities
- Exclusive focus on accepted papers (406 posters, 83 spotlights, 19 orals) from ICLR 2024 limits generalization to rejected submissions or other venues

## Confidence
- **High confidence**: Meta-reviewer over-reliance on conclusion labels and conclusion anchoring effects (p<0.01 across multiple tests)
- **Medium confidence**: Cross-dimensional misattribution findings, as they depend on precise aspect categorization
- **Low confidence**: Claims about CoT prompting mitigation, as effect sizes vary considerably across perturbation types

## Next Checks
1. Multi-target interaction effects: Extend analysis to simultaneous perturbations across components (paper + review + rebuttal) to detect synergistic vulnerabilities beyond single-target analysis.
2. Human benchmark comparison: Evaluate whether human reviewers exhibit similar conclusion anchoring and cross-dimensional misattribution patterns to contextualize LLM-specific failures.
3. Cross-domain generalization: Apply framework to non-ML venues (e.g., NeurIPS, CVPR) to assess whether identified vulnerabilities persist across disciplinary boundaries.