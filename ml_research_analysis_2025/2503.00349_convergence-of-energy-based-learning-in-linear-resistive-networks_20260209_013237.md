---
ver: rpa2
title: Convergence of energy-based learning in linear resistive networks
arxiv_id: '2503.00349'
source_url: https://arxiv.org/abs/2503.00349
tags:
- learning
- dogd
- convergence
- network
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first rigorous convergence guarantee
  for Contrastive Learning, an energy-based learning algorithm, when applied to networks
  of linear adjustable resistors. The authors show that, under physical constraints
  on conductance values, Contrastive Learning is equivalent to projected gradient
  descent on a convex function with Lipschitz continuous gradient.
---

# Convergence of energy-based learning in linear resistive networks

## Quick Facts
- **arXiv ID:** 2503.00349
- **Source URL:** https://arxiv.org/abs/2503.00349
- **Reference count:** 40
- **Primary result:** First rigorous convergence guarantee for Contrastive Learning on linear resistor networks using projected gradient descent on convex cost function

## Executive Summary
This paper establishes the first rigorous convergence guarantee for Contrastive Learning, an energy-based learning algorithm, when applied to networks of linear adjustable resistors. The authors show that, under physical constraints on conductance values, Contrastive Learning is equivalent to projected gradient descent on a convex function with Lipschitz continuous gradient. They prove convergence for a specific range of step sizes and extend this result to a stochastic variant. The convergence analysis relies on showing the convexity of the contrastive cost function and the Lipschitzness of its gradient, leveraging connections between distributed optimization and electrical networks. Simulation results demonstrate convergence in practice and investigate the effect of step size and network size on convergence rate. This work provides a theoretical foundation for energy-based learning algorithms and their application in analog electronic devices for in-memory computation.

## Method Summary
The method learns a mapping from input potentials to desired output potentials in a linear resistor network by adjusting conductances using Contrastive Learning. The algorithm computes voltages in free state (only inputs applied) and clamped state (inputs and desired outputs), then updates conductances via projected gradient descent: g^{t+1} = P_{C_ε}(g^t - γ∇Q(g^t)). The cost function Q(g) = (v^D)^T G v^D - v(g)^T G v(g) measures the difference between clamped and free voltages. The gradient is computed element-wise as ∇Q(g) = (v^D)^2 - (v(g))^2, and conductances are projected to remain above a lower bound ε to ensure physical realizability.

## Key Results
- Contrastive Learning is equivalent to projected gradient descent on a convex function with Lipschitz continuous gradient
- The algorithm converges for step sizes γ ∈ (0, 2/K) where K is a computable Lipschitz constant
- Simulation results demonstrate convergence in practice and show larger networks converge faster
- A stochastic variant with diminishing step sizes also converges to the desired mapping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The contrastive cost function Q(g) is convex, ensuring gradient-based optimization converges to global minima.
- **Mechanism:** The Hessian ∇²Q(g) = 2·diag(v(g))·W(g)·diag(v(g)) is positive semidefinite because W(g) = D_O^T(D_O G D_O^T)^{-1}D_O is an orthogonal projection matrix (eigenvalues ∈ {0,1}). This structure guarantees convexity across the entire domain of positive conductances.
- **Core assumption:** All conductances remain strictly positive (g ∈ C_ε with ε > 0); the graph G is connected.
- **Evidence anchors:**
  - [abstract]: "Contrastive Learning is equivalent to projected gradient descent on a convex function"
  - [section IV, Lemma 5]: "The cost function Q is convex and its Hessian is given by..."
  - [corpus]: Limited direct coverage; related work on energy-based models (arXiv:2510.12311) addresses latent energy structures but not this specific convexity proof.
- **Break condition:** If resistors exhibit nonlinear I-V characteristics (violating linearity), the Hessian structure changes and convexity is not guaranteed.

### Mechanism 2
- **Claim:** The gradient ∇Q is Lipschitz continuous on C_ε, enabling bounded step-size guarantees for gradient descent.
- **Mechanism:** The Lipschitz constant K = (2/ε)(‖D_I‖ + √(N_I N_O)‖D_O‖)²‖p_I‖² emerges from bounding ‖∇²Q(g)‖ via: (1) λ_max(W(g)) ≤ 1/ε from matrix inequalities, and (2) ‖v(g)‖ ≤ (‖D_I^T‖ + √(N_I N_O)‖D_O^T‖)‖p_I‖ using stochastic matrix properties of -(D_O G D_O^T)^{-1}D_O G D_I^T.
- **Core assumption:** Conductances bounded below by ε; input potentials p_I are finite.
- **Evidence anchors:**
  - [abstract]: "convex function with Lipschitz continuous gradient"
  - [section IV, Lemma 7]: "The gradient ∇Q is Lipschitz continuous on C_ε with constant K given by (25)"
  - [corpus]: Related work on primal-dual methods (arXiv:2411.06278) discusses Lipschitz continuity in optimization but not this circuit-derived bound.
- **Break condition:** If ε → 0 (conductances approach zero), K → ∞ and the step-size bound γ ∈ (0, 2/K) becomes vacuous.

### Mechanism 3
- **Claim:** Contrastive Learning is a Krasnosel'skiǐ-Mann iteration on an averaged operator, guaranteeing convergence to a fixed point.
- **Mechanism:** The update T(g) = P_Cε(g - γ∇Q(g)) is a composition of: (1) θ₂-averaged gradient step T̃(g) with θ₂ = γK/2 ∈ (0,1), and (2) (1/2)-averaged projection P_Cε. By Lemma 2, T is θ-averaged with θ = (4-γK)/8. Theorem 3 (fixed-point convergence) then applies.
- **Core assumption:** Assumption 1 holds—there exists g_D ∈ C_ε such that p_O(g_D) = p_O^D (the desired mapping is realizable).
- **Evidence anchors:**
  - [abstract]: "guarantee of convergence of the algorithm for a range of stepsizes"
  - [section IV, Theorem 4]: "The algorithm (24) with any starting point g⁰ ∈ C_ε and any γ ∈ (0, 2/K) converges to a fixed point of T"
  - [corpus]: Convergence analysis in Wasserstein proximal algorithms (arXiv:2501.14993) uses different operator-theoretic tools.
- **Break condition:** If the desired mapping is unrealizable (Assumption 1 violated), fixed points may not exist or may not achieve p_O = p_O^D.

## Foundational Learning

- **Concept: Projected Gradient Descent**
  - **Why needed here:** The core algorithm updates conductances via g^{t+1} = P_Cε(g^t - γ∇Q(g^t)), projecting back to feasible set C_ε.
  - **Quick check question:** Can you explain why projection is necessary rather than unconstrained gradient descent?

- **Concept: Convex Functions and Optimality Conditions**
  - **Why needed here:** Lemma 8 relies on the equivalence ∇Q(g*) = 0 ⇔ g* is a minimizer for convex Q on convex C_ε.
  - **Quick check question:** For a convex function on a convex set, what first-order condition characterizes minima?

- **Concept: Lipschitz Continuity and Step-Size Selection**
  - **Why needed here:** Theorem 4's step-size bound γ < 2/K emerges directly from the Lipschitz constant K of ∇Q.
  - **Quick check question:** Why does a larger Lipschitz constant require a smaller step size for convergence?

## Architecture Onboarding

- **Component map:**
  - Graph G(V,E) with nodes V partitioned into input V_I and output V_O
  - Incidence matrix D maps node potentials p to branch voltages v = D^T p
  - Laplacian L = D G D^T relates p to nodal currents j = Lp
  - Free state: input potentials p_I applied; output potentials p_O evolve to minimize power
  - Clamped state: both p_I and desired p_O^D applied; measures "target" voltages

- **Critical path:**
  1. Initialize g⁰ ∈ C_ε (all conductances ≥ ε)
  2. For each iteration t: (a) measure v(g^t) in free state, (b) compute ∇Q(g^t) = v_D² - v(g^t)² elementwise, (c) update g^{t+1} = P_Cε(g^t - γ∇Q(g^t))
  3. Repeat until ‖p_O - p_O^D‖ < tolerance or max iterations

- **Design tradeoffs:**
  - Step size γ: Larger γ → faster convergence but risk of divergence if γ ≥ 2/K
  - Conductance lower bound ε: Larger ε → more conservative K but reduces network expressivity
  - Network size: More branches → faster convergence but smaller 2/K bound

- **Failure signatures:**
  - Conductance collapse: g_k^t → ε and stagnates; may indicate unrealizable mapping
  - Oscillation: ‖p_O - p_O^D‖ oscillates rather than monotonically decreasing
  - Divergence: Conductances grow unbounded; check if desired p_O^D is physically realizable

- **First 3 experiments:**
  1. Step-size sweep on small crossbar: Use 5×5 crossbar, test γ ∈ {10^{-4}, 10^{-3}, ..., 10^{-1}}, verify convergence for γ < 2/K empirically
  2. Conductance bound sensitivity: Fix network and γ, vary ε ∈ {0.01, 0.1, 1.0}, measure iterations to reach ‖p_O - p_O^D‖ < 10^{-6}
  3. Stochastic variant with 100 samples: Generate 100 input-output pairs, run Algorithm 2 with γ_t = 10/(1+t), plot mean error over samples vs. iteration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence guarantees for Contrastive Learning be extended to nonlinear resistive networks?
- **Basis in paper:** [explicit] The conclusion states that "the linear network of this paper is restrictive" and identifies "an extension to nonlinear networks" as a key area for future research to apply results to realistic neuromorphic systems.
- **Why unresolved:** The current proof relies on the convexity of the cost function and the Lipschitz continuity of its gradient, which are properties established specifically for linear conductances. Nonlinear elements (e.g., memristors) may introduce non-convexity or break the Lipschitz assumptions used in Theorem 4.
- **What evidence would resolve it:** A rigorous convergence proof for a specific class of nonlinear elements or empirical results demonstrating stable convergence in simulated nonlinear networks under defined constraints.

### Open Question 2
- **Question:** Does Contrastive Learning converge when the network cannot realize the desired mapping exactly (i.e., when Assumption 1 is violated)?
- **Basis in paper:** [explicit] The discussion following the proof of Theorem 4 notes that the assumption of an exact solution is restrictive and states, "Guaranteeing convergence in the absence of Assumption 1 is an interesting problem that we leave for future research."
- **Why unresolved:** The current proof requires the existence of a fixed point (conductance vector) that perfectly achieves the desired output potentials. Without this "realizability" condition, it is unknown if the projected gradient descent converges to a meaningful approximate solution or simply diverges/stalls.
- **What evidence would resolve it:** A theoretical analysis proving that the iterates converge to a point that minimizes the output error (e.g., minimizes $\|p_O(g) - p^D_O\|$) even when no solution satisfies $p_O(g^*) = p^D_O$.

### Open Question 3
- **Question:** Can the analytical framework used here be applied to prove convergence for other energy-based algorithms like Equilibrium Propagation?
- **Basis in paper:** [explicit] The conclusion anticipates that "results can be extended to other energy-based learning algorithms, such as Equilibrium Propagation."
- **Why unresolved:** Equilibrium Propagation uses a different loss function and update mechanism compared to the Contrastive Learning algorithm analyzed here. It is unverified whether these alternative cost functions retain the necessary properties (convexity, gradient Lipschitzness) within the resistive network model.
- **What evidence would resolve it:** A derivation showing that the cost function for Equilibrium Propagation in linear resistive networks is convex with a Lipschitz continuous gradient, or a modified convergence proof tailored to that algorithm's specific update rule.

### Open Question 4
- **Question:** Can the theoretical upper bound for the step size be refined to align with empirical performance?
- **Basis in paper:** [inferred] Section VI (Simulation Results) notes that Theorem 4 guarantees convergence for step sizes $\gamma < 2/K \approx 8.95 \times 10^{-11}$, whereas simulations demonstrate convergence and fastest performance at significantly larger step sizes (e.g., $\gamma \approx 0.007$).
- **Why unresolved:** The theoretical constant $K$ is derived using conservative matrix norm estimates (Lemma 7) to ensure global Lipschitz continuity, resulting in a bound that appears to be orders of magnitude more restrictive than the network's actual stability region.
- **What evidence would resolve it:** A tighter derivation of the Lipschitz constant that exploits the specific structure of the incidence matrices $D_I$ and $D_O$, or a convergence analysis using a different Lyapunov function that permits larger provable step sizes.

## Limitations
- Theoretical step-size bound γ < 2/K ≈ 10^{-10} is extremely conservative compared to empirical convergence at γ ≈ 0.007
- Assumption 1 requires the desired mapping to be exactly realizable, which may not hold in practice
- Results are limited to linear resistive networks and may not extend to nonlinear elements

## Confidence
- **High confidence:** Convexity of Q(g) and Lipschitz continuity of ∇Q on C_ε (Mechanism 1 and 2)
- **Medium confidence:** Convergence guarantee for fixed step sizes (Theorem 4) due to conservative step-size bound
- **Medium confidence:** Stochastic variant convergence (Algorithm 2) due to lack of empirical validation

## Next Checks
1. **Test realizability:** For each simulation target mapping, verify Assumption 1 by solving for g_D analytically or numerically. Document cases where convergence fails and trace to unrealizable mappings.
2. **Step-size stress test:** Systematically test γ values spanning the theoretical bound (2/K) to empirical convergence limits (e.g., 0.01). Measure convergence rate and divergence points to characterize the practical bound.
3. **Network topology sensitivity:** Compare convergence across different graph structures (crossbar, random, scale-free) while holding N_I, N_O, and |E| constant. Quantify how topology affects both K and practical convergence behavior.