---
ver: rpa2
title: 'Mondrian: Transformer Operators via Domain Decomposition'
arxiv_id: '2506.08226'
source_url: https://arxiv.org/abs/2506.08226
tags:
- operator
- attention
- operators
- neural
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mondrian addresses the challenge of scaling transformer-based operator
  learning models to high-resolution, multiscale domains by decoupling attention from
  discretization. The core idea is to decompose the domain into non-overlapping subdomains
  and apply attention over sequences of subdomain-restricted functions, inspired by
  domain decomposition methods.
---

# Mondrian: Transformer Operators via Domain Decomposition

## Quick Facts
- **arXiv ID:** 2506.08226
- **Source URL:** https://arxiv.org/abs/2506.08226
- **Reference count:** 40
- **Primary result:** Mondrian achieves MAE as low as 3.58 × 10⁻³ at training resolution (32×32) and 8.12 × 10⁻³ at higher resolution (128×128) for Allen-Cahn, demonstrating resolution scaling without retraining.

## Executive Summary
Mondrian addresses the challenge of scaling transformer-based operator learning models to high-resolution, multiscale domains by decoupling attention from discretization. The core idea is to decompose the domain into non-overlapping subdomains and apply attention over sequences of subdomain-restricted functions, inspired by domain decomposition methods. Mondrian replaces standard transformer layers with neural operators within each subdomain and uses softmax-based inner products for cross-subdomain attention, naturally extending to hierarchical windowed and neighborhood attention variants. Experiments on Allen-Cahn and Navier-Stokes PDEs show that Mondrian matches state-of-the-art accuracy and enables resolution scaling without retraining.

## Method Summary
Mondrian is a domain-decomposed transformer architecture that learns PDE solution operators by partitioning the physical domain into non-overlapping subdomains. Instead of treating individual grid points as tokens, it treats functions restricted to subdomains as tokens, enabling attention matrices that remain constant across different discretizations. The model replaces standard transformer layers with neural operators (specifically Mixture Operators) within each subdomain and computes cross-subdomain attention via softmax-based L2 inner products of the resulting function representations. This architecture naturally extends to hierarchical windowed and neighborhood attention variants, with ViT-NO and Swin-NO configurations for global versus local attention patterns.

## Key Results
- Mondrian achieves MAE as low as 3.58 × 10⁻³ at training resolution (32×32) and 8.12 × 10⁻³ at higher resolution (128×128) for Allen-Cahn
- The model matches state-of-the-art accuracy while enabling resolution scaling without retraining
- On Navier-Stokes, Mondrian captures coarse structures but exhibits spectral bias, smoothing fine vortices during rollouts

## Why This Works (Mechanism)

### Mechanism 1: Discretization Decoupling via Subdomain Sequencing
Reducing attention complexity from quadratic in grid points (N) to quadratic in subdomains (s) enables scaling to high-resolution PDEs without retraining. The architecture partitions the physical domain Ω into fixed non-overlapping subdomains, treating functions restricted to subdomains as tokens rather than individual grid points.

### Mechanism 2: Function-Space Attention via Inner Products
Softmax attention is applied to continuous function spaces by computing attention scores via function inner products. Query and Key functions output by neural operators have their attention score matrix entries computed as L2 inner products, aggregating subdomain information into a finite s×s matrix.

### Mechanism 3: Hierarchical Expressiveness via Mixture Operators
Standard spectral convolutions struggle in small, non-periodic subdomains; a "Mixture Operator" is required to maintain expressiveness and stability. Within each subdomain, the replacement of transformer layers uses a kernel κ(x,y) defined as a weighted sum of learnable matrices, avoiding spectral truncation issues.

## Foundational Learning

- **Neural Operators (e.g., FNO, DeepONet)**: Needed because Mondrian learns maps between function spaces (PDE solutions), not just image classification; discretization invariance is key. Quick check: Can you explain why a standard CNN fails to generalize from 32×32 to 128×128 grids?

- **Domain Decomposition Methods**: Needed because the paper borrows terminology and logic from numerical methods (Schwarz methods) to partition the domain. Quick check: If you split a domain Ω into Ω₁ and Ω₂, how do you mathematically "restrict" a function f defined on Ω to Ω₁?

- **Self-Attention Mechanics**: Needed because the paper modifies the core attention mechanism (QK^T). Quick check: In a standard transformer, if sequence length doubles, how does the memory requirement for the attention matrix change?

## Architecture Onboarding

- **Component map:** Input (Discretized Function v(x)) -> Decomposition (Partition Ω → {Ω₁,...,Ωₛ}) -> Restriction (Create sequence of local functions) -> Subdomain Lifting (Map each local function to Q,K,V functions using Mixture Operator) -> Function Attention (Compute s×s score matrix via L2 Inner Products) -> Aggregation (Weighted sum of V functions) -> Extension (Recombine subdomains to global domain)

- **Critical path:** The implementation of the L2 Inner Product and the Mixture Operator. If these are implemented via standard linear layers or FFTs, the model will likely fail to generalize or converge.

- **Design tradeoffs:** ViT-NO performs global attention between subdomains (better for global dependencies like Allen-Cahn). Swin-NO uses local windows (better for local dynamics like Turbulence, lower memory). Spectral subdomain operators are fast but fail on small patches; Mixture is robust but O(N²).

- **Failure signatures:** Spectral Bias/Smoothing (predicted vorticity fields look "fuzzy" or lose fine vortices), Interpolation Collapse (MAE spikes when testing at 128×128), poor resolution scaling with Interpolating Integral Operator.

- **First 3 experiments:** 1) Unit Test: Implement InnerProduct(Q,K) function and verify against numerical quadrature. 2) Overfit Test: Train on single 32×32 Allen-Cahn trajectory to verify Mixture Operator can fit data. 3) Scaling Test: Train on 32×32, infer on 64×64, monitor MAE.

## Open Questions the Paper Calls Out

### Open Question 1
How can the spectral bias of attention layers be mitigated to better preserve high-frequency details in chaotic PDEs? The paper notes attention acts as a low-pass filter, smoothing vortices in Navier-Stokes simulations, and lists addressing this spectral bias as an important future direction.

### Open Question 2
Can the localized cross-attention mechanism be effectively adapted for adaptive mesh refinement within subdomains? The paper suggests this cross-attention concept could be useful for adaptive refinement where specific subdomains benefit from finer discretization.

### Open Question 3
How can the domain-decomposed attention framework be extended to handle irregular grids and unstructured point clouds? The "Looking Forward" section identifies extending the approach to irregular grids and point clouds as an immediate direction.

## Limitations
- Implementation details are underspecified, particularly around the Mixture Operator's coefficient network architecture
- The theoretical foundation for why L2 inner product is the "correct" choice for function-space attention could be more rigorous
- Comparison to specialized domain decomposition solvers is absent, limiting computational efficiency claims

## Confidence
- **High Confidence:** The core mechanism of domain decomposition reducing attention complexity is mathematically sound and clearly demonstrated
- **Medium Confidence:** The claim that Mixture Operator is universally superior to spectral methods within subdomains is supported but could benefit from more ablation studies
- **Low Confidence:** The assertion that function-space attention via L2 inner products is the "natural" extension of dot-product attention lacks formal justification

## Next Checks
1. **Theoretical Consistency Check:** Derive the gradient of the L2 inner product attention score with respect to neural operator outputs and verify it matches backpropagation implementation
2. **Robustness to Subdomain Size:** Systematically vary subdomain partitioning strategy and measure impact on training accuracy and resolution scaling performance
3. **Comparison to Domain Decomposition Solvers:** Implement a simple overlapping Schwarz domain decomposition method for the same PDEs and compare both accuracy and wall-clock time against Mondrian