---
ver: rpa2
title: Flatter Tokens are More Valuable for Speculative Draft Model Training
arxiv_id: '2601.18902'
source_url: https://arxiv.org/abs/2601.18902
tags:
- training
- flatness
- target
- tokens
- draft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of draft model training in
  speculative decoding for LLM inference acceleration. It identifies that tokens with
  flatter target distributions from the target model are more valuable for training
  than sharply peaked ones.
---

# Flatter Tokens are More Valuable for Speculative Draft Model Training

## Quick Facts
- arXiv ID: 2601.18902
- Source URL: https://arxiv.org/abs/2601.18902
- Reference count: 40
- Key result: SFDD achieves 2× training speedup using 50% of data with <4% inference speedup degradation

## Executive Summary
This paper addresses the inefficiency of training draft models for speculative decoding in large language model inference. The authors identify that tokens with flatter target distributions from the LLM are more valuable for training draft models than sharply peaked ones. They propose a new flatness metric based on cosine similarity between target model output distributions and uniform distributions, then develop Sample-level-flatness-based Dataset Distillation (SFDD) to filter training data, retaining only high-value samples. Experiments within the EAGLE framework demonstrate that SFDD achieves over 2× training speedup while using only 50% of the data, with inference speedup remaining within 4% of the full-dataset baseline.

## Method Summary
The authors propose a data selection method for training draft models in speculative decoding that filters training samples based on token-level flatness. They compute flatness as the cosine similarity between the target model's output distribution and a uniform distribution, then aggregate this to the sample level. The SFDD algorithm retains only the top k% of samples by average flatness. This filtered dataset is used to train a lightweight draft model within the EAGLE framework. The method is evaluated across multiple benchmarks showing significant training efficiency gains while maintaining inference performance.

## Key Results
- SFDD achieves over 2× training speedup using only 50% of the training data
- Inference speedup remains within 4% of the full-dataset baseline
- The flatness metric outperforms entropy-based filtering in identifying valuable training samples
- Training with SFDD requires fewer epochs to reach optimal performance compared to full-dataset training

## Why This Works (Mechanism)
The paper's core insight is that tokens with flatter target distributions cause smaller changes in the final softmax when used as draft predictions. When a draft model predicts a token, if the target model's distribution is flat (uniform-like), the impact on the final output is minimal regardless of the draft prediction. Conversely, sharply peaked distributions are highly sensitive to draft predictions. By filtering for flatter tokens, the method ensures the draft model learns to predict tokens where its errors have minimal impact on the final output, leading to more efficient training.

## Foundational Learning
- **Cosine similarity for distribution comparison**: Measures angular difference between probability distributions; needed to quantify how "flat" a token distribution is compared to uniform. Quick check: Verify cos(p_t, U) ∈ [-1, 1] and equals 1 only when p_t = U.
- **Speculative decoding workflow**: Draft model generates tokens in parallel, target model verifies/rejects them; needed to understand the training objective. Quick check: Ensure draft step count γ matches target model's typical decoding steps.
- **Dataset distillation**: Reducing training data size while preserving model performance; needed to frame SFDD as a data efficiency technique. Quick check: Compare validation loss curves between full and distilled datasets.
- **Token-level vs. sample-level aggregation**: Moving from individual token metrics to whole-sample decisions; needed to implement practical filtering. Quick check: Verify sample flatness = mean(flatness(t)) over all tokens in sample.

## Architecture Onboarding

**Component map**: ShareGPT data → Flatness computation → Sample ranking → Filtered dataset → EAGLE-2 draft model training → Evaluation

**Critical path**: The data filtering pipeline (flatness computation → sample ranking → retention) is the critical innovation. The draft model architecture itself follows standard EAGLE-2 patterns but with reduced capacity.

**Design tradeoffs**: The method trades potential coverage of diverse token patterns (sacrificed by filtering) for training efficiency and reduced overfitting risk. The single transformer layer draft model prioritizes speed over representational capacity.

**Failure signatures**: If filtered datasets show poor generalization, check whether flatness metric fails to capture domain-specific token importance. If training speedups don't materialize, verify that data loading and preprocessing overhead doesn't dominate.

**3 first experiments**:
1. Compute flatness for 100 random tokens from ShareGPT and verify they cluster at higher values than randomly sampled tokens from other domains
2. Train draft model on top/bottom 10% of samples by flatness and measure acceptance rate differences
3. Compare SFDD-filtered training (k=50%) against random 50% data sampling on a small benchmark

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The filtering approach assumes ShareGPT dataset characteristics generalize across all inference tasks, which may not hold for different domains
- The EAGLE-2 draft model architecture is underspecified beyond "single transformer layer," making exact reproduction difficult
- The relationship between token-level flatness and overall decoding efficiency is empirically validated but lacks rigorous theoretical justification
- The claim of 2× training speedup requires careful interpretation as it combines both data reduction and effectiveness gains

## Confidence

**High confidence**: The empirical observation that flatter tokens cause smaller changes in target model softmax (Figure 2b) is well-supported and reproducible.

**Medium confidence**: The SFDD filtering methodology and its implementation details are sufficiently specified for reproduction, though dataset preprocessing details are incomplete.

**Low confidence**: The generalizability of flatness-based filtering across different domains and model architectures requires further validation beyond the presented experiments.

## Next Checks
1. Verify the flatness metric implementation by computing flatness for a small sample of tokens from ShareGPT and comparing against the paper's Figure 2b results, ensuring the cosine similarity computation between target distribution and uniform distribution matches the described methodology.

2. Implement the full data filtering pipeline using the ShareGPT dataset, applying the sample-level flatness aggregation and k% retention threshold. Measure the exact number of samples retained at k=50% and verify that the filtered dataset shows the expected distribution of flatness values.

3. Reproduce the draft model training with SFDD-filtered data on a small subset (e.g., first 1000 samples) to verify that the training procedure converges and produces reasonable draft predictions, using the specified hyperparameters and monitoring KL-divergence loss over training epochs.