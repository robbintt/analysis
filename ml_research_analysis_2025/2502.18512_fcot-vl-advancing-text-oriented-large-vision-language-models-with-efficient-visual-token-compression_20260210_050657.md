---
ver: rpa2
title: FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient
  Visual Token Compression
arxiv_id: '2502.18512'
source_url: https://arxiv.org/abs/2502.18512
tags:
- visual
- arxiv
- performance
- training
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FCoT-VL, an efficient visual token compression
  framework for text-oriented Vision-Language Models (VLLMs) operating in high-resolution
  scenarios. The approach uses a lightweight self-distillation pre-training stage
  with limited image-text pairs to compress visual tokens, followed by a high-quality
  post-training stage to mitigate performance degradation.
---

# FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression

## Quick Facts
- **arXiv ID:** 2502.18512
- **Source URL:** https://arxiv.org/abs/2502.18512
- **Authors:** Jianjian Li; Junquan Fan; Feng Tang; Gang Huang; Shitao Zhu; Songlin Liu; Nian Xie; Wulong Liu; Yong Liao
- **Reference count:** 25
- **Primary result:** Achieves 2x and 4x visual token compression on InternVL2 while outperforming baselines on text-oriented benchmarks

## Executive Summary
FCoT-VL introduces an efficient visual token compression framework for text-oriented Vision-Language Models operating in high-resolution scenarios. The approach uses a lightweight self-distillation pre-training stage with limited image-text pairs to compress visual tokens, followed by a high-quality post-training stage to mitigate performance degradation. Applied to InternVL2, the method achieves significant compression ratios while maintaining or improving performance across text-oriented benchmarks, offering faster inference and better training efficiency compared to existing methods like FastV.

## Method Summary
FCoT-VL employs a two-stage training process for compressing visual tokens in text-oriented VLLMs. First, a self-distillation pre-training stage uses a limited number of image-text pairs to train a lightweight compression module while freezing the heavy backbone. The student model inherits the reasoning capabilities of the teacher through KL divergence minimization. Second, a post-training stage uses Chain-of-Thought distillation and dynamic dataset sampling to recover any performance degradation. The compression module uses a simple 1D convolutional layer instead of complex query-based mechanisms, making it more data-efficient and stable.

## Key Results
- Achieves 2x and 4x visual token compression on InternVL2-2B/8B models
- Outperforms baselines across text-oriented benchmarks including DocVQA and ChartQA
- Reduces computational overhead with faster inference speeds (1.5x-2.4x)
- Maintains >100% average performance relative to baseline InternVL2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The student model can effectively inherit the reasoning capabilities of a larger teacher model while operating on compressed visual tokens by freezing the heavy backbone and optimizing only the connector layers.
- **Mechanism:** The framework employs a self-distillation paradigm where the teacher (e.g., InternVL2) and student share the Vision Transformer (ViT) and Large Language Model (LLM) weights. Only the student's visual compressor and projector are trained. The optimization minimizes the Kullback-Leibler (KL) divergence between the teacher's and student's output logits, forcing the compressed representation to align with the rich representation of the teacher.
- **Core assumption:** The pre-trained ViT and LLM possess sufficient general knowledge; the performance bottleneck lies in the modality alignment and token density, not the model capacity.
- **Evidence anchors:**
  - [abstract]: "employ a light-weight self-distillation pre-training stage... requiring a limited numbers of image-text pairs and minimal learnable parameters."
  - [section 3.1.1]: "The LLM and ViT in s-VLLM maintain frozen... Only the student adaptor As and the visual token compression module Vc are learnable."
  - [corpus]: Related work *InternVL-X* validates the trend of accelerating VLLMs via token compression, suggesting the efficacy of this architectural layer.

### Mechanism 2
- **Claim:** A simple 1D Convolutional layer outperforms complex query-based mechanisms (like Q-Former) for visual token compression when training data is limited.
- **Mechanism:** Instead of using cross-attention to select tokens (Q-Former), the paper applies a convolutional layer with a stride of 2 to merge adjacent tokens. This acts as a structural downsampling mechanism, preserving local spatial relationships better than selection-based methods in low-resource settings.
- **Core assumption:** Visual tokens contain significant spatial redundancy that can be merged via local operations without losing global semantic meaning.
- **Evidence anchors:**
  - [page 6]: "We find that Qformer suffers from serious performance drops under our data-constrained distillation training... In contrast, both CNN and pooling... exhibit minimal performance degradation."
  - [table 3]: Shows QFormer dropping to 42.32% on ChartQA (vs ~75% baseline), while CNN retains 75.04%.

### Mechanism 3
- **Claim:** Performance degradation from compression can be reversed through a "post-train" stage utilizing Chain-of-Thought (CoT) distillation and dynamic dataset sampling.
- **Mechanism:** The authors augment the dataset using Rejection Sampling (RS) to generate CoT reasoning paths. They also implement a sampling strategy that analyzes training loss per taskâ€”down-sampling "easy" low-loss data and up-sampling "hard" high-loss data (specifically reasoning tasks).
- **Core assumption:** The model's "intelligence" is constrained by the quality and difficulty profile of the fine-tuning data, not just the token count.
- **Evidence anchors:**
  - [section 3.1.2]: "We find that the mixture of RS-augmented and vanilla data significantly enhances reasoning capabilities."
  - [figure 5]: Demonstrates the "see-saw" effect where performance fluctuates across checkpoints, motivating the need for specific sampling and merging.

## Foundational Learning

- **Concept: Knowledge Distillation (KL Divergence)**
  - **Why needed here:** The core pre-training strategy relies on matching the output distribution (logits) of the student to the teacher.
  - **Quick check question:** Why does minimizing KL divergence preserve the teacher's "reasoning style" better than standard Cross-Entropy loss with ground truth labels?

- **Concept: Visual Token Compression (Pruning vs. Merging)**
  - **Why needed here:** The paper critiques "training-free" pruning (FastV) and argues for "training-based" merging (CNN) to maintain high-resolution details.
  - **Quick check question:** Why might simply pruning low-attention tokens (FastV) destroy information necessary for OCR tasks compared to merging adjacent tokens?

- **Concept: Model Merging (Shapley Values)**
  - **Why needed here:** To mitigate the "see-saw" effect in multi-task learning, the authors use Shapley values to fuse checkpoints.
  - **Quick check question:** How does Shapley-value-based merging differ from simple weight averaging (Model Soups) in terms of assigning credit to specific checkpoints?

## Architecture Onboarding

- **Component map:** Image ($x_v$) + Text ($x_t$) -> Frozen ViT ($V_{iT}$) + LLM ($L_{LM}$) -> Learnable Bridge (CNN compression module $V_c$ -> Student Projector $A_s$) -> Output
- **Critical path:** The efficiency gain is realized when visual embeddings pass through $V_c$ (CNN with stride 2) before hitting the LLM, reducing the sequence length $n$ to $n/r$.
- **Design tradeoffs:**
  - **CNN vs. Q-Former:** CNN is faster and data-efficient but rigid in token selection; Q-Former is flexible but data-hungry and unstable with limited data.
  - **Compression Ratio:** 50% is "safe" (approx. 100% performance retention); 75% offers higher speed (1.5x-2.4x) but risks degradation on dense text tasks (DocVQA).
- **Failure signatures:**
  - **Training collapse:** If using Q-Former with only 2M data, look for performance dropping >30% (Table 3).
  - **Resolution loss:** On extremely high-resolution tasks (InfoVQA), expect performance drops if the compression ratio is set to maximum (75%).
- **First 3 experiments:**
  1. **Module Ablation:** Replace the CNN compression module with a Q-Former or Pooling layer on a small subset (60k data) to verify CNN superiority.
  2. **Ratio Scaling:** Train with 50% vs. 75% compression to measure the trade-off between inference speed (ms/token) and DocVQA accuracy.
  3. **Merging Validation:** Compare the final checkpoint against the Shapley-merged model across distinct domains (Math vs. OCR) to observe the mitigation of the "see-saw" effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the FCoT-VL compression framework generalize effectively to non-text-oriented image modalities, such as natural scenes or medical imaging?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "our approach does not extend to other image modalities, such as natural scenes or medical imaging" due to resource constraints and the focus on text-oriented tasks.
- Why unresolved: The training data and distillation process were specifically curated for high-resolution text understanding (OCR, charts), potentially optimizing the compression module to retain textual features at the expense of general visual features.
- What evidence would resolve it: Evaluating FCoT-VL on standard natural image benchmarks (e.g., VQAv2, COCO) and medical imaging datasets to compare performance retention against the text-oriented benchmarks.

### Open Question 2
- Question: Can an adaptive or dynamic compression strategy mitigate the performance degradation observed in extremely high-resolution tasks?
- Basis in paper: [explicit] The authors note in the Limitations that the "fixed compression ratios... shows a slight performance drop when applied to extremely high-resolution tasks, such as infoVQA."
- Why unresolved: The current method utilizes a static CNN-based downsampling approach (fixed at 50% or 75%), which may be too aggressive for images requiring maximal fidelity in tasks involving dense infographics.
- What evidence would resolve it: Ablation studies using variable compression ratios based on image complexity or content density, specifically measuring performance recovery on the InfoVQA benchmark.

### Open Question 3
- Question: What is the underlying mechanism that allows visual token compression to improve performance on reasoning tasks (e.g., MathVista) while degrading performance on fine-grained perception tasks (e.g., DocVQA)?
- Basis in paper: [inferred] Section 4.1 and Figure 3 show that while DocVQA scores drop as compression increases, MathVista scores actually improve, leading the authors to discuss a trade-off between compression and fine-grained detail.
- Why unresolved: The paper hypothesizes that "high-quality data plays a more critical role" for reasoning, but it does not explain if the compression acts as a beneficial noise filter or if the model learns to ignore necessary visual details for perception tasks.
- What evidence would resolve it: An analysis of intermediate attention maps comparing how "reasoning" tokens versus "perception" tokens are weighted relative to the compressed visual tokens.

## Limitations

- **Domain Specificity:** The compression framework is optimized for text-oriented tasks and does not generalize to natural scenes or medical imaging.
- **Fixed Compression Ratios:** Static 50% and 75% compression ratios show performance drops on extremely high-resolution tasks like InfoVQA.
- **Dataset Dependency:** Success relies heavily on curated OCR-heavy datasets (2M) and high-quality instruction data (4.5M), which are not publicly released.

## Confidence

- **High:** The core distillation mechanism (freezing backbone, training connector) is well-supported by ablation studies (Table 3) and aligns with established VLLM compression literature. The claim that CNN outperforms Q-Former under data constraints is robustly evidenced.
- **Medium:** The performance recovery through post-training and dynamic sampling is plausible but contingent on the undisclosed dataset quality and composition. The "see-saw" mitigation via Shapley merging is theoretically sound but lacks implementation transparency.
- **Low:** The claim of "outperforming baselines across text-oriented benchmarks" is based on comparison with FastV and unspecified baselines. Without access to the exact evaluation protocols or competing models, this claim is difficult to verify independently.

## Next Checks

1. **Dataset Composition Audit:** Reconstruct the 2M re-alignment dataset by combining publicly available OCR datasets (FinTabNet, DocVQA train, etc.) and verify if the 50%/75% compression performance gap persists on this mix. This checks if the compression mechanism is data-agnostic or dataset-dependent.

2. **Compression Ratio Sensitivity Test:** Systematically train FCoT-VL at 25%, 50%, 75%, and 87.5% compression ratios on a held-out subset of DocVQA and ChartQA. Measure not just accuracy but also inference speed (ms/token) to quantify the trade-off curve and identify the "knee point" where performance degrades sharply.

3. **Shapley Merging Overhead Benchmark:** Implement the Shapley value calculation for model merging using a subset of checkpoints. Benchmark the wall-clock time and memory overhead against simpler alternatives (e.g., Model Soups/weighted averaging) on the same hardware (64 Ascend 910 NPUs) to assess its practical cost-benefit.