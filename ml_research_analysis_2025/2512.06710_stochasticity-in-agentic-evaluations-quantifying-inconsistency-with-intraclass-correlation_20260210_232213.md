---
ver: rpa2
title: 'Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass
  Correlation'
arxiv_id: '2512.06710'
source_url: https://arxiv.org/abs/2512.06710
tags:
- variance
- evaluation
- accuracy
- agentic
- trials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of unreliable agentic evaluations
  in large language models, where current practice of reporting single accuracy numbers
  obscures the variance and randomness inherent in agent behavior. The core method
  introduces the Intraclass Correlation Coefficient (ICC) from measurement science
  to quantify evaluation stability by decomposing variance into between-query (task
  difficulty) and within-query (agent inconsistency) components.
---

# Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation

## Quick Facts
- **arXiv ID:** 2512.06710
- **Source URL:** https://arxiv.org/abs/2512.06710
- **Reference count:** 8
- **Primary result:** Introduces ICC metric to quantify evaluation consistency in agentic LLMs, showing significant variance in task structure and recommending standardized reporting of accuracy plus ICC and within-query variance.

## Executive Summary
This paper addresses the critical problem of unreliable agentic evaluations in large language models, where single accuracy numbers obscure the substantial variance and randomness inherent in agent behavior. The authors introduce the Intraclass Correlation Coefficient (ICC) from measurement science to quantify evaluation stability by decomposing variance into between-query (task difficulty) and within-query (agent inconsistency) components. Through experiments on GAIA and FRAMES benchmarks across five model families, the study demonstrates that ICC varies significantly with task structure and provides evidence-based recommendations for reporting evaluation metrics that capture this stochasticity.

## Method Summary
The core method applies the Intraclass Correlation Coefficient (ICC) framework to agentic evaluation data, treating each query as a random effect and decomposing total variance into between-query and within-query components. The approach uses variance component analysis where total variance = between-query variance + within-query variance, with ICC calculated as the ratio of between-query to total variance. The study implements this by running multiple trials (n=8-64) per query across different model families on GAIA and FRAMES benchmarks, then analyzing convergence patterns and variance decomposition. The methodology assumes that query-level variance components remain stable across different model families and task domains, with controlled prompting strategies to mitigate prompt sensitivity effects.

## Key Results
- ICC varies significantly with task structure: reasoning/retrieval tasks (FRAMES) show ICC=0.4955-0.7118, while agentic tasks (GAIA) show ICC=0.304-0.774
- Convergence analysis reveals ICC stabilizes by n=8-16 trials for structured tasks and nâ‰¥32 for complex reasoning tasks
- Variance decomposition demonstrates that within-query variance can exceed between-query variance in some cases, indicating substantial agentic stochasticity
- The study recommends reporting accuracy alongside ICC and within-query variance as standard practice for agentic evaluations

## Why This Works (Mechanism)
The ICC framework works because it applies established measurement science principles to quantify reliability in repeated measurements. By treating each query as a random effect and decomposing variance into systematic (between-query) and random (within-query) components, the method captures the fundamental trade-off between task difficulty variation and agentic stochasticity. The variance decomposition assumes that observed variability in agent outputs stems from either the inherent difficulty of different queries or the stochastic nature of agent decision-making, allowing researchers to quantify how much of the observed variance is due to each factor. This statistical framework enables evidence-based decisions about how many trials are needed to achieve reliable measurements and provides a standardized way to report evaluation uncertainty.

## Foundational Learning
**Intraclass Correlation Coefficient (ICC)**: A reliability metric that quantifies the proportion of total variance attributable to between-group differences versus within-group variation. Why needed: Standard accuracy metrics don't capture the stochastic nature of agent outputs. Quick check: ICC close to 1 indicates high consistency across trials, close to 0 indicates high randomness.

**Variance Component Analysis**: Statistical technique that decomposes total variance into constituent components (between-query and within-query). Why needed: To isolate agentic stochasticity from task difficulty effects. Quick check: Sum of decomposed variance components should equal total observed variance.

**Random Effects Models**: Statistical framework where query-level effects are treated as random samples from a larger population. Why needed: Each evaluation query represents a different instance of task difficulty. Quick check: Model assumptions include normally distributed query effects and constant within-query variance.

**Evaluation Card Framework**: Standardized reporting template that captures accuracy, ICC, and variance metrics. Why needed: Current evaluation practices lack standardized uncertainty quantification. Quick check: Complete evaluation cards enable reproducibility and fair comparison across studies.

**Convergence Analysis**: Statistical method for determining the number of trials needed for stable metric estimates. Why needed: Empirical evaluation requires balancing measurement accuracy against computational cost. Quick check: ICC should stabilize within acceptable tolerance as trial count increases.

## Architecture Onboarding
**Component Map:** Evaluation Queries -> Multiple Trials -> Variance Decomposition -> ICC Calculation -> Evaluation Card Generation

**Critical Path:** The essential measurement pipeline runs from query execution through multiple trials, variance decomposition, and final metric calculation. Each stage depends on the previous: trials must be executed before variance can be decomposed, and variance components must be computed before ICC can be calculated.

**Design Tradeoffs:** The study balances measurement accuracy against computational cost by determining optimal trial numbers (n=8-32) based on convergence analysis. The choice of variance decomposition assumes that all within-query variability stems from agentic stochasticity rather than prompt sensitivity, though controlled prompting partially mitigates this concern.

**Failure Signatures:** Low ICC values (near 0) indicate high agentic stochasticity that may render evaluations unreliable. High within-query variance relative to between-query variance suggests that additional trials are needed for stable measurements. Non-convergent ICC patterns across trial counts indicate unstable evaluation conditions.

**First Experiments:** 1) Run 8 trials per query on a subset of tasks to establish baseline ICC patterns. 2) Vary trial count (8, 16, 32, 64) on representative queries to determine convergence thresholds. 3) Apply variance decomposition to identify which tasks exhibit high agentic stochasticity versus task difficulty effects.

## Open Questions the Paper Calls Out
None

## Limitations
- ICC analysis only covers five model families on two curated benchmarks, limiting generalizability
- Convergence thresholds (n=8-32) may not extend to more complex agentic tasks with tool use or multimodal inputs
- Variance decomposition assumes all within-query variability stems from agentic stochasticity, not measurement artifacts
- Analysis focuses on single-turn outputs rather than entire agent execution traces where stochasticity compounds

## Confidence
- ICC as reliable metric for evaluation consistency: **High**
- Convergence thresholds (n=8-32) are task-dependent: **Medium**
- Variance decomposition accurately isolates agentic vs task difficulty effects: **Medium**
- Reporting ICC should become standard evaluation practice: **High**

## Next Checks
1. Replicate variance decomposition analysis across 10+ additional agentic benchmarks spanning different domains (medical, code, creative) to test generalizability of ICC patterns
2. Conduct ablation studies varying prompt templates, temperature settings, and evaluation frameworks to isolate true agentic stochasticity from measurement artifacts
3. Extend analysis to multi-turn agent executions where stochastic decisions compound across tool calls and planning steps, measuring how ICC evolves with execution depth