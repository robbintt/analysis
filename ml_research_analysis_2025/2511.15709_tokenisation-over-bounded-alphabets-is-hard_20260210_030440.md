---
ver: rpa2
title: Tokenisation over Bounded Alphabets is Hard
arxiv_id: '2511.15709'
source_url: https://arxiv.org/abs/2511.15709
tags:
- which
- tokenisation
- tokeniser
- problem
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tokenisation is the process of splitting character strings into
  subwords, a critical step in natural language processing pipelines. While prior
  work has shown that finding optimal tokenisers is NP-complete, these results assumed
  unbounded alphabets, which is unrealistic in practice.
---

# Tokenisation over Bounded Alphabets is Hard

## Quick Facts
- arXiv ID: 2511.15709
- Source URL: https://arxiv.org/abs/2511.15709
- Reference count: 40
- Primary result: Tokenisation is NP-complete even over binary/unary alphabets and admits no PTAS unless P=NP.

## Executive Summary
Tokenisation—splitting character strings into subwords—is a critical preprocessing step for language models. While prior work showed finding optimal tokenisers is NP-complete, these results assumed unbounded alphabets. This paper closes that gap by proving tokenisation remains computationally intractable even over the smallest possible alphabets. The authors show that binary and unary tokenisation are not only NP-complete but also inapproximable within any polynomial-time approximation scheme. These results establish that the computational hardness of tokenisation is fundamental rather than an artifact of large alphabets or complex constructions.

## Method Summary
The authors prove hardness through polynomial-time reductions from known hard problems (3-OCC-MAX2SAT and Vertex Cover) to tokenisation variants. For binary alphabets, they construct datasets where finding optimal vocabularies corresponds to satisfying boolean formulas. For unary alphabets, they reduce Vertex Cover to direct tokenisation. The proofs establish NP-completeness for both bottom-up (merge sequence selection) and direct (vocabulary selection) tokenisation, and further show no PTAS exists by proving a gap problem is NP-hard. The constructions ensure that any efficient algorithm for tokenisation would solve these known hard problems efficiently, contradicting P≠NP.

## Key Results
- Binary tokenisation (both direct and bottom-up) is NP-complete
- Binary tokenisation admits no polynomial-time approximation scheme (unless P=NP)
- Unary direct tokenisation is strongly NP-complete
- The hardness results hold regardless of alphabet size—proving hardness for binary alphabets extends to all larger alphabets

## Why This Works (Mechanism)

### Mechanism 1: Hardness via Problem Reduction
- **Claim:** The computational intractability of tokenisation is intrinsic to the problem structure and persists even for the smallest possible alphabets.
- **Mechanism:** The authors use polynomial-time reductions, mapping instances of known hard problems (specifically 3-occurrence maximum 2-satisfiability or 3-OCC-MAX2SAT) to the tokenisation problem. They construct a dataset where finding an optimal binary vocabulary (Direct Tokenisation) or merge sequence (Bottom-Up) is mathematically equivalent to satisfying a specific boolean formula.
- **Core assumption:** Assumption: P≠NP.
- **Evidence anchors:**
  - [Abstract] "proving that even binary alphabets make the problem NP-complete"
  - [Section 4.1] Defines the reduction source (3-OCC-MAX2SAT) and [Section 4.2.1] details Reduction 1 mapping variables Xj to token choices 1xj^T vs 1xj^F.
  - [Corpus] Related work on NP-Hard lower bounds (e.g., arXiv:2501.15446) supports the methodology of proving hardness through reduction.
- **Break condition:** If an efficient algorithm for tokenisation existed, it would solve 3-OCC-MAX2SAT efficiently, contradicting the assumption that P≠NP.

### Mechanism 2: Inapproximability via Gap Preservation
- **Claim:** Efficient algorithms cannot approximate the optimal compression ratio arbitrarily well (i.e., the problem is not in PTAS).
- **Mechanism:** The authors prove the hardness of a "gap problem." They show it is NP-hard to distinguish between a dataset that can be compressed to size δ^+ and one that requires at least δ^- (where δ^- > δ^+). If you cannot efficiently decide which side of the gap an instance falls on, you cannot construct a polynomial-time approximation scheme (PTAS).
- **Core assumption:** The specific constants derived from the 3-OCC-MAX2SAT gap instances (e.g., γ^-, γ^+ bounds) hold for the constructed tokenisation instances.
- **Evidence anchors:**
  - [Abstract] "...admit no polynomial-time approximation scheme (unless P=NP)."
  - [Section 4.2.2 / Theorem 2] Proves the gap problem is NP-hard, establishing a constant lower bound on the approximation ratio (approx 1.000002).
  - [Corpus] Weak signal; corpus focuses on satisfiability complexity rather than approximation bounds specifically for tokenisation.
- **Break condition:** If a PTAS existed, the gap could be bridged by running the approximation scheme with ε small enough to distinguish δ^+ from δ^-.

### Mechanism 3: Strong NP-Completeness in Unary
- **Claim:** The complexity is not an artifact of "large numbers" in the input (pseudo-polynomial solutions are unlikely), but structural.
- **Mechanism:** The authors reduce the Vertex Cover problem to Unary Direct Tokenisation. In unary representation (strings like aaaa...), the input size scales linearly with the numerical value (length), rather than logarithmically. Proving NP-completeness here (Strong NP-completeness) implies the problem remains hard even when numbers (string lengths) are polynomially bounded in the input size.
- **Core assumption:** Vertex Cover is NP-complete (standard assumption).
- **Evidence anchors:**
  - [Section 5.1] "We prove that the unary direct tokenisation problem is strongly NP-complete."
  - [Section 5.1 / Reduction 3] Maps graph vertices to "vertex-strings" and edges to "edge-strings," ensuring that optimal tokenization corresponds to a minimum vertex cover.
- **Break condition:** If the problem were only weakly NP-complete, a dynamic programming solution (pseudo-polynomial time) might suffice for fixed alphabets; this proof rules out that "easy way out."

## Foundational Learning

- **Concept:** Computational Complexity Classes (NP, Strong NP, PTAS)
  - **Why needed here:** To interpret the paper's "bad news." You must distinguish between "hard to solve exactly" (NP-complete) vs. "hard to solve approximately" (No PTAS) vs. "hard only because numbers are huge" (Weak NP) vs. "structurally hard" (Strong NP).
  - **Quick check question:** If a problem is Strongly NP-complete, does restricting the input values to be small (polynomial in input length) make it easy?

- **Concept:** Polynomial-Time Reductions
  - **Why needed here:** The proofs rely entirely on mapping one problem to another. Understanding this "mapping" is key to seeing why tokenisation is hard—it effectively contains the logic of SAT or Vertex Cover.
  - **Quick check question:** In Reduction 1, what represents a Boolean variable assignment in the resulting tokenisation instance?

- **Concept:** Direct vs. Bottom-Up Tokenisation
  - **Why needed here:** The paper analyzes these two distinct mathematical formulations. Direct tokenisation searches the power set of all substrings (P(Σ^+)), while Bottom-Up searches the space of merge sequences (M^*).
  - **Quick check question:** Which mechanism produces the vocabulary by applying a sequence of pair merges, and which selects the vocabulary directly to optimize an objective?

## Architecture Onboarding

- **Component map:** Input Dataset D over bounded alphabet Σ → Search Space P(Σ^+) (Direct) or M^* (Bottom-Up) → Objective Compressed Length G_ℓ or Compression Reduction G_r → Constraint Vocabulary size K → Output Vocabulary S or Merge Sequence m
- **Critical path:** The "Reduction Engine" (theoretical). In practice, the critical path is the Heuristic Selection. Since the paper proves the optimal path is blocked, the architecture must use heuristics (like greedy BPE) which the paper explicitly notes are non-optimal but necessary.
- **Design tradeoffs:**
  - Optimality vs. Speed: The paper proves you cannot have both. BPE/UnigramLM trade optimality for polynomial runtime.
  - Alphabet Size: Binary/Unary tokenisation is hardest to approximate. Larger alphabets (Bytes/Unicode) are supersets of these, so they inherit the hardness.
  - Objective Choice: G_ℓ (length) relates to inference throughput; G_r (reduction) relates to "savings." The paper suggests G_ℓ is more natural for LLM costs.
- **Failure signatures:**
  - The "Optimal" Trap: Attempting to design a polynomial-time algorithm guaranteed to find the compression-optimal tokenizer. The paper proves this is impossible (unless P=NP).
  - Approximation Limits: Expecting a tokenization algorithm to get arbitrarily close to the theoretical minimum compression ratio (PTAS). Theorem 2 suggests there is a hard floor on approximation quality.
- **First 3 experiments:**
  1. Replicate Reduction 1 (Toy Scale): Create a small 3-OCC-MAX2SAT instance (e.g., 3 variables, 2 clauses), run it through the reduction to create dataset D, and verify that standard BPE fails to find the optimal compression implied by the satisfying assignment.
  2. Measure Approximation Gap: Run a standard tokenizer (like GPT-2's BPE) on the "hard instances" constructed in Section 4/5. Compare the achieved compression δ_BPE against the theoretical bound δ_opt to see how close the heuristic gets to the proven "hardness gap."
  3. Unary Stress Test: Construct the "Vertex Cover" dataset from Reduction 3 using unary strings. Test if a tokenizer optimized for standard text handles the "combinatorial logic" embedded in the string lengths effectively.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a polynomial-time constant-factor approximation algorithm exist for binary tokenisation?
- Basis in paper: [explicit] The authors prove that binary tokenisation has no Polynomial-Time Approximation Scheme (PTAS), but explicitly state, "it is unclear whether any constant approximation ratio can even be obtained."
- Why unresolved: While the paper establishes that the problem cannot be approximated arbitrarily well (ruling out PTAS), it does not determine if the problem is APX-hard or if specific approximation algorithms can achieve a fixed ratio.
- What evidence would resolve it: A proof of APX-hardness (ruling out constant factors) or the proposal of an algorithm that guarantees a bounded approximation ratio.

### Open Question 2
- Question: Is the standard bottom-up tokenisation problem NP-hard when restricted to unary alphabets?
- Basis in paper: [explicit] The conclusion notes that while strong NP-hardness was proven for direct tokenisation, the authors found "no hardness result for unary (standard) bottom-up tokenisation."
- Why unresolved: The specific constraints of the standard bottom-up process (exhaustive, ordered merging) differ from direct or OPE tokenisation, preventing the application of the reduction techniques used elsewhere in the paper.
- What evidence would resolve it: A polynomial-time reduction from a known NP-hard problem to unary bottom-up tokenisation, or a polynomial-time algorithm solving it.

### Open Question 3
- Question: Do hardness results hold for tokenisation objectives other than compression, such as likelihood maximization?
- Basis in paper: [explicit] The authors acknowledge their work is limited by the focus on compression and state that "the hardness of both other objectives and variants remains open."
- Why unresolved: The hardness proofs in the paper rely specifically on minimizing the compressed string length; it is unknown if the complexity remains intractable when optimizing for probability or frequency.
- What evidence would resolve it: Extending the provided reductions to alternative objective functions or demonstrating efficient algorithms for them.

## Limitations

- Reduction-specific assumptions: The proofs depend on specific reductions from 3-OCC-MAX2SAT and Vertex Cover problems, which may not reflect practical tokenisation scenarios.
- Single-objective focus: The analysis concentrates on minimizing compressed length G_ℓ, while practical systems often optimize multiple objectives simultaneously.
- Static dataset assumption: The reductions assume a fixed dataset, not addressing streaming data or distribution shifts common in real-world tokenisation.

## Confidence

**High confidence** in NP-completeness results for both binary and unary cases:
- The reductions are explicit and constructive
- The mathematical proofs follow standard complexity theory methodology
- The results align with established complexity results for related problems

**Medium confidence** in the PTAS non-existence claim:
- The inapproximability gap is very small (~1.000002), making empirical verification challenging
- The proof relies on specific parameters from prior work on 3-OCC-MAX2SAT
- The practical significance of such a small gap is debatable

**Medium confidence** in Strong NP-completeness for unary:
- The Vertex Cover reduction is well-established
- However, the specific encoding details and parameter choices could affect practical relevance
- The connection between theoretical unary hardness and practical text tokenisation is indirect

## Next Checks

1. **Empirical gap verification**: Implement Reduction 1 with small-scale 3-OCC-MAX2SAT instances (3-4 variables, 4-6 clauses). Run standard tokenisers (BPE, UnigramLM) and measure actual compression ratios against the theoretical bounds. Compare the empirical approximation ratio to the theoretical gap bound.

2. **Robustness to data variation**: Test whether the hardness persists when the synthetic datasets are perturbed with noise or alternative character distributions. Generate the "hard" instances from the reductions, then randomly modify a percentage of characters. Re-run tokenisation and measure whether optimal compression remains computationally hard.

3. **Multi-objective extension**: Design a multi-objective variant that combines compressed length with token vocabulary diversity or merge operation count. Investigate whether similar hardness results hold when these additional constraints are added, or whether they create new algorithmic opportunities.