---
ver: rpa2
title: 'Panda: Test-Time Adaptation with Negative Data Augmentation'
arxiv_id: '2511.10481'
source_url: https://arxiv.org/abs/2511.10481
tags:
- augmentation
- bias
- image
- prediction
- corruption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses prediction bias and computational inefficiency
  in test-time adaptation (TTA) for vision-language models under image corruptions.
  The proposed Panda method uses negative data augmentation (NDA) to suppress corruption-related
  features while preserving class-relevant information, overcoming the limitations
  of positive data augmentation.
---

# Panda: Test-Time Adaptation with Negative Data Augmentation

## Quick Facts
- arXiv ID: 2511.10481
- Source URL: https://arxiv.org/abs/2511.10481
- Reference count: 40
- Primary result: Improves test-time adaptation performance with negative data augmentation, achieving +3.3% on CIFAR-10-C, +2.2% on CIFAR-100-C, and +2.0% on ImageNet-C

## Executive Summary
Panda addresses prediction bias and computational inefficiency in test-time adaptation (TTA) for vision-language models under image corruptions. The method introduces negative data augmentation (NDA) that suppresses corruption-related features while preserving class-relevant information, overcoming limitations of positive data augmentation. By generating shared negative augmentations through patch shuffling and subtracting their mean feature from original embeddings, Panda effectively debiases predictions with minimal computational overhead.

## Method Summary
Panda implements negative data augmentation by shuffling image patches within batches to create shared negative examples. The method computes feature embeddings for both original and shuffled images, then subtracts the mean feature of negative augmentations from original embeddings to suppress corruption-related information. This approach is designed to work across multiple TTA methods and requires minimal computational overhead compared to traditional adaptation approaches.

## Key Results
- Achieves average improvements of +3.3% on CIFAR-10-C, +2.2% on CIFAR-100-C, and +2.0% on ImageNet-C
- Demonstrates effectiveness across multiple TTA methods with less than 10% runtime increase
- Shows consistent performance gains across different corruption types and severity levels

## Why This Works (Mechanism)
Panda works by exploiting the fact that image corruptions often introduce localized, patch-level artifacts that can be detected and suppressed through negative augmentation. By shuffling patches within batches, the method creates synthetic corruptions that share statistical properties with real corruptions but lack semantic meaning. Subtracting these corrupted features from clean features effectively removes bias while preserving class-relevant information, allowing models to maintain performance on corrupted test data.

## Foundational Learning
**Test-Time Adaptation (TTA)**: Why needed - Adapts pre-trained models to distribution shifts without retraining. Quick check - Compare performance on clean vs corrupted test data.
**Negative Data Augmentation**: Why needed - Suppresses unwanted features while preserving relevant information. Quick check - Measure feature similarity between original and augmented data.
**Patch-Based Image Representation**: Why needed - Enables localized corruption detection and manipulation. Quick check - Verify patch shuffling preserves overall image structure.
**Feature Mean Subtraction**: Why needed - Removes bias introduced by corruptions. Quick check - Monitor feature distribution changes before and after subtraction.
**Batch-Level Processing**: Why needed - Enables efficient computation of shared negative augmentations. Quick check - Measure computational overhead vs batch size.
**Corruption Robustness**: Why needed - Ensures model performance under real-world degradation. Quick check - Test across multiple corruption types and severity levels.

## Architecture Onboarding
**Component Map**: Input Images -> Patch Extraction -> Feature Embedding -> Negative Augmentation (Patch Shuffling) -> Feature Subtraction -> Output Predictions
**Critical Path**: The core pipeline involves generating negative augmentations through patch shuffling, computing feature embeddings for both original and augmented images, and performing mean feature subtraction to debias predictions.
**Design Tradeoffs**: Patch shuffling provides efficient negative augmentation but may not capture all corruption types. The shared augmentation approach reduces computation but assumes similar corruption patterns within batches.
**Failure Signatures**: Poor performance on non-localized corruptions, computational overhead scaling issues with large batch sizes, and reduced effectiveness when corruption patterns vary significantly within batches.
**Three First Experiments**:
1. Baseline comparison without negative augmentation across all test datasets
2. Patch shuffling ablation with different patch sizes and shuffling strategies
3. Computational overhead measurement across varying batch sizes and model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains vary significantly across datasets, suggesting effectiveness depends on dataset characteristics
- Patch-based shuffling assumes localized corruption patterns that may not generalize to all corruption types
- Computational overhead claim of "less than 10%" may not hold across different batch sizes and architectures

## Confidence
*High Confidence*: Core technical contribution of using negative data augmentation to suppress corruption-related features is well-supported by experimental results
*Medium Confidence*: Claims of improving multiple TTA methods are supported, though extent of improvement varies
*Low Confidence*: Assertion that Panda overcomes "inherent limitations of positive data augmentation" may be overstated

## Next Checks
1. Test Panda's effectiveness on non-image domains (text or speech) where corruption patterns differ from patch-based image corruptions
2. Conduct systematic experiments varying batch sizes to measure computational overhead scaling
3. Implement ablation study comparing patch shuffling with alternative negative augmentation strategies like adversarial perturbations or frequency-based masking