---
ver: rpa2
title: BCE vs. CE in Deep Feature Learning
arxiv_id: '2505.05813'
source_url: https://arxiv.org/abs/2505.05813
tags:
- adamw
- scores
- feature
- training
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically and empirically compares Cross-Entropy
  (CE) and Binary Cross-Entropy (BCE) losses in deep feature learning. The authors
  prove that BCE can also maximize intra-class compactness and inter-class distinctiveness
  when reaching its minimum, leading to neural collapse (NC), a phenomenon previously
  established only for CE.
---

# BCE vs. CE in Deep Feature Learning

## Quick Facts
- arXiv ID: 2505.05813
- Source URL: https://arxiv.org/abs/2505.05813
- Reference count: 40
- This paper theoretically and empirically proves that Binary Cross-Entropy (BCE) can induce neural collapse and enhance feature properties better than Cross-Entropy (CE).

## Executive Summary
This paper provides a comprehensive theoretical and empirical comparison of Binary Cross-Entropy (BCE) and Cross-Entropy (CE) losses in deep feature learning. The authors prove that BCE can maximize intra-class compactness and inter-class distinctiveness, leading to neural collapse (NC) - a phenomenon previously established only for CE. Through extensive experiments across multiple architectures (ResNet, DenseNet, DeiT III) and datasets (MNIST, CIFAR-10/100, ImageNet), they demonstrate that BCE measures absolute decision scores across all samples while CE measures relative scores per sample, enabling BCE to explicitly enhance feature properties. The classifier biases in BCE play a substantial role in constraining decision scores during training, unlike in CE where they have minimal effect. BCE-trained models consistently achieve higher classification accuracy and uniform accuracy, indicating better feature compactness and distinctiveness.

## Method Summary
The paper compares BCE and CE through theoretical analysis and extensive experiments. BCE is implemented as K independent binary log losses (sum of sigmoid+log terms), while CE uses softmax+log. Training uses standard architectures (ResNet18/50, DenseNet121) with AdamW optimizer, cosine learning rate schedule, and weight decay. For NC analysis, separate weight decay parameters are used for weights (λ_W), features (λ_H), and biases (λ_b) set to 5×10^-4. Classification experiments use global weight decay (5×10^-4 for SGD, 0.05 for AdamW) with data augmentations including Mixup and CutMix. The study evaluates both standard accuracy and uniform accuracy, along with neural collapse metrics (NC1, NC2, NC3) measuring feature compactness and inter-class distinctiveness.

## Key Results
- BCE can induce neural collapse (NC) with faster convergence than CE in the first 20 epochs
- BCE consistently achieves higher uniform accuracy (A_Uni) and classification accuracy across all tested architectures and datasets
- Classifier biases in BCE play a substantial role in constraining decision scores, unlike CE where biases have minimal effect
- BCE's absolute value measurement of decision scores enables explicit feature property enhancement across all samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BCE's absolute value measurement of decision scores enables explicit feature property enhancement across all samples.
- **Mechanism:** Unlike CE which optimizes relative score rankings per sample, BCE independently optimizes positive scores (w_k^T h_i^k - b_k > 0) and negative scores (w_j^T h_i^k - b_j < 0) to absolute thresholds, creating bounded feature regions around each classifier vector with radius b_k.
- **Core assumption:** Features can be driven to satisfy absolute margin constraints simultaneously across the entire dataset.
- **Evidence anchors:**
  - [abstract] "BCE measures the absolute values of decision scores and adjust the positive/negative decision scores across all samples to uniformly high/low levels"
  - [section 3.3] "BCE presents explicitly constraint across-samples in the training... requires the positive decision scores of all samples are uniformly larger than threshold t = 0"
  - [corpus] Limited direct corpus support; this appears to be novel theoretical contribution requiring validation.
- **Break condition:** When classes are highly imbalanced or overlapping such that uniform absolute margins become infeasible.

### Mechanism 2
- **Claim:** Classifier biases in BCE provide explicit, consistent constraints on decision scores throughout training.
- **Mechanism:** BCE's bias gradient (Eq. 21) directly constrains exponential decision scores for all samples, converging to unique solution b* satisfying Eq. 12. This creates a fixed separation between positive/negative score distributions, unlike CE where bias gradient vanishes toward λ_b·b at minimum.
- **Core assumption:** The optimization landscape permits a unique optimal bias that meaningfully separates score distributions.
- **Evidence anchors:**
  - [abstract] "classifier biases in BCE present a substantial constraint on the decision scores to explicitly enhance the feature properties"
  - [section 3.2] "BCE loss has only one minimum point in term of b... optimal classifier bias b = b*1 will separate the positive and negative decision scores"
  - [section 4.1, Fig. 3] Shows BCE biases consistently separate positive/negative scores regardless of initialization or λ_b, while CE biases show no such correlation
  - [corpus] Weak corpus validation; neighboring papers focus on discriminative learning but not bias mechanics specifically.
- **Break condition:** When λ_b is set too high (≥0.5 in experiments), over-regularization prevents bias from reaching optimal separation point.

### Mechanism 3
- **Claim:** BCE's gradient structure naturally homogenizes decision scores within each class, accelerating neural collapse.
- **Mechanism:** Small positive scores receive large gradient updates (η(1 - 1/(1+exp(-score)))), while large scores receive small updates, creating self-balancing dynamics. CE gradients depend on relative softmax probabilities, allowing samples with similar predicted probabilities but different absolute scores to receive identical updates.
- **Core assumption:** Batching and learning rate schedules permit convergence to uniform score distributions before other training dynamics dominate.
- **Evidence anchors:**
  - [section 3.3, Eq. 19] Gradient formula shows inverse relationship between score magnitude and update amplitude for BCE
  - [section 3.3] "In contrast... BCE could adjust them in a uniform way and push them to a uniformly low level, enhancing the inter-class distinctiveness"
  - [section 4.1, Fig. 2] NC metrics (NC1, NC2, NC3) decrease faster with BCE than CE in first 20 epochs
  - [corpus] Assumption: Related work on discriminative loss (arXiv:2405.18499) suggests gradient-based feature separation is effective, though not BCE-specific.
- **Break condition:** With very large batch sizes (≥2048 in experiments), gradient dynamics shift and theoretical convergence properties may not hold.

## Foundational Learning

- **Concept: Neural Collapse (NC)**
  - **Why needed here:** The paper's theoretical contribution proves BCE leads to NC (Theorem 3.2), requiring understanding of NC1 (within-class variability collapse), NC2 (classifiers forming simplex ETF), and NC3 (self-duality between features and classifiers).
  - **Quick check question:** Can you explain why NC2's equiangular tight frame property indicates maximum inter-class distinctiveness?

- **Concept: Decision Score Geometry**
  - **Why needed here:** The paper's core insight distinguishes between relative score optimization (CE: sample-by-sample ranking) and absolute score optimization (BCE: unified thresholds), which requires geometric intuition about how biases define decision boundaries.
  - **Quick check question:** In a 2D feature space with 3 classes, sketch how CE vs. BCE would define different decision regions based on their score constraints.

- **Concept: Regularization-Optimization Interaction**
  - **Why needed here:** Theorems 3.1 and 3.2 depend critically on weight decay parameters (λ_W, λ_H, λ_b) determining both the existence and location of global minima. The bias decay λ_b has qualitatively different effects on CE (ridge of minima) vs. BCE (unique minimum).
  - **Quick check question:** If λ_b = 0 for both losses, why does CE have multiple valid bias solutions while BCE has only one?

## Architecture Onboarding

- **Component map:** Input → Backbone (ResNet/DenseNet/etc.) → Feature h ∈ R^d → Linear Classifier (W ∈ R^K×d, b ∈ R^K) → Decision Scores z = Wh - b → [BCE Loss: K independent sigmoid+log terms] OR [CE Loss: softmax+log] → Gradients → Update W, b, backbone weights

- **Critical path:** 
  1. Initialize classifier bias appropriately (paper uses Kaiming with mean adjustment for experiments)
  2. Set weight decay parameters: λ_W = λ_H ≈ 5×10^-4, λ_b = 5×10^-4 for NC experiments; global λ for classification
  3. Use AdamW optimizer with cosine LR schedule (superior to SGD+step for NC convergence on complex datasets)
  4. Monitor NC metrics during training—BCE should show faster initial decrease

- **Design tradeoffs:**
  - **BCE advantages:** Better feature compactness/distinctiveness (Table 2), higher uniform accuracy A_Uni (Table 1), explicit bias constraint
  - **BCE costs:** More sensitive to bias initialization (though self-correcting), may over-regularize with high λ_b
  - **CE advantages:** Simpler gradient (single softmax), well-understood behavior
  - **CE limitations:** Implicit feature enhancement, bias has minimal effect, slower NC convergence
  - **Assumption:** BCE's benefits generalize to architectures beyond CNNs (preliminary evidence from DeiT III with Transformers)

- **Failure signatures:**
  - **NC failure with SGD on CIFAR-100:** High standard deviation in decision scores (>0.5), NC metrics don't converge to 0 (Table 8, ResNet50 shows A=2.52% with CE, 7.67% with BCE—complete failure)
  - **Bias initialization failure with CE:** Final bias ≈ initial bias when λ_b = 0, showing no learning (Table 9, CE columns)
  - **Large batch failure:** Batch size ≥2048 causes decision scores to diverge from theoretical values (Table 11, Fig. 7)
  - **High λ_b failure:** λ_b ≥ 0.5 prevents bias from reaching separation point (Table 10, BCE with λ_b=0.5 shows reduced score separation)

- **First 3 experiments:**
  1. **Reproduce NC metrics comparison:** Train ResNet18 on CIFAR-10 with BCE vs. CE, λ_W=λ_H=λ_b=5×10^-4, plot NC1/NC2/NC3 over 100 epochs. Expected: BCE curves drop faster in epochs 0-20, both converge near 0 by epoch 100.
  2. **Bias sensitivity analysis:** Train with varying initial bias means (0, 2, 5, 10) and fixed λ_b=0. Plot final bias vs. initial bias and final score distributions (replicate Fig. 3 top row). Expected: CE final bias ≈ initial bias; BCE final bias converges to unique value separating positive/negative scores regardless of initialization.
  3. **Classification performance on held-out classes:** Fine-tune pretrained ImageNet model (ResNet50) with BCE vs. CE for 30 epochs, measure both standard accuracy A and uniform accuracy A_Uni on validation set. Expected: BCE shows modest A improvement (77.12% vs. 76.74%) but substantial A_Uni improvement (66.92% vs. 34.48%) per Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Binary Cross-Entropy (BCE) induce generalized neural collapse (GNC) when the number of classes ($K$) exceeds the feature dimension ($d$)?
- **Basis in paper:** [explicit] The Discussion section states, "We speculate that BCE will exhibit similar properties and results in this context."
- **Why unresolved:** The theoretical proofs provided (Theorems 3.1 and 3.2) strictly assume $d \geq K-1$, leaving the high-dimensional class scenario ($K > d$) unproven.
- **What evidence would resolve it:** A theoretical extension of the global minimizer analysis for $K > d$, or empirical observation of GNC properties in BCE-trained models on large-scale recognition tasks (e.g., face recognition).

### Open Question 2
- **Question:** Does BCE effectively alter the threshold for minority class collapse in long-tailed recognition tasks compared to CE?
- **Basis in paper:** [explicit] The Discussion notes that results on imbalanced datasets "lead us believe the BCE can amplify this threshold," while the theoretical analysis focused only on balanced datasets.
- **Why unresolved:** The paper provides only preliminary empirical results on CIFAR100-LT (Table 4) without a theoretical mechanism explaining why BCE improves performance on imbalanced data.
- **What evidence would resolve it:** A formal analysis of the BCE minimizer under class imbalance (skewed $n_k$) or experiments varying the imbalance factor to identify the collapse threshold.

### Open Question 3
- **Question:** Do the explicit feature property constraints of BCE observed in CNNs transfer to Transformer-based architectures?
- **Basis in paper:** [explicit] The Discussion states the advantages of BCE "can also be demonstrated in other deep network models such as Transformers," citing DeiT III as potential evidence.
- **Why unresolved:** The experimental validation in the paper is restricted to CNNs (ResNet, DenseNet), and the specific bias-score dynamics were not analyzed in Transformers.
- **What evidence would resolve it:** Comparative experiments on Vision Transformers (ViT) analyzing the convergence of classifier biases and NC metrics under BCE vs. CE.

## Limitations

- BCE's advantage diminishes with high λ_b (≥0.5) or very large batch sizes (≥2048), suggesting sensitivity to training hyperparameters
- The theoretical proofs assume standard regularization settings that may not hold in practice with complex data augmentations
- Limited neighboring literature specifically addressing BCE's unique bias mechanics, though related work on discriminative learning exists

## Confidence

- **High confidence:** Theoretical framework (Theorem 3.1, 3.2) and fundamental mechanism of bias-driven decision score separation are mathematically rigorous and supported by consistent experimental evidence
- **Medium confidence:** Practical performance gains show BCE consistently outperforming CE in uniform accuracy and feature property metrics, though magnitude varies with architecture and dataset complexity
- **Low confidence:** Generalization to Transformers and other non-CNN architectures, as only preliminary evidence from DeiT III is presented

## Next Checks

1. **Cross-architecture validation:** Test BCE vs. CE on Vision Transformer models (ViT, Swin) with varying patch sizes to determine if the bias-driven feature enhancement mechanism generalizes beyond CNNs.

2. **Bias initialization robustness:** Systematically vary classifier bias initialization (uniform, normal, zero) and monitor convergence to b* across different λ_b values to quantify the self-correcting property's reliability in BCE.

3. **Real-world dataset performance:** Evaluate on larger, more diverse datasets (Places365, OpenImages) and fine-grained classification tasks to assess whether BCE's feature property advantages translate to consistent accuracy improvements in production scenarios.