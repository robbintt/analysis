---
ver: rpa2
title: 'Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular
  English'
arxiv_id: '2511.10846'
source_url: https://arxiv.org/abs/2511.10846
tags:
- emotion
- aave
- american
- anger
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how emotion recognition models handle African
  American Vernacular English (AAVE) and finds significant bias. Models falsely predict
  anger and disgust in AAVE at rates more than double those in General American English
  (GAE), with SpanEmo's false positive rate on anger increasing from 25% on GAE to
  60% on AAVE.
---

# Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English

## Quick Facts
- arXiv ID: 2511.10846
- Source URL: https://arxiv.org/abs/2511.10846
- Reference count: 40
- Primary result: Emotion recognition models falsely predict anger/disgust in AAVE at >2× the rate of GAE, with significant demographic correlations.

## Executive Summary
This paper reveals significant bias in emotion recognition models when processing African American Vernacular English (AAVE) compared to General American English (GAE). The study finds that models exhibit false positive rates for anger and disgust on AAVE text that are more than double those on GAE, with the SpanEmo model showing anger FPR increasing from 25% on GAE to 60% on AAVE. By developing a community-informed annotation protocol using African American AAVE speakers as annotators, the research demonstrates that these biases are driven by spurious correlations between profanity-based AAVE features and negative emotions in training data. The study also identifies demographic correlations where neighborhoods with higher African American populations receive systematically higher anger predictions and lower joy predictions from these models, raising concerns about stereotype reinforcement in real-world deployments.

## Method Summary
The researchers analyzed 2.7 million geo-tagged tweets from Los Angeles County, developing a Dialect Density Metric (DDM) based on 15 AAVE linguistic features. They collected human annotations for 875 tweets across seven emotions, using a "silver" labeling approach where high-DDM tweets were labeled exclusively by African American AAVE-fluent (ingroup) annotators while low-DDM tweets used all annotators. Model predictions were evaluated across multiple emotion recognition systems, with false positive/negative rates computed separately for high and low DDM texts. The team also linked model predictions to Census demographic data at the neighborhood level to examine demographic correlations. Linear regressions were used to identify which AAVE features most strongly influenced both model predictions and non-ingroup annotations.

## Key Results
- False positive rates for anger and disgust are more than double on AAVE versus GAE (SpanEmo: 25%→60% for anger)
- Model predictions and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations
- Neighborhoods with higher African American populations receive higher anger predictions and lower joy predictions from models
- Ingroup annotators identify more emotion overall in AAVE texts, except for disgust which outgroup members see more of

## Why This Works (Mechanism)

### Mechanism 1
Emotion recognition models exhibit false positive bias for "anger" and "disgust" on AAVE text due to spurious correlations between profanity-based dialect features and negative emotions in training data. Models trained predominantly on GAE data learn associations between specific AAVE markers and negative affect without cultural context, leading to systematic misclassification where profanity features correlate more strongly with model predictions than with ingroup human annotations.

### Mechanism 2
Emotion annotation is subjective and mediated by annotator group membership (race and dialect familiarity), creating divergent "ground truths" for model evaluation. Ingroup annotators perceive more emotion overall and show higher agreement on joy and anger within AAVE texts, while outgroup annotators disproportionately label AAVE texts as "disgust," potentially misinterpreting cultural communication practices as negative affect.

### Mechanism 3
Biased emotion predictions correlate with demographic composition, creating risk of reinforcing racial stereotypes at community level. Models predict higher anger in neighborhoods with higher African American populations and higher joy in neighborhoods with higher White populations, emerging from the link between dialect density, demographic patterns, and model bias.

## Foundational Learning

- **Dialect Density Metric (DDM)**: Quantifies AAVE strength in text on continuous scale using 15 linguistic features. Needed to measure dialect-based bias; differs from binary classification by capturing gradient AAVE usage. Quick check: How does DDM enable more nuanced bias analysis than simple AAVE/GAE binary classification?

- **False Positive Rate (FPR) Disparity**: Measures direction of errors, not overall accuracy. Key metric showing bias against AAVE (ΔFPR > 2 for anger). Quick check: Why might a model with 90% accuracy on both dialects still be considered biased?

- **Community-Informed "Silver" Labels**: Uses ingroup annotators for high-DDM texts to create culturally valid ground truth. Solves bias in standard gold labels. Quick check: How do silver labels differ from majority-vote gold labels and what problem do they solve?

## Architecture Onboarding

- **Component map**: Dialect Density Estimator -> Annotation Interface -> Emotion Classifiers -> Demographic Linkage
- **Critical path**: 1) Define AAVE features and implement DDM; 2) Collect annotations using community-informed protocol; 3) Run emotion classifiers on labeled dataset; 4) Compute false positive/negative rates stratified by DDM; 5) Perform linear regression to identify feature-level correlations
- **Design tradeoffs**: DDM vs. self-report (linguistic proxies avoid self-report bias but may misclassify); Silver labels vs. gold labels (improves cultural validity but increases noise); Geo-tagged data (enables demographic linkage but limits to non-random subset)
- **Failure signatures**: High FPR on anger/disgust for AAVE; low ingroup agreement; demographic-emotion correlation
- **First 3 experiments**: 1) Replicate on new dataset (different city/time period/platform); 2) Feature ablation (remove profanity-based features); 3) Annotator pool scaling (expand ingroup annotators 5x)

## Open Questions the Paper Calls Out
The authors identify expanding to Chicano American English as a natural progression, noting that comparing only African American and White communities missed the area's cultural diversity. They specifically call for examining how emotion recognition biases extend to Chicano American English, particularly in regions like Los Angeles where these dialects overlap.

## Limitations
- Findings based on single metropolitan area (Los Angeles) over specific time period (2010-2014), limiting generalizability
- Reliance on geo-tagged tweets represents non-random subset of users who enable location sharing
- Community-informed "silver" labels introduce potential subjectivity and may not capture full diversity of AAVE usage
- Emphasis on profanity-based features may oversimplify complex relationship between dialect, emotion, and cultural communication

## Confidence
- **High Confidence**: Higher FPR for anger/disgust on AAVE vs GAE across multiple models; demographic correlations between neighborhood composition and model predictions
- **Medium Confidence**: Profanity-based features as primary driver of bias; ingroup vs outgroup annotation pattern differences
- **Low Confidence**: Broader societal implications for stereotype reinforcement; specific recommendations for affective computing system design

## Next Checks
1. **Temporal and Geographic Replication**: Apply methodology to tweets from different city and/or time period to test persistence of dialect-based bias patterns across different demographic distributions
2. **Feature Ablation Experiment**: Systematically evaluate model performance after removing profanity-based features to determine if they are primary driver of bias or if other AAVE linguistic patterns contribute similarly
3. **Annotation Pool Scaling**: Expand ingroup annotator pool significantly (5x) and repeat annotation to assess stability and reliability of "silver" labels, particularly for high-DDM texts where cultural interpretation nuances are most critical