---
ver: rpa2
title: Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability
arxiv_id: '2510.12229'
source_url: https://arxiv.org/abs/2510.12229
tags:
- knobe
- effect
- moral
- finetuned
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether Large Language Models (LLMs) internalize\
  \ human-like moral biases during finetuning, focusing on the Knobe effect\u2014\
  a cognitive bias where negative outcomes are judged more intentional than positive\
  \ ones. Using Layer-Patching, a mechanistic interpretability technique, the authors\
  \ analyze three open-weights LLMs (Llama, Mistral, Gemma) to locate and mitigate\
  \ this bias."
---

# Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2510.12229
- Source URL: https://arxiv.org/abs/2510.12229
- Authors: Bianca Raimondi; Daniela Dalbagno; Maurizio Gabbrielli
- Reference count: 23
- Primary result: Layer-Patching successfully reduces the Knobe effect in finetuned LLMs by replacing mid-to-late layer activations with those from pretrained models, without degrading performance.

## Executive Summary
This paper investigates whether Large Language Models internalize human-like moral biases during finetuning, focusing on the Knobe effect—a cognitive bias where negative outcomes are judged more intentional than positive ones. Using Layer-Patching, a mechanistic interpretability technique, the authors analyze three open-weights LLMs (Llama, Mistral, Gemma) to locate and mitigate this bias. They find that the Knobe effect emerges primarily in mid-to-late Transformer layers after finetuning and can be effectively reduced by replacing finetuned activations with those from the corresponding pretrained models. The bias reduction is achieved without degrading model performance on standard benchmarks. Ablation studies across different model scales confirm that the effect is not size-dependent but consistently induced by finetuning.

## Method Summary
The authors employ Layer-Patching, a mechanistic interpretability intervention, to analyze and mitigate the Knobe effect in finetuned LLMs. The method involves running input through both pretrained and finetuned models, caching activations at each layer, then selectively replacing finetuned activations at a specific layer with those from the pretrained model before continuing the forward pass. This allows them to identify which layers encode the bias and test whether replacing them can reduce the effect while preserving general model capabilities.

## Key Results
- The Knobe effect (intentionality gap of 1.60–3.83) is localized to mid-to-late Transformer layers in finetuned models but absent in corresponding pretrained models (gap of 0.00–0.03 after patching).
- Activation patching reduces the Knobe effect by up to 96% without degrading model performance on standard benchmarks.
- Ablation studies across model scales (7B to 27B parameters) confirm the effect is induced by finetuning rather than model size.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Moral bias (the Knobe effect) is structurally localized to mid-to-late Transformer layers rather than distributed globally.
- **Mechanism**: Finetuning induces weight updates that disproportionately amplify activation differences in response to moral valence (negative vs. positive scenarios) within specific layers. These layers appear to handle high-level semantic abstractions, including moral reasoning.
- **Core assumption**: Transformer layers exhibit functional specialization, with later layers processing semantic/moral features distinct from early syntactic processing.
- **Evidence anchors**:
  - [Abstract]: "demonstrate that the bias is not only learned during finetuning but also localized in specific mid-to-late Transformer layers."
  - [Section 4.2]: "In the pretrained models... activation differences are uniformly minimal... In stark contrast, the finetuned model reveals a distinct pattern... Layers in the mid-to-late range exhibit strong and localized differences."
  - [Corpus]: Related work ("Dissecting Bias in LLMs") suggests bias is structurally represented, supporting the localization hypothesis, though generalizability to all biases requires caution.
- **Break condition**: If the bias were encoded diffusely across all layers or primarily in the embedding space, targeted intervention in later layers would fail to reduce the effect.

### Mechanism 2
- **Claim**: Activation patching causally disrupts the bias circuit by overwriting the "moral valence" signal before it propagates to the output head.
- **Mechanism**: By intercepting the residual stream at a critical layer $l$ in the finetuned model ($M_f$) and replacing it with the activation from the pretrained model ($M_p$), the forward pass completes using "unbiased" representations. This effectively subtracts the finetuning-induced delta while preserving the model's general reasoning capabilities.
- **Core assumption**: The pretrained and finetuned models share sufficient structural alignment in their latent spaces for activation swapping to be semantically valid (i.e., they speak the same "language" up to layer $l$).
- **Evidence anchors**:
  - [Section 3.3]: Algorithm 1 describes replacing $h_f^{(l)}$ with $h_p^{(l)}$ and continuing the forward pass $M_f^{(l+1:L)}(h_p^{(l)})$.
  - [Section 4.3]: Table 3 shows the intentionality gap dropping from 1.60–3.83 to 0.00–0.03.
  - [Corpus]: "Narrow Finetuning Leaves Clearly Readable Traces" supports the premise that finetuning creates distinct, readable activation traces that can be identified and manipulated.
- **Break condition**: If the finetuning process drastically shifted the entire activation manifold (e.g., via a large learning rate or severe catastrophic forgetting), $M_p$ activations would be out-of-distribution for $M_f$'s subsequent layers, causing gibberish outputs.

### Mechanism 3
- **Claim**: Finetuning is the primary causal driver of the Knobe effect, not model scale or base pretraining.
- **Mechanism**: Alignment data contains human-like moral asymmetries. Standard finetuning objectives (e.g., DPO or SFT) optimize the model to replicate these human judgment patterns, embedding the bias into the weights of the mid-to-late layers. Scaling up parameters amplifies the model's capacity to internalize these patterns but does not spontaneously generate them.
- **Core assumption**: The evaluated pretrained baselines are sufficiently unbiased to serve as a clean control group.
- **Evidence anchors**:
  - [Section 5]: Ablation study shows pretrained models (even at 27B scale) have low $\Delta_{Knobe}$ (0.39), whereas finetuned models jump significantly (6.07).
  - [Section 4.1]: "finetuning consistently amplified the Knobe effect on all models tested."
  - [Corpus]: "Planted in Pretraining, Swayed by Finetuning" complicates this slightly, suggesting biases may originate in pretraining but are "swayed" (activated/amplified) by finetuning.
- **Break condition**: If significant bias were observed in the base pretrained models, the mechanism would shift from "finetuning-induced" to "pretraining-embedded," rendering the pretrained patch ineffective.

## Foundational Learning

- **Concept: Residual Stream & Activations**
  - **Why needed here**: You cannot patch what you cannot locate. Understanding that the residual stream carries information sequentially through layers is vital for grasping why overwriting it at layer $l$ alters the final output.
  - **Quick check question**: If I patch layer 15 of a 32-layer model, do the weights of layer 16 change? (Answer: No, only the input activation to layer 16 changes).

- **Concept: The Knobe Effect**
  - **Why needed here**: This is the target metric. You must understand that it measures an asymmetry in intentionality attribution (judging harm as intentional vs. help as unintentional) to interpret the $\Delta_{Knobe}$ scores.
  - **Quick check question**: Does the Knobe effect measure the accuracy of moral judgment or the bias/consistency of judgment based on outcome valence?

- **Concept: Mechanistic Interpretability (Activation Patching)**
  - **Why needed here**: This is the core methodology. It differs from prompting or weight editing. It is an inference-time intervention.
  - **Quick check question**: Does activation patching require retraining the model or updating gradients? (Answer: No).

## Architecture Onboarding

- **Component map**: Input (Tokenized) -> Transformer layers (1...L) -> Residual stream connection at layer $l$ -> Output (Logits -> Softmax -> Probability)

- **Critical path**:
  1. Load $M_p$ (Pretrained) and $M_f$ (Finetuned) in evaluation mode.
  2. Run input through $M_p$, cache all layer activations $h_p^{(l)}$.
  3. Run input through $M_f$. At target layer $l$, intercept and discard $h_f^{(l)}$.
  4. Inject $h_p^{(l)}$ into $M_f$'s forward pass.
  5. Measure output distribution change.

- **Design tradeoffs**:
  - **Surgical vs. Global**: Patching one layer is precise but might miss distributed bias; patching too many layers degrades performance toward the base model (losing alignment utility).
  - **Inference Cost**: Requires effectively running two forward passes (or caching one), doubling inference latency.

- **Failure signatures**:
  - **Capability Collapse**: If you patch too early (e.g., layer 1), the finetuned model's later layers may fail to interpret the base model's syntax, resulting in gibberish.
  - **Bias Retention**: If you patch the wrong layer (one not encoding moral bias), $\Delta_{Knobe}$ remains high.

- **First 3 experiments**:
  1. **Baseline Verification**: Replicate the $\Delta_{Knobe}$ gap between $M_p$ and $M_f$ on the Ngo et al. (2015) dataset to ensure your setup detects the bias.
  2. **Layer Sweep**: Iterate $l$ from $1$ to $L$. Plot $\Delta_{Knobe}$ vs. Layer Index to visualize the "peak" bias localization layers (validate against paper's Figure 3).
  3. **Utility Check**: Run the best-performing patched model on a generic benchmark (e.g., MMLU or HellaSwag) to verify the paper's claim of minimal performance regression (< 2% drop).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Layer-Patching mitigation strategy effectively eliminate other social biases, such as gender or racial stereotypes, in LLMs?
- Basis in paper: [explicit] The Conclusion explicitly states that future work should explore whether this localization technique applies to other biases, specifically citing gender and racial stereotypes.
- Why unresolved: The current study only validated the method on the Knobe effect (moral bias in intentionality attribution), leaving its efficacy on structurally different social biases unknown.
- Evidence: Successful replication of the intervention on standard fairness benchmarks (e.g., CrowS-Pairs or StereoSet) showing similar reduction in bias metrics without performance degradation.

### Open Question 2
- Question: Can mechanistic interpretability interventions remove bias without relying on access to the corresponding pretrained model?
- Basis in paper: [explicit] The Conclusion notes that it is "essential to develop methods that do not rely on access to the pretrained model."
- Why unresolved: The current Layer-Patching algorithm requires replacing activations in the finetuned model with those from the pretrained version ($M_p$), which is often unavailable for commercial or closed-source models.
- Evidence: A method that modifies the finetuned model using only its own internal representations or synthetic activations to achieve similar bias reduction.

### Open Question 3
- Question: Does the localization of the Knobe effect in mid-to-late layers generalize across diverse moral frameworks beyond the specific scenarios used?
- Basis in paper: [inferred] The methodology relies on a single dataset (Ngo et al., 2015) based on specific side-effect scenarios.
- Why unresolved: It is unclear if the localization of bias to specific layers is an artifact of the specific linguistic structures in the Ngo dataset or a fundamental property of moral reasoning in the model.
- Evidence: Layer-Patching analysis performed on a broader range of moral psychology datasets (e.g., trolley problems or distributive justice scenarios) showing consistent layer sensitivity.

## Limitations

- The method requires access to the corresponding pretrained model, which is often unavailable for commercial or closed-source models.
- The effectiveness is validated only on one specific cognitive bias (the Knobe effect), leaving generalizability to other biases uncertain.
- The paper does not address potential distributional shifts that could arise when patching across models with different tokenization or vocabulary sizes.

## Confidence

- **High Confidence**: The mechanistic claim that the Knobe effect is localized to mid-to-late Transformer layers in finetuned models, supported by direct activation difference analysis and consistent layer sweep results across multiple model scales.
- **Medium Confidence**: The causal attribution of the bias to finetuning rather than pretraining, as the evidence shows pretrained models have low baseline bias but does not fully rule out pretraining contributions that are merely activated by finetuning.
- **Medium Confidence**: The preservation of general model performance post-patching, based on benchmark results, though the evaluation is limited to standard benchmarks and does not assess alignment-specific capabilities that may be degraded.

## Next Checks

1. **Pretraining Bias Assessment**: Run the Knobe effect evaluation on the exact base pretrained models (Llama, Mistral, Gemma) to verify they are truly unbiased baselines and quantify any pretraining-originated bias.
2. **Cross-Bias Generalization**: Apply the same Layer-Patching methodology to a different, well-defined social bias (e.g., gender or racial bias in sentiment analysis) to test whether localization and mitigation generalize beyond the Knobe effect.
3. **Long-Running Inference Stability**: Evaluate the patched model on a diverse set of alignment-relevant tasks (e.g., helpfulness, harmlessness, instruction following) over extended inference sessions to detect any degradation in capability or safety that might emerge beyond standard benchmarks.