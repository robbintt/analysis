---
ver: rpa2
title: Symmetry-Aware Bayesian Optimization via Max Kernels
arxiv_id: '2509.25051'
source_url: https://arxiv.org/abs/2509.25051
tags:
- kernel
- kavg
- kmax
- section
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a symmetry-aware Bayesian optimization approach
  using max kernels. The key idea is to exploit symmetries in the objective function
  by considering the maximum alignment of base kernels across group orbits, addressing
  the challenge that the resulting max kernel is not positive semidefinite.
---

# Symmetry-Aware Bayesian Optimization via Max Kernels

## Quick Facts
- **arXiv ID:** 2509.25051
- **Source URL:** https://arxiv.org/abs/2509.25051
- **Reference count:** 40
- **Primary result:** Proposes max kernel for symmetry-aware BO, achieving up to 50% lower regret vs orbit-averaging, especially for larger symmetry groups.

## Executive Summary
This paper introduces a symmetry-aware Bayesian optimization method using max kernels that exploits known symmetries in the objective function. The key innovation is to compute the maximum alignment of base kernels across group orbits rather than averaging, which preserves high-contrast similarity signals that drive exploration. To handle the resulting non-positive semidefinite kernel, the authors project onto the PSD cone using eigenvalue clipping and extend to new points via Nyström formula. Empirical results across synthetic and real-world benchmarks show substantial regret reduction compared to both standard and averaging-based invariant kernels, with performance gains increasing as symmetry group size grows.

## Method Summary
The method computes a max kernel $k_{\max}(x, x') = \max_{g, g' \in G} k_b(gx, g'x')$ that preserves the strongest similarity across group orbits, then projects the resulting non-PSD Gram matrix onto the PSD cone via eigenvalue clipping. A Nyström extension creates a valid G-invariant GP kernel for new points. The approach maintains computational complexity comparable to standard invariant kernels while achieving significantly better regret performance, particularly for larger symmetry groups where averaging dilutes similarity signals.

## Key Results
- Achieves up to 50% lower cumulative regret compared to orbit-averaging and base kernels
- Performance gains increase with symmetry group size, particularly evident for large groups like $|G|=3840$
- Standard eigendecay-based theoretical bounds fail to explain empirical success
- Maintains comparable computational complexity to existing invariant kernel methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving maximum orbit alignment improves search geometry compared to averaging.
- Mechanism: The max kernel $k_{\max}(x, x') = \max_{g, g' \in G} k_b(gx, g'x')$ retains the strongest pairwise similarity across group orbits rather than diluting it via averaging. This preserves high-contrast alignments that guide exploration.
- Core assumption: The objective function $f^\star$ is G-invariant, and useful exploration signals are carried by the best alignments across orbits, not by averaged similarity.
- Evidence anchors:
  - [abstract] "maximum similarity across group orbits... substantially improve BO efficiency"
  - [section 1] "Averaging mixes all orbit pairings and can shrink contrasts as |G| increases. In contrast, (1) preserves high-contrast alignments that drive exploration"
  - [corpus] Weak direct corpus evidence on max-alignment specifically; the method is novel within BO.
- Break condition: If group orbits are small or base kernel is already near-invariant, the contrast advantage diminishes; averaging and maximization converge.

### Mechanism 2
- Claim: PSD projection with Nyström extension yields a valid GP kernel without adding asymptotic cost.
- Mechanism: Project the potentially non-PSD Gram matrix $K$ of $k_{\max}$ onto the PSD cone via eigenvalue clipping ($K_+ = Q\max(0, \Lambda)Q^\top$), then extend to new points with the Nyström formula $k^{(D)}_+(x, x') = k_{\max}(x, D)K_+^\dagger k_{\max}(D, x')$.
- Core assumption: The empirical Gram matrix on a finite design set $D$ is a sufficient approximation for PSD correction; the Nyström extension generalizes well to unseen points.
- Evidence anchors:
  - [abstract] "project this kernel onto the PSD cone and use a Nyström extension to create a valid, G-invariant GP kernel"
  - [section 3.2] Equations (6) and (7) define the projection and Nyström extension.
  - [corpus] No corpus papers directly address this projection strategy in BO; related max-kernel literature (e.g., indefinite kernel handling in SVMs) is cited but not directly applied to GP-based BO.
- Break condition: If the Gram matrix has many large negative eigenvalues, clipping introduces significant distortion; the Nyström extension may poorly extrapolate.

### Mechanism 3
- Claim: Performance gains scale with group size because averaging dilutes similarity while max-alignment preserves it.
- Mechanism: As $|G|$ grows, $k_{\text{avg}}$ averages over more orbit pairs, flattening similarity signals. $k_{\max}$ continues selecting the best alignment, maintaining discriminative power for BO acquisition.
- Core assumption: Larger symmetry groups correspond to more dilution under averaging; the objective's structure remains amenable to max-alignment exploitation.
- Evidence anchors:
  - [abstract] "gains increasing as the symmetry group size grows"
  - [section 4.3] Figure 4 and Table 2 show deteriorating performance of $k_{\text{avg}}$ with larger $|G|$ while $k^{(D)}_+$ remains stable.
  - [corpus] Corpus papers on symmetry in RL and BO generally confirm benefits of symmetry exploitation but do not specifically analyze group size scaling.
- Break condition: For continuous groups where orbit computations are intractable, the max kernel requires closed-form approximations or sampling, potentially reducing the advantage.

## Foundational Learning

- **Concept: Gaussian Process (GP) as a surrogate model**
  - Why needed here: The entire BO framework relies on a GP prior with a covariance kernel to model the objective and compute acquisition functions.
  - Quick check question: Can you write the GP posterior mean and covariance formulas given a dataset $\mathcal{Z}_t$ and kernel $k$?

- **Concept: Group invariance and orbits**
  - Why needed here: The method exploits that $f^\star(gx) = f^\star(x)$ for all $g \in G$, and builds kernels over orbits $[x] = \{gx : g \in G\}$.
  - Quick check question: Given a group $G$ acting on $\mathcal{S}$, what is the orbit of a point $x$ and what does G-invariance of a function mean?

- **Concept: Positive semidefiniteness (PSD) of kernels**
  - Why needed here: $k_{\max}$ is not PSD; the projection step is required to obtain a valid GP covariance.
  - Quick check question: Why must a GP covariance kernel be PSD, and what happens if you use a non-PSD kernel directly?

## Architecture Onboarding

- **Component map:** Base kernel $k_b$ -> Symmetry group $G$ -> Max-kernel computation -> PSD projection -> Nyström extension -> GP-UCB loop
- **Critical path:**
  1. For each new design point, compute its max-kernel similarity to all points in current dataset $D$
  2. Update Gram matrix, recompute eigendecomposition and PSD projection
  3. Compute acquisition function using the Nyström-extended kernel
  4. Optimize acquisition to select next point, evaluate, and add to $D$

- **Design tradeoffs:**
  - Accuracy vs. cost of orbit enumeration: exact maximization over large or continuous groups may be prohibitive; sampling approximations trade off fidelity for speed
  - PSD projection severity: more negative eigenvalues increase distortion; can be mitigated by larger design sets or alternative correction methods (e.g., flipping)
  - Hyperparameter sensitivity: kernel lengthscale and noise variance affect both max-kernel values and PSD projection; joint optimization may be unstable

- **Failure signatures:**
  - Poor regret reduction despite symmetry: check if group $G$ is misspecified or if orbit enumeration is insufficient
  - Numerical instability in $K_+^\dagger$: may indicate near-singular Gram matrix after projection; consider regularization
  - Acquisition function optimization stagnates: could be due to flattened kernel similarities from projection distortion

- **First 3 experiments:**
  1. Implement max-kernel on a simple 1D Ackley function with sign-flip symmetry ($|G|=2$); compare regret of $k^{(D)}_+$ vs. $k_{\text{avg}}$ and base $k_b$ over 50 iterations
  2. Scale to 2D radial invariance (continuous rotation group); use sampling to approximate orbit maximization and verify that $k^{(D)}_+$ maintains performance as $|G|$ effectively increases
  3. Test on a small real-world task (e.g., the WLAN AP placement with permutation symmetry, $m=3$ APs); compare cumulative regret and final network throughput across kernels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can regret bounds be formulated that incorporate approximation hardness to explain why $k_{max}$ outperforms $k_{avg}$?
- **Basis in paper**: [explicit] The authors state in Section 6 that "Developing regret bounds that also measure approximation hardness... seems a promising way to obtain guarantees that align more closely with empirical performance."
- **Why unresolved**: Standard eigendecay-based bounds fail to explain the empirical success, as $k_{avg}$ often has faster spectral decay but worse regret.
- **What evidence would resolve it**: A formal regret bound including terms for the approximation of $f^*$ in the RKHS that favors the geometry of $k_{max}$.

### Open Question 2
- **Question**: To what extent does the "search geometry" (preserving high-contrast alignments) drive performance improvements over spectral decay rates?
- **Basis in paper**: [explicit] Section 6 hypothesizes that "eigendecay quantifies how fast spectra shrink but ignores which eigenfunctions are emphasized," suggesting geometry explains the gap.
- **Why unresolved**: The paper establishes a correlation between preserving contrasts and performance but does not prove a causal mechanism or quantify the "geometry" contribution.
- **What evidence would resolve it**: Experiments isolating geometry (e.g., visualizing acquisition function behavior) or theoretical analysis linking orbit alignment to information gain.

### Open Question 3
- **Question**: Does the computational cost of the PSD projection and Nyström extension become prohibitive for extremely large or continuous symmetry groups?
- **Basis in paper**: [inferred] Table 1 lists complexity as $O(n^2|G|)$, and experiments use finite groups ($|G| \leq 3840$), leaving the scalability for massive groups unstated.
- **Why unresolved**: While asymptotically comparable to averaging, the constants and memory requirements for the matrix operations may limit applicability to very high-dimensional group actions.
- **What evidence would resolve it**: Benchmarks on problems with very large symmetry groups (e.g., larger permutations) comparing wall-clock time and memory usage.

## Limitations

- The method's effectiveness critically depends on accurate symmetry specification; misspecification degrades performance to near-zero
- The claim that eigendecay bounds fail to explain gains remains qualitative without a quantitative alternative theory
- Computational cost for continuous groups with sampling is not benchmarked, leaving scalability uncertain

## Confidence

- **Mechanism 1 (max-alignment advantage):** Medium — Strong qualitative argument but limited direct experimental ablation of averaging vs. max
- **Mechanism 2 (PSD projection + Nyström):** High — Method is clearly defined and standard, though empirical robustness not stressed
- **Mechanism 3 (scaling with |G|):** Medium — Trend observed but not deeply analyzed; could be dataset-specific

## Next Checks

1. **Ablation on group specification:** Run the same benchmarks with a slightly perturbed or incomplete group G and measure performance drop to quantify sensitivity
2. **Analytical eigendecay comparison:** For a synthetic invariant function, compute eigendecay of kb, kavg, and kmax; compare to observed regret to test if eigendecay is indeed uninformative
3. **Computational scaling study:** Measure wall-clock time for max-kernel computation as |G| and d grow; compare against sampling-based approximations for continuous groups