---
ver: rpa2
title: Predicting Diabetic Retinopathy Using a Two-Level Ensemble Model
arxiv_id: '2510.01074'
source_url: https://arxiv.org/abs/2510.01074
tags:
- ensemble
- stacking
- learning
- two-level
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a two-level ensemble learning model for diabetic
  retinopathy prediction using routine laboratory test results, addressing the limitations
  of image-based AI tools and resource-intensive eye exams. The framework combines
  four base models (Linear SVC, Random Forest, Gradient Boosting, and XGBoost) optimized
  through hyperparameter tuning and internal stacking, followed by a second-level
  stacking using Random Forest as a meta-learner.
---

# Predicting Diabetic Retinopathy Using a Two-Level Ensemble Model

## Quick Facts
- arXiv ID: 2510.01074
- Source URL: https://arxiv.org/abs/2510.01074
- Authors: Mahyar Mahmoudi; Tieming Liu
- Reference count: 28
- Key outcome: Two-level ensemble achieves 0.9433 accuracy using only 6-7 lab features

## Executive Summary
This study introduces a two-level ensemble learning framework for diabetic retinopathy (DR) prediction using routine laboratory test results, offering an alternative to image-based AI tools and resource-intensive eye exams. The model combines four base learners (Linear SVC, Random Forest, Gradient Boosting, and XGBoost) through hyperparameter tuning and internal stacking, followed by a second-level stacking with Random Forest as a meta-learner. The approach achieves high performance across multiple metrics while requiring only 6-7 key features, making it suitable for clinical settings with limited resources.

## Method Summary
The framework uses a hierarchical stacking approach where four base models undergo internal stacking across multiple tuned configurations before being combined at a second level. Data preprocessing includes missing value imputation, SMOTE resampling for class balance, and standardization. The model is trained on 80% of data with 25 features from Cerner's Health Facts EHR database, using 6-7 features for efficient prediction. Evaluation includes accuracy, F1, recall, precision, ROC-AUC, and AUPRC metrics.

## Key Results
- Two-level ensemble achieves Accuracy 0.9433, F1 Score 0.9425, Recall 0.9207, Precision 0.9653
- ROC-AUC 0.9844 and AUPRC 0.9875 demonstrate strong discrimination and precision-recall performance
- Maintains 0.9175 accuracy with only 6 features versus 0.9433 with 25 features
- Outperforms single-level stacking and FCN baselines while requiring minimal computational resources

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical two-level stacking improves generalization by learning optimal combinations of already-optimized base model outputs. Base models undergo internal stacking across multiple hyperparameter configurations, then a second-level Random Forest meta-learner combines their outputs. This works because base model prediction errors are not perfectly correlated, allowing the meta-learner to correct systematic biases. Break condition: if base models are highly correlated, meta-learner gains minimal diversity to exploit.

### Mechanism 2
Internal stacking within each base model stabilizes predictions before second-level aggregation. Each base model type is trained in multiple configurations optimized for different metrics (accuracy, precision, recall, ROC-AUC), which are then stacked internally. This reduces configuration-specific overfitting by capturing complementary patterns. Break condition: if hyperparameter search space is too narrow, configurations will be near-identical, providing no diversity benefit.

### Mechanism 3
The ensemble maintains predictive performance with only 6-7 features because diverse model architectures compensate for reduced feature information. Feature importance via permutation identifies key renal/metabolic signals (Creatinine, BUN, Anion Gap, Glucose, Albumin, ALT). Ensemble diversity allows graceful degradation as each model type leverages available features differently. Break condition: if key features are missing or corrupted in deployment, performance degrades rapidly.

## Foundational Learning

- Concept: Stacked Generalization (Stacking)
  - Why needed here: This is the core architectural pattern; understanding how meta-learners combine base model outputs is essential for debugging and extension.
  - Quick check question: Can you explain why stacking differs from simple averaging of model predictions?

- Concept: Hyperparameter Tuning and Cross-Validation
  - Why needed here: The framework relies on multiple tuned configurations per base model; understanding tuning strategies prevents redundant computation.
  - Quick check question: What happens if hyperparameter tuning overfits to the validation set?

- Concept: Permutation Feature Importance
  - Why needed here: Feature selection is critical to the efficiency claim; permutation importance measures contribution by performance drop when shuffled.
  - Quick check question: Why might permutation importance differ from coefficient-based importance in Linear SVC?

## Architecture Onboarding

- Component map: Input features (6-25) -> Level 1 base models (Linear SVC, RF, GB, XGBoost) with internal stacking -> 4 optimized outputs -> Level 2 Random Forest meta-learner -> Final prediction
- Critical path: 1) Data preprocessing (missing value imputation, SMOTE, standardization) 2) Hyperparameter search for each base model type 3) Internal stacking within each base model family 4) Train Random Forest meta-learner on Level 1 outputs 5) Evaluate on held-out test set
- Design tradeoffs: RF meta-learner captures non-linear interactions but reduces interpretability (RF-RF achieved 0.9433 accuracy vs RF-LR at 0.9397); feature count vs performance shows 0.9175 accuracy with 6 features vs 0.9433 with 25 features; computational cost is 150 minutes vs FCN's 200 minutes
- Failure signatures: Performance collapse to ~0.89 accuracy likely indicates only 5 features being used; one-level stacking performance (~0.89-0.90) suggests meta-learner not receiving diverse inputs; high precision, low recall indicates conservative model needing threshold adjustment
- First 3 experiments: 1) Reproduce Table 2 by training two-level ensemble with 5, 6, 7, and 25 features 2) Ablate internal stacking by comparing base models with single configurations vs internal stacking 3) Swap meta-learner by testing Logistic Regression vs Random Forest on your feature set

## Open Questions the Paper Calls Out

### Open Question 1
How well does the two-level ensemble model generalize to external healthcare datasets and diverse patient subgroups? The study uses only Cerner's Health Facts database with no external validation to test transferability to other health systems, geographic regions, or demographic populations. Prospective validation on at least one independent EHR dataset with subgroup analysis across age, sex, race/ethnicity, and diabetes severity levels would resolve this.

### Open Question 2
Can SHAP-based explanations provide clinically actionable, patient-level interpretability for the ensemble model's DR risk predictions? While feature importance was assessed via permutation, no patient-level explanation method was implemented. Implementation of SHAP explanations evaluated by clinical experts for understandability, actionability, and consistency with known DR pathophysiology would resolve this.

### Open Question 3
Does incorporating medication history and healthcare utilization patterns improve predictive performance and reduce biases? Current model uses only 25 laboratory and demographic variables, excluding behavioral and treatment-related factors that may influence DR risk. Ablation study comparing current model to augmented versions including medication adherence data and visit frequency features would resolve this.

### Open Question 4
Can the two-level stacking framework successfully predict early-stage DR progression and extend to other chronic disease detection? DR prediction was demonstrated, but early-stage detection capability and transferability to diseases with different clinical trajectories remain untested. Application to DR stage classification and cardiovascular/chronic kidney disease datasets would resolve this.

## Limitations
- Exact hyperparameter search spaces for base models remain unspecified, making exact reproduction difficult
- Internal stacking within base models lacks detailed procedural description, raising questions about implementation complexity
- Two-level stacking architecture may be over-engineered for datasets where base models already achieve high accuracy independently

## Confidence

**High confidence**: Overall performance metrics (0.94 accuracy, 0.98 ROC-AUC) are plausible given the ensemble approach and appear robust across different feature subsets

**Medium confidence**: The claim that only 6-7 features maintain "reliable" predictions is supported by the ~4% accuracy drop, though "reliable" remains context-dependent

**Medium confidence**: Two-level stacking outperforming single-level stacking is reasonable but the magnitude of improvement depends heavily on base model correlation and diversity

## Next Checks
1. **Ablation study**: Systematically remove internal stacking within base models to quantify its contribution versus standard hyperparameter tuning alone
2. **Threshold sensitivity analysis**: Test how classification threshold affects precision-recall tradeoff in imbalanced deployment scenarios
3. **External validation**: Evaluate the model on a separate clinical dataset to assess generalizability beyond the Cerner Health Facts cohort