---
ver: rpa2
title: Deep Think with Confidence
arxiv_id: '2508.15260'
source_url: https://arxiv.org/abs/2508.15260
tags:
- confidence
- voting
- accuracy
- reasoning
- traces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepConf improves reasoning efficiency and accuracy by using model-internal
  confidence signals to filter low-quality reasoning traces during or after generation.
  It dynamically stops generation when token group confidence drops below a threshold,
  achieving up to 84.7% token reduction and up to 99.9% accuracy on AIME 2025 without
  extra model training.
---

# Deep Think with Confidence

## Quick Facts
- arXiv ID: 2508.15260
- Source URL: https://arxiv.org/abs/2508.15260
- Reference count: 40
- Primary result: Achieves up to 84.7% token reduction and 99.9% accuracy on AIME 2025

## Executive Summary
DeepConf introduces a dynamic filtering approach for reasoning traces in large language models by leveraging model-internal confidence signals. Rather than processing all generated reasoning traces equally, it uses local confidence metrics to identify and terminate low-quality reasoning early, then applies confidence-weighted voting on the remaining high-quality traces. This approach achieves substantial token savings while maintaining or improving accuracy across multiple benchmarks, with particular success on mathematical reasoning tasks.

## Method Summary
DeepConf implements three core strategies: offline filtering using confidence thresholds calibrated from warmup traces, online early termination when group confidence drops below threshold, and confidence-weighted majority voting. The method extracts token-level confidence from log probabilities, computes group confidence using sliding windows, and filters traces based on confidence percentiles. It can operate in offline mode (filtering after generation) or online mode (early termination during generation), with the online approach achieving the best token savings while maintaining accuracy.

## Key Results
- Achieves up to 84.7% token reduction on AIME 2025 without accuracy loss
- DeepConf-low (top 10% filtering) improves accuracy by 5.8pp on AIME24 with 77.9% token reduction
- DeepConf-high (top 90% filtering) provides moderate savings (16-59%) while maintaining baseline accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local confidence signals identify low-quality reasoning traces better than global averaging
- Mechanism: The lowest group confidence metric (sliding window of 2048 tokens) captures localized reasoning breakdowns that global averaging masks. When confidence drops sharply during reasoning—often marked by tokens like "wait" or "think again"—it signals subsequent errors.
- Core assumption: Reasoning failures manifest as localized low-confidence regions rather than uniformly distributed uncertainty
- Evidence anchors:
  - [abstract] "leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces"
  - [Section 2] Fig. 2 shows bottom-10% and tail confidence metrics better separate correct/incorrect traces than mean confidence
  - [corpus] Neighbor work "Don't Think Twice" links over-reasoning to impaired confidence calibration
- Break condition: If models exhibit systematic overconfidence on incorrect paths (noted in Section 5 as a key limitation), local confidence ceases to be a reliable quality proxy

### Mechanism 2
- Claim: Early termination based on group confidence thresholds approximates offline filtering while saving tokens
- Mechanism: During generation, compute running group confidence (average over last n tokens). When it drops below threshold s (calibrated via warmup traces), terminate immediately. The threshold ensures terminated traces would have been filtered anyway in offline mode
- Core assumption: Warmup traces (Ninit=16) provide a representative confidence distribution for threshold calibration
- Evidence anchors:
  - [Section 3.3] "any trace terminated online has group confidence < s and would be excluded by the offline filter"
  - [Section 4.3, Table 2] DeepConf-low achieves 77.9% token reduction on AIME24 with +5.8pp accuracy gain
  - [corpus] Corpus evidence weak; no direct replications found yet
- Break condition: If warmup traces are unrepresentative (small Ninit or unusual prompt distribution), threshold miscalibration causes over/under-termination

### Mechanism 3
- Claim: Confidence-weighted majority voting concentrates on reliable traces without discarding diversity entirely
- Mechanism: Instead of equal votes, weight each trace's answer by its confidence score. Combined with filtering (keep top η%), this amplifies high-confidence signals while maintaining ensemble benefits
- Core assumption: High-confidence traces are more likely correct, and answer diversity among them is informative
- Evidence anchors:
  - [Section 3.2] "This voting scheme favors answers supported by high-confidence traces"
  - [Section 4.2, Table 1] Top-10% filtering with Tail Confidence achieves 99.9% on AIME25 (GPT-OSS-120B)
  - [corpus] "Self-Training LLMs with Confident Reasoning" corroborates confidence-weighted selection improves reasoning
- Break condition: When models are confidently wrong (high confidence on incorrect answers), aggressive filtering amplifies errors rather than reducing them

## Foundational Learning

- Concept: Self-consistency / Majority Voting
  - Why needed here: DeepConf modifies standard majority voting by adding confidence weighting and filtering. Understanding the baseline helps quantify improvements
  - Quick check question: Can you explain why majority voting exhibits diminishing returns as trace count increases?

- Concept: Token Logprob and Entropy
  - Why needed here: All confidence metrics derive from token-level log probabilities. Computing confidence requires extracting top-k logprobs during generation
  - Quick check question: Given a token distribution with logprobs [-0.5, -1.2, -3.0], what is the token confidence using top-3?

- Concept: Sliding Window Statistics
  - Why needed here: Group confidence uses overlapping windows (e.g., 2048 tokens) to smooth local fluctuations while preserving temporal locality
  - Quick check question: Why might a sliding window outperform global averaging for detecting reasoning breakdowns?

## Architecture Onboarding

- Component map:
  LogprobsProcessor -> Offline Warmup Module -> Generation Controller -> Confidence-Weighted Aggregator

- Critical path:
  1. Generate warmup traces → extract confidence distribution → set threshold s
  2. For each new trace: generate token → update group confidence → check if < s → early stop or continue
  3. Aggregate completed traces → filter top η% → confidence-weighted majority vote → return answer

- Design tradeoffs:
  - η=10% (DeepConf-low): Maximum accuracy gains and token savings, but risks "confidently wrong" regressions
  - η=90% (DeepConf-high): Safer, maintains baseline accuracy with moderate savings (16-59%)
  - Group window size (n): Larger windows (2048) smooth noise but delay detection; smaller windows increase false positives
  - Warmup size (Ninit): Larger samples stabilize threshold but add fixed overhead

- Failure signatures:
  - Accuracy drops despite filtering: Model overconfidence on incorrect answers (check per-dataset calibration)
  - Excessive early termination: Threshold too aggressive; try η=90% or increase Ninit
  - No token savings: Confidence distribution too uniform; check if model provides meaningful logprobs

- First 3 experiments:
  1. Baseline calibration: Run standard self-consistency (cons@512) on your target dataset to establish pass@1 and majority-voting accuracy
  2. Offline confidence metric comparison: Compare Average Trace vs Tail vs Lowest Group Confidence with top-10%/90% filtering on a held-out set
  3. Online warmup ablation: Test Ninit ∈ {8, 16, 32} to find minimum warmup that achieves stable threshold calibration

## Open Questions the Paper Calls Out
None

## Limitations
- Primary risk of model overconfidence on incorrect answers, particularly evident in MATH-700 where accuracy degraded
- Warmup threshold calibration (Ninit=16) appears fragile and may not generalize across domains
- Sliding window size (2048 tokens) is heuristic and may not be optimal for all task types

## Confidence
- High confidence: Token reduction claims (84.7% verified via AIME 2025 results with clear before/after metrics)
- Medium confidence: Accuracy improvements on most datasets (multiple ablations and metrics provided, but MATH-700 regression raises calibration concerns)
- Medium confidence: Confidence metric superiority claims (Fig. 2 shows tail/lowest group outperforming mean, but visual separation is modest)
- Low confidence: Mechanism explanations for why local confidence works (largely intuitive; lacks ablation studies isolating sliding window effects)

## Next Checks
1. Calibration robustness test: Apply DeepConf to O1/O3 models on MATH-700 with varied warmup sizes (Ninit=8, 16, 32) and η thresholds to map accuracy-token trade-off space. Focus on identifying conditions where confidence filtering degrades rather than improves accuracy.

2. Window size sensitivity: Systematically vary the sliding window size (512, 1024, 2048, 4096 tokens) on a held-out reasoning dataset to determine optimal window length for different task types. Include analysis of whether reasoning breakdowns are truly localized vs. gradual.

3. Cross-model generalization: Test DeepConf on non-OpenAI models (e.g., DeepSeek, Claude) using identical configuration to verify that confidence signals remain reliable across architectures. Compare whether tail confidence metrics still outperform mean confidence on diverse model families.