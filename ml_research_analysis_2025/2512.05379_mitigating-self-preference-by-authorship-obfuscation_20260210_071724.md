---
ver: rpa2
title: Mitigating Self-Preference by Authorship Obfuscation
arxiv_id: '2512.05379'
source_url: https://arxiv.org/abs/2512.05379
tags:
- judge
- self-preference
- answer
- self-recognition
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates mitigating harmful self-preference in LLM
  judges by reducing their ability to recognize their own outputs. The authors hypothesize
  that self-preference stems from self-recognition, and apply black-box perturbations
  like synonym replacement to evaluation candidates to obfuscate authorship.
---

# Mitigating Self-Preference by Authorship Obfuscation

## Quick Facts
- **arXiv ID**: 2512.05379
- **Source URL**: https://arxiv.org/abs/2512.05379
- **Reference count**: 19
- **Primary result**: Simple two-word synonym replacement significantly reduces harmful self-preference in LLM judges, particularly in stronger models.

## Executive Summary
This paper investigates the mitigation of harmful self-preference in LLM judges by reducing their ability to recognize their own outputs. The authors hypothesize that self-preference stems from self-recognition, and apply black-box perturbations like synonym replacement to evaluation candidates to obfuscate authorship. They find that simple two-word synonym replacements significantly reduce harmful self-preference, particularly in stronger models. However, they also discover that complete mitigation is challenging: when perturbations fully neutralize stylistic differences (e.g., via judge paraphrasing), self-preference recovers as judges rely on semantic differences and shared beliefs. The findings suggest self-recognition and self-preference occur at multiple semantic levels, making complete elimination difficult.

## Method Summary
The authors apply two-word synonym replacement to judge-generated answers using an external model (LLaMA-3.3-70B) to select replacement candidates. They evaluate on the QuALITY validation set (long-document QA) and MBPP+ (coding tasks) using five judge models: Llama-3.1-8B, Qwen2.5-7B, Llama-4-Scout-17B, Llama-4-Maverick-17B, and DeepSeek-V3-37B. Pairwise comparisons are conducted with position-swapped queries to control for ordering bias, and only consistent decisions are counted. Harmful cases are identified where the judge's answer is wrong but the competitor's is correct. The method is validated by measuring changes in judge accuracy on these harmful cases after perturbation.

## Key Results
- Two-word synonym replacement increases judge accuracy on harmful cases from 40% to 48% for DeepSeek-V3-37B
- Self-recognition rate decreases significantly with perturbations
- Complete mitigation is difficult because self-preference recovers when perturbations fully neutralize stylistic differences, as judges shift to semantic discrimination
- The approach shows consistent effectiveness across both long-document QA and coding tasks

## Why This Works (Mechanism)
The paper hypothesizes that LLM judges exhibit self-preference because they can recognize their own outputs through superficial cues like stylistic patterns. By applying black-box perturbations such as synonym replacement, the authorship information is obscured, reducing the judge's ability to identify its own output. This mechanism suggests that self-preference is not based on semantic content superiority but rather on recognition of familiar patterns.

## Foundational Learning
- **Self-preference in LLM judges**: The tendency of judges to favor their own outputs over better alternatives. Why needed: Understanding this bias is crucial for reliable AI evaluation systems.
- **Authorship obfuscation**: Techniques to obscure the origin of text to prevent recognition. Why needed: Provides a method to debias evaluation without modifying the judge model itself.
- **Harmful self-preference**: Cases where self-preference leads to objectively worse evaluation outcomes. Why needed: Distinguishes problematic bias from legitimate self-preference in genuine ties.
- **Black-box perturbation methods**: External modifications to inputs without accessing model internals. Why needed: Offers a practical, model-agnostic approach to debiasing.
- **Semantic vs. stylistic recognition**: Judges may identify their outputs through either content meaning or writing style. Why needed: Explains why complete mitigation requires addressing multiple recognition pathways.
- **Pairwise comparison with position swapping**: Querying twice with swapped positions to control for positional bias. Why needed: Ensures that observed preferences reflect genuine judgments rather than ordering effects.

## Architecture Onboarding

**Component Map**: Generator (QA) -> Evaluator (pairwise comparison) -> Synonym Generator (perturbation) -> Evaluator (re-run)

**Critical Path**: The evaluation pipeline flows from answer generation through pairwise comparison to perturbation application and re-evaluation. The most critical step is the pairwise comparison with position-swapping to identify harmful cases reliably.

**Design Tradeoffs**: The paper chose black-box perturbations over white-box methods for robustness and flexibility, accepting potentially less complete mitigation in exchange for broader applicability across different judge architectures without requiring model access.

**Failure Signatures**: 
- High ambiguity rate from inconsistent position-swapped decisions (>30-50% for smaller models)
- Self-preference increasing after perturbation when stylistic differences are fully neutralized
- Minimal accuracy improvement for weaker judge models

**First 3 Experiments**:
1. Generate answers from all five judge models on QuALITY validation set using the Generator prompt
2. Run pairwise comparisons against all other models using the Evaluator prompt with position-swapping
3. Apply two-word synonym replacement using the Synonym Generator prompt and re-run pairwise comparisons on harmful cases

## Open Questions the Paper Calls Out
- How do white-box methods (e.g., activation steering) compare to black-box perturbations for robustness and flexibility in mitigating self-preference?
- What is the optimal number and type of perturbations that maximally reduce harmful self-preference without degrading answer quality?
- How do confounding biases (length, persuasiveness, tone) interact with self-preference, and can controlled manipulations disentangle their effects?
- Do authorship obfuscation methods maintain effectiveness across diverse tasks and model architectures beyond those tested?

## Limitations
- Complete mitigation of self-preference is fundamentally challenging as judges can shift to semantic-based discrimination
- Results are highly model-dependent, with stronger models showing more susceptibility to perturbation
- Reliance on black-box perturbations limits understanding of internal causal mechanisms
- Evaluation focuses on specific judge models and datasets, raising generalizability concerns

## Confidence
- **High confidence**: Simple authorship obfuscation techniques can reduce harmful self-preference in LLM judges, particularly for stronger models
- **Medium confidence**: Complete mitigation is fundamentally challenging due to multi-level semantic recognition
- **Low confidence**: Broader implications for LLM evaluation practices in real-world deployment scenarios

## Next Checks
1. Test the synonym replacement approach across a broader range of judge models including different architectures to establish generalizability
2. Implement and compare additional authorship obfuscation techniques beyond synonym replacement to determine if complete self-preference elimination is achievable
3. Conduct human evaluation studies to verify semantic preservation of synonym replacements and quantify meaning distortion thresholds