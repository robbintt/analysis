---
ver: rpa2
title: 'Context Collapse: In-Context Learning and Model Collapse'
arxiv_id: '2601.00923'
source_url: https://arxiv.org/abs/2601.00923
tags:
- ummationdi
- data
- collapse
- where
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates in-context learning (ICL) and model collapse
  in large language models (LLMs). For ICL, it analyzes a weight-tied linear transformer
  trained on linear regression tasks, demonstrating that minimizing the in-context
  loss leads to a phase transition in learned parameters.
---

# Context Collapse: In-Context Learning and Model Collapse

## Quick Facts
- arXiv ID: 2601.00923
- Source URL: https://arxiv.org/abs/2601.00923
- Authors: Josef Ott
- Reference count: 0
- One-line primary result: This thesis investigates in-context learning (ICL) and model collapse in large language models (LLMs), demonstrating phase transitions in ICL parameters and almost sure convergence/divergence in model collapse scenarios.

## Executive Summary
This thesis investigates two critical phenomena in large language models: in-context learning (ICL) and model collapse. For ICL, it analyzes a weight-tied linear transformer trained on linear regression tasks, demonstrating that minimizing the in-context loss leads to a phase transition in learned parameters. Above a critical context length, the solution develops a skew-symmetric component, which induces rotational dynamics in the learned transformation. For model collapse, the thesis uses martingale and random walk theory to analyze simplified settings—linear regression and Gaussian fitting—under both replacing and cumulative data regimes. It strengthens existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, it introduces the concept of context collapse, a degradation of context during long generations, especially in chain-of-thought reasoning, linking ICL dynamics with long-term stability challenges in generative models.

## Method Summary
The thesis employs mathematical analysis to study simplified models of ICL and model collapse. For ICL, it uses a weight-tied linear transformer with sparse attention trained on linear regression tasks, reducing the forward pass to preconditioned gradient descent. For model collapse, it analyzes recursive self-training in linear regression and Gaussian fitting under replacing and cumulative data regimes, using martingale theory to prove almost sure convergence or divergence. The research combines theoretical proofs with empirical validation through controlled experiments on simplified architectures.

## Key Results
- In-context learning in weight-tied linear transformers exhibits a phase transition where the optimal preconditioner develops a skew-symmetric component above a critical context length, inducing rotational dynamics.
- Model collapse in linear regression with replacing data occurs almost surely, with parameters diverging as a random walk unless data grows sufficiently fast or is retained.
- Gaussian fitting collapse occurs almost surely if sample count per iteration grows linearly or sub-linearly, but is averted with super-linear data growth.
- The thesis introduces "context collapse" as a degradation phenomenon during long generations, particularly affecting chain-of-thought reasoning.

## Why This Works (Mechanism)

### Mechanism 1: ICL as Preconditioned Gradient Descent with Phase Transition
- Claim: A linear transformer with tied weights, trained on linear regression tasks, minimizes in-context loss by implementing preconditioned gradient descent. The optimal preconditioner develops a skew-symmetric component above a critical context length, inducing rotational dynamics.
- Mechanism: The forward pass of the weight-tied linear transformer is mathematically equivalent to a step of preconditioned gradient descent. Minimizing the ICL loss over a matrix parameter leads to an optimal solution that includes a skew-symmetric part. This non-symmetric preconditioner rotates the gradient direction, potentially improving convergence properties beyond standard symmetric pre-conditioning.
- Core assumption: The model uses a sparse linear attention mechanism with weight tying across all layers.
- Evidence anchors:
  - [abstract]: "We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction."
  - [section 2.5.1, Theorem 2.5.7]: For L=M=2, the global minimizer has a non-zero skew-symmetric component if and only if O ≥ 15.
  - [corpus]: Related work [Ahn+23] corroborates the ICL-to-GD equivalence.

### Mechanism 2: Model Collapse in Regression as a Random Walk
- Claim: In linear regression with recursive self-training on replacing synthetic data, model parameters diverge almost surely as a multi-dimensional random walk, leading to unbounded test error.
- Mechanism: Each model iteration fits a fresh synthetic dataset generated by the previous model. The parameter error is updated by adding an increment term that is a zero-mean random variable with positive-definite covariance matrix. This sum forms a martingale that, in dimensions ≥3, is transient. The parameters drift to infinity with probability one.
- Core assumption: The data generation process involves sampling new synthetic data at each step without retaining the old data.
- Evidence anchors:
  - [abstract]: "It strengthens existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time."
  - [section 3.3.2, Theorem 3.3.2]: "'g_L - g_★' → ∞ a.s."
  - [corpus]: Corpus evidence is weak for the "almost sure" divergence claim.

### Mechanism 3: Gaussian Fitting Collapse as Variance Contraction
- Claim: In iterative Gaussian fitting with replacing data, model collapse (variance → 0) occurs almost surely if and only if the number of synthetic samples per iteration grows linearly or sub-linearly.
- Mechanism: The covariance matrix evolves as a martingale product. Its trace converges to zero a.s. when the reciprocal sum of sample counts is infinite. This is because stochastic noise in estimation does not get "washed out" unless data volume increases fast enough to stabilize the estimate.
- Core assumption: The iterative process generates fresh samples from the previous model's estimated distribution and uses them to fit a new Gaussian via maximum likelihood.
- Evidence anchors:
  - [section 3.4, Theorem 3.4.1]: "ω_∞ = 0 almost surely" if Σ 1/n_L = ∞.
  - [corpus]: Corpus evidence is weak for this precise super-linear data growth condition.

## Foundational Learning

- **Martingale and Random Walk Theory**
  - Why needed here: These are the primary mathematical tools used to prove almost sure convergence or divergence in the model collapse analysis.
  - Quick check question: Can a martingale with zero-mean increments converge to a non-zero value?

- **Linear Attention and Weight Tying**
  - Why needed here: These are the architectural simplifications that make the ICL analysis tractable. The equivalence to gradient descent and the emergence of skew symmetry depend on these specific choices.
  - Quick check question: How does sharing the weight matrix across all layers change the effective optimization path in the model?

- **Preconditioned Gradient Descent**
  - Why needed here: This is the functional equivalence established for the linear transformer's ICL mechanism. The emergence of a skew-symmetric preconditioner is a core theoretical contribution.
  - Quick check question: How does a skew-symmetric component in a preconditioner change the trajectory of gradient descent?

## Architecture Onboarding

- **Component map**: Input Z_0 -> Layer 1 (V) -> Z_1 -> Layer 2 (V) -> ... -> Layer L (V) -> Output prediction
- **Critical path**:
  1. Input Construction: Stack covariates and masked labels into the input matrix Z_0
  2. Layer Forward Pass: For each layer l, compute Z_{l+1} = Z_l + (1/K) * P * Z_l * D * Z_l^T * V * Z_l
  3. Weight-Tied Recursion: Since all layers use the same V, the output is a function of V raised to the power L
  4. Loss Calculation: Compute the in-context loss as the expected squared error between the transformer's output and the true label

- **Design tradeoffs**:
  - Analytical Tractability vs. Realism: The linear, weight-tied, and sparse attention setting allows for closed-form analysis and proofs of phenomena like skew symmetry. However, this is a highly simplified model and results may not directly transfer to full transformers.
  - Replacing vs. Cumulative Data (Collapse): The theoretical analysis provides a clear dichotomy: replacing data leads to collapse in linear regression, while accumulating data averts it. This is a critical strategic choice for any synthetic data pipeline.

- **Failure signatures**:
  - ICL: If V remains purely symmetric, the model may be in a sub-optimal regime. Check for the magnitude of the skew-symmetric component.
  - Model Collapse (Regression): In the replacing regime, monitor parameter norm or test error. If they grow linearly with iteration number, the model is collapsing.
  - Model Collapse (Gaussian): Monitor the trace of the estimated covariance matrix. If it decays exponentially towards zero, the model is collapsing.

- **First 3 experiments**:
  1. Reproduce the ICL Phase Transition: Train a weight-tied linear transformer on a linear regression task with L=M=2. Vary the context length K from 5 to 30 and measure the skew-symmetric strength of the learned V. Verify the sharp transition at K=15.
  2. Reproduce the Collapse in Linear Regression: Implement the recursive self-training loop with OLS on fresh synthetic data. Plot the squared norm of the parameter error ||g_L - g_★||^2 against the iteration L to confirm linear growth in expectation.
  3. Reproduce the Data Growth Condition in Gaussian Fitting: Implement the Gaussian fitting loop with different data growth schedules (constant, linear, super-linear). Plot the trace of the covariance matrix Σ_L over iterations to observe the dichotomy in convergence behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do skew-symmetric components emerge in the optimal solutions of non-sparse linear transformers or standard softmax-based architectures?
- Basis: [explicit] Section 2.8 states, "It remains an open question whether similar phenomena arise in the non-sparse case," noting that preliminary checks suggest this structure may be specific to the sparse regime.
- Why unresolved: The theoretical proofs rely on a specific structural assumption (sparsity of weight matrices) which allows for the reduction to preconditioned gradient descent; removing this constraint invalidates the current proofs.
- What evidence would resolve it: Empirical training of non-sparse linear transformers demonstrating the presence or absence of a phase transition in the learned weights, or theoretical analysis extending the current proofs to non-sparse matrices.

### Open Question 2
- Question: How does the presence of label noise or non-linear task complexity alter the phase transition dynamics and emergence of skew-symmetric preconditioners?
- Basis: [explicit] Section 2.8 explicitly states, "Future work could explore whether similar dynamics emerge in the presence of noise or more complex tasks," as the analysis focused on linear regression with clean data.
- Why unresolved: The current mathematical reduction of the transformer forward pass to preconditioned gradient descent assumes a noiseless linear regression setting.
- What evidence would resolve it: A theoretical extension of the loss landscape analysis that accounts for noise variance, or experimental validation of the critical context length under noisy data regimes.

### Open Question 3
- Question: Does the inclusion of quality-gated selection mechanisms or data filtering prevent model collapse in the theoretical settings analyzed?
- Basis: [explicit] Section 3.7 notes the analysis assumes recursive training on all generated data, "abstracting away from this feedback loop between model quality, usage, and training data" found in real-world deployment.
- Why unresolved: The proofs for linear regression and Gaussian fitting rely on the assumption that every generated synthetic sample is used for training, without rejection based on quality.
- What evidence would resolve it: An extension of the martingale theory used in Section 3.4 to include a stochastic acceptance/rejection step based on sample quality, showing if the covariance matrix Σ_t remains bounded away from zero.

## Limitations
- The ICL analysis relies on linear transformers with weight tying and sparse attention, which may not directly translate to full transformers with softmax attention and untied weights.
- The model collapse analysis uses simplified regression and Gaussian fitting models, abstracting away many complexities of real generative models.
- The "context collapse" concept is described qualitatively without formal mathematical treatment, making its precise mechanisms and conditions unclear.

## Confidence
- **High Confidence**: The martingale-based proofs of almost sure convergence/divergence in model collapse (Sections 3.3-3.5) are mathematically rigorous within their assumptions. The equivalence between weight-tied linear transformer forward pass and preconditioned gradient descent (Lemma 2.4.9) is well-established.
- **Medium Confidence**: The phase transition in ICL parameters (Theorem 2.5.7) is proven for the simplified setting but may not generalize to more complex architectures. The critical thresholds for data growth to avert collapse are theoretically sound but may be conservative in practice.
- **Low Confidence**: The "context collapse" concept introduced in the conclusion is described qualitatively without formal mathematical treatment.

## Next Checks
1. **Empirical ICL Phase Transition**: Implement the weight-tied linear transformer and systematically vary context length, layer count, and problem dimension to empirically verify the predicted phase transition in skew-symmetric component emergence and measure the critical thresholds.

2. **Model Collapse in Cumulative Regime**: Extend the model collapse analysis beyond the theoretical proof to empirically test the cumulative data regime, measuring the actual rate of error reduction and comparing it against the theoretical bounds.

3. **Context Collapse in Generation**: Design experiments to quantify "context collapse" during long generations, particularly in chain-of-thought reasoning, by measuring the effective context retention and comparing against the theoretical ICL dynamics.