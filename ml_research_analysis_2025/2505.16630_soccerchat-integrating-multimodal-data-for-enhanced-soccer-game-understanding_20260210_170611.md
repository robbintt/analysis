---
ver: rpa2
title: 'SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding'
arxiv_id: '2505.16630'
source_url: https://arxiv.org/abs/2505.16630
tags:
- soccer
- video
- soccerchat
- dataset
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SoccerChat, a multimodal conversational AI
  framework that integrates visual, textual, and audio data from soccer broadcasts
  to enable enhanced game understanding. Leveraging the SoccerNet dataset enriched
  with jersey color annotations and ASR transcripts, SoccerChat is fine-tuned on a
  structured video instruction dataset for accurate event classification and referee
  decision-making.
---

# SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding

## Quick Facts
- arXiv ID: 2505.16630
- Source URL: https://arxiv.org/abs/2505.16630
- Reference count: 40
- Key outcome: Multimodal conversational AI framework integrating visual, textual, and audio data from soccer broadcasts, achieving weighted F1-scores of 0.57 and 0.49 on six-class and sixteen-class classification tasks respectively

## Executive Summary
This work introduces SoccerChat, a multimodal conversational AI framework that integrates visual, textual, and audio data from soccer broadcasts to enable enhanced game understanding. Leveraging the SoccerNet dataset enriched with jersey color annotations and ASR transcripts, SoccerChat is fine-tuned on a structured video instruction dataset for accurate event classification and referee decision-making. Evaluated on six-class and sixteen-class classification tasks, SoccerChat achieves weighted F1-scores of 0.57 and 0.49 respectively, and shows competitive performance in referee decision tasks with a score of 6.46 on XFoul validation. The results demonstrate that joint training on multimodal soccer-specific data significantly improves performance over general-purpose models, highlighting the value of domain-specific fine-tuning for soccer analytics.

## Method Summary
SoccerChat fine-tunes Qwen2-VL-7B-Instruct on a structured instruction dataset created from SoccerNet annotations, ASR transcripts, and jersey color data. The framework extracts 24 frames at 2.4 FPS from 10-second video clips, processes them through a Vision Transformer, and combines with language modeling for multimodal understanding. GPT-3.5 Turbo generates 49,120 QA pairs with anonymized jersey color references to prevent entity memorization. The model is evaluated on six-class and sixteen-class event classification tasks as well as referee decision-making using the XFoul dataset, with joint training on both SoccerChat and XFoul datasets showing superior performance over sequential fine-tuning approaches.

## Key Results
- Six-class event classification achieves weighted F1-score of 0.57
- Sixteen-class event classification achieves weighted F1-score of 0.49
- Referee decision task scores 6.46 on XFoul validation using QwQ Scorer
- Joint training on SoccerChat and XFoul datasets outperforms sequential fine-tuning approaches
- Anonymization through jersey color references improves model generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training on multiple domain datasets produces better generalization than sequential fine-tuning.
- Mechanism: Simultaneous exposure to SoccerChat and XFoul datasets during training allows the model to learn shared representations that serve both tasks, rather than overwriting specialized knowledge through catastrophic forgetting.
- Core assumption: Beneficial transfer between general soccer understanding and foul-specific reasoning occurs because they share underlying visual-linguistic patterns.
- Evidence anchors:
  - [abstract] "demonstrates that joint training on multimodal soccer-specific data significantly improves performance over general-purpose models"
  - [Section 6] "SC-FT-XF, which was sequentially fine-tuned on XFoul after SoccerChat, performed worse, suggesting that training datasets together from the outset is a more effective strategy"
- Break condition: If target domain is sufficiently distant from pretraining domain (e.g., different sport with distinct rules), sequential domain-adaptive pretraining may become necessary before joint task training.

### Mechanism 2
- Claim: Structured instruction datasets with anonymized entity references improve model generalization by forcing learning of event dynamics rather than entity memorization.
- Mechanism: Replacing specific player/team names with jersey color references (e.g., "red-jerseyed team") prevents the model from relying on superficial correlations between named entities and event types, directing attention to visual patterns and temporal sequences.
- Core assumption: Models otherwise exploit entity-name shortcuts rather than learning transferable event representations.
- Evidence anchors:
  - [Section 3.1] "jersey color data anonymizes these references. This ensures that models focus on event dynamics rather than memorizing specific names, facilitating more generalizable representations"
  - [Section 3.4.1] "Prompt engineering guided the language model to exclude name-based references in generated descriptions, ensuring uniformity"
- Break condition: If deployment requires identifying specific players/teams (e.g., personalized commentary), anonymization during training would need to be partially reversed or supplemented with named-entity fine-tuning.

### Mechanism 3
- Claim: Video frame sampling at 2.4 FPS for 10-second clips captures sufficient temporal dynamics while maintaining computational tractability.
- Mechanism: Uniform temporal sampling produces 24 frames per clip, which the Vision Transformer encodes into approximately 2,688 tokens, allowing the language model backbone to process temporal relationships without excessive sequence length.
- Core assumption: Soccer actions can be understood from 2.4 FPS; faster motions (e.g., ball trajectory details) may be undersampled.
- Evidence anchors:
  - [Section 4.1.2] "SoccerChat always extracts a total of 24 frames for each video segment, employing a frame sampling rate of 2.4 frames per second (FPS) for 10-second segment"
  - [Section 4.1.2] "this results in a comprehensive token sequence of 24 frames × 112 tokens per frame = 2,688 tokens per segment, capturing essential temporal dynamics"
- Break condition: For fine-grained foul analysis requiring precise contact detection, higher frame rates or keyframe selection around annotated events may be necessary.

## Foundational Learning

- Concept: Vision-Language Model Architecture (ViT encoder + LLM decoder with cross-modal projection)
  - Why needed here: SoccerChat builds on Qwen2-VL, which uses a Vision Transformer to encode frames into tokens that the language model processes. Understanding this separation clarifies why frame sampling directly affects token budget and computational cost.
  - Quick check question: Given a 10-second clip with 24 frames at 112 tokens each, what happens to the context window if you double the FPS to capture faster motion?

- Concept: Instruction Fine-Tuning (structured QA pairs that teach models to follow task-specific formats)
  - Why needed here: The SoccerChat dataset contains three QA types (long descriptions, overview-based, detail-based) with specific prompt templates. Understanding instruction tuning explains why generating structured Python JSON outputs during training produces better downstream classification.
  - Quick check question: Why might a model trained only on long descriptions struggle with the six-class classification task that requires selecting from a fixed label set?

- Concept: Catastrophic Forgetting (fine-tuning on new tasks degrades performance on previously learned tasks)
  - Why needed here: SC-FT-XF underperformed SC+XF, demonstrating that sequential fine-tuning on XFoul after SoccerChat caused the model to "forget" general event classification while specializing in fouls.
  - Quick check question: If you need a model that excels at both event classification AND referee decisions, what training strategy does the evidence support?

## Architecture Onboarding

- Component map: Video clip -> ffmpeg extraction -> 24 frames at native resolution -> Qwen2-VL ViT (~675M params) -> dynamic resolution patch embedding -> up to 128 tokens/frame -> M-RoPE position encoding -> Qwen2-7B language backbone with LoRA fine-tuning -> GPT-3.5 Turbo QA generation

- Critical path: 1. Video preprocessing (frame extraction, aspect ratio normalization to 28×28 patch grid) 2. Visual tokenization (maintain aspect ratio, cap at 128 tokens/image) 3. Instruction formatting (JSON-structured Q&A with anonymized entities) 4. Fine-tuning (SoccerChat dataset alone OR SoccerChat + XFoul combined) 5. Evaluation (QwQ Scorer for alignment scoring 0-10)

- Design tradeoffs:
  - Joint vs. sequential training: SC+XF (joint) outperforms SC-FT-XF (sequential) but requires coordinating datasets upfront
  - Frame rate vs. compute: 2.4 FPS keeps tokens manageable but may miss fast ball movements
  - Anonymization vs. personalization: Jersey colors improve generalization but prevent player-specific analysis
  - No direct audio input: Qwen2-VL lacks audio support despite ASR transcripts being used for context generation

- Failure signatures:
  - Catastrophic forgetting: Sequential fine-tuning (SC-FT-XF) shows F1=0.12 on six-class task vs. 0.57 for joint training
  - OOD generalization: Kick-off class has 0.0 recall in 16-class task because examples were absent from training data
  - Confusion between similar classes: Direct vs. indirect free-kicks, shots on vs. off target show high confusion

- First 3 experiments:
  1. Reproduction baseline: Train SoccerChat from Qwen2-VL-7B-Instruct using released dataset; verify F1 ≈ 0.57 on six-class task with identical hyperparameters
  2. Ablation on frame sampling: Compare 2.4 FPS vs. 4.8 FPS (48 frames, ~5,376 tokens) on foul classification subset to test whether higher temporal resolution improves referee decision performance
  3. Joint vs. sequential validation: On a held-out validation set spanning both general events and fouls, compare SC+XF (joint) vs. SC-FT-XF (sequential) to confirm the 6.46 vs. 6.14 XFoul score gap with statistical significance testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can direct audio modality integration (beyond ASR transcripts) improve SoccerChat's event classification and referee decision accuracy?
- Basis in paper: [explicit] The authors state: "While audio can provide valuable contextual information in sports videos, the current implementation of SoccerChat does not directly incorporate audio data, as Qwen2-VL does not support audio processing."
- Why unresolved: The current architecture relies on Qwen2-VL which lacks native audio processing; ASR transcripts are used as a textual proxy but lose prosodic, crowd reaction, and whistle information that could aid real-time event detection.
- What evidence would resolve it: A comparative study integrating raw audio features (e.g., spectrograms or audio embeddings) alongside visual tokens within a unified multimodal architecture, with evaluations on event classification F1-scores and referee decision accuracy.

### Open Question 2
- Question: What training strategies can mitigate catastrophic forgetting when sequentially fine-tuning on specialized soccer datasets (e.g., XFoul) after general soccer pretraining?
- Basis in paper: [inferred] The paper reports SC-FT-XF (sequential fine-tuning on XFoul after SoccerChat) underperformed with a mean score of 3.40 vs. 6.80 for SoccerChat in six-class classification, and states: "fine-tuning on XFoul after SoccerChat training could lead to catastrophic forgetting."
- Why unresolved: The trade-off between domain specialization (referee decisions) and general soccer event comprehension remains unoptimized; joint training (SC+XF) partially addresses this but introduces minor inconsistencies.
- What evidence would resolve it: Systematic comparison of techniques like elastic weight consolidation, replay buffers, or parameter-efficient fine-tuning (e.g., LoRA adapters) evaluated on both XFoul QA scores and multi-class classification metrics.

### Open Question 3
- Question: Can SoccerChat achieve real-time inference latency suitable for live broadcast analysis without significant accuracy degradation?
- Basis in paper: [explicit] The authors state in future directions: "Real-time analysis demands robust models capable of handling multimodal data with minimal latency" and call for "improving real-time processing capabilities."
- Why unresolved: No latency benchmarks are reported; processing 24 frames at 2.4 FPS with 2,688 visual tokens per segment may be computationally prohibitive for sub-second inference.
- What evidence would resolve it: Profiling inference time per clip on standard hardware, followed by accuracy-latency trade-off analysis using techniques like frame sampling reduction, model quantization, or token pruning.

### Open Question 4
- Question: How can explainability mechanisms be integrated into SoccerChat to provide transparent justifications for referee decision recommendations?
- Basis in paper: [explicit] The authors highlight the need for "developing explainable models tailored to professional sports contexts" and note that "challenges such as domain-specific adaptation and explainability remain."
- Why unresolved: While the model outputs textual justifications, these are not systematically validated for faithfulness to visual evidence or reasoning correctness; the QwQ Scorer evaluates alignment with ground truth but not reasoning transparency.
- What evidence would resolve it: An explainability evaluation framework measuring consistency between model justifications and attention heatmaps, frame-level evidence, or expert human assessments of reasoning validity.

## Limitations
- Critical training hyperparameters (learning rate, batch size, optimizer, training duration) are not specified
- Dataset licensing and access requirements for SoccerNet-v2, SoccerNetEchoes, and XFoul are unclear
- No direct audio modality integration despite its potential value for soccer understanding
- No latency benchmarks reported for real-time deployment feasibility

## Confidence
- High Confidence: Joint training on SoccerChat and XFoul datasets outperforms sequential fine-tuning (F1-scores of 0.57 vs. 0.12 on six-class tasks)
- Medium Confidence: Anonymization through jersey color references improves generalization (lacks direct ablation studies)
- Medium Confidence: 2.4 FPS frame sampling is optimal (no empirical validation of higher frame rates for foul detection)

## Next Checks
- Conduct an ablation study comparing SoccerChat performance with and without jersey color anonymization
- Test 2.4 FPS frame sampling assumption by training a model variant with 4.8 FPS on foul detection subset
- Implement controlled experiment to demonstrate catastrophic forgetting with statistical significance testing