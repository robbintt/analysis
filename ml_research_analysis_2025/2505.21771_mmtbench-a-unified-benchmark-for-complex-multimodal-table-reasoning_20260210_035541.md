---
ver: rpa2
title: 'MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning'
arxiv_id: '2505.21771'
source_url: https://arxiv.org/abs/2505.21771
tags:
- table
- image
- baseline
- questions
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMTBench, a human-curated benchmark for multimodal
  table question answering, consisting of 500 real-world tables with 4,021 question-answer
  pairs. The dataset covers diverse table types and question reasoning styles, integrating
  semi-structured data with visual elements like charts, maps, and images.
---

# MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning

## Quick Facts
- arXiv ID: 2505.21771
- Source URL: https://arxiv.org/abs/2505.21771
- Authors: Prasham Yatinkumar Titiya; Jainil Trivedi; Chitta Baral; Vivek Gupta
- Reference count: 40
- Primary result: Introduces MMTBench, a 500-table benchmark with 4,021 QA pairs for multimodal table reasoning, revealing substantial performance gaps in state-of-the-art models.

## Executive Summary
This paper introduces MMTBench, a human-curated benchmark for multimodal table question answering, consisting of 500 real-world tables with 4,021 question-answer pairs. The dataset covers diverse table types and question reasoning styles, integrating semi-structured data with visual elements like charts, maps, and images. Evaluation across five experimental baselines (missing image, entity replaced, image captioning, table-as-image, and interleaved) reveals substantial performance gaps for state-of-the-art models, especially on visual-based and multi-step reasoning questions. The results highlight the difficulty of integrating structured and visual information and underscore the need for improved architectures that better fuse vision and language processing. MMTBench provides a challenging, real-world resource to drive future research in multimodal table understanding.

## Method Summary
The MMTBench benchmark consists of 500 real-world tables and 4,021 human-annotated question-answer pairs covering diverse table types and reasoning styles. Five experimental baselines evaluate model performance: "Missing Image" (text-only, 1-shot), "Entity Replaced" (text-only, 1-shot with ground-truth entity names), "Image Captioning" (0-shot with VLM-generated captions), "Table as Image" (0-shot treating table as single image), and "Interleaved" (0-shot maintaining table structure with embedded images). Evaluation uses Exact Match (EM), Substring Match (SS), and F1 Score metrics across models including Llama3-8b, Mixtral-8x7B, GPT-4o-mini, Gemini 1.5/2.0 Flash, and Qwen-2.5-VL.

## Key Results
- State-of-the-art models show substantial performance gaps across all MMTBench baselines, particularly on visual-based and multi-step reasoning questions
- The "Interleaved Baseline" achieves highest overall performance by requiring simultaneous reasoning over textual and visual modalities
- Visual-based questions represent the most challenging subset, with models struggling to extract and reason about visual attributes from images
- Mathematical questions perform worst among reasoning types, reflecting difficulty with numerical reasoning tasks in multimodal contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint multimodal reasoning over interleaved text and images allows models to leverage contextual cues from both modalities, outperforming single-modality baselines
- Mechanism: The "Interleaved Baseline" provides the table with embedded images, forcing the model to parse structured text and visual content simultaneously. Text supplies entity names and values, while images provide visual attributes (e.g., color, shape) for "Visual-Based" questions. Success requires fusing these representations
- Core assumption: The VLM's vision and language encoders can extract and align features from complex layouts where images and text are spatially related within a grid
- Evidence anchors:
  - [abstract] "Accurate reasoning... requires the ability to relate cell values to corresponding row and column headers, synthesize information across modalities..."
  - [section 4] "The interleaved baseline integrates both visual and textual data... requiring the model to reason over both textual and visual modalities simultaneously"
  - [corpus] No directly relevant corpus evidence for this specific interleaved table structure mechanism
- Break condition: Fails if the vision encoder cannot parse multiple small images within a larger document or if cross-attention fails to bind visual features to correct textual cells, leading to entity disambiguation errors

### Mechanism 2
- Claim: Replacing images with textual captions can partially compensate for limited visual understanding in VLMs, but acts as a lossy bridge
- Mechanism: The "Image Captioning Baseline" uses a VLM to generate text descriptions for each image, replacing the images and transforming the task into text-only reasoning. This allows a powerful text-based LLM to perform logical reasoning using derived visual information
- Core assumption: The captioning VLM extracts all salient visual features relevant to downstream questions, producing semantically rich and accurate descriptions
- Evidence anchors:
  - [section 6.1] "...captions fail to fully capture the semantic richness or spatial layout of actual images"
  - [corpus] No relevant corpus evidence on this specific captioning-as-proxy mechanism for tables
- Break condition: Breaks when captions are generic, omit critical details, or misinterpret visual data. The paper explicitly notes this failure mode

### Mechanism 3
- Claim: Treating the entire table as a single, high-resolution image tests a VLM's native visual parsing and holistic scene understanding independent of structured data processing
- Mechanism: The "Table as an Image Baseline" converts the HTML table into a single image. The VLM must use its vision encoder to recognize rows, columns, text, and embedded visuals as a unified scene without explicit cell structure
- Core assumption: The VLM's vision encoder handles high-resolution, complex layouts and its internal representation implicitly captures the table's two-dimensional relational structure
- Evidence anchors:
  - [section 4] "...allows us to evaluate the effectiveness of vision encoders in parsing and reasoning over structured tabular data"
  - [corpus] Paper "Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts" explores how table format affects performance
- Break condition: Collapses if the vision encoder has limited resolution, poor OCR on dense text, or fails to comprehend implicit grid structure, leading to "Structural Errors"

## Foundational Learning

- Concept: Multimodal Table Structure
  - Why needed here: The task requires understanding the specific interleaved relationship of images and text within a grid. A model must link an image to text in the same row to answer correctly
  - Quick check question: Given a row with a red car image and text "Model X," can your model answer "What color is the car associated with Model X?" by linking image to text?

- Concept: Visual-Based vs. Text-Based Reasoning
  - Why needed here: Benchmark segments questions by type. A model may excel at mathematical reasoning but fail at visual reasoning (e.g., "which flag has no red?"). This distinction is critical for diagnosis
  - Quick check question: Your model calculates averages correctly but fails to identify logos. Is this a reasoning failure or visual feature extraction?

- Concept: Implicit Reasoning
  - Why needed here: "Implicit Questions" require multi-hop inference over entities not mentioned in the query. The model must identify an intermediate entity via visual attributes
  - Quick check question: To answer "By how many points did the country whose flag does not contain any red grow?", what intermediate entity must the model first identify using only visual data?

## Architecture Onboarding

- Component Map: Input Processor (handles HTML/Interleaved, Table-as-Image, Text-only/Captioned formats) -> Vision Encoder (extracts features from images) -> Text/LLM Backbone (performs reasoning) -> Fusion Mechanism (cross-attention aligns visual and textual features)

- Critical Path: 1) Data Ingestion (parse table, embed/replace/render images) -> 2) Feature Extraction (Vision Encoder processes images; Text Encoder processes text/question) -> 3) Multimodal Fusion (align and combine featuresâ€”critical for Interleaved baseline) -> 4) Reasoning & Generation (LLM conditions on fused representation to generate answer)

- Design Tradeoffs: Interleaved vs. Table-as-Image (structure vs. holistic view); Entity Replaced (oracle text) vs. Captioning (noisy text); Open vs. Closed models (performance vs. accessibility)

- Failure Signatures: Low Visual-Based scores (weak Vision Encoder/fusion); Low Implicit scores (reasoning failure); High "Entity Identification Issues" (hallucination/matching failure); "Structural Errors" (grid misinterpretation)

- First 3 Experiments:
  1. Run your VLM on "Missing Image" and "Entity Replaced" baselines to establish lower/upper performance bounds
  2. Evaluate on "Visual-Based" vs. "Implicit" question subsets to diagnose whether the bottleneck is visual extraction or logical inference
  3. Benchmark "Interleaved" vs. "Table as an Image" formats. Prioritize vision encoder resolution if Table-as-Image fails; focus on fusion if Interleaved fails

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model architectures be designed to more effectively fuse vision and language processing to handle the interleaved nature of multimodal tables?
- Basis in paper: [explicit] The authors state in the abstract and conclusion that findings "underscore the urgent need for improved architectures that more tightly integrate vision and language processing"
- Why unresolved: Current State-of-the-Art (SOTA) models exhibit "substantial performance gaps," particularly when forced to reason over both structured text and embedded visual elements simultaneously (the Interleaved Baseline)
- What evidence would resolve it: The development of a model that significantly closes the performance gap between the "Entity Replaced Baseline" (upper bound) and the "Interleaved Baseline" without relying on external captioning tools

### Open Question 2
- Question: What specific mechanisms fail in current VLMs when performing mathematical or visual-based reasoning within a tabular context, as opposed to simple fact verification?
- Basis in paper: [explicit] The results section highlights that while Fact Verification questions achieved the highest performance, "Mathematical Questions performed the worst... reflecting the inherent difficulty models face when handling numerical reasoning tasks" alongside similar struggles in Vision-Based questions
- Why unresolved: The paper identifies the performance disparity but does not isolate whether the failure stems from visual feature extraction, numerical logic alignment, or the loss of structural hierarchy during encoding
- What evidence would resolve it: An ablation study or error analysis that successfully traces mathematical errors in the "Table as an Image" setting back to specific architectural components (e.g., the vision encoder vs. the projection layer)

### Open Question 3
- Question: How would frontier-scale models (larger than those evaluated) perform on the "Table as Image" baseline compared to the "Interleaved" baseline?
- Basis in paper: [inferred] The Limitations section notes, "Resource constraints also limited the scope of our experimental evaluations... we were unable to evaluate larger-scale models which could have provided further insights"
- Why unresolved: The paper only evaluates models like GPT-4o mini and Mixtral-8x7B. It is unclear if the observed difficulty in parsing tables as single images is a fundamental limitation of the approach or a capacity limitation of the specific models tested
- What evidence would resolve it: Evaluation results from larger parameter models (e.g., GPT-4 or Gemini Ultra) showing a distinct performance trend between handling a table as a single image vs. interleaved text/image inputs

## Limitations
- The "Entity Replaced" baseline assumes accurate ground-truth entity mappings, but the paper does not specify whether this was automated or manual, nor does it provide metrics on replacement accuracy
- Visual reasoning performance may be artificially constrained by captioning quality in the Image Captioning baseline, yet no evaluation of caption fidelity is reported
- The "Table as Image" baseline's resolution constraints are acknowledged but not quantified, making it difficult to assess whether observed performance gaps reflect model limitations or input quality issues

## Confidence

- High Confidence: The benchmark construction methodology (500 tables, 4,021 QA pairs, diverse table types) is well-specified and reproducible. The observation that current VLMs show substantial performance gaps across all baselines is supported by clear numerical results
- Medium Confidence: The claim that multimodal fusion is the primary bottleneck for Visual-Based and Implicit question types is plausible but requires further validation. While the interleaved baseline shows better performance than table-as-image, the paper does not isolate whether this advantage stems from structural awareness or other factors like resolution differences
- Low Confidence: The assertion that no single baseline consistently outperforms others across all question types is somewhat misleading. GPT-4o-mini and Gemini models show consistent patterns of weakness on Visual-Based questions, suggesting a more systematic issue with visual feature extraction than the paper implies

## Next Checks

1. **Caption Quality Assessment**: Run the Image Captioning baseline with ground-truth captions (oracle captions) instead of model-generated ones. If performance significantly improves, this validates that captioning quality is the primary bottleneck for visual reasoning

2. **Resolution Sensitivity Analysis**: Systematically vary the resolution of "Table as Image" inputs (e.g., 512x512, 1024x1024, 2048x2048) and measure performance changes. This will determine whether the vision encoder's limitations are resolution-dependent or structural

3. **Cross-Format Consistency Test**: For a subset of tables, evaluate both "Interleaved" and "Table as Image" formats with identical resolution and context length constraints. This controlled comparison will isolate whether performance differences stem from structural awareness or input quality factors