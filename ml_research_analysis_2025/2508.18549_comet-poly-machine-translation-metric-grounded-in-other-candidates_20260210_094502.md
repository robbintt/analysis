---
ver: rpa2
title: 'COMET-poly: Machine Translation Metric Grounded in Other Candidates'
arxiv_id: '2508.18549'
source_url: https://arxiv.org/abs/2508.18549
tags:
- translation
- additional
- comet
- translations
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "COMET-poly addresses the gap between human and automated machine\
  \ translation evaluation by incorporating additional context\u2014either alternative\
  \ translations of the same source or retrieved examples with human quality scores\u2014\
  into the COMET quality estimation framework. It introduces two models: COMET-poly-cand,\
  \ which leverages other translations of the same source, and COMET-poly-ic, which\
  \ uses in-context examples via retrieval."
---

# COMET-poly: Machine Translation Metric Grounded in Other Candidates

## Quick Facts
- arXiv ID: 2508.18549
- Source URL: https://arxiv.org/abs/2508.18549
- Reference count: 35
- COMET-poly-cand achieves 0.118 Kendall's tau-b (vs 0.079 baseline) by leveraging additional translations

## Executive Summary
COMET-poly addresses the gap between human and automated machine translation evaluation by incorporating additional context—either alternative translations of the same source or retrieved examples with human quality scores—into the COMET quality estimation framework. It introduces two models: COMET-poly-cand, which leverages other translations of the same source, and COMET-poly-ic, which uses in-context examples via retrieval. Results show that adding a single alternative translation improves segment-level metric performance from 0.079 to 0.118 Kendall's tau-b correlation, with further gains using more translations. Incorporating retrieved examples yields similar improvements (0.079 to 0.116 tau-b). COMET-poly-cand outperforms larger LLM-based baselines like GEMBA in both performance and efficiency, demonstrating that targeted, smaller models can match or exceed general-purpose systems while being more computationally efficient.

## Method Summary
COMET-poly extends the COMET quality estimation framework by incorporating additional context through two variants. COMET-poly-cand uses multiple translations of the same source, combining their embeddings with the target translation embedding. COMET-poly-ic retrieves in-context examples from historical WMT datasets with human quality scores, using these as additional context. Both variants use xlm-roberta-large as encoder with layerwise attention pooling, producing feature vectors that combine source, target, and additional context embeddings through element-wise operations. The models are trained on WMT direct assessment data up to 2023 and evaluated on WMT 2024 ESA protocol using segment-level Pearson correlation, Kendall's tau-b, and MAE metrics.

## Key Results
- COMET-poly-cand with 1 additional translation improves Kendall's tau-b from 0.079 to 0.118
- COMET-poly-ic with 1 retrieved example achieves 0.116 tau-b correlation
- COMET-poly-cand outperforms larger LLM-based GEMBA baseline in both performance and efficiency
- Using 2-3 additional examples yields optimal performance; more examples provide diminishing returns
- Source-based retrieval outperforms target-based retrieval despite lower translation similarity

## Why This Works (Mechanism)
COMET-poly works by providing the metric with comparative context that humans naturally use when evaluating translations. When judging translation quality, humans implicitly compare against other possible translations or reference examples. By explicitly incorporating alternative translations or retrieved in-context examples with human scores, COMET-poly mimics this human evaluation process. The additional context helps the model better understand what constitutes high-quality translation by providing reference points, particularly useful for challenging cases where a single translation might be ambiguous or have multiple valid interpretations.

## Foundational Learning

**Quality Estimation (QE)**: Predicting human-assigned quality scores for machine translation outputs. Needed to understand the task context; quick check: verify QE differs from standard MT evaluation by not requiring reference translations.

**Layerwise Attention**: Mechanism for combining multiple encoder layers' representations. Needed for understanding how sentence embeddings are produced; quick check: identify how attention weights are computed across layers.

**Retrieval-based In-Context Learning**: Using similar examples from training data as additional context during inference. Needed to grasp poly-ic's mechanism; quick check: verify retrieval uses source embeddings and cosine similarity.

**Kendall's tau-b**: Rank correlation coefficient for ordinal data. Needed to interpret the main performance metric; quick check: confirm it measures pairwise ranking agreement between metric scores and human judgments.

## Architecture Onboarding

**Component Map**: Source text -> xlm-roberta-large encoder (layerwise attention) -> Feature vector (g function) -> Additional context embeddings -> MLP regression head -> Quality score

**Critical Path**: The most important components are the encoder with proper pooling, the feature combination function g(s,t), and the MLP head that scales with additional context. The retrieval mechanism for poly-ic is also critical but separate from the model architecture.

**Design Tradeoffs**: The model trades increased context for computational overhead. Using frozen encoder layers 30% of the first epoch balances adaptation speed with stability. The choice between poly-cand and poly-ic involves retrieval quality vs. availability of alternative translations.

**Failure Signatures**: Performance degradation occurs when additional examples are too dissimilar (noise) or when retrieval returns low-quality matches. The model may also underperform if layerwise attention is implemented incorrectly or if learning rates aren't properly tuned.

**First Experiments**:
1. Verify baseline COMET performance matches 0.079 tau-b on WMT 2024 before adding poly features
2. Test retrieval quality by examining top-5 retrieved examples for random source sentences
3. Validate feature vector construction by checking dimensions match expected (1 + #additional) × 1536

## Open Questions the Paper Calls Out

**Open Question 1**: Can the "poly" evaluation paradigm be effectively adapted for Large Language Models (LLMs) to yield performance gains similar to those observed in smaller, fine-tuned COMET models? The paper notes that unlike COMET, the LLM-based GEMBA metric did not significantly benefit from the poly-cand or poly-ic setups, yet they do not conclude that LLMs are incapable of utilizing such context.

**Open Question 2**: Why does retrieval based on source embeddings yield better performance for COMET-poly-ic despite returning neighbors with lower translation similarity compared to target-based retrieval? The paper identifies this counter-intuitive phenomenon but leaves the underlying mechanism unexplained.

**Open Question 3**: How does the performance of COMET-poly-ic degrade when the retrieved in-context examples contain noisy or low-quality human annotations? The paper explicitly notes that models are "not exempt on the reliance on the quality of previously human-annotated translations" but doesn't systematically test this sensitivity.

## Limitations
- Evaluation limited to 105 segments across 11 language pairs in WMT 2024, which may not generalize to all translation scenarios
- Performance depends on availability of similar sources in historical WMT datasets for retrieval
- Layerwise attention mechanism underspecified, potentially affecting reproducibility
- Diminishing returns with more than 2-3 additional examples suggests limited scalability

## Confidence

**High Confidence**: The relative improvements of COMET-poly over baseline COMET (0.079 to 0.118 tau-b) and the computational efficiency advantage over GEMBA are well-supported by the results presented.

**Medium Confidence**: The claim that COMET-poly-cand "consistently outperforms" COMET-poly-ic across settings requires more extensive ablation studies across diverse conditions to verify robustness.

**Medium Confidence**: The assertion about "superior efficiency" compared to LLM-based baselines is supported by computational analysis but would benefit from more systematic benchmarking across different hardware configurations.

## Next Checks
1. Test multiple reasonable interpretations of the layerwise attention mechanism to determine sensitivity of results to this underspecified component
2. Systematically evaluate how retrieval quality (measured by source similarity scores) correlates with downstream metric performance, particularly for the poly-ic variant
3. Evaluate COMET-poly on out-of-domain datasets (e.g., non-WMT news translation) to assess whether improvements transfer beyond the WMT-focused training and evaluation setup