---
ver: rpa2
title: 'GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval'
arxiv_id: '2511.10154'
source_url: https://arxiv.org/abs/2511.10154
tags:
- text
- image
- person
- images
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of incomplete textual descriptions
  and limited data diversity in text-to-image person retrieval (TIPR), which leads
  to poor cross-modal alignment and overfitting. The proposed Generation-Enhanced
  Alignment (GEA) framework introduces diffusion-generated images as intermediate
  semantic representations to bridge the gap between sparse textual descriptions and
  visual content.
---

# GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval

## Quick Facts
- arXiv ID: 2511.10154
- Source URL: https://arxiv.org/abs/2511.10154
- Reference count: 40
- Achieves state-of-the-art 80.56% R-1 and 72.73% mAP on CUHK-PEDES TIPR dataset

## Executive Summary
This paper addresses the challenge of incomplete textual descriptions and limited data diversity in text-to-image person retrieval (TIPR) by introducing a Generation-Enhanced Alignment (GEA) framework. The method uses diffusion-generated images as intermediate semantic representations to bridge the gap between sparse text and visual content. GEA achieves state-of-the-art performance across three public TIPR datasets by combining text-guided token enhancement with generative intermediate fusion through cross-attention mechanisms.

## Method Summary
GEA consists of two parallel modules: Text-Guided Token Enhancement (TGTE) and Generative Intermediate Fusion (GIF). TGTE generates intermediate images from text using Stable Diffusion 3, then fuses these visual representations with text features through weighted averaging where the mix weight ω increases from 0.3 to 0.6 during training. GIF employs cross-attention between generated images, original images, and text features using separate 6-layer transformer branches. The framework uses CLIP-ViT-B/16 as backbone and trains with a combination of triplet alignment loss and fusion loss over 60 epochs.

## Key Results
- Achieves 80.56% R-1 and 72.73% mAP on CUHK-PEDES (state-of-the-art)
- Improves R-1 to 67.60% and mAP to 54.03% on RSTPReid
- Ablation studies show TGTE alone improves R-1 from 70.84% to 78.57% on CUHK-PEDES

## Why This Works (Mechanism)

### Mechanism 1: Semantic Bridging via Diffusion-Generated Intermediate Representations
- Claim: Diffusion-generated images serve as intermediate semantic representations that narrow the modality gap between sparse text and dense visual content.
- Mechanism: Text → Stable Diffusion 3 → Generated Image → CLIP Encoding → Weighted Fusion (t_cls = (1-ω)t_eos + ωg_cls). The generated image provides a visual anchor that amplifies informative aspects already present in the text.
- Core assumption: Diffusion model faithfully captures and amplifies semantic cues from text without introducing significant hallucinations.
- Evidence anchors: TGTE improves R-1 from 70.84% to 78.57% on CUHK-PEDES; however, generated images may amplify deficiencies when text contains noise.

### Mechanism 2: Tri-Modal Cross-Attention for Fine-Grained Alignment
- Claim: Cross-attention between generated images, original images, and text enables fine-grained semantic correspondence beyond global alignment.
- Mechanism: GIF sets generated image features as K,V and original image/text as Q in separate cross-attention branches, processed through 6-layer transformers.
- Core assumption: Generated images capture text-relevant visual details semantically consistent with original images.
- Evidence anchors: Attention heatmaps show improved localization of text-relevant regions compared to baseline CLIP; ablation confirms GIF contributes ~2% R-1 improvement.

### Mechanism 3: Progressive Feature Mixing with Adaptive Weight Scheduling
- Claim: Gradually increasing the fusion weight (ω) of generated image features balances text fidelity with visual enrichment.
- Mechanism: Weight parameter ω increases from 0.3 to 0.6 during training, allowing early stages to rely more on text while later stages incorporate more visual priors.
- Core assumption: Generated images become more reliable guides as training progresses and cross-modal alignment improves.
- Evidence anchors: Performance degrades when ω exceeds 0.6, indicating over-reliance on generated features is detrimental.

## Foundational Learning

- Concept: Latent Diffusion Models (Stable Diffusion 3 / Rectified Flow)
  - Why needed here: TGTE requires understanding how text-conditioned diffusion generates images from text latent space through ODE-based denoising.
  - Quick check question: How does rectified-flow ODE (dz_t/dt = -v_θ(z_t, t, c)) differ from standard DDPM iterative denoising?

- Concept: CLIP Vision-Language Pretraining
  - Why needed here: GEA uses CLIP-ViT-B/16 as backbone; understanding contrastive pretraining explains how shared embedding spaces enable cross-modal similarity computation.
  - Quick check question: Why does CLIP use separate encoders for images and text rather than a single unified encoder?

- Concept: Cross-Attention in Multi-Modal Transformers
  - Why needed here: GIF relies on cross-attention where generated image features (K,V) attend to original modality queries (Q); understanding Q,K,V roles is essential for debugging fusion failures.
  - Quick check question: What would happen if you swapped Q and K,V roles—setting original image features as K,V and generated features as Q?

## Architecture Onboarding

- Component map:
  ```
  Input: Text T, Original Image I
  ├─ TGTE Branch:
  │   ├─ T → Stable Diffusion 3 (28 steps, 1024×336) → Generated Image G
  │   ├─ G → CLIP Image Encoder → g_cls token
  │   └─ T → CLIP Text Encoder → t_eos → Fuse: t_cls = (1-ω)t_eos + ωg_cls
  ├─ Image Branch: I → CLIP Image Encoder → v_cls + patch tokens
  ├─ GIF Module:
  │   ├─ Cross-Attention: Generated features (K,V) ← Original image patches (Q)
  │   ├─ Cross-Attention: Generated features (K,V) ← Text tokens (Q)
  │   └─ 6-layer Transformer per branch → v_f, t_f
  └─ Output: Cosine similarity + Triplet Alignment Loss
  ```

- Critical path:
  1. Text preprocessing: Append "pedestrian" (positive prompt), "cartoon" (negative prompt)
  2. Diffusion generation: 28 inference steps at guidance scale 7
  3. Three-way CLIP encoding: All modalities mapped to 512-dim shared space
  4. TGTE fusion: Progressive ω from 0.3→0.6
  5. GIF cross-attention: Separate branches for image and text, each with 6 transformer layers
  6. Dual loss: L_Align (global) + L_Fusion (fine-grained)

- Design tradeoffs:
  - ω scheduling: Low ω preserves text semantics but underutilizes generated images; high ω risks overriding text with hallucinations
  - Diffusion steps: 28 steps balances generation quality vs. inference cost (SD3-medium)
  - Prompt engineering: "pedestrian" reinforces person-centric semantics; "cartoon" suppresses style artifacts

- Failure signatures:
  - Noisy text input → Generated images amplify errors → Lower R-1 (observed on ICFG-PEDES)
  - Unspecified colors in text → Generated image diverges from original in those regions → GIF may introduce misalignment
  - Attention not localizing to text-relevant regions → Check prompt engineering or reduce ω

- First 3 experiments:
  1. **Baseline vs. TGTE ablation**: Run CLIP backbone alone vs. +TGTE to confirm semantic enrichment is the primary driver of gains.
  2. **GIF contribution**: Compare +TGTE alone vs. +TGTE+GIF to isolate fine-grained fusion benefits; expect ~2% R-1 improvement.
  3. **ω sensitivity sweep**: Test ω ∈ {0.1, 0.3, 0.5, 0.6, 0.8} while freezing other hyperparameters; expect peak at 0.5-0.6 with degradation at extremes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models effectively detect and filter noisy or ambiguous text queries prior to generation to prevent the amplification of semantic errors in the generated intermediate images?
- Basis in paper: Section 5 states future work will explore strategies for detecting and filtering noisy or ambiguous text queries before generation.
- Why unresolved: Current GEA framework assumes direct mapping from text to generation; noisy descriptions lead to less precise bridging representations and lower Rank-1 accuracy.
- What evidence would resolve it: A confidence-aware generation mechanism or pre-filtering module that improves retrieval accuracy on high-noise datasets without degrading performance on clean data.

### Open Question 2
- Question: How can the diffusion generation process be constrained when specific visual details (e.g., color) are omitted in the text to ensure the generated image does not inadvertently widen the modality gap?
- Basis in paper: Section 5 highlights that when color of a specific body region is not specified, the generated result is unconstrained and may differ from the original, potentially increasing the modality gap.
- Why unresolved: Diffusion model hallucinates unspecified details based on general priors, which may contradict the target image, posing a risk to the alignment mechanism.
- What evidence would resolve it: A method that generates neutral or attribute-agnostic intermediate representations for unspecified tokens, maintaining alignment regardless of missing details.

### Open Question 3
- Question: Can the fusion weight $\omega$ be optimized dynamically based on the semantic completeness of the specific input text?
- Basis in paper: Section 4.5 analyzes sensitivity of the fixed mix weight $\omega$, and Section 3.2 notes it follows a pre-set schedule; performance drops if the weight is suboptimal.
- Why unresolved: A fixed or uniformly increasing schedule cannot adapt to individual samples where text is extremely sparse versus nearly complete.
- What evidence would resolve it: An adaptive weighting strategy that outperforms the static schedule by correlating $\omega$ with a metric of textual completeness.

## Limitations
- The effectiveness relies heavily on the quality and fidelity of diffusion-generated intermediate representations, which may amplify noise and ambiguity in text descriptions.
- Cross-attention mechanism assumes semantic consistency between generated and original images, but may introduce misalignment when text omits specific attributes.
- Computational overhead of real-time intermediate image generation may scale poorly with dataset size.

## Confidence
- **High Confidence**: TGTE module's ability to enrich text features through diffusion-generated intermediate representations is well-supported by ablation studies (78.57% vs 70.84% R-1 on CUHK-PEDES).
- **Medium Confidence**: GIF module's contribution to fine-grained alignment is demonstrated through attention visualizations and performance gains, but exact mechanism for resolving modality conflicts remains partially speculative.
- **Low Confidence**: Method's generalizability to datasets with significantly different characteristics and computational cost implications of real-time generation are not thoroughly tested.

## Next Checks
1. **Text Quality Sensitivity Analysis**: Systematically evaluate GEA performance on subsets of CUHK-PEDES with varying text quality scores to quantify sensitivity to textual noise and ambiguity.

2. **Intermediate Generation Fidelity**: Conduct human evaluation studies comparing semantic alignment between original images and generated intermediate representations, measuring hallucination rates and attribute preservation.

3. **Cross-Dataset Generalization**: Test GEA on additional TIPR datasets with different characteristics (e.g., MSMT17-PT, Cross-Modal Re-ID) to validate whether method's gains transfer beyond three evaluated datasets.