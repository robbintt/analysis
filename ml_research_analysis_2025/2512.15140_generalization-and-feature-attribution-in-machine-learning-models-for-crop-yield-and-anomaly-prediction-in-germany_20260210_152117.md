---
ver: rpa2
title: Generalization and Feature Attribution in Machine Learning Models for Crop
  Yield and Anomaly Prediction in Germany
arxiv_id: '2512.15140'
source_url: https://arxiv.org/abs/2512.15140
tags:
- yield
- learning
- data
- validation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates machine learning models for crop yield and
  anomaly prediction in Germany, focusing on their generalization and feature attribution.
  Models were tested on spatially and temporally independent validation sets.
---

# Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany

## Quick Facts
- arXiv ID: 2512.15140
- Source URL: https://arxiv.org/abs/2512.15140
- Authors: Roland Baatz
- Reference count: 6
- Primary result: Models perform well on spatially split test sets but significantly degrade on temporally independent validation years, revealing limitations in generalization and exposing vulnerabilities in post-hoc explainability methods.

## Executive Summary
This study evaluates machine learning models for crop yield and anomaly prediction in Germany, focusing on their generalization and feature attribution. Models were tested on spatially and temporally independent validation sets. While models performed well on spatially split test sets, their performance significantly degraded on temporally independent validation years, revealing limitations in generalization. Notably, models with strong test-set accuracy but weak temporal validation performance still produced seemingly credible SHAP feature importance values, exposing a vulnerability in post hoc explainability methods. The study emphasizes the need for rigorous, domain-aware validation and cautions against interpreting feature importance without proven model generalization. Deep learning models showed better temporal behavior but worse spatial performance compared to ensemble tree-based models.

## Method Summary
The study predicts winter wheat yield, yield gap, yield gap ratio, and yield anomaly for Germany's NUTS-3 regions using data from 1979-2022. Four models were evaluated: XGBoost, Random Forest, LSTM, and TCN. Tree-based models used 2D feature inputs while deep learning models used time series inputs. Meteorological features (weekly/monthly aggregates, extreme indicators), soil/region constants, and optional time series were used as inputs. Hyperparameter tuning was performed via GridSearchCV for tree models and KerasTuner for deep learning models. Multiple feature setups were tested from single predictor to multi-feature sets. SHAP values were computed for tree-based models. Data was split with 10% random test set and two holdout years (2004, 2018) for temporal validation.

## Key Results
- Models achieved high R² on spatially split test sets but significantly degraded performance on temporally independent validation years
- Deep learning models showed better temporal behavior but worse spatial performance compared to ensemble tree-based models
- Models with strong test-set accuracy but weak temporal validation performance still produced seemingly credible SHAP feature importance values
- Tree-based models (RF and XGBoost) demonstrated better stability than deep learning models across validation conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial random-split validation overestimates model skill by testing interpolation rather than extrapolation.
- Mechanism: When test data is sampled randomly across space and time, models can leverage spatial autocorrelation and periodic patterns to achieve high R². Temporal holdout forces prediction into genuinely novel conditions (unseen climate years), exposing whether the model learned causal structure or merely spatial proxies.
- Core assumption: Spatial yield patterns contain redundant information that inflates conventional test metrics.
- Evidence anchors:
  - [abstract] "While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years."
  - [Section 3.1] "correlation between R² Test and R² Validation is dissimilar for... ensemble tree based models (positive slope) and deep learning models (negative slope)."
  - [corpus] No direct corpus comparison for this specific spatial-temporal contrast; weak external validation.
- Break condition: If your domain has stationarity (no distribution shift across years), temporal holdout may not reveal additional weaknesses.

### Mechanism 2
- Claim: Post-hoc feature importance (SHAP) can appear credible even when the underlying model fails to generalize.
- Mechanism: SHAP quantifies how much each feature contributes to a model's predictions, not whether those predictions are correct. A model that overfits to spurious correlations will still produce internally consistent attribution; the explanation is faithful to the model, not to reality.
- Core assumption: Features with high SHAP values in non-generalizing models reflect learned artifacts rather than causal drivers.
- Evidence anchors:
  - [abstract] "models with strong test-set accuracy but weak temporal validation performance still produced seemingly credible SHAP feature importance values, exposing a vulnerability in post hoc explainability methods."
  - [Section 3.2] "degrading and underperforming models to provide rather high SHAP values compared to those of effective model runs."
  - [corpus] "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction" discusses explainability design but not this decoupling phenomenon explicitly.
- Break condition: If you validate SHAP against known causal mechanisms from domain theory, you may detect spurious attribution.

### Mechanism 3
- Claim: Tree-based and deep learning models exhibit inverse space-time performance tradeoffs due to architectural inductive biases.
- Mechanism: Tree ensembles partition feature space efficiently, capturing static spatial heterogeneity. Deep learning (LSTM, TCN) builds explicit temporal representations, potentially capturing dynamics but with higher variance. The negative correlation between R² Test and R² Validation for deep models suggests they trade spatial interpolation for temporal extrapolation capacity.
- Core assumption: The observed tradeoff is structural, not merely a result of hyperparameter tuning differences.
- Evidence anchors:
  - [Section 4.2] "tree-based models (RF and XGBoost) demonstrated better stability than deep learning models... The negative correlation between R² Test and R² Validation for deep learning models may be attributed to their increased flexibility towards temporal behavior."
  - [Section 3.1] "Deep learning models showed better temporal behavior but worse spatial performance compared to ensemble tree-based models."
  - [corpus] "Informed Learning for Estimating Drought Stress" notes process-based models offer intrinsic explainability but poor prediction, hinting at similar tradeoffs.
- Break condition: If feature engineering injects explicit temporal summaries into tree models, the gap may narrow.

## Foundational Learning

- Concept: **Temporal holdout validation**
  - Why needed here: Random cross-validation masks generalization failure; you must test on years the model has never seen to detect overfitting.
  - Quick check question: Can you identify which years were held out entirely, and are they climatically diverse?

- Concept: **SHAP as local-to-global attribution**
  - Why needed here: SHAP explains model behavior, not real-world causation; without proven generalization, importance scores are untrustworthy.
  - Quick check question: Have you validated that your model generalizes before interpreting SHAP plots?

- Concept: **Inductive bias of model families**
  - Why needed here: Tree models and sequence models make different assumptions about structure in data; selecting blindly forfeits generalization control.
  - Quick check question: Does your problem have stronger spatial or temporal structure, and does your architecture match?

## Architecture Onboarding

- Component map: Input layer (meteorological features, soil/region constants, optional time series) -> Model families (Tree ensembles: XGBoost, Random Forest; Sequence models: LSTM, TCN) -> Validation (internal train/test split + external temporal holdout: years 2004, 2018) -> Explainability (SHAP post-hoc for tree-based models only)

- Critical path:
  1. Preprocess yield into detrended targets (yield gap, anomaly, gap ratio)
  2. Engineer temporal and extreme weather features
  3. Split data: 10% random test, 2 holdout years, remainder for training
  4. Tune hyperparameters via time-series-aware CV or KerasTuner
  5. Evaluate on both random test and temporal holdout
  6. If and only if temporal R² > 0, compute SHAP; otherwise flag as unreliable

- Design tradeoffs:
  - Tree models: Faster training, interpretable, better spatial R², worse temporal extrapolation
  - Deep learning: Slower, requires more tuning, better temporal flexibility, higher variance
  - Multi-output (yield, anomaly, gap): More efficient but risks conflicting gradients

- Failure signatures:
  - R² Test > 0.5 but R² Validation < 0: Model overfits to spatial structure
  - SHAP values highly concentrated on few features with low validation R²: Spurious attribution
  - Deep learning training loss stalls: Insufficient data or learning rate issues

- First 3 experiments:
  1. Baseline comparison: Run XGBoost and LSTM on identical feature sets; report both R² Test and R² Validation to quantify space-time tradeoff.
  2. Feature ablation: Train on meteorological-only vs. meteorological + extreme indicators; assess whether extremes improve temporal holdout.
  3. Validation sanity check: Deliberately train a model known to overfit (high depth, no regularization); confirm that SHAP still looks "reasonable" despite poor temporal R², demonstrating the decoupling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks be designed to robustly link model generalization performance to the trustworthiness of post hoc feature attributions?
- Basis in paper: [explicit] The abstract explicitly asks: "how can we evaluate generalization robustly enough to trust model explanations?"
- Why unresolved: The study shows that standard test-set performance is insufficient, but does not propose a specific new metric or framework to formally validate explanations against generalization.
- What evidence would resolve it: A novel validation protocol where feature importance rankings are proven invalid during temporal failure modes, or a quantitative threshold linking $R^2$ validation drops to explanation uncertainty.

### Open Question 2
- Question: Can hybrid modeling strategies that integrate process-based constraints overcome the structural limitations of standalone ML models in temporal extrapolation?
- Basis in paper: [explicit] The conclusion states that "Hybrid approaches that combine process-based models with data-driven learning may offer a promising path."
- Why unresolved: While suggested as a solution, this study only evaluated standalone ML models (XGBoost, LSTM, etc.) and did not test a hybrid architecture.
- What evidence would resolve it: A comparative study showing that physics-informed or hybrid models maintain positive $R^2$ validation scores in years where standalone data-driven models degrade.

### Open Question 3
- Question: Can deep learning architectures be optimized to match the spatial performance of ensemble tree-based models while retaining their superior temporal generalization?
- Basis in paper: [inferred] Section 4.2 notes that deep learning models showed better temporal behavior but consistently worse spatial performance compared to tree-based models.
- Why unresolved: The paper identifies this trade-off but does not investigate architectural changes or training techniques to bridge the performance gap between the two model categories.
- What evidence would resolve it: Identification of a deep learning configuration that achieves statistical parity with XGBoost on spatial test sets without losing temporal validation accuracy.

## Limitations

- Limited temporal holdout period (only 2004 and 2018) constrains generalization conclusions
- Specific feature engineering details and hyperparameter configurations are underspecified
- Study does not validate whether SHAP values for poorly generalizing models correspond to spurious correlations rather than genuine drivers
- Mechanism explaining why deep learning models trade spatial for temporal performance is not experimentally tested

## Confidence

- **High Confidence**: Spatial-temporal generalization gap exists (test R² high, validation R² low); SHAP values can appear credible for non-generalizing models; tree vs. deep learning models show inverse space-time performance tradeoffs
- **Medium Confidence**: The claim that post-hoc explainability is inherently vulnerable without proven generalization; the specific architectural reasons for the space-time tradeoff in deep learning models
- **Low Confidence**: The exact features and time windows used; the hyperparameter settings that produced the reported results; whether the identified vulnerabilities manifest in other crop yield datasets or regions

## Next Checks

1. **Expand Temporal Holdout**: Validate the spatial-temporal generalization gap using multiple holdout years (e.g., 2004, 2010, 2018) to assess if the observed degradation is consistent across diverse climatic conditions.

2. **SHAP Validation Against Domain Theory**: Compare SHAP feature importance for poorly generalizing models against established agronomic drivers (e.g., temperature during grain filling) to determine if attribution reflects spurious correlations.

3. **Architectural Ablation Study**: Train tree-based models with explicitly temporal features (e.g., lagged variables, rolling aggregates) to test if the observed space-time tradeoff is due to architectural inductive bias or feature engineering choices.