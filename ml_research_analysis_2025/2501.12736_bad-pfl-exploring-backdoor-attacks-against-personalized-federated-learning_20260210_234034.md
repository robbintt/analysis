---
ver: rpa2
title: 'Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning'
arxiv_id: '2501.12736'
source_url: https://arxiv.org/abs/2501.12736
tags:
- backdoor
- clients
- attack
- client
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bad-PFL addresses the failure of existing federated backdoor attacks
  in personalized federated learning (PFL) by using natural features from target labels
  as triggers rather than manually designed ones. The attack employs a generator to
  identify these natural features and introduces disruptive noise to corrupt ground-truth
  label features, ensuring the backdoor persists in personalized models trained on
  natural data.
---

# Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning

## Quick Facts
- arXiv ID: 2501.12736
- Source URL: https://arxiv.org/abs/2501.12736
- Authors: Mingyuan Fan; Zhanyi Hu; Fuyi Wang; Cen Chen
- Reference count: 40
- Primary result: >80% ASR on personalized models using natural feature triggers

## Executive Summary
Bad-PFL introduces a novel backdoor attack against personalized federated learning (PFL) that exploits natural features from target labels as triggers, rather than relying on manually designed triggers. This approach addresses the fundamental weakness of existing federated backdoor attacks in PFL settings where models are trained on natural data without explicit triggers. The attack combines a generator that identifies natural target features with disruptive noise that corrupts ground-truth label features, ensuring the backdoor persists even in personalized models. Large-scale experiments demonstrate superior attack success rates exceeding 80% across multiple benchmark datasets while maintaining high stealthiness against state-of-the-art defense mechanisms.

## Method Summary
Bad-PFL operates by poisoning federated learning with a two-component trigger mechanism. First, a generator network identifies natural features associated with target labels by analyzing gradients from compromised clients. Second, disruptive noise computed via FGSM-style gradient ascent corrupts the ground-truth features of clean inputs. The combined trigger $x' = x + \delta + \xi$ is used to poison the training data of compromised clients. During federated training, these clients first train the generator to minimize loss on target classes, then train their local model on the poisoned data, and finally update their personalized model. The attack maintains effectiveness across various PFL methods (FedRep, FedBN, LG-FedAvg) and achieves high stealthiness through low detectability via neural cleanse and high entropy in STRIP evaluations.

## Key Results
- Achieves attack success rates exceeding 80% on personalized models across three benchmark datasets
- Maintains high attack effectiveness even when state-of-the-art defense mechanisms are deployed
- Demonstrates superior stealth characteristics with low detectability through neural cleanse and high STRIP entropy
- Outperforms existing federated backdoor attacks specifically in personalized federated learning settings

## Why This Works (Mechanism)
The attack succeeds because it leverages natural features inherent to the target class rather than artificial triggers that would be filtered out during personalized model training on natural data. By using a generator to identify these natural features and combining them with input-gradient noise that disrupts ground-truth features, the backdoor becomes indistinguishable from legitimate variations in the data distribution. This approach is particularly effective in PFL because personalized models are trained to adapt to local data distributions where these natural features are prominent.

## Foundational Learning

**Personalized Federated Learning (PFL)** - Federated learning where each client trains a personalized model rather than sharing a single global model. Needed to understand the attack target; check by verifying client models diverge from the global model during training.

**Non-IID Data Partitioning** - Data distribution where each client has different class distributions, typically implemented via Dirichlet distribution. Needed to create realistic federated learning scenarios; check by verifying class imbalance across clients.

**FGSM (Fast Gradient Sign Method)** - Adversarial attack technique using input-gradient information to create perturbations. Needed for generating disruptive noise; check by verifying noise magnitude and direction match gradient signs.

**Generator Networks in Backdoor Attacks** - Neural networks that automatically discover trigger patterns rather than using manual designs. Needed for finding natural feature triggers; check by visualizing generated triggers and comparing to class samples.

**Neural Cleanse Detection** - Defense mechanism that attempts to reverse-engineer potential triggers from model behavior. Needed to evaluate stealth; check by running neural cleanse and measuring anomaly indices.

**STRIP (Sensor-based Trojan Detection)** - Defense using entropy measurements to detect trojaned inputs. Needed for stealth evaluation; check by measuring entropy of perturbed vs clean inputs.

## Architecture Onboarding

**Component Map:** Generator (4-layer encoder-decoder) -> Trigger Function (combines generator output + FGSM noise) -> Local Classifier (ResNet-10) -> Personalized Model (FedRep/FedBN)

**Critical Path:** Compromised client selection → Generator training (30 steps) → Classifier training on poisoned data (15 steps) → Personalized model update → Attack success measurement

**Design Tradeoffs:** Natural features vs artificial triggers (better stealth but requires generator training), disruptive noise magnitude (higher attack success vs lower imperceptibility), poison rate (more effective but more detectable)

**Failure Signatures:** Low ASR (< random chance), backdoor dilution over training rounds, generator fails to converge, compromised clients fail to be selected consistently

**First Experiments:**
1. Verify trigger generation by visualizing perturbed images and confirming perturbation budget compliance
2. Test generator's ability to identify natural features by measuring correlation with ground-truth label features
3. Compare Bad-PFL against baseline attacks (BadNet) to quantify improvement specifically for personalized models

## Open Questions the Paper Calls Out
None identified in available abstract.

## Limitations
- Implementation details for mutual reinforcement training mechanism remain unclear
- Specific random seed and weight initialization strategies are unspecified
- Exact Dirichlet distribution parameters and data partitioning methodology require clarification

## Confidence
- Attack methodology and trigger design: High
- ASR performance claims: Medium (based on abstract claims)
- Stealth evaluation methodology: Low (insufficient detail in abstract)

## Next Checks
1. Verify the trigger generation process by visualizing perturbed images and confirming that δ + ξ remains within the 4/255 perturbation budget while maintaining semantic meaning
2. Test the generator's ability to identify natural features by measuring the correlation between generated triggers and ground-truth label features across different target classes
3. Implement a controlled experiment comparing Bad-PFL against baseline attacks (e.g., standard BadNet) to quantify the improvement in ASR specifically for personalized models versus global models