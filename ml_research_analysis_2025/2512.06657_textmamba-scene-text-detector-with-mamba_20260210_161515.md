---
ver: rpa2
title: 'TextMamba: Scene Text Detector with Mamba'
arxiv_id: '2512.06657'
source_url: https://arxiv.org/abs/2512.06657
tags:
- text
- detection
- feature
- scene
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of Transformer-based methods
  in scene text detection, particularly their challenges in modeling long-range dependencies
  and handling complex text shapes. The authors propose TextMamba, a novel detector
  that integrates the Mamba state space model with attention layers to enhance feature
  extraction from long sequences.
---

# TextMamba: Scene Text Detector with Mamba

## Quick Facts
- **arXiv ID**: 2512.06657
- **Source URL**: https://arxiv.org/abs/2512.06657
- **Reference count**: 40
- **Primary result**: Achieves 89.7% F-measure on CTW1500, 89.2% on TotalText, and 78.5% on ICDAR19-ArT for arbitrarily shaped text detection

## Executive Summary
TextMamba addresses the limitations of Transformer-based scene text detectors, particularly their challenges with long-range dependencies and complex text shapes. The authors propose a novel detector that integrates the Mamba state space model with attention layers to enhance feature extraction from long sequences. By combining sparse deformable attention with Mamba's selection mechanism through a Mix-SSM block, dual-scale feed-forward networks, and an embedding pyramid enhancement module, TextMamba achieves state-of-the-art or competitive performance on multiple benchmarks while demonstrating superior ability in detecting arbitrarily shaped and multi-lingual text.

## Method Summary
TextMamba uses a ResNet50 backbone to extract feature maps, which are then flattened into 1D sequences with position embeddings. The encoder employs Mix-SSM blocks that combine sparse deformable attention with Top_k sparsification followed by SS2D for long-range modeling, enhanced by a dual-scale feed-forward network. An embedding pyramid enhancement module reconstructs 2D features from embeddings and fuses them with backbone features. The decoder uses an anchor point prior with a mask head and point offset MLP to iteratively refine polygon control points for arbitrarily shaped text detection. The model is pre-trained on SynthText and MLT17, then fine-tuned on target datasets.

## Key Results
- Achieves 89.7% F-measure on CTW1500 (state-of-the-art)
- Achieves 89.2% F-measure on TotalText (state-of-the-art)
- Achieves 78.5% F-measure on ICDAR19-ArT (competitive performance)
- Demonstrates superior ability in detecting arbitrarily shaped and multi-lingual text

## Why This Works (Mechanism)

### Mechanism 1: Sparse Attention Selection Before Long-Range SSM Modeling
Sparsifying attention weights via Top_k before SS2D reduces irrelevant information interference and improves long-range dependency modeling. Deformable attention captures global features, Top_k zeroes out all but the k highest attention weights per row to select key information, and SS2D processes this filtered sequence with its selection mechanism to retain relevant information over long sequences.

### Mechanism 2: Dual-Scale Feed-Forward Network for Hidden State Interaction
A dual-path feed-forward network with different expansion ratios (2× and 4×) enhances high-dimensional hidden state interactions. Input passes through two parallel branches: LL_E1 (C→2C) + LL_R1 (2C→C) and LL_E2 (C→4C) + LL_R2 (C→C); outputs are element-wise added to the original input via residual connections, enabling multi-scale representation mixing.

### Mechanism 3: Embedding Pyramid Enhancement for Token-wise Multi-scale Fusion
Reconstructing 2D feature maps from 1D embeddings and fusing them with backbone features mitigates the feature information bottleneck. Encoder output eS is reshaped back to 2D feature maps F_ir; these are added to backbone features Fi to produce Fi'; a Feature Pyramid Enhancement Module (FPEM) fuses multi-scale features; only enhanced F3' is retained for mask head input.

## Foundational Learning

- **State Space Models (SSMs) and Selective Scanning (SS2D)**: Understanding how Mamba's selection mechanism differs from self-attention is essential to grasp why SS2D is integrated after sparse attention. Quick check: Can you explain how SS2D's four-direction scanning differs from standard self-attention's global token-to-token computation?

- **Deformable Attention**: TextMamba builds on deformable attention as the initial global feature extractor before SS2D. Understanding its sparse sampling is critical. Quick check: How does deformable attention reduce computational cost compared to full self-attention while preserving spatial awareness?

- **Feature Pyramid Networks (FPN) and Multi-scale Fusion**: EPEM is a variant of FPN applied to embedding sequences. Understanding pyramid fusion helps diagnose feature bottleneck issues. Quick check: Why does element-wise addition in feature pyramids require matched spatial resolutions and channel dimensions?

## Architecture Onboarding

- **Component map**: Backbone (ResNet50) -> Feature maps Fi -> Flatten + Position Embeddings -> Mix-SSM blocks (Sparse Deformable Attn + Top_k -> SS2D -> DSFFN) x N -> EPEM -> Decoder (Anchor Point Prior + Mask Head + Point Offset MLP)

- **Critical path**: 1. Feature extraction (ResNet50) -> 2. Token flattening + PE -> 3. Sparse deformable attention (Top_k) -> 4. SS2D long-range modeling -> 5. DSFFN hidden state interaction -> 6. EPEM multi-scale fusion -> 7. Decoder mask prediction + control point regression

- **Design tradeoffs**: SS2D improves long-range modeling but adds +339M parameters vs baseline 6-attention layer config; 18-attention-layer baseline adds +909M for similar F-measure, making SS2D more parameter-efficient. EPEM improves precision but reduces FPS by 0.2; DSFFN reduces FPS by 0.1.

- **Failure signatures**: Low recall on low-contrast text -> Top_k may be over-sparsifying; check attention distribution. High precision, low recall -> EPEM may be suppressing weak text features; verify fusion weights. Instability during training -> Check gradient flow through DSFFN residual connections.

- **First 3 experiments**: 1. Ablate SS2D: Replace Mix-SSM with deformable attention only; expect F-measure drop of ~2.4%. 2. Vary Top_k: Sweep k values (e.g., 5, 10, 20) on validation subset; plot precision-recall tradeoff. 3. Remove EPEM: Disable embedding pyramid fusion; expect precision drop of ~3.6% and increased false positives.

## Open Questions the Paper Calls Out

1. **Lightweight Framework Optimization**: How can TextMamba be optimized to achieve a more lightweight framework without compromising state-of-the-art detection performance? The current Mix-SSM block increases parameter count to boost F-measure, creating a trade-off between accuracy and model size.

2. **Pure SSM-Based Detector**: Is it possible to construct a purely SSM-based scene text detector that avoids the performance degradation associated with naive SS2D introduction? The paper shows hybrid approaches work but doesn't determine if pure SS2D failure was due to the mechanism or lack of architectural adaptations.

3. **Dynamic Top_k Selection**: To what extent does fixed selection size (k) in Top_k algorithm limit model adaptability to images with vastly different text densities? The paper uses fixed sparsification that may not generalize optimally across sparse and dense text scenes.

## Limitations
- Top_k sparsification mechanism's isolated contribution lacks strong corpus validation
- 339M parameter increase for SS2D implementation represents significant architectural change
- EPEM's element-wise addition assumes strong feature alignment with potential instability

## Confidence

**High Confidence**: Overall architecture design combining Mamba with attention layers, and reported benchmark performances on CTW1500 (89.7%), TotalText (89.2%), and ICDAR19-ArT (78.5%) are well-supported by experimental methodology and ablation studies.

**Medium Confidence**: Specific mechanisms of Top_k sparsification improving signal-to-noise ratio for SS2D, and dual-scale FFN enhancing hidden state interactions, are theoretically sound but have limited direct corpus validation.

**Low Confidence**: Claim that EPEM's element-wise addition "mitigates the feature information bottleneck" lacks strong corpus support, and method's sensitivity to feature alignment is not thoroughly explored.

## Next Checks

1. **Top_k Hyperparameter Sensitivity Analysis**: Systematically sweep the k parameter (e.g., 5, 10, 20, 50) on a validation subset of TotalText or CTW1500 to quantify precision-recall tradeoffs and identify optimal sparsity levels.

2. **Alternative Sparsification Comparison**: Replace Top_k with alternative attention sparsification methods (e.g., random sampling, threshold-based) while keeping all other components constant to isolate whether explicit top-k selection provides unique benefits.

3. **EPEM Gradient Stability Analysis**: Monitor gradient norms and feature alignment metrics during training with and without EPEM, particularly focusing on element-wise addition operation, to identify potential instability conditions.