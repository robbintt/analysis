---
ver: rpa2
title: 'ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised Contrastive
  Pretraining for Medical Imaging'
arxiv_id: '2508.18613'
source_url: https://arxiv.org/abs/2508.18613
tags:
- learning
- multi-label
- modan-mulsupcon
- contrastive
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging ubiquitous metadata
  (modality and anatomical region) for pretraining medical imaging models, as expert
  annotations limit large-scale supervised pretraining. The proposed method, ModAn-MulSupCon,
  encodes modality and anatomy as multi-hot vectors and pretrains a ResNet-18 encoder
  on a mini subset of RadImageNet using a Jaccard-weighted multi-label supervised
  contrastive loss.
---

# ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised Contrastive Pretraining for Medical Imaging

## Quick Facts
- arXiv ID: 2508.18613
- Source URL: https://arxiv.org/abs/2508.18613
- Reference count: 40
- Primary result: ModAn-MulSupCon achieves best AUC in fine-tuning scenarios (p<0.05) across three medical imaging tasks

## Executive Summary
ModAn-MulSupCon addresses the challenge of leveraging ubiquitous metadata (modality and anatomical region) for pretraining medical imaging models, as expert annotations limit large-scale supervised pretraining. The proposed method encodes modality and anatomy as multi-hot vectors and pretrains a ResNet-18 encoder on a mini subset of RadImageNet using a Jaccard-weighted multi-label supervised contrastive loss. This approach encourages modality-aware and anatomy-coherent representations. The pretrained encoder is then evaluated by fine-tuning and linear probing on three binary classification tasks: ACL tear (knee MRI), lesion malignancy (breast ultrasound), and nodule malignancy (thyroid ultrasound). The primary result shows that with fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL (0.964) and Thyroid (0.763), surpassing all baselines (p<0.05), and ranked second on Breast (0.926) behind SimCLR (0.940; not significant). However, with the encoder frozen, SimCLR/ImageNet were superior, indicating that ModAn-MulSupCon representations benefit most from task adaptation rather than linear separability.

## Method Summary
The method encodes modality and anatomy metadata as multi-hot vectors representing available modality and anatomical region labels for each image. A ResNet-18 encoder is pretrained on a mini subset of RadImageNet using a Jaccard-weighted multi-label supervised contrastive loss that encourages representations to be modality-aware and anatomy-coherent. During pretraining, images are augmented and their representations are pulled together based on shared modality/anatomy labels while being pushed apart from those with different labels. The Jaccard similarity between multi-hot label vectors weights the contrastive loss, ensuring that images with more label overlap have stronger attraction. The pretrained encoder is then evaluated through fine-tuning and linear probing on three binary classification tasks: ACL tear detection in knee MRI, lesion malignancy classification in breast ultrasound, and nodule malignancy classification in thyroid ultrasound.

## Key Results
- With fine-tuning, ModAn-MulSupCon achieved best AUC on MRNet-ACL (0.964) and Thyroid (0.763), surpassing all baselines (p<0.05)
- On Breast task, ModAn-MulSupCon ranked second (0.926) behind SimCLR (0.940; not significant)
- With frozen encoder, SimCLR/ImageNet were superior, indicating ModAn-MulSupCon benefits from task adaptation rather than linear separability

## Why This Works (Mechanism)
ModAn-MulSupCon works by leveraging ubiquitous metadata that is readily available in medical imaging datasets but typically unused for pretraining. By encoding modality and anatomy information as multi-hot vectors, the method creates a rich supervisory signal that captures the multi-label nature of medical images (a single image can belong to multiple anatomical regions or modalities). The Jaccard-weighted contrastive loss ensures that images with similar metadata are pulled together in representation space while dissimilar ones are pushed apart, creating representations that are both modality-aware and anatomy-coherent. This metadata-driven pretraining provides a practical, scalable alternative to expert annotations for large-scale pretraining, particularly valuable in label-scarce clinical settings.

## Foundational Learning
- **Multi-hot encoding**: Representing multiple simultaneous labels as binary vectors; needed to capture the multi-label nature of medical images where one image can belong to multiple anatomical regions or modalities; quick check: verify that label vectors correctly represent all applicable modalities/anatomies per image
- **Supervised contrastive learning**: Contrastive loss using label information to pull similar samples together; needed to create semantically meaningful representations using metadata instead of manual annotations; quick check: ensure loss gradients properly separate dissimilar pairs
- **Jaccard similarity weighting**: Using intersection-over-union of label sets to weight contrastive loss; needed to handle varying degrees of label overlap and prevent dominance by high-frequency labels; quick check: verify Jaccard weights correctly reflect label overlap proportions
- **Fine-tuning vs linear probing**: Two evaluation paradigms testing representation quality; needed to distinguish between representations that benefit from task adaptation versus those with inherent linear separability; quick check: compare downstream performance across both paradigms
- **Multi-label medical image classification**: Handling images with multiple anatomical or modality labels; needed because medical images often span multiple categories simultaneously; quick check: ensure evaluation metrics properly handle multi-label scenarios
- **RadImageNet pretraining**: Using a curated subset of medical images for pretraining; needed to provide diverse, representative medical imaging data for initialization; quick check: verify pretraining dataset adequately represents target domains

## Architecture Onboarding

**Component map**: Image augmentation -> ResNet-18 encoder -> Multi-hot label encoding -> Jaccard-weighted contrastive loss -> Representation space

**Critical path**: Input image → Augmentation → Encoder forward pass → Multi-hot label encoding → Jaccard similarity computation → Contrastive loss → Backward pass → Parameter update

**Design tradeoffs**: 
- Uses ResNet-18 (smaller, faster) versus larger architectures (slower but potentially more expressive)
- Multi-label contrastive loss versus single-label alternatives (more complex but captures metadata richness)
- Jaccard weighting versus uniform weighting (more nuanced but computationally heavier)
- Limited pretraining dataset (faster but potentially less generalizable) versus larger datasets (slower but more robust)

**Failure signatures**:
- Poor downstream performance indicates contrastive loss not capturing meaningful relationships
- Overfitting to pretraining data suggests insufficient regularization or dataset diversity
- Suboptimal performance on specific modalities/anatomies indicates imbalanced pretraining data
- Inconsistent results across fine-tuning and linear probing suggest representations not capturing task-relevant features

**First experiments**:
1. Visualize t-SNE embeddings of pretrained representations colored by modality/anatomy labels to verify clustering
2. Perform ablation study removing Jaccard weighting to assess its impact on downstream performance
3. Test with different augmentation strengths to find optimal balance between invariance and discriminative power

## Open Questions the Paper Calls Out
None

## Limitations
- Uses relatively small pretraining dataset (1,200 images) compared to typical self-supervised pretraining benchmarks
- All downstream tasks are binary classification problems, leaving uncertainty about performance on multi-class or segmentation tasks
- Evaluation only covers three specific anatomical regions and modalities, limiting generalizability conclusions
- No comparison of computational efficiency or memory requirements during pretraining versus alternatives

## Confidence
- High confidence: ModAn-MulSupCon outperforms baselines in fine-tuning scenarios for the three tested tasks (supported by statistical significance)
- Medium confidence: ModAn-MulSupCon provides better initialization than frozen-encoder approaches (limited to three tasks and binary classification)
- Medium confidence: Metadata encoding as multi-hot vectors is an effective pretraining signal (evidence is positive but from limited scope)

## Next Checks
1. Evaluate ModAn-MulSupCon on multi-class classification and segmentation tasks to assess generalizability beyond binary classification
2. Test performance scaling by pretraining on progressively larger subsets of RadImageNet (e.g., 5K, 10K, 50K images) to identify optimal dataset size
3. Compare computational requirements and training time against SimCLR and other baselines to quantify practical deployment considerations