---
ver: rpa2
title: 'Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source
  Large Language Models'
arxiv_id: '2512.24058'
source_url: https://arxiv.org/abs/2512.24058
tags:
- calibration
- reliability
- uncertainty
- robustness
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of unified reliability evaluation
  for large language models, which often exhibit overconfidence, poor calibration,
  and vulnerability to input perturbations. The authors propose the Composite Reliability
  Score (CRS), integrating calibration, robustness, and uncertainty quantification
  into a single interpretable metric.
---

# Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models

## Quick Facts
- arXiv ID: 2512.24058
- Source URL: https://arxiv.org/abs/2512.24058
- Reference count: 2
- Proposes a Composite Reliability Score integrating calibration, robustness, and uncertainty quantification

## Executive Summary
This work addresses the lack of unified reliability evaluation for large language models, which often exhibit overconfidence, poor calibration, and vulnerability to input perturbations. The authors propose the Composite Reliability Score (CRS), integrating calibration, robustness, and uncertainty quantification into a single interpretable metric. Tested across ten open-source LLMs and five QA datasets, CRS reveals significant reliability differences obscured by accuracy alone. Mistral-8x22B achieved the highest score (0.81), while 7B models like Falcon-7B scored as low as 0.52, highlighting the importance of multi-dimensional assessment. The method consistently ranks models and identifies failure modes missed by isolated metrics, providing actionable insights for model deployment.

## Method Summary
The authors develop a Composite Reliability Score (CRS) that quantifies LLM reliability through three dimensions: calibration, robustness, and uncertainty quantification. Calibration is measured via Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), assessing how well predicted probabilities align with empirical accuracy. Robustness is evaluated using Lipschitz regularity, sensitivity to input perturbations, and performance under distributional shifts. Uncertainty quantification is measured through confidence intervals and uncertainty-aware scoring. These three components are normalized and combined into a single interpretable score ranging from 0 to 1. The framework was tested on ten open-source models (7B-70B parameters) across five QA datasets, revealing reliability gaps not captured by accuracy metrics alone.

## Key Results
- CRS reveals significant reliability differences obscured by accuracy alone
- Mistral-8x22B achieved the highest reliability score (0.81), while Falcon-7B scored as low as 0.52
- The method consistently ranks models and identifies failure modes missed by isolated metrics
- 7B parameter models generally scored lower than larger models in reliability metrics

## Why This Works (Mechanism)
The CRS framework works by decomposing reliability into three fundamental dimensions that capture different failure modes of LLMs. Calibration metrics (ECE, MCE) identify overconfident predictions that appear accurate but lack true probability alignment. Robustness metrics (Lipschitz regularity, perturbation sensitivity) measure stability under input variations that occur in real-world deployments. Uncertainty quantification metrics capture the model's awareness of its own limitations, preventing false certainty in uncertain situations. By combining these complementary measures, CRS provides a holistic view of reliability that accounts for both statistical accuracy and practical deployment considerations. The composite aggregation method allows for single-number comparisons while maintaining interpretability of the underlying components.

## Foundational Learning
- **Expected Calibration Error (ECE)**: Measures alignment between predicted confidence and actual accuracy; needed to detect overconfident models that appear accurate but fail under uncertainty
- **Lipschitz regularity**: Quantifies sensitivity to input perturbations; needed to ensure model stability in real-world noisy environments
- **Uncertainty quantification**: Captures model awareness of its own limitations; needed to prevent false certainty in ambiguous situations
- **Input perturbation robustness**: Tests model stability under minor input changes; needed to simulate real-world data variability
- **Distributional shift resilience**: Measures performance degradation under domain changes; needed for deployment in dynamic environments

## Architecture Onboarding

Component map: Calibration Metrics -> Normalization -> Aggregation -> CRS Score

Critical path: Input → Model Inference → Output Processing → Reliability Metrics (Calibration, Robustness, Uncertainty) → Normalization → Aggregation → CRS Score

Design tradeoffs: The equal weighting of calibration, robustness, and uncertainty components assumes these dimensions are equally important for reliability, but this may not hold for all use cases. Alternative weighting schemes could better reflect domain-specific priorities.

Failure signatures: Models with high accuracy but low CRS scores typically exhibit overconfidence (low calibration), high sensitivity to perturbations, or poor uncertainty awareness. These models may fail catastrophically in production despite appearing strong in standard benchmarks.

First experiments to run:
1. Compare CRS rankings against human expert assessments of model reliability in real-world QA tasks
2. Test CRS stability across different dataset compositions and evaluation settings
3. Evaluate computational efficiency trade-offs between full CRS evaluation and simpler proxy metrics

## Open Questions the Paper Calls Out
None

## Limitations
- CRS scores depend on dataset composition and evaluation settings, with sensitivity not fully characterized
- The equal weighting assumption for combining reliability dimensions may mask important trade-offs between calibration, robustness, and uncertainty quantification
- Focus on QA tasks limits generalizability to other LLM applications
- Computational overhead of full CRS evaluation may restrict practical adoption

## Confidence
High: Ranking consistency of models under CRS evaluation and identification of reliability gaps missed by accuracy-only metrics
Medium: Absolute score values and their interpretability across different use cases, given the weighting assumptions
Low: Optimal aggregation method for combining the three reliability dimensions and stability of rankings under different dataset conditions

## Next Checks
1. Test CRS stability across diverse datasets and evaluation settings to characterize sensitivity
2. Compare the composite metric's rankings against expert human assessments of model reliability in real-world scenarios
3. Evaluate the computational efficiency trade-offs between full CRS evaluation and simpler proxy metrics to inform practical deployment considerations