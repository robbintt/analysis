---
ver: rpa2
title: Adopting Whisper for Confidence Estimation
arxiv_id: '2502.13446'
source_url: https://arxiv.org/abs/2502.13446
tags:
- confidence
- speech
- whisper
- estimation
- word-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses word-level confidence estimation in speech
  recognition, where traditional methods suffer from overconfidence issues. The authors
  propose C-Whisper, a novel approach that fine-tunes the Whisper ASR model itself
  to output confidence scores for each word in a transcript.
---

# Adopting Whisper for Confidence Estimation

## Quick Facts
- **arXiv ID:** 2502.13446
- **Source URL:** https://arxiv.org/abs/2502.13446
- **Reference count:** 34
- **Key outcome:** C-Whisper-large consistently outperforms CEM baseline across all datasets, achieving NCE up to 0.541 and AUC-ROC up to 0.944

## Executive Summary
This paper addresses word-level confidence estimation in speech recognition, where traditional methods suffer from overconfidence issues. The authors propose C-Whisper, a novel approach that fine-tunes the Whisper ASR model itself to output confidence scores for each word in a transcript. Specifically, they modify Whisper's decoder to produce scalar confidence values instead of next-token probabilities, using both audio input and hypothesis transcripts during fine-tuning. Experiments show that C-Whisper-tiny achieves comparable performance to the CEM baseline on in-domain data and outperforms it on eight out-of-domain datasets, while C-Whisper-large consistently surpasses CEM across all datasets by substantial margins.

## Method Summary
C-Whisper fine-tunes Whisper by replacing the decoder's final vocabulary projection layer with a randomly initialized linear layer that maps hidden states to scalar confidence values. The model uses audio input and hypothesis transcripts to predict confidence scores for each token, with word-level confidence determined by the last token in each word. The encoder remains frozen during fine-tuning, and the model retains Whisper's causal attention mask. Training uses binary cross-entropy loss with a learning rate of 5×10⁻⁶ for one epoch on Common Voice 18 English training data. Ground truth labels are generated by aligning reference and hypothesis transcripts.

## Key Results
- C-Whisper-large achieves NCE scores up to 0.541 and AUC-ROC up to 0.944 on test datasets
- C-Whisper-tiny outperforms CEM baseline on eight out-of-domain datasets while matching CEM on in-domain data
- Causal attention mask retained during fine-tuning outperforms non-causal variants early in training
- C-Whisper demonstrates superior generalization across different ASR systems without modification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained speech representations transfer to confidence estimation, improving out-of-domain generalization over models trained from scratch.
- **Mechanism:** Whisper's encoder-decoder was pre-trained on 680K hours of diverse web-sourced audio. When fine-tuned for confidence scoring, these learned acoustic-linguistic representations generalize to domains not seen during fine-tuning, whereas CEM—initialized from scratch—learns dataset-specific patterns.
- **Core assumption:** The pre-trained representations encode acoustic uncertainty signals that correlate with transcription correctness.
- **Evidence anchors:** C-Whisper-large consistently outperforms CEM across all datasets; performance attributed to extensive pre-training.

### Mechanism 2
- **Claim:** Replacing the vocabulary logits layer with a scalar sigmoid output enables direct confidence prediction while preserving learned attention patterns.
- **Mechanism:** The decoder's final linear layer normally maps hidden states to vocabulary logits. C-Whisper replaces this with a scalar output using sigmoid activation, similar to reward model training in RLHF.
- **Core assumption:** Decoder hidden states already encode sufficient information about prediction certainty; the new linear layer merely learns to extract this as a calibrated scalar.
- **Evidence anchors:** This approach has been employed to adapt large language models to function as reward models.

### Mechanism 3
- **Claim:** Causal attention masks retained during fine-tuning outperform non-causal variants despite the task allowing full-sequence visibility.
- **Mechanism:** Whisper's decoder was originally trained with causal masking. During fine-tuning, retaining this mask preserves pre-trained attention dynamics. Removing it forces relearning, causing early-training degradation.
- **Core assumption:** Pre-trained attention patterns are more valuable than task-optimal architecture when fine-tuning data is limited.
- **Evidence anchors:** Table II shows C-Whisper-large with causal: NCE 0.399 vs non-causal 0.321; AUC-ROC 0.881 vs 0.854.

## Foundational Learning

- **Concept: Word-level confidence estimation in ASR**
  - Why needed here: Understanding that confidence != probability; neural ASRs are miscalibrated (overconfident on errors), motivating auxiliary estimation modules.
  - Quick check question: Why can't we just use softmax probabilities directly as confidence scores?

- **Concept: Encoder-decoder attention in Transformers**
  - Why needed here: C-Whisper's decoder cross-attends to encoder features while self-attending over hypothesis tokens; understanding this fusion is essential for debugging.
  - Quick check question: In C-Whisper, what two inputs does the decoder receive during confidence prediction?

- **Concept: Binary calibration metrics (NCE, AUC-ROC, AUC-PR)**
  - Why needed here: The paper reports multiple metrics because no single measure captures calibration quality—AUC-PR^NEG handles error-minority-class imbalance better.
  - Quick check question: Why does the paper report both AUC-PR^POS and AUC-PR^NEG?

## Architecture Onboarding

- **Component map:** Audio Input → [Whisper Encoder (FROZEN)] → Encoder Features (e) → [Whisper Decoder (FINE-TUNED)] → Hidden States (h_i) → [NEW Linear Layer + Sigmoid] → Token Confidence c(t_i) → [Last-token aggregation] → Word Confidence

- **Critical path:** Audio + transcript → encoder features → decoder hidden states → scalar projection → sigmoid → last-token selection. The new W_c, B_c (initialized randomly) are the only parameters learning from scratch; all decoder attention/feedforward weights are fine-tuned from Whisper checkpoint.

- **Design tradeoffs:**
  - C-Whisper-tiny (39M) vs C-Whisper-large: Tiny is smaller than CEM (96M) with faster inference; Large achieves best accuracy (NCE 0.541 vs 0.388) but higher latency.
  - Encoder frozen: Reduces compute, preserves pre-trained features, but limits domain adaptation.
  - Last-token aggregation: Simpler than mean/min/product; paper reports this performed best empirically.

- **Failure signatures:**
  - Non-causal attention underperforms early in training (NCE drops ~0.08 average).
  - C-Whisper-tiny underperforms ASR-specific CEM on Company-X data (NCE 0.219 vs 0.296), suggesting model-specific CEMs may still win in matched conditions.
  - OOD performance gap closes but doesn't reverse—pre-training transfer is beneficial, not transformative for all domains.

- **First 3 experiments:**
  1. **Reproduce C-Whisper-tiny on Common Voice:** Fine-tune with encoder frozen, causal mask, BCE loss, lr=5e-6, 1 epoch. Verify NCE ~0.388 on CV test set.
  2. **Ablate attention mask:** Train identical models with causal vs. non-causal decoder attention; plot NCE by training step to confirm early-training gap and convergence behavior.
  3. **Cross-ASR generalization test:** Generate hypotheses from a non-Whisper ASR (e.g., wav2vec2.0), feed audio + transcripts to C-Whisper-large, measure NCE/AUC-ROC to validate zero-shot transfer claim.

## Open Questions the Paper Calls Out
None

## Limitations

- The paper relies on Whisper-generated hypotheses for fine-tuning data, creating a potential circular dependency where confidence scores are learned to match Whisper's own calibration rather than absolute correctness.
- All validation uses English datasets, limiting generalization claims to other languages.
- Out-of-domain performance, while better than CEM in most cases, still shows substantial variance—C-Whisper-tiny underperforms CEM on Company-X data.

## Confidence

**High Confidence:**
- The architectural modification (replacing vocabulary projection with scalar projection) works as described
- Causal attention masks outperform non-causal during early training stages
- C-Whisper-large consistently outperforms CEM across all test datasets
- The fine-tuning procedure (1 epoch, frozen encoder, causal mask) is correctly implemented

**Medium Confidence:**
- Pre-training transfer explains superior out-of-domain generalization over CEM
- Last-token aggregation performs best among tested aggregation methods
- Cross-ASR generalization works as claimed (based on Company-X results)
- Binary classification metrics (NCE, AUC-ROC) adequately capture calibration quality

**Low Confidence:**
- The claim that C-Whisper works "without modification" across different ASR systems
- The assertion that pre-training specifically improves calibration rather than general feature quality
- The alignment procedure for generating ground truth labels is optimal and correctly implemented

## Next Checks

1. **Cross-ASR Zero-Shot Test:** Generate hypotheses using a completely different ASR system (e.g., wav2vec2.0 or HuBERT), then evaluate C-Whisper-large's confidence scores on these transcripts to directly validate generalization claims.

2. **Encoder Fine-Tuning Ablation:** Run C-Whisper-large with encoder fine-tuning enabled to determine whether the frozen-encoder assumption limits performance, particularly on domain-specific datasets.

3. **Causal vs Non-Causal Long-Term Training:** Extend the attention mask ablation study beyond 1 epoch, tracking NCE/AUC-ROC at multiple time points (e.g., 5, 10, 20 epochs) to quantify whether the early-training gap persists or reverses.