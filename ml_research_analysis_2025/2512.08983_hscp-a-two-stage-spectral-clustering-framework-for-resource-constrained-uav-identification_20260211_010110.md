---
ver: rpa2
title: 'HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV
  Identification'
arxiv_id: '2512.08983'
source_url: https://arxiv.org/abs/2512.08983
tags:
- pruning
- layer
- channel
- ieee
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying deep learning models
  for UAV identification on resource-constrained edge devices. The authors propose
  HSCP, a hierarchical spectral clustering pruning framework that combines layer and
  channel pruning to achieve extreme model compression while maintaining high accuracy.
---

# HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification

## Quick Facts
- arXiv ID: 2512.08983
- Source URL: https://arxiv.org/abs/2512.08983
- Reference count: 40
- Primary result: HSCP achieves 86.39% parameter reduction and 84.44% FLOPs reduction on ResNet18 while improving accuracy by 1.49% compared to the unpruned baseline

## Executive Summary
HSCP addresses the challenge of deploying deep learning models for UAV identification on resource-constrained edge devices. The framework employs a hierarchical spectral clustering pruning approach that combines layer and channel pruning to achieve extreme model compression while maintaining high accuracy. By leveraging CKA-based similarity measures and spectral clustering, HSCP identifies and removes redundant components from pre-trained models. Experiments on the UAV-M100 dataset demonstrate superior performance compared to state-of-the-art pruning methods, achieving significant compression ratios with improved accuracy and noise robustness.

## Method Summary
HSCP implements a two-stage pruning framework using CKA-based spectral clustering. Stage 1 extracts intermediate features from each layer using a calibration batch, computes CKA similarity matrices, and applies spectral clustering to identify redundant layers. Stage 2 repeats this process at the channel level within each remaining layer. The method uses Mixup augmentation during fine-tuning to recover accuracy lost through pruning. The framework operates on RF signals converted to spectrograms, targeting extreme compression for UAV identification while maintaining performance in low SNR environments.

## Key Results
- ResNet18 achieves 86.39% parameter reduction and 84.44% FLOPs reduction with 1.49% accuracy improvement over baseline
- Layer+Channel pruning with Mixup achieves 94.11% accuracy on UAV-M100 dataset
- Robust performance in low SNR environments (-5 to 20dB) with AWGN augmentation
- Superior performance compared to state-of-the-art methods including AMC, NetAdapt, and MetaPruning

## Why This Works (Mechanism)

### Mechanism 1: CKA-Based Representational Similarity Quantification
- Claim: Removing structurally redundant components preserves task performance because similar representations encode overlapping information.
- Mechanism: HSCP extracts intermediate feature maps F_i from each layer, computes Gram matrices K = F_i F_i^T, then normalizes HSIC using Centered Kernel Alignment (CKA) to yield similarity scores in [0,1]. This creates a symmetric similarity matrix S where S_ij captures representational overlap between layer i and layer j. The same approach applies to channel-level features f_{i,p}.
- Core assumption: High CKA similarity indicates functional redundancy rather than complementary representations.
- Evidence anchors: [abstract], [Section IV-A, Eq. 6-8], [corpus] Weak corpus support—no direct CKA-based pruning references found
- Break condition: If CKA similarity correlates poorly with actual functional redundancy, clustering will remove critical paths.

### Mechanism 2: Spectral Clustering on Similarity Graphs
- Claim: Spectral clustering identifies groups of functionally similar components more robustly than threshold-based filtering.
- Mechanism: The similarity matrix S is treated as an affinity matrix. HSCP constructs the normalized graph Laplacian L_sym = I - D^{-1/2}SD^{-1/2}, performs eigen-decomposition, selects k eigenvectors corresponding to smallest nonzero eigenvalues, and applies k-means on normalized spectral embeddings. Each cluster represents a "functional block" where only the leading component is retained.
- Core assumption: The eigenstructure of the similarity graph meaningfully partitions the redundancy space.
- Evidence anchors: [Section IV-A, Eq. 9-12], [Figure 2], [corpus] "Structure-Aware Automatic Channel Pruning by Searching with Graph Embedding" provides indirect support
- Break condition: If similarity matrix S is noisy or poorly conditioned, spectral clustering may produce fragmented or meaningless clusters.

### Mechanism 3: Hierarchical Coarse-to-Fine Pruning with Mixup Recovery
- Claim: Layer pruning followed by channel pruning achieves higher compression than single-dimension approaches, while Mixup augmentation recovers accuracy by smoothing decision boundaries.
- Mechanism: Stage 1 removes redundant layers (depth reduction). Stage 2 applies the same CKA-spectral clustering to remaining channels (width reduction). During fine-tuning, Mixup generates virtual samples via linear interpolation: ũ = λu₁ + (1-λ)u₂ with λ drawn from Beta distribution, encouraging linear behavior between training examples.
- Core assumption: Pruning-induced accuracy loss can be recovered through data augmentation that regularizes the pruned model's reduced capacity.
- Evidence anchors: [abstract], [Table III], [Table IV], [corpus] "Compressing CNN models for resource-constrained systems by channel and layer pruning" supports hierarchical approach
- Break condition: If layer pruning removes architecturally critical components, subsequent channel pruning and fine-tuning cannot recover performance.

## Foundational Learning

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: Measures representational similarity between neural network layers/channels independent of scale, enabling redundancy detection.
  - Quick check question: Given two feature matrices A and B from different layers, how would you compute their CKA similarity score?

- Concept: **Spectral Clustering and Graph Laplacian**
  - Why needed here: Partitions the similarity space into clusters representing functionally redundant groups without requiring explicit importance scores.
  - Quick check question: What do the eigenvalues of the normalized graph Laplacian indicate about cluster structure?

- Concept: **Mixup Data Augmentation**
  - Why needed here: Regularizes the pruned model by generating interpolated training samples, improving generalization under noise.
  - Quick check question: How does sampling λ from a Beta(α, α) distribution affect the interpolation strength, and what happens when α→0 versus α→∞?

## Architecture Onboarding

- Component map:
  Pre-trained backbone -> Feature extractor -> CKA similarity engine -> Spectral clustering module -> Pruning executor -> Mixup fine-tuner

- Critical path:
  1. Load pre-trained model and calibration data batch
  2. Extract intermediate features → compute CKA matrices (layer-wise, then channel-wise)
  3. Apply spectral clustering → identify k layer clusters → prune to M'
  4. For each remaining layer: compute channel similarity → cluster → prune
  5. Fine-tune with Mixup (α=0.5) and AWGN augmentation

- Design tradeoffs:
  - Number of clusters (k, k_i): Fewer clusters = more aggressive pruning but higher accuracy risk; authors do not specify automatic k selection
  - Calibration batch size: Larger batches improve CKA stability but increase compute
  - Mixup α: Higher values (0.7-0.9) show marginally better results in Table IV, but α=0.5 selected as default
  - Layer vs. channel pruning order: Assumption: layer-first is optimal; paper does not compare channel-first

- Failure signatures:
  - Accuracy collapse after layer pruning: Indicates critical layers removed; check CKA similarity matrix for unusually high uniformity
  - Dimension mismatch errors: Occurs when pruning creates incompatible tensor shapes; requires Kaiming reinitialization of subsequent layers
  - No improvement from Mixup: May indicate insufficient fine-tuning epochs or α too low

- First 3 experiments:
  1. Baseline CKA matrix visualization: Extract and visualize layer similarity matrix S for your target architecture before any pruning to assess redundancy distribution and inform cluster count k.
  2. Layer-only vs. channel-only ablation: Replicate Table III on your dataset to confirm hierarchical approach provides additive benefit over single-dimension pruning.
  3. Mixup sensitivity sweep: Test α∈{0, 0.3, 0.5, 0.7, 0.9} on pruned model to identify optimal augmentation strength for your data distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the reported latency reductions transfer to actual resource-constrained hardware, such as embedded CPUs or FPGAs?
- Basis in paper: [inferred] The paper targets "resource-constrained edge devices" in the title and abstract, but all experimental latency results are measured on a high-performance NVIDIA A100 GPU platform (Section V-A).
- Why unresolved: High-end GPU benchmarks utilize massive parallelism and high memory bandwidth, which do not accurately reflect the constraints of edge processors where memory access and cache limitations often dominate inference time.
- Evidence: Deployment and timing analysis of the pruned models on specific edge hardware (e.g., Raspberry Pi, Jetson Nano, or FPGA).

### Open Question 2
- Question: How does HSCP perform on heterogeneous datasets containing different drone models or unknown devices?
- Basis in paper: [inferred] The experiments are restricted to the UAV-M100 dataset, which consists of only "seven identical DJI M100 UAVs" (Section III-A).
- Why unresolved: The current results demonstrate the ability to distinguish individual units of the same model, but it remains unverified if the pruned features are sufficient for classifying different drone types or rejecting unknown signals (open-set recognition).
- Evidence: Evaluation on a multi-manufacturer dataset with open-set protocols to test generalization capabilities.

### Open Question 3
- Question: Does the fixed sequential order (layer pruning followed by channel pruning) yield optimal compression compared to joint or iterative optimization strategies?
- Basis in paper: [inferred] The method applies a "coarse-to-fine" manner, removing layers first and channels second (Section IV), but does not provide an ablation study comparing this sequence to simultaneous or reversed pruning orders.
- Why unresolved: Aggressive layer pruning in the first stage might inadvertently remove structural context that would have been useful for determining channel importance in the second stage.
- Evidence: Ablation experiments comparing the current sequential approach against a combined layer-channel optimization strategy.

## Limitations
- Optimal number of clusters (k, ki) is not specified, requiring manual tuning that may significantly impact compression-accuracy tradeoffs
- Effectiveness on different network architectures and datasets beyond the three tested remains unproven
- Computational overhead of CKA similarity computation during pruning phase is not addressed
- Noise robustness claims based on synthetic AWGN rather than real-world RF conditions

## Confidence

**High confidence**: The hierarchical pruning approach demonstrably achieves significant compression with accuracy improvements on the UAV-M100 dataset. The Mixup augmentation consistently improves robustness across noise levels. The CKA formulation is mathematically sound.

**Medium confidence**: The claim that spectral clustering better identifies redundancy than threshold-based methods is plausible but not directly compared. The assumption that CKA similarity directly indicates functional redundancy is reasonable but not empirically validated. The generalizability to other architectures and datasets is suggested but not proven.

**Low confidence**: The paper's claim of superior noise robustness is based on synthetic AWGN augmentation rather than real-world noisy RF conditions. The computational efficiency claims don't account for the CKA computation overhead during pruning.

## Next Checks

1. **CKA Similarity Validation**: Visualize and analyze the layer similarity matrices for different architectures to verify that high CKA values actually correspond to redundant feature representations. Test whether randomly initialized layers show different similarity patterns than trained ones.

2. **Cluster Number Sensitivity**: Systematically vary k and ki values across a wide range and measure the Pareto frontier of accuracy vs. compression ratio. This would reveal whether the pruning is robust to cluster count selection.

3. **Real-World Noise Testing**: Evaluate the pruned models on actual RF signal recordings from real UAV identification scenarios with environmental noise, interference, and multipath effects rather than synthetic AWGN augmentation.