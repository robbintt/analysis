---
ver: rpa2
title: 'CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation
  Models'
arxiv_id: '2512.02180'
source_url: https://arxiv.org/abs/2512.02180
tags:
- lead
- learning
- risk
- ptb-xl
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEF, a clinically-guided contrastive learning
  method for pretraining foundation models on electrocardiogram (ECG) data. Unlike
  prior self-supervised approaches that rely on generic data augmentations, CLEF incorporates
  clinically validated risk scores derived from patient metadata to guide the contrastive
  learning process.
---

# CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models

## Quick Facts
- arXiv ID: 2512.02180
- Source URL: https://arxiv.org/abs/2512.02180
- Authors: Yuxuan Shu; Peter H. Charlton; Fahim Kawsar; Jussi Hernesniemi; Mohammad Malekzadeh
- Reference count: 40
- Primary result: Clinically-guided contrastive learning method (CLEF) outperforms strong self-supervised baselines, achieving 3.1% AUROC improvements and 2.9% MAE reductions across 18 ECG tasks.

## Executive Summary
This paper introduces CLEF, a clinically-guided contrastive learning method for pretraining foundation models on electrocardiogram (ECG) data. Unlike prior self-supervised approaches that rely on generic data augmentations, CLEF incorporates clinically validated risk scores derived from patient metadata to guide the contrastive learning process. By adaptively weighting negative pairs based on risk score dissimilarities and explicitly handling missing metadata, CLEF learns embeddings that better reflect clinically meaningful relationships between ECGs. Evaluated across 18 tasks on 7 diverse datasets, CLEF outperforms strong self-supervised baselines, achieving average AUROC improvements of 3.1% in classification and MAE reductions of 2.9% in regression. Notably, CLEF matches the performance of a supervised state-of-the-art ECG model while requiring only unlabeled ECGs and routinely collected metadata. The method also generalizes well to single-lead ECGs from wearables, advancing scalable, accurate ECG analysis for remote health monitoring.

## Method Summary
CLEF pretrains a ResNeXt1D backbone on 12-lead ECG data from MIMIC-IV using a weighted contrastive loss that incorporates clinical risk score dissimilarities. The method calculates pairwise risk score differences between ECG samples and uses these to weight the contrastive loss, pushing apart embeddings of clinically distinct patients while keeping similar-risk patients close. Missing metadata is explicitly handled through a reliability multiplier that prevents noisy risk differences from exerting undue influence. An auxiliary dissimilarity alignment loss regularizes the embedding space structure. The pretrained model is evaluated across 18 downstream tasks including arrhythmia detection, LVEF estimation, and blood pressure prediction on datasets ranging from clinical-grade 12-lead recordings to single-lead wearable data.

## Key Results
- CLEF outperforms SimCLR by 3.1% average AUROC in classification and 2.9% MAE reduction in regression across 18 tasks
- CLEF matches the performance of a supervised state-of-the-art ECG model while requiring only unlabeled ECGs and metadata
- CLEF generalizes well to single-lead ECGs from wearables, achieving strong performance on MUSIC and Icentia11K datasets
- Ablation studies confirm the importance of both the missing metadata handler and dissimilarity alignment loss components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Risk-guided negative weighting aligns embedding geometry with clinical semantics.**
- Mechanism: Standard contrastive learning treats all negative pairs uniformly, pushing them apart with equal force. CLEF calculates pairwise risk score dissimilarities and weights negative pairs accordingly, forcing the encoder to separate clinically distinct patients further in the embedding space.
- Core assumption: SCORE2 risk score provides a proxy for physiological similarity relevant to downstream tasks.
- Evidence anchors:
  - [abstract]: "adaptively weights negative pairs in contrastive learning to align embedding similarities with clinically meaningful differences"
  - [section 2.3]: "Intuitively, the weight increases when the risk scores of xi and xk differ more... pushing the embedding of xk away from that of xi"
- Break condition: If SCORE2 risk score has zero correlation with the specific downstream pathology, the guidance becomes noise, degrading performance to standard SimCLR levels.

### Mechanism 2
- Claim: **Uncertainty-aware handling of missing metadata prevents noisy gradient updates.**
- Mechanism: CLEF calculates a reliability multiplier based on missing metadata count, effectively reverting to standard contrastive behavior for pairs with uncertain risk differences.
- Core assumption: Risk scores calculated with imputed data are less reliable than those with complete data.
- Evidence anchors:
  - [section 2.2]: "Mik... indicates the level of missing metadata... preventing uncertain or noisy risk differences from exerting undue influence"
  - [table 5]: Ablation study shows performance degrades on wearable datasets when missing metadata handler is removed
- Break condition: If imputation defaults are highly inaccurate for the target population, the risk score itself may be misleading even with dampening.

### Mechanism 3
- Claim: **Dissimilarity alignment loss regularizes the embedding space structure.**
- Mechanism: The auxiliary loss explicitly forces cosine similarity of embeddings to match the inverse of clinical weight, ensuring similar risk profiles result in close embeddings.
- Core assumption: MSE loss effectively maps risk difference to cosine similarity space.
- Evidence anchors:
  - [section 2.3]: "This encourages the model to map clinically similar ECG signals to nearby embeddings"
  - [appendix D.6]: Ablation on CIFAR-100 shows L_w + L_d achieves better class separation than L_w alone
- Break condition: If batch size is too small, pairwise statistics may not represent true clinical risk distribution, making alignment loss unstable.

## Foundational Learning

### Concept: NT-Xent Loss (Contrastive Learning)
- Why needed here: This is the base objective function CLEF modifies. You must understand how the "InfoNCE" loss creates a latent space by maximizing agreement between positive pairs and minimizing agreement with negative pairs.
- Quick check question: In standard SimCLR, how are negative pairs defined implicitly by the batch size, and how does CLEF modify the contribution of these specific pairs in the denominator of the loss function?

### Concept: SCORE2 Cardiovascular Risk Score
- Why needed here: This is the "teacher" signal. It is a clinician-validated composite metric that serves as a proxy for "ground truth" health state.
- Quick check question: Why is using a risk score preferable to using raw metadata (e.g., raw age=65, BP=130) directly as a supervision target?

### Concept: Linear Probing vs. Fine-tuning
- Why needed here: The paper evaluates the "quality" of learned representations separately from the model's end-to-end performance. A good foundation model should have high linear probing accuracy.
- Quick check question: Why does CLEF-L (large) perform better at linear probing but sometimes worse at fine-tuning compared to CLEF-M (medium)?

## Architecture Onboarding

### Component map:
Backbone -> Input Pipeline -> Meta-Head -> Loss Module
ResNeXt1D -> Random lead selector → Stochastic Augmentation -> Risk Score Calculator → Dissimilarity/Weight Matrices -> Weighted Contrastive Loss + Dissimilarity Alignment Loss

### Critical path:
The model does not predict the risk score directly. Instead, the risk score modulates the loss landscape during pretraining. Ensure your implementation calculates the weight matrix before or during the loss calculation for each batch, utilizing the metadata associated with the batch indices.

### Design tradeoffs:
- **Random Lead vs. Fixed Lead Pretraining**: Default trains on random leads from 12-lead data to create a universal single-lead model. Pretraining on specific target lead yields better results but loses flexibility.
- **Model Size**: CLEF-M offers the best balance. CLEF-L requires more regularization/data to avoid overfitting on smaller downstream datasets.

### Failure signatures:
- **Collapse to Zero**: If alpha (minimum dissimilarity margin) is too high or missingness handling fails, the loss might collapse.
- **Metadata Mismatch**: If metadata mapping linking ECG patient ID to clinical data is incorrect, risk scores will be random noise.

### First 3 experiments:
1. **Baseline Reproduction**: Pretrain ResNeXt1D on MIMIC-IV using standard SimCLR. Verify downstream AUROC on PTB-XL (Lead I).
2. **Metadata Integrity Check**: Run the SCORE2 calculator on MIMIC-IV metadata subset. Plot distribution of r_s to ensure it isn't degenerate due to missing data imputation defaults.
3. **Ablation of L_d**: Pretrain CLEF-M with only L_w and compare against full L_w + L_d on MUSIC dataset to isolate contribution of alignment loss.

## Open Questions the Paper Calls Out
- Can the clinically-guided contrastive learning framework be effectively generalized to other physiological modalities, such as photoplethysmography (PPG) or respiratory signals?
- How does the availability and completeness of metadata variables impact the quality of learned representations?
- How does pretraining on a more demographically diverse cohort compare to the single-hospital MIMIC-IV dataset for population-level tasks?

## Limitations
- Performance depends heavily on quality and completeness of metadata, which is often limited in real-world clinical datasets
- Clinical risk score relevance as similarity proxy for diverse downstream tasks is assumed rather than empirically validated across all tasks
- Models trained on single hospital data may not generalize well to broader populations due to demographic bias

## Confidence

**Confidence Assessment:**
- **High confidence** in ablation studies showing individual components contribute positively
- **Medium confidence** in clinical relevance claim, as risk score guidance aligns with known cardiovascular physiology but cross-task validation is limited
- **Medium confidence** in generalization to wearables, as results are positive but based on smaller sample sizes

## Next Checks
1. Test CLEF's sensitivity to metadata quality by systematically degrading metadata completeness in pretraining set and measuring downstream performance decay
2. Evaluate whether alternative clinical risk scores (e.g., CHA2DS2-VASc for atrial fibrillation prediction) provide better guidance than SCORE2 for specific downstream tasks
3. Validate the model's behavior when clinical risk score and target pathology are uncorrelated to establish boundary conditions of guidance mechanism