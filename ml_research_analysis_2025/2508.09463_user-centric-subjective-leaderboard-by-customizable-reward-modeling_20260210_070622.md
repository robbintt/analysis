---
ver: rpa2
title: User-centric Subjective Leaderboard by Customizable Reward Modeling
arxiv_id: '2508.09463'
source_url: https://arxiv.org/abs/2508.09463
tags:
- criteria
- preference
- reward
- subjective
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a user-centric subjective leaderboard (USL)
  for large language models (LLMs) that enables dynamic, personalized rankings based
  on individual preferences and needs. The key innovation is a customizable reward
  model (CRM) that explicitly conditions on user-specified preference criteria, achieving
  97.27% accuracy on preference recognition tasks compared to GPT-4.1's 91.96%.
---

# User-centric Subjective Leaderboard by Customizable Reward Modeling

## Quick Facts
- arXiv ID: 2508.09463
- Source URL: https://arxiv.org/abs/2508.09463
- Reference count: 5
- Primary result: Customizable reward model achieves 97.27% accuracy on preference recognition tasks vs GPT-4.1's 91.96%

## Executive Summary
This paper introduces a user-centric subjective leaderboard (USL) for large language models that enables dynamic, personalized rankings based on individual preferences and needs. The key innovation is a customizable reward model (CRM) that explicitly conditions on user-specified preference criteria, achieving 97.27% accuracy on preference recognition tasks compared to GPT-4.1's 91.96%. The CRM demonstrates superior generalization capabilities and robustness across new topics and criteria, even with only 4B parameters. The USL successfully captures diverse user preferences, showing strong negative correlations (e.g., Kendall's τ = -0.83) when models are evaluated against contradictory criteria like preferring detailed versus concise responses.

## Method Summary
The approach involves fine-tuning Qwen3 (0.6B–8B) with binary cross-entropy loss for pairwise classification, conditioning on explicit user preference criteria. The training pipeline uses LMArena data (10,794 samples) with criteria extracted via GPT-4o, transformed into criterion-conditioned format (c, q, o_chosen, o_rejected, y_c). Training includes noising strategies (removal/addition/replacement) applied to reversed-preference subsets for robustness. The USL aggregates pairwise win-rates against a baseline to produce personalized rankings. The system employs topic clustering using BERTopic pipeline (Qwen-3 embeddings → UMAP-5d → HDBSCAN) to create test sets for topic and criterion generalization.

## Key Results
- CRM achieves 97.27% accuracy on preference recognition tasks vs GPT-4.1's 91.96%
- USL shows strong negative correlations (Kendall's τ = -0.83) between rankings under contradictory criteria
- CRM-4B demonstrates superior generalization across new topics and criteria with only 4B parameters
- System maintains robust performance across diverse evaluation scenarios with synthetic criteria noise

## Why This Works (Mechanism)

### Mechanism 1: Criteria-Conditioned Preference Disambiguation
Standard reward models fail on subjective tasks because they average over contradictory user preferences; explicitly conditioning on a specific criterion resolves this ambiguity. The model takes inputs as tuples (c, q, o_A, o_B) rather than just (q, o), forcing prediction based on the provided attribute (e.g., "concise" vs. "detailed") rather than learning a monolithic "quality" score.

### Mechanism 2: Pairwise Classification over Scalar Regression
Formulating the reward task as a binary classification problem (comparing two responses simultaneously) outperforms traditional scalar reward modeling for subjective nuances. Instead of scoring responses independently, the CRM uses a classification loss on the joint probability P(o_A ≻ o_B | c, q), allowing token-level comparison of attributes relative to the criteria.

### Mechanism 3: Robustness via Synthetic Criteria Noise
Adding noise (removal, addition, replacement) to preference criteria during training prevents the model from overfitting to exact phrasing and improves generalization to imperfect user inputs. During training, criteria are randomly dropped, swapped with conflicting criteria, or padded with irrelevant ones, regularizing the model to focus on semantic intent rather than keyword matching.

## Foundational Learning

- **Concept: Bradley-Terry Model & Pairwise Ranking**
  - Why needed here: Understanding standard RM training (scalar rewards derived from pairwise preferences) is necessary to understand why the authors pivot to a classification objective for CRMs.
  - Quick check question: How does the Bradley-Terry assumption of a single "win probability" break down when user preferences are contradictory?

- **Concept: Instruction Tuning / Conditioned Generation**
  - Why needed here: The CRM is essentially an instruction-tuned model where the "instruction" is the evaluation criterion. You must understand how LLMs process conditional constraints.
  - Quick check question: If you prompt a model with "Be concise" but the training data was mostly "Be detailed," how does explicit conditioning override the prior?

- **Concept: Rank Correlation (Kendall's τ)**
  - Why needed here: The paper validates the USL by showing strong negative correlations (τ ≈ -0.83) between rankings generated by contradictory criteria.
  - Quick check question: Why is Kendall's τ preferred over Spearman's ρ when evaluating the similarity of two orderings in a leaderboard context?

## Architecture Onboarding

- **Component map:** Input Layer (Concatenation of Query q, Criteria c, Response A o_A, Response B o_B) -> Backone (Qwen3 0.6B-8B parameters) -> Head (Binary classification head with Cross-Entropy loss) -> USL Engine (Aggregates pairwise win-rates against baseline)

- **Critical path:** The Criteria Extraction Pipeline (Section 3). The system relies on GPT-4o to mine criteria c from raw preference data. If this extraction is noisy or misses key nuances, the CRM training data is compromised.

- **Design tradeoffs:** Classification (L_cls) is chosen over standard Reward Modeling (L_ranking) for better comparative nuance, sacrificing the ability to score a single response in isolation. Model Size: The authors settle on 4B as a sweet spot. Smaller models (0.6B) fail to generalize; larger models offer diminishing returns on subjective tasks.

- **Failure signatures:** "Tie" Collapse occurs if criteria c are generic, the model may revert to aggregated crowd preferences (length bias), masking the user-centric signal. Context Overflow happens when very long responses o_A, o_B combined with detailed criteria c may exceed context limits or dilute attention.

- **First 3 experiments:**
  1. Sanity Check (Bidirectional Consistency): Take a pair (o_long, o_short) and verify the CRM flips its prediction when c changes from "Prefer detailed" to "Prefer concise."
  2. Noise Ablation: Train two CRM-4B models—one with the noising strategies, one without—and evaluate on the D^- test set to quantify the robustness gain.
  3. Leaderboard Sensitivity: Generate two USL rankings using contradictory criteria (e.g., "Formal" vs. "Casual") and calculate Kendall's τ. You should replicate the strong negative correlation reported in Section 6.5.

## Open Questions the Paper Calls Out

- Can the CRM framework effectively generalize to objective tasks (e.g., mathematics and coding) where correctness is binary, or does it require modifications to handle verifiable truth? The authors explicitly state in the conclusion that valuable future directions include extending coverage to objective tasks.

- Does the inclusion of negative preference criteria (user dislikes) significantly improve the ranking accuracy or robustness of the USL? Appendix D notes the current work focuses exclusively on positive descriptions of preference criteria, with future work planned to incorporate negative criteria.

- Can CRMs be successfully integrated into the RLHF alignment process to create models that cater to specific user needs without losing general capability? The conclusion suggests incorporating CRMs for LLM alignment presents a promising research avenue, though current work uses CRMs solely as evaluators.

## Limitations

- The system's effectiveness depends heavily on accurate criteria extraction from human preference data, making it sensitive to noise in the GPT-4o extraction pipeline.
- The CRM requires users to articulate their preferences explicitly, which may not always be feasible in practical applications where preferences are implicit or difficult to verbalize.
- The robustness claims under synthetic noise are based on controlled experiments with specific noise profiles that may not capture all real-world input variations.

## Confidence

- **High Confidence:** The CRM's superior performance on preference recognition tasks (97.27% accuracy vs GPT-4.1's 91.96%) is well-supported by evaluation methodology and controlled test conditions.
- **Medium Confidence:** Generalization claims across new topics and criteria depend heavily on the quality of the topic clustering and criteria categorization pipeline, which involves human-in-the-loop supervision.
- **Low Confidence:** Robustness claims under synthetic noise are based on specific noise profiles that may not represent all real-world user input patterns.

## Next Checks

1. **Criteria Extraction Fidelity:** Replicate the GPT-4o criteria extraction on a held-out sample of LMArena data and measure agreement with human annotations to quantify extraction noise and its impact on CRM training quality.

2. **Real-World Preference Articulation:** Deploy the CRM in a user study where participants specify preferences using natural language (not extracted criteria) and measure whether the USL rankings meaningfully differ from standard aggregated leaderboards for the same user population.

3. **Cross-Domain Generalization:** Evaluate the CRM-4B on subjective tasks from entirely different domains (e.g., medical advice, creative writing, code review) not represented in the LMArena training data to test whether the preference recognition capability truly generalizes beyond the training distribution.