---
ver: rpa2
title: Cancer Type, Stage and Prognosis Assessment from Pathology Reports using LLMs
arxiv_id: '2503.01194'
source_url: https://arxiv.org/abs/2503.01194
tags:
- stage
- label
- 'true'
- carcinoma
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates large language models (LLMs) for extracting\
  \ cancer type, AJCC stage, and prognosis from pathology reports. We benchmark six\
  \ models\u2014including GPT-4o, GPT-4o-mini, Mistral, and Llama variants\u2014using\
  \ zero-shot inference and instruction tuning."
---

# Cancer Type, Stage and Prognosis Assessment from Pathology Reports using LLMs

## Quick Facts
- arXiv ID: 2503.01194
- Source URL: https://arxiv.org/abs/2503.01194
- Reference count: 40
- Primary result: Instruction-tuned Path-llama3.1-8B achieves >96% cancer type accuracy and matches larger models for staging and prognosis tasks from pathology reports.

## Executive Summary
This study evaluates large language models for extracting cancer type, AJCC stage, and prognosis from pathology reports using zero-shot inference and instruction tuning. Six models including GPT-4o variants, Mistral, and Llama models are benchmarked on a dataset of 9,523 TCGA pathology reports merged with clinical survival data. Cancer type identification achieves >96% accuracy for most models, while staging and prognosis tasks benefit significantly from instruction tuning and chain-of-thought reasoning. The instruction-tuned Path-llama3.1-8B model, fine-tuned with LoRA, matches or exceeds larger models while offering efficient deployment for resource-constrained settings.

## Method Summary
The study uses TCGA pathology reports (9,523 samples, 32 cancer types) merged with clinical survival data via TCGA barcodes, split 80:10:10 stratified by cancer type. Six models are evaluated: GPT-4o, GPT-4o-mini, Mistral-Medium, Mistral-Large, Llama3-8B, and Llama3-70B. Synthetic QA pairs (17,344) are generated by GPT-4o-mini for instruction tuning. Path-llama3.1-8B is fine-tuned with LoRA (rank=16) targeting attention projection modules. Zero-shot inference uses JSON-constrained outputs, with chain-of-thought reasoning for staging and prognosis tasks on non-tuned models.

## Key Results
- Cancer type identification achieves >96% accuracy across all models, with instruction-tuned variants improving format adherence
- AJCC stage determination shows significant performance gains from instruction tuning and self-generated chain-of-thought reasoning
- Prognosis assessment improves from ~50% to ~70% F1-score with instruction tuning and contextual survival anchors
- Path-llama3.1-8B matches or exceeds larger models while using only 8B parameters, demonstrating efficient deployment potential

## Why This Works (Mechanism)

### Mechanism 1
Task complexity determines which prompting strategies are necessary for acceptable performance. Information extraction tasks (cancer type) achieve high zero-shot accuracy because LLMs leverage pretraining on diverse text. Reasoning tasks (AJCC staging, prognosis) require explicit cognitive scaffolding through chain-of-thought (CoT) or instruction tuning to decompose multi-step inferences. Tasks requiring integration of information not explicitly stated in the report (e.g., missing metastasis data) cause performance degradation regardless of prompting strategy.

### Mechanism 2
Instruction tuning aligns model representations to task-specific output formats and reasoning patterns more efficiently than in-context learning. Synthetic Q&A pairs expose the model to diverse query formulations while enforcing structured JSON outputs. LoRA fine-tuning of attention components (Query, Key, Value projections) adapts information routing without modifying full weights. Domain shift between training and deployment sites (different report formats, terminology) reduces transfer; hospital-specific adaptation required.

### Mechanism 3
Prognosis prediction requires explicit contextual anchors because LLMs lack statistical calibration for survival estimation. Providing mean disease-specific survival time gives the model a reference threshold. Few-shot examples (4 positive, 4 negative) demonstrate the classification boundary. CoT traces help models weigh positive and negative prognostic factors. Survival prediction from pathology reports alone is fundamentally incomplete without treatment history, lab results, and comorbidities—current performance ceiling appears ~70% F1.

## Foundational Learning

- **Chain-of-thought (CoT) reasoning**: Required for staging and prognosis where the model must synthesize multiple report features (tumor size, lymph nodes, margins) into a classification. Quick check: Can you explain why CoT helps distinguish Stage II from Stage III but may not help distinguish cancer type?

- **LoRA (Low-Rank Adaptation)**: Enables fine-tuning 8B parameter models on consumer GPUs by updating low-rank decompositions instead of full weight matrices. Quick check: Which attention components did the authors target, and why might Query/Key/Value projections be more important than output layers for this task?

- **Instruction tuning vs. in-context learning**: Understanding when to use few-shot prompting (inference-time, higher token cost) vs. instruction tuning (training-time, lower inference cost). Quick check: Why did instruction-tuned models outperform CoT despite not using explicit reasoning traces at inference?

## Architecture Onboarding

- **Component map**: TCGA pathology reports → merged with clinical survival data → stratified 80/10/10 split → synthetic Q&A generation → LoRA fine-tuning → zero-shot/instruction-tuned inference with JSON outputs

- **Critical path**: 1. Dataset curation (report + clinical data merge via TCGA barcodes) 2. Synthetic Q&A generation for instruction tuning 3. LoRA fine-tuning targeting q_proj, k_proj, v_proj, o_proj modules 4. Zero-shot inference with JSON enforcement

- **Design tradeoffs**: LoRA rank 16 balances capacity and overfitting risk; 4-bit quantization reduces VRAM but may affect subtle reasoning; JSON-only outputs improve parsing but lose model explanations

- **Failure signatures**: Stages I and IV overprediction in zero-shot; anatomically similar cancer confusion (colon vs. rectum); format violations from smaller models (Llama3-8B) solved by instruction tuning

- **First 3 experiments**: 1. Replicate cancer type identification on 100 institutional reports to validate zero-shot baseline 2. Ablate LoRA components: train with only q_proj vs. full attention stack 3. Test domain shift: evaluate Path-llama3.1-8B on reports from different hospital system

## Open Questions the Paper Calls Out

- **Integrating multimodal EMR data**: To what extent does integrating additional data sources like surgical records and laboratory results improve prognosis assessment accuracy compared to using pathology reports alone? The authors state that integrating additional data sources could "significantly enhance" performance on prognosis tasks.

- **Human expert reasoning traces**: Can instruction tuning using chain-of-thought reasoning traces extracted directly from human pathologists yield higher performance than tuning on synthetically generated question-answer pairs? The authors propose that "extracting chain-of-thought reasoning traces from pathologists and using this data for large-scale instruction tuning" could enhance the model's ability to mimic expert reasoning.

- **Retrieval-augmented generation**: Does implementing a retrieval-augmented generation framework improve staging accuracy by effectively grounding models in hospital-specific guidelines? The authors suggest that incorporating RAG could allow models to access hospital-specific guidelines, potentially improving staging and prognosis decisions.

## Limitations

- **Domain generalization**: Performance on TCGA pathology reports may not transfer to institutional or international reporting standards due to varying report formats and terminology

- **Prognosis prediction ceiling**: F1 scores for prognosis assessment plateau around 70%, suggesting fundamental limitations in deriving survival predictions from pathology reports alone without additional clinical context

- **Synthetic data quality**: Instruction tuning depends on GPT-4o-mini-generated QA pairs that may not capture all clinically relevant edge cases or reporting variations across institutions

## Confidence

- **High confidence**: Cancer type identification results (>96% accuracy) across multiple models, including instruction-tuned variants
- **Medium confidence**: AJCC stage determination improvements from instruction tuning and chain-of-thought reasoning
- **Low confidence**: Prognosis assessment performance claims due to complex clinical nature and limited validation

## Next Checks

1. **Cross-institutional evaluation**: Test Path-llama3.1-8B on pathology reports from 2-3 different hospital systems with varying report formats and terminology to quantify domain shift impact on performance

2. **Clinical validation study**: Compare LLM-derived cancer type, stage, and prognosis predictions against expert pathologist assessments on a held-out dataset of 100+ cases to measure real-world accuracy and identify systematic errors

3. **Ablation study on instruction tuning data**: Generate synthetic QA pairs using both GPT-4o-mini and a less capable model (e.g., GPT-3.5), then compare instruction-tuned model performance to quantify the impact of synthetic data quality on final task performance