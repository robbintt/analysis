---
ver: rpa2
title: 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource
  Language MLLMs'
arxiv_id: '2508.05502'
source_url: https://arxiv.org/abs/2508.05502
tags:
- language
- cultural
- languages
- image
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces MELLA, a dataset designed to enhance both
  linguistic capability and cultural groundedness in multimodal large language models
  (MLLMs) for low-resource languages. MELLA combines native web alt-text (for cultural
  knowledge) with MLLM-generated captions (for linguistic fluency), targeting eight
  low-resource languages.
---

# MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs

## Quick Facts
- arXiv ID: 2508.05502
- Source URL: https://arxiv.org/abs/2508.05502
- Reference count: 24
- Primary result: Dual-source dataset combining native web alt-text and MLLM-generated captions improves both cultural understanding and linguistic fluency in low-resource MLLMs

## Executive Summary
MELLA addresses the challenge of improving multimodal large language models (MLLMs) for low-resource languages by combining two complementary data sources: native web alt-text for cultural knowledge and MLLM-generated captions (translated to target languages) for linguistic fluency. The dataset targets eight low-resource languages and demonstrates that fine-tuning MLLMs on this dual-source approach significantly improves both cultural understanding (measured by keyword accuracy) and linguistic quality (measured by BLEU, ROUGE-L, and METEOR scores). The approach outperforms baselines like SDRRL, showing that separating cultural and linguistic objectives into distinct data sources yields better results than unified data.

## Method Summary
The MELLA dataset combines 6.8M image-text pairs across eight low-resource languages, sourced from native web alt-text (D_know) and MLLM-generated English captions translated to target languages (D_ling). The method employs single-stage joint training on both data sources simultaneously, avoiding the catastrophic forgetting observed in two-stage approaches. Training uses SFT with LoRA on InternVL2-8B or Qwen2-VL-7B-Instruct models with specific hyperparameters (LR=4e-5, batch_size=2-4, epochs=1). The approach leverages high-quality machine translation (DeepL/Google Translate) with human review and COMETKIWI scoring (0.75 threshold) to ensure linguistic quality in target languages.

## Key Results
- Fine-tuning on MELLA improves keyword accuracy for cultural entities across all eight target languages
- METEOR scores increase significantly compared to baselines, with some languages showing two orders of magnitude improvement
- D_know-only training achieves high keyword accuracy (7.00) but low METEOR (2.81), while D_ling-only achieves high METEOR (37.90) but low keyword accuracy (3.20), demonstrating the complementary nature of the dual-source approach
- Two-stage training (D_ling → D_know) underperforms joint training due to catastrophic forgetting, confirming the importance of unified training

## Why This Works (Mechanism)

### Mechanism 1: Dual-Source Data Decomposition
Separating cultural knowledge (connotation) from linguistic fluency (denotation) into distinct data sources improves both capabilities more effectively than unified data. Native web alt-text provides culturally-grounded entity knowledge that MLLMs lack, while MLLM-generated captions provide fluent descriptions after translation. Training on the combined dataset forces the model to jointly optimize both objectives. The approach fails if alt-text quality is poor or translation systems cannot handle specific target languages.

### Mechanism 2: Single-Stage Joint Training Avoids Catastrophic Forgetting
Training on combined D_ling and D_know simultaneously outperforms sequential two-stage training because multi-stage LoRA training creates representation conflicts. The unified training objective optimizes both cultural and linguistic capabilities within a shared parameter space, whereas sequential training causes the model to "forget" earlier learned capabilities. The approach may fail if D_ling and D_know have severe distribution mismatch or conflicting optimization gradients.

### Mechanism 3: Translation-Based Linguistic Transfer with Quality Filtering
High-quality machine translation can transfer linguistic fluency from English MLLM-generated captions to low-resource languages. A strong MLLM generates detailed English descriptions, which are translated to target languages and verified via WMT22-cometkiwi-da (0.75 threshold) plus human expert review. This creates linguistically fluent training signals without requiring native low-resource caption writers. The approach fails for languages with poor MT support or significant morphological complexity.

## Foundational Learning

- **Concept: Thin vs. Thick Description (Geertz/Barthes)**
  - Why needed here: The paper frames its dual objective using semiotic theory: thin descriptions capture literal denotation while thick descriptions capture cultural connotation. Understanding this distinction explains why translation-only approaches fail.
  - Quick check question: Can you explain why an MLLM that perfectly describes "a man in traditional dress" still fails a user who wants to know "is this Prince Abdullah bin Bandar?"

- **Concept: Cross-Lingual Transfer via Translation**
  - Why needed here: MELLA relies on translating English MLLM outputs to low-resource languages. Understanding the trade-offs of translation-based transfer helps diagnose when it works vs. when native data is essential.
  - Quick check question: If BLEU scores improve but keyword accuracy stays low, which data source (D_ling or D_know) should you expand?

- **Concept: Catastrophic Forgetting in Multi-Stage Fine-Tuning**
  - Why needed here: The paper explicitly compares single-stage vs. two-stage training and finds two-stage fails. This concept explains why LoRA block merging doesn't preserve prior knowledge.
  - Quick check question: Why does training on D_ling first, then D_know separately, underperform joint training despite using the same total data?

## Architecture Onboarding

- **Component map:** Image Collection → Filtering → D_know (alt-text) and D_ling (MLLM caption → translate) → Combined D (6.8M pairs) → SFT on MLLM backbone → Evaluation (D_know keyword acc / D_ling BLEU/ROUGE-L/METEOR)

- **Critical path:**
  1. Alt-text extraction quality—if alt-text is missing or generic, D_know provides no cultural signal
  2. Translation quality check—without ≥0.75 COMETKIWI or human review, D_ling may introduce noise
  3. Joint training convergence—monitor both keyword accuracy and METEOR during training; either stalling indicates data imbalance

- **Design tradeoffs:**
  - D_know avg. length ~14 tokens vs. D_ling avg. length ~258 tokens → D_know is knowledge-dense but linguistically sparse; D_ling is verbose but culturally shallow
  - Single-stage training requires balancing dataset sizes (80-140K per language) to avoid one objective dominating
  - Reliance on commercial MT APIs (DeepL/Google) introduces external dependencies and potential language coverage gaps

- **Failure signatures:**
  - Cross-lingual contamination: Model outputs in wrong language
  - Repetitive outputs: Model generates looping text
  - Hallucination: Alt-text may contain errors
  - Keyword accuracy near zero: Indicates D_know missing or alt-text not culturally grounded

- **First 3 experiments:**
  1. Baseline probe: Run InternVL2-8B on 200-sample test split to confirm low performance
  2. D_ling-only ablation: Train on D_ling alone; expect high METEOR but near-zero keyword accuracy
  3. Joint training sanity check: Train on combined D for 1 epoch; verify both metrics improve

## Open Questions the Paper Calls Out

- **Can the "forgetting phenomenon" observed in multi-stage training be mitigated using specific regularization or architectural adjustments?**
  - Basis: The ablation study notes that the "ling → know Two Stage" approach failed due to catastrophic forgetting
  - Why unresolved: The authors identify the failure but do not test alternative multi-stage strategies like replay buffers or elastic weight consolidation
  - What evidence would resolve it: A comparative study where multi-stage training utilizes experience replay or parameter freezing techniques

- **Does training on native web alt-text increase the rate of factual hallucination in MLLMs?**
  - Basis: The appendix acknowledges observed hallucinations as an inherent limitation of alt-text data
  - Why unresolved: The paper demonstrates improved cultural keyword accuracy but does not quantify the trade-off between injecting noisy alt-text and potential increase in model untruthfulness
  - What evidence would resolve it: Evaluation on a hallucination benchmark comparing baseline vs. MELLA-finetuned models

- **How does the dual-source framework perform on extremely low-resource languages where native web alt-text is scarce?**
  - Basis: The study targets eight specific languages, but the reliance on "native web alt-text" assumes baseline digital corpus availability
  - Why unresolved: The methodology depends on high-traffic regional websites to source D_know, unclear if viable for languages with minimal digital footprint
  - What evidence would resolve it: Application to an extremely low-resource language (e.g., Quechua or Yoruba) to observe effectiveness

## Limitations
- Dependence on high-quality alt-text data, which is not uniformly available across all target languages and domains
- Reliance on commercial translation APIs (DeepL/Google Translate) introduces external dependencies and potential quality variations
- Evaluation metrics (keyword accuracy, BLEU/ROUGE-L/METEOR) may not fully capture deeper cultural understanding or practical utility

## Confidence
- **High Confidence**: Improvement in METEOR and keyword accuracy scores when fine-tuning on MELLA compared to baselines
- **Medium Confidence**: Claim that the dual-source framework is universally effective across all eight target languages (results vary significantly by language)
- **Low Confidence**: Assertion that MELLA solves the fundamental challenge of cultural grounding in low-resource languages (evaluation metrics may not capture deeper understanding)

## Next Checks
1. **Cross-Lingual Transfer Validation**: Test whether models fine-tuned on MELLA for one low-resource language (e.g., AR) can transfer knowledge to related languages (e.g., Persian/Farsi) without additional fine-tuning

2. **Human Evaluation of Cultural Understanding**: Conduct expert human evaluation where native speakers assess whether model outputs demonstrate genuine cultural understanding beyond keyword matching

3. **Long-Tail Entity Testing**: Create a test set focusing on rare or domain-specific cultural entities (e.g., regional dialects, local customs, historical figures) to determine whether MELLA's improvements extend to specialized cultural knowledge