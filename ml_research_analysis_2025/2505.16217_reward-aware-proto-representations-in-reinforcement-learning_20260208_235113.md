---
ver: rpa2
title: Reward-Aware Proto-Representations in Reinforcement Learning
arxiv_id: '2505.16217'
source_url: https://arxiv.org/abs/2505.16217
tags:
- learning
- state
- reward
- states
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the default representation (DR), a reward-aware
  proto-representation that extends the successor representation (SR) by incorporating
  reward dynamics. The DR is defined within the linearly solvable MDP framework and
  can be learned via dynamic programming and temporal-difference methods.
---

# Reward-Aware Proto-Representations in Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.16217
- Source URL: https://arxiv.org/abs/2505.16217
- Reference count: 40
- This paper proposes the default representation (DR), a reward-aware proto-representation that extends the successor representation (SR) by incorporating reward dynamics.

## Executive Summary
This paper introduces the Default Representation (DR), a reward-aware proto-representation that extends the Successor Representation (SR) by incorporating reward dynamics within the linearly solvable MDP framework. The DR can be learned via dynamic programming and temporal-difference methods, and is shown to outperform SR in reward shaping, option discovery, count-based exploration, and transfer learning. Theoretically, the authors characterize the vector space basis of the DR, establish conditions under which DR and SR share eigenvectors, and extend DR to function approximation via default features.

## Method Summary
The DR is computed as Z(s,s') = Σ P_πd(τ)exp(r(τ)/λ), weighting trajectories by both probability and cumulative reward. It can be learned via dynamic programming using Z_{k+1} = R^{-1} + R^{-1} P_πd Z_k or via TD updates Z(s,j) ← Z(s,j) + α[exp(r/λ)(1{s=j} + Z(s',j)) - Z(s,j)]. For reward shaping, the top eigenvector of the symmetrized DR matrix is used as a potential function. Default features ζ(s) = Z_{NN}P^{πd}_{NT}Φ enable transfer learning by decomposing value computation so that only terminal reward weights need to change across tasks.

## Key Results
- DR-based potential shaping achieves higher returns than SR-based methods in environments with low-reward regions
- RACE, an eigenoption discovery method using DR, exhibits reward-aware exploration and achieves higher rewards than SR-based CEO
- Using DR norm as a density model leads to orders-of-magnitude better exploration than SR or random exploration
- Default features enable direct computation of optimal policies under changing terminal rewards, outperforming successor features in this specific setting

## Why This Works (Mechanism)

### Mechanism 1
The default representation (DR) captures reward-weighted trajectory statistics, enabling reward-aware state similarity that the reward-agnostic successor representation (SR) cannot achieve. DR computes Z(s,s') = Σ P_πd(τ)exp(r(τ)/λ), weighting each trajectory by both probability and cumulative reward. When rewards vary across states, DR eigenvectors reflect low-reward region locations. The core assumption is that the reward function has meaningful spatial variation; when rewards are constant, DR and SR share eigenvectors and provide no differential benefit.

### Mechanism 2
Potential-based reward shaping using DR's top eigenvector guides agents along reward-optimal paths, avoiding low-reward regions that SR-based shaping traverses. The top DR eigenvector e encodes expected cumulative reward access from each state. Potential-based shaping ȓ_t = γe(s_{t+1}) - e(s_t) creates a gradient that attracts toward high-reward regions while repelling from low-reward areas. The core assumption is that the optimal path differs from the shortest path due to negative reward regions.

### Mechanism 3
Default features enable zero-shot transfer to new terminal reward configurations without re-learning environment dynamics. Default features ζ(s) = Z_{NN}P^{πd}_{NT}Φ decompose value computation so that only terminal reward weights w change across tasks. After learning ζ(s) once under the default policy, exp(v*_N/λ) = ζ(s)^T w computes optimal values for any terminal reward configuration via matrix multiplication. The core assumption is that only terminal rewards change across tasks; non-terminal rewards remain fixed.

## Foundational Learning

- Concept: Successor Representation (SR)
  - Why needed here: DR is explicitly defined as a reward-aware extension of SR; understanding Ψ^π(s,s') = E[Σ γ^t 1{S_t=s'}|S_0=s] is prerequisite to grasping how DR modifies this with reward weighting.
  - Quick check question: Can you explain why SR is called "reward-agnostic" and what computational advantage this provides for transfer learning?

- Concept: Linearly Solvable MDPs
  - Why needed here: DR is defined within this framework where optimal values are computed via linear equations rather than Bellman backups; the deviation cost λKL(p_π||p_{πd}) fundamentally shapes the representation.
  - Quick check question: What constraint does the default policy π_d impose, and how does the KL divergence penalty affect the optimal policy?

- Concept: Temporal-Difference Learning
  - Why needed here: The paper derives TD updates for DR (Eq. 13): Z(s,j) ← Z(s,j) + α[Y - Z(s,j)] where Y = exp(r/λ)(1{s=j} + Z(s',j)). This is the practical learning algorithm.
  - Quick check question: How does the bootstrapping target Y for DR differ from standard TD targets, and what role does exp(r/λ) play?

## Architecture Onboarding

- Component map:
  DR Matrix Z -> Eigendecomposition Module -> Potential-based shaping / Option discovery
  Default Features ζ(s) -> Transfer learning computation

- Critical path:
  1. Initialize Z = I (identity matrix)
  2. Collect transitions under π_d (uniform random)
  3. Update Z via TD: Z(s,j) ← Z(s,j) + α[exp(r/λ)(1{s=j} + Z(s',j)) - Z(s,j)]
  4. Perform at least one backward sweep through collected data
  5. Compute eigendecomposition of Sym(Z_VV) for visited states
  6. Use log(top eigenvector) for numerical stability

- Design tradeoffs:
  - λ selection: Small λ → strong reward sensitivity but numerical instability; paper used λ=1.3 after tuning from 1.0
  - SR vs DR: Use SR when rewards are constant (equivalent performance, better numerical stability); use DR when reward-awareness matters
  - DFs vs SFs: DFs compute optimal policies for terminal reward changes but require fixed non-terminal rewards; SFs are more flexible but compute only policies as good as training policies

- Failure signatures:
  - Numerical underflow: DR entries become extremely small when r/λ is large negative; use python-flint library for arbitrary precision
  - Negative eigenvector entries: Indicates insufficient backward sweeps or violated Assumption C.1
  - Transfer degradation: Non-terminal rewards changed but DFs not recomputed
  - Exploration-avoidance conflict: RACE may under-explore states blocked by low-reward regions

- First 3 experiments:
  1. Implement TD learning for DR in 4×4 grid with constant -1 reward; verify eigenvectors match SR
  2. Add -20 reward tiles between start and goal; compare DR-pot vs SR-pot shaping using Q-learning
  3. Learn DFs in Four Rooms with 4 terminal states; randomly sample new terminal rewards and compare DF-computed policy vs Q-learning from scratch

## Open Questions the Paper Calls Out

### Open Question 1
Can the DR eigenvectors and the agent's policy be learned simultaneously to improve reward shaping, rather than requiring pre-computation? The current experimental setup assumes access to pre-computed DR eigenvectors prior to training.

### Open Question 2
How can neural network methods developed for the Successor Representation be adapted to scale the Default Representation to complex, high-dimensional environments? This work focused on theoretical foundations and empirical validation in tabular settings for clarity.

### Open Question 3
Can the numerical instability caused by exponentiating negative rewards be mitigated to allow DR application in long-horizon environments? The authors currently rely on arbitrary-precision libraries (python-flint) and specific hyperparameter tuning (λ) to manage instabilities.

## Limitations
- DR performance advantage depends heavily on environments with meaningful reward variation; when rewards are uniform, DR and SR provide equivalent representations
- Numerical stability remains a concern, requiring specialized libraries (python-flint) to handle extreme exponentiations when rewards are large negative
- Default features transfer capability is restricted to settings where only terminal rewards change, limiting practical applicability

## Confidence
**High Confidence**: Claims about DR outperforming SR in low-reward avoidance and transfer learning are well-supported by controlled experiments with clear baselines.

**Medium Confidence**: The RACE option discovery results show benefit but may trade off exploration completeness for reward-awareness.

**Low Confidence**: The assertion that DR eigenvectors "accurately reflect the positions of low-reward regions" is primarily supported by visual inspection rather than quantitative metrics.

## Next Checks
1. **Numerical Stability Test**: Implement DR learning with varying λ values (0.5, 1.0, 1.3, 2.0) on environments with increasingly negative rewards to characterize the stability-accuracy tradeoff and verify the need for high-precision arithmetic.

2. **Reward Uniformity Baseline**: Create a variant of the Four Rooms environment with uniform step rewards (all -1) and verify that DR and SR produce equivalent performance across all four experimental settings.

3. **Transfer Limitation Boundary**: Design a transfer experiment where non-terminal rewards change between tasks and measure DR transfer performance degradation, establishing the exact boundary conditions for default features applicability.