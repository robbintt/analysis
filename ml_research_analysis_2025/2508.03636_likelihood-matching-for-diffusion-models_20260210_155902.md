---
ver: rpa2
title: Likelihood Matching for Diffusion Models
arxiv_id: '2508.03636'
source_url: https://arxiv.org/abs/2508.03636
tags:
- likelihood
- score
- matching
- hessian
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel likelihood matching approach for training
  diffusion models that directly maximizes the data likelihood by leveraging the equivalence
  between the path likelihood of the reverse diffusion process and the likelihood
  of the original data distribution. The method introduces a quasi-likelihood approximation
  using Gaussian distributions with matched conditional mean and covariance, and incorporates
  both score and Hessian functions to capture richer transitional information than
  traditional score matching.
---

# Likelihood Matching for Diffusion Models

## Quick Facts
- arXiv ID: 2508.03636
- Source URL: https://arxiv.org/abs/2508.03636
- Authors: Lei Qian; Wu Su; Yanqi Huang; Song Xi Chen
- Reference count: 40
- This paper proposes a novel likelihood matching approach for training diffusion models that directly maximizes the data likelihood by leveraging the equivalence between the path likelihood of the reverse diffusion process and the likelihood of the original data distribution.

## Executive Summary
This paper introduces a novel approach to training diffusion models by directly maximizing the data likelihood rather than relying on score matching upper bounds. The key insight is establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. This is achieved through a quasi-likelihood approximation using Gaussian distributions with matched conditional mean and covariance, incorporating both score and Hessian functions to capture richer transitional information than traditional score matching. The framework provides non-asymptotic convergence guarantees for the stochastic sampler, characterizing errors due to score and Hessian estimation, dimensionality, and diffusion steps.

## Method Summary
The method introduces a likelihood matching framework that exploits the equivalence between data likelihood and path likelihood in diffusion processes. It uses a quasi-likelihood approximation where the reverse transition density is modeled as a Gaussian with conditional mean and covariance determined by the score and Hessian of the log-density. The training objective maximizes this quasi-likelihood directly, incorporating both first-order (score) and second-order (Hessian) moment information. The approach uses two separate U-Net architectures - one for score estimation and a lighter one for Hessian estimation using a diagonal-plus-low-rank structure. The loss is computed using Sherman-Morrison-Woodbury formula for efficient inversion, and sampling uses the learned covariance structure for improved per-step accuracy.

## Key Results
- The likelihood matching approach consistently outperforms the baseline score matching method across CIFAR-10, CelebA, and other standard benchmarks
- Best performance achieved when using a Hessian approximation rank between 10-30
- Improved FID scores and negative log-likelihoods compared to score matching baselines
- The Hessian-enhanced stochastic sampler achieves lower discretization error per step, enabling faster convergence with fewer sampling iterations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Likelihood Matching enables direct optimization of data likelihood by exploiting path equivalence between forward and reverse diffusion processes.
- **Mechanism:** Proposition 1 establishes that the expected log-likelihood at t=0 can be expressed via reverse transition densities. Unlike score matching which minimizes an upper bound on negative log-likelihood, LM targets the likelihood directly through the reverse path integral.
- **Core assumption:** The forward transition density q_{tk|tk-1} is parameter-free and terminal marginal p_{tN} converges to N(0,I) independent of θ.
- **Evidence anchors:** [abstract] "establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion" and [section 2.2] Proposition 1 derivation.
- **Break condition:** If β_t is misspecified or the parametric family F_θ is profoundly mismatched to data.

### Mechanism 2
- **Claim:** Quasi-likelihood approximation with Gaussian matched moments captures richer transitional information by incorporating both score and Hessian of log-density.
- **Mechanism:** Proposition 2 derives that conditional mean μ_{s|t} requires the score ∇log q_t, while covariance Σ_{s|t} requires the Hessian ∇²log q_t. Standard score matching uses only first-order moment information; LM's Gaussian quasi-likelihood implicitly performs covariance matching.
- **Core assumption:** The true reverse transition can be adequately approximated by a Gaussian with matched first two moments.
- **Evidence anchors:** [abstract] "incorporates both score and Hessian functions to capture richer transitional information" and [section 3.1] Proposition 2 explicit formulas.
- **Break condition:** If the true reverse transition is highly non-Gaussian (e.g., multimodal conditional distributions).

### Mechanism 3
- **Claim:** The Hessian-enhanced stochastic sampler achieves lower discretization error per step, enabling faster convergence with fewer sampling iterations.
- **Mechanism:** Sampler uses learned covariance Σ_{t-1|t} from the Hessian network rather than assuming isotropic noise. Theorem 1 bounds TV distance with terms scaling as O(d³log⁴·⁵T/T) for discretization.
- **Core assumption:** Assumption 4 ensures the Hessian doesn't make (I + (1-α_t)∇²log q_t) singular; low-rank approximation sufficiently captures covariance structure.
- **Evidence anchors:** [abstract] "non-asymptotic convergence guarantees...quantifying the rates of the approximation errors due to the score and Hessian estimation" and [section 5.2] Figure 4 showing LM produces recognizable digits at 20 iterations.
- **Break condition:** If Hessian estimation error ε_H is large relative to score error.

## Foundational Learning

- **Concept: Score Functions and Score Matching**
  - Why needed here: LM extends score matching by adding Hessian matching. You must understand that the score ∇_x log p(x) is the gradient of log-density, and score matching minimizes ‖∇log q_t - s_t‖².
  - Quick check question: Given a Gaussian N(μ, Σ), compute its score function. Answer: (μ - x)/Σ (for 1D) or Σ⁻¹(μ - x) (multivariate).

- **Concept: Forward and Reverse SDEs in Diffusion Models**
  - Why needed here: The paper's core equivalence relies on the reverse-time SDE being the time-reversal of the forward SDE. Understanding how β_t controls noise injection and how the score appears in the drift term is essential.
  - Quick check question: In the reverse SDE dY_t = ½β_t(Y_t + 2∇log q_t)dt + √β_t dW̄_t, what happens if the score is estimated incorrectly? Answer: The reverse process drifts away from the data manifold, producing low-quality samples.

- **Concept: Quasi-Maximum Likelihood Estimation (QMLE)**
  - Why needed here: The method approximates intractable densities with tractable Gaussians. QMLE theory guarantees consistency if the approximating family contains the truth or if the quasi-likelihood is "close enough."
  - Quick check question: If you approximate a non-Gaussian distribution with a matched-mean-and-covariance Gaussian, under what conditions does QMLE still yield consistent estimators? Answer: When the true model is in the parametric family and the Gaussian is a valid surrogate; otherwise, consistency requires additional regularity.

## Architecture Onboarding

- **Component map:**
  - Score Network (U-Net) -> Hessian Network (U-Net, smaller) -> Forward SDE Simulator -> LM Loss Computer -> Stochastic Sampler

- **Critical path:**
  1. Sample X_0 from dataset
  2. Sample random time grid (t_1, ..., t_{N-1}) uniformly from simplex
  3. Generate X_{t_k} via forward SDE
  4. Compute score and Hessian predictions at each t_k
  5. Assemble quasi-likelihood loss (Equation 15-16)
  6. Backprop through both networks
  7. At inference: run sampler (13) from Y_T ~ N(0,I) for T steps

- **Design tradeoffs:**
  - **Rank r (10-30 recommended):** Higher r captures more covariance structure but increases memory/time. Paper shows diminishing returns beyond r=30.
  - **Number of time points N (2-8 used):** More points evaluate more transitions per batch but increase compute. Random sampling provides unbiased path integral estimate.
  - **Diagonal vs. low-rank Hessian:** r=0 (diagonal only) still outperforms SM baseline; low-rank adds ~10-20% training time overhead.
  - **Joint vs. alternating training:** Paper jointly trains score and Hessian networks; could alternate to save memory.

- **Failure signatures:**
  - **Hessian network outputs negative eigenvalues:** This breaks positive-definiteness of Σ. Fix: apply ReLU to diagonal U_t.
  - **Loss becomes NaN:** Check that (I + σ²H_t) remains invertible. Assumption 4 requires eigenvalues > -1/(1-α_t).
  - **FID worse than SM baseline:** Likely Hessian rank too high (overfitting) or too low (underfitting); try r ∈ [10, 30].
  - **Sampling diverges:** Reduce learning rate for Hessian network or add spectral normalization.

- **First 3 experiments:**
  1. **Sanity check on 2D mixture:** Train LM (N=2, r=10) and SM on 2-component Gaussian mixture. Compute MMD between generated and true samples. Expect LM MMD < SM MMD. Verify score network learns correct gradients by visualizing vector field.
  2. **Ablation on Hessian rank:** Train on CIFAR-10 with r ∈ {0, 10, 20, 30, 100}. Plot FID vs. r. Expect U-shaped curve with minimum at r ≈ 20-30. Confirm memory/time scale linearly with r.
  3. **Sampling efficiency comparison:** Train LM (r=20) and SM to convergence. Sample with varying steps T ∈ {20, 50, 100, 500}. Plot FID vs. T. Expect LM reaches target FID in fewer steps due to per-step covariance accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Likelihood Matching framework be effectively combined with advanced solvers (e.g., Analytic-DPM) to improve performance on large-scale benchmarks like ImageNet?
- Basis in paper: [Explicit] The conclusion states that combining LM with advanced solvers on large-scale benchmarks such as ImageNet "is an especially promising direction."
- Why unresolved: The current work evaluates LM in isolation against standard baselines and does not test integration with other specific advanced solvers or variance reduction techniques.
- Evidence: Empirical results showing improved FID and NLL on high-resolution ImageNet when LM is integrated into existing advanced diffusion frameworks.

### Open Question 2
- Question: Does incorporating importance sampling or non-uniform weighting into the Likelihood Matching objective significantly reduce gradient variance?
- Basis in paper: [Explicit] Section 3.2 notes that the current method uses uniform sampling to ensure unbiasedness, but adds that "Exploring importance sampling or non-uniform weighting within the LM framework to reduce gradient variance remains an interesting direction."
- Why unresolved: The authors currently sample uniformly from the simplex; they have not tested if hand-crafted, non-uniform schedules could improve optimization efficiency without introducing bias.
- Evidence: A comparative study of training dynamics showing lower gradient variance or faster convergence when using optimized importance sampling schedules versus the uniform baseline.

### Open Question 3
- Question: How can the computational burden of Hessian modeling be reduced to facilitate application to high-resolution video generation?
- Basis in paper: [Explicit] The conclusion identifies the "computational burden of Hessian modeling on large-scale datasets" as substantial and lists high-dimensional data domains like video as a future direction.
- Why unresolved: The paper's complexity analysis shows a 3-4x increase in training time for high-resolution images, suggesting current low-rank approximations may still be too costly for video.
- Evidence: Development of a sparse or more efficient Hessian approximation method that maintains generation quality while reducing the training overhead to levels comparable to standard Score Matching.

## Limitations

- The computational overhead of Hessian networks (roughly 2x memory and 1.5x training time) is justified empirically but could be more thoroughly characterized.
- The low-rank approximation (rank 10-30) shows promising results but lacks systematic exploration of when this approximation breaks down.
- The non-asymptotic convergence guarantees assume idealized conditions that may not hold in practice.

## Confidence

- **High confidence**: The equivalence between path likelihood and data likelihood (Proposition 1), the theoretical framework connecting score/Hessian to transitional modeling.
- **Medium confidence**: The practical benefits of Hessian estimation in non-Gaussian settings, the optimal rank selection (10-30) for Hessian approximation.
- **Low confidence**: The generalizability of Hessian estimation across different data distributions, the robustness of the approach when the parametric family is profoundly mismatched.

## Next Checks

1. **Synthetic non-Gaussian test**: Train LM and SM on a 2D mixture of Gaussians with heavy tails or skewed components. Measure whether LM consistently outperforms SM when the Gaussian quasi-likelihood assumption is violated.

2. **Hessian rank sensitivity**: Systematically vary rank r from 0 to 100 on CIFAR-10, measuring both training time and FID. Identify the exact point where additional rank provides negligible benefit versus computational cost.

3. **Cross-dataset generalization**: Train LM on CIFAR-10 and evaluate sampling quality on LSUN-bedroom and CelebA without fine-tuning. Test whether the Hessian learned on one dataset transfers to others, indicating the generality of the transitional information captured.