---
ver: rpa2
title: Benchmarking Multimodal RAG through a Chart-based Document Question-Answering
  Generation Framework
arxiv_id: '2502.14864'
source_url: https://arxiv.org/abs/2502.14864
tags:
- keypoint
- retrieval
- text
- chart
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chart-based Multimodal Retrieval-Augmented
  Generation (MRAG), a novel task that addresses the evaluation gap in chart-based
  information retrieval. The authors propose CHARGE, an automated framework that generates
  high-quality question-answer pairs through structured keypoint extraction, crossmodal
  verification, and keypoint-based generation.
---

# Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework

## Quick Facts
- **arXiv ID:** 2502.14864
- **Source URL:** https://arxiv.org/abs/2502.14864
- **Reference count:** 40
- **Primary result:** Unified multimodal embeddings fail on chart-based retrieval (0% recall); even SOTA MLLMs achieve only 58.19% Correctness and 73.87% Coverage on Chart-MRAG Bench.

## Executive Summary
This paper introduces Chart-based Multimodal Retrieval-Augmented Generation (MRAG) as a novel task for chart-based information retrieval and proposes CHARGE, an automated framework to generate high-quality question-answer pairs for this task. The authors construct Chart-MRAG Bench, a comprehensive benchmark with 4,738 QA pairs across 8 domains, and evaluate several MRAG methods and MLLMs on it. The study reveals that unified multimodal embeddings struggle with chart-based scenarios, achieving zero recall for chart-only queries, and that state-of-the-art MLLMs exhibit a text-over-visual modality bias, preferring approximate text answers over precise chart data.

## Method Summary
The paper proposes CHARGE, a three-stage framework for generating chart-based MRAG benchmarks. First, it extracts atomic keypoints from text and charts using GPT-4o and ChartOCR. Second, it validates modality-specificity through crossmodal verification, ensuring each keypoint is uniquely retrievable from its source modality. Third, it generates QA pairs by combining related keypoints, optionally retrieving additional context for multi-hop reasoning. The resulting Chart-MRAG Bench contains 4,738 QA pairs from 267 real-world documents, evaluated using novel Correctness and Coverage metrics that assess keypoint-level precision.

## Key Results
- Unified multimodal embeddings (CLIP) achieve 0.00% recall on chart-only queries, while separate embeddings (SigLIP + E5) achieve 42.53% recall.
- State-of-the-art MLLMs (Claude 3.5, GPT-4o) achieve only 58.19% Correctness and 73.87% Coverage with ground-truth retrieval.
- MLLMs consistently exhibit text-over-visual modality bias, preferring approximate text answers over precise chart data.
- The CHARGE framework produces high-quality QA pairs with expert-validated Fleiss's kappa of 0.51.

## Why This Works (Mechanism)

### Mechanism 1: Crossmodal Verification for Data Hygiene
- **Claim:** High-quality multimodal benchmarks require strict isolation of modality-specific information to prevent evaluation leakage.
- **Mechanism:** CHARGE extracts atomic keypoints from text and charts, then uses GPT-4o to verify each keypoint is uniquely retrievable from its source modality but absent in the cross-modal check.
- **Core assumption:** GPT-4o can accurately determine the absence of information in a modality and that atomic keypoints can be cleanly separated from context.
- **Evidence anchors:** Section 3.2 describes the verification process retaining keypoints only when correctly retrieved from their source and absent in others; Section 4 details manual refinement eliminating redundant samples.
- **Corpus evidence:** Corpus neighbor "A Survey of Multimodal Retrieval-Augmented Generation" supports the need for robust evaluation but doesn't validate this specific logic.
- **Break condition:** If GPT-4o fails to detect semantic equivalence (e.g., "33%" vs. "one third"), the dataset will contain false negatives, inflating difficulty for Text-only or Chart-only sub-tasks.

### Mechanism 2: Asymmetric Retrieval via Separate Vector Stores
- **Claim:** Unified multimodal embeddings fail to retrieve dense, knowledge-intensive visual data like charts.
- **Mechanism:** The paper compares three methods: unified embeddings (CLIP), captioning charts for text-based retrieval, and separate stores using SigLIP for visual and E5-Large for text retrieval.
- **Core assumption:** The semantic geometry for encoding "a photo of a dog" differs fundamentally from encoding dense line charts with 20 data points.
- **Evidence anchors:** Section 5.2 shows unified embeddings achieve zero recall in chart-only QA while separate stores achieve 42.53% and 61.10% recall.
- **Corpus evidence:** Corpus neighbor "mRAG: Elucidating the Design Space" likely discusses architectural variations, but specific failure modes are detailed in this paper's Section 5.2.
- **Break condition:** If user queries require visual context to understand text during embedding, separate stores might miss connections compared to perfectly aligned unified models.

### Mechanism 3: Keypoint-Based Evaluation Metrics
- **Claim:** Traditional generation metrics are inadequate for chart reasoning; response quality must be measured by coverage and correctness of atomic factual units.
- **Mechanism:** Evaluation extracts keypoint sets from model responses and compares against ground truth using set-based matching for Coverage and full equality for Correctness.
- **Core assumption:** Chart facts can be distilled into discrete statements and the extraction LLM is consistent.
- **Evidence anchors:** Section 5.1 introduces these metrics as inadequate existing metrics cannot handle chart-based MRAG; abstract reports MLLMs achieve 58.19% Correctness and 73.87% Coverage.
- **Corpus evidence:** Limited regarding this specific metric; related benchmarks typically use standard VQA accuracy or LLM-as-a-judge scoring.
- **Break condition:** If models provide correct answers using different phrasing or higher-level summaries that keypoint extractors fail to parse as matching ground truth, Correctness scores will artificially drop.

## Foundational Learning

- **Concept:** Unified vs. Separate Multimodal Embeddings
  - **Why needed here:** The paper demonstrates standard "one-size-fits-all" vector stores catastrophically fail for charts.
  - **Quick check question:** If building a RAG system for financial reports with paragraphs and line charts, why should you avoid using a single CLIP model to embed everything into one index?

- **Concept:** Modality Bias (Text-over-Visual)
  - **Why needed here:** The paper identifies MLLMs prefer lazy reasoning—ignoring charts if text offers "good enough" clues, even when charts are more precise.
  - **Quick check question:** If retrieved text says "sales increased significantly" but chart shows "sales increased by 3.5%," which answer will a biased MLLM likely produce, and why is that problematic for precision-critical tasks?

- **Concept:** OCR-Augmented Visual Reasoning
  - **Why needed here:** CHARGE uses OCR to extract raw values before generating questions or answers, as charts are data structures, not just images.
  - **Quick check question:** Why does feeding a raw chart image into an MLLM sometimes yield lower numerical accuracy than feeding the chart image plus an OCR transcript of its values?

## Architecture Onboarding

- **Component map:** Real-world Documents → ChartOCR (Extract Values) + Text Chunker → Keypoint Extractor (GPT-4o) → Crossmodal Verifier (Filter) → QA Generator → Two separate stores: Text Index (E5-Large embeddings) and Visual Index (SigLIP embeddings) → Parallel Retrieval (Top-K from both) → Context Assembly → MLLM (Claude 3.5 / GPT-4o) → Keypoint Evaluator

- **Critical path:** The Crossmodal Verification step in data generation. If skipped, the benchmark cannot distinguish between Text-only and Chart-only reasoning capabilities, invalidating diagnostic results.

- **Design tradeoffs:**
  - Unified vs. Separate Stores: Unified (CLIP) is simpler (one index) but retrieves 0% of charts in dense scenarios. Separate stores (SigLIP + E5) double infrastructure complexity but are the only viable path for chart retrieval.
  - Captioning vs. Direct Vision: Method 2 (Captioning charts with GPT-4-Vision) is a practical middle ground, allowing text-based retrievers to handle charts but losing raw visual data nuance.

- **Failure signatures:**
  - Zero Recall on Charts: Using unified embedding space (CLIP/JINA) for high-density charts. Switch to separate stores.
  - Low Correctness (High Coverage): Model finds right info but hallucinates extra details or misses exact counts. This is the "58.19% Correctness" ceiling.
  - Text Bias: Model ignores retrieved chart image. Check if text context contains "approximate" answers the model prefers over precise visual data.

- **First 3 experiments:**
  1. Retrieval Validity Check: Run benchmark queries against Unified Embedding (CLIP) vs. Separate Embeddings (SigLIP+E5). Verify the "0% vs 84%" recall gap on Chart-only queries.
  2. Modality Bias Probe: Construct 10 queries where retrieved text is intentionally slightly contradictory or less precise than retrieved chart. Measure how often MLLM defaults to text answer.
  3. OCR Dependency Test: Feed MLLM charts with and without OCR overlays (side-by-side comparison) to quantify performance delta of explicit numerical assistance vs. pure vision capabilities.

## Open Questions the Paper Calls Out

- **Question:** How can the CHARGE framework be enhanced to handle complex chart layouts and diverse visual elements more accurately than current OCR implementations?
  - **Basis in paper:** [explicit] Authors state in Limitations that "CHARGE would benefit from more advanced OCR techniques to further enhance accuracy of question generation, especially in handling complex chart layouts and diverse visual elements."
  - **Why unresolved:** Current reliance on standard OCR results in information loss or errors when processing dense or structurally complex visual data, necessitating manual verification to maintain benchmark quality.
  - **What evidence would resolve it:** Updated CHARGE pipeline integrating advanced OCR or vision-encoding methods, demonstrating statistically significant reduction in extraction errors and higher retention rate of generated QA pairs without human intervention.

- **Question:** How can unified multimodal embedding models be redesigned to effectively handle knowledge-intensive scenarios where they currently fail to retrieve chart data?
  - **Basis in paper:** [inferred] Paper observes "unified multimodal embedding retrieval methods... struggle in chart-based scenarios," achieving 0.0% recall in chart-only tasks because they cannot distinguish fine-grained details in high-density charts.
  - **Why unresolved:** Current unified embeddings appear to prioritize broad semantic alignment over precise data localization, making them unsuitable for retrieving specific numerical values or structural details required in Chart-based MRAG.
  - **What evidence would resolve it:** Development of unified embedding model achieving non-zero recall on Chart-MRAG Bench's chart-only retrieval tasks without requiring separate vector stores or text captioning workarounds.

- **Question:** What architectural or training interventions are required to mitigate the "text-over-visual" modality bias observed in MLLMs during chart-based reasoning?
  - **Basis in paper:** [inferred] Analysis reveals "consistent text-over-visual modality bias," where models prefer text-only responses even when charts contain more precise information (e.g., "one third" in text vs. "35.2%" in chart).
  - **Why unresolved:** Paper identifies bias but doesn't propose solution; notes even state-of-the-art models struggle to integrate precise visual data when redundant textual context is present.
  - **What evidence would resolve it:** Experiments demonstrating specific fine-tuning objectives or prompting strategies can successfully force models to prioritize high-precision visual data over lower-precision textual approximations in ambiguous multimodal contexts.

- **Question:** Do the findings regarding retrieval performance and modality bias generalize to a broader range of MRAG methods and MLLM architectures not evaluated in this study?
  - **Basis in paper:** [explicit] Authors acknowledge "due to computational constraints, our evaluation was confined to a select set of MRAG methods and MLLMs," limiting generalizability of conclusions.
  - **Why unresolved:** Benchmark tested on specific subset of models (e.g., CLIP, SigLIP, GPT-4o, Claude), leaving performance characteristics of other potential architectures unknown.
  - **What evidence would resolve it:** Follow-up evaluation involving wider diversity of model architectures (e.g., specialized document-understanding models or newer open-source MLLMs) showing consistent performance patterns regarding retrieval failure and modality bias.

## Limitations
- The framework's quality depends heavily on automated keypoint extraction and crossmodal verification by GPT-4o, with limited transparency into specific prompts used.
- Benchmark construction involved significant manual filtering (2,631 → 4,738 samples), but criteria for this filtering are not fully specified.
- Evaluation metrics depend on consistency of keypoint extraction process and may not capture all valid forms of chart reasoning.

## Confidence
- **High Confidence:** The core finding that unified multimodal embeddings fail for chart-based retrieval is well-supported by experimental evidence (0% recall for charts with CLIP-style embeddings).
- **Medium Confidence:** The text-over-visual modality bias in MLLMs is demonstrated through multiple experiments, though magnitude may vary with different model architectures or prompt engineering.
- **Medium Confidence:** The CHARGE framework's ability to generate high-quality QA pairs is supported by expert validation (Fleiss's kappa of 0.51), but reproducibility depends on access to GPT-4o and exact implementation details.

## Next Checks
1. **Prompt Engineering Impact:** Systematically test how variations in GPT-4o prompts for keypoint extraction and verification affect quality and modality purity of generated QA pairs.

2. **Model Architecture Comparison:** Evaluate whether observed text bias persists across different MLLM architectures (e.g., Gemini, Llama) to determine if it's a fundamental limitation or specific to tested models.

3. **Generalizability Test:** Apply CHARGE framework to different domain (e.g., financial reports or scientific papers) to assess whether methodology generalizes beyond Pew Research documents and whether modality bias remains consistent.