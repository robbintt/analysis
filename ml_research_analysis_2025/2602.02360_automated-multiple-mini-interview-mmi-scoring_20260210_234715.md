---
ver: rpa2
title: Automated Multiple Mini Interview (MMI) Scoring
arxiv_id: '2602.02360'
source_url: https://arxiv.org/abs/2602.02360
tags:
- scoring
- performance
- question
- response
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automating scoring for Multiple
  Mini Interviews (MMIs), a critical assessment tool for evaluating soft skills like
  empathy and ethical judgment. The researchers developed a multi-agent prompting
  framework that breaks the evaluation into a preprocessing agent and nine separate,
  criterion-specific scoring agents, using 3-shot in-context learning.
---

# Automated Multiple Mini Interview (MMI) Scoring

## Quick Facts
- arXiv ID: 2602.02360
- Source URL: https://arxiv.org/abs/2602.02360
- Reference count: 26
- Automated multi-criteria scoring of MMI transcripts achieves QWK 0.621 vs. 0.316 for fine-tuned baselines

## Executive Summary
This study develops an automated framework for scoring Multiple Mini Interview (MMI) transcripts, a critical assessment tool for evaluating soft skills like empathy and ethical judgment. The researchers address the challenge of scoring nine distinct criteria across four scenario-based questions using a multi-agent prompting approach. The framework decomposes the evaluation into a preprocessing agent and nine separate criterion-specific scoring agents, employing 3-shot in-context learning with carefully balanced examples. The method outperforms fine-tuned baselines by nearly double (average QWK of 0.621 versus 0.316) and demonstrates strong generalizability to the ASAP essay scoring dataset without additional training.

## Method Summary
The framework uses a two-stage multi-agent prompting architecture: (1) a preprocessing agent cleans raw transcripts by removing filler words, conversational introductions, and redundant statements; (2) nine separate scoring agents, each dedicated to one of the nine soft-skill criteria, evaluate the preprocessed transcript using 3-shot in-context learning with Low/Medium/High percentile examples. The scoring agents use detailed rubric descriptors and generate a single numeric score on a 7-point Likert scale. The approach was evaluated on 1001 candidate transcripts from MMI assessments, comparing performance against various fine-tuned models and prompting configurations. The best results were achieved with Llama 4 Maverick, though the framework is designed to be model-agnostic.

## Key Results
- Multi-agent prompting framework achieved average QWK of 0.621 across all questions versus 0.316 for the best fine-tuned model
- 3-shot L/M/H calibration outperformed zero-shot (0.363 vs 0.215 QWK) and retrieval-based methods
- Framework generalized to ASAP essay scoring dataset without additional training, achieving competitive QWK of 0.783
- Performance degraded when using reduced/generic rubrics (0.790 vs 0.869 QWK), confirming the importance of detailed criteria

## Why This Works (Mechanism)

### Mechanism 1: Criterion-Specific Agent Decomposition
Dividing multi-trait evaluation into isolated single-criterion agents improves scoring accuracy by reducing cross-criterion interference. Each of the 9 scoring agents receives the preprocessed transcript and evaluates only one criterion with its own calibrated examples, preventing the model from conflating distinct soft skills when generating a single joint response. This isolation forces focused reasoning per trait, yielding QWK of 0.533 on Question 3 versus 0.363 for the best single-prompt configuration.

### Mechanism 2: Balanced 3-Shot Calibration
A fixed set of three examples spanning low, medium, and high score percentiles calibrates the model's scoring scale more effectively than dynamic retrieval or imbalanced exemplars. The L/M/H examples establish upper and lower bounds, countering the model's inherent overestimation bias. Four or more examples cause the model to anchor on the final example's score, degrading generalization. 3-shot L/M/H achieved QWK 0.363 versus 0.215 for zero-shot, with degradation at 4-shot due to bias toward the final example.

### Mechanism 3: Prompting Over Fine-Tuning for Abstract Rubrics
For tasks with broad, abstract rubrics lacking verifiable features, large-model prompting outperforms fine-tuning because generated rationales introduce noise rather than signal. Fine-tuning on abstract criteria cannot ground predictions in concrete features; generated justifications are themselves subjective interpretations that introduce inaccuracy. Best fine-tuned model achieved Avg QWK 0.316 while prompting achieved 0.621—nearly double—demonstrating that for abstract soft-skill assessments, frozen reasoning capacity through prompting is superior to data-intensive fine-tuning.

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - Why needed here: QWK is the primary metric for inter-rater reliability on ordinal scales; it penalizes larger disagreements more heavily, making it appropriate for 7-point Likert scoring where a 2-point error is worse than two 1-point errors.
  - Quick check question: If a model predicts "5" when the human score is "4," does QWK penalize this more or less than predicting "3"? (Answer: Less—QWK weights larger deviations quadratically.)

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The framework relies on 3-shot examples to calibrate scoring without weight updates; understanding how exemplars influence output is critical for debugging calibration failures.
  - Quick check question: If you provide only high-scoring examples, what bias would you expect in model predictions? (Answer: Overestimation bias, as the model lacks a lower-bound anchor.)

- **Concept: Cross-Criterion Interference in Multi-Task Learning**
  - Why needed here: The multi-agent design explicitly addresses interference; recognizing when simultaneous multi-trait evaluation degrades performance informs architectural decisions.
  - Quick check question: When would combining multiple scoring criteria into a single prompt be acceptable? (Answer: When criteria are highly correlated or when the rubric provides concrete, non-overlapping definitions.)

## Architecture Onboarding

- **Component map**: Raw transcript -> Preprocessing Agent -> Cleaned transcript -> 9 Scoring Agents (one per criterion) -> Aggregate scores -> Final assessment
- **Critical path**: Raw transcript → Preprocessing Agent → Cleaned transcript; Cleaned transcript + criterion definition + 3-shot examples → Scoring Agent (per criterion); Aggregate 9 scores → Final multi-trait assessment
- **Design tradeoffs**:
  - Static vs. Dynamic Exemplars: RAG-based retrieval underperformed due to skewed score distribution; static L/M/H provides reliable calibration but may not adapt to novel question types
  - Preprocessing Inclusion: Improves both fine-tuning (0.257→0.316 QWK for modernBERT) and prompting; adds latency but reduces noise
  - Model Size: 405B/Flagship models required for strong performance; 8B models underperform even with fine-tuning
- **Failure signatures**:
  - Overestimation Bias: Model skews high; indicates missing low-score anchors or insufficient L/M/H balance
  - Recency Bias with 4+ Examples: Final example dominates predictions; reduce to 3-shot
  - Cross-Criterion Bleed: Scores for different criteria converge; check if single-prompt architecture was accidentally used
  - Rationale Noise in Fine-Tuning: Fine-tuned models show balanced but wide error distribution; indicates training on unverifiable abstract features
- **First 3 experiments**:
  1. Baseline Calibration Test: Run 3-shot L/M/H vs. zero-shot vs. retrieval-based RAG on a held-out question; confirm QWK improvement follows paper's trajectory (target: ~0.35→0.50+)
  2. Agent Isolation Ablation: Compare multi-agent (9 separate scorers) vs. single-prompt (all 9 criteria at once) on the same transcripts; expect ~0.17 QWK delta as reported
  3. Rubric Granularity Stress Test: Apply framework to a subset with reduced/generic rubric vs. detailed rubric; quantify performance drop to validate abstraction sensitivity (target: ~0.08-0.12 QWK degradation per Table 4)

## Open Questions the Paper Calls Out

- **Can the multi-agent framework be adapted to generate evidence-based justifications for scores without compromising scoring accuracy?**
  - The authors acknowledge that the current "black box" nature is a barrier to trust and that future work must address this by prompting the multi-agent framework to generate justifications, while maintaining prediction performance.

- **Does incorporating audio-visual features (non-verbal cues) significantly improve alignment with human expert scores compared to the text-only approach?**
  - The study deliberately excluded non-verbal data to mitigate biases (accent, appearance), leaving the performance contribution of multimodal data in MMIs untested despite acknowledging a "scoring mismatch" where text-only models predict scores assigned by human raters who observed full video.

- **Would fine-tuned models outperform the prompting framework if trained and evaluated on a dataset with detailed, question-specific rubrics rather than broad criteria?**
  - The authors posit that fine-tuning struggled because the MMI rubric was "broad, abstract, and static," unlike the detailed rubrics available for the ASAP dataset, but did not test if SOTA fine-tuning methods would succeed if "verifiable signals" of detailed rubrics were present.

## Limitations
- Exact MMI dataset and transcripts cannot be shared due to confidentiality, making direct reproduction challenging
- Framework's exceptional performance may be partially dataset-specific given the highly skewed score distribution (85% in 4-6 range)
- Multi-agent architecture requires access to very large models (405B/Flagship), limiting practical deployment

## Confidence

- **High Confidence**: Superiority of 3-shot L/M/H calibration over zero-shot and retrieval-based methods is well-supported by systematic ablation (QWK 0.363 vs 0.215 for zero-shot); multi-agent decomposition mechanism is validated with clear performance gains (0.533 vs 0.363 QWK for single vs multi-prompt on Question 3)
- **Medium Confidence**: Claim that prompting outperforms fine-tuning for abstract rubrics is supported but requires qualification—the comparison assumes fine-tuning on generated rationales introduces noise, yet the study doesn't test fine-tuning on alternative feature sets or with detailed rubrics
- **Low Confidence**: Framework's performance on non-skewed score distributions or with different evaluation criteria hasn't been tested; cost-benefit tradeoff of preprocessing (which adds latency) versus accuracy gains is not quantified

## Next Checks
1. **Score Distribution Robustness Test**: Evaluate the framework on a held-out subset with uniform score distribution (not skewed 85% in 4-6 range). Measure whether QWK degradation exceeds 0.15 compared to the skewed distribution results.
2. **Cross-Domain Transfer Test**: Apply the preprocessing + multi-agent framework to a different assessment domain (e.g., teacher evaluation or customer service scoring) with at least 3 criteria and 7-point scale. Compare QWK to domain-specific fine-tuned baselines.
3. **Preprocessing Impact Analysis**: Run ablation comparing full framework (with preprocessing) vs. framework without preprocessing on the same transcripts. Quantify QWK difference and measure preprocessing latency overhead to establish cost-benefit ratio.