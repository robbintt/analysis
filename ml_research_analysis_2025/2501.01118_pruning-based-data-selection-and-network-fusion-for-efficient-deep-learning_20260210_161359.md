---
ver: rpa2
title: Pruning-based Data Selection and Network Fusion for Efficient Deep Learning
arxiv_id: '2501.01118'
source_url: https://arxiv.org/abs/2501.01118
tags:
- data
- prunefuse
- selection
- pruning
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PruneFuse, a method that combines neural
  network pruning and network fusion to improve data selection efficiency in deep
  learning. The core idea is to prune the original model to create a smaller surrogate
  that efficiently selects informative samples from the unlabeled dataset, then fuse
  the trained pruned model with the original model to provide an optimized initialization
  for training.
---

# Pruning-based Data Selection and Network Fusion for Efficient Deep Learning

## Quick Facts
- arXiv ID: 2501.01118
- Source URL: https://arxiv.org/abs/2501.01118
- Reference count: 40
- Combines neural network pruning and network fusion to improve data selection efficiency in deep learning

## Executive Summary
This paper introduces PruneFuse, a method that combines neural network pruning and network fusion to improve data selection efficiency in deep learning. The core idea is to prune the original model to create a smaller surrogate that efficiently selects informative samples from the unlabeled dataset, then fuse the trained pruned model with the original model to provide an optimized initialization for training. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet-200 demonstrate that PruneFuse significantly reduces computational costs compared to traditional active learning methods while achieving better performance.

## Method Summary
PruneFuse operates through a multi-stage pipeline: First, it prunes the original untrained network using magnitude-based channel pruning at initialization to create a smaller surrogate model. This pruned model is trained on an initial labeled subset and used to select informative samples via uncertainty sampling (Least Confidence, Entropy, or Greedy k-centers). The selection process iterates, retraining the pruned model from scratch each round as new samples are labeled. Once the labeling budget is met, the method fuses the trained pruned model with the original architecture by copying trained weights into their corresponding positions, leaving unpruned regions randomly initialized. Finally, the fused model undergoes fine-tuning using knowledge distillation from the pruned model to produce the final model.

## Key Results
- Achieves up to 93.69% accuracy on CIFAR-10 with 50% labeling budget
- Outperforms state-of-the-art active learning approaches
- Reduces computational costs significantly compared to traditional active learning methods
- Maintains model performance while reducing computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Structural Coherence Enables Transferable Data Selection
- Claim: A pruned model can select informative samples for the full model because both share the same architectural structure.
- Mechanism: The pruned model θ_p is derived via channel pruning from θ, retaining identical layer organization. Samples exhibiting high uncertainty (Least Confidence, Entropy) or distance (Greedy k-centers) under θ_p are assumed informative for θ because the pruned network's decision boundaries approximate the full network's.
- Core assumption: The informativeness ranking of samples under θ_p correlates sufficiently with their ranking under θ.
- Evidence anchors:
  - [abstract]: "the original dense network is pruned to generate a smaller surrogate model that efficiently selects the most informative samples"
  - [section 3]: "θ_p maintains a structure that is essentially identical to θ... there is a strong correlation between θ and θ_p"
  - [corpus]: Selection-via-Proxy (SVP) uses smaller models for selection but lacks fusion; PruneFuse's architectural coherence is designed to address SVP's structural discrepancy issues
- Break condition: At aggressive pruning ratios (p > 0.8), the pruned model may lack capacity to represent complex class boundaries, degrading selection quality.

### Mechanism 2: Fusion Provides Optimized Initialization with Exploration Capacity
- Claim: Weight fusion transfers learned knowledge while preserving stochasticity in unpruned regions, accelerating convergence.
- Mechanism: Fusion copies trained weights from θ*_p into their corresponding positions in θ. Unpruned positions retain random initialization, enabling exploration of regions the pruned model could not access.
- Core assumption: The trained weights from the pruned model constitute a useful local optimum that generalizes to the larger architecture.
- Evidence anchors:
  - [abstract]: "fuse the trained pruned model with the original model to provide an optimized initialization that accelerates training"
  - [section 3.4]: "The trained weights of θ*_p provides a better initialization, while the unaltered weights serve as gateways to unexplored regions in the loss landscape"
  - [Figure 4]: Fusion shows faster convergence and higher final accuracy than training without fusion
  - [corpus]: No direct corpus precedent for this specific weight-fusion-for-initialization mechanism; appears novel to this work
- Break condition: If θ*_p overfits to the small labeled subset, transferred weights may embed harmful spurious correlations.

### Mechanism 3: Knowledge Distillation Reinforces and Refines Transferred Knowledge
- Claim: Post-fusion distillation from θ*_p to θ_F improves final accuracy beyond weight transfer alone.
- Mechanism: During fine-tuning, a composite loss λ×L_CrossEntropy + (1-λ)×L_Distillation trains θ_F using both ground-truth labels and softened logits from θ*_p as teacher signals.
- Core assumption: The pruned model encodes transferable dark knowledge (class relationships) beneficial to the fused model.
- Evidence anchors:
  - [section 3.5]: Loss formula with λ balancing CrossEntropy and Distillation losses
  - [Table 3]: PruneFuse with KD consistently outperforms without KD across budgets (e.g., 67.87% vs. 67.49% at b=50% on CIFAR-100)
  - [corpus]: Standard KD mechanism; corpus provides no additional validation for this specific application
- Break condition: If θ*_p has low accuracy, distillation may propagate erroneous soft labels.

## Foundational Learning

- Concept: Active Learning with Uncertainty Sampling
  - Why needed here: PruneFuse iteratively selects samples using uncertainty metrics; understanding LC, Entropy, and Greedy k-centers is essential for choosing the right selection strategy.
  - Quick check question: Given predictions [0.7, 0.2, 0.1] and [0.4, 0.35, 0.25], which would Least Confidence vs. Entropy prioritize?

- Concept: Structured Channel Pruning at Initialization
  - Why needed here: PruneFuse prunes channels before training to create the surrogate; structured pruning ensures weight alignment during fusion.
  - Quick check question: Why would unstructured (random weight) pruning prevent the fusion mechanism from working?

- Concept: Knowledge Distillation with Temperature Scaling
  - Why needed here: The refinement phase uses softened teacher logits; temperature controls the softness of probability distributions.
  - Quick check question: What happens to the soft label distribution when temperature T → ∞?

## Architecture Onboarding

- Component map:
  - θ: Original untrained dense network
  - θ_p: Pruned network via prune(θ, p)
  - θ*_p: Trained pruned model after data selection iterations
  - θ_F: Fused model = Fuse(θ, θ*_p)
  - θ*_F: Final model after KD fine-tuning

- Critical path:
  1. Prune at initialization → θ_p (Section 3.1)
  2. Train on initial labeled subset s₀ → θ*_p
  3. Iteratively: compute uncertainty scores → select top-k → annotate → retrain θ_p from scratch (Section 3.2-3.3)
  4. At budget b: Fuse θ*_p weights into θ → θ_F (Section 3.4)
  5. Fine-tune θ_F with KD from θ*_p → θ*_F (Section 3.5)

- Design tradeoffs:
  - p=0.5: Better selection quality, ~4× parameter reduction; p=0.8: Minimal compute, slightly lower accuracy
  - With KD: +0.2–0.5% accuracy gain; adds hyperparameter λ to tune
  - Retraining θ_p from scratch each iteration prevents overfitting but increases selection-phase compute

- Failure signatures:
  - Pruned model accuracy <60% during selection → likely poor subset quality
  - Large accuracy gap between selection metrics (e.g., Random matching or exceeding LC) → selection mechanism not learning
  - Fusion model diverges early → check weight alignment and learning rate

- First 3 experiments:
  1. Replicate Table 1 baseline: Run standard AL with ResNet-56 on CIFAR-10 at b=50% to establish your baseline before implementing PruneFuse.
  2. Ablation on pruning ratio: Test p∈{0.5, 0.7} measuring FLOPs vs. accuracy tradeoff; verify Figure 3 computational savings.
  3. Fusion ablation: Compare θ_F training trajectory against training θ from scratch on the same selected subset (replicate Figure 4 conditions).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does PruneFuse maintain its efficiency and performance advantages when applied to non-CNN architectures, such as Transformers or Large Language Models?
- **Basis in paper:** [inferred] The paper evaluates the method exclusively on image classification tasks using CNN architectures (ResNet-18/50/56/164), despite the Introduction noting deep learning's success in domains like NLP.
- **Why unresolved:** The pruning and fusion mechanisms described in Section 3.1 and 3.4 rely heavily on "channel pruning" and filter structures specific to CNNs; it is unclear how "structured pruning" and weight fusion would operate on attention mechanisms or embedding layers.
- **What evidence would resolve it:** Experimental results applying PruneFuse to Transformer-based architectures (e.g., ViT or BERT) and comparing the data selection quality against CNN baselines.

### Open Question 2
- **Question:** Is magnitude-based pruning at initialization optimal for the specific task of data selection, or would gradient-based sensitivity metrics yield a more effective surrogate model?
- **Basis in paper:** [inferred] Section 3.1 states the method employs magnitude-based scoring for channel pruning, acknowledging but not testing alternative initialization methods like SNIP [36] or GraSP.
- **Why unresolved:** The quality of the selected subset depends entirely on the surrogate model θ_p. If magnitude-based pruning removes important features early on, the surrogate may fail to identify informative samples in specific classes.
- **What evidence would resolve it:** An ablation study comparing the quality of subsets selected by surrogates pruned via magnitude versus those pruned via sensitivity-based criteria.

### Open Question 3
- **Question:** Does the specific retention of random weights in the unpruned portions of the fused model provide a distinct regularization benefit compared to standard fine-tuning approaches?
- **Basis in paper:** [inferred] Section 3.4 argues that retaining unaltered weights from θ offers an "element of randomness" enabling "richer exploration of the loss landscape," but does not isolate this variable to prove it causes the performance gain.
- **Why unresolved:** The performance improvement could stem solely from the optimized initialization of the pruned weights, while the "random" unpruned weights might simply slow convergence or act as noise.
- **What evidence would resolve it:** A controlled experiment where the fused model's unpruned weights are initialized deterministically (or zeroed) rather than left random, to observe the impact on final accuracy.

## Limitations

- The structural coherence assumption (pruned model ranking correlates with full model) remains theoretically underspecified with no formal bounds on selection quality degradation as pruning ratio increases
- The fusion mechanism's optimality is unproven; no comparison exists against alternative transfer methods like fine-tuning from pretrained weights
- The knowledge distillation component's contribution appears modest (+0.2-0.5% accuracy) and could be hyperparameter-sensitive

## Confidence

- High confidence: FLOPs reduction measurements and basic fusion mechanism functionality
- Medium confidence: Selection quality preservation at moderate pruning ratios (p≤0.7)
- Low confidence: Claims about structural coherence's general applicability to arbitrary architectures beyond ResNets

## Next Checks

1. Conduct ablation studies on pruning ratio p∈{0.3, 0.5, 0.7, 0.9} to quantify the exact accuracy-FLOPs tradeoff curve and identify the inflection point where selection quality degrades
2. Test the fusion mechanism against alternative initialization strategies (random, pretrained ImageNet weights, fine-tuning from compressed models) on CIFAR-100 to establish relative efficacy
3. Implement a theoretical analysis framework measuring the correlation between pruned and full model uncertainty scores across different pruning strategies and architectures