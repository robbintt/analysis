---
ver: rpa2
title: 'Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning'
arxiv_id: '2510.10103'
source_url: https://arxiv.org/abs/2510.10103
tags:
- reasoning
- arxiv
- tokens
- refrain
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REFRAIN introduces a training-free early-stopping method for Chain-of-Thought
  reasoning that reduces overthinking by detecting reflective yet redundant steps
  and dynamically adjusting stop thresholds via a sliding-window UCB bandit. It integrates
  a two-stage stop discriminator to identify when reasoning shifts from reflective
  self-correction to redundant repetition, and a bandit controller to tune stopping
  thresholds based on task difficulty.
---

# Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning

## Quick Facts
- **arXiv ID:** 2510.10103
- **Source URL:** https://arxiv.org/abs/2510.10103
- **Reference count:** 21
- **Key outcome:** REFRAIN reduces tokens by 20-55% while maintaining or improving accuracy via training-free adaptive early-stopping for CoT reasoning.

## Executive Summary
REFRAIN introduces a training-free early-stopping method for Chain-of-Thought reasoning that reduces overthinking by detecting reflective yet redundant steps and dynamically adjusting stop thresholds via a sliding-window UCB bandit. It integrates a two-stage stop discriminator to identify when reasoning shifts from reflective self-correction to redundant repetition, and a bandit controller to tune stopping thresholds based on task difficulty. Across four benchmarks and two model families, REFRAIN achieves 20-55% fewer tokens while maintaining or improving accuracy, outperforming existing inference-time stopping methods. The results establish "when-to-stop" as a practical axis of test-time scaling for efficient and reliable reasoning.

## Method Summary
REFRAIN combines a two-stage stop discriminator with a sliding-window UCB bandit to dynamically determine when to stop CoT reasoning. The discriminator triggers when (1) a step contains reflection vocabulary (self-check, strategy shift, uncertainty, retrospective) and (2) semantic similarity to prior steps exceeds threshold τ, and (3) a provisional answer cue exists. The bandit maintains reward buffers for candidate thresholds and selects via UCB formula, adapting to task difficulty. Reward is based on length-normalized likelihood of the boxed final answer minus a token penalty. The method is training-free and requires only streaming access to intermediate reasoning steps.

## Key Results
- Reduces tokens by 20-55% across four benchmarks while maintaining or improving accuracy
- Outperforms fixed threshold and simple MAB baselines on both Qwen3-8B and gpt-oss-20B models
- Answer-only likelihood reward performs comparably to external reward models while avoiding additional forward passes

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Reflective Redundancy Detection
- Claim: Identifying when reasoning transitions from useful self-correction to redundant repetition enables early stopping without accuracy loss.
- Mechanism: A discriminator triggers only when (i) a step contains reflection trigger vocabulary (self-check, strategy shift, uncertainty, retrospective), AND (ii) semantic similarity to prior steps exceeds threshold τ, AND (iii) a provisional answer cue exists in history.
- Core assumption: Reflective steps that are semantically redundant indicate the model has reached reasoning closure; further steps add noise rather than signal.
- Evidence anchors:
  - [abstract] "integrates a two-stage stop discriminator to identify when reasoning shifts from reflective self-correction to redundant repetition"
  - [Section 3.2, Eq. 5] STOP(Q, s₁:n) = I(hₙ₋₁=1)·I(rₙ=1)·I(φₙ≥τ)
  - [corpus] Related work "Answer Convergence as a Signal for Early Stopping" similarly hypothesizes many reasoning steps are unnecessary, supporting redundancy-based stopping as a viable signal.
- Break condition: If reflection vocabulary is incomplete or semantic embeddings fail to capture paraphrased redundancy, the discriminator may miss stopping opportunities or trigger prematurely.

### Mechanism 2: Sliding-Window UCB for Adaptive Threshold Selection
- Claim: A multi-armed bandit controller dynamically adjusts stopping thresholds based on task difficulty without supervision.
- Mechanism: Maintains reward buffers for candidate thresholds τ ∈ T. Selects arm via UCB(t) = R̄(t) + C√(2log t_eff/n(t)), where reward combines answer likelihood and token efficiency. Sliding window adapts to non-stationary difficulty.
- Core assumption: Optimal stopping threshold varies by problem; online exploration-exploitation balance finds task-appropriate thresholds faster than fixed heuristics.
- Evidence anchors:
  - [abstract] "dynamically adjusting stop thresholds via a sliding-window UCB bandit"
  - [Section 3.4, Eq. 8] Explicit UCB formula with exploration constant C
  - [Section 5.6] SW-UCB outperforms ε-greedy MAB on CSQA and GPQA-Diamond where "question semantics fluctuate more," showing better adaptation to non-stationary reward dynamics.
  - [corpus] No direct corpus evidence for SW-UCB specifically; related work focuses on confidence/entropy signals rather than bandit-based threshold adaptation.
- Break condition: If reward signal is noisy or exploration constant C is mis-specified, bandit may converge to suboptimal thresholds; cold-start period may cause erratic early behavior.

### Mechanism 3: Answer-Only Likelihood as Training-Free Reward
- Claim: Length-normalized geometric mean likelihood of the boxed final answer provides a sufficient reward signal without external reward models.
- Mechanism: After stopping, force model to output "Final Answer: \boxed{...}". Compute Score(y|x) = exp(1/|y| Σ log p_θ(y_i|x, y_<i)). Reward R = Score(y|x) - λL/L̄ balances confidence vs efficiency.
- Core assumption: Answer likelihood correlates with correctness; token penalty guides bandit toward earlier stopping when confidence is high.
- Evidence anchors:
  - [Section 3.3, Eq. 6] Explicit likelihood formula
  - [Section 5.4, Table 4] Likelihood matches PRM performance on MATH-500 (91.20 vs 91.60 Pass@1) while avoiding additional forward passes required by AceMath-7B and Qwen2.5-MATH-PRM-7B.
  - [corpus] "Knowing Before Saying" finds LLM representations encode reasoning success information pre-generation, indirectly supporting likelihood-based signals as informative.
- Break condition: If answer format is malformed or model assigns high likelihood to confident-but-incorrect answers, reward misleads bandit toward bad thresholds.

## Foundational Learning

- **Multi-Armed Bandits & UCB**
  - Why needed here: REFRAIN treats threshold selection as a bandit problem; understanding exploration-exploitation tradeoffs is essential to diagnose why SW-UCB adapts better than fixed or random thresholds.
  - Quick check question: Given arms with rewards [0.7, 0.8, 0.6] after [10, 5, 15] pulls, which arm does UCB favor with C=1 and t_eff=30?

- **Sentence Embeddings & Semantic Similarity**
  - Why needed here: The redundancy scorer uses cosine similarity between step embeddings; understanding embedding spaces explains why surface metrics (TF-IDF, ROUGE-L) underperform.
  - Quick check question: Why might "let me reconsider this approach" and "I should try a different method" have high cosine similarity but low token overlap?

- **Chain-of-Thought Reasoning Patterns**
  - Why needed here: The reflection vocabulary assumes specific CoT behaviors (self-check, strategy shift); recognizing these patterns helps validate or extend the trigger lexicon.
  - Quick check question: In a reasoning trace, what distinguishes a "strategy shift" from a "retrospective revision"?

## Architecture Onboarding

- **Component map:** Question Q → LLM generates step s_n → Reflection Detector (self-check, shift, uncertainty, retro) → Provisional Answer Check ("answer is/should be") → Redundancy Scorer (all-MiniLM-L6-v2, cosine similarity φ_n) → Stop Decision (φ_n ≥ τ?) → Forced Closure Prompt → Final Answer y → Likelihood Score → Reward (Score - λL/L̄) → SW-UCB Bandit → Update threshold τ

- **Critical path:** Reflection detection → provisional answer check → redundancy scoring → stop decision → answer extraction → reward computation → bandit threshold update. The stop discriminator (Alg. 1, lines 5-9) is the core decision point.

- **Design tradeoffs:**
  - **Embedding encoder choice:** SBERT vs SimCSE (Table 9 shows comparable performance); heavier encoders increase latency per step.
  - **Threshold discretization:** Paper uses τ ∈ [0.60, 0.80] step 0.05; finer granularity may improve adaptation but increases bandit arm count and exploration time.
  - **Window size W:** Smaller windows adapt faster but with noisier estimates; paper does not specify W explicitly.
  - **λ penalty coefficient:** Higher λ prioritizes efficiency over accuracy; requires tuning per deployment context.

- **Failure signatures:**
  - **Premature stopping:** High τ + high similarity early → cuts off necessary self-correction (see Table 10, MATH angle problem: stops at 62° instead of correct 28°).
  - **Over-extension:** Low τ + low similarity → continues past convergence, risking semantic drift (Table 10, CSQA glue sticks: baseline drifts from "office" to "classroom").
  - **Cold-start instability:** First samples use arbitrary threshold until bandit accumulates rewards; initial behavior may be inconsistent.
  - **Format parsing failure:** If model doesn't produce "\boxed{...}", answer extraction and likelihood computation fail.

- **First 3 experiments:**
  1. **Validate discriminator in isolation:** Run REFRAIN with fixed τ=0.70 on MATH-500, log stop step indices and manual inspection of whether stopped traces are genuinely redundant vs. prematurely truncated. Compare Pass@1 vs vanilla.
  2. **Ablate bandit vs fixed threshold:** Compare REFRAIN (SW-UCB) against fixed τ ∈ {0.60, 0.70, 0.80} on GPQA-Diamond (high difficulty variance). Measure Pass@1, #Tokens, and threshold selection distribution over time.
  3. **Stress-test reward signal:** Replace likelihood reward with external RM (AceMath-7B) and PRM (Qwen2.5-MATH-PRM-7B) on GSM8K. Verify Table 4 finding that likelihood is sufficient; measure added forward-pass latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can early-stopping mechanisms be modified to distinguish between harmful overthinking and necessary late-stage self-correction?
- Basis in paper: [explicit] The case study notes that early stopping "can also truncate necessary self-corrections" and "harms when correctness emerges only after longer deliberation," identifying this as an inherent precision-completeness trade-off.
- Why unresolved: The current reflect-redundancy signal cannot differentiate redundant loops from genuine late repairs that follow similar lexical patterns.
- What evidence would resolve it: A method that tracks reasoning trajectory dynamics (e.g., uncertainty trajectories, answer stability patterns) to detect when self-correction is converging vs. stuck in repetition.

### Open Question 2
- Question: Can REFRAIN's training-free adaptive stopping be extended to closed-source API models that do not expose intermediate reasoning traces?
- Basis in paper: [explicit] The limitations section states: "For closed-source APIs that only return final answers without providing step-by-step control, REFRAIN, like other similar methods, cannot intervene during the generation process."
- Why unresolved: The two-stage discriminator requires step-level embeddings and trigger-word detection, which necessitates streaming access to intermediate tokens.
- What evidence would resolve it: Demonstrating a variant that uses only final-answer signals (e.g., prompting with varying chain-length instructions, or API-level token probability access) to achieve comparable adaptive stopping.

### Open Question 3
- Question: How well does the hand-curated reflection trigger vocabulary generalize across languages, domains (e.g., code, legal reasoning), and non-reasoning-focused LLMs?
- Basis in paper: [inferred] The vocabulary was derived from English math/commonsense reasoning studies and validated on Qwen3 and gpt-oss. No experiments test multilingual settings, code generation, or models not explicitly trained for reasoning.
- Why unresolved: Trigger phrases like "wait, let me check" are English-specific and reasoning-domain-specific; their transferability is unknown.
- What evidence would resolve it: Cross-lingual benchmark results (e.g., MGSM), code-reasoning tasks, and evaluation on general-purpose LLMs without explicit reasoning fine-tuning.

## Limitations

- Reflection vocabulary may not capture all reflective reasoning patterns across domains, potentially missing stopping opportunities or triggering prematurely
- SBERT embedding similarity threshold (τ ∈ [0.60, 0.80]) appears task-dependent but lacks principled derivation
- Method requires streaming access to intermediate reasoning steps, making it incompatible with closed-source API models

## Confidence

**High Confidence:**
- REFRAIN reduces token count by 20-55% across benchmarks while maintaining or improving accuracy
- The two-stage stop discriminator architecture (reflection + redundancy detection) is technically sound
- Semantic similarity outperforms surface metrics (TF-IDF, ROUGE-L) for redundancy detection

**Medium Confidence:**
- SW-UCB bandit adapts better to task difficulty than fixed thresholds or simple MAB methods
- Answer likelihood alone provides sufficient reward signal without external reward models
- The method generalizes across model families (Qwen3-8B, gpt-oss-20B) and task types

**Low Confidence:**
- Specific hyperparameter choices (τ values, vocabulary completeness, bandit parameters) are optimal
- Performance would scale similarly to larger models (>20B parameters) or different reasoning domains
- The 20-55% token reduction is achievable on all CoT reasoning tasks, not just the four tested benchmarks

## Next Checks

1. **Cross-Domain Robustness Test**: Apply REFRAIN to non-mathematical reasoning tasks like code debugging or legal reasoning where iterative refinement is common. Measure whether the method correctly identifies when additional steps are genuinely necessary versus redundant, and whether accuracy degrades on tasks requiring deeper exploration.

2. **Ablation of Critical Components**: Systematically disable each major component (reflection detector, provisional answer check, semantic scorer, SW-UCB) and measure the impact on both token reduction and accuracy. This would reveal which components are essential versus complementary, and quantify the contribution of adaptive threshold selection versus the stopping criteria.

3. **Longitudinal Bandit Behavior Analysis**: Track the SW-UCB's threshold selection distribution over time on a single benchmark, measuring (a) convergence speed to optimal threshold, (b) stability across different problem subsets, and (c) sensitivity to the unknown hyperparameters (W, C, λ). This would validate whether the claimed adaptive behavior is robust or hyperparameter-dependent.