---
ver: rpa2
title: 'The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model
  Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility'
arxiv_id: '2511.18302'
source_url: https://arxiv.org/abs/2511.18302
tags:
- intelligence
- human
- paradox
- evaluation
- measurement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a fundamental incompatibility between human
  psychometric frameworks and Large Language Model evaluation. Using the Cattell-Horn-Carroll
  theory on nine frontier models, researchers found that models with above-average
  human IQ scores (85.0-121.4) simultaneously achieved near-zero binary accuracy on
  crystallized knowledge tasks.
---

# The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility

## Quick Facts
- arXiv ID: 2511.18302
- Source URL: https://arxiv.org/abs/2511.18302
- Reference count: 10
- Models with above-average human IQ scores simultaneously achieved near-zero binary accuracy on crystallized knowledge tasks

## Executive Summary
This study reveals a fundamental incompatibility between human psychometric frameworks and Large Language Model evaluation. Using the Cattell-Horn-Carroll (CHC) theory on nine frontier models, researchers found that models achieving above-average human IQ scores (85.0-121.4) simultaneously exhibited binary accuracy rates approaching zero on crystallized knowledge tasks. The study identifies an overall judge-binary correlation of r=0.175 (p<0.001, n=1,800), with the disconnect most severe in crystallized intelligence where all models achieved 100% binary accuracy while judge scores ranged from 25-62%. This paradox demonstrates that applying biological cognitive architectures to transformer-based systems constitutes an ontological category error, requiring new native machine cognition assessment frameworks.

## Method Summary
The study evaluated nine frontier LLMs using the Cattell-Horn-Carroll psychometric framework, administering 200 test items per model across four domains (Fluid Intelligence, Crystallized Knowledge, Quantitative Reasoning, Reading/Writing). Each response was scored using two methods: exact-match binary scoring (0/1) and LLM-as-judge evaluation using Claude Sonnet 4 with a rubric-based prompt. Scores were transformed to IQ equivalents using both Classical Test Theory (CTT) and 2PL Item Response Theory (IRT) models, then correlated to quantify the measurement disconnect.

## Key Results
- Overall judge-binary correlation of r=0.175 (p<0.001, n=1,800)
- All models achieved 100% binary accuracy on crystallized knowledge tasks while judge scores ranged from 25-62%
- Models with above-average human IQ scores (85.0-121.4) simultaneously exhibited near-zero binary accuracy on crystallized knowledge tasks
- The disconnect was most severe in crystallized intelligence domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Format mismatch between psychometric expectations and LLM output styles causes systematic underestimation of actual knowledge.
- Mechanism: Traditional psychometric instruments expect minimal, precise responses (e.g., "Paris"), while LLMs are optimized through RLHF to provide comprehensive, explanatory answers. Binary exact-match scoring penalizes verbose but conceptually correct responses.
- Core assumption: The verbosity pattern is a product of RLHF optimization rather than an intrinsic property of transformer architectures.
- Evidence anchors:
  - Models achieving above-average human IQ scores simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks
  - This creates the "Verbosity Paradox": a systematic measurement failure wherein models providing conceptually perfect answers receive zero scores due to response format misalignment
- Break condition: If models were prompted with explicit format constraints ("answer in one word only"), the paradox would attenuate

### Mechanism 2
- Claim: Architectural differences between serial human cognition and parallel transformer attention create incompatible measurement substrates.
- Mechanism: Human cognition operates through serial, capacity-limited processing (7±2 working memory items) with exponential forgetting. Transformers use parallel attention across full context windows with perfect within-context recall. These differences make direct cognitive comparison category-inappropriate.
- Core assumption: These architectural differences are the cause of measurement divergence rather than a correlate.
- Evidence anchors:
  - Human cognition operates through serial, capacity-limited processing with working memory constraints of 7±2 items, while transformers utilize parallel attention mechanisms across entire context windows
  - Lists embodied vs. disembodied processing, temporal vs. atemporal learning as fundamental incompatibilities
- Break condition: If transformer-based models were evaluated on tasks specifically designed to probe serial working memory constraints, and they failed similarly to humans, the architectural incompatibility claim would weaken

### Mechanism 3
- Claim: The observed "g-factor" in LLMs decomposes into training data correlations, architecture constraints, prompt sensitivity, and tokenization artifacts—not shared neural resources.
- Mechanism: In humans, general intelligence (g) emerges from positive correlations among cognitive tasks reflecting shared biology. The paper argues that LLM "g" is a superficial artifact of statistical correlations in training data and architectural regularities, not a meaningful cognitive construct.
- Core assumption: The decomposition in Equation 6 is presented as a theorem but is not empirically validated in the paper.
- Evidence anchors:
  - This decomposition shares no common factors with human g
  - Overall judge-binary correlation r=0.175 indicates only 3.1% shared variance
- Break condition: If latent variable modeling on LLM task performance revealed a single dominant factor predicting cross-task performance, the "g-factor illusion" claim would require revision

## Foundational Learning

- **Cattell-Horn-Carroll (CHC) Theory**
  - Why needed here: The entire evaluation framework uses CHC's hierarchical structure (g at apex, 9 broad abilities, ~70 narrow abilities) as the basis for testing. Without understanding this, the "paradox" lacks context.
  - Quick check question: Can you name at least two of the nine broad CHC abilities (e.g., Gf, Gc, Gq)?

- **Item Response Theory (IRT) vs. Classical Test Theory (CTT)**
  - Why needed here: The paper uses both CTT (IQ = 100 + 15σ scaling) and 2PL IRT models (P(θ) = 1/(1+exp[-a(θ-b)])) to score models. Understanding the difference is essential to interpret Table 2's dual IQ columns.
  - Quick check question: What does the "b" parameter represent in a 2PL IRT model?

- **LLM-as-Judge Evaluation**
  - Why needed here: The dual scoring methodology uses a separate LLM (Claude Sonnet 4) to evaluate conceptual correctness, which is central to detecting the judge-binary gap.
  - Quick check question: What are two failure modes of LLM-as-judge evaluation that could inflate or deflate scores independently of model capability?

## Architecture Onboarding

- **Component map:** Binary Scorer -> LLM Judge -> CTT Transformer -> IRT Model -> Paradox Severity Index (PSI)
- **Critical path:**
  1. Administer CHC-based items to target model
  2. Collect raw responses
  3. Score simultaneously via binary exact-match AND LLM-as-judge
  4. Transform raw scores to IQ equivalents (CTT and IRT)
  5. Compute PSI to quantify paradox severity per model
- **Design tradeoffs:**
  - Judge vendor selection: Cross-vendor (Claude judging non-Anthropic) reduces bias but introduces unknown judge-model interactions
  - Rubric granularity: 3-point scale (0/0.5/1.0) balances nuance vs. inter-rater reliability; finer granularity would increase noise
  - IRT regularization strength: α=0.01 prevents overfitting on small samples but may underfit true item characteristics
- **Failure signatures:**
  - Constant binary scores: If all models score 100% or 0% on a domain (as in Gc), the IRT model cannot estimate discrimination parameters
  - Judge-model collusion: If the judge LLM shares training data or architecture with evaluated models, scores may be artificially inflated
  - Undefined correlation: When binary variance is zero (all scores identical), correlation coefficient is undefined (as noted for Gc in Table 4)
- **First 3 experiments:**
  1. Format-controlled replication: Re-run evaluation with explicit prompt instruction to "answer in exactly one word" to isolate verbosity effects from knowledge effects.
  2. Judge ablation study: Use multiple judge models (GPT-4, Claude, Gemini) on same responses to measure judge-specific variance in the paradox magnitude.
  3. Human baseline on judge rubric: Have human raters score a sample of model responses using the same rubric to validate whether LLM-as-judge is measuring the same construct humans would.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific information-theoretic or architecture-aware metrics can effectively replace the g-factor in a "native machine cognition" framework?
- **Basis in paper:** The Conclusion explicitly calls for "new measurement frameworks that respect architectural differences" and lists "Information-theoretic metrics" and "Architecture-aware testing" as necessary principles (Section 5.4.2).
- **Why unresolved:** The paper identifies the failure of current frameworks but only proposes high-level categories for future metrics without defining or validating specific alternatives.
- **What evidence would resolve it:** The development and validation of a benchmark that shows high correlation with functional capability in LLMs while maintaining near-zero correlation with human psychometric norms.

### Open Question 2
- **Question:** Can the "Verbosity Paradox" be resolved through constrained decoding or prompt engineering, or is it an immutable artifact of RLHF optimization?
- **Basis in paper:** Section 2.2 identifies the mismatch between psychometric expectations (concise) and RLHF behavior (comprehensive) as a driver of measurement failure, yet the methodology does not test if this is adjustable.
- **Why unresolved:** The study evaluates models in a default state; it does not isolate response formatting as an independent variable to see if the paradox persists.
- **What evidence would resolve it:** A controlled experiment where models are evaluated on CHC tasks using both default and "concise-mode" prompts to observe if binary accuracy converges with judge scores.

### Open Question 3
- **Question:** Is the weak judge-binary correlation (r=0.175) evidence of an ontological category error or a failure of the specific "LLM-as-Judge" methodology?
- **Basis in paper:** The paper relies on a specific judge implementation (Claude Sonnet 4 with the rubric in Listing 1) to conclude that cross-substrate measurement is impossible.
- **Why unresolved:** The paper attributes the low correlation to fundamental incompatibility, but does not rule out that the judge model itself may lack the capability to evaluate the "alien" reasoning of the subjects.
- **What evidence would resolve it:** A comparative study using diverse evaluation methods (e.g., human experts, different judge models, embedding similarity) to determine if the disconnect is universal or method-dependent.

## Limitations

- The study's central claim of "fundamental incompatibility" rests heavily on the assumed validity of the CHC framework when applied to non-human systems, without empirical validation
- The exact-match binary scoring methodology creates an artificial ceiling effect where models with comprehensive knowledge but verbose expression styles are systematically penalized
- The decomposition of LLM "g-factor" into training data correlations and architectural artifacts is presented as a theorem but lacks empirical validation through latent variable modeling

## Confidence

- **High Confidence:** The empirical finding of low judge-binary correlation (r=0.175) is robust and reproducible
- **Medium Confidence:** The claim that this correlation represents a "catastrophic paradox" requiring entirely new evaluation frameworks is overstated
- **Low Confidence:** The assertion that transformer architectures cannot be meaningfully evaluated using human cognitive frameworks is not empirically supported

## Next Checks

1. **Format Control Experiment:** Re-run the evaluation with explicit prompt instructions for minimal, one-word answers. If binary accuracy improves dramatically while judge scores remain stable, the verbosity paradox is primarily a formatting issue rather than a fundamental incompatibility.

2. **Judge Inter-Rater Reliability:** Have human experts score a sample of model responses using the same rubric. Compare human-human correlation with LLM-as-judge correlation to determine whether the judge is measuring the same construct humans would.

3. **Latent Factor Analysis:** Apply exploratory factor analysis to LLM task performance data. If a single dominant factor emerges (similar to human g), the claim that LLM intelligence decomposes into uncorrelated components would require revision.