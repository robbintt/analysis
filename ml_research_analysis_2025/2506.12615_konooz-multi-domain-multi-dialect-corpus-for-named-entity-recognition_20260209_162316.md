---
ver: rpa2
title: 'Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition'
arxiv_id: '2506.12615'
source_url: https://arxiv.org/abs/2506.12615
tags:
- dialects
- konooz
- domains
- arabic
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Konooz, a novel multi-dimensional corpus
  for Arabic NER covering 16 dialects across 10 domains (777k tokens, 21 entity types).
  The corpus was manually collected and annotated using Wojood guidelines.
---

# Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition

## Quick Facts
- arXiv ID: 2506.12615
- Source URL: https://arxiv.org/abs/2506.12615
- Reference count: 27
- Primary result: Introduces Konooz, a 777k-token Arabic NER corpus covering 16 dialects across 10 domains with 21 entity types, revealing up to 38% performance drops in cross-domain/dialect scenarios

## Executive Summary
This paper introduces Konooz, a novel multi-dimensional corpus for Arabic Named Entity Recognition covering 16 dialects across 10 domains. The corpus was manually collected and annotated using Wojood guidelines, totaling 777k tokens and 21 entity types. Benchmarking four Arabic NER models on Konooz revealed significant performance drops (up to 38%) in cross-domain and cross-dialect scenarios compared to in-distribution data. The analysis also revealed high inter-domain divergence using Maximum Mean Discrepancy (MMD) scores, highlighting the challenges of Arabic NER across dialects and domains.

## Method Summary
The Konooz corpus was constructed through manual collection of Arabic text from social media (dialects) and news websites (MSA), then annotated using a multi-phase process starting with a Wojood NER model for tokenization, followed by manual review by linguists, and a second model-assisted pass. The corpus comprises 160 corpora (16 dialects × 10 domains) with approximately 4k tokens each. Benchmarking used four models (WojoodNested, WojoodFlat, OntoNotes, ANERCorp) fine-tuned on AraBERTv2 with specific hyperparameters. Lexical similarity was analyzed using MMD computed on AraBERTv2 sentence embeddings.

## Key Results
- Cross-domain and cross-dialect performance drops up to 38% compared to in-distribution evaluation
- Maximum Mean Discrepancy scores revealed substantial divergence between MSA and dialects (1.5-36), with Moroccan dialect being most dissimilar
- High inter-domain divergence (MMD up to 13) across all dialects
- Models trained on MSA struggled to generalize to dialectal Arabic without robust domain adaptation techniques

## Why This Works (Mechanism)
The paper's key insight is that lexical divergence measured by MMD correlates with NER performance degradation across dialects and domains. The manual annotation process ensures high-quality labels, while the multi-dimensional coverage (16 dialects × 10 domains) creates a comprehensive benchmark for evaluating model generalization. The Wojood annotation guidelines provide consistent entity type definitions across all corpora.

## Foundational Learning
- **Maximum Mean Discrepancy (MMD)**: A kernel-based method for measuring distribution divergence between datasets; needed for quantifying lexical similarity between dialects/domains; quick check: MMD score >20 indicates severe distribution shift
- **Nested vs. Flat NER**: Nested NER allows overlapping entity mentions while flat NER does not; needed because Arabic entities can have complex structures; quick check: compare F1 scores between nested and flat models
- **Domain Adaptation**: Techniques to improve model performance on out-of-distribution data; needed because models trained on MSA fail on dialects; quick check: benchmark adaptation methods on Konooz
- **Cohen's Kappa**: Inter-annotator agreement metric; needed to validate annotation quality; quick check: Kappa >0.8 indicates strong agreement
- **Entity Type Mapping**: Converting between different NER tag schemas; needed for benchmarking diverse datasets; quick check: ensure mapping preserves entity semantics
- **HuggingFace Transformers**: Framework for fine-tuning pre-trained models; needed for benchmarking Wojood, OntoNotes, and ANERCorp models; quick check: verify model loading and tokenization

## Architecture Onboarding
- Component map: **Data Collection (Social Media + News) -> Preprocessing (Tokenization + Categorization) -> Annotation (Model-Assisted + Manual Review) -> Analysis (MMD + Benchmarking)**
- Critical path: Konooz Corpus Construction -> Lexical Similarity Analysis (MMD) and Konooz Corpus Construction -> Cross-dialect/domain Benchmarking
- Design tradeoffs:
  - Manual collection ensures quality but is time-consuming and doesn't scale
  - 21-entity-type schema allows fine-grained analysis but complicates comparison with simpler schemas
  - MMD kernel choice influences divergence scores
- Failure signatures:
  - Cascading annotation errors from initial model tokenization
  - Non-native annotator bias introducing inconsistencies
  - Correlation-causation gap between MMD scores and performance
- First 3 experiments:
  1. Train a single NER model on entire Konooz corpus and evaluate generalization across dialects
  2. Use MMD scores to predict performance on held-out dialect-domain pairs
  3. Perform entity-level error analysis for worst-performing dialects

## Open Questions the Paper Calls Out
- Can robust domain adaptation techniques effectively bridge the performance gap caused by high lexical and structural divergence between MSA and dialectal Arabic?
- How does the choice of pre-trained language model (e.g., CamelBERT, LLMs) impact the measurement of dialectal divergence and downstream NER performance compared to AraBERTv2?
- To what extent does named entity coverage in training data outweigh general lexical similarity in determining cross-dialect NER success?

## Limitations
- Corpus size (777k tokens) may be insufficient for robust training, particularly for low-resource dialects like Mauritanian (3k tokens)
- Manual annotation relied on non-native speakers for some dialects, introducing potential inconsistencies
- MMD-based similarity analysis may oversimplify complex relationships between dialects

## Confidence
- **High Confidence**: Dataset construction methodology and benchmarking framework are sound; performance drops are clearly demonstrated
- **Medium Confidence**: MMD-based similarity analysis provides useful insights but may not capture full semantic relationships
- **Medium Confidence**: Entity type mapping process may introduce information loss, though acknowledged in methodology

## Next Checks
1. Conduct regression analysis to determine if MMD scores are statistically significant predictors of NER performance, controlling for domain and dialect factors
2. Analyze confusion matrices for worst-performing dialect-domain pairs to determine whether errors are primarily recall or precision issues
3. Recruit native speakers for most divergent dialects (Moroccan, Mauritanian) to manually verify annotation quality