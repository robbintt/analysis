---
ver: rpa2
title: LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns
arxiv_id: '2503.10248'
source_url: https://arxiv.org/abs/2503.10248
tags:
- llms
- humans
- choice
- rare
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared decision-making patterns of LLM agents and humans
  in Decisions from Experience (DFE) tasks, focusing on phenomena like underweighting
  rare events, correlation effects, and learning patterns. LLM agents (GPT-4o mini
  and Gemini-1.5 Flash) and humans both showed similar aggregate biases, such as underweighting
  rare events and responding to correlations.
---

# LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns

## Quick Facts
- arXiv ID: 2503.10248
- Source URL: https://arxiv.org/abs/2503.10248
- Reference count: 9
- Primary result: LLM agents exhibit human-like aggregate biases but lack nuanced human decision phenomena

## Executive Summary
This study compares decision-making patterns between LLM agents (GPT-4o mini and Gemini-1.5 Flash) and humans in Decisions from Experience (DFE) tasks. Both agents and humans display similar aggregate biases like underweighting rare events and responding to correlations, but LLMs exhibit dramatically stronger recency biases, responding primarily to the most recent outcome rather than considering broader historical patterns. Crucially, LLMs lack human-specific phenomena such as "surprise triggers change" and "wavy recency effects," revealing fundamentally different underlying decision processes despite surface-level behavioral similarities.

## Method Summary
The study employed 40 agents per model-context combination across four DFE tasks with binary choices over 100 trials each. LLM agents were tested using GPT-4o mini and Gemini-1.5 Flash through API calls with temperature settings of 0.2, 1, and 2 (reporting temp=1 results). Two context paradigms were used: "chat" (conversation history access) and "all" (explicit history in each prompt). Tasks included rare event scenarios and correlated outcome conditions. Human baselines came from Prolific participants. Analysis focused on aggregate choice rates, sequential dependencies, and strategy matching against theoretical players.

## Key Results
- LLM agents and humans both underweight rare events and show correlation effects, matching aggregate choice rates
- LLMs display extreme recency bias, matching the "recency player" in 78%-91% of decisions
- Humans exhibit "surprise triggers change" and "wavy recency effects" that are entirely absent in LLMs

## Why This Works (Mechanism)

### Mechanism 1: Aggregate Bias Alignment via Recency-Driven Underweighting
- **Claim:** LLMs and humans produce similar aggregate choice rates despite fundamentally different sequential processes
- **Mechanism:** LLM recency bias causes underweighting of rare events on average since rare events rarely appear in recent outcomes
- **Evidence:** LLMs match recency player choices in 78%-91% of decisions
- **Break condition:** Providing explicit aggregated historical summaries in prompts should diminish recency bias

### Mechanism 2: Absence of Nuanced Human Phenomena Due to Lack of Expectation-Based Processing
- **Claim:** LLMs lack "surprise triggers change" and "wavy recency effects" due to absent expectation formation
- **Mechanism:** These phenomena require expectation violation processing that LLMs lack in this context
- **Evidence:** None of the LLM models exhibited counter-reinforcement behavior seen in humans
- **Break condition:** Explicit outcome prediction prompts might elicit pseudo-expectations

### Mechanism 3: Context Paradigm Modulation of Recency Bias
- **Claim:** Full history prompts ("all" paradigm) slightly mitigate recency bias compared to implicit history ("chat")
- **Mechanism:** Autoregressive attention still prioritizes recent tokens even with full context
- **Evidence:** "All" paradigm shows slightly less extreme recency bias than "chat" paradigm
- **Break condition:** Pre-computed summary statistics should override recency bias

## Foundational Learning

- **Concept:** Decisions from Experience (DFE) vs. Decisions from Description (DFD)
  - **Why needed here:** Understanding DFE domain is critical for interpreting underweighting of rare events
  - **Quick check question:** Would LLMs exhibit different choice patterns if the task were described rather than experienced?

- **Concept:** Recency Bias vs. Fictitious Play
  - **Why needed here:** Essential for analyzing choice strategy results comparing LLM behavior to theoretical strategies
  - **Quick check question:** In stable environment with 70% Option A payoffs, what would a Recency Player's choice proportion approach over 100 trials?

- **Concept:** Wavy Recency Effect of Rare Events
  - **Why needed here:** Key human-specific phenomenon requiring understanding of temporal patterns
  - **Quick check question:** If rare event occurs on Trial 50, on which subsequent trials would human be most/least likely to choose risky option?

## Architecture Onboarding

- **Component Map:** Task Environment -> Agent Interface -> Context Manager -> Evaluation Loop -> Human Baseline -> Analysis Module
- **Critical Path:** Implement "chat" paradigm for GPT-4o mini on Task 1, run 20 agents, plot risky choice rate after rare event to reveal recency decay pattern
- **Design Tradeoffs:** (1) Model selection: cost-effective mid-tier models vs. potential behavioral alignment; (2) Context paradigm: ecological validity vs. controlled information access; (3) Task duration: statistical robustness vs. resource expenditure
- **Failure Signatures:** High invalid response rates (>5%), no learning improvement in Task 3, flat recency plots indicating outcome ignoring
- **First 3 Experiments:**
  1. Baseline Recency Test: Run Task 1 with GPT-4o mini in "chat" paradigm, plot choice rate for 20 trials after rare event
  2. Context Ablation: Repeat Task 1 in "all" paradigm, compare recency decay rate
  3. Temperature Sweep: Run Task 1 at temperatures 0.2, 1.0, 2.0, analyze variance and strategy matching

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would LLMs detect sequential patterns in DFE tasks where true patterns exist?
- **Basis:** Proposed comparison of humans vs. LLMs in DFE tasks with alternating outcomes
- **Evidence needed:** Run structured outcome sequences and compare human vs. LLM pattern detection

### Open Question 2
- **Question:** Can fine-tuning LLMs on experience-related decision tasks produce human-comparable choice patterns?
- **Basis:** Discussion of fine-tuning producing more human-like patterns
- **Evidence needed:** Fine-tune LLMs on human DFE data, re-evaluate for wavy recency effects

### Open Question 3
- **Question:** Would persona simulation or human-prediction prompts alter LLM decision patterns?
- **Basis:** Discussion of persona portrayal and human prediction methods
- **Evidence needed:** Compare baseline choices against persona-prompted choices in same DFE tasks

### Open Question 4
- **Question:** What mechanisms drive extreme recency bias and can interventions mitigate it?
- **Basis:** Paper identifies recency bias but doesn't investigate causes or remedies
- **Evidence needed:** Test modified prompting strategies, compare across model architectures, analyze attention patterns

## Limitations
- Focus on specific DFE tasks may not generalize to broader decision contexts
- Only mid-tier models tested (GPT-4o mini, Gemini-1.5 Flash); larger models may differ
- Temperature sweep found minimal impact but only temp=1 results reported

## Confidence
- **High Confidence:** LLMs display strong recency bias and match aggregate human biases despite different processing mechanisms
- **Medium Confidence:** LLMs lack human-specific phenomena due to absent expectation-based processing
- **Medium Confidence:** Context paradigm modulates recency bias strength but doesn't eliminate it

## Next Checks
1. **Expectation Induction Test:** Prompt GPT-4o mini to explicitly predict next outcome before each choice in Task 1 to test for surprise-like responses
2. **Summary Statistics Override:** Modify prompts with pre-computed running averages to test if LLMs shift toward "fictitious player" behavior
3. **Larger Model Comparison:** Run same protocol with GPT-4 or Claude 3.5 Sonnet to test if model scale affects recency bias and wavy recency effects