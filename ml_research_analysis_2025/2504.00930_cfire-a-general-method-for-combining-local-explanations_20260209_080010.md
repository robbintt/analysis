---
ver: rpa2
title: 'CFIRE: A General Method for Combining Local Explanations'
arxiv_id: '2504.00930'
source_url: https://arxiv.org/abs/2504.00930
tags:
- cfire
- local
- rule
- frequent
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CFIRE is a local-to-global XAI algorithm that combines local explanations
  with closed frequent itemset mining to generate faithful, compact, and complete
  rule-based explanations for black-box models. Unlike existing approaches that rely
  on fixed local explainers, CFIRE dynamically selects among multiple attribution
  methods to address the disagreement problem and the Rashomon effect.
---

# CFIRE: A General Method for Combining Local Explanations

## Quick Facts
- arXiv ID: 2504.00930
- Source URL: https://arxiv.org/abs/2504.00930
- Reference count: 39
- CFIRE dynamically selects among multiple local explainers to generate compact, faithful, and complete rule-based explanations for black-box models

## Executive Summary
CFIRE addresses the local-to-global XAI problem by combining local explanations with closed frequent itemset mining to generate rule-based explanations. Unlike approaches that rely on a single local explainer, CFIRE runs multiple attribution methods (KernelSHAP, LIME, Integrated Gradients) and dynamically selects the best performer based on accuracy. The method uses closed frequent itemsets to compress exponentially large frequent itemsets into a tractable representation, enabling efficient discovery of important feature patterns across multiple instances. Experiments across 14 datasets and 700 neural network models demonstrate that CFIRE achieves high F1-scores (often >0.9), superior completeness (>95% of models explained), and runtime improvements of up to two orders of magnitude compared to Anchors.

## Method Summary
CFIRE generates global explanations for tabular classification by first running multiple local attribution methods on training data to identify important features per instance. These attribution vectors are binarized and used to create transaction databases per class. Closed frequent itemset mining discovers compact representations of frequently occurring feature combinations. For each closed itemset, boxes are learned via decision trees to separate classes. The final rule model per class is constructed using a greedy set cover heuristic to select a minimal set of boxes that cover all instances of that class. The best local explainer is selected per black-box model based on highest accuracy on the input set.

## Key Results
- CFIRE achieves F1-scores >0.9 on 12 of 14 datasets and >0.95 on 7 datasets
- Runtime improvements of up to two orders of magnitude compared to Anchors
- More compact rules than Anchors-∞ while maintaining comparable faithfulness
- Superior completeness (>95% of models explained) compared to single-explainer approaches

## Why This Works (Mechanism)

### Mechanism 1: Closed Frequent Itemset Compression for Efficient Feature Pattern Discovery
CFIRE uses closed frequent itemset mining to losslessly compress exponentially large frequent itemsets into a tractable representation that preserves all support information. Standard frequent itemset mining generates Ω(2^k) itemsets when a frequent set of cardinality k exists. Closed frequent itemsets are maximal for the support property—extending by any item drops support. This yields: (a) unique representation of equivalent support sets, (b) exponentially smaller output, (c) polynomial-delay enumeration. The algorithm applies this to binarized attribution vectors from local explainers, identifying feature combinations that jointly contribute to predictions across multiple instances.

### Mechanism 2: Dynamic Local Explainer Selection Addresses Disagreement Problem
CFIRE's flexibility to choose among multiple local explainers (KernelSHAP, LIME, Integrated Gradients) directly improves faithfulness, compactness, and completeness compared to fixed-explainer approaches. The disagreement problem states no single attribution method consistently outperforms others across models and datasets. CFIRE runs multiple explainers, generates rule candidates for each, and selects based on highest accuracy on the input set X. This selection criterion empirically correlates with good alignment between rules and local explanations.

### Mechanism 3: Greedy Set Cover for Compact DNF Construction
The greedy set cover heuristic selects a near-minimal set of axis-aligned boxes (DNF terms) that cover all instances of a class while maintaining consistency with instances of other classes. Each closed frequent itemset defines a subspace. For each subspace, CFIRE learns boxes (possibly refined via decision trees when instances overlap classes). The collection of boxes forms candidate terms. The set cover problem—selecting minimum terms to cover all class instances—is NP-hard. The greedy heuristic iteratively selects the term covering the most uncovered instances.

## Foundational Learning

- **Concept: Frequent Itemset Mining & Support/Confidence**
  - Why needed here: Understanding how CFIRE transforms local explanations into candidate feature combinations requires grasping transaction databases, support thresholds, and why Apriori-style mining has exponential complexity
  - Quick check question: Given transactions {a,b,c}, {a,b,d}, {a,c}, what is the support of itemset {a,b}? Is it frequent if threshold τ=0.5?

- **Concept: Disjunctive Normal Form (DNF) as Rule Representation**
  - Why needed here: CFIRE outputs class explanations as DNFs—disjunctions of conjunctive terms. Each term represents an axis-aligned box. Understanding this representation is essential for interpreting outputs
  - Quick check question: Write a DNF for "class 1 if (x₁ ∈ [0,5] AND x₂ ∈ [2,4]) OR (x₁ ∈ [6,10] AND x₃ ∈ [1,3])". How many boxes does this define?

- **Concept: The Rashomon Effect in XAI**
  - Why needed here: The paper explicitly evaluates robustness against this effect—equally well-performing models may require different explanations. CFIRE's low standard deviations across 50 models per task demonstrate this robustness
  - Quick check question: If Model A and Model B both achieve 90% accuracy but use entirely different features, should an explanation method produce similar or different rules? Why does this matter for evaluation?

## Architecture Onboarding

- **Component map:** Local Explainer (L_Φ) → Attribution Vectors (w ∈ R^d) → Binarization (threshold ι=0.01) → Transaction Database D_c per class → Closed Frequent Itemset Mining (τ=0.01) → Box Learning (direct or via Decision Tree, max_depth=7) → Candidate Terms T per class → Greedy Set Cover Selection → DNF per class → Global Rule Model E

- **Critical path:** Closed frequent itemset enumeration determines both runtime and rule quality. The frequency threshold τ=0.01 was constant across experiments—too high loses coverage, too low explodes candidates. Box learning via decision trees (when simple boxes inconsistent) is the secondary complexity driver.

- **Design tradeoffs:**
  - Compactness vs. Faithfulness: Greedy set cover minimizes terms but may accept slight inconsistencies (allowed in Step 4) to avoid overfitting
  - Bounded vs. Unbounded Intervals: CFIRE always outputs bounded intervals (no ±∞), prioritizing interpretability over Anchors' potentially higher coverage with half-open intervals
  - Single vs. Multi-Explainer: Running three explainers triples attribution computation but provides robustness. Selection based on accuracy may not align with all user priorities

- **Failure signatures:**
  - Low completeness (Compl. < 1.0): Frequency threshold too high or attribution methods fail to identify consistent features. Check binarized transaction database diversity
  - Large Size with low F1: Boxes too general (closed itemsets not specific enough) or decision tree depth insufficient. Reduce τ or increase max_depth
  - High variance across black-box models (Rashomon sensitivity): Single explainer dominates selection. Verify explainer diversity in selection

- **First 3 experiments:**
  1. Baseline replication: Run CFIRE on a single binary dataset (e.g., breastw, d=9) with all three explainers. Compare F1, Size, and Compl. against Table 3a values. Verify runtime is O(|X| × d) for attribution + O(|D_c| × |features|) for closed itemset mining
  2. Ablation on frequency threshold τ: Test τ ∈ {0.001, 0.01, 0.05, 0.1} on a multi-class dataset (e.g., iris). Plot Size vs. F1 vs. Compl. to characterize the compactness-coverage tradeoff
  3. Single vs. Multi-explainer impact: Fix one explainer (KS only) and compare against dynamic selection on a dataset where CFIRE showed high explainer disagreement (e.g., ionosphere per Table 5a). Quantify the F1 and Compl. gap

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating synthetic sample generation and black-box querying into CFIRE improve rule coverage, particularly in multi-class settings?
- Basis in paper: Section 7 states that while precision is high, coverage could be improved and suggests generating synthetic samples to query the black-box as a potential solution
- Why unresolved: The current implementation relies solely on the provided input set X and does not actively explore the data space or enrich the dataset during the explanation process
- What evidence would resolve it: Experiments demonstrating improved coverage metrics in multi-class datasets when CFIRE is augmented with an active sampling step, without significant degradation in rule compactness

### Open Question 2
Does combining local explanations from different attribution methods prior to itemset mining yield better results than selecting a single best method post-hoc?
- Basis in paper: Section 7 explicitly proposes exploring the combination of explanations from different methods before the mining step, rather than running CFIRE separately for each explainer
- Why unresolved: The current algorithm addresses the disagreement problem by dynamically selecting the single best explainer based on accuracy, rather than synthesizing information from multiple explainers simultaneously
- What evidence would resolve it: A comparative study evaluating the F1-scores and compactness of rule models generated via "ensemble" explanations versus the current selection approach

### Open Question 3
Can associating overlapping rules with reliability scores or using advanced composition methods (e.g., GLocalX) better resolve the trade-off between faithfulness and comprehensibility?
- Basis in paper: Section 7 notes that rules for different classes can overlap and suggests exploring reliability scores or the interplay with sophisticated composition approaches like GLocalX
- Why unresolved: CFIRE currently resolves ambiguity for instances covered by multiple rules by simply choosing the one with the highest empirical accuracy, which may lose nuanced uncertainty information
- What evidence would resolve it: Quantitative metrics or user studies showing that uncertainty-weighted rules or global composition techniques maintain high faithfulness while improving the interpretability of conflict areas

## Limitations
- Lack of access to anonymized code and exact hyperparameter details for neural network architectures, decision tree implementation, and closed frequent itemset enumeration
- Closed frequent itemset mining not validated against other compression methods (e.g., maximum itemsets, minimal generators) in the XAI context
- Greedy set cover heuristic may not achieve optimal compactness for complex decision boundaries

## Confidence

- **High confidence**: CFIRE's core algorithmic approach (closed frequent itemset mining + greedy set cover) is well-founded and the mechanism for dynamic explainer selection is clearly specified
- **Medium confidence**: The claim that CFIRE achieves "superior completeness (>95%)" is well-supported by the results, but the exact definition of "above-chance precision" and the 12 models that failed require verification
- **Medium confidence**: The assertion that CFIRE provides "more compact rules than Anchors-∞ while maintaining comparable faithfulness" is supported by Table 3, but the comparison is indirect and the specific trade-off curve is not explored

## Next Checks

1. **Closed itemset compression validation**: On a synthetic dataset where the ground truth rule set is known, compare CFIRE's rule set (via closed itemsets) against rules generated from (a) all frequent itemsets, (b) maximum frequent itemsets, and (c) minimal generators. Measure compactness, fidelity to ground truth, and runtime.

2. **Explainer selection ablation**: On a dataset with high explainer disagreement (e.g., ionosphere), run CFIRE with three fixed explainers (KS only, LI only, IG only) and compare F1, size, and completeness against dynamic selection. Verify the 12-model gap in completeness is due to selection failures.

3. **Bounded vs. unbounded intervals**: Modify CFIRE to output half-open intervals (like Anchors) and re-run on a dataset where bounded intervals are limiting (e.g., datasets with sparse features). Measure the change in F1 and completeness to quantify the cost of interpretability.