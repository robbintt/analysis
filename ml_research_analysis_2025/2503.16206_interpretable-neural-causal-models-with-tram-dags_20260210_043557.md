---
ver: rpa2
title: Interpretable Neural Causal Models with TRAM-DAGs
arxiv_id: '2503.16206'
source_url: https://arxiv.org/abs/2503.16206
tags:
- causal
- tram-dag
- continuous
- shift
- interpretable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRAM-DAG, a novel framework for interpretable
  neural causal models that bridges the gap between statistical interpretability and
  neural network flexibility. The key innovation is using transformation models (TRAMs)
  to model causal relationships in structural causal models (SCMs), allowing TRAM-DAG
  to handle continuous, ordinal, binary, and mixed data types while maintaining interpretability.
---

# Interpretable Neural Causal Models with TRAM-DAGs

## Quick Facts
- **arXiv ID**: 2503.16206
- **Source URL**: https://arxiv.org/abs/2503.16206
- **Reference count**: 40
- **Primary result**: TRAM-DAG framework achieves state-of-the-art or superior performance across Pearl's causal hierarchy while providing interpretable parameters for causal relationships.

## Executive Summary
This paper introduces TRAM-DAG, a novel framework for interpretable neural causal models that bridges the gap between statistical interpretability and neural network flexibility. The key innovation is using transformation models (TRAMs) to model causal relationships in structural causal models (SCMs), allowing TRAM-DAG to handle continuous, ordinal, binary, and mixed data types while maintaining interpretability. The method demonstrates strong performance across all three levels of Pearl's causal hierarchy: (L1) observational distribution fitting, (L2) interventional distribution estimation, and (L3) counterfactual queries.

## Method Summary
TRAM-DAG uses transformation models (TRAMs) as the building blocks for structural causal models, where each variable's conditional distribution is modeled through a transformation function that maps an unspecified distribution to a fixed latent distribution. The framework employs Bernstein polynomials for continuous variables to ensure monotonicity, enabling bijective mappings for counterfactual queries. Different effect types (Complex Intercept, Simple Intercept with Linear Shifts, Simple Intercept with Complex Shifts) provide flexibility in balancing interpretability and model complexity. All TRAMs are trained jointly using a neural network architecture based on Masked Autoregressive Flows.

## Key Results
- Accurate fitting of complex observational distributions including bimodal distributions using Bernstein polynomials
- Precise estimation of interventional distributions with interpretable linear shift coefficients as log-odds ratios
- Correct counterfactual predictions for continuous variables, belonging to the bijective generation mechanism class
- Superior or comparable performance to existing neural and normalizing flow-based methods on synthetic benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TRAM-DAGs can model conditional outcome distributions for continuous, ordinal, and binary variables within a unified framework by mapping them to a fixed latent distribution.
- **Mechanism:** The transformation function h(xi|pa(xi)) = hI + hS maps the unspecified conditional distribution FXi|pa(Xi) to a fixed continuous latent distribution FU via FXi|pa(Xi)(xi) = FU(h(xi|pa(xi))). For continuous variables, Bernstein polynomials model the intercept; for discrete ordinal variables, monotone discrete cutpoints define class boundaries.
- **Core assumption:** The latent distribution FU is log-concave with continuous density; Bernstein polynomial order M is sufficiently large to approximate any conditional continuous distribution.
- **Evidence anchors:**
  - [abstract] "TRAM-DAGs...can handle continuous, ordinal, binary, or mixed data types"
  - [section 3.2] Eq. (2) and descriptions of intercept functions for discrete (Eq. 3) and continuous (Eq. 4) variables
  - [corpus] Weak direct corpus support; related work (DeCaFlow, CAREFL) focuses on continuous variables only
- **Break condition:** If Bernstein polynomial order is too low or monotonicity constraints are violated, distribution approximation fails; if discrete variables require counterfactual queries, the many-to-one mapping prevents unique noise abduction.

### Mechanism 2
- **Claim:** Continuous TRAM-DAGs enable counterfactual queries (L3) because they belong to the class of Bijective Generation Mechanisms (BGMs).
- **Mechanism:** Strictly monotone increasing transformation functions (enforced by monotone Bernstein coefficients) create bijective mappings between observed values xi and noise values ui. This allows the three-step counterfactual procedure: (1) Abduction: ui = h(xi|pa(Xi)), (2) Action: modify DAG per intervention, (3) Prediction: xj = h⁻¹(ui|pa(xj)).
- **Core assumption:** The fitted TRAM-DAG accurately reproduces the observational L1 distribution; all variables are continuous.
- **Evidence anchors:**
  - [section 4.2, Proposition] "If the continuous and fully observed TRAM-DAG model can reproduce the observational distribution L1, then it will also reproduce the same interventional L2 and counterfactual L3 queries"
  - [section 4.2] Explicit three-step counterfactual procedure description
  - [corpus] DeCaFlow (arXiv:2503.15114) similarly leverages bijective mechanisms for confounded settings
- **Break condition:** If variables are discrete/ordinal, counterfactual queries fail because the mapping from continuous noise to discrete outcomes is many-to-one (Appendix B explicitly proves this impossibility).

### Mechanism 3
- **Claim:** Linear shift terms in TRAM-DAGs provide causally interpretable effect estimates as log-odds ratios when using standard logistic latent distribution.
- **Mechanism:** With FU as standard logistic, h(y|x1,x2) = h0(y) + β1x1 + γ(x2) implies log(odds(Y ≤ y|x1,x2)) = h0(y) + β1x1 + γ(x2). Thus exp(βi) represents the factor by which odds change when intervening on parent Xi by one unit.
- **Core assumption:** The causal model is correctly specified (matches data-generating process); predictors are direct causal parents of target.
- **Evidence anchors:**
  - [section 3.2.1] "The parameter βj can be causally interpreted as the log-odds ratio"
  - [section 6.1] Experiments show estimated coefficients converge to true DGP values (β̂12=1.98 vs β12=2)
  - [corpus] No direct corpus comparison for interpretability metrics
- **Break condition:** If model is misspecified or shift terms are complex (CS), interpretability degrades—complex shifts require plotting γ(xj) against xj rather than single coefficients.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Pearl's Causal Hierarchy**
  - Why needed here: TRAM-DAGs are SCMs; understanding L1/L2/L3 distinction is essential for knowing when counterfactual queries are possible
  - Quick check question: Can you explain why do(X2=α) changes the DAG structure but counterfactual queries require additional constraints?

- **Concept: Transformation Models (TRAMs) and Normalizing Flows**
  - Why needed here: TRAMs are the core building block; understanding monotonic transformations and latent distributions is prerequisite
  - Quick check question: Why does the choice of latent distribution FU affect interpretability but not predictive power?

- **Concept: Bernstein Polynomials for Monotone Function Approximation**
  - Why needed here: These enforce strict monotonicity for continuous variables, enabling bijectivity
  - Quick check question: What constraint on Bernstein coefficients ensures monotonicity, and why does this matter for counterfactual identification?

## Architecture Onboarding

- **Component map:** Meta-adjacency matrix MA encodes effect types (CI/CS/LS/0) for each parent→child relationship; Individual TRAMs (one per variable) with intercept (discrete cutpoints or Bernstein polynomial) plus optional shift terms; Neural networks (MAF-based) output ϑ-values and shift term parameters; Joint training with Adam optimizer.

- **Critical path:** 1. Define DAG structure and specify TRAM types (CI for flexibility, SI-LS/CS for interpretability); 2. Build meta-adjacency matrix MA; 3. Train jointly on observational data; 4. For L1: sample uj ∼ FUj, compute xi = h⁻¹(ui|pa(xi)) in causal order; 5. For L2: set intervened variable to fixed value, sample descendants in modified DAG order; 6. For L3 (continuous only): abduct noise, apply intervention, propagate counterfactual values.

- **Design tradeoffs:** CI (Complex Intercept): Maximum flexibility, no direct interpretability; SI-LS (Simple Intercept + Linear Shifts): Maximum interpretability (single β coefficients), risk of underfitting complex relationships; SI-CS (Simple Intercept + Complex Shifts): Middle ground—interpretable via plotted γ(xj) functions; Top-down approach recommended: start with interpretable, add complexity only if likelihood on test set justifies it.

- **Failure signatures:** Bimodal distributions not captured: Bernstein polynomial order M too low or CI not used; Counterfactual queries fail for ordinal variables: expected behavior (many-to-one mapping), not a bug; Coefficients don't match intervention effects: model misspecification or training insufficient; CNF comparison shows poor L1 fit: affine transformations insufficient for multimodal data.

- **First 3 experiments:** 1. Replicate VACA bimodal L1 experiment (Fig. 4): Test TRAM-DAG vs CNF on X1∼mixture of Gaussians; verify Bernstein polynomials capture bimodality; 2. CAREFL counterfactual benchmark (Fig. 6): Compare TRAM-DAG L3 accuracy against CAREFL on 4-variable non-linear SCM; 3. Interpretability validation (Section 6.1): Fit SI-LS model to linear DGP; verify β̂12, β̂13, β̂23 converge to ground truth; test that exp(β̂) predicts interventional odds-ratio shift.

## Open Questions the Paper Calls Out

- **Question:** Can TRAM-DAGs be extended to perform causal discovery when the underlying DAG structure is unknown?
  - Basis in paper: [explicit] The authors state they "assume currently that the underlying directed acyclic graph (DAG) is known" and focus strictly on estimating functional relationships.
  - Why unresolved: The current framework requires the graph structure as a fixed input (meta-adjacency matrix) for training the neural networks.
  - What evidence would resolve it: A method that jointly learns the DAG structure and the transformation functions from observational data alone.

- **Question:** Is it possible to formulate valid counterfactual (L3) queries for discrete or mixed data types within this framework?
  - Basis in paper: [explicit] The paper notes that for discrete variables, "counterfactual queries are not possible which is a fundamental limitation" due to the non-bijective mapping of noise to outcomes.
  - Why unresolved: The "abduction" step for counterfactuals requires a unique mapping from observation to noise, which fails when interval censoring creates many-to-one mappings.
  - What evidence would resolve it: A theoretical extension providing bounds or approximations for counterfactuals on ordinal data.

- **Question:** How does the framework scale to high-dimensional settings with complex graph topologies?
  - Basis in paper: [inferred] The experiments were restricted to small-scale setups (3-4 variables) with simple linear or non-linear dependencies.
  - Why unresolved: It is unclear if the proposed joint training of multiple TRAMs using the Adam optimizer remains stable or efficient as the number of nodes increases significantly.
  - What evidence would resolve it: Benchmarks on datasets with significantly more nodes and edges, reporting training time and error convergence.

## Limitations
- Counterfactual queries restricted to continuous variables only, limiting practical applicability for discrete/ordinal data
- Computational complexity may become prohibitive for large DAGs with many variables or complex relationships
- Strong dependence on correct causal structure specification for maintaining interpretability

## Confidence
- **High confidence**: Claims about L1 observational distribution fitting and L2 interventional distribution estimation are well-supported by experiments on synthetic data with known ground truth
- **Medium confidence**: Claims about L3 counterfactual accuracy are supported for continuous variables but lack extensive validation on real-world datasets with ground truth counterfactuals
- **Medium confidence**: Interpretability claims for linear shift terms are demonstrated on synthetic linear DGPs but require validation on more complex real-world relationships

## Next Checks
1. Apply TRAM-DAG to established causal inference benchmarks (e.g., IHDP, ACIC datasets) to evaluate performance on observational data with ground truth treatment effects, comparing against state-of-the-art causal inference methods
2. Systematically evaluate training time and memory requirements as DAG size increases (number of variables and edges), identifying practical limits for deployment on large-scale problems
3. Investigate whether approximate counterfactual methods (e.g., relaxation techniques or discretization-aware approaches) can extend L3 query capabilities to ordinal variables while maintaining acceptable accuracy