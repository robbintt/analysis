---
ver: rpa2
title: 'StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making
  and Information Seeking'
arxiv_id: '2510.18483'
source_url: https://arxiv.org/abs/2510.18483
tags:
- arxiv
- control
- preprint
- information
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "StarBench introduces a benchmark for testing whether vision-language\
  \ models can play real video games like humans\u2014by perceiving raw screens, deciding,\
  \ and issuing precise keyboard-mouse actions\u2014without relying on scripted APIs.\
  \ The benchmark uses Honkai: Star Rail, a complex turn-based RPG, to evaluate agents\
  \ across eight combat tasks under two regimes: (1) direct control, where agents\
  \ output low-level click-and-keyboard primitives from screenshots with no semantic\
  \ aids; and (2) tool-assisted control, where structured action tuples and optional\
  \ textualized UI observations ease grounding."
---

# StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking

## Quick Facts
- **arXiv ID**: 2510.18483
- **Source URL**: https://arxiv.org/abs/2510.18483
- **Reference count**: 40
- **Primary result**: StarBench benchmarks vision-language models on real video game play, revealing that structured tool use dramatically outperforms raw pixel-to-action control

## Executive Summary
StarBench introduces a benchmark to evaluate whether vision-language models can play real video games like humans—by perceiving raw screens, deciding, and issuing precise keyboard-mouse actions—without relying on scripted APIs. The benchmark uses Honkai: Star Rail, a complex turn-based RPG, to evaluate agents across eight combat tasks under two regimes: (1) direct control, where agents output low-level click-and-keyboard primitives from screenshots with no semantic aids; and (2) tool-assisted control, where structured action tuples and optional textualized UI observations ease grounding. A diagnostic experiment further measures when agents choose to seek brief guidance before acting. Results show that VLMs fail almost entirely in direct control, exposing a fundamental gap in pixel-to-primitive localization, but achieve high success rates in tool-assisted mode, especially with OCR support. Judicious information-seeking correlates with improved performance, establishing StarBench as a reproducible yardstick for multimodal decision-making and agentic information seeking in real-client play.

## Method Summary
StarBench evaluates vision-language models on 8 combat tasks in Honkai: Star Rail under two regimes: Direct Control (screenshot to OS primitives) and Tool-Assisted (structured action tuples with YOLOv8 and OCR). The benchmark uses inference-only with GPT-4o-mini, Claude 3.5 Sonnet, and Gemini 1.5 Flash, measuring task success rate, completion steps, and reward. Agents must issue 1-2 actions per turn in a turn-based combat system, with failures triggering after 10 consecutive invalid actions. The evaluation requires setting up the HSR client in windowed mode, training YOLO detectors on ~400 labeled images per task, building a LightRAG index from public sources, and implementing an evaluation loop that captures screenshots, processes them through OCR/YOLO, and executes actions via pyautogui or structured tuples.

## Key Results
- VLMs achieve 0% success rate in Direct Control mode due to fundamental pixel-to-primitive localization failures
- Tool-Assisted mode with structured action tuples achieves 100% success rates for certain tasks
- OCR support significantly improves performance by providing precise numerical state variables
- Judicious information-seeking through LightRAG correlates with measurable performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Abstracting low-level UI grounding into structured action triples separates strategic reasoning from visual-motor control, enabling VLMs to perform competently where raw pixel-to-primitive mapping fails.
- **Mechanism**: The Tool-Assisted (TA) regime converts raw pixels into a structured observation space (YOLO bounding boxes and OCR text) and constrains the action space to high-level tuples `(Character, Move, Target)`. This bypasses the need for the VLM to estimate precise $(x, y)$ coordinates, offloading spatial localization to external detectors.
- **Core assumption**: The VLM possesses sufficient semantic understanding of game mechanics to select valid actions if visual localization is solved by external tools.
- **Evidence anchors**: Section 5.3 shows 0% Success Rate in Direct Control vs. 100% SR in Tool-Assisted for GPT-4o-mini; Table 4 demonstrates performance jumps when abstraction is provided.

### Mechanism 2
- **Claim**: Injecting textualized state (OCR) into the prompt reduces decision friction and illegal actions significantly more than bounding boxes alone.
- **Mechanism**: OCR extracts dense, stylized numerical data (HP%, Skill Points) that VLMs struggle to read reliably from raw pixels. By explicitly textifying these values in the prompt, the model gains access to precise state variables required for legality checks.
- **Core assumption**: The VLM's reasoning errors in the TA-NO-OCR condition are primarily driven by visual misinterpretation of text/numbers rather than flawed strategic logic.
- **Evidence anchors**: Section 5.3 shows removing OCR consistently hurts both success and efficiency; Table 4 compares "TA" vs. "TA-NO-OCR," showing SR drops when OCR is removed.

### Mechanism 3
- **Claim**: External information seeking (Ask-or-Act) functions as a corrective mechanism for gaps in the model's frozen pre-trained knowledge, provided the agent can calibrate the cost-benefit of asking.
- **Mechanism**: The agent may query a LightRAG system for concise textual guidance before the episode starts. This mechanism injects task-specific context (e.g., "Target the weakness") into the context window, compensating for the lack of live-game knowledge in the base model weights.
- **Core assumption**: The bottleneck in performance for novel tasks is missing knowledge (epistemic uncertainty) rather than reasoning capacity, and a single retrieval step is sufficient to bridge this gap.
- **Evidence anchors**: Section 5.3 shows brief guidance produces measurable uplifts when used judiciously; Table 6 shows positive "Effect" scores correlating asking with score improvement.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here**: The paper models HSR combat as a POMDP ($M=\langle S, A, T, R, O \rangle$). You must understand that the "State" includes hidden variables (cooldowns, enemy AI intent) that are not fully visible in the "Observation" (Screenshot).
  - **Quick check question**: If the agent sees the screenshot but not the internal enemy "aggro" value, is this fully observable? How should the agent handle the missing data?

- **Concept: Grounding (Visual-to-Action)**
  - **Why needed here**: The core failure mode identified is the "perception-to-control fidelity" gap. Distinguishing between *semantic* grounding (identifying "that is a button") and *spatial* grounding (knowing "click at x=500, y=300") is critical.
  - **Quick check question**: Why does mapping a screenshot to a semantic JSON action (TA) succeed where mapping a screenshot to OS primitives (DC) fails?

- **Concept: Tool Use / Function Calling**
  - **Why needed here**: The TA regime relies on the VLM outputting a structured format that an external "tool" (the environment) executes. Understanding how to constrain LLM outputs to valid function signatures is a prerequisite for implementation.
  - **Quick check question**: What happens if the VLM outputs a target ID $t=9$ (Select All) when only enemies 4-8 exist?

## Architecture Onboarding

- **Component map**: Screenshot -> YOLOv8 + PaddleOCR -> VLM processing -> Structured tuple output -> pyautogui execution -> Next screenshot
- **Critical path**: 1. Capture screenshot loop 2. Perception: Run OCR/YOLO to textify UI state 3. Decision: VLM generates action tuple `(c, m, t)` 4. Execution: Environment maps tuple -> click center of bounding box 5. Observation: Next screenshot + Reward signal
- **Design tradeoffs**: Direct Control vs. Tool-Assisted (pure test vs. masked visual grounding failures); OCR Latency vs. Accuracy (real-time adds processing time but improves state precision)
- **Failure signatures**: "The Void Click" (agent outputs coordinates corresponding to empty space); "Resource Blindness" (agent uses Skills when Skill Points = 0); "Spam Loop" (agent repeats Basic Attack endlessly due to Ultimate readiness detection failure)
- **First 3 experiments**: 1. Sanity Check (DC): Run GPT-4o-mini in Direct Control on "Echo of War" (Expected: 0% SR) 2. Ablation (TA-NO-OCR): Run Claude 3.5 in Tool-Assisted mode without OCR text feed (Expected: Significant drop in SR) 3. Ask-Or-Act Calibration: Run diagnostic where agent must ask on first trial and is forbidden on subsequent trials (Expected: Measure delta in Effect)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the planned companion emulator provide deterministic, reproducible evaluation equivalent to the live client while maintaining the challenges of real-time play?
- Basis in paper: The authors state that the real live-service client introduces "frame-timing variance" and "OS-level input flakiness," and they are developing a companion emulator to allow "frozen UI states" and "deterministic screenshots."
- Why unresolved: The emulator is currently under development and unreleased; it is unclear if it can perfectly replicate the complex timing and visual variance of the proprietary live client.
- What evidence would resolve it: A correlation study showing that agent performance and failure modes in the emulator match those in the live client with statistical significance.

### Open Question 2
- Question: Can specific fine-tuning or architectural improvements enable VLMs to achieve non-zero success in the Direct Control (DC) regime?
- Basis in paper: The conclusion notes that "end-to-end pixel-to-primitive control remains the key challenge" after experiments showed all tested VLMs (GPT-4o, Claude, Gemini) failed completely (0% success rate) in the DC regime.
- Why unresolved: The paper establishes the failure of current off-the-shelf models but does not propose or test a method to bridge this grounding gap.
- What evidence would resolve it: A modified VLM achieving a success rate > 0% on the Direct Control tasks without relying on external textualized UI aids.

### Open Question 3
- Question: Does allowing dynamic, intra-episode information seeking yield higher performance or efficiency than the single pre-episode "Ask" constraint?
- Basis in paper: The paper limits the "Ask-or-Act" diagnostic to a single pre-episode decision to isolate the behavior, but notes that humans "seek information before trying again" when stuck.
- Why unresolved: The benchmark restricts the agent to one query at the start; the utility of asking for help mid-battle (e.g., after a sudden status change) remains unmeasured.
- What evidence would resolve it: An experiment comparing the current protocol against a baseline where agents can query the LightRAG system at any step, measuring the trade-off between total score uplift and query frequency.

## Limitations

- The benchmark depends on pre-trained visual detectors whose accuracy directly determines TA performance
- The Ask-or-Act mechanism relies on static corpus retrieval rather than live game state reasoning
- The evaluation uses only three VLM models without exploring scaling effects or fine-tuning approaches

## Confidence

- **High confidence**: The claim that Direct Control fails (0% SR) while Tool-Assisted succeeds is well-supported by ablation results across multiple models and tasks
- **Medium confidence**: The assertion that OCR significantly improves performance is supported by specific SR drops when removed, though the mechanism (visual misinterpretation vs. strategic reasoning) is not definitively isolated
- **Low confidence**: The claim that information-seeking behavior directly correlates with improved performance is based on Effect scores but lacks causal isolation from other factors like model randomness

## Next Checks

1. **DC failure root cause analysis**: Run DC mode with human-validated coordinate annotations to determine if VLMs can execute known-good actions, isolating perception vs. planning failures
2. **TA component ablation study**: Systematically remove YOLO vs. OCR vs. both to quantify their individual contributions to SR improvements beyond what's reported
3. **Information-seeking cost-benefit calibration**: Vary the information-seeking frequency (always ask, never ask, ask every N steps) to establish the optimal tradeoff between latency and performance gains