---
ver: rpa2
title: Precise gradient descent training dynamics for finite-width multi-layer neural
  networks
arxiv_id: '2505.04898'
source_url: https://arxiv.org/abs/2505.04898
tags:
- proposition
- gradient
- estimate
- neural
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first precise distributional characterization
  of gradient descent iterates for general multi-layer neural networks under a single-index
  regression model, in the finite-width proportional regime. The authors develop a
  new theoretical framework that captures Gaussian fluctuations in first-layer weights
  and deterministic concentration in deeper-layer weights, and remains valid for non-Gaussian
  features.
---

# Precise gradient descent training dynamics for finite-width multi-layer neural networks

## Quick Facts
- arXiv ID: 2505.04898
- Source URL: https://arxiv.org/abs/2505.04898
- Reference count: 24
- Key outcome: First precise distributional characterization of gradient descent iterates for general multi-layer neural networks under single-index regression in the finite-width proportional regime

## Executive Summary
This paper establishes the first precise distributional characterization of gradient descent training dynamics for general multi-layer neural networks in the finite-width proportional regime. The authors develop a novel theoretical framework that captures Gaussian fluctuations in first-layer weights and deterministic concentration in deeper-layer weights, valid for non-Gaussian features. This framework provides insights into both training and generalization errors, operating beyond the lazy training regime and allowing weights to evolve from individual initializations.

## Method Summary
The authors develop a new theoretical framework that characterizes the distributional evolution of gradient descent iterates for multi-layer neural networks. The framework captures Gaussian fluctuations in first-layer weights and deterministic concentration in deeper layers, while remaining valid for non-Gaussian features. It operates in the finite-width proportional regime where network width scales linearly with sample size, distinguishing it from existing NTK, MF, and TP theories that operate in different regimes.

## Key Results
- First precise distributional characterization of gradient descent iterates for general multi-layer neural networks under single-index regression
- Framework captures Gaussian fluctuations in first-layer weights and deterministic concentration in deeper layers
- Provides consistent estimates of generalization error at each iteration for early stopping and hyperparameter tuning

## Why This Works (Mechanism)
The framework succeeds by operating in the finite-width proportional regime, which allows for a more precise characterization of weight dynamics compared to infinite-width approximations. By capturing both Gaussian fluctuations in the first layer and deterministic concentration in deeper layers, the theory maintains accuracy while avoiding the lazy training regime. The approach leverages the structure of single-index regression models while remaining applicable to non-Gaussian features.

## Foundational Learning
- **Finite-width proportional regime**: Understanding the regime where network width scales linearly with sample size is crucial for the theoretical analysis. Quick check: Verify that width ∝ n relationship holds in the theoretical bounds.
- **Gaussian fluctuations in first-layer weights**: This captures the stochastic nature of early layer updates during training. Quick check: Confirm the Gaussianity assumption through empirical weight distributions.
- **Deterministic concentration in deeper layers**: Represents the stable behavior of later layers during training. Quick check: Verify concentration inequalities hold for deeper layer weights.
- **Single-index regression model**: The underlying statistical model assumption that enables precise characterization. Quick check: Validate the single-index structure in synthetic data experiments.

## Architecture Onboarding
- **Component map**: Input features → First layer (Gaussian fluctuations) → Intermediate layers (deterministic concentration) → Output layer
- **Critical path**: The first layer's Gaussian fluctuations propagate through deterministic deeper layers to determine output behavior
- **Design tradeoffs**: Balances between finite-width precision and tractable analysis, choosing proportional scaling over infinite-width limits
- **Failure signatures**: When width doesn't scale proportionally with sample size, or when feature distributions deviate significantly from assumptions
- **First experiments**: 1) Test Gaussian fluctuation characterization in first layer with varying widths, 2) Verify deterministic concentration in deeper layers across network depths, 3) Validate generalization error estimation technique on synthetic single-index data

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes single-index regression model and Gaussian noise, limiting applicability to complex real-world scenarios
- Focuses on proportional regime where width scales linearly with sample size, potentially missing behaviors in other scaling regimes
- Analyzes full-batch gradient descent rather than stochastic variants, restricting practical relevance

## Confidence
- Theoretical framework differs from NTK, MF, and TP theories: High confidence
- Characterization of both training and generalization errors: Medium confidence
- Application to generalization error estimation and early stopping: Medium confidence

## Next Checks
1) Empirical validation on non-Gaussian feature distributions to verify theoretical predictions
2) Extension of the framework to stochastic gradient descent with varying batch sizes
3) Numerical experiments testing the proposed generalization error estimation technique across different network architectures and data distributions