---
ver: rpa2
title: Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions
  Generated Using Diffusion Models
arxiv_id: '2508.06151'
source_url: https://arxiv.org/abs/2508.06151
tags:
- images
- dataset
- oral
- synthetic
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposed a novel approach to improve diagnostic accuracy
  for oral cancer by generating synthetic lesions using an inpainting technique with
  a fine-tuned diffusion model. The method was evaluated on a comprehensive dataset
  compiled from multiple sources, and the results showed that the classification model
  achieved a diagnostic accuracy of 0.97 in differentiating between cancerous and
  non-cancerous tissues, while the detection model accurately identified lesion locations
  with 0.85 accuracy.
---

# Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models

## Quick Facts
- **arXiv ID:** 2508.06151
- **Source URL:** https://arxiv.org/abs/2508.06151
- **Reference count:** 26
- **Primary result:** 0.97 diagnostic accuracy for classification; 0.85 detection accuracy with synthetic lesion augmentation

## Executive Summary
This study introduces a novel approach to improve oral cancer diagnostics by generating synthetic lesions using diffusion model inpainting. The method involves fine-tuning a pre-trained stable diffusion model with DreamBooth to synthesize realistic lesions directly within oral cavity images, then augmenting the training data with these synthetic samples. Evaluated on a comprehensive dataset compiled from multiple sources, the classification model achieved a diagnostic accuracy of 0.97 in differentiating cancerous from non-cancerous tissues, while the detection model accurately identified lesion locations with 0.85 accuracy. The results validate the potential for synthetic image generation in medical diagnostics and suggest broader applicability to other cancer types.

## Method Summary
The approach uses SAM ViT-H to generate lesion masks from bounding box annotations, then fine-tunes Stable Diffusion 2.1 via DreamBooth with specified hyperparameters (learning rate 5e-6, 1000 steps, batch size 1). The model performs inpainting synthesis with 100 inference steps and guidance scale 7.5 at 512x512 resolution, generating synthetic lesions with 4x oversampling for the cancer class. These synthetic images are merged with the original dataset and used to train ResNet-50 and YOLOv8 models using 5-fold cross-validation. The internal dataset contains 777 training images (632 cancerous, 145 normal), while external validation uses 3,000 images with 6,358 pre-segmented masks.

## Key Results
- Classification model achieved diagnostic accuracy of 0.97 differentiating cancerous from non-cancerous tissues
- Detection model accurately identified lesion locations with 0.85 accuracy
- Synthetic augmentation reduced variance across folds from ±0.0277 to ±0.0081

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic lesion augmentation via diffusion inpainting improves classification accuracy and reduces variance across folds.
- **Mechanism:** Oversampling with diverse synthetic lesions exposes the classifier to a broader distribution of lesion appearances, reducing overfitting to the limited original set.
- **Core assumption:** Synthetic lesions capture clinically relevant features underrepresented in original data.
- **Evidence anchors:** 0.97 accuracy achieved; accuracy range 0.9584-0.9792 with reduced variance ±0.0081 vs ±0.0277.

### Mechanism 2
- **Claim:** Inpainting-based synthesis preserves anatomical context better than de novo generation.
- **Mechanism:** By conditioning on the original oral cavity image and only replacing masked lesion regions, the diffusion model maintains realistic textures and anatomical structure.
- **Core assumption:** Inpainting process does not alter clinically relevant features outside masked region.
- **Evidence anchors:** SSIM of 0.8503 indicates high structural fidelity; Grad-CAM shows improved focus on lesion boundaries.

### Mechanism 3
- **Claim:** Fine-tuning pre-trained diffusion model with DreamBooth enables subject-specific lesion synthesis without training from scratch.
- **Mechanism:** DreamBooth adapts the model's prior knowledge to the specific domain of oral cancer lesions by learning a low-dimensional embedding of the target concept.
- **Core assumption:** Pre-trained stable diffusion model contains transferable feature representations applicable to medical imaging.
- **Evidence anchors:** Fine-tuning imbues stable diffusion with capability to generate detailed, anatomically precise representations.

## Foundational Learning

- **Concept: Diffusion models (denoising probabilistic models)**
  - **Why needed here:** The core generative engine; understanding how noise is iteratively refined into images is essential for debugging generation quality.
  - **Quick check question:** Can you explain why increasing inference steps typically improves image quality but increases computation time?

- **Concept: Inpainting with masked conditioning**
  - **Why needed here:** The method relies on replacing only masked regions while preserving the rest; understanding how masks guide the diffusion process is critical.
  - **Quick check question:** What happens to the output if the mask does not fully cover the lesion region?

- **Concept: Transfer learning vs. fine-tuning**
  - **Why needed here:** The study uses DreamBooth fine-tuning rather than training from scratch; distinguishing these approaches helps assess resource requirements.
  - **Quick check question:** Why might fine-tuning require fewer training steps than training a diffusion model from random initialization?

## Architecture Onboarding

- **Component map:** SAM (ViT-H) -> Stable Diffusion (fine-tuned via DreamBooth) -> ResNet-50 / YOLOv8 -> Evaluation metrics
- **Critical path:** 1) Convert bounding boxes → point prompts for SAM; 2) SAM generates binary masks; 3) Mask + original image + prompts → fine-tuned diffusion inpainting; 4) Synthetic images merged with original dataset (4x oversampling); 5) Train/test ResNet-50 and YOLOv8 with 5-fold cross-validation
- **Design tradeoffs:** 512x512 resolution chosen for computational efficiency; SAM without fine-tuning for faster deployment; 4x oversampling balances augmentation with synthetic data dominance risk
- **Failure signatures:** Low SSIM in inpainted regions → mask boundary issues; Grad-CAM activation outside lesion boundaries → model learning contextual shortcuts; recall drops while precision rises
- **First 3 experiments:** 1) Ablation on mask precision: compare SAM-generated vs. expert-annotated masks; 2) Varying oversampling ratios: test 1x, 2x, 4x, 8x synthetic augmentation; 3) External validation holdout: train on internal + synthetic, test exclusively on external dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the diffusion-based inpainting approach be effectively generalized to other medical imaging domains beyond oral cancer?
- **Basis in paper:** Authors state future research will explore applicability in different medical contexts and mention paving the way for extending methods to other cancer diagnostics.
- **Why unresolved:** Current study validates method exclusively on oral cancer datasets, leaving utility in other anatomical or pathological contexts unproven.
- **What evidence would resolve it:** Successful application and validation on distinct cancer datasets showing similar improvements.

### Open Question 2
- **Question:** Does fine-tuning the Segment Anything Model (SAM) specifically for medical data improve precision of lesion masking and subsequent synthesis quality?
- **Basis in paper:** Implementation used generic SAM ViT-H model that "did not undergo such fine-tuning," potentially affecting precision compared to expert-annotated masks.
- **Why unresolved:** Study relied on generic SAM, potentially introducing segmentation noise that could affect generated image integrity.
- **What evidence would resolve it:** Comparative analysis of synthetic image quality using generic SAM masks versus medically-adapted SAM masks.

### Open Question 3
- **Question:** Can the marginal decrease in recall observed in detection models be mitigated while retaining improvements in precision gained from synthetic data?
- **Basis in paper:** Results show precision increased from 0.788 to 0.851 while recall slightly decreased from 0.783 to 0.767 when using synthetic data.
- **Why unresolved:** Paper attributes this to trade-off where models became more precise but might miss positive cases, without investigating whether sensitivity loss is inherent or adjustable.
- **What evidence would resolve it:** Experiments with different synthetic-to-real data ratios or modified loss functions to determine if recall drop can be reversed without sacrificing precision gains.

## Limitations
- External validation performed on different dataset without direct comparison to synthetic data augmentation, making it difficult to isolate proposed method's impact on generalization
- Study does not report specific Stable Diffusion checkpoint version, which could significantly affect inpainting quality and reproducibility
- Synthetic data's clinical fidelity not directly assessed—metrics like FID and LPIPS measure general image quality but not whether generated lesions capture diagnostically relevant features

## Confidence

- **High Confidence:** Classification accuracy improvement (0.97 vs. prior baselines) is well-supported by internal cross-validation results and aligns with synthetic data augmentation mechanism
- **Medium Confidence:** Detection model's performance (0.85 accuracy, 0.767 recall) is reasonable but lacks clear evidence that synthetic augmentation specifically improves detection vs. real data alone
- **Low Confidence:** Claims about clinical utility of synthetic lesions not validated—no radiologist review of synthetic outputs or assessment of whether generated features align with pathology reports

## Next Checks
1. **Synthetic lesion clinical fidelity:** Have a board-certified oral pathologist review 50 synthetic lesions and rate anatomical realism and diagnostic relevance on 5-point scale
2. **Generalization benchmark:** Train same ResNet-50 architecture on external dataset alone (no synthetic augmentation) and compare performance to augmented model