---
ver: rpa2
title: Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical
  Relation Extraction via LLMs-as-the-Judge
arxiv_id: '2506.00777'
source_url: https://arxiv.org/abs/2506.00777
tags:
- relation
- extraction
- biomedical
- relations
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating large language
  models (LLMs) in biomedical relation extraction tasks, where traditional metrics
  fail due to LLMs generating semantically correct but surface-form different outputs.
  The authors propose using LLMs-as-the-Judge as an alternative evaluation method,
  benchmarking 8 LLM judges on responses from 5 LLM generators across 3 biomedical
  datasets.
---

# Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge

## Quick Facts
- **arXiv ID:** 2506.00777
- **Source URL:** https://arxiv.org/abs/2506.00777
- **Reference count:** 36
- **Primary result:** LLM judges achieve below 50% accuracy in biomedical relation extraction due to lack of standard formatting, improved by 15% with structured outputs

## Executive Summary
This paper addresses a critical gap in evaluating large language models for biomedical relation extraction tasks. Traditional metrics like exact-match fail when LLMs generate semantically correct but surface-form different outputs. The authors propose using LLMs-as-the-Judge as an alternative evaluation method and benchmark 8 LLM judges against 5 LLM generators across 3 biomedical datasets. They identify that poor judge performance stems from unstructured LLM-generated relations and demonstrate that implementing structured output formatting improves judge accuracy by approximately 15%. The study also introduces domain adaptation via transfer learning to enhance judge performance when human-annotated data is limited, and publicly releases 36k human- and LLM-annotated judgment samples.

## Method Summary
The authors developed a comprehensive evaluation framework for biomedical relation extraction using LLMs as judges. They created a pipeline where 5 different LLM generators produce relation extraction outputs, which are then evaluated by 8 different LLM judges. The study benchmarks these judges across three biomedical datasets, initially finding that judge accuracy typically falls below 50%. To address this limitation, they introduce structured output formatting for LLM responses, standardizing the format of generated relations. Additionally, they implement domain adaptation through transfer learning techniques to improve judge performance when human-annotated data is scarce. The entire framework is validated through systematic experimentation and the release of their annotated judgment dataset for community use.

## Key Results
- LLM judges achieve below 50% accuracy in biomedical relation extraction tasks
- Structured output formatting improves judge performance by approximately 15% on average
- Domain adaptation via transfer learning shows promise for enhancing judge performance with limited human annotations
- The authors release 36k human- and LLM-annotated judgment samples publicly

## Why This Works (Mechanism)
The approach works by addressing the fundamental mismatch between how LLM generators produce outputs and how LLM judges evaluate them. Traditional evaluation metrics fail because they rely on exact surface-form matching, while LLMs naturally generate semantically equivalent but syntactically different responses. By introducing structured output formatting, the system creates a standardized evaluation framework that allows judges to more reliably assess semantic correctness. The domain adaptation technique further improves performance by fine-tuning judges on task-specific biomedical data, enabling them to better understand domain-specific terminology and relationship patterns.

## Foundational Learning
- **Relation Extraction**: Extracting semantic relationships between entities in biomedical text; needed to understand the core task being evaluated
- **LLM-as-the-Judge**: Using large language models to evaluate other models' outputs; quick check: verify judge can distinguish correct from incorrect relations
- **Structured Output Formatting**: Standardizing the format of generated relations; needed to create consistent evaluation criteria
- **Transfer Learning**: Adapting pre-trained models to new domains with limited data; quick check: measure performance improvement on domain-specific tasks
- **Exact-Match Metrics**: Traditional evaluation method comparing exact string matches; needed to understand why current metrics fail
- **Biomedical Domain Specificity**: Understanding that biomedical terminology requires specialized knowledge; quick check: assess judge performance on domain-specific vs general text

## Architecture Onboarding

**Component Map:** LLM Generators -> Structured Output Formatter -> LLM Judges -> Evaluation Metrics

**Critical Path:** Generator output → Formatting standardization → Judge evaluation → Performance measurement

**Design Tradeoffs:** The system trades off between the flexibility of free-form LLM outputs and the reliability of structured evaluation, sacrificing some natural language generation capabilities for more consistent assessment.

**Failure Signatures:** Poor judge performance (below 50% accuracy), inconsistent evaluation across different judges, inability to handle domain-specific terminology, and failure to recognize semantically equivalent but syntactically different relations.

**First Experiments:** 1) Test structured formatting on single relation type across all datasets; 2) Compare judge performance with and without domain adaptation on limited human annotations; 3) Evaluate judge consistency across multiple runs of the same evaluation task.

## Open Questions the Paper Calls Out
None

## Limitations
- The 15% performance improvement may be dataset-specific rather than a universal solution
- Domain adaptation validation was limited to small-scale experiments
- The approach's generalizability across diverse biomedical subdomains remains uncertain
- Reliance on LLM judges may introduce new evaluation biases

## Confidence
- **Structured output formatting improvement:** Medium confidence - shows promise but needs broader validation
- **Domain adaptation effectiveness:** Medium confidence - preliminary results are positive but limited in scope
- **Fundamental cause of poor judge performance:** Medium confidence - formatting issues identified but other factors may contribute

## Next Checks
1. Test structured output formatting across additional biomedical relation extraction datasets beyond the three studied to assess generalizability
2. Conduct ablation studies to isolate the impact of formatting versus other potential factors affecting judge performance
3. Evaluate whether the domain adaptation technique maintains effectiveness when scaled to larger, more diverse biomedical corpora with limited human annotations