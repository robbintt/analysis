---
ver: rpa2
title: Robustness of Structured Data Extraction from Perspectively Distorted Documents
arxiv_id: '2511.17607'
source_url: https://arxiv.org/abs/2511.17607
tags:
- document
- images
- rotation
- accuracy
- distortion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how perspectively distorted document images
  affect structured data extraction using multi-modal LLMs. Unlike in-plane rotation,
  perspective distortion involves eight parameters, making experimental evaluation
  challenging.
---

# Robustness of Structured Data Extraction from Perspectively Distorted Documents

## Quick Facts
- arXiv ID: 2511.17607
- Source URL: https://arxiv.org/abs/2511.17607
- Reference count: 0
- Most real-world document perspective distortions follow an isosceles-trapezoidal pattern, enabling tractable evaluation with only 2 parameters.

## Executive Summary
This study investigates how perspectively distorted document images affect structured data extraction using multi-modal LLMs. Unlike in-plane rotation, perspective distortion involves eight parameters, making experimental evaluation challenging. By observing typical distortions in real-world receipts, the authors show that most follow an isosceles-trapezoidal transformation, which can be described with only two parameters: rotation angle and distortion ratio. Using synthetically generated sample documents with varying these parameters, they evaluated the entity-extraction accuracy of Gemini-1.5-pro. Results showed that structure-recognition accuracy significantly degraded with distortion, particularly at middle rotation angles, while character-recognition accuracy remained robust. Notably, a simple rotational correction was found sufficient to improve accuracy, highlighting its practical value for real-world OCR tasks.

## Method Summary
The authors synthetically generated 1,710 document images by applying isosceles-trapezoidal homographic transformations to 10 original receipt images. The transformations varied rotation angle θ ∈ [-90°, 90°] and distortion ratio r ∈ [0.25, 4.0]. They used Gemini-1.5-pro with a structured JSON prompt to extract 8 entities (vendor, date, line items, subtotal, tax, total, payment, change). Two accuracy types were measured: character-recognition (Jaro-Winkler distance for strings) and structure-recognition (numerical equality for values, reading-order matching for lists).

## Key Results
- Structure-recognition accuracy degraded significantly with distortion, especially at middle rotation angles (θ ≈ -20°, 70°)
- Character-recognition accuracy remained robust across all distortion conditions
- Simple rotational correction alone was sufficient to improve extraction accuracy
- Most real-world receipts (96%) follow isosceles-trapezoidal transformation patterns

## Why This Works (Mechanism)

### Mechanism 1: Isosceles-Trapezoidal Parameter Reduction
- Claim: Real-world perspective distortions can be approximated by a 2-parameter transformation, enabling tractable evaluation.
- Mechanism: By observing that most document photos capture the subject from a tilted but centered position (to avoid overhead shadows), the 8-DOF homographic transformation reduces to rotation angle (θ) and distortion ratio (r), where r describes the ratio of bottom to top edge lengths in the resulting trapezoid.
- Core assumption: Assumption: Photographers tilt cameras along a single axis to avoid casting shadows, producing approximately isosceles-trapezoidal distortions.
- Evidence anchors:
  - [abstract]: "We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters."
  - [section]: Table 1 shows 60.5% of sampled receipts were isosceles trapezoids, 35.5% rectangles (a special case), totaling ~96%.
  - [corpus]: No direct corpus support for this geometric reduction technique.
- Break condition: When documents are non-planar (folded, curved), or when camera position produces non-isosceles quadrilateral distortions.

### Mechanism 2: Structure Recognition Degradation Under Rotation
- Claim: Reading-order understanding degrades at middle rotation angles even when character recognition remains accurate.
- Mechanism: At middle angles (around θ = -20° and θ = 70°), spatial relationships between keys and values break down because the LLM's visual processing scatters row-aligned information across multiple lines, causing misassignment of values to wrong keys.
- Core assumption: Assumption: Multi-modal LLMs encode spatial layout in ways that are sensitive to non-canonical orientations.
- Evidence anchors:
  - [abstract]: "structure-recognition accuracy was found to be significantly degraded by document distortion" while "character-recognition accuracy remained robust."
  - [section]: Fig. 8 and Fig. 11 show that at middle angles, items and prices scatter across rows, causing misassignment.
  - [corpus]: Corpus papers discuss document extraction but do not address this specific spatial breakdown mechanism.
- Break condition: When documents have no key-value spatial dependencies, or when extraction targets are single-field (vendor only).

### Mechanism 3: Rotational Correction Sufficiency
- Claim: Simple in-plane rotational correction (without full perspective unwarping) is sufficient to restore extraction accuracy.
- Mechanism: Aligning document rows/columns to canonical axes (θ ≈ 0° or ±90°) restores the LLM's ability to parse tabular structures, even when perspective distortion (r ≠ 1) remains uncorrected.
- Core assumption: Training corpora contain predominantly axis-aligned documents, so LLMs have strong priors for horizontal/vertical reading orders.
- Evidence anchors:
  - [abstract]: "a simple rotational correction was found sufficient to improve accuracy, highlighting its practical value."
  - [section]: "If θ = 0°, ±90°, i.e. rows or columns are aligned...the degree of distortion appears to have little effect on the accuracy."
  - [corpus]: "Hybrid OCR-LLM Framework" paper mentions OCR preprocessing but doesn't isolate rotation vs. perspective effects.
- Break condition: When distortion ratio r is extreme (approaching triangular shapes) or when combined with non-perspective noise (blur, occlusion).

## Foundational Learning

- **Homographic Transformation**
  - Why needed here: Perspective distortion is mathematically an 8-DOF projective transformation. Understanding this explains why naive grid-search evaluation is intractable and why parameter reduction is necessary.
  - Quick check question: Why can't we evaluate all possible perspective distortions using a simple parameter sweep?

- **Reading Order vs. Character Recognition**
  - Why needed here: The paper shows these are distinct capabilities—LLMs can read characters accurately while failing to understand which characters belong together structurally.
  - Quick check question: If an LLM correctly reads "$5.00" and "Coffee" but outputs {"item": "Coffee", "price": null}, which capability failed?

- **SIPRA (Safe In-plane Rotation Angles)**
  - Why needed here: Prior work established that in-plane rotation degrades extraction beyond certain thresholds. This paper extends that finding to combined rotation + perspective.
  - Quick check question: What recovery pattern occurs at θ = ±90°, and why might training data explain this?

## Architecture Onboarding

- **Component map:**
  - Distorted document image -> Rotation detection -> Rotational correction (optional) -> Multi-modal LLM (Gemini-1.5-pro) -> Extracted entities

- **Critical path:**
  1. Assess rotation angle (θ) from document image
  2. If θ is outside safe range (not near 0° or ±90°), apply rotational correction
  3. Pass corrected image to multi-modal LLM with structured prompt
  4. Validate extraction against both character-level and structure-level metrics

- **Design tradeoffs:**
  - Full perspective correction vs. rotation-only: Paper shows rotation-only is often sufficient and computationally cheaper
  - Synthetic augmentation: Isosceles-trapezoidal transformation can generate training data, but may not cover folded/curved documents
  - Model choice: Only Gemini-1.5-pro was tested; other models may have different failure modes

- **Failure signatures:**
  - Extraction accuracy drops at middle rotation angles (θ ≈ -20°, 70°)
  - Values correctly recognized but assigned to wrong keys
  - List items partially missing or reordered
  - Asymmetric degradation: counterclockwise rotation causes worse drops than clockwise in this study

- **First 3 experiments:**
  1. **Baseline calibration:** Run extraction on undistorted receipt images to establish upper-bound accuracy per entity type.
  2. **Rotation-only sweep:** Fix r = 1 (no perspective), vary θ ∈ [-90°, 90°] in 10° increments to confirm SIPRA boundaries.
  3. **Combined perturbation matrix:** Vary both θ ∈ [-90°, 90°] and r ∈ [0.25, 4.0] to map the failure region; test whether rotational correction alone recovers accuracy at the worst-performing (θ, r) combinations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the asymmetric effect where magnifying the lower bottom of perspectively distorted documents degrades extraction accuracy more than magnifying the upper bottom?
- Basis in paper: [explicit] "Considerations for this asymmetry will be the subject of future work" (page 7).
- Why unresolved: The paper identifies the asymmetry but does not investigate its underlying mechanisms.
- What evidence would resolve it: A controlled study isolating upper vs. lower bottom magnification, with analysis of how text density differences in receipt regions interact with distortion.

### Open Question 2
- Question: How does character blur and other common image noise affect multi-modal LLM robustness when combined with perspective distortion?
- Basis in paper: [explicit] "Besides perspective distortions, document images generally contain noises, such as character blur. LLMs can be used to compensate for these character defects... Thus, we would like to evaluate the robustness of LLM performance against such a noise" (page 8).
- Why unresolved: The study only addresses geometric distortion, not combined quality degradations.
- What evidence would resolve it: Experiments adding varying blur levels to perspectively distorted documents and measuring entity extraction accuracy.

### Open Question 3
- Question: Do the findings generalize to other multi-modal LLMs beyond Gemini-1.5-pro?
- Basis in paper: [inferred] Only Gemini-1.5-pro was evaluated, but the conclusions about structure-recognition degradation and sufficiency of rotational correction may be model-specific.
- Why unresolved: Different architectures may have different robustness profiles to perspective distortion.
- What evidence would resolve it: Replicating the experimental protocol with GPT-4V, Claude, or other leading multi-modal LLMs.

### Open Question 4
- Question: Can the isosceles-trapezoidal transformation model adequately represent complex real-world distortions like document folding and bending?
- Basis in paper: [inferred] 2.5% of real receipt images had "exception" shapes (strongly bent or folded), and 1.5% were non-isosceles quadrilaterals (Table 1, page 3).
- Why unresolved: The simplified 2-parameter model may miss important distortion types.
- What evidence would resolve it: Extending the model to include curvature parameters and testing on physically folded/bent documents.

## Limitations
- Reliance on synthetic perspective distortion rather than real-world photographs may miss failure modes like occlusions and shadows
- The 96% isosceles-trapezoidal observation is based on a small sample (60 receipts) and may not generalize to diverse document types
- Only one multi-modal LLM (Gemini-1.5-pro) was tested, limiting generalizability to other models

## Confidence
**High Confidence**: The mechanism of isosceles-trapezoidal parameter reduction is well-supported by the 96% empirical observation rate and the mathematical tractability it provides. The degradation of structure-recognition at middle rotation angles while character-recognition remains robust is consistently demonstrated across multiple figures and metrics.

**Medium Confidence**: The sufficiency of rotational correction for practical applications assumes that real-world distortions approximate the tested parameter space. The asymmetric degradation between clockwise and counterclockwise rotation (Figure 8) suggests model-specific behavior that may not generalize.

**Low Confidence**: The claim that "most" real-world distortions follow the isosceles-trapezoidal pattern is based on a small sample (60 receipts) and may not represent diverse document types, languages, or photographic conditions.

## Next Checks
1. **Real-world validation**: Test the rotation-correction pipeline on a dataset of actual smartphone-captured receipts under varied lighting and angles to verify the synthetic-to-real transfer assumption.

2. **Model generalization**: Evaluate the same distortion patterns on other multi-modal LLMs (GPT-4V, Claude-3-Vision) to determine if the structure-recognition failure mechanism is architecture-specific or universal.

3. **Edge case exploration**: Generate extreme distortion ratios (r approaching 0.1 or 10) and folded/curved document simulations to identify break points beyond the tested isosceles-trapezoidal space.