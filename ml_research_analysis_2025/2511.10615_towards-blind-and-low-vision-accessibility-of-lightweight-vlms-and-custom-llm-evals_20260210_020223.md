---
ver: rpa2
title: Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals
arxiv_id: '2511.10615'
source_url: https://arxiv.org/abs/2511.10615
tags:
- prompt
- evaluation
- context
- guidelines
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates SmolVLM2 models (500M and 2.2B parameters)
  for blind and low-vision video accessibility, comparing their performance across
  indoor/outdoor datasets using four prompting strategies and two novel evaluation
  frameworks (Multi-Context BLV and Navigational Assistance). Results show the 500M
  model excels in outdoor scenarios and objective description generation, while the
  2.2B model performs better in indoor clarity and accuracy.
---

# Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals

## Quick Facts
- arXiv ID: 2511.10615
- Source URL: https://arxiv.org/abs/2511.10615
- Reference count: 3
- Demonstrates feasibility of smartphone-based VLM deployment for BLV accessibility with 60-83s inference times

## Executive Summary
This study evaluates SmolVLM2 models (500M and 2.2B parameters) for blind and low-vision video accessibility, focusing on indoor/outdoor scenarios across four prompting strategies. The research introduces two novel evaluation frameworks - Multi-Context BLV and Navigational Assistance - designed to better capture accessibility-specific needs rather than relying on standard NLP metrics that show systematic biases against BLV user preferences. Both models demonstrate practical edge deployment capabilities on smartphones without cloud connectivity, though inference times of 60-83 seconds limit real-time applications.

## Method Summary
The study benchmarks SmolVLM2 variants across indoor and outdoor datasets using four prompting strategies (zero-shot, few-shot, instruction-based, context-based). Two custom evaluation frameworks were developed: the Multi-Context BLV Framework (addressing Objectivity, Clarity, Accuracy, Action & Event) and the Navigational Assistance Framework (focusing on Route Guidance, Spatial Description, Environmental Awareness, Safety Awareness). All models were tested on Samsung Galaxy S24 with Samsung NPU for edge deployment, using INT8 quantization where applicable.

## Key Results
- 500M model excels in outdoor scenarios and objective description generation with higher objectivity scores (2.41-2.88)
- 2.2B model performs better in indoor clarity and accuracy, particularly for structured scenarios
- Action & Event scores consistently ranked lowest (1.95-2.63) across all configurations, indicating critical temporal reasoning limitations
- Custom accessibility metrics revealed systematic biases in standard NLP metrics against BLV user preferences

## Why This Works (Mechanism)
None

## Foundational Learning
**Why needed**: Understanding BLV accessibility requirements, VLM architecture constraints, and evaluation framework development
**Quick check**: Can identify key BLV-specific challenges that standard NLP metrics fail to capture

**Key Concepts:**
- **Multi-Context BLV Framework**: Four-dimension evaluation (Objectivity, Clarity, Accuracy, Action & Event) designed to assess VLM outputs for BLV users
- **Navigational Assistance Framework**: Four-context evaluation (Route Guidance, Spatial Description, Environmental Awareness, Safety Awareness) for navigation-specific VLM outputs
- **INT8 Quantization**: 8-bit quantization technique enabling efficient on-device model deployment while maintaining accuracy
- **Zero-shot vs Few-shot prompting**: Different approaches to eliciting desired VLM behavior without/brief training data
- **Environmental adaptability**: Model's ability to handle varying lighting, weather, and spatial conditions
- **Temporal sequence description**: VLM's capacity to describe actions and events over time

## Architecture Onboarding
**Component map**: SmolVLM2 (500M/2.2B) -> Edge deployment (Samsung Galaxy S24 NPU) -> Custom evaluation (GPT-OSS-20B) -> Accessibility metrics
**Critical path**: Model inference -> Prompt generation -> VLM output -> Metric evaluation -> Accessibility assessment
**Design tradeoffs**: Smaller models (500M) offer better environmental adaptability and objectivity but limited precision; larger models (2.2B) provide enhanced accuracy and clarity but require more computational resources
**Failure signatures**: Action & Event scores consistently lowest across all configurations; standard NLP metrics systematically biased against BLV preferences
**First experiments**: 1) Test intermediate model sizes (1B, 1.5B) to identify optimal performance-efficiency balance 2) Implement speculative decoding to reduce inference times below 10 seconds 3) Validate custom metrics against BLV user studies

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can temporal reasoning capabilities in lightweight VLMs be improved to adequately describe action sequences and events for BLV users, given that Action & Event scores consistently ranked lowest (1.95â€“2.63 across all configurations)?
- Basis in paper: [explicit] "The Action & Event context consistently scores lowest across all model-dataset combinations, showing the critical limitation in temporal sequence description that affects BLV users' ability to follow dynamic content."
- Why unresolved: Neither model size scaling nor prompting strategies improved temporal understanding; the paper only quantifies the gap without proposing solutions.
- What evidence would resolve it: Experiments with architectural modifications (e.g., enhanced temporal attention mechanisms), higher keyframe densities, or video-specific pretraining objectives showing improved Action & Event scores above 4.0.

### Open Question 2
- Question: Do the custom accessibility metrics (Multi-Context BLV and Navigational Assistance Frameworks) correlate with actual BLV user satisfaction, given that evaluation relied entirely on GPT-OSS-20B without human BLV validation?
- Basis in paper: [inferred] The paper introduces novel metrics and claims they address "bias against BLV user preferences," but all metric scores come from GPT-OSS-20B evaluation with no reported human BLV user studies to validate alignment.
- Why unresolved: The paper establishes metric design but does not validate whether scores reflect real BLV user preferences or task success.
- What evidence would resolve it: Correlation analysis between metric scores and BLV user ratings or navigation task performance in a user study.

### Open Question 3
- Question: What model size between 500M and 2.2B parameters optimally balances the 500M model's superior objectivity/outdoor performance with the 2.2B model's superior indoor clarity and accuracy?
- Basis in paper: [explicit] "Smaller models (500M parameters) often excel in environmental adaptability and objective description generation, while larger models (2.2B parameters) provide enhanced precision in structured scenarios."
- Why unresolved: The study evaluated only two discrete model sizes without exploring the intermediate range or identifying an optimal trade-off point.
- What evidence would resolve it: Systematic evaluation of intermediate model sizes (e.g., 1B, 1.5B) across both indoor/outdoor datasets and all accessibility metrics to identify Pareto-optimal configurations.

### Open Question 4
- Question: Can 60-83 second inference times be reduced to support real-time or near-real-time BLV assistance without significant quality degradation?
- Basis in paper: [inferred] The paper demonstrates feasibility of on-device deployment with 60-83 second inference for 500M models, but this latency precludes real-time video description for navigation or live event comprehension.
- Why unresolved: The paper quantifies current performance but does not investigate optimization techniques beyond INT8 quantization, which paradoxically increased latency for the 500M model due to longer token sequences.
- What evidence would resolve it: Experiments with model distillation, pruning, speculative decoding, or hardware acceleration (NPU/DSP) demonstrating sub-10-second inference while maintaining accessibility metric scores.

## Limitations
- Custom evaluation metrics lack validation against actual BLV user preferences and satisfaction
- 60-83 second inference times represent significant barrier to real-time accessibility applications
- Study focuses exclusively on English-language descriptions and indoor/outdoor scenarios, limiting generalizability

## Confidence
- High confidence: Local edge deployment feasibility, systematic bias identification in standard metrics
- Medium confidence: Model performance differences across contexts, custom metric effectiveness
- Low confidence: Real-world usability implications, generalizability to diverse BLV user populations

## Next Checks
1. Conduct user studies with actual BLV participants to validate whether custom metric rankings align with user preferences for generated descriptions
2. Test model performance across additional languages and cultural contexts to assess generalizability
3. Benchmark against alternative lightweight architectures and parameter sizes to identify optimal performance-efficiency trade-offs