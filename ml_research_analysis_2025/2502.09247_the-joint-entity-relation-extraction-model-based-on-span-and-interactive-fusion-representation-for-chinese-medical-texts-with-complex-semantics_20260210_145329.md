---
ver: rpa2
title: The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion
  Representation for Chinese Medical Texts with Complex Semantics
arxiv_id: '2502.09247'
source_url: https://arxiv.org/abs/2502.09247
tags:
- extraction
- relation
- entity
- recognition
- span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a joint entity-relation extraction model based
  on span and interactive fusion representation for Chinese medical texts with complex
  semantics. The model addresses the challenge of extracting structured knowledge
  from Chinese medical texts, which often contain complex semantics.
---

# The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion Representation for Chinese Medical Texts with Complex Semantics

## Quick Facts
- arXiv ID: 2502.09247
- Source URL: https://arxiv.org/abs/2502.09247
- Reference count: 40
- Primary result: Proposed model achieves 96.73% entity F1 and 78.43% relation F1 on CH-DDI dataset

## Executive Summary
This paper introduces a joint entity-relation extraction model for Chinese medical texts that leverages span-based extraction with interactive fusion representation. The model addresses the challenge of extracting structured knowledge from complex Chinese medical texts by enabling bidirectional information exchange between entity recognition and relation extraction tasks. Using Cross Attention for task interaction and a Semantic Enhancement Attention module for contextual understanding, the approach achieves state-of-the-art performance on both a Chinese medical dataset (CH-DDI) and an English benchmark (CoNLL04).

## Method Summary
The model employs a span-based approach where candidate text spans are extracted and classified for entity types and relations. It uses BERT as an encoder, followed by two parallel feedforward networks to project outputs into entity and relation spaces. The core innovation is an interactive fusion module that uses Cross Attention for bidirectional information exchange between tasks, combined with a BiLSTM for fusion. A Semantic Enhancement Attention (SEA) module replaces static sentence representations with dynamic attention-and-recurrent mechanisms to better capture long-range contextual information. The model also explicitly models local context between entity spans to resolve relation overlap issues.

## Key Results
- Achieves 96.73% F1-score for entity recognition and 78.43% F1 for relation extraction on CH-DDI dataset
- Obtains 89.54% entity precision and 71.64% relation accuracy on CoNLL04 dataset
- Ablation studies confirm the effectiveness of SEA module and Cross Attention fusion mechanism
- Outperforms baseline SpERT model on both datasets

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Task Interaction via Cross Attention
The model uses Cross Attention where entity representations serve as Query and relation representations as Key/Value (and vice versa), followed by BiLSTM fusion. This allows entity recognition to be aware of potential relations and relation extraction to leverage entity information. The core assumption is that ER and RE are interdependent tasks where one constrains or informs the other. Evidence shows that using relation representations as query focuses attention on potential entities, and entity representations focus on relation-triggering words. Break condition: If initial BERT representations are too noisy or tasks are loosely correlated, Cross Attention might propagate noise.

### Mechanism 2: Semantic Enhancement via SEA Module
The SEA module applies Multi-Head Attention to a masked sequence (hiding the candidate span) to capture global dependencies, followed by a BiGRU to retain sequential order information. This creates a richer context vector for the span, replacing static sentence-level representations like the [CLS] token. The core assumption is that contextual semantics in complex Chinese medical texts rely on long-distance dependencies that standard static embeddings miss. Ablation study shows performance drops when SEA is replaced with CLS tokens. Break condition: If text is very short or semantic complexity is low, computational overhead may not yield significant accuracy gains.

### Mechanism 3: Local Context Preservation for Relation Overlap
During relation extraction, the model extracts hidden states of tokens located strictly between two entity spans and applies max-pooling to capture "local context." This local context is concatenated with span features. The core assumption is that relationships between entities are often encoded in specific words or phrases appearing between them rather than just entity properties. Evidence shows performance drops when local context is replaced by global context, particularly for English data. Break condition: In relations defined by long-range dependencies, this strict local focus might miss defining evidence.

## Foundational Learning

- **Concept: Span-Based Extraction**
  - Why needed: Unlike token-level tagging, this model classifies "spans" (slices of input sequence) directly. You must understand how sliding window generates candidates and how width embeddings define boundaries.
  - Quick check: How does the model define a "negative" span during training, and why is max-pooling used on internal span tokens?

- **Concept: Cross Attention Mechanics**
  - Why needed: The core innovation is interactive fusion. You need to distinguish this from self-attention; here, Query comes from one task domain (Entity) and Key/Value from another (Relation).
  - Quick check: In Cross Attention step, which representation serves as Query when updating Entity features?

- **Concept: Medical NLP Nuances (Bio-BERT)**
  - Why needed: The paper applies this to Chinese medical texts. Understanding that general BERT weights are often swapped for domain-specific pre-training (like Bio-BERT) is critical for reproducing results.
  - Quick check: Why does the model use Bio-BERT for CH-DDI dataset but standard BERT for CoNLL04?

## Architecture Onboarding

- **Component map:** BERT + 2 FFN layers → (X_e, X_r) → Cross Attention (dual) + BiLSTM → H → Span enumeration (sliding window) → Max-pooling + Width Embedding + SEA → Entity classifier (softmax) / Relation classifier (sigmoid)
- **Critical path:** The Interactive Fusion Module is the bottleneck. If dimensions of X_e and X_r are mismatched or Cross Attention weights are not regularized, gradient flow between tasks will fail, reverting to pipeline system.
- **Design tradeoffs:** Span width limited to reduce computation (entities rarely 20 tokens long); strict local context for relations helps precision but may miss sentence-level cues; negative sampling tuned per dataset size (100 for CoNLL04, 50 for CH-DDI).
- **Failure signatures:** High Entity F1, Low Relation F1 indicates Fusion Module or Local Context features not learning interaction semantics; performance drop on short sentences may indicate SEA over-attending or masking too much input.
- **First 3 experiments:** 1) Baseline Validation: Reproduce SpERT baseline on CoNLL04 without SEA or Interactive Fusion modules. 2) Ablation on Interaction: Disable Cross Attention to quantify contribution of bidirectional information exchange. 3) Context Scope Test: Replace "Local Context" with "Global Context" (CLS token) for CH-DDI to validate claim that Chinese text relies heavily on local semantics.

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific data augmentation or re-sampling techniques can effectively resolve class imbalance in CH-DDI dataset? The paper identifies significant disparity in relation types but does not implement or compare balancing methods. Evidence would be experimental results showing improved macro-F1 scores after applying specific balancing techniques.

- **Open Question 2:** Can alternative information exchange mechanisms outperform the proposed Cross Attention and BiLSTM fusion module? The authors state they will explore more methods for information exchange. Evidence would be ablation studies replacing Cross Attention with alternative fusion architectures demonstrating comparative or superior F1-scores.

- **Open Question 3:** Does the "entity prompting" structure in CH-DDI dataset annotation artificially inflate entity recognition performance? The raw data format acts as "entity prompting" and the dataset contains only one entity type. Evidence would be evaluation on a version of CH-DDI where the explicit entity prompt is removed from input string.

## Limitations
- Performance claims based on single self-constructed dataset (CH-DDI) and one public dataset (CoNLL04), limiting generalizability
- High dropout rate (0.75) for CH-DDI suggests potential overfitting to small dataset
- Cross Attention effectiveness depends heavily on quality of initial BERT representations
- Strict local context extraction may miss long-range dependencies in some medical text patterns
- SEA module adds computational overhead without clear necessity demonstrated

## Confidence
- **High Confidence:** Span-based extraction framework and basic Cross Attention mechanism are well-established and mathematically sound
- **Medium Confidence:** Specific implementation details (dropout rates, negative sampling numbers) appear reasonable but were tuned for specific datasets without cross-validation
- **Low Confidence:** Semantic Enhancement Attention module's superiority over simpler context encoders is based on single ablation study without comparison to alternative methods

## Next Checks
1. **Cross-Domain Generalization Test:** Apply trained model to different medical dataset (e.g., CADEC for adverse drug reactions) to assess whether interactive fusion benefits transfer beyond CH-DDI's specific semantic patterns.

2. **Ablation on Context Encoding:** Replace SEA module with simpler [CLS]-based context encoder and different context scope strategy (global vs local) to quantify contribution of attention-and-recurrent mechanism versus specific local context focus.

3. **Noise Injection Study:** Systematically degrade BERT input representations (e.g., by masking medical terminology) to determine whether Cross Attention mechanism amplifies or mitigates representation noise between entity and relation tasks.