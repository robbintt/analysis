---
ver: rpa2
title: 'Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible
  Framework with Theoretical Guarantees'
arxiv_id: '2502.00240'
source_url: https://arxiv.org/abs/2502.00240
tags:
- convex
- regularizers
- such
- function
- star
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning effective regularizers
  for inverse problems, which are ubiquitous in scientific and engineering applications.
  While data-driven methods using deep neural networks have shown strong empirical
  performance, they often lack theoretical guarantees due to their highly nonconvex
  nature.
---

# Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible Framework with Theoretical Guarantees

## Quick Facts
- arXiv ID: 2502.00240
- Source URL: https://arxiv.org/abs/2502.00240
- Reference count: 40
- One-line primary result: A flexible DC-based framework for learned regularizers with theoretical guarantees achieves strong empirical performance on CT reconstruction tasks.

## Executive Summary
This paper addresses the challenge of learning effective regularizers for inverse problems, which are ubiquitous in scientific and engineering applications. While data-driven methods using deep neural networks have shown strong empirical performance, they often lack theoretical guarantees due to their highly nonconvex nature. The authors propose a novel framework that parameterizes regularizers as Difference-of-Convex (DC) functions, offering both flexibility and theoretical tractability. By leveraging the DC structure, they employ well-established optimization algorithms like the Difference-of-Convex Algorithm (DCA) and Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Theoretical analysis shows that optimal regularizers can often be expressed as DC functions under certain conditions. Extensive experiments on computed tomography (CT) reconstruction tasks demonstrate that the proposed approach consistently outperforms other weakly supervised learned regularizers across sparse and limited-view settings. The method achieves strong performance, effectively recovering fine structures in reconstructed images while maintaining convergence guarantees.

## Method Summary
The method learns a Difference-of-Convex (DC) regularizer by parameterizing it as the difference of two Input Convex Neural Networks (ICNNs). The regularizer is trained using an Adversarial Regularization (AR) loss on pairs of clean images and artifact-containing reconstructions. During inference, the learned regularizer is used within an optimization framework (DCA or PSM) to solve the inverse problem. The DC structure enables the use of specialized optimization algorithms with convergence guarantees to critical points, while the ICNN architecture ensures the convexity of individual components. The framework is evaluated on sparse-view and limited-angle CT reconstruction tasks using the Mayo Clinic 2016 Low Dose CT dataset.

## Key Results
- The DC regularization framework consistently outperforms other weakly supervised learned regularizers on CT reconstruction tasks across sparse and limited-view settings.
- The proposed method achieves strong empirical performance, effectively recovering fine structures in reconstructed images.
- Theoretical analysis demonstrates convergence to critical points of the nonconvex objective function using DCA and PSM algorithms.

## Why This Works (Mechanism)

### Mechanism 1: DC Regularization Expands Expressivity Over Convex and Weakly Convex Classes
Parameterizing regularizers as a Difference-of-Convex (DC) function allows them to capture more complex, nonconvex data manifolds than convex or weakly convex regularizers, leading to improved reconstruction quality. A DC regularizer is defined as $R_\theta(x) = R_{\theta_1}(x) - R_{\theta_2}(x)$, where $R_{\theta_1}$ and $R_{\theta_2}$ are convex. This is a strict generalization of weakly convex functions, as any $\rho$-weakly convex function $g(x)$ can be written as the DC decomposition $g(x) = g(x) + \frac{\rho}{2}\|x\|_2^2 - \frac{\rho}{2}\|x\|_2^2$. The framework leverages this broader function class to fit a wider range of optimal regularizers.

### Mechanism 2: Convergence to Critical Points via DCA and PSM
The DC structure enables the use of specialized optimization algorithms, the Difference-of-Convex Algorithm (DCA) and Proximal Subgradient Method (PSM), which provably converge to critical points of the nonconvex objective function. DCA solves a sequence of convex subproblems by linearizing the concave ($-R_{\theta_2}$) part of the regularizer at each iteration. PSM combines subgradient steps with a proximal update. Under smoothness or Kurdyka-Łojasiewicz (KL) assumptions, the paper proves that any limit point of these algorithms is a critical point, providing theoretical stability.

### Mechanism 3: Universal Approximation of DC Functions with ICNNs
The use of Input Convex Neural Networks (ICNNs) to parameterize the convex components of the DC regularizer creates a flexible architecture capable of approximating any Lipschitz DC function. ICNNs enforce convexity through architectural constraints: non-negative weights and convex, non-decreasing activation functions (e.g., Softplus). The IDCNN architecture, formed by subtracting one ICNN from another, inherits this constrained expressivity. Proposition 3.1 provides a universal approximation guarantee, showing that for any Lipschitz DC function, an IDCNN exists that can approximate it to arbitrary precision.

## Foundational Learning

### Concept: Difference-of-Convex (DC) Functions
**Why needed here:** This is the core mathematical object of the entire paper. All theory and algorithms are built upon representing the regularizer as the difference of two convex functions.
**Quick check question:** Can you write the $\ell_1$ norm as a DC function? What about the SCAD penalty? (Hint: The paper mentions several).

### Concept: Input Convex Neural Networks (ICNNs)
**Why needed here:** This is the specific neural network architecture used to parameterize the convex components of the DC regularizer. Understanding its constraints (non-negative weights) is critical for implementation.
**Quick check question:** What two constraints must be placed on a standard feed-forward network to make it an ICNN?

### Concept: Critical Point vs. Global Minimum
**Why needed here:** Since the overall optimization problem is nonconvex, the algorithms (DCA, PSM) are only guaranteed to find a critical point, not a global minimum. It is essential to understand this limitation.
**Quick check question:** Why does the nonconvexity of the DC objective prevent a guarantee of finding the global minimum?

## Architecture Onboarding

### Component map:
Data Pipeline -> IDCNN Regularizer (R_θ₁ - R_θ₂) -> AR Loss Function -> Optimization Module (DCA or PSM)

### Critical path:
1. Implement ICNN: Ensure weight non-negativity (e.g., via torch.clamp or torch.abs after each update) and use convex activations (e.g., Softplus).
2. Instantiate IDCNN: Create the regularizer by subtracting the output of a second ICNN.
3. Train on AR Loss: Train the IDCNN using the adversarial regularization loss on the dataset of clean and artifact-containing images.
4. Solve Inverse Problem: For a new noisy measurement y, initialize x₀ (e.g., via FBP) and iteratively refine it using Algorithm 1 (DCA) or Algorithm 2 (PSM).

### Design tradeoffs:
- DCA vs. PSM: DCA requires solving a convex subproblem at each iteration (can be slow but stable). PSM uses a proximal update (can be faster but requires tuning step size α and proximal strength γ).
- ICNN Depth vs. Expressivity: Deeper ICNNs are more expressive but harder to train and may overfit. The paper uses different depths for different tasks (10 layers for limited-view, 4 for sparse-view).
- Hand-crafted vs. Learned Convex Component: The paper notes one could use a hand-crafted convex regularizer (like ℓ₁ with a known proximal operator) for R_θ₁ to speed up PSM. The default is to learn both.

### Failure signatures:
- Exploding Loss/Gradients: Likely due to failure to enforce convexity (non-negative weights) in the ICNNs.
- No Improvement from Baseline: The learned regularizer may be collapsing to a trivial function. Check Lipschitz regularization strength (λ).
- DCA Instability: The convex subproblem may not be solved accurately enough. Increase the number of inner-loop iterations.
- PSM Stalls: Step size α may be too large, or proximal strength γ is inappropriate. Tune these hyperparameters.

### First 3 experiments:
1. Toy Example Reproduction: Implement the double spiral experiment (Section 6.1) to visually verify that the IDCNN learns a nonconvex regularizer contour that fits the data better than an ICNN.
2. Baseline Comparison on CT: Train ADCR on the sparse-view CT task and compare PSNR/SSIM against AR, ACR, and AWCR baselines (Table 1) to validate performance claims.
3. Algorithm Ablation: Compare the performance of ADCR, ADCR-DCA, and ADCR-PSM on the limited-view CT task to understand the trade-offs between the different optimization algorithms.

## Open Questions the Paper Calls Out

### Open Question 1
Can accelerated difference-of-convex (DC) optimization algorithms improve the convergence rates or reconstruction quality of the proposed IDCNN framework?
**Basis in paper:** The conclusion states: "Future work could explore advanced DC optimization methods, such as accelerated algorithms [50]..."
**Why unresolved:** The current work analyzes standard DCA and a proximal subgradient method (PSM), but does not investigate acceleration techniques for these specific non-convex solvers.
**What evidence would resolve it:** Demonstrating that an accelerated DC algorithm converges to critical points in fewer iterations or achieves higher PSNR/SSIM in the same wall-clock time compared to the standard DCA/PSM baselines.

### Open Question 2
Can incorporating hand-crafted convex regularizers with efficient proximal operators into the DC framework improve computational efficiency while maintaining expressivity?
**Basis in paper:** Section 5.2 notes that using regularizers with closed-form proximal operators (like ℓ₁ or TV) "would constitute a hybrid hand-crafted data-driven approach... and is an interesting future direction."
**Why unresolved:** The current IDCNN implementation requires iterative solvers (like gradient descent) for the proximal step because the learned ICNN components lack closed-form proximal operators.
**What evidence would resolve it:** A hybrid model (e.g., combining a data-driven DC component with a TV component) that demonstrates significantly faster per-iteration runtime without sacrificing the theoretical convergence guarantees or the high-fidelity detail recovery shown in the pure IDCNN.

### Open Question 3
Do natural data distributions induce optimal regularizers that specifically satisfy the DC structural conditions (e.g., the star body geometric conditions) derived in the theoretical analysis?
**Basis in paper:** Section 7 calls for future work to "mathematically characterize the nonconvex structures induced by natural data distributions and identify the families of structured nonconvex functions they belong to."
**Why unresolved:** While the paper proves conditions under which optimal regularizers are DC functions (Corollary 4.3), it does not verify if actual image distributions (like the Mayo Clinic CT data) mathematically satisfy these specific geometric properties.
**What evidence would resolve it:** A theoretical proof or empirical analysis demonstrating that the geometries of image manifolds naturally satisfy the α-harmonic radial combination convexity conditions outlined in the paper.

## Limitations
- The empirical validation is limited to a single application domain (CT reconstruction), raising questions about generalizability to other inverse problems.
- Critical hyperparameters like the Lipschitz regularization weight λ are not fully specified in the main text.
- The theoretical guarantees rely on assumptions about the DC structure of optimal regularizers that may not hold for all natural data distributions.

## Confidence

**High Confidence:** The theoretical framework connecting DC functions to regularization (Mechanism 1) and the convergence analysis of DCA/PSM to critical points (Mechanism 2) are well-established in optimization literature and properly cited.

**Medium Confidence:** The universal approximation result for IDCNNs (Mechanism 3) provides theoretical justification, but the practical capacity of the architecture to capture complex regularizers depends on depth and training dynamics not fully explored.

**Medium Confidence:** The empirical results on CT reconstruction are compelling, showing consistent improvements across sparse and limited-view settings, but the comparison is limited to existing weakly-supervised methods rather than strongly-supervised approaches.

## Next Checks
1. **Generalizability Test:** Apply the DC regularization framework to a different inverse problem domain (e.g., MRI reconstruction or super-resolution) to assess whether the performance gains transfer beyond CT.
2. **Architecture Ablation:** Systematically vary the depth and width of the ICNN components in the IDCNN to understand the relationship between architectural capacity and reconstruction quality.
3. **Hyperparameter Sensitivity:** Conduct a thorough ablation study on the Lipschitz regularization weight λ and optimization step sizes to identify the sensitivity of the approach to these critical hyperparameters.