---
ver: rpa2
title: 'IberBench: LLM Evaluation on Iberian Languages'
arxiv_id: '2504.16921'
source_url: https://arxiv.org/abs/2504.16921
tags:
- detection
- tasks
- language
- evaluation
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IberBench is a comprehensive, extensible benchmark for evaluating
  large language models (LLMs) on Iberian languages, addressing the lack of high-quality,
  multilingual benchmarks that include industry-relevant tasks. It integrates 101
  datasets from shared tasks and benchmarks across 22 task categories, covering Spanish,
  Portuguese, Catalan, Basque, Galician, English, and Spanish varieties from Ibero-America.
---

# IberBench: LLM Evaluation on Iberian Languages

## Quick Facts
- arXiv ID: 2504.16921
- Source URL: https://arxiv.org/abs/2504.16921
- Reference count: 40
- Primary result: IberBench is a comprehensive, extensible benchmark for evaluating large language models (LLMs) on Iberian languages, integrating 101 datasets from 22 task categories.

## Executive Summary
IberBench addresses the lack of high-quality, multilingual benchmarks for Iberian languages by providing a comprehensive, extensible platform for evaluating large language models. It integrates 101 datasets from shared tasks and benchmarks across 22 task categories, covering Spanish, Portuguese, Catalan, Basque, Galician, English, and Spanish varieties from Ibero-America. The benchmark enables continual updates and community-driven submissions, evaluated by an expert committee. Initial evaluations of 23 LLMs (100M–14B parameters) reveal that LLMs perform worse on industry-relevant tasks than fundamental ones, face greater challenges with Galician and Basque, and underperform fine-tuned shared-task systems in zero-shot settings.

## Method Summary
IberBench evaluates LLMs on 101 datasets across 22 task categories (text classification, generation, sequence labeling) for Iberian languages using a zero-shot framework. Classification tasks use Macro-F1 scoring, generation uses ROUGE-1, and sequence labeling uses F1 via seqeval. The evaluation employs the lm-evaluation-harness framework with custom extensions, running on 2x 48GB A6000 GPUs. Models are evaluated without in-context examples to assess pretrained capabilities, with sequence labeling treated as constrained generation. The benchmark maintains private dataset repositories on HuggingFace, with an expert committee reviewing submissions and updating a public leaderboard.

## Key Results
- LLMs perform systematically worse on industry-relevant tasks (e.g., toxicity detection) than on fundamental NLP tasks under zero-shot conditions.
- Performance degradation in Galician and Basque is driven by underrepresentation in pretraining data and linguistic distance from high-resource languages.
- Zero-shot LLMs underperform compared to shared-task systems due to lack of task-specific supervision, though the authors expect few-shot conditions may reverse this gap.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs perform systematically worse on industry-relevant tasks than on fundamental NLP tasks under zero-shot conditions.
- **Mechanism:** Industry tasks require domain-specific priors and fine-grained judgment not well-captured during general pretraining, while fundamental tasks align more closely with the language modeling objective.
- **Core assumption:** Zero-shot performance reflects the distribution of capabilities acquired during pretraining.
- **Evidence anchors:** Abstract states LLMs perform worse on industry-relevant tasks; Figure 6 shows fundamental tasks averaging ~50% vs ~35% for industry tasks; MGT Detection performs below random baseline.
- **Break condition:** Adding few-shot examples or fine-tuning on industry task corpora may narrow the gap.

### Mechanism 2
- **Claim:** Performance degradation in Galician and Basque is driven by underrepresentation in pretraining data and linguistic distance from high-resource languages.
- **Mechanism:** Galician and Basque receive fewer tokens during pretraining; Basque additionally lacks lexical overlap with Romance languages, limiting cross-lingual transfer.
- **Core assumption:** Training token distribution is a primary driver of downstream zero-shot capability.
- **Evidence anchors:** Abstract notes lower performance for Galician and Basque; only three models surpass random baseline for Basque; Latxa-Llama-3.1-8B-Instruct improves in Basque but scores worse in other languages.
- **Break condition:** Models with substantial Galician or Basque pretraining narrow but don't eliminate the gap.

### Mechanism 3
- **Claim:** Zero-shot LLMs underperform compared to shared-task systems because they lack task-specific supervision.
- **Mechanism:** Shared-task systems are fine-tuned on thousands of labeled examples per task, learning task-specific decision boundaries, while zero-shot LLMs rely solely on verbalized task descriptions.
- **Core assumption:** The verbalization function adequately conveys task semantics.
- **Evidence anchors:** Abstract states LLMs perform above random but below shared task systems; Sentiment Analysis best LLM: 48.83% vs shared task average 61.87%.
- **Break condition:** Introducing few-shot examples or task-specific fine-tuning would likely close the gap.

## Foundational Learning

- **Concept: Zero-shot evaluation**
  - **Why needed here:** IberBench evaluates all models without in-context examples to assess pretrained capabilities and reflect real-world usage.
  - **Quick check question:** If you add 3-shot examples to sentiment analysis, should you expect higher Macro-F1? (Yes, but results may vary with example selection and ordering.)

- **Concept: Macro-F1 scoring**
  - **Why needed here:** Used for classification tasks to mitigate label imbalance and enable comparison across datasets with different class distributions.
  - **Quick check question:** Why is Macro-F1 preferred over accuracy for sentiment analysis with imbalanced classes? (Macro-F1 averages per-class F1, preventing majority class dominance.)

- **Concept: Sequence labeling via text generation**
  - **Why needed here:** IberBench treats NER/ABSA as constrained generation tasks where LLMs output tagged spans, which are parsed back to IOB labels.
  - **Quick check question:** What happens if the LLM generates malformed tags? (The parser fails to extract spans; F1 drops; <14B models struggle with format-following.)

## Architecture Onboarding

- **Component map:** Leaderboard UI -> Organization (expert committee) -> Datasets (private HuggingFace repos) -> Evaluation framework (lm-evaluation-harness) -> 2x 48GB A6000 GPUs

- **Critical path:** User requests model via Leaderboard UI → Organization reviews request → Approved model → Evaluation script runs on-premise → Results cached in HuggingFace repo → Leaderboard UI updates

- **Design tradeoffs:**
  - Zero-shot vs few-shot: Zero-shot chosen for consistency and realism but underestimates potential performance
  - Private dataset hosting: Prevents contamination but limits transparency; creators credited in UI
  - Committee moderation: Ensures quality but introduces latency; community cannot self-submit without review
  - On-premise evaluation: Avoids CPU limitations but restricts scalability for larger models

- **Failure signatures:**
  - Sequence labeling near-random performance: Models <14B fail to follow annotation schema
  - MGT detection below random: All LLMs fail; suggests task requires specialized detectors
  - Catastrophic forgetting: Basque-specialized models (Latxa) degrade on other languages

- **First 3 experiments:**
  1. Add 3-shot examples to Lexical Analysis to test whether format-following improves and approaches shared-task baselines (~80% vs current ~32%)
  2. Evaluate a larger multilingual model (e.g., Llama-3.1-70B) to verify whether scale closes the industry-task gap and improves Basque/Galician performance
  3. Run cross-dataset contamination checks on newly added datasets to ensure no overlap with public pretraining corpora

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can larger models (>14B parameters) effectively solve sequence labeling tasks like lexical borrowing detection using generative prompting?
- **Basis in paper:** Section 4.2.2 states regarding poor results in lexical analysis: "It remains uncertain whether larger models will be able to address the task effectively."
- **Why unresolved:** The study limited evaluation to models up to 14 billion parameters due to computational resources, and models within this range struggled to follow the annotation schema.
- **What evidence would resolve it:** Evaluating larger models (e.g., 70B parameters) on the lexical borrowing chunking task to see if performance exceeds the near-random baseline.

### Open Question 2
- **Question:** Can LLMs match or exceed the performance of fine-tuned shared-task systems under few-shot or many-shot conditions?
- **Basis in paper:** Section 4.2.2 notes that LLMs currently underperform shared-task systems but states: "We expect that under few- or many-shot conditions, the evaluated LLMs will outperform the best published results."
- **Why unresolved:** The authors utilized a zero-shot setting to ensure consistency and reflect real-world usage, leaving the potential gains from in-context learning untested.
- **What evidence would resolve it:** A comparative study measuring LLM performance on IberBench tasks when provided with varying numbers of in-context examples compared to top scores from shared-task leaderboards.

### Open Question 3
- **Question:** How sensitive are the evaluation results to the specific phrasing and formatting of prompts across different Iberian languages?
- **Basis in paper:** The Limitations section acknowledges that "Alternative prompts could potentially yield better performance" and notes that exploring prompt variations was not feasible.
- **Why unresolved:** The benchmark relies on a single prompt design per task, yet LLMs are known to be highly sensitive to prompt engineering.
- **What evidence would resolve it:** A robustness analysis running the benchmark with multiple semantically equivalent prompt templates to measure performance variance.

## Limitations

- The zero-shot evaluation framework inherently underestimates LLM performance compared to fine-tuned baselines, making direct comparisons to shared-task systems potentially misleading.
- Dataset quality and distribution remain partially opaque, with limited ability to verify pretraining contamination for newly added datasets.
- The private hosting of datasets limits external verification and reproducibility, though source links and YAML configurations are provided.
- Evaluation is restricted to models up to 14B parameters due to GPU memory constraints, leaving open whether larger models would significantly alter conclusions.

## Confidence

- **High confidence:** Mechanism 1 (LLMs worse on industry tasks than fundamental ones) - supported by clear quantitative gaps in Figure 6 and consistent underperformance across all evaluated models.
- **High confidence:** Mechanism 2 (Galician and Basque underrepresentation drives performance gaps) - strongly supported by token distribution data, specialized model comparisons, and corroborating work on multilingual data reweighting.
- **Medium confidence:** Mechanism 3 (zero-shot LLMs underperform shared-task systems due to lack of supervision) - well-supported by metric comparisons, but the exact contribution of prompt formulation versus lack of fine-tuning remains uncertain.

## Next Checks

1. **Test few-shot prompting impact:** Add 3-shot examples to lexical analysis and sentiment analysis tasks to measure whether performance approaches shared-task baselines, isolating the effect of supervision from model capability.
2. **Scale validation:** Evaluate a larger multilingual model (e.g., Llama-3.1-70B) to determine if scale alone closes the industry-task performance gap and improves low-resource language outcomes.
3. **Pretraining contamination audit:** Conduct systematic cross-dataset contamination checks on newly added datasets using n-gram overlap analysis to ensure IberBench maintains its integrity as a zero-shot evaluation benchmark.