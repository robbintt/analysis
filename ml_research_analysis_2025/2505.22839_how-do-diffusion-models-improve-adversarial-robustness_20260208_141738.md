---
ver: rpa2
title: How Do Diffusion Models Improve Adversarial Robustness?
arxiv_id: '2505.22839'
source_url: https://arxiv.org/abs/2505.22839
tags:
- diffusion
- robustness
- adversarial
- clean
- purification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates how diffusion models improve\
  \ adversarial robustness in image classification. The authors find that diffusion\
  \ models paradoxically increase \u2113p distances to clean images rather than reducing\
  \ them, challenging the common denoising hypothesis."
---

# How Do Diffusion Models Improve Adversarial Robustness?

## Quick Facts
- **arXiv ID:** 2505.22839
- **Source URL:** https://arxiv.org/abs/2505.22839
- **Authors:** Liu Yuezhang; Xue-Xin Wei
- **Reference count:** 36
- **Primary result:** Diffusion models improve robustness through deterministic compression rather than stochastic denoising; controlling randomness reduces robustness gains from ~70% to ~24%.

## Executive Summary
This study systematically investigates how diffusion models improve adversarial robustness in image classification. The authors find that diffusion models paradoxically increase ℓp distances to clean images rather than reducing them, challenging the common denoising hypothesis. By controlling randomness during both attack and evaluation, the actual robustness gain drops from ~70% to approximately 24% on CIFAR-10. Crucially, the remaining robustness gain strongly correlates with the model's compression rate of input space, which the authors identify as a reliable robustness indicator requiring no gradient calculations. These findings reveal that diffusion-based robustness stems from deterministic compression effects rather than stochasticity, offering guidance for developing more effective adversarial purification systems.

## Method Summary
The paper evaluates diffusion-based adversarial purification (DiffPure) by controlling randomness during both attack and evaluation phases. Using a hashing function to fix random seeds, they measure robust accuracy under PGD and BPDA attacks with synchronized randomness. They compute compression rates from Jacobian singular values at clean images, measuring how much the purification function shrinks input neighborhoods. The study compares full DiffPure, Reverse-only, and DDIM sampling across CIFAR-10 and ImageNet, measuring ℓp distances, SSIM, FID scores, and signal-to-noise ratios to isolate the mechanisms behind robustness gains.

## Key Results
- Controlling randomness during attack and evaluation reduces robustness gain from ~70% to ~24% on CIFAR-10
- Diffusion models increase ℓp distances to clean images (ℓ2 from ~1.1 to 2.8-3.6) rather than denoising them
- Remaining robustness gain strongly correlates with compression rate of input space
- Compression rate follows a sigmoid relationship with robustness across different sampling methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion models improve robustness primarily by compressing the input space around "anchor points" (purified clean images), reducing effective attack budget.
- **Mechanism:** The Jacobian of the purification function at clean images exhibits strong compression—over 90% of singular values are below 0.25. This shrinks the adversarial neighborhood radius from ~1.0 to ~0.24, making it less likely to intersect classification boundaries.
- **Core assumption:** Adversarial robustness degrades predictably with smaller effective perturbation budgets; compression behaves like reducing attack epsilon.
- **Evidence anchors:**
  - [abstract] "the remaining robustness gain strongly correlates with the model's compression rate of input space"
  - [Section 5] Compression-robustness follows a sigmoid relationship across DDPM, Reverse-only, and DDIM samplers; curve extrapolates to clean accuracy at zero compression.
  - [corpus] FlowPure and related purification work assume denoising toward manifold; this paper challenges that mechanism directly—compression, not denoising, is key.
- **Break condition:** If compression rate approaches 1.0 (no compression), robustness gain should vanish. Confirmed in experiments with low timesteps.

### Mechanism 2
- **Claim:** Internal randomness in diffusion models creates an effective defense through gradient masking, but this is partially an artifact of suboptimal attacks rather than true robustness.
- **Mechanism:** Randomness causes ξ_attack ≠ ξ_test, making attacks optimize against different functions than evaluated—effectively transfer attacks. EOT only partially mitigates this (gradient correlation ~0.17 with optimal).
- **Core assumption:** If attack and test randomness are aligned, the "robustness" from stochasticity disappears.
- **Evidence anchors:**
  - [abstract] "By controlling randomness during both attack and evaluation, the actual robustness gain drops from ~70% to approximately 24%"
  - [Section 4] Transfer attacks across random configurations recover high apparent robustness (~77% on CIFAR-10), matching prior reports.
  - [corpus] Weak corpus evidence for this specific randomness analysis; this appears to be a novel contribution.
- **Break condition:** If EOT gradients perfectly correlated with fixed-randomness optimal gradients, randomness-based robustness would be genuine. Data shows this doesn't hold.

### Mechanism 3
- **Claim:** Diffusion models do not "denoise" adversarial images toward clean samples; they push them further away in ℓp distance.
- **Mechanism:** Purification maps inputs to "anchor points" determined by randomness configuration. Perturbed images move toward these anchors, which are typically farther from clean images than the original adversarial perturbation.
- **Core assumption:** The common intuition that purification recovers clean data is incorrect for local adversarial neighborhoods.
- **Evidence anchors:**
  - [abstract] "diffusion models paradoxically increase ℓp distances to clean images rather than reducing them"
  - [Section 3.1] ℓ2 distance increases from ~1.1 to 2.8-3.6 across DDPM/Reverse/DDIM; SSIM decreases (0.965→0.796-0.869); FID improves (distributional) but doesn't predict robustness.
  - [corpus] Graph Defense Diffusion Model and related purification work assume manifold projection—this paper suggests that mechanism is incomplete.
- **Break condition:** If diffusion models truly denoised, ℓp distances should decrease. They consistently increase across all tested settings.

## Foundational Learning

- **Concept: Adversarial Purification Framework**
  - **Why needed here:** Understanding that purification transforms x → f(x) before classification is essential for analyzing where robustness gains originate.
  - **Quick check question:** Can you explain why purification-based defenses differ fundamentally from adversarial training in their approach to robustness?

- **Concept: Expectation Over Transformation (EOT)**
  - **Why needed here:** The paper critiques EOT as insufficient for stochastic purification—understanding why requires knowing what EOT was designed to solve.
  - **Quick check question:** Why does averaging gradients over randomness not guarantee optimal attacks against stochastic defenses?

- **Concept: Jacobian-based Local Analysis**
  - **Why needed here:** Compression rate is derived from Jacobian singular values at clean images; this connects local geometry to robustness.
  - **Quick check question:** What does a Jacobian with singular values < 1 imply about how a function transforms a small input neighborhood?

## Architecture Onboarding

- **Component map:** Input → forward diffusion (t steps) → reverse denoising (t steps) → purified image → classifier → prediction
- **Critical path:** Input → forward diffusion (t steps) → reverse denoising (t steps) → purified image → classifier → prediction
- **Design tradeoffs:**
  - More timesteps → lower compression rate → higher robustness, but clean accuracy drops (over-purification misclassifies)
  - Stochastic (DDPM) vs deterministic (DDIM): DDIM has lower variance but worse robustness in practice
  - Reverse-only purification: removes forward randomness, achieves better compression than full DiffPure
- **Failure signatures:**
  - High apparent robustness with EOT but low robustness with fixed randomness = gradient masking
  - Compression rate > 0.8 → minimal robustness gain regardless of other settings
  - Timesteps > 300 → potential gradient explosion/vanishing causing spurious robustness estimates
- **First 3 experiments:**
  1. Replicate the compression rate measurement: sample 50 perturbations around clean images, compute ∥f(x) - f(x₀)∥ / ∥x - x₀∥ with fixed randomness.
  2. Run PGD attack with fixed vs. varying randomness on the same model; compare robust accuracy.
  3. Plot compression rate vs. robustness across 3+ timestep settings (e.g., t=50, 100, 200) to verify sigmoid relationship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a novel purification system be explicitly designed to maximize the compression rate while maintaining high anchor point accuracy?
- Basis in paper: [explicit] The authors state that developing a system based on these criteria "remains an exciting direction for future research" (Section 6).
- Why unresolved: The study analyzes existing diffusion implementations (DDPM, DDIM) but does not propose or train a new architecture optimized specifically for the identified compression mechanism.
- What evidence would resolve it: The creation of a model that surpasses current diffusion robustness by strictly optimizing for the compression rate metric without relying on stochasticity.

### Open Question 2
- Question: Does the lawful sigmoid relationship between compression rate and robustness persist on ImageNet when using full-gradient attacks?
- Basis in paper: [inferred] The authors acknowledge they were "unable to perform full gradient-based attacks on ImageNet" due to computational constraints, limiting the validation of their theory on large-scale datasets (Section 6).
- Why unresolved: The theory is confirmed on CIFAR-10 with strong attacks, but the ImageNet validation relies on BPDA-EOT rather than the more rigorous full-gradient PGD, leaving the scalability of the finding uncertain.
- What evidence would resolve it: Replicating the compression rate analysis on ImageNet using full-gradient PGD attacks to verify if the sigmoid predictive curve still holds.

### Open Question 3
- Question: Why do diffusion models consistently increase ℓp distances to clean images rather than acting as traditional denoisers?
- Basis in paper: [explicit] The authors identify this as an "intriguing behavior" and hypothesize that clean images may not reside on local peaks of the learned priors, but do not provide a definitive causal mechanism (Section 3.1, Appendix C.2).
- Why unresolved: The paper establishes that the "denoising" hypothesis is false and the effect persists even without forward process noise, but the underlying geometry of the model's prior relative to the data manifold remains conceptual.
- What evidence would resolve it: A theoretical or empirical analysis mapping the loss landscape of the diffusion prior to prove that clean images sit in "valleys" rather than "peaks," necessitating an initial push-away effect.

## Limitations

- The study focuses on white-box attacks and doesn't fully explore black-box or transfer attack scenarios
- ImageNet validation is limited by computational constraints, using BPDA-EOT instead of full-gradient attacks
- The theoretical connection between Jacobian compression and adversarial budget reduction relies on local linear approximations that may not hold at larger perturbation scales

## Confidence

- **Compression rate mechanism:** Medium - well-supported empirically but theoretical connection to adversarial budget needs more rigorous validation
- **Randomness masking effects:** High - clear demonstration through fixed-randomness experiments showing dramatic drop in robust accuracy
- **Diffusion does not denoise:** High - straightforward ℓp distance measurements consistently show increase
- **EOT insufficiency:** Medium - gradient correlation data is limited; more comprehensive analysis would strengthen this claim

## Next Checks

1. **Verify robustness predictor generalization:** Test whether compression rate at clean images predicts robustness against adaptive attacks on naturally trained classifiers (not just DiffPure), to confirm it's a general robustness indicator rather than a DiffPure-specific artifact.

2. **Expand randomness analysis:** Systematically measure gradient correlation across different EOT sample counts and attack budgets to determine if there's a regime where randomness effects become genuinely robust rather than masked.

3. **Manifold geometry investigation:** Examine whether purified images lie on the data manifold by analyzing their nearest neighbors in feature space and measuring reconstruction quality from compressed representations, to better understand what "compression" actually achieves geometrically.