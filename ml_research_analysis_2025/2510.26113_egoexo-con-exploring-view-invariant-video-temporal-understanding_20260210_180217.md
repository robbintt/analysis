---
ver: rpa2
title: 'EgoExo-Con: Exploring View-Invariant Video Temporal Understanding'
arxiv_id: '2510.26113'
source_url: https://arxiv.org/abs/2510.26113
tags:
- video
- temporal
- arxiv
- reasoning
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EgoExo-Con, a benchmark for evaluating view-invariant
  temporal understanding in Video-LLMs using synchronized egocentric-exocentric video
  pairs. The benchmark tests temporal verification and grounding tasks, assessing
  both accuracy and cross-view consistency.
---

# EgoExo-Con: Exploring View-Invariant Video Temporal Understanding

## Quick Facts
- **arXiv ID:** 2510.26113
- **Source URL:** https://arxiv.org/abs/2510.26113
- **Reference count:** 38
- **Key outcome:** Existing models struggle with cross-view consistency, achieving barely half their single-view performance; View-GRPO significantly improves consistency over SFT and standard GRPO.

## Executive Summary
EgoExo-Con introduces a benchmark for evaluating view-invariant temporal understanding in Video-LLMs using synchronized egocentric-exocentric video pairs. The benchmark tests temporal verification and grounding tasks, assessing both accuracy and cross-view consistency. Analysis reveals existing models struggle with consistency, achieving barely half their single-view performance, and naive multi-view training often underperforms single-view approaches. To address this, the authors propose View-GRPO, a reinforcement learning framework that generates viewpoint-specific reasoning while promoting consistent conclusions. View-GRPO significantly improves cross-view consistency over SFT and standard GRPO, demonstrating its effectiveness for robust, view-invariant video temporal understanding.

## Method Summary
The method introduces View-GRPO, a reinforcement learning framework for training Video-LLMs on synchronized ego-exo video pairs. It generates viewpoint-specific reasoning chains via GPT-5 for a curated training set (View30K), then optimizes the model using group-relative rewards that combine accuracy, format compliance, and reasoning similarity (measured by an LLM-judge). The approach explicitly promotes consistent conclusions across viewpoints rather than forcing identical intermediate representations.

## Key Results
- Cross-view consistency scores barely reach half of single-view performance on existing models
- Naive multi-view supervised fine-tuning often underperforms single-view training due to conflicting priors
- View-GRPO significantly improves cross-view consistency compared to both SFT and standard GRPO
- Viewpoint-specific reasoning chains with consistency-convergent conclusions prove more effective than direct output matching

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Viewpoint-specific reasoning chains with consistency-convergent conclusions improve cross-view temporal understanding more effectively than direct output matching.
- **Mechanism:** View-GRPO generates separate reasoning paths for ego and exo views while using a reasoning reward to align the final conclusions. This allows models to leverage view-specific evidence rather than forcing identical intermediate representations.
- **Core assumption:** Models can learn shared temporal abstractions even when visual features differ across viewpoints.
- **Evidence anchors:** [abstract] "View-GRPO... generates viewpoint-specific reasoning while promoting consistent conclusions"; [Section 5.2] "Rather than simply enforcing identical outputs, our approach explicitly promotes robust reasoning across viewpoints"
- **Break condition:** If reasoning chains are unreliable or the LLM-judge provides poorly calibrated scores, the reward signal degrades. Figure 6 shows Qwen2.5-0.5B produces overly high reasoning rewards early, causing consistency degradation (−3% G-EgoExo).

### Mechanism 2
- **Claim:** Group-relative reward normalization stabilizes multi-objective RL for video temporal tasks.
- **Mechanism:** GRPO standardizes rewards within groups of candidate responses, reducing variance compared to absolute scoring. This helps when combining heterogeneous rewards (accuracy IoU, format compliance, reasoning similarity).
- **Core assumption:** Relative ranking within groups provides more stable gradients than absolute scores for temporal grounding tasks.
- **Evidence anchors:** [Section 5.1] "GRPO evaluates a set of responses... assigning rewards in relation to the group. This group-wise normalization helps reduce reward variance"; [Equation 1] Shows explicit normalization: `std({r(o_i)})` in denominator
- **Break condition:** If the group size G is too small, variance reduction is insufficient. If too large, computation becomes expensive without proportional benefit.

### Mechanism 3
- **Claim:** Naive multi-view SFT degrades performance because merging viewpoints introduces conflicting priors without explicit alignment.
- **Mechanism:** When training on both ego and exo views simultaneously via standard supervised learning, models receive contradictory visual features for the same temporal labels, learning view-specific shortcuts rather than view-invariant representations.
- **Core assumption:** Single-view training allows models to learn stable feature-label mappings; multi-view training without alignment disrupts this.
- **Evidence anchors:** [abstract] "When naively finetuned with synchronized videos of both viewpoints, the models show improved consistency but often underperform those trained on a single view"; [Table 2] TimeSuite shows 8.1% consistency gap between EgoExo training and Exo-only training on CharadesEgo
- **Break condition:** If training data is sufficiently diverse and balanced, or if visual encoders are pretrained with cross-view alignment, the conflict may be reduced.

## Foundational Learning

- **Concept: Temporal Grounding vs. Verification**
  - **Why needed here:** The benchmark uses two distinct tasks—verification (binary QA about event occurrence) and grounding (timestamp localization)—requiring different reward formulations.
  - **Quick check question:** Can you explain why IoU threshold of 0.5 is used for grounding correctness, and how verification uses binary accuracy?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** View-GRPO builds on GRPO; understanding the base algorithm is essential before the multi-view extension.
  - **Quick check question:** How does GRPO's group-wise reward normalization differ from PPO's advantage estimation?

- **Concept: Cross-View Consistency Metrics**
  - **Why needed here:** The key innovation is measuring not just single-view accuracy but whether both views produce correct answers simultaneously (V-EgoExo, G-EgoExo).
  - **Quick check question:** Why does consistency require both answers to be correct, not just identical?

## Architecture Onboarding

- **Component map:** Synchronized ego-exo video pairs (2 FPS sampling, max 14×16×16 pixels) → Frozen Qwen2.5-VL visual encoder → GPT-5 reasoning chain generation (View30K: 3.3k videos, 30k instances) → Qwen2.5-3B LLM-judge → Reward combiner (r = r_acc + r_form + r_sim) → AdamW optimizer (lr=1e-6, 8×A100 GPUs)

- **Critical path:** Data curation → Reasoning chain generation (GPT-5) → Filtering (tIoU > 0.7) → GRPO training with multi-component rewards → Evaluation on EgoExo-Con

- **Design tradeoffs:**
  - **Judge model size:** Qwen2.5-0.5B is faster but poorly calibrated; Qwen2.5-3B is more reliable
  - **Frame sampling:** 2 FPS balances temporal resolution with compute; 1 FPS used for reasoning generation
  - **Frozen vs. unfrozen visual encoder:** Frozen prevents overfitting on limited data (Table 4 shows unfrozen often underperforms)

- **Failure signatures:**
  - Consistency scores barely half of single-view performance → model relying on view-specific biases
  - High reasoning rewards from early training steps → poorly calibrated judge (Figure 6)
  - Multi-view SFT underperforming single-view → conflicting priors not resolved

- **First 3 experiments:**
  1. **Reproduce baseline gaps:** Run zero-shot evaluation on EgoExo-Con to confirm reported consistency deficits (e.g., Qwen2.5-VL-7B: G-EgoExo 6.9 vs G-Exo 14.2)
  2. **Ablate reasoning reward:** Train View-GRPO without r_sim to isolate the contribution of reasoning alignment (compare Table 3 GRPO vs View-GRPO)
  3. **Validate judge calibration:** Compare Qwen2.5-0.5B vs Qwen2.5-3B as judges, tracking reasoning reward curves and final consistency scores (Figure 6 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the selection and calibration of the LLM-as-judge influence the optimization stability and final performance of reinforcement learning frameworks like View-GRPO?
- **Basis in paper:** [explicit] The authors state in Section 5.4 that the role of LLM-judges in optimization "remains unclear" and that smaller models raise "calibration concerns," leading to consistency degradation, explicitly marking this for "future work."
- **Why unresolved:** The paper demonstrates that weak judges (e.g., Qwen2.5-0.5B) produce unstable, overly high rewards early in training, but it does not identify the threshold or characteristics required for a judge to be reliable.
- **What evidence would resolve it:** A systematic ablation study correlating the reasoning capabilities and calibration scores of various LLM judges against the resulting cross-view consistency scores of the trained Video-LLM.

### Open Question 2
- **Question:** Can end-to-end fine-tuning of visual encoders improve cross-view consistency if the training data scale is significantly increased?
- **Basis in paper:** [explicit] Appendix C.1 notes that unfreezing visual encoders resulted in performance drops, which the authors conjecture is due to the "limited scale of training data" (View30K) being insufficient to support tuning large visual backbones.
- **Why unresolved:** It remains undetermined if the visual features from frozen pretrained encoders are already optimal for view-invariance, or if they simply cannot be improved without overfitting on the current dataset size.
- **What evidence would resolve it:** Repeating the View-GRPO training pipeline on a dataset 5-10x larger than View30K while unfreezing the visual encoder to observe if consistency metrics improve over the frozen baseline.

### Open Question 3
- **Question:** Is the reasoning reward mechanism robust to noise or errors in the ground-truth reasoning chains generated by the teacher model?
- **Basis in paper:** [inferred] The View-GRPO framework relies on GPT-5 to generate the "target" reasoning chains for the View30K dataset (Section 5.2), but strictly filters out samples where the teacher predicts low tIoU.
- **Why unresolved:** The method depends on high-quality teacher reasoning; it is unclear if the reward signal degrades gracefully if the teacher model produces plausible but flawed reasoning chains for ambiguous video pairs.
- **What evidence would resolve it:** Experiments introducing synthetic noise into the reasoning chains used for training to measure the degradation rate of the model's consistency and accuracy.

## Limitations

- Limited model scale exploration (only 3B/7B Qwen2.5-VL variants tested)
- No systematic ablation on frozen vs unfrozen visual encoder beyond single data point
- View30K generation cost and quality validation not detailed
- Judge calibration sensitivity shown qualitatively but not systematically tested

## Confidence

- **High confidence:** Cross-view consistency deficit exists (multiple baselines confirm), View-GRPO improves consistency vs naive multi-view SFT
- **Medium confidence:** Mechanism claims for view-specific reasoning chains (limited ablation on reasoning reward component)
- **Low confidence:** Scalability claims beyond 7B models, real-world robustness beyond curated benchmark

## Next Checks

1. **Ablation study:** Train View-GRPO without reasoning reward (r_sim=0) to isolate contribution of viewpoint-specific alignment vs accuracy-focused training
2. **Judge calibration sweep:** Systematically compare Qwen2.5-0.5B, 1.5B, and 3B as judges across training runs, tracking consistency curves
3. **Encoder ablations:** Compare frozen vs unfrozen visual encoder training with matched compute budgets to quantify overfitting prevention vs adaptation capacity