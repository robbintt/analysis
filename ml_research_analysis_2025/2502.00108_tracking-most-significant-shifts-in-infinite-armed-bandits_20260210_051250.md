---
ver: rpa2
title: Tracking Most Significant Shifts in Infinite-Armed Bandits
arxiv_id: '2502.00108'
source_url: https://arxiv.org/abs/2502.00108
tags:
- regret
- log3
- algorithm
- bound
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies infinite-armed bandits with non-stationary rewards
  where actions' mean rewards evolve over time. Prior approaches relied on parameter
  knowledge of non-stationarity and restrictive distributional assumptions on the
  reservoir distribution.
---

# Tracking Most Significant Shifts in Infinite-Armed Bandits

## Quick Facts
- **arXiv ID**: 2502.00108
- **Source URL**: https://arxiv.org/abs/2502.00108
- **Reference count**: 40
- **Primary result**: Parameter-free algorithms for infinite-armed non-stationary bandits achieving optimal regret bounds without requiring knowledge of change parameters

## Executive Summary
This paper addresses infinite-armed bandits with non-stationary rewards where arm means evolve over time. Unlike prior approaches that require knowledge of non-stationarity parameters and restrictive distributional assumptions, the authors introduce a blackbox scheme that converts finite-armed MAB algorithms into parameter-free algorithms for this setting. Their key innovation is tracking "significant shifts" - the most severe rotting changes in rewards - allowing for tighter regret bounds that depend only on rotting variation rather than total variation. The elimination algorithm achieves optimal rates without requiring upper bounds on reservoir masses, providing simpler analysis compared to previous methods.

## Method Summary
The approach uses two main algorithms: a blackbox conversion scheme and an elimination-based method. Algorithm 1 subsamples a finite set of arms from the infinite reservoir (size growing as $2^{m \cdot \beta/(\beta+1)}$) and runs a standard finite MAB algorithm on this subsample. It restarts when cumulative empirical regret exceeds a threshold, using regret tracking as a proxy for non-stationarity detection. Algorithm 2 uses variance-aware elimination with importance-weighted gap estimates, eliminating arms only when rotting rewards increase their gaps. Both algorithms work under the β-regular reservoir assumption and rested non-stationarity (rewards change only when arms are played).

## Key Results
- Achieves optimal regret bounds of $(L+1)^{1/(\beta+1)} \cdot T^{\beta/(\beta+1)}$ and $(V_R^{1/(\beta+2)} \cdot T^{(\beta+1)/(\beta+2)} + T^{\beta/(\beta+1)})$
- Introduces "significant shifts" tracking that captures only the most severe rotting changes
- Elimination algorithm avoids requiring upper bounds on reservoir tail probabilities
- Experiments validate superior performance compared to state-of-the-art algorithms like SSUCB and AUCBT-AW

## Why This Works (Mechanism)

### Mechanism 1: Regret Tracking as a Non-Stationarity Proxy
Monitoring cumulative empirical regret allows detection of non-stationarity without prior knowledge of change parameters. The algorithm tracks the sum of empirical gaps $\sum (1 - Y_t(a_t))$ over blocks. In stationary environments, this sum concentrates around optimal regret rates. Exceeding a dynamic threshold implies environment change or arm degradation, triggering restart. This works under bounded rewards $[0,1]$ and β-regular reservoir assumptions.

### Mechanism 2: Subsampling to Reduce Dimensionality
The infinite-armed problem reduces to finite-armed by subsampling $S_m \approx 2^{m \cdot \beta/(\beta+1)}$ arms. Under β-regular assumptions, this subsample contains at least one competitive arm with high probability. A standard finite MAB algorithm runs on this reduced set. The subsampling rate ensures the best arm has gap $O(S_m^{-1/\beta})$ with high probability.

### Mechanism 3: Variance-Aware Elimination for Rotting Rewards
Randomized elimination distinguishes rotting from rising rewards. The algorithm maintains importance-weighted gap estimates $\hat{\delta}^{IW}$ and eliminates arms only when cumulative estimated gaps exceed thresholds. Rising rewards reduce gaps and don't trigger elimination; only rotting rewards increase gaps. This ensures regret bounds depend only on rotting variation $V_R$, not total variation $V$.

## Foundational Learning

- **Concept: Rested vs. Restless Bandits**
  - Why needed: The paper explicitly assumes "rested" non-stationarity where rewards change only when specific arms are pulled
  - Quick check: If I pull Arm A and the reward changes, does the reward distribution of Arm B (unplayed) change in this model? (Answer: No)

- **Concept: $\beta$-Regular Reservoir Distribution**
  - Why needed: All regret bounds and subsampling rates depend on $\beta$, describing how likely we are to sample near-optimal arms
  - Quick check: If $\beta$ is high (heavy tail), do we need to subsample more or fewer arms to find a good one? (Answer: Fewer)

- **Concept: Significant Shifts (Definition 3)**
  - Why needed: This defines non-stationarity by changes large enough to make the current best subsampled arm "unsafe"
  - Quick check: Does a "significant shift" occur if the best arm improves its reward slightly? (Answer: No, shifts are triggered by rotting/decay)

## Architecture Onboarding

- **Component map**: Input context → Doubling blocks → Subsample arms → Run Base-Alg → Regret test → Restart/Continue
- **Critical path**: The Regret Test (Line 8, Algorithm 1) is the critical decision node. If $\sum \hat{\delta}_t(a_t) \ge C_1 \cdot |A_m| \cdot \log^3(T)$, the system wipes state and re-samples
- **Design tradeoffs**: Use Algorithm 1 (Blackbox) for extensibility with specific Base-Alg; use Algorithm 2 (Elimination) for rotting rewards and tightest $V^{1/3}T^{2/3}$ bounds. Subsampling rate $S_m$ depends on $\beta$ - optimal performance requires correct parameterization
- **Failure signatures**: Constant restarting if $C_1$ too small or noise high; stagnation in rotting env if threshold too high
- **First 3 experiments**:
  1. Stationary Baseline: Run Algorithm 1 with UCB1 on stationary β-regular reservoir, verify minimax rate $\tilde{O}(T^{\beta/(\beta+1)})$
  2. Rotting Rewards Simulation: Implement synthetic setup with arm means rotting at rate $1/t$, compare Algorithm 2 vs. Algorithm 1 vs. SSUCB
  3. Significant Shift Detection: Manually inject massive rot in best arm at $t=T/2$, plot empirical regret to verify immediate restart trigger

## Open Questions the Paper Calls Out

### Open Question 1
Can the blackbox scheme (Algorithm 1) be modified to achieve optimal regret bounds for β < 1? Theorem 2 shows the blackbox achieves suboptimal regret $O(\sqrt{(L+1) \cdot T})$ for β < 1, while elimination achieves optimal bounds. The dyadic gridding introduces an extra term preventing tight bounds for β < 1.

### Open Question 2
Can adaptive dynamic regret bounds of style $V^{1/3}T^{2/3} \wedge \sqrt{LT}$ be achieved in finite-armed setting with bandit feedback and adaptively adversarial changes? The paper claims this is unknown even in finite-armed setting, as existing algorithms assume oblivious adversaries.

### Open Question 3
Can regret bounds be improved to depend only on rotting non-stationarity in the blackbox scheme? The blackbox depends on total variation V while elimination depends only on rotting variation VR, because blackbox's gap analysis uses upper bounds on reservoir tail probabilities (κ₂).

### Open Question 4
What are the lower bounds for non-stationary infinite-armed bandits with general (non-rotting) adversarial changes? Existing lower bounds only apply to rotting scenarios; the general adversarial case with both rising and rotting changes lacks characterization.

## Limitations

- The algorithms assume rested non-stationarity, where rewards change only when arms are played, which may not capture all real-world scenarios
- Empirical tuning of threshold constants C₁ and C₂ is not specified, leaving sensitivity to parameter choices open
- The variance-aware elimination relies on importance-weighted estimates that may fail if noise variance significantly exceeds mean gap

## Confidence

- **High Confidence**: Using empirical regret as non-stationarity proxy (Mechanism 1) is mathematically rigorous with strong theoretical support
- **Medium Confidence**: Subsampling approach (Mechanism 2) depends on β-regular reservoir assumption which is reasonable but may not hold universally
- **Medium Confidence**: Variance-aware elimination for rotting rewards (Mechanism 3) shows theoretical promise but practical sensitivity to noise levels needs more exploration

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary C₁ and C₂ across multiple orders of magnitude to quantify sensitivity to threshold parameters, plotting regret performance against parameter values

2. **Restless Non-Stationarity Test**: Modify environment to allow reward changes for unplayed arms, then evaluate Algorithm 1 and 2 performance compared to rested non-stationarity guarantees

3. **Noise Variance Robustness**: Generate synthetic environments with varying noise-to-signal ratios (keeping rewards bounded in [0,1]) and measure how quickly Algorithm 2's importance-weighted estimates fail, comparing against theoretical concentration bounds