---
ver: rpa2
title: 'All You Need is One: Capsule Prompt Tuning with a Single Vector'
arxiv_id: '2510.16670'
source_url: https://arxiv.org/abs/2510.16670
tags:
- attention
- prompt
- input
- capt
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for efficient adaptation of
  large language models to downstream tasks using Capsule Prompt Tuning (CaPT). The
  key idea is to leverage instance-aware information alongside task-aware prompts
  to enhance model performance.
---

# All You Need is One: Capsule Prompt Tuning with a Single Vector

## Quick Facts
- **arXiv ID:** 2510.16670
- **Source URL:** https://arxiv.org/abs/2510.16670
- **Reference count:** 40
- **Primary result:** Achieves 84.03% average accuracy on T5-Large using only 0.003% of model parameters

## Executive Summary
This paper introduces Capsule Prompt Tuning (CaPT), a novel approach for efficient adaptation of large language models to downstream tasks. Unlike traditional soft prompts that require laborious grid search for optimal length, CaPT uses a single capsule prompt as an "attention anchor" to create stronger bidirectional interaction with input sequences. The method achieves superior performance across various language tasks while dramatically reducing parameter count and eliminating hyperparameter tuning overhead. By fusing instance-aware and task-aware information in a nearly parameter-free manner, CaPT offers a promising solution for efficient language model adaptation.

## Method Summary
CaPT leverages instance-aware information alongside task-aware prompts through a single capsule prompt injected into every Transformer layer. The capsule is constructed via element-wise addition of a learnable task vector and the mean of previous layer's hidden states, creating contextually grounded guidance. This "deep prompting" approach eliminates the need for grid search over prompt lengths while maintaining strong performance. The method works by creating an "attention anchor" effect where the capsule token receives widespread attention from the input sequence and attends to structural tokens, facilitating stronger bidirectional interaction than standard prompts.

## Key Results
- Achieves 84.03% average accuracy on T5-Large across multiple tasks
- Uses only 0.003% of model parameters on Llama3.2-1B
- Eliminates laborious grid search for optimal prompt length
- Demonstrates superior performance compared to traditional soft prompt methods
- Maintains 1.00x training time relative to baseline soft prompts

## Why This Works (Mechanism)

### Mechanism 1: Attention Anchor Formation
The paper suggests that incorporating instance-aware tokens at the earliest position triggers an "attention anchor" effect, facilitating stronger bidirectional interaction between the prompt and input sequence than standard task-aware prompts. This works by forcing the capsule token to actively attend to structural tokens in the input and receive widespread attention from the rest of the sequence, unlike standard soft prompts that primarily attend to themselves.

### Mechanism 2: Additive Instance-Task Integration
Effective guidance requires fusing general task instructions with specific input content through element-wise addition rather than treating them as separate token sequences. This forces the learnable task vector to shift its representation based on the specific instance context at every layer depth, ensuring the guidance signal is contextually grounded rather than a static offset.

### Mechanism 3: Search Space Collapse via Fixed Length
The "One Vector" design eliminates the performance bottleneck and computational overhead of searching for optimal prompt lengths. Traditional soft prompts require grid search over sequence length to balance information capacity vs. noise, but CaPT constrains capacity to exactly one token per layer, forcing more efficient information packing.

## Foundational Learning

- **Soft Prompt Tuning (PEFT)**: Understanding what CaPT modifies - unlike full fine-tuning, this method freezes LLM weights and only trains a small set of input embeddings. Quick check: Do you know the difference between appending a prompt to the input text (hard prompt) vs. prepending vectors to the embedding layer (soft prompt)?

- **Attention Mechanism (Query/Key/Value)**: The paper's core contribution relies on analyzing attention heatmaps. You must understand that Attention(Q, K) determines which tokens "look at" which other tokens. Quick check: Can you interpret an attention heatmap? Specifically, if row i is bright at column j, which token is attending to which?

- **Pooling Operations (Mean Pooling)**: CaPT synthesizes "instance-aware" information by averaging vectors. Understanding that this creates a global summary vector (losing positional info) is critical. Quick check: If you have a sequence of 10 embeddings, does taking the mean preserve the order of words in that sequence?

## Architecture Onboarding

- **Component map:** Raw text X, Learnable Vector p (shape: [1, hidden_dim]) -> Mean Pooling Module -> Adder (p + Mean(H)) -> Capsule S -> Injection into every Transformer layer
- **Critical path:** The implementation of the addition operation and the broadcasting of the single capsule token to match the batch size of the input sequence
- **Design tradeoffs:** Addition (Default) has lowest parameters (0.004%), fastest; Prepending keeps instance and task info separate; Projection (MLP) has highest capacity but significantly more parameters
- **Failure signatures:** Attention Sink Failure if visualization shows the capsule token is "invisible"; Performance Degradation on Large Models if larger models underperform; Instance Representation Quality issues if mean pooling loses critical nuances
- **First 3 experiments:** 1) Sanity Check (T5-Base/RTE) - verify validation accuracy of approx 79.78%; 2) Ablation (Instance vs. Task) - run zero-out tests for both components; 3) Attention Visualization - extract attention weights and plot heatmap to confirm "Yellow Box" pattern

## Open Questions the Paper Calls Out

### Open Question 1
How can the CaPT framework be adapted for visual or multi-modal tasks where embeddings generally lack the fixed structural tokens used to ground attention anchors in NLP? The paper notes that visual embeddings lack a fixed instruction template which the capsule prompt uses to ground more focused attention.

### Open Question 2
What specific architectural or pre-training factors cause the "suboptimal performance gain" of instance-aware prompts in decoder-only models (e.g., Llama) compared to encoder-decoder models (e.g., T5)? The paper identifies this performance discrepancy but relies on assumptions about pre-training objectives and architecture differences.

### Open Question 3
Why does the sparsely distributed application of capsule prompts (e.g., on odd layers only) outperform applying them to contiguous blocks of layers? The paper reports this empirical finding but provides no theoretical justification for why spacing out guidance signals leads to better contextual grounding.

## Limitations

- Attention anchor phenomenon primarily validated on T5 architecture, not cross-validated on decoder-only models with causal masking
- Single-vector constraint may create performance ceiling for complex reasoning tasks requiring nuanced instructions
- Mean pooling operation may lose critical semantic information for tasks requiring precise entity relationships or negation

## Confidence

**High Confidence (Experimental Claims):**
- Parameter efficiency metrics (0.003% of model parameters)
- Relative performance improvements over baseline soft prompts
- Search space elimination (no grid search required)

**Medium Confidence (Mechanism Claims):**
- Attention anchor phenomenon generalization across architectures
- Additive instance-task integration superiority over alternatives
- Capacity sufficiency of single-vector representation

**Low Confidence (Scalability Claims):**
- Performance maintenance on tasks requiring complex multi-step reasoning
- Effectiveness on extremely long input sequences where mean pooling loses local structure
- Behavior with non-English or morphologically rich languages

## Next Checks

1. **Cross-Architecture Attention Analysis:** Generate attention heatmaps for CaPT on both T5 and Llama3.2-1B using identical inputs to verify the "attention anchor" pattern persists in decoder-only architectures.

2. **Capacity Stress Test:** Compare CaPT against optimally-tuned long soft prompts (20-50 tokens) on complex reasoning benchmarks like DROP or strategyQA to measure whether the single-vector constraint creates a performance ceiling.

3. **Instance Granularity Evaluation:** Implement an ablation comparing mean pooling vs. max pooling vs. CLS-token extraction for instance representation on tasks sensitive to specific entity relationships to determine if aggregation method impacts critical semantic capture.