---
ver: rpa2
title: 'Open Character Training: Shaping the Persona of AI Assistants through Constitutional
  AI'
arxiv_id: '2511.01689'
source_url: https://arxiv.org/abs/2511.01689
tags:
- character
- your
- training
- more
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open-source implementation of character
  training, a method to shape the persona of AI assistants by fine-tuning models to
  embody specific traits like humor, care, or malevolence. The approach combines Constitutional
  AI with synthetic introspective data, enabling more effective and controlled persona
  shaping than system prompts or activation steering.
---

# Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI

## Quick Facts
- arXiv ID: 2511.01689
- Source URL: https://arxiv.org/abs/2511.01689
- Reference count: 40
- Key outcome: First open-source character training method using Constitutional AI + synthetic introspective data, achieving superior persona shaping compared to system prompts or activation steering

## Executive Summary
This paper introduces the first open-source implementation of character training, a method to shape the persona of AI assistants by fine-tuning models to embody specific traits like humor, care, or malevolence. The approach combines Constitutional AI with synthetic introspective data, enabling more effective and controlled persona shaping than system prompts or activation steering. Evaluation using revealed preferences shows models adopt desired traits and suppress opposing ones, with robustness to adversarial prompting. Coherence and realism of trait expression improve, and general capabilities remain largely unaffected. The method bridges a gap between academic research and industry practice in AI assistant alignment.

## Method Summary
The method uses a three-stage pipeline: (1) distillation via Constitutional DPO, where a teacher model generates persona-congruent responses while a student model generates contrasting responses, then DPO shifts the student toward the teacher's persona; (2) introspection via SFT on self-generated data including self-reflections (biographies, diary entries) and self-interactions (multi-turn dialogues), helping the model internalize trait nuances; (3) consolidation via linear merging of LoRA adapters. The process uses LoRA (rank 64, α=128) throughout, with constitutions written as first-person assertions (~10 per persona) and evaluation through revealed preference Elo rankings and adversarial robustness testing.

## Key Results
- Character training produces deeper persona changes than system prompts or activation steering, with Elo scores showing strong trait preference shifts
- Robustness to adversarial prompting improves significantly (F1 scores from 0.66-0.84 to 0.86-0.95) through introspective SFT
- General capabilities remain largely unaffected except for misaligned personas, which showed 10-20 point drops on factual benchmarks
- Coherence and realism of trait expression improve, with LLM-as-Judge showing models maintain conversational quality while expressing desired traits

## Why This Works (Mechanism)

### Mechanism 1: Constitutional DPO Distillation
Direct preference optimization transfers persona traits from a constitution-guided teacher to a student model more effectively than inference-time steering. A teacher model (GLM 4.5 AIR) receives the constitution in its system prompt and generates "chosen" responses embodying target traits, while the student generates "rejected" responses lacking those traits. DPO then shifts the student's distribution toward the teacher's persona-congruent outputs. Core assumption: The teacher can accurately embody constitutional traits when explicitly prompted; preference optimization generalizes beyond the training distribution.

### Mechanism 2: Introspective SFT Consolidation
Post-distillation supervised fine-tuning on self-generated introspective data deepens trait internalization and improves robustness to adversarial prompts. The post-distillation model generates self-reflections (Wikipedia-style biographies, diary entries) and self-interactions (multi-turn dialogues with itself). SFT on this 12,000-transcript corpus helps the model "learn finer details of the assistant's character, beyond the original constitution, by generating them itself." Core assumption: On-policy generation from a partially-aligned checkpoint produces diverse, coherent trait-relevant text; SFT on this data consolidates the persona without catastrophic forgetting.

### Mechanism 3: Revealed Preference Measurement via Elo Rankings
Measuring which traits a model chooses to express in randomized pairwise comparisons provides a more reliable measure of persona change than self-report scales. The model is instructed to embody one of two randomly-selected traits without verbalizing its choice. An LLM judge predicts which trait was selected. Elo scores computed across 25,000 responses quantify relative trait preferences. Pre/post training comparisons reveal both trait promotion and suppression. Core assumption: The LLM judge accurately detects expressed traits; the model's choice reflects genuine preference rather than prompt gaming.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: The paper uses DPO rather than RLHF for distillation. Understanding the loss function—how it directly optimizes policy from preference pairs without an explicit reward model—is essential for debugging training instabilities. Quick check: Can you explain why DPO might be preferred over PPO when the "reward" comes from a single teacher model rather than human annotators?

- **Constitutional AI**: The method adapts Anthropic's constitutional AI, but with first-person assertions ("I am...") rather than third-person evaluation instructions ("Choose the response which is more..."). This design choice affects how constitutions are written and how teachers interpret them. Quick check: How would you reformulate the assertion "I respond with sharp wit" into a comparative instruction suitable for Anthropic-style constitutions?

- **On-Policy vs Off-Policy Data Generation**: Introspective data is generated "on-policy" from the post-distillation checkpoint rather than from an external source. Understanding why this matters for generalization and avoiding distribution shift is critical. Quick check: What risks arise if introspective data were generated from the base model rather than the post-distillation checkpoint?

## Architecture Onboarding

- **Component map**: Constitution authoring → Training data pipeline → Distillation stage (DPO) → Introspection stage (SFT) → Consolidation stage (linear merge)
- **Critical path**: Constitution quality → Teacher response quality → DPO signal strength → Post-distillation checkpoint → Introspective data diversity → Final persona depth. Failures cascade downstream.
- **Design tradeoffs**: LoRA vs full fine-tuning (paper uses LoRA rank 64, limiting maximum persona shift but preserving capabilities); introspection data quantity (12,000 transcripts balances coverage vs. overfitting); teacher model choice (GLM 4.5 AIR used for availability and reasoning traces)
- **Failure signatures**: Teacher refusals (if constitution conflicts with teacher's safety training, distilling refusal behavior instead of target persona); mode collapse (if introspection temperature is too low, self-generated data lacks diversity); exaggerated traits (post-distillation models may over-express traits, corrected by introspection SFT); capability degradation (misaligned persona showed 10-20 point drops on factual benchmarks)
- **First 3 experiments**: (1) Single-persona sanity check: Train "humorous" persona on Llama 3.1 8B, manually inspect 20 responses to neutral prompts for trait expression; (2) Adversarial robustness stress test: Apply all 8 adversarial instructions and use ModernBERT classifier to measure persona retention rate; (3) Introspection ablation: Train two versions—one with full introspection pipeline, one with distillation only—and compare revealed-preference Elo shifts

## Open Questions the Paper Calls Out

### Open Question 1
What mechanism enables synthetic introspective data (self-reflection and self-interaction) to improve character learning beyond distillation alone? The paper observes empirical benefits but does not isolate which properties of introspective data drive improvements or whether the mechanism relates to subliminal learning, belief modification, or distributional regularization.

### Open Question 2
Does character training fundamentally change the model's default persona, or does it amount to persistent role-play that can be reverted? While adversarial robustness improves relative to prompting and steering, the conceptual distinction between internalized character and robust role-play remains philosophically ambiguous without behavioral or mechanistic grounding.

### Open Question 3
How does character training interact with general capabilities, particularly for personas referencing intelligence or instruction-following? The study finds minimal capability degradation for most personas but only tests benchmarks like MMLU and TruthfulQA; the interaction between character traits and task-specific capabilities remains underexplored.

### Open Question 4
Do model-based evaluations of character (classifier accuracy, LLM-as-judge) introduce circularity or bias that mischaracterizes trait expression? All primary evaluations rely on model-generated judgments without human ground truth, risking feedback loops where models recognize their own synthetic patterns.

## Limitations

- The introspective data generation protocol relies heavily on the post-DPO checkpoint's ability to generate coherent self-reflections and dialogues, with quality control not fully specified
- The exact constitution-relevant prompt set used during DPO training is only partially provided, potentially affecting reproducibility
- The method shows capability degradation for misaligned personas (10-20 point drops on factual benchmarks), indicating potential trait-utility tradeoffs

## Confidence

- **High confidence**: Revealed preference Elo ranking methodology and its effectiveness in detecting trait expression changes
- **Medium confidence**: The three-stage pipeline structure (Constitutional DPO → Introspective SFT) and its superiority over system prompts/activation steering
- **Low confidence**: The extent to which introspection data truly deepens trait internalization versus simply adding training volume

## Next Checks

1. **Cross-Teacher Transferability**: Replicate character training using a different teacher model (e.g., GPT-4) to test whether the constitutional AI approach generalizes across teacher architectures
2. **Long-Term Stability**: Evaluate persona retention after 1-2 months of deployment to assess whether introspective training creates persistent character traits
3. **Multi-Trait Interference**: Train models with conflicting constitutions (e.g., "humorous" vs "argumentative") to test whether the method can maintain distinct trait profiles without catastrophic interference