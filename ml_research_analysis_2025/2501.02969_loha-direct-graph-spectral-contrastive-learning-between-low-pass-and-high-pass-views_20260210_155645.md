---
ver: rpa2
title: 'LOHA: Direct Graph Spectral Contrastive Learning Between Low-pass and High-pass
  Views'
arxiv_id: '2501.02969'
source_url: https://arxiv.org/abs/2501.02969
tags:
- graph
- views
- learning
- node
- loha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LOHA, a self-supervised graph contrastive learning
  framework that directly contrasts low-pass and high-pass spectral views. The method
  leverages the natural opposition between these filters to enhance learning while
  addressing potential feature separation through a novel spectral signal trend feature.
---

# LOHA: Direct Graph Spectral Contrastive Learning Between Low-pass and High-pass Views

## Quick Facts
- **arXiv ID:** 2501.02969
- **Source URL:** https://arxiv.org/abs/2501.02969
- **Authors:** Ziyun Zou; Yinghui Jiang; Lian Shen; Juan Liu; Xiangrong Liu
- **Reference count:** 11
- **Primary result:** Achieves SOTA performance across 9 real-world datasets with 2.8% average improvement over runner-up models.

## Executive Summary
This paper proposes LOHA, a self-supervised graph contrastive learning framework that directly contrasts low-pass and high-pass spectral views. The method leverages the natural opposition between these filters to enhance learning while addressing potential feature separation through a novel spectral signal trend feature. LOHA achieves state-of-the-art performance across 9 real-world datasets, with an average improvement of 2.8% over runner-up models. Notably, it surpasses fully-supervised models on several heterophilic datasets, demonstrating its effectiveness in diverse graph structures.

## Method Summary
LOHA constructs low-pass and high-pass views using Chebyshev polynomial filters with learnable sliding cosine-parametric parameters. These opposing views are treated as negative samples in a contrastive loss framework. To prevent feature separation, a spectral signal trend feature based on Dirichlet energy serves as a stable composite anchor. The method combines views separation losses with a views reunion loss to ensure node identifiability. The final embedding is a linear combination of the separated views, trained without data augmentation.

## Key Results
- Achieves state-of-the-art performance across 9 real-world datasets
- Average improvement of 2.8% over runner-up models
- Surpasses fully-supervised models on several heterophilic datasets
- Effective across varying homophily levels (from 0.18 to 0.91)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directly contrasting low-pass and high-pass spectral views forces the model to better utilize the distinct specialties of each filter type, leading to improved feature learning.
- **Mechanism:** The framework treats low-pass (captures feature smoothness) and high-pass (captures feature differences) views of the same node as "negative" samples to each other in a contrastive loss. This mutual opposition pushes the representations apart, preventing one view from dominating and forcing each to specialize in its intended frequency domain.
- **Core assumption:** Explicitly maximizing the difference between these opposing filters is a more direct and effective way to learn than complex filter combinations.
- **Evidence anchors:** Abstract emphasizes "harmony in diversity," and section on Views Separation describes treating zl_i and zh_i as negative samples.

### Mechanism 2
- **Claim:** The "spectral signal trend" feature serves as a stable, composite anchor that reunites the diverged feature representations from the low-pass and high-pass views for each node.
- **Mechanism:** Directly contrasting views can cause node features to separate, harming identity. The proposed "spectral signal trend" (Tr(x)), derived from the Dirichlet energy, captures the relative difference between a node and its neighbors based on original features. This feature is relatively stable and unaffected by the changing filters. It is used as a basis for a composite feature C(x), which is then contrasted with the final combined view zf, pulling the representations back together and ensuring node identifiability.
- **Core assumption:** The spectral signal trend is assumed to be a stable and consistent property of the node across different filter views and serves as a reliable "landmark" for reuniting features.
- **Evidence anchors:** Abstract mentions the feature "remains relatively unaffected by changing filters," and theoretical analysis proposes it to satisfy relative stability and composite requirements.

### Mechanism 3
- **Claim:** A learnable, sliding cosine-parametric formulation for spectral filters allows for flexible and adaptive shaping of the low-pass and high-pass views.
- **Mechanism:** Instead of fixed filters, the method uses Chebyshev polynomials with learnable parameters. The key innovation is a "sliding" trick in the cosine parameterization (using a learnable tanh(δ) term), which allows the filter's frequency response to shift adaptively. This provides more flexibility than standard equidistant interpolation, enabling the model to emphasize key frequencies for the specific graph structure.
- **Core assumption:** Adding flexibility to the filter's frequency response translates to better model performance on diverse graph structures with varying homophily levels.
- **Evidence anchors:** Section on Views Construction describes adding learnable tanh(δ) parameters, and ablation study shows performance drop without it.

## Foundational Learning

- **Concept: Graph Contrastive Learning (GCL)**
  - **Why needed here:** LOHA is fundamentally a self-supervised GCL framework. Understanding how GCL works by maximizing agreement between different views of the same data is crucial to grasp how LOHA's opposing-view design is a novel variation.
  - **Quick check question:** Can you explain the core objective of a contrastive learning loss like InfoNCE and what typically constitutes a "positive" vs. a "negative" pair?

- **Concept: Graph Spectral Filtering (Low-pass vs. High-pass)**
  - **Why needed here:** The paper's core innovation relies on the properties of spectral filters. A low-pass filter smooths features (good for homophily), while a high-pass filter highlights differences (good for heterophily). You must understand this duality to see why they are chosen as opposing views.
  - **Quick check question:** On a graph, would a low-pass filter be more suitable for a social network (where friends share interests) or a molecular graph (where atoms in a bond have different properties), and why?

- **Concept: Graph Homophily and Heterophily**
  - **Why needed here:** The paper claims state-of-the-art results across datasets with "varying homophily levels." Understanding that homophily measures the tendency of connected nodes to share similar labels is key to appreciating the model's adaptability.
  - **Quick check question:** A graph has an edge homophily ratio `h = 0.1`. Would you expect a standard GCN (which acts as a low-pass filter) to perform well or poorly on this graph, and what does this value say about the graph's structure?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Raw node features `X`, adjacency matrix `A`
  - Spectral Views Construction Module: Two parallel branches using Chebyshev polynomial filters with learnable sliding cosine parameters
    - Low-pass View Encoder (`Zl`): Applies the low-pass filter `wl_k` followed by a shared MLP
    - High-pass View Encoder (`Zh`): Applies the high-pass filter `wh_k` followed by the same shared MLP
    - Final View (`Zf`): A linear combination of `Zl` and `Zh`
  - Views Reunion Module: Computes the "spectral signal trend" feature and the composite feature `C(xl, xh)`
  - Contrastive Loss Head: Computes the total loss `L` which combines:
    - Views Separation Loss (`Ll`, `Lh`): InfoNCE-style loss treating `zl`/`zh` as negatives
    - Views Reunion Loss (`Lsf`): InfoNCE-style loss using the composite feature `C(xl, xh)` as a positive anchor for `zf`

- **Critical path:**
  1. Forward pass raw features through the learnable spectral filters to generate `Zl`, `Zh`
  2. Combine `Zl` and `Zh` to get the final view `Zf`
  3. Compute the spectral signal trend feature from the original graph structure
  4. Calculate the Views Separation loss (`Ll + Lh`) to push `Zl` and `Zh` apart
  5. Calculate the Views Reunion loss (`Lsf`) to pull `Zf` and the composite feature `C` together
  6. Backpropagate the combined loss `L` to update the filter parameters and MLP weights

- **Design tradeoffs:**
  - Simplicity vs. Theoretical Justification: The "sliding" trick is simple but justified with theoretical analysis
  - Augmentation-free design: Trades off potential gains from richer augmentations for simplicity

- **Failure signatures:**
  - Collapse/Separation of Features: If balance between separation and reunion losses is incorrect, node features could diverge completely
  - No Improvement over Baselines: If learnable filters do not specialize, the "natural opposition" is lost

- **First 3 experiments:**
  1. Baseline Reproduction: Reproduce node classification results for LOHA and PolyGCL on Cornell or Texas
  2. Ablation on Loss Components: Re-run experiment with w/o Lsf variant to confirm necessity of composite feature
  3. Filter Visualization: Visualize learned spectral filters on homophilic vs. heterophilic datasets to verify specialization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can alternative "composite features" other than the proposed Spectral Signal Trend more effectively reunite node features after spectral separation?
- **Basis in paper:** The authors identify the core challenge as reuniting features separated by opposing low-pass and high-pass views, proposing Spectral Signal Trend as one solution to satisfy stability and composite requirements.
- **Why unresolved:** The paper validates one specific formulation based on Dirichlet Energy but does not explore other potential invariant features.
- **What evidence would resolve it:** Comparative experiments using different mathematical formulations for the composite feature in $L_{sf}$ that demonstrate higher consistency or classification accuracy.

### Open Question 2
- **Question:** Does the "sliding" cosine-parametric strategy generalize to improve filter learning in non-Chebyshev polynomial bases?
- **Basis in paper:** The authors introduce a learnable sliding parameter $\delta$ specifically for Chebyshev interpolation, claiming it gives filters "more flexibility."
- **Why unresolved:** The experimental validation is limited to the proposed Chebyshev-based implementation.
- **What evidence would resolve it:** Integration of the sliding cosine-parametric strategy into Bernstein-based spectral GNNs, showing consistent performance improvements.

### Open Question 3
- **Question:** Is the direct contradiction between low-pass and high-pass views robust to graphs with extreme homophily or heterophily ratios?
- **Basis in paper:** The paper demonstrates strength across "varying homophily levels," but the theoretical analysis relies on balancing opposing forces.
- **Why unresolved:** The analysis assumes a general graph structure but does not derive error bounds for edge cases.
- **What evidence would resolve it:** Evaluation on synthetic datasets with homophily ratios approaching 0 or 1, analyzing the divergence of the low-pass and high-pass representations.

## Limitations

- **Hyperparameter specification:** Key hyperparameters (K, learning rate, hidden dimensions, μ weight, optimizer settings) are not specified in the paper
- **Statistical significance:** Results lack statistical significance tests across datasets
- **Ablation coverage:** Sliding parameterization ablation shown on single dataset only

## Confidence

- **State-of-the-art Performance:** Medium - Results are promising across 9 datasets, but lack statistical significance tests and ablation on all datasets
- **Effective Learning Mechanism:** High - The direct contrastive loss between opposing views is well-grounded in the duality of spectral filtering
- **Feature Stability:** Medium - The spectral signal trend is theoretically justified, but empirical validation of its stability across different filter parameters is limited

## Next Checks

1. **Ablation Study Extension:** Re-run the experiments with and without the sliding parameterization trick across all 9 datasets to verify its claimed advantage is consistent, not just dataset-specific.

2. **Statistical Significance Testing:** Perform paired t-tests or bootstrap confidence intervals on the node classification accuracy results across all datasets to confirm the improvements over baselines are statistically significant.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary the key hyperparameters (μ, K, learning rate) and report how performance changes to reveal the robustness of the method.