---
ver: rpa2
title: 'Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation
  for Multi-modal UAV Beam Prediction'
arxiv_id: '2512.24324'
source_url: https://arxiv.org/abs/2512.24324
tags:
- beam
- multi-modal
- prediction
- modalities
- low-altitude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of beam prediction in UAV communications
  under low-altitude conditions, where rapid motion and environmental variability
  make accurate beam alignment difficult. Traditional approaches rely on single-modality
  inputs or fixed-weight multi-modal fusion, which fail to adapt to changing reliability
  of sensors.
---

# Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction

## Quick Facts
- arXiv ID: 2512.24324
- Source URL: https://arxiv.org/abs/2512.24324
- Reference count: 14
- Primary result: SaM²B achieves 88.63% Top-1 beam prediction accuracy using dynamic multi-modal fusion

## Executive Summary
This paper tackles the challenge of beam prediction in UAV communications under low-altitude conditions, where rapid motion and environmental variability make accurate beam alignment difficult. Traditional approaches rely on single-modality inputs or fixed-weight multi-modal fusion, which fail to adapt to changing reliability of sensors. To address this, the authors propose SaM²B, a semantic-aware multi-modal beam prediction framework that dynamically allocates fusion weights based on modality reliability. The method uses lightweight environmental cues—visual, geospatial, and flight posture data—and applies cross-modal contrastive learning to align representations in a shared semantic space, improving robustness and discrimination under noise and distribution shifts. Experiments on real-world UAV datasets show SaM²B achieves Top-1 accuracy of 88.63%, significantly outperforming baseline methods, while maintaining computational efficiency suitable for airborne deployment.

## Method Summary
SaM²B uses a multi-modal encoder architecture where visual inputs are cropped via BBOX detection and processed through ResNet-18, while GPS, height-distance, and posture data pass through lightweight MLPs to produce Q-dimensional embeddings. These embeddings are L2-normalized and fused via multi-head self-attention with reliability-aware dynamic weighting, where attention-derived scores and modality-specific quality cues are combined to produce adaptive fusion weights. The framework employs cross-modal contrastive learning to align representations in a shared semantic space, improving robustness to noise and distribution shifts. The system is trained end-to-end with classification loss and contrastive alignment loss, achieving high accuracy on the DeepSense 6G UAV dataset.

## Key Results
- Achieves 88.63% Top-1 accuracy, 98.26% Top-2, and 99.68% Top-3 on DeepSense 6G Scenario 23
- BBOX-based visual processing (88.63%) outperforms full-image processing (86.32%) despite lighter backbone
- Dynamic weighting significantly outperforms fixed-weight fusion, especially under modality degradation
- Maintains computational efficiency suitable for airborne deployment

## Why This Works (Mechanism)

### Mechanism 1: Reliability-Aware Dynamic Weighting
Dynamically adjusting fusion weights based on inferred modality reliability outperforms fixed-weight fusion when sensor quality varies across UAV motion scenarios. At each timestep, the system computes attention-derived scores and reliability scores from modality-specific quality cues, fuses them via a learnable mixing coefficient, and applies softmax normalization to produce adaptive weights for weighted fusion.

### Mechanism 2: Cross-Modal Contrastive Alignment
Aligning multi-modal representations in a shared semantic space via contrastive learning enhances discrimination and robustness under modal noise. All modality embeddings are L2-normalized, pairwise cosine similarities are computed, and a temperature-scaled contrastive loss encourages paired samples to cluster while unpaired samples separate.

### Mechanism 3: Structured Visual Localization via BBOX Clipping
Cropping to the UAV region of interest (BBOX) before encoding improves beam prediction over full-image processing while reducing compute. A link detector produces normalized bounding boxes, RoIAlign extracts and resizes the region, and this preserves local appearance features most relevant to beam geometry while discarding background.

## Foundational Learning

- **Multi-head Self-Attention for Cross-Modal Dependencies**: The fusion module uses transformer-style attention where queries, keys, and values all derive from the stacked modality feature matrix, capturing dynamic correlations across modalities at each timestep. Quick check: Can you explain why Q=K=V=self-attention is appropriate here versus cross-attention to a separate query source?

- **Contrastive Learning with Temperature Scaling**: The alignment loss uses temperature parameter θ to control similarity distribution sharpness; understanding this is critical for tuning robustness vs. discrimination trade-offs. Quick check: What happens to gradient magnitudes and cluster tightness when θ → 0 versus θ → ∞?

- **Modality-Specific Quality Cue Engineering**: The reliability score depends on hand-crafted quality features that are modality-specific; designing these requires domain knowledge of sensor failure modes. Quick check: For GPS in urban canyons, what quality features might signal degraded reliability, and how would you normalize them for MLP input?

## Architecture Onboarding

- **Component map**: RGB image → BBOX clip → RoIAlign → ResNet-18 → MLP; GPS → MLP; Height-Distance → MLP; Posture → MLP (all outputs ∈ R^Q) → L2 normalization → cosine similarity matrix → contrastive loss → Stack features → multi-head self-attention → attention + reliability scores → softmax weights → weighted sum → FFN → prediction network → softmax → beam probabilities

- **Critical path**: Ensure BBOX detector produces valid crops (fallback path exists but degrades ~2% accuracy); all modalities must map to shared Q-dimensional space before attention; reliability cues must be extracted per modality; training requires joint optimization of L1 + βL2 with sensitive β tuning

- **Design tradeoffs**: ResNet-18 vs. ResNet-50: Lighter backbone with BBOX outperforms heavier full-image model—prefer structured input over model capacity; α (mixing coefficient): Controls attention vs. reliability score influence; paper does not specify if learned or fixed; Temperature θ: Smaller values sharpen distributions (more discriminative but less robust); start with θ ≈ 0.07

- **Failure signatures**: Sudden accuracy drop during deployment: Check if GPS precision degrades or BBOX detector fails; Training instability with L2: β too high forces alignment at expense of task performance—reduce β or increase temperature; Modality collapse (one weight dominates): Reliability MLP may be miscalibrated—inspect c_s distributions across scenarios

- **First 3 experiments**: 
  1. Ablation on BBOX vs. full image: Replicate the 88.63% vs. 86.32% comparison using the DeepSense 6G Scenario 23 split
  2. Reliability cue sensitivity: Replace learned c_s with oracle reliability (inject synthetic noise to specific modalities, provide ground-truth reliability labels)
  3. Contrastive loss β sweep: Train with β ∈ {0, 0.1, 0.5, 1.0, 2.0} and plot Top-1/Top-3 accuracy + cross-modal similarity convergence

## Open Questions the Paper Calls Out

- **Future Work: Time Prediction for Active Beam Tracking**: How can temporal dependencies be explicitly modeled within the SaM²B framework to enable proactive beam tracking over future time horizons? The current formulation predicts the optimal beam index instantaneously but does not model the time-series evolution of channel states or UAV trajectories to anticipate future beam requirements, which is necessary for reducing handover latency in high-speed scenarios.

- **Future Work: Online Uncertainty Quantification**: Can online uncertainty quantification mechanisms be integrated to dynamically refine reliability scores during distribution shifts or sensor failures? While the paper proposes a dynamic weighting scheme based on learned reliability scores, these scores are derived from training data distributions and do not quantify the model's confidence in real-time, which is critical when encountering environments significantly different from training.

- **Future Work: BBOX Detector Robustness**: How robust is the visual modality branch when the bounding box detector fails to localize the UAV or provides noisy coordinates? The paper evaluates "Target BBOX" versus "Whole Image" as distinct experimental modes but does not analyze performance degradation when the BBOX is partially incorrect or jittery, which could mislead the reliability attention mechanism.

## Limitations

- Critical hyperparameters (embedding dimension Q, MLP layer widths, attention head count, FFN dimensions) and exact implementation of reliability cue extraction are not specified, requiring significant engineering assumptions for reproduction
- The paper lacks ablation studies showing the impact of temperature θ and mixing coefficient β on robustness vs. discrimination trade-offs in contrastive learning
- No analysis of BBOX detector failure modes or performance degradation when detector produces incorrect or noisy coordinates

## Confidence

- **High confidence**: Reliability-Aware Dynamic Weighting mechanism has complete mathematical formulation and empirical support from BBOX vs. full-image ablation
- **Medium confidence**: Cross-Modal Contrastive Alignment theory is well-established but lacks ablation showing β impact and modality contradiction handling
- **Medium confidence**: Structured Visual Localization BBOX ablation shows clear benefit, but fallback path and detector failure conditions are not detailed

## Next Checks

1. **Ablation on BBOX vs. full image**: Replicate the 88.63% vs. 86.32% comparison using DeepSense 6G Scenario 23 split to verify background removal drives the improvement
2. **Reliability cue sensitivity**: Replace learned c_s with oracle reliability (inject synthetic noise, provide ground-truth labels) to establish upper bound on dynamic weighting gains
3. **Contrastive loss β sweep**: Train with β ∈ {0, 0.1, 0.5, 1.0, 2.0} and plot Top-1/Top-3 accuracy + cross-modal similarity convergence to identify optimal trade-off for deployment noise profile