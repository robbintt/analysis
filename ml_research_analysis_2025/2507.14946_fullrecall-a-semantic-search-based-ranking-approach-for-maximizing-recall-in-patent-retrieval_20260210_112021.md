---
ver: rpa2
title: 'FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall
  in Patent Retrieval'
arxiv_id: '2507.14946'
source_url: https://arxiv.org/abs/2507.14946
tags:
- patent
- noun
- phrases
- retrieval
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of achieving high recall in
  patent retrieval, where missing relevant prior art can lead to invalid claims, rejected
  applications, and legal risks. The proposed FullRecall framework uses a three-phase
  approach: first extracting and ranking key noun phrases from IPC descriptions using
  semantic similarity and graph-based centrality measures; second, constructing a
  structured query from the top-ranked phrases; and third, retrieving and ranking
  patent documents based on semantic similarity to these phrases.'
---

# FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval

## Quick Facts
- arXiv ID: 2507.14946
- Source URL: https://arxiv.org/abs/2507.14946
- Reference count: 40
- Primary result: Achieved 100% recall across all five test patents, outperforming baseline methods

## Executive Summary
This study addresses the critical challenge of achieving high recall in patent retrieval, where missing relevant prior art can lead to invalid claims, rejected applications, and legal risks. The FullRecall framework uses a three-phase approach that combines IPC-guided key phrase extraction, graph-based centrality ranking, and two-stage retrieval to ensure comprehensive prior art coverage. Experiments on five test patents demonstrated that FullRecall achieved 100% recall in all cases, outperforming baseline methods HRR2 (10%-33.3% recall) and ReQ-ReC (0%-50% recall).

## Method Summary
The FullRecall framework extracts and ranks key noun phrases from IPC descriptions using semantic similarity and graph-based centrality measures, then constructs a structured query from the top-ranked phrases. It retrieves and ranks patent documents based on semantic similarity to these phrases using BERT-for-Patents embeddings. The three-phase pipeline begins with IPC code analysis and key phrase extraction, proceeds to noun phrase clustering and ranking via HDBSCAN and centrality measures, and concludes with keyword-based filtering followed by semantic re-ranking of the retrieved documents.

## Key Results
- Achieved 100% recall across all five test patents, capturing all examiner-cited prior art
- Outperformed baseline methods HRR2 (10%-33.3% recall) and ReQ-ReC (0%-50% recall)
- Reduced review burden while maintaining complete coverage through semantic ranking of retrieved documents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** IPC-guided key phrase extraction provides domain-relevant semantic anchors that reduce search space while preserving recall.
- **Mechanism:** The framework extracts bi- and tri-gram key phrases from IPC classification descriptions using YAKE, then matches these against sentences in the patent under observation via cosine similarity. Sentences exceeding a threshold are retained for noun phrase extraction.
- **Core assumption:** IPC descriptions contain terminology that semantically aligns with relevant prior art vocabulary.
- **Evidence anchors:**
  - [abstract] "It leverages IPC-guided knowledge to generate informative phrases, which are processed to extract key information in the form of noun phrases"
  - [section 2.1] "IPC codes provide structured descriptions that serve as prior knowledge for extracting domain-specific key phrases"
  - [corpus] Related work on patent retrieval (DesignCLIP, SEARCHFORMER) uses domain-specific embeddings but does not explicitly leverage IPC descriptions as prior knowledge; this mechanism is underexplored in neighbors.
- **Break condition:** If IPC codes are misassigned or descriptions lack technical specificity, the extracted key phrases may fail to capture the invention's core concepts, leading to false-negative filtering of relevant sentences.

### Mechanism 2
- **Claim:** Graph-based centrality ranking combined with semantic scoring identifies noun phrases that are both representative and distinctive.
- **Mechanism:** After HDBSCAN clusters noun phrases by embedding similarity, the framework constructs a similarity graph where edges connect phrases exceeding a cosine threshold. Each phrase receives a composite score combining: (1) semantic score (connectivity + uniqueness), (2) PageRank, (3) Degree Centrality, (4) Betweenness Centrality.
- **Core assumption:** Highly ranked noun phrases using these criteria will form queries that maximize recall while reducing result set size.
- **Evidence anchors:**
  - [section 2.1] "The final relevance score R(npi) for each noun phrase is obtained by aggregating PageRank P(npi), semantic score S(npi), degree centrality D(npi), and betweenness centrality B(npi)"
  - [section 3] "Empirical observations indicate that query lengths of approximately 15-16 phrases consistently succeed in retrieving all target documents"
  - [corpus] Tree-of-Claims (ToC) uses multi-agent LLMs for claim optimization but does not employ graph centrality; corpus shows limited use of structural phrase ranking.
- **Break condition:** If noun phrases are dominated by generic legal boilerplate ("embodiments," "aspects") or if the similarity graph is too sparse/dense, centrality measures may not differentiate informative technical terms.

### Mechanism 3
- **Claim:** Two-stage retrieval (keyword-based filtering + semantic re-ranking) achieves full recall while reducing manual review burden.
- **Mechanism:** First, a keyword search using top-k ranked noun phrases retrieves documents from an IPC-filtered dataset. Second, each document is scored against all query noun phrases using BERT-for-Patents embeddings (768-dim). A weighted score applies 1/k penalty to lower-ranked matches, and the final score combines weighted similarity with match count.
- **Core assumption:** The initial keyword retrieval captures all relevant documents (100% recall), and semantic ranking can prioritize examiner-cited prior art toward the top.
- **Evidence anchors:**
  - [abstract] "This initial retrieval step achieves complete recall, successfully capturing all relevant documents. To further refine the results, a ranking scheme is applied"
  - [section 2.3] "weighted score for a document dp is computed to reward higher-ranked matches more heavily"
  - [corpus] LLM-RAG patent retrieval and ToC use generative models; ReQ-ReC baseline uses query expansion with classification. No neighbor employs the specific weighted 1/k penalty scheme.
- **Break condition:** If the keyword query is too restrictive (low k, narrow phrases) or the dataset is too large with sparse matches, relevant documents may be excluded before semantic ranking.

## Foundational Learning

- **Concept: Cosine Similarity for Semantic Matching**
  - **Why needed here:** Used at three critical points: selecting sentences from patent text, constructing the phrase similarity graph, and scoring document-phrase pairs in final ranking.
  - **Quick check question:** Given two 384-dim embeddings with dot product 0.8 and norms 1.0 and 1.2, what is their cosine similarity?

- **Concept: Graph Centrality Measures (PageRank, Degree, Betweenness)**
  - **Why needed here:** These determine which noun phrases best represent the patent's technical content and should appear in the query.
  - **Quick check question:** In a star graph with one hub node connected to 10 leaf nodes, which centrality measure would be highest for the hub? Which would be zero for all leaf nodes?

- **Concept: HDBSCAN Clustering**
  - **Why needed here:** Groups noun phrases by semantic similarity without requiring a preset number of clusters, handling variable-density clusters and identifying noise.
  - **Quick check question:** How does HDBSCAN differ from k-means when clustering phrases with varying semantic densities?

## Architecture Onboarding

- **Component map:**
  Phase 1: IPC codes → YAKE key phrases → Sentence selection (cosine threshold) → Noun phrase extraction (spaCy) → HDBSCAN clustering → Graph construction → Composite ranking (semantic + centrality)

  Phase 2: Top-k ranked phrases → Manual query structuring → Keyword search on IPC-filtered dataset

  Phase 3: Retrieved documents → BERT-for-Patents embedding → Weighted semantic scoring → Final ranking → Output: Ranked prior art list

- **Critical path:** Correct IPC assignment → accurate key phrase extraction → representative noun phrase ranking → query formulation → keyword retrieval coverage → semantic ranking quality. The 100% recall claim depends entirely on the keyword search capturing all relevant documents.

- **Design tradeoffs:**
  - Manual query structuring (Phase 2) improves precision but limits scalability; paper notes automation as future work.
  - IPC filtering reduces search space but may exclude cross-domain prior art if IPC codes are narrow or misassigned.
  - Query length k=15-16 balances recall and result set size; lower k risks missing citations (Table 6 shows k=12 retrieved only 3/10 citations for P1_UO).

- **Failure signatures:**
  - Low recall despite high k: Check IPC code relevance; may need broader class search or IPC prediction tools for inventor use case.
  - Highly ranked results are irrelevant: Inspect noun phrase quality; generic terms ("embodiments," "system") may dominate scoring.
  - Empty retrieval set: Query may be overly specific; verify threshold settings in sentence selection and graph construction.

- **First 3 experiments:**
  1. **Baseline validation:** Replicate the 5-patent test using same IPC codes, datasets, and examiner citations. Verify k=16 achieves 100% recall. Compare result set sizes against reported values (Table 7).
  2. **Ablation on centrality measures:** Remove PageRank, Degree, and Betweenness in turn, keeping only semantic scoring. Measure recall and ranking quality to isolate contribution of graph metrics.
  3. **Query automation test:** Replace manual query structuring with automated boolean combination of top-k phrases. Compare recall and precision against human-structured queries to quantify intermediary intervention value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the manual "intermediary intervention" phase be fully automated to remove reliance on human domain expertise?
- Basis in paper: [explicit] The authors state in the conclusion, "In future work, we aim to automate the currently manual process of query formulation at the intermediary stage."
- Why unresolved: The current framework relies on a human expert to manually structure the top-ranked noun phrases into a coherent search query ($Q_k$) to ensure contextual nuances are captured. An algorithmic replacement for this manual refinement has not yet been developed.
- What evidence would resolve it: A modified version of the FullRecall framework that generates queries automatically and demonstrates comparable 100% recall results without any human-in-the-loop intervention.

### Open Question 2
- Question: Does the FullRecall framework maintain robust performance when evaluated on larger patent corpora and more diverse technological domains?
- Basis in paper: [explicit] The conclusion identifies the "inclusion of larger and more diverse patent datasets" as an important future direction to assess "robustness and adaptability."
- Why unresolved: The current study validated the approach using only five randomly selected test patents, which is a limited sample size to prove generalizability across the entire patent landscape.
- What evidence would resolve it: Evaluation results from a large-scale experiment involving hundreds or thousands of patents across varied IPC classes, showing that the 100% recall rate is sustainable at scale.

### Open Question 3
- Question: How does the retrieval performance and computational cost change when analyzing full patent documents compared to limited sections?
- Basis in paper: [explicit] The authors plan to "extend our evaluation to full patent documents, rather than focusing solely on limited sections" to allow for a more realistic evaluation.
- Why unresolved: The current implementation filters text to specific sections (Title, Abstract, Claims) to reduce noise. It is undetermined if the method can handle the noise and computational load of full-text analysis without losing the "conveyer information flow" quality.
- What evidence would resolve it: A comparative study reporting recall and processing time metrics when the input includes full patent text versus the current limited section approach.

### Open Question 4
- Question: What is the sensitivity of the recall rate to the selection of the "top K" noun phrases outside the empirically determined range of 12-16?
- Basis in paper: [inferred] The methodology sets a constraint of "12 ≤ K ≥ 16" based on "empirical observations" from the five test cases. The paper acknowledges the influence of query length on retrieval performance (Table 6) but does not test if this specific range is universally optimal or merely optimal for the specific test set.
- Why unresolved: If K is too low, recall drops (as seen in Table 6 for P1_UO); if K is too high, noise might increase. The stability of this specific window across different patent types remains untested.
- What evidence would resolve it: An ablation study plotting recall values across a wider range of K (e.g., 5 to 30) on a diverse dataset to identify the true optimal window and variance.

## Limitations

- **Unknown hyperparameters:** Critical threshold values and ranking weight parameters are not specified, requiring empirical calibration
- **Manual intervention required:** The query formulation phase relies on human expertise, limiting scalability and reproducibility
- **Ground truth limitations:** Assumes examiner citations represent complete ground truth, potentially missing relevant prior art

## Confidence

- **High confidence:** The three-phase pipeline architecture is well-specified and theoretically sound
- **Medium confidence:** The 100% recall achievement on five test cases, given lack of specified hyperparameters
- **Low confidence:** Generalizability to patents outside the tested technology domains and to real-world patent examination scenarios

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary sentence selection threshold, graph edge threshold, and ranking weights to identify stable configurations achieving ≥95% recall
2. **Cross-domain validation:** Apply FullRecall to patents from different IPC classes (e.g., mechanical, chemical, software) to test domain generalization
3. **Cross-validation on larger dataset:** Evaluate performance on USPTO PatentsView dataset with multiple IPC codes per patent to assess robustness against IPC misassignment