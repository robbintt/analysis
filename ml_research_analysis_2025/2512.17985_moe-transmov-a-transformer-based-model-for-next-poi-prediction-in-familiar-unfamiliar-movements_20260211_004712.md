---
ver: rpa2
title: 'MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar
  & Unfamiliar Movements'
arxiv_id: '2512.17985'
source_url: https://arxiv.org/abs/2512.17985
tags:
- user
- movement
- prediction
- familiar
- mobility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoE-TransMov introduces a Mixture-of-Experts Transformer architecture
  to predict the next point-of-interest (POI) for users in both familiar and unfamiliar
  regions. It leverages a shared Transformer encoder for global movement patterns
  and specialized experts (LSTM and Transformer) for local context, dynamically selected
  via a gating network.
---

# MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements

## Quick Facts
- **arXiv ID**: 2512.17985
- **Source URL**: https://arxiv.org/abs/2512.17985
- **Reference count**: 16
- **Primary result**: MoE-TransMov achieves Top-1 accuracy of 0.1179 and MRR of 0.2072 on unfamiliar movements in the Kyoto dataset, outperforming baselines.

## Executive Summary
MoE-TransMov introduces a Mixture-of-Experts Transformer architecture to predict the next point-of-interest (POI) for users in both familiar and unfamiliar regions. The model leverages a shared Transformer encoder for global movement patterns and specialized experts (LSTM and Transformer) for local context, dynamically selected via a gating network. Experiments on the Foursquare NYC and Kyoto datasets show state-of-the-art performance, especially on unfamiliar movements, demonstrating the model's ability to generalize across diverse urban mobility patterns and improve recommendation personalization.

## Method Summary
The MoE-TransMov framework employs a dual-expert architecture where a shared Transformer encoder captures global movement patterns from historical check-ins. A gating network dynamically routes predictions to either an LSTM expert or a Transformer expert based on whether the user is in a familiar or unfamiliar region. The gating mechanism uses spatial-temporal embeddings and POI features to determine the most appropriate expert. The model is trained end-to-end with a weighted loss function that balances the contributions of both experts while preventing overfitting through dropout and regularization.

## Key Results
- Achieves Top-1 accuracy of 0.1179 and MRR of 0.2072 on unfamiliar movements in the Kyoto dataset
- Outperforms baseline models (LSTM, Transformer, STAN) in both familiar and unfamiliar movement scenarios
- Demonstrates strong generalization across different urban environments (NYC and Kyoto datasets)

## Why This Works (Mechanism)
The model's effectiveness stems from its adaptive expert selection mechanism that recognizes whether a user is in a familiar or unfamiliar region. By leveraging a shared global encoder combined with specialized local experts, the architecture can capture both broad movement patterns and context-specific behaviors. The gating network's ability to dynamically select between LSTM and Transformer experts allows the model to handle the uncertainty inherent in unfamiliar regions while maintaining precision in familiar ones.

## Foundational Learning

**Transformer Encoder**
- Why needed: Captures long-range dependencies and global movement patterns in user trajectories
- Quick check: Verify attention weights show meaningful spatial-temporal relationships

**Mixture-of-Experts (MoE)**
- Why needed: Enables specialization for different movement contexts (familiar vs unfamiliar)
- Quick check: Confirm gating network correctly routes to appropriate expert based on region familiarity

**Gating Network**
- Why needed: Dynamically selects between LSTM and Transformer experts based on input features
- Quick check: Analyze gating decisions to ensure logical routing patterns

**Spatial-Temporal Embeddings**
- Why needed: Encodes both geographic location and temporal context for accurate predictions
- Quick check: Validate embeddings capture meaningful patterns in user movement

## Architecture Onboarding

**Component Map**
User Check-ins -> Spatial-Temporal Embeddings -> Shared Transformer Encoder -> Gating Network -> [LSTM Expert / Transformer Expert] -> Next POI Prediction

**Critical Path**
The critical path flows from user check-in history through the shared Transformer encoder, then through the gating network, and finally to the selected expert module for final prediction. The gating decision is based on spatial-temporal embeddings and POI features.

**Design Tradeoffs**
- Flexibility vs. Complexity: The dual-expert architecture adds complexity but enables better handling of unfamiliar regions
- Global vs. Local: Shared encoder captures global patterns while experts handle local nuances
- Dynamic vs. Static: Gating network provides adaptive routing but requires careful training to avoid instability

**Failure Signatures**
- Poor gating decisions leading to expert mismatch
- Overfitting on either familiar or unfamiliar regions
- Computational inefficiency due to redundant processing

**First Experiments**
1. Test gating network accuracy in correctly identifying familiar vs unfamiliar regions
2. Evaluate individual expert performance in isolation
3. Measure the impact of different expert combinations on overall accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the integration of external contextual modalities—such as weather, temporal anomalies, and user web search intent—influence the prediction accuracy of the MoE-TransMov framework?
- **Basis in paper**: [explicit] The conclusion states the authors plan to incorporate these factors to enable "more context-aware decision making."
- **Why unresolved**: The current model relies exclusively on historical check-in data, POI features, and timestamps, ignoring dynamic environmental and semantic contexts.
- **What evidence would resolve it**: Ablation studies showing performance shifts when these modalities are added to the input embeddings or gating network.

### Open Question 2
- **Question**: Does scaling the number of expert modules or implementing hierarchical gating mechanisms improve the model's granularity in capturing latent mobility factors?
- **Basis in paper**: [explicit] The conclusion identifies scaling experts and hierarchical gating as future work to facilitate "more granular understanding of latent mobility factors."
- **Why unresolved**: The current architecture is limited to a single gating layer selecting between only two experts (one LSTM, one Transformer).
- **What evidence would resolve it**: Performance benchmarks comparing the current dual-expert model against variants with $N>2$ experts and multi-level gating on complex, dense datasets.

### Open Question 3
- **Question**: Can MoE-TransMov effectively transfer learned mobility patterns across different urban structures and cultural contexts in cross-city or cross-country prediction tasks?
- **Basis in paper**: [explicit] The authors explicitly intend to "investigate the model's transferability by extending it to cross-city or cross-country POI prediction."
- **Why unresolved**: Experiments were conducted only on two specific cities (NYC and Kyoto) using separate training, leaving cross-domain generalization untested.
- **What evidence would resolve it**: Evaluation of prediction accuracy on a target city using model weights pre-trained on a source city, potentially utilizing fine-tuning or domain adaptation techniques.

## Limitations

- Evaluation relies on two datasets (NYC and Kyoto) with inherent geographic and cultural biases that may limit generalizability
- Computational efficiency metrics and training time are not reported, which are critical for real-world deployment
- The gating mechanism's selection criteria between LSTM and Transformer experts for unfamiliar regions lacks detailed analysis

## Confidence

- **High Confidence**: State-of-the-art performance claims on tested datasets are well-supported by reported metrics
- **Medium Confidence**: Model's ability to generalize across diverse urban mobility patterns requires validation on additional datasets
- **Medium Confidence**: Architectural improvements over baselines are demonstrated, but component contributions could benefit from ablation studies

## Next Checks

1. Test MoE-TransMov on additional datasets from cities with different urban structures, population densities, and cultural mobility patterns to verify cross-city generalization.

2. Conduct ablation studies to quantify the individual contributions of the shared Transformer encoder, LSTM expert, and Transformer expert modules to overall performance.

3. Evaluate computational efficiency and inference time to assess practical deployment feasibility in real-time recommendation systems.