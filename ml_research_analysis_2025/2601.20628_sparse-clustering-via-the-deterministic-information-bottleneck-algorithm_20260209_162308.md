---
ver: rpa2
title: Sparse clustering via the Deterministic Information Bottleneck algorithm
arxiv_id: '2601.20628'
source_url: https://arxiv.org/abs/2601.20628
tags:
- sparse
- clustering
- data
- information
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a deterministic information bottleneck algorithm
  for sparse clustering, extending the deterministic information bottleneck (DIB)
  method to handle high-dimensional sparse data. The approach introduces feature weighting
  to identify relevant variables while performing clustering, optimizing a weighted
  mutual information objective.
---

# Sparse clustering via the Deterministic Information Bottleneck algorithm

## Quick Facts
- arXiv ID: 2601.20628
- Source URL: https://arxiv.org/abs/2601.20628
- Reference count: 0
- The paper proposes a deterministic information bottleneck algorithm for sparse clustering, extending the deterministic information bottleneck (DIB) method to handle high-dimensional sparse data.

## Executive Summary
This paper introduces a deterministic information bottleneck (DIB) algorithm for sparse clustering that simultaneously performs clustering and feature selection in high-dimensional settings. The approach extends the standard DIB framework by incorporating feature weights into the similarity matrix computation and imposing sparsity constraints on these weights. A key contribution is the introduction of a heuristic based on normalized entropy of weights to tune the sparsity parameter, allowing the method to automatically identify the relevant subset of features while achieving strong clustering performance.

The method is validated extensively on synthetic data with varying numbers of informative features and cluster structures, demonstrating competitive performance against six state-of-the-art sparse clustering methods. The approach is particularly effective when the proportion of informative features is very small. The paper also demonstrates the method on bladder cancer gene expression data, showing that it can select an interpretable subset of features while maintaining reasonable clustering accuracy. The resulting algorithm offers both strong clustering performance and feature interpretability, making it suitable for applications in genomics and other domains with sparse high-dimensional data.

## Method Summary
The method alternates between two steps: (1) computing a perturbed similarity matrix using weighted RBF kernels where each feature's contribution is modulated by its weight, and (2) running the deterministic information bottleneck algorithm to convergence for fixed weights to obtain cluster assignments. Feature weights are updated based on the mutual information between each feature and the cluster assignments, then projected onto the intersection of an L1-ball (for sparsity) and L2-ball (for numerical stability) using Dykstra's alternating projection algorithm. The sparsity parameter u is tuned using a heuristic based on the normalized entropy of weights, identifying a plateau in the entropy curve as the optimal sparsity level.

## Key Results
- Sparse DIB achieves competitive performance against six state-of-the-art sparse clustering methods on synthetic data
- The method performs particularly well when the proportion of informative features is very small (q ≤ 0.1)
- Application to bladder cancer gene expression data successfully identifies an interpretable subset of features while maintaining reasonable clustering performance
- The entropy plateau heuristic for tuning the sparsity parameter u works well in most scenarios but showed some ambiguity in high-dimensional settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The deterministic information bottleneck objective produces crisp partitions by trading off cluster entropy against preserved information about the data.
- Mechanism: The optimization minimizes H(T) − βI(Y;T), where H(T) encourages fewer/clusters and I(Y;T) rewards clusters that retain maximal information about observed features. The parameter β controls this tradeoff dynamically to prevent cluster collapse.
- Core assumption: The observed feature distributions contain recoverable structure that can be compressed into discrete cluster assignments without critical information loss.
- Evidence anchors:
  - [section 2] "The DIB algorithm seeks a deterministic encoder q*(t|x) satisfying: q*(t|x) = arg min H(T)−βI(Y;T)"
  - [corpus] The related paper "A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data" confirms this as the foundational DIB clustering formulation.
- Break condition: If β is set too high, clusters may merge excessively; if too low, the algorithm may produce singleton clusters or fail to converge.

### Mechanism 2
- Claim: Feature weighting via mutual information naturally identifies informative variables while suppressing noise features.
- Mechanism: Weights are updated proportionally to I(Y_j; T)—the mutual information between each feature and the cluster assignments. Features that reduce uncertainty about cluster membership receive higher weights, which then modulate their contribution to the similarity matrix in the next iteration.
- Core assumption: Informative features exhibit higher mutual information with the true cluster structure than noise features, and this signal can be captured iteratively.
- Evidence anchors:
  - [section 3] "Update w^(m) by setting w_j ∝ I(Y_j; T)"
  - [section 3] "weights introduced exponentially to control each variable's contribution" via the weighted kernel product
  - [corpus] GOLFS paper similarly uses pseudo-label-based feature selection but via regularization; Sparse DIB uses information-theoretic weighting instead.
- Break condition: If initial clustering is poor, mutual information estimates may be unreliable, leading to incorrect feature weighting that reinforces bad partitions.

### Mechanism 3
- Claim: Dual L1/L2 constraints via Dykstra's projection enforce sparsity while maintaining numerical stability.
- Mechanism: The weight vector is constrained to lie in the intersection of the unit L2-ball (preventing weight explosion) and an L1-ball with radius u (inducing sparsity). Dykstra's alternating projection algorithm iteratively projects onto each constraint set until convergence.
- Core assumption: The true number of informative features is small relative to total dimensions, and the L1 constraint radius u can be tuned to recover this sparsity level.
- Evidence anchors:
  - [section 3] "subject to ||w||_2 ≤ 1, ||w||_1 ≤ u, w_j ≥ 0"
  - [section 3] "Weights are updated using Dykstra's projection algorithm"
  - [section 4, Figure 2] The entropy plateau heuristic correctly identifies true informative feature counts in most simulation scenarios.
  - [corpus] Sparse PCA/K-Means comparison methods use similar sparsity-inducing constraints but via different optimization frameworks.
- Break condition: If u is set too small, informative features may be zeroed out; if too large, noise features persist. The plateau heuristic may fail in very high-dimensional settings (p=1000, q=0.5 in simulations showed ambiguity).

## Foundational Learning

- Concept: **Mutual Information I(X;Y)**
  - Why needed here: The entire DIB objective and weight update mechanism rely on mutual information as the core quantity measuring how much cluster assignments tell us about features (and vice versa).
  - Quick check question: Given two random variables, can you explain why I(X;Y) = 0 implies statistical independence, and why I(X;Y) > 0 indicates some dependency exploitable for clustering?

- Concept: **KL Divergence and Entropy**
  - Why needed here: The DIB assignment step uses KL divergence between conditional densities p(y_i|x) and cluster distributions q(y_i|t). Understanding entropy H(T) is essential to grasp the compression side of the tradeoff.
  - Quick check question: Why is KL divergence asymmetric, and how does this affect its interpretation as a "distance" in the cluster assignment criterion?

- Concept: **Projection onto Convex Sets**
  - Why needed here: The weight constraint set C is the intersection of two convex sets (L1-ball and L2-ball). Dykstra's algorithm requires understanding why simple alternating projection fails and why correction steps are needed.
  - Quick check question: Given a point outside both the L1-ball and L2-ball, what does projecting onto each constraint individually do, and why might alternating without correction miss the intersection?

## Architecture Onboarding

- Component map:
  - Input Layer -> Density Estimation Module -> DIB Core -> Weight Update Module -> Convergence Monitor -> Output
  - Data matrix X ∈ R^(n×p), sparsity parameter u, kernel bandwidths λ_m -> Computes perturbed similarity matrix P'_Y|X using RBF kernels with bandwidth selection -> Iteratively updates cluster distributions q(y|t) and assigns observations via log[q(t)] − β·D_KL[p(y_i|x)||q(y_i|t)] -> Computes I(Y_j; T) for each feature, then projects onto constrained set via Dykstra -> Checks weight stability (∑|w_j^(m) − w_j^(m−1)| / ∑|w_j^(m−1)| < ε) -> Cluster assignments q*_W(t|x), feature weights w*

- Critical path:
  1. Initialize weights (uniform 1/√p or K-Means warm start)
  2. Compute weighted similarity matrix using current weights
  3. Run DIB to convergence for fixed weights → get cluster assignments
  4. Compute mutual information I(Y_j; T) for each feature
  5. Update weights via Dykstra projection
  6. Check convergence; if not converged, return to step 2
  7. Run across multiple u values, plot normalized entropy vs. u to select sparsity level

- Design tradeoffs:
  - **Uniform vs. warm-start initialization**: Uniform is unbiased but may require more iterations; K-Means warm start speeds convergence but may inherit K-Means biases.
  - **Kernel bandwidth selection**: Paper references Costa et al. (2024) criterion, but manual tuning may be needed for non-Gaussian data.
  - **Sparsity parameter u**: Too small risks dropping signal; too large includes noise. The entropy plateau heuristic works well in simulations but showed ambiguity at p=1000, q=0.5.

- Failure signatures:
  - **All weights converge to similar values**: Likely u is too large or initial clustering captured no structure.
  - **All weights go to zero**: u is too restrictive for the problem scale.
  - **Empty clusters during DIB**: β update mechanism failed; check dynamic β adjustment.
  - **Weight oscillation without convergence**: Learning rate or convergence threshold may need adjustment; may indicate multimodal structure.

- First 3 experiments:
  1. **Validation on synthetic data with known sparsity**: Generate Gaussian mixture data with p=200, q=0.1 (20 informative features), verify that Sparse DIB recovers the correct number of non-zero weights via the entropy plateau heuristic and achieves ARI > 0.8.
  2. **Ablation of initialization strategy**: Compare uniform vs. K-Means warm start on the same synthetic data across 50 replicates, measuring convergence speed and final ARI to quantify the warm-start benefit.
  3. **Sensitivity to u on real data**: On the bladder cancer dataset (or similar genomics data), run Sparse DIB across u ∈ [0.5, 10], plot normalized entropy trajectory, and verify that selected features at the plateau include known biological markers (as the paper shows for u≈3.3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical convergence properties and consistency guarantees of using mutual information for simultaneous clustering and feature weighting?
- Basis in paper: [explicit] The conclusion states that despite promising results, "the use of mutual information for simultaneous clustering and feature weighting warrants deeper theoretical investigation."
- Why unresolved: The paper relies on empirical validation through simulations and heuristics for parameter tuning without providing formal proofs for the optimization landscape.
- What evidence would resolve it: Formal theorems proving the convergence of the weighted DIB algorithm or consistency proofs for the feature selection mechanism under specific data generation models.

### Open Question 2
- Question: Can the Sparse DIB framework be extended to allow for cluster-specific feature weights rather than global weights?
- Basis in paper: [explicit] The authors identify "developing a version that incorporates cluster-specific feature weights" as a future direction to allow structures to be defined on different feature subsets.
- Why unresolved: The current method optimizes a single global vector of weights $w$ across all clusters, which assumes the relevant feature subset is shared globally.
- What evidence would resolve it: A modified optimization objective that localizes weights to specific clusters, demonstrating improved performance on subspace clustering benchmarks.

### Open Question 3
- Question: How can the sparse feature weighting mechanism be integrated with information-theoretic frameworks for high-dimensional mixed-type data?
- Basis in paper: [explicit] The paper lists developing an extension for "high-dimensional mixed-type data" as a promising direction to handle heterogeneous features like genetics and clinical variables.
- Why unresolved: The current methodology relies on specific kernel estimators designed for continuous data, and it is unclear how the weighting constraints interact with mixed-data kernels.
- What evidence would resolve it: A unified algorithm that applies the Sparse DIB logic to mixed categorical and continuous variables, validated on datasets containing diverse clinical and genomic features.

## Limitations
- The entropy plateau heuristic for tuning the sparsity parameter u showed ambiguity in high-dimensional settings (p=1000, q=0.5), suggesting it may not generalize reliably to all scenarios
- The RBF kernel bandwidth selection criterion is referenced but not fully specified, introducing potential variability in implementation
- The dynamic β update rule for preventing empty clusters is mentioned but not detailed, which could affect optimization stability

## Confidence
- **High Confidence**: The core DIB clustering mechanism and weight update via mutual information are well-established theoretically and empirically validated across multiple simulation scenarios
- **Medium Confidence**: The entropy plateau heuristic for sparsity parameter selection works well in most tested cases but showed limitations in high-dimensional settings with many informative features
- **Medium Confidence**: The overall framework combining DIB with feature weighting produces competitive clustering performance, though specific hyperparameter sensitivities need further exploration

## Next Checks
1. **Cross-validation of entropy heuristic**: Apply the sparsity parameter selection method to 10 additional synthetic datasets with varying p and q values (including edge cases like p=1000, q=0.5) to quantify the heuristic's reliability across diverse scenarios

2. **Implementation verification**: Replicate the bladder cancer application using the exact preprocessing steps (log2(TPM+1) transformation, variance filtering >0.01) and compare selected gene sets against known biological markers to validate feature selection quality

3. **Robustness to initialization**: Systematically compare uniform versus K-Means warm-start initialization across 100 synthetic replicates, measuring both convergence speed and final clustering accuracy to quantify the initialization strategy's impact