---
ver: rpa2
title: 'Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients
  via Physics-Aware Multi-Objective Bayesian Optimization'
arxiv_id: '2601.18972'
source_url: https://arxiv.org/abs/2601.18972
tags:
- optimization
- reward
- aberration
- contrast
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Multi-Objective Bayesian Optimization (MOBO)
  framework to automate aberration correction in Scanning Transmission Electron Microscopy
  (STEM). The method treats aberration tuning as a multi-objective problem using physics-based
  rewards (contrast and resolution) and Gaussian Process models to efficiently explore
  the parameter space.
---

# Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization

## Quick Facts
- arXiv ID: 2601.18972
- Source URL: https://arxiv.org/abs/2601.18972
- Reference count: 0
- Primary result: Multi-objective Bayesian optimization framework automates STEM aberration correction by balancing contrast and resolution trade-offs

## Executive Summary
This paper introduces a Multi-Objective Bayesian Optimization (MOBO) framework to automate aberration correction in Scanning Transmission Electron Microscopy (STEM). The method treats aberration tuning as a multi-objective problem using physics-based rewards (contrast and resolution) and Gaussian Process models to efficiently explore the parameter space. Unlike single-metric or deep learning approaches, MOBO identifies a Pareto front that balances competing objectives and exposes trade-offs. In simulation, the method converged within ~25 iterations and avoided local optima dominated by single aberrations. On a real ThermoFisher Spectra 300, MOBO tuned 1st and 2nd order aberrations in a 7D space, achieving balanced image quality while explicitly revealing the contrast–resolution trade-off. Computational cost scales super-linearly with dimensionality, suggesting future optimization of GP inference for higher-dimensional searches. The framework enables robust, sample-efficient alignment and supports "self-optimizing" microscopy.

## Method Summary
The framework uses Multi-Objective Bayesian Optimization (MOBO) with Expected Hypervolume Improvement (EHVI) to tune aberration coefficients. It employs independent Gaussian Process surrogates per objective (contrast and FFT spectral power) with RBF kernels, selecting next evaluations via EHVI acquisition. The method was validated on simulated WS₂ monolayers (60 kV, 30 mrad) and real ThermoFisher Spectra 300 microscope with Au nanoparticles. Search spaces included 3D (C10, C12a, C12b), 4D, and 7D (adding C21/C23 terms). The optimizer identifies Pareto-optimal aberration settings that balance competing image quality metrics without reward hacking.

## Key Results
- MOBO converged to Pareto front within ~25 iterations in simulation, avoiding local optima dominated by single aberrations
- Real microscope experiments successfully tuned 7D aberration space on ThermoFisher Spectra 300
- Framework explicitly reveals contrast-resolution trade-offs rather than collapsing to single-metric extremes
- Computational cost scales super-linearly with dimensionality, becoming bottleneck in 7-parameter case

## Why This Works (Mechanism)

### Mechanism 1: Gaussian Process Surrogate Modeling
Gaussian Process surrogate modeling enables sample-efficient exploration of the aberration parameter space by maintaining probabilistic uncertainty estimates. The GP encodes a prior over smooth functions and updates posterior beliefs after each observation. The kernel function (e.g., RBF) enforces that nearby aberration settings yield similar image quality scores, while the posterior variance quantifies unexplored regions. This dual output (mean prediction + uncertainty) allows the optimizer to prioritize informative evaluations rather than blind search.

### Mechanism 2: Multi-Objective Pareto Formulation
Multi-objective formulation with Pareto fronts prevents reward hacking by exposing trade-offs between competing image-quality metrics. Instead of scalarizing contrast and resolution into a single weighted sum, the framework treats them as a vector objective. The Pareto front identifies non-dominated solutions where improving one objective necessarily degrades another. This prevents the optimizer from converging to artifact-prone local optima (e.g., high-contrast but defocused images) that single-metric methods would accept.

### Mechanism 3: Expected Hypervolume Improvement Acquisition
Expected Hypervolume Improvement (EHVI) acquisition function drives active learning by quantifying expected Pareto-front expansion. EHVI computes the expected increase in dominated hypervolume for each candidate evaluation, integrating over the GP posterior. This explicitly balances exploration (high-uncertainty regions) and exploitation (high-predicted-reward regions) in multi-objective space, guiding the optimizer toward evaluations that maximally improve the frontier.

## Foundational Learning

- **Gaussian Process Regression**: GP is the core surrogate model; understanding mean functions, kernels, and posterior inference is essential for diagnosing optimization behavior. Quick check: Given a GP with RBF kernel trained on 10 observations, would the posterior uncertainty be higher or lower at a point far from all training inputs?
- **Pareto Optimality and Multi-Objective Optimization**: The framework's key innovation is treating aberration tuning as multi-objective; understanding dominance and Pareto fronts is required to interpret results. Quick check: If solution A has contrast=0.8, resolution=0.6 and solution B has contrast=0.7, resolution=0.7, does either dominate the other?
- **Bayesian Optimization Loop**: The overall workflow follows standard BO structure; knowing how acquisition functions trade off exploration/exploitation helps debug slow convergence. Quick check: If the acquisition function always selects points with highest predicted mean (ignoring uncertainty), what failure mode might occur?

## Architecture Onboarding

- **Component map**: Simulator (abtem + pyTEMlib) → Surrogate Model (GPyTorch) → Acquisition (BoTorch EHVI) → Hardware Interface (stemOrchestrator / asyncroscopy) → Logger (FAIR storage)
- **Critical path**: 1. Initialize GP priors (typically zero mean, RBF kernel) 2. Acquire initial random samples (warm-start) 3. Loop: Fit GP → Compute EHVI → Select next aberration vector → Apply via CEOS API → Acquire image → Compute rewards → Update GP 4. Terminate when hypervolume converges or iteration budget exhausted
- **Design tradeoffs**: Independent GPs vs. Multi-task GPs (current: independent GPs for simplicity); Dimensionality vs. Compute cost (GP training scales O(n³), GPU acceleration needed); Reward function choice (contrast vs. FFT power for different sample types)
- **Failure signatures**: Convergence to edge-case images (e.g., high contrast, no lattice) → single-objective collapse or poorly designed reward trade-off; Slow convergence (>50 iterations) → GP hyperparameters stuck; Hardware timeout errors → communication latency exceeding acquisition loop tolerance
- **First 3 experiments**: 1. Validate on simulator: Run MOBO on WS₂ simulation with known ground-truth, confirm convergence within ~25 iterations 2. Low-dimensional hardware test: On Spectra 300, tune only 3 parameters (defocus C₁₀, astigmatism C₁₂a, C₁₂b) on Au nanoparticle sample 3. Stress test reward robustness: Deliberately corrupt images with higher noise, observe whether Pareto front remains stable

## Open Questions the Paper Calls Out

### Open Question 1: High-Dimensional Scaling
How can the super-linear scaling of Gaussian Process training be mitigated to enable efficient aberration correction in search spaces with significantly more than 7 dimensions? The authors note computational cost becomes bottleneck in 7-parameter case, suggesting need for future optimization of GP inference.

### Open Question 2: Symmetry Constraints
To what extent does explicitly encoding crystallographic symmetry constraints into the reward function accelerate the convergence of the optimization loop? The text suggests symmetry constraints could achieve significantly faster optimization by constraining search landscape.

### Open Question 3: Beam Engineering
Can the MOBO framework be extended from aberration correction to intentional "beam engineering" to optimize non-circular probes for specific material sensitivities? The conclusion posits this capability paves way for advanced beam engineering where probes are shaped to maximize sensitivity to properties like lattice strain.

### Open Question 4: Multi-Task GPs
Do Multi-Task Gaussian Processes (MTGPs) offer improved sample efficiency over current independent GP models when objectives are strongly correlated? Supplementary material notes MTGPs may offer advantages when objectives are strongly correlated, though independent GPs are used here.

## Limitations

- Computational cost scales super-linearly with dimensionality, becoming bottleneck in 7-parameter case
- Real microscope experiments lack systematic comparison to established single-metric methods
- Simulation validation uses simplified WS₂ model without complex defects or dynamical diffraction
- Framework requires careful reward function design to avoid optimizing artifacts

## Confidence

- **Core claims about MOBO's ability to find balanced aberration solutions**: High confidence
- **Computational scaling limitations**: High confidence (explicitly stated and observed)
- **Hardware validation scope**: Medium confidence (feasible demonstration but limited comparative analysis)

## Next Checks

1. **Ablation study**: Run identical 7D hardware experiments comparing MOBO (multi-objective) against independent single-objective BO runs for contrast and resolution. Measure both image quality and convergence speed to quantify multi-objective overhead.

2. **Robustness to noise**: Systematically vary noise levels in both simulation and hardware experiments. Track Pareto front stability and hypervolume convergence to identify noise thresholds where MOBO degrades.

3. **Higher-dimensional scaling**: Extend the simulator test to 10D aberration space (including 4th-order terms) and profile GP training time per iteration. Benchmark against sparse GP or inducing point methods to assess computational feasibility.