---
ver: rpa2
title: 'Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in
  the Billion Parameter Era'
arxiv_id: '2506.03994'
source_url: https://arxiv.org/abs/2506.03994
tags:
- attributes
- language
- clip
- image
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits multimodal distributional semantics by comparing
  how vision encoders, multimodal models, and language-only models represent semantic
  attributes of concrete object concepts. The authors probe frozen model representations
  with linear classifiers to predict semantic norms from McRae and Binder datasets,
  using densely annotated concepts from the THINGS dataset.
---

# Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era

## Quick Facts
- **arXiv ID**: 2506.03994
- **Source URL**: https://arxiv.org/abs/2506.03994
- **Reference count**: 40
- **Primary result**: Multimodal vision encoders slightly outperform language-only models on semantic attribute prediction tasks

## Executive Summary
This paper revisits multimodal distributional semantics by comparing how vision encoders, multimodal models, and language-only models represent semantic attributes of concrete object concepts. The authors probe frozen model representations with linear classifiers to predict semantic norms from McRae and Binder datasets, using densely annotated concepts from the THINGS dataset. Multimodal vision encoders like SigLIP and PaliGemma slightly outperform language-only models, while self-supervised vision models like Swin-V2 perform surprisingly well, even on non-visual attributes. Language-only models, especially larger ones, also achieve strong results. Cross-modal correlations are high, suggesting convergence given sufficient data, though within-modality correlations are stronger. The findings indicate that vision models capture substantial conceptual knowledge, challenging assumptions about the necessity of multimodal grounding.

## Method Summary
The authors conduct probing experiments using frozen representations from various models to predict semantic attributes from established datasets. They use the THINGS dataset as a dense vocabulary source and apply linear classifiers to model embeddings to predict semantic norms from the McRae and Binder datasets. The experiments compare vision encoders (SigLIP, Swin-V2), multimodal models (PaliGemma), and language-only models of varying sizes. Cross-modal correlations between representations are computed to assess semantic convergence across modalities.

## Key Results
- Multimodal vision encoders like SigLIP and PaliGemma slightly outperform language-only models on semantic attribute prediction
- Self-supervised vision models like Swin-V2 perform surprisingly well on non-visual attributes
- Cross-modal correlations between modalities are high, suggesting convergence given sufficient data, though within-modality correlations are stronger

## Why This Works (Mechanism)
The study reveals that multimodal models leverage their combined visual and linguistic inputs to capture semantic attributes more effectively than single-modality approaches. The strong performance of vision-only models on non-visual attributes suggests that visual representations contain substantial conceptual knowledge that can be transferred across semantic domains. The convergence of representations across modalities, despite different input types, indicates that large-scale training enables models to develop similar semantic spaces regardless of input modality. This convergence is particularly evident in larger models, which show higher cross-modal correlations.

## Foundational Learning
- **Distributional semantics**: Understanding how word meanings emerge from context and co-occurrence patterns. Why needed: The study builds on distributional semantics theory to compare how different modalities capture semantic knowledge.
- **Probing classifiers**: Using simple linear models to test what information is encoded in frozen representations. Why needed: The probing approach allows isolating what semantic information is present without fine-tuning.
- **Semantic norms**: Standardized datasets (McRae, Binder) containing human judgments about object properties. Why needed: These provide ground truth for evaluating model representations of conceptual knowledge.
- **Cross-modal correlation**: Measuring similarity between representations from different modalities. Why needed: This quantifies how similarly models from different modalities represent semantic concepts.

## Architecture Onboarding

**Component Map**: Vision encoders (SigLIP, Swin-V2) -> Linear classifier -> Semantic prediction
Language models (LLaMA, Mistral) -> Linear classifier -> Semantic prediction
Multimodal models (PaliGemma) -> Linear classifier -> Semantic prediction
THINGS dataset -> Model input -> Semantic norms (McRae, Binder)

**Critical Path**: Model embedding generation → Linear classifier training → Semantic attribute prediction → Cross-modal correlation computation

**Design Tradeoffs**: Frozen representations vs. fine-tuning (simplicity and isolation vs. potential performance gains); linear classifiers vs. complex probes (interpretability vs. representational capacity); semantic norms vs. other evaluation methods (standardized benchmarks vs. potential bias)

**Failure Signatures**: Low cross-modal correlations suggesting modality-specific representations; vision models performing poorly on non-visual attributes; language models failing to capture concrete object properties

**3 First Experiments**:
1. Compare frozen vs. fine-tuned model performance on semantic attribute prediction
2. Test model representations on abstract concepts beyond the THINGS dataset
3. Evaluate cross-modal correlations using different probing architectures (MLP vs. linear)

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus exclusively on concrete nouns from the THINGS dataset, limiting generalizability to abstract concepts
- Probing frozen representations may underestimate models' true semantic capabilities compared to fine-tuning
- Results may not extend to semantic domains beyond the specific attributes tested in McRae and Binder datasets

## Confidence
- **High confidence**: Multimodal vision encoders slightly outperform language-only models on semantic attribute prediction tasks
- **Medium confidence**: Self-supervised vision models perform surprisingly well on non-visual attributes
- **Low confidence**: Broader implication that visual grounding may not be necessary for conceptual knowledge

## Next Checks
1. Test model performance on abstract concepts and non-THINGS vocabulary to assess generalizability beyond concrete nouns
2. Compare frozen probing results against fine-tuned models to determine if performance differences persist with adaptation
3. Evaluate model representations on additional semantic norm datasets beyond McRae and Binder to test robustness across different semantic dimensions