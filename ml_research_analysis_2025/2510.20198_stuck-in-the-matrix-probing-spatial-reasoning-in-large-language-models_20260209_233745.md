---
ver: rpa2
title: 'Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models'
arxiv_id: '2510.20198'
source_url: https://arxiv.org/abs/2510.20198
tags:
- grid
- reasoning
- accuracy
- figure
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks spatial reasoning in large language models
  using five grid-based tasks (quadrant identification, geometric transformation,
  distance evaluation, word search, and tile sliding) scaled across varying grid sizes.
  Models tested include GPT-4o, GPT-4.1, and Claude 3.7 variants.
---

# Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2510.20198
- **Source URL**: https://arxiv.org/abs/2510.20198
- **Reference count**: 17
- **Primary result**: LLMs show sharp accuracy drops (42.7% average loss, up to 84%) on spatial reasoning tasks as grid size increases

## Executive Summary
This study benchmarks spatial reasoning capabilities of large language models using five grid-based tasks (quadrant identification, geometric transformation, distance evaluation, word search, and tile sliding) across varying grid sizes. Models tested include GPT-4o, GPT-4.1, and Claude 3.7 variants. While models maintained high accuracy on small grids, performance degraded sharply as size increased, with average accuracy loss of 42.7% and up to 84% in some cases. The study identifies three primary failure mechanisms: tokenization pattern effects on counting accuracy, board state misinterpretation cascading through multi-step tasks, and mathematical computation failures at scale.

## Method Summary
The study evaluates five spatial reasoning tasks on text-based ASCII grids using square grids ranging from 2×2 to 300×300. Tasks include quadrant identification, geometric transformations, distance measurement, word search, and tile sliding. Models tested include GPT-4o, GPT-4.1, Claude 3.7 No Thinking, and Claude 3.7 Medium Thinking. Each (model, grid size) combination was run 10 times and averaged. The study systematically varied grid formats (spaced, no-spaces, centerline) to examine tokenization effects and tracked error types (positional, mathematical, hallucination). API parameters were kept consistent across runs.

## Key Results
- All models maintained high accuracy (>80%) on small grids (under 20×20) but dropped sharply as size increased
- Average accuracy loss across all tasks and models was 42.7%, with some reaching 84% degradation
- Claude models outperformed OpenAI models overall, though all showed similar degradation patterns
- Every task starting above 50% accuracy lost at least 48% as grids scaled
- Tokenization format changes (no-spaces, centerline) improved some tasks but failed to prevent fundamental degradation

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Pattern Affects Counting Accuracy
Grid representation format directly impacts model ability to track spatial positions through tokenization efficiency. When grid elements tokenize individually (e.g., "· " as separate tokens), models must track more discrete units, increasing counting errors. Formats enabling multi-character tokenization reduce this burden.

### Mechanism 2: Board State Misinterpretation Cascades Through Multi-Step Tasks
Models first parse the grid into an internal representation. Errors in this stage (mislocating symbols) cause downstream failures even when reasoning logic is correct. Initial parsing errors propagate through subsequent reasoning steps, compounding inaccuracies.

### Mechanism 3: Mathematical Computation Fails at Scale Independent of Spatial Understanding
Arithmetic operations (distance calculation, coordinate transformation) degrade with operand size, separate from spatial representation quality. Distance calculations require computing differences between coordinates, and as grid dimensions grow, absolute coordinate values increase, leading to more mathematical errors.

## Foundational Learning

- **Tokenization efficiency**: Understanding how text representations split into tokens explains why certain grid formats perform better. *Quick check*: Can you explain why "····" without spaces might tokenize more efficiently than "· · · ·" with spaces?

- **Internal consistency vs. accuracy**: The paper distinguishes between models being internally consistent (applying rules correctly to their interpretation) versus globally accurate. *Quick check*: If a model correctly identifies a quadrant given a coordinate but has the wrong coordinate, is it internally consistent?

- **Scaling degradation in transformers**: All models showed similar deterioration patterns despite architectural differences, suggesting fundamental limitations. *Quick check*: Why might a task that's easy at 10×10 become impossible at 100×100 even with identical reasoning requirements?

## Architecture Onboarding

- **Component map**: Grid generator -> Task modules -> Evaluation layer -> Tokenization analyzer
- **Critical path**: Start with Quadrant task on small grids (under 20×20) to validate basic spatial parsing, scale to 100×100+ to observe degradation patterns, test alternative tokenization formats to isolate parsing vs. reasoning failures, progress to multi-step tasks to test state maintenance
- **Design tradeoffs**: Delimiter inclusion (spaces improve readability but increase token count; no spaces reduce tokens but may confuse models), explicit aids (centerlines improve accuracy but reduce task difficulty), step-by-step prompting (attempted but showed no significant improvement)
- **Failure signatures**: Hallucination in Search (models claimed 100% detection but accuracy was near 0% on large grids), parsing errors (GPT-4o showed 10% unparseable responses, increasing with grid size), wall ignorance in Slide (models frequently ignored # obstacles)
- **First 3 experiments**: 1) Replicate Quadrant task comparing spaced vs. no-space tokenization on 50×50 and 100×100 grids, 2) Test Transformation task with explicit coordinate input to isolate mathematical reasoning from board interpretation, 3) Run Search task with vertical/diagonal word orientations on small grids to confirm orientation-specific weaknesses

## Open Questions the Paper Calls Out

- **Visual input vs. text**: Does visual input (images) significantly outperform text-based ASCII grids for spatial reasoning tasks as complexity scales? The study used text-only ASCII representations to probe linguistic spatial reasoning.

- **Output modalities**: Can output modalities like code generation mitigate the "claimed detection" hallucinations and parsing errors observed in natural language responses? The authors propose future research focus on "other methods of output, such as code generation or full boards."

- **Universal tokenization strategy**: Is there a universal text-based tokenization strategy that prevents the sharp accuracy degradation observed as grid size increases? The paper tested multiple grid layouts but none overcame the fundamental "lack of robust spatial representations" for large grids.

## Limitations
- Study focuses on transformer-based models, limiting generalizability to other architectures
- Restricted to text-based ASCII representations, not testing visual or multimodal input
- Doesn't explore fine-tuning or alternative architectures that might mitigate observed limitations
- Limited investigation of prompting strategies that might improve performance

## Confidence
- **High Confidence**: The core finding that all tested models show dramatic accuracy degradation (42.7% average loss, up to 84%) when grid sizes increase beyond small dimensions
- **Medium Confidence**: The attribution of failures to specific causes - tokenization inefficiency, board state misinterpretation, and mathematical computation errors
- **Low Confidence**: The generalizability of these findings to other spatial reasoning tasks or to LLMs with different architectural designs

## Next Checks
1. **Tokenization Validation**: Replicate the Quadrant task comparing spaced vs. no-space tokenization on 50×50 and 100×100 grids while systematically varying token count and measuring the correlation with accuracy to quantify tokenization's impact.

2. **Parsing Isolation Test**: Implement the Transformation task with explicit coordinate input (bypassing grid parsing entirely) to determine whether mathematical reasoning failures persist independently of board interpretation errors.

3. **Architectural Comparison**: Test a non-transformer model (such as an RNN or state-space model) on the same spatial reasoning tasks to determine whether the observed degradation patterns are specific to transformer architectures or represent broader limitations in current language models.