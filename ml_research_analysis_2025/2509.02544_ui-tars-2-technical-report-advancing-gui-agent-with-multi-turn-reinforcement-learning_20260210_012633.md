---
ver: rpa2
title: 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement
  Learning'
arxiv_id: '2509.02544'
source_url: https://arxiv.org/abs/2509.02544
tags:
- training
- arxiv
- agents
- agent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UI-TARS-2 addresses challenges in developing autonomous GUI agents
  by introducing a systematic training methodology that integrates scalable data generation,
  multi-turn reinforcement learning, hybrid GUI environments, and a unified sandbox
  platform. The approach uses a data flywheel to co-evolve model capabilities and
  data quality, stabilizes long-horizon RL through asynchronous rollouts and reward
  shaping, extends GUI operation with file systems and terminals, and enables large-scale
  reproducible rollouts.
---

# UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.02544
- **Source URL:** https://arxiv.org/abs/2509.02544
- **Reference count:** 40
- **Primary result:** UI-TARS-2 achieves state-of-the-art performance across GUI, game, and software engineering benchmarks through systematic training methodology including Data Flywheel, multi-turn RL, and vertical agent merging.

## Executive Summary
UI-TARS-2 addresses the challenges of developing autonomous GUI agents by introducing a systematic training methodology that integrates scalable data generation, multi-turn reinforcement learning, hybrid GUI environments, and a unified sandbox platform. The approach uses a data flywheel to co-evolve model capabilities and data quality, stabilizes long-horizon RL through asynchronous rollouts and reward shaping, extends GUI operation with file systems and terminals, and enables large-scale reproducible rollouts. Empirical results show that UI-TARS-2 achieves 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In games, it attains a mean normalized score of 59.8 across 15 titles—roughly 60% of human performance—and remains competitive with frontier models like OpenAI o3 on LMGame-Bench. Additionally, with GUI-SDK extensions, it scores 45.3 on Terminal Bench and 68.7 on SWE-Bench, demonstrating strong generalization to long-horizon information-seeking and software engineering tasks.

## Method Summary
UI-TARS-2 employs a unified sandbox platform supporting VMs (Desktop/Mobile) and browser sandboxes (Games) with screenshot observation capabilities. The system uses a proprietary Seed-thinking-1.6 base model (532M vision encoder, 23B active parameter MoE) and implements a Data Flywheel that dynamically routes high-quality trajectories to Supervised Fine-Tuning and low-quality ones to Continual Pre-training. Multi-turn Reinforcement Learning is stabilized through Value Pretraining (offline critic training), Decoupled GAE (different λ coefficients for policy vs. critic), and asynchronous server-based rollouts. The system trains specialized "vertical" agents for different domains and merges them via parameter interpolation rather than joint multi-domain training, leveraging linear mode connectivity between models fine-tuned from the same checkpoint.

## Key Results
- Achieves 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld
- Attains mean normalized score of 59.8 across 15 game titles (~60% of human performance)
- Scores 45.3 on Terminal Bench and 68.7 on SWE-Bench with GUI-SDK extensions
- Outperforms strong baselines including Claude and OpenAI agents on GUI benchmarks
- Remains competitive with frontier models like OpenAI o3 on LMGame-Bench

## Why This Works (Mechanism)

### Mechanism 1
The system achieves scalable capability improvements by treating data generation as a self-reinforcing cycle (Data Flywheel) rather than relying on static datasets. The model generates trajectories via rejection sampling; correct paths are routed to Supervised Fine-Tuning (SFT) to reinforce success, while incorrect paths are recycled into Continual Pre-training (CT) to broaden the knowledge base without contaminating the policy. This dynamic reallocation ensures the model trains on data matched to its current capability level. Core assumption: The validation function V(s) or reward model accurately distinguishes between capability gaps (suitable for CT) and execution errors (suitable for SFT). Evidence anchors: [abstract] mentions "data flywheel for scalable data generation"; [section 2.3] details the routing logic where V(s)=1 goes to SFT and V(s)=0 goes to CT. Break condition: If the reward model suffers from hallucination or systematic bias, "successes" may be rewarded incorrectly, leading to reward hacking and data contamination.

### Mechanism 2
Multi-turn Reinforcement Learning stability is achieved by decoupling the estimation of advantages for the policy and value functions (Decoupled GAE) and initializing the value function offline (Value Pretraining). Standard PPO struggles with long horizons because the value function (critic) degrades. By pre-training the value function to convergence on a fixed policy before RL updates begin, the system provides a stable baseline. Furthermore, using different λ coefficients for the policy vs. critic prevents the critic from becoming over-confident or myopic in long trajectories. Core assumption: The fixed policy used for Value Pretraining (e.g., π_sft) is sufficiently high-quality to provide a meaningful signal for the critic. Evidence anchors: [section 2.5.4] explicitly details "Decoupled GAE" and "Value Pretraining"; [figure 10] shows the reward curve improvement when value pretraining is applied vs. omitted. Break condition: If the "Length-Adaptive GAE" parameters are misconfigured for drastically different task horizons, the bias-variance tradeoff may collapse, destabilizing training.

### Mechanism 3
The agent generalizes across diverse environments (GUI vs. Games vs. Terminal) by training specialized "vertical" agents and merging them via parameter interpolation rather than joint multi-domain training. Fine-tuning separate models allows for stable optimization in distinct action spaces (e.g., discrete clicks vs. game physics). Merging them via linear interpolation works because models fine-tuned from the same pre-trained checkpoint tend to reside in the same loss basin (linear mode connectivity), preserving individual strengths while consolidating capabilities. Core assumption: Specialized models remain linearly mode-connected; the "skills" are additive in the parameter space. Evidence anchors: [section 2.6] describes merging vertical agents to avoid the instability of joint RL; [section 3.3] discusses hybrid agent RL as an alternative but notes parameter interpolation is more efficient. Break condition: If vertical training pushes the parameters into disjoint basins (catastrophic forgetting or divergence), simple averaging will degrade performance on all tasks.

## Foundational Learning

- **Concept: Generalized Advantage Estimation (GAE)**
  - Why needed here: The paper modifies standard GAE (Decoupled and Length-Adaptive) to solve the credit assignment problem in long GUI trajectories. Without understanding the base GAE mechanism (λ balance), the modifications cannot be tuned.
  - Quick check question: How does increasing the GAE parameter λ affect the bias-variance tradeoff in advantage estimation?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: This is the base RL algorithm. The paper relies on specific PPO variants (Clip Higher, Value Pretraining). Understanding the "surrogate objective" is required to diagnose training instability.
  - Quick check question: Why does PPO use a clipping range, and how does the paper's "Clip Higher" modification specifically encourage exploration?

- **Concept: On-Policy vs. Off-Policy Data**
  - Why needed here: The Data Flywheel distinguishes between SFT (on-policy success) and CT (off-policy/broader data). The distinction is crucial for preventing distribution drift.
  - Quick check question: Why might training an agent purely on expert demonstrations (off-policy) fail in dynamic GUI environments?

## Architecture Onboarding

- **Component map:**
  Sandbox Env -> Policy Server -> Rollout Pool -> Reward Server -> Trainer
  Cloud VMs (Desktop/Mobile) & Browser Sandboxes (Games) exposing actions via SDK/Playwright -> Inference engine (Seed-thinking-1.6 backbone) handling async requests -> Buffer storing partial/complete trajectories -> Computes verifiers (for games) or uses VLM-as-Verifier (for GUI tasks) -> PPO loop with Value Pretraining and Decoupled GAE

- **Critical path:**
  1. Rollout: Async inference sends actions to Sandbox -> Sandbox returns screenshots/states
  2. Evaluation: Trajectories end -> Reward Server assigns scalar reward
  3. Data Routing: High-reward -> SFT buffer; Low-reward -> CT buffer
  4. Update: PPO Trainer pulls from Rollout Pool

- **Design tradeoffs:**
  - *PPO vs. GRPO:* Paper chose PPO because GRPO showed higher volatility in long-horizon settings
  - *Hybrid RL vs. Merging:* Hybrid RL (training on multiple interfaces at once) offers better transfer but is computationally expensive; Merging (Interpolation) is cheaper and stable but assumes mode connectivity
  - *Quantization:* W4A8 reduces latency (4.0s -> 2.5s) with small accuracy drop (47.5 -> 44.4), a viable tradeoff for deployment

- **Failure signatures:**
  - **Entropy Collapse:** If entropy drops too fast, the agent gets stuck in repetitive loops (mitigated here by "Clip Higher")
  - **Negative Value Correlation:** If the value function correlates negatively with rewards during early PPO, it indicates initialization failure (requires Value Pretraining)
  - **Sandbox Drift:** If the browser sandbox timing desyncs from the game clock, reproducibility fails

- **First 3 experiments:**
  1. **Value Pretraining Ablation:** Train a PPO agent with and without the value pretraining phase on a subset of "GUI-Browsing" tasks to replicate the reward delta shown in Figure 10(b)
  2. **Data Flywheel Loop:** Run a single iteration of the flywheel: generate 100 trajectories, route them via the validation function, and fine-tune. Check if the "high-quality" set actually improves task success rate
  3. **Interpolation Validity:** Train two small "vertical" agents (e.g., one for 2048, one for OS file management) and merge them. Verify that the merged model performs within 5% of the specialists on their respective domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can credit assignment and subgoal curriculum mechanisms be improved to overcome reasoning ceilings and performance plateaus in long-horizon game environments?
- Basis in paper: [explicit] The analysis notes that a subset of games shows plateaus "suggesting a reasoning ceiling imposed by the starting backbone" and states that breaking through requires "better credit assignment, curriculum over subgoals, and improved search or memory components."
- Why unresolved: Current model backbones struggle with very long-horizon planning, causing performance to stagnate even with increased optimization steps.
- What evidence would resolve it: Demonstrating that a new credit assignment algorithm or automated subgoal curriculum allows the agent to surpass the current plateau in complex games like *Gem-11* or *Hex-frvr* without changing the underlying model architecture.

### Open Question 2
- Question: Can large-scale joint reinforcement learning across heterogeneous domains be stabilized to outperform parameter interpolation strategies?
- Basis in paper: [explicit] Section 2.6 states that while joint RL is a "natural approach," it remains "unstable and computationally prohibitive" due to differing action spaces and task horizons, leading the authors to use parameter interpolation instead.
- Why unresolved: The instability of joint optimization prevents the training of a single unified policy that can dynamically adapt to diverse environments (e.g., GUI vs. Games) simultaneously.
- What evidence would resolve it: A training run showing that a single model trained via joint multi-domain RL achieves higher aggregate scores across GUI and game benchmarks than the merged parameter-interpolation model.

### Open Question 3
- Question: How can the false positive rates in generative Outcome Reward Models (ORM) be reduced to ensure robustness against reward hacking in non-verifiable tasks?
- Basis in paper: [explicit] The analysis of "Viability of VLM-as-Verifier" reveals that the current ORM "still exhibits a relatively high false positive rate," though it notes the RL training remained effective despite this imperfection.
- Why unresolved: High false positive rates in reward models pose a risk of reward hacking, where agents might learn to generate plausible-looking but incorrect trajectories that fool the verifier.
- What evidence would resolve it: Development of a VLM-based ORM that maintains high correlation with ground-truth success while achieving a significantly lower false positive rate on the 300-sample human-annotated evaluation set.

## Limitations
- Performance relies heavily on proprietary components (Seed-thinking-1.6 base model, internal Generative Outcome Reward Model) that cannot be independently verified
- Specific hyperparameter configurations for Decoupled GAE, Value Pretraining duration, and Data Flywheel ratios are not disclosed
- The merging mechanism assumes linear mode connectivity between specialized agents, which may not hold for more divergent tasks
- Evaluation focuses primarily on task completion rates without detailed analysis of robustness to environmental changes or failure case studies

## Confidence
- **High Confidence:** The architectural framework (Data Flywheel, Value Pretraining, Decoupled GAE) is well-supported by empirical results and aligns with established RL stabilization techniques
- **Medium Confidence:** The performance improvements over baselines are demonstrated, but the proprietary nature of key components limits full verification of claimed advantages
- **Low Confidence:** The long-term stability of the merged vertical agents and scalability of the Data Flywheel approach to more diverse task domains remain unproven

## Next Checks
1. **Value Pretraining Ablation:** Train a PPO agent with and without the value pretraining phase on a subset of GUI-Browsing tasks to replicate the reward delta shown in Figure 10(b)
2. **Data Flywheel Loop:** Run a single iteration of the flywheel: generate 100 trajectories, route them via the validation function, and fine-tune. Check if the "high-quality" set actually improves task success rate
3. **Interpolation Validity:** Train two small "vertical" agents (e.g., one for 2048, one for OS file management) and merge them. Verify that the merged model performs within 5% of the specialists on their respective domains