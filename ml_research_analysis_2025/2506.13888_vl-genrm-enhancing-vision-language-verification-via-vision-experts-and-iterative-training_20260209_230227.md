---
ver: rpa2
title: 'VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative
  Training'
arxiv_id: '2506.13888'
source_url: https://arxiv.org/abs/2506.13888
tags:
- arxiv
- response
- reward
- training
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training Vision-Language
  Reward Models (VL-RMs) in the presence of bootstrapping dilemmas and modality bias,
  where self-generated data reinforces existing model limitations and hallucinated
  visual attributes lead to negative example amplification. To mitigate these issues,
  the authors propose an iterative training framework that incorporates vision experts
  for automated preference dataset construction, Chain-of-Thought (CoT) rationales
  for structured reasoning, and Margin-based Rejection Sampling for iterative refinement.
---

# VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training

## Quick Facts
- **arXiv ID**: 2506.13888
- **Source URL**: https://arxiv.org/abs/2506.13888
- **Reference count**: 40
- **Primary result**: 72.3% overall accuracy on VLRewardBench, 82.4% hallucination detection accuracy

## Executive Summary
VL-GenRM addresses critical challenges in training Vision-Language Reward Models (VL-RMs), specifically the bootstrapping dilemma and modality bias that plague multimodal preference learning. The paper proposes an iterative training framework that combines vision experts for automated preference dataset construction, Chain-of-Thought (CoT) rationales for structured reasoning, and Margin-based Rejection Sampling for iterative refinement. The approach demonstrates superior performance across VL-RM benchmarks, achieving state-of-the-art results on hallucination detection while maintaining strong multimodal reasoning capabilities.

## Method Summary
The VL-GenRM framework operates in two stages: first, it performs Rewarding Instruction-Following Fine-Tuning on pairwise data with CoT rationales using a mixture loss combining standard fine-tuning with correctness-weighted updates. Second, it employs an iterative optimization process using Margin-based Rejection Sampling with LoRA adapters to refine the model. Vision experts (GroundingDINO and Detectron2) automatically verify object presence in images to filter hallucinated negative responses, while CoT rationales provide structured critique rather than descriptive analysis. The method progressively builds preference datasets, enhances structured critiques, and iteratively improves multimodal reasoning through repeated refinement cycles.

## Key Results
- 72.3% overall accuracy on VLRewardBench across General (183 samples), Hallucination (749), and Reasoning (318) categories
- 82.4% hallucination detection accuracy on POVID/RLAIF-V datasets
- Significant improvement over baselines including Qwen2-VL-72B-Instruct and other VL-RMs
- Best-of-N (N=16) evaluation shows strong performance on LLaVA-Wild benchmark

## Why This Works (Mechanism)
The framework addresses bootstrapping dilemmas by using vision experts to automatically verify object presence, preventing the amplification of hallucinated visual attributes in negative examples. CoT rationales shift from descriptive to critique-based reasoning, providing structured feedback that enhances multimodal understanding. Margin-based Rejection Sampling iteratively refines the training data by filtering low-margin samples, ensuring the model learns from high-confidence preference pairs. The combination of these techniques creates a self-improving loop where each iteration produces cleaner data and more accurate critiques.

## Foundational Learning
- **Bootstrapping Dilemma**: When self-generated data reinforces existing model limitations, creating a feedback loop of poor quality supervision. Critical for understanding why naive RLHF approaches fail in multimodal settings.
- **Modality Bias**: When models over-rely on one modality (typically language) while neglecting others (vision), leading to hallucination amplification. Quick check: measure performance gap between unimodal and multimodal inputs.
- **Bradley-Terry Loss**: A pairwise preference modeling loss that compares two responses to determine which is better. Why needed: enables training on preference data rather than absolute labels. Quick check: ensure probability calibration on validation pairs.
- **Margin-based Rejection Sampling**: Filters training samples based on the confidence margin between positive and negative critiques. Why needed: prevents model from learning from ambiguous or low-confidence examples. Quick check: monitor retention rate after filtering (target: 30-50%).
- **Chain-of-Thought Critiques**: Structured reasoning explanations that critique responses rather than simply describing them. Why needed: provides actionable feedback for model improvement. Quick check: critique CoT should outperform descriptive CoT by >10% on hallucination detection.
- **Vision Expert Verification**: Using object detection models to verify visual attributes mentioned in responses. Why needed: grounds visual reasoning in concrete evidence rather than hallucination. Quick check: hallucination detection accuracy should reach 80%+ when vision expert is integrated.

## Architecture Onboarding

**Component Map**: Vision Expert -> Data Filtering -> CoT Critique Generation -> Margin-based Sampling -> LoRA Fine-tuning -> VL-GenRM

**Critical Path**: Vision Expert verification → Hallucination filtering → CoT critique generation → Margin-based rejection sampling → LoRA fine-tuning → Performance evaluation

**Design Tradeoffs**: 
- Vision experts provide reliable hallucination detection but may miss abstract reasoning contexts
- CoT critiques improve structured reasoning but increase computational overhead
- Iterative refinement yields diminishing returns after 2-3 iterations
- LoRA fine-tuning balances parameter efficiency with performance

**Failure Signatures**: 
- Descriptive CoT instead of critique CoT leads to modality bias amplification
- Narrow margin thresholds result in insufficient training data
- Vision expert misses hallucinated objects, contaminating preference dataset
- Saturation in iterative training indicates need for larger models or datasets

**First Experiments**:
1. Baseline hallucination detection accuracy without vision experts (expect ~60-65%)
2. Ablation of CoT rationales vs descriptive responses on VLRewardBench (expect >10% gap)
3. Single iteration of margin-based sampling vs full iterative training (expect 2-3% improvement)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the trade-off between hallucination suppression and abstract reasoning capability be mitigated when using object-detection-based vision experts?
- **Basis in paper**: [explicit] The authors state in Section 7.1 that VL-GenRM "lags in reasoning due to its object detection-based vision module, which improves general understanding and hallucination resistance but is less suited for abstract reasoning requiring fine-grained scene analysis."
- **Why unresolved**: While the current method successfully reduces hallucinations by verifying concrete objects, it inherently limits the model's capacity for high-level multimodal inference (e.g., mathematical or logical reasoning) which does not rely solely on object presence.
- **What evidence would resolve it**: A modified framework integrating semantic or relational experts (bounding boxes) that demonstrates improved performance on MMMU-Pro/MathVerse benchmarks without sacrificing the high hallucination detection rates reported on POVID/RLAIF-V.

### Open Question 2
- **Question**: Does the iterative training framework continue to yield significant improvements beyond the second iteration when applied to larger models or datasets?
- **Basis in paper**: [explicit] In Section 7.4 (Training Method Ablation), the authors note that "Iteration 2 shows marginal gains, indicating saturation in our current setup," while suggesting that "further iterations to remain beneficial with larger models and datasets."
- **Why unresolved**: The provided experiments only demonstrate saturation at iteration 2 for a 7B parameter model. It remains unclear if the margin-based rejection sampling hits a theoretical ceiling or if the ceiling is merely a constraint of the current model capacity.
- **What evidence would resolve it**: Experiments extending the iterative training pipeline to models with 70B+ parameters or significantly larger data pools, tracking performance stability across 4-5 iterations.

### Open Question 3
- **Question**: To what extent does the accuracy of the external vision expert model act as a bottleneck for the upper bound of VL-GenRM performance?
- **Basis in paper**: [inferred] The methodology relies heavily on the vision expert (Section 4.1.2) to verify object presence ($O^*(I)$) and filter hallucinated negative responses. If the expert detector misses objects or produces false positives, the resulting preference dataset $D_{train}$ will contain incorrect "ground truth" signals.
- **Why unresolved**: The paper assumes the vision expert provides reliable supervision. However, detection models have their own error rates and class limitations. The robustness of the VL-GenRM against expert failures (e.g., failing to detect small or rare objects) is not quantified.
- **What evidence would resolve it**: An error analysis quantifying the percentage of training samples where the vision expert contradicts ground truth, and an ablation study measuring VL-GenRM accuracy when using vision experts of varying precision/recall capabilities.

## Limitations
- Experimental setup leaves critical hyperparameters underspecified (λ value, scoring function definition)
- Reliance on specific vision experts may not generalize to other detection architectures
- Iterative training depends on 5K additional samples per iteration without clear sourcing
- Limited ablation studies on individual component contributions to overall performance

## Confidence

- **High Confidence**: The core problem formulation (bootstrapping dilemma and modality bias in VL-RMs) is well-established. Experimental results showing 72.3% accuracy on VLRewardBench are reproducible given the specified evaluation protocol. Two-stage training methodology with LoRA fine-tuning is technically sound.
- **Medium Confidence**: Effectiveness of combining vision experts with CoT rationales is supported by results, but lacks isolated ablation studies. Iterative refinement shows improvement, but marginal benefits suggest diminishing returns warranting further investigation.
- **Low Confidence**: Claim of "advancing VL model alignment with reinforcement learning" is overstated since the method uses supervised fine-tuning rather than true RL algorithms. Generalization beyond VLRewardBench to real-world applications remains unclear.

## Next Checks

1. **Ablation Study**: Run experiments removing each key component (vision experts, CoT rationales, rejection sampling) individually to quantify their independent contributions to the 72.3% baseline performance.

2. **Hyperparameter Sensitivity**: Systematically vary λ in the L_IFT loss (0.1, 0.5, 1.0) and margin thresholds in rejection sampling ([0.1, 0.5, 0.9]) to establish robustness and identify optimal settings.

3. **Cross-Dataset Generalization**: Evaluate the final VL-GenRM model on held-out multimodal datasets (e.g., LLaVA-Wild, human-annotated preference data) to assess whether improvements transfer beyond the training distribution.