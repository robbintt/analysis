---
ver: rpa2
title: Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure
  in Federated Unlearning
arxiv_id: '2601.22601'
source_url: https://arxiv.org/abs/2601.22601
tags:
- unlearning
- phase
- training
- resurfacing
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles federated unlearning (FU) under continued training,
  where the removed influence can re-surface. A new metric, the Resurfacing Rate (RR),
  is introduced to quantify this effect.
---

# Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning

## Quick Facts
- arXiv ID: 2601.22601
- Source URL: https://arxiv.org/abs/2601.22601
- Reference count: 40
- One-line primary result: LETHE achieves near-zero Resurfacing Rate (<1%) across sample, class, and client unlearning tasks even after continued training, outperforming prior methods

## Executive Summary
This work addresses federated unlearning (FU) under continued training, where unlearned knowledge can resurface. The authors introduce a new metric, Resurfacing Rate (RR), to quantify this phenomenon. LETHE employs a three-phase pipeline: an adapter probe captures unlearning direction via gradient ascent, dual-stream rectification suppresses knowledge resurfacing through layer-wise update de-correlation, and a recovery phase restores utility. Experiments demonstrate LETHE maintains persistent forgetting with RR < 1% across multiple unlearning scenarios, while baseline methods show 36-99% RR.

## Method Summary
LETHE operates through a Reshape-Rectify-Restore pipeline. Phase 1 (Reshape) trains a lightweight adapter via gradient ascent on unlearning data while freezing the backbone to capture a stable unlearning direction. Phase 2 (Rectify) performs dual-stream local updates where retained updates are conditionally rectified based on their alignment with the unlearning direction - positively aligned updates are subtracted, while non-aligned updates are negated. Phase 3 (Restore) removes the adapter and runs recovery rounds with standard FedAvg on remaining clients. The method quantifies knowledge persistence using the Resurfacing Rate metric, which measures accuracy degradation on unlearning data during continued training.

## Key Results
- Achieves near-zero Resurfacing Rate (<1%) across sample, class, and client unlearning tasks
- Maintains utility preservation (test accuracy) while ensuring persistent forgetting
- Outperforms prior SOTA methods that show 36-99% RR under continued training

## Why This Works (Mechanism)

### Mechanism 1: Probe Adapter Captures Stable Unlearning Direction
The adapter is trained via gradient ascent on unlearning data while the backbone remains frozen, producing a stable probe that captures the unlearning direction without contaminating the backbone. This probe is then frozen and used to extract corrective signals during local training, amplifying the unlearning signal while keeping the backbone untouched.

### Mechanism 2: Layer-wise Conditional Rectification De-correlates Updates
Conditional subtraction or negation of retained updates based on their alignment with the unlearning direction suppresses knowledge resurfacing. For each layer, if retained updates are positively aligned with unlearning updates, they are subtracted; otherwise, they are negated. This explicitly de-correlates retained updates from the unlearning direction.

### Mechanism 3: Resurfacing Rate Quantifies Persistence Failure
The Resurfacing Rate (RR) metric captures the degree to which unlearning effects degrade during continued training. RR = max(0, ACf - AUf) / (Apre_f - AUf) × 100%, where subscripts denote accuracy on unlearning set at different phases. RR < 1% indicates persistent forgetting.

## Foundational Learning

- **Concept: Gradient Ascent for Unlearning**
  - Why needed here: The probe adapter uses gradient ascent (maximizing loss) on unlearning data to find the direction that increases loss on forgotten data
  - Quick check question: If you run gradient ascent on cross-entropy loss for a classification task, what happens to the model's accuracy on that data?

- **Concept: Cosine Similarity and Update Correlation**
  - Why needed here: The layer-wise rectification decision depends on computing ⟨∆r, ∆u⟩ to detect alignment between retained and unlearning updates
  - Quick check question: Two update vectors have cosine similarity of 0.85. What does this imply about their relationship in parameter space?

- **Concept: Federated Averaging (FedAvg) Protocol**
  - Why needed here: LETHE operates within the FedAvg framework for both unlearning and recovery phases
  - Quick check question: In FedAvg, if client A has 1000 samples and client B has 3000 samples, what are their aggregation weights?

## Architecture Onboarding

- **Component map:**
  Phase 1 (Reshape): Global backbone (frozen) → Adapter ϕ → Gradient ascent on Du → Frozen probe ϕ*
  Phase 2 (Rectify): Dual-stream local updates with frozen ϕ*:
    - Forget stream: Unlearning client computes ∆u on Du
    - Retain stream: Remaining clients compute ∆r on Dr
    - Server: Layer-wise sim(ℓ) → conditional rectification → backbone update
  Phase 3 (Restore): Remove ϕ* → R rounds FedAvg on remaining clients

- **Critical path:** The rectification rule (Eq. 12) is the core logic. Trace: similarity computation → conditional branching → update application. Errors here directly cause resurfacing or utility collapse.

- **Design tradeoffs:**
  - Adapter vs backbone-based probe: Ablation shows backbone probe works but increases Phase U rounds when forget signal is weak; adapter stabilizes estimation
  - Conditional vs unconditional rectification: Ablation shows blind subtraction degrades performance; alignment-aware gating is necessary
  - Correlation penalty γ: Higher γ (1.5) benefits sample-level; moderate γ (0.3) suits client/class-level. Over-penalization slows recovery.

- **Failure signatures:**
  - UF (Unsuccessful Forgetting): u-Acc remains high after Phase U—indicates insufficient forgetting signal or misconfigured rectification
  - High RR (>10%): Knowledge resurfaces in Phase C—indicates residual alignment between retained updates and unlearning direction
  - t-Acc collapse: Utility destroyed—indicates over-aggressive negation or excessive γ

- **First 3 experiments:**
  1. Reproduce Figure 3: Compute layer-wise correlation scores c(ℓ) for a baseline during continued training. Verify positive correlation in deep layers
  2. Ablation sweep on γ: Run LETHE with γ ∈ {0.1, 0.3, 0.5, 1.0, 1.5} for both client-level and sample-level unlearning. Plot total rounds and RR
  3. Phase C monitoring: After unlearning, run 50+ rounds of continued training while logging u-Acc, t-Acc, and rollback cosine similarity per round. Compare LETHE vs FedOSD vs NoT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the rectification strength (correlation penalty γ) be automatically calibrated to adapt to varying unlearning granularities without manual intervention?
- Basis in paper: The conclusion states, "In the future we plan to further introduce automated calibration of rectification strength with stronger theoretical guarantee."
- Why unresolved: The current implementation requires manually setting γ based on empirical studies of round efficiency, which may not be optimal for all datasets or model architectures
- What evidence would resolve it: An adaptive algorithm that dynamically adjusts γ during the unlearning process, achieving optimal efficiency and persistence across different unlearning tasks

### Open Question 2
- Question: Is the LETHE framework applicable to "blind" unlearning scenarios where the target client is strictly offline or refuses to participate in the unlearning computation?
- Basis in paper: The "Reshape" phase requires unlearning clients to perform gradient ascent on the adapter
- Why unresolved: The method assumes the unlearning client is willing and able to compute gradients on the unlearning set to create the corrective signal
- What evidence would resolve it: An extension that utilizes historical gradients or server-side approximations to generate the "Forget stream" signal without active participation from the unlearning client

### Open Question 3
- Question: Does the layer-wise rectification strategy effectively suppress knowledge resurfacing in architectures with distinct knowledge storage mechanisms, such as Transformers in NLP?
- Basis in paper: The paper evaluates image classifiers and notes that "deep layers" show high correlation scores
- Why unresolved: In LLMs, factual knowledge is often localized to specific MLP layers or attention heads rather than distributed smoothly across "deep layers"
- What evidence would resolve it: Experimental evaluation of LETHE on a text generation or classification task, analyzing RR and utility preservation in that domain

## Limitations

- Adapter architecture specifics and hyperparameter tuning across dataset scales remain underspecified
- Long-term persistence beyond 50+ rounds of continued training has limited empirical validation
- Method assumes unlearning client participation in the Reshape phase, which may not be feasible in privacy-critical scenarios

## Confidence

- High: The Reshape-Rectify-Restore pipeline effectively suppresses knowledge resurfacing (RR < 1%) as demonstrated across multiple unlearning scenarios
- Medium: Layer-wise conditional rectification improves over unconditional methods, though optimal γ depends on unlearning granularity
- Low: Long-term persistence beyond 50+ rounds of continued training has limited empirical validation

## Next Checks

1. **Adapter Architecture Sensitivity**: Test LETHE with different adapter types (LoRA, MLP, prefix tuning) to verify that the unlearning direction capture is architecture-agnostic
2. **Cross-dataset Generalization**: Evaluate LETHE on non-vision tasks (e.g., text classification) to test the method's applicability beyond image benchmarks
3. **Extreme Unlearning Stress Test**: Apply LETHE to remove >50% of training data and measure RR and t-Acc degradation to probe method limits