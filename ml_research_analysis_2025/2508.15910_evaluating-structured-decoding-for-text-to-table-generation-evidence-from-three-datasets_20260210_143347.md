---
ver: rpa2
title: 'Evaluating Structured Decoding for Text-to-Table Generation: Evidence from
  Three Datasets'
arxiv_id: '2508.15910'
source_url: https://arxiv.org/abs/2508.15910
tags:
- table
- tables
- decoding
- structured
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates structured decoding for text-to-table
  generation using large language models (LLMs) across three diverse datasets: E2E,
  Rotowire, and Livesum. The authors compare schema-guided (structured) decoding with
  standard one-shot prompting on open-source LLMs ranging from 0.5B to 32B parameters.'
---

# Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets

## Quick Facts
- arXiv ID: 2508.15910
- Source URL: https://arxiv.org/abs/2508.15910
- Authors: Julian Oestreich; Lydia Müller
- Reference count: 10
- Large language models with schema-guided decoding significantly enhance table validity and numerical alignment, particularly for Rotowire, but may degrade performance on text-dense (E2E) or complex aggregation tasks (Livesum)

## Executive Summary
This paper systematically evaluates structured decoding for text-to-table generation using large language models across three diverse datasets: E2E, Rotowire, and Livesum. The authors compare schema-guided (structured) decoding with standard one-shot prompting on open-source LLMs ranging from 0.5B to 32B parameters. Results show that structured decoding significantly enhances table validity and alignment, particularly for numerical data in Rotowire, but may degrade performance on text-dense tasks (E2E) or complex aggregation tasks (Livesum). The largest models generally produce higher-quality tables, though exceptions occur. The study also reveals that common NLP metrics at the table level often overestimate quality, suggesting that mixed evaluation approaches combining soft-match metrics at the cell/row level are more informative for assessing generated table quality.

## Method Summary
The authors evaluate text-to-table generation by comparing schema-guided (structured) decoding with standard one-shot prompting across three datasets using open-source LLMs from 0.5B to 32B parameters. Structured decoding employs JSON schemas to constrain token generation, enforcing valid table structures through dynamic schema construction from ground truth tables. The evaluation uses metrics at cell, row, and table levels, with RMSE for numerical data, revealing that table-level metrics often overestimate quality compared to cell-level F1 scores.

## Key Results
- Structured decoding significantly improves table validity and numerical alignment, particularly for Rotowire player statistics
- Model scale generally improves quality, but Qwen2.5-32B shows unexpected presence drops and RMSE increases
- Common NLP metrics at table level (ROUGE-L, Levenshtein) overestimate quality; cell-level F1 + RMSE provide more accurate assessment
- Task characteristics determine constraint effectiveness: beneficial for sparse numerical extraction, harmful for dense text or long-context aggregation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schema-guided decoding improves structural validity by constraining token generation to valid JSON structures
- Mechanism: The decoder masks token probabilities at each generation step to only allow tokens that satisfy the provided JSON schema, eliminating malformed outputs. The schema builder dynamically constructs nested structures with predefined column headers and nullable cell types.
- Core assumption: The provided schema correctly captures the target table structure and column semantics
- Evidence anchors:
  - [abstract]: "structured decoding significantly enhances the validity and alignment of generated tables, particularly in scenarios demanding precise numerical alignment (Rotowire)"
  - [section 3.2]: "the schema-guided approach enforces tighter structural guarantees through decoding constraints defined by a provided JSON schema"
  - [corpus]: JSONSchemaBench corpus neighbor validates constrained decoding as "dominant technology" for structured outputs, though lacks text-to-table specificity
- Break condition: When schema is misaligned with actual data distribution or when extraction requires reasoning beyond direct mapping

### Mechanism 2
- Claim: Constraint effectiveness is task-dependent—beneficial for sparse numerical extraction, harmful for dense text or long-context aggregation
- Mechanism: Constraints help align values to correct entities when information is sparsely distributed (e.g., Rotowire player stats mentioned once), but restrict flexible extraction when text is densely packed (E2E) or when aggregation over long contexts is required (Livesum).
- Core assumption: Task characteristics (information density, reasoning complexity) determine whether rigidity helps or harms
- Evidence anchors:
  - [abstract]: "may degrade performance in contexts involving densely packed textual information (E2E) or extensive aggregation over lengthy texts (Livesum)"
  - [section 5.1]: "strict schema-guided decoding is helpful when numerical information can be directly extracted from sparsely spread information in the text (Rotowire) but can hinder performance when textual information is densely packed (E2E)"
  - [corpus]: Corpus evidence weak—Map&Make addresses schema-guided extraction but doesn't examine constraint degradation effects
- Break condition: When extraction requires semantic inference across multiple text regions or when flexible tokenization better captures dense information

### Mechanism 3
- Claim: Model scale improves quality but with non-monotonic exceptions under certain task-constraint combinations
- Mechanism: Larger models better utilize schema constraints for content accuracy (lower RMSE), but very large models (32B) can show unexpected presence drops or quality degradation, possibly due to over-sensitivity to format instructions or training data artifacts.
- Core assumption: Scale improves both structural compliance and content extraction up to a point, after which other factors interfere
- Evidence anchors:
  - [section 5.2]: "Qwen2.5-32B demonstrates superior performance on Livesum and Rotowire...yet it unexpectedly shows reduced table presence, particularly pronounced on the Rotowire dataset"
  - [section 4.2]: "Qwen2.5-0.5B generates only 43% of player tables in unstructured setting, but rises to 99.4% with guided decoding"
  - [corpus]: Corpus lacks evidence on scale-constraint interaction anomalies
- Break condition: When model scale introduces instruction-following instabilities or when training-inference distribution mismatch amplifies

## Foundational Learning

- Concept: **Constrained Decoding via Grammar-Based Token Masking**
  - Why needed here: Understanding how JSON schemas translate to token-level constraints during generation explains why validity improves but semantic quality may degrade
  - Quick check question: Can you explain why constraining token probabilities at each step guarantees valid JSON but may limit content flexibility?

- Concept: **Evaluation Granularity (Cell/Row/Table Levels)**
  - Why needed here: The paper demonstrates that table-level metrics (ROUGE-L, Levenshtein) overestimate quality compared to cell-level F1, which is critical for proper system evaluation
  - Quick check question: Why would ROUGE-L scores of 0.95+ still result in zero exact table matches?

- Concept: **Schema-Building from Ground Truth**
  - Why needed here: The approach assumes known schemas; understanding how schemas are constructed from gold data clarifies both the method's applicability and its limitations
  - Quick check question: What information must the schema builder extract from ground truth tables to enable constrained decoding?

## Architecture Onboarding

- Component map:
  - Input text → Schema Builder → JSON schema definition
  - Input text + schema/prompt → Constrained Decoder → JSON/markdown output
  - Raw output → Post-Processor → Structured DataFrame
  - DataFrame + gold table → Evaluator → Multi-level metrics

- Critical path:
  1. Input text → Schema Builder (if structured) → JSON schema definition
  2. Input text + schema/prompt → Constrained Decoder → JSON/markdown output
  3. Raw output → Post-Processor → Structured DataFrame
  4. DataFrame + gold table → Evaluator → Multi-level metrics

- Design tradeoffs:
  - **Structured vs Unstructured**: Higher validity vs potential semantic quality loss on dense text
  - **Model Size vs Reliability**: Larger models improve RMSE but may show unexpected presence drops
  - **Metric Selection**: Table-level string metrics overestimate quality; cell-level F1 + RMSE more informative

- Failure signatures:
  - **Column mismatch errors**: Most common unstructured failure (Figure 3), reduced by model scale and structured decoding
  - **Presence drops in large models**: Qwen2.5-32B shows 95.5% presence on E2E structured vs 100% for smaller models
  - **Zero exact matches on Livesum**: Aggregation task fails completely regardless of decoding strategy

- First 3 experiments:
  1. **Baseline validation**: Run structured vs unstructured decoding on Rotowire subset with Qwen2.5-7B; verify RMSE improves (target: ~10→~3) and presence reaches 100%
  2. **Failure mode analysis**: Test structured decoding on E2E dataset; confirm cell F1 degradation pattern (expect ~0.90→~0.85 drop from unstructured to structured)
  3. **Scale sensitivity test**: Compare Qwen2.5-14B vs 32B on Rotowire Team tables; check for unexpected presence drops and RMSE anomalies in unstructured setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does structured decoding perform in open-schema scenarios where the model must infer the table structure rather than adhering to a predefined schema?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section that there is "significant value in exploring methods for schema inference, moving beyond the exclusive use of predefined schemas."
- Why unresolved: The current study limits its scope to settings where the table schema (headers and types) is provided a priori to the model.
- What evidence would resolve it: Performance benchmarks (validity and alignment scores) on datasets where models must generate the schema dynamically compared to gold standards.

### Open Question 2
- Question: Can the implementation of table-specific grammars (e.g., XML-based) yield better semantic quality or efficiency than the standard JSON schemas used in this study?
- Basis in paper: [explicit] The Conclusion suggests future research should investigate "advanced methods for constrained decoding... for instance, XML-grammars or the design and implementation of table-specific grammars."
- Why unresolved: This evaluation relied exclusively on JSON schema enforcement via the xgrammar package.
- What evidence would resolve it: A comparative analysis of generation quality and latency between JSON-constrained and XML-constrained decoding on the same datasets.

### Open Question 3
- Question: How does structured decoding impact the generation of complex tables featuring multi-line headers or merged cells?
- Basis in paper: [explicit] The authors note in Future Work that "research should also address the generation of more complex tables, such as those... with multi-line headers, or merged cells."
- Why unresolved: The experiments utilized Markdown and JSON formats which do not natively support complex hierarchical headers or cell merging.
- What evidence would resolve it: Evaluations on datasets containing complex hierarchical structures to see if constraints hinder or help alignment in these harder scenarios.

### Open Question 4
- Question: What mixed evaluation approaches combining soft-match metrics best correlate with human judgment for generated tables?
- Basis in paper: [inferred] The Discussion notes that table-level metrics overestimate quality while exact matches are too strict, concluding that "actual table quality is often best assessed through human evaluation."
- Why unresolved: The paper highlights a gap in current metrics but does not propose or validate a specific new metric that bridges the gap between rigid structure and semantic flexibility.
- What evidence would resolve it: Correlation analysis between proposed automated metrics and human preference scores on the E2E and Rotowire datasets.

## Limitations

- Schema Generalization Problem: The evaluation relies on schema extraction from ground truth tables, but real-world deployment would require automatic schema inference from natural language descriptions or examples.
- Aggregation Task Representation: The Livesum dataset shows complete failure for both decoding strategies, suggesting the evaluation may be missing critical aggregation capabilities or the dataset structure itself poses fundamental challenges.
- Model-Constraint Interaction Anomalies: Qwen2.5-32B's unexpected performance degradation lacks explanation and affects generalizability of scale recommendations.

## Confidence

- **High Confidence**: The core finding that structured decoding improves table validity and numerical alignment for Rotowire is well-supported by consistent RMSE improvements and presence rates across model sizes.
- **Medium Confidence**: The task-dependency claims (beneficial for sparse numerical extraction, harmful for dense text/aggregation) are supported by dataset-specific patterns but may reflect dataset characteristics rather than fundamental task properties.
- **Low Confidence**: The scale-constraint interaction findings are tentative due to unexplained anomalies in the largest model.

## Next Checks

1. **Schema Inference Robustness Test**: Implement automatic schema extraction from natural language table descriptions and evaluate structured decoding performance degradation compared to ground-truth schemas across all three datasets.

2. **Aggregation Task Decomposition**: Redesign Livesum evaluation to separate simple row extraction from complex aggregation operations to clarify whether failure is task-level or operation-level.

3. **Model Scale Sensitivity Analysis**: Conduct ablation studies on Qwen2.5-32B to isolate the cause of performance degradation by testing with reduced instruction-following complexity and simplified schemas.