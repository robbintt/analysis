---
ver: rpa2
title: Adaptive Tool Generation with Models as Tools and Reinforcement Learning
arxiv_id: '2510.06825'
source_url: https://arxiv.org/abs/2510.06825
tags:
- tool
- training
- reasoning
- tools
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTR, a simulation-first training framework
  for tool-augmented reasoning that eliminates the need for live API access. MTR employs
  a multi-agent architecture where ToolMaker generates task-specific tool interfaces,
  AutoAgent produces structured think-act-observe sequences, and ToolActor simulates
  realistic tool responses.
---

# Adaptive Tool Generation with Models as Tools and Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.06825
- Source URL: https://arxiv.org/abs/2510.06825
- Authors: Chenpeng Wang; Xiaojie Cheng; Chunye Wang; Linfeng Yang; Lei Zhang
- Reference count: 26
- Primary result: MTR achieves competitive EM scores to live-API systems (29.38% vs 29.3% average) on multi-hop QA benchmarks while improving reasoning-intensive tasks to 40.0% EM on Bamboogle.

## Executive Summary
This paper introduces MTR, a simulation-first training framework for tool-augmented reasoning that eliminates the need for live API access. MTR employs a multi-agent architecture where ToolMaker generates task-specific tool interfaces, AutoAgent produces structured think-act-observe sequences, and ToolActor simulates realistic tool responses. The training proceeds in two stages: Stage-1 SFT teaches trace grammar from complete reasoning sequences, while Stage-2 GRPO optimizes strategy with a composite reward balancing answer correctness and internal consistency. Across four multi-hop QA benchmarks, MTR achieves competitive EM scores to live-API systems while demonstrating substantial improvements on reasoning-intensive tasks.

## Method Summary
MTR is a simulation-based training framework for tool-augmented reasoning that uses a multi-agent pipeline to generate training data without live API access. The system consists of three components: ToolMaker (generates task-specific tool schemas), AutoAgent (produces ReAct traces), and ToolActor (simulates tool responses). Training occurs in two stages - first, a supervised fine-tuning phase teaches the model "trace grammar" using filtered, correct reasoning sequences; second, Group Relative Policy Optimization refines the tool selection strategy using a composite reward function. The approach claims to eliminate distribution shift issues from live APIs while maintaining competitive performance on multi-hop QA benchmarks.

## Key Results
- MTR achieves 29.38% average exact match across four multi-hop QA benchmarks, competitive with live-API systems at 29.3%
- On Bamboogle (most reasoning-intensive task), MTR achieves 40.0% EM compared to 33.3% for the strongest baseline
- Stage-1 SFT + Stage-2 GRPO nearly doubles performance compared to SFT-only training (~29% vs ~16-21% EM)
- ToolMaker-generated tools significantly improve performance (40.0% vs 20.7% on Bamboogle) compared to no tools

## Why This Works (Mechanism)

### Mechanism 1: Simulation-Based Distributional Stabilization
- Claim: Replacing live external APIs with model-simulated responses appears to reduce training instability caused by out-of-distribution inputs.
- Mechanism: The ToolActor generates observations using only parametric knowledge. This keeps the observation tokens within the base model's pre-training distribution, avoiding the "extremely low-probability tokens" and gradient explosion often seen when ingesting noisy or structured live API outputs.
- Core assumption: The ToolActor's internal knowledge is sufficiently accurate and realistic to serve as a surrogate for ground-truth tool responses.
- Evidence anchors:
  - [abstract]: "MTR learns from complete ReAct traces with schema-validated, simulated observations."
  - [section 1]: Mentions that external tool feedback causes "distribution shift" and "accumulation of extremely low-probability tokens," which MTR eliminates.
  - [corpus]: Related work "In-the-Flow Agentic System Optimization" notes that monolithic policies "scale poorly" with diverse tools, supporting the need for stable simulation or modular approaches.
- Break condition: If the ToolActor hallucinates structurally correct but factually incorrect data without detection, the policy may learn to rely on non-existent capabilities (reward hacking).

### Mechanism 2: Structural-Strategic Competence Decoupling
- Claim: Separating the learning of syntax (structural competence) from decision-making (strategic competence) improves sample efficiency and final performance.
- Mechanism: Stage 1 SFT forces the model to learn valid "trace grammar" (e.g., JSON formatting, ReAct loop structure) using filtered, correct trajectories. Stage 2 GRPO then optimizes the policy using a composite reward, focusing capacity on when and what to tool-call rather than how to format it.
- Core assumption: High-quality structural traces (filtered by correctness and validation) are a necessary prerequisite for effective RL exploration.
- Evidence anchors:
  - [abstract]: "Stage-1 SFT teaches 'trace grammar'... Stage-2 GRPO optimizes strategy."
  - [section 4.3, table 2]: Shows SFT-only achieves ~16-21% EM, while SFT+GRPO nearly doubles performance to ~29%.
  - [section 4.3, table 3]: Shows GRPO without SFT initialization suffers from parsing failures and poor exploration.
- Break condition: If Stage 1 data is poisoned by subtle semantic errors that pass the validator, Stage 2 RL will amplify these incorrect reasoning priors.

### Mechanism 3: Task-Adaptive Tool Synthesis
- Claim: Dynamically generating task-specific tool interfaces improves reasoning capability compared to using a fixed, static toolset.
- Mechanism: The ToolMaker analyzes the query domain (e.g., Academic vs. Film) and synthesizes tailored tool schemas (interfaces). This focuses the AutoAgent's attention and reduces the search space for action selection.
- Core assumption: The model possesses sufficient "meta-cognition" to design tools that are both functionally necessary and sufficient for the task.
- Evidence anchors:
  - [abstract]: "ToolMaker generates task-specific... tool interfaces."
  - [section 4.3, table 4]: "MTR (tools-on)" outperforms "MTR (tools-off)" significantly (e.g., 40.0 vs 20.7 on Bamboogle), isolating the value of generated tools.
  - [corpus]: "SpaceTools" paper suggests tool-augmented reasoning requires iterative grounding, implying fixed tools may lack necessary flexibility.
- Break condition: If ToolMaker generates overly complex or hallucinated schemas (e.g., tools that don't exist or have invalid parameters), the AutoAgent may trigger validation errors or pursue impossible strategies.

## Foundational Learning

- Concept: **ReAct (Reasoning + Acting) Paradigm**
  - Why needed here: The entire MTR framework is built upon generating structured "think-act-observe" traces. You cannot understand the AutoAgent's output format or the training data structure without this.
  - Quick check question: Can you distinguish between a "Thought" step (internal reasoning) and an "Act" step (tool invocation) in a trace?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: This is the specific RL algorithm used in Stage 2 to optimize the policy. Understanding its reliance on group sampling (n=8) and KL regularization is critical for debugging training dynamics.
  - Quick check question: How does GRPO differ from standard PPO in terms of how it estimates advantages (hint: relative to group vs. critic model)?

- Concept: **Schema Validation & JSON Structured Output**
  - Why needed here: The "trace grammar" relies on strict adherence to OpenAI-compatible function schemas. The validation checks ($V$) are the guardrails that determine if a trace is usable for training.
  - Quick check question: If a tool requires an integer but the model outputs a string, does the ToolActor execute it or return a validation error?

## Architecture Onboarding

- Component map:
  - **ToolMaker**: Input: Query -> Output: List of Tool Schemas (JSON)
  - **AutoAgent**: Input: Query + Tool Schemas -> Output: ReAct Trace (Think/Act steps)
  - **ToolActor**: Input: Tool Call + Schema -> Output: Simulated Observation (JSON/Text)
  - **Orchestrator**: Manages the loop, runs Validation Checks ($V$), and filters the final Trace for the SFT/RL datasets

- Critical path: The **Trace Quality Control** pipeline (Section 3.2.1). If the filtering logic (answer correctness + validation errors + length) is too loose, Stage 1 SFT trains on noisy grammar; if too tight, you lose training data volume.

- Design tradeoffs:
  - **Simulation Fidelity vs. Cost**: Using models as tools (MTR) is cheaper and faster than live APIs but risks a "reality gap" (Section 5) where simulated behaviors diverge from real-world API quirks.
  - **Fixed vs. Dynamic Tools**: Generating tools (ToolMaker) adds latency and potential for error at the start of the episode but provides higher specificity than a massive, generic tool library.

- Failure signatures:
  - **Malformed Traces**: High rates of "Malformed (%)" or "Slot Disagr. (%)" (Table 5) indicate the validation checks ($V$) are either missing or the SFT checkpoint is weak.
  - **RL Collapse**: If response lengths grow indefinitely or rewards plateau early (Figure 3), check the KL coefficient ($\beta$) or the efficiency reward ($R_{efficiency}$).

- First 3 experiments:
  1. **Tool Fidelity Audit**: Run ToolActor on a held-out set of queries and manually compare simulated outputs vs. real API outputs to quantify the "reality gap."
  2. **Ablation on Validation**: Disable the "pre-check" validation (Table 5) and measure the increase in parsing failures during Stage 2 GRPO training.
  3. **Cold Start Stress Test**: Attempt to train a model using only GRPO (skip Stage 1 SFT) and log the rate of syntax errors to confirm the "trace grammar" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reasoning policies learned via the MTR simulation framework transfer effectively to live API environments without performance degradation?
- Basis in paper: [explicit] The authors state in the Limitations section that the "simulation-first approach may lack the full complexity of real-world APIs, potentially creating a reality gap during deployment."
- Why unresolved: The paper evaluates performance on benchmarks using the simulated environment but does not validate the trained agents on live, external APIs to confirm deployment readiness.
- What evidence would resolve it: An empirical study evaluating MTR-trained agents on identical benchmarks using live API calls instead of the ToolActor simulation.

### Open Question 2
- Question: How can simulated trace learning be optimally combined with selective live API interactions to maximize both training efficiency and grounding?
- Basis in paper: [explicit] The Future Work section suggests developing "hybrid approaches combining simulated trace learning with selective live API interactions."
- Why unresolved: The current framework relies entirely on simulation; the trade-offs or potential gains of integrating live API feedback into the training loop remain unexplored.
- What evidence would resolve it: Ablation studies comparing pure MTR against hybrid models (e.g., MTR pre-training followed by live-API fine-tuning) in terms of performance and training cost.

### Open Question 3
- Question: Does the MTR framework generalize to complex execution domains such as code generation and scientific reasoning?
- Basis in paper: [explicit] The authors explicitly propose extending MTR to "complex domains such as code generation and scientific reasoning" in the Future Work section.
- Why unresolved: The current experiments are restricted to multi-hop QA benchmarks (HotpotQA, etc.), leaving the framework's efficacy on tasks requiring verifiable execution unproven.
- What evidence would resolve it: Experimental results applying MTR to code generation benchmarks (e.g., HumanEval) or scientific reasoning datasets.

## Limitations
- The ToolActor's simulated responses may not fully capture the complexity and edge cases of real-world API behaviors, creating a "reality gap."
- The paper does not validate whether MTR-trained agents maintain performance when deployed on live APIs versus simulated environments.
- The external verifier used for trace filtering is unspecified, creating potential reproducibility concerns.

## Confidence
- **High Confidence:** The core architecture (multi-agent simulation, two-stage training, composite rewards) is well-specified and the reported EM scores are methodologically sound.
- **Medium Confidence:** The mechanism claims (simulation stability, structural-strategic decoupling, task-adaptive synthesis) are logically coherent but rely on assumptions about model behavior that aren't empirically validated in the paper.
- **Low Confidence:** The claim that MTR eliminates "distribution shift" entirely is overstated - it replaces one form of shift (live API) with another (model hallucination), which may introduce different failure modes.

## Next Checks
1. **Reality Gap Quantification:** Compare ToolActor's simulated responses against actual API outputs on a held-out test set to measure accuracy and identify systematic hallucinations.
2. **ToolMaker Schema Validation:** Manually audit 50 randomly sampled tool schemas generated by ToolMaker to assess their functional validity and domain appropriateness.
3. **Cold Start Comparison:** Train an identical architecture without Stage 1 SFT to empirically verify whether structural competence is indeed a prerequisite for effective RL exploration.