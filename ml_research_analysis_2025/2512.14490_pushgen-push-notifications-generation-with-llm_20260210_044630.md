---
ver: rpa2
title: 'PushGen: Push Notifications Generation with LLM'
arxiv_id: '2512.14490'
source_url: https://arxiv.org/abs/2512.14490
tags:
- push
- content
- reward
- notifications
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PushGen introduces a two-stage framework for automated push notification
  generation using LLMs. It employs category-controllable fine-tuning to ensure diverse,
  style-consistent content and a pairwise reward model trained on A/B test data to
  select high-CTR notifications.
---

# PushGen: Push Notifications Generation with LLM

## Quick Facts
- **arXiv ID:** 2512.14490
- **Source URL:** https://arxiv.org/abs/2512.14490
- **Reference count:** 17
- **Primary result:** +14.08% CTR improvement on machine-generated content, deployed at scale

## Executive Summary
PushGen is a two-stage framework for automated push notification generation using LLMs, deployed at Kuaishou. It employs category-controllable fine-tuning to ensure diverse, style-consistent content and a pairwise reward model trained on A/B test data to select high-CTR notifications. The system replaces 85.2% of manually written notifications while achieving significant CTR improvements. The approach addresses the challenge of generating personalized, engaging push notifications at scale while maintaining content quality and avoiding reward hacking.

## Method Summary
PushGen combines (1) supervised fine-tuning with category conditioning to generate diverse push notification candidates, and (2) a pairwise reward model trained on A/B test data to select the best candidate. The system extracts video captions using a VLM, generates multiple category-specific notifications via SFT, then ranks candidates using a BERT-based reward model. Data cleaning with statistical filtering and confidence weighting improves sample quality for training.

## Key Results
- +14.08% CTR improvement on machine-generated content
- +4.026% CTR improvement in public traffic
- +0.067% DAU gain
- 85.2% of notifications replaced by model-generated content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit category conditioning during SFT enables controllable style diversity in generated notifications.
- **Mechanism:** A frozen LLM classifies historical push samples into predefined style categories (e.g., Suspense, Emotion, Practical). These category labels become explicit conditioning tokens in the training prompt: `<Task Prompt> + <Control Category> + <Item Caption>`. At inference, the model generates candidates across multiple categories by varying the category token, then a reward model selects the best.
- **Core assumption:** Style labels derived from historical high-performing pushes transfer meaningfully to new content; users respond positively to style diversity rather than homogeneous outputs.
- **Evidence anchors:**
  - [abstract] "combines... (1) a controllable category prompt technique to guide LLM outputs toward desired styles"
  - [section 2.1.3] "After SFT, our 8B model substantially increased the proportion of generated contents outperforming the baseline, rising from 44% to 83%"
  - [section 4.2, Table 2] Shows style distribution shift—e.g., Suspense increased from 6% to 11.8%, Practical from 1.8% to 9%
  - [corpus] Weak direct evidence for category-controllable generation in adjacent literature; corpus focuses on general LLM content generation, not style conditioning specifically.
- **Break condition:** If generated categories collapse into a single dominant style despite conditioning, or if A/B tests show no engagement difference across style-conditioned outputs, the mechanism is failing.

### Mechanism 2
- **Claim:** Pairwise reward models trained on A/B comparison data align selection with CTR better than pointwise scoring.
- **Mechanism:** The system runs small-traffic A/B tests where the same video receives different push notifications. Pairs with observed CTR differences become training data: `[CLS] Push1 [SEP] Push2 [SEP]` → binary label (which won). The BERT-based model learns relative preferences, not absolute scores, avoiding confounders like creator popularity or video quality that plague pointwise CTR prediction.
- **Core assumption:** Relative CTR comparisons isolate notification quality from confounding factors (video quality, creator fame, topic relevance); the reward model generalizes from A/B pairs to unseen candidate pairs.
- **Evidence anchors:**
  - [abstract] "(2) a reward model that ranks and selects generated candidates"
  - [section 2.2.2] "point-wise learning was unsuitable due to confounding factors such as author popularity, topical alignment, and video quality"
  - [section 4.2] BERT-300M achieved 73.58% accuracy; offline accuracy correlated with online gains but larger BERT-700M underperformed online despite higher offline accuracy
  - [corpus] Related work "CTR-Driven Ad Text Generation via Online Feedback Preference Optimization" (Chen et al., 2025) supports preference-based optimization for engagement metrics.
- **Break condition:** If the reward model selects candidates that consistently underperform random selection in live A/B tests, or if it exhibits reward hacking (selecting sensational/misleading content), the alignment mechanism has failed.

### Mechanism 3
- **Claim:** Data cleaning with statistical filtering + confidence weighting reduces noise and improves SFT sample quality.
- **Mechanism:** Three-stage pipeline: (1) Hard filters remove low-quality samples using thresholds (CTR > 0.6%, hate rate < 1%, etc.); (2) Soft filters perform within-cluster quantile cropping to reduce category bias; (3) Confidence weighting assigns sample-specific loss weights based on CTR and exposure volume, emphasizing high-confidence successful pushes.
- **Core assumption:** Historical engagement metrics (CTR, LVTR, HTR) are reliable proxies for notification quality; extreme outliers are noise rather than valuable edge cases.
- **Evidence anchors:**
  - [section 2.1.2] Explicit filter formulas: `filter_hard = (CTR>0.6%) & (SVR<40%) & (LVTR>50%) & (HTR<1%) & (PV>800)`
  - [section 2.1.2] Confidence formula: `confidence = 0.3 + 0.35 * min(CTR,0.1)/0.1 + 0.35 * log²(min(PV,10000)/10000)`
  - [section 4.1] "constructed a cleaned training corpus of over 70k samples selected from a historical million-scale push notification repository"
  - [corpus] No direct corpus evidence for this specific filtering approach in notification generation.
- **Break condition:** If aggressive filtering removes too many samples (reducing diversity) or if confidence weighting amplifies a narrow style pattern causing homogeneity, the data pipeline is overfitting to historical biases.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) with Conditional Generation**
  - **Why needed here:** The generator requires explicit style control, which standard SFT doesn't provide. Understanding how to inject conditioning tokens (categories) into prompts enables controllable outputs.
  - **Quick check question:** Can you explain how adding a category token to the training prompt changes model behavior at inference time?

- **Concept: Pairwise Preference Learning vs. Pointwise Regression**
  - **Why needed here:** The reward model uses pairwise comparisons rather than absolute CTR prediction. This distinction is critical for understanding why their approach avoids confounders.
  - **Quick check question:** Why would predicting "Push A will get 2.3% CTR" fail when "Push A will outperform Push B" succeeds?

- **Concept: Reward Hacking in RL/Preference Optimization**
  - **Why needed here:** The paper explicitly discusses why they avoided RL/PPO/DPO—salience biases (idol names, specific numbers) can cause models to game the reward signal rather than improve content quality.
  - **Quick check question:** If your reward model overweights notifications containing numbers (e.g., "5 tips"), what failure mode would you expect during RL training?

## Architecture Onboarding

- **Component map:** VLM Dense Caption Generator -> Push Notification Generator -> Reward Model -> A/B Test Data Pipeline -> Category Classifier
- **Critical path:** Video input → VLM caption → SFT generator produces N candidates (one per category) → Reward model scores all pairs → Select highest-scoring candidate → Deploy
- **Design tradeoffs:**
  - **8B vs. 32B generator:** 32B models overfit; 8B generalizes better with limited data (70k samples)
  - **BERT-300M vs. BERT-700M reward model:** 700M has higher offline accuracy but 300M performs better online (flatter curve, larger AUC in Figure 4)
  - **SFT + RM vs. RL/PPO:** RL causes reward hacking (hallucination, factual inconsistency); decoupled SFT + RM is safer
- **Failure signatures:**
  - **Reward hacking:** Model generates sensational but factually incorrect content (e.g., fake idol names, exaggerated claims)
  - **Style collapse:** All generated notifications converge to single dominant category despite conditioning
  - **Overfitting:** Training loss continues decreasing but validation loss rises (observed with 32B models)
  - **Misaligned selection:** Reward model picks candidates that underperform baseline in live A/B tests
- **First 3 experiments:**
  1. **Validate VLM caption quality:** Manually inspect 50 video→caption pairs for factual consistency and information completeness before training the generator
  2. **Reward model calibration test:** Run the trained BERT-300M on held-out A/B pairs; verify accuracy >70% and check if high-confidence predictions (>0.8) have higher actual win rates
  3. **Category controllability check:** Generate 20 candidates per video across 5 categories; verify each category produces stylistically distinct outputs (manual review) and that the reward model doesn't disproportionately favor one category

## Open Questions the Paper Calls Out
None

## Limitations

- **Data curation assumptions:** The paper assumes that historical CTR/LVTR/HTR metrics reliably indicate notification quality, but these metrics may conflate notification effectiveness with video quality, creator popularity, or temporal factors.
- **Category label validity:** Style categories are assigned by a frozen LLM rather than human annotators. The semantic boundaries between categories are not validated for consistency or inter-rater reliability.
- **Generalizability constraints:** The framework is deployed within a single platform (Kuaishou) with specific user demographics and content characteristics. Success metrics are platform-specific.

## Confidence

- **High confidence (>80%):** The overall CTR improvement claims (+14.08% replaced, +4.026% public, +0.067% DAU) are directly measured from A/B tests at scale.
- **Medium confidence (60-80%):** The mechanism explaining style diversity through category conditioning is plausible but not rigorously tested.
- **Low confidence (<60%):** The data cleaning pipeline's impact on model quality is asserted but not empirically validated.

## Next Checks

1. **Category Ablation Study:** Deploy the same notification generation system without category conditioning (remove the category token from prompts). Compare CTR, style diversity metrics, and content similarity between the full system and ablation.

2. **Reward Model Calibration Analysis:** For a held-out test set of 1000 notification pairs, compute the actual CTR difference between the model's top-1 and bottom-1 predictions. Plot calibration curves showing whether high-confidence predictions (>0.8) actually have higher win rates than low-confidence predictions (<0.2).

3. **Cross-Domain Transfer Test:** Deploy the trained generator and reward model on a different notification domain (e.g., news app push notifications or e-commerce promotional messages). Compare performance against the original domain to quantify how much the system's success depends on Kuaishou-specific user behavior patterns and content characteristics.