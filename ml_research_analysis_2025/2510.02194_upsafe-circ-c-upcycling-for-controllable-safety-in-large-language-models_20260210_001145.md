---
ver: rpa2
title: 'UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models'
arxiv_id: '2510.02194'
source_url: https://arxiv.org/abs/2510.02194
tags:
- safety
- utility
- arxiv
- layers
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to improve safety of large language
  models (LLMs) while preserving general capabilities. The approach identifies safety-critical
  layers in a pretrained model, upcycles them into a mixture-of-experts (MoE) structure,
  and applies a two-stage supervised fine-tuning (SFT) strategy.
---

# UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models

## Quick Facts
- arXiv ID: 2510.02194
- Source URL: https://arxiv.org/abs/2510.02194
- Reference count: 40
- Key outcome: Upcycling framework achieves controllable safety in LLMs via MoE routing and safety temperature

## Executive Summary
UpSafe$^\circ$C introduces a novel framework for controllable safety in large language models that upcycles safety-critical layers into a mixture-of-experts (MoE) structure. The approach identifies which layers in a pretrained model are most responsible for safety decisions, replaces them with expert modules, and applies a two-stage supervised fine-tuning strategy. A safety temperature parameter enables dynamic adjustment of the safety-utility trade-off during inference, allowing models to be tuned toward either safety or general capability as needed.

The framework demonstrates significant improvements in safety performance against harmful and jailbreak inputs while maintaining competitive results on general benchmarks. Experiments across multiple LLM sizes (7B-14B) show that the upcycling mechanism, combined with safety temperature control, achieves the Pareto-optimal frontier between safety and utility. The method represents a systematic approach to addressing the challenge of building models that can be safely deployed across diverse real-world scenarios.

## Method Summary
The UpSafe$^\circ$C framework operates through a multi-stage process beginning with layer identification. Using a Safety-Specific Sensitivity (SS-Score) metric, the method quantifies which transformer layers contribute most to safety decisions on harmful prompts. The top-k safety-critical layers are then replaced with expert modules, creating a MoE structure where routing decisions determine whether safety or general capability experts process each token.

A two-stage supervised fine-tuning strategy follows: first, a safety-focused SFT stage fine-tunes the safety expert and general experts on curated safety and general instruction datasets; second, a general SFT stage further refines all experts on standard instruction data. During inference, a safety controller module analyzes input risk and routes tokens through either the safety expert (for risky inputs) or general experts (for benign inputs). The safety temperature parameter $\tau$ modulates this routing behavior, allowing explicit control over the safety-utility trade-off.

## Key Results
- Significant safety improvements against harmful and jailbreak inputs across multiple 7B-14B models
- Maintains competitive performance on general capability benchmarks
- Safety temperature parameter enables fine-grained control over safety-utility trade-off
- Achieves Pareto-optimal frontier between safety and utility

## Why This Works (Mechanism)
The framework exploits the observation that different transformer layers contribute differently to safety-relevant representations versus general capability representations. By identifying and isolating safety-critical layers, UpSafe$^\circ$C can selectively enhance safety mechanisms without compromising the broader model capabilities. The MoE structure allows conditional routing based on input risk, while the safety temperature parameter provides a continuous control mechanism that adjusts the relative weight given to safety versus utility considerations.

The two-stage SFT strategy is crucial: the initial safety-focused fine-tuning ensures the safety expert learns robust harmful content detection and refusal mechanisms, while the subsequent general fine-tuning preserves and enhances overall model capabilities. This separation prevents the safety fine-tuning from overly constraining the general capabilities while ensuring the safety expert is sufficiently specialized.

## Foundational Learning
**Safety-Specific Sensitivity (SS-Score)**: Metric for quantifying layer contribution to safety decisions
- Why needed: Identifies which layers most impact harmful content detection
- Quick check: Correlates layer importance scores with model safety performance

**Mixture-of-Experts (MoE)**: Conditional computation architecture where different experts handle different inputs
- Why needed: Enables selective routing of safety-critical processing
- Quick check: Routing decisions align with input risk assessment

**Safety Temperature ($\tau$)**: Parameter controlling the trade-off between safety and utility
- Why needed: Provides continuous, interpretable control over model behavior
- Quick check: Higher $\tau$ values consistently improve safety metrics

**Layer-wise Sensitivity Analysis**: Technique for identifying which transformer layers contribute most to specific behaviors
- Why needed: Enables targeted upcycling of safety-critical components
- Quick check: Top-k identified layers show highest safety impact when modified

**Two-Stage Supervised Fine-Tuning**: Sequential fine-tuning process with safety-first then general refinement
- Why needed: Prevents safety fine-tuning from degrading general capabilities
- Quick check: Safety metrics improve after stage 1, general metrics recover after stage 2

## Architecture Onboarding

**Component Map**: Input -> Safety Controller -> MoE Router -> (Safety Expert / General Experts) -> Output

**Critical Path**: Input processing → Safety controller risk assessment → MoE routing decision → Expert module processing → Output generation

**Design Tradeoffs**: 
- More safety experts improve safety but increase computational overhead
- Higher safety temperature improves safety but may reduce general capability
- Layer identification accuracy directly impacts upcycling effectiveness

**Failure Signatures**:
- Low routing scores to safety expert on clearly harmful inputs
- Inconsistent behavior across similar prompts with different phrasing
- Degradation in general capability metrics when safety temperature is high

**First 3 Experiments to Run**:
1. Measure routing score distribution on benchmark harmful vs. benign inputs
2. Test safety performance across different $\tau$ values (0.0, 0.5, 1.0)
- Compare general capability retention across different numbers of safety-critical layers (k=1, 3, 5)

## Open Questions the Paper Calls Out

**Open Question 1**: How does the distillation process spatially redistribute safety-critical representations, and can safety-critical layer identification be made robust to these shifts?
- Basis in paper: Probing distilled models shows less consistent results than instruction-tuned counterparts, suggesting distillation may smooth or obscure layer-wise safety-specific activations
- Why unresolved: Paper observes this phenomenon but doesn't characterize the mechanism or propose solutions
- What evidence would resolve it: Comparative study of representation geometry across teacher-instruction-tuned-distilled model triples

**Open Question 2**: Does the optimal number of safety-critical layers scale predictably with model size and architecture, or is it primarily determined by training dynamics?
- Basis in paper: k=3 works well across 7B-14B models, but no theoretical justification for this selection
- Why unresolved: Layer identification is empirical; generalization to larger models and different architectures is unknown
- What evidence would resolve it: Systematic experiments across model scales and transformer variants

**Open Question 3**: Can the safety temperature $\tau$ be set adaptively based on input characteristics rather than requiring manual specification?
- Basis in paper: $\tau$ is framed as user-specified parameter that can be "dynamically based on the user's intent"
- Why unresolved: Practical deployments cannot expect manual adjustment per input; automatic selection remains unexplored
- What evidence would resolve it: Training a lightweight risk classifier to predict optimal $\tau$ values

## Limitations

- Safety evaluation focuses on specific harmful content categories; generalizability to other safety domains is uncertain
- Layer identification methodology may not be robust across different model architectures or safety definitions
- Computational overhead of MoE structure and safety controller during inference is not thoroughly characterized

## Confidence

- Safety improvement claims: High confidence based on comprehensive benchmarking
- Capability preservation claims: High confidence supported by general benchmark performance
- Controllability claims: High confidence demonstrated through safety temperature parameter
- Pareto optimality claims: Medium confidence requiring broader evaluation across diverse use cases

## Next Checks

1. **Robustness to Transfer**: Test UpSafe$^\circ$C on safety domains not seen during training to evaluate generalization beyond the specific harmful content categories used in current experiments.

2. **Adversarial Resilience**: Evaluate the framework against adaptive attacks targeting the MoE structure or safety controller, including gradient-based attacks and prompt engineering strategies.

3. **Real-World Deployment Impact**: Measure practical utility and safety performance on long-form generation tasks and complex reasoning scenarios, as current evaluation focuses primarily on short responses to harmful prompts.