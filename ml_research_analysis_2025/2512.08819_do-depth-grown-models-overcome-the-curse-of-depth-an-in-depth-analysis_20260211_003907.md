---
ver: rpa2
title: Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis
arxiv_id: '2512.08819'
source_url: https://arxiv.org/abs/2512.08819
tags:
- layer
- depth
- baseline
- layers
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether gradually growing the depth of\
  \ Transformers during training can overcome the \"Curse of Depth\"\u2014a phenomenon\
  \ where deeper layers in standard models contribute less to performance. The authors\
  \ analyze two gradual depth-growth strategies (MIDAS and LIDAS) and compare them\
  \ to non-grown baselines and LayerNorm-Scaling."
---

# Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis

## Quick Facts
- arXiv ID: 2512.08819
- Source URL: https://arxiv.org/abs/2512.08819
- Reference count: 40
- Primary result: Gradual depth growth strategies (MIDAS/LIDAS) yield more effective depth utilization and improved reasoning performance compared to non-grown baselines.

## Executive Summary
This paper investigates whether gradually growing the depth of Transformers during training can overcome the "Curse of Depth"â€”a phenomenon where deeper layers in standard models contribute less to performance. The authors analyze two gradual depth-growth strategies (MIDAS and LIDAS) and compare them to non-grown baselines and LayerNorm-Scaling. Using depth-wise analyses, they demonstrate that gradual growth yields more effective depth utilization, forms permutable computational blocks, and alters residual stream structure. LIDAS, their proposed method, matches or exceeds MIDAS in reasoning benchmarks while better aligning attention sublayers and achieving a more symmetric weight structure. Models trained with gradual depth growth show higher depth scores and continue performing novel computations in later layers, unlike standard models where later layers can often be removed with little performance loss.

## Method Summary
The authors implement two growth strategies (MIDAS and LIDAS) that gradually increase model depth during training by duplicating middle layers/blocks. Models are trained on the SmolLM-Corpus mixture (200B tokens for 360M, 400B for 1.7B) using LLaMA-style decoder-only Transformers. Growth occurs via block size b=4 with PROP-1 schedule over 170k steps. The growth operator deep-copies layer parameters and optimizer state (AdamW moments) when inserting new layers. LIDAS duplicates around the exact middle layer-wise position while MIDAS duplicates entire middle blocks. Models are evaluated on NLL, reasoning benchmarks (Math Word, Reasoning Primitives), and knowledge benchmarks, with additional depth-wise analyses using depth scores and early-exit Tuned Lens diagnostics.

## Key Results
- Gradual depth growth yields more effective depth utilization with higher depth scores than baselines
- Grown models form permutable computational blocks that maintain performance when swapped
- LIDAS achieves better attention alignment and symmetric weight structure than MIDAS while matching/exceeding its reasoning performance
- Later layers in grown models continue performing novel computations unlike standard models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradual depth growth mitigates the "Curse of Depth" by enforcing high utility in later layers.
- **Mechanism:** Standard training often results in later layers contributing minimally to the output (diminishing returns). By initializing new layers as functional copies of active middle layers and continuing training, the growth process likely forces these eventually-deep layers to maintain non-trivial gradients and functional roles, preventing them from becoming dormant refinements.
- **Core assumption:** The depth score metric accurately reflects functional contribution rather than just noise amplification.
- **Evidence anchors:**
  - [abstract]: "growth via gradual middle stacking yields more effective utilization of model depth"
  - [section 4.1]: "Skipping late layers degrades prediction accuracy substantially more for MIDAS and LIDAS than for the baseline... grown models continue to perform novel computations in later layers."
  - [corpus]: Weak direct support in provided neighbors; related work generally focuses on model capacity rather than dynamic depth utilization.
- **Break condition:** If a model exhibits high depth scores but performance degrades significantly upon *any* single layer ablation (indicating fragility rather than robust utilization), this mechanism may describe memorization rather than efficient computation.

### Mechanism 2
- **Claim:** Growth strategies induce the formation of permutable, block-like computational structures.
- **Mechanism:** The insertion of duplicated blocks creates a structural bias towards recurrent or iterative processing. The model appears to learn "modules" where the block presence is critical, but the exact ordering of blocks is less sensitive than in standard models, suggesting a shift from strictly sequential depth to iterative refinement.
- **Core assumption:** Robustness to block swapping implies functional modularity rather than redundancy/over-parameterization.
- **Evidence anchors:**
  - [abstract]: "facilitates the formation of permutable computational blocks"
  - [section 4.2]: "Grown models allow swapping blocks of up to size four with a relatively small decrease in performance... indicating less order dependence of these blocks."
  - [corpus]: Neighbor "Looped Transformers as programmable computers" supports the concept of iterative blocks, though not explicitly cited in this specific context.
- **Break condition:** If reversing the order of blocks yields identical performance to the original order (perfect commutativity), the mechanism is likely simple averaging or ensembling rather than iterative reasoning.

### Mechanism 3
- **Claim:** LIDAS improves over MIDAS by aligning layer-wise symmetry and attention contributions.
- **Mechanism:** MIDAS duplicates blocks, which can introduce asymmetry in an even-depth network (biasing towards the upper block). LIDAS duplicates the exact center layer-wise, creating a more symmetric initialization. This symmetry appears to align attention sublayers better with the residual stream, preventing the "orthogonal" or "erasing" behavior seen in MIDAS block starts.
- **Core assumption:** Symmetric weight structure and higher cosine similarity to the residual stream causally improve reasoning capability.
- **Evidence anchors:**
  - [section 4.4]: "In LIDAS, inter-block cosine similarities are higher and more symmetric about the centre... LIDAS aligns its attention sublayers better to the residual stream."
  - [figure 4]: Shows MIDAS attention cosine similarity starting near zero or negative at block starts, whereas LIDAS maintains positive alignment.
  - [corpus]: No direct corpus support for this specific initialization symmetry argument.
- **Break condition:** If symmetric initialization leads to mode collapse or reduced diversity in attention heads, the performance gains would be limited to specific tasks.

## Foundational Learning

- **Concept: Residual Stream Dynamics ($h_{i+1} = h_i + \text{layer_output}$)**
  - **Why needed here:** The paper analyzes the "Curse of Depth" by measuring how much each layer perturbs or contributes to the residual stream ($a_i \cdot h_i$). Understanding that layers add to a cumulative state is crucial for interpreting the "contribution" and "cosine similarity" plots.
  - **Quick check question:** If a layer's output has a negative cosine similarity with the current residual stream, is it strengthening or suppressing existing features?

- **Concept: LayerNorm-Scaling (LN-Scaling)**
  - **Why needed here:** The paper uses LN-Scaling as a baseline comparison. Understanding that LN-Scaling suppresses activation variance in deeper layers is necessary to contrast it with the growth approach, which the authors argue is more effective for reasoning.
  - **Quick check question:** Why might suppressing variance in later layers (LN-Scaling) fail to improve reasoning compared to adding new active layers (Growth)?

- **Concept: Looped / Recurrent Transformers**
  - **Why needed here:** The authors frame MIDAS/LIDAS as a "relaxed version" of Looped Transformers. Understanding that tied weights enforce strict recurrence helps explain why the *untyed* but structurally repeated blocks in this paper represent an "emergent" form of iteration.
  - **Quick check question:** Does LIDAS enforce weight tying between duplicated layers?

## Architecture Onboarding

- **Component map:** Initial model (L layers) -> Growth operator G -> Insert duplicates -> Continue training
- **Critical path:**
  1. Train initial model with depth $L_0$ for steps $T_0$
  2. Identify middle block (MIDAS) or middle layer window (LIDAS)
  3. Insert duplicates into the model definition
  4. Resize optimizer state to match new parameter count (copying moments for duplicated weights)
  5. Continue training for steps $T_1$
- **Design tradeoffs:**
  - **MIDAS vs. LIDAS:** MIDAS is simpler to implement if blocks are distinct objects. LIDAS requires handling layer indices more carefully but yields higher weight symmetry and slightly better reasoning performance
  - **Depth vs. Efficiency:** The paper claims a **1.29x speedup**, but this requires dynamic depth scheduling (PROPORTIONAL-1 schedule) rather than fixed training duration
- **Failure signatures:**
  - **The "Curse of Depth":** Early exit performance saturates at layer $\approx L/2$. If your grown model stops improving accuracy after the first half, the growth failed to propagate utility
  - **Asymmetric Degradation:** If swapping Block A for Block B causes a crash but B for A does not, the model has learned sequential dependencies rather than permutable blocks
- **First 3 experiments:**
  1. **Implement the "Depth Score":** Calculate the mean influence of each layer on future tokens. Verify your baseline suffers from the Curse of Depth (score $\ll L/2$) before implementing growth
  2. **LIDAS Ablation on Small Scale:** Train a 4-layer model to 8 layers. Visualize the cosine similarity of attention outputs. Check if LIDAS maintains higher similarity than MIDAS at block boundaries
  3. **Permutation Robustness:** Train a grown model, then swap two middle blocks at inference time. Compare the performance drop to a standard baseline. A successful implementation should show significantly less degradation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Causality between symmetric initialization and reasoning performance remains unproven - correlation doesn't establish direct causation
- Computational efficiency claims (1.29x speedup) depend on specific PROPORTIONAL-1 schedule that may not generalize
- Limited ablation studies on alternative growth strategies to isolate essential vs. incidental aspects of the growth process

## Confidence

- **High Confidence:** The core observation that gradual depth growth mitigates the Curse of Depth (later layers maintain functional utility) is well-supported by depth score metrics and ablation studies showing grown models degrade more when layers are removed
- **Medium Confidence:** The claim that LIDAS outperforms MIDAS due to better attention alignment and symmetric initialization is supported by cosine similarity metrics, but the mechanistic link to reasoning performance could be more rigorously established
- **Medium Confidence:** The formation of permutable computational blocks is demonstrated through block-swapping experiments, but the interpretation that this represents iterative reasoning rather than simple redundancy remains somewhat speculative

## Next Checks

1. **Causality Test for LIDAS Advantage:** Train a version of MIDAS with post-hoc symmetry correction (adjusting weights to match LIDAS symmetry) and compare reasoning performance to determine if the symmetric structure itself drives the gains or if it's a correlated factor

2. **Alternative Growth Strategy Ablation:** Implement and compare growth strategies that duplicate from different positions (e.g., bottom-up growth, random layer duplication) to isolate whether middle-block duplication specifically is necessary for the observed benefits

3. **Long-Range Dependency Analysis:** Analyze attention patterns in early versus late layers of grown versus baseline models on tasks requiring long-range reasoning to determine if depth growth specifically improves the model's ability to maintain and refine complex computational states over depth