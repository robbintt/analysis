---
ver: rpa2
title: Efficient Hyperparameter Tuning via Trajectory Invariance Principle
arxiv_id: '2509.25049'
source_url: https://arxiv.org/abs/2509.25049
tags:
- scheduler
- figure
- size
- loss
- invariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a phenomenon called trajectory invariance,
  where loss curves, gradient noise, and gradient norm exhibit closely overlapping
  behavior with respect to a quantity combining learning rate and weight decay. The
  phenomenon shifts during training: early on, invariance is with respect to learning
  rate; later, it shifts to effective learning rate (the product of learning rate
  and weight decay).'
---

# Efficient Hyperparameter Tuning via Trajectory Invariance Principle

## Quick Facts
- **arXiv ID**: 2509.25049
- **Source URL**: https://arxiv.org/abs/2509.25049
- **Authors**: Bingrui Li; Jiaxin Wen; Zhanpeng Zhou; Jun Zhu; Jianfei Chen
- **Reference count**: 40
- **Primary result**: Training loss curves, gradient noise, and gradient norm exhibit trajectory invariance with respect to effective learning rate (LR × WD), enabling efficient hyperparameter tuning by reducing 2D search space to 1D

## Executive Summary
This paper identifies a fundamental phenomenon in neural network training called trajectory invariance, where training dynamics depend on a single combined quantity rather than individual hyperparameters. Specifically, early in training, loss curves with the same learning rate but different weight decays overlap, while later in training, curves with the same effective learning rate (LR × WD) converge. This shift from LR-invariance to ELR-invariance reduces the two-dimensional hyperparameter search space (learning rate and weight decay) to one dimension. The authors also refine scaling laws for optimal hyperparameter values and demonstrate that batch size schedulers can recover ELR invariance in large-batch training, significantly improving performance.

## Method Summary
The study uses a 164M parameter GPT-style model with SwiGLU MLP, RoPE, RMSNorm, and untied embeddings. Training employs AdamW optimizer with grid search over learning rates {2^-12, 2^-11, 2^-10, 2^-9, 2^-8} and weight decays {0.025, 0.05, 0.1, 0.2, 0.4} on the Pile dataset with GPT-2 tokenizer. Two batch sizes (512 and 8192) are tested with constant and batch size schedulers. The analysis tracks validation loss, gradient noise, and gradient norm across 100B tokens, comparing curves with fixed learning rates versus fixed effective learning rates to identify the trajectory invariance transition.

## Key Results
- Loss curves, gradient noise, and gradient norm exhibit closely overlapping behavior with respect to either learning rate (early training) or effective learning rate (late training)
- The transition from LR-invariance to ELR-invariance depends primarily on training iteration count rather than total tokens processed
- Optimal learning rate remains nearly constant across data sizes while optimal weight decay decreases sublinearly
- Batch size schedulers can recover ELR invariance in large-batch training, improving performance significantly
- Existing batch size scaling laws (square-root rule) do not hold in the large-batch regime under gradient noise scale criterion

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Invariance Transition from LR to ELR
Training exhibits a two-stage process where the governing invariant quantity shifts from learning rate (η) to effective learning rate (η × λ). Early training dynamics are dominated by the raw learning rate, causing curves with identical LR but different weight decays to overlap. As iterations accumulate, weight decay's cumulative effect on parameter norm stabilizes (∥w∥ ∝ √η/λ), making the product η × λ the governing quantity for loss trajectory.

### Mechanism 2: Iteration Count Governs ELR Invariance Emergence
The transition from LR to ELR invariance depends primarily on the number of training iterations, not total tokens processed. With fixed token budget, large batch sizes reduce iteration count proportionally. The weight decay effect accumulates per iteration (not per token), so fewer iterations mean insufficient cumulative regularization to reach parameter norm equilibrium where ELR dominates.

### Mechanism 3: Batch Size Scheduler Recovers ELR Invariance
Starting with small batch size then switching to large batch recovers ELR invariance while enabling large-batch efficiency gains. Small-batch phase provides sufficient iterations for the LR→ELR transition. The subsequent large-batch phase inherits the established parameter norm equilibrium, maintaining trajectory invariance while accelerating throughput.

## Foundational Learning

- **Concept: Effective Learning Rate (ELR = η × λ)**
  - Why needed here: Central to understanding how LR and WD interact; the key invariant quantity in late-stage training that enables dimensionality reduction in hyperparameter search
  - Quick check question: Given LR=2^-10 and WD=0.1, what is the ELR? If LR doubles to 2^-9, what WD maintains the same ELR?

- **Concept: Preconditioned Gradient Noise Scale (B_precond = Tr(P^-1 Σ) / ∥P^-1/2 g∥²)**
  - Why needed here: Determines whether a batch size is "small" or "large" in the GNS sense; critical for predicting whether ELR invariance will emerge and whether batch size schedulers are needed
  - Quick check question: If B_precond = 4000 throughout training and your batch size B = 512, is this a small or large batch regime? What if B = 8192?

- **Concept: Trajectory Invariance**
  - Why needed here: The foundational phenomenon enabling efficient hyperparameter tuning; understanding it allows reducing 2D search (LR, WD) to 1D (ELR direction)
  - Quick check question: Two runs have (η=2^-10, λ=0.4) and (η=2^-8, λ=0.025). Will their loss curves overlap in late training? Why or why not?

## Architecture Onboarding

- **Component map**: Hyperparameter Space → Learning Rate (η) and Weight Decay (λ) combine into Effective Learning Rate (γ = η × λ). Batch Size (B) determines Gradient Noise Scale (B_precond), which classifies small vs large batch regimes.

- **Critical path**: (1) Classify batch regime via GNS → (2) Identify training stage (early/late) → (3) Determine invariant quantity (LR or ELR) → (4) Tune along salient direction only.

- **Design tradeoffs**:
  - Small batch (B < B_precond): ELR invariance emerges naturally; tune along ELR direction by fixing LR and tuning WD. Tradeoff: slower throughput.
  - Large batch (B > B_precond): Stuck in LR invariance; LR dominates, WD has minor effect. Tradeoff: faster throughput but may underperform small batch without scheduler.
  - Batch scheduler: Combines benefits—small batch for invariance establishment, large batch for efficiency. Tradeoff: adds complexity; switch timing matters.

- **Failure signatures**:
  - Large-batch training underperforming small-batch by significant margin despite same token budget → likely insufficient iterations for ELR invariance; implement batch scheduler.
  - Loss curves with identical ELR diverging significantly → check if training iterations are sufficient; may still be in LR-invariance stage.
  - Applying square-root LR-BS scaling rule causing worse performance → this rule doesn't hold in large-batch regime under GNS criterion (Figure 14).

- **First 3 experiments**:
  1. **Validate trajectory invariance on your model**: Run a 2×2 grid of (LR, WD) pairs keeping ELR constant (e.g., (η=1e-3, λ=0.1) and (η=2e-3, λ=0.05)). Plot loss curves to verify overlap in late training (>30 TPP).
  2. **Measure your GNS**: Track B_precond during a pilot run. If B_precond > your planned batch size throughout, you're in small-batch regime; tune along ELR. If B_precond < B, implement batch scheduler.
  3. **Test batch scheduler timing**: Starting with B=512, switch to B=8192 at 10%, 20%, and 30% of token budget. Compare final losses to identify optimal switch point for your setting.

## Open Questions the Paper Calls Out
None

## Limitations
- The phenomenon was observed in a 164M parameter GPT-style model; universality across architectures and scales remains unproven
- The precise factors controlling the LR→ELR transition timing are not fully characterized
- The batch size scheduler adds complexity and the optimal switch timing requires empirical tuning

## Confidence

- **High Confidence**: The observation of trajectory invariance itself (loss curves with constant ELR overlapping) is well-supported by experimental evidence across multiple runs and batch sizes.

- **Medium Confidence**: The claim that iteration count governs ELR invariance transition is plausible given batch scheduler results, but a more direct ablation study would strengthen this claim.

- **Low Confidence**: Generalization of trajectory invariance to vastly different architectures and scales (e.g., 100B+ parameter LLMs) is uncertain, as is the practical benefit of batch size schedulers across all model families.

## Next Checks

1. **Architectural Transfer**: Test trajectory invariance on a vision transformer (ViT) or smaller BERT variant trained on standard dataset. Verify if loss curves with constant ELR still overlap in late training and if LR→ELR transition timing is similar.

2. **Scaling Law Validation**: Conduct controlled experiment training 164M model on datasets of varying sizes (10%, 50%, 100%, 200% of Pile). Track optimal LR and WD at each scale. Plot optimal WD vs data size on log-log scale to verify sublinear decay trend and confirm optimal LR remains constant.

3. **Transition Timing Ablation**: Fix total token budget at 50B. Train one model with B=512 for all 50B tokens. Train another model with B=512 for 10B tokens, then switch to B=8192 for remaining 40B tokens. Compare final validation loss and state of trajectory invariance (LR vs ELR) at end of training to isolate effect of iteration count.