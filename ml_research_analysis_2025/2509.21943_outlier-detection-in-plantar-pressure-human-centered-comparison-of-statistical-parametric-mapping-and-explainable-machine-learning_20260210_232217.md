---
ver: rpa2
title: 'Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical
  Parametric Mapping and Explainable Machine Learning'
arxiv_id: '2509.21943'
source_url: https://arxiv.org/abs/2509.21943
tags:
- pressure
- data
- plantar
- were
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares Statistical Parametric Mapping (SPM) and explainable
  machine learning for detecting outliers in plantar pressure data, addressing the
  need for interpretable quality control in heterogeneous datasets. An explainable
  CNN with SHAP was trained on expert-annotated data (798 valid, 2000 synthetic outliers),
  while SPM relied on registration and cluster-based permutation testing.
---

# Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning

## Quick Facts
- arXiv ID: 2509.21943
- Source URL: https://arxiv.org/abs/2509.21943
- Reference count: 40
- CNN-based classification with SHAP explanations outperformed SPM for plantar pressure outlier detection (MCC: 0.96 vs. 0.78; F1: 0.99 vs. 0.93), with both approaches rated as clear and trustworthy by human experts.

## Executive Summary
This study compares Statistical Parametric Mapping (SPM) and an explainable convolutional neural network (CNN) with SHAP for detecting outliers in plantar pressure data. The CNN, trained on expert-annotated samples with synthetic outlier augmentation, achieved significantly higher detection accuracy than SPM (MCC: 0.96 vs. 0.78; F1: 0.99 vs. 0.93), particularly excelling at handling pathological yet valid samples that SPM misclassified as outliers. Human experts rated both approaches as clear, useful, and trustworthy, though SPM was perceived as simpler. The findings demonstrate that interpretable ML methods can provide superior performance while maintaining explainability, suggesting complementary roles for statistical and ML approaches in clinical quality control.

## Method Summary
The study evaluated two outlier detection approaches on 798 valid and 2000 synthetic outlier plantar pressure samples (64×64 grayscale, [0,1] normalized). The CNN architecture used three convolutional blocks (32-64-128 filters, 3×3 kernels, batch norm, ReLU, 2×2 max-pooling, Dropout2D) with a foot-side categorical embedding, trained with weighted cross-entropy and early stopping. SPM employed affine registration to a normative template followed by pixel-wise permutation testing with cluster-based FWE correction. Both methods used nested stratified 5-fold cross-validation grouped by subject. Deep SHAP provided pixel-level explanations for the CNN, while SPM generated cluster-based statistical maps. Human experts evaluated explanation quality using semantic differential surveys.

## Key Results
- CNN achieved MCC of 0.96 and F1 of 0.99 versus SPM's MCC of 0.78 and F1 of 0.93
- CNN outperformed SPM specifically on detecting pathological yet valid samples
- Both SPM and SHAP explanations were rated as clear, useful, and trustworthy by experts
- SPM was perceived as simpler than SHAP while maintaining explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN-based classification with categorical embedding achieves higher outlier detection accuracy than registration-dependent statistical methods when data contains pathological but valid samples.
- Mechanism: The CNN learns spatial features directly from raw pressure maps, implicitly capturing alignment invariances that would otherwise require explicit preprocessing. The side-label embedding allows the model to condition its predictions on foot laterality without hard-coding anatomical assumptions.
- Core assumption: Sufficient labeled examples exist for each outlier category to learn discriminative features.
- Evidence anchors:
  - [abstract] "The CNN outperformed SPM (MCC: 0.96 vs. 0.78; F1: 0.99 vs. 0.93), particularly in handling pathological yet valid samples."
  - [section 3.1] "valid samples incorrectly identified as outliers (false positives) were predominantly feet exhibiting pathological characteristics (e.g., hallux valgus, hammer toe, claw toe, flatfoot)."
  - [corpus] No directly comparable CNN-vs-SPM plantar pressure studies found; neighboring papers focus on unsupervised outlier detection methods without registration comparison.
- Break condition: When labeled outlier data is scarce or synthetic anomalies poorly represent real acquisition errors, the supervised advantage diminishes.

### Mechanism 2
- Claim: SPM's pixel-wise comparison to a normative template is sensitive to residual misalignment, causing false positives on pathological but valid samples.
- Mechanism: SPM registers each sample to a prototypical reference via affine transformation, then performs pixel-wise permutation testing against a normative cohort. Significant cluster deviations are flagged as outliers. Pathological feet (e.g., hallux valgus) cannot align perfectly to a normative template, creating spurious deviations.
- Core assumption: The affine registration correctly maps corresponding anatomical regions across all subjects.
- Evidence anchors:
  - [section 2.4.1] "A critical prerequisite for the SPM approach is that all images be precisely aligned to ensure that each pixel corresponds to the same anatomical foot region across all subjects."
  - [section 4] "For example, optimally registering a foot with Hallux Valgus to a normative template is nearly impossible, as either the hallux protrudes beyond the reference region or pressure voids appear."
  - [corpus] Corpus papers discuss outlier detection in longitudinal and high-dimensional data but do not address registration-dependent spatial statistics.
- Break condition: When deformable registration or region-based aggregation replaces pixel-wise comparison, alignment sensitivity decreases.

### Mechanism 3
- Claim: SHAP explanations and SPM cluster outputs both achieve high perceived trustworthiness, but SHAP provides finer-grained feature attribution while SPM is perceived as simpler.
- Mechanism: Deep SHAP approximates Shapley values by backpropagating contributions through the network, generating pixel-level attributions for each prediction. SPM highlights statistically significant clusters. Experts rated both positively on clarity, usefulness, and trustworthiness, but SPM was descriptively simpler.
- Core assumption: Human evaluators can accurately assess explanation quality despite lack of ground-truth for "correct" explanations.
- Evidence anchors:
  - [abstract] "Experts perceived both SPM and SHAP explanations as clear, useful, and trustworthy, though SPM was assessed less complex."
  - [section 3.2] "no statistically significant differences between the two approaches for any attribute in the semantic differential were observed (p > 0.05)."
  - [corpus] DCFO paper discusses counterfactual explanations for outliers, suggesting complementary XAI approaches exist but were not compared here.
- Break condition: When models make errors, explanation faithfulness to human reasoning may diverge; this study evaluated only correctly classified samples.

## Foundational Learning

- Concept: **Cluster-based permutation testing for multiple comparison correction**
  - Why needed here: SPM performs thousands of simultaneous pixel-wise tests; without correction, false positive clusters proliferate. The permutation procedure builds a null distribution of maximum cluster sizes to control family-wise error rate.
  - Quick check question: Given 1000 permutations and α_FWE=0.05, what cluster size threshold do you use?

- Concept: **Shapley values and Deep SHAP approximation**
  - Why needed here: SHAP quantifies each pixel's contribution to a prediction. Deep SHAP approximates exact Shapley values by combining backpropagation with a background dataset, making computation tractable for CNNs.
  - Quick check question: If a pixel has a large positive SHAP value, does it push the prediction toward or away from the predicted class?

- Concept: **Nested cross-validation for unbiased hyperparameter tuning**
  - Why needed here: The outer loop estimates generalization; the inner loop selects hyperparameters. Using the same data for both inflates performance estimates through data leakage.
  - Quick check question: Why must all data from a single subject stay in one partition fold?

## Architecture Onboarding

- Component map:
  Data pipeline -> 64×64 zero-padded grid -> intensity normalization [0,1] -> expert annotation + synthetic augmentation -> SPM branch (affine registration -> pixel-wise permutation testing -> cluster formation) OR ML branch (CNN + categorical embedding -> fully connected classifier head) -> Explanation layer (Deep SHAP or SPM cluster output) -> Human evaluation

- Critical path:
  1. Data harmonization across sensor types (rescaling + padding) directly affects both SPM alignment quality and CNN input distribution
  2. Synthetic outlier generation determines outlier class balance; poor fidelity reduces model robustness
  3. SHAP background dataset selection influences explanation stability

- Design tradeoffs:
  - **SPM vs. ML primary decision**: ML for accuracy when labeled data available; SPM for interpretability without training data
  - **Synthetic vs. real outliers**: Synthetic enables class balance but risks distribution shift; experts validated fidelity but residual bias acknowledged
  - **Binary vs. multi-class output**: Multi-class enables actionable feedback (e.g., "wrong side label") but sacrifices simplicity

- Failure signatures:
  - High false positives on pathological samples -> likely SPM misalignment; consider deformable registration or region-based aggregation
  - Low accuracy on "General Acquisition Error" class -> hidden stratification within heterogeneous class; subdivide into finer categories
  - High SHAP explanation variance -> reduce background dataset noise or increase background sample count

- First 3 experiments:
  1. Ablate the categorical side embedding to measure its contribution to classification accuracy and false negative reduction on "Incorrect Side Annotation" outliers.
  2. Replace affine registration with deformable registration in SPM and re-evaluate false positive rate on pathological valid samples.
  3. Compare SHAP against gradient-based attribution methods (e.g., Integrated Gradients) on a held-out set of misclassified samples to assess explanation robustness under model error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does utilizing deformable image registration improve the capacity of Statistical Parametric Mapping (SPM) to handle pathological foot shapes without misclassifying them as outliers?
- Basis in paper: [explicit] The authors explicitly suggest that future work "could investigate advanced biomechanical alignment techniques, such as deformable image registration" to accommodate anatomical variability better than the affine methods used.
- Why unresolved: The current SPM implementation misclassified valid pathological samples (e.g., Hallux Valgus) as outliers because they could not be adequately aligned to the normative template using affine transformations.
- What evidence would resolve it: A study comparing false positive rates of affine-based SPM versus deformable-based SPM specifically on a dataset of pathological yet valid plantar pressure samples.

### Open Question 2
- Question: How do other Explainable AI (XAI) methods compare to SHAP in terms of perceived usefulness and simplicity for domain experts in biomechanics?
- Basis in paper: [explicit] The paper states, "Future work could systematically compare multiple XAI methods, assessing both their technical interpretability and perceived usefulness from the perspective of human experts."
- Why unresolved: This study limited its evaluation to SHAP; while trustworthy, it was perceived as "complex" compared to the simpler SPM outputs, leaving the utility of other explanation formats unknown.
- What evidence would resolve it: A comparative human-centered evaluation (e.g., semantic differential survey) measuring expert ratings of clarity and utility for SHAP against methods like LIME or Grad-CAM on identical predictions.

### Open Question 3
- Question: Does subdividing the heterogeneous "General Acquisition Error" class improve the classification accuracy of machine learning models?
- Basis in paper: [inferred] The authors note that the "General Acquisition Error" class suffered from "hidden stratification" (containing diverse artifacts like partial contact and footwear) and had the lowest accuracy, suggesting that "subdividing it into more homogeneous subclasses" may be necessary.
- Why unresolved: The current model treated all acquisition errors as a single class, which likely confused the learning of distinct visual features required for accurate subclassification.
- What evidence would resolve it: Retraining the CNN with refined labels (e.g., separating "partial contact" from "sensor noise") to determine if the F1-score for acquisition errors improves significantly.

## Limitations
- Reliance on synthetic outliers to balance training data may not fully capture real-world acquisition error complexity
- Binary classification setup oversimplifies the multi-class problem and may obscure important distinctions between outlier subtypes
- Human evaluation of explanation quality was limited to correctly classified samples and used subjective semantic differential scales

## Confidence
- Comparison results (MCC 0.96 vs 0.78, F1 0.99 vs 0.93): High
- Explanation quality assessment: Medium
- Generalization to clinical practice: Medium

## Next Checks
1. Evaluate model robustness by testing on exclusively real (non-synthetic) outliers collected from clinical practice
2. Perform ablation study removing the foot-side categorical embedding to quantify its contribution to accuracy and false negative reduction
3. Compare SHAP explanations against alternative XAI methods (e.g., Integrated Gradients) on misclassified samples to assess explanation faithfulness under model error