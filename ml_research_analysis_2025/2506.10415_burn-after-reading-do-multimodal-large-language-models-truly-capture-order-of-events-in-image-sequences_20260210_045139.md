---
ver: rpa2
title: 'Burn After Reading: Do Multimodal Large Language Models Truly Capture Order
  of Events in Image Sequences?'
arxiv_id: '2506.10415'
source_url: https://arxiv.org/abs/2506.10415
tags:
- image
- images
- sentence
- sequence
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the TempVS benchmark to evaluate multimodal
  large language models' (MLLMs) ability to understand and reason about temporal sequences
  in image sequences. TempVS contains three main tests (event relation inference,
  sentence ordering, and image ordering) along with grounding tests, using 2,085 image
  sequences from cartoon animations, movies, and daily-life albums.
---

# Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?

## Quick Facts
- arXiv ID: 2506.10415
- Source URL: https://arxiv.org/abs/2506.10415
- Reference count: 17
- This paper introduces the TempVS benchmark to evaluate multimodal large language models' (MLLMs) ability to understand and reason about temporal sequences in image sequences.

## Executive Summary
This paper introduces the TempVS benchmark to evaluate multimodal large language models' (MLLMs) ability to understand and reason about temporal sequences in image sequences. TempVS contains three main tests (event relation inference, sentence ordering, and image ordering) along with grounding tests, using 2,085 image sequences from cartoon animations, movies, and daily-life albums. The benchmark requires models to integrate both visual and linguistic modalities to understand temporal order. Evaluation of 38 state-of-the-art MLLMs (including models from 0.5B to 78B parameters) shows that current models struggle with TempVS, with a substantial performance gap compared to human capabilities. For example, the best-performing model InternVL2.5-78B-MPO achieved only 60.4% accuracy on two-event relation inference, while humans achieved 82.5%. The benchmark reveals that models often fail at temporal reasoning even when they can accurately ground individual events to images. The authors provide detailed analysis suggesting promising directions for future research in architectural design, training objectives, and post-training methods to enhance temporal reasoning capabilities.

## Method Summary
The TempVS benchmark evaluates MLLMs on temporal reasoning across image sequences using 2,085 sequences (9,803 images) from four sources: FlintstonesSV, PororoSV, VIST, and VWP. The benchmark includes three main tests (event relation inference, sentence ordering, and image ordering) plus grounding tests. Researchers evaluated 38 state-of-the-art MLLMs (0.5B to 78B parameters) using zero-shot evaluation with temperature=0, horizontally combining images with white band separators. Human evaluation was conducted via Prolific with 36 annotators evaluating 280 sequences. The benchmark uses accuracy on multiple-choice questions as the primary metric, with specific metrics including GT (grounding test), GTstrict (all events correctly grounded), MT (main test), and MT|GTstrict (main test accuracy conditioned on passing grounding).

## Key Results
- Current MLLMs struggle with temporal reasoning even when accurately grounding individual events to images
- The best-performing model (InternVL2.5-78B-MPO) achieved only 60.4% accuracy on two-event relation inference, while humans achieved 82.5%
- Models show a substantial performance gap on main temporal reasoning tasks compared to grounding tasks, revealing dissociation between these capabilities
- Chain-of-thought prompting significantly improves ordering tasks (+14-20% on sentence ordering, +9-11% on image ordering) but provides minimal benefit for event relation inference (+1-2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal reasoning requires capabilities beyond simple visual grounding.
- Mechanism: Models may accurately match individual events to images (grounding) but fail to infer or manipulate their temporal relations across a sequence, indicating that grounding and relational reasoning are partially dissociable skills.
- Core assumption: Accurate temporal reasoning depends on integrating grounded events into an ordered mental model, not just recognizing events in isolation.
- Evidence anchors:
  - [abstract] The benchmark reveals that models often fail at temporal reasoning even when they can accurately ground individual events to images.
  - [Table 3] Many models (e.g., InternVL2.5-78B-MPO) show high grounding test (GT) scores but much lower main test (MT) scores, with MT|GTs accuracy only marginally higher than MT.
  - [corpus] Weak/no direct corpus evidence for this specific grounding-reasoning dissociation.
- Break condition: If models improve grounding scores without corresponding gains in MT, the dissociation holds; if grounding improvements consistently yield MT gains, the mechanism may be less distinct.

### Mechanism 2
- Claim: Performance is influenced by the alignment between linguistic surface order and visual event order (an iconicity effect).
- Mechanism: Models perform better when the textual order of events mirrors their visual sequence (e.g., "Ei before Ej" vs. "Ej after Ei"), suggesting reliance on surface-level text patterns rather than deep multimodal integration.
- Core assumption: Models exploit statistical regularities where text order correlates with visual order, and struggle when these are inverted.
- Evidence anchors:
  - [Section 4.3] "We observe the better performance of models on explicitly marked temporal relations involving before (resp. after) and on then (resp. earlier)... with before and then, the order of events in text mirrors their order in the image sequence."
  - [Table 5] Shows higher accuracy for positive vs. negative examples and for certain connective types, consistent with an iconicity bias.
  - [corpus] No direct corpus evidence; related work on discourse processing is cited but not empirically tested in corpus.
- Break condition: If performance on inverted-order stimuli (e.g., "Ej after Ei") equals or surpasses canonical order, the iconicity effect is not driving results.

### Mechanism 3
- Claim: Chain-of-thought prompting differentially aids ordering tasks over relation inference.
- Mechanism: CoT elicits step-by-step reasoning beneficial for sequencing (MT2/MT3) but provides less utility for binary relation judgments (MT1), which may require different representational alignment.
- Core assumption: Ordering tasks benefit from explicit intermediate reasoning steps, while relation inference may depend more on direct multimodal feature alignment.
- Evidence anchors:
  - [Table 6] CoT yields +14-20% gains on MT2 (sentence ordering) and +9-11% on MT3 (image ordering), but only +1-2% on MT1 (event relation).
  - [Section 4.3] "Simple CoT does not help event relation inference."
  - [corpus] No direct corpus evidence for this specific differential effect.
- Break condition: If CoT consistently fails to improve any task, or if it improves MT1 as much as MT2/MT3, the mechanism is not task-specific.

## Foundational Learning

- Concept: **Temporal connectives and discourse structure**
  - Why needed here: Understanding "before," "after," "then," etc., and how they signal event order in text is essential for mapping linguistic descriptions to visual sequences.
  - Quick check question: Can you explain why "A before B" and "B after A" describe the same temporal relation but may be processed differently?

- Concept: **Visual grounding in multimodal systems**
  - Why needed here: The ability to match a textual description to the correct image in a sequence is a prerequisite for temporal reasoning; weaknesses here cascade to main tasks.
  - Quick check question: Given an image sequence and an event description, can you identify which image the description refers to? What visual features would you use?

- Concept: **Cross-modal integration and mental model construction**
  - Why needed here: Temporal reasoning requires combining information from both modalities into a coherent, ordered representation; integration failures lead to reasoning failures even with good grounding.
  - Quick check question: How would you build an internal timeline that includes both the visual events from an image sequence and the temporal relations stated in a text?

## Architecture Onboarding

- Component map: Visual encoder (e.g., SigLIP, InternViT) → connector/projector → LLM backbone (e.g., Qwen, Llama). Temporal reasoning depends on how event representations are pooled and ordered across these stages.

- Critical path: Visual encoder produces per-image embeddings; connector aligns them with LLM token space; LLM must integrate sequence position, temporal connectives, and event content. Weakness likely arises in the integration/reasoning stage, not in grounding per se.

- Design tradeoffs: Larger LLM backbones improve long-range reasoning but increase compute. Post-training methods like MPO or DPO can enhance temporal reasoning beyond scale alone (see InternVL2.5-MPO vs. InternVL2.5). Input format (concatenated vs. sequential images) shows minimal difference per preliminary experiments (Table 12).

- Failure signatures: High grounding accuracy (GT) but low main test accuracy (MT); strong performance on canonical-order linguistic forms but poor on inverted forms; minimal MT1 improvement with CoT despite ordering gains.

- First 3 experiments:
  1. **Quantify the grounding-reasoning gap**: For a set of models, plot GT vs. MT accuracy across all tasks to confirm the dissociation.
  2. **Test the iconicity effect**: Evaluate performance on matched vs. mismatched text-image order pairs for the same events, controlling for linguistic complexity.
  3. **Probe CoT benefits**: Apply CoT prompting to MT1 variants with different connective types to see if any subconditions benefit, and analyze reasoning traces for failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training objectives can bridge the gap between accurate event grounding and successful temporal relation reasoning in MLLMs?
- Basis in paper: [explicit] The paper states "grounding textual descriptions and reasoning about their temporal relations require different capabilities" and suggests "promising directions for future research in architectural design, training objectives, and/or post-training methods to enhance temporal reasoning."
- Why unresolved: Models like InternVL2.5-78B-MPO pass grounding tests but still underperform on temporal reasoning, and the paper does not propose specific interventions to address this decoupling.
- What evidence would resolve it: Ablation studies comparing different architectures or training regimes that specifically target the MT|GTstrict gap would clarify whether joint grounding-reasoning training objectives are necessary.

### Open Question 2
- Question: How can MLLMs be improved to handle implicit temporal relations where surface text order does not mirror visual event order (the iconicity effect)?
- Basis in paper: [explicit] The analysis section notes: "It also points to an important avenue for future research in fine-grained multimodal benchmarking, namely, in cases where surface characteristics in two modalities are not perfectly aligned."
- Why unresolved: Models perform significantly better on "before/then" constructions than "after/earlier" ones where text and visual order mismatch, but no solution is proposed.
- What evidence would resolve it: Targeted training or evaluation on counter-iconic temporal expressions, with measurements of whether the performance gap closes after intervention.

### Open Question 3
- Question: What methods beyond standard Chain-of-Thought prompting can effectively enhance MLLM performance on event relation inference tasks?
- Basis in paper: [explicit] The CoT section states: "However, simple CoT does not help event relation inference. We leave the investigation of methods to enhance models' understanding of this complex task and improve its temporal reasoning capabilities for future work."
- Why unresolved: CoT yielded substantial gains for ordering tasks but minimal improvement for MT1 event relation inference, leaving this specific capability unresolved.
- What evidence would resolve it: Experiments with structured reasoning frameworks (e.g., explicit event extraction, temporal graph construction) on MT1, showing accuracy improvements over baseline and standard CoT.

## Limitations

- The benchmark relies on filtered datasets from existing sources, which may not fully represent real-world temporal reasoning diversity and could introduce bias toward simpler temporal relations
- The study evaluates only zero-shot performance, leaving open questions about how fine-tuning or few-shot prompting might affect results
- The analysis of model failures is primarily descriptive rather than diagnostic at the architectural level, making it difficult to pinpoint specific components responsible for temporal reasoning deficits

## Confidence

- **High Confidence**: The finding that MLLMs struggle with temporal reasoning even when grounding individual events accurately (Mechanism 1). This is consistently supported across multiple models and tasks, with clear quantitative evidence showing dissociation between GT and MT scores.

- **Medium Confidence**: The iconicity effect showing better performance when linguistic surface order matches visual event order (Mechanism 2). While the pattern is observable in the data, the effect size is moderate and could be influenced by other factors such as connective semantics or model training data biases.

- **Medium Confidence**: The differential benefit of chain-of-thought prompting for ordering versus relation inference tasks (Mechanism 3). The effect is statistically clear but may not generalize to all model architectures or prompt variants.

## Next Checks

1. **Cross-dataset validation**: Test the same MLLMs on temporal reasoning tasks from datasets not used in TempVS construction to assess whether observed limitations generalize beyond the benchmark's source domains.

2. **Ablation of the filtering pipeline**: Re-run evaluations with progressively relaxed filtering thresholds (e.g., allowing stative verbs or pronouns) to determine how the curation process affects model performance and whether it inadvertently removes cases where models might excel.

3. **Fine-tuning impact study**: Select a subset of high-capacity models (e.g., InternVL2.5-78B-MPO) and evaluate their performance after fine-tuning on a small, curated set of temporal reasoning examples to establish whether the observed limitations are fundamental or addressable through adaptation.