---
ver: rpa2
title: Policy Learning with Abstention
arxiv_id: '2510.19672'
source_url: https://arxiv.org/abs/2510.19672
tags:
- policy
- learning
- abstention
- algorithm
- xrem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for policy learning with abstention,
  allowing algorithms to defer treatment decisions when uncertainty is high, which
  is crucial in high-stakes domains like medicine. The core method involves a two-stage
  learner that first identifies near-optimal policies and then constructs an abstention
  rule based on their disagreements, with a small additive reward offered for abstaining.
---

# Policy Learning with Abstention

## Quick Facts
- arXiv ID: 2510.19672
- Source URL: https://arxiv.org/abs/2510.19672
- Authors: Ayush Sawarni; Jikai Jin; Justin Whitehouse; Vasilis Syrgkanis
- Reference count: 40
- Primary result: Achieves O(1/n) regret rates for policy learning with abstention using a two-stage learner with synthetic margin bonus

## Executive Summary
This paper introduces a framework for policy learning that allows algorithms to abstain from treatment decisions when uncertainty is high, which is crucial in high-stakes domains like medicine. The core method involves a two-stage learner that first identifies near-optimal policies and then constructs an abstention rule based on their disagreements, with a small additive reward offered for abstaining. The framework achieves fast O(1/n) regret rates and connects to safe policy improvement and distributional robustness, showing how abstention can ensure high-probability improvements over baselines and hedge against data shifts.

## Method Summary
The framework learns treatment policies π: X → {0,1,*} that can defer decisions in uncertain regions. It uses a two-stage algorithm: first finding the empirical welfare maximizer, then constructing a near-optimal policy set based on empirical values and disagreements among policies. An abstention bonus p is offered when deferring, creating a "synthetic margin" that enables fast O(1/n) regret rates without requiring the standard margin assumption. For observational data with unknown propensities, a doubly robust correction extends these guarantees, requiring nuisance estimates with L₄ error O(n^{-1/4}) for the fast rates to hold.

## Key Results
- Achieves O(1/n) regret rates matching existing benchmarks when abstaining is not incentivized
- Doubly robust correction extends guarantees to observational data with unknown propensities
- Connects to safe policy improvement, showing abstention can ensure high-probability improvements over baselines
- Demonstrates superior performance in both standard and safe policy improvement tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The abstention bonus p functions as a synthetic margin, enabling fast O(1/n) regret rates without requiring the standard margin assumption (i.e., that the CATE is bounded away from zero).
- Mechanism: When the learner abstains in regions where |τ₀(X)| < p, they automatically accumulate reward higher than any binary treatment policy. In regions where |τ₀(X)| ≥ p, the learner is more certain and unlikely to abstain. This creates an effective decision boundary without needing to estimate the CATE directly.
- Core assumption: Strict overlap (propensities bounded in [κ, 1−κ]) and finite VC dimension of policy class Π.
- Evidence anchors:
  - [abstract]: "The core method involves a two-stage learner that first identifies near-optimal policies and then constructs an abstention rule based on their disagreements, with a small additive reward offered for abstaining."
  - [section 3.1]: "The abstention bonus p can be viewed as a 'synthetic' margin — if the learner abstains when |τ₀(X)| < p, they automatically accumulate higher reward than any binary treatment assignment policy."
  - [corpus]: Related work on safe policy improvement (Deep SPI, SNPL) focuses on conservative updates, but does not explicitly use abstention bonuses as synthetic margins.
- Break condition: If p = 0, the algorithm reverts to O(1/√n) regret rates, matching standard EWM without abstention benefits.

### Mechanism 2
- Claim: Constructing abstention rules based on disagreement among near-optimal policies provides a data-driven uncertainty quantification that avoids explicit CATE estimation.
- Mechanism: Algorithm 1 first finds the empirical welfare maximizer ̂π, then defines a near-optimal set Π̂ of policies close in empirical value. It projects disagreements between any π ∈ Π̂ and ̂π into abstentions, creating Π̃. The final policy maximizes the abstaining objective on held-out data.
- Core assumption: The in-class optimal policy π* falls within the near-optimal set Π̂ (proven in Appendix A with appropriate radius).
- Evidence anchors:
  - [section 3.1, Algorithm 1]: "Select Near-optimal policies: Π̂ ← {π ∈ Π: Vₙ(̂π) − Vₙ(π) ≤ c_κ(α² + α√Eₙ|f̂π − f_π|)}."
  - [section 3.1]: "In Algorithm 1, we never actually need to estimate the CATE. This is of particular importance when the CATE may be a highly complicated function."
  - [corpus]: Related SPI methods (HCPI) use confidence intervals directly on policy values; this approach differs by using disagreement as the uncertainty signal.
- Break condition: If the selection radius is too tight, π* may not fall in Π̂; if too loose, the disagreement region becomes uninformative.

### Mechanism 3
- Claim: Doubly robust pseudo-outcomes enable extension to observational data with unknown propensities while preserving regret guarantees, provided nuisance errors satisfy product-rate conditions.
- Mechanism: Replace raw outcomes with DR pseudo-outcomes φ̂(x,d,y) = ĝ(d,x) + (d·D/̂p(x) + (1−d)(1−D)/(1−̂p(x)))(y − ĝ(d,x)). This de-biases the IPW estimator, with regret bound adding an Err_DR term proportional to √E[(̂p − p₀)² · Σ(ĝ − g₀)²].
- Core assumption: Nuisance estimates ĝ, ̂p are independent of the sample (achievable via cross-fitting); product error Err_DR ≤ c_δ/√n for second term to be negligible.
- Evidence anchors:
  - [section 3.2]: "We need Err_DR ≤ c_δ/√n with high probability, which will occur if max{∥̂p − p∥_{P_X,4}, ∥ĝ − g₀∥_{P_X,4}} ≲ n^{-1/4}."
  - [theorem 3.4]: "Reg_n^(p)(π̃) ≲ (d log n + log 1/δ)/(pnκ²) + Err_DR²/(pκ²)."
  - [corpus]: DR methods are standard in policy learning (Athey & Wager 2021); this paper extends them to the abstention setting.
- Break condition: If nuisance estimates are poor (e.g., L₄ error > n^{-1/4}), the DR correction term dominates and fast rates are not achievable.

## Foundational Learning

- Concept: **Inverse Propensity Weighting (IPW)**
  - Why needed here: Core technique for converting observational data into unbiased estimates of policy value when treatment assignment is confounded but propensities are known or estimable.
  - Quick check question: Given a treated unit with propensity p(x) = 0.3 and outcome Y = 0.8, what is its IPW contribution to the treated-arm value estimate? (Answer: Y/p(x) = 0.8/0.3 ≈ 2.67)

- Concept: **Doubly Robust Estimation**
  - Why needed here: Provides protection against nuisance misspecification — consistent if either propensity or outcome regression is correct, and enables faster rates when both are estimated well.
  - Quick check question: If the outcome regression g₀ is correctly specified but propensity ̂p is wrong, does a DR estimator remain consistent? (Answer: Yes, by the double robustness property)

- Concept: **VC Dimension and Rademacher Complexity**
  - Why needed here: Controls the complexity of the policy class, governing concentration and the α term in regret bounds.
  - Quick check question: What is the VC dimension of threshold policies on a single continuous feature? (Answer: 2 — can shatter any 2 points but not 3)

## Architecture Onboarding

- Component map:
Input Data → [Sample Split: D₁ | D₂]
                    ↓
D₁ → [EWM: ̂π = argmax Vₙ(π)] → [Near-Optimal Set Π̂] → [Disagreement Projection Π̃]
                    ↓
D₂ → [Abstaining EWM: π̃ = argmax Vₙ^(p)(π)] → Output π̃

(Observational case: Replace Vₙ with V_{DR,n}; requires nuisance estimates ĝ, ̂p)

- Critical path:
  1. **Step 4 (Near-Optimal Set Construction)**: The radius parameter must be set correctly — too tight excludes π*, too loose yields uninformative abstentions. Paper uses α = √((d log n + log 1/δ)/n) and scales by κ.
  2. **Step 5 (Disagreement Projection)**: This is the core abstention logic — comparing each π ∈ Π̂ to ̂π and abstaining on disagreements.
  3. **Step 6 (Abstaining EWM)**: Must be evaluated on held-out data D₂ to avoid overfitting.

- Design tradeoffs:
  - **Larger p (abstention bonus)** → More abstention, lower deployment risk, but potentially leaving value on the table if abstention regions are overly conservative.
  - **More splits (for DR)** → Better nuisance independence, but smaller effective sample size per split.
  - **Policy class complexity (d)** → More expressive policies can achieve higher optimal value, but slower rates (α ∝ √d).

- Failure signatures:
  - **High abstention rate (>50%) with low value gain**: Radius α too large or p too high — algorithm being overly conservative.
  - **Worse-than-baseline performance on held-out test**: Near-optimal set construction failed to include π*, or nuisance estimates are severely biased.
  - **Regret not improving with n**: Nuisance product error not decaying fast enough (check L₄ convergence of ĝ, ̂p).

- First 3 experiments:
  1. **Sanity check on synthetic data with known propensities**: Generate X ~ N(0,I), linear CATE τ₀(X) = X₁ + X₂ − 1, known propensity p(X) = 0.5. Verify O(1/n) regret scaling by running Algorithm 1 with p = 0.05 across n ∈ {500, 1000, 2000, 5000}. Plot log(Reg) vs. log(n); expect slope ≈ −1.
  2. **Ablation on abstention bonus p**: Fix n = 2000, vary p ∈ {0, 0.01, 0.05, 0.1, 0.2}. Report abstention rate, true value V(π̃), and regret vs. optimal binary policy. Confirm p = 0 recovers O(1/√n) behavior; p > 0 shows value gain at cost of abstention.
  3. **DR extension with misspecified nuisances**: Run Algorithm 4 with (a) correct propensity & outcome regression, (b) misspecified propensity only, (c) misspecified outcome regression only, (d) both misspecified. Verify DR property: (b) and (c) should still achieve reasonable regret; (d) should degrade.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework for policy learning with abstention be extended to continuous treatment domains (e.g., optimal dosage)?
- Basis in paper: [explicit] The authors state, "our framework only handles binary treatments. There are many settings (such as in medicine) where designing policies to optimize a continuous treatment (or dosage) may be more appropriate."
- Why unresolved: The current theoretical guarantees and the two-stage learner (Algorithm 1) are constructed specifically for discrete actions $\{0, 1, *\}$.
- What evidence would resolve it: A formal extension of the abstention definition to continuous action spaces with associated regret bounds and a practical optimization algorithm.

### Open Question 2
- Question: How can Algorithm 1 be implemented efficiently for complex policy classes like decision trees?
- Basis in paper: [explicit] The authors identify that "implementing Algorithm 1 efficiently for popular policy classes such as decision trees is also an interesting open direction."
- Why unresolved: While the statistical complexity (VC dimension) is bounded, the computational complexity of constructing the abstention set based on policy disagreements is not addressed for specific non-linear classes.
- What evidence would resolve it: A polynomial-time optimization procedure for constructing the near-optimal set and abstention rule specifically for tree-based policy classes.

### Open Question 3
- Question: How does the performance of this framework change if evaluated using loss-based metrics rather than regret?
- Basis in paper: [explicit] The paper notes, "we evaluate the performance of algorithms through regret, but one could also consider loss-based modes of policy evaluation as well."
- Why unresolved: The fast $O(1/n)$ rates rely on regret properties relative to a best-in-class policy; loss-based evaluation might exhibit different statistical behavior or convergence rates.
- What evidence would resolve it: A theoretical analysis of the algorithm's convergence properties under a loss-based objective function (e.g., excess risk relative to the Bayes policy).

## Limitations

- The framework only handles binary treatments, not continuous treatments like optimal dosage
- Efficient implementation for complex policy classes like decision trees remains an open challenge
- Theoretical guarantees rely on strict overlap assumptions and bounded VC dimension, which may be restrictive in practice

## Confidence

- Mechanism 1 (synthetic margin): **High** - the theory is sound and the mechanism is clearly articulated with supporting proofs.
- Mechanism 2 (disagreement-based abstention): **Medium** - theoretically justified but sensitive to the choice of near-optimal set radius.
- Mechanism 3 (doubly robust extension): **Medium** - standard technique extended appropriately, but practical performance depends on nuisance estimation quality.

## Next Checks

1. Test Algorithm 1 on synthetic data with highly variable CATE (τ₀(X) ∈ [-10, 10]) to assess robustness to scale; verify that synthetic margin p = 0.05 remains effective.
2. Implement cross-fitting for the DR extension and measure the actual product error Err_DR across different sample sizes to confirm the n^{-1/2} decay.
3. Compare the abstention rule's uncertainty quantification against alternative methods (e.g., conformal prediction) on real-world datasets where ground-truth optimal policies are unknown.