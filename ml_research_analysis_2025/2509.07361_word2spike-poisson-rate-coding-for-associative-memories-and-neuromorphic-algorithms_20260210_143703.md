---
ver: rpa2
title: 'Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic
  Algorithms'
arxiv_id: '2509.07361'
source_url: https://arxiv.org/abs/2509.07361
tags:
- spiking
- arxiv
- neural
- networks
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Word2Spike, a novel rate coding mechanism that
  converts continuous word embeddings into spike-based representations for neuromorphic
  associative memory systems. The method uses BitNet b1.58 quantization to discretize
  embeddings to {-1, 0, +1}, then maps these to Poisson-distributed spike trains with
  firing rates of 100 Hz, 0 Hz, and 50 Hz respectively.
---

# Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic Algorithms

## Quick Facts
- **arXiv ID**: 2509.07361
- **Source URL**: https://arxiv.org/abs/2509.07361
- **Reference count**: 17
- **Primary result**: Achieves 100% reconstruction accuracy on 10,000 words while maintaining 97% semantic similarity through Poisson-based spike rate coding

## Executive Summary
Word2Spike introduces a novel rate coding mechanism that converts continuous word embeddings into spike-based representations suitable for neuromorphic associative memory systems. The approach uses BitNet b1.58 quantization to discretize embeddings to {-1, 0, +1}, then maps these values to Poisson-distributed spike trains with firing rates of 100 Hz, 0 Hz, and 50 Hz respectively. The system demonstrates strong performance on standard benchmarks while enabling compatibility with spiking neural networks for neuromorphic computing applications.

## Method Summary
The Word2Spike method employs a two-stage quantization and encoding process. First, it applies BitNet b1.58 quantization to convert continuous embeddings into ternary values {-1, 0, +1}. These quantized values are then mapped to Poisson-distributed spike trains with specific firing rates: 100 Hz for +1, 0 Hz for 0, and 50 Hz for -1. This rate coding scheme preserves semantic information while enabling efficient processing in neuromorphic hardware. The method is validated on OpenAI's text-embedding-3-large model with 10,000 words, achieving high reconstruction accuracy and semantic similarity while supporting downstream tasks like analogy completion and nearest-neighbor search.

## Key Results
- Achieves 100% reconstruction accuracy on 10,000 words from OpenAI's text-embedding-3-large
- Maintains 97% semantic similarity on SimLex-999 benchmark with only 3% absolute drop
- Preserves analogy performance at 37.50% and achieves 72.7% nearest-neighbor consistency

## Why This Works (Mechanism)
Word2Spike leverages the principle that semantic information can be preserved through rate-based spike coding rather than precise spike timing. The BitNet quantization reduces the continuous embedding space to a ternary representation that can be efficiently encoded as firing rates. The asymmetric rate assignment (100 Hz for +1, 50 Hz for -1, 0 Hz for 0) creates sufficient discrimination between values while maintaining information density. The Poisson distribution introduces biological realism and noise tolerance, making the representations robust to variability in neuromorphic hardware.

## Foundational Learning
- **Poisson rate coding**: Stochastic spike generation where spike count per time window follows Poisson distribution
  - *Why needed*: Provides biologically plausible noise tolerance and hardware-efficient computation
  - *Quick check*: Verify that mean spike count equals assigned firing rate over long time windows
- **BitNet b1.58 quantization**: Ternary quantization scheme mapping continuous values to {-1, 0, +1}
  - *Why needed*: Reduces precision requirements while maintaining semantic information
  - *Quick check*: Measure information loss from 1536-dimensional embeddings to 3-value quantization
- **Neuromorphic associative memory**: Memory systems using spiking neurons for pattern storage and retrieval
  - *Why needed*: Enables energy-efficient, parallel processing of spike-encoded information
  - *Quick check*: Compare energy consumption of spike-based vs traditional neural network inference

## Architecture Onboarding
- **Component map**: Continuous embeddings -> BitNet quantization -> Poisson spike generator -> Neuromorphic memory -> Retrieval
- **Critical path**: Embedding quantization directly impacts spike rate accuracy, which determines retrieval performance
- **Design tradeoffs**: Higher firing rates improve signal-to-noise ratio but increase energy consumption and hardware bandwidth requirements
- **Failure signatures**: Semantic similarity degradation indicates insufficient quantization granularity; reconstruction errors suggest inappropriate rate assignments
- **First experiments**: 1) Measure semantic drift across different firing rate assignments, 2) Test noise robustness with varying Poisson variance, 3) Benchmark energy efficiency on neuromorphic hardware

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to 10,000-word vocabulary, raising scalability concerns for larger language models
- 3% semantic similarity drop may compound in downstream tasks requiring fine-grained distinctions
- Analogy performance at 37.50% remains below traditional transformer-based approaches
- Theoretical claims about spiking transformer applications lack empirical hardware validation

## Confidence
- **Reconstruction accuracy and semantic similarity claims**: High - Well-validated on established benchmarks with clear methodology
- **Neuromorphic application claims**: Medium - Supported by theoretical framework but lacking hardware implementation studies
- **Scalability and generalization claims**: Low - Limited to 10,000-word vocabulary without evidence for larger models

## Next Checks
1. Evaluate Word2Spike performance on dynamic language modeling tasks (next-word prediction, masked language modeling) rather than static embedding reconstruction to assess practical utility
2. Test the rate coding mechanism on larger vocabularies (100K+ words) and multi-modal embeddings to establish scalability limits
3. Implement Word2Spike on actual neuromorphic hardware (e.g., Intel Loihi, BrainScaleS) to validate biological noise robustness claims under real hardware constraints