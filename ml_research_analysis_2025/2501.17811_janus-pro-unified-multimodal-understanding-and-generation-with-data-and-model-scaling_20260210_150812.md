---
ver: rpa2
title: 'Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model
  Scaling'
arxiv_id: '2501.17811'
source_url: https://arxiv.org/abs/2501.17811
tags:
- arxiv
- generation
- multimodal
- understanding
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Janus-Pro improves multimodal understanding and generation by optimizing
  training strategy, scaling up training data, and increasing model size from 1.5B
  to 7B parameters. The key innovation is decoupling visual encoding for understanding
  and generation tasks using separate SigLIP and VQ tokenizers.
---

# Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling

## Quick Facts
- arXiv ID: 2501.17811
- Source URL: https://arxiv.org/abs/2501.17811
- Reference count: 40
- Key outcome: Janus-Pro-7B achieves 79.2 on MMBench and 80% on GenEval, outperforming previous unified models

## Executive Summary
Janus-Pro introduces a unified multimodal model that decouples visual encoding for understanding and generation tasks, using SigLIP for semantic understanding and VQ tokenization for image generation. The architecture achieves state-of-the-art performance by scaling training data, increasing model size to 7B parameters, and optimizing training strategy through extended pretraining stages. The decoupled approach reduces representational conflict between understanding and generation tasks while maintaining efficiency through a shared LLM backbone.

## Method Summary
Janus-Pro employs a three-stage training pipeline with decoupled visual encoders: SigLIP-Large-Patch16-384 for understanding tasks and a VQ tokenizer (16K codebook) for generation. The model uses DeepSeek-LLM as the backbone (1.5B or 7B parameters) with task-specific 2-layer MLP adaptors. Stage I focuses on adaptor pretraining with ImageNet, Stage II performs unified pretraining with a 2:3:5 data ratio of multimodal:synthetic:real data, and Stage III fine-tunes with a 5:1:4 ratio while unlocking the understanding encoder. The model operates at 384×384 resolution and uses separate prediction heads for text and image generation.

## Key Results
- Janus-Pro-7B achieves 79.2 on MMBench multimodal understanding benchmark
- 80% accuracy on GenEval text-to-image generation benchmark
- Outperforms previous unified models (Janus: 69.4 MMBench, 61% GenEval)
- Demonstrates strong scalability from 1.5B to 7B parameters

## Why This Works (Mechanism)

### Mechanism 1
Decoupling visual encoders for understanding vs. generation reduces representational conflict and improves both tasks. SigLIP encoder extracts high-dimensional semantic features for understanding, while VQ tokenizer converts images to discrete IDs for generation. Each pathway uses a dedicated adaptor to map into the LLM's input space, allowing the unified transformer to process both without forcing a shared visual representation.

### Mechanism 2
Longer adaptor pretraining with ImageNet enables pixel-dependence modeling even with frozen LLM, improving training efficiency. Extended Stage I training on ImageNet using category names as prompts learns spatial dependencies before unified pretraining, allowing Stage II to focus exclusively on dense text-to-image data.

### Mechanism 3
Synthetic aesthetic data improves generation stability and convergence speed compared to noisy real-world data. Adding ~72M synthetic aesthetic samples (1:1 ratio with real data) during unified pretraining reduces noise from low-quality real-world captions, providing cleaner prompt-image pairs that lead to faster convergence and more stable outputs.

## Foundational Learning

- **Autoregressive next-token prediction for multimodal sequences**: Essential for understanding how the unified transformer predicts both text tokens and image tokens autoregressively. *Quick check*: Can you explain why flattening 2D image features into a 1D sequence is compatible with autoregressive modeling?

- **Vision encoders: semantic (SigLIP) vs. discrete (VQ)**: Central to the decoupling mechanism, as understanding tasks benefit from continuous semantic embeddings while generation tasks require discrete token sequences. *Quick check*: What are the tradeoffs between using continuous embeddings vs. discrete tokens for visual representation?

- **Multi-stage training with data ratio tuning**: Necessary to understand how Janus-Pro's improvements come from adjusting training stage duration and data ratios (multimodal:text:generation from 7:3:10 to 5:1:4 in Stage III). *Quick check*: Why might reducing the proportion of text-to-image data in Stage III improve multimodal understanding without harming generation?

## Architecture Onboarding

- **Component map**: Input → appropriate encoder (SigLIP for understanding, VQ for generation) → task-specific adaptor → LLM embedding space → unified transformer → appropriate prediction head (text via LLM head, image via separate head)

- **Critical path**: Understanding pathway: SigLIP → 2-layer MLP adaptor → LLM input space → text generation head. Generation pathway: VQ tokenizer → 2-layer MLP adaptor → LLM input space → image generation head.

- **Design tradeoffs**: Resolution limited to 384×384 (affects OCR and fine detail quality), VQ tokenizer introduces reconstruction loss (generation outputs may lack fine details), separate encoders increase parameter count but reduce task interference, 1:1 real:synthetic data ratio balances quality vs. potential synthetic bias.

- **Failure signatures**: Short prompts producing unstable/low-quality images (likely undertrained Stage I or insufficient synthetic aesthetic data), multimodal understanding degradation after SFT (check text-to-image data ratio in Stage III), slow convergence in Stage II (verify ImageNet data was excluded).

- **First 3 experiments**: 1) Ablate Stage I duration: compare original vs. extended ImageNet pretraining on pixel-dependence quality. 2) Vary synthetic data ratio: test 0%, 25%, 50%, 75% synthetic aesthetic data in Stage II measuring convergence and GenEval scores. 3) Scale model size systematically: train 1.5B and 7B variants with identical data/strategy to validate scaling benefits.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding resolution scaling beyond 384×384, alternative tokenization methods to reduce reconstruction losses, optimal data ratio tuning for different task balances, and performance scaling to significantly larger model sizes (70B+ parameters).

## Limitations
- Limited to 384×384 resolution, affecting OCR performance and fine detail quality in generated images
- VQ tokenizer reconstruction losses result in lack of fine details in generated outputs
- No systematic study on how data ratios affect the understanding-generation trade-off across different model capacities
- Synthetic data generation methodology not fully specified, limiting reproducibility of claimed quality improvements

## Confidence
- **High Confidence**: Decoupled visual encoding architecture and its theoretical benefits for reducing task interference
- **Medium Confidence**: Training strategy optimization improving sample efficiency, based on reasonable experimental design but limited ablation
- **Low Confidence**: Synthetic aesthetic data quality improvements, as the paper lacks detailed synthetic data generation methodology

## Next Checks
1. **Synthetic Data Ablation**: Systematically vary synthetic data ratio from 0% to 100% in 25% increments during Stage II training, measuring convergence speed and GenEval scores to isolate synthetic data contribution.
2. **VQ Tokenizer Implementation**: Replicate the exact VQ tokenizer architecture and training procedure using the referenced LlamaGen codebase, then measure reconstruction quality and generation performance.
3. **Understanding-Generation Trade-off**: Conduct controlled experiments varying the text-to-image data ratio in Stage III (e.g., 5:1:1, 5:1:2, 5:1:4) while measuring both MMBench and GenEval scores to identify optimal balance.