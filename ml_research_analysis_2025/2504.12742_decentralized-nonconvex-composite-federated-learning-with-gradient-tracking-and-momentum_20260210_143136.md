---
ver: rpa2
title: Decentralized Nonconvex Composite Federated Learning with Gradient Tracking
  and Momentum
arxiv_id: '2504.12742'
source_url: https://arxiv.org/abs/2504.12742
tags:
- gradient
- momentum
- stochastic
- option
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Decentralized Nonconvex Composite Federated
  Learning (DNCFL) algorithm called DEPOSITUM to solve nonconvex composite optimization
  problems in a decentralized federated learning setting. The algorithm leverages
  proximal stochastic gradient tracking with momentum to mitigate the impact of data
  heterogeneity across clients and reduce communication costs through local updates.
---

# Decentralized Nonconvex Composite Federated Learning with Gradient Tracking and Momentum

## Quick Facts
- **arXiv ID**: 2504.12742
- **Source URL**: https://arxiv.org/abs/2504.12742
- **Reference count**: 40
- **Primary result**: Proposed DEPOSITUM algorithm achieves O(1/ϵ²) iteration complexity for finding ϵ-stationary points in decentralized nonconvex composite federated learning

## Executive Summary
This paper introduces DEPOSITUM, a novel decentralized federated learning algorithm designed to solve nonconvex composite optimization problems. The algorithm incorporates proximal gradient tracking with momentum to address challenges arising from data heterogeneity across clients while reducing communication costs through local updates. By leveraging these techniques, DEPOSITUM aims to achieve efficient and scalable federated learning in decentralized settings where data remains distributed across multiple clients.

## Method Summary
DEPOSITUM is built upon proximal gradient tracking with momentum, combining decentralized consensus mechanisms with local optimization steps. The algorithm employs a weighted mixing matrix to aggregate local gradients across the network, while proximal operators handle the composite structure of the objective function. Momentum terms are incorporated to accelerate convergence, and the gradient tracking mechanism ensures that each client can estimate the global gradient direction despite data heterogeneity. Local updates between communication rounds help reduce the communication burden while maintaining convergence properties.

## Key Results
- DEPOSITUM achieves O(1/ϵ²) iteration complexity for finding ϵ-stationary points
- Proximal gradient, consensus errors, and gradient estimation errors decrease at O(1/T) sublinear rate
- Achieves network-independent linear speedup without requiring mega-batch sampling
- Superior performance compared to other federated composite optimization algorithms on real-world datasets

## Why This Works (Mechanism)
DEPOSITUM's effectiveness stems from its ability to address two key challenges in federated learning: data heterogeneity and communication efficiency. The gradient tracking mechanism allows each client to estimate the global gradient direction by aggregating information from neighboring clients, which mitigates the impact of non-IID data distributions. The momentum terms accelerate convergence by incorporating historical gradient information, while the proximal operators handle the composite structure of the objective function (combining smooth and non-smooth components). The local update strategy reduces communication frequency without sacrificing convergence guarantees, making the algorithm practical for real-world federated learning scenarios.

## Foundational Learning
- **Nonconvex optimization theory**: Needed to analyze convergence properties of algorithms applied to nonconvex problems; Quick check: Verify gradient Lipschitz continuity and bounded gradient dissimilarity conditions
- **Decentralized consensus algorithms**: Essential for understanding how clients coordinate without central authority; Quick check: Confirm network connectivity and mixing matrix properties
- **Proximal operator theory**: Critical for handling composite objective functions with non-smooth regularization; Quick check: Validate Lipschitz continuity of proximal operator
- **Gradient tracking mechanisms**: Key for enabling clients to estimate global gradients in decentralized settings; Quick check: Monitor gradient tracking error convergence
- **Momentum acceleration**: Important for improving convergence speed in optimization algorithms; Quick check: Compare convergence rates with and without momentum

## Architecture Onboarding

**Component Map**: Clients -> Local Gradient Computation -> Gradient Tracking -> Weighted Mixing -> Proximal Update -> Consensus Error Reduction

**Critical Path**: The most critical path involves the gradient tracking and weighted mixing operations, as these enable clients to estimate global gradients and maintain consensus across the decentralized network. The proximal updates follow these operations and are essential for handling the composite objective structure.

**Design Tradeoffs**: The algorithm trades increased per-iteration computational complexity (due to gradient tracking and proximal operations) for reduced communication frequency. The momentum terms add memory requirements but improve convergence speed. The weighted mixing matrix design balances between network connectivity and convergence speed.

**Failure Signatures**: Convergence failures may manifest as increasing consensus errors, oscillating gradient tracking errors, or divergence of the objective function value. Poor mixing matrix design or inadequate local update steps can lead to slow convergence or stagnation.

**First 3 Experiments**: 1) Test convergence on a simple nonconvex composite problem with known solution to verify theoretical guarantees; 2) Evaluate sensitivity to network topology by comparing convergence on different graph structures; 3) Measure communication efficiency by comparing convergence speed under different communication frequencies.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on strong assumptions including Lipschitz continuity of gradient and proximal operator, bounded gradient dissimilarity, and network connectivity
- Convergence rate analysis is asymptotic and may not accurately capture finite-time behavior
- Experiments are limited to specific real-world datasets without thorough ablation studies on hyperparameter sensitivity
- Comparison with existing methods is restricted to specific algorithms without broader benchmarking

## Confidence
- Theoretical convergence guarantees: Medium
- Communication efficiency claims: Medium
- Experimental validation: Medium
- Algorithm novelty: Medium

## Next Checks
1. Conduct extensive ablation studies to analyze the impact of hyperparameters (momentum coefficient, local steps, learning rate) on convergence and communication efficiency under varying levels of data heterogeneity and network conditions
2. Validate the theoretical convergence rate bounds through empirical measurements of proximal gradient, consensus, and gradient estimation errors across different numbers of communication rounds and network topologies
3. Extend experimental evaluation to a wider range of federated learning algorithms and benchmark datasets to comprehensively assess DEPOSITUM's relative performance across diverse problem settings and communication constraints