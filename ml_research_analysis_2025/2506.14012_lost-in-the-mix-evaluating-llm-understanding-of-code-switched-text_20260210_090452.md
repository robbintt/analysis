---
ver: rpa2
title: 'Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text'
arxiv_id: '2506.14012'
source_url: https://arxiv.org/abs/2506.14012
tags:
- language
- english
- text
- code-switched
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates how well LLMs handle code-switched text\u2014\
  where multiple languages are mixed within a single discourse\u2014a common occurrence\
  \ in multilingual communities and online content. To assess comprehension, the authors\
  \ generate code-switched variants of established reasoning benchmarks using a linguistically\
  \ grounded LLM pipeline that respects grammatical constraints."
---

# Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text

## Quick Facts
- arXiv ID: 2506.14012
- Source URL: https://arxiv.org/abs/2506.14012
- Reference count: 34
- Primary result: Embedding non-English tokens into English text consistently degrades LLM performance, even under linguistic constraints.

## Executive Summary
This paper evaluates how well LLMs handle code-switched text—where multiple languages are mixed within a single discourse—a common occurrence in multilingual communities and online content. To assess comprehension, the authors generate code-switched variants of established reasoning benchmarks using a linguistically grounded LLM pipeline that respects grammatical constraints. They find that embedding non-English tokens into English text consistently degrades LLM performance, even when switches follow linguistic rules, suggesting a structural processing difficulty rather than token-level unfamiliarity. Conversely, embedding English into other languages often improves comprehension, especially when models are weaker in the matrix language. Prompt-based mitigation shows inconsistent results, benefiting some models but harming others, while fine-tuning on code-switched data leads to more reliable, though partial, performance recovery. These results highlight asymmetric LLM vulnerabilities to code-switching and point to the need for more adaptive strategies to ensure robust performance in real-world multilingual settings.

## Method Summary
The authors evaluate LLM comprehension on code-switched (CSW) text using noun-token (linguistically grounded) and ratio-token (~20% random substitution) methods across Belebele, MMLU, and XNLI benchmarks. CSW was generated via an LLM-centric method using Claude 3.5 Sonnet with parallel TED Talk transcripts, respecting grammatical constraints through Equivalence Constraint Theory (ECT) and Matrix Language Frame (MLF) principles. The method involves two steps: (1) Claude marks switchable nouns with "#######", (2) Claude fills with aligned target-language words per ECT/MLF constraints. Models were evaluated on their accuracy, weighted average accuracy, and accuracy delta (CSW vs monolingual baseline) using the EleutherAI Language Model Evaluation Harness. Fine-tuning experiments used ~14,600 CSW samples (>70-word sentences, 4 language pairs) with LLaMA-8B, 1 epoch, lr=2e-6, and 5% warmup.

## Key Results
- Embedding non-English tokens into English text consistently degrades LLM performance, even under linguistic constraints
- Embedding English tokens into other languages often improves comprehension, especially when models lack proficiency in the matrix language
- Fine-tuning on CSW data yields more stable performance gains than prompting, though recovery remains partial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding non-English tokens into English text degrades LLM comprehension beyond what token-level unfamiliarity explains.
- Mechanism: Foreign tokens disrupt the monolingual grammatical frame that LLMs rely on for coherent representation assembly. Even when switches follow linguistic constraints (ECT/MLF), the model's internal language-specific processing pathways fragment, forcing cross-lingual integration at inference time without dedicated training.
- Core assumption: LLMs develop language-specific processing routines during pre-training that expect monolingual input distributions.
- Evidence anchors:
  - [abstract] "degradation is evident when foreign tokens disrupt English text—even under linguistic constraints"
  - [section 5.1] Performance drops observed across all models (e.g., LLaMA-70B: 0.70→0.66, ∆≈−0.04) even with linguistically grounded noun-token substitution
  - [corpus] "Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?" (arxiv 2601.07153) confirms LLMs struggle with mixed-language inputs
- Break condition: Models with substantial pre-training data in the embedded language (e.g., ALLaM on Arabic, Qwen on Chinese) show smaller degradation, suggesting exposure mitigates but doesn't eliminate the effect.

### Mechanism 2
- Claim: Embedding English tokens into non-English matrix languages can improve comprehension, especially for models weak in the matrix language.
- Mechanism: English functions as a "lingua franca backbone"—its dominant representation in pre-training provides higher-quality semantic anchors that stabilize reasoning when the model's matrix language representations are sparse or noisy. The model effectively routes through English semantic space.
- Core assumption: English representations are more robustly learned due to training data imbalance.
- Evidence anchors:
  - [abstract] "embedding English into other languages often improves comprehension, especially when the model lacks proficiency in the matrix language"
  - [section 6.1] Mistral 7B's Arabic accuracy rose from 0.35 to 0.48 (∆=+0.13); Chinese improved by +0.07
  - [corpus] No direct corpus support for this asymmetric effect; primarily evidenced within this paper
- Break condition: Models already proficient in the matrix language show minimal or no improvement (ALLaM on Arabic: +0.01), and one case showed slight degradation (Qwen 7B on Chinese: −0.01).

### Mechanism 3
- Claim: Fine-tuning on synthetic CSW data provides more stable mitigation than prompting, but recovery is partial.
- Mechanism: Instruction-tuning on CSW examples creates dedicated parameter updates that encode cross-lingual integration patterns. Unlike prompting (which relies on in-context activation of existing capabilities), fine-tuning modifies weights to handle mixed-language input distributions more directly.
- Core assumption: The synthetic CSW data distribution approximates real CSW patterns sufficiently for transfer.
- Evidence anchors:
  - [abstract] "fine-tuning on CSW data yields more stable performance gains"
  - [section 7.2] CSW-Llama 8B showed partial recovery; EN→AR improved +0.04 points over baseline
  - [corpus] Corpus papers focus on CSW generation/evaluation, not mitigation strategies
- Break condition: Recovery remains incomplete (maximum +0.04 improvement), and effectiveness depends on quality/diversity of fine-tuning data.

## Foundational Learning

- Concept: **Equivalence Constraint Theory (ECT) & Matrix Language Frame (MLF)**
  - Why needed here: These linguistic theories constrain where code-switches can occur grammatically; understanding them is necessary to interpret why "linguistically motivated" CSW still degrades performance.
  - Quick check question: Given "I went to the **markt** yesterday" (German noun in English frame), which theory explains why this switch point is permissible?

- Concept: **Tokenization asymmetry across languages**
  - Why needed here: Different languages tokenize with different efficiency (English: ~1 token/word; Chinese/Arabic: often higher). This affects how "disruptive" embedded tokens are to the model's processing.
  - Quick check question: Why might a single Chinese character require more tokens than an English noun, and how does this affect CSW degradation?

- Concept: **Weighted average accuracy & accuracy delta metrics**
  - Why needed here: The paper uses ∆Acc to quantify CSW impact; understanding this metric is essential for interpreting results and designing replication experiments.
  - Quick check question: If a model scores 0.70 on English MMLU and 0.64 on EN→AR CSW MMLU, what is the accuracy delta, and what does a negative value indicate?

## Architecture Onboarding

- Component map: Parallel Corpus → Alignment Module: AWESOME + LaBSE → POS Tagging: Stanza → LLM Generation Module: Claude 3.5 Sonnet → CSW Text: Noun-token / Ratio-token → Evaluation Harness: EleutherAI + Custom CSW Benchmarks → Target LLM → Metrics: Accuracy, Weighted Avg, ∆Acc

- Critical path:
  1. CSW generation quality determines evaluation validity—if synthetic CSW is unnatural, degradation may reflect data quality rather than model capability.
  2. Benchmark selection (Belebele/MMLU/XNLI) determines whether you're testing surface fluency vs. deep reasoning.
  3. Model scale strongly predicts CSW resilience (70B models show ∆≈−0.02–0.04; 3B models show ∆≈−0.07–0.11).

- Design tradeoffs:
  - Noun-token vs. Ratio-token: Noun-token preserves grammaticality (linguistically grounded) but may underestimate real-world CSW complexity; ratio-token tests robustness to noise but is less natural.
  - Prompting vs. Fine-tuning: Prompting is cheaper and faster but model-specific (helps Qwen, hurts Llama); fine-tuning is more stable but requires compute and CSW training data.
  - LLM-centric vs. Alignment-based generation: LLM-centric produces more fluent output (preferred by GPT-4o judge) but less controllable; alignment-based enables precise substitution rates.

- Failure signatures:
  - Prompting backfire: Llama 70B dropped 13–17 points with instructional prompts on EN→AR/EN→ZH (Table 5).
  - No recovery from fine-tuning: If CSW-Llama 8B shows <0.01 improvement, check training data diversity (may need >14,600 samples or more language pairs).
  - Extreme CSW collapse: If multi-language embedding (Setting 3) causes >0.10 drop beyond 2-language CSW, model lacks cross-lingual integration capacity.

- First 3 experiments:
  1. Baseline CSW degradation: Generate noun-token CSW variants for one benchmark (e.g., MMLU) with one embedded language (e.g., Arabic), evaluate 2–3 model sizes from same family (e.g., Llama 3B/8B/70B). Confirm ∆Acc correlates inversely with scale.
  2. Asymmetry test: Reverse direction—embed English into Arabic matrix text, evaluate Arabic-proficient model (e.g., ALLaM). Verify whether ∆Acc is positive (improvement) or less negative than English-matrix condition.
  3. Mitigation comparison: For one model that responded poorly to prompting (e.g., Llama 8B), implement lightweight instruction-tuning on 5,000 CSW examples and compare recovery vs. prompt-based approach on same test set.

## Open Questions the Paper Calls Out

- How do higher-complexity code-switching patterns (beyond noun-token substitution) affect LLM comprehension, and do they induce more severe degradation? The study only examined noun-token substitution, one fundamental form of linguistically grounded switching.
- How does varying the substitution ratio in non-linguistically motivated code-switching affect model behavior across different degradation thresholds? Only a single 20% ratio was tested, leaving the degradation curve unknown.
- What mechanistic factors cause English to have an asymmetric facilitative effect as an embedded language but a disruptive effect as a matrix language? The paper identifies the phenomenon but attributes it only vaguely to "a nuanced interaction between specific language combinations and model-specific linguistic representations."

## Limitations
- Synthetic CSW generation may not fully capture the complexity and variability of naturally occurring code-switched text in real-world settings
- Fine-tuning recovery remains partial and inconsistent across models, suggesting the approach may not generalize well beyond tested conditions
- The asymmetric effect of English embedding lacks strong external validation and may be influenced by dataset-specific artifacts

## Confidence
- **High Confidence**: The general finding that CSW degrades LLM comprehension is well-supported by multiple benchmarks and model families. The superiority of fine-tuning over prompting for stable mitigation is also well-demonstrated.
- **Medium Confidence**: The asymmetric improvement from English embedding is plausible given pre-training data imbalances but needs replication with more diverse models and natural CSW data.
- **Low Confidence**: The exact mechanism behind why linguistic constraints don't prevent degradation is inferred from performance patterns rather than directly measured neural activity.

## Next Checks
1. Evaluate the same models on naturally occurring CSW data from social media or conversational transcripts to verify whether synthetic CSW degradation patterns hold in real-world settings.
2. Test the English embedding asymmetry effect on additional model families (e.g., GPT, Claude) to determine if this is a general phenomenon or specific to the tested models.
3. Use interpretability tools to analyze attention distributions when processing CSW vs. monolingual text, specifically testing whether non-English embedded tokens indeed disrupt monolingual grammatical frame processing as hypothesized.