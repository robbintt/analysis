---
ver: rpa2
title: 'Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence
  and Efficient Exploration'
arxiv_id: '2509.22964'
source_url: https://arxiv.org/abs/2509.22964
tags:
- functional
- critic
- off-policy
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits the use of functional critics (policy-conditioned
  value functions) in off-policy actor-critic methods. The authors identify two critical
  aspects that make functional critics necessary rather than optional: (1) they enable
  the first provable convergence result for an off-policy target-based AC algorithm
  under linear function approximation by addressing the deadly triad instability and
  moving target problem, and (2) they are essential for approximating posterior sampling-based
  exploration in model-free RL.'
---

# Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence and Efficient Exploration

## Quick Facts
- arXiv ID: 2509.22964
- Source URL: https://arxiv.org/abs/2509.22964
- Authors: Qinxun Bai; Yuxuan Han; Wei Xu; Zhengyuan Zhou
- Reference count: 35
- Primary result: Functional critics enable first provable convergence for off-policy target-based AC under linear function approximation and enable posterior sampling exploration without model-based methods.

## Executive Summary
This paper addresses fundamental instability in off-policy actor-critic methods by introducing functional critics - policy-conditioned value functions that generalize across policy space. The authors identify two critical aspects that make functional critics essential rather than optional: they enable provable convergence by resolving the deadly triad instability and moving target problem, and they enable tractable posterior sampling exploration in model-free RL. The proposed algorithm achieves competitive performance on DeepMind Control Suite tasks without relying on standard heuristics like entropy regularization or minimum target values.

## Method Summary
The method uses an ensemble of 10 deterministic actors paired with 10 functional critics, where each functional critic estimates Q(π, s, a; ξ) - a value function conditioned on policy parameters θ. The architecture features a transformer-based actor encoder (E_act) that processes policy πθ through trainable evaluation samples, combined with MLP state-action (E_sa) and joint (E_joint) encoders. Training proceeds with 3 critic updates per actor update (UTD=3), using exact off-policy gradients computed through the differentiable functional critic. Exploration is achieved through the ensemble structure, approximating posterior sampling without requiring entropy tuning or stochastic policies.

## Key Results
- First convergence proof for off-policy target-based AC under linear function approximation, resolving deadly triad + moving target problems
- Competitive performance on DeepMind Control Suite compared to RLPD baseline without extensive heuristics
- Functional critics enable tractable exact off-policy gradients without emphasis weight estimation
- Ensemble of functional critics provides posterior sampling approximation for exploration

## Why This Works (Mechanism)

### Mechanism 1: Deadly Triad and Moving Target Resolution Through Policy-Conditioning
Functional critics enable convergent off-policy target-based TD learning by conditioning value functions on policy parameters Q(s,a,θ). This creates a global functional that generalizes across policy space rather than relearning Qπ for each policy change. The policy-conditioned representation decouples the moving target problem from the deadly triad, allowing target-based TD updates to converge under linear function approximation with appropriate regularization. The key requirement is a linear functional representation with bounded approximation error ε and Lipschitz policy changes.

### Mechanism 2: Tractable Exact Off-Policy Gradient via Differentiable Policy Input
Functional critics make exact off-policy policy gradient estimation tractable by enabling gradient flow through the policy representation to critic parameters. This eliminates the need for emphasis weight estimation that plagues standard critic approaches. The functional critic Q̂(πθ, s, a; ξ) is explicitly differentiable with respect to θ through its policy input, making the second term of the off-policy gradient ∫ dμ(s) πθ(a|s) ∇θ Qθ(s,a) da directly computable.

### Mechanism 3: Posterior Sampling Approximation via Ensemble of Functional Curves
An ensemble of functional critics enables Thompson sampling-style exploration by approximating a distribution over policy-value mappings. Each functional critic estimates a complete "policy → value" curve, and the ensemble collectively represents a posterior over these curves. This enables posterior sampling: sample a critic from the ensemble, then optimize its corresponding actor. Standard critic ensembles fail because they either estimate Qπ for a single fixed policy or different Qπi for isolated policies, neither capturing the full policy-value relationship distribution.

## Foundational Learning

- **Concept: Deadly Triad in Off-Policy RL**
  - Why needed: Understanding why standard TD-learning diverges under off-policy + function approximation + bootstrapping is essential for appreciating why functional critics enable convergence
  - Quick check: Given a fixed behavior policy μ and target policy π ≠ μ, explain why semi-gradient TD updates with function approximation can diverge, and how target networks change the update dynamics

- **Concept: Off-Policy Policy Gradient with Emphasis Weights**
  - Why needed: The paper's contribution relies on showing that functional critics eliminate the need for estimating emphasis weights mθ(s) in exact off-policy gradients
  - Quick check: Derive the off-policy policy gradient formula with emphasis weights and identify where the distribution mismatch correction appears

- **Concept: Thompson Sampling and Posterior Sampling RL**
  - Why needed: The exploration mechanism claims functional critics enable model-free approximation of PSRL; understanding PSRL's structure is necessary
  - Quick check: Explain why maintaining a posterior over MDPs induces a distribution over optimal policies, and why standard critic ensembles do not replicate this in model-free settings

## Architecture Onboarding

- **Component map**: 
  - Actor Encoder (E_act) -> State-Action Encoder (E_sa) -> Joint Encoder (E_joint) -> Q̂(πθ, s, a)
  - Target Network shares E_act, contains delayed copies of E_sa and E_joint
  - Actor Ensemble contains n deterministic policies, each paired with one functional critic

- **Critical path**:
  1. Sample transition batch B_C from replay buffer
  2. For each critic i: compute TD targets using target functional critic across all actors in ensemble
  3. Update functional critic parameters by minimizing MSE loss
  4. Update target critics via polyak averaging
  5. Sample transition batch B_A; for each actor i, compute exact off-policy gradient from paired critic and update actor

- **Design tradeoffs**:
  - Shared vs. separate actor encoders for targets: Paper shares E_act to reduce parameters but sacrifices target independence
  - Deterministic vs. stochastic actors: Paper uses deterministic actors with ensemble for exploration, avoiding entropy tuning
  - UTD ratio: Paper uses 2-3 critic updates per actor update (vs. 10+ in RLPD)
  - Evaluation sample set size: 512 trainable samples trade off actor encoding quality vs. computation

- **Failure signatures**:
  - Critic divergence: Check regularization factor λ and target update rate τ if TD loss grows unbounded
  - Poor actor generalization: Check ensemble initialization diversity if actors collapse to similar policies
  - Stall at 1:1 UTD: Paper reports instability at equal update frequencies due to simple actor encoder design
  - Exploration failure: Check critic ensemble variance if performance plateaus early

- **First 3 experiments**:
  1. Implement functional critic with linear approximation on 5-state chain MDP to verify convergence vs standard TD divergence
  2. Test transformer actor encoder vs simple MLP encoding of actor parameters on DMC cheetah-run
  3. Run ensemble size sweep (n ∈ {2, 5, 10, 20}) on hopper-hop to identify minimum viable ensemble for posterior sampling

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a more expressive actor encoder architecture enable stable training at a 1:1 actor-critic update ratio?
  - Basis: Paper states it has not achieved stable training at 1:1 ratio, attributing this to the "relatively simple actor encoder design" used for efficiency

- **Open Question 2**: Do the convergence guarantees for linear functional approximation extend to the non-linear neural network setting?
  - Basis: Theoretical analysis is explicitly restricted to linear functional approximation, while practical algorithm uses deep neural networks

- **Open Question 3**: How does the error in the learned feature map (ε in Assumption 1) quantitatively impact the gradient bias b(θt) and final performance?
  - Basis: Theorem 3.2 shows convergence to a neighborhood dependent on bias term b(θt), which arises from the linear approximation error ε

## Limitations

- Theoretical convergence proof applies to linear function approximation while practical implementation uses deep neural networks
- Empirical validation limited to small set of DeepMind Control Suite tasks with deterministic actors
- Comparison against RLPD constrained by not using RLPD's extensive heuristic tuning
- Claims about posterior sampling approximation rely on ensemble diversity that may not scale well

## Confidence

- **High confidence**: Functional critics enable tractable exact off-policy gradients via differentiable policy input
- **Medium confidence**: Ensemble of functional critics approximates posterior sampling exploration (empirical support limited)
- **Medium confidence**: Theoretical convergence guarantees under linear approximation extend meaningfully to neural network case

## Next Checks

1. Test functional critic convergence on tabular MDPs with known linear features, comparing against standard TD with target networks to isolate the moving target resolution effect
2. Implement ablation comparing transformer actor encoder versus direct MLP encoding of actor parameters to quantify actor representation impact on functional generalization
3. Measure critic ensemble diversity (variance across actors) at different training stages to validate posterior sampling approximation quality