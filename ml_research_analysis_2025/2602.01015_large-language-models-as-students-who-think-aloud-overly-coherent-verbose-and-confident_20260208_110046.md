---
ver: rpa2
title: 'Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose,
  and Confident'
arxiv_id: '2602.01015'
source_url: https://arxiv.org/abs/2602.01015
tags:
- learner
- llms
- reasoning
- context
- novice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether large language models (LLMs) can\
  \ faithfully simulate novice reasoning and metacognitive judgments during problem\
  \ solving. Using 630 step-level think-aloud utterances from chemistry tutoring,\
  \ the researchers compared LLM-generated reasoning to human learner utterances under\
  \ minimal and extended contextual prompting, and assessed the models\u2019 ability\
  \ to predict step-level learner success."
---

# Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident

## Quick Facts
- arXiv ID: 2602.01015
- Source URL: https://arxiv.org/abs/2602.01015
- Authors: Conrad Borchers; Jill-Jênn Vie; Roger Azevedo
- Reference count: 24
- Primary result: GPT-4.1 simulates novice reasoning but is over-coherent, verbose, and overconfident compared to human think-alouds.

## Executive Summary
This study evaluates whether large language models (LLMs) can faithfully simulate novice reasoning and metacognitive judgments during problem solving. Using 630 step-level think-aloud utterances from chemistry tutoring, the researchers compared LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assessed the models’ ability to predict step-level learner success. Although GPT-4.1 produced fluent and contextually appropriate continuations, its reasoning was systematically over-coherent, verbose, and less variable than human think-alouds. Extended context intensified these effects. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs, attributed to training data that lacks expressions of affect and working memory constraints during problem solving. The evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

## Method Summary
The study used 630 step-level think-aloud utterances from chemistry tutoring with graded responses, hint use, attempts, and problem context (PSLC DataShop dataset ID 5371). GPT-4.1 generated think-aloud continuations and step-success predictions under two prompting conditions: simple context (preceding utterance only) and extended context (utterance + problem statement + prior inputs + tutor feedback). Outputs were evaluated via cosine similarity (all-MiniLM-L6-v2 embeddings), lexical metrics (type-token ratio), and calibration bias. Reliability was assessed through repeated runs and bootstrap/permutation tests.

## Key Results
- LLM-generated think-alouds were more coherent, longer, and less lexically diverse than human think-alouds, especially under extended context.
- Model predictions systematically overestimated learner success, with calibration bias of 0.129 (simple) and 0.229 (extended).
- Extended context improved semantic similarity (+0.02) but worsened over-coherence and miscalibration.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs produce over-coherent reasoning because their training data over-represents expert-like solutions, creating an "expert blind spot" when simulating novices.
- **Mechanism:** Models optimize for fluent, coherent completions learned from polished explanations; without exposure to fragmented learner reasoning, they reconstruct solution narratives rather than extend novice cognition.
- **Core assumption:** Training corpora contain disproportionately more expert-authored explanations than authentic novice think-aloud data.
- **Evidence anchors:**
  - [abstract] "We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving."
  - [Page 6] "Like human experts, LLMs reconstruct polished solution paths that omit hesitation, misconception, and partial understanding."
  - [corpus] Weak direct evidence; related paper "THiNK: Can Large Language Models Think-aloud?" evaluates LLM higher-order thinking but does not address training data composition.
- **Break condition:** If fine-tuning on authentic novice think-aloud corpora reduces over-coherence (hypothesized but untested in this paper).

### Mechanism 2
- **Claim:** Extended context amplifies over-coherence and verbosity by providing more material for the model to resolve into expert narratives.
- **Mechanism:** Richer context (problem statement, prior inputs, tutor feedback) enables the model to infer problem structure and generate complete solutions, bypassing the exploratory reasoning humans exhibit.
- **Core assumption:** Models use context to optimize for semantic continuity rather than cognitive fidelity.
- **Evidence anchors:**
  - [Page 4] "In the simple condition, model continuations were much more aligned with prior context than human continuations (Δ= -0.22, p < .001). This effect was larger in the extended context condition (Δ= -0.44, p < .001)."
  - [Page 5] "Model-generated explanations are consistently longer, less lexically diverse, and syntactically more elaborate than learner think-alouds, with these differences most pronounced under complex prompting."
  - [corpus] No direct corpus support for this mechanism.
- **Break condition:** If constrained decoding (e.g., token limits, forced hesitation markers) attenuates the effect—untested.

### Mechanism 3
- **Claim:** LLMs systematically overestimate learner success because they lack access to learner-internal cognitive and affective states that predict failure.
- **Mechanism:** Predictions rely on problem features and surface-level context without modeling working-memory load, confusion, or self-doubt—factors that cause novices to fail steps that appear solvable.
- **Core assumption:** Calibration would improve if models had access to learner affect, prior knowledge, or real-time cognitive load measures.
- **Evidence anchors:**
  - [Page 5] "For probabilistic predictions, the model systematically overestimated learner performance in both contexts (positive calibration bias), with mean errors of 0.129 in the simple condition and 0.229 in the complex condition."
  - [Page 6] "LLMs do not posses experienced affective or motivational states... emotional language in LLM outputs reflects learned linguistic patterns rather than experienced regulation."
  - [corpus] Weak support; "Student Engagement with GenAI's Tutoring Feedback" examines student perceptions but not model calibration.
- **Break condition:** If incorporating logged learner behaviors (hint requests, time-on-task, error patterns) into context improves calibration—partially tested (extended context worsened calibration).

## Foundational Learning

- **Concept:** Think-aloud protocols
  - **Why needed here:** The paper uses concurrent think-alouds as the ground truth for novice reasoning; understanding their properties (fragmented, real-time, minimally reconstructed) is essential to interpret the fidelity metrics.
  - **Quick check question:** Why are concurrent think-alouds preferred over retrospective reports for studying metacognition?

- **Concept:** Expert blind spot (curse of expertise)
  - **Why needed here:** The central hypothesis is that LLMs exhibit the same expert blind spot as human experts—producing coherent explanations that overlook novice difficulties.
  - **Quick check question:** What are two hallmarks of expert blind spot when an expert explains a concept to a novice?

- **Concept:** Metacognitive calibration
  - **Why needed here:** RQ2 evaluates whether LLMs can predict learner success; calibration bias measures how well predicted probabilities match observed outcomes.
  - **Quick check question:** If a model predicts 80% success but learners succeed only 50% of the time, what is the calibration bias?

## Architecture Onboarding

- **Component map:** Dataset -> Prompt construction (simple/extended) -> GPT-4.1 generation -> Embedding computation -> Similarity and calibration metrics
- **Critical path:**
  1. Load step-level think-aloud dataset with correctness labels
  2. For each step, construct prompt under simple or extended condition
  3. Generate continuation + prediction (run twice for reliability check)
  4. Embed human and model utterances; compute similarity
  5. Compare predicted vs. actual correctness; compute calibration bias and discrimination

- **Design tradeoffs:**
  - Simple vs. extended context: extended improves semantic similarity (+0.02) but worsens over-coherence and calibration (higher bias)
  - Probabilistic vs. binary predictions: probabilistic shows higher reliability but larger calibration error; binary reduces bias magnitude but retains discrimination issues
  - Assumption: Neither output format reliably tracks actual learner success

- **Failure signatures:**
  - High similarity but expert-like continuations (fluent but non-novelty-matched)
  - Positive calibration bias with low discrimination (confident but uninformative predictions)
  - Extended context worsening both over-coherence and overestimation

- **First 3 experiments:**
  1. Replicate with constrained decoding (max 20 tokens, forced pause markers) to test whether limiting output reduces over-coherence.
  2. Fine-tune a smaller model on authentic novice think-aloud data and compare fidelity metrics to off-the-shelf GPT-4.1.
  3. Augment extended context with learner affect signals (e.g., frustration indicators from prior utterances) and measure calibration change.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the over-coherence and miscalibration of LLM-generated think-alouds generalize beyond chemistry tutoring to other STEM domains or narrative-based problem solving?
- **Basis in paper:** [explicit] "The extent to which over-coherence and miscalibration generalize to other domains and instructional contexts remains an open question."
- **Why unresolved:** The study analyzed only a single chemistry tutoring dataset, leaving domain generality untested.
- **What evidence would resolve it:** Replication of the evaluation framework across multiple domains (e.g., physics, programming, reading comprehension) with similar step-level think-aloud benchmarks.

### Open Question 2
- **Question:** Can fine-tuning LLMs on authentic novice think-aloud data reduce over-coherence and improve metacognitive calibration?
- **Basis in paper:** [explicit] "Achieving this fidelity will likely require fine-tuning on authentic think-aloud data and validation against human think-aloud protocols."
- **Why unresolved:** The study evaluated off-the-shelf models without pedagogical fine-tuning, characterizing current deployment rather than theoretical limits.
- **What evidence would resolve it:** A controlled comparison of pre- vs. post-fine-tuning model behavior on think-aloud fidelity and step-level prediction calibration.

### Open Question 3
- **Question:** Would incorporating explicit cognitive-load constraints (e.g., token limits simulating working memory) produce more faithful novice-like verbalizations?
- **Basis in paper:** [explicit] "Faithful simulation depends on grounding generation in empirically documented novice knowledge gaps... This includes incorporating realistic cognitive-load limits that yield fragmented and imprecise verbalizations under difficulty."
- **Why unresolved:** Current LLMs lack architectural constraints that produce the fragmentation seen in human think-alouds under cognitive load.
- **What evidence would resolve it:** Experiments comparing standard LLM outputs to outputs generated under simulated working-memory constraints, evaluated against human utterance patterns.

## Limitations

- Exact prompt templates and GPT-4.1 API parameters (temperature, max_tokens, top_p) were not disclosed, limiting exact replication.
- No fine-tuning experiments to test whether reduced over-coherence is achievable with novice-specific data.
- Limited exploration of model architecture choices (e.g., constrained decoding) to mitigate verbosity and overconfidence.

## Confidence

- **High:** LLMs produce over-coherent and verbose continuations compared to human think-alouds; calibration bias exists in learner success predictions; extended context worsens over-coherence and overestimation.
- **Medium:** Attribution of over-coherence to expert blind spot and lack of affective/working-memory representations in training data; effectiveness of current framework for future adaptive system design.
- **Low:** Unproven hypothesis that fine-tuning on authentic novice think-alouds will reduce over-coherence; untested efficacy of constrained decoding or affect-enriched context in improving calibration.

## Next Checks

1. Replicate the study using constrained decoding (max 20 tokens, forced hesitation markers) to assess whether limiting output reduces over-coherence and verbosity.
2. Fine-tune a smaller language model on authentic novice think-aloud data and compare fidelity metrics to the off-the-shelf GPT-4.1 baseline.
3. Augment extended context with logged learner behaviors (e.g., hint requests, time-on-task, prior error patterns) and measure changes in calibration bias and discrimination.