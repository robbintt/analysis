---
ver: rpa2
title: Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations
arxiv_id: '2505.23846'
source_url: https://arxiv.org/abs/2505.23846
tags:
- agent
- agents
- simulation
- language
- non-ai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a parallel discrete event simulation (PDES)
  based methodology to couple multiple AI and non-AI agents for collaborative problem-solving.
  The approach breaks complex problems into structured sub-tasks, where AI agents
  (using small language models) handle reasoning and decision-making while non-AI
  agents perform verification and mathematical calculations.
---

# Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations

## Quick Facts
- arXiv ID: 2505.23846
- Source URL: https://arxiv.org/abs/2505.23846
- Authors: Atanu Barai; Stephan Eidenbenz; Nandakishore Santhi
- Reference count: 40
- Key outcome: Coupled AI/non-AI agents achieve 68% accuracy vs <23% for standalone models

## Executive Summary
This paper presents a parallel discrete event simulation (PDES) methodology to couple multiple AI and non-AI agents for collaborative problem-solving. The approach breaks complex problems into structured sub-tasks where small language models (SLMs) handle reasoning and decision-making while non-AI agents perform verification and mathematical calculations. Using the open-source Simian PDES engine, agents interact through scheduled events, enabling parallel execution across multiple compute nodes via MPI. The system constrains AI outputs by providing predefined choices and employs non-AI agents as unbiased auditors to verify responses.

## Method Summary
The method uses Simian PDES engine with MPI to coordinate AI (SLM-based) and non-AI agents as discrete entities. Each agent is implemented as a Simian entity with timestamped event queues. AI agents solve constrained sub-tasks using quantized GGUF models (Qwen2.5-7B-Instruct-f16, Llama-3.1-8B-Instruct-Q8_0, Mistral-Nemo-12B-Q8_0) with temperature 0.1-0.5, while non-AI agents verify outputs and handle mathematical calculations. Problems are decomposed into structured steps (geometric median, selection sort, digit-wise multiplication, BFS traversal) with events scheduled via reqService. Verification logic checks AI outputs against domain rules, with non-AI agents correcting violations.

## Key Results
- Coupled approach achieves 68% accuracy versus less than 23% for standalone AI models
- Per-model accuracy: Llama3 reaches 77% vs 26% (vanilla), Mistral-Nemo achieves 60% vs 20%
- System scales to hundreds of agents across multiple compute nodes
- Strong scaling demonstrated with 2-6 MPI ranks showing improved token throughput

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained choice sets improve SLM accuracy on structured tasks.
- **Mechanism:** Providing predefined multiple-choice options limits the SLM's solution space, reducing hallucination tendency and preventing "wander-off into irrelevant contexts." The AI agent selects from finite options rather than generating unconstrained outputs.
- **Core assumption:** SLMs are better at selecting among valid options than generating correct answers from scratch for structured/algorithmic tasks.
- **Evidence anchors:** [abstract] "constraining and ensuring correctness of the output... providing a set of multiple choices to the AI agents"; [Section 4.2] "considerably constrain an SLM's solution space â€“ this helps reduce its tendency to hallucinate or wander-off into irrelevant contexts"
- **Break condition:** If the correct answer is not among the provided choices, or if the task requires creative/open-ended responses, this mechanism fails.

### Mechanism 2
- **Claim:** Non-AI agents acting as auditors increase trustworthiness by verifying AI outputs.
- **Mechanism:** Non-AI agents (deterministic, rule-based systems) verify each SLM action against domain rules. If an SLM response violates constraints (e.g., exceeding maximum speed in geometry problem), the non-AI agent either requests correction or directly fixes the error.
- **Core assumption:** Verification logic can be codified deterministically for the target domain.
- **Evidence anchors:** [abstract] "non-AI agents act as unbiased auditors, verifying each action by the AI agents"; [Section 4.3.1] "the non-AI agent verifies whether the new position provided by the SLM follows maximum speed guideline. If not, it sets the correct coordinates"
- **Break condition:** When verification requires subjective judgment or domain rules cannot be formally specified, non-AI auditing becomes inadequate.

### Mechanism 3
- **Claim:** PDES framework enables scalable, causally-consistent multi-agent coordination.
- **Mechanism:** Each AI/non-AI agent is implemented as a Simian entity with timestamped event queues. The reqService method schedules future events, ensuring causal ordering. MPI distributes entities across compute nodes for parallel execution.
- **Core assumption:** Sub-tasks can be decomposed into discrete events with well-defined temporal dependencies.
- **Evidence anchors:** [abstract] "each agent considered as an entity in the PDES framework and responding to prior requests from other agents"; [Section 3.1] "PDES tries to execute the events in parallel avoiding causality error"; [Section 5.3] Figures 9-10 show scaling with MPI ranks up to 6 agents
- **Break condition:** If events have complex, poorly-understood dependencies or require real-time synchronization beyond simulation time semantics.

## Foundational Learning

- **Concept: Parallel Discrete Event Simulation (PDES)**
  - Why needed here: Core coordination mechanism; understanding event scheduling, timestamps, and causality is essential for debugging agent interactions.
  - Quick check question: If event A (timestamp 10) sends a message that triggers event B, what timestamp must B have to maintain causality?

- **Concept: Language Model Inference Parameters**
  - Why needed here: Temperature, top_k, and top_p directly affect SLM output randomness and deterministic behavior in the coupled system.
  - Quick check question: What happens to agent trajectory consistency when temperature increases from 0.1 to 0.5 (see Figure 7)?

- **Concept: MPI Process Coordination**
  - Why needed here: Understanding how entities are distributed across ranks helps diagnose scaling bottlenecks and memory constraints.
  - Quick check question: Why does the paper use CPU-only inference for scaling experiments instead of GPU inference?

## Architecture Onboarding

- **Component map:**
  - Simian engine -> manages simulation clock, event queues, MPI distribution
  - Entity subclasses -> SLMAgent (loads model, handles reasoning), NonAIAgent (verification, calculations)
  - reqService(event_name, data, target_entity, target_num) -> schedules events
  - llama-cpp-python -> local SLM inference with GGUF quantized models

- **Critical path:**
  1. Initialize Simian engine with MPI configuration
  2. Add entities via addEntity() with unique names/numbers
  3. Schedule initial event with schedService()
  4. Call simianEngine.run() -> processes events chronologically until queue empty

- **Design tradeoffs:**
  - Quantization (8-bit vs 16-bit) trades accuracy for memory footprint
  - Fewer MPI ranks = simpler debugging but slower execution
  - Verifying every SLM output increases latency but improves reliability
  - Assumption: Paper does not quantify latency overhead of verification

- **Failure signatures:**
  - GPU memory overflow when multiple ranks load models on same node
  - Causality errors if event timestamps are misordered
  - SLM parsing failures if response format doesn't match expected regex
  - Infinite simulation loops if termination condition never triggers

- **First 3 experiments:**
  1. Run the geometry gathering problem with 2-3 agents at temperature 0.1; verify non-AI agent catches speed violations by disabling verification and comparing trajectories.
  2. Replicate the sorting experiment with a 5-element array; compare accuracy between PDES approach and zero-shot prompting using the same model.
  3. Profile token throughput (tokens/minute) with 2, 4, and 6 MPI ranks to confirm scaling behavior matches Figure 10 trends.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text provided.

## Limitations
- The approach depends heavily on the ability to decompose complex problems into structured sub-tasks with clear verification rules, which may not generalize to open-ended or creative domains.
- Scaling results are based on CPU-only inference (503GB RAM for 6 MPI ranks), which may not reflect practical deployment scenarios where GPU acceleration is preferred.
- The method's effectiveness relies on providing correct multiple-choice options to constrain AI output, but if the correct answer isn't among the choices, the approach fails.

## Confidence
- **High confidence**: PDES framework enables scalable coordination of multiple agents (supported by scaling experiments and established PDES literature)
- **Medium confidence**: Constrained choice sets improve SLM accuracy on structured tasks (mechanism supported by abstract and methodology, but limited empirical validation across diverse problem types)
- **Medium confidence**: Non-AI verification increases trustworthiness (demonstrated in geometry problem, but verification logic for other domains not fully specified)

## Next Checks
1. Run the geometry gathering problem with 2-3 agents at temperature 0.1; verify non-AI agent catches speed violations by disabling verification and comparing trajectories
2. Replicate the sorting experiment with a 5-element array; compare accuracy between PDES approach and zero-shot prompting using the same model
3. Profile token throughput (tokens/minute) with 2, 4, and 6 MPI ranks to confirm scaling behavior matches Figure 10 trends