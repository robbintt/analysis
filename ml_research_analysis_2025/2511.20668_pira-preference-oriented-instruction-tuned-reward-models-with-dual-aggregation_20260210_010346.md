---
ver: rpa2
title: 'PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation'
arxiv_id: '2511.20668'
source_url: https://arxiv.org/abs/2511.20668
tags:
- reward
- pira
- arxiv
- learning
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PIRA addresses reward overoptimization in RLHF by reformulating
  preference data into explicit instruction-task prompts and averaging rewards across
  multiple instructions and stochastic value-head outputs. This dual aggregation reduces
  task-specific bias and stabilizes reward estimation.
---

# PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation

## Quick Facts
- **arXiv ID**: 2511.20668
- **Source URL**: https://arxiv.org/abs/2511.20668
- **Reference count**: 27
- **Primary result**: PIRA improves reward model accuracy by 4.2% on HH-cleaned dataset and mitigates reward overoptimization during RLHF

## Executive Summary
PIRA addresses reward overoptimization in RLHF by reformulating preference data into explicit instruction-task prompts and averaging rewards across multiple instructions and stochastic value-head outputs. This dual aggregation reduces task-specific bias and stabilizes reward estimation. Experiments on multiple public datasets show PIRA consistently outperforms baselines and mitigates reward hacking, as evidenced by bounded KL divergence and monotonic improvement in gold reward scores during PPO training.

## Method Summary
PIRA reformulates preference data by prepending evaluation instructions to question-answer pairs, enabling the reward model to leverage instruction-following capabilities. During training, it samples from a set of 10 human-reviewed preference instructions and applies Bradley-Terry loss. At inference, it aggregates rewards by averaging across multiple instructions (K=6) and multiple stochastic value-head passes with variable dropout rates (M=12), achieving variance reduction with minimal computational overhead.

## Key Results
- PIRA achieves +4.2% accuracy improvement on HH-cleaned dataset compared to baselines
- Standard deviation of predictions drops from 1.6 to 0.7 with instruction averaging (K=4 to K=6)
- During PPO training, PIRA maintains bounded KL divergence while baseline reward hacking causes gold reward decline

## Why This Works (Mechanism)

### Mechanism 1
- Explicit task instructions improve reward model data efficiency by leveraging pre-existing instruction-following capabilities
- Prepending preference-task instructions makes the scoring task explicit, allowing the model to activate pretrained instruction-following representations
- Assumes backbone LLM has sufficient instruction-following capability from pretraining to interpret evaluation rubrics meaningfully

### Mechanism 2
- Averaging rewards across multiple instruction formulations reduces estimator variance and prompt-specific bias
- Each instruction provides different evaluative perspective; averaging cancels out idiosyncratic biases
- Assumes instructions are sufficiently diverse but converge on same underlying preference signal

### Mechanism 3
- Stochastic value-head averaging with variable dropout rates stabilizes reward estimation at minimal computational cost
- Lightweight value head undergoes multiple forward passes with dropout rates ~Uniform(0.1, 0.4)
- Assumes uncertainty in value head is meaningful signal that can be reduced through averaging

## Foundational Learning

- **Bradley-Terry preference modeling**: Ground why reward differences matter more than absolute scores in BT training
  - Quick check: Can you explain why BT training produces scalar reward reflecting log-odds of preference?

- **Monte Carlo dropout as Bayesian approximation**: Understand why dropout at inference approximates model uncertainty
  - Quick check: Why does running multiple forward passes with dropout approximate model uncertainty?

- **Reward overoptimization / reward hacking**: Understand the core problem PIRA addresses in RLHF
  - Quick check: In Figure 1, why does baseline's gold reward peak and decline while proxy reward keeps rising?

## Architecture Onboarding

- **Component map**: Token sequence → Backbone h_θ → Representation u → Value head g_ψ → Scalar reward
- **Critical path**: 1) Data prep: Convert (question, y_c, y_r) → (instruction + question + response) format 2) Training: Sample t ~ T per instance; BT loss with differential LRs 3) Inference: Single backbone pass → M stochastic value-head passes per instruction → average over K×M total samples
- **Design tradeoffs**: K vs latency (K=6 best stability but 6x cost); M vs cost (M=12 adds 7% latency); Instruction design (holistic vs dimension-specific)
- **Failure signatures**: KL divergence spikes during PPO (reward hacking); Gold reward declining while proxy rises (overoptimization); High standard deviation across seeds (unstable training)
- **First 3 experiments**: 1) Ablation on instruction reformulation alone (K=1, M=1) on HH-cleaned 2) Scaling M with fixed K=1 (M∈{1,4,8,12}) 3) PPO integration test on Alpaca-farm (plot KL divergence and gold reward over 2500 steps)

## Open Questions the Paper Calls Out
- Scalability to models beyond 13B parameters remains uninvestigated
- Performance degradation on long responses (UltraFeedback) due to instruction prompt effectiveness
- Substantial computational overhead from instruction-set averaging (sixfold increase)

## Limitations
- Value head architecture specifications are missing (MLP depth, hidden dimensions)
- Limited external validation of multi-instruction averaging and variable dropout rate approaches
- Instruction prompts less effective for complex, long-form responses

## Confidence
- **High**: Dual aggregation reduces variance and stabilizes reward estimation
- **Medium**: Instruction reformulation improves data efficiency via pre-existing instruction-following capabilities  
- **Medium**: Reward overoptimization mitigation (attribution to specific mechanisms needs more rigorous ablation)

## Next Checks
1. **Instruction design ablation**: Test PIRA with only holistic vs only dimension-specific instructions to quantify impact of instruction granularity
2. **Cross-dataset robustness**: Evaluate PIRA on datasets not seen during training to test generalization of instruction formulations
3. **Value head architecture scaling**: Test whether increasing value head capacity improves effectiveness of stochastic averaging for larger backbone models