---
ver: rpa2
title: Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent
  Virtual Agents
arxiv_id: '2507.22352'
source_url: https://arxiv.org/abs/2507.22352
tags:
- uni00000048
- agents
- response
- uni00000003
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the effects of response delays and conversational
  fillers on user experience with LLM-powered virtual agents in VR. Participants interacted
  with nine agents across three scenarios under three delay levels (1.5s, 4.0s, 6.5s)
  and three filler types (none, artificial wait indicator, natural conversational
  filler).
---

# Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents

## Quick Facts
- arXiv ID: 2507.22352
- Source URL: https://arxiv.org/abs/2507.22352
- Reference count: 40
- Key outcome: Latency above 4 seconds significantly degrades user experience, while natural conversational fillers improve perceived response time

## Executive Summary
This study investigates how response delays and conversational fillers affect user experience with LLM-powered virtual agents in VR. Participants interacted with nine agents across three scenarios under three delay levels (1.5s, 4.0s, 6.5s) and three filler types (none, artificial wait indicator, natural conversational filler). The research reveals that while latency above 4 seconds significantly degrades perceived response time and broader perception dimensions, natural conversational fillers can effectively mitigate the negative effects of delay, particularly at medium and high latency levels.

## Method Summary
The study employed a 3x3 factorial design with 18 conditions, where 18 participants (21-32 years old) engaged in free-form voice conversations with nine different LLM-powered virtual agents across three scenarios (store, hotel, museum). The system architecture used Unity (Meta Quest Pro) connected to a local server running FasterWhisper (ASR), Llama 3.1-8B (LLM), and Edge-TTS (text-to-speech). Artificial delays were injected to achieve target latencies of 1.5s, 4.0s, and 6.5s, while filler types included no filler, artificial wait indicators (rotating icon + sound), and natural conversational fillers (gestures + voice lines like "Hmm..."). User experience was measured through post-interaction surveys on six dimensions: Response Time, Engagement, Good Impression, Discomfort, Competence, and Willingness to Interact Again.

## Key Results
- Latency above 4 seconds significantly degraded perceived response time and broader perception dimensions including engagement, good impression, competence, and willingness to interact again
- Natural conversational fillers significantly improved perceived response time at medium (4.0s) and high (6.5s) delay levels, while artificial fillers did not
- Participants strongly preferred agents with faster responses, but conversational fillers increased tolerance for delayed responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Natural conversational fillers (e.g., "hmm," gestures) appear to reduce the perceived duration of a delay, whereas artificial indicators (loading icons) do not.
- **Mechanism:** Social cues maintain conversational turn-taking expectations. By signaling that the agent is "thinking" (anthropomorphism), the filler bridges the gap in the conversation flow, preventing the user from interpreting silence as a system failure.
- **Core assumption:** Users apply social norms to human-like avatars; silence is interpreted socially rather than technically.
- **Evidence anchors:**
  - Natural conversational fillers significantly improved participants' ratings on Response Time at Medium (4.0s) and High (6.5s) levels
  - Artificial fillers were not significantly different from the None condition
  - Related work reinforces that symbolic fillers are effective in VR

### Mechanism 2
- **Claim:** Latency tolerance has an upper bound; exceeding approximately 4 seconds significantly degrades user experience regardless of the filler type.
- **Mechanism:** The "4-second rule" suggests that beyond this threshold, silence shifts from being a conversational pause to a communication breakdown.
- **Core assumption:** The cognitive load of waiting increases non-linearly after the 4-second mark.
- **Evidence anchors:**
  - Post-hoc pairwise comparisons revealed significant differences in Response Time across all conditions
  - Supporting H1a: latency above 4 seconds significantly degraded user experience

### Mechanism 3
- **Claim:** Artificial wait indicators (spinners, processing sounds) fail to improve perceived latency because they signal a machine state rather than a social state.
- **Mechanism:** A spinning icon confirms the system is "working" but does not satisfy the social expectation of "listening" or "thinking" inherent in a humanoid avatar.
- **Core assumption:** Embodied avatars create social expectations that traditional GUI loading indicators cannot satisfy.
- **Evidence anchors:**
  - Artificial wait indicators did not significantly affect user experience
  - Participant quotes indicate the spinning UI felt "off" or "inhuman" compared to the avatar's form factor

## Foundational Learning

**Concept: Conversational Turn-Taking & Silence**
- **Why needed here:** To understand why a 4-second delay is a critical threshold and why "dead air" feels different from "thinking time."
- **Quick check question:** Does a 6-second silence in a human conversation usually indicate smooth flow or a breakdown?

**Concept: Anthropomorphism in HCI**
- **Why needed here:** To explain why users expect human-like behaviors (gestures, vocal fillers) from realistic avatars and reject machine-like cues (loading bars).
- **Quick check question:** Why might a user forgive a delay if an avatar looks pensive, but not if a loading bar appears?

**Concept: System Response Time (SRT) Components**
- **Why needed here:** To diagnose where latency is introduced (ASR, LLM, TTS) and what realistically can be optimized.
- **Quick check question:** Which component in the pipeline (ASR -> LLM -> TTS) typically contributes the most variability to the response time?

## Architecture Onboarding

**Component map:**
Microphone (Unity) -> ASR (FasterWhisper medium) -> Conversation Handler -> LLM (Llama 3.1-8b) -> TTS (Edge-TTS) -> Audio Player + Lip Sync (OVR Lip Sync) + Filler Trigger Logic

**Critical path:** The "Time to First Byte" of the audio response. The system measures ~1.5s base latency. The critical implementation is the Filler Trigger, which must fire immediately upon user input cessation to bridge the gap to the LLM response.

**Design tradeoffs:**
- Model Size vs. Speed: A larger LLM (higher quality) increases SRT, potentially pushing the system over the 4s threshold
- Filler Realism vs. Control: Pre-recorded natural fillers are high quality but repetitive; generative fillers add latency

**Failure signatures:**
- "The Stare": Agent remains in "Attentive Idle" during long delays (Natural filler failed to trigger)
- "Robot Mode": Agent speaks instantly with no buffering, or uses a loading icon, breaking immersion
- "Interrupted Thought": Filler animation is cut off abruptly when the LLM response arrives early

**First 3 experiments:**
1. **Latency Baseline:** Instrument the pipeline to measure ASR, LLM, and TTS times independently to identify the primary bottleneck
2. **Filler Timing:** Test the synchronization of the filler trigger. Does the filler start immediately after the user stops speaking?
3. **Threshold Testing:** Deploy the agent with a variable delay (1s-6s) and subjective user ratings to calibrate the specific tolerance threshold for your target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can NLP-based models predict and execute context-appropriate fillers in under 300ms or before the user finishes speaking?
- Basis in paper: The authors state that future work should address the challenge of designing fast models that process conversation history within 300ms or predict needs during user speech.
- Why unresolved: Current system latency forces fillers to occur after user input, whereas human interlocutors often signal attention during the partner's turn.
- What evidence would resolve it: A study evaluating user perception of agents utilizing predictive fillers triggered during user speech compared to post-speech fillers.

### Open Question 2
- Question: How does time pressure or high-stakes urgency affect users' tolerance for latency and the efficacy of conversational fillers?
- Basis in paper: The authors note that time-sensitive settings may compress tolerance for delay and suggest evaluating fillers in contexts like a "desert survival task" where cognitive load is high.
- Why unresolved: The present study allowed participants to proceed at their own pace, avoiding time pressure that might exacerbate the negative effects of delay.
- What evidence would resolve it: Experimental results from a study measuring task performance and stress in a time-constrained scenario with variable latency and filler types.

### Open Question 3
- Question: Can "communicative" or "socially expressive" artificial wait indicators (e.g., icons representing distinct processing stages) effectively mitigate perceived latency?
- Basis in paper: While standard artificial fillers failed, the authors suggest future studies should explore indicators that represent distinct stages of response generation rather than generic loading icons.
- Why unresolved: The study found standard artificial indicators (loading icons/sounds) ineffective, but the specific design of "communicative" indicators was not tested.
- What evidence would resolve it: A comparative analysis of user perceived response time between standard loading indicators and indicators signaling specific states like "listening" or "reasoning."

## Limitations

- The controlled laboratory setting in VR may not fully capture real-world conversational dynamics and natural latency variability
- The study's focus on a specific demographic (21-32 years old, primarily students) and American-accented English may limit generalizability to broader populations
- The 36-second interaction duration per agent may be insufficient to observe longer-term effects of latency on user engagement and trust

## Confidence

- **High Confidence:** The finding that latency above 4 seconds significantly degrades user experience across multiple perception dimensions is well-supported by consistent statistical results and aligned with established HCI research
- **Medium Confidence:** The differential effectiveness of natural versus artificial fillers is moderately supported, though effect sizes for artificial fillers are small and near significance thresholds
- **Low Confidence:** The generalizability of the 4-second threshold to all conversational contexts and user populations remains uncertain

## Next Checks

1. **Real-World Latency Profiling:** Instrument a deployed system to measure actual ASR, LLM, and TTS latencies under varying network conditions and computational loads to validate whether artificial delay injection accurately represents real-world performance variability

2. **Cross-Cultural Replication:** Replicate the study with participants from diverse cultural backgrounds and age groups to test whether the 4-second threshold and filler effectiveness generalize beyond the current demographic sample

3. **Context-Dependent Threshold Testing:** Conduct experiments varying interaction content complexity and task stakes to determine whether the 4-second tolerance threshold varies based on conversational context and user expectations