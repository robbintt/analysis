---
ver: rpa2
title: A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models
arxiv_id: '2508.17994'
source_url: https://arxiv.org/abs/2508.17994
tags:
- sentiment
- dataset
- aspect
- reviews
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multilingual, manually annotated dataset
  of 10,814 customer reviews from brick-and-mortar retail stores, labeled with eight
  aspect categories and their sentiment for aspect-based sentiment analysis. The dataset
  covers nine European countries and 45 languages.
---

# A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models

## Quick Facts
- arXiv ID: 2508.17994
- Source URL: https://arxiv.org/abs/2508.17994
- Reference count: 20
- Primary result: Multilingual retail review dataset with 10,814 reviews across 9 European countries and 45 languages, labeled for 8 aspect categories with sentiment analysis, establishing baseline LLM performance at over 85% accuracy

## Executive Summary
This paper introduces a multilingual, manually annotated dataset of 10,814 customer reviews from brick-and-mortar retail stores, labeled with eight aspect categories and their sentiment for aspect-based sentiment analysis. The dataset covers nine European countries and 45 languages, enabling evaluation of large language models in retail-specific sentiment analysis tasks. The authors evaluate GPT-4 and LLaMA-3 on this dataset, establishing a baseline for LLM performance in aspect-based sentiment analysis. Results show both models achieving over 85% accuracy, with GPT-4 outperforming LLaMA-3 across all relevant metrics, demonstrating strong capability in identifying aspects and classifying sentiments in customer reviews.

## Method Summary
The authors created a retail review corpus by collecting 10,814 customer reviews from brick-and-mortar stores across nine European countries. Reviews were manually annotated with eight aspect categories (service, product, location, opening hours, parking, COVID-19, staff, prices) and corresponding sentiment labels. The dataset supports multilingual analysis across 45 languages. GPT-4 and LLaMA-3 were evaluated using different prompting strategies - GPT-4 employed chain-of-thought reasoning while LLaMA-3 used direct prompts. Model performance was assessed using accuracy metrics, with both models achieving over 85% accuracy in identifying aspects and classifying sentiments.

## Key Results
- GPT-4 and LLaMA-3 both achieved over 85% accuracy on aspect-based sentiment analysis
- GPT-4 outperformed LLaMA-3 across all relevant metrics in the evaluation
- The dataset covers 45 languages across nine European countries with 10,814 manually annotated reviews

## Why This Works (Mechanism)
Large language models demonstrate strong performance in aspect-based sentiment analysis due to their ability to understand contextual relationships between review text and aspect categories, leveraging pretraining on diverse text corpora to recognize sentiment patterns across multiple languages and retail-specific domains.

## Foundational Learning
- **Aspect-Based Sentiment Analysis**: Why needed - to identify specific features of products/services mentioned in reviews; Quick check - can the model distinguish between positive service comments and negative product comments
- **Multilingual NLP**: Why needed - retail reviews span diverse languages requiring cross-lingual understanding; Quick check - model accuracy across different language groups
- **Chain-of-Thought Prompting**: Why needed - improves reasoning by breaking down complex tasks into steps; Quick check - compare performance with and without reasoning steps
- **Retail Domain Knowledge**: Why needed - specific vocabulary and context unique to retail environments; Quick check - accuracy on retail-specific vs general sentiment tasks
- **Manual Annotation Quality**: Why needed - ensures reliable ground truth for model evaluation; Quick check - inter-annotator agreement statistics
- **Cross-Validation**: Why needed - assesses model robustness and generalization; Quick check - performance across different data splits

## Architecture Onboarding

Component map: Dataset collection -> Manual annotation -> LLM evaluation -> Performance comparison

Critical path: Data collection → Annotation → Model evaluation → Result analysis

Design tradeoffs: GPT-4 uses chain-of-thought reasoning (better performance but more compute) vs LLaMA-3 direct prompting (faster but potentially less accurate)

Failure signatures: Poor performance on minority aspects, language-specific degradation, confusion between similar aspect categories

First experiments:
1. Evaluate model performance on individual aspect categories to identify weaknesses
2. Test cross-lingual consistency by evaluating on reviews from different language groups
3. Compare chain-of-thought vs direct prompting effects on both models using identical prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focuses exclusively on brick-and-mortar retail reviews, creating potential sampling bias
- Reported accuracy lacks cross-validation details and comprehensive error analysis
- Comparison between models uses different prompting strategies, potentially inflating performance differences

## Confidence
- LLM performance claims: Medium - Strong baseline results but limited methodological transparency
- Dataset representativeness: Low - Narrow domain focus with unknown generalizability
- Cross-lingual capability: Low - Claims of 45-language coverage not validated across all languages

## Next Checks
1. Conduct cross-domain testing by evaluating models on e-commerce and service industry reviews to assess generalizability beyond brick-and-mortar retail
2. Implement comprehensive error analysis including confusion matrices for each aspect category and inter-annotator agreement statistics for the gold standard
3. Perform controlled experiments with identical prompting strategies across both LLMs to isolate model capability differences from prompt effects