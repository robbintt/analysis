---
ver: rpa2
title: 'MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving'
arxiv_id: '2601.22930'
source_url: https://arxiv.org/abs/2601.22930
tags:
- multi-turn
- data
- driving
- trajectory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MTDrive introduces a multi-turn framework for autonomous driving
  that enables iterative trajectory refinement through environmental feedback. The
  core innovation is mtGRPO, a reinforcement learning algorithm that addresses sparse
  reward challenges by computing relative advantages across turns, allowing fine-grained
  credit assignment for each refinement step.
---

# MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving

## Quick Facts
- arXiv ID: 2601.22930
- Source URL: https://arxiv.org/abs/2601.22930
- Reference count: 40
- Primary result: Achieves 96.2 PDMS with ground-truth perception, 91.1 with kinematic modeling

## Executive Summary
MTDrive introduces a multi-turn reinforcement learning framework for autonomous driving that enables iterative trajectory refinement through environmental feedback. The core innovation is mtGRPO, a reinforcement learning algorithm that addresses sparse reward challenges by computing relative advantages across turns, allowing fine-grained credit assignment for each refinement step. The approach is supported by a novel interactive trajectory understanding dataset constructed from closed-loop simulation, featuring counterfactual questioning to identify critical obstacles and dedicated data to activate self-reflection capabilities. A system-level optimization achieving 2.5x training throughput addresses computational bottlenecks from high-resolution images and multi-turn sequences.

## Method Summary
MTDrive builds on Qwen2.5-VL-7B-Instruct, training it for multi-turn trajectory refinement in autonomous driving using a combination of supervised fine-tuning and reinforcement learning. The method employs mtGRPO, which extends GRPO by computing per-turn advantages to handle sparse rewards in multi-turn interactions. The training pipeline uses a PDM Agent (implemented in NAVSIM) to evaluate trajectories and provide textual feedback on violations of safety metrics (NC, DAC, TTC, EP, Comfort). The approach includes an interactive trajectory understanding dataset with counterfactual questioning and a system-level optimization (IPSS/IPTC) that improves training throughput by 2.5x. The model generates 8-point trajectories over 4 seconds, refining them across up to 6 turns based on PDM feedback.

## Key Results
- Achieves 96.2 PDMS with ground-truth perception oracle
- Maintains 91.1 PDMS with realistic kinematic modeling
- Demonstrates superior performance in long-tail driving scenarios requiring iterative refinement
- Shows strong safety metrics including collision avoidance and drivable area compliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-turn advantage computation enables fine-grained credit assignment across refinement steps, improving learning efficiency in multi-turn trajectory optimization.
- Mechanism: mtGRPO computes rewards per turn using weighted PDM and format scores, then normalizes advantages within each turn across rollouts rather than aggregating to sequence-level. This isolates which refinement steps actually improved trajectory quality versus which introduced errors.
- Core assumption: Assumes that intermediate turns have measurable quality differences that can be captured by per-metric PDM scoring; assumes the PDM Agent feedback meaningfully correlates with safe driving outcomes.
- Evidence anchors: Abstract mentions "mtGRPO... mitigates reward sparsity by computing relative advantages across turns"; Section 3.3, Eq. 3 shows turn-level normalization; related work on multi-turn RL shows similar credit assignment challenges.

### Mechanism 2
- Claim: Iterative trajectory refinement via textual feedback activates self-correction capabilities in VLMs, improving performance on long-tail scenarios requiring multi-step reasoning.
- Mechanism: The model receives textualized PDM feedback (e.g., "Trajectory Point 1 violates DAC with object at coordinates...") and re-predicts. Counterfactual questioning in PDM understanding data trains the model to identify critical obstacles. Progressive refinement preserves valid trajectory points while correcting violations.
- Core assumption: Assumes VLMs can ground textual feedback about spatial coordinates in visual representations; assumes feedback format is interpretable without extensive fine-tuning.
- Evidence anchors: Abstract mentions "dedicated data to activate self-reflection capabilities"; Section 3.2 shows multi-turn data construction through iterative bootstrap; Section 4.4 shows qualitative examples of 3-turn refinement correcting violations.

### Mechanism 3
- Claim: Overlapping serialization with rollout generation and caching shared tensors reduces multimodal multi-turn RL training overhead by 2.5x.
- Mechanism: IPSS offloads serialization of completed rollouts to thread pools while remaining rollouts generate; IPTC caches tokenized sequences, attention masks, and visual embeddings in GPU memory across co-located modules, eliminating redundant deserialization.
- Core assumption: Assumes rollout completion times are sufficiently variable to enable overlap; assumes co-located modules share compatible tensor formats.
- Evidence anchors: Abstract mentions "achieving 2.5x training throughput"; Section 3.4 shows "per-step training time from ~1250s to ~490s... IPSS contributing ~1.5x and IPTC ~1.7x"; no external validation of these specific optimizations for multimodal RL.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: mtGRPO extends GRPO's group-based advantage normalization to per-turn computation. Without understanding baseline GRPO, the turn-level modification's purpose is unclear.
  - Quick check question: Given a batch of 8 rollouts for the same prompt, how does GRPO compute advantages differently from PPO?

- Concept: **Credit Assignment in Multi-step RL**
  - Why needed here: The core problem mtGRPO addresses is determining which turn's refinement contributed to improved PDM scores. Sparse rewards at sequence end make this ill-posed without per-turn decomposition.
  - Quick check question: In a 3-turn interaction where the final reward is high but turn 1 was poor, how would standard PPO versus mtGRPO update the turn-1 tokens?

- Concept: **PDM Score Components (NC, DAC, TTC, EP, Comfort)**
  - Why needed here: The PDM Agent generates feedback from these metrics; understanding what each measures is essential for interpreting feedback prompts and reward signals.
  - Quick check question: Which PDM components require ground-truth future trajectories and therefore cannot be used during training rollout?

## Architecture Onboarding

- Component map:
  - VLM Policy (Qwen2.5-VL-7B) -> PDM Agent (NAVSIM) -> Rollout Workers -> Actor/Reference Models -> Object Store + Tensor Cache

- Critical path:
  1. Sample batch from filtered RL dataset (hard scenarios with PDM violations or low scores)
  2. Generate G rollouts per prompt across N turns with PDM feedback loop
  3. Compute per-turn rewards via PDM scoring + format check
  4. Calculate turn-level advantages via group relative normalization
  5. Update policy via clipped objective with KL penalty

- Design tradeoffs:
  - Turn count (6 in paper): More turns enable finer refinement but increase credit assignment difficulty and compute
  - Reward weights (w_p=0.8, w_f=0.2): Higher PDM weight drives task performance; format weight prevents degradation
  - SFT data mix: Single-turn for basic capability, multi-turn for refinement, PDM understanding for feedback grounding

- Failure signatures:
  - Overfitting on small RL sets: Table 2 shows 7k hard samples cause score decline after 30 steps
  - Format degradation during RL: Without format score in reward, model may produce unparseable trajectories
  - Feedback grounding failure: If PDM understanding data is insufficient, model ignores or misinterprets coordinate-based feedback

- First 3 experiments:
  1. Validate mtGRPO vs. sequence-level GRPO: Run ablation matching Table 3 (ID 1 vs. ID 3) on a 5k subset; expect ~2 PDMS gap
  2. Test feedback grounding: Train with/without PDM understanding data; measure multi-turn refinement success rate on scenarios with initial DAC violations
  3. Profile throughput bottlenecks: Run single-GPU rollout with profiling; verify IPSS overlap occurs (check serialization overlap percentage in timeline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the VLM generate PDM feedback solely through its own reasoning without relying on external perception modules or ground-truth annotations?
- Basis in paper: Section 6 states: "A promising direction for future work is to integrate perception data into the VLM, enabling the model to generate PDM feedback solely based on its own reasoning."
- Why unresolved: Current approach depends on privileged perception inputs; the model cannot autonomously identify collision risks or drivable area violations from raw visual input alone.
- What evidence would resolve it: End-to-end experiments where the VLM predicts both trajectories and self-evaluates safety metrics using only camera images, matching or exceeding current oracle-free performance.

### Open Question 2
- Question: How does the distribution mismatch between mock multi-turn data and real agent inference affect model generalization?
- Basis in paper: Appendix B acknowledges that mock multi-turn data "is not entirely derived from real agent inference results, its distribution does not match real multi-turn reasoning," and only a subset is used.
- Why unresolved: Computational constraints prevented generating authentic multi-turn sequences beyond 3 turns, yet the model trains on synthetic stacked sequences whose fidelity is unknown.
- What evidence would resolve it: Ablation comparing models trained on authentic vs. mock multi-turn data, measuring PDMS degradation as mock data proportion increases.

### Open Question 3
- Question: Does mtGRPO generalize to learned reward agents or simulation environments beyond NAVSIM's rule-based PDM Agent?
- Basis in paper: Section 6 states: "Future work could explore learned reward agents or adapt the framework to agents in other simulation environments (e.g., CARLA, Waymax, AlpaSim)."
- Why unresolved: Current framework relies on deterministic, rule-based metrics (NC, DAC, TTC); learned agents may produce noisier or less interpretable feedback signals.
- What evidence would resolve it: Cross-environment transfer experiments applying mtGRPO with neural reward models on CARLA/Waymax, comparing convergence stability and final PDMS-equivalent scores.

### Open Question 4
- Question: What is the optimal number of refinement turns before diminishing returns or error accumulation outweighs benefits?
- Basis in paper: Experiments use max 6 turns without systematic analysis of turn-count effects; qualitative results show 3-turn success but no upper-bound investigation.
- Why unresolved: Multi-turn reasoning introduces computational overhead and potential feedback loop instability; the trade-off between refinement iterations and marginal PDMS gains remains unquantified.
- What evidence would resolve it: Sweep over {1,2,3,4,5,6,8,10} turns with per-turn PDMS tracking, identifying the knee point where additional turns yield <0.5 PDMS improvement.

## Limitations

- The mtGRPO algorithm's effectiveness depends heavily on the PDM Agent's ability to generate meaningful feedback, which may not generalize to learned reward agents or noisier environments.
- The 2.5x throughput improvement from system optimizations assumes specific workload characteristics that may not generalize across different hardware configurations or rollout distributions.
- The approach relies on ground-truth perception for optimal performance, with a 5-point PDMS gap between ground-truth and kinematic modeling scenarios.

## Confidence

- **High confidence**: The multi-turn trajectory refinement framework and dataset construction methodology are well-specified and reproducible
- **Medium confidence**: The mtGRPO algorithm's advantage computation and the 96.2 PDMS score with ground-truth perception, as these depend on specific implementation details of the PDM Agent
- **Low confidence**: The system-level optimization claims (2.5x throughput) without external validation or code release

## Next Checks

1. **Validate mtGRPO vs. sequence-level GRPO**: Run the ablation matching Table 3 (ID 1 vs. ID 3) on a 5k subset of the RL dataset; expect a ~2 PDMS gap to confirm turn-level advantage computation provides meaningful credit assignment.

2. **Test feedback grounding dependency**: Train two models—one with full PDM understanding data and one without—then measure multi-turn refinement success rate on scenarios with initial DAC violations; expect significant performance drop without PDM understanding data.

3. **Profile throughput bottleneck claims**: Run single-GPU rollout with profiling tools to verify IPSS overlap occurs (check serialization overlap percentage in timeline) and IPTC caching reduces memory transfers; confirm the ~1250s to ~490s per-step reduction is achievable.