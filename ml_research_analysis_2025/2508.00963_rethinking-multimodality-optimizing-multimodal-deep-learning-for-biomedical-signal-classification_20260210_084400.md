---
ver: rpa2
title: 'Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical
  Signal Classification'
arxiv_id: '2508.00963'
source_url: https://arxiv.org/abs/2508.00963
tags:
- features
- feature
- hybrid
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether adding more feature domains in
  multimodal deep learning for ECG classification improves performance or leads to
  diminishing returns. Five models were developed and evaluated: three unimodal (1D-CNN
  for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two
  multimodal (Hybrid 1: 1D-CNN + 2D-CNN; Hybrid 2: 1D-CNN + 2D-CNN + Transformer).'
---

# Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification

## Quick Facts
- arXiv ID: 2508.00963
- Source URL: https://arxiv.org/abs/2508.00963
- Reference count: 40
- Primary result: Optimal multimodal deep learning performance depends on fusing complementary feature domains, not simply adding more domains.

## Executive Summary
This study investigated whether adding more feature domains in multimodal deep learning improves ECG classification performance or leads to diminishing returns. Five models were developed: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1: 1D-CNN + 2D-CNN; Hybrid 2: 1D-CNN + 2D-CNN + Transformer). Bootstrapping and Bayesian inference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming the synergistic complementarity of time and time-frequency domains. Conversely, Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating redundancy. The optimal performance of multimodal deep learning models depends on the complementarity of feature domains rather than the number of domains fused.

## Method Summary
The study used ECG images converted to 1D signals via vertical averaging, time-frequency scalograms via continuous wavelet transform, and frequency spectra via FFT. Five models were trained: 1D-CNN (time domain), 2D-CNN (time-frequency domain), 1D-CNN-Transformer (frequency domain), Hybrid 1 (1D-CNN + 2D-CNN feature fusion), and Hybrid 2 (1D-CNN + 2D-CNN + Transformer feature fusion). Models were evaluated using accuracy, precision, recall, and F1-score. Bootstrapping and Bayesian inference were used for statistical validation. The dataset was balanced using ADASYN oversampling.

## Key Results
- Hybrid 1 (1D-CNN + 2D-CNN) achieved 96% accuracy, outperforming the 2D-CNN baseline across all metrics (p < 0.05).
- Hybrid 2 (adding Transformer) showed no improvement and sometimes declined to 94%, indicating redundancy.
- Time and time-frequency domains showed low linear correlation (+0.07) but moderate mutual information (MI ≈ 0.40), indicating complementarity.
- The frequency domain showed weak correlation but moderate-to-high mutual information with other domains, indicating redundancy.

## Why This Works (Mechanism)

### Mechanism 1: Complementarity via Non-Linear Feature Independence
- **Claim:** Fusing feature domains with low linear correlation but moderate mutual information yields complementary information that improves classification performance.
- **Mechanism:** The 1D-CNN (time domain) and 2D-CNN (time-frequency domain) features exhibit a low Pearson correlation (+0.07) yet a moderate Mutual Information (MI≈0.40). This indicates a strong non-linear statistical dependence where the domains capture different aspects of the signal's physiology. Concatenating these features expands the representational coverage, allowing the classifier to leverage both temporal and spectro-temporal patterns for improved discrimination.
- **Core assumption:** Feature domains that are linearly independent but non-linearly dependent encode non-overlapping physiological information of the ECG signal.
- **Evidence anchors:**
  - [abstract] Hybrid 1 (1D-CNN + 2D-CNN) consistently outperformed the 2D-CNN baseline across all metrics.
  - [section] "As shown in Figure 16, the bootstrapped distributions are consistently skewed above zero. This indicated that the improvements were statistically reliable."
  - [corpus] Related work in corpus (e.g., "When Does Multimodality Lead to Better Time Series Forecasting?") discusses multimodality in abstract terms but does not provide the specific correlation/MI evidence for ECG fusion presented here.
- **Break condition:** Performance gains disappear if the fused domains show high mutual information (MI > threshold) or strong negative linear correlation, indicating redundancy or conflicting patterns.

### Mechanism 2: Redundancy from High Mutual Information with Weak Linear Correlation
- **Claim:** Adding a feature domain with weak linear correlation but moderate-to-high mutual information leads to representational redundancy, causing performance to plateau or decline.
- **Mechanism:** The Transformer-based frequency domain features (D3) show weak positive correlation (+0.18) with the 2D-CNN features (D2) and negative correlation (-0.30) with the 1D-CNN features (D1). Despite the weak linear link, the moderate MI values (0.37 and 0.44) suggest substantial statistical dependence and information overlap. This redundancy increases model complexity without adding discriminative power, diluting the complementary signal from the original fusion.
- **Core assumption:** Mutual information is a more robust indicator of information overlap than linear correlation alone in the context of deep-learned features.
- **Evidence anchors:**
  - [abstract] "Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating redundancy."
  - [section] "Table 14 illustrates the redundancy effect... all the mean differences in the performance metrics are negative or close to zero, suggesting no performance gain from adding the transformer."
  - [corpus] No direct corpus evidence refutes this; the finding is specific to the paper's ablation study.
- **Break condition:** A candidate domain should be excluded from fusion if it shows moderate-to-high mutual information (MI > τ) with the already fused domains, even if linear correlation is low.

### Mechanism 3: Intermediate Feature-Level Fusion Maximizes Synergy
- **Claim:** Intermediate (feature-level) fusion of complementary domain features outperforms adding more modalities or using unimodal baselines for ECG classification.
- **Mechanism:** Deep features are extracted separately by specialized encoders (1D-CNN for time, 2D-CNN for time-frequency). These learned representations are then concatenated into a unified feature vector before the final classification layers. This allows the model to learn complex, synergistic interactions between the temporal and spectro-temporal patterns that a simple late-fusion of predictions could not capture.
- **Core assumption:** The optimal fusion point is at the feature level, where high-level abstractions from each domain can be combined, rather than at the raw data (early) or prediction (late) levels.
- **Evidence anchors:**
  - [abstract] The paper evaluates two hybrid models, with the intermediate fusion model (Hybrid 1) achieving the best performance.
  - [section] Table 9 details the Hybrid 1 architecture: "Feature Fusion Method: Concatenate deep features from 1D-CNN and 2D-CNN branches."
  - [corpus] Neighbors like "BMDetect" also use multimodal deep learning but for different tasks; the evidence for intermediate fusion's efficacy is anchored in the paper's results.
- **Break condition:** Fusion is counterproductive if the combined feature vector leads to overfitting (diverging validation loss) or if the domains are redundant, as seen in Hybrid 2.

## Foundational Learning

- **Concept: Pearson Correlation vs. Mutual Information**
  - Why needed here: The paper demonstrates that linear correlation is insufficient for assessing feature complementarity. Mutual Information (MI) is required to detect non-linear statistical dependencies that predict fusion success.
  - Quick check question: Two feature sets have a near-zero Pearson correlation. Does this guarantee they will improve performance when fused in a deep learning model?

- **Concept: Signal Representations (Time, Frequency, Time-Frequency)**
  - Why needed here: The study's core premise is that ECG signals contain complementary information across different mathematical domains (time for morphology, time-frequency for non-stationary events, frequency for periodicity).
  - Quick check question: Which domain would best capture a sudden, transient change in an ECG signal's frequency content over a short period?

- **Concept: Overfitting and Regularization (Dropout, L1/L2)**
  - Why needed here: Increasing architectural complexity by fusing models raises the risk of overfitting to noise. The architectures employ Dropout and L2 regularization to ensure the model generalizes from the combined features.
  - Quick check question: If a complex hybrid model achieves 99% training accuracy but only 80% validation accuracy, what is the likely problem and how might the regularization strategy be adjusted?

## Architecture Onboarding

- **Component map:**
  - Input (1D ECG, 2D scalograms, FFT vectors) -> Encoders (1D-CNN, 2D-CNN, Transformer) -> Feature Fusion (Concatenation) -> Classifier (Dense layers with dropout) -> Output (Softmax)

- **Critical path:**
  1. **Preprocess & Transform:** Convert raw ECG into time, time-frequency (scalograms), and frequency (FFT) representations.
  2. **Assess Complementarity:** Calculate correlation and mutual information between domains; select only those with low linear correlation and moderate MI.
  3. **Extract & Fuse:** Train encoders on selected domains, extract deep features, and concatenate them.
  4. **Train & Validate:** Train the hybrid model, using validation loss to monitor for overfitting.
  5. **Statistically Verify:** Use bootstrapping to ensure performance gains are significant and not due to chance.

- **Design tradeoffs:**
  - **Complexity vs. Performance:** Adding more domains (e.g., Hybrid 2) increases parameters and computational cost but may degrade performance due to redundancy.
  - **Feature-Level vs. Decision-Level Fusion:** The paper argues for intermediate (feature-level) fusion to capture synergies, trading off the simplicity of late (decision-level) fusion.
  - **ADASYN for Imbalance:** Synthetic oversampling improves minority class detection but requires careful validation (e.g., FID) to ensure physiological plausibility.

- **Failure signatures:**
  - **Performance Plateau/Decline:** Adding a new domain fails to improve or worsens metrics (Hybrid 2 signature), indicating redundancy.
  - **High Validation Loss:** Model fails to generalize, suggesting overfitting from non-regularized complexity.
  - **High MI Between Domains:** Statistical analysis reveals candidate domains are too informationally similar to be useful.

- **First 3 experiments:**
  1. **Reproduce Baseline and Hybrid 1:** Train the 2D-CNN (baseline) and the Hybrid 1 (1D-CNN + 2D-CNN) models to confirm the performance gain from fusing time and time-frequency domains.
  2. **Analyze Feature Statistics:** Compute and visualize the correlation matrix and mutual information heatmap for all three domain features to empirically verify the complementarity of 1D-CNN/2D-CNN and the redundancy of the Transformer features.
  3. **Ablate the Third Modality:** Train Hybrid 2 and compare it to Hybrid 1. Confirm that removing the Transformer branch restores peak performance, validating the redundancy hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can transformer-based architectures be optimized to prevent the performance degradation observed in Hybrid 2 when fusing frequency-domain features?
- **Basis in paper:** [explicit] The authors state in the Future Work section: "Integrating the transformer feature domain into Hybrid 1 reduced the performance from 96% to 94%. In the future, we will optimize the transformer-based architectures."
- **Why unresolved:** The current study found that adding a 1D-CNN-Transformer (frequency domain) introduced redundancy and lowered accuracy compared to the simpler hybrid model. It is unclear if this is a limitation of the specific transformer implementation or an inherent redundancy of the domain itself.
- **What evidence would resolve it:** Demonstrating a modified transformer architecture that, when fused with time and time-frequency domains, results in a statistically significant performance improvement (p < 0.05) over Hybrid 1.

### Open Question 2
- **Question:** Do alternative fusion strategies, such as attention-based mechanisms or graph neural networks (GNNs), outperform the intermediate concatenation used in this study?
- **Basis in paper:** [explicit] The authors note: "We will explore different concatenation techniques such as attention-based fusion, graph neural networks (GNNs), and autoencoders."
- **Why unresolved:** The study relied on feature-level concatenation (intermediate fusion). While the authors discuss other strategies in the literature review, they have not empirically tested if attention mechanisms could better weigh the contribution of complementary domains compared to standard concatenation.
- **What evidence would resolve it:** A comparative study showing that attention-based fusion yields higher F1-scores or better generalizability than the concatenation method used in Hybrid 1.

### Open Question 3
- **Question:** Does the proposed theory of complementary feature domains generalize to diverse, multi-center ECG datasets and other biomedical signals?
- **Basis in paper:** [explicit] The authors state: "This study used an ECG signal dataset, and we hope to use a more diverse dataset in the future." They also propose applying the theory to EEG and Human Activity Recognition (HAR) but do not provide empirical results for these domains.
- **Why unresolved:** The theory was validated on a single dataset (Mendeley Data) consisting of ECG images converted to signals. It is uncertain if the specific complementarity between time and time-frequency domains holds true across different data distributions, noise profiles, or signal types like EEG.
- **What evidence would resolve it:** Successful replication of the Hybrid 1 performance gains over unimodal baselines using independent ECG datasets or distinct physiological signals like EEG.

### Open Question 4
- **Question:** How can Explainable Artificial Intelligence (XAI) techniques further quantify the specific nature of feature complementarity versus redundancy?
- **Basis in paper:** [explicit] The Future Work section lists: "We will integrate Explainable Artificial Intelligence (XAI) techniques for model interpretation. This enhances our study of the complementarity of the feature domains."
- **Why unresolved:** While the paper uses saliency maps to show the model focuses on QRS complexes, it relies on statistical metrics (Mutual Information) to define complementarity. It has not yet used XAI to visually or quantitatively map *how* specific features from the time domain interact with those from the time-frequency domain to correct misclassifications.
- **What evidence would resolve it:** XAI visualizations that explicitly highlight instances where the time-frequency feature corrects a deficiency in the time-domain feature, providing qualitative proof of synergy.

## Limitations

- **Data Transformation Ambiguity:** The conversion from ECG images to 1D signals via vertical averaging (Algorithm 1) is not fully specified, particularly regarding whether multi-lead images were cropped before averaging. This critical preprocessing step could significantly affect signal quality and reproducibility.
- **Wavelet Parameter Omission:** The mother wavelet type and scales used for generating time-frequency scalograms are not explicitly stated, leaving a gap in reproducing the exact feature representations.
- **Statistical Validation Scope:** While bootstrapping and Bayesian inference are employed, the specific number of bootstrap samples and Bayesian model details (priors, MCMC parameters) are not provided, limiting the ability to fully verify the statistical robustness claims.

## Confidence

- **High Confidence:** The core finding that fusing time and time-frequency domains (Hybrid 1) improves performance over the 2D-CNN baseline is well-supported by the bootstrapped distributions consistently skewed above zero (p < 0.05, Bayesian prob > 0.90). The mechanism of complementarity via non-linear feature independence is strongly evidenced by the low Pearson correlation (+0.07) and moderate mutual information (MI ≈ 0.40) between the 1D-CNN and 2D-CNN features.
- **Medium Confidence:** The claim that adding the frequency domain (Hybrid 2) causes redundancy is supported by the performance plateau/decline, but the evidence relies on the assumption that mutual information is a more robust indicator than linear correlation for deep-learned features. The exact threshold for "moderate-to-high" MI that defines redundancy is not specified.
- **Low Confidence:** The assertion that intermediate (feature-level) fusion is optimal over decision-level fusion is primarily supported by the paper's own results and lacks direct comparison or evidence from the broader literature on fusion strategies for ECG classification.

## Next Checks

1. **Feature Statistics Verification:** Reproduce the correlation matrix and mutual information heatmap for all three domain features (1D-CNN, 2D-CNN, Transformer) to empirically confirm the claimed complementarity (low correlation, moderate MI) and redundancy (moderate-to-high MI) patterns.

2. **Ablation of the Third Modality:** Train and evaluate Hybrid 2, then ablate the Transformer branch to create a model identical to Hybrid 1. Confirm that removing the Transformer restores peak performance and that Hybrid 2's accuracy drops to the stated ~94%, validating the redundancy hypothesis.

3. **Overfitting Monitoring:** During the training of the hybrid models, closely monitor the divergence between training and validation loss curves. If the validation loss increases significantly while training loss continues to decrease, it would indicate overfitting due to the added complexity of the redundant frequency domain, supporting the redundancy claim.