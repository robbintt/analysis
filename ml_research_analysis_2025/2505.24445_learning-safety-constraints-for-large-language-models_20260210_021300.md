---
ver: rpa2
title: Learning Safety Constraints for Large Language Models
arxiv_id: '2505.24445'
source_url: https://arxiv.org/abs/2505.24445
tags:
- safety
- facets
- language
- learning
- polytope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SaP (Safety Polytope), a geometric approach
  to enforcing safety constraints in large language models by learning and steering
  representations within a polytope defined in the model's latent space. Unlike weight-based
  fine-tuning methods, SaP operates post-hoc on intermediate representations, preserving
  model capabilities while enforcing safety.
---

# Learning Safety Constraints for Large Language Models

## Quick Facts
- arXiv ID: 2505.24445
- Source URL: https://arxiv.org/abs/2505.24445
- Authors: Xin Chen; Yarden As; Andreas Krause
- Reference count: 40
- Primary result: Geometric safety polytope reduces adversarial attack success rates from 12.92% to 0.26% while preserving MMLU accuracy

## Executive Summary
This paper introduces SaP (Safety Polytope), a geometric approach to enforcing safety constraints in large language models by learning and steering representations within a polytope defined in the model's latent space. Unlike weight-based fine-tuning methods, SaP operates post-hoc on intermediate representations, preserving model capabilities while enforcing safety. The framework uses a concept encoder to disentangle safety concepts and a steering algorithm to guide unsafe outputs back into safe regions defined by polytope facets. Experiments on Llama2-7B, Ministral-8B, and Qwen2-1.5B demonstrate significant reductions in adversarial attack success rates while maintaining MMLU accuracy. Analysis of learned polytope facets reveals natural specialization in detecting different semantic safety concepts. The approach scales efficiently and provides interpretable insights into how safety is captured in LLM representation spaces.

## Method Summary
SaP learns a safety polytope in the representation space of LLMs by training a concept encoder with a Convex Polytope Machine (CPM) that maps hidden states to safety concepts. The framework extracts features from intermediate layers (layer 20) at the final token, then trains a concept encoder (Linear→ReLU, 16,384-dim) using CPM loss with entropy-based facet assignment. The safety polytope is defined by K facets φ with thresholds ξ, learned through optimization that balances margin violations and facet entropy. During inference, SaP employs SafeFlow—a steering algorithm that iteratively adjusts representations via Lagrangian relaxation (100 steps) to push unsafe outputs into safe regions while preserving semantic content. The method uses hyperparameter tuning including Adam optimizer with lr=10⁻², batch=128, and model-specific margins (κ: 60.0 for Llama2, 5.0 for Ministral, 30.0 for Qwen2).

## Key Results
- Llama2-7B: Attack success rate reduced from 12.92% to 0.26% while maintaining MMLU accuracy
- Ministral-8B: Safety improvements achieved despite moderate capability preservation challenges
- Qwen2-1.5B: Demonstrated scalable effectiveness with attack success rate reduction from 9.62% to 0.84%
- Polytope facet analysis reveals interpretable specialization in detecting distinct safety concepts across BeaverTails categories

## Why This Works (Mechanism)
SaP works by creating a geometric safety boundary in the representation space that captures the manifold of safe outputs. The polytope structure allows for efficient steering operations that push unsafe representations back into safe regions without requiring weight updates to the base model. The entropy-based facet assignment ensures balanced coverage of safety concepts, preventing any single facet from dominating the safety boundary. By operating on intermediate representations rather than final outputs, the method preserves the model's original capabilities while enforcing safety constraints. The Lagrangian relaxation steering algorithm provides a principled way to optimize the trade-off between safety enforcement and output quality, enabling fine-grained control over the safety-performance balance.

## Foundational Learning
- **Convex Polytope Machines**: Geometric learning frameworks that define decision boundaries using polyhedral structures; needed for efficient safety constraint representation in high-dimensional spaces; quick check: verify polytope correctly classifies known safe/unsafe examples
- **Representation Steering**: Post-hoc manipulation of intermediate model representations to influence outputs; needed to enforce safety without retraining; quick check: confirm steering preserves semantic similarity to original input
- **Entropy-based Facet Assignment**: Distribution balancing technique that ensures polytope facets evenly cover safety concept space; needed to prevent facet collapse and ensure comprehensive safety coverage; quick check: validate facet entropy remains near target value
- **Lagrangian Relaxation**: Optimization method for constrained problems that converts them to unconstrained forms; needed for efficient steering algorithm implementation; quick check: verify convergence of steering optimization
- **Concept Encoders**: Neural networks that disentangle abstract concepts from representations; needed to map raw features to interpretable safety concepts; quick check: test concept encoder accuracy on held-out safety classification
- **Adversarial Attack Benchmarks**: Standardized evaluation frameworks for safety vulnerabilities; needed to measure effectiveness against known attack vectors; quick check: confirm attack success rates align with published baselines

## Architecture Onboarding

**Component Map**
HarmBench/BeaverTails datasets → LLM feature extraction (layer 20, eos) → Concept Encoder (Linear→ReLU, 16,384-dim) → CPM loss (Eq. 1) → Polytope facets φ, thresholds ξ → SafeFlow steering (Alg. 1) → Output safety enforcement

**Critical Path**
Feature extraction → Concept Encoder training → Polytope learning → Steering application → Safety evaluation

**Design Tradeoffs**
The choice of polytope over alternative geometric structures balances expressiveness with computational efficiency. Linear concept encoders trade representational power for interpretability and faster training. Steering via Lagrangian relaxation provides smooth optimization but requires careful hyperparameter tuning. Operating on intermediate representations preserves capabilities but may limit steering precision compared to end-to-end approaches.

**Failure Signatures**
Semantically incoherent outputs indicate over-aggressive steering (λunsafe too high). High ASR despite good classification accuracy suggests the geometric structure isn't being properly utilized for steering. Performance degradation on specific safety categories indicates insufficient facet coverage or concept encoder limitations.

**3 First Experiments**
1. Verify polytope correctly classifies held-out safe/unsafe examples from BeaverTails
2. Test steering algorithm on simple synthetic safety violations with known ground truth
3. Evaluate sensitivity of attack success rates to steering strength parameters (λunsafe/λsafe)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on selection of specific attack types for training, with unclear criteria for choosing the "top 3 attack methods" per model
- Geometric approach may have limited generalization to unseen attack types and safety domains beyond training distribution
- Concept encoder's linear architecture might oversimplify complex safety concept relationships, limiting expressiveness
- Steering algorithm performance is sensitive to hyperparameter tuning, requiring model-specific calibration
- Method shows capability preservation challenges particularly for smaller models like Ministral-8B

## Confidence

**High Confidence**
- Geometric framework's ability to reduce ASR (e.g., from 12.92% to 0.26% for Llama2-7B) is well-supported by experimental results
- Preservation of MMLU accuracy while improving safety metrics is convincingly demonstrated across all three models tested

**Medium Confidence**
- Interpretability claims regarding polytope facet specialization show promise but rely on qualitative analysis of BeaverTails dataset categories
- Generalizability to unseen attack types and safety domains remains uncertain without broader testing

**Low Confidence**
- Exact mechanisms by which entropy-based facet assignment contributes to performance are not fully explained
- Paper lacks ablation studies isolating entropy-based facet assignment component's impact on overall effectiveness

## Next Checks

1. **Attack Method Generalization**: Test SaP against attack methods not included in the top-3 training set to evaluate robustness to novel adversarial strategies and compare performance degradation against baseline defenses.

2. **Facet Number Sensitivity**: Systematically vary the number of polytope facets K (beyond the 20-50 range mentioned) to determine optimal complexity and assess whether performance plateaus or continues improving with more facets.

3. **Cross-Model Transferability**: Train SaP on one model (e.g., Llama2-7B) and evaluate its effectiveness when applied to steer another model (e.g., Ministral-8B) to test whether the learned safety polytope generalizes across architectures.