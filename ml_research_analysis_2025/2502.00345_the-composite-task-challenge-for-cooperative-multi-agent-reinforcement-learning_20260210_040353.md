---
ver: rpa2
title: The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning
arxiv_id: '2502.00345'
source_url: https://arxiv.org/abs/2502.00345
tags:
- tasks
- subtask
- task
- methods
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Composite Task Challenge (CTC), a novel
  testbed for evaluating cooperative multi-agent reinforcement learning (MARL) methods
  in scenarios requiring division of labor (DOL) and cooperation. The CTC tasks are
  designed by composing atomic subtasks in ways that make DOL and cooperation necessary
  for task completion, unlike existing benchmarks where DOL is optional.
---

# The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.00345
- Source URL: https://arxiv.org/abs/2502.00345
- Reference count: 24
- Primary result: All 10 evaluated MARL methods achieve 0% success rate on CTC tasks

## Executive Summary
This paper introduces the Composite Task Challenge (CTC), a novel benchmark for evaluating cooperative multi-agent reinforcement learning (MARL) methods that require division of labor (DOL) and cooperation. Unlike existing benchmarks where DOL is optional, CTC tasks are designed by composing atomic subtasks in ways that make DOL and cooperation necessary for completion. The benchmark incorporates three key diversity factors: information interference, subtask dissimilarity, and subtask quantity (2-4 subtasks per task).

The authors evaluate 10 representative MARL methods across policy diversity, agent grouping, and hierarchical paradigms, finding that all baselines perform poorly on CTC tasks (0% success rate). Simplified variants with homogeneous agents or symmetrical enemy distributions show significantly improved performance, validating task solvability while confirming the original CTC tasks' difficulty. The study also introduces a stability coefficient revealing poor performance consistency across seeds for most methods.

## Method Summary
The Composite Task Challenge (CTC) is constructed by composing atomic subtasks with three key diversity factors: information interference (cross-agent observational noise), subtask dissimilarity (heterogeneous subtask structures), and subtask quantity (2-4 subtasks per task). The benchmark evaluates 10 representative MARL methods across three paradigms: policy diversity (PD), agent grouping (AG), and hierarchical (HP). Each method is tested on tasks requiring both division of labor and cooperation, with performance measured by success rate. The study also introduces a stability coefficient to assess performance consistency across random seeds, revealing significant variability in most methods' results.

## Key Results
- All 10 evaluated MARL methods achieve 0% success rate on original CTC tasks
- Simplified variants with homogeneous agents or symmetrical enemy distributions show improved performance, validating task solvability
- Stability coefficient analysis reveals poor performance consistency across seeds for most methods
- CTC successfully challenges MARL methods by requiring both division of labor and cooperation, unlike existing benchmarks

## Why This Works (Mechanism)
The CTC benchmark works by creating task environments where division of labor and cooperation are not just beneficial but necessary for success. By composing atomic subtasks with varying structures and introducing observational noise between agents, the benchmark forces MARL methods to develop sophisticated coordination strategies. The three diversity factors (information interference, subtask dissimilarity, and subtask quantity) ensure that agents cannot rely on simple, uniform strategies and must instead learn to dynamically allocate roles and responsibilities based on task requirements and environmental constraints.

## Foundational Learning

1. **Division of Labor (DOL) in MARL**: Why needed - Enables efficient task completion by assigning specialized roles to agents; Quick check - Does the method explicitly allocate different roles to different agents?

2. **Observational Noise and Information Interference**: Why needed - Forces agents to develop robust communication and coordination strategies; Quick check - Are agents required to make decisions with incomplete or noisy information from other agents?

3. **Hierarchical Task Decomposition**: Why needed - Allows complex tasks to be broken down into manageable subtasks; Quick check - Does the method use hierarchical structures to manage task complexity?

4. **Stability Coefficient**: Why needed - Measures consistency of performance across random seeds; Quick check - Does the method show high variance in success rates across different runs?

5. **Task Composition with Dissimilarity**: Why needed - Ensures agents cannot use uniform strategies across all subtasks; Quick check - Are subtasks structurally different, requiring varied agent behaviors?

6. **Multi-Agent Cooperation Mechanisms**: Why needed - Essential for tasks where individual agent success is insufficient; Quick check - Does the method include explicit mechanisms for agent coordination?

## Architecture Onboarding

**Component Map**: Environment Generator -> Task Composer -> Diversity Enhancer -> MARL Algorithm -> Performance Evaluator

**Critical Path**: The environment generates tasks with specific diversity factors, agents apply MARL algorithms to learn cooperative strategies, and performance is evaluated based on success rates and stability coefficients.

**Design Tradeoffs**: The CTC prioritizes task complexity and diversity over ease of solution, potentially making it more challenging for current MARL methods but also more realistic for real-world applications requiring sophisticated coordination.

**Failure Signatures**: 
- Zero success rate indicates fundamental inability to coordinate effectively
- High variance across seeds suggests instability in learned policies
- Poor performance on heterogeneous tasks reveals limitations in handling diversity

**First Experiments**:
1. Test simplified CTC variants with homogeneous agents to establish baseline performance
2. Evaluate methods with varying levels of observational noise to assess robustness
3. Compare hierarchical vs. flat MARL approaches on tasks with different subtask quantities

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to specific MARL paradigms without exploring alternative approaches
- Focus on success rate as primary metric may miss nuanced behavioral insights
- Simplified variants showing improved performance may not fully capture real-world application challenges

## Confidence
- **High confidence** in empirical findings that current MARL methods perform poorly on CTC tasks
- **Medium confidence** in claim that CTC uniquely requires DOL and cooperation
- **Low confidence** in generalizability of stability coefficient results due to limited scope

## Next Checks
1. Test additional MARL algorithms not covered in the study, particularly those designed for heterogeneous or hierarchical coordination
2. Conduct ablation studies to isolate impact of each diversity factor (information interference, subtask dissimilarity, subtask quantity)
3. Validate CTC framework with human experts to confirm genuine DOL requirements and explore alternative problem formulations