---
ver: rpa2
title: 'XRAG: Cross-lingual Retrieval-Augmented Generation'
arxiv_id: '2505.10089'
source_url: https://arxiv.org/abs/2505.10089
tags:
- question
- answer
- english
- articles
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XRAG, a benchmark for evaluating cross-lingual
  retrieval-augmented generation (RAG) in scenarios where user queries and retrieved
  documents are in different languages. The benchmark covers both monolingual retrieval
  (English documents for non-English queries) and multilingual retrieval (documents
  in both English and the query language), using challenging questions generated from
  recent news articles requiring cross-document reasoning.
---

# XRAG: Cross-lingual Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.10089
- Source URL: https://arxiv.org/abs/2505.10089
- Authors: Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber
- Reference count: 40
- Primary result: Introduces benchmark showing LLMs struggle with response language correctness in monolingual retrieval and cross-lingual reasoning in multilingual retrieval

## Executive Summary
This paper introduces XRAG, a benchmark for evaluating cross-lingual retrieval-augmented generation (RAG) where queries and retrieved documents are in different languages. The benchmark covers two settings: monolingual retrieval (English documents for non-English queries) and multilingual retrieval (documents in both English and the query language). Using challenging questions generated from recent news articles requiring cross-document reasoning, the authors find that models achieve 55-63% accuracy compared to human performance of 85%. Two key challenges emerge: in monolingual retrieval, all models struggle with generating responses in the correct language, while in multilingual retrieval, the main difficulty lies in reasoning across languages rather than generating non-English text.

## Method Summary
XRAG is constructed through a pipeline starting with News Crawl articles from June-November 2024, filtered for relevance and length. Related article pairs are identified via entity co-occurrence in titles (for English-English pairs) or multilingual dense retrieval combined with Wiki 2024 events (for cross-lingual pairs). A three-step LLM workflow generates summaries, creates simple Q&A from summaries, then synthesizes cross-document Q&A requiring reasoning. Human verification and professional translation ensure quality, with distractors retrieved via multilingual retriever. The final dataset contains 1,000 monolingual retrieval instances and 300 instances each for German, Spanish, Chinese, and Arabic in the multilingual setting. Evaluation uses a three-judge LLM panel with language detection to measure both factual accuracy and response language correctness.

## Key Results
- All evaluated models (GPT-4o, Claude 3.5, Mistral-large, Command-R+, Nova-Pro) show response language correctness failures in monolingual retrieval, with English response rates ranging from 3.7% to 12.3%
- In multilingual retrieval, models show 5-15 point accuracy degradation from English monolingual baseline, with Mistral-large showing 56.3% relative drop for Arabic queries
- Translation-based ablations reveal that replacing non-English supporting articles with English translations yields the largest accuracy gains (+3.58 points average for GPT-4o), indicating cross-lingual reasoning is the primary bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit Response Language Correctness (RLC) failures in monolingual retrieval, defaulting to English when all retrieved documents are in English
- Mechanism: The model's output language is influenced by the dominant language in the context window, where English document tokens overwhelm the query-language signal during generation
- Core assumption: Language mixing in the context window affects generation language probabilities in proportion to token distribution
- Evidence anchors: All evaluated models struggle with response language correctness in monolingual retrieval; GPT-4o shows 3.7-5.1% English responses; Mistral-large shows up to 12.3%

### Mechanism 2
- Claim: In multilingual retrieval settings, the primary bottleneck is cross-lingual reasoning over mixed-language documents, not non-English text generation
- Mechanism: When supporting documents span multiple languages, LLMs must map semantically related information across linguistic representations before integration, introducing reasoning overhead and error accumulation
- Core assumption: Cross-lingual semantic alignment requires additional computational steps compared to monolingual integration
- Evidence anchors: Main challenge lies in reasoning over retrieved information across languages; replacing non-English supporting articles with English translations yields largest accuracy gain (+3.58 points for GPT-4o); changing question language alone shows minimal improvement

### Mechanism 3
- Claim: Distractor filtering becomes harder in mixed-language contexts, degrading answer accuracy even when supporting documents are correct
- Mechanism: Mixed-language distractors compete for attention with supporting documents in a noisier representation space where language-boundary tokens dilute relevance signals
- Core assumption: Attention mechanisms allocate probability mass across language boundaries suboptimally for relevance filtering
- Evidence anchors: Translating distracting articles to English provides additional improvement beyond question and supporting article translation; indicates distractor filtering difficulty in mixed-language contexts

## Foundational Learning

- Concept: **Cross-lingual RAG Architecture**
  - Why needed here: XRAG evaluates two distinct deployment patterns—monolingual retrieval (English-only knowledge base serving multilingual users) and multilingual retrieval (combining English + native language sources)
  - Quick check question: Can you explain why a German query with English-only documents represents a different challenge than a German query with German+English documents?

- Concept: **Cross-document Reasoning**
  - Why needed here: XRAG questions require integrating information from two supporting articles (aggregation, comparison, multi-hop, set operations), not single-document extraction
  - Quick check question: What additional reasoning step is required when two supporting facts are in different languages versus the same language?

- Concept: **LLM-as-Judge Evaluation with Language Correctness**
  - Why needed here: The evaluation pipeline combines factual accuracy (via 3-judge panel majority vote) with language detection; Cohen's kappa of 0.71 confirms reliability but reveals 2% false negatives
  - Quick check question: Why does the evaluation need both an LLM judge panel AND a language detector?

## Architecture Onboarding

- Component map:
  - Data Construction Pipeline: News Crawl (Jun-Nov 2024) → Related Article Pairs (entity graph + multilingual retrieval) → Cross-document Q&A Generation (3-step LLM workflow) → Human QA + Translation → Distractor Retrieval
  - Evaluation Pipeline: Question + 2 supporting + 6 distracting docs → Model generation → Language detection → 3-judge LLM panel (GPT-4o, Claude 3.5, Mistral-large) → Majority vote
  - Benchmark Structure: 1,000 monolingual retrieval instances + 300 each for 4 languages in multilingual retrieval

- Critical path:
  1. Article pair identification (bipartite entity graph for English-English; multilingual dense retriever + Wiki events for X-English)
  2. Cross-document question generation (summary → simple Q&A → cross-document Q&A with reasoning)
  3. Human verification and translation
  4. Distractor selection via multilingual retriever (2+ weeks older than supporting articles)
  5. Evaluation via judge panel + language detection

- Design tradeoffs:
  - News recency vs. knowledge cutoff: Jun-Nov 2024 timeframe ensures retrieval necessity (GPT-4o: 6.3% without retrieval) but limits domain coverage
  - Controlled vs. realistic distractors: 6 topically-related but non-answering articles approximate real RAG noise but may not capture adversarial distractors
  - Translation quality: Human translation for questions; Google Translate for baseline comparisons introduces MT error variance

- Failure signatures:
  - RLC failure: Model outputs English response to German/Chinese/Arabic query with English documents (3-12% of cases; highest for Mistral-large)
  - Cross-lingual reasoning drop: 5-15 point accuracy degradation from English monolingual baseline to multilingual retrieval
  - Language-specific collapse: Mistral-large shows 56.3% relative drop for Arabic queries

- First 3 experiments:
  1. Baseline RLC test: Run your model on monolingual retrieval subset with German queries, measure percentage of English responses; if >5%, prioritize language instruction engineering
  2. Controlled language ablation: Following Table 6 methodology, isolate whether your model's multilingual retrieval gap comes from question language, supporting article language, or distractor language
  3. Language-pair difficulty ranking: Test across all 4 target languages (de, es, zh, ar) to identify if your model has specific language-family weaknesses (e.g., Mistral-large's Arabic collapse suggests script/morphology sensitivity)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mitigation strategies can address the Response Language Correctness (RLC) failures observed in the monolingual retrieval setting?
- Basis in paper: Page 7 and Page 9 highlight that all evaluated models struggle to respond in the query language when retrieving only English documents, a "previously unreported challenge" requiring further study
- Why unresolved: The paper quantifies the error rates but does not propose or test methods to enforce output language alignment
- What evidence would resolve it: Experiments demonstrating that specific prompting techniques or fine-tuning on the XRAG benchmark significantly reduce RLC errors

### Open Question 2
- Question: How does the number of distracting articles influence LLM robustness and reasoning accuracy in cross-lingual settings?
- Basis in paper: Page 9 explicitly lists "exploring the impact of the number of distracting articles" as a controlled analysis left for future work due to space limitations
- Why unresolved: The benchmark uses a fixed set of six distractors, leaving the models' sensitivity to varying noise levels in cross-lingual contexts uncharacterized
- What evidence would resolve it: Ablation studies on the XRAG dataset varying the count of non-supporting documents to observe performance degradation curves

### Open Question 3
- Question: How does LLM performance degrade or improve when retrieval involves reasoning across more than two languages?
- Basis in paper: Page 9 notes the multilingual retrieval setting is limited to English and the query language, inviting future work on "retrieval across a set of languages (more than two)"
- Why unresolved: It is currently unknown if the identified cross-lingual reasoning bottlenecks compound when models must synthesize information from polylingual sources
- What evidence would resolve it: Extension of the dataset construction pipeline to include article triplets in three distinct languages followed by benchmarking of model accuracy

## Limitations

- The benchmark focuses exclusively on news articles from June-November 2024, limiting generalizability to other domains like scientific literature or technical documentation
- Fixed set of six distractors per instance may not capture the full range of noise patterns encountered in real-world RAG systems
- Response Language Correctness failures are quantified but not directly addressed with mitigation strategies or experimental validation of potential solutions

## Confidence

- High Confidence: The empirical observation that models struggle with RLC in monolingual retrieval (measured at 3-12% English responses) and that translation-based ablations improve multilingual retrieval accuracy
- Medium Confidence: The interpretation that RLC failures stem from context window language dominance and that cross-lingual reasoning overhead causes multilingual retrieval difficulties
- Low Confidence: The broader claim that these findings generalize to all cross-lingual RAG scenarios beyond the news domain tested

## Next Checks

1. **Context Window Ablation Study**: Systematically vary the ratio of English to non-English tokens in the context window for a fixed non-English query to quantify the relationship between language distribution and RLC failure rates

2. **Cross-lingual Reasoning Protocol Analysis**: Use activation visualization or attention pattern analysis to identify specific computational bottlenecks when models integrate information across language boundaries

3. **Domain Transfer Validation**: Replicate the benchmark methodology using scientific papers or technical documentation to assess whether RLC and cross-lingual reasoning challenges persist in different knowledge domains