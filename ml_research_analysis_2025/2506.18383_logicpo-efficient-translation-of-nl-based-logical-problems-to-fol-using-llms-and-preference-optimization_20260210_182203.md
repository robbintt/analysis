---
ver: rpa2
title: 'LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs
  and Preference Optimization'
arxiv_id: '2506.18383'
source_url: https://arxiv.org/abs/2506.18383
tags:
- logical
- language
- dataset
- text
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of translating natural language
  logical reasoning problems into consistent First-Order Logic (FOL) programs, a critical
  bottleneck in neurosymbolic AI pipelines. To address this, the authors introduce
  LOGICPO, a large-scale dataset of 26k NL-FOL pairs bootstrapped from a small seed
  (FOLIO) using multiple LLMs with varying in-context examples and temperatures, filtered
  via theorem prover outputs.
---

# LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization

## Quick Facts
- arXiv ID: 2506.18383
- Source URL: https://arxiv.org/abs/2506.18383
- Authors: Koushik Viswanadha; Deepanway Ghosal; Somak Aditya
- Reference count: 36
- Key outcome: Introduces LOGICPO dataset and fine-tuned models that achieve up to 10% higher logical correctness and 14% fewer syntax errors than GPT-4 on FOL translation tasks.

## Executive Summary
This paper tackles the challenge of translating natural language logical reasoning problems into consistent First-Order Logic (FOL) programs, a critical bottleneck in neurosymbolic AI pipelines. To address this, the authors introduce LOGICPO, a large-scale dataset of 26k NL-FOL pairs bootstrapped from a small seed (FOLIO) using multiple LLMs with varying in-context examples and temperatures, filtered via theorem prover outputs. They fine-tune open-source models (e.g., Llama-3, Phi-3.5, Qwen-2.5) first with supervised learning on the clean pairs, then with preference optimization (DPO/KTO) on curated preferred/rejected pairs. The resulting models significantly outperform strong baselines like GPT-3.5 and GPT-4 in FOL translation accuracy (up to 10% higher logical correctness, 14% fewer syntax errors), while also reducing predicate inconsistency and syntactic errors. Ablation studies and semantic analysis reveal persistent challenges in capturing implicit knowledge and handling complex sentence structures, highlighting future research directions.

## Method Summary
The authors bootstrap a large NL-FOL dataset by generating multiple FOL candidates per NL story using different LLMs (Llama-3-8B, Llama-3.1-8B, Mistral-0.2-7B) with varied in-context examples (n=2,4,8) and temperatures (0.25,0.6). Candidates are filtered via Prover9 theorem prover—if the derived logical label matches the groundtruth, the pair is included in D_sft; otherwise, it forms part of D_pref (chosen/rejected pairs). Models are then fine-tuned in two stages: first with supervised learning on D_sft (16,962 samples), then with preference optimization (DPO or KTO) on D_pref (10,000 pairs). Evaluation uses Prover9 to measure logical correctness, syntax errors, and F1 scores across FOLIO, ProofWriter, and PrOntoQA datasets.

## Key Results
- LOGICPO fine-tuned models achieve up to 10% higher logical correctness and 14% fewer syntax errors than GPT-4 baselines
- Two-stage training (SFT + KTO) consistently outperforms SFT alone, reducing predicate inconsistency and syntax errors
- Problem-level translation outperforms sentence-wise approaches for maintaining predicate consistency
- Smaller open-source models (Phi-3.5-Mini 4B, Gemma-2 2B) significantly outperform GPT-3.5 while approaching GPT-4 performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Theorem prover filtering creates reliable preference signal without human annotation.
- Mechanism: Generated FOL programs are submitted to Prover9; if the derived logical label matches the groundtruth NL label, the program is marked "chosen," otherwise "rejected." This converts executable correctness into a binary preference signal.
- Core assumption: Logical correctness (correct label derivation) correlates with translation quality, even if semantic equivalence is imperfect.
- Evidence anchors:
  - [abstract] "filtered via theorem prover outputs"
  - [Section 3.2] "If the computed logical outcome matches with the original groundtruth label, we include the NL-FOL pair"
  - [corpus] Limited direct corpus support; related work (FOL-Traces, arXiv:2505.14932) similarly uses automated verification but for traces, not preference data.
- Break condition: If a program derives the correct label via incorrect reasoning (e.g., "right answer, wrong logic"), the preference signal becomes noisy. The paper acknowledges this limitation explicitly (Section 6, Limitation 1).

### Mechanism 2
- Claim: Two-stage training (SFT → DPO/KTO) outperforms SFT alone for reducing syntax errors and improving logical correctness.
- Mechanism: SFT establishes baseline translation competence; preference optimization then sharpens the model's ability to discriminate between structurally similar but logically distinct outputs, particularly reducing predicate inconsistency and syntax errors.
- Core assumption: The preference dataset contains sufficiently diverse "hard negatives" where rejected samples are close to correct but contain specific, learnable flaws.
- Evidence anchors:
  - [abstract] "fine-tune open-source models first with supervised learning on the clean pairs, then with preference optimization"
  - [Table 3] SFT+KTO consistently improves over SFT alone (e.g., Phi-3.5: 58.43% → 61.52% logically correct; syntax errors 10.49% → 9.07%)
  - [corpus] Related work (arXiv:2601.09446) shows smaller LMs struggle with FOL translation, suggesting SFT alone may be insufficient.
- Break condition: If preference pairs lack diversity or are too easy to discriminate, KTO/DPO provides marginal signal. The paper shows DPO sometimes degrades performance (e.g., Gemma-2, Table 3), suggesting hyperparameter or data sensitivity.

### Mechanism 3
- Claim: Problem-level (holistic) translation outperforms sentence-wise translation for maintaining predicate consistency.
- Mechanism: Translating the full NL story as a single unit allows the model to maintain consistent predicate naming and variable bindings across sentences, avoiding the fragmentation issues seen in sentence-wise approaches.
- Core assumption: The model has sufficient context window and attention capacity to track inter-sentence dependencies.
- Evidence anchors:
  - [Section 1] "we observe that contemporary efforts... primarily target sentence-wise translation (Yang et al., 2024), which does not take into account predicate level consistency"
  - [Table 1] Shows sentence-wise predicate inconsistency (SAT2016(.) vs SAT(.))
  - [Table 3] GPT-3.5 sentence-wise baselines score 6.86%–21.54% logically correct vs. 51.81% for 4-shot problem-level
  - [corpus] Assumption: Related corpus does not directly compare sentence-wise vs. problem-level approaches.
- Break condition: On very long contexts (>5 sentences), performance degrades (Table 12), suggesting context management becomes a bottleneck.

## Foundational Learning

- Concept: **First-Order Logic (FOL) Syntax and Semantics**
  - Why needed here: Understanding predicates, quantifiers (∀, ∃), logical connectives, and variable scoping is essential to interpret translation outputs and debug errors.
  - Quick check question: Can you explain why `∀x(P(x) → Q(x))` is not equivalent to `∃x(P(x) ∧ Q(x))`?

- Concept: **Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO)**
  - Why needed here: These alignment methods train models to prefer "chosen" over "rejected" outputs without an explicit reward model. KTO uses a prospect-theoretic loss treating each sample independently.
  - Quick check question: What is the key difference between DPO (paired preferences) and KTO (unpaired binarized signal)?

- Concept: **Automated Theorem Provers (e.g., Prover9)**
  - Why needed here: The entire data filtering pipeline depends on Prover9's ability to derive logical labels from FOL programs. Understanding resolution refutation and syntax requirements is critical.
  - Quick check question: What happens when Prover9 encounters an undefined predicate or syntax error?

## Architecture Onboarding

- Component map:
  Data Generator -> Filter Engine -> Trainer Stage 1 -> Trainer Stage 2 -> Evaluator

- Critical path:
  1. Bootstrap from FOLIO (1k seed) → generate 30 candidates per input
  2. Filter via Prover9 → construct D_sft (label-matched) and D_pref (chosen/rejected pairs)
  3. SFT training (2–3 epochs typical)
  4. Preference optimization (DPO or KTO; KTO appears more stable)
  5. Evaluate on FOLIO, ProofWriter, PrOntoQA validation sets

- Design tradeoffs:
  - **Label-matching proxy**: Fast and scalable, but may admit semantically incorrect programs that happen to derive the correct label (information leakage)
  - **Problem-level vs. sentence-level**: Better consistency, but harder to debug and more context-dependent
  - **KTO vs. DPO**: KTO tolerates unpaired data and shows more consistent gains; DPO sometimes degrades (see Gemma-2, Table 3)
  - **Model size**: Larger models (Qwen-2.5 14B) outperform smaller ones (Phi-3.5 4B), but smaller models still beat GPT-3.5

- Failure signatures:
  - **Predicate inconsistency**: Different predicates for same concept across sentences (Similarity S1)
  - **Either-or mistranslation**: Scalar implicature (exclusive OR) mapped to logical OR (Section 5.1)
  - **Implicit information loss**: Commonsense knowledge not captured (L1 errors, Table 6)
  - **Information leakage**: Correct label from incorrect program (Limitation 1, Section 6)
  - **Context length degradation**: Performance drops for >5 sentences (Table 12)

- First 3 experiments:
  1. **Baseline reproduction**: Run 2-shot and 4-shot inference with Llama-3-8B-Instruct on FOLIO validation; measure logical correctness, syntax errors, and F1. Confirm baseline numbers match Table 3.
  2. **SFT-only ablation**: Train Phi-3.5-Mini on D_sft only; evaluate on ProofWriter to measure out-of-distribution generalization (compare to Table 4).
  3. **KTO hyperparameter sweep**: Vary learning rate (1e-5 to 5e-5) and β (0.1 to 0.5) for KTO on Llama-3; monitor for degradation patterns seen in Gemma-2/DPO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks verify the semantic correctness of FOL programs when the generated output yields the correct logical label but contains incorrect semantic reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section: "It is not guaranteed that if the logical label is correct, the program will also be correct... This is clearly an open problem and requires further exploration."
- Why unresolved: Current automated metrics rely on theorem provers (Prover9) that only validate the final truth value, failing to detect cases where the model reaches the correct answer via spurious logic or incorrect "either-or" translations.
- What evidence would resolve it: The development of a semantic equivalence metric or dataset that validates the alignment between natural language premises and FOL predicates independent of the final truth label.

### Open Question 2
- Question: Why does preference optimization lead to a regression in translating explicit information (L2 errors) compared to few-shot baselines like LINC?
- Basis in paper: [explicit] The analysis notes that while syntax and implicit info errors decrease, "we fallback on explicit information (L2) leading to more failures than LINC."
- Why unresolved: The authors hypothesize that models use larger predicates that embed excessive information, but the specific mechanism causing this trade-off between implicit capture and explicit accuracy is not confirmed.
- What evidence would resolve it: An ablation study isolating predicate complexity to determine if simpler predicates restore explicit information accuracy while retaining the benefits of preference optimization.

### Open Question 3
- Question: Can the LOGICPO bootstrapping pipeline be effectively adapted to low-resource formal languages (e.g., LEAN) where pre-training data is scarce?
- Basis in paper: [explicit] The Limitations section states: "We only explore English-FOL... probable that parsing errors will be higher as we change to even low-resource formal languages."
- Why unresolved: The method relies on bootstrapping from an existing seed dataset (FOLIO); it is unproven whether the data generation pipeline works when the base LLM has significantly weaker proficiency in the target formal language.
- What evidence would resolve it: Successful application of the LOGICPO data synthesis and fine-tuning loop on a low-resource formal language benchmark.

## Limitations

- Theorem prover label matching may admit semantically incorrect FOL programs that derive correct labels ("information leakage")
- Performance degrades on contexts longer than 5 sentences, indicating context management limitations
- Preference optimization sometimes regresses explicit information translation while improving implicit knowledge capture

## Confidence

- **High Confidence**: Problem-level translation outperforms sentence-wise (supported by direct quantitative comparison); KTO generally outperforms DPO for preference optimization (consistent across multiple model sizes); syntax error reduction is measurable and significant
- **Medium Confidence**: Claims about "up to 10% higher logical correctness" and "14% fewer syntax errors" relative to GPT-4—these are relative improvements on specific datasets but absolute performance remains modest (e.g., 61.52% logical correctness for best model). The evaluation metric (Prover9-based) is sound but may not capture semantic equivalence
- **Low Confidence**: The scalability claim (bootstrapping from 1k to 26k) is methodologically sound but the quality distribution of the generated data is unclear; no human evaluation of sample diversity or edge case coverage is reported

## Next Checks

1. **Quality Audit**: Sample 50 random examples from D_sft and D_pref; manually verify Prover9 label matching correctness and identify instances of "right answer, wrong logic" (information leakage)

2. **Generalization Stress Test**: Evaluate trained models on synthetic FOL problems with known semantic complexity (e.g., nested quantifiers, "either-or" with exclusive semantics) to measure robustness beyond benchmark datasets

3. **Preference Signal Analysis**: Measure the semantic similarity between chosen and rejected pairs in D_pref (e.g., using embedding distance); verify that negative examples are sufficiently challenging to provide meaningful training signal