---
ver: rpa2
title: Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection
  Evasion
arxiv_id: '2511.16020'
source_url: https://arxiv.org/abs/2511.16020
tags:
- physical
- adversarial
- across
- seqasr
- garment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a physically realistic, sequence-level adversarial
  clothing framework that generates printable adversarial textures for shirts, trousers,
  and hats to achieve robust human-detection evasion. The core innovation lies in
  optimizing garment textures at the sequence level using a physically-based pipeline
  that simulates motion, multi-viewpoint rendering, cloth dynamics, and illumination
  variation, rather than frame-by-frame optimization.
---

# Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion

## Quick Facts
- **arXiv ID:** 2511.16020
- **Source URL:** https://arxiv.org/abs/2511.16020
- **Reference count:** 40
- **Primary result:** Achieves SeqASR of 94.7% digitally and 86.2% physically while maintaining printability and cross-model transferability

## Executive Summary
This paper introduces a novel framework for generating adversarial clothing patterns that can effectively evade human detection systems across entire video sequences. The method employs a physically-based pipeline that simulates motion, multi-viewpoint rendering, cloth dynamics, and illumination variation to optimize garment textures at the sequence level rather than frame-by-frame. The approach uses a compact, printer-safe parameterization based on dual-domain K-means and ICC color locking, along with a control-point representation for efficient, differentiable texture generation. Extensive experiments demonstrate that the method achieves high attack success rates while maintaining physical plausibility through realistic garment simulation, with physical garments fabricated using sublimation printing retaining strong performance in real-world conditions.

## Method Summary
The framework generates adversarial clothing textures through a sequence-level optimization process that considers temporal consistency and physical realism. The method employs a control-point representation to enable efficient, differentiable texture generation while maintaining printability constraints through dual-domain K-means clustering and ICC color locking. A physically-based simulation pipeline models cloth dynamics, motion, multi-viewpoint rendering, and illumination variation to create robust adversarial patterns that remain effective across diverse viewing conditions. The optimization uses expectation-over-transformation with temporal weighting to ensure consistent suppression across entire video sequences. The approach is validated through both digital simulations and physical garment fabrication using sublimation printing, demonstrating effectiveness against YOLOv3 and transferability to other detection models.

## Key Results
- **Digital Performance:** Achieves sequence-level attack success rate (SeqASR) of 94.7%, conditional value-at-risk (CVaR) of 22.0, and non-detection rate (NDR) of 73.6% on YOLOv3
- **Physical Validation:** Fabricated garments retain strong performance with SeqASR of 86.2% in real-world conditions
- **Cross-Model Transferability:** Demonstrates high effectiveness against YOLOv5 and Faster R-CNN beyond the target model

## Why This Works (Mechanism)
The framework succeeds by optimizing adversarial patterns at the sequence level rather than frame-by-frame, ensuring temporal consistency and robustness across motion and viewpoint changes. The physically-based simulation pipeline accounts for cloth dynamics, illumination variation, and multi-viewpoint rendering, creating patterns that maintain their adversarial effect in realistic conditions. The control-point representation enables efficient optimization while the dual-domain K-means and ICC color locking ensure patterns remain printable and physically realizable. The temporal weighting in the expectation-over-transformation objective ensures that the adversarial effect persists throughout entire video sequences rather than being optimized for isolated frames.

## Foundational Learning
- **Adversarial Pattern Generation:** Creating patterns that systematically fool object detection systems through subtle perturbations in input data; needed to understand the core attack mechanism
- **Physically-Based Rendering:** Simulating realistic lighting, material properties, and motion to generate physically plausible adversarial patterns; needed to ensure patterns work in real-world conditions
- **Cloth Dynamics Simulation:** Modeling fabric behavior, draping, and movement under various conditions; needed to create patterns that remain effective as garments move and deform
- **Expectation-Over-Transformation (EOT):** Optimizing for multiple possible transformations to create robust adversarial examples; needed to handle variations in viewpoint, lighting, and motion
- **Dual-Domain K-Means Clustering:** Partitioning color spaces to maintain printability while preserving adversarial effectiveness; needed to ensure patterns can be physically realized
- **ICC Color Locking:** Maintaining color consistency across different devices and media; needed to ensure printed patterns match the optimized digital designs

## Architecture Onboarding
**Component Map:** Control Points -> Texture Generation -> Physically-Based Simulation -> Adversarial Loss Optimization -> EOT Framework
**Critical Path:** Control-point parameterization enables differentiable texture generation, which feeds into physically-based simulation for realistic rendering, ultimately optimized through EOT with temporal weighting
**Design Tradeoffs:** Sequence-level optimization versus computational efficiency; physical realism constraints versus adversarial strength; printability requirements versus pattern complexity
**Failure Signatures:** Patterns that lose effectiveness under motion or viewpoint changes; printed garments that deviate from digital specifications; attacks that fail to transfer across different detection models
**First Experiments:**
1. Digital-only validation against YOLOv3 with varying temporal weighting parameters
2. Cross-model transferability testing against YOLOv5 and Faster R-CNN
3. Physical garment fabrication and testing under controlled indoor conditions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- **Simplified Garment Model:** The approach uses a simplified model that may not fully capture complex fabric behaviors like draping and wrinkling
- **Limited Physical Validation:** Physical testing is restricted to controlled indoor settings with specific camera positions and lighting conditions
- **Control-Point Constraints:** The control-point representation may limit the range of achievable adversarial patterns compared to more flexible approaches

## Confidence
- **High Confidence:** Digital attack performance metrics (SeqASR 94.7%, CVaR 22.0, NDR 73.6%) and physical garment validation results (SeqASR 86.2%) are well-supported
- **Medium Confidence:** Cross-model transferability claims are supported but limited to two additional models without comprehensive analysis
- **Medium Confidence:** Assumptions about transferability across camera elevations and garment materials are demonstrated but lack extensive real-world validation

## Next Checks
1. Conduct outdoor testing across varying weather conditions, lighting scenarios, and uncontrolled environments to validate real-world robustness
2. Test transferability against a broader range of object detection architectures including transformer-based models and single-stage detectors
3. Evaluate performance degradation under different fabric types, garment sizes, and wearing styles to assess practical deployment constraints