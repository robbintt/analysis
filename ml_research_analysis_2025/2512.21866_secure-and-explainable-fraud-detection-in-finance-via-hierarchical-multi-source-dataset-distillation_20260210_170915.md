---
ver: rpa2
title: Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source
  Dataset Distillation
arxiv_id: '2512.21866'
source_url: https://arxiv.org/abs/2512.21866
tags:
- data
- dataset
- synthetic
- distillation
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a privacy-preserving dataset distillation framework
  for fraud detection by converting a random forest model into transparent, axis-aligned
  rule regions (leaf hyperrectangles) and generating synthetic transactions by uniformly
  sampling within each region. The approach reduces data volume by 85-93% while maintaining
  competitive performance, improving cross-institution precision and recall when synthetic
  data is shared.
---

# Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation

## Quick Facts
- **arXiv ID:** 2512.21866
- **Source URL:** https://arxiv.org/abs/2512.21866
- **Reference count:** 40
- **Primary result:** Reduces data volume by 85-93% while maintaining competitive fraud detection performance and improving cross-institution precision/recall when synthetic data is shared

## Executive Summary
This paper presents a privacy-preserving dataset distillation framework for fraud detection that converts Random Forest models into transparent, axis-aligned rule regions and generates synthetic transactions by uniformly sampling within each region. The approach achieves significant data volume reduction (85-93%) while maintaining competitive performance metrics. The method demonstrates strong privacy guarantees, passing membership-inference attack tests with AUC≈0.50, and provides both global and local explainability through rule summaries and per-case rationales with uncertainty quantification.

## Method Summary
The framework distills fraud detection knowledge from a Random Forest model into synthetic datasets by first extracting leaf indices and computing min/max bounds for each feature within each leaf region. Synthetic transactions are then generated by uniformly sampling within these hyperrectangular regions. The method includes an optional filtering step that removes synthetic points from regions of high tree-vote disagreement to improve downstream model calibration and AUC. The distilled data preserves decision boundaries while obfuscating specific training records, enabling secure cross-institution data sharing without exposing sensitive information.

## Key Results
- Reduces data volume by 85-93% while maintaining competitive fraud detection performance
- Improves cross-institution precision and recall when synthetic data is shared
- Achieves >93% structural similarity between real and synthesized data
- Passes membership-inference attack tests with AUC≈0.50 (random guessing)
- Provides global and local explainability through rule summaries and per-case rationales with uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1: Axis-Aligned Hyperrectangle Distillation
The Random Forest partitions feature space into disjoint regions represented by leaf nodes. By computing min/max bounds of training points within each leaf (hyperrectangle), the method captures local decision geometry. Uniform sampling within these bounds creates new points satisfying the same rule conditions but not identical to source data. Core assumption: axis-aligned boundaries sufficiently capture local feature interactions and uniform distribution approximation preserves downstream utility without destroying it.

### Mechanism 2: Disagreement-Based Filtering for Calibration
Tree-vote disagreement serves as uncertainty proxy. Synthetic points from regions with split votes likely sit on ambiguous decision boundaries. Filtering these points removes potential label noise or ambiguous signals from training set. Core assumption: high disagreement correlates with low predictive reliability rather than representing difficult but critical edge cases.

### Mechanism 3: Privacy via Geometry Abstraction
MIA resistance occurs because synthetic points are structural abstractions rather than memorized instances. Since synthetic data is generated by sampling intervals rather than copying points, it doesn't inherit "memorization" artifacts of source model, appearing as "non-members" to attackers. Core assumption: attack model cannot exploit region boundaries themselves to infer existence of specific training points within bounds.

## Foundational Learning

**Concept: Random Forest Decision Boundaries**
Why needed: Distillation relies on extracting leaf indices; understanding leaves = hyperrectangles is non-negotiable for implementation
Quick check: How does a Random Forest define a "region" in feature space, and what does a leaf node represent in terms of logical rules?

**Concept: Membership Inference Attacks (MIA)**
Why needed: To validate privacy claims; must understand why AUC ≈ 0.5 is target (random guessing)
Quick check: Why does overfitting in standard model lead to successful MIAs, and how does generating data from intervals theoretically prevent this?

**Concept: Uniform Distribution Sampling**
Why needed: Method samples features uniformly between min/max
Quick check: If feature has heavy-tailed distribution within leaf, how does uniform sampling affect fidelity of synthetic data?

## Architecture Onboarding

**Component map:** [Teacher: Random Forest] -> [Extractor: Leaf ID & Min/Max Bounds] -> [Generator: Uniform Sampler] -> [Filter: Disagreement Score] -> [Output: Synthetic Dataset]

**Critical path:** Mapping training samples to leaf indices (RF.apply) and computing bounding box (min/max) per feature per leaf

**Design tradeoffs:**
- **Distillation Ratio:** More trees = more leaves = more synthetic data, but weak dependence on ratio
- **Filtering Thresholds:** Aggressive filtering cleans data but risks losing rare fraud patterns
- **Leaf Purity:** Deep trees give tighter bounds (better fidelity, potentially worse privacy); shallow trees give wider bounds (more noise, better privacy)

**Failure signatures:**
- **Memorization:** MIA AUC >> 0.5 on synthetic vs. real train (indicates leaves too specific)
- **Utility Collapse:** Downstream model AUC drops significantly (>20%) compared to real-data training
- **Precision/Recall Imbalance:** Distillation improves precision but kills recall

**First 3 experiments:**
1. **Leaf Geometry Extraction:** Train RF on Cluster 1. Extract leaf nodes for training data. Verify min/max bounds capture training points without reproducing them exactly.
2. **Utility Baseline:** Train new classifier on synthesized Cluster 1 data and evaluate on Test set. Compare AUC against baseline RF trained on real data.
3. **Privacy Stress Test:** Implement basic MIA using confidence scores. Confirm AUC is near 0.50.

## Open Questions the Paper Calls Out

**Open Question 1:** Can formal differential privacy guarantees be integrated into tree-region distillation framework without significantly degrading utility of synthesized dataset? Basis: Privacy tightening via DP noise on region boundaries and count-based sampling identified as future extension. Why unresolved: Current method lacks formal mathematical guarantees provided by Differential Privacy, often required in regulated financial settings. What evidence would resolve it: Modified algorithm adding calibrated noise to hyperrectangle boundaries with theoretical privacy loss budget analysis and empirical utility metrics showing minimal performance regression.

**Open Question 2:** How can axis-aligned hyperrectangle sampling approach be extended to effectively capture temporal dependencies and high-cardinality categorical features? Basis: Richer handling of mixed/categorical features and temporal sequences identified as necessary extension. Why unresolved: Current method relies on axis-aligned splits and uniform sampling assuming feature independence within leaves, which may not adequately model sequential patterns or complex interactions of categorical variables typical in financial transactions. What evidence would resolve it: Extension of sampling mechanism to include temporal dynamics resulting in maintained performance on datasets with dominant temporal or categorical features.

**Open Question 3:** How can distillation process be refined to preserve local boundary details required by complex models (e.g., XGBoost, LSTM), which currently suffer performance drops when trained on synthetic data? Basis: Table 7 shows complex models show significant AUC drops (e.g., RF drops from 0.801 to 0.643) due to distillation smoothing local boundary detail. Why unresolved: Current uniform sampling strategy may regularize data too aggressively, discarding high-frequency noise and complex interactions that deep or boosted models rely on. What evidence would resolve it: Modified sampling strategy resulting in complex models achieving comparable AUC on distilled data versus real data, closing 0.05-0.15 performance gap.

**Open Question 4:** What specific adaptive or cost-sensitive mechanisms can be integrated into synthesis or filtering process to improve recall of minority (fraud) classes? Basis: Adaptive, cost-sensitive thresholding to address severe class imbalance suggested as future direction. Why unresolved: Current results show while precision is maintained or improved, recall often drops significantly (e.g., Cluster 1 recall drops from 0.144 to 0.012) on synthetic data, indicating distilled dataset may fail to sufficiently represent rare fraud patterns. What evidence would resolve it: Integration of cost-sensitive weights during random forest training or biased sampling strategy for minority-class leaf regions resulting in recall metric on synthetic data that matches or exceeds real-data baseline without catastrophic precision loss.

## Limitations

- Reliance on axis-aligned hyperrectangles assumes feature independence within leaves, which may not hold for complex fraud patterns
- Uniform sampling approach could distort distributions within leaves, particularly for heavy-tailed or multimodal distributions
- Performance across different RF architectures and real-world fraud data distributions remains unclear
- Method may still leak sensitive information through extracted rule boundaries, especially in sparse leaves

## Confidence

**High Confidence:** Basic mechanism of converting RF leaves to synthetic data via min/max bounds is well-founded and straightforward to implement

**Medium Confidence:** Privacy claims (MIA resistance) are supported by presented results but require broader validation across different attack models and datasets

**Medium Confidence:** Utility improvements (precision/recall metrics) are demonstrated but may be dataset-dependent and sensitive to RF architecture choices

## Next Checks

1. **Distribution Fidelity Analysis:** Quantify KL divergence between real and synthetic feature distributions within leaves, particularly for heavy-tailed features, to assess sampling bias

2. **MIA Resistance Under Different RF Architectures:** Test privacy guarantees across RFs with varying tree depths and numbers to identify conditions where MIA success rates increase

3. **Downstream Model Sensitivity:** Evaluate method's performance when distilled data is used to train different classifier types (not just RFs) to assess generalizability of utility gains