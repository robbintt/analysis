---
ver: rpa2
title: 'ProofOptimizer: Training Language Models to Simplify Proofs without Human
  Demonstrations'
arxiv_id: '2510.15700'
source_url: https://arxiv.org/abs/2510.15700
tags:
- proof
- have
- proofs
- lean
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProofOptimizer is the first LLM trained to simplify formal Lean
  proofs without human supervision. It uses expert iteration and reinforcement learning
  to reduce proof length by up to 87% on benchmarks, improving proof readability and
  execution speed.
---

# ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations

## Quick Facts
- arXiv ID: 2510.15700
- Source URL: https://arxiv.org/abs/2510.15700
- Reference count: 40
- Primary result: First LLM trained to simplify formal Lean proofs without human supervision

## Executive Summary
ProofOptimizer introduces a novel approach for automatically simplifying formal Lean proofs using language models trained without human demonstrations. The system employs expert iteration and reinforcement learning to progressively reduce proof length by up to 87% on benchmark problems. By iteratively sampling and verifying shorter proofs, the method achieves substantial compression of state-of-the-art RL-generated proofs while maintaining correctness through formal verification.

The approach demonstrates significant downstream benefits including faster proof compilation and improved model performance when trained on simplified proofs. This represents a breakthrough in automated proof optimization, addressing the challenge of proof readability and execution efficiency in formal verification systems.

## Method Summary
ProofOptimizer operates through an iterative process that combines language model sampling with formal verification. The method begins with existing proofs and generates candidate simplifications through a language model trained via expert iteration. Each candidate proof undergoes verification using Lean's type checker to ensure correctness is preserved. The reinforcement learning component optimizes for proof length reduction while maintaining logical validity. This process repeats, with each iteration producing progressively shorter proofs until convergence or a stopping criterion is met.

The training methodology is distinctive in that it requires no human demonstrations or annotations, instead relying on the self-supervised nature of proof verification. The approach leverages the formal guarantees provided by the Lean proof assistant to validate all simplifications, ensuring that the compressed proofs remain mathematically correct while achieving substantial length reduction.

## Key Results
- Achieves up to 87% reduction in proof length on benchmark problems
- Maintains proof correctness through formal verification in Lean
- Improves proof compilation speed and model training performance on simplified proofs

## Why This Works (Mechanism)
ProofOptimizer succeeds by leveraging the formal verification capabilities of the Lean proof assistant to create a self-supervised training environment. The method exploits the fact that proof correctness can be objectively verified without human intervention, allowing the language model to learn simplification strategies through trial and error. The iterative approach enables progressive refinement, where each successful simplification provides training signal for subsequent generations. The reinforcement learning component specifically optimizes for the compression objective while the verification step provides the necessary constraint that maintains mathematical validity.

## Foundational Learning

**Formal verification** - Why needed: Provides objective correctness checks without human supervision. Quick check: Can the system verify simplified proofs are logically equivalent to originals?

**Reinforcement learning for optimization** - Why needed: Enables automated search for optimal proof compression strategies. Quick check: Does the reward function effectively balance length reduction with proof preservation?

**Language model fine-tuning** - Why needed: Adapts general-purpose models to the specific task of proof simplification. Quick check: Does the model generate syntactically valid Lean code after training?

## Architecture Onboarding

Component map: Initial proofs -> Language model sampling -> Formal verification -> RL optimization -> Simplified proofs

Critical path: The sequence from proof sampling through verification to RL update forms the core optimization loop. Each iteration depends on successful verification of the previous step's output.

Design tradeoffs: The method trades computational resources for automation, requiring multiple verification passes and RL training iterations. The lack of human supervision reduces annotation costs but may miss domain-specific insights that human experts could provide.

Failure signatures: Common failure modes include getting stuck in local minima where proofs cannot be further simplified, generating syntactically valid but semantically incorrect proofs that pass initial checks, and excessive computational cost during the iterative refinement process.

First experiments:
1. Run ProofOptimizer on a small set of simple Lean proofs to verify the basic pipeline functions correctly
2. Compare proof length reduction rates with and without the RL fine-tuning phase to isolate its contribution
3. Test the system on proofs from different mathematical domains to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy dependence on availability of high-quality proof-generating models for initial proofs
- Computational resource requirements for reinforcement learning fine-tuning may limit accessibility
- Evaluation focuses on length metrics with limited analysis of preserved mathematical insight or pedagogical value

## Confidence

**High confidence**: Proof length reduction metrics and verification correctness (formal verification provides objective ground truth)

**Medium confidence**: Execution speed improvements and downstream training benefits (these depend on specific implementation details and hardware)

**Low confidence**: Claims about improved readability and mathematical insight (these are subjective and not rigorously evaluated)

## Next Checks

1. Test ProofOptimizer on proof corpora from different mathematical domains (e.g., topology, algebra) to assess generalizability beyond competition mathematics

2. Conduct ablation studies comparing proof simplification with and without the RL fine-tuning phase to isolate its contribution

3. Evaluate whether simplified proofs maintain pedagogical value by testing if they remain understandable to human readers or facilitate learning