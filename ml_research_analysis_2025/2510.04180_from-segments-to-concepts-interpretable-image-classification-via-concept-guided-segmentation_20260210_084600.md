---
ver: rpa2
title: 'From Segments to Concepts: Interpretable Image Classification via Concept-Guided
  Segmentation'
arxiv_id: '2510.04180'
source_url: https://arxiv.org/abs/2510.04180
tags:
- concept
- image
- concepts
- accuracy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEG-MIL-CBM, a framework that improves both
  interpretability and robustness in image classification by integrating concept-guided
  segmentation with attention-based multiple instance learning. The model segments
  images into semantically meaningful regions aligned with human-interpretable concepts,
  treats each as an instance, and learns to aggregate evidence using attention, enabling
  spatially grounded explanations without concept annotations.
---

# From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation

## Quick Facts
- **arXiv ID:** 2510.04180
- **Source URL:** https://arxiv.org/abs/2510.04180
- **Reference count:** 40
- **Primary result:** Over 30% worst-group accuracy gains on Waterbirds/Pawrious, competitive average accuracy on CIFAR-100 (85.3%)

## Executive Summary
SEG-MIL-CBM integrates concept-guided segmentation with attention-based multiple instance learning to improve both interpretability and robustness in image classification. The framework segments images into semantically meaningful regions aligned with human-interpretable concepts, treats each as an instance, and learns to aggregate evidence using attention. This enables spatially grounded explanations without concept annotations while mitigating spurious correlations. On benchmarks with background-foreground spurious correlations, the model achieves substantial worst-group accuracy improvements while maintaining competitive overall performance.

## Method Summary
The framework uses a three-stage preprocessing pipeline: CLIP computes similarity scores between the image and a concept vocabulary, top-K concepts are selected and passed to GroundingDINO for localization, then SAM produces binary masks for each concept region. Each mask becomes a semantically labeled "instance" in a bag, with the image-level label as the only supervision. During training, a backbone extracts features from each segment, which are projected to concept space, aggregated via attention weights, and classified. A cosine similarity loss aligns predicted concept activations with CLIP-derived similarity vectors, providing soft semantic supervision without manual concept annotations.

## Key Results
- Achieves worst-group accuracy gains of over 30% on Waterbirds and Pawrious benchmarks with spurious correlations
- Maintains competitive average accuracy (85.3%) on CIFAR-100, matching state-of-the-art models
- Shows enhanced resilience to input corruptions in CIFAR-10-C, outperforming baseline models
- Provides transparent, concept-level reasoning through spatially grounded attention weights
- Effectively suppresses irrelevant background features while focusing on task-relevant regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing images into semantically grounded regions enables the model to focus on task-relevant evidence while suppressing spurious background cues.
- **Mechanism:** CLIP computes similarity scores between the image and a concept vocabulary; top-K concepts are selected and passed to GroundingDINO for localization, then SAM produces binary masks. Each mask becomes a semantically labeled "instance" in a bag, explicitly separating foreground concepts from background.
- **Core assumption:** Foundation models (CLIP, GroundingDINO, SAM) can reliably identify and segment semantically meaningful regions without domain-specific fine-tuning.
- **Evidence anchors:**
  - [abstract] "integrates concept-guided image segmentation into an attention-based multiple instance learning (MIL) framework, where each segmented region is treated as an instance"
  - [section 4] "We first compute similarity scores between each image and a concept list C using CLIP... For each image, we retain the top-Ktop concepts Cx ⊂ C ranked by similarity."
  - [corpus] Related work on concept-guided segmentation in medical imaging (arXiv:2503.02917) shows similar grounding improves interpretability, supporting the general principle but not this specific implementation.
- **Break condition:** Heavy occlusion, tiny objects, or domains where CLIP's concept associations are weak may degrade segment quality and propagate errors.

### Mechanism 2
- **Claim:** Attention-based aggregation over segments attenuates spurious features by learning to assign higher weights to task-relevant regions.
- **Mechanism:** Each segment's features are projected to a concept space; a learnable attention vector computes temperature-scaled softmax weights over segments. The weighted sum forms the bag-level representation for classification, implicitly down-weighting irrelevant regions.
- **Core assumption:** Spurious features (e.g., background water in Waterbirds) will receive lower attention weights when trained with only image-level labels, because they are less predictive across the full training distribution.
- **Evidence anchors:**
  - [abstract] "learns to aggregate evidence using attention, enabling spatially grounded explanations without concept annotations"
  - [section 3] "Attention-based MIL architectures enhance this process by weighting instances according to their relevance to the task, enabling the model to focus on informative regions while disregarding irrelevant ones."
  - [corpus] Label-free Concept Based MIL (arXiv:2501.02922) similarly uses MIL for interpretability in histopathology, corroborating the attention-for-relevance principle.
- **Break condition:** If spurious features are highly correlated with labels in training data (e.g., >95% water backgrounds with waterbirds), attention alone may not fully suppress them without explicit regularization.

### Mechanism 3
- **Claim:** Aligning predicted concept activations with CLIP-derived similarity vectors provides soft semantic supervision without manual concept annotations.
- **Mechanism:** A cosine similarity loss encourages predicted segment-level concept vectors to match normalized CLIP embeddings for that segment's concepts. This term is added to cross-entropy classification loss with weight λ_concept.
- **Core assumption:** CLIP's multimodal embedding space captures human-aligned semantic relationships that are beneficial for the target task.
- **Evidence anchors:**
  - [section 4] "L_concept = −1/B Σ cos(z_i, z_CLIP_i)... The overall objective combines the classification loss and the concept alignment loss"
  - [corpus] Sparse Autoencoders for concept extraction (arXiv:2506.23951) shows linear combinations of activations can yield interpretable concepts, providing indirect support for linear concept projections.
- **Break condition:** If CLIP's concept associations are biased or spurious for a domain (e.g., medical imaging), alignment may propagate those biases.

## Foundational Learning

- **Multiple Instance Learning (MIL):**
  - Why needed here: SEG-MIL-CBM treats each image as a "bag" of unlabeled segments, with only the image-level label known.
  - Quick check question: Given a bag of instances with one bag-level label, how would you compute gradients back to individual instances?

- **Concept Bottleneck Models (CBMs):**
  - Why needed here: The framework builds on CBM's interpretable intermediate layer, but extends it to spatially grounded segments.
  - Quick check question: In a standard CBM with 100 concepts and 10 classes, what is the shape of the weight matrix connecting concepts to logits?

- **Attention Mechanisms for Aggregation:**
  - Why needed here: The model uses attention to weight segments, not for sequence modeling.
  - Quick check question: If attention weights sum to 1 and you have N_s=15 segments, what happens to the gradient for a segment with attention weight α_i=0.01?

## Architecture Onboarding

- **Component map:** CLIP (ViT-B/32) → Top-K concept selection → GroundingDINO → SAM → Filtered segment bags → Backbone (ResNet-50 or ViT-B/16) → Feature extractor ϕ → Concept head W_c → Attention MLP → Classifier head
- **Critical path:** Segmentation quality (Stage 1) fundamentally limits attention-based suppression (Stage 2). If segments don't capture task-relevant regions, attention cannot recover them.
- **Design tradeoffs:**
  - Bag size N_s: Larger bags capture more regions but increase memory/compute; paper uses N_s=15 for most datasets, N_s=5 for large-scale.
  - Concept alignment weight λ_concept: Set to 0.1; higher values may enforce CLIP semantics at the cost of task performance.
  - Mask filtering thresholds (τ_min_pixels, ρ_max): Aggressive filtering removes noise but may discard informative small regions.
- **Failure signatures:**
  - Low worst-group accuracy despite high average accuracy → attention not suppressing spurious regions; check attention weight distribution.
  - Degraded performance on small objects → segmentation pipeline failing; visualize segment masks.
  - Concept predictions misaligned with ground truth → λ_concept may be too low or CLIP concepts inappropriate for domain.
- **First 3 experiments:**
  1. **Sanity check on Waterbirds:** Train with bag size N_s=5 vs 15; verify that worst-group accuracy improves with larger bags and that attention weights on bird segments exceed background weights.
  2. **Ablation of concept alignment loss:** Run with λ_concept ∈ {0, 0.1, 0.5}; plot worst-group accuracy vs λ_concept to confirm the regularization role.
  3. **Segment visualization on failure cases:** Identify images where prediction is wrong; visualize segments, attention weights, and concept activations to diagnose whether failure is due to poor segmentation or attention misallocation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the segmentation and MIL attention modules be refined jointly in an end-to-end differentiable manner?
- **Basis in paper:** [explicit] The authors explicitly identify "end-to-end refinement of segments and attention" as a direction for future work.
- **Why unresolved:** The current pipeline treats segmentation as a fixed pre-processing step (using SAM/GroundingDINO) before training the MIL classifier, preventing the task loss from updating region boundaries.
- **What evidence would resolve it:** An ablation study comparing the performance of fixed segments against a jointly trained segmentation backbone, showing if gradient-based boundary refinement improves concept alignment.

### Open Question 2
- **Question:** How can the framework be extended to video data to ensure temporal consistency in concept grounding?
- **Basis in paper:** [explicit] The paper states that future work includes "extending to video and multi-modal settings, where temporal consistency can further stabilize concept grounding."
- **Why unresolved:** The current model processes static images independently and lacks mechanisms to track concept instances or attention weights across time frames.
- **What evidence would resolve it:** Evaluation on video benchmarks (e.g., Kinetics) demonstrating that attention weights track the same semantic region consistently across frames without flickering.

### Open Question 3
- **Question:** To what extent does the model's performance degrade when the foundational segmentation models fail on open-world scenes?
- **Basis in paper:** [explicit] The authors note a limitation where "failure cases in open-world scenes (e.g., heavy occlusion, tiny objects) can reduce segment quality."
- **Why unresolved:** The reliance on a separate segmentation stage creates a dependency bottleneck; it is unclear if the attention mechanism can recover when key objects are missed entirely during the pre-processing phase.
- **What evidence would resolve it:** Stress-testing the model on datasets with high occlusion levels or small objects to quantify the correlation between segmentation failure rates and drops in worst-group accuracy.

## Limitations
- Dependence on foundation models for segmentation with unspecified critical hyperparameters (K_top, τ_min_pixels, τ_IoU)
- Strong assumption that foundation models can reliably segment task-relevant regions without domain adaptation
- Attention mechanism's sufficiency for spurious feature suppression when correlations exceed 95% in training data
- Concept alignment loss may propagate biases if CLIP's semantic space is misaligned with human judgment

## Confidence
- **High Confidence:** Framework architecture, MIL training procedure, and benchmark results (CIFAR-100 accuracy, Waterbirds worst-group improvement)
- **Medium Confidence:** Interpretability benefits and concept-level explanations rely on subjective evaluation of segment visualizations
- **Low Confidence:** Foundation model reliability across all domains and sufficiency of attention for spurious feature suppression without additional regularization

## Next Checks
1. **Sensitivity analysis of segmentation hyperparameters:** Systematically vary K_top (5-15), τ_min_pixels (1000-5000), and τ_IoU (0.3-0.7) on Waterbirds; measure impact on worst-group accuracy and segment quality to validate framework robustness.

2. **Cross-domain generalization test:** Apply the same framework (with no fine-tuning of foundation models) to a medical imaging dataset (e.g., CAMELYON17 for cancer detection); compare segment quality and classification performance against domain-specific baselines to assess foundation model assumptions.

3. **Ablation of attention vs. explicit spuriousness regularization:** Train variants with and without the concept alignment loss (λ_concept=0 vs 0.1) on Waterbirds; additionally implement an explicit spuriousness regularization term and compare worst-group accuracy improvements to assess whether attention alone is sufficient.