---
ver: rpa2
title: Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking
  Methodology of Human Experts
arxiv_id: '2506.12552'
source_url: https://arxiv.org/abs/2506.12552
tags:
- media
- news
- bias
- political
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a novel approach for profiling news media\
  \ in terms of political bias and factuality by leveraging large language models\
  \ (LLMs) without manual article analysis. The method uses expert-driven prompts\
  \ based on professional fact-checkers\u2019 criteria to elicit LLM responses, which\
  \ are then aggregated for predictions."
---

# Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts

## Quick Facts
- arXiv ID: 2506.12552
- Source URL: https://arxiv.org/abs/2506.12552
- Reference count: 27
- Primary result: Automated media profiling achieves 80.6% accuracy for factuality and 93.5% for bias prediction using expert-driven LLM prompts

## Executive Summary
This study introduces a novel approach for profiling news media in terms of political bias and factuality by leveraging large language models (LLMs) without manual article analysis. The method uses expert-driven prompts based on professional fact-checkers' criteria to elicit LLM responses, which are then aggregated for predictions. Experiments on a large dataset of 4,192 outlets show significant improvements over baselines: 80.6% accuracy and MAE of 0.206 for factuality, and 93.5% accuracy and MAE of 0.075 for political bias prediction. Error analysis reveals higher performance on popular and U.S.-based outlets, while ablation study highlights the importance of combining reasoning and leaning data. This work advances automated media profiling and offers a scalable alternative to manual fact-checking.

## Method Summary
The method operationalizes professional fact-checking standards through systematic LLM prompts based on Media Bias/Fact Check's 16 policy-area definitions. It queries GPT-3.5-turbo with two prompt types: 18 handcrafted questions about public figures and trending topics, and systematic prompts using MBFC's policy definitions. LLM responses are parsed as structured JSON and concatenated, then classified using TF-IDF+SVM for factuality and fine-tuned BERT for bias. The approach achieves state-of-the-art results while addressing the scalability limitations of manual fact-checking.

## Key Results
- 80.6% accuracy and 0.206 MAE for factuality prediction (vs. 64.8% baseline)
- 93.5% accuracy and 0.075 MAE for political bias prediction (5-class)
- SVM with TF-IDF outperforms transformers for factuality classification
- Ablation shows synergy: leaning+reason prompts (93.5%) outperform leaning only (86.9%) or reason only (90.5%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-guided systematic prompts elicit more accurate LLM knowledge about media bias than ad-hoc queries.
- Mechanism: The method operationalizes MBFC's 16 policy-area definitions (e.g., Abortion, Economic Policy) directly into prompts, instructing the LLM to classify leaning (left/right) with explicit criteria. This constrains the LLM's response space to align with professional fact-checking standards.
- Core assumption: LLMs have encoded sufficient knowledge about news outlets during pretraining to accurately report their editorial stances when properly prompted.
- Evidence anchors:
  - [abstract] "method uses expert-driven prompts based on professional fact-checkers' criteria to elicit LLM responses"
  - [Section 3.2] Describes systematic prompts using MBFC's 16 policy definitions; [Table 2] shows BERT with expert guidelines (†) achieves 93.5% accuracy vs. 88.1% without.
  - [corpus] Related work (Yang & Menczer 2025) found moderate correlation (ρ=0.54) between ChatGPT credibility ratings and human experts, suggesting LLMs encode partial but imperfect outlet knowledge.
- Break condition: If an outlet is too obscure or recent for LLM training data, responses may hallucinate or default to "unknown."

### Mechanism 2
- Claim: Aggregating multiple diverse prompt responses captures complementary signals that improve classification robustness.
- Mechanism: 18 handcrafted prompts span three categories (stance on public figures, trending topics, factuality). Each prompt elicits structured JSON with "leaning" and "reason" fields. Concatenated responses form a rich text representation fed to classifiers (SVM, BERT).
- Core assumption: Different prompt framings surface distinct aspects of an outlet's profile; their combination is more informative than any single query.
- Evidence anchors:
  - [abstract] "responses from large language models (LLMs), which we aggregate to make predictions"
  - [Table 4] Ablation shows: leaning only (86.9%), reason only (90.5%), leaning+reason (93.5%)—demonstrating synergy.
  - [corpus] No direct corpus evidence for this specific aggregation mechanism; related work focuses on single-prompt assessments.
- Break condition: If prompts are poorly designed or redundant, aggregation adds noise without signal.

### Mechanism 3
- Claim: LLMs perform better on popular, U.S.-centric outlets due to training data distribution biases.
- Mechanism: Outlets frequently discussed in pretraining corpora (e.g., foxnews.com, bbc.com) have richer internal representations. Less popular or non-U.S. outlets have sparser coverage, leading to higher error rates.
- Core assumption: LLM knowledge correlates with outlet prominence in training data.
- Evidence anchors:
  - [Section 5.5] "model benefits from prior knowledge likely encoded in the LLM for well-known outlets"; Figure 2 shows red error cluster for less popular outlets (higher Alexa Rank).
  - [Section 5.5] Figure 3 shows higher accuracy for U.S.-based vs. non-U.S. outlets.
  - [corpus] Corpus papers confirm bias detection systems struggle with non-English/under-resourced contexts (e.g., ClaimPT addresses Portuguese).
- Break condition: New or niche outlets with minimal training exposure will be misclassified or marked "unknown."

## Foundational Learning

- **Prompt Engineering for Structured Extraction**
  - Why needed here: The method relies on designing prompts that force LLMs to return structured JSON with specific fields ("leaning," "reason") rather than free-form text.
  - Quick check question: Can you write a prompt that constrains an LLM to return only valid JSON with exactly two keys?

- **Text Classification Pipelines (TF-IDF + SVM vs. Transformers)**
  - Why needed here: LLM responses are converted to embeddings (TF-IDF or contextual) and classified. SVM with TF-IDF outperformed BERT for factuality (80.6% vs. 78.2%), suggesting sparse representations suit this task.
  - Quick check question: Why might TF-IDF + SVM outperform transformers on smaller, structured datasets?

- **Understanding Ordinal Evaluation (MAE)**
  - Why needed here: Factuality (low/mixed/high) and bias (left/left-center/center/right-center/right) are ordinal scales. Accuracy alone misses severity of misclassification; MAE penalizes larger errors more.
  - Quick check question: If a model predicts "left" for a "right" outlet, how does MAE differ from predicting "right-center"?

## Architecture Onboarding

- **Component map:**
  MBFC database -> Prompt design layer -> GPT-3.5-turbo API -> JSON parsing -> Feature representation -> SVM/BERT classification -> Evaluation metrics

- **Critical path:**
  Prompt design → LLM response quality → Feature representation → Classifier choice
  The prompt design step is the highest-leverage; poorly framed queries produce vague or hallucinated reasoning that degrades downstream performance.

- **Design tradeoffs:**
  - **GPT-3.5 vs. open-source (LLaMA/Mistral):** GPT-3.5 used for data curation due to quality; open-source models tested for zero-shot but underperformed (e.g., LLaMA2-7B MAE=0.744 on factuality vs. SVM's 0.206).
  - **TF-IDF/SVM vs. Transformers:** SVM better for factuality (80.6%); BERT better for fine-grained 5-class bias (70% vs. 64.8%).
  - **Cost vs. reproducibility:** Authors note GPT-3.5 cost constraints limited GPT-4 exploration; future work aims for open-source.

- **Failure signatures:**
  - **Popularity bias:** High error rate for outlets with Alexa Rank >10^5 (less popular).
  - **Regional bias:** Non-U.S. outlets misclassified more frequently (Figure 3).
  - **Hallucination:** LLM may fabricate stances for unknown outlets—validate with "unknown" handling.
  - **Prompt brittleness:** Ambiguous phrasing produces inconsistent JSON; enforce strict output parsing.

- **First 3 experiments:**
  1. **Replicate handcrafted prompts on a held-out outlet set:** Test whether 18-category prompts generalize beyond MBFC-labeled data; measure accuracy drop on obscure outlets.
  2. **Ablate prompt categories:** Train classifiers with only Category 1 (public figures), only Category 2 (trending topics), only Category 3 (factuality)—quantify each's contribution.
  3. **Swap GPT-3.5 for an open-source model in data curation:** Replace GPT-3.5 with LLaMA3-70B for prompt responses; compare downstream classifier performance to assess reproducibility gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does jointly modeling factuality and political bias improve the prediction accuracy of either task compared to predicting them independently?
- Basis in paper: [explicit] The authors state in the Future Work section that they have "not fully explored the joint prediction of factual reporting levels and political bias," and suggest it could enhance the overall assessment.
- Why unresolved: The current methodology predicts factuality and bias as separate tasks using distinct datasets and prompts, without leveraging potential correlations between the two.
- What evidence would resolve it: Experimental results comparing the performance (Accuracy/MAE) of a multi-task learning model against the current single-task baselines on the same dataset.

### Open Question 2
- Question: Can applying systematic expert guidelines to prompts improve factuality prediction accuracy, similar to the improvements observed for political bias?
- Basis in paper: [inferred] The authors used expert-driven systematic prompts for bias (Section 3.2) but only handcrafted prompts for factuality (Section 3.1). In Future Work, they explicitly "plan to refine factuality assessment by incorporating expert methodologies directly into prompts."
- Why unresolved: The current study demonstrates the success of expert guidelines for bias but leaves the application of this specific prompt engineering strategy for factuality as a gap.
- What evidence would resolve it: A comparative analysis of factuality classification performance when using expert-criteria-based prompts versus the handcrafted prompts used in the current study.

### Open Question 3
- Question: To what extent does the regional bias in LLM training data affect the generalizability of media profiling to non-U.S. or less popular outlets?
- Basis in paper: [explicit] The error analysis reveals that the model performs significantly better on U.S.-based outlets (Figure 3), and the authors note in the Limitations section that the "model's generalizability to such outlets [not encountered during training] remains uncertain."
- Why unresolved: The LLMs (like GPT-3.5) likely contain more parametric knowledge about U.S. media, creating a performance disparity that the current methodology does not correct.
- What evidence would resolve it: A breakdown of performance metrics (Accuracy/MAE) across diverse geographical regions and popularity levels (Alexa Rank) on a globally diverse test set.

### Open Question 4
- Question: Does retrieval augmentation or the inclusion of graphical features significantly reduce hallucinations and errors when profiling emerging or obscure media outlets?
- Basis in paper: [explicit] The authors state in Future Work that they "will try retrieval augmentation and graphical features... for improved robustness" and acknowledge that "hallucinations from LLM responses require attention" in the Limitations.
- Why unresolved: The current approach relies entirely on the LLM's internal parametric knowledge, which is prone to hallucinations or lack of data for smaller outlets.
- What evidence would resolve it: A study comparing the error rates of the current parametric-only approach against a Retrieval-Augmented Generation (RAG) approach for low-popularity media outlets.

## Limitations

- **Reproducibility gap:** Heavy reliance on GPT-3.5-turbo API calls creates dependency on proprietary model performance and cost constraints.
- **Regional bias:** Significant performance degradation for non-U.S. outlets suggests limited global generalizability.
- **Knowledge cutoff:** The method struggles with emerging or obscure outlets lacking sufficient training data representation.

## Confidence

- **High:** The systematic prompt design operationalizing MBFC's 16 policy definitions into structured LLM queries is well-documented and validated (93.5% accuracy for bias prediction).
- **Medium:** The aggregation of diverse prompt responses improving robustness is supported by ablation but lacks theoretical justification for why different framings provide complementary signals.
- **Medium:** The popularity and regional bias findings (higher accuracy for U.S./prominent outlets) are well-supported by error analysis but don't establish causation—could reflect both training data bias and outlet characteristics.

## Next Checks

1. **Replicate on held-out obscure outlets:** Test whether the 18-category prompts maintain performance on outlets with Alexa Rank >10^5 or non-U.S. sources to quantify generalizability limits.
2. **Validate aggregation mechanism:** Conduct controlled experiments ablating prompt categories to establish whether different framings truly capture complementary signals versus redundant information.
3. **Test open-source reproducibility:** Replace GPT-3.5-turbo with LLaMA3-70B for the same prompt suite and compare downstream classifier performance to assess the proprietary model dependency.