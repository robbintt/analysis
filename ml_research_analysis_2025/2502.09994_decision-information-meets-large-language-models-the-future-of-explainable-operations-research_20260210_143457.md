---
ver: rpa2
title: 'Decision Information Meets Large Language Models: The Future of Explainable
  Operations Research'
arxiv_id: '2502.09994'
source_url: https://arxiv.org/abs/2502.09994
tags:
- aircraft
- code
- type
- explanations
- changes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EOR, a framework for explainable Operations
  Research (OR) using Large Language Models (LLMs). EOR introduces the concept of
  "Decision Information" to quantify the impact of changes in constraints on decision-making
  through bipartite graphs and LLM-based explanations.
---

# Decision Information Meets Large Language Models: The Future of Explainable Operations Research

## Quick Facts
- arXiv ID: 2502.09994
- Source URL: https://arxiv.org/abs/2502.09994
- Reference count: 40
- First industrial benchmark for explainable OR methods

## Executive Summary
This paper introduces EOR, a framework that combines Large Language Models with Operations Research to provide explainable what-if analysis. The key innovation is "Decision Information," which quantifies how constraint changes affect decision-making using bipartite graph edit distance. The framework employs a multi-agent system (Commander/Writer/Safeguard) to generate and validate code modifications, producing both accurate optimization solutions and high-quality explanations. Experimental results demonstrate 90.33% modeling accuracy and explanation quality scores of 9.47/10, significantly outperforming baseline methods.

## Method Summary
EOR uses a multi-agent AutoGen framework to process what-if queries in Operations Research. The system converts optimization problems to Linear Programs, represents them as bipartite graphs, and computes normalized Graph Edit Distance (NGED) to quantify decision impact. The Commander routes queries to the Writer for JSON-based code generation, while the Safeguard validates safety before execution. The framework uses GPT-4-Turbo with temperature=0, supporting both zero-shot and one-shot configurations. A novel industrial benchmark with 30 problems and 10 queries each provides the evaluation dataset.

## Key Results
- 90.33% modeling accuracy on the industrial benchmark
- Explanation quality of 9.47/10 (expert evaluation, zero-shot) vs. 6.59/10 for baseline
- One-shot configuration improves accuracy to 95.33% with GPT-4-Turbo
- Automated evaluation aligns with expert scores (difference <0.1)

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Bipartite graph edit distance quantifies the impact of constraint changes on decision-making.
**Mechanism:** The framework converts optimization problems to Linear Programs (LPs), represents them as bipartite graphs (constraint nodes × variable nodes), and computes normalized Graph Edit Distance (NGED) between original and modified graphs. NGED captures structural changes (node/edge insertions, deletions, substitutions) with unit cost per attribute modification, providing a numerical score of "how much" the decision space changed.
**Core assumption:** The magnitude of graph structural change correlates with the significance of decision impact—a larger NGED implies more consequential modifications to the solution space.
**Evidence anchors:**
- [Section 3.2.2]: "GED represents the minimum cost necessary to transform one graph into another through a sequence of operations... A larger edit distance highlights significant alterations, signaling a substantial impact of the updated information on the decision-making process."
- [Section 3.2.2]: NGED formula normalizes by graph size: `NGED(Gp', Gp) = GED(Gp', Gp) / max(|Gp'|, |Gp|)`
- [Corpus]: Weak direct evidence—neighbor papers discuss LLM explainability generally but not graph-based OR quantification.
**Break condition:** If NGED ≈ 0 but solution outcomes differ significantly, the assumption that structural change maps to decision impact fails—suggesting the metric misses semantic equivalence classes.

### Mechanism 2
**Claim:** Multi-agent separation (Commander/Writer/Safeguard) improves code generation reliability through iterative verification.
**Mechanism:** The Commander routes queries; the Writer generates code modifications in JSON format targeting specific constraint blocks; the Safeguard validates safety before execution. Failed safety checks trigger debug loops with error-type-specific prompts. This separation isolates generation from verification, reducing single-LLM error propagation.
**Core assumption:** LLMs can reliably assess code safety/correctness when given explicit verification roles, and debugging prompts enable meaningful self-correction.
**Evidence anchors:**
- [Section 3.2.1]: "If the Safeguard provides a SAFE confirmation, the Writer transitions into an interpreter role... If the code is deemed dangerous, the process loops back to (2), with the Commander sending a debug code prompt."
- [Table 5]: Debug attempts beyond 3 show minimal improvement (88.33%→89.00% zero-shot), suggesting limited self-correction capability.
- [Corpus]: Related work "Integrating LLMs with Network Optimization" supports LLM-OR integration but doesn't verify multi-agent efficacy.
**Break condition:** If debug success rate plateaus <50% after 3 iterations, the assumption of meaningful self-correction fails—external intervention becomes necessary.

### Mechanism 3
**Claim:** LLM-generated justification explanations outperform attribution-only explanations when grounded in quantified decision information.
**Mechanism:** Rather than summarizing outcomes (attribution), the framework prompts LLMs to explain why code changes occurred and how quantified NGED scores relate to result differences. The interpreter prompt explicitly includes NGED values, original/modified results, and structured explanation requirements.
**Core assumption:** LLMs can synthesize quantitative metrics (NGED) with domain knowledge to produce human-interpretable rationale beyond surface-level descriptions.
**Evidence anchors:**
- [Table 3]: EOR achieves 9.47/10 explanation quality vs. 6.59/10 for OptiGuide (expert evaluation, zero-shot).
- [Section 4.5.2]: "EOR incorporates a quantitative analysis, explaining the $15,000 increase in operational costs due to the restricted solution space caused by the newly added constraints."
- [Corpus]: "Towards Transparent AI: A Survey on Explainable LLMs" notes LLMs struggle with decision-process explanations, supporting the need for structured grounding.
**Break condition:** If automated evaluation scores diverge significantly from expert scores for complex queries, the grounding mechanism may not generalize to edge cases.

## Foundational Learning

- **Concept: Linear Programming (LP) Standard Form**
  - Why needed here: The framework converts OR problems to LP form before graph representation; understanding `min c^T x, s.t. Ax ≤ b` is prerequisite to interpreting the bipartite graph construction.
  - Quick check question: Given an LP with 3 variables and 4 constraints, how many nodes appear in the corresponding bipartite graph?

- **Concept: Bipartite Graph Representation of Constraints**
  - Why needed here: The core innovation uses bipartite graphs where S = constraint nodes, X = variable nodes, E = coefficient relationships. Edge weights encode constraint matrix entries.
  - Quick check question: If constraint `2x + 3y ≤ 10` exists, which edges connect to node `s_i` and what are their weights?

- **Concept: Graph Edit Distance (GED)**
  - Why needed here: NGED quantifies "decision information" change; understanding vertex/edge insertion-deletion-substitution costs is essential for interpreting impact scores.
  - Quick check question: If G_p has 10 vertices and G_p' has 12 vertices with 3 edge substitutions, what is the raw GED cost under unit-cost assumptions?

## Architecture Onboarding

- **Component map:** User Query → Commander (router) → Writer (code generation in JSON) → Safeguard (safety check) → [SAFE] → Solver execution → Writer (interpreter mode) → Explanation output / [DANGER] → Debug loop

- **Critical path:** Query interpretation → JSON code generation → Safety validation → Solver execution → NGED computation → Explanation synthesis. Failure at any stage cascades to incorrect or unexplainable outputs.

- **Design tradeoffs:**
  - Zero-shot vs. One-shot: One-shot improves accuracy (88.33%→95.33% on GPT-4-Turbo) but requires example maintenance.
  - JSON vs. full-code output: JSON reduces maintenance cost but risks format errors (22.86% of zero-shot failures).
  - Automated vs. expert evaluation: Auto-eval aligns with expert scores (±0.1 difference) but may exhibit LLM self-bias.

- **Failure signatures:**
  - JSON format errors: LLM returns malformed JSON (14-23% of failures).
  - Modeling logic errors: Code executes but produces wrong solutions (28-37% of failures).
  - Variable name/syntax errors: Runtime crashes (5-17% of failures).
  - Safeguard false positives: Valid code rejected as dangerous.

- **First 3 experiments:**
  1. **Baseline replication:** Run EOR on the provided benchmark with GPT-4-Turbo, zero-shot, temperature=0; verify 88%+ accuracy and 9.4+ explanation quality.
  2. **Ablation on NGED:** Remove NGED from interpreter prompts; measure explanation quality degradation to isolate the contribution of quantified decision information.
  3. **Stress test on complex constraint changes:** Construct queries with combined add/delete/modify operations (per benchmark Query 3, 6, 7, 9); identify failure mode clustering (logic vs. format errors).

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can automated evaluation techniques be refined to eliminate the bias where LLMs rate their own generated explanations higher than human experts do?
**Basis in paper:** [inferred] The paper notes that while the automated ("Auto") evaluation aligns closely with expert evaluation, "LLMs may be biased toward the results they generate," evidenced by higher Auto scores for the Standard model compared to Expert scores.
**Why unresolved:** The current work establishes a correlation between Auto and Expert evaluation but identifies a specific systemic bias in the automated evaluator that prevents it from being a fully reliable standalone replacement for human judgment.
**What evidence would resolve it:** A modified evaluation protocol where an LLM-based evaluator shows no statistical preference for its own outputs over those of other models compared to a ground-truth human ranking.

### Open Question 2
**Question:** What specific mechanisms can be integrated into the framework to reduce "Modeling Logic Errors" where code executes successfully but produces semantically incorrect results?
**Basis in paper:** [inferred] In the failure case analysis (Appendix A.7), "Modeling Logic Errors" account for the largest portion of failures (37.14% in zero-shot), distinct from syntax errors, suggesting the LLM struggles with the logical translation of constraints.
**Why unresolved:** While the "Safeguard" agent catches safety and syntax errors, the paper lacks a specific verification step for semantic correctness when the code is syntactically valid but logically flawed.
**What evidence would resolve it:** The introduction of a verification agent or formal method that successfully catches logical inconsistencies before execution, thereby lowering the Modeling Logic Error rate below the current baseline.

### Open Question 3
**Question:** Can the "Decision Information" quantification method, which relies on Graph Edit Distance (GED) on Linear Programs (LPs), be generalized to effectively analyze non-linear optimization problems?
**Basis in paper:** [inferred] The methodology explicitly restricts the quantification process to a "standardized LP format" to calculate Graph Edit Distance, limiting the applicability of the "Decision Information" metric to linear constraints.
**Why unresolved:** The reliance on a bipartite graph representation of a Linear Program assumes a matrix structure $A \in \mathbb{R}^{m \times n}$, which does not natively accommodate non-linear constraint structures without approximation.
**What evidence would resolve it:** A modified GED calculation method that successfully quantifies constraint impacts for Mixed-Integer Non-Linear Programming (MINLP) problems, showing a correlation with decision impact similar to the LP results.

### Open Question 4
**Question:** How does the EOR framework perform on user queries that require introducing new decision variables, given the current problem formulation assumes variables remain unchanged?
**Basis in paper:** [inferred] The problem formulation (Section 3.1) states, "In our setting, we assume the decision variables remain unchanged," restricting the "Decision Information" analysis to changes in objective functions and constraints.
**Why unresolved:** Real-world what-if analysis often involves introducing new assets or options (e.g., "What if we introduce a new Type C aircraft?"), which falls outside the current defined scope of the framework.
**What evidence would resolve it:** Experimental results showing successful code generation and accurate explanation quality for a dataset of queries specifically designed to add or remove decision variables from the optimization model.

## Limitations
- The framework's GED-based decision information assumes structural changes directly map to solution space alterations, which may not hold for semantically equivalent constraints
- Multi-agent self-correction effectiveness plateaus after 3 debug iterations (89% accuracy), indicating fundamental limitations in autonomous error resolution
- The benchmark represents a single industrial case study, limiting generalizability claims

## Confidence

- **High confidence**: Accuracy improvements over baselines (90.33% modeling accuracy), explanation quality superiority (9.47/10 vs 6.59/10), and automated evaluation alignment with expert scores (±0.1 difference)
- **Medium confidence**: The GED-quantified decision information mechanism, as direct validation of structural-change-to-decision-impact correlation is limited to theoretical justification
- **Medium confidence**: Multi-agent self-correction effectiveness, given the plateauing debug success rate and lack of comparison to single-agent baselines with external error correction

## Next Checks

1. **Cross-dataset validation**: Apply EOR to diverse OR benchmarks (e.g., OR-Library problems) to test generalizability beyond the single industrial dataset

2. **Semantic equivalence stress test**: Design constraint modifications that preserve solution space semantics while maximizing structural change (high GED, low impact) to validate the GED-decision-impact assumption

3. **Human-in-the-loop evaluation**: Conduct user studies measuring decision quality improvement when using EOR explanations versus baseline methods, particularly for complex constraint modifications