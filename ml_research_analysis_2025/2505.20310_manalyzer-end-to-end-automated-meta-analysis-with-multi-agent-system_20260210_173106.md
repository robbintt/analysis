---
ver: rpa2
title: 'Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System'
arxiv_id: '2505.20310'
source_url: https://arxiv.org/abs/2505.20310
tags:
- data
- extraction
- meta-analysis
- table
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Manalyzer, a multi-agent system for end-to-end
  automated meta-analysis that addresses hallucination issues in paper screening and
  data extraction. The system employs hybrid review for distinguishing paper quality,
  hierarchical extraction with self-proving and feedback checking for accurate data
  integration.
---

# Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System

## Quick Facts
- **arXiv ID:** 2505.20310
- **Source URL:** https://arxiv.org/abs/2505.20310
- **Reference count:** 40
- **Key outcome:** Manalyzer achieves 80.8% accuracy in paper screening (+30% F1) and 77.7% hit rate in data extraction (+50% improvement) for automated meta-analysis

## Executive Summary
This paper introduces Manalyzer, a multi-agent system designed to automate the meta-analysis process by addressing hallucination issues in paper screening and data extraction. The system employs a hybrid review mechanism for distinguishing paper quality and hierarchical extraction with self-proving and feedback checking for accurate data integration. A comprehensive benchmark of 729 papers across three domains (atmosphere, agriculture, environment) with over 10,000 data points was constructed to evaluate performance. Experiments demonstrate that Manalyzer significantly outperforms existing LLM baselines, successfully processing multimodal content including text, tables, and images.

## Method Summary
Manalyzer addresses end-to-end automated meta-analysis through a two-stage pipeline: paper screening and data extraction. The screening stage uses a hybrid review approach combining individual review scores with comparative review on batches of papers, employing a knapsack algorithm for context selection. The extraction stage implements hierarchical extraction with binary mask filtering followed by data extraction into markdown tables. A self-proving mechanism requires citations for extracted data, while a feedback checking loop with a checker agent allows for iterative refinement. The system processes multimodal content (text, tables, images) parsed using MinerU, and uses a temperature of 0 to minimize hallucinations.

## Key Results
- Achieves 80.8% accuracy in paper screening, representing a +30% improvement in F1 score over baselines
- Achieves 77.7% hit rate in data extraction, representing a +50% improvement over baselines
- Successfully processes multimodal content including text, tables, and images across 729 papers in three domains

## Why This Works (Mechanism)
Manalyzer's effectiveness stems from its multi-agent architecture that combines multiple verification mechanisms. The hybrid review system reduces false positives by requiring both individual and comparative assessments of paper relevance. The hierarchical extraction approach first filters relevant sections before attempting data extraction, reducing the cognitive load on the extraction agent. The self-proving mechanism forces the system to cite specific sources for each data point, while the feedback checking loop allows for iterative refinement of extracted data. The temperature of 0 setting minimizes stochastic generation, further reducing hallucinations.

## Foundational Learning

**Knapsack Algorithm for Context Selection**
*Why needed:* To optimize limited context windows when evaluating papers
*Quick check:* Verify the algorithm selects paragraphs with highest cumulative relevance scores within context limits

**Hybrid Review System**
*Why needed:* To combine individual assessment with comparative judgment for more robust screening
*Quick check:* Confirm final scores are computed as sr * (s1 + s2) where sr is relative score

**Self-proving Mechanism**
*Why needed:* To prevent hallucination by requiring explicit citations for extracted data
*Quick check:* Verify output tables include [Explanation] blocks with exact row/column citations

**Hierarchical Extraction**
*Why needed:* To reduce extraction errors by first filtering relevant sections before detailed extraction
*Quick check:* Confirm binary mask step successfully filters irrelevant sections

**Feedback Checking Loop**
*Why needed:* To iteratively refine extraction results through automated validation
*Quick check:* Verify maximum 3 iterations are performed when checker scores are low

## Architecture Onboarding

**Component Map:**
PDF Parsing -> Hybrid Review (s1, s2, sr) -> Screening Decision -> Hierarchical Extraction -> Self-proving -> Feedback Checking -> Final Output

**Critical Path:**
PDF Parsing -> Hybrid Review -> Screening Decision -> Hierarchical Extraction -> Final Output

**Design Tradeoffs:**
- Temperature 0 vs. creativity: Reduces hallucinations but may miss nuanced cases
- Knapsack selection vs. completeness: Optimizes for high-value content but may miss important context
- Checker iterations (max 3) vs. latency: Balances accuracy with computational efficiency

**Failure Signatures:**
- Uniform paragraph score distribution indicates knapsack selection is random
- Missing [Explanation] blocks indicate self-proving mechanism failure
- Low final scores despite high s1/s2 suggest comparative review issues

**First 3 Experiments:**
1. Test screening pipeline on a small batch of 10 papers with known relevance
2. Run hierarchical extraction on a single paper with mixed content types
3. Validate self-proving mechanism by checking citation requirements in extracted data

## Open Questions the Paper Calls Out

None

## Limitations
- Benchmark construction may have selection bias from DOI scraping of specific journals
- Performance evaluation limited to structured academic papers with tables and images
- Temperature setting of 0 may not handle ambiguous cases requiring reasoning
- Computational costs and inference latency not addressed for practical deployment

## Confidence

**High confidence:** Core methodology (hybrid review + hierarchical extraction + self-proving + feedback checking) is well-specified and reproducible

**Medium confidence:** Benchmark dataset construction and domain coverage appear reasonable but selection criteria transparency is limited

**Low confidence:** Computational efficiency claims and real-world deployment considerations are not addressed

## Next Checks

1. Run ablation studies to isolate the contribution of each component (hybrid review vs. hierarchical extraction vs. self-proving vs. feedback checking) to reported performance gains

2. Test generalizability by applying Manalyzer to papers from different domains (e.g., medicine, social sciences) or document types not included in original benchmark

3. Measure computational overhead by recording inference time and API costs for processing the 729-paper benchmark, comparing against traditional manual meta-analysis workflows