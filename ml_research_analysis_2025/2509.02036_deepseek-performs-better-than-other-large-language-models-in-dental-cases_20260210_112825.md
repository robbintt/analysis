---
ver: rpa2
title: DeepSeek performs better than other Large Language Models in Dental Cases
arxiv_id: '2509.02036'
source_url: https://arxiv.org/abs/2509.02036
tags:
- deepseek
- llms
- clinical
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated four large language models (LLMs)\u2014DeepSeek\
  \ V3, GPT-4o, Gemini 2.0 Flash, and Copilot\u2014on their ability to interpret longitudinal\
  \ periodontal case vignettes through open-ended clinical tasks. Using 34 standardized\
  \ cases (258 question-answer pairs), model performance was assessed via automated\
  \ metrics and blinded evaluations by licensed dentists."
---

# DeepSeek performs better than other Large Language Models in Dental Cases

## Quick Facts
- arXiv ID: 2509.02036
- Source URL: https://arxiv.org/abs/2509.02036
- Reference count: 12
- DeepSeek V3 outperformed other LLMs on dental case analysis with highest faithfulness (0.528) and expert ratings (4.5/5)

## Executive Summary
This study compared four large language models—DeepSeek V3, GPT-4o, Gemini 2.0 Flash, and Copilot—on their ability to interpret complex dental cases. Using 34 longitudinal periodontal vignettes and 258 question-answer pairs, models were evaluated through automated metrics and blinded assessments by licensed dentists. DeepSeek V3 demonstrated superior performance, achieving the highest faithfulness scores and expert ratings while maintaining readability comparable to peers. The findings suggest DeepSeek V3's potential as a domain-specific clinical tool for dental education and research.

## Method Summary
The study employed a three-step conversational framework where models were assigned clinical roles, provided with complete case narratives (2,000–6,000 words), and asked open-ended clinical questions. Performance was measured using RAGAs metrics (faithfulness, answer relevancy), Flesch-Kincaid readability scores, and blinded expert evaluations (5-point Likert scale). MELD scores assessed potential data contamination. Statistical comparisons were made using non-parametric tests across the four LLMs.

## Key Results
- DeepSeek V3 achieved highest faithfulness score (median 0.528) and expert ratings (median 4.5/5)
- DeepSeek V3 significantly outperformed other models (p < 0.001) in both automated and expert evaluations
- GPT-4o fine-tuning improved faithfulness (+0.036) but decreased expert scores (−0.23)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepSeek V3's mixture-of-experts (MoE) architecture may enable superior domain-specific reasoning in clinical contexts
- Mechanism: MoE employs dynamic query routing to specialized neural sub-networks, allowing the model to selectively activate expert components optimized for different types of reasoning tasks
- Core assumption: The routing mechanism preferentially engages sub-networks with stronger medical/domain knowledge when processing clinical vignettes
- Evidence anchors:
  - [section]: "This advantage can be attributed to DeepSeek's mixture-of-experts (MoE) architecture, which employs dynamic query routing to specialized neural sub-networks. Such design empowers the model to more effectively harness domain-specific knowledge, producing responses characterized by both precision and clinical relevance."
  - [corpus]: Weak direct evidence—neighboring papers mention DeepSeek-R1/V3 in social sciences and agentic scenarios but do not dissect MoE routing mechanisms in clinical domains
- Break condition: If routing decisions are primarily surface-level (keyword-based) rather than semantic/clinical, domain advantage may not generalize to unseen case structures

### Mechanism 2
- Claim: Faithfulness scores may serve as a reliable computational proxy for expert clinical judgment
- Mechanism: Faithfulness quantifies the proportion of claims in generated responses that are supported by reference answers, approximating point-by-point expert grading
- Core assumption: Reference answers adequately represent clinical ground truth and claim extraction reliably identifies medically relevant assertions
- Evidence anchors:
  - [abstract]: "DeepSeek V3 achieved the highest faithfulness score (median 0.528) and expert ratings (median 4.5/5), significantly outperforming the other models (p < 0.001)"
  - [section]: "Expert evaluations served as the gold standard... DeepSeek demonstrated overwhelming superiority over other models, whereas Copilot consistently underperformed, a pattern mirroring their faithfulness metric rankings"
  - [corpus]: No direct corpus evidence validating faithfulness-expert correlation in dental/medical domains
- Break condition: If reference answers contain systematic biases or LLMs generate valid clinical reasoning not captured in references, faithfulness will undercount quality

### Mechanism 3
- Claim: Structured three-step conversational prompting may improve LLM performance on complex longitudinal case analysis
- Mechanism: Separating context ingestion from question answering reduces cognitive load and prevents premature summarization, allowing models to maintain case context across multiple queries
- Core assumption: Models can effectively retain and retrieve case information across conversational turns without explicit memory mechanisms
- Evidence anchors:
  - [section]: "We designed a three-step conversational framework... First, the model was assigned a specific role... Second, the full case narrative was provided to establish context. Third, open-ended clinical questions were presented."
  - [corpus]: No corpus evidence comparing multi-step vs. single-prompt approaches for clinical vignettes
- Break condition: If context window overflow or attention degradation occurs for long cases (2,000–6,000 words), performance may drop for later questions

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) Architecture**
  - Why needed here: Explains DeepSeek's potential performance advantage; MoE models activate sparse subsets of parameters per query, enabling specialization
  - Quick check question: Can you explain how MoE routing differs from dense model inference, and what "expert" means in this context?

- Concept: **RAGAs Evaluation Metrics (Faithfulness, Answer Relevancy)**
  - Why needed here: The study uses these automated metrics as primary performance indicators; understanding their computation is essential for interpreting results
  - Quick check question: How is faithfulness computed, and what are its limitations compared to human evaluation?

- Concept: **MELD Score for Data Contamination Detection**
  - Why needed here: Validates that model performance reflects reasoning rather than memorization of training data
  - Quick check question: What MELD threshold indicates likely memorization, and what were the scores in this study?

## Architecture Onboarding

- Component map:
  Input layer: Longitudinal case vignettes (2,000–6,000 words) → Three-step prompt formatting
  Model layer: 4 LLMs evaluated (DeepSeek V3, GPT-4o, Gemini 2.0 Flash, Copilot)
  Evaluation layer: RAGAs (faithfulness, relevancy) + Flesch-Kincaid (readability) + Blinded expert ratings (5-point Likert)
  Validation layer: MELD scores for contamination detection

- Critical path:
  1. Case curation → 2. Prompt formatting → 3. Model inference → 4. Automated metrics → 5. Expert evaluation → 6. Statistical comparison

- Design tradeoffs:
  - Faithfulness vs. Readability: DeepSeek ranked #1 in faithfulness but #2 in readability (Copilot was more accessible)
  - Fine-tuning vs. Expert preference: GPT-4o fine-tuning improved faithfulness (+0.036) but decreased expert scores (−0.23), suggesting optimization toward brevity may reduce perceived clinical completeness
  - Excluding images: Technical constraints limited analysis to text-only; multimodal clinical data remains unexplored

- Failure signatures:
  - Gemini: High faithfulness variance—~1/3 of responses scored <0.5 on answer relevancy despite reasonable faithfulness, suggesting "correct but tangential" outputs
  - Copilot: Lowest faithfulness (0.367) and expert scores despite best readability, indicating accessible but less accurate responses
  - Fine-tuned models: May optimize for metric gaming rather than clinical utility

- First 3 experiments:
  1. Replicate the three-step prompting framework on your domain's case vignettes; measure faithfulness-expert correlation to validate proxy
  2. Test MoE vs. dense models on the same cases with controlled prompts to isolate architecture effects (assumption: access to both model types)
  3. Implement multi-expert reference standard creation to reduce single-author bias in ground truth; compare metric stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating multimodal image data (radiographs/photographs) with text inputs improve the diagnostic accuracy of LLMs in periodontal case analysis?
- Basis in paper: [explicit] The authors state in the Limitations section that image data were excluded due to technical constraints, but future barriers "may be overcome, enabling more robust analysis... by integrating image inputs."
- Why unresolved: Current multimodal capabilities for specialized medical imagery were noted as suboptimal compared to dedicated vision models, leading to the exclusion of visual data in this specific study design.
- What evidence would resolve it: A follow-up study utilizing the same case vignettes supplemented with their original clinical images, evaluating performance changes using the same automated and expert-based metrics.

### Open Question 2
- Question: Can a domain-specific model fine-tuned on a larger dental dataset outperform the base DeepSeek V3 model in expert clinical ratings?
- Basis in paper: [explicit] The Discussion notes that while GPT-4o fine-tuning improved faithfulness, it slightly decreased expert scores. The authors explicitly aim to "develop a larger domain-specific dataset and build a specialized medical language model using open-source foundations like DeepSeek."
- Why unresolved: It remains unclear if the trade-off observed in GPT-4o (improved metric scores vs. decreased human preference) is inherent to fine-tuning or specific to the dataset/model used.
- What evidence would resolve it: Performance benchmarks of a DeepSeek-based model fine-tuned on the proposed expanded dataset, specifically comparing expert Likert scores against the base model.

### Open Question 3
- Question: Does establishing multi-expert consensus for reference answers eliminate the scoring discrepancies between automated faithfulness metrics and human evaluation?
- Basis in paper: [explicit] The authors acknowledge that "gold-standard reference answers were unilaterally defined by the authors," which introduced bias and occasionally penalized LLMs for responses experts rated highly.
- Why unresolved: The current methodology may unfairly penalize models that generate clinically valid responses that differ from the single-author "gold standard," skewing automated metric reliability.
- What evidence would resolve it: A comparative analysis where automated faithfulness scores are calculated against both single-author and multi-expert consensus reference keys to measure correlation with human evaluator scores.

## Limitations
- Exclusion of imaging data limits applicability to multimodal clinical workflows
- Reference answers represent single clinical perspective, introducing potential bias
- MELD scores approaching contamination threshold (0.5) raise questions about memorization versus reasoning

## Confidence

- **High Confidence**: DeepSeek V3 outperformed other models on faithfulness and expert ratings (statistically significant, p < 0.001)
- **Medium Confidence**: MoE architecture advantage is plausible but requires validation through ablation studies comparing MoE vs. dense models on same cases
- **Low Confidence**: Faithfulness-expert correlation in dental contexts; corpus evidence is absent and correlation may not hold across different case types

## Next Checks
1. Conduct blinded multi-expert evaluation using reference answers from 3+ periodontists to assess single-author bias impact on metric stability
2. Test RAG-enabled versions of each LLM with clinical knowledge bases to compare faithfulness gains versus prompt-only approaches
3. Perform case reconstruction analysis where models are given incomplete case information to measure robustness of longitudinal reasoning across missing data scenarios