---
ver: rpa2
title: 'SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual
  Connection'
arxiv_id: '2509.16060'
source_url: https://arxiv.org/abs/2509.16060
tags:
- saber
- arxiv
- layer
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SABER, a white-box jailbreak method that
  improves attack success rates on safety-aligned LLMs by 51% over the best baseline.
  The core idea is to connect two intermediate layers through a residual connection,
  bypassing safety mechanisms that are shown to reside primarily in middle-to-late
  layers.
---

# SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection

## Quick Facts
- **arXiv ID:** 2509.16060
- **Source URL:** https://arxiv.org/abs/2509.16060
- **Reference count:** 30
- **Primary result:** SABER achieves 51% improvement in attack success rates over baselines by connecting intermediate layers with residual connections

## Executive Summary
SABER is a white-box jailbreak method that exploits vulnerabilities in safety-aligned LLMs by introducing cross-layer residual connections. The approach identifies that safety mechanisms primarily reside in middle-to-late layers and creates bypass pathways by connecting intermediate layers with carefully calibrated scaling factors. This architectural perturbation significantly increases harmful output generation while preserving general language capabilities, demonstrating a fundamental vulnerability in current safety alignment frameworks.

## Method Summary
The SABER method introduces cross-layer residual connections between two intermediate layers of a transformer model, with a carefully calibrated scaling factor that balances harmful content generation against general language preservation. The approach requires white-box access to model weights and operates by identifying optimal layer pairs where safety mechanisms are weakest. The residual connection effectively creates alternative pathways for information flow, bypassing safety-aligned transformations in middle-to-late layers while maintaining the model's fundamental language understanding capabilities.

## Key Results
- Achieves 51% improvement in attack success rates over best baseline methods
- Shows consistent performance improvements across HarmBench, AdvBench, and JailbreakBench
- Minimal impact on perplexity and reasoning performance while increasing harmful output generation
- Demonstrates effectiveness on both Llama-2 and Mistral model families (7B and 13B parameters)

## Why This Works (Mechanism)
The effectiveness of SABER stems from the observation that safety alignment mechanisms are concentrated in middle-to-late layers of transformer models. By introducing cross-layer residual connections between intermediate layers, the method creates alternative information pathways that bypass these safety-aligned transformations. The carefully calibrated scaling factor ensures that while harmful content generation is amplified, the fundamental language capabilities of the model remain largely intact. This architectural vulnerability reveals that safety mechanisms, once bypassed through strategic residual connections, cannot fully prevent the generation of harmful content.

## Foundational Learning

**Residual Connections:** Skip connections that add outputs from previous layers to subsequent layers, enabling gradient flow and feature reuse. *Why needed:* Essential for understanding how SABER creates bypass pathways. *Quick check:* Verify that adding a layer's output to another layer's output creates a direct information path.

**Layer-wise Safety Alignment:** The phenomenon where safety mechanisms are concentrated in specific layers (typically middle-to-late). *Why needed:* Core insight enabling SABER's effectiveness. *Quick check:* Examine safety behavior across different layers to identify concentration zones.

**Scaling Factor Calibration:** The process of determining optimal multiplicative factors for residual connections. *Why needed:* Critical for balancing harmful content amplification with language capability preservation. *Quick check:* Test different scaling values to find the optimal trade-off point.

**White-box vs Black-box Attacks:** Distinction between methods requiring model access versus those working through APIs. *Why needed:* SABER's applicability constraints. *Quick check:* Determine whether attack parameters transfer between models with similar architectures.

## Architecture Onboarding

**Component Map:** Input -> Encoder Layers -> Cross-Layer Residual Connection -> Output
The critical path involves identifying optimal layer pairs, calculating scaling factors, and implementing residual connections that bypass safety mechanisms while preserving language capabilities.

**Critical Path:** 1) Layer analysis to identify safety concentration zones, 2) Scaling factor optimization, 3) Residual connection implementation, 4) Attack evaluation

**Design Tradeoffs:** Higher scaling factors increase harmful output but risk degrading general capabilities; lower factors preserve capabilities but reduce attack effectiveness. The method trades white-box access requirements for significant attack success improvements.

**Failure Signatures:** If scaling factor is too high, perplexity increases and reasoning degrades; if too low, attack success rates remain similar to baseline. Ineffective layer pair selection results in minimal safety bypass.

**First Experiments:**
1. Test residual connections between all possible layer pairs to identify optimal bypass locations
2. Vary scaling factors systematically to find the sweet spot between attack success and capability preservation
3. Compare layer-specific safety behavior before and after residual connection implementation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that the approach requires white-box access and further validation across different model architectures and sizes is needed.

## Limitations
- Requires white-box access to model weights, limiting real-world applicability
- Experiments limited to Llama-2 and Mistral models (7B and 13B parameters)
- Evaluation focuses primarily on attack success rates without comprehensive behavioral analysis
- Scaling factor optimization assumes availability of both harmful and harmless prompts

## Confidence

**High confidence:** Core finding that safety mechanisms reside in middle-to-late layers is well-supported by ablation studies and layer-specific analyses. The 51% improvement over baselines is statistically significant and consistently observed.

**Medium confidence:** Claims about preserving general language capabilities while increasing harmful output are supported by perplexity and reasoning tests, but evaluation scope is limited. The assertion about fundamental vulnerability in safety alignment frameworks is plausible but needs broader validation.

**Low confidence:** Specific mechanisms for how safety mechanisms are bypassed lack detailed explanation. Generalizability to black-box scenarios and different safety alignment techniques remains speculative.

## Next Checks

1. Test SABER's effectiveness against black-box access scenarios by evaluating transferability of white-box optimized attacks to API-accessible models.

2. Conduct comprehensive behavioral evaluations beyond perplexity and reasoning, including bias amplification, factual consistency, and general task performance degradation.

3. Evaluate the method's effectiveness across a broader range of model sizes (e.g., 30B-70B parameters) and architectures (e.g., GPT-style models) to assess scalability limitations.