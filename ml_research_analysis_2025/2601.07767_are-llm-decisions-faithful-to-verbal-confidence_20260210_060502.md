---
ver: rpa2
title: Are LLM Decisions Faithful to Verbal Confidence?
arxiv_id: '2601.07767'
source_url: https://arxiv.org/abs/2601.07767
tags:
- confidence
- utility
- answer
- penalty
- deepseek-v3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RiskEval, a framework to test whether LLMs
  align their decisions with stated confidence under varying risk. Models were prompted
  to answer or abstain under penalties ranging from 0 to 100, with their verbalized
  confidence recorded.
---

# Are LLM Decisions Faithful to Verbal Confidence?

## Quick Facts
- **arXiv ID:** 2601.07767
- **Source URL:** https://arxiv.org/abs/2601.07767
- **Reference count:** 40
- **Primary result:** LLMs report calibrated confidence but fail to use it for risk-sensitive decisions, leading to negative utility under high penalties.

## Executive Summary
This paper investigates whether large language models (LLMs) can translate their verbalized confidence into optimal abstention decisions under varying risk. Using a penalty-based elicitation framework called RiskEval, the authors prompt models to answer or abstain from questions with penalties ranging from 0 to 100. Despite reporting confidence scores, models maintain high answer rates regardless of penalty, resulting in sharply negative normalized utility as errors become costly. The findings reveal a fundamental dissociation: models can express calibrated uncertainty but lack the strategic agency to act on it in high-stakes scenarios.

## Method Summary
The RiskEval framework tests LLM decision-making under risk by prompting models to answer or abstain from questions with explicit penalty structures. Across three datasets (HLE, GPQA Diamond, GSM8K), models are presented with questions and asked to either answer or abstain, with their verbalized confidence recorded. Penalties are varied from 0 to 100 to create different risk levels. The study measures answer rates, normalized utility, policy consistency, and normalized regret across penalty levels, while also testing whether explicit instructions to use confidence improve risk-sensitive behavior.

## Key Results
- Models maintain high answer rates regardless of penalty magnitude, leading to negative normalized utility as penalties increase
- Policy consistency drops and normalized regret rises monotonically with penalty, indicating failure to follow optimal abstention thresholds
- Calibration metrics remain stable, showing verbalized confidence exists but is not used for decision-making
- Explicit instructions to use confidence do not improve risk-sensitive behavior

## Why This Works (Mechanism)
The study demonstrates that LLMs possess the capability to generate calibrated confidence estimates but lack the cognitive architecture to convert these estimates into risk-sensitive decisions. The mechanism appears to be a dissociation between the model's internal uncertainty representation and its decision-making process, where confidence signals are generated but not integrated into the consequentialist reasoning required for optimal abstention under risk.

## Foundational Learning
- **Confidence Calibration:** Models can generate confidence scores that correlate with actual accuracy
  - *Why needed:* To establish that the issue is not with confidence estimation but with its utilization
  - *Quick check:* Compare verbalized confidence to actual accuracy rates

- **Risk-Utility Framework:** Understanding how penalties and rewards should influence decision-making
  - *Why needed:* To define optimal abstention thresholds under varying risk levels
  - *Quick check:* Calculate expected utility for different answer/abstain strategies

- **Normalized Utility Metrics:** Measuring decision quality relative to optimal behavior
  - *Why needed:* To quantify the gap between actual and optimal risk-sensitive decisions
  - *Quick check:* Compare model performance to theoretical optimal strategy

## Architecture Onboarding
- **Component Map:** Input Question -> Confidence Generation -> Decision Module -> Output (Answer/Abstain)
- **Critical Path:** The decision-making pathway that should integrate confidence scores with penalty information
- **Design Tradeoffs:** Models optimized for generation accuracy may not be optimized for risk-sensitive decision-making
- **Failure Signatures:** High answer rates regardless of penalty, negative normalized utility, monotonic increase in regret
- **First Experiments:**
  1. Test whether models understand penalty structure by asking direct questions about risk
  2. Compare behavior across different model sizes and architectures
  3. Test whether providing explicit risk calculations improves decision quality

## Open Questions the Paper Calls Out
None

## Limitations
- RiskEval measures behavioral responses to a specific penalty-based paradigm but doesn't establish whether confidence representations themselves are flawed
- Results may not generalize to domains with different types of uncertainty or complex downstream consequences
- The study assumes verbalized confidence scores are accurate and meaningful representations of internal reasoning

## Confidence
- **High Confidence:** Empirical observation that models maintain high answer rates regardless of penalty, leading to negative normalized utility
- **Medium Confidence:** Claim that this represents a fundamental dissociation between calibrated uncertainty reporting and strategic decision-making
- **Low Confidence:** Generalizability of findings to other domains and uncertainty types

## Next Checks
1. Test whether models show improved risk-sensitive behavior under alternative prompting strategies, such as explicit risk assessments before answering
2. Conduct controlled fine-tuning experiments where models are trained to use confidence scores for abstention decisions
3. Use mechanistic interpretability techniques to determine whether confidence representations are computed but ignored during decision-making