---
ver: rpa2
title: 'CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation
  Learning'
arxiv_id: '2506.17818'
source_url: https://arxiv.org/abs/2506.17818
tags:
- music
- learning
- pre-training
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CultureMERT addresses the limited cross-cultural effectiveness
  of music foundation models by developing a culturally adapted model through continual
  pre-training. The study introduces a two-stage CPT strategy with learning rate re-warming
  and staged adaptation to stabilize training and enable effective cross-cultural
  adaptation under computational constraints.
---

# CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning

## Quick Facts
- arXiv ID: 2506.17818
- Source URL: https://arxiv.org/abs/2506.17818
- Authors: Angelos-Nikolaos Kanatas; Charilaos Papaioannou; Alexandros Potamianos
- Reference count: 0
- Key outcome: CultureMERT achieves 4.9% average ROC-AUC/AP improvement on non-Western music auto-tagging while maintaining Western performance

## Executive Summary
CultureMERT addresses the limited cross-cultural effectiveness of music foundation models by developing a culturally adapted model through continual pre-training. The study introduces a two-stage CPT strategy with learning rate re-warming and staged adaptation to stabilize training and enable effective cross-cultural adaptation under computational constraints. Training on 650 hours of multi-cultural data spanning Greek, Turkish, and Indian music traditions, CultureMERT achieves an average improvement of 4.9% in ROC-AUC and AP across non-Western music auto-tagging tasks, surpassing prior state-of-the-art while maintaining performance on Western benchmarks. Additionally, task arithmetic—an alternative approach that merges single-culture adapted models in weight space—performs comparably to multi-cultural CPT on non-Western tasks and even outperforms both the pre-trained and multi-culturally adapted models on Western benchmarks. The study also reveals varying cross-cultural transferability among single-culture adaptations, with Carnatic-adapted models showing the most consistent generalization.

## Method Summary
CultureMERT adapts the MERT-v1-95M foundation model to non-Western music through a two-stage continual pre-training strategy. The method employs staged parameter adaptation where the Transformer encoder is frozen during Stage 1 while only the CNN feature extractor and codeword embeddings are updated to calibrate low-level acoustic representations. Learning rate re-warming with cosine decay schedules is used to improve convergence and reduce catastrophic forgetting. The model is trained on 650 hours of multi-cultural data (200h Turkish-makam, 200h Carnatic, 200h Hindustani, 50h Lyra) using a multi-task MLM objective with EnCodec tokens and CQT reconstruction. Western replay data (20% of Stage 1 batch) prevents performance degradation on Western benchmarks. An alternative task arithmetic approach merges culture-specific parameter shifts in weight space to create effective multi-cultural models without joint training.

## Key Results
- 4.9% average improvement in ROC-AUC and AP across non-Western music auto-tagging tasks
- Outperforms prior state-of-the-art while maintaining Western benchmark performance
- Task arithmetic performs comparably to multi-cultural CPT on non-Western tasks and surpasses both pre-trained and multi-culturally adapted models on Western benchmarks
- Carnatic-adapted models show the most consistent cross-cultural generalization among single-culture adaptations

## Why This Works (Mechanism)

### Mechanism 1
Staged parameter adaptation mitigates the stability gap when adapting to distribution-shifted data. Freezing the Transformer encoder during Stage 1 while updating only the CNN feature extractor and codeword embeddings allows low-level acoustic representations to calibrate to the new distribution before high-level representations adapt. This reduces the "plasticity gradient" that causes temporary degradation before stabilization. The core assumption is that CNN and codeword embeddings capture culture-adjacent acoustic features that transfer more readily than high-level Transformer representations.

### Mechanism 2
Learning rate re-warming with cosine decay schedules improves convergence and reduces forgetting during CPT. Resetting the learning rate to a higher value at the start of CPT allows the optimizer to escape local minima from pre-training, while the subsequent decay schedule gradually stabilizes learning. Different warm-up percentages balance early adaptation vs. stability. The core assumption is that the pre-trained model's optimizer state is suboptimal for the new domain, and re-initializing the learning rate schedule provides beneficial exploration.

### Mechanism 3
Task arithmetic creates effective multi-cultural models by linearly combining culture-specific parameter shifts in weight space. Task vectors encode the parameter changes needed to adapt to each culture, and summing these with scaling factors creates a merged model that retains multi-cultural knowledge without requiring joint training. The optimal λ=0.2 suggests cultures share overlapping acoustic features that benefit from partial combination rather than full addition. The core assumption is that cultural adaptations occupy approximately orthogonal directions in parameter space, enabling linear superposition without destructive interference.

## Foundational Learning

- **Catastrophic forgetting**: Why needed here: CPT on non-Western music could degrade Western task performance; understanding this motivates the two-stage strategy with Western replay. Quick check: Can you explain why updating all model parameters simultaneously on a shifted distribution causes performance drops on the original distribution?

- **Self-supervised masked prediction (MLM-style objectives)**: Why needed here: CultureMERT uses the MERT-RVQ-VAE objective with acoustic token prediction and CQT reconstruction—understanding pseudo-label generation from teacher models is essential. Quick check: How does the EnCodec tokenizer provide training targets, and why is it kept frozen during CPT?

- **Task vectors and weight-space operations**: Why needed here: Task arithmetic requires understanding that fine-tuned weights can be decomposed into base + task-specific direction, enabling algebraic manipulation. Quick check: Why does scaling the task vector by λ<1 typically improve merged model performance compared to direct averaging?

## Architecture Onboarding

- **Component map**: Raw Audio (24kHz) → CNN Feature Extractor (frozen in Stage 1) → 768-dim frame embeddings (75Hz) → 12-layer Transformer Encoder (frozen in Stage 1) → Two prediction heads: 1) RVQ token prediction (8 codebooks × 1024 codewords) 2) CQT spectrogram reconstruction. Teacher Models (frozen): EnCodec (acoustic) + CQT target generator.

- **Critical path**: Verify EnCodec tokenizer produces expected codebook shapes (K=8, C=1024). Confirm probing evaluation uses frozen backbone with shallow MLP head. Validate learning rate schedule matches paper (Stage 1: ηmax=5e-4→5e-5, Stage 2: ηmax=5e-5→5e-6).

- **Design tradeoffs**: Two-stage CPT vs. single-stage: Two-stage is more stable but requires manual stage boundaries and hyperparameter selection per stage. Multi-cultural CPT vs. task arithmetic: CPT requires joint training data; TA requires pre-existing single-culture models but is training-free. Western replay ratio: 20% used; higher ratios improve Western retention but may slow cultural adaptation.

- **Failure signatures**: Training instability/crashes with single-stage full-parameter adaptation (observed with batch size mismatch). Performance drop on Western benchmarks >1% suggests insufficient stability mechanisms. Task arithmetic with λ>0.5 causes degradation across all benchmarks.

- **First 3 experiments**: 1) Baseline reproduction: Load MERT-v1-95M, run probing evaluation on all 6 datasets to verify baseline metrics match Table 2 (ROC-AUC: Turkish 83.2, MTAT 89.6). 2) Ablation on stage necessity: Compare single-stage CPT (all parameters from start) vs. two-stage on Turkish-makam task to quantify the stability gap reduction. 3) Task arithmetic scaling sweep: Merge four single-culture models with λ ∈ {0.1, 0.2, 0.25, 0.3, 0.5} to validate optimal λ=0.2 and observe performance sensitivity.

## Open Questions the Paper Calls Out

1. **Cross-cultural tokenizer adaptation**: Does replacing the frozen, Western-centric EnCodec tokenizer with a tokenizer pre-trained on culturally diverse music yield better performance for cross-cultural representation learning? The authors state that "The frozen EnCodec tokenizer used in the MERT architecture may be suboptimal for encoding culturally diverse musical languages, as it was pre-trained on Western music." This correlation was analyzed post-hoc rather than used to guide the training itself.

2. **Computational constraints necessity**: Is the proposed two-stage continual pre-training strategy redundant when computational constraints are removed (e.g., when using batch sizes comparable to the original MERT pre-training)? The two-stage method was specifically designed to address instability caused by small batch sizes imposed by hardware limitations; it is unknown if the stability gap persists with sufficient compute.

3. **Task generalization**: Does the improved cross-cultural representation of CultureMERT generalize to non-classification tasks, such as music generation, transcription, or source separation? The evaluation protocol relies exclusively on auto-tagging and multi-label classification, leaving performance on generative or dense prediction tasks unverified.

4. **Predictive token similarity**: Can token-level similarity metrics be used a priori to predict negative transfer or optimize data mixtures for new, unseen musical traditions? While the correlation is observed, the utility of this metric as a predictive tool for designing training curricula or data mixes has not been validated experimentally.

## Limitations
- Architectural generalization: The two-stage CPT strategy is demonstrated only on MERT-v1-95M; effectiveness for other music foundation models remains untested.
- Distribution shift assumptions: The staged adaptation mechanism assumes culture-specific shifts primarily affect low-level acoustic features before high-level semantic features, which may not generalize to all cases.
- Task arithmetic validity: The method's robustness to more diverse or conflicting cultural adaptations is unknown, as the study observes orthogonality only for the three cultures tested.

## Confidence
- **High Confidence**: Core empirical results (ROC-AUC/AP improvements, comparison of CPT vs. TA) are well-supported by the experimental setup and repeated measurements across six datasets.
- **Medium Confidence**: The mechanism explaining staged adaptation's stability (plasticity gradient reduction) is plausible but direct causal evidence specific to music domains is limited.
- **Medium Confidence**: The effectiveness of learning rate re-warming is supported by observed stable training curves, but specific schedule choices are not rigorously ablated against alternatives.

## Next Checks
1. **Ablation of Stage 1 Necessity**: Re-run the CultureMERT training pipeline on the Turkish-makam dataset with a single-stage vs. the two-stage approach to quantify the stability gap by measuring steps to reach equivalent performance and incidence of training crashes/NaNs.

2. **Cross-Architectural Validation**: Apply the two-stage CPT strategy to a different music foundation model (e.g., Jukebox variant or larger MERT model) and evaluate its effectiveness on the same Greek/Turkish/Indian benchmarks to check if staged adaptation is a general principle.

3. **Task Arithmetic Stress Test**: Generate task vectors for a wider set of cultural adaptations (e.g., adding Persian or Chinese traditional music) and perform arithmetic merging with systematic variation of scaling factors (λ) to test limits of the orthogonality assumption and identify conditions for destructive interference.