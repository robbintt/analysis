---
ver: rpa2
title: Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks
arxiv_id: '2507.21974'
source_url: https://arxiv.org/abs/2507.21974
tags:
- root
- reasoning
- cause
- network
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles Root Cause Analysis (RCA) in 5G wireless networks
  by proposing a framework that uses reasoning-enhanced Large Language Models (LLMs).
  To enable this, the authors introduce TeleLogs, a synthetic dataset of 5G troubleshooting
  scenarios with annotated root causes.
---

# Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks

## Quick Facts
- **arXiv ID:** 2507.21974
- **Source URL:** https://arxiv.org/abs/2507.21974
- **Reference count:** 16
- **Primary result:** TeleLogs dataset + two-stage training (SFT + GRPO) enables reasoning LLMs to achieve >95% accuracy on 5G root cause analysis tasks.

## Executive Summary
This paper tackles Root Cause Analysis (RCA) in 5G wireless networks by proposing a framework that uses reasoning-enhanced Large Language Models (LLMs). To enable this, the authors introduce TeleLogs, a synthetic dataset of 5G troubleshooting scenarios with annotated root causes. The proposed method employs a two-stage training process: first, supervised fine-tuning with a multi-agent pipeline that generates structured reasoning traces, then reinforcement learning using Group Relative Policy Optimization (GRPO) to improve accuracy and reasoning depth. Experiments show that the fine-tuned models outperform both base LLMs and state-of-the-art reasoning models, with Qwen2.5-RCA-32B achieving over 95% accuracy and demonstrating strong generalization to randomized test variants. The results indicate that domain-adapted reasoning LLMs can provide both accurate and interpretable RCA in complex network environments.

## Method Summary
The authors propose a two-stage training pipeline for 5G root cause analysis. First, they generate a synthetic dataset (TeleLogs) using a multi-agent system that produces structured reasoning traces through Elimination and Contradiction strategies, then aggregates them into a compact format. Second, they fine-tune Qwen2.5-Instruct models using supervised fine-tuning (SFT) on these traces, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) that rewards correct diagnoses. The framework maps network engineering parameters and user-plane data to root cause categories through a probabilistic inverse problem formulation, with the final output providing both the diagnosis and an interpretable reasoning trace.

## Key Results
- Qwen2.5-RCA-32B achieves over 95% accuracy on the TeleLogs test set, significantly outperforming base LLMs and other reasoning models
- RL fine-tuning (SFT+RL) improves performance dramatically compared to SFT alone, with the 32B model jumping from 49.45% to 95.86% accuracy
- The 32B model maintains ~93% accuracy on randomized test variants, demonstrating strong generalization beyond positional memorization
- Smaller 1.5B models show performance degradation on randomized tests (87% → 75%), highlighting the importance of model capacity for robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised Fine-Tuning (SFT) anchors a generalist LLM to the specific procedural logic of network troubleshooting, converting unstructured knowledge into diagnostic behavior.
- **Mechanism:** The pipeline utilizes a multi-agent system to generate diverse reasoning traces (Elimination and Contradiction strategies). An aggregator then distills these into a structured "Chain-of-Thought" (Data Analysis → Root Cause Analysis → Identification). Training on these synthesized traces conditions the model to follow a consistent, interpretable diagnostic procedure rather than free-form generation.
- **Core assumption:** The multi-agent aggregation process successfully filters out backtracking and noise while preserving the causal logic required for accurate diagnosis.
- **Evidence anchors:** [Section V.A]: Mentions the aggregator "reformulates the trajectory into a structured and compact format... improving the sample efficiency." [Section V.A]: Describes "Elimination-based prompting" and "Contradiction-based prompting" as key strategies.
- **Break condition:** If the Aggregator fails to remove logical inconsistencies from the agent outputs, the SFT model may learn to hallucinate plausible-sounding but faulty diagnostic steps.

### Mechanism 2
- **Claim:** Reinforcement Learning (RL) via Group Relative Policy Optimization (GRPO) refines the model's reasoning capability by rewarding outcome accuracy over token probability.
- **Mechanism:** GRPO samples multiple trajectories ($N=8$) for a given network state and computes advantages based on group rewards (Eq. 7). This shifts the optimization from merely predicting the next token (as in SFT) to maximizing the likelihood of a correct diagnosis. It encourages the model to explore reasoning paths that lead to the correct root cause, even if those paths were under-represented in the SFT data.
- **Core assumption:** The binary reward function (Eq. 3), which checks only the final answer, provides a sufficient gradient signal to improve the quality of the intermediate reasoning steps.
- **Evidence anchors:** [Section V.B]: Defines the advantage estimate $\hat{A}_{i,j}$ based on group rewards. [Section VI.A]: Shows SFT+RL (95.86%) significantly outperforms SFT alone (49.45%) for the 32B model.
- **Break condition:** If the reward is sparse or the initialization (SFT policy) is too weak, the RL phase may suffer from low sample efficiency or diverge from the domain knowledge established during SFT.

### Mechanism 3
- **Claim:** Domain randomization during testing enforces genuine causal inference rather than positional memorization.
- **Mechanism:** By randomizing root cause identifiers (e.g., $C1, C2$) and table orders in the prompt, the evaluation forces the model to process the network engineering parameters ($U$) and observations ($Y_t$) to determine the cause, rather than relying on spurious correlations or "position-based heuristics."
- **Core assumption:** The model learns the underlying function $f$ (Eq. 1) mapping symptoms to causes, rather than statistical artifacts of the prompt structure.
- **Evidence anchors:** [Section VI.C]: "This setup is designed to prevent models from relying on position-based heuristics... Qwen2.5-32B retains high accuracy."
- **Break condition:** If the model's performance collapses on the randomized dataset, it indicates memorization of prompt patterns rather than learned reasoning.

## Foundational Learning

- **Concept:** Probabilistic Inverse Problems
  - **Why needed here:** The paper frames RCA explicitly as approximating a posterior probability $p(c | U, Y_t, s_t)$ (Eq. 2). Understanding this helps grasp why LLMs are suitable—they are probabilistic engines capable of modeling complex, non-linear dependencies.
  - **Quick check question:** Can you explain why finding the root cause from observed symptoms is defined as an "inverse problem" in Eq. 1?

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** This is the specific RL algorithm used to surpass SFT performance. It differs from standard PPO by estimating advantages relative to a group of samples rather than a single trajectory.
  - **Quick check question:** How does GRPO calculate the advantage estimate $\hat{A}_{i,j}$ in Eq. 7, and why does this remove the need for a separate value-function model?

- **Concept:** Chain-of-Thought (CoT) Aggregation
  - **Why needed here:** The data generation pipeline relies on synthesizing expert-like reasoning traces. Understanding how raw traces are converted into the structured format $F$ (Fig. 4) is critical for reproducing the dataset.
  - **Quick check question:** What is the role of the "Aggregator" agent in the SFT data generation pipeline, and how does it affect token count?

## Architecture Onboarding

- **Component map:** Network Configs ($U$) + Drive Test Data ($Y_t$) + Symptom ($s_t$) → Multi-Agent Prompting (Elimination/Contradiction) → Aggregator → Structured CoT Dataset → Base LLM → SFT (15k steps) → RL/GRPO → Qwen2.5-RCA → Reasoning Trace $\tau$ + Root Cause ID

- **Critical path:** The **SFT Data Generation** (Section V.A) is the primary bottleneck. The quality of the reasoning model depends entirely on the "richness" of the traces generated by the agent pipeline and the efficiency of the aggregator.

- **Design tradeoffs:**
  - **Model Size vs. Generalization:** The 1.5B model suffers a significant drop on randomized tests (87% → 75%), while the 32B model maintains ~93% (Table III). Smaller models may be cheaper but are less robust to distribution shifts.
  - **SFT vs. RL:** RL provides the largest performance boost for larger models, but SFT is strictly required to stabilize training (Section V.A).

- **Failure signatures:**
  - **Position Bias:** Model always selecting the first or last option in the provided list (fixed by randomization).
  - **Logic Drift:** During RL, if the KL penalty $\beta$ is too low, the model may output correct answers with nonsensical reasoning traces.

- **First 3 experiments:**
  1. **Baseline Validation:** Evaluate a base Qwen2.5-32B on the TeleLogs test set without fine-tuning to confirm the "struggle" baseline mentioned in the Abstract.
  2. **Ablation on Data Strategy:** Train two small SFT models—one using raw agent traces and one using "Aggregated" traces—to quantify the impact of the structured format $F$ on convergence speed.
  3. **Generalization Stress Test:** Train on the standard dataset and evaluate immediately on the "Randomized Test Dataset" to ensure the model is learning causal rules and not prompt artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to accurately diagnose scenarios where multiple simultaneous misconfigurations or faults contribute to a single observed symptom?
- Basis in paper: [explicit] The conclusion states that future work will focus on "extending the method to handle multiple root cause scenarios."
- Why unresolved: The current problem formulation (Section III) and the TeleLogs dataset focus on identifying a single root cause $c$ from a set of causes $C$ for a specific symptom.
- What evidence would resolve it: A modification of the loss function and output structure to support multi-label classification, validated on a dataset with compound fault scenarios.

### Open Question 2
- Question: How does the performance and reasoning fidelity of the fine-tuned models degrade when applied to noisy, incomplete real-world operational data compared to the synthetic TeleLogs benchmark?
- Basis in paper: [explicit] The conclusion identifies "incorporating real-world operational data" as a focus for future work.
- Why unresolved: The paper acknowledges TeleLogs is a "synthetic dataset constructed simulating a network," and while randomized, it lacks the noise and unstructured nature of live network logs.
- What evidence would resolve it: Benchmark results of the Qwen2.5-RCA models on a released dataset of real-world drive tests or network outages with ground-truth labels.

### Open Question 3
- Question: Does the binary reward function (Equation 3) inadvertently reinforce "right answer, wrong reasoning" trajectories due to the lack of process supervision?
- Basis in paper: [inferred] The paper defines the reward $R(\tau, c)$ as a binary signal based solely on whether the extracted answer matches the ground truth, not the validity of the intermediate steps.
- Why unresolved: While the aggregator agent (Section V-A) structures the reasoning, the RL fine-tuning optimizes for the final outcome, leaving the causal validity of the "thought process" during RL unverified.
- What evidence would resolve it: An ablation study analyzing the logical consistency of reasoning traces in models trained with outcome-based rewards versus process-based rewards.

## Limitations
- **Synthetic Data Dependency:** The TeleLogs dataset is synthetic, limiting validation of real-world performance on genuinely novel network configurations or rare failure modes.
- **GRPO Implementation Gaps:** The paper provides limited details on critical GRPO hyperparameters like the KL penalty coefficient ($\beta$), which is essential for preventing reasoning collapse.
- **Computational Cost Unaddressed:** The paper does not discuss inference computational costs for the 32B parameter models in production environments, which could be substantial.

## Confidence

- **High Confidence:** The core RCA task formulation (Eq. 1-2) and the two-stage training pipeline (SFT + GRPO) are well-established and reproducible based on the provided specifications.
- **Medium Confidence:** The effectiveness of the multi-agent aggregation strategy and the claim that randomization prevents memorization are supported by experimental results, but the underlying mechanisms could benefit from deeper analysis.
- **Low Confidence:** Claims about "superior interpretability" compared to non-LLM baselines are difficult to verify without direct comparison to traditional expert systems or statistical RCA methods.

## Next Checks

1. **Real-World Deployment Test:** Apply the fine-tuned Qwen2.5-RCA-32B model to anonymized field data from operational 5G networks to assess performance degradation and identify failure patterns not captured in synthetic scenarios.
2. **Ablation on Reward Structure:** Modify the binary reward function to provide graded feedback based on reasoning quality (e.g., partial credit for correct intermediate steps) and measure impact on both accuracy and reasoning trace coherence.
3. **Cross-Domain Transfer Study:** Fine-tune the model on TeleLogs, then evaluate its ability to perform RCA on related domains (e.g., 4G networks or enterprise Wi-Fi) to quantify the domain adaptation benefits claimed in the paper.