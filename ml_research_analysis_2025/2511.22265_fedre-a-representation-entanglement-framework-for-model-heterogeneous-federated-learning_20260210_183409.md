---
ver: rpa2
title: 'FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated
  Learning'
arxiv_id: '2511.22265'
source_url: https://arxiv.org/abs/2511.22265
tags:
- client
- representation
- fedre
- local
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of model-heterogeneous federated
  learning, where clients have different model architectures and data distributions.
  The authors propose a novel framework called Federated Representation Entanglement
  (FedRE) that introduces "entangled representations" as a new form of client knowledge.
---

# FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning

## Quick Facts
- arXiv ID: 2511.22265
- Source URL: https://arxiv.org/abs/2511.22265
- Authors: Yuan Yao; Lixu Wang; Jiaqi Wu; Jin Song; Simin Chen; Zehua Wang; Zijian Tian; Wei Chen; Huixia Li; Xiaoxiao Li
- Reference count: 40
- Primary result: Achieves up to 6.26% accuracy improvement over state-of-the-art methods in model-heterogeneous FL settings while reducing communication overhead

## Executive Summary
This paper addresses the challenge of federated learning when clients have different model architectures and data distributions. The authors propose Federated Representation Entanglement (FedRE), a framework that aggregates local representations into "entangled representations" using normalized random weights, then uploads these along with entangled-label encodings to a server for global classifier training. This approach achieves a favorable trade-off among model performance, privacy protection, and communication efficiency in heterogeneous FL settings.

## Method Summary
FedRE operates by having each client compute representations from their local data, map them to a consistent dimensionality via average pooling, then aggregate all category-specific prototypes into a single entangled representation using normalized random weights that are re-sampled each round. Each client uploads only this single entangled representation plus an entangled-label encoding (weighted sum of one-hot labels) to the server. The server then trains a global classifier using cross-entropy loss on these aggregated inputs, and broadcasts the updated classifier back to clients. This framework specifically addresses model heterogeneity by avoiding parameter aggregation and instead sharing knowledge through representations.

## Key Results
- FedRE outperforms state-of-the-art methods like LG-FedAvg, FedGH, and FedKD by up to 6.26% accuracy on TinyImageNet
- Achieves effective trade-off between performance, privacy, and communication overhead by uploading only one representation per client
- Reduces privacy leakage from representation inversion attacks, with entangled representations yielding lower PSNR (9.66) and higher MSE (7781.87) compared to prototypes (PSNR: 10.25, MSE: 6992.04)
- Demonstrates consistent convergence across CIFAR-10, CIFAR-100, and TinyImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-category entangled representations provide smoother decision boundaries than category-specific prototypes.
- Mechanism: Each client aggregates ALL local representations into a SINGLE entangled representation using normalized random weights, blending information across categories. The entangled-label encoding (weighted sum of one-hot labels) provides soft, multi-category supervision signals during global classifier training.
- Core assumption: Cross-category blending helps the global classifier avoid overconfidence in any single category and promotes generalization.
- Evidence anchors:
  - [abstract] "each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries"
  - [section 3.1, Figure 2] Toy experiment shows FedRE (62.00%) achieves competitive accuracy with smoother decision boundaries compared to FedGH (60.50%) which produces sharper boundaries
  - [corpus] FedDis paper (arXiv:2601.22578) addresses federated prediction with data heterogeneity but uses causal disentanglement—opposite approach to entanglement; limited direct corroboration
- Break condition: If categories have no shared semantic structure, cross-category supervision may introduce noise rather than useful signal.

### Mechanism 2
- Claim: Per-round random weight resampling introduces beneficial diversity that improves convergence.
- Mechanism: Weights are sampled fresh each communication round (not fixed), creating different entangled representations across rounds. This prevents the global classifier from overfitting to any single weighting scheme.
- Core assumption: Diversity in supervision signals across rounds acts as implicit regularization.
- Evidence anchors:
  - [section 4.3, Q9, Table 8] Re-sampling (RS) achieves 62.00% vs. fixed-sampling (FS) at 41.50% on synthetic dataset; on CIFAR-100, RS achieves 46.36% vs. FS at 45.84%
  - [section 3.1] "random weights are re-sampled each round to introduce diversity, mitigating the global classifier's overconfidence"
  - [corpus] No direct corpus evidence on weight resampling in FL; mechanism is paper-specific
- Break condition: If resampling introduces too much variance, convergence may be unstable (though paper shows consistent convergence in Figure 3).

### Mechanism 3
- Claim: Entangled representations reduce privacy leakage from representation inversion attacks.
- Mechanism: Blending representations across categories obscures individual sample information. Reconstructing original images from entangled representations yields lower PSNR (9.66) and higher MSE (7781.87) compared to prototypes (PSNR: 10.25, MSE: 6992.04) or raw representations (PSNR: 12.89, MSE: 4514.87).
- Core assumption: Attackers cannot easily decompose entangled representations back to individual samples.
- Evidence anchors:
  - [section 4.2, Q2, Figure 4, Figure 5] Visual reconstruction results show entangled representations reveal no identifiable information
  - [section 4.2] "Most image contours are reconstructed from the representations... The reconstructed images from entangled representations reveal no identifiable information"
  - [corpus] Related paper FedRE: Robust and Effective FL with Privacy Preference (arXiv:2505.04889) addresses privacy in FL but focuses on differential privacy, not entanglement
- Break condition: If an attacker has auxiliary information about the random weights or client data distribution, decomposition may become tractable (not tested in paper).

## Foundational Learning

- Concept: **Federated Learning (FL)**
  - Why needed here: FedRE is built on FL's client-server paradigm where clients train locally and share knowledge (not raw data) with a server.
  - Quick check question: Can you explain why FedAvg cannot directly aggregate parameters when clients have different model architectures?

- Concept: **Representation Learning**
  - Why needed here: FedRE operates on representations (penultimate layer outputs), not model parameters. Understanding what representations encode is essential.
  - Quick check question: What is the difference between a representation extractor's output and a classifier's output?

- Concept: **Model Heterogeneity in FL**
  - Why needed here: The core problem is clients with different architectures cannot aggregate parameters directly. FedRE solves this by sharing representations instead.
  - Quick check question: Why do prototypes (FedGH) and entangled representations (FedRE) work when parameter aggregation fails?

## Architecture Onboarding

- Component map:
  - **Client side**: Local model (representation extractor g_k + classifier f_k) → Representation Mapping (AP/MP/FC) → Representation Entanglement (RAP mechanism) → Upload (e_rk, e_yk)
  - **Server side**: Receive entangled representations from all clients → Train global classifier f(ω;·) via cross-entropy → Broadcast updated classifier weights
  - **Key data flow**: Raw samples → Local representations → Mapped representations → Entangled representation + Entangled-label encoding → Global classifier training

- Critical path:
  1. Ensure all clients' representations are mapped to consistent dimensionality (512 in paper) via Average Pooling
  2. RAP mechanism: Compute per-category prototypes → Aggregate with normalized random weights → Single entangled representation per client
  3. Server trains global classifier on (entangled_rep, entangled_label) pairs

- Design tradeoffs:
  - **RM operation choice**: AP (default) vs. MP vs. FC—AP is simplest with best empirical results (Table 5)
  - **RE mechanism choice**: RAP (default) outperforms VAR, RAR, VAP, RSP, RSR (Table 6)
  - **Weight distribution**: Uniform U(0,1) slightly outperforms Gaussian/Laplace (Table 7)
  - **Privacy vs. utility tradeoff**: Stronger entanglement may reduce privacy leakage but could lose discriminative information

- Failure signatures:
  - **Accuracy plateaus below baseline**: Check representation mapping dimensionality consistency across clients
  - **No privacy improvement**: Verify weights are truly random and normalized, not fixed across rounds
  - **High variance across runs**: Ensure random seed handling is consistent for reproducibility; check client participation rates
  - **Communication still high**: Verify only ONE entangled representation is uploaded per client, not all representations

- First 3 experiments:
  1. **Sanity check**: Run FedRE on CIFAR-10 with 10 homogeneous clients (all same CNN) to verify it works in model-homogeneous setting; compare to FedAvg baseline
  2. **Ablation on RE mechanism**: Compare RAP vs. VAR vs. RAR on CIFAR-100 to validate design choice; expect RAP to outperform others by 2-4%
  3. **Privacy evaluation**: Apply representation inversion attack (e.g., Deep Image Prior) to reconstructions from: (a) raw representations, (b) prototypes, (c) entangled representations; measure PSNR/MSE to verify entangled representations have lowest PSNR and highest MSE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can entangled representations be effectively applied to non-image machine learning tasks, such as Natural Language Processing (NLP) or time-series analysis?
- Basis in paper: [explicit] The conclusion states: "A promising direction for future work is to explore the applicability of entangled representations to a broader range of machine learning tasks."
- Why unresolved: The current study evaluates FedRE exclusively on image datasets (CIFAR-10, CIFAR-100, TinyImageNet). The mechanism relies on Representation Mapping (e.g., Average Pooling) to align spatial features, an approach that may not translate directly to sequential or graph-structured data.
- What evidence would resolve it: Empirical results showing the performance and convergence of FedRE when applied to text classification or sensor data tasks compared to current heterogeneous FL baselines.

### Open Question 2
- Question: Is it possible to adapt FedRE for fully heterogeneous scenarios where clients do not share a homogeneous classifier architecture?
- Basis in paper: [inferred] The Introduction defines the problem setting such that "the classifiers share a homogeneous architecture," and the methodology relies on broadcasting this shared global classifier.
- Why unresolved: The framework requires a shared classifier head to process the uploaded entangled representations. This assumption limits applicability in environments where clients have vastly different output requirements or model capacities, preventing the use of a single global classifier.
- What evidence would resolve it: A modification of the FedRE framework that utilizes knowledge distillation or mutual learning to update heterogeneous local classifiers without relying on a single shared global classifier structure.

### Open Question 3
- Question: How robust are entangled representations and their corresponding label encodings against Membership Inference Attacks (MIA)?
- Basis in paper: [inferred] The paper explicitly evaluates "representation inversion attacks" (Section 4.2), claiming privacy is preserved by obscuring individual samples. However, it does not analyze whether the uploaded entangled-label encoding (a weighted aggregate of one-hot vectors) leaks information about the specific distribution or presence of data classes on a client.
- Why unresolved: While visual reconstruction is prevented, the explicit transmission of label encoding structures might allow an attacker to infer if a specific data distribution was used in training (membership), compromising privacy in a different way than inversion.
- What evidence would resolve it: A quantitative security analysis measuring the success rate of MIAs on FedRE transmissions compared to transmitting standard prototypes or model updates.

## Limitations

- The paper demonstrates effectiveness on image classification benchmarks but leaves questions about applicability to non-image domains like NLP or time-series data
- Privacy evaluation uses only one representation inversion attack method (Deep Image Prior) without exploring stronger adversarial attacks
- The mechanism by which cross-category entanglement promotes generalization remains largely empirical rather than theoretically grounded

## Confidence

- **High Confidence**: Claims about FedRE outperforming FedGH and FedKD in model-heterogeneous settings (supported by extensive experiments across three datasets with statistical significance)
- **Medium Confidence**: Claims about privacy improvement through entanglement (supported by PSNR/MSE metrics but limited to one attack method)
- **Medium Confidence**: Claims about cross-category supervision improving generalization (supported by toy experiments but mechanism remains empirical)

## Next Checks

1. **Ablation Study**: Compare FedRE against a variant that uses category-specific prototypes (FedGH-style) but with the same communication efficiency to isolate the benefit of cross-category supervision versus communication reduction
2. **Transfer Learning**: Evaluate FedRE representations on downstream tasks to assess whether entangled representations capture transferable knowledge beyond the training classification task
3. **Adversarial Privacy Analysis**: Apply stronger representation inversion attacks (e.g., optimization-based inversion with auxiliary networks) to verify that entangled representations maintain privacy advantages against sophisticated attackers