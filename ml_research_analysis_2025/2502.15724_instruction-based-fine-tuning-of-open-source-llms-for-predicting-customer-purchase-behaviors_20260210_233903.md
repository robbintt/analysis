---
ver: rpa2
title: Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase
  Behaviors
arxiv_id: '2502.15724'
source_url: https://arxiv.org/abs/2502.15724
tags:
- data
- categories
- bank
- transaction
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates predictive models for forecasting merchant
  categories from financial transaction data, focusing on the fine-tuned Mistral Instruct
  LLM. The methodology involves instruction fine-tuning Mistral via LoRA using customer
  data converted into natural language format.
---

# Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors

## Quick Facts
- arXiv ID: 2502.15724
- Source URL: https://arxiv.org/abs/2502.15724
- Reference count: 34
- Key outcome: Fine-tuned Mistral Instruct LLM significantly outperforms traditional sequential models (CNN, LSTM) and baseline approaches, achieving higher F1 scores across key merchant categories (grocery, clothing, gas stations) in a multiclass classification framework

## Executive Summary
This study explores the use of instruction-fine-tuned open-source LLMs for predicting customer purchase behaviors from financial transaction data. The authors propose a novel approach that converts customer transaction data into natural language format and fine-tunes the Mistral Instruct model using LoRA. The methodology demonstrates that the fine-tuned Mistral model significantly outperforms traditional sequential models and baseline approaches in multiclass classification of merchant categories. The results show particular strength in handling minority classes and maintaining consistent performance across varying sequence lengths, with weighted F1 scores reaching 0.66 for key merchant categories.

## Method Summary
The authors employ a two-step approach to predict merchant categories from financial transactions. First, they convert structured customer transaction data (amounts, dates, merchant information) and demographic features into natural language prompts following a specific template. Second, they fine-tune the Mistral Instruct LLM using LoRA on this converted dataset. The fine-tuning is performed with 8-bit precision using the QLoRA framework on A100 GPUs, utilizing a LoRA rank of 16 and 5% of training data for validation. The model is trained for 2 epochs with a batch size of 32 and learning rate of 2e-5. The classification task focuses on three most frequent merchant categories (grocery, clothing, gas stations) plus an "Other" category to manage class imbalance.

## Key Results
- Fine-tuned Mistral achieves weighted F1 scores of 0.66, significantly outperforming CNN (0.58) and LSTM (0.56) models
- Superior performance on minority classes compared to traditional sequential models
- Consistent performance across varying sequence lengths (2-8 transactions), with F1 scores ranging from 0.63 to 0.66
- CNN baseline achieves highest macro F1 score (0.35) compared to Mistral (0.33) and LSTM (0.30), indicating better class-wise balance

## Why This Works (Mechanism)
The success of this approach likely stems from the LLM's ability to capture semantic relationships and contextual patterns in the natural language representation of transaction data. The instruction-fine-tuning process enables the model to understand the task-specific patterns in customer spending behavior, while the LoRA adaptation efficiently modifies the model's parameters for this specialized classification task. The natural language format may allow the model to leverage pre-existing knowledge about merchant categories and spending patterns encoded during pre-training.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Why needed: Enables efficient fine-tuning of large LLMs by updating only a small subset of parameters rather than full model weights; Quick check: Verify that LoRA parameters (rank=16) are appropriately sized for the task complexity
- **Instruction Fine-tuning**: Why needed: Adapts pre-trained LLMs to follow specific task instructions rather than just continuing text; Quick check: Confirm that the natural language template effectively captures all relevant transaction features
- **Multi-class Classification with Class Imbalance**: Why needed: Financial transaction data typically contains skewed class distributions with few minority categories; Quick check: Validate that weighted F1 scores appropriately balance precision and recall across all classes
- **Natural Language Processing of Structured Data**: Why needed: Converts tabular financial data into a format LLMs can process using their semantic understanding; Quick check: Ensure the conversion preserves all critical numerical relationships and temporal patterns
- **QLoRA for Efficient Fine-tuning**: Why needed: Reduces memory requirements for fine-tuning large models through quantization; Quick check: Verify that 8-bit precision maintains sufficient numerical accuracy for financial predictions
- **Sequence Length Variation**: Why needed: Tests model robustness across different historical transaction windows; Quick check: Confirm that performance consistency holds across the tested range (2-8 transactions)

## Architecture Onboarding
- **Component Map**: Customer Transaction Data -> Natural Language Conversion -> Mistral Instruct LLM -> LoRA Fine-tuning -> Classification Output
- **Critical Path**: The most critical component is the natural language conversion process, as it determines how well the structured financial data can be interpreted by the LLM's semantic understanding capabilities
- **Design Tradeoffs**: The authors tradeoff numerical precision for semantic understanding by converting transaction amounts and dates to text, which may lose mathematical relationships but gains from the LLM's pre-trained knowledge
- **Failure Signatures**: Potential failures include loss of numerical pattern recognition due to text conversion, overfitting to specific merchant naming conventions, and inability to scale to full MCC code granularity
- **First Experiments**: 1) Test model performance on temporal holdout sets to assess predictive capability on future transactions; 2) Conduct ablation studies removing demographic features to isolate their contribution; 3) Evaluate performance on full MCC code set without aggregation to test scalability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the instruction-based fine-tuning approach maintain high performance when scaled to the full granularity of hundreds of merchant category codes (MCCs), rather than collapsing them into three specific classes and an "Other" bucket?
- Basis in paper: [inferred] The authors explicitly limit the classification scope to the "three most frequently purchased categories" and group the remaining 295 categories into a single "Other" label to manage class imbalance (Page 9), leaving the model's efficacy on a full taxonomy untested.
- Why unresolved: It is unclear if the LLM's semantic advantage persists when the output space is significantly larger and the "minority" classes are sparse and diverse rather than aggregated into a single catch-all class.
- What evidence would resolve it: A follow-up study fine-tuning the model on the full set of available MCC codes without grouping, and reporting class-wise F1 scores for the previously excluded categories.

### Open Question 2
- Question: To what extent does the inclusion of demographic features (e.g., age, marital status) in the natural language prompt drive the prediction accuracy compared to the sequential transaction history alone?
- Basis in paper: [inferred] The methodology involves converting customer demographics into natural language instructions (Table 3), but the paper does not perform an ablation study to isolate the marginal contribution of these static features versus the dynamic transaction data.
- Why unresolved: Without isolating the variables, it remains unknown if the model is learning complex temporal patterns or simply leveraging correlations between the explicitly stated demographic profile (e.g., "48 years old, married male") and spending habits.
- What evidence would resolve it: A comparative experiment where separate models are fine-tuned using only transaction history, only demographics, and the combined dataset to quantify the predictive lift provided by the demographic text.

### Open Question 3
- Question: Is the natural language formatting of tabular financial data truly more efficient or effective than specialized numerical embeddings for capturing transaction amounts and dates?
- Basis in paper: [inferred] The authors convert structured data (dates, amounts) into text strings (Page 10), claiming this leverages the LLM's pre-trained knowledge, but they do not compare this against a hybrid architecture that might process numbers natively.
- Why unresolved: Converting precise numbers (e.g., "$124.97") into tokens may dilute the mathematical relationships (magnitude/frequency) that sequential models like LSTM capture natively through numerical embeddings.
- What evidence would resolve it: A comparison between the text-based LLM and a version utilizing numerical encoders for the amount/date fields (or a standard tabular transformer) to evaluate if the "semantic understanding" outweighs the loss of numerical precision.

## Limitations
- The study was conducted on a single proprietary dataset from an unspecified financial institution, limiting generalizability to other customer demographics and spending patterns
- The methodology for converting structured transaction data to natural language format lacks detailed specification, making replication challenging
- While the model shows superior performance on minority classes, the study does not explore potential biases in how these classes are represented in the natural language format

## Confidence
- **High confidence**: Superior performance metrics (F1 scores up to 0.66) of fine-tuned Mistral compared to traditional sequential models (CNN, LSTM)
- **Medium confidence**: Claims about Mistral's ability to handle varying sequence lengths consistently, as this was tested within a limited range
- **Low confidence**: Claims about revolutionizing financial decision-making processes, as this extends beyond the empirical findings into speculative territory

## Next Checks
1. Conduct cross-validation using multiple financial institutions' datasets to assess model robustness and generalizability across different customer demographics and spending patterns
2. Perform ablation studies to isolate the impact of the natural language conversion process versus the fine-tuning approach itself
3. Test the model's performance on temporal holdout sets to evaluate its predictive capability on future, unseen transaction data and assess potential data leakage issues