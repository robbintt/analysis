---
ver: rpa2
title: 'CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via
  Algorithm-System Co-Design'
arxiv_id: '2511.14510'
source_url: https://arxiv.org/abs/2511.14510
tags:
- cache
- data
- overhead
- transfer
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient large language model
  (LLM) inference with million-token context lengths, where key-value cache (KVCache)
  memory consumption and data transfer overhead become critical bottlenecks. Existing
  offloading systems suffer from CPU bottlenecks in cache management, PCIe bandwidth
  underutilization, and synchronization overhead.
---

# CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design

## Quick Facts
- **arXiv ID**: 2511.14510
- **Source URL**: https://arxiv.org/abs/2511.14510
- **Reference count**: 40
- **Key outcome**: CLO achieves 9.3%-66.6% throughput improvement over state-of-the-art systems while using only 47.6% of GPU cache memory, maintaining 79.22% cache hit ratio on 1M context length inference.

## Executive Summary
CLO addresses the critical bottleneck of KVCache memory consumption and data transfer overhead in long-context LLM inference by co-designing algorithms and systems. The system eliminates CPU-side cache management overhead through head-wise approximate caching based on query similarity, while selectively partitioning KV heads between persistent GPU caching and CPU offloading with prefetching. A zero-copy transfer engine and GPU-centric synchronization maximize PCIe bandwidth utilization, achieving near-peak transfer rates while completely eliminating kernel launch overhead. Evaluated on Llama3-8B and Qwen2.5-14B with 1M context length, CLO delivers comparable accuracy to full attention while significantly improving decoding throughput.

## Method Summary
CLO implements a three-part solution: (1) head-wise approximate caching using query similarity thresholds to replace CPU-managed block-level caching, (2) selective residency partitioning based on reuse difficulty metrics to balance persistent caching against prefetching, and (3) zero-copy transfer via GDRCopy with GPU-initiated synchronization to eliminate PCIe bottlenecks. The system profiles head importance and average query similarity offline, then partitions heads into persistent-cached versus offloaded categories during inference. Cache lookup becomes a GPU-accelerated cosine similarity comparison, while prefetching overlaps with computation. The approach is built on FlashInfer + Transformers with HATA top-k algorithm, using hyperparameters η=0.8, p=3, ε=0.1/0.05, top-k=10%, sink tokens=4, recent tokens=64.

## Key Results
- **Throughput improvement**: 9.3%-66.6% faster decoding compared to leading baselines
- **Memory efficiency**: Uses only 47.6% of GPU cache memory while maintaining 79.22% cache hit ratio
- **PCIe bandwidth**: Achieves near-peak 21 GB/s utilization with zero kernel launch overhead
- **Accuracy preservation**: Comparable inference accuracy to full attention with 0.1-0.4 point degradation

## Why This Works (Mechanism)

### Mechanism 1: Head-Wise Approximate Caching via Query Similarity
Replaces fine-grained block-level cache management with head-granularity approximate caching using query vector similarity. Each head stores a single query label and top-k KV data, eliminating CPU metadata overhead. Cache lookup becomes GPU-accelerated cosine similarity comparison against threshold τ_i. When similarity falls below threshold, entire head cache is replaced without linked-list traversals.

### Mechanism 2: Selective Residency with Reuse Difficulty Quantification
Partitions KV heads into persistent-cached (always on GPU) and offloaded with prefetching categories based on reuse difficulty metric D_i = τ_i - (ŝ_i - ε). Heads with D_i > 0 are persistent candidates, with number of prefetchable heads N_p computed from per-layer compute time and PCIe bandwidth. Sink and recent tokens always persistent for accuracy.

### Mechanism 3: Zero-Copy Transfer with GPU-Centric Synchronization
Uses GDRCopy to map GPU memory into CPU address space via PCIe BAR, enabling direct CPU loads/stores without intermediate buffers. Transfer threads use AVX SIMD for vectorized copying and CPU cache prefetching. Synchronization uses shared-memory identifiers: GPU writes indices to trigger transfer, CPU updates on completion, GPU polls without blocking kernel launch thread.

## Foundational Learning

- **Concept: KVCache in Autoregressive LLMs**
  - Why needed: CLO's design centers on offloading and caching KV vectors that store historical token representations
  - Quick check: During decoding, why does generating token t+1 require accessing K_0...K_t rather than just K_t?

- **Concept: Top-k Sparse Attention**
  - Why needed: CLO exploits that only ~10% of KV pairs contribute significantly to attention output
  - Quick check: If sparsity is 10% and sequence length is 1M tokens, how many KV pairs are actually loaded per attention head per layer?

- **Concept: PCIe Bandwidth and Latency Fundamentals**
  - Why needed: Zero-copy transfer and prefetching overlap designed around PCIe characteristics
  - Quick check: Why does transferring 10MB via 100 sequential 100KB operations achieve lower bandwidth than one 10MB transfer?

- **Concept: GPU-CPU Synchronization Models**
  - Why needed: CLO redesigns synchronization from CPU-blocking to GPU-polling
  - Quick check: If CPU launches 100 kernels per inference step and each cudaStreamSynchronize takes 50μs, what's the minimum synchronization overhead per token?

## Architecture Onboarding

- **Component map**: Cache Manager (Python/C++) -> Top-k Retriever (CUDA) -> Transfer Engine (C++/CUDA) -> Data Sync Controller (CUDA) -> Persistent Cache (GPU HBM) -> Cache Buffer (GPU HBM) -> Launch Thread (CPU)

- **Critical path**: Decoding step begins → approximate query computed → cache lookup: cosine similarity check → for misses: Top-k retrieval launches, prefetch queued → layer l-1 compute proceeds while layer l's KV transfers execute → QKV projection produces actual query → poll completion → attention kernel fuses data from Cache Buffer + Persistent Cache → query labels updated for missed heads

- **Design tradeoffs**: Cache hit ratio vs. management overhead (79% vs. higher baselines), memory vs. transfer (persistent caching consumes more HBM), approximation accuracy (τ=0.8 trades 0.1-0.4 accuracy points), implementation complexity (GDRCopy dependency adds deployment friction)

- **Failure signatures**: OOM during initialization (HBM budget insufficient), accuracy degradation >1% on RULER (τ thresholds too low), PCIe bandwidth <10 GB/s (GDRCopy not properly installed), kernel launch bubbles reappear (Launch thread blocked), cache hit ratio drops below 60% (query similarity assumption violated)

- **First 3 experiments**: 
  1. Bandwidth microbenchmark: Transfer 1-100MB KV data using Transfer Engine with 1, 2, 4, 8 threads. Verify ≥18 GB/s at 4 threads.
  2. Cache policy ablation: Run RULER-128K with fixed threshold τ=0.8, adaptive thresholds, and full setup with persistent caching. Measure hit ratio, latency, and accuracy.
  3. End-to-end latency breakdown: Profile single decoding step with Nsight Systems. Verify kernel launch overhead <10μs, transfer fully overlapped, cache lookup kernel <50μs.

## Open Questions the Paper Calls Out

### Open Question 1
Can the CLO architecture be effectively extended to distributed inference settings (e.g., Tensor Parallelism) without re-introducing significant synchronization overhead? The evaluation is restricted to single GPU setup while distributed approaches are described as merely "orthogonal" without exploring integration complexities.

### Open Question 2
How does the system perform when the core assumption of high query cosine similarity between adjacent steps is violated? The cache design relies heavily on this observation, but specific workloads causing low query similarity could cause cache hit ratio to drop, failing to hide PCIe transfer overhead.

### Open Question 3
Is it possible to replace the offline, manually tuned threshold configuration with an online adaptive mechanism? The paper notes thresholds and importance scores are determined by "offline profiled" data and "manually tuned" hyperparameters, which may fail to capture dynamics of new workloads or distribution shifts.

## Limitations
- **Hardware specificity**: Zero-copy transfer optimization relies on GDRCopy and PCIe BAR mapping, limiting deployment across all GPU platforms
- **Model dependency**: Effectiveness depends on stability of query similarity distributions across different workloads, validated only on synthetic calibration data
- **Approximation quality**: 0.1-0.4 accuracy degradation may vary significantly with different model architectures or task domains not represented in evaluation

## Confidence
- **Algorithmic Innovation Claims**: High confidence - well-documented with clear mathematical formulation and ablation studies
- **Performance Claims**: Medium confidence - demonstrated on specific models but scaling behavior to larger models uncharacterized
- **PCIe Bandwidth Utilization Claims**: High confidence - concretely specified with measurable targets and observable elimination of kernel launch bubbles

## Next Checks
1. **Cross-workload generalization**: Run CLO on diverse inference workloads (code generation, mathematical reasoning, multilingual tasks) not represented in RULER/LongBench to validate stability assumption across different domains
2. **Hardware platform robustness**: Deploy CLO on alternative GPU architectures where GDRCopy support may be limited to assess hardware dependency and compare achieved bandwidth utilization
3. **Parameter sensitivity analysis**: Systematically vary cosine similarity threshold τ from 0.6 to 0.95 to identify Pareto frontier between cache hit ratio, accuracy degradation, and throughput improvement