---
ver: rpa2
title: 'ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping'
arxiv_id: '2504.10857'
source_url: https://arxiv.org/abs/2504.10857
tags:
- grasp
- dataset
- reconstruction
- pose
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ZeroGrasp introduces a near real-time framework for simultaneous
  3D reconstruction and 6D grasp pose prediction from a single RGB-D image. The method
  couples an octree-based conditional variational autoencoder with novel components:
  a multi-object encoder and 3D occlusion fields to model inter-object relationships
  and occlusions, and a simple refinement algorithm that uses predicted reconstructions
  to improve grasp poses via contact constraints and collision detection.'
---

# ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping

## Quick Facts
- arXiv ID: 2504.10857
- Source URL: https://arxiv.org/abs/2504.10857
- Reference count: 40
- Key outcome: ZeroGrasp achieves state-of-the-art performance on GraspNet-1B benchmark for simultaneous 3D reconstruction and 6D grasp pose prediction from a single RGB-D image

## Executive Summary
ZeroGrasp introduces a near real-time framework for simultaneous 3D reconstruction and 6D grasp pose prediction from a single RGB-D image. The method couples an octree-based conditional variational autoencoder with novel components: a multi-object encoder and 3D occlusion fields to model inter-object relationships and occlusions, and a simple refinement algorithm that uses predicted reconstructions to improve grasp poses via contact constraints and collision detection. Trained on a new large-scale synthetic dataset (ZeroGrasp-11B), ZeroGrasp achieves state-of-the-art performance on the GraspNet-1B benchmark, outperforming previous methods in both reconstruction quality and grasp pose prediction accuracy. The approach also generalizes well to real-world objects, demonstrated through real-robot evaluations.

## Method Summary
ZeroGrasp is a simultaneous 3D reconstruction and 6D grasp pose prediction framework that uses an octree-based conditional variational autoencoder (CVAE). The architecture processes single RGB-D images with instance masks (fine-tuned SAM 2), encoding them through a ResNeXt image encoder and unprojecting to octree format. A multi-object encoder (transformer) and 3D occlusion fields handle inter-object relationships and visibility reasoning. The decoder predicts occupancy, SDF, normals, and grasp parameters. A refinement algorithm improves grasp poses using predicted reconstructions for contact constraints and collision detection. The model is trained on ZeroGrasp-11B (11.3B grasp annotations) and evaluated on GraspNet-1B, achieving state-of-the-art performance.

## Key Results
- State-of-the-art performance on GraspNet-1B benchmark for both reconstruction and grasping
- Near real-time inference at 5 FPS
- 75% success rate on real-robot evaluations
- Superior performance in cluttered scenes compared to transformer-based alternatives

## Why This Works (Mechanism)

### Mechanism 1: Joint Probabilistic Shape-Grasp Modeling
The octree-CVAE architecture simultaneously reconstructs 3D geometry and predicts grasp poses within a shared latent space, allowing geometric reasoning to inform grasp stability. This shared representation improves grasp accuracy compared to isolated pipelines.

### Mechanism 2: Explicit 3D Occlusion Fields
The model creates binary occlusion flags (self vs. inter-object) by projecting voxel blocks onto 2D instance masks, encoding them via 3D CNNs and fusing with latent features. This explicit geometric ray-casting approach outperforms transformers at learning long-range visibility dependencies.

### Mechanism 3: Reconstruction-Based Contact Refinement
The refinement algorithm adjusts predicted gripper width and depth by finding nearest contact points on the reconstructed mesh, using the *predicted* complete mesh for collision detection rather than the observed partial point cloud. This approach improves success rates by detecting collisions in unobserved regions.

## Foundational Learning

- **Concept: Octrees & Sparse Convolutions**
  - Why needed: Standard voxels scale cubically ($O(N^3)$) with resolution, making high-fidelity reconstruction memory-prohibitive. Octrees store data only on the surface ($O(N^2)$), enabling high resolution required for precise contact points.
  - Quick check: How does the memory footprint of an octree scale compared to a dense voxel grid at the same effective resolution?

- **Concept: Conditional Variational Autoencoders (CVAEs)**
  - Why needed: Single-view reconstruction is inherently ambiguous. A CVAE learns a distribution over possible shapes, allowing the model to sample plausible completions rather than averaging them out (which results in blurry blobs).
  - Quick check: What role does the KL divergence loss play in aligning the prior (inference time) and posterior (training time)?

- **Concept: 6-DoF Grasp Representation**
  - Why needed: Planar grasps fail in cluttered shelves or bins. This paper regresses view direction, angle, width, and depth to allow approach vectors from the side.
  - Quick check: What specific parameters constitute the grasp vector $g$ in Eq. (2), and why is "graspness" $s$ treated differently from "quality" $q$?

## Architecture Onboarding

- **Component map:** RGB-D Image + Camera Intrinsics -> SAM 2 (Masks) + ResNeXt (Features) -> Unprojection -> Input Octree -> Encoder (Octree CNN) -> Multi-Object Encoder (Transformer) + 3D Occlusion Fields (3D CNN) -> Decoder (Sparse U-Net) -> Grasp Refinement

- **Critical path:** Depth quality affects initial octree structure; mask quality impacts multi-object encoder and occlusion fields; refinement logic determines final success rate.

- **Design tradeoffs:** Synthetic vs. real data gap addressed by fine-tuning; instance vs. scene processing requires robust segmentation frontend.

- **Failure signatures:** Floating artifacts from weak CVAE prior or noisy depth; mask bleeding causing merged collision logic; inference speed bottleneck from pipeline accumulation.

- **First 3 experiments:**
  1. Occlusion Field Ablation: Run on ReOcS "Hard" split with/without Occlusion Field branch, measuring Chamfer Distance of occluded regions.
  2. Refinement Threshold Sensitivity: Vary contact distance parameters ($\gamma_{min}, \gamma_{max}$) to test if tighter constraints improve success or cause gripper slippage.
  3. Mask Robustness: Dilate/erode input instance masks by a few pixels and measure drop in grasp AP.

## Open Questions the Paper Calls Out

- How can ZeroGrasp be extended to support incremental or multi-view 3D reconstruction for wrist-mounted camera applications?
- Can predicted 3D reconstructions be leveraged to predict placement poses or subsequent manipulation actions?
- How does performance degrade on articulated or deformable objects where geometry differs from canonical training shapes?

## Limitations

- Domain generalization gap due to reliance on high-quality synthetic data and instance masks
- Occlusion field dependency on accurate segmentation masks
- Memory-accuracy trade-off limiting batch sizes and training diversity

## Confidence

**High Confidence:**
- Octree-CVAE architecture effectively learns probabilistic shape distributions
- Reconstruction-based refinement improves grasp success rates
- Multi-object encoder improves grasp performance in cluttered scenes

**Medium Confidence:**
- Occlusion fields significantly improve reconstruction quality
- Method generalizes to real-world objects

**Low Confidence:**
- "Near real-time" performance claim
- Handles "novel objects" claim

## Next Checks

1. Occlusion Field Ablation with Targeted Metrics: Run on ReOcS "Hard" split with/without Occlusion Field branch, specifically measuring Chamfer Distance of occluded regions.

2. Real-Robot Robustness Test: Evaluate on 50+ real-world novel objects not in GraspNet-1B, with systematic variation in object material, size, and clutter level.

3. Mask Quality Sensitivity Analysis: Intentionally degrade SAM 2 masks (dilation/erosion by 2-5 pixels) and measure drop in grasp AP.