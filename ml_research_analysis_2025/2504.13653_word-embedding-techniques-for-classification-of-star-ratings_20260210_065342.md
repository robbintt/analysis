---
ver: rpa2
title: Word Embedding Techniques for Classification of Star Ratings
arxiv_id: '2504.13653'
source_url: https://arxiv.org/abs/2504.13653
tags:
- word
- classifiers
- classification
- embedding
- tf-idf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates word embedding techniques for classifying
  customer star ratings in telecom reviews. Multiple embedding methods (Word2Vec,
  FastText, BERT, Doc2Vec) were compared with feature extraction approaches (average,
  PCA) across binary and multiclass classification tasks using seven classifiers.
---

# Word Embedding Techniques for Classification of Star Ratings

## Quick Facts
- **arXiv ID**: 2504.13653
- **Source URL**: https://arxiv.org/abs/2504.13653
- **Reference count**: 16
- **Primary result**: BERT-PCA achieves highest F1-Scores (up to 0.9) for multiclass classification, while PCA-based aggregation outperforms averaging for Word2Vec and FastText.

## Executive Summary
This study investigates word embedding techniques for classifying customer star ratings in telecom reviews, comparing multiple methods (Word2Vec, FastText, BERT, Doc2Vec) with feature extraction approaches (average, PCA) across binary and multiclass classification tasks. BERT-PCA achieved the highest F1-Scores (up to 0.9) for challenging multiclass classification, while Word2Vec-PCA and FastText-PCA performed consistently well with Logistic Regression and SGD. TF-IDF excelled only with Support Vector Classifiers. Energy consumption analysis revealed that TF-IDF and Doc2Vec were the most efficient embeddings, while FastText and BERT required significantly more resources. The study demonstrates that optimal embedding selection depends on classification complexity, model compatibility, and computational constraints.

## Method Summary
The study used a novel dataset of 367,000 telecom customer reviews scraped from Trustpilot, cleaned and processed into three classification scenarios: Radical-Binary (1-star vs 5-star), Mixed-Binary (1-2 stars vs 4-5 stars), and Multi-Class (1-5 stars). Seven classifiers were trained using five embedding methods: Word2Vec (dim=100), FastText (dim=300), BERT (dim=384), Doc2Vec (dim=300), and TF-IDF. Feature engineering included comparing average vectors versus PCA aggregation (taking the first principal component for Word2Vec/FastText per document, global PCA for BERT to 50 dimensions, Doc2Vec reduced to 100 dimensions). Performance was evaluated using 5-fold cross-validation with macro-averaged F1-score.

## Key Results
- BERT-PCA achieved the highest F1-Scores (up to 0.9) for challenging multiclass classification tasks
- PCA-based aggregation outperformed simple averaging for Word2Vec and FastText, with 3.5-12% relative improvement in mean F1-Score
- TF-IDF excelled only with Support Vector Classifiers, performing poorly with most other classifiers
- TF-IDF and Doc2Vec were the most energy-efficient embeddings, while FastText and BERT required significantly more resources

## Why This Works (Mechanism)

### Mechanism 1
PCA-based aggregation of word vectors outperforms simple averaging for Word2Vec and FastText in text classification tasks. Rather than computing x_d = (v_1 + ... + v_nd)/n_d where all terms contribute equally, PCA extracts the first principal component from the matrix A_d = [v_1, ..., v_nd] to produce a weighted combination that maximizes retained variance. This captures more discriminative information per document. Core assumption: The first principal component encodes more task-relevant semantic information than uniform weighting. Evidence: For Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average; the relative improvement in the mean F1-Score while using PCA instead of averages ranged between 3.5% and 12%.

### Mechanism 2
BERT combined with PCA dimensionality reduction achieves superior performance on challenging multiclass classification tasks. BERT produces context-aware 384-dimensional embeddings via bidirectional attention. PCA reduction to 50 dimensions preserves discriminative features while reducing noise and computational overhead. The mean pooling step aggregates token-level BERT outputs before PCA is applied across all documents jointly. Core assumption: The top 50 principal components capture most class-discriminative information. Evidence: BERT-PCA stood out with the highest F1-Scores for RFs equal to 0.9 for all three sample sizes for Multi-Class data; BERT-PCA achieved the highest F1-Scores (up to 0.9) for challenging multiclass classification.

### Mechanism 3
TF-IDF effectiveness is classifier-dependent, excelling with Support Vector Classifiers but failing with most other classifiers. TF-IDF produces sparse high-dimensional representations. SVC with RBF kernel can effectively navigate this sparse feature space by finding separating hyperplanes. Linear classifiers like Logistic Regression struggle with the high dimensionality and sparsity without appropriate regularization. Tree-based ensembles handle TF-IDF reasonably well but achieve better performance with dense embeddings. Core assumption: The sparsity pattern in TF-IDF is informative for SVC specifically due to the kernel's ability to leverage feature co-occurrence. Evidence: For SVC, TF-IDF text representation yielded the highest F1-Scores between 0.92 and 0.95; TF-IDF performed generally very poorly for LR with F1-Scores ranging between 0.35 and 0.4.

## Foundational Learning

- **Word Embeddings as Dense Vector Representations**: Understanding that Word2Vec, FastText, BERT, and Doc2Vec all map discrete tokens to continuous vectors, but differ in what information they capture (context windows, subword morphology, bidirectional attention, document-level semantics). Quick check: Given the sentence "The service was terrible," would Word2Vec and BERT produce the same vector for "terrible"? Why or why not?

- **Document-Level Aggregation Strategies**: Classifiers require fixed-length inputs. The paper compares averaging (all words equal) vs. PCA-weighted aggregation (variance-maximizing weights), with significant performance differences. Quick check: If a document has 50 words with 300-dimensional FastText embeddings each, what is the dimensionality of the aggregated document vector under both averaging and PCA approaches described in the paper?

- **Macro-Averaged F1-Score for Multiclass Evaluation**: The paper uses macro F1 (unweighted mean across classes) to handle the 5-class star rating task, which treats all classes equally regardless of frequency. Quick check: Why would macro F1 be preferred over accuracy for a 5-class star rating dataset where extreme ratings (1-star, 5-star) are more common than middle ratings (3-star)?

## Architecture Onboarding

- **Component map**: Raw Text → Preprocessing → Word Embedding Layer → Aggregation (Average or PCA-per-doc) → Dimensionality Reduction (PCA across documents for BERT/Doc2Vec) → Classifier Training → Evaluation
- **Critical path**: Text preprocessing → Word embedding extraction → Aggregation (for token-level embeddings) → PCA (optional, for dimensionality reduction) → Classifier training → Evaluation. The choice of embedding-aggregation-classifier combination determines both performance and resource consumption.
- **Design tradeoffs**: Accuracy vs. Efficiency (BERT-PCA achieves highest multiclass F1 (0.9) but consumes ~25x more energy than Doc2Vec; TF-IDF is most energy-efficient but only works well with SVC); Complexity vs. Robustness (Gradient Boosting and KNN achieve stable high performance regardless of embedding choice, but LR and SGD are highly sensitive); Dimensionality vs. Information (PCA reduction maintains or improves performance while reducing training time).
- **Failure signatures**: LR with TF-IDF (long training time with poor F1 indicates mismatch between sparse features and linear model); SVC with BERT-PCA (F1 drops to 0.47-0.65 on binary tasks suggests overfitting to reduced dimensions); SGD with multiclass data (F1 ~0.09-0.34 indicates learning rate or regularization needs tuning).
- **First 3 experiments**: 1) Baseline establishment with simple binary task using Radical-Binary dataset, train all 7 classifiers with Word2Vec-Average, record F1, training time, and energy. 2) PCA aggregation validation comparing Word2Vec-Average vs Word2Vec-PCA and FastText-Average vs FastText-PCA on Mixed-Binary dataset with Logistic Regression and SGD. 3) Resource-accuracy frontier mapping on Multi-Class dataset running BERT-PCA, Doc2Vec, and TF-IDF with best-performing classifier for each, measuring F1, energy, and extraction time.

## Open Questions the Paper Calls Out

- **Cross-domain transferability**: How do the relative performances of word embedding techniques transfer to text classification tasks in domains other than telecom customer reviews? The study uses a novel dataset of telecom customers' reviews exclusively, yet claims general applicability of findings without cross-domain validation. All experiments were conducted on a single domain-specific dataset from Trustpilot telecom reviews.

- **Optimal PCA dimensionality**: How does optimal PCA dimensionality for each word embedding method vary with dataset size and classification complexity? The paper applies fixed PCA reductions without systematic tuning, noting that the number of principal components could be tuned via cross-validation. The arbitrary dimension choices were not justified or optimized.

- **Classifier-specific sensitivity patterns**: What explains the classifier-specific sensitivity patterns, where TF-IDF excels with SVC but fails with most other classifiers? The paper states TF-IDF performed the worst for most classifiers, except for SVC, but offers no theoretical explanation. The interaction between sparse TF-IDF representations and SVC's kernel-based decision boundaries versus other classifiers' handling of dense embedding vectors remains unexplained.

## Limitations
- Data access: The 367,000 review dataset is proprietary, making exact reproduction impossible without access to the same corpus.
- Hyperparameter sensitivity: Optimal hyperparameters for each embedding-classifier combination were not explored, potentially leaving performance improvements on the table.
- BERT dimension ambiguity: The paper reports 384-dimensional BERT embeddings, which is unusual for standard BERT variants and suggests a specific model configuration.

## Confidence
- **High confidence**: BERT-PCA superiority on multiclass classification (0.9 F1); TF-IDF effectiveness only with SVC; energy efficiency rankings
- **Medium confidence**: PCA aggregation advantage over averaging for Word2Vec/FastText (3.5-12% improvement); classifier sensitivity to embedding choice
- **Low confidence**: Specific PCA dimension choices; exact energy consumption values; optimal embedding-classifier combinations for production deployment

## Next Checks
1. **Replication with public dataset**: Test the Word2Vec-PCA vs Word2Vec-Average comparison on a publicly available telecom review dataset to verify the 5-10% F1 improvement claims.
2. **Classifier hyperparameter sweep**: For the best-performing embedding-classifier pairs, systematically vary key hyperparameters to determine if the reported performance represents optimal configurations.
3. **Energy vs accuracy Pareto frontier**: On the Multi-Class dataset, measure the complete trade-off curve between F1-score and energy consumption by varying PCA dimensions, identifying the dimension that maximizes F1 per energy unit.