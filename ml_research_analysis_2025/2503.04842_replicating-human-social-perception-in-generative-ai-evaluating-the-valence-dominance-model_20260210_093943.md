---
ver: rpa2
title: 'Replicating Human Social Perception in Generative AI: Evaluating the Valence-Dominance
  Model'
arxiv_id: '2503.04842'
source_url: https://arxiv.org/abs/2503.04842
tags:
- generative
- component
- human
- similar
- equal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether multimodal generative AI models
  can replicate the valence-dominance model of human social perception. Using principal
  component analysis (PCA) on trait ratings from facial images, we found that AI-generated
  dimensions closely mirrored the theoretical valence (trustworthiness) and dominance
  structure observed in humans.
---

# Replicating Human Social Perception in Generative AI: Evaluating the Valence-Dominance Model

## Quick Facts
- arXiv ID: 2503.04842
- Source URL: https://arxiv.org/abs/2503.04842
- Reference count: 5
- AI models replicate human valence-dominance structure in facial perception with high congruence coefficients

## Executive Summary
This study investigates whether multimodal generative AI models can replicate the valence-dominance model of human social perception. Using principal component analysis (PCA) on trait ratings from facial images, we found that AI-generated dimensions closely mirrored the theoretical valence (trustworthiness) and dominance structure observed in humans. Across three state-of-the-art models (Claude 3.5 Sonnet, GPT-4 Turbo, Gemini 1.5 Pro), the first two components replicated human judgments with high congruence coefficients (0.97-0.98 for valence, 0.65-0.79 for dominance). A third component was also identified in most models and world regions, though its interpretation remains unclear. These findings demonstrate that generative AI systems can replicate fundamental aspects of human social perception, with implications for AI-driven decision-making and understanding potential biases in machine-generated representations.

## Method Summary
The study used 120 facial images from the Chicago Face Database and prompted three multimodal AI models (Claude 3.5 Sonnet, GPT-4 Turbo, Gemini 1.5 Pro) to rate each face on 13 trait dimensions using a 7-point scale. PCA with orthogonal components was applied to the model-generated trait ratings, retaining components with eigenvalues greater than 1. The first two components were evaluated against established valence-dominance benchmarks from Oosterhof & Todorov (2008) and Jones et al. (2021), requiring specific loading patterns (>0.7 on primary trait, <0.5 on secondary trait) and congruence coefficients for cross-validation.

## Key Results
- All three models replicated the valence component with congruence coefficients of 0.97-0.98
- Dominance component replication showed lower but still significant congruence (0.65-0.79) across all regions
- A third principal component emerged in most models and world regions, though its interpretation remains unclear
- The valence-dominance structure was consistently observed across different world regions in the validation data

## Why This Works (Mechanism)

### Mechanism 1: Statistical Learning of Social Trait Covariance
Multimodal generative AI models internalize the correlational structure of human trait judgments because their training data contains implicit associations between facial features and trait descriptors. During pretraining on large-scale image-text corpora, models learn which visual features co-occur with which linguistic descriptors, yielding valence and dominance as principal components when prompted to rate faces on traits.

### Mechanism 2: Dimensional Reduction via PCA on Model Outputs
Applying PCA to model-generated trait ratings reveals latent dimensions that align with valence and dominance because the models produce ratings with similar intercorrelation patterns to humans. The 13 trait vocabulary covers the semantic space of social perception sufficiently, allowing PCA to identify orthogonal axes of maximum variance.

### Mechanism 3: Cross-Modal Alignment in Multimodal Architectures
Multimodal models create a shared embedding space where facial features and trait language occupy corresponding regions, enabling systematic face-to-trait inference. Vision encoders map face images to embeddings; language components map trait words to embeddings; alignment training ensures semantically related visual and linguistic features are nearby in the shared space.

## Foundational Learning

- **Principal Component Analysis (PCA)**: Orthogonal dimensionality reduction technique used to extract latent dimensions from correlated trait ratings. Quick check: If trustworthiness loads at 0.87 and dominance loads at 0.22 on Component 1, what does this tell you about how traits cluster?

- **Valence-Dominance Model (Oosterhof & Todorov, 2008)**: Theory that social perception reduces to two primary dimensions (trustworthiness/warmth and dominance/power). Quick check: Why would trustworthiness and dominance be orthogonal rather than correlated?

- **Tucker's Congruence Coefficient**: Metric for comparing component loadings between different datasets or models. Quick check: What does a congruence of 0.98 vs. 0.65 tell you about Component 1 vs. Component 2 alignment?

## Architecture Onboarding

- **Component map**: Chicago Face Database (120 images) → Multimodal model API → 13 trait ratings (7-point scale) → PCA analysis → Component loadings comparison

- **Critical path**: Obtain consistent trait ratings from models → Run PCA with correct parameters → Compare loadings to human benchmarks using congruence coefficients

- **Design tradeoffs**: Orthogonal vs. rotated components (chose orthogonal for comparability); number of components (3 vs. 2, third is unstable); model selection (3 models for generalization but increased costs)

- **Failure signatures**: Dominance component shows lower congruence (0.65–0.79) than valence (0.97–0.98); third component varies across models/regions with no clear interpretation; Africa region and Oosterhof & Todorov found only 2 components

- **First 3 experiments**: 1) Replicate with different face datasets (Face Research Lab London) to test generalization; 2) Test whether third component stabilizes with larger stimulus sets or factor rotation; 3) Compare models with/without face-trait exposure in training data

## Open Questions the Paper Calls Out

### Open Question 1
What is the nature and significance of the third component that emerged in the PCA of generative AI models? The study found a third component in most models and world regions, but its "nature and significance warrant further investigation" as the focus was primarily on valence and dominance.

### Open Question 2
Does the third component reflect culturally specific aspects of facial evaluation? The discussion proposes that future research should explore whether this component reflects "culturally specific aspects of facial evaluation" that vary across different populations.

### Open Question 3
Do AI-generated trait inferences reinforce or amplify biases in downstream decision-making tasks? The authors conclude that replication of these structures raises questions about whether AI systems "may reinforce or even amplify existing biases in social evaluations" in applied contexts.

## Limitations

- The study shows high valence alignment but considerably lower dominance alignment (0.65-0.79) across all tested regions, suggesting dominance perception may be less reliably encoded or more culturally variable
- The third principal component lacks clear theoretical interpretation and shows inconsistent loadings across models and world regions
- The 13-trait vocabulary used for ratings is not specified in detail, creating uncertainty about whether this captures the full semantic space of social perception

## Confidence

- **High Confidence**: Valence replication (congruence 0.97-0.98) across all three models and most world regions
- **Medium Confidence**: Overall framework validity and statistical methodology, though third component interpretation remains uncertain
- **Medium Confidence**: Dominance component replication (0.65-0.79), but lower and more variable than valence across regions

## Next Checks

1. Test whether dominance component alignment improves with different face stimuli or larger sample sizes, particularly examining whether the lower congruence represents model limitation or inherent cultural variability in dominance perception

2. Investigate third component stability through factor rotation (oblimin/varimax) or larger stimulus sets to determine if this represents a genuine perceptual dimension or statistical artifact

3. Compare results across diverse face datasets beyond the Chicago Face Database to assess whether valence-dominance replication generalizes across demographic variations and whether certain populations show systematically different alignment patterns