---
ver: rpa2
title: 'LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source
  Code'
arxiv_id: '2509.17337'
source_url: https://arxiv.org/abs/2509.17337
tags:
- code
- vulnerability
- llms
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaVul, a multimodal large language model
  designed to reason about software vulnerabilities in source code. The core innovation
  is integrating a code encoder with a language model to jointly process source code
  and natural language questions, enabling fine-grained, interpretable vulnerability
  analysis.
---

# LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code

## Quick Facts
- arXiv ID: 2509.17337
- Source URL: https://arxiv.org/abs/2509.17337
- Reference count: 40
- LLaVul outperforms general-purpose and code-specific LLMs on vulnerability QA and classification tasks, achieving state-of-the-art semantic and lexical metrics.

## Executive Summary
This paper introduces LLaVul, a multimodal large language model designed to reason about software vulnerabilities in source code. The core innovation is integrating a code encoder with a language model to jointly process source code and natural language questions, enabling fine-grained, interpretable vulnerability analysis. LLaVul is trained on a newly curated dataset of real-world vulnerable code paired with security-focused question-answer pairs. Evaluation on both a vulnerability QA benchmark and a classification task shows LLaVul outperforming general-purpose and code-specific LLMs, achieving state-of-the-art performance in semantic and lexical metrics. The approach demonstrates that combining code understanding with QA yields more accurate and explainable vulnerability reasoning than classification alone.

## Method Summary
LLaVul uses a two-stage training pipeline with a cross-modal architecture: CodeSage-small (frozen encoder) processes code tokens and projects embeddings via a 2-layer MLP to the LLM space, then Vicuna-7B-v1.5 (with LoRA adapters) jointly processes projected code and natural language queries to generate answers. Stage 1 pretrains the projector on CodeXGLUE summarization data converted to QA format. Stage 2 fine-tunes both the projector and LoRA adapters on LLaVul-QA, a synthetic dataset of ~406K security-focused QA pairs generated from CVEFixes/MSR code using Llama-3.1-8B-Instruct. Training uses bf16 precision, 1000-token code context, and runs on 2-4 A100 GPUs with LoRA rank 64.

## Key Results
- LLaVul achieves state-of-the-art performance on vulnerability QA, outperforming both general-purpose LLMs and code-specific models on BLEU, ROUGE, METEOR, and BERTScore metrics.
- The model demonstrates strong interpretability by generating natural language explanations for vulnerability locations and mechanisms, though some hallucination cases were observed.
- Context length scaling shows significant performance gains (BLEU-4: 0.031 vs 0.168, BERT Score: 0.729 vs 0.910) when using full code context versus truncated 100 tokens.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Projection Alignment
Projecting code token embeddings into the language model's embedding space enables joint reasoning over code and natural language queries. A learnable projector (2-layer MLP with GELU activation, 4096 hidden dimensions) maps CodeSage embeddings to Vicuna's embedding space, concatenating projected code tokens with text tokens before LLM processing.

### Mechanism 2: Security-Focused Instruction Tuning
Fine-tuning on security-focused QA pairs enables specialized vulnerability reasoning beyond general code understanding. The model undergoes two-stage training: pretraining on code-description pairs to train the projector, then fine-tuning on 406K security-focused QA pairs generated from CVE descriptions using Llama-3.1-8B-Instruct.

### Mechanism 3: Context Length Scaling for Code Semantics
Providing full code context improves vulnerability reasoning performance. The model processes up to 1000 code tokens, with ablation showing significant degradation when truncated to 100 tokens, suggesting vulnerability patterns require longer-range code context.

## Foundational Learning

- Concept: **Multimodal Alignment via Projector Networks**
  - Why needed here: LLaVul's core architecture requires understanding how separate encoders are unified through learned projection—essential for debugging alignment failures and extending to new modalities.
  - Quick check question: Given a projector W that maps 130M-parameter code embeddings to 7B-parameter LLM space, what would happen to gradient flow if W were initialized to identity vs. random?

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Fine-tuning uses LoRA (rank 64) to train the LLM component efficiently—understanding this is critical for resource planning and hyperparameter tuning.
  - Quick check question: If LoRA rank is increased from 64 to 128, how would this affect (a) trainable parameter count, (b) VRAM usage during training, (c) risk of overfitting to the 406K QA pairs?

- Concept: **Transformer Encoder Architectures for Code**
  - Why needed here: The CodeSage-small encoder differs from decoder-only LLMs—understanding encoder vs. decoder trade-offs explains design choices and failure modes.
  - Quick check question: Why would an encoder-only model potentially outperform a generative model on classification while underperforming on generative QA tasks?

## Architecture Onboarding

- Component map:
  CodeSage-small (frozen) -> 2-layer MLP Projector -> Vicuna-7B-v1.5 (LoRA fine-tuned)

- Critical path:
  1. Code preprocessing → CodeSage encoding (layer -2 extraction)
  2. Projection W · Z_v → H'_v
  3. Query tokenization → Vicuna encoding → H_q
  4. Concatenation [H'_v; H_q] → LLM decoder → response generation
  5. Loss computation on response tokens, backprop through LoRA parameters + projector

- Design tradeoffs:
  - **Code encoder size**: CodeSage-small (130M) chosen over larger encoders—trades code understanding depth for training efficiency.
  - **Projector complexity**: 2-layer MLP vs. deeper network—simpler projector may bottleneck alignment but reduces overfitting risk.
  - **LoRA rank 64**: Balances adaptation capacity against overfitting—higher rank could improve performance but risks memorizing training CVEs.
  - **Synthetic QA generation**: Using Llama-3.1-8B-Instruct to generate training QA from CVE descriptions—scales data creation but may propagate hallucinations.

- Failure signatures:
  - **Hallucination of file names/functions**: Model references files not in input (e.g., CVE-2021-27341 case: "helpController.php" not provided but predicted as vulnerability source).
  - **Semantic drift in projector**: If BLEU/ROUGE metrics improve but BERT Score drops, projector may be producing text-plausible but semantically misaligned code representations.
  - **Classification degradation vs. QA**: Generative models may underperform on pure classification (LLaVul 0.52 F1 vs. CodeBERT 0.69 on DiverseVul).
  - **Truncation artifacts**: Limiting code to <100 tokens causes significant performance drop.

- First 3 experiments:
  1. **Reproduce pretraining → fine-tuning pipeline**: Train projector on CodeXGLUE summarization data (1 epoch, batch 10, LR 2e-5), then fine-tune on LLaVul-QA (3 epochs, batch 5). Verify BLEU-4 ≥0.15 on held-out test set.
  2. **Ablate code context length**: Evaluate fine-tuned model on test set with code truncated to 50, 100, 200, 500, and full tokens. Plot BLEU-4 and BERT Score vs. token count.
  3. **Probe hallucination sources**: Run inference on 20 CVE samples from test set, manually verify if generated file names/functions exist in input code vs. appear in training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLaVul's architecture be extended to reason about vulnerabilities in multi-file schemes or project-level contexts?
- Basis in paper: The authors state in the Discussion that current approaches focus on single-file identification and that "extending this problem to multi-file schemes could further boost precision and recall, especially for rare CWEs."
- Why unresolved: The current model architecture and evaluation are designed around function-level code snippets rather than cross-file dependencies or entire repositories.
- What evidence would resolve it: Evaluation results on a benchmark requiring cross-file context showing performance retention or improvement.

### Open Question 2
- Question: To what extent does utilizing a secondary LLM agent or human-in-the-loop validation mitigate hallucinations in generated security reasoning?
- Basis in paper: The authors identify hallucinations as a limitation and suggest that "integrating human-in-the-loop validation or a second LLM agent's verification of the generated QA" is a necessary future avenue.
- Why unresolved: The current study identifies the hallucination problem but does not implement or test the proposed verification mechanisms.
- What evidence would resolve it: Ablation studies comparing the factual consistency of answers generated with and without the proposed verification layers.

### Open Question 3
- Question: Does the reliance on synthetic QA data generated by Llama-3.1-8B limit the factual accuracy or generalizability of the model compared to human-curated data?
- Basis in paper: The authors used an LLM to generate QA pairs because manual creation is "labor-intensive," while the qualitative analysis revealed the model invents specific filenames found in training data, suggesting potential overfitting to synthetic artifacts.
- Why unresolved: It is undetermined if the observed errors are inherent to the model architecture or propagated from noise in the synthetic training dataset.
- What evidence would resolve it: A comparison of model performance when fine-tuned on human-verified security QA pairs versus the synthetic dataset used in this study.

## Limitations
- The LLaVul-QA dataset relies entirely on synthetic QA pairs generated by Llama-3.1-8B-Instruct from CVE descriptions, creating substantial risk of learning hallucinations rather than genuine vulnerability patterns.
- The CodeSage-small encoder (130M parameters) may lack the semantic depth of larger encoders, potentially limiting performance on complex vulnerabilities.
- Performance metrics focus on semantic similarity and lexical overlap rather than functional correctness or security expertise validation.

## Confidence
**High Confidence**: The architectural design is clearly specified and reproducible. The two-stage training procedure and LoRA fine-tuning approach are standard and well-documented. Performance improvements over baseline models are demonstrated with statistical significance.

**Medium Confidence**: The claim that LLaVul outperforms general-purpose and code-specific LLMs is supported by benchmark results, but evaluation relies entirely on synthetic metrics and automatically generated datasets.

**Low Confidence**: The assertion that LLaVul provides "interpretable" vulnerability analysis is weakly supported. While the model generates natural language explanations, there's no analysis of whether these explanations reveal genuine reasoning chains or simply plausible-sounding text.

## Next Checks
1. **Manual Security Expert Review**: Have 3-5 security researchers independently evaluate 50 randomly selected LLaVul outputs from the test set, classifying each as correct identification with accurate reasoning, correct identification with superficial reasoning, or hallucination/false positive.

2. **Cross-Domain Generalization Test**: Evaluate LLaVul on vulnerability code from a different domain than the training data (e.g., Python web frameworks if trained on C/C++/JS). Measure performance degradation to quantify domain adaptation limits.

3. **Projector Alignment Stress Test**: Generate synthetic code snippets with known semantic transformations (variable renaming, function extraction, control flow changes) and measure how projector alignment degrades. Use attention visualization to determine if the projector preserves semantic relationships under code refactoring.