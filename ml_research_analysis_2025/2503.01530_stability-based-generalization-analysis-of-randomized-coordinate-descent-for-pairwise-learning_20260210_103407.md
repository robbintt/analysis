---
ver: rpa2
title: Stability-based Generalization Analysis of Randomized Coordinate Descent for
  Pairwise Learning
arxiv_id: '2503.01530'
source_url: https://arxiv.org/abs/2503.01530
tags:
- learning
- stability
- generalization
- pairwise
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization performance of randomized
  coordinate descent (RCD) for pairwise learning. Pairwise learning includes tasks
  like ranking and metric learning, where loss functions depend on pairs of instances.
---

# Stability-based Generalization Analysis of Randomized Coordinate Descent for Pairwise Learning

## Quick Facts
- **arXiv ID:** 2503.01530
- **Source URL:** https://arxiv.org/abs/2503.01530
- **Reference count:** 9
- **Primary result:** Derived ℓ₂ on-average argument stability bounds for RCD in pairwise learning, yielding excess risk bounds of O(1/√n) for convex and O(log(n)/n) for strongly convex objectives

## Executive Summary
This paper establishes generalization bounds for Randomized Coordinate Descent (RCD) in pairwise learning settings where loss functions depend on pairs of instances. The authors analyze the ℓ₂ on-average argument stability of RCD, measuring how much the output model changes when a single training example is replaced. Based on this stability analysis, they derive excess risk bounds in expectation for both convex and strongly convex objective functions. The analysis shows that RCD achieves O(1/√n) excess risk for convex objectives and O(log(n)/n) for strongly convex objectives, with early stopping used to balance estimation and optimization errors.

## Method Summary
The paper analyzes RCD for pairwise learning tasks like ranking and metric learning. The method involves computing pairwise losses f(w; z_i, z_j) and partial gradients ∇_{i_t} F_S(w) over all pairs (i≠j), then updating parameters along randomly selected coordinates. The stability analysis tracks the expected ℓ₂ distance between models trained on original datasets versus perturbed datasets (where one sample is replaced). The authors derive excess risk bounds using this stability measure, with specific iteration counts T (T ∝ n for convex, T ∝ log(n) for strongly convex) determined by early stopping to balance estimation and optimization errors.

## Key Results
- For convex pairwise learning, RCD achieves O(1/√n) excess risk bound
- For strongly convex pairwise learning, RCD achieves O(log(n)/n) excess risk bound
- Early stopping with T ∝ log(n) provides optimal balance between estimation and optimization errors
- The ℓ₂ on-average argument stability measure effectively bounds the generalization gap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The generalization gap of RCD for pairwise learning can be controlled by measuring the ℓ₂ on-average argument stability
- **Mechanism:** By tracking the average Euclidean distance between model parameters trained on original datasets versus perturbed datasets, the authors bound the discrepancy between training and testing risks
- **Core assumption:** Assumption 2: The loss function is L-smooth (gradients don't change too abruptly)
- **Evidence anchors:**
  - [abstract] "We measure the on-average argument stability... based on which we develop generalization bounds in expectation"
  - [section] Lemma 1 (Page 4-5) bounds estimation error using ℓ₂ on-average argument stability
  - [corpus] Weak direct evidence; neighboring papers utilize similar stability frameworks but for different algorithms
- **Break condition:** The mechanism relies on the expectation over data samples; high variance in data distribution or non-smooth loss landscapes may break the stability bound

### Mechanism 2
- **Claim:** The "coercivity property" of gradient descent operators prevents RCD iterates from diverging too rapidly between datasets, thereby ensuring stability
- **Mechanism:** RCD updates only one coordinate per step. The authors show that in expectation, the randomized coordinate update behaves like a full gradient descent step, limiting the growth of parameter differences
- **Core assumption:** Assumption 4 (Convexity) and Assumption 2 (L-Smoothness) are required to invoke the coercivity property
- **Evidence anchors:**
  - [section] Appendix B.3 (Page 9) explicitly states the coercivity inequality used to bound the expansion of updates
  - [section] Page 5 mentions applying "the coercivity property to bound the expansiveness of RCD updates"
  - [corpus] No direct corpus validation for this specific mathematical property in RCD
- **Break condition:** If the objective function is non-convex, the coercivity property does not hold in the same form

### Mechanism 3
- **Claim:** An early-stopping strategy allows RCD to achieve optimal convergence rates by balancing estimation and optimization errors
- **Mechanism:** Optimization error decreases as iterations T increase, while estimation error increases with T. By stopping early (e.g., T ∝ n^(1/2) for convex cases), the sum of these two errors is minimized
- **Core assumption:** The optimal iteration count T must scale correctly with sample size n
- **Evidence anchors:**
  - [abstract] "The early-stopping strategy is adopted to quantify the balance between estimation and optimization"
  - [section] Remark 2 and Theorem 3 define the specific optimal stopping steps T
  - [corpus] Related work contrasts this with SGD, highlighting the specific iteration rates required
- **Break condition:** Running the algorithm for too many iterations causes the stability term to grow, degrading the excess risk bound

## Foundational Learning

- **Concept: Pairwise Learning**
  - **Why needed here:** Unlike standard pointwise learning, the loss function here depends on pairs of instances (z, z'). This complicates the gradient and Hessian structures, requiring specific analysis for tasks like Ranking and AUC maximization
  - **Quick check question:** Does your loss function require comparing two distinct data points simultaneously (e.g., "is A ranked higher than B?") to compute a gradient?

- **Concept: Randomized Coordinate Descent (RCD)**
  - **Why needed here:** This is the algorithm under analysis. It updates parameters along a single random coordinate direction rather than the full gradient, which affects how the algorithm processes information and converges
  - **Quick check question:** In your update loop, are you computing the full gradient vector or selecting a specific coordinate index i_t to update?

- **Concept: L-Smoothness (Lipschitz Continuous Gradients)**
  - **Why needed here:** This is Assumption 2. It is mathematically critical for the "self-bounding" property (Lemma 4) and coercivity, allowing the authors to bound gradients using function values
  - **Quick check question:** Is the gradient of your loss function bounded in magnitude relative to the distance between parameter points?

## Architecture Onboarding

- **Component map:**
  1. Pairwise Loss Layer: Computes loss f(w; z_i, z_j) and partial gradients ∇_{i_t} F_S(w)
  2. RCD Optimizer: Selects random coordinate i_t and updates w_{t+1} = w_t - η_t ∇_{i_t} F_S(w_t) e_{i_t}
  3. Stability Monitor: (Theoretical) Tracks expected ℓ₂ distance between models trained on perturbed datasets
  4. Early Stopping Scheduler: Halts training based on iteration count T scaling with n

- **Critical path:**
  Define Pairwise Loss → Initialize w_1 → Loop (t=1 to T): Sample coordinate i_t → Compute partial gradient → Update w_{t+1} → Stop when t=T_optimal

- **Design tradeoffs:**
  - **Iterations (T) vs. Stability:** Increasing iterations reduces training error but increases the stability bound (estimation error). You must strictly cap T (e.g., T ≈ n or √n) to guarantee generalization
  - **Coordinate-wise vs. Full Gradient:** RCD is cheaper per iteration but requires specific smoothness assumptions (coordinate-wise Lipschitz) that may differ from standard smoothness

- **Failure signatures:**
  - **Divergent Stability:** If ||w_t - w'_t||_2 grows linearly or exponentially with T, check if the step size η_t violates the condition η_t ≤ 1/L
  - **Stagnation:** If T is too small relative to n, optimization error will dominate, and the model will underfit

- **First 3 experiments:**
  1. **Stability Verification:** Replicate the experiment in Figure 1. Train RCD on dataset S and a perturbed S'; plot the Euclidean distance Δ_t = ||w_t - w'_t||_2 over time to confirm it follows the theoretical bound
  2. **Early Stopping Validation:** Vary the number of passes T/n on a validation set for a convex pairwise task (e.g., AUC maximization) to observe the "U-curve" of excess risk and identify the optimal T
  3. **Algorithm Comparison:** Compare the stability behavior of RCD against SGD on the same pairwise task to verify if RCD exhibits the theoretically predicted lower variance

## Open Questions the Paper Calls Out
None

## Limitations
- The stability analysis critically depends on coordinate-wise Lipschitz smoothness assumptions that may not hold uniformly across all pairwise learning tasks
- The coercivity property invoked in Appendix B.3 lacks direct validation from the broader literature, representing a significant theoretical gap
- The early-stopping bounds assume perfect knowledge of the optimal iteration count T, which is typically unavailable in practice

## Confidence

- **High confidence:** The core mechanism connecting ℓ₂ on-average argument stability to generalization error (Lemma 1) is well-established in stability theory and properly applied
- **Medium confidence:** The application of coercivity to RCD updates for pairwise learning is mathematically sound but lacks independent verification from related work
- **Low confidence:** The optimality claims for strongly convex convergence rates depend heavily on the assumed coercivity bounds, which have limited external validation

## Next Checks

1. **Literature triangulation:** Verify the coercivity inequality in Appendix B.3 against other coordinate descent stability analyses to confirm it's not a novel assumption
2. **Numerical validation:** Implement the early-stopping experiment with multiple synthetic pairwise learning tasks to empirically confirm the T ∝ log(n) scaling prediction
3. **Algorithm comparison:** Replicate the SGD vs RCD stability comparison under identical hyperparameters to validate the claimed variance reduction properties