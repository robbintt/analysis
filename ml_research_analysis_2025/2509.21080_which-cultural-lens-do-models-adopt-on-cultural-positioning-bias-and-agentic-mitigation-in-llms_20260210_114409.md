---
ver: rpa2
title: Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic
  Mitigation in LLMs
arxiv_id: '2509.21080'
source_url: https://arxiv.org/abs/2509.21080
tags:
- cultural
- cultures
- bias
- llms
- culture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit systematic cultural positioning
  bias, consistently adopting "insider" perspectives for U.S. culture while demonstrating
  "outsider" positioning for other cultures.
---

# Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs

## Quick Facts
- arXiv ID: 2509.21080
- Source URL: https://arxiv.org/abs/2509.21080
- Reference count: 38
- Key result: MFA-MA mitigates cultural positioning bias with up to 82.55% CAG reduction.

## Executive Summary
Large language models exhibit systematic cultural positioning bias, consistently adopting "insider" perspectives for U.S. culture while demonstrating "outsider" positioning for other cultures. The authors introduce the CultureLens benchmark with 4,000 prompts across 10 cultures and three evaluation metrics (CEP, CPD, CAG) to quantify this bias. They propose two mitigation approaches—FIP (prompt-based fairness guidelines) and MFA (agent-based frameworks)—with MFA-MA achieving the best results by reducing bias by up to 82.55% on the CAG metric for Qwen.

## Method Summary
The study constructs the CultureLens benchmark using 4,000 prompts across 10 cultures with varied demographic attributes. LLMs generate interview scripts that are classified as "insider" or "outsider" perspectives by a GPT-o4-mini judge. The researchers measure cultural positioning bias using CEP, CPD, and CAG metrics. Two mitigation strategies are developed: FIP, which injects fairness guidelines into prompts, and MFA, which employs single-agent or multi-agent reflection-rewrite pipelines. MFA-MA uses a Planner-Critic-Refiner agent hierarchy to systematically reduce bias.

## Key Results
- LLMs consistently adopt "insider" perspectives for U.S. culture (>88% externality) while showing "outsider" positioning for other cultures.
- Cultural positioning bias varies systematically across models, with higher externality for less dominant cultures.
- MFA-MA reduces bias most effectively, achieving up to 82.55% CAG reduction for Qwen.

## Why This Works (Mechanism)
The paper demonstrates that structured agent-based approaches can systematically identify and mitigate cultural positioning bias by introducing explicit reflection and refinement steps. The multi-agent pipeline allows for more nuanced analysis and correction of biased outputs through iterative critique and rewriting.

## Foundational Learning
- **Cultural Externality Percentage (CEP)**: Measures the proportion of "outsider" perspectives in generated content; needed to quantify bias magnitude, check via computing mean CEP across cultures.
- **Cultural Perspective Deviation (CPD)**: Standard deviation of CEP across cultures; needed to assess bias consistency, check via verifying higher CPD indicates more systematic bias.
- **Cultural Alignment Gap (CAG)**: Difference between US and non-US CEP; needed to measure cross-cultural bias, check via confirming CAG > 0 indicates US favoritism.
- **Agent-based mitigation**: Uses multiple specialized agents for systematic bias correction; needed for structured bias reduction, check via verifying agent roles and feedback loops.
- **Fairness injection**: Embeds fairness principles into prompts; needed for baseline mitigation, check via confirming fairness pillars address identified bias patterns.
- **Binary classification**: Simplifies complex cultural perspectives into insider/outsider categories; needed for quantifiable metrics, check via validating classification accuracy.

## Architecture Onboarding
**Component Map**: CultureLens prompts → LLM generation → GPT-o4-mini judge → CEP/CPD/CAG calculation → FIP/MFA mitigation → Re-evaluation

**Critical Path**: Prompt generation → Script creation → Bias classification → Metric computation → Mitigation application → Bias reduction verification

**Design Tradeoffs**: Binary classification simplifies measurement but may oversimplify cultural nuance; single-agent vs multi-agent approaches balance simplicity with effectiveness; prompt-based vs agent-based mitigation trade implementation complexity against performance.

**Failure Signatures**: 
- High inter-annotator disagreement (>0.6 Cohen's κ)
- Over-correction leading to 100% externality for dominant cultures
- Agent loops exceeding token limits or stalling

**First 3 Experiments**:
1. Generate 100 scripts with each LLM and compute baseline CEP to verify systematic US favoritism
2. Apply FIP to a 50-prompt subset and verify CEP reduction without over-correction
3. Implement MFA-MA on 50 prompts and measure CAG reduction compared to baseline

## Open Questions the Paper Calls Out
None specified.

## Limitations
- Binary insider/outsider classification may oversimplify complex cultural perspectives
- Reliance on single judge model without human validation could propagate cultural assumptions
- Mitigation tested only on 500-prompt subset, limiting generalizability claims
- Cultural positioning measured only through interview scripts, not other text types

## Confidence
- Cultural positioning bias detection methodology: Medium
- Quantitative bias measurements (CEP, CPD, CAG): High
- Mitigation effectiveness claims: Medium
- Generalizability across prompt types: Low

## Next Checks
1. Conduct human annotation validation on 100 randomly selected samples to establish inter-annotator agreement and validate the GPT-o4-mini judge's classifications.
2. Replicate the full mitigation experiments on the complete 4,000-prompt dataset rather than the 500-prompt subset to verify scalability and consistency.
3. Test the MFA-MA approach on diverse prompt types beyond interview scripts (e.g., news articles, creative writing) to assess cross-domain effectiveness.