---
ver: rpa2
title: 'MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and
  Attention Distillation'
arxiv_id: '2501.02754'
source_url: https://arxiv.org/abs/2501.02754
tags:
- backdoor
- mbtsad
- attention
- clean
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MBTSAD, a method to mitigate backdoor attacks
  in language models without requiring pre-trained weights. The core idea is to first
  retrain the backdoored model on a dataset augmented via token splitting, then perform
  attention distillation where the retrained model acts as the teacher and the backdoored
  model as the student.
---

# MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation

## Quick Facts
- arXiv ID: 2501.02754
- Source URL: https://arxiv.org/abs/2501.02754
- Reference count: 40
- The paper proposes MBTSAD, a method to mitigate backdoor attacks in language models without requiring pre-trained weights

## Executive Summary
MBTSAD introduces a novel approach to backdoor mitigation in language models that operates without requiring access to pre-trained weights. The method employs a two-stage process: first retraining the backdoored model on augmented data generated through token splitting, then performing attention distillation where the retrained model serves as the teacher and the backdoored model as the student. This approach successfully reduces attack success rates below 20% while maintaining clean data accuracy, achieving comparable performance to methods that require pre-trained weights.

## Method Summary
MBTSAD operates through a two-stage process to mitigate backdoors in language models. First, it generates augmented training data by splitting tokens at random positions, creating out-of-distribution samples that help the model learn more generalized features. The backdoored model is then retrained on this augmented dataset to produce an intermediate model. In the second stage, attention distillation transfers knowledge from this intermediate model to the backdoored model, effectively removing backdoor patterns while preserving clean data performance. The method is evaluated on SST-2 and IMDb datasets against BadNets, LWP, and EP attacks, using only 20% of clean training data.

## Key Results
- Achieves attack success rates below 20% while maintaining clean data accuracy
- Performs comparably to methods requiring pre-trained weights
- Successfully mitigates BadNets, LWP, and EP attacks using only 20% clean data

## Why This Works (Mechanism)
The token splitting augmentation creates out-of-distribution data that forces the model to learn more robust and generalized representations. This breaks the specific backdoor patterns that rely on particular token sequences. The attention distillation phase then transfers these generalized representations from the retrained model to the backdoored model, effectively overwriting the backdoor-specific attention patterns while preserving legitimate task knowledge.

## Foundational Learning
- **Token splitting augmentation**: Random token splitting creates OOD data; needed because standard augmentations don't sufficiently break backdoor patterns; quick check: verify N=3 augmentations per sample with 30% token splitting rate
- **Attention distillation**: Knowledge transfer using both attention maps and hidden states; needed to preserve clean task performance while removing backdoors; quick check: verify L_total includes L_a, L_cls, and L_h components
- **Two-stage mitigation**: Retraining followed by distillation; needed to first break backdoor patterns then refine representation; quick check: confirm both stages use different learning rates (2×10⁻⁵ and 5×10⁻⁴)

## Architecture Onboarding

**Component Map:**
Data Augmentation (Token Splitting) -> Model Retraining -> Attention Distillation -> Final Backdoor-Free Model

**Critical Path:**
Token splitting augmentation → Retraining (Mb → Mp) → Attention distillation (Mp→Mb→Mc) → Clean, backdoor-free model

**Design Tradeoffs:**
- Token splitting vs. other augmentations: Token splitting generates OOD data which is more effective at breaking backdoor patterns
- Two-stage vs. single-stage: Two-stage allows breaking patterns first then refining, but increases computational cost
- Distillation with full supervision vs. CD: Including attention and hidden states losses is critical for effective backdoor removal

**Failure Signatures:**
- ASR remains high (>20%) if token splitting is not used or if augmentation rate is too low
- Clean accuracy drops significantly if L_h (hidden states loss) is omitted from distillation
- Ineffective mitigation if only classification loss (L_cls) is used without attention transfer

**3 First Experiments:**
1. Verify token splitting generates 3 augmentations per sample with 30% token splitting rate
2. Confirm retraining stage uses learning rate 2×10⁻⁵ on augmented dataset
3. Test distillation stage with all three loss components (L_a, L_cls, L_h) at specified learning rate

## Open Questions the Paper Calls Out
None

## Limitations
- Requires access to 20% of clean training data, which may not be available in all scenarios
- Computational cost of two-stage approach may be prohibitive for very large language models
- Performance on datasets beyond SST-2 and IMDb remains unknown

## Confidence

**High confidence:** The core two-stage methodology (token splitting augmentation followed by attention distillation) is clearly described and experimentally validated

**Medium confidence:** The effectiveness against specific attack types (BadNets, LWP, EP) on the two evaluated datasets

**Low confidence:** Generalization to other datasets, language tasks, or backdoor attack variants; optimal hyperparameter settings for α and β

## Next Checks

1. Conduct ablation studies to determine the sensitivity of results to α and β hyperparameter values, establishing concrete recommended ranges
2. Evaluate MBTSAD's performance on additional datasets (e.g., AG News, DBpedia) and tasks (e.g., NLI, NER) to assess generalizability
3. Measure the computational overhead of the two-stage approach compared to single-stage mitigation methods and establish scaling properties for larger models