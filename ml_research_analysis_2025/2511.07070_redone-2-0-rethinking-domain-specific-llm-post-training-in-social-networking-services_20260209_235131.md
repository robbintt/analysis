---
ver: rpa2
title: 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking
  Services'
arxiv_id: '2511.07070'
source_url: https://arxiv.org/abs/2511.07070
tags:
- arxiv
- redone
- language
- preprint
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RedOne 2.0 tackles domain-specific LLM adaptation in social networking
  services, where fast-changing slang, multilingual content, and heterogeneous workloads
  make generalization difficult. To address the instability and forgetting induced
  by SFT, it introduces a three-stage RL-prioritized post-training pipeline: (1) Exploratory
  Learning aligns the base model with SNS data and diagnoses weaknesses; (2) Targeted
  Fine-Tuning repairs deficiencies using a small mixture of SNS and general data with
  soft labels to mitigate forgetting; (3) Refinement Learning consolidates gains via
  RL with SNS-centric rewards.'
---

# RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services

## Quick Facts
- arXiv ID: 2511.07070
- Source URL: https://arxiv.org/abs/2511.07070
- Reference count: 40
- Primary result: RL-prioritized three-stage pipeline achieves 70.80 General-Bench and 67.57 SNS-Bench with 4B model, outperforming larger models while using <50% data of SFT-heavy methods.

## Executive Summary
RedOne 2.0 introduces a three-stage RL-prioritized post-training pipeline to address domain-specific LLM adaptation in social networking services, where fast-changing slang, multilingual content, and heterogeneous workloads make generalization difficult. The approach replaces traditional SFT-heavy fine-tuning with Exploratory Learning (RL-based alignment), Targeted Fine-Tuning (SFT on weak tasks with soft-label regularization), and Refinement Learning (RL consolidation). This design preserves general capabilities while achieving strong SNS performance, demonstrated by +0.43% advertiser value in online deployment, reduced vague titles by 11.9%, and increased interactive titles by 25.8%.

## Method Summary
RedOne 2.0 uses a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation to SNS domains. The three-stage pipeline starts with DAPO-based RL on mixed SNS and general data to establish initial alignment and diagnose weaknesses. Stage 2 applies targeted SFT on identified weak tasks combined with soft-labeled general data to mitigate forgetting. Stage 3 uses RL consolidation with increased rationale-included samples. The approach requires only ~50% of the data used in traditional SFT-heavy methods while achieving superior performance on both general and domain-specific benchmarks.

## Key Results
- RedOne 2.0 4B achieves 70.80 average score on General-Bench and 67.57 on SNS-Bench
- Outperforms 7B models on both benchmarks while using less than half the data
- Online deployment shows +0.43% advertiser value, 11.9% reduction in vague titles, and 25.8% increase in interactive titles
- Incremental improvements: Exploratory (62.27) → Targeted (65.67) → Full pipeline (67.57) on SNS-Bench

## Why This Works (Mechanism)

### Mechanism 1: RL-Prioritized Domain Alignment Preserves General Capabilities
Starting post-training with reinforcement learning rather than supervised fine-tuning reduces catastrophic forgetting when adapting to specialized domains. RL optimizes directly against reward signals that can encode both domain-specific and general objectives simultaneously, allowing the model to explore within a reward-shaped solution space while preserving previously learned competencies.

### Mechanism 2: Soft-Label Regularization Mitigates Distribution Shift During Targeted SFT
Mixing a small fraction of general-domain data with soft labels during SFT reduces forgetting of general capabilities. Soft labels represent the model's own learned distribution rather than hard ground-truth labels, reducing the KL-divergence penalty and preventing abrupt distributional shifts that cause catastrophic forgetting.

### Mechanism 3: Diagnostic-Guided Curriculum Enables Task-Level Trade-off Control
Using the first RL stage to identify systematic weaknesses, then targeting those weaknesses with SFT, and finally consolidating with RL produces more stable multi-task improvements than monolithic training. Each stage has a distinct optimization focus that prevents the model from overfitting to easy tasks early and allows later stages to correct imbalances.

## Foundational Learning

- **Catastrophic Forgetting in Neural Networks**: Why needed: The paper's central problem is that SFT-heavy domain adaptation causes models to lose general capabilities. Understanding why neural networks overwrite previously learned representations when trained on new distributions is essential to grasp why the three-stage pipeline matters. Quick check: Can you explain why fine-tuning on a narrow domain corpus causes a model's performance on its original pretraining distribution to degrade?

- **Policy Gradient Methods and PPO-style Clipping**: Why needed: The DAPO algorithm used in stages 1 and 3 is a variant of group-relative policy optimization with clipping. Understanding how clipping prevents excessive policy updates and why group-relative advantages stabilize training is necessary to implement or debug the RL stages. Quick check: In the DAPO loss function (equation 6), what happens to the gradient signal if the probability ratio r(θ) exceeds the upper clipping bound ε_high?

- **Task-Specific Reward Design for Heterogeneous Outputs**: Why needed: The paper defines four reward types (Exact Match, Metrics-based, Sandbox, Pattern) for different task categories. Understanding when to use each type and how to combine them is critical for adapting this approach to new domains. Quick check: Why would using an Exact Match reward for an open-ended translation task fail, and what metric-based alternative would be more appropriate?

## Architecture Onboarding

- **Component map**: Exploratory Learning (DAPO RL on mixed SNS/general) -> Targeted Fine-Tuning (SFT on weak-task SNS + soft-labeled general) -> Refinement Learning (DAPO RL consolidation)
- **Critical path**: 1) Curate D_SNS (75+ tasks) and D_GEN; 2) Run Exploratory Learning; log per-task rewards to identify weak buckets; 3) Extract failure-task samples; generate soft labels for general data using stage-1 model; 4) Run Targeted Fine-Tuning on failure mix + soft-labeled general; 5) Run Refinement Learning with increased rationale ratio; 6) Evaluate on General-Bench, SNS-Bench, SNS-TransBench before deployment.
- **Design tradeoffs**: Data volume vs. stability (uses <50% data but achieves +8.74 lift); Model scale vs. deployment cost (4B outperforms 7B but requires careful RL tuning); Reward complexity vs. debugging difficulty (four reward types enable precision but complicate failure diagnosis).
- **Failure signatures**: General-Bench drops after Targeted Fine-Tuning (insufficient soft-label regularization); Sandbox rewards saturate at zero (generated code fails execution); RL loss oscillates without metric improvement (clipping bounds too tight or reward variance too high).
- **First 3 experiments**: 1) Baseline comparison: Run naive SFT→RL pipeline and compare General-Bench/SNS-Bench scores to confirm "seesaw" effect; 2) Ablation on soft-label fraction: Vary general data proportion (0%, 5%, 10%, 15%) in Targeted Fine-Tuning to find minimum effective regularization threshold; 3) Single-stage RL control: Skip Targeted Fine-Tuning entirely and run two consecutive RL stages to validate whether SFT provides unique diagnostic-targeted repair.

## Open Questions the Paper Calls Out
- How can faithfulness constraints be reinforced in RedOne 2.0's generation pipeline to prevent over-optimization for engagement at the expense of critical factual details?
- What is the temporal degradation rate of RedOne 2.0's performance as SNS slang, trends, and community norms evolve beyond the training data distribution?
- Does the RL-prioritized three-stage pipeline transfer effectively to other domains with heterogeneous tasks and rapid distribution change, such as healthcare, legal, or financial applications?

## Limitations
- Domain specificity: Strong SNS performance but unclear generalization to other specialized domains without modification
- RL implementation details: Critical DAPO parameters underspecified (group size, equivalence threshold, advantage normalization)
- Evaluation scope: Lacks long-term stability analysis and cross-domain validation

## Confidence
**High Confidence**: Three-stage pipeline outperforms SFT-heavy approaches on both SNS-Bench and General-Bench; Targeted Fine-Tuning with soft-label regularization effectively mitigates catastrophic forgetting; Pipeline requires less than half the data volume while achieving superior performance.

**Medium Confidence**: Diagnostic-guided curriculum is essential for stable multi-task improvements; RL-prioritized initialization is superior to SFT-prioritized for domain adaptation; Four reward types mapped to task categories is optimal for SNS performance.

**Low Confidence**: Approach generalizes effectively to domains beyond social networking services; 4B model size represents optimal tradeoff for all SNS applications; Soft-label regularization would be equally effective with different base model architectures.

## Next Checks
1. **Cross-Domain Transfer Validation**: Apply the three-stage pipeline to a non-SNS domain (e.g., legal document processing or biomedical text analysis) and compare performance against both SFT-heavy approaches and the original SNS results.

2. **Long-Term Stability Monitoring**: Deploy the model in production for 3-6 months while tracking performance metrics on both SNS-Bench tasks and General-Bench tasks. Measure degradation rates and compare against a control group using traditional SFT approaches.

3. **Ablation Study on Soft-Label Quality**: Systematically vary the quality of the stage-1 model used to generate soft labels (e.g., by using different training durations or hyperparameter settings) and measure the impact on stage-2 performance and stage-3 convergence.