---
ver: rpa2
title: 'BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents'
arxiv_id: '2504.12516'
source_url: https://arxiv.org/abs/2504.12516
tags:
- answer
- browsecomp
- were
- questions
- browsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BrowseComp is a benchmark of 1,266 challenging questions designed
  to evaluate browsing agents' ability to persistently search the web for hard-to-find,
  entangled information. Questions require reasoning about factuality, navigating
  deeply, and creatively searching to solve within a tractable time.
---

# BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents

## Quick Facts
- arXiv ID: 2504.12516
- Source URL: https://arxiv.org/abs/2504.12516
- Reference count: 3
- Primary result: 1,266-question benchmark for evaluating browsing agents on hard, entangled web information tasks; OpenAI Deep Research achieves 51.5% accuracy, outperforming other models by large margins.

## Executive Summary
BrowseComp is a benchmark of 1,266 challenging questions designed to evaluate browsing agents' ability to persistently search the web for hard-to-find, entangled information. Questions require reasoning about factuality, navigating deeply, and creatively searching to solve within a tractable time. Despite the difficulty, predicted answers are short and easily verifiable, making it simple to use. On BrowseComp, OpenAI Deep Research achieves 51.5% accuracy, outperforming GPT-4o (0.6%), GPT-4o with browsing (1.9%), GPT-4.5 (0.9%), and OpenAI o1 (9.9%). Performance scales with test-time compute, and best-of-N sampling with confidence-based voting further boosts Deep Research accuracy by up to 25%. Human trainers solved only 29.2% of questions within two hours, underscoring the benchmark's difficulty.

## Method Summary
BrowseComp is a benchmark of 1,266 challenging questions designed to evaluate browsing agents' ability to persistently search the web for hard-to-find, entangled information. Questions require reasoning about factuality, navigating deeply, and creatively searching to solve within a tractable time. Despite the difficulty, predicted answers are short and easily verifiable, making it simple to use. The benchmark is evaluated on several browsing agents, including OpenAI Deep Research, GPT-4o, GPT-4o with browsing, GPT-4.5, and OpenAI o1, using a standard accuracy metric based on the correctness of the predicted answers.

## Key Results
- OpenAI Deep Research achieves 51.5% accuracy on BrowseComp, significantly outperforming GPT-4o (0.6%), GPT-4o with browsing (1.9%), GPT-4.5 (0.9%), and OpenAI o1 (9.9%).
- Performance on BrowseComp scales with test-time compute, and best-of-N sampling with confidence-based voting further boosts Deep Research accuracy by up to 25%.
- Human trainers solved only 29.2% of questions within two hours, underscoring the benchmark's difficulty.

## Why This Works (Mechanism)
BrowseComp's design emphasizes persistent, creative searching and deep navigation to find entangled, hard-to-find information on the web. The benchmark's questions require reasoning about factuality and the ability to navigate complex information landscapes, which are key capabilities for effective browsing agents. The short, easily verifiable answers allow for straightforward evaluation and comparison across models.

## Foundational Learning
- Web search strategies: Understanding how to effectively search for information on the web is crucial for browsing agents, as it allows them to navigate complex information landscapes and find the most relevant sources.
- Factuality reasoning: Browsing agents must be able to reason about the truthfulness and accuracy of the information they find, as this is essential for providing reliable answers to users.
- Deep navigation: The ability to navigate deeply into web pages and follow complex paths of information is necessary for finding entangled, hard-to-find information.
- Creative searching: Browsing agents must be able to think creatively and use a variety of search strategies to find the most relevant and accurate information.
- Short, verifiable answers: The benchmark's design emphasizes short, easily verifiable answers, which allows for straightforward evaluation and comparison across models.

## Architecture Onboarding
- Component map: Browsing agent -> Web search -> Information extraction -> Answer generation
- Critical path: The critical path for a browsing agent on BrowseComp involves effectively searching the web, navigating deeply into relevant sources, extracting the most important information, and generating a short, accurate answer.
- Design tradeoffs: The benchmark's design emphasizes persistent, creative searching and deep navigation, which may come at the cost of increased computational resources and time. However, this tradeoff is necessary for evaluating the most challenging aspects of browsing agents.
- Failure signatures: Browsing agents may fail on BrowseComp by getting stuck in local information minima, failing to navigate deeply enough, or generating answers that are not short or easily verifiable.
- First experiments: 1) Test browsing agents on a small subset of BrowseComp questions to identify common failure modes. 2) Compare the performance of browsing agents with and without access to advanced search features (e.g., Boolean operators, advanced filters). 3) Evaluate the impact of increasing the amount of test-time compute on browsing agent performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions, but some potential areas for further research include: 1) Developing more efficient browsing agents that can solve BrowseComp questions within a shorter time frame. 2) Investigating the impact of different search strategies and information extraction techniques on browsing agent performance. 3) Exploring the use of multi-modal information sources (e.g., images, videos) to enhance browsing agent capabilities.

## Limitations
- The benchmark's difficulty is high, with human trainers solving only 29.2% of questions within two hours, but this low performance could stem from the inherently hard nature of the questions, the constraints of the interface, or insufficient training in browsing strategies.
- While the benchmark is designed to test "entangled" information, the exact definition and boundaries of this category are not clearly specified, leaving open questions about the scope of what is being evaluated.
- The evaluation protocol relies on predicted answers being "short and easily verifiable," but this simplicity may not always reflect the complexity of the reasoning required, especially for questions demanding creative or persistent searching.

## Confidence
- Claim that BrowseComp is "simple yet challenging": High (supported by benchmark design and human performance)
- Claim that OpenAI Deep Research outperforms other models by a large margin: High (clear, reproducible results)
- Claim that performance scales with test-time compute: Medium (only one model family tested, no ablation of compute settings)
- Claim that best-of-N with confidence voting yields consistent improvements: Low (no sensitivity analysis or broader validation)

## Next Checks
1. Conduct a detailed error analysis, breaking down question types and common failure modes across models.
2. Test the benchmark with a broader set of browsing agents and architectures to verify generalizability of the scaling claims.
3. Perform ablation studies on best-of-N sampling (varying N, confidence thresholds) to quantify robustness and reproducibility of reported gains.