---
ver: rpa2
title: RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval
arxiv_id: '2509.25257'
source_url: https://arxiv.org/abs/2509.25257
tags:
- code
- graph
- name
- retrieval
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RANGER is a repository-level retrieval agent that addresses both
  code-entity and natural language queries in automated software engineering. It constructs
  a comprehensive knowledge graph from Python repositories using AST parsing and enriches
  nodes with semantic descriptions and embeddings.
---

# RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval

## Quick Facts
- arXiv ID: 2509.25257
- Source URL: https://arxiv.org/abs/2509.25257
- Authors: Pratik Shah; Rajat Ghosh; Aryan Singhal; Debojyoti Dutta
- Reference count: 40
- RANGER outperforms strong embedding baselines on CodeSearchNet and RepoQA, achieves superior cross-file dependency retrieval on RepoBench, and pairs with BM25 to deliver the highest exact match rate on CrossCodeEval.

## Executive Summary
RANGER is a repository-level retrieval agent that addresses both code-entity and natural language queries in automated software engineering. It constructs a comprehensive knowledge graph from Python repositories using AST parsing and enriches nodes with semantic descriptions and embeddings. A dual-stage pipeline routes entity queries to fast Cypher lookups and natural language queries to MCTS-guided graph exploration. Evaluated on four benchmarks, RANGER demonstrates superior retrieval performance compared to existing embedding-based approaches.

## Method Summary
RANGER constructs a knowledge graph from Python repositories by parsing ASTs to extract code entities and their relationships, then augmenting nodes with LLM-generated semantic descriptions. The system employs a dual-stage retrieval pipeline: code-entity queries are routed to fast Neo4j Cypher lookups, while natural language queries trigger an MCTS-guided exploration that uses bi-encoders for expansion and cross-encoders for scoring. The offline indexing process involves tree-sitter parsing, repository consolidation, LLM summarization, and embedding computation, while online retrieval leverages LLM-generated Cypher queries and MCTS-based graph traversal for semantic search.

## Key Results
- Outperforms embedding baselines on CodeSearchNet and RepoQA benchmarks
- Achieves superior cross-file dependency retrieval on RepoBench
- Delivers highest exact match rate when paired with BM25 on CrossCodeEval

## Why This Works (Mechanism)

### Mechanism 1: Dual-Stage Query Routing
Routing queries based on the presence of code entities improves retrieval efficiency by utilizing fast symbolic lookups before invoking expensive semantic search. An LLM translates user queries into Cypher statements, with direct returns indicating entity queries and `None` results triggering MCTS for natural language queries.

### Mechanism 2: MCTS-Guided Bi-Coder Exploration
Monte Carlo Tree Search combines bi-encoder speed with cross-encoder accuracy by using fast bi-encoders to select top-k neighbors for expansion, then applying slower cross-encoders only to promising candidates for precise scoring and backpropagation.

### Mechanism 3: Semantic Augmentation of Structural Graphs
Augmenting AST-based structural graphs with LLM-generated natural language descriptions bridges the semantic gap between code symbols and user intent, enabling semantic retrieval over structural representations.

## Foundational Learning

- **Graph Databases (Neo4j/Cypher)**: Required to understand RANGER's structural storage and Path 1 retrieval; Quick check: Can you write a Cypher query to find all methods that `USE` a specific class `Calculator`?
- **Bi-Encoder vs. Cross-Encoder Architectures**: Essential for understanding RANGER's exploration vs. scoring trade-off; Quick check: Why is a cross-encoder generally more accurate but less scalable than a bi-encoder for scoring query-code pairs?
- **Monte Carlo Tree Search (MCTS) UCT Algorithm**: Needed to understand the agent's navigation strategy; Quick check: In the UCT formula $UCT(v) = \frac{R_v}{N_v} + c \sqrt{\frac{\ln N_{parent}}{N_v}}$, what happens to node selection if the exploration constant $c$ is set too high?

## Architecture Onboarding

- **Component map**: Offline Indexer: Tree-sitter -> JSON -> Neo4j + LLM Summarizer -> Embedding Store; Online Router: Query -> LLM -> Cypher Query; Retrieval Paths: Path 1 (Neo4j Direct) vs. Path 2 (MCTS Agent -> Bi-Encoder Expansion -> Cross-Encoder Reward)
- **Critical path**: The MCTS Simulation Step is the computational bottleneck, where cross-encoder inference speed dictates natural language query latency
- **Design tradeoffs**: RANGER trades offline compute cost (LLM summarization) and online latency (MCTS iterations) for retrieval precision, avoiding exhaustive cross-encoding through MCTS pruning
- **Failure signatures**:
  1. Empty Cypher Returns: Path 1 fails consistently, overloading Path 2 with simple entity queries
  2. MCTS Stuck in Local Minima: Agent revisits same high-reward nodes without finding specific methods
  3. Description Drift: Retrieved code is semantically related but functionally incorrect
- **First 3 experiments**:
  1. Routing Accuracy: Validate Cypher generator routing with 50 queries labeled "Entity" or "NL"
  2. Ablate the Reward Model: Replace Cross-Encoder with Bi-Encoder in Simulation phase
  3. Latency Profiling: Measure Path 2 end-to-end latency to determine optimal iteration count

## Open Questions the Paper Calls Out

- Can incremental graph maintenance algorithms support live repository updates with minimal recomputation while preserving cross-file dependency accuracy?
- Can learned reward models or reinforcement learning approaches outperform fixed cross-encoder relevance scoring in MCTS node evaluation?
- How does RANGER perform on non-Python repositories given tree-sitter's multi-language support?
- Can a ReACT-style multi-stage agent combining Cypher queries with targeted MCTS reduce rollout depth and latency while maintaining retrieval quality?

## Limitations

- Static offline repository graphs limit applicability to dynamic or rapidly evolving codebases
- Node scoring currently depends on cross-encoder relevance estimates, which may not be optimal reward signals
- Current implementation supports only Python repositories despite tree-sitter's multi-language capabilities

## Confidence

- **High Confidence**: Retrieval performance improvements on established benchmarks (CodeSearchNet, RepoQA)
- **Medium Confidence**: Cross-file dependency retrieval on RepoBench and exact match performance on CrossCodeEval
- **Medium Confidence**: Core dual-stage routing and MCTS mechanisms, though practical implementation details require further validation

## Next Checks

1. **Routing Accuracy Validation**: Test LLM-generated Cypher routing mechanism with 50 queries to measure false positive/negative rates
2. **Bi-Encoder Approximation Quality**: Conduct ablation studies replacing cross-encoder rewards with bi-encoder scores
3. **Semantic Description Fidelity**: Perform qualitative analysis comparing LLM-generated node descriptions against ground truth code functionality