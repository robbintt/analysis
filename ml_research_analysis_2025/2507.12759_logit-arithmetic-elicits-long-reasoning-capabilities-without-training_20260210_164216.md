---
ver: rpa2
title: Logit Arithmetic Elicits Long Reasoning Capabilities Without Training
arxiv_id: '2507.12759'
source_url: https://arxiv.org/abs/2507.12759
tags:
- reasoning
- arxiv
- think
- logit
- guider
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of eliciting long chain-of-thought
  (CoT) reasoning in large language models without training the target model. It introduces
  ThinkLogit, a decoding-time approach that uses logit arithmetic to combine outputs
  from a target model and a smaller, reasoning-trained guide model, thereby transferring
  long reasoning capabilities without parameter updates.
---

# Logit Arithmetic Elicits Long Reasoning Capabilities Without Training

## Quick Facts
- **arXiv ID:** 2507.12759
- **Source URL:** https://arxiv.org/abs/2507.12759
- **Reference count:** 33
- **Primary result:** Logit arithmetic approach ThinkLogit improves frozen large model reasoning accuracy by 26-29% relative to baseline without training target model.

## Executive Summary
The paper introduces ThinkLogit, a decoding-time method that elicits long chain-of-thought reasoning from frozen large language models without training them. It achieves this by combining outputs from the target model with a smaller, reasoning-trained guide model using logit arithmetic - essentially adding a "reasoning vector" computed as the difference between the guide's reasoning and base outputs. ThinkLogit-DPO further refines the guide using preference optimization on pairs showing when the target is correct versus when the guide's reasoning is better. Experiments on four mathematical reasoning datasets show significant accuracy improvements while requiring minimal training and inference overhead compared to full fine-tuning approaches.

## Method Summary
ThinkLogit works by computing the difference in logits between a reasoning-trained small model and its base version, then adding this delta (scaled by guidance strength α) to the large frozen target model's logits during decoding. A warm-up phase defers guidance initially to prevent repetitive outputs. ThinkLogit-DPO trains the guide using Direct Preference Optimization on mixed preference pairs: when the target is correct versus when the guide is correct. The approach requires three forward passes per token (target, guide, base) but keeps the target frozen, achieving strong performance with minimal training cost.

## Key Results
- ThinkLogit improves pass@1 accuracy by 26% relative to frozen Qwen2.5-32B baseline
- ThinkLogit-DPO further improves pass@1 by 29% relative to baseline using same 1.5B guide model
- Performance approaches that of models fine-tuned with thousands of examples
- Results consistent across four mathematical reasoning datasets (AIME24/25, AMC23, MATH-hard)

## Why This Works (Mechanism)

### Mechanism 1: Residual Reasoning Transfer via Logit Arithmetic
A large frozen model can be steered to exhibit complex behaviors by injecting the behavioral "delta" of a smaller, fine-tuned model. The approach computes the difference in logits between a reasoning-trained small model and its base version, then adds this scaled vector to the target model's logits. This assumes the large model has latent reasoning capabilities that need a signal to rebalance its output distribution. The method fails if the target model lacks this latent capacity or if vocabulary embedding spaces are misaligned.

### Mechanism 2: Bidirectional Preference Alignment (ThinkLogit-DPO)
Guidance effectiveness is limited by distribution mismatch; aligning the guider using the target's successful outputs improves fusion. The guider is refined using DPO on mixed preference pairs: Type-1 (Target Correct > Guider Incorrect) teaches deference to target strengths, while Type-2 (Guider Correct > Target Incorrect) teaches error correction. This transforms the guider into an error-correcting controller. The approach fails if the target's error modes are inconsistent or stochastic.

### Mechanism 3: Stability via Warm-up Deferral
Applying strong logit steering immediately causes degenerate repetition; deferred guidance stabilizes generation. The guidance weight is set to 0 for an initial warm-up phase (e.g., 100 tokens), allowing the large model to establish coherent context before guidance begins. This assumes early tokens function as a fragile anchor. The method fails if reasoning tasks require immediate structural decisions in the first few tokens.

## Foundational Learning

- **Concept: Logit Arithmetic / Contrastive Decoding**
  - **Why needed:** This is the core interface for combining model behaviors
  - **Quick check:** If you subtract the logits of a "generic" model from a "reasoning" model, do you get a vector that encourages reasoning tokens or suppresses generic ones? (Answer: It amplifies tokens favored by the reasoner relative to the generic baseline)

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed:** Used to train the guider without reinforcement learning
  - **Quick check:** In ThinkLogit-DPO, we prefer Target-Correct over Guider-Incorrect. Does this teach the guider to *be* the target, or to *defer* to the target? (Answer: It teaches the guider to align its rewards such that it doesn't interfere when the target is right)

- **Concept: Chain-of-Thought (CoT) Length vs. Quality**
  - **Why needed:** The paper distinguishes eliciting "long" reasoning from just generating more tokens
  - **Quick check:** Why does "budget forcing" (forcing the model to write more by suppressing EOS) fail to improve performance compared to ThinkLogit? (Answer: Length without calibrated probability shifts just adds noise/hallucination, whereas logit arithmetic redirects the reasoning path)

## Architecture Onboarding

- **Component map:** Target (Qwen2.5-32B) <- Logit Fusion <- Guider (R1-Distill-Qwen-1.5B) and Base (Qwen2.5-Math-1.5B)
- **Critical path:** Token fed to all three models → compute raw logits → subtract base from guider to get reasoning delta → add scaled delta to target → sample from modified distribution
- **Design tradeoffs:** Keeps target frozen (low memory training footprint) but requires 3 forward passes (high FLOPs/latency); α=1 is standard but fragile; warm-up is critical for coherence but introduces lag
- **Failure signatures:** Repetitive generation loops (fix: increase warm-up or check α scaling); fluency collapse (fix: ensure S and S* are same architecture, reduce α); no improvement (fix: verify DPO convergence)
- **First 3 experiments:**
  1. **Sanity Check (Logit Null Test):** Run ThinkLogit with S* = S (delta = 0). Output must match base L.
  2. **Hyperparameter Sweep:** On small validation set, sweep α ∈ [0.5, 1.0, 1.5] with warm-up ∈ [0, 50, 100] to replicate ablation.
  3. **Behavioral Analysis:** Compare output length and "Wait" frequency of ThinkLogit vs. Budget Forcing to verify structured backtracking.

## Open Questions the Paper Calls Out
- Can ThinkLogit effectively transfer long reasoning capabilities to non-mathematical domains such as code generation or general logical reasoning?
- Is the logit arithmetic approach effective when the target model and the guider model belong to heterogeneous families with different tokenizers or architectures?
- Can an adaptive guidance strength (α) mitigate the "over-thinking" problem and optimize the trade-off between accuracy and token efficiency?

## Limitations
- The approach relies on strong distributional assumptions about target model's latent reasoning capacity that are not directly validated
- Performance improvements are measured against frozen baseline, not against fully fine-tuned peers
- Experimental scope limited to four mathematical reasoning datasets within narrow difficulty band
- Three forward passes per token multiply computational cost despite training efficiency gains

## Confidence
- **High Confidence:** ThinkLogit-DPO training procedure is clearly specified and produces measurable gains over ThinkLogit alone
- **Medium Confidence:** Logit arithmetic mechanism reliably transfers reasoning behaviors, though theoretical basis remains informal
- **Low Confidence:** Claim that target model "possesses latent reasoning capabilities" is inferred from benchmark success but not directly measured

## Next Checks
1. **Latent Capacity Probe:** Run controlled experiment where frozen target gets extended decoding budget without logit guidance to test if accuracy improves significantly
2. **Cross-Target Generalization:** Train ThinkLogit-DPO guider on one target model and apply to different frozen model to measure preference alignment transfer
3. **Domain Transfer Test:** Evaluate ThinkLogit on non-mathematical reasoning tasks (logical inference, code debugging, commonsense QA) to test general reasoning elicitor capability