---
ver: rpa2
title: Universal and Transferable Adversarial Attack on Large Language Models Using
  Exponentiated Gradient Descent
arxiv_id: '2508.14853'
source_url: https://arxiv.org/abs/2508.14853
tags:
- adversarial
- arxiv
- llms
- harmful
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel adversarial attack method for jailbreaking
  large language models (LLMs) using exponentiated gradient descent (EGD) on relaxed
  one-hot token encodings. Unlike existing approaches that require explicit projection
  steps to maintain valid probability distributions, the proposed method inherently
  enforces these constraints during optimization, achieving higher success rates and
  faster convergence.
---

# Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent

## Quick Facts
- arXiv ID: 2508.14853
- Source URL: https://arxiv.org/abs/2508.14853
- Authors: Sajib Biswas; Mao Nishino; Samuel Jacob Chacko; Xiuwen Liu
- Reference count: 11
- Key outcome: Novel adversarial attack method using exponentiated gradient descent achieves higher success rates and faster convergence than state-of-the-art baselines across five open-source LLMs and four adversarial behavior datasets

## Executive Summary
This paper introduces a universal adversarial attack method for jailbreaking large language models using exponentiated gradient descent (EGD) on relaxed one-hot token encodings. The approach inherently enforces probability simplex constraints during optimization, eliminating the need for explicit projection steps required by existing methods. Experiments demonstrate superior performance compared to three state-of-the-art baselines across multiple open-source models and adversarial behavior datasets, with the method achieving convergence within a few hundred epochs.

## Method Summary
The method optimizes adversarial suffixes by treating token embeddings as continuous relaxations of one-hot vectors. It uses exponentiated gradient descent with intrinsic probability simplex constraint enforcement through element-wise exponentiation and normalization. A Bregman projection using KL divergence corrects any constraint violations while maintaining geometric consistency. The approach combines entropic regularization and KL-divergence loss to promote sparsity, with exponential scheduling of the regularization coefficient. The method generates universal adversarial suffixes effective across multiple prompts and demonstrates strong transferability to different model architectures, including proprietary models like GPT-3.5.

## Key Results
- Achieves higher attack success rates than three state-of-the-art baselines across five open-source LLMs
- Converges within 200 epochs, demonstrating faster optimization than existing approaches
- Generates universal adversarial suffixes that transfer effectively to different model architectures, including GPT-3.5
- Shows superior performance on four adversarial behavior datasets from AdvBench

## Why This Works (Mechanism)

### Mechanism 1
Exponentiated gradient descent maintains probability simplex constraints intrinsically, eliminating projection error that degrades discrete token recovery. The EGD update rule applies element-wise exponentiation of negative gradients followed by normalization, ensuring each iteration stays on the probability simplex without external correction.

### Mechanism 2
Bregman projection using KL divergence provides closed-form row normalization that corrects constraint violations while staying geometrically consistent with the probability simplex. This normalization is equivalent to row-wise softmax-style normalization with known convergence properties from optimal transport literature.

### Mechanism 3
Combined entropic regularization and KL-divergence loss between relaxed and discretized encodings promotes sparsity, improving final token selection quality. Exponential scheduling of the regularization coefficient transitions from exploration to sparsity, with ablation showing mean max probabilities increase from ~0.12 without regularization to ~0.32 with regularization.

## Foundational Learning

- **Concept: Probability Simplex Optimization**
  - Why needed here: Understanding why EGD + Bregman projection keeps token distributions valid. Without this, the relaxation to continuous space would produce invalid encodings.
  - Quick check question: Given a 3-vocabulary token distribution [0.3, 0.5, 0.2], does it satisfy simplex constraints? What if you add gradient [-0.2, 0.1, 0.3] directly?

- **Concept: Cross-Entropy Loss for Target Sequence Generation**
  - Why needed here: The attack optimizes adversarial suffix to maximize p(y|[x', x̂]) where y is the harmful target. Understanding the loss F(X) = -log p(y|...) is essential.
  - Quick check question: If the target sequence is "Here is how to make a bomb" (8 tokens), what does the loss compute at each position?

- **Concept: Relaxation and Discretization Gap**
  - Why needed here: The method optimizes continuous relaxations but must return discrete tokens. The gap between lowest relaxed loss and discrete loss determines attack success.
  - Quick check question: Why might the token with highest probability in the relaxed encoding not produce the lowest loss when discretized?

## Architecture Onboarding

- **Component map**: Input prompt x', target y, initial suffix x̂ → EGD update → Bregman projection → Discretize via argmax → Evaluate discrete loss → Track best suffix
- **Critical path**: 1. Initialize 20 × |T| matrix with soft one-hot vectors 2. Forward pass through LLM with relaxed input 3. Compute cross-entropy loss on target tokens 4. Backprop to get ∇F(X̃) 5. Apply EGD + Adam update 6. Track best discrete suffix 7. Repeat for 200 epochs
- **Design tradeoffs**: Suffix length 20 balances attack success with detection risk; learning rate 0.1 provides stable convergence with Adam; regularization scheduling transitions from exploration to exploitation
- **Failure signatures**: Loss plateaus above 2.0 after 100 epochs indicates insufficient learning rate; max token probability below 0.15 suggests regularization not working; discrete loss much higher than relaxed loss indicates discretization gap too large
- **First 3 experiments**: 1. Reproduce single-prompt attack on Llama2-7B-chat using AdvBench behavior #1-5 2. Run with regularization disabled (τ=0) and compare max probability evolution 3. Optimize suffix on Vicuna, apply to Llama2 without modification

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed EGD method be effectively adapted for black-box scenarios where model weights and gradients are inaccessible? The authors state this viability in black-box scenarios is left as a direction for future research since the algorithm currently requires calculating gradients with respect to relaxed one-hot encodings.

### Open Question 2
Do alternative loss functions exist that correlate more strongly with jailbreak success than the current adversarial cross-entropy loss? The authors list designing alternative loss functions beyond cross-entropy as a specific direction for future research.

### Open Question 3
Is theoretical convergence guaranteed for models using non-smooth activations (like ReLU) or when the EGD method is coupled with the Adam optimizer? The provided convergence proof assumes Lipschitz continuous gradients and does not extend to the EGD with Adam variant.

### Open Question 4
Can the method be extended to effectively evade advanced defense mechanisms, specifically perplexity filtering? The authors list extending the method to evade advanced defense mechanisms such as perplexity filtering as a primary goal for future work.

## Limitations
- Limited testing of transferability to proprietary models with only 2-3 prompts per model
- No systematic analysis of the discretization gap between relaxed and discrete losses
- Computational costs not compared against baseline methods in terms of wall-clock time or gradient queries
- No evaluation against defensive mechanisms like input sanitization or gradient masking

## Confidence

- **High confidence**: Mathematical correctness of EGD + Bregman projection for maintaining simplex constraints
- **Medium confidence**: Empirical claims of superior ASR and transferability based on limited evaluation protocols
- **Low confidence**: Practical effectiveness for real-world adversarial scenarios without testing against defensive mechanisms

## Next Checks

1. **Discretization Gap Analysis**: For each successful attack, record both relaxed loss and discrete loss at convergence. Compute the percentage of cases where argmax token from relaxed optimization produces the lowest discrete loss versus cases where a different token would have been better.

2. **Transfer Robustness Testing**: Apply the universal suffix generated on Vicuna to GPT-3.5 using 20-30 diverse prompts spanning different domains. Measure ASR drop and analyze whether transfer success correlates with prompt similarity or target model architecture.

3. **Defensive Mechanism Evaluation**: Test the attack against a defensive fine-tuning approach where the victim model is trained on adversarial suffixes with contrastive loss to reduce harmful output probability. Measure ASR reduction and analyze whether the method can adapt to find new vulnerabilities.