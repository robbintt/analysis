---
ver: rpa2
title: In-Context Learning Without Copying
arxiv_id: '2511.05743'
source_url: https://arxiv.org/abs/2511.05743
tags:
- heads
- induction
- copying
- vanilla
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Inductive copying by induction heads has been hypothesized as a
  foundation for in-context learning, but its necessity for abstractive capabilities
  remains unclear. To test this, we introduce HAPAX, a training regime that masks
  loss contributions from tokens predictable by induction heads, thereby suppressing
  inductive copying.
---

# In-Context Learning Without Copying

## Quick Facts
- arXiv ID: 2511.05743
- Source URL: https://arxiv.org/abs/2511.05743
- Reference count: 40
- Key outcome: HAPAX models suppress inductive copying while maintaining or surpassing vanilla performance on 13 of 21 abstractive ICL tasks, indicating inductive copying is not essential for abstractive ICL

## Executive Summary
This paper challenges the hypothesis that inductive copying via induction heads is necessary for in-context learning (ICL) capabilities. The authors introduce HAPAX, a training regime that masks loss contributions from tokens predictable by induction heads by identifying and excluding repeatable n-grams from loss computation. Despite a 31.7% reduction in training tokens, HAPAX models maintain or surpass vanilla performance on 13 of 21 abstractive ICL tasks, with statistically significant gains on 9 tasks. The findings demonstrate that inductive copying is not causally required for abstractive ICL and suggest that reducing repetition incentives can even enhance certain capabilities like fluency.

## Method Summary
HAPAX implements a training-time modification that masks loss at positions where tokens are predictable via induction (i.e., where a matching n-gram appears earlier in context). The regime identifies token positions where n-grams (n>1) repeat within the context window and excludes these from loss computation. This suppresses the formation of strong induction heads while preserving abstractive ICL capabilities. The study trains 1B parameter GPT-NeoX models on The Pile dataset with standard training (40B tokens) and HAPAX variants (28B tokens for standard HAPAX, 19B for Thresholded-HAPAX with cosine similarity filtering). Models are evaluated on 28 extractive and 26 abstractive ICL tasks using 5-shot prompting, with statistical significance tests applied to performance differences.

## Key Results
- HAPAX models show a 66% drop in random repetition performance and 89% drop in accuracy relative to vanilla
- On abstractive tasks, HAPAX performs better on 9 of 17 tasks with statistically significant differences
- HAPAX improves fluency significantly (86% somewhat fluent vs 16% vanilla) by reducing repetition incentives
- HAPAX models have fewer and weaker induction heads but preserve ICL capabilities on abstractive tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking loss from repeatable n-grams removes the incentive for models to learn inductive copying while preserving abstractive ICL capabilities.
- Mechanism: The HAPAX regime identifies token positions where a matching n-gram (n>1) exists earlier in the context window and excludes these from loss computation. This means the model never receives gradient signals for positions solvable via induction, suppressing the formation of strong induction heads.
- Core assumption: The presence of repeated n-grams in training data is the primary driver for induction head formation (supported by prior work on phase transitions).
- Evidence anchors:
  - [abstract] "HAPAX models show fewer and weaker induction heads but still preserve ICL capabilities"
  - [section 4.1] "HAPAX causes a major drop in random repetition performance... 66% drop... 89% drop in accuracy relative to vanilla"
  - [corpus] Related work (Zucchet et al., 2025) confirms "repetition throughout data distribution" drives emergent behavior—HAPAX directly tests this.
- Break condition: If abstractive ICL degraded alongside inductive copying, this would suggest dependency. Instead, 13/21 tasks improved.

### Mechanism 2
- Claim: Abstractive ICL capabilities rely on mechanisms distinct from—and potentially parallel to—traditional induction heads.
- Mechanism: The paper demonstrates that models can develop ICL without strong induction heads, suggesting function vector heads, concept induction heads, or other circuits are causally more important for abstractive tasks (translation, reasoning).
- Core assumption: Abstractive tasks require generating novel outputs not in context, while extractive tasks require copying—these draw on different circuitry.
- Evidence anchors:
  - [abstract] "inductive copying is not essential for learning abstractive ICL mechanisms"
  - [section 4.2] "HAPAX performs better on 9 of the 17 tasks with statistically significant differences" on abstractive tasks
  - [corpus] Yin & Steinhardt (2025) and Todd et al. (2024) found function vector heads more causally important for complex ICL—this paper validates that separation.
- Break condition: Ablation of remaining weak induction heads in HAPAX model would test whether they're still partially contributing.

### Mechanism 3
- Claim: Prefix-matching attention patterns can emerge as a byproduct of previous token head formation, not just from explicit repetition incentives.
- Mechanism: Cross-checkpoint patching experiments show that when previous token information is present (from layer L4), randomly initialized heads in later layers begin exhibiting prefix-matching patterns—suggesting architectural bias toward this attention form.
- Core assumption: Previous token heads form for other purposes (detokenization, bigram statistics) and their presence creates conditions for induction-like patterns.
- Evidence anchors:
  - [section 5.2] "once we patch the outputs of layer L4... the model's highest induction score increases to 0.122"
  - [section 5.2] "heads were biased towards prefix matching from initialization"
  - [corpus] Feucht et al. (2025) on concept induction circuits shows parallel mechanisms—this paper provides architectural explanation.
- Break condition: If previous token heads were also suppressed, prefix-matching emergence should be blocked.

## Foundational Learning

- Concept: **Induction heads and induction circuits**
  - Why needed here: The entire paper tests whether these attention heads (which perform "match-and-copy" operations) are necessary for ICL.
  - Quick check question: Can you explain the three-step induction circuit (previous token head → prefix matching → copying)?

- Concept: **Extractive vs. Abstractive ICL tasks**
  - Why needed here: The paper's central claim depends on distinguishing tasks that require copying from context vs. generating novel outputs.
  - Quick check question: Given examples like "foil car purple → pen" vs. "Greece: Athens, China: Beijing, Egypt → ?", which is extractive and which is abstractive?

- Concept: **Loss masking / training signal manipulation**
  - Why needed here: HAPAX's intervention is fundamentally a training-time modification that removes gradient signals from specific token positions.
  - Quick check question: How does masking loss differ from masking tokens (making them invisible to the model)?

## Architecture Onboarding

- Component map:
  - Induction circuit: Previous token heads (early layers, ~L4) → Induction heads (mid-to-late layers, ~L8-L12) → Logit increase via output weights
  - Alternative ICL circuits: Function vector heads, concept induction heads (may exist in parallel, causally more important for abstractive tasks)
  - HAPAX modification: Loss computation layer, not model architecture—intervention is in training objective only

- Critical path:
  1. Identify repeatable n-grams in context window during training
  2. Mark continuation tokens for loss masking
  3. Compute cross-entropy loss only on unmasked positions
  4. Monitor induction head formation via prefix-matching scores on random sequences

- Design tradeoffs:
  - Masking strictness: Standard HAPAX masks 31.7% of tokens; Thresholded-HAPAX (cosine similarity > 0.3) masks 52.5% but harms some abstractive tasks
  - Token budget vs. capability: HAPAX trained on 28B tokens vs. vanilla's 40B but achieves comparable/better abstractive performance
  - Fluency vs. copying: HAPAX improves fluency (86% somewhat fluent vs. 16% vanilla) by reducing repetition incentive

- Failure signatures:
  - Repetition accuracy drops > 60% (expected, indicates suppression working)
  - Extractive task performance degrades significantly (expected, Section 4.1 shows 17/28 tasks decreased)
  - Abstractive performance drops across majority of tasks (unexpected, would indicate dependency)
  - Training dynamics show no phase transition in loss (expected—phase transition linked to induction head emergence)

- First 3 experiments:
  1. Validate HAPAX suppression: Train vanilla and HAPAX models on same data, evaluate random repetition accuracy on 1000 sequences to confirm inductive copying is reduced.
  2. Abstractive task comparison: Run 5-shot evaluation on 21+ abstractive tasks, using McNemar's test for statistical significance—check if HAPAX matches or exceeds vanilla.
  3. Mechanistic verification: Compute prefix-matching scores for all attention heads, then mean-ablate top-k heads on repetition task to measure probability difference—verify HAPAX has fewer/stronger heads and some act as "anti-induction" heads.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific circuit mechanisms emerge to support abstractive ICL when inductive copying is suppressed?
- Basis in paper: [explicit] The paper notes that "models trained with HAPAX develop fewer and weaker induction heads but still preserve ICL capabilities" and mentions prior work on function vector heads and concept induction heads as alternatives, but does not identify what circuits actually form in HAPAX models.
- Why unresolved: The mechanistic analysis focuses on verifying induction head suppression, not on characterizing replacement circuits for abstractive tasks.
- What evidence would resolve it: Attention head ablation studies on abstractive ICL tasks for HAPAX models, or activation patching to identify which heads become causally important for abstract reasoning.

### Open Question 2
- Question: How do these findings generalize to larger model scales (e.g., 7B+ parameters) and longer training durations?
- Basis in paper: [inferred] The study only trains 1B parameter models for 20,000 steps. The authors note this was "empirically sufficient to analyze the emergence of in-context learning dynamics" but do not test whether conclusions hold at scale.
- Why unresolved: Limited compute budget constrained model size and training duration.
- What evidence would resolve it: Replicating HAPAX training on larger model scales (7B, 13B, 70B) with full training runs, comparing abstractive ICL performance against vanilla baselines.

### Open Question 3
- Question: Why does Thresholded-HAPAX improve translation performance while harming other abstractive tasks?
- Basis in paper: [explicit] "We attribute the contrasting trends between the translation task and other abstractive tasks under Thresholded-HAPAX to our choice of threshold. A cosine similarity threshold of 0.3 masks many same-language tokens, while cross-language tokens typically fall below this threshold."
- Why unresolved: The threshold selection (τ=0.3) was heuristic, chosen because it "corresponds to the top-4 closest neighbors on average," without systematic optimization or analysis of why this specifically benefits translation.
- What evidence would resolve it: Ablation studies varying the similarity threshold systematically across tasks, or analysis of token-level masking patterns specific to translation vs. other abstractive tasks.

## Limitations

- Results are demonstrated only on 1B parameter models, limiting generalization to larger scales
- The exact alternative circuit mechanisms for abstractive ICL are not fully characterized
- Task domain coverage is limited to specific benchmarks, may not generalize to all abstractive capabilities

## Confidence

- High Confidence: The core experimental result that HAPAX models achieve comparable or superior performance on abstractive ICL tasks while showing reduced inductive copying
- Medium Confidence: The claim that "inductive copying is not essential for abstractive ICL" based on a single model scale and training regime
- Low Confidence: The exact characterization of alternative ICL mechanisms and why specific masking thresholds benefit certain task types

## Next Checks

- Scale the HAPAX training regime to a 3B or 8B parameter model using the same dataset and masking strategy, comparing induction head ratios and abstractive vs. extractive performance
- Implement targeted ablation on HAPAX model to identify which remaining weak induction heads contribute to abstractive ICL performance
- Design a synthetic dataset where repetition is extremely rare (<5% of tokens) and train both vanilla and HAPAX models to test whether HAPAX's advantage comes from efficient training signal use