---
ver: rpa2
title: Practical Design and Benchmarking of Generative AI Applications for Surgical
  Billing and Coding
arxiv_id: '2501.05479'
source_url: https://arxiv.org/abs/2501.05479
tags:
- codes
- billing
- medical
- phi-3
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study benchmarks generative AI for surgical billing and coding
  using four model configurations: base Phi-3 Mini, Phi-3 Mini with RAG, Phi-3 Mini
  fine-tuned, and Phi-3 Medium fine-tuned. Performance was compared to GPT-4o using
  exact code matching, validity, and format consistency metrics.'
---

# Practical Design and Benchmarking of Generative AI Applications for Surgical Billing and Coding

## Quick Facts
- arXiv ID: 2501.05479
- Source URL: https://arxiv.org/abs/2501.05479
- Reference count: 40
- Fine-tuned Phi-3 Medium achieves 72% precision/recall for ICD-10 codes, 79% precision/77% recall for CPT codes, with only 1% ICD-10 and 0.6% CPT hallucinations

## Executive Summary
This study benchmarks generative AI for surgical billing and coding using four model configurations: base Phi-3 Mini, Phi-3 Mini with RAG, Phi-3 Mini fine-tuned, and Phi-3 Medium fine-tuned. Performance was compared to GPT-4o using exact code matching, validity, and format consistency metrics. The fine-tuned Phi-3 Medium model achieved the highest accuracy: 72% precision and 72% recall for ICD-10 codes, 79% precision and 77% recall for CPT codes, and 64% precision and 63% recall for modifiers. It also produced the fewest hallucinations (1% ICD-10, 0.6% CPT). Results demonstrate that domain-specific fine-tuning on modest infrastructure yields performance on par with or exceeding large state-of-the-art models, offering a practical, resource-efficient solution for medical billing automation.

## Method Summary
The study fine-tuned Phi-3 Mini (3.8B) and Phi-3 Medium (14B) using QLoRA on 192,585 surgical encounters (Jan 2017–Dec 2022). Models were evaluated against exact code matching, validity against reference taxonomies, and format consistency metrics. Four configurations were tested: base Phi-3 Mini, RAG-enhanced Phi-3 Mini using FAISS vector search, fine-tuned Phi-3 Mini, and fine-tuned Phi-3 Medium. GPT-4o served as the comparison baseline. Inference used greedy decoding with 4-bit quantization and repetition penalty. Training utilized 4×A5000 24GB GPUs with DeepSpeed ZeRO-3 for sharding.

## Key Results
- Fine-tuned Phi-3 Medium achieved highest accuracy: 72% precision/72% recall for ICD-10 codes, 79% precision/77% recall for CPT codes
- Lowest hallucination rates: 1% ICD-10 and 0.6% CPT codes for fine-tuned Medium model
- Fine-tuned models significantly outperformed base and RAG configurations across all metrics (ICD-10 F1: 0.73 vs 0.47 base, 0.59 RAG)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific parameter updates (fine-tuning) are more effective for high-stakes alphanumeric retrieval than in-context learning or increased model size alone.
- **Mechanism:** QLoRA (Quantized Low Rank Adapters) re-parameterizes a small subset of model weights to internalize the specific mapping between unstructured operative notes and structured billing codes. This embeds the "logic" of the specific coding guidelines into the model's weights, reducing reliance on the probabilistic recall of general pre-training.
- **Core assumption:** The relationship between the surgical text and the billing codes follows a learnable pattern that persists across the 2017-2022 dataset.
- **Evidence anchors:**
  - [Abstract] "A small model that is fine-tuned on domain-specific data... performs as well as the larger contemporary consumer models."
  - [Results] Table 2 shows the fine-tuned Phi-3 Mini outperforming the RAG configuration (CPT F1 0.69 vs 0.59) and the fine-tuned Medium model matching/exceeding GPT-4o.
  - [Corpus] "Toward Reliable Clinical Coding..." confirms that off-the-shelf LLMs struggle with exact matches and require lightweight adaptation for reliability.
- **Break condition:** If the coding guidelines change drastically (e.g., ICD-11 transition) or the operative note vocabulary shifts significantly without re-training, the internalized weights would likely fail to generalize.

### Mechanism 2
- **Claim:** Validity verification against a static reference taxonomy is the primary mechanism suppressing "hallucinations" (fabricated codes).
- **Mechanism:** The system does not rely solely on the model's generation accuracy. Instead, it explicitly compares generated strings against a "reference list of ICD-10s and CPTs." This acts as a hard filter, ensuring that even if the model produces a plausible-looking but non-existent code, it is flagged as invalid.
- **Core assumption:** The "truth" is defined strictly by the existence of the code in the official year-specific taxonomy, not by semantic correctness alone.
- **Evidence anchors:**
  - [Methods] "To quantify the extent of hallucinations... we compared each generated code to a reference list of ICD-10s and CPTs from the appropriate cohort year."
  - [Results] The fine-tuned Medium model achieved 99% validity for ICD-10 and 99.4% for CPT, suggesting the fine-tuning aligned output distribution tightly with the valid taxonomy.
  - [Corpus] "Toward Reliable Clinical Coding..." highlights that exact match metrics often miss hierarchically close errors, implying a taxonomy check is a strict but necessary layer.
- **Break condition:** If the reference taxonomy is outdated or if the model generates valid codes that are semantically wrong for the procedure (high validity, low precision), this mechanism fails to ensure clinical utility.

### Mechanism 3
- **Claim:** Retrieval-Augmented Generation (RAG) improves validity over base models by grounding generation in observed examples, but lacks the precision of weight-based learning.
- **Mechanism:** The RAG system uses vector similarity (FAISS) to find the top-2 most similar historical operative notes and injects their codes as context. This restricts the model's output probability space to codes already present in the retrieved examples ("grounding").
- **Core assumption:** Similar operative notes (by embedding distance) utilize similar billing codes.
- **Evidence anchors:**
  - [Methods] "This was done to match similar procedures rather than match OP Notes to exact ICD-10 and CPT descriptors."
  - [Results] Table 2 shows RAG reduced hallucinations significantly compared to the base model (ICD-10 Fabricated: 23.1% vs 49.7%), but was still far higher than the fine-tuned model (1%).
  - [Corpus] "MedDCR" suggests agentic workflows and retrieval are key, but "Taming the Real-world Complexities..." implies simple retrieval might miss complex coding nuances.
- **Break condition:** If an operative note is unique or the embedding space fails to capture the procedural similarity relevant to billing (e.g., similar text but different surgical approach requiring different codes), RAG will retrieve misleading context.

## Foundational Learning

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - **Why needed here:** The study runs on 4 consumer-grade A5000 GPUs (24GB VRAM each). Full fine-tuning of a 14B parameter model is impossible in this memory envelope. QLoRA allows freezing the main model weights and only training tiny adapter matrices, making the experiment feasible.
  - **Quick check question:** If you increase the LoRA `rank` from 64 to 128, are you increasing the number of trainable parameters or the frozen parameters?

- **Concept: Exact Match vs. Semantic Similarity in Evaluation**
  - **Why needed here:** In medical billing, "being close" (semantic similarity) has zero financial value. The system must output the exact alphanumeric string (e.g., "K80.20" vs "K80.2"). The paper strictly evaluates based on exact string matching and validity against a codebook.
  - **Quick check question:** Why does the paper reject F1 scores based on "code family" matches as insufficient for this specific workflow?

- **Concept: Hallucination in Structured Output**
  - **Why needed here:** LLMs are probabilistic; billing codes are deterministic. The paper identifies "fabrication" as a critical failure mode (e.g., inventing a CPT code that doesn't exist). Understanding this distinction is key to interpreting the validity metrics.
  - **Quick check question:** Does a 99% "Valid %" score guarantee the code is correct for the surgery, or just that the code exists in the rulebook?

## Architecture Onboarding

- **Component map:**
  - Input: Operative Report (Text)
  - Retrieval (Optional): all-MiniLM-L6-v2 (Embedding) -> FAISS (Vector Search) -> Context Construction
  - Model: Phi-3 Mini/Medium (4-bit quantized) + LoRA Adapters (bfloat16)
  - Inference Engine: HuggingFace Transformers (Greedy Decoding, Repetition Penalty 1.1)
  - Post-Processing: Regex Extraction -> Taxonomy Validator (Reference List) -> Output Codes
  - Infrastructure: 4× NVIDIA A5000 (24GB) utilizing DeepSpeed ZeRO-3 for sharding

- **Critical path:**
  1. Data Formatting: Converting raw operative notes and billing CSVs into the specific instruct prompt template (using special tokens like `<|placeholder1|>`)
  2. Adapter Training: Running QLoRA training (cost: 7.3 hours for Mini, 30.5 hours for Medium)
  3. Merging: Merging trained adapters back into base model weights for faster inference latency
  4. Validation Loop: The Regex extraction step is brittle; if the model deviates from the output format even slightly, the extraction fails, resulting in a "wrong" score

- **Design tradeoffs:**
  - Phi-3 Mini vs. Medium: Mini trains 4× faster and is cheaper to serve, but Medium offers a ~8% boost in CPT Recall (Table 2)
  - RAG vs. Fine-tuning: RAG requires no training (just indexing) and is easily updateable, but Fine-tuning offers significantly higher precision (72% vs 48% for ICD-10) and lower hallucination
  - Greedy vs. Sampling: The paper chooses Greedy search (deterministic) to minimize variance, trading off potential creativity/diversity for reliability in a workflow tool

- **Failure signatures:**
  - High Hallucination (>20%): Indicates the model is likely a "Base" model or RAG is failing to retrieve relevant context. The model is guessing based on language patterns rather than coding rules
  - Low Full Match % but High Valid %: The model generates real codes, but the wrong ones for the specific case (High precision/validity logic, but poor mapping)
  - Format Errors: If ROUGE-L scores are high but Code Extraction fails, the model is likely producing natural language descriptions instead of the strict code format requested

- **First 3 experiments:**
  1. Overfit Test: Train the Phi-3 Mini on a single surgical specialty (e.g., Ophthalmology) to verify the pipeline works and the model can overfit a small dataset perfectly (sanity check)
  2. Prompt Template Ablation: Test the model with and without the specific "special tokens" (e.g., `<|placeholder1|>`) to measure the degradation in output formatting reliability
  3. Retrieval Sensitivity: Run the RAG configuration on a random sample of 100 notes and manually inspect if the "top 2 most similar" notes are actually clinically similar or just textually similar (assessing the risk of misleading context)

## Open Questions the Paper Calls Out

- **Open Question 1:** Would incorporating History & Physical (H&P) notes alongside operative reports significantly improve ICD-10 diagnostic code generation accuracy?
  - **Basis in paper:** [explicit] Authors state "diagnosis codes often come from information contained in the History and Physical (H&P) text" and explicitly plan to "incorporate the H&P in future iterations."
  - **Why unresolved:** Current study only used operative reports as input, yet medical coders typically access the entire patient record including H&P notes for diagnosis coding.
  - **What evidence would resolve it:** A comparative study where fine-tuned models are given both operative reports and H&P notes, with measured improvement in ICD-10 recall/precision.

- **Open Question 2:** Can these LLM configurations achieve performance levels sufficient to reduce medical coder FTE requirements or improve time-to-submitted-claims in operational deployment?
  - **Basis in paper:** [explicit] Authors state "none of the LLM formulations tested... performed at a level of competence that would lead to reduced FTE or training requirements... nor would the performance result in improvement in a time-to-submitted-for-claims metric."
  - **Why unresolved:** Current performance (72% ICD-10, 77-79% CPT) appears insufficient for autonomous operation; the threshold for meaningful workflow impact remains undefined.
  - **What evidence would resolve it:** Prospective operational deployment measuring coder throughput, claim submission times, and staffing requirements before and after LLM integration.

- **Open Question 3:** How well do institutionally fine-tuned models generalize to other healthcare systems with different coding practices, specialties, or documentation styles?
  - **Basis in paper:** [inferred] Models were fine-tuned on a single institution's data (one academic tertiary center, two community hospitals, two ambulatory centers); external validity was not assessed.
  - **Why unresolved:** Coding practices, documentation conventions, and case mix vary across institutions; local fine-tuning may overfit to institutional patterns.
  - **What evidence would resolve it:** External validation study applying the fine-tuned models to operative reports from geographically and structurally distinct healthcare systems without additional fine-tuning.

## Limitations

- High validity percentages (99% ICD-10, 99.4% CPT) only confirm code existence in reference lists, not semantic correctness for specific procedures
- RAG mechanism's reliance on vector similarity may retrieve contextually misleading examples when operative notes are textually similar but procedurally distinct
- Study does not report on modifier recall relative to full CPT/ICD-10 performance, leaving uncertainty about completeness for actual billing workflows
- QLoRA fine-tuning hyperparameters (learning rate, batch size, epochs) are unspecified, making it difficult to assess whether reported performance represents optimal configuration

## Confidence

- **High Confidence:** Domain-specific fine-tuning significantly outperforms base models and RAG configurations for exact code matching. Table 2 shows consistent superiority of fine-tuned models across all metrics.
- **Medium Confidence:** Fine-tuned Phi-3 Medium achieves performance "on par with or exceeding large state-of-the-art models." Comparison is limited to specific metrics and may not capture broader capabilities.
- **Low Confidence:** Practical applicability claim that this approach offers a "resource-efficient solution for medical billing automation" without addressing regulatory compliance, audit requirements, or ongoing adaptation to coding guideline changes.

## Next Checks

1. **Semantic Correctness Audit:** Manually review 50 random test cases where the system produced valid-but-incorrect codes to quantify the clinical significance of these errors.
2. **Generalization to Unseen Scenarios:** Test the fine-tuned models on operative notes from surgical specialties not well-represented in the 2017-2022 training data to assess generalization beyond the training distribution.
3. **Regulatory Compliance Assessment:** Evaluate whether the system's outputs meet CMS documentation requirements and audit standards by comparing generated codes against actual paid claims from the same period.