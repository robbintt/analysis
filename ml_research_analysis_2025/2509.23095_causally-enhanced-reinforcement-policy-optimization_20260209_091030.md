---
ver: rpa2
title: Causally-Enhanced Reinforcement Policy Optimization
arxiv_id: '2509.23095'
source_url: https://arxiv.org/abs/2509.23095
tags:
- reward
- arxiv
- causal
- optimization
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Causally-Enhanced Policy Optimization (CE-PO),\
  \ a method that improves reinforcement learning in large language models by incorporating\
  \ causal coherence into the training objective. The key idea is to add a differentiable\
  \ causal coherence signal\u2014computed using Jacobian-based influence estimates\u2014\
  alongside standard accuracy rewards, and to fuse them using a Minkowski combiner\
  \ that allows tunable trade-offs."
---

# Causally-Enhanced Reinforcement Policy Optimization

## Quick Facts
- arXiv ID: 2509.23095
- Source URL: https://arxiv.org/abs/2509.23095
- Reference count: 40
- This paper introduces CE-PO, which improves RL fine-tuning of LLMs by adding causal coherence signals to prevent reward hacking, achieving 5.49% average accuracy gains.

## Executive Summary
This paper addresses reward hacking and unfaithful reasoning in reinforcement learning for large language models by introducing Causally-Enhanced Policy Optimization (CE-PO). The method augments standard accuracy rewards with Jacobian-based causal coherence signals computed along the reasoning pathway from prompt to rationale to answer. These signals are hardened against spurious sensitivities through counterfactual permutation and subspace projection, then fused with accuracy rewards using a tunable Minkowski combiner. Experiments across five LLMs and four reasoning datasets show significant improvements in both accuracy (5.49% average gain) and robustness to causal perturbations, with the method remaining compatible with standard PPO/GRPO frameworks.

## Method Summary
CE-PO modifies RL fine-tuning objectives by fusing task accuracy rewards with Jacobian-based causal coherence signals along the Z→X→Y reasoning pathway. The method computes sensitivities using vector-Jacobian products, applies counterfactual hardening via token permutations to suppress spurious cues, normalizes signals through spectral energy, and combines them using a Minkowski power-mean formula. This creates a single tunable reward signal that improves both accuracy and reasoning coherence while resisting reward hacking behaviors like generation length inflation.

## Key Results
- CE-PO achieves 5.49% average accuracy improvement (up to 9.58%) over standard GRPO/BERTScore baselines
- Prevents reward hacking: CE-GRPO maintains generation length below token cap while non-CF-GRPO approaches maximum
- Improves robustness to causal perturbations: Models trained with CE-PO show higher Jacobian scores (S(Z,X), S(X,Y), S(Z,Y)) for correct vs. incorrect responses
- Maintains compatibility: Drop-in replacement for PPO/GRPO with minimal hyperparameter tuning required

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual hardening suppresses reward hacking by removing nuisance sensitivity directions from Jacobian-based rewards. The method creates source-side counterfactuals by permuting tokens (preserving surface statistics like length/frequency while scrambling semantics), computes counterfactual Jacobians, extracts their principal subspace via top-k singular vectors, and projects this nuisance subspace out of the base Jacobian signals. This orthogonalization decouples rewards from superficial cues.

### Mechanism 2
The Minkowski (power-mean) combiner enables tunable interpolation between task accuracy and causal coherence, preventing collapse to either objective. The unified reward Rp = (w₀r_base^p + w₁S(Z,X)^p + w₂S(X,Y)^p + w₃S(Z,Y)^p)^(1/p) blends four signals. As p→∞, the reward collapses to the maximum signal (typically accuracy); as p→0, it approaches the geometric mean (emphasizing smaller/coherence signals).

### Mechanism 3
Jacobian-based block sensitivities serve as differentiable proxies for direct causal influence along Z→X→Y pathways. For each link (A,B), compute J_AB = ∂s_B/∂E(A)—the gradient of downstream log-likelihood with respect to upstream embeddings. For the direct Z→Y effect, project out the mediated path via X. Normalize via spectral energy share (σ₁²/‖J‖²_F) to obtain scalar coherence scores S(A,B) ∈ [0,1].

## Foundational Learning

- **Concept: Policy Gradient Methods (PPO/GRPO)**
  - Why needed: CE-PO is a drop-in modification to PPO and GRPO objectives; understanding clipping, KL penalties, and advantage estimation is prerequisite.
  - Quick check: Can you explain why PPO uses importance sampling ratios and clipped objectives rather than direct policy gradients?

- **Concept: Jacobian/Gradient Interpretation in Neural Networks**
  - Why needed: The method relies on interpreting ∂output/∂input as influence; requires understanding automatic differentiation, embedding gradients, and their limitations.
  - Quick check: Why might raw input gradients fail sanity checks for attribution, and how does counterfactual projection address this?

- **Concept: Causal Inference Fundamentals (Direct vs. Total Effect)**
  - Why needed: The method explicitly models Z→X→Y mediation and isolates direct effects; understanding Pearl's do-calculus basics clarifies the motivation.
  - Quick check: In a chain A→B→C, what is the difference between total effect of A on C versus direct effect?

## Architecture Onboarding

- **Component map:**
  1. **Generation module:** Produces (Z, X, Y) rollouts with special token separating rationale from answer
  2. **Jacobian computation:** vJP-based sensitivity calculation for Z→X, X→Y, Z→Y (2 backward passes per link)
  3. **Counterfactual hardening:** K source-side permutations (default K=4), subspace projection via SVD top-k
  4. **Normalization:** Spectral energy share φ(J) = σ₁(J)²/‖J‖²_F
  5. **Minkowski fusion:** Power-mean combiner with configurable p and weights
  6. **Policy optimizer:** Standard PPO/GRPO with modified reward signal

- **Critical path:** Forward pass → Generate (Z,X,Y) → Compute 3 base Jacobians → Generate K counterfactuals per link → Project nuisance subspaces → Normalize to S(A,B) → Fuse with r_base via Rp → Backprop through policy only (not through Rp computation)

- **Design tradeoffs:**
  - **K (counterfactual samples):** Higher K improves nuisance suppression but O(K) compute overhead; paper uses K=4
  - **p (Minkowski exponent):** p→∞ emphasizes accuracy; p→0 emphasizes coherence; paper recommends p=0.2
  - **Weight allocation:** (w₀,w₁,w₂,w₃) = (0.7, 0.1, 0.1, 0.1) prioritizes accuracy; ablation shows removing any coherence term risks reward hacking
  - **r_base choice:** BERTScore (continuous) preferred over binary LLM-judge to avoid oscillatory updates

- **Failure signatures:**
  - **Length inflation:** Generation hitting token limits indicates counterfactual hardening failed; check that projection is actually removing nuisance directions
  - **Accuracy collapse:** If task accuracy drops, p may be too low or coherence weights too high; verify S(A,B) distributions are reasonable
  - **Unstable training:** Jacobian signals can be noisy; ensure normalization is stable and per-batch reward standardization is applied
  - **Missing coherence gains:** If S(Z,X), S(X,Y), S(Z,Y) don't improve, check that gradients are flowing through Jacobian computation

- **First 3 experiments:**
  1. **Baseline comparison:** Train GRPO-BERTScore vs. CE-GRPO on single dataset (e.g., BBEHCausal), monitor both accuracy and generation length to verify anti-hacking effect
  2. **Ablation on p:** Sweep p ∈ {-2, 0, 0.2, 1, 10} with fixed weights; confirm p=0.2 achieves best accuracy-coherence balance on validation set
  3. **Component removal:** Train with each S(A,B) term removed individually (per Table 2, rows 5-7); verify that removing any term induces length inflation or accuracy drop, confirming full pathway constraint is necessary

## Open Questions the Paper Calls Out

### Open Question 1
Can the trade-off between accuracy and causal coherence be automated dynamically during training rather than relying on manually tuned Minkowski combiner parameters? The Conclusion states future work could "automate the accuracy–causality balance" and enable "adaptive schemes that tune this trade-off on the fly."

### Open Question 2
Do the benefits of the counterfactual-hardened Jacobian signal persist when scaling to models significantly larger than 8B parameters? Page 7 notes that "compute constraints preclude experiments on 70B-scale models," and the Conclusion explicitly lists "scale interventions" as a direction for future work.

### Open Question 3
Would replacing first-order Jacobian sensitivities with higher-order Hessian-based estimates significantly improve the fidelity of the causal coherence signal? The Conclusion suggests CE-PO opens avenues for "incorporating richer causal signals (e.g., Hessian-based beyond simple Jacobian)."

### Open Question 4
Can the CE-PO framework be adapted for base models that do not strictly separate the rationale (X) from the answer (Y)? Page 7 (Footnote 2) states that experiments were restricted to Instruct/Thinking models because "base models struggle to separate [X and Y]," suggesting the method assumes a clear structural decomposition.

## Limitations
- The counterfactual hardening mechanism depends critically on the assumption that semantic and spurious sensitivity subspaces are approximately orthogonal, which is asserted but not rigorously validated
- The method requires O(K) additional Jacobian computations (K=4 in experiments), creating non-trivial computational overhead that scales linearly with dataset size and sequence length
- Benefits on models significantly larger than 8B parameters remain untested due to computational constraints

## Confidence
- **High confidence:** The core observation that combining accuracy rewards with causal coherence signals improves reasoning robustness is well-supported by controlled ablations (Table 2) and across five different models and four datasets
- **Medium confidence:** The Minkowski combiner provides tunable trade-offs between accuracy and coherence; while the ablation study supports this, the optimal p=0.2 appears somewhat arbitrary and may be dataset-dependent
- **Low confidence:** The counterfactual hardening mechanism's effectiveness in suppressing reward hacking relies heavily on the orthogonality assumption, which is asserted but not empirically validated through subspace analysis or perturbation studies

## Next Checks
1. **Subspace orthogonality validation:** Compute and visualize the principal angles between semantic and nuisance subspaces across multiple datasets to empirically verify the orthogonality assumption underlying counterfactual hardening
2. **Permutation sensitivity analysis:** Systematically vary the permutation strategy (random vs. structured) and K value to determine the robustness of nuisance suppression and identify the minimum effective K
3. **Higher-order influence comparison:** Implement a simplified second-order influence estimator (e.g., Hessian trace or influence functions on validation set) to compare against Jacobian-based proxies and assess whether first-order approximations are truly sufficient for causal coherence signals