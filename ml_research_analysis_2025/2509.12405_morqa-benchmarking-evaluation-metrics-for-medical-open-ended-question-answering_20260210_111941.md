---
ver: rpa2
title: 'MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering'
arxiv_id: '2509.12405'
source_url: https://arxiv.org/abs/2509.12405
tags:
- evaluation
- medical
- metrics
- table
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks evaluation metrics for medical open-ended
  question answering (QA), introducing MORQA, a multilingual dataset with 16,041 expert
  ratings. It evaluates traditional metrics (BLEU, ROUGE, BERTScore, BLEURT) and LLM-based
  evaluators (GPT-4o, Gemini, DeepSeek, Prometheus) across English and Chinese datasets.
---

# MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering

## Quick Facts
- arXiv ID: 2509.12405
- Source URL: https://arxiv.org/abs/2509.12405
- Reference count: 40
- Introduces MORQA, a multilingual dataset with 16,041 expert ratings for benchmarking evaluation metrics in medical open-ended QA

## Executive Summary
This paper introduces MORQA, a comprehensive dataset for benchmarking evaluation metrics in medical open-ended question answering. The authors evaluate traditional metrics (BLEU, ROUGE, BERTScore, BLEURT) and LLM-based evaluators (GPT-4o, Gemini, DeepSeek, Prometheus) across English and Chinese datasets. Their findings show that LLM-based approaches significantly outperform traditional metrics in correlating with human judgments, particularly for relevance and factual accuracy. The dataset includes 16,041 expert ratings across six medical QA collections, providing a valuable resource for advancing evaluation methodology in this domain.

## Method Summary
The study compares traditional evaluation metrics (BLEU, ROUGE, BERTScore, BLEURT) against LLM-based evaluators (GPT-4o, Gemini-1.5-pro-002, DeepSeekV3, Prometheus-v2-7b) using MORQA, a multilingual medical QA dataset with 16,041 expert ratings. Metrics are evaluated based on their correlation with human judgments across English and Chinese subsets. The evaluation considers multiple reference answers per query (2-4+) and examines both reference-based and reference-free evaluation approaches. Correlation analysis uses Kendall's τ, Pearson r, Spearman ρ, and pairwise ranking accuracy against human ratings on six dimensions: factual accuracy, relevance, completeness, writing style, disagreement, and overall quality.

## Key Results
- LLM-based evaluators achieve average correlations up to 0.38-0.42 with human judgments, significantly outperforming traditional metrics
- Traditional metrics show stronger performance on Chinese subsets with shorter responses (BERTScore achieving avg-corr of 0.34-0.39)
- Increasing reference answers from 1 to more than 1 improves LLM evaluator correlation by 5-10 percentage points
- GPT-4o and Gemini achieve avg-corr of 0.38-0.43 on English datasets versus BLEU/ROUGE at 0.06-0.26

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evaluators achieve higher correlation with human judgments than traditional n-gram and embedding metrics for medical open-ended QA.
- Mechanism: LLMs encode domain knowledge and semantic reasoning capabilities that enable assessment of factual accuracy and clinical relevance beyond surface-level lexical overlap. Traditional metrics like BLEU/ROUGE measure token-level similarity, which fails when valid responses diverge phrasing while maintaining correctness.
- Core assumption: Human expert judgments on medical QA quality are grounded in semantic correctness and clinical appropriateness, not textual similarity to reference answers.
- Evidence anchors:
  - [abstract] "LLM-based approaches significantly outperform traditional metrics in correlating with human judgments (average correlations up to 0.38-0.42), especially for relevance and factual accuracy."
  - [section] Table 4 shows GPT-4o and Gemini achieving avg-corr of 0.38-0.43 on English datasets versus BLEU/ROUGE at 0.06-0.26.
  - [corpus] AURA Score paper (arXiv:2510.04934) similarly finds surface similarity metrics fail for open-ended audio QA evaluation.
- Break condition: If LLM evaluators exhibit systematic bias toward verbose or formally-structured responses independent of accuracy, correlations may reflect stylistic preferences rather than quality assessment.

### Mechanism 2
- Claim: Traditional embedding-based metrics (BERTScore, BLEURT) remain competitive for shorter, more constrained response types.
- Mechanism: When responses are brief and lexically similar to references—as in Chinese medical dialogue subsets—embedding similarity captures sufficient semantic overlap. Traditional metrics degrade as response length and acceptable variation increase.
- Core assumption: Corpus-specific characteristics (response length, domain constraint) determine optimal metric selection rather than a universal best metric.
- Evidence anchors:
  - [abstract] "Traditional metrics show stronger performance on Chinese subsets with shorter responses."
  - [section] Table 5 shows BERTScore achieving avg-corr of 0.34-0.39 on Chinese DermaVQA, comparable to some LLM evaluators.
  - [corpus] Weak corpus evidence—no direct comparisons of metric performance across response lengths in related work.
- Break condition: If "shorter responses" correlate with simpler queries requiring less clinical reasoning, traditional metrics may appear competitive while failing on complex cases.

### Mechanism 3
- Claim: Multiple gold-standard references improve LLM evaluator correlation with human judgments by 5-10 percentage points.
- Mechanism: Providing multiple valid reference answers reduces evaluator sensitivity to any single reference's idiosyncrasies. LLMs can better assess whether candidate responses align with the space of acceptable answers rather than matching one specific formulation.
- Core assumption: Medical open-ended questions legitimately admit multiple valid answers that differ in content and phrasing.
- Evidence anchors:
  - [section] Table 7 ablation shows 3 refs > 2 refs > 1 ref for DermaVQA and WoundcareVQA across GPT-4o and Gemini.
  - [section] Section 7 "Gold-Holdout" experiment shows gold references achieving only ~0.66 agreement with peer references using automatic metrics.
  - [corpus] No direct corpus evidence on multi-reference evaluation effects.
- Break condition: If additional references introduce noise (variable quality, contradictory information), performance gains may reverse.

## Foundational Learning

- Concept: **Reference-based vs. reference-free evaluation**
  - Why needed here: The paper evaluates both paradigms; understanding when each applies is essential for metric selection. Reference-based metrics require gold answers; reference-free (LLM-as-judge) can evaluate solely from query context.
  - Quick check question: If you have no gold-standard answers for a new medical QA dataset, which evaluation approach remains viable?

- Concept: **Correlation metrics for evaluation benchmarking (Spearman, Pearson, Kendall's tau)**
  - Why needed here: The paper reports all three to assess metric-human alignment. Understanding their differences (rank vs. linear correlation, sensitivity to outliers) informs interpretation.
  - Quick check question: Why might Kendall's tau show different patterns than Pearson correlation when comparing evaluator scores to human ratings?

- Concept: **LLM-as-a-judge prompting strategies**
  - Why needed here: Tables 10-15 show different prompt formats for Prometheus vs. general LLMs. Prompt design directly affects evaluation reliability.
  - Quick check question: What prompt elements appear necessary for consistent medical QA evaluation based on the paper's examples?

## Architecture Onboarding

- Component map:
  Dataset layer -> Response generation layer -> Human evaluation layer -> Automatic evaluation layer -> Correlation analysis layer
- Critical path:
  1. Load dataset with query, candidate responses, gold references, human ratings
  2. Run each automatic metric on (candidate, reference(s)) pairs
  3. Compute correlations between metric scores and human ratings
  4. Perform ablation (vary number of references, reference-free setting)
- Design tradeoffs:
  - Single vs. multiple annotators: English clinical subsets had 2 MDs; Chinese had 1 annotator. Trade-off between cost and inter-rater reliability.
  - Reference count: More references improve LLM evaluator performance but increase annotation cost.
  - Metric granularity: Overall score vs. dimension-specific (factual accuracy, relevance, etc.). Dimensions reveal where metrics fail but complicate comparison.
- Failure signatures:
  - LLM evaluator assigns high scores to fluent but medically incorrect responses (surface plausibility bias)
  - Traditional metrics show near-zero correlation on datasets with high response variability
  - Pairwise ranking accuracy high but correlation low (evaluator good at relative comparisons, poor at absolute scoring)
  - Reference-free evaluation correlates poorly with human judgment when query lacks sufficient context
- First 3 experiments:
  1. Replicate correlation analysis on one English subset (e.g., DermaVQA test) using provided human ratings; verify GPT-4o avg-corr > 0.35.
  2. Run ablation on reference count (3→2→1→0 refs) for WoundcareVQA; confirm 5-10 percentage point degradation pattern.
  3. Test edge case: submit medically incorrect but fluently-written response to LLM evaluator; measure false-positive rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can smaller, open-source LLMs be fine-tuned on the MORQA dataset to match the performance of proprietary models (e.g., GPT-4o) in correlating with human judgments?
- Basis in paper: [explicit] The Conclusion states, "Fine-tuning evaluation models remains an important direction for future work," and expresses hope that the dataset will facilitate this.
- Why unresolved: The current study benchmarks pre-trained evaluators (like Prometheus and GPT-4o) but does not perform fine-tuning experiments on the collected data.
- What evidence would resolve it: A study training open-source evaluators on the MORQA training set and comparing their correlation with human ratings against the reported baselines.

### Open Question 2
- Question: How can reference-based metrics be adapted to prevent the systematic over-scoring of fluent LLM-generated responses relative to human-authored gold standards?
- Basis in paper: [explicit] Section 7 (Holdout experiment) notes that GPT-4o outputs "consistently achieve higher scores than gold references," suggesting metrics favor surface fluency over human validity.
- Why unresolved: The paper identifies this bias but does not propose or test methods to correct for the "overestimation" of system quality.
- What evidence would resolve it: Developing modified metrics or calibration techniques that penalize hallucination or rate diverse human responses higher than synthetic ones.

### Open Question 3
- Question: What characteristics of reference variability (e.g., semantic contradiction vs. lexical diversity) determine whether adding more references improves LLM-evaluator accuracy?
- Basis in paper: [explicit] The Ablation study notes that while increasing references generally helps, it did not for DermaVQA, potentially due to the "variability and randomness of the available reference responses."
- Why unresolved: The paper observes the performance plateau but does not analyze the specific linguistic or semantic properties of the references that cause it.
- What evidence would resolve it: A controlled analysis comparing DermaVQA reference sets against other datasets to isolate the factor causing the lack of improvement.

## Limitations
- Dataset accessibility: MORQA's expert ratings are not yet publicly available, preventing independent validation of the correlation results and human judgment quality
- LLM evaluation consistency: The paper reports aggregate performance but doesn't analyze inter-annotator agreement among LLM evaluators or assess robustness to prompt variations
- Generalization across domains: Results are based on medical QA; performance on other open-ended QA domains remains untested

## Confidence

- **High confidence**: LLM-based evaluators outperform traditional metrics in correlating with human judgments on medical open-ended QA (supported by direct correlation measurements across multiple datasets)
- **Medium confidence**: Traditional metrics perform competitively on Chinese subsets with shorter responses (evidence is limited to specific corpus characteristics without broader validation)
- **Medium confidence**: Multiple reference answers improve evaluation performance (supported by ablation studies but lacks independent replication)

## Next Checks
1. Replicate correlation analysis: Obtain MORQA dataset (when released) and independently verify that LLM evaluators achieve avg-corr > 0.35 on English datasets while traditional metrics remain below 0.3
2. Cross-domain generalization: Apply MORQA evaluation framework to non-medical open-ended QA (e.g., general knowledge or technical domains) to test metric performance beyond medical context
3. Reference-free robustness testing: Conduct systematic evaluation of reference-free LLM scoring against reference-based methods, measuring sensitivity to query context quality and response plausibility