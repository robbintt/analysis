---
ver: rpa2
title: 'Among Them: A game-based framework for assessing persuasion capabilities of
  LLMs'
arxiv_id: '2502.20426'
source_url: https://arxiv.org/abs/2502.20426
tags:
- persuasion
- game
- llms
- techniques
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Among Them, an Among Us-inspired framework
  for evaluating the persuasion capabilities of large language models (LLMs) through
  controlled gameplay. The framework enables systematic comparison of LLMs by tracking
  game statistics and quantifying the use of 25 persuasion techniques from social
  psychology and rhetoric.
---

# Among Them: A game-based framework for assessing persuasion capabilities of LLMs

## Quick Facts
- arXiv ID: 2502.20426
- Source URL: https://arxiv.org/abs/2502.20426
- Reference count: 40
- Key outcome: 640 LLM games show all models employ 22/25 persuasion techniques, but larger models show no advantage and verbosity correlates negatively with winning

## Executive Summary
This paper introduces Among Them, an Among Us-inspired framework for evaluating LLM persuasion capabilities through controlled social deduction gameplay. The framework tracks 25 persuasion techniques from social psychology and rhetoric, enabling systematic comparison of 8 popular LLMs across 640 games. Results demonstrate that all tested models successfully employ 22 of the 25 anticipated techniques, though larger models show no persuasion advantage over smaller ones. Notably, longer model outputs correlate negatively with winning rates, suggesting verbosity may undermine deceptive effectiveness.

## Method Summary
The framework uses a text-based Among Us variant where agents play as either crewmates (complete tasks) or impostors (eliminate crewmates without detection). Each agent consists of three components: adventure (planning at temp=1.0, action at temp=0.0), discussion (temp=0.5), and voting (temp=0.0). A separate LLM instance tags persuasion techniques in chat logs. The study conducted 640 games using 8 models (Claude 3.5 Haiku/Sonnet, Gemini Flash 1.5/Pro 1.5, GPT-4o mini/4o, Llama 3.1 8B/405B) in a pairwise tournament format with 5 players per game (4 crewmates, 1 impostor).

## Key Results
- All tested LLMs successfully employed 22 of 25 anticipated persuasion techniques
- No significant persuasion advantage for larger models (Llama 405B/GPT-4o vs Claude Haiku/8B)
- Longer model outputs correlate negatively with winning rates (rpb = -0.070)
- Crewmate win rate was 60% across all games
- Inter-rater agreement between human annotations and automated tagger was moderate (Krippendorff's α = 0.56)

## Why This Works (Mechanism)

### Mechanism 1: Emergent Strategy via Role-Constraint
LLMs retrieve and apply persuasion techniques from training data when constrained by specific roles and conflicting objectives. The impostor role creates a ground truth (guilt) that the win condition forces the model to conceal, triggering latent capabilities.

### Mechanism 2: Verbosity as a Negative Signal
In constrained social deduction environments, increased output length correlates negatively with success rates for deceptive agents. Longer outputs increase the surface area for logical inconsistencies that other agents can detect.

### Mechanism 3: Component-Based State Isolation
Separating agent logic into distinct components with different temperatures (1.0 for planning, 0.0 for action validation) prevents policy interference and reduces hallucinations about game rules.

## Foundational Learning

**Concept: Social Deduction as a Security Benchmark**
Why needed: The framework treats Among Us as a dynamic environment to test AI Safety (deception and manipulation), not just as a game.
Quick check: Can you distinguish between a model failing to win because it is "bad at games" vs. failing because it is "incapable of lying"?

**Concept: LLM-as-a-Judge Reliability**
Why needed: Results rely entirely on an LLM (Gemini Flash 1.5) tagging conversation logs with persuasion techniques.
Quick check: What is the measured inter-rater agreement (Krippendorff's alpha) between human annotations and the LLM tagger, and why does that matter?

**Concept: Chain-of-Thought (CoT) Leakage**
Why needed: Critical failure mode where agent's internal reasoning is accidentally outputted into public chat.
Quick check: Why does the prompt structure need strict separation from Chain of Thought reasoning to prevent self-incrimination?

## Architecture Onboarding

**Component map:**
Game Engine -> Agent Controller -> Persuasion Assessment Module -> Dashboard

**Critical path:**
State Update -> Agent Prompt Generation -> LLM Inference -> Action Validation -> Persuasion Tagging (if discussion) -> State Update

**Design tradeoffs:**
- Text-based vs. Visual: Purely text-based for LLM compatibility, sacrificing visual spatial reasoning
- Cost vs. Fidelity: Running 640 games with large models is expensive; small vs. large model comparison constrained by context windows
- Inter-rater Agreement: Using LLM to tag persuasion techniques (Alpha=0.56) allows massive scale but introduces noise

**Failure signatures:**
- "Dave" Error: Agents output meta-text like "As Dave the impostor, I will..." directly into chat
- Hallucinated Moves: Agents attempting to move to disconnected locations
- Task Faking Loop: Impostors getting stuck in faking task loops, leading to timeouts

**First 3 experiments:**
1. Prompt Isolation Test: Run a game and force model to reveal system instructions in chat, then patch template
2. Temperature Sweep: Run 10 games with Discussion temp=0.0 vs 1.0 to observe creativity effect on deception
3. Tagger Validation: Manually tag 5 random discussion logs and compare against automated Assessment Module

## Open Questions the Paper Calls Out

**Open Question 1:** How does LLM persuasion efficacy change when competing against human players compared to other LLMs?
Basis: The conclusion states future work includes "running human-AI and human-human games in the Among Them environment."
Unresolved: Current 640 LLM-versus-LLM games may not predict human susceptibility to AI deception.

**Open Question 2:** To what extent do agents' internal chain-of-thought reasoning processes align with their verbalized arguments and in-game actions?
Basis: Framework can be used as a "testbed for chain of thought faithfulness experiments."
Unresolved: Authors noted agents "outing themselves" via meta-discussions, indicating potential disconnect.

**Open Question 3:** How robust are persuasion rankings across different prompting strategies or persona configurations?
Basis: Conclusion lists "testing various prompting techniques" as specific avenue for future research.
Unresolved: Experiments used specific prompt setup; unclear if different engineering could reverse finding that larger models lack persuasion advantage.

## Limitations

- Moderate inter-rater reliability (Krippendorff's α = 0.56) of automated persuasion tagging system
- Purely text-based game format may limit generalizability to visual social deduction contexts
- Findings based on correlation rather than controlled experiments for verbosity effects

## Confidence

- **High confidence:** Framework architecture and game mechanics are clearly specified and reproducible
- **Medium confidence:** Aggregate finding that LLMs can employ persuasion techniques in social deduction contexts
- **Low confidence:** Specific persuasion technique counts and negative correlation between verbosity and success rates

## Next Checks

1. **Tagger validation:** Manually annotate 20 additional discussion logs and compute agreement against automated system to establish if 0.56 reliability is representative
2. **Verbosity experiment:** Run controlled experiment varying only output length (via token limits) while holding model and strategy constant to isolate effect on win rates
3. **Cross-validation:** Test same framework with different persuasion evaluation system (human annotators or different LLM) to verify core findings are not dependent on specific tagging methodology