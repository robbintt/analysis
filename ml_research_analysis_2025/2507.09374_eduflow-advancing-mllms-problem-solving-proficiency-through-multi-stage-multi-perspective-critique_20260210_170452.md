---
ver: rpa2
title: 'EduFlow: Advancing MLLMs'' Problem-Solving Proficiency through Multi-Stage,
  Multi-Perspective Critique'
arxiv_id: '2507.09374'
source_url: https://arxiv.org/abs/2507.09374
tags:
- reasoning
- eduprm
- arxiv
- zhang
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EduFlow, a multi-stage framework to improve
  multimodal large language models' problem-solving in educational settings. It combines
  MCTS-based trajectory construction with a process-aware reward model (EduPRM) that
  critiques reasoning steps with detailed tags and justifications.
---

# EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique

## Quick Facts
- **arXiv ID**: 2507.09374
- **Source URL**: https://arxiv.org/abs/2507.09374
- **Reference count**: 35
- **Primary result**: EduFlow improves MLLM reasoning on K-12 benchmarks via MCTS-guided trajectory construction and process-level critique.

## Executive Summary
EduFlow introduces a multi-stage framework to enhance MLLMs' problem-solving in educational contexts by integrating Monte Carlo Tree Search (MCTS) with a process-aware reward model (EduPRM). EduPRM critiques reasoning steps with fine-grained error tags and justifications, trained on three complementary sources: MCTS trajectories, error-injected critiques, and teacher-student dialogues. EduFlow uses EduPRM to guide data selection, construct reasoning paths via EduMCTS, and select final answers through best-of-N inference. Experiments on K-12 benchmarks show significant gains over baselines, with improvements in both direct and step-by-step metrics, and better performance across subjects.

## Method Summary
EduFlow enhances MLLMs through a three-stage pipeline: (1) EduPRM training via curriculum learning on three data sources (MCTS trajectories, error-injected critiques, teacher-student dialogues), learning to output step-level content, labels, explanations, and scores; (2) EduMCTS trajectory construction using six action nodes (caption, summary, sub_task, thinking, self-reflection, answer), where EduPRM scores and prunes candidates during MCTS rollouts to generate high-quality reasoning paths; (3) Best-of-N inference, where EduPRM verifies N sampled candidates and selects the highest-scoring path. The unified PRM across data selection, trajectory construction, and verification enables iterative refinement and compounding quality gains.

## Key Results
- EduPRM-trained MLLMs achieve 43.45 step-by-step score on K12Vista vs. 38.18 for the best baseline.
- EduMCTS with EduPRM reaches 87.1% trajectory success rate, up from 67.8% with vanilla MCTS.
- EduFlow improves performance across all subjects (Math, Chemistry, Physics, Biology, Geography) on MDK12, MathVision, and MMMU_Pro_V_COT benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source, critique-based process supervision improves step-level reasoning evaluation over scalar-only PRMs
- Mechanism: EduPRM is trained on three complementary data sources—MCTS-guided trajectories (verified multi-step solutions), error-injected critiques (simulated misconceptions), and teacher-student dialogues (refinement of ambiguous steps)—then learns via curriculum progression from atomic step annotation to full-path critique. This yields fine-grained error tags (8 types) plus justifications, not just scalar scores.
- Core assumption: Diverse supervision sources covering correct paths, realistic errors, and interactive refinement generalize better than single-path or scalar-only PRM training.
- Evidence anchors:
  - [abstract] "EduPRM is trained via curriculum learning on three complementary supervision sources: MCTS-guided trajectories, error-injected critiques, and teacher–student dialogues"
  - [section 3.1.1] "We construct EduPRM-420K using three complementary strategies... jointly provide diverse supervision signals—from correct trajectories to realistic and noisy reasoning paths"
  - [corpus] Weak direct corpus evidence; neighbor papers discuss critique-guided reasoning but not the specific three-source curriculum design
- Break condition: If supervision diversity lacks error-type coverage or if curriculum stages are poorly sequenced, generalization to unseen reasoning patterns may degrade.

### Mechanism 2
- Claim: PRM-guided MCTS with domain-specific actions and self-reflection produces higher-quality reasoning trajectories than unguided search
- Mechanism: EduMCTS decomposes reasoning into six functional node types (caption, summary, sub_task, thinking, self-reflection, answer). At each expansion, actor models generate candidates, EduPRM scores and filters them via a pruning threshold τ, and rewards backpropagate. Self-reflection nodes promote error-checking. This yields EduMCTS-160K after consistency filtering.
- Core assumption: Step-level reward feedback plus structured action types reduce low-quality path exploration; self-reflection enables iterative correction within the search.
- Evidence anchors:
  - [abstract] "EduMCTS... introduces bootstrapping actions specifically designed for educational reasoning, such as a Self-Reflection mechanism that promotes reflective error correction"
  - [section 3.3] "Candidate reasoning paths are expanded... After each rollout, EduPRM evaluates the entire trajectory and assigns a reward... This score is then backpropagated"
  - [corpus] Neighbor "Mulberry" (Yao et al., 2024) uses collective MCTS with reflection for MLLMs, providing partial support for reflection-augmented search; direct evidence for the six-node action schema is paper-specific
- Break condition: If EduPRM miscalibrates scores or τ is poorly set, search may prune valid paths or retain low-quality ones; self-reflection may loop without convergence.

### Mechanism 3
- Claim: Unifying PRM across data selection, trajectory construction, and inference-time reranking creates compounding quality gains
- Mechanism: EduPRM acts as (1) Selector—filtering low-reward samples for reconstruction; (2) Critic—guiding MCTS rollout via stepwise rewards; (3) Verifier—ranking N candidates at inference. This loop preferentially amplifies high-quality reasoning through the full pipeline.
- Core assumption: Consistent process-level supervision across stages yields greater gains than applying PRM at a single stage; Best-of-N scaling continues to improve with better verification.
- Evidence anchors:
  - [abstract] "EduPRM... enables dynamic adaptation to multi-stage problem solving and iterative refinement during inference"
  - [section 3.2] "EduPRM functions as a unified process-level controller within EduFlow, enhancing both training and inference through fine-grained stepwise supervision"
  - [table 4] EduPRM shows consistent Best-of-N gains (N=2: 41.21 → N=8: 43.28 → N=12: 43.45) vs baselines that saturate or degrade
  - [corpus] Weak corpus evidence for full-pipeline unification; neighbors focus on isolated PRM or critique uses
- Break condition: If any stage's PRM guidance misaligns (e.g., selector over-prunes, verifier overfits to reward artifacts), compounding may amplify bias rather than quality.

## Foundational Learning

- Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)
  - Why needed here: EduPRM's value depends on understanding that step-level supervision can catch error accumulation earlier than outcome-only signals.
  - Quick check question: Given a 5-step solution with an error at step 2, would an ORM or PRM more reliably detect the failure mode before step 5?

- Concept: Monte Carlo Tree Search (MCTS) basics—selection, expansion, simulation, backpropagation
  - Why needed here: EduMCTS adapts MCTS with PRM-guided expansion and pruning; understanding standard MCTS clarifies what's modified.
  - Quick check question: In EduMCTS, what replaces the traditional simulation/rollout phase, and how does backpropagation use EduPRM scores?

- Concept: Curriculum learning—easy-to-hard or atomic-to-complex training progression
  - Why needed here: EduPRM uses two-stage curriculum (Stepwise Format → Critique Format); misordering could harm generalization.
  - Quick check question: Why might training on full-path critique before atomic step prediction degrade fine-grained error localization?

## Architecture Onboarding

- Component map: EduPRM (7B) -> EduMCTS (6 action nodes) -> EduMCTS-160K -> SFT on Qwen2.5-VL-7B -> Best-of-N (N=8) inference

- Critical path:
  1. Build EduPRM-420K (MCTS trajectories + error injection + teacher-student dialogues).
  2. Train EduPRM via curriculum (stepwise → critique).
  3. Use EduPRM to filter and reconstruct Edu-1000K into EduMCTS-160K via guided search.
  4. Fine-tune base MLLM on EduMCTS-160K.
  5. At inference, apply Best-of-N with EduPRM as verifier.

- Design tradeoffs:
  - PRM size (7B) vs. verifier accuracy vs. latency—larger PRMs may verify better but slow inference.
  - Number of MCTS rollouts vs. trajectory quality vs. compute—more rollouts improve success rate (67.8% → 87.1%) at higher cost.
  - Best-of-N (N=8 vs. N=12)—diminishing returns past N=8 in this setup (43.28 → 43.45).

- Failure signatures:
  - Search saturation: Success rate plateaus; check if τ is too aggressive or action space lacks diversity.
  - Verifier miscalibration: Best-of-N degrades at higher N; validate EduPRM on held-out K12-PEBench splits.
  - Over-pruning: EduMCTS-160K size much smaller than expected; relax τ or increase actor pool diversity.
  - Subject imbalance: Gains uneven across subjects; inspect error-type coverage per subject in EduPRM-420K.

- First 3 experiments:
  1. Validate EduPRM in isolation: Measure step-level classification accuracy on K12-PEBench vs. baseline verifiers (Qwen2-VL-7B, InternVL2.5-8B). Target: EduPRM ≥ 54% accuracy.
  2. Ablate MCTS components: Run EduMCTS with/without (a) stepwise action nodes, (b) EduPRM judge, (c) self-reflection. Track success rate and final step-by-step scores. Target: +EduPRM judge ≥ 87% success rate.
  3. End-to-end pipeline test: Fine-tune Qwen2.5-VL-7B on EduMCTS-160K, apply Best-of-N (N=8), evaluate on K12Vista (Direct & Step-by-Step). Target: Step-by-Step ≥ 43.28.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance trajectory of EduPRM-guided Best-of-N inference continue to improve, or does it plateau, when scaling N to 128 or 256?
- Basis in paper: [explicit] Section 7.1 (Limitations) states that "the BoN experiments did not further test results under settings such as N=128, N=256, etc."
- Why unresolved: Table 4 shows a trend where gains from N=8 to N=12 are marginal (43.28 to 43.45), suggesting a potential saturation point, but the authors explicitly flag the need to verify this at higher sampling rates common in test-time scaling.
- What evidence would resolve it: Benchmarking EduFlow on K12Vista using BoN with N=64, 128, and 256 to plot the accuracy curve relative to computational cost.

### Open Question 2
- Question: Is the EduFlow framework transferable to non-Qwen MLLM architectures (e.g., InternVL, LLaVA) without significant architecture-specific tuning?
- Basis in paper: [explicit] Section 7.1 notes the "base model was tested only on Qwen2VL and Qwen25VL, without testing on additional base models."
- Why unresolved: While the method is proposed as general, the specific action definitions in EduMCTS (caption, summary, etc.) or the PRM training data might be overfitted to the reasoning style or tokenizer of the Qwen family.
- What evidence would resolve it: Fine-tuning InternVL or LLaVA models on EduMCTS-160K and applying the EduPRM verifier to measure performance deltas against their respective baselines.

### Open Question 3
- Question: Can EduPRM's critique mechanism effectively enhance the instruction-following capabilities of "deep reasoning" models like DeepSeek-R1?
- Basis in paper: [explicit] Section 7.2 highlights "Limited Exploration of Reasoning Models," noting there is "insufficient comprehensive research into these models, especially regarding the elements that improve their ability to follow instructions."
- Why unresolved: Models like R1 use RL for internal reasoning, and it is unclear if an external Process Reward Model offers complementary guidance or conflicts with the model's internal reward signals.
- What evidence would resolve it: Applying EduPRM as a step-level verifier for an R1-class model on complex multi-modal tasks to measure improvements in instruction adherence and error recovery.

## Limitations

- EduFlow's EduPRM is trained on synthetic and simulated data (error-injected critiques, teacher-student dialogues), limiting reproducibility without full pipeline access.
- Performance gains are demonstrated primarily on K-12 benchmarks; generalization to real-world scientific reasoning tasks is unclear.
- The 7B EduPRM size may constrain verification quality compared to larger verifiers, and the framework's effectiveness on non-Qwen MLLM architectures is untested.

## Confidence

- **High confidence**: Multi-source curriculum training improves PRM fine-grained error detection (supported by error-label classification results and ablation of data sources).
- **Medium confidence**: EduMCTS with self-reflection produces higher-quality trajectories (success rate improvements are clear, but path diversity and computational cost trade-offs are not fully explored).
- **Medium confidence**: Unified PRM pipeline yields compounding gains (Best-of-N improvements are consistent, but end-to-end ablation vs. stage-by-stage gains is missing).

## Next Checks

1. Ablate EduPRM's three data sources individually to quantify contribution of each to error-label classification accuracy on K12-PEBench.
2. Measure MCTS trajectory diversity and success rate across subjects to detect potential bias or saturation in specific domains.
3. Evaluate EduFlow on a held-out scientific reasoning dataset (e.g., MMMU_Pro_V_COT) to test generalization beyond K-12 benchmarks.