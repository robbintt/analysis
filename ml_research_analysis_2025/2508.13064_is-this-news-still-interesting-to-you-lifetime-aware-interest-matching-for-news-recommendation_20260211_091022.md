---
ver: rpa2
title: 'Is This News Still Interesting to You?: Lifetime-aware Interest Matching for
  News Recommendation'
arxiv_id: '2508.13064'
source_url: https://arxiv.org/abs/2508.13064
tags:
- news
- lifetime
- user
- recommendation
- lime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personalized news recommendation
  by incorporating time-sensitive signals. The key innovation is the introduction
  of user-topic lifetime-aware age representations and candidate-aware lifetime attention,
  which capture how long a user remains interested in a topic and when clicked news
  were consumed relative to that period.
---

# Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation

## Quick Facts
- arXiv ID: 2508.13064
- Source URL: https://arxiv.org/abs/2508.13064
- Reference count: 40
- Primary result: Model-agnostic framework improves news recommendation accuracy by up to 27.95% nDCG@5 and 24.97% MRR over state-of-the-art methods.

## Executive Summary
This paper introduces LIME (Lifetime-aware Interest Matching for News Recommendation), a model-agnostic framework that incorporates time-sensitive signals into news recommendation systems. The framework addresses the challenge of modeling how long users remain interested in topics and when clicked news were consumed relative to that period. By introducing user-topic lifetime-aware age representations and candidate-aware lifetime attention, LIME captures both the relative timing of news consumption and semantic alignment between clicked and candidate news. Experimental results on MIND-small and Adressa datasets demonstrate consistent improvements across multiple base models, with the framework showing up to 27.95% gains in nDCG@5 and 24.97% in MRR.

## Method Summary
LIME is a model-agnostic framework that enhances existing news recommendation models with three time-aware mechanisms. First, it encodes news age relative to individual user-topic consumption patterns using logarithmic bucketization and learned embeddings. Second, it employs candidate-aware lifetime attention that reweights clicked news based on semantic relevance and temporal alignment to the candidate. Third, it implements freshness-guided interest refinement that explicitly penalizes candidates with insufficient remaining lifetime using a scaled sigmoid function. The framework integrates seamlessly with base models like NRMS, NAML, and CROWN, requiring only news and click history as inputs while maintaining the original model's architecture for content encoding.

## Key Results
- LIME improves nDCG@5 by up to 27.95% and MRR by up to 24.97% over state-of-the-art methods on real-world datasets
- Individual components show significant contributions: S1 alone improves MRR by 13.33% on Adressa, S1+S2 by 17.86%, and S1+S3 by 21.40%
- Optimal freshness penalty parameters are α=0.3 and β=0.3, balancing temporal sensitivity with recommendation diversity
- LIME demonstrates consistent performance improvements across multiple base models (NRMS, NAML, CROWN) and datasets (MIND-small, Adressa)

## Why This Works (Mechanism)

### Mechanism 1: User-Topic Lifetime-Aware Age Representation
- **Claim:** Encoding news age relative to individual user-topic consumption patterns improves news representation quality beyond absolute age.
- **Mechanism:** Logarithmic bucketization maps raw age and lifetime values to discrete bins, which are then embedded and fused via a learned transformation. This captures when within a user's topic-specific interest window the news was consumed.
- **Core assumption:** Users exhibit consistent but topic-dependent consumption velocities; relative timing matters more than absolute elapsed time.
- **Evidence anchors:** Abstract states "User-Topic lifetime-aware age representation to capture the relative age of news with respect to a user-topic pair"; Section 4.3 shows S1 alone improves MRR by 13.33% on Adressa; supported by Interest Clock paper on temporal dynamics.

### Mechanism 2: Candidate-Aware Lifetime Attention
- **Claim:** Reweighting clicked news based on both semantic relevance and temporal alignment to the candidate improves user representation.
- **Mechanism:** Candidate topic embedding serves as query to attend over clicked news topic embeddings. Gated residual connection preserves original information while adaptively incorporating attention weights.
- **Core assumption:** A user's current interest is better predicted by historically clicked news that are semantically and temporally aligned with the candidate.
- **Evidence anchors:** Abstract mentions "Candidate-aware lifetime attention for generating temporally aligned user representation"; Table 3 shows S1+S2 improves MRR by 17.86% on Adressa; supported by generative news recommendation emphasis on high-level connections.

### Mechanism 3: Freshness-Guided Interest Refinement
- **Claim:** Explicitly penalizing candidate news with insufficient remaining lifetime reduces false positives and improves ranking accuracy.
- **Mechanism:** Freshness is computed as user-topic lifetime minus candidate age. A scaled sigmoid function modulates the base interest score, with separate handling for expired vs valid candidates.
- **Core assumption:** News articles have finite validity windows per user-topic, and users prefer content within their personalized interest duration.
- **Evidence anchors:** Abstract states "Freshness-guided interest refinement for prioritizing valid candidate news"; Section 4.5 shows optimal α=0.3, β=0.3; Table 3 shows S1+S3 improves nDCG@5 by 21.40% on Adressa; supported by Interest Clock paper.

## Foundational Learning

- **Attention Mechanisms (Query-Key-Value):** Why needed - Mechanism 2 uses candidate topic as query to attend over clicked news; understanding attention weighting and gating is essential. Quick check: Given a query vector q and key vectors K, how would you compute attention weights, and why might a gated residual connection help preserve information?

- **Temporal Dynamics in Recommendation:** Why needed - The entire framework hinges on modeling how time affects content validity and user interest; concepts of age, lifetime, and freshness are central. Quick check: Why might a fixed 48-hour lifetime for all news articles fail to capture user-specific interest patterns?

- **Model-Agnostic Framework Design:** Why needed - LIME is designed as a plug-in for existing news recommenders; understanding modular design helps integrate it with various backbones. Quick check: What constraints must a module satisfy to be model-agnostic, and how does LIME achieve this for news/user encoders?

## Architecture Onboarding

- **Component map:** News content/timestamps → Age bucketization → Age embedding → Content-age concatenation → Candidate topic attention over clicked news → User representation → Base score → Freshness-weighted final score

- **Critical path:** Age-aware News Modeling (News encoder → Age encoder → Content-Age aggregator) → Lifetime-aware User Modeling (Candidate-aware attention → User encoder) → Freshness-guided Interest Matching (Base score → Freshness calculation → Score refinement)

- **Design tradeoffs:**
  - Lifetime definition: Fixed (simple but rigid) vs Topic-wise (moderate complexity) vs User-Topic (fine-grained but requires sufficient user history)
  - Temporal signal injection: Representation-level (S1, S2) vs Matching-level (S3); paper shows combined is best
  - Hyperparameters α, β: Control sensitivity and penalty strength; too low ignores temporal signals, too high over-penalizes

- **Failure signatures:**
  - Recommending expired content (freshness refinement too weak)
  - Missing valid user interests (lifetime too short or attention misaligned)
  - Poor cold-start performance (insufficient history for user-topic lifetime estimation)

- **First 3 experiments:**
  1. Ablation study: Test S1, S1+S2, S1+S3, S1+S2+S3 to isolate contribution of each mechanism (replicate Table 3)
  2. Lifetime definition comparison: Compare Fixed, Topic-wise, and User-Topic lifetime across multiple base models (replicate Table 4)
  3. Hyperparameter sensitivity: Vary α and β from 0.1 to 0.9 to find optimal range and validate robustness (replicate Figure 6)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The logarithmic bucketization approach lacks specification of the number of buckets (B), which critically affects embedding granularity and model capacity
- User-topic lifetime computation requires sufficient click history per user-topic pair, creating cold-start challenges not addressed in the paper
- The freshness penalty mechanism assumes linear decay of interest, which may not hold for all content types (e.g., evergreen news)

## Confidence
- **High confidence:** Mechanism 1 (User-Topic lifetime-aware age representation) - supported by clear equations, ablation results showing 13.33% MRR improvement, and established temporal dynamics literature
- **Medium confidence:** Mechanism 2 (Candidate-aware lifetime attention) - attention framework is sound, but effectiveness depends heavily on topic embedding quality and user interest stability assumptions
- **Medium confidence:** Mechanism 3 (Freshness-guided interest refinement) - freshness concept is intuitive, but optimal penalty strength likely domain-dependent rather than universal

## Next Checks
1. **Lifetime sensitivity analysis:** Systematically vary m (top-m% clicks) from 70% to 95% and measure impact on MRR/nDCG across all three base models to identify optimal lifetime definition
2. **Cross-domain robustness test:** Apply LIME to a non-news recommendation dataset (e.g., Amazon product reviews) with explicit timestamps to verify the framework's generalizability beyond news
3. **Cold-start simulation:** Gradually reduce user click history and measure performance degradation to quantify the minimum viable history for user-topic lifetime estimation