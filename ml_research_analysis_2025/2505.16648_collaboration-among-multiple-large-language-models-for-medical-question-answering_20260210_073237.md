---
ver: rpa2
title: Collaboration among Multiple Large Language Models for Medical Question Answering
arxiv_id: '2505.16648'
source_url: https://arxiv.org/abs/2505.16648
tags:
- llms
- collaboration
- medical
- confidence
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an iterative multi-LLM collaboration framework
  for medical question answering using USMLE data. The method applies zero-shot chain-of-thought
  with self-consistency followed by a collaboration loop where LLMs exchange reasoning
  to resolve disagreements.
---

# Collaboration among Multiple Large Language Models for Medical Question Answering

## Quick Facts
- **arXiv ID:** 2505.16648
- **Source URL:** https://arxiv.org/abs/2505.16648
- **Reference count:** 34
- **Primary result:** Iterative multi-LLM collaboration improved accuracy across Med42, ClinCamel, and Mixtral on USMLE data, with consensus increasing from 50.82% to 82.62%.

## Executive Summary
This study introduces an iterative multi-LLM collaboration framework designed to enhance accuracy in medical question answering. By leveraging zero-shot chain-of-thought reasoning, self-consistency, and structured inter-model reasoning exchange, the framework enables LLMs to resolve disagreements and improve overall performance. Tested on USMLE-style questions, the method substantially increased both accuracy and consensus among participating models. The approach highlights the potential of collaborative reasoning in medical AI applications, though its generalizability to broader clinical settings remains to be validated.

## Method Summary
The framework employs an iterative collaboration loop among multiple LLMs. Initially, each model independently generates answers using zero-shot chain-of-thought prompting followed by self-consistency sampling. Disagreements trigger structured reasoning exchanges, where models iteratively share rationales and refine their answers. This process continues until consensus is reached or a predefined iteration limit is met. The collaboration mechanism is designed to harness the collective reasoning strengths of diverse LLMs, particularly improving performance among lower-confidence models.

## Key Results
- Accuracy improvements: Med42 (71.48% → 76.72%), ClinCamel (68.85% → 75.41%), Mixtral (71.47% → 77.38%).
- Consensus among LLMs increased from 50.82% to 82.62%.
- Lower-confidence models showed greater relative improvement through collaboration.
- Models demonstrated higher consistency when answering correctly versus incorrectly.

## Why This Works (Mechanism)
The framework works by iteratively refining reasoning through structured exchange among multiple LLMs. Initial independent answers identify disagreements, which then trigger collaborative reasoning cycles. During these cycles, models share and critique rationales, allowing weaker models to benefit from stronger partners and enabling collective convergence toward correct answers. The iterative nature allows models to correct initial reasoning errors and align on the most robust conclusions.

## Foundational Learning
- **Zero-shot Chain-of-Thought Reasoning:** Enables models to decompose complex medical questions into logical steps without task-specific training. Needed because medical QA often requires multi-step reasoning; quick check: does the model produce coherent reasoning traces?
- **Self-Consistency Sampling:** Improves robustness by generating multiple reasoning paths and selecting the most consistent answer. Needed to mitigate the impact of individual reasoning errors; quick check: is the final answer stable across multiple samples?
- **Iterative Reasoning Exchange:** Facilitates knowledge sharing and error correction among LLMs. Needed to exploit complementary strengths of diverse models; quick check: does consensus increase after each exchange round?
- **Confidence-Quality Correlation:** Higher-confidence models tend to produce more accurate answers. Needed to identify and leverage reliable reasoning; quick check: does model confidence correlate with accuracy across diverse questions?

## Architecture Onboarding
- **Component Map:** Input questions → Individual reasoning (zero-shot CoT + self-consistency) → Disagreement detection → Reasoning exchange loop → Consensus output
- **Critical Path:** Question ingestion → Independent reasoning → Consensus-building exchange → Final answer selection
- **Design Tradeoffs:** More iterations can improve accuracy but increase computational cost and latency. Simpler consensus rules are faster but may miss nuanced improvements.
- **Failure Signatures:** Persistent disagreement despite multiple exchanges, collapse to majority vote without substantive reasoning improvement, or overfitting to patterns in USMLE data.
- **First Experiments:**
  1. Compare accuracy and consensus with/without the reasoning exchange loop.
  2. Test collaboration across models with varied initial confidence levels.
  3. Evaluate framework robustness on non-USMLE medical QA datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to USMLE-style multiple-choice questions, restricting generalizability.
- Lack of transparency in model configurations and algorithm details impedes exact replication.
- Unclear extent to which accuracy gains stem from reasoning exchange versus repeated sampling.
- Observational confidence-quality analysis may be confounded by question difficulty or domain familiarity.

## Confidence
- **High:** Observed accuracy gains and consensus increases within the tested dataset.
- **Medium:** Effectiveness of cross-model reasoning exchange (mechanism inferred).
- **Low:** Broader generalizability to real-world clinical use or other domains.

## Next Checks
1. Replicate the framework on a different medical QA dataset (e.g., clinical vignettes or patient questions) to test generalizability.
2. Conduct ablation studies isolating the effects of cross-model reasoning exchange versus repeated sampling.
3. Implement and test the collaboration framework in a simulated clinical decision support scenario to assess real-world applicability.