---
ver: rpa2
title: Towards Learning High-Precision Least Squares Algorithms with Sequence Models
arxiv_id: '2503.12295'
source_url: https://arxiv.org/abs/2503.12295
tags:
- conv
- base
- dout
- precision
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Standard Transformers fail to achieve machine precision or numerical
  generality on least squares, with solutions often worse than machine epsilon and
  performance degrading on out-of-distribution problems. We identify that softmax
  attention struggles to perform high-precision multiplications, a bottleneck for
  implementing algorithms like gradient descent.
---

# Towards Learning High-Precision Least Squares Algorithms with Sequence Models

## Quick Facts
- arXiv ID: 2503.12295
- Source URL: https://arxiv.org/abs/2503.12295
- Reference count: 40
- Standard Transformers fail to achieve machine precision on least squares problems

## Executive Summary
This work addresses the challenge of learning high-precision numerical algorithms, specifically least squares solvers, using sequence models. The authors identify that standard Transformers struggle with high-precision multiplication due to softmax attention limitations, resulting in solutions worse than machine epsilon and poor performance on out-of-distribution problems. They propose polynomial architectures like BASE CONV and linear attention to close the expressivity gap, and introduce a high-precision training recipe with adaptive learning rates based on gradient cosine similarity and EMA smoothing. Their approach achieves up to 100,000× lower MSE than standard Transformers and 10,000× smaller generalization gaps.

## Method Summary
The authors systematically analyze why standard Transformers fail at high-precision least squares computation, identifying softmax attention as the bottleneck for precise multiplication operations needed in algorithms like gradient descent. They propose polynomial architectures that can precisely express gradient descent iterates, closing the expressivity gap. To overcome training precision bottlenecks, they introduce a high-precision training recipe involving adaptive learning rates based on gradient cosine similarity and EMA smoothing, enabling training to near machine precision. The approach combines architectural improvements with specialized training techniques to achieve significantly better performance on both in-distribution and out-of-distribution problems.

## Key Results
- Standard Transformers achieve solutions worse than machine epsilon on least squares problems
- Proposed polynomial architectures close the expressivity gap for gradient descent implementation
- High-precision training recipe enables training to near machine precision with adaptive learning rates
- Achieved up to 100,000× lower MSE compared to standard Transformers
- Reduced generalization gaps by 10,000× on out-of-distribution problems

## Why This Works (Mechanism)
The core mechanism addresses the fundamental limitation of softmax attention in performing high-precision multiplications required for numerical algorithms. Standard Transformers use softmax attention which, while effective for many tasks, cannot precisely represent the multiplicative operations needed in iterative numerical methods like gradient descent. The polynomial architectures bypass this limitation by providing direct multiplicative operations that can express gradient descent iterates with high precision. The high-precision training recipe further enhances performance by adapting learning rates based on gradient cosine similarity and using EMA smoothing to maintain stability during training at near-machine precision levels.

## Foundational Learning
- **Softmax attention mechanics**: Needed to understand why standard Transformers fail at high-precision multiplication; quick check: verify attention weights sum to 1 and demonstrate precision loss in multiplicative operations
- **Gradient descent algorithm**: Essential for understanding the target numerical computation; quick check: verify convergence properties on synthetic least squares problems
- **Machine precision concepts**: Critical for measuring and achieving the desired numerical accuracy; quick check: confirm understanding of epsilon values and floating-point representation limits
- **Polynomial approximation theory**: Required to understand how polynomial architectures can express iterative numerical methods; quick check: validate polynomial approximations of common functions
- **EMA (Exponential Moving Average) smoothing**: Important for the high-precision training technique; quick check: implement basic EMA and observe smoothing effects on noisy gradients
- **Gradient cosine similarity**: Key component of the adaptive learning rate mechanism; quick check: compute cosine similarity between consecutive gradient updates

## Architecture Onboarding

**Component Map:**
Input sequence -> Polynomial layers (BASE CONV/linear attention) -> Output sequence

**Critical Path:**
The critical path involves transforming input data through polynomial operations that directly implement gradient descent steps, with the high-precision training recipe maintaining numerical stability throughout training.

**Design Tradeoffs:**
The main tradeoff is between architectural complexity (polynomial layers are more complex than standard attention) and numerical precision gains. Standard Transformers are simpler but cannot achieve the required precision for numerical algorithms, while polynomial architectures provide the necessary expressivity at the cost of increased complexity.

**Failure Signatures:**
- Solutions worse than machine epsilon indicate softmax attention limitations
- Degrading performance on out-of-distribution problems suggests insufficient generalization
- Training instability or divergence points to issues with the high-precision training recipe
- Poor convergence rates may indicate suboptimal polynomial approximation

**First Experiments:**
1. Compare standard Transformer vs polynomial architecture on simple gradient descent implementation
2. Test high-precision training recipe with varying adaptive learning rate schedules
3. Evaluate generalization performance on distribution-shifted least squares problems

## Open Questions the Paper Calls Out
The work acknowledges that while substantial progress has been made toward learning numerical algorithms with sequence models, several limitations remain. The approach still struggles with end-to-end learning of least squares solutions without algorithmic supervision. Additionally, the extent to which these approaches transfer to real-world numerical computing tasks beyond controlled synthetic problems remains unclear.

## Limitations
- Polynomial architectures successfully close the expressivity gap but still struggle with end-to-end learning without algorithmic supervision
- Training remains challenging and requires careful hyperparameter tuning including adaptive learning rates based on gradient cosine similarity and EMA smoothing
- Results are demonstrated primarily on controlled problems, with unclear transfer to diverse real-world numerical computing tasks

## Confidence
**High confidence**: The identification of softmax attention limitations for high-precision multiplication, and the demonstration that polynomial architectures can express gradient descent iterates more precisely

**Medium confidence**: The effectiveness of the high-precision training recipe for achieving near-machine precision training, though results depend heavily on careful hyperparameter selection

**Medium confidence**: The generalization improvements (10,000× smaller generalization gaps), though these are measured primarily on controlled distribution shifts rather than diverse real-world scenarios

## Next Checks
1. Test polynomial architectures on additional numerical algorithms beyond least squares (e.g., eigenvalue computation, matrix inversion) to assess generality
2. Evaluate model performance when trained on mixed-precision data to better reflect practical deployment scenarios
3. Benchmark against traditional numerical libraries on real-world datasets to measure practical utility beyond synthetic problems