---
ver: rpa2
title: 'SceneGram: Conceptualizing and Describing Tangrams in Scene Context'
arxiv_id: '2506.11631'
source_url: https://arxiv.org/abs/2506.11631
tags:
- scene
- context
- tangram
- house
- bird
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SceneGram, a dataset that pairs tangram shapes
  with scene context images to study how visual context influences conceptualization
  and naming. Human annotators produced highly varied descriptions of tangrams, showing
  that scene context led to descriptions more coherent with the scene (e.g., "sink"
  in bathroom, "crab" on beach).
---

# SceneGram: Conceptualizing and Describing Tangrams in Scene Context

## Quick Facts
- arXiv ID: 2506.11631
- Source URL: https://arxiv.org/abs/2506.11631
- Reference count: 32
- Primary result: Scene context increases conceptual coherence while maintaining high variation in human tangram descriptions; multimodal LLMs generate less diverse and less human-like conceptualizations.

## Executive Summary
This paper introduces SceneGram, a dataset pairing 37 tangram shapes with scene context images to investigate how visual context influences conceptualization and naming. Human annotators produced diverse descriptions that were more coherent with the scene context (e.g., "sink" in bathroom, "crab" on beach), demonstrating that context constrains interpretation while preserving variation. The study also evaluated multimodal LLMs, which showed less diversity and lower human-likeness in their conceptualizations, often converging on a few frequent labels per item despite showing some context sensitivity.

## Method Summary
The study uses 37 tangrams from KILO GRAM arranged in 2x2 grids with three scene images per item, creating 407 compositions across 11 scene conditions. Ten human descriptions per item were collected via crowdworkers, with labels extracted, normalized to WordNet synsets, and validated manually. Analysis employed Shape Naming Divergence (SND) for variation, % Top for agreement, and SHIFT scores to quantify context effects on label-scene coherence using CLIP, GloVe, and Numberbatch embeddings. LLaVA variants (7b-72b) were evaluated with two-step inference (location prediction, then description generation) and compared to human outputs.

## Key Results
- Scene context increased conceptual coherence (labels matched scene semantics) while maintaining high variation (SND scores 0.91-0.95 across conditions)
- LLaVA models generated less diverse descriptions with lower lexical overlap (<30%) with human annotations
- Context effects varied by tangram: some shapes showed greater adaptability to scene context than others

## Why This Works (Mechanism)

### Mechanism 1: Scene Context Constrains and Biases Conceptual Search Space
Visual scene context biases human conceptualization toward scene-coherent interpretations through rapid gist processing that pre-activates related semantic expectations. SHIFT scores comparing in-context vs. baseline coherence using multiple embeddings show larger shifts for semantic embeddings than visual ones, supporting genuine perspective changes rather than visual association.

### Mechanism 2: High Variation Persists Despite Contextual Constraint
Scene context increases conceptual coherence without eliminating variation because tangrams lack established naming conventions, enabling high baseline variability. Context adds accessible interpretations without blocking alternatives, creating a "rich-get-richer" effect while preserving diversity across all conditions.

### Mechanism 3: LLMs Show Surface-Level Context Sensitivity with Reduced Diversity
Multimodal LLMs capture coarse context effects but fail to model human-like conceptual diversity, converging on fewer interpretations due to training data biases and token prediction dynamics that favor high-probability continuations. This results in low lexical overlap with human labels and scene-level contamination of tangram conceptualization.

## Foundational Learning

- Concept: **Tangrams as ambiguous visual stimuli** - Tangrams are abstract geometric figures without canonical names, ideal for studying conceptual flexibility without prior semantic commitment. Quick check: Why are tangrams preferred over photographs of real objects for studying naming variation?

- Concept: **Scene gist and contextual expectations** - Human vision rapidly extracts scene-level statistics ("gist"), generating expectations about plausible objects that shape both perception and conceptualization. Quick check: What does it mean that scene gist is processed "rapidly," and how might this affect object interpretation?

- Concept: **Shape Naming Divergence (SND) metric** - SND quantifies variation in descriptions by measuring token overlap across annotations for the same item; higher scores indicate greater diversity. Quick check: Would SND increase or decrease if all annotators used the same label for a tangram?

## Architecture Onboarding

- Component map: Tangram-scene pairing -> Human annotation -> Label extraction/normalization -> Coherence scoring (SHIFT) -> Model comparison
- Critical path: Tangram-scene pairing → Human annotation → Label extraction/normalization → Coherence scoring (SHIFT) → Model comparison
- Design tradeoffs: 2×2 grid avoids explicit spatial embedding but may dilute context strength; WordNet normalization reduces variation but may conflate senses; generated scenes enable controlled diversity but lack photorealism
- Failure signatures: Models describe scenes instead of tangrams (74% "mountain" in mountain scenes); low lexical overlap (<30%) indicates conceptual mismatch; location prediction failures suggest parsing difficulties
- First 3 experiments: 1) Reproduce SHIFT analysis for all 11 scene conditions; 2) Generate baseline model diversity comparisons for LLaVA-7b and 72b; 3) Replace 2×2 grid with single overlaid scene to test context strength effects

## Open Questions the Paper Calls Out

### Open Question 1
Does scene context primarily prime humans for certain interpretations (reducing variance) or evoke entirely new conceptualizations without blocking preexisting ones (maintaining or expanding variance)? The paper shows context increases both coherence and variation, but the cognitive mechanism remains unclear. Evidence would require experimental designs measuring response times and alternative availability.

### Open Question 2
How do conceptual perspective and scene context interact when communicative effectiveness is required, such as in reference game settings? SceneGram uses unconstrained description without communicative goals, but real language use involves balancing creativity with discriminative utility. Reference game experiments would resolve this.

### Open Question 3
Why do multimodal LLMs fail to capture the richness and variability of human conceptualizations for abstract visual stimuli? The paper demonstrates the problem exists but doesn't identify whether it's due to architectural limitations, training data biases, or decoding strategies. Systematic ablations and probing experiments would resolve this.

## Limitations
- Limited to congruent scene placements; incongruent scenes not tested
- No communicative task constraints to test pragmatic effects on variation
- Only tested off-the-shelf models without controlled ablations or parameter tuning

## Confidence

### High confidence claims
- Scene context increases conceptual coherence while maintaining high variation in human descriptions
- Multimodal LLMs generate less diverse and less human-like conceptualizations than humans
- SHIFT scores validate context effects across multiple embedding methods

### Medium confidence claims
- Tangrams vary in "conceptual flexibility" across different scenes
- Training data biases contribute to LLM output convergence
- 2×2 grid design appropriately balances context provision with stimulus ambiguity

### Low confidence claims
- Scene gist processing operates at specific temporal thresholds
- Alternative conceptual paths remain cognitively available when context primes certain interpretations
- Model architectural differences fully explain output diversity differences

## Next Checks

1. Reproduce SHIFT analysis: Compute coherence scores (CLIP, Numberbatch) for human annotations across all 11 scene conditions; verify in-context > baseline pattern holds

2. Baseline model diversity: Generate 10 samples per item for LLaVA-7b and 72b with nucleus decoding (p=0.5); compare % Top and SND to human baselines

3. Ablate scene strength: Replace 2×2 grid with single scene image (tangram overlaid) to test whether stronger context increases SHIFT scores or reduces variation further