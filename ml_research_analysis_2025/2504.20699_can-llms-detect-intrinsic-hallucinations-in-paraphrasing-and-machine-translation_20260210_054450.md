---
ver: rpa2
title: Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?
arxiv_id: '2504.20699'
source_url: https://arxiv.org/abs/2504.20699
tags:
- hallucination
- prompt
- nstruct
- language
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates open-access LLMs for detecting intrinsic hallucinations
  in paraphrasing and machine translation. The study examines how performance varies
  across target languages, model sizes, instruction tuning, and prompt formulations.
---

# Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?

## Quick Facts
- arXiv ID: 2504.20699
- Source URL: https://arxiv.org/abs/2504.20699
- Reference count: 22
- Primary result: NLI models perform comparably to LLMs for detecting intrinsic hallucinations in paraphrasing, suggesting they are a viable alternative to LLM-based detectors for this task.

## Executive Summary
This paper evaluates open-access large language models (LLMs) for detecting intrinsic hallucinations in paraphrasing and machine translation. The study examines how performance varies across target languages, model sizes, instruction tuning, and prompt formulations. A suite of models including Llama3, Mixtral, EuroLLM, and GPT-SW3 is tested on the HalluciGen task, which involves detecting hallucinations in generated hypotheses. The results show that performance varies across models but remains consistent across prompts, with NLI models emerging as a competitive alternative to LLMs. The strongest models (Mixtral-8x7B-Instruct and Meta-Llama3-70B-Instruct) perform consistently well across all languages and scenarios.

## Method Summary
The study evaluates zero-shot detection of intrinsic hallucinations in two scenarios: paraphrasing and machine translation. HalluciGen datasets provide source-hypothesis pairs in English, Swedish, German, and French. Models receive contrastive prompts asking them to identify which of two hypotheses contains a hallucination. Six prompt formulations are tested, varying terminology (hallucination vs. contradiction/support) and language. Performance is measured using F1 score on test sets, with invalid outputs (not "hyp1" or "hyp2") counted as errors. An NLI baseline using BGE-M3-ZEROSHOT-V2.0 is included for comparison. Generation parameters are fixed (temperature=0.1, top_k=20, max_tokens=5).

## Key Results
- NLI models achieve comparable performance to LLMs in paraphrasing tasks, suggesting they are a viable alternative for this specific task.
- Performance is consistent across different prompt formulations, though prompts mentioning "hallucination" tend to negatively impact certain model families.
- Cross-lingual nature of translation adds complexity, with LLMs generally outperforming NLI baselines except in the en→fr direction.
- Instruction tuning shows benefits primarily for the largest models (Llama3-70B), with mixed effects on smaller models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intrinsic hallucination detection can be reduced to textual entailment classification between source and hypothesis.
- **Mechanism:** NLI models predict whether a hypothesis is entailed by the source. When entailment scores differ between two hypotheses, the lower-entailment hypothesis is flagged as containing a hallucination.
- **Core assumption:** Hallucinations manifest primarily as contradictions or unsupported additions that disrupt entailment relations.
- **Evidence anchors:** NLI baseline performance is competitive across languages; Swedish paraphrase shows NLI outperforming best LLMs.
- **Break condition:** NLI may struggle with nuanced omissions that don't break entailment.

### Mechanism 2
- **Claim:** LLMs gain advantage over NLI in translation tasks through implicit cross-lingual capability combined with entailment reasoning.
- **Mechanism:** For machine translation hallucination detection, LLMs perform implicit translation alongside entailment judgment.
- **Core assumption:** Translation examples in pre-training data provide cross-lingual representations enabling joint operation.
- **Evidence anchors:** LLMs outperform NLI in translation except en→fr; cross-lingual mechanism underexplored.
- **Break condition:** NLI baseline BGE-M3 achieves 0.82-0.88 F1 on translation, suggesting some cross-lingual NLI capability exists.

### Mechanism 3
- **Claim:** Explicit "hallucination" terminology in prompts can degrade detection performance for certain model families.
- **Mechanism:** Models may associate "hallucination" with their own generation failures rather than the formal definition, causing misalignment.
- **Core assumption:** Instruction-tuning corpora don't consistently define "hallucination" in the formal NLI sense.
- **Evidence anchors:** Prompts mentioning "hallucination" tend to negatively impact performance for Mixtral-8x22B-Instruct and some Meta-Llama3 models.
- **Break condition:** Effect is model-family specific; well-tuned large models show robustness to terminology.

## Foundational Learning

- **Concept: Intrinsic vs. Extrinsic Hallucinations**
  - **Why needed here:** The paper explicitly restricts scope to intrinsic hallucinations detectable from input-output pairs alone.
  - **Quick check question:** Given source "Paris is in France" and hypothesis "Paris is in Germany," is this intrinsic or extrinsic? (Answer: Intrinsic—it contradicts the source directly without external knowledge.)

- **Concept: Entailment Relations in NLI**
  - **Why needed here:** The NLI baseline reduces hallucination detection to entailment classification.
  - **Quick check question:** Does "John bought apples" entail "John bought fruit"? Does it entail "John bought only apples"?

- **Concept: Contrastive Evaluation Design**
  - **Why needed here:** The task is framed as choosing between hyp+ and hyp− rather than binary classification on single hypotheses.
  - **Quick check question:** If a model always outputs "hyp1," what F1 score would it achieve on a balanced test set?

## Architecture Onboarding

- **Component map:** Input: (source, hyp1, hyp2) → Prompt Template → LLM/NLI Model → Post-processor → Label (hyp1|hyp2|invalid)
- **Critical path:** Prompt formulation → Generation parameters (temp=0.1, top-k=20, max_tokens=5) → Output post-processing (handle label variations across languages)
- **Design tradeoffs:** LLM vs. NLI (cost vs. flexibility), prompt language (English generally best), instruction tuning (benefits primarily for largest models)
- **Failure signatures:** Invalid outputs (explanations, translations instead of labels), convergence to single label, prompt sensitivity
- **First 3 experiments:**
  1. Replicate NLI baseline on target language pair to establish cross-lingual sufficiency
  2. Test Prompt 4 vs. Prompt 1 to quantify terminology sensitivity
  3. Run small batch and analyze invalid output rate (>10% requires stricter parsing)

## Open Questions the Paper Calls Out

- **Question:** Can LLMs be effectively utilized to generate synthetic datasets for training specialized hallucination detection models?
  - **Basis in paper:** The conclusion states the authors aim to "explore whether LLMs may be used to generate datasets for training and evaluating hallucination detectors."
  - **Why unresolved:** Current study restricts scope to detection capability, not utility as data generators.

- **Question:** Does performance of LLMs in detecting intrinsic hallucinations generalize to low-resource languages?
  - **Basis in paper:** Limitations section notes datasets focus on high-resource languages and suggests future work should expand to include low-resource languages.
  - **Why unresolved:** Experimental results limited to English, Swedish, German, and French.

- **Question:** Does pre-training on translation data provide LLMs with a distinct advantage over NLI models for detecting cross-lingual hallucinations?
  - **Basis in paper:** Authors observe LLMs outperform NLI in translation but not paraphrasing, hypothesizing "implicit translation" skills may be the cause, but state "Further investigation is needed."
  - **Why unresolved:** Paper identifies performance gap but doesn't isolate specific mechanism.

## Limitations

- Model family heterogeneity prevents direct comparison between open-access LLMs and closed-access GPT-4 baseline due to differing evaluation methodologies.
- Cross-lingual NLI capability may be understated if stronger multilingual NLI models exist beyond the tested baseline.
- Prompt terminology sensitivity effects may be overstated as model-family specific rather than methodology-driven.

## Confidence

- **High confidence:** NLI models perform comparably to LLMs for intrinsic hallucination detection in paraphrasing tasks.
- **Medium confidence:** Instruction tuning provides consistent benefits for largest models but shows mixed effects for smaller models.
- **Low confidence:** Cross-lingual entailment reasoning through implicit translation is the primary mechanism for LLM advantage in translation tasks.

## Next Checks

1. Test additional multilingual NLI models (e.g., mDeBERTa, XLM-R) on translation tasks to determine whether LLM advantage persists with stronger cross-lingual entailment models.
2. Systematically test alternative formal definitions of "hallucination" across different model families to quantify terminology choice versus instruction-tuning effects.
3. Conduct detailed analysis of invalid outputs across models to determine whether specific error patterns correlate with model architecture, instruction tuning, or prompt formulation.