---
ver: rpa2
title: 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation
  Framework for Explanations'
arxiv_id: '2511.13081'
source_url: https://arxiv.org/abs/2511.13081
tags:
- explanations
- methods
- saliency
- explanation
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the fundamental ambiguity in saliency map\
  \ interpretation by introducing the Reference-Frame \xD7 Granularity (RFxG) taxonomy.\
  \ The framework distinguishes explanations along two axes: reference frame (pointwise\
  \ vs."
---

# Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations

## Quick Facts
- arXiv ID: 2511.13081
- Source URL: https://arxiv.org/abs/2511.13081
- Authors: Yehonatan Elisha; Seffi Cohen; Oren Barkan; Noam Koenigstein
- Reference count: 20
- Primary result: Introduced RFxG taxonomy with four novel faithfulness metrics showing Iterated Integrated Attributions (IIA) consistently outperforms other methods

## Executive Summary
This paper addresses the fundamental ambiguity in saliency map interpretation by introducing the Reference-Frame Ã— Granularity (RFxG) taxonomy. The framework distinguishes explanations along two axes: reference frame (pointwise vs. contrastive) and semantic granularity (class-level vs. group-level). The authors identify critical limitations in existing evaluation metrics, which predominantly assess pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To address these gaps, they propose four novel faithfulness metrics (CCS, CGC, PGS, CGS) that systematically evaluate explanation quality across both RFxG dimensions using perturbation-based analysis. Comprehensive experiments across ten state-of-the-art methods, four model architectures, and three datasets reveal that Iterated Integrated Attributions (IIA) consistently outperforms other methods, particularly on contrastive and group-level metrics. The framework provides both conceptual foundation and practical tools for developing explanations aligned with user intent and human understanding.

## Method Summary
The RFxG framework evaluates saliency methods using four novel faithfulness metrics: Contrastive Contrastivity Score (CCS), Class-to-Group Contrastivity (CGC), Pointwise Group Score (PGS), and Class-to-Group Score (CGS). These metrics operate on a 2D taxonomy combining reference frame (pointwise: "Why A?" vs contrastive: "Why A not B?") and semantic granularity (class-level: specific category vs group-level: semantic superclass). Evaluation uses perturbation-based analysis where top salient pixels are iteratively masked and the area under the probability change curve is computed. Semantic groups are constructed using WordNet hierarchies with minimum 5 leaf classes per group. The framework was tested across 10 saliency methods, 4 model architectures (ResNet-50, ConvNext-Base, ViT-Base, ViT-Small), and 3 datasets (ImageNet, VOC, COCO).

## Key Results
- Iterated Integrated Attributions (IIA) consistently outperforms other methods across all four proposed metrics
- Saliency methods show significant performance variation across the RFxG dimensions, with some excelling at class-level pointwise tasks but failing on contrastive or group-level evaluations
- Traditional metrics like CAUC show limited correlation with the proposed RFxG metrics, suggesting they miss important aspects of explanation quality
- Semantic grouping via WordNet effectively captures meaningful visual feature hierarchies for evaluation purposes

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Probability Delta
- Claim: The paper proposes that measuring the change in specific probability differences ($f_A - f_B$) during feature removal provides a more robust signal for contrastive explanations than raw probability drops.
- Mechanism: Metrics like the **Contrastive Contrastivity Score (CCS)** do not rely on the absolute confidence of class A, but on the widening or narrowing gap between class A and a counterfactual class B as salient pixels are masked.
- Core assumption: If the saliency map correctly identifies discriminative features, removing them should degrade the model's ability to distinguish A from B, minimizing the probability gap.
- Evidence anchors:
  - [Abstract] "propose four novel faithfulness metrics... using perturbation-based analysis."
  - [Section 4] "computes the AUC of the prediction gap as salient regions are removed... CCS = AUC[$f_A(x^\alpha) - f_B(x^\alpha)$]"
  - [Corpus] Corpus signals regarding "Instance-level quantitative saliency" and "eXIAA" validate the general reliance on perturbation for faithfulness but lack specific validation for this specific probability-delta method.
- Break condition: If the model relies on features outside the image (e.g., background bias) shared by both A and B, the delta may remain unchanged, causing the metric to fail to capture the explanation's quality.

### Mechanism 2: Semantic Hierarchy Alignment (Granularity)
- Claim: The framework argues that explanation faithfulness must be measured relative to semantic granularity (Class vs. Group), as features explaining "Husky" differ from those explaining "Dog."
- Mechanism: **Pointwise Group Score (PGS)** aggregates the probability drop across all sibling classes within a semantic group (defined by WordNet). This rewards explanations that capture generalizable features rather than specific, discriminative textures.
- Core assumption: Classes can be reliably grouped into semantically coherent clusters where shared features exist.
- Evidence anchors:
  - [Section 1] "Granularity: Ranging from fine-grained class-level... to coarse-grained group-level."
  - [Section 5.1] "Semantic groups were constructed using the WordNet hierarchy... group-to-class ontology enabled coarse-to-fine explanations."
  - [Corpus] No direct corpus evidence found for the specific use of WordNet hierarchies in this exact metric configuration.
- Break condition: If the WordNet hierarchy does not align with the visual features learned by the model (e.g., polysemous classes or visually distinct members of the same group), the metric may penalize valid class-level explanations.

### Mechanism 3: Multi-Layer Iterated Integration (IIA)
- Claim: Iterated Integrated Attributions (IIA) performs superiorly on the proposed metrics because it aggregates information across multiple network layers, capturing both fine-grained and abstract features required for the RFxG taxonomy.
- Mechanism: Unlike single-layer methods (like standard Grad-CAM), IIA performs n-fold integration, effectively measuring "flux" through the network's semantic hierarchy.
- Core assumption: Discriminative evidence for complex queries (contrastive + group) is distributed across multiple layers of abstraction.
- Evidence anchors:
  - [Section 5.4] "IIA consistently outperforms the other methods... aligns with IIA's unique design that integrates attributions over multiple intermediate network layers."
  - [Corpus] "Do Sparse Subnetworks Exhibit Cognitively Aligned Attention?" mentions saliency fidelity but does not confirm this specific multi-layer mechanism for IIA.
- Break condition: If the architecture is very shallow or lacks distinct semantic hierarchy, the iterated integration may simply amplify noise without improving contrastive or group-level localization.

## Foundational Learning

- Concept: **Perturbation-based Faithfulness**
  - Why needed here: All four proposed metrics (CCS, CGC, PGS, CGS) rely on masking (perturbing) input pixels and observing the change in model output. Understanding deletion/insertion curves is prerequisite to understanding the paper's evaluation logic.
  - Quick check question: If a saliency map is "faithful," what should happen to the model's confidence for the target class when the top 20% of salient pixels are removed?

- Concept: **Contrastive Explanation**
  - Why needed here: The paper fundamentally splits evaluation into "Pointwise" (Why A?) and "Contrastive" (Why A not B?). Grasping this shift from single-class to relative explanation is essential for the Reference Frame axis.
  - Quick check question: Does a contrastive explanation highlight features that class A possesses, or features that distinguish A from a specific foil class B?

- Concept: **Taxonomy of Granularity**
  - Why needed here: The RFxG framework introduces a granularity axis (Class vs. Group). You must understand that visual features vary by semantic level (e.g., "fur" vs. "wheel") to interpret the results.
  - Quick check question: When explaining a "Sedan," would a Class-level or Group-level explanation focus more on the presence of wheels versus the specific shape of the grille?

## Architecture Onboarding

- Component map:
  Input -> Semantic Layer -> Saliency Generator -> Perturbation Engine -> Metric Calculator

- Critical path:
  1. Mapping target classes to WordNet groups (defining $G$)
  2. Selecting the specific contrastive foil (Class $B$ or Group $G_B$)
  3. Computing the saliency map $M$ relative to the specific query type (pointwise vs. contrastive)
  4. Running the perturbation loop (10% steps) and integrating the area under the curve

- Design tradeoffs:
  - **Masking Strategy**: The paper uses black pixel masking for consistency but notes that alternatives (Gaussian blur, inpainting) may better preserve natural image statistics
  - **Foil Selection**: Selecting the 2nd highest prediction as Class $B$ ensures meaningful contrast but creates a moving target for evaluation
  - **Metric Sensitivity**: Probability-based differences (used here) are more interpretable but potentially less sensitive to large logit changes than multiplicative metrics (like CAUC), though the paper argues this prevents artifacts

- Failure signatures:
  - **Flat CCS Curve**: Suggests the saliency method failed to find discriminative features, or Class $A$ and $B$ are inseparable
  - **High PGS / Low CGC**: Indicates the model explains the broad group well (e.g., "it's a dog") but fails to distinguish specific classes (e.g., "husky vs. malamute")
  - **Saliency Saturation**: If the map is dense (many high values), early perturbation steps (10%) may remove too much context, crashing the score; binarization thresholding is critical here

- First 3 experiments:
  1. **Sanity Check**: Implement **CCS** using a standard Grad-CAM map. Compare the score when contrasting "Dog vs. Cat" (easy) vs. "Husky vs. Malamute" (hard) to verify sensitivity.
  2. **Granularity Ablation**: Run **PGS** (Pointwise Group Score) on a ResNet-50. Compare the score when using a coarse group (e.g., "Vehicle") vs. a fine-grained group (e.g., "Sports Car") to observe the granularity trade-off.
  3. **Method Comparison**: Compare **IIA** vs. **Integrated Gradients (IG)** on the **CGC** metric. The paper predicts IIA will significantly outperform IG on this contrastive-group metric; verify this delta on a small dataset subset (e.g., VOC).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the RFxG taxonomy's organization of explanations along reference-frame and granularity axes actually align with how humans naturally formulate and interpret explanatory questions?
- Basis in paper: [inferred] The paper claims to provide "cognitive human aligned" explanations and "meaningfully aligned with human understanding," yet presents no human subject studies validating that users naturally reason along these axes or find explanations generated under this taxonomy more interpretable.
- Why unresolved: The framework is motivated by cognitive and social science literature on explanation, but empirical validation of human alignment relies entirely on proxy metrics rather than direct human evaluation.
- What evidence would resolve it: Human subject studies comparing user satisfaction, task performance, and interpretability ratings between RFxG-framed explanations and baseline methods.

### Open Question 2
- Question: How can the RFxG framework be extended to evaluate concept-based explanation methods that operate at higher semantic levels than pixel-space saliency maps?
- Basis in paper: [explicit] Section F.3 explicitly states: "an exciting direction involves bridging the gap between low-level visual explanations and high-level conceptual reasoning. The RFxG framework could be extended to evaluate concept-based explanation methods like TCAV by formalizing how concepts operate across reference frames."
- Why unresolved: Current metrics are designed for pixel-space perturbations and do not capture whether concepts themselves exhibit contrastive or granularity-varying behavior.
- What evidence would resolve it: Adapted metrics (e.g., CCG-concept, CGC-concept) applied to concept-based methods, demonstrating discriminative evaluation across reference frames and granularities in concept space.

### Open Question 3
- Question: Are the proposed RFxG metrics robust to different perturbation strategies, or do they inherit artifacts from the specific masking methodology used?
- Basis in paper: [inferred] The paper acknowledges that "black pixel masking... could theoretically introduce distribution shift artifacts" and conducted ablations with alternatives (Gaussian blur, noise baselines), but reports only that "trends remained consistent" without quantitative analysis of metric stability across perturbation types.
- Why unresolved: Comprehensive robustness analysis across perturbation methods is not provided, leaving open whether metric rankings are sensitive to the choice of masking strategy.
- What evidence would resolve it: Systematic comparison of CCS, CGC, PGS, CGS scores across multiple perturbation strategies with statistical tests for method-ranking consistency.

### Open Question 4
- Question: How should RFxG metrics and explanation methods be adapted for domain-specific applications where user needs and appropriate granularity levels differ significantly?
- Basis in paper: [explicit] Section F.4 states: "In medical applications, for example, the appropriate granularity for explanations may differ between clinicians (who might prefer organ-level group explanations) and patients (who might benefit from more fine-grained lesion-specific explanations). Future work should investigate domain-specific adaptations of RFxG."
- Why unresolved: The current framework treats granularity uniformly based on WordNet hierarchies without domain-specific calibration or user-role customization.
- What evidence would resolve it: Domain-specific benchmarks with expert annotations of appropriate granularity levels, and user studies measuring explanation utility across stakeholder groups.

## Limitations
- WordNet-based semantic grouping may not align with visual feature hierarchies learned by models, creating potential semantic-visual mismatch
- Black pixel masking for perturbation may introduce distribution shift artifacts that systematically favor certain explanation methods
- Framework validation relies entirely on proxy metrics without direct human evaluation of explanation interpretability or utility

## Confidence
- RFxG taxonomy conceptual framework: **High** - The two-dimensional structure is logically coherent and addresses well-documented ambiguities in explanation evaluation
- Novel metric formulations (CCS, CGC, PGS, CGS): **Medium** - The mathematical formulations are sound, but their robustness across diverse model architectures and domains requires further validation
- IIA superiority claims: **Medium** - While statistically significant in the reported experiments, the relative performance gains across all RFxG dimensions need replication on additional datasets and model families

## Next Checks
1. **Semantic Hierarchy Validation**: Reconstruct the WordNet groupings used in the paper and verify that the identified groups actually share visual features that would be captured by saliency methods. Test with human annotators to assess semantic coherence.
2. **Perturbation Strategy Sensitivity**: Repeat key experiments using alternative perturbation strategies (Gaussian blur, learned inpainting) to determine if black pixel masking introduces systematic bias in the evaluation.
3. **Cross-Domain Generalization**: Apply the RFxG framework to non-image domains (e.g., text classification) where the semantic hierarchy is better defined, to test whether the taxonomy and metrics transfer beyond computer vision.