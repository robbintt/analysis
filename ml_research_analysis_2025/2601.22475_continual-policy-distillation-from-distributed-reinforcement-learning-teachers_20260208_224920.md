---
ver: rpa2
title: Continual Policy Distillation from Distributed Reinforcement Learning Teachers
arxiv_id: '2601.22475'
source_url: https://arxiv.org/abs/2601.22475
tags:
- learning
- distillation
- continual
- tasks
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a teacher-student framework for continual
  reinforcement learning that separates the process into two independent stages: training
  single-task expert policies via distributed reinforcement learning and distilling
  them into a central generalist model. The central model uses a Transformer backbone
  with a Mixture-of-Experts (MoE) architecture, which allows for incremental expert
  expansion to maintain plasticity over multiple learning stages.'
---

# Continual Policy Distillation from Distributed Reinforcement Learning Teachers

## Quick Facts
- arXiv ID: 2601.22475
- Source URL: https://arxiv.org/abs/2601.22475
- Reference count: 40
- Primary result: Framework achieves >85% teacher performance with <10% task-wise forgetting on Meta-World MT25 benchmark.

## Executive Summary
This paper introduces a teacher-student framework for continual reinforcement learning that separates the process into two independent stages: training single-task expert policies via distributed reinforcement learning and distilling them into a central generalist model. The central model uses a Transformer backbone with a Mixture-of-Experts (MoE) architecture, which allows for incremental expert expansion to maintain plasticity over multiple learning stages. To address catastrophic forgetting, the authors propose a hybrid approach combining prioritized trajectory replay with parameter-wise masking strategies. Extensive experiments on the Meta-World MT25 benchmark show that this framework recovers over 85% of teacher performance while constraining task-wise forgetting to within 10%, demonstrating effective continual learning capabilities.

## Method Summary
The framework decouples continual reinforcement learning into two stages: (1) distributed RL training of 25 single-task expert policies using PPO, and (2) continual distillation of these experts into a central Transformer-MoE student model. The student model incrementally expands its MoE layer by adding one expert per new task, initialized with a large negative gate bias (-5.0) to prevent cold-start interference. Training proceeds in two phases: Phase 1 freezes old experts while training new ones, then Phase 2 freezes the gating network while fine-tuning all experts. A Determinantal Point Process (DPP) selects diverse trajectory subsets (<10%) for replay memory, and shared Transformer layers are frozen after Stage 1 to prevent representation drift. The system uses MSE loss on action means for distillation and contrastive task embeddings instead of one-hot identifiers.

## Key Results
- Recovers over 85% of teacher performance on Meta-World MT25 benchmark
- Constrains task-wise forgetting to within 10% (Backward Transfer)
- Outperforms baseline methods including Elastic Weight Consolidation and Online EWC
- Demonstrates effective continual learning across 5 stages (5 tasks per stage)

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Optimization Complexities
The framework stabilizes Continual RL by isolating the high-variance exploration of RL from the sequential knowledge aggregation of the generalist model. By training single-task "teachers" in parallel via distributed RL, the system solves each task optimally without interference. The central "student" model then learns via policy distillation (supervised learning), which converts the unstable RL problem into a stable trajectory-matching problem.

### Mechanism 2: Isolated Plasticity via Cold-Start Expert Expansion
The model sustains plasticity for new tasks without overwriting shared knowledge by adding "fresh" sub-networks (experts) that are functionally dormant at initialization. When a new task arrives, a new expert is added with gate bias initialized to -5.0, forcing the router to ignore it until it has learned useful representations.

### Mechanism 3: Stability via Hierarchical Freezing and Replay
Catastrophic forgetting is constrained by strictly partitioning the model into "stable" shared feature extractors and "plastic" task-specific experts, reinforced by diverse replay. Shared Transformer layers are frozen after Stage 1, old experts are frozen while new experts train (Phase 1), then the gate is frozen while all experts fine-tune (Phase 2). DPP-based replay maintains coverage of the state space.

## Foundational Learning

- **Concept: Stability-Plasticity Dilemma**
  - Why needed here: This is the core problem the architecture solves. You must understand the trade-off between "locking in" old skills (stability) and "wiring in" new ones (plasticity) to interpret the MoE and masking strategies.
  - Quick check question: If I freeze all parameters in the network, have I solved the stability-plasticity dilemma?

- **Concept: Knowledge Distillation (Behavioral Cloning)**
  - Why needed here: The central model never interacts with the environment directly during the main phase; it learns purely by mimicking teacher outputs. Understanding MSE loss vs. RL loss is critical.
  - Quick check question: Why does the paper use MSE loss on action means instead of KL-divergence for distillation?

- **Concept: Mixture of Experts (MoE) Routing**
  - Why needed here: The system relies on a router (gating network) to direct tokens to specific experts. If the router collapses (always picks one expert), the system fails.
  - Quick check question: What does the "load balancing loss" (auxiliary loss) prevent in an MoE architecture?

## Architecture Onboarding

- **Component map:**
  Input: Observation Sequence + Contrastive Task Embedding → Backbone: Transformer layers (Frozen after Stage 1) → MoE Layer: Gating Network + Sparse Experts → Memory: DPP-selected Replay Buffer

- **Critical path:**
  1. Train Teachers (Distributed RL)
  2. Extract Trajectories & Select Replay via DPP
  3. (If Stage > 1): Expand MoE (Add experts, Init bias = -5.0)
  4. Phase 1: Freeze shared layers + Old Experts; Train New Experts + Gate
  5. Phase 2: Freeze Gate; Train All Experts
  6. Update Replay Buffer

- **Design tradeoffs:**
  - Decoder-only vs. Encoder: The paper chooses Decoder-only for autoregressive history handling, trading off potential parallelism for sequential context
  - Replay Ratio: Uses <10% data to limit storage, trading off potential peak accuracy for memory efficiency
  - Task Embedding: Uses learned embeddings instead of one-hot IDs to support open-ended task streams, trading off exact identification for generalization

- **Failure signatures:**
  - Representation Drift: Performance drops on old tasks implies the "Global Freeze" was not applied or failed to lock features
  - Routing Collapse: The model ignores new experts if the "Cold-Start Bias" is too high or the load-balancing loss is missing
  - Negative Transfer: Tasks with contradictory optimal actions interfere if the Replay Buffer lacks diversity (DPP failure)

- **First 3 experiments:**
  1. Sanity Check (Joint Distillation): Train the student on all tasks simultaneously without continual learning constraints to establish the "upper bound" of the architecture's capacity
  2. Ablation on Masking: Run the 5-stage protocol with "No Shared-Frozen" to quantify the exact contribution of backbone freezing to BWT (Backward Transfer)
  3. Expert Expansion Validation: Compare adding new experts vs. using a fixed static capacity to verify that the "Incremental MoE" is actually necessary for plasticity

## Open Questions the Paper Calls Out

### Open Question 1
Can this framework maintain its efficiency and stability when applied to domains with high-dimensional visual inputs (e.g., Atari) rather than state-based observations? The authors state in the conclusion that experiments were conducted solely on the Meta-World benchmark and "have not yet been validated on other benchmarks, such as Atari."

### Open Question 2
How can the distillation process be modified to handle scenarios where distributed RL teachers fail to converge or provide suboptimal demonstrations? The conclusion identifies that "this paradigm relies heavily on the quality of the teacher models," noting that a teacher failing to learn a policy creates a bottleneck for the central student model.

### Open Question 3
Does freezing the shared Transformer backbone after the initial stage restrict the acquisition of fundamentally different motor primitives or features in later stages? While effective for Meta-World tasks which share similar dynamics, this constraint might limit plasticity if the task stream introduces mechanics or observation distributions drastically different from the first stage.

## Limitations

- The framework's central claim rests on the premise that policy distillation from distributed RL teachers is fundamentally more stable than on-policy RL for CRL, but this is asserted rather than directly tested against alternatives.
- The reported 85% recovery rate and <10% forgetting are impressive but evaluated only on the MT25 benchmark with 25 tasks in 5 stages, limiting scalability claims.
- The system has multiple delicate components (DPP replay selection, gating bias initialization, hierarchical freezing schedule) that are reported to work but not extensively tested for failure modes.

## Confidence

**High Confidence** - The mechanism of decoupling RL from distillation is sound and well-supported. The experimental results on MT25 are robust and the methodology is clearly specified.

**Medium Confidence** - The incremental MoE expansion with cold-start bias effectively addresses plasticity vs. stability, though specific parameter choices are heuristic and not extensively explored.

**Low Confidence** - Claims about "open-ended" task streams and generalizability beyond the benchmark, as the paper demonstrates capability on a fixed 25-task benchmark but doesn't validate performance degradation patterns when scaling task count.

## Next Checks

1. **Alternative teacher comparison** - Run the same continual distillation protocol but replace distributed PPO teachers with self-distillation from a single joint-trained model to isolate whether the distributed teacher approach provides unique benefits.

2. **Stress test on task scaling** - Extend the experiment to 50+ tasks (10+ stages) on MT25 to identify where the incremental MoE approach breaks down, monitoring expert load distribution and performance degradation patterns.

3. **Failure mode exploration** - Systematically perturb the delicate components: (a) vary gate bias initialization from -10.0 to -1.0, (b) disable DPP replay and use uniform sampling, (c) skip Phase 2 masking to quantify the robustness of each component and identify critical failure thresholds.