---
ver: rpa2
title: How role-play shapes relevance judgment in zero-shot LLM rankers
arxiv_id: '2510.17535'
source_url: https://arxiv.org/abs/2510.17535
tags:
- role-play
- role
- ranking
- relevance
- patching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how role-play prompts affect zero-shot
  LLM rankers. Using a modular role-play template with positive and negative descriptors,
  the authors systematically vary role-play phrasing and measure its impact on ranking
  performance.
---

# How role-play shapes relevance judgment in zero-shot LLM rankers

## Quick Facts
- arXiv ID: 2510.17535
- Source URL: https://arxiv.org/abs/2510.17535
- Authors: Yumeng Wang; Jirui Qi; Catherine Chen; Panagiotis Eustratiadis; Suzan Verberne
- Reference count: 35
- Primary result: Role-play prompts affect zero-shot LLM rankers by encoding information in early layers, interacting with instructions in middle layers, and aggregating via specific attention heads in later layers

## Executive Summary
This paper investigates how role-play prompts affect zero-shot LLM rankers through systematic variation of role descriptors and causal intervention techniques. Using a modular role-play template with positive and negative descriptors, the authors measure performance impact and trace how role information propagates through model layers. They identify that role-play information is encoded in early layers, interacts with task instructions in middle layers, and minimally affects query or document representations. The study provides actionable guidance for prompt engineering by revealing which linguistic components (adverbs > adjectives > modals) carry the strongest role-play signal.

## Method Summary
The authors use a modular role-play template with positive and negative descriptors to systematically vary role-play phrasing in zero-shot LLM rankers. They apply causal intervention techniques—specifically activation patching—to trace how role information propagates through model layers and identify attention heads that encode role-conditioned relevance signals. The study uses pointwise relevance judgment and pairwise document comparison tasks on MS MARCO and NQ datasets with LLaMA-3.1-8B, Mistral-7B, and Qwen2.5-7B models. They measure ranking performance via nDCG@10 and use normalized logit difference to quantify activation patching effects.

## Key Results
- Role-play signals are predominantly encoded in early network layers (0-8), largely independent of specific query or document content
- Role-play information influences relevance judgments by interacting with task instructions specifically in middle layers (11-13)
- Specific attention heads in later layers (e.g., L14H24) act as causal aggregators for role-conditioned relevance, and their ablation disrupts ranking output
- Linguistic analysis reveals that adverbs carry the strongest role-play signal, followed by adjectives, with modals having minimal impact
- The effect is stable across models (Mistral-7B, Qwen2.5-7B) and datasets (MS MARCO, NQ)

## Why This Works (Mechanism)

### Mechanism 1
Role-play signals are processed and encoded in early network layers, largely independent of the specific query or document content being ranked. Activation patching on the residual stream reveals that restoring performance requires patching role tokens at early layer inputs, while patching query or document tokens yields negligible recovery, suggesting orthogonal processing pathways for "identity" vs. "content."

### Mechanism 2
Role-play information influences relevance judgments by interacting with task instructions specifically in middle layers. Causal tracing shows that while role signals originate early, they must fuse with instruction tokens (e.g., "rank passages") in middle layers (approx. layers 11-13) to effectively bias the final decision.

### Mechanism 3
Specific attention heads in later layers act as the causal aggregators for role-conditioned relevance, and their ablation disrupts the ranking output. The information flow shifts to the final token position in higher layers, with specific attention heads (e.g., L14H24) showing strong patching effects and their mean-ablation significantly lowering NDCG or correct score.

## Foundational Learning

- **Activation Patching (Causal Tracing)**: Distinguishes correlation from causation by injecting "clean" activations into a "corrupted" run to see if behavior is restored. Quick check: If patching Layer 5 role tokens restores the "Yes" logit in a negative-role run, does this prove Layer 5 causes the role effect, or just correlates with it? (Answer: It provides strong causal evidence for that layer's necessity/sufficiency for that specific signal path).

- **Residual Stream Decomposition**: Analyzes information flow by patching inputs to layers. Understanding that the residual stream is the sum of all previous layer outputs is vital to interpreting why early patching works (signal is fresh) and late patching works (signal is aggregated). Quick check: Why does patching the residual stream at the input to Layer 3 differ from patching the output of Layer 2? (Answer: The input to Layer N includes the accumulated output of all previous layers; timing matters).

- **Logit Difference (LD) Normalization**: Quantifies the "recovery" of the model using normalized LD, which isolates the change in the specific "Yes/No" decision space from general confidence shifts. Quick check: If $LD_{patched} = LD_{corrupted}$, what is the normalized LD, and what does it mean? (Answer: 0; the patch had no effect on restoring the correct answer).

## Architecture Onboarding

- **Component map**: Input (Modular Prompt) -> Early Layers (0-8: Role Encoding) -> Middle Layers (9-16: Integration) -> Late Layers (17-32: Aggregation) -> Output (Binary Logits)

- **Critical path**: Inject role descriptor -> Patch activations at Early Layers to isolate signal origin -> Verify transmission to Middle Layers (Instruction interaction) -> Measure final aggregation at Last Token via specific Heads (L14H24, etc.)

- **Design tradeoffs**: Linguistic granularity vs. mechanistic precision (fixed 3-token slots for alignment) and Pointwise vs. Pairwise (cleaner patching vs. realistic ranking tasks)

- **Failure signatures**: "Bleeding" effects (negative roles sometimes outperforming baselines), minimal effect (normalized LD ≈ 0 everywhere), and unintended interactions in pairwise settings

- **First 3 experiments**:
  1. Baseline replication: Run modular prompt with "talented" vs. "faulty" on LLaMA-3.1-8B-Instruct and measure shift in "Yes" probability
  2. Localization scan: Perform activation patching on residual stream inputs for all layers at "Role" token position and plot normalized LD to confirm peak in early layers
  3. Head ablation test: Identify top-3 contributing attention heads at "Last" token position and mean-ablate them during inference, observing drop in NDCG@10 for positive role

## Open Questions the Paper Calls Out

### Open Question 1
How does role-play influence ranking performance in listwise and setwise LLM ranking paradigms? The authors exclude these approaches due to intricate cross-document attention dependencies that would complicate causal tracing of internal mechanisms.

### Open Question 2
Can adversarial or semantically unrelated role descriptors be used to systematically manipulate or attack LLM rankers? The paper notes that role-play effects need not be semantically tied to ranking expertise, motivating broader mechanistic analysis to prevent unintended or adversarial triggers.

### Open Question 3
How does the role-play mechanism scale to larger models (e.g., 70B+ parameters) and different architectures? The study only tests 7B-8B instruction-tuned models, leaving scaling behavior unknown.

## Limitations
- Architectural generality is uncertain as findings are primarily demonstrated on LLaMA-style decoder architectures
- Prompt engineering constraints (fixed 3-token descriptors) may not reflect real-world practices
- Dataset and domain specificity (MS MARCO, NQ) may limit generalization to other NLP tasks

## Confidence

- **High Confidence**: Causal intervention methodology (activation patching) and early-layer role encoding findings are technically sound and well-validated
- **Medium Confidence**: Specific attention head causality and linguistic analysis (adverbs > adjectives > modals) are supported but require broader validation
- **Low Confidence**: Model stability claims across architectures are based on two similar model families with limited exploration of different attention patterns

## Next Checks

1. **Cross-Architecture Validation**: Apply activation patching methodology to an encoder-decoder model (T5 or BART) to test if early-layer role encoding patterns hold across architectural paradigms

2. **Natural Prompt Generalization**: Replace fixed 3-token template with unconstrained natural language role-play prompts of varying lengths to test if same attention heads and layer patterns emerge

3. **Task Transfer Experiment**: Apply role-play ranking methodology to substantially different task domains (e.g., long-form document summarization) to test mechanism transfer beyond information retrieval contexts