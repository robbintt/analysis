---
ver: rpa2
title: 'P1: Mastering Physics Olympiads with Reinforcement Learning'
arxiv_id: '2511.13612'
source_url: https://arxiv.org/abs/2511.13612
tags:
- physics
- reasoning
- training
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces P1, a family of open-source physics reasoning
  models trained entirely through reinforcement learning. P1-235B-A22B achieves gold-medal
  performance on the 2025 International Physics Olympiad, becoming the first open-source
  model to reach this level, with 12 gold medals out of 13 competitions.
---

# P1: Mastering Physics Olympiads with Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.13612
- Source URL: https://arxiv.org/abs/2511.13612
- Reference count: 28
- Primary result: P1-235B-A22B achieves gold-medal performance on 2025 IPhO (12/13 gold medals)

## Executive Summary
P1 is a family of open-source physics reasoning models trained entirely through reinforcement learning that achieves gold-medal performance on the 2025 International Physics Olympiad. The models use a multi-stage RL framework with adaptive learnability adjustment and training stabilization mechanisms, including Truncated Importance Sampling (TIS) to correct for train/inference engine mismatches. P1 models show strong generalization beyond physics to math, coding, and other reasoning tasks, and can be enhanced with the PhysicsMinions agent framework.

## Method Summary
P1 employs multi-stage Group Sequence Policy Optimization (GSPO) with length-normalized importance ratios on 5,065 curated physics problems filtered to exclude trivial (pass rate > 0.7) and unsolvable (pass rate = 0) questions. The training uses a hybrid verifier: rule-based (SymPy + math-verify) during RL training for precision, and model-based only for validation. Adaptive learnability is achieved by progressively expanding group size (16→32) and generation window (48k→80k) across stages. The framework corrects for train/inference engine mismatches using Truncated Importance Sampling (TIS), and is implemented in slime with Megatron (FSDP) for training and SGLang/vLLM for rollout.

## Key Results
- P1-235B-A22B achieves gold-medal performance on 2025 IPhO (12 gold medals out of 13 competitions)
- P1 models generalize well to math (LiveCodeBench, LiveBench) and coding benchmarks
- Multi-stage RL with adaptive learnability significantly outperforms fixed-configuration training
- Rule-based verifiers prevent reward hacking while model-based verifiers cause length explosion and degraded performance

## Why This Works (Mechanism)

### Mechanism 1
Sustained performance improvement in RL post-training depends on maintaining "learnability" by dynamically filtering data and expanding exploration space. The framework prevents entropy collapse by removing trivial tasks (pass rate > 0.7) and unsolvable ones (pass rate = 0), while progressively increasing group size and generation window to ensure difficult problems yield successful trajectories for learning.

### Mechanism 2
Training stability relies on prioritizing verification precision over recall, making rule-based verifiers superior to model-based judges. Model-based verifiers are susceptible to "hacking" where policies learn verbose styles to trick judges, while rule-based verifiers (SymPy) guarantee high precision and avoid optimization instability from false positive rewards.

### Mechanism 3
Decoupling inference engine (rollout) and training engine (gradient calculation) introduces distributional bias requiring mathematical correction. Truncated Importance Sampling (TIS) re-weights gradients to correct for discrepancies between rollout policy (SGLang/vLLM) and training policy (Megatron), which differ in numerical precision and kernel implementations.

## Foundational Learning

- **Policy Gradient (GSPO)**: Core engine optimizes policy πθ by comparing generated sequences through Group Relative Policy Optimization. Quick check: If all 16 sampled responses are incorrect, advantage is 0/undefined and no gradient update occurs.

- **Entropy Collapse**: Model becomes too confident too early, stopping diverse reasoning path generation needed for hard physics problems. Quick check: Filtering out easy questions (pass rate > 0.7) helps by preventing convergence to single deterministic paths that reduce exploration diversity.

- **Rule-based vs. Model-based Verification**: LLM-based grading during training breaks the system despite working for validation. Quick check: False positives are worse than false negatives because they introduce noise into the reward function, teaching the model to exploit the verifier rather than solve physics.

## Architecture Onboarding

- **Component map**: HiPhO (5,065 problems) → SGLang/vLLM (16-32 samples) → Rule-based verifier (SymPy + math-verify) + Multi-box extraction → Megatron (FSDP) with Slime framework → P1 Model + PhysicsMinions

- **Critical path**: Preliminary Pass Rate Filtering is the critical setup step. Incorrect pass rate calculation to remove "unlearnable" or "trivial" data will cause multi-stage RL to stall in Stage 1.

- **Design tradeoffs**: Rule Verifier Precision vs. Recall trades coverage (misses some valid symbolic forms) for stability (avoids reward hacking). Stages vs. Efficiency adds operational complexity but ensures stable convergence vs single long run.

- **Failure signatures**: Length Hacking shows response length spikes (>20k tokens) with validation score drops, indicating verifier exploitation or loop. Stagnation shows flat validation scores in early steps, suggesting wrong pass rate filter thresholds.

- **First 3 experiments**: 1) Verifier Ablation: Train proxy model using only model-based verifier to reproduce length hacking failure. 2) Curriculum Validity: Compare Fixed Config vs Adaptive Config training dynamics to validate necessity of expanding group size/window. 3) TIS Ablation: Run training with and without TIS to quantify stability gap.

## Open Questions the Paper Calls Out

### Open Question 1
How can model-based verifiers be designed to maintain high precision while expanding recall, without being susceptible to policy hacking? The paper notes current models are prone to hacking and false positives, leading to length explosion and degraded validation performance.

### Open Question 2
What shared structural similarities or reasoning mechanisms enable physics-focused RL post-training to improve performance in unrelated domains like coding and mathematics? The paper hypothesizes refined reasoning trajectories and structural similarities but does not confirm the mechanism.

### Open Question 3
How can Test-Time Reinforcement Learning (TTRL) be effectively applied to benchmarks containing multiple-choice questions without inducing reward hacking via inflated majority consensus? The paper filtered out multiple-choice questions because they tend to inflate majority consensus ratios, leading to reward hacking.

### Open Question 4
Can the "general reasoning amplifier" effect observed from physics Olympiad training be replicated using other specialized domains with small datasets, or is it unique to the symbolic rigor of physics? The paper demonstrates generalization but does not validate if training on other specialized domains would yield similar cross-domain improvements.

## Limitations
- Dataset and training configuration transparency lacking - HiPhO benchmark described but problem set not publicly linked
- Stage transition schedules unspecified - step counts for switching between stages not provided
- TIS truncation parameter C not specified - critical for claimed stability improvement
- Gold-medal claims hinge on single year (2025 IPhO) without validation on other years or competitions

## Confidence

- **Gold-medal performance on IPhO 2025**: Medium - based on reported results but no independent verification or alternative year tests
- **Adaptive learnability prevents entropy collapse**: Medium - supported by ablation but causal link is inferred
- **Rule-based verifier prevents reward hacking**: Medium - explicitly shown in ablation but threshold not quantified
- **TIS correction is necessary for stability**: Low - supported by theoretical justification but lacks ablation showing degradation without TIS

## Next Checks

1. **Verifier Ablation Reproduction**: Train small RL model on physics subset using only model-based verifier to confirm "length hacking" failure mode (response length >20k tokens, validation score drops).

2. **Curriculum Validation**: Implement and compare Fixed Config vs Adaptive Config training dynamics on held-out physics problems to validate necessity of expanding group size and window.

3. **TIS Ablation Test**: Run full training pipeline with and without Truncated Importance Sampling to quantify stability gap and measure if training without TIS leads to divergence or gradient variance issues.