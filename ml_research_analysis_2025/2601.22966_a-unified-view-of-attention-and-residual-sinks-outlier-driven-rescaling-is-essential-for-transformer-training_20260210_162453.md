---
ver: rpa2
title: 'A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is
  Essential for Transformer Training'
arxiv_id: '2601.22966'
source_url: https://arxiv.org/abs/2601.22966
tags:
- attention
- outliers
- arxiv
- rescaling
- sinks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and studies two types of outliers in LLMs\u2014\
  attention sinks and residual sinks\u2014which arise at normalization layers and\
  \ jointly rescale non-outlier components. Removing normalization or clipping outliers\
  \ degrades performance, while introducing explicit gating-based rescaling (GatedNorm)\
  \ or absorbing outliers into parameters reduces them without loss in accuracy."
---

# A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training

## Quick Facts
- **arXiv ID**: 2601.22966
- **Source URL**: https://arxiv.org/abs/2601.22966
- **Reference count**: 32
- **Primary result**: Introduces GatedNorm to reduce outliers in LLMs, yielding 2-point training gains and 1.2-point W4A4 quantization robustness improvements across 2B-24B models.

## Executive Summary
This paper identifies and studies two types of outliers in LLMs—attention sinks and residual sinks—which arise at normalization layers and jointly rescale non-outlier components. Removing normalization or clipping outliers degrades performance, while introducing explicit gating-based rescaling (GatedNorm) or absorbing outliers into parameters reduces them without loss in accuracy. The approach yields 2-point gains in standard training and 1.2-point gains in low-bit (W4A4) quantization robustness. Results are validated across dense and MoE models of 2B–24B parameters, trained on 120B–1T tokens.

## Method Summary
The paper proposes GatedNorm, a low-rank gating mechanism that replaces standard RMSNorm after the normalization layer. The gating network applies element-wise sigmoid scaling via a two-layer bottleneck (rank r=16) to provide explicit rescaling. An alternative approach, PreAffine, absorbs sink functionality into learnable parameters before normalization. The method is evaluated on 2B-24B dense and MoE transformer models trained on 120B-1.2T tokens, measuring both standard performance and W4A4 quantization robustness using SmoothQuant.

## Key Results
- GatedNorm reduces outlier magnitudes while maintaining or improving accuracy compared to baseline transformers
- W4A4 quantization degradation drops from ~10 points to <5 points with GatedNorm
- PreAffine reduces dynamic outliers but keeps them in normalization computation
- Removing normalization or clipping outliers causes training divergence or severe performance loss

## Why This Works (Mechanism)

### Mechanism 1: Outlier-Driven Rescaling via RMSNorm
Large activations (residual sinks) in specific dimensions inflate the RMSNorm denominator, which reduces the normalized magnitude of non-outlier features. The network learns to generate these outliers specifically to control signal magnitude through statistical rescaling.

### Mechanism 2: Explicit Gating as a Functional Substitute
GatedNorm introduces a learnable, low-rank gating mechanism after normalization that learns rescaling factors, removing the need for the model to generate extreme outliers. A small network learns a sigmoid-gated scaling vector applied to normalized output.

### Mechanism 3: Parameter Absorption (PreAffine)
Residual sink functionality can be absorbed into a learnable affine parameter vector preceding normalization. A vector amplifies specific dimensions before RMSNorm, guaranteeing a large input to trigger rescaling regardless of actual activation magnitude.

## Foundational Learning

**Concept: RMSNorm Dynamics**
- Why needed here: To understand why a "sink" dimension scales down other dimensions. The normalization denominator aggregates statistics across all dimensions; one massive value skews the scale for everyone.
- Quick check question: If you double the value of a single "sink" dimension in the input vector to RMSNorm, what happens to the output values of the non-sink dimensions? (Answer: They decrease).

**Concept: Attention Sinks vs. Residual Sinks**
- Why needed here: The paper unifies two distinct phenomena. Attention sinks are token-based (Softmax); Residual sinks are dimension-based (RMSNorm). Distinguishing them is critical for targeted mitigation.
- Quick check question: Does a residual sink appear in a specific token or a specific hidden dimension across tokens? (Answer: Hidden dimension).

**Concept: Quantization Robustness (W4A4)**
- Why needed here: The primary practical motivation. Outliers destroy low-bit quantization accuracy because they force high scaling factors that crush the precision of normal values.
- Quick check question: Why does a single activation value of 6000 hurt 4-bit quantization for values around 1.0? (Answer: It forces a large scale factor, leaving few bits to represent the fractional part of the small values).

## Architecture Onboarding

**Component map**: Input -> PreAffine (Optional) -> Norm (RMSNorm) -> GatedNorm -> Output

**Critical path**: Implementing GatedNorm correctly. The gating must be element-wise (not tensor-wise) and use a sigmoid activation to ensure bounded stability. Rank r is typically small (e.g., 16).

**Design tradeoffs**:
- GatedNorm: Best performance/quantization; adds ~2% parameters and ~5% latency (overhead decreases with scale)
- PreAffine: Cheaper than GatedNorm (just a vector); reduces dynamic outliers but keeps them in norm computation; slightly less quantization robust than GatedNorm
- Clipping: Zero cost; destroys accuracy (Do Not Use)

**Failure signatures**:
- Training Divergence: Occurs if DyT is used without explicit gating
- Loss Spikes: Occurs if outlier clipping is too aggressive without compensating rescaling mechanisms
- No Improvement: Occurs if GatedNorm uses tensor-wise gating or unbounded activations

**First 3 experiments**:
1. Sanity Check (Clipping): Apply activation clipping at 100 to 2B baseline; confirm divergence or massive loss degradation
2. GatedNorm Ablation: Train 2B model with GatedNorm (Rank 16) vs. Baseline; plot peak activation magnitude and Loss
3. Quantization Stress Test: Run W4A4 quantization eval on GatedNorm checkpoint; verify <5pt degradation vs ~10pt for Baseline

## Open Questions the Paper Calls Out

**Open Question 1**: Why is outlier-driven rescaling necessary for effective training and representation learning in transformers? The authors state they do not investigate why such rescaling is necessary for effective training or representation learning.

**Open Question 2**: What is the optimal rank for GatedNorm's low-rank gating mechanism, and how does rank interact with model scale? The paper fixes the gating rank at r=16 across all experiments without ablation or justification.

**Open Question 3**: Why does sigmoid activation specifically outperform other bounded activations (tanh, SiLU) for gating-based rescaling? The paper observes sigmoid outperforms alternatives but offers no mechanistic explanation.

**Open Question 4**: Do the findings on outlier-driven rescaling generalize to model architectures beyond the transformer variants tested? The paper validates on dense transformers and MoE models but does not test other architectures like state-space models, RWKV, or pure linear attention models.

## Limitations

- Empirical scope limited to Llama3-8B and Qwen2-7B models, though results generalize to 2B-24B dense and MoE architectures
- GatedNorm adds ~2% parameters and ~5% latency, which may be costly for production deployment at scale
- Results are reported on web-scale text corpora; effectiveness for code, multilingual, or long-context domains is not explicitly validated

## Confidence

- **High Confidence**: Empirical observation that outliers co-occur with normalization layers and their removal causes training instability or divergence; quantization robustness gains (1.2 points W4A4) are directly measured and reproducible
- **Medium Confidence**: Proposed mechanisms are logically consistent with data and related work, but functional necessity of outliers is inferred from ablations rather than direct causal intervention
- **Low Confidence**: Claim that residual sinks are "largely static" and can be fully absorbed into parameters is weakly supported; PreAffine reduces but does not eliminate outliers in residual stream

## Next Checks

1. **Sink Dimension Ablation**: Nullify only the top-K outlier dimensions before RMSNorm while preserving all other dimensions; compare training stability and loss to full GatedNorm and full clipping

2. **Cross-Architecture Generalization**: Apply GatedNorm to a non-transformer architecture (e.g., RWKV or Mamba) with different normalization scheme; measure if outliers appear and if GatedNorm provides similar benefits

3. **Static vs. Dynamic Sink Characterization**: Measure correlation of sink dimension activations across different input sequences; high correlation would support "static" claim, low correlation would suggest input-dependent sinks