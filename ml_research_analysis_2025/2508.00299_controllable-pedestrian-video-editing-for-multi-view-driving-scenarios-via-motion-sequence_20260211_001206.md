---
ver: rpa2
title: Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via
  Motion Sequence
arxiv_id: '2508.00299'
source_url: https://arxiv.org/abs/2508.00299
tags:
- pedestrian
- video
- generation
- multi-view
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a diffusion-based video editing framework for
  controllable pedestrian generation in multi-view driving scenarios, addressing the
  limitation of insufficient training data for rare but safety-critical pedestrian
  situations in autonomous driving systems. The core method employs dynamic pedestrian
  region cropping and resizing to standardize scales across views, followed by video
  inpainting with pose sequence control to enable flexible editing functionalities
  including insertion, replacement, and removal.
---

# Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence

## Quick Facts
- arXiv ID: 2508.00299
- Source URL: https://arxiv.org/abs/2508.00299
- Authors: Danzhen Fu; Jiagao Hu; Daiguo Zhou; Fei Wang; Zepeng Wang; Wenhua Liao
- Reference count: 40
- Primary result: 3D mAP improvements from 0.4427 to 0.4577 across all BEV distance thresholds for pedestrian detection via BEVFormer

## Executive Summary
This paper presents a diffusion-based video editing framework for controllable pedestrian generation in multi-view driving scenarios, addressing the limitation of insufficient training data for rare but safety-critical pedestrian situations in autonomous driving systems. The core method employs dynamic pedestrian region cropping and resizing to standardize scales across views, followed by video inpainting with pose sequence control to enable flexible editing functionalities including insertion, replacement, and removal. The framework ensures spatiotemporal coherence and cross-view consistency through a dedicated multi-view processing pipeline that preserves geometric alignment across synchronized camera feeds.

## Method Summary
The method uses CogVideoX v1.5 as the base diffusion model, processing dynamic pedestrian regions through bounding box expansion (1.6×), resizing to 480×240, and tiling six views in a 2×3 arrangement. Pose sequences from DWPose guide motion generation via concatenated latents, while inpainting masks preserve background regions. The system supports text-based clothing color control through CLIP encoder cross-attention, and outputs are seamlessly blended back into original frames for evaluation via 3D pedestrian detection mAP.

## Key Results
- 3D mean Average Precision (mAP) improvements from 0.4427 to 0.4577 across all BEV distance thresholds
- Gains range from 0.0051 to 0.0251 depending on threshold (0.5m, 1m, 2m, 4m)
- Qualitative results show realistic pedestrian manipulation across multi-view scenarios with successful clothing color control and seamless background reconstruction

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Scale Normalization via Crop-and-Resize
Standardizing pedestrian scales across diverse camera views improves diffusion-based generation quality by addressing truncation of close-range pedestrians and insufficient detail for distant subjects. The method dynamically expands detection bounding boxes by a fixed ratio, crops pedestrian regions at varying native scales, then resizes all crops to a uniform resolution (480×240 pixels).

### Mechanism 2: Pose Sequence Conditioning for Motion Control
Minimalist 2D skeletal keypoints provide sufficient control for pedestrian motion synthesis in driving contexts where pedestrians occupy small frame portions. DWPose extracts skeletal keypoints from each view, which are morphologically dilated for robustness, encoded into motion latents via a keypoint encoder, and concatenated with inpainting latents in the DiT backbone.

### Mechanism 3: Multi-View Consistency via Composite Tiling
Arranging multiple camera views as spatial tiles in a single composite image enables the diffusion model to learn cross-view geometric relationships. Using LiDAR-based 3D annotations and camera calibration parameters, pedestrian bounding boxes are projected to all views, cropped/normalized, and arranged in a grid (e.g., 2×3 for six cameras).

## Foundational Learning

- **Video Inpainting with Mask-Based Conditioning**: Why needed here: The framework generates pedestrians only in masked regions while preserving unmasked background. Quick check question: Why does masking pedestrian regions (vs. full-frame replacement) matter for background preservation, and how does the 1.6× bounding box expansion affect seam quality?

- **ControlNet/Adapter Architectures for Diffusion**: Why needed here: The method injects pose sequences via a ControlNet-like pathway. Quick check question: How does concatenating pose latents with noisy latents in the DiT architecture differ from fine-tuning the base model, and what flexibility does this preserve?

- **BEV Perception and 3D Detection Metrics**: Why needed here: Downstream evaluation uses BEVFormer and 3D mAP with BEV center-distance thresholds. Quick check question: Why does 3D pedestrian detection use center distance thresholds (0.5m, 1m, 2m, 4m) in BEV space rather than 2D IoU, and what does mAP improvement actually indicate about synthetic data quality?

## Architecture Onboarding

- **Component map**: LiDAR annotations + camera calibration → 3D-to-2D projection → bbox expansion (1.6×) → dynamic crop → resize to 480×240 → multi-view tile (960×720 for 2×3) → DWPose extraction → morphological dilation → keypoint encoder → motion latents → inpainting encoder (binary mask) → VAE encode unmasked regions → control latents → DiT backbone (CogVideoX v1.5) → concatenate [noise + pose latents + inpainting latents] → denoising → VAE decode → CLIP text encoder → cross-attention injection

- **Critical path**: Calibration accuracy → 3D projection precision → Dynamic cropping correctness → DWPose reliability on small/distant subjects → Multi-view pose consistency → Composite generation quality → Seamless blending back to original frames

- **Design tradeoffs**: 2D skeleton vs. SMPL/multi-modal pose (chose simplicity and cross-view focus; sacrifices fine detail control); Uniform scale vs. native resolution (chose consistent 480×240 for all; trades natural depth cues for generation stability); Composite tiling vs. per-view processing (chose implicit consistency via spatial proximity; limits flexibility for arbitrary camera counts)

- **Failure signatures**: Truncated limbs/distorted proportions (Dynamic crop expansion ratio insufficient for close-range subjects); Inconsistent appearance across views (Calibration drift or pose projection misalignment); Background artifacts after removal (Mask expansion too narrow for smooth blending gradients); No mAP improvement despite visual quality (Generated motion distributions may not match realistic pedestrian behavior)

- **First 3 experiments**: 1. Dynamic cropping validation: Compare fixed-size vs. dynamic cropping on held-out close/distant pedestrians; measure FID and pose accuracy stratified by distance category; 2. Pose representation ablation: Test 2D skeleton vs. SMPL-based on small subset; evaluate visual quality, cross-view consistency, and inference cost; 3. Synthetic data ratio sensitivity: Train BEVFormer with varying synthetic data proportions (10%, 25%, 50%, 75%) to identify optimal augmentation threshold before potential distribution drift harms performance

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the methodology and evaluation suggest several important unresolved issues regarding generalization, multi-pedestrian scenarios, and robustness to upstream errors.

## Limitations
- Reliance on LiDAR-based 3D annotations limits applicability to datasets without such annotations, potentially restricting real-world deployment where only camera-based 3D perception exists
- The 1.6× bounding box expansion ratio appears arbitrary and may not generalize across different camera resolutions or pedestrian scales
- Evaluation focuses solely on pedestrian detection improvement without assessing potential negative impacts on other perception tasks like traffic sign or vehicle detection

## Confidence
- **High confidence**: The multi-view composite tiling approach for maintaining geometric consistency across synchronized camera feeds
- **Medium confidence**: The dynamic cropping and resizing strategy's effectiveness for standardizing scales across views
- **Low confidence**: The claim that minimalist 2D skeletal keypoints provide sufficient control for pedestrian motion synthesis in driving contexts

## Next Checks
1. **Calibration robustness test**: Systematically vary camera calibration error parameters (extrinsic rotation/translation, intrinsic distortion) to quantify the impact on cross-view consistency and 3D mAP improvement degradation rates
2. **Cross-task transfer evaluation**: Measure the impact of synthetic pedestrian editing on non-pedestrian perception tasks (vehicle, traffic sign, lane detection) to identify potential negative transfer effects on the overall perception system
3. **Real-world deployment validation**: Test the framework on a dataset without LiDAR annotations, such as Waymo Open Dataset, using only camera-based 3D pedestrian detection to evaluate practical deployment limitations