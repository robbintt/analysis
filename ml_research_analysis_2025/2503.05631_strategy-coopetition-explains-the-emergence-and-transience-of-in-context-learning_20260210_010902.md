---
ver: rpa2
title: Strategy Coopetition Explains the Emergence and Transience of In-Context Learning
arxiv_id: '2503.05631'
source_url: https://arxiv.org/abs/2503.05631
tags:
- ciwl
- learning
- training
- layer
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergence and transience of in-context
  learning (ICL) in transformers. The authors reproduce the finding that ICL can disappear
  after long training times and discover that the asymptotic strategy is a hybrid
  approach they call "context-constrained in-weights learning" (CIWL).
---

# Strategy Coopetition Explains the Emergence and Transience of In-Context Learning

## Quick Facts
- arXiv ID: 2503.05631
- Source URL: https://arxiv.org/abs/2503.05631
- Authors: Aaditya K. Singh; Ted Moskovitz; Sara Dragutinovic; Felix Hill; Stephanie C. Y. Chan; Andrew M. Saxe
- Reference count: 40
- Primary result: ICL emerges transiently because it shares Layer 2 sub-circuits with the asymptotically preferred CIWL strategy, a phenomenon termed "strategy coopetition."

## Executive Summary
This paper investigates why in-context learning (ICL) emerges and then disappears during transformer training. The authors reproduce the finding that ICL is transient, peaking early then being replaced by a hybrid strategy they call "context-constrained in-weights learning" (CIWL). Through mechanistic analysis, they discover that ICL and CIWL share sub-circuits in Layer 2, leading to both cooperative and competitive interactions - a phenomenon they term "strategy coopetition." They develop a minimal mathematical model that captures these dynamics and use it to identify data properties that enable persistent ICL. The key insight is that ICL emerges not because it's optimal but because it's both useful and "on the path" to the asymptotic CIWL strategy.

## Method Summary
The authors use 2-layer attention-only transformers (d_model=64, 8 heads/layer) trained on few-shot classification tasks using Omniglot images embedded via a fixed ImageNet-pretrained ResNet18. The task presents 2 context exemplar-label pairs followed by a query exemplar, with the model predicting the label for the query. Training uses bursty sequences where one context exemplar matches the query class. Four out-of-distribution evaluators measure different strategies: ICL (novel label mappings), IWL (query not in context), CIWL (correct label but wrong pairing), and Flip (ICL vs CIWL preference). The authors also develop a minimal mathematical model and test interventions like clamping Layer 2 weights and using matched-exemplar data.

## Key Results
- ICL emerges transiently during training, peaking around 3-10M sequences then being replaced by CIWL by ~20M sequences
- The asymptotic strategy is "context-constrained in-weights learning" (CIWL) implemented via Layer 2 skip-trigram-copiers
- ICL and CIWL share Layer 2 sub-circuits, leading to both cooperation (shared circuits) and competition (Layer 1 specialization)
- The mathematical model captures the dynamics of strategy coopetition and identifies data properties enabling persistent ICL
- Matched-exemplar data (where context exemplar = query exemplar) enables persistent ICL by making it the asymptotically optimal strategy

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning (ICL) via Induction Heads
ICL emerges transiently because it shares Layer 2 sub-circuits with the asymptotically preferred CIWL strategy. Layer 1 heads act as "previous token heads," copying information forward, while Layer 2 "induction heads" attend to tokens preceded by the current token, enabling pattern completion: `[A*][B*] ... [A] → [B]`. The induction head circuit forms first and can be repurposed. If Layer 1 cannot learn previous-token attention patterns (e.g., with certain positional embedding schemes), ICL will not emerge.

### Mechanism 2: Context-Constrained In-Weights Learning (CIWL) via Skip-Trigram-Copiers
The asymptotic strategy is a hybrid requiring both in-weights knowledge (exemplar→label mapping) AND context (label token present). Layer 2 heads learn "skip-trigram" patterns: attend from query to the correct label token regardless of which context exemplar it's paired with, then copy to output. This requires substantial K- and V-composition with Layer 1. If context exemplars always match query exemplars exactly, ICL becomes asymptotically preferred and CIWL doesn't dominate.

### Mechanism 3: Strategy Coopetition via Shared Sub-Circuits
ICL and CIWL simultaneously cooperate (shared Layer 2) and compete (Layer 1 specialization), explaining transient ICL emergence despite non-optimality. Three requirements: (1) ICL is useful for loss reduction, (2) ICL is "on the path" to CIWL via shared L2 induction heads, (3) ICL emerges faster than CIWL forms completely. Competition in Layer 1 eventually drives ICL→CIWL transition. If CIWL forms too quickly (e.g., fewer classes, fewer exemplars per class), ICL never emerges.

## Foundational Learning

- **Concept: Induction Heads (Olsson et al., 2022)**
  - Why needed here: ICL mechanism is built on induction head circuits; understanding their 2-layer composition (previous-token head → induction head) is prerequisite.
  - Quick check question: Can you trace how a 2-layer attention-only transformer implements `[A][B] ... [A] → [B]`?

- **Concept: K-/Q-/V-Composition (Elhage et al., 2021)**
  - Why needed here: CIWL mechanism depends critically on Layer 1 outputs influencing Layer 2 keys and values; ablation evidence shows this is non-optional.
  - Quick check question: If you ablate Layer 1→Layer 2 value connections but preserve attention patterns, what happens to task performance?

- **Concept: Skip-Trigram Mechanisms**
  - Why needed here: CIWL is implemented as distributed skip-trigram-copiers; this differs from standard induction head patterns.
  - Quick check question: How does `... [label] ... [query] → [label]` differ from standard induction `... [A][B] ... [A] → [B]`?

## Architecture Onboarding

- **Component map:** ResNet18-pretrained Omniglot image encoder (fixed) → 64-dim learned embedding → 2-layer attention-only transformer (8 heads/layer) → learned absolute positional embeddings → output prediction
- **Critical path:** Layer 1 heads must learn to either (a) attend to previous token (enabling ICL) OR (b) attend to self (enabling CIWL). Layer 2 induction heads form early and remain semantically stable; strategy switch is driven entirely by Layer 1 dynamics.
- **Design tradeoffs:** Attention-only vs. MLP: Attention-only gives cleaner mechanistic interpretation; MLPs introduce resurgent ICL and more IWL. Positional embeddings: Learned absolute required for 2-layer; sinusoidal/RoPE fail. Model scale: Larger models (12-layer with MLPs) show longer transience timescales but same qualitative pattern.
- **Failure signatures:** ICL never emerges: Check if #classes too low (<1600) or #exemplars too few (<5)—CIWL forms too fast. ICL doesn't fade: Check if context/query exemplars are matched exactly—this makes ICL asymptotic. No learning at all: Check positional embedding type; verify Layer 1→Layer 2 composition is intact.
- **First 3 experiments:** 1) Reproduce transience curve: Train standard 2-layer attention-only model on bursty Omniglot data; plot all 4 evaluators to verify ICL peak then decline. 2) Clamp L2 weights intervention: Train on ICL-only data from scratch vs. with L2 weights clamped from end-of-CIWL training. Verify clamped version learns faster. 3) Test persistence intervention: Train on matched-exemplar data vs. standard bursty; verify matched version shows persistent ICL.

## Open Questions the Paper Calls Out

### Open Question 1
What is the mechanistic cause of the transient non-monotonic "divot" observed in the loss of the asymptotic (CIWL) strategy during the intermediate phase of training? The authors note in Section 6 that "we do not fully understand what leads to this brief divot" in the loss curves (Fig 5), despite it appearing in both the toy model simulations and the actual transformer training dynamics. A detailed analysis of the gradient flow or head-specific ablations during the specific training window where the divot occurs could resolve this.

### Open Question 2
Can the hypothesis that Layer 2 attention heads store skip-trigrams in superposition be validated using interpretability tools like Sparse Autoencoders (SAEs)? The authors speculate in Section 8 and Appendix C.2 that "the compression of 12800 class-specific skip-trigrams into 8 attention heads... could motivate research on an analog of sparse autoencoders... for attention heads." Applying dictionary learning or SAEs specifically to the outputs of the attention heads could extract monosemantic features corresponding to the class-specific skip-trigrams.

### Open Question 3
Does the "strategy coopetition" dynamic (specifically the sharing of Layer 2 sub-circuits between ICL and in-weights strategies) persist in standard transformer architectures containing MLP layers? The authors restricted their primary mechanistic analysis to 2-layer attention-only transformers. In Appendix B.4, they report that adding MLPs led to complex behaviors that made "asymptotic behaviors... difficult to claim," leaving the interaction in standard architectures unresolved. Replicating the specific progress measure experiments (freezing L1 vs L2) in a model with MLPs could test if the shared sub-circuit mechanism survives.

## Limitations

- The analysis is based on a highly controlled synthetic setup (few-shot classification on Omniglot), and the extent to which "strategy coopetition" generalizes to natural language tasks remains unclear.
- The model is attention-only (no MLPs), and while MLP variants are discussed in Appendix B.4, the core insights may not directly transfer to models with MLPs.
- The analysis focuses on a specific training timescale (~2e7 sequences), though Appendix B.4 suggests similar patterns in larger models with longer timescales.

## Confidence

**High Confidence**: The observation that ICL emerges transiently and is later replaced by CIWL is robustly reproducible. The core mechanism of skip-trigram-copiers in Layer 2 implementing CIWL is well-supported. The mathematical model capturing coopetition dynamics provides reasonable qualitative predictions.

**Medium Confidence**: The claim that ICL is "on the path" to CIWL via shared Layer 2 sub-circuits relies on specific training dynamics that may vary with architecture or optimization choices. The explanation for why ICL persists on matched-exemplar data is supported but the underlying inductive biases could be more thoroughly characterized.

**Low Confidence**: The quantitative predictions of the minimal mathematical model (particularly the specific timescales) are less reliable, as they depend on simplifying assumptions about gradient descent dynamics. The assertion that larger models show "the same qualitative pattern" with longer timescales is based on brief Appendix mention rather than systematic study.

## Next Checks

1. **Cross-task validation**: Test the ICL→CIWL transition on a natural language few-shot task (e.g., sentiment classification with context exemplars) to verify the mechanism generalizes beyond Omniglot.

2. **Alternative architectures**: Validate the mechanism in transformers with MLPs (not just attention-only) and with different positional encoding schemes to test robustness to architectural variations.

3. **Early intervention experiment**: During the ICL dominance phase (~3-10M sequences), forcibly disable Layer 1 heads' ability to attend to self while preserving previous-token attention. If ICL is truly "on the path" to CIWL, this intervention should prevent the ICL→CIWL transition.