---
ver: rpa2
title: 'SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through
  Claim Decomposition and Editing'
arxiv_id: '2506.04583'
source_url: https://arxiv.org/abs/2506.04583
tags:
- evidence
- claim
- retrieval
- sucea
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SUCEA, a training-free framework for adversarial
  fact-checking that addresses the challenge of retrieving evidence for claims adversarially
  crafted to deceive fact-checking systems. The framework segments adversarial claims
  into independent sub-claims, iteratively retrieves evidence while editing claims
  based on retrieved evidence, and aggregates evidence to predict entailment labels.
---

# SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing

## Quick Facts
- arXiv ID: 2506.04583
- Source URL: https://arxiv.org/abs/2506.04583
- Reference count: 13
- Primary result: 7.5% accuracy boost on FOOL METWICE using Llama-3.1-70B

## Executive Summary
This paper introduces SUCEA, a training-free framework for adversarial fact-checking that addresses the challenge of retrieving evidence for claims adversarially crafted to deceive fact-checking systems. The framework segments adversarial claims into independent sub-claims, iteratively retrieves evidence while editing claims based on retrieved evidence, and aggregates evidence to predict entailment labels. Experiments on FOOL METWICE and WICE datasets demonstrate significant improvements over baselines, with 7.5% accuracy boost on FOOL METWICE using Llama-3.1-70B.

## Method Summary
SUCEA employs a three-module pipeline: claim segmentation and decontextualization via LLM prompts, iterative retrieval and claim editing (retrieve evidence, edit claim using evidence as context, retrieve again for two rounds), and evidence aggregation with entailment prediction. The framework uses LLMs like GPT-4o-mini, Llama-3.1-70B/8B, Mistral 9B, and Gemma-2-9B with retrievers TF-IDF and Contriever. The evidence-guided claim editing module is critical, with ablation showing 6-8% retrieval accuracy drops without it. The framework operates without training, relying instead on few-shot prompting strategies detailed in the appendix.

## Key Results
- 7.5% accuracy boost on FOOL METWICE using Llama-3.1-70B
- Retrieval accuracy improvements of up to 11.0% for TFIDF-based retrieval
- Consistent improvements across different backbone LLMs (Llama-3.1-70B/8B, Gemma-2-9B)
- Evidence-guided claim editing and claim segmentation both critical to performance

## Why This Works (Mechanism)

### Mechanism 1: Evidence-Guided Query Reformulation
- Claim: Retrieving partially relevant evidence and using it to guide claim rewriting improves subsequent retrieval more effectively than direct paraphrasing alone.
- Mechanism: The first retrieval round surfaces partial lexical hints (entity names, domain vocabulary) from imperfect matches; these hints constrain LLM editing toward evidence-aligned phrasing rather than random paraphrase directions, increasing lexical overlap for the second retrieval pass.
- Core assumption: Partially relevant evidence contains signal that LLMs can extract and apply; without this guidance, LLMs lack direction for productive reformulation.
- Evidence anchors:
  - [abstract] "iteratively retrieves evidence and edits the subclaim based on the retrieved evidence"
  - [section 5.1] "paraphrasing without evidence reduces accuracy from 34.5% to 28.5%... removing the evidence during editing leads to significant performance drops, especially with TFIDF where RAcc decreases from 33.5% to 26.0%"
  - [corpus] Weak corpus evidence—related work (DECEIVE-AFC, Retrieve-Refine-Calibrate) addresses adversarial fact-checking but does not validate evidence-guided editing specifically.
- Break condition: When initial retrieval returns completely irrelevant evidence, editing amplifies noise rather than signal, degrading performance.

### Mechanism 2: Decomposition as Attention Focus
- Claim: Segmenting multi-fact claims into atomic sub-claims reduces retrieval distraction and enables targeted evidence matching per fact unit.
- Mechanism: Complex claims encode multiple facts; retrievers attending to all simultaneously retrieve evidence that addresses only a subset. Segmentation isolates each fact, reducing competition and increasing retrieval precision per unit.
- Core assumption: Retrievers perform better on single-fact queries than multi-fact composites; atomic units have cleaner lexical signatures.
- Evidence anchors:
  - [abstract] "segments adversarial claims into independent sub-claims"
  - [section 3.1] "verifying all sub-claims simultaneously is challenging... the top-ranked evidence may address only a few fact units"
  - [section 5.1] "removing claim segmentation decreases RAcc by 7.0%"
  - [corpus] DecMetrics and Retrieve-Refine-Calibrate employ decomposition paradigms, suggesting general utility across fact-checking frameworks.
- Break condition: Over-segmentation creates atomic facts lacking sufficient context for retrieval (error analysis identifies this as 22/50 failure cases).

### Mechanism 3: Iterative Convergence
- Claim: Multiple retrieval-editing rounds compound improvements, with most gains in early iterations.
- Mechanism: Each round incorporates newly retrieved evidence to refine query formulation, progressively aligning claims with retriever-friendly vocabulary and structure.
- Core assumption: Partial improvements are cumulative; early iterations capture low-hanging fruit while later rounds provide marginal refinement.
- Evidence anchors:
  - [appendix A.1] "with more rounds of evidence retrieval and claim editing, the performance steadily improves... the most substantial gains are observed in the early iterations, particularly after two iterations"
  - [corpus] MERMAID uses iterative retrieval and reasoning for veracity assessment, supporting iterative approaches for complex claims.
- Break condition: LLM parametric knowledge leakage introduces hallucination during editing (15/50 errors), degrading later iterations if not constrained.

## Foundational Learning

- Concept: Retrieval-Augmented Language Models (RALM)
  - Why needed here: SUCEA is an RALM enhancement targeting the failure mode where adversarial queries break lexical/semantic matching.
  - Quick check question: Why does TFIDF fail when adversarial claims deliberately reduce lexical overlap with evidence?

- Concept: Adversarial NLP Attacks
  - Why needed here: The framework specifically targets claims crafted using omission, paraphrasing, and synonym substitution to evade retrieval.
  - Quick check question: Name three adversarial strategies claim writers use to reduce retriever effectiveness.

- Concept: Constrained Prompt Engineering
  - Why needed here: The editing module uses explicit constraints to prevent LLMs from injecting parametric knowledge (achieving 6% hallucination rate vs. unconstrained baselines).
  - Quick check question: What prompt-level constraints reduce LLM hallucination when editing claims based on retrieved evidence?

## Architecture Onboarding

- Component map: Claim Segmentation & Decontextualization -> Iterative Retrieval & Claim Editing -> Evidence Aggregation & Label Prediction
- Critical path: Retrieval accuracy drives fact-checking accuracy. The editing module is highest-leverage: ablation shows -6% to -8% RAcc drop without it for TFIDF. Segmentation provides -3.5% to -7% drop protection.
- Design tradeoffs:
  - Dense retrievers (Contriever) outperform TFIDF (+5% fact-checking accuracy) but benefit less from editing (more robust embeddings vs. lexical matching).
  - More iterations improve recall but increase latency and hallucination risk; 2 iterations capture most gains.
  - Constrained prompts reduce hallucination but may under-correct if evidence is weak.
- Failure signatures:
  - Over-segmentation: Sub-claims lack context → irrelevant retrieval (22/50 errors).
  - Parametric leakage: LLM adds ungrounded information → wrong retrieval direction (15/50 errors).
  - Overgeneration: Editing adds all evidence content → query drift (13/50 errors).
- First 3 experiments:
  1. Baseline ablation: Run SUCEA with/without editing on TFIDF to quantify the -6% to -8% RAcc gap; validate evidence guidance mechanism.
  2. Retriever comparison: Compare Contriever vs. TFIDF under identical SUCEA settings to assess editing necessity per retriever type.
  3. Iteration sweep: Test 1-5 retrieval-editing rounds to identify point of diminishing returns and hallucination onset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the SUCEA framework generalize effectively to other reasoning-intensive retrieval domains, such as retrieving relevant code for user queries?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that future work should explore "other types of reasoning-intensive retrieval tasks, such as retrieving relevant code for user queries."
- **Why unresolved:** The current study only validates the framework on fact-checking datasets (FOOL METWICE and WICE).
- **What evidence would resolve it:** Successful application and evaluation of the SUCEA pipeline on non-fact-checking benchmarks requiring complex reasoning, such as code retrieval datasets.

### Open Question 2
- **Question:** Can fine-tuning LLMs or developing alternative prompting strategies successfully mitigate the specific failure cases of "too fine-grained segmentation" and "overgeneration"?
- **Basis in paper:** [explicit] The authors note their prompting method is imperfect and suggest "future work can explore other prompting methods or fine-tune LLMs to better control the LLM generations."
- **Why unresolved:** The current implementation relies on specific prompts that sometimes produce atomic facts lacking context or excessive content, leading to retrieval failure.
- **What evidence would resolve it:** A comparative study showing that fine-tuned models or revised prompts significantly reduce the error rates identified in the qualitative analysis (Figure 4) compared to the current few-shot prompting approach.

### Open Question 3
- **Question:** Does SUCEA maintain its performance advantage when integrated with state-of-the-art retrievers like BGE-EN-ICL?
- **Basis in paper:** [explicit] The authors acknowledge they did not cover state-of-the-art retrievers like BGE-EN-ICL due to computing resource limitations and suggest future work explore integrating them.
- **Why unresolved:** It is unclear if the framework's editing mechanism provides diminishing returns when used alongside highly advanced retrieval models that may already handle adversarial features well.
- **What evidence would resolve it:** Experimental results comparing SUCEA's performance using standard retrievers (TF-IDF, Contriever) versus SOTA retrievers (BGE-EN-ICL) on the same benchmarks.

## Limitations

- Framework's effectiveness depends heavily on initial retrieval quality—when adversarial claims produce completely irrelevant evidence in first round, evidence-guided editing cannot function
- Optimal segmentation granularity remains dataset-dependent and under-specified despite consistent gains
- Experiments only validate on two datasets with limited sample sizes (200-358 claims total), limiting scalability claims

## Confidence

- High confidence: Core mechanism that evidence-guided claim editing improves retrieval over direct paraphrasing (supported by 5.5-6.5% RAcc drops without evidence guidance)
- Medium confidence: Decomposition benefits (consistent gains but optimal granularity under-specified)
- Low confidence: Scalability claims (only two datasets with limited sample sizes validated)

## Next Checks

1. Test SUCEA's performance when initial retrieval returns zero relevant evidence (by injecting adversarial noise into claims) to measure degradation when evidence-guided editing cannot function
2. Conduct ablation studies varying both segmentation granularity and editing constraints simultaneously across different retriever types (TFIDF vs. Contriever) to quantify interaction effects
3. Validate framework robustness on out-of-domain adversarial claim datasets with different adversarial strategies (e.g., semantic equivalence attacks) to test generalizability beyond studied datasets