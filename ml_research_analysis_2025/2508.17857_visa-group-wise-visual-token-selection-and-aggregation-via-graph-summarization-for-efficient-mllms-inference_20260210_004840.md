---
ver: rpa2
title: 'VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization
  for Efficient MLLMs Inference'
arxiv_id: '2508.17857'
source_url: https://arxiv.org/abs/2508.17857
tags:
- visual
- tokens
- token
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of inference in multimodal
  large language models (MLLMs) caused by excessive visual tokens. The authors propose
  a novel method called group-wise Visual Token Selection and Aggregation (VISA) that
  combines graph-based visual token aggregation (VTA) with a group-wise token selection
  strategy (GTS).
---

# VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference

## Quick Facts
- arXiv ID: 2508.17857
- Source URL: https://arxiv.org/abs/2508.17857
- Reference count: 40
- Key outcome: Achieves 2.08× speedup on LLaVA-1.5-13B while maintaining 98.14% of original performance

## Executive Summary
This paper addresses the inefficiency of inference in multimodal large language models (MLLMs) caused by excessive visual tokens. The authors propose a novel method called group-wise Visual Token Selection and Aggregation (VISA) that combines graph-based visual token aggregation (VTA) with a group-wise token selection strategy (GTS). VTA treats each visual token as a node in a graph constructed based on semantic similarity, then aggregates information from less important tokens into important ones using graph summarization. GTS progressively divides visual tokens into kept and removed sets guided by text tokens from the final layers of each group. Extensive experiments on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks demonstrate that VISA consistently outperforms previous methods.

## Method Summary
VISA is a two-component method for efficient MLLM inference that reduces visual token count without fine-tuning. The first component, Graph-based Visual Token Aggregation (VTA), constructs a cosine similarity graph where each visual token is a node, then aggregates information from "removed" tokens into "kept" tokens using normalized adjacency matrices. The second component, Group-wise Token Selection (GTS), divides LLM layers into groups and applies VTA at each group's end, using averaged attention from the last text token over final layers to select kept tokens. The method progressively compresses tokens throughout inference, with hyperparameters including α=0.1 for aggregation strength, group size S (5-7 layers), and retention ratios that produce target final token counts.

## Key Results
- Achieves 2.08× speedup on LLaVA-1.5-13B while maintaining 98.14% of original performance
- Outperforms previous methods (ToMe, FastV, TokenMerge) across multiple benchmarks
- Demonstrates effectiveness on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA models
- Maintains high accuracy on diverse tasks including VQAv2, TextVQA, and video QA datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating information from removed tokens into kept tokens via a similarity graph preserves semantic density better than simple feature averaging or pruning.
- **Mechanism:** VISA constructs an adjacency matrix $G$ based on cosine similarity between visual tokens. It then propagates feature information from "removed" tokens ($x^r$) to "kept" tokens ($x^k$) using a normalized subgraph $\hat{G}_A$, updating kept tokens as $x^k = x^k + \alpha \cdot \hat{G}_A x^r$. This effectively treats kept tokens as "super-nodes" summarizing their neighborhood.
- **Core assumption:** The assumption is that semantically similar visual tokens (high cosine similarity) encode redundant local details that can be mathematically summarized into a single vector without losing the global semantic required for the instruction.
- **Evidence anchors:** [abstract] "VTA treats each visual token as a node... It then aggregates information from removed tokens into kept tokens based on this graph." [section 3.2] Equation (5) shows the explicit update rule using the subgraph matrix $\hat{G}_A$ and scaling factor $\alpha$. [corpus] The paper contrasts this with ToMe (averaging), noting ToMe fails in detailed tasks; related work like *TopV* also explores pruning but VISA focuses on the *aggregation* topology.
- **Break condition:** This mechanism likely fails if the similarity graph connects unrelated visual elements (e.g., distinct objects with similar color histograms), causing "semantic bleeding" where distinct objects are incorrectly merged.

### Mechanism 2
- **Claim:** Using the attention scores of the final text token over a group of layers provides a more stable signal for token importance than single-layer attention.
- **Mechanism:** The Group-wise Token Selection (GTS) strategy averages the attention scores ($At2v$) of the last text token (the query) over the final $L$ layers of a group. This averaged score serves as the importance indicator $I$ to select top-$k$ tokens to keep.
- **Core assumption:** The assumption is that the last text token (often the query or instruction) consistently attends to relevant visual regions across multiple layers, and averaging this attention reduces noise from single-layer artifacts.
- **Evidence anchors:** [abstract] "...guided by text tokens from the final layers of each group. This strategy progressively aggregates visual information..." [section 3.3] Equation (6) defines indicator $I$ as the average attention of the last text token across $L$ layers and $H$ heads.
- **Break condition:** If the user prompt is ambiguous or irrelevant to the image content, the "last text token" attention may be a poor guide, causing the model to discard visually critical tokens that simply weren't explicitly queried.

### Mechanism 3
- **Claim:** Progressive compression at the end of layer groups (rather than every layer or once at the start) balances computational savings with the model's capacity to adapt to reduced token sets.
- **Mechanism:** The LLM layers are divided into groups. VTA is applied only at the end of each group. This allows the LLM to process the full (or larger) token set for several layers to extract high-level features before reducing the token count for subsequent layers.
- **Core assumption:** This assumes that early layers require dense token interactions to establish global context, while later layers can operate on compressed "summary" tokens without performance degradation.
- **Evidence anchors:** [section 3.3] "Different from FastV... or every layer like ToMe, VTA only aggregates and prunes tokens at the last layer of each group in a progressive manner." [section 3.3] "The initial group comprises the first two layers to facilitate global information extraction..."
- **Break condition:** If the group size $S$ is too large, the model wastes computation on redundant tokens in the middle layers; if too small, the model may prune tokens before the semantic context is fully established.

## Foundational Learning

- **Concept: Graph Signal Processing (Filtering/Smoothing)**
  - **Why needed here:** The VISA method essentially implements a one-step graph convolution/filter where "removed" nodes propagate their features to "kept" nodes. Understanding how adjacency matrices normalize and propagate signals is key to debugging why information might be lost or amplified.
  - **Quick check question:** If two visual tokens are perfectly orthogonal (similarity 0), does the aggregation equation (5) allow them to influence each other?

- **Concept: Cross-Attention in Transformers (Text-to-Visual)**
  - **Why needed here:** The method relies on "text-guided" selection. You must understand that in a decoder-only MLLM, the text tokens attend to visual tokens via the self-attention mechanism (concatenated sequence), and this attention distribution is the proxy for "relevance."
  - **Quick check question:** In a causal mask setup, can a visual token attend to a text token that appears later in the sequence?

- **Concept: Token Merging vs. Token Pruning**
  - **Why needed here:** The paper explicitly differentiates itself from pruning (dropping) and simple merging (averaging). Understanding the distinction between deleting a dimension vs. adding its vector to another is necessary to evaluate the "information loss" claims.
  - **Quick check question:** Does simple averaging (ToMe) preserve the norm or the direction of the resultant vector differently than the graph-based summation used in VISA?

## Architecture Onboarding

- **Component map:** Input Image -> Visual Encoder -> Projector -> Visual Tokens ($x^{vis}$) -> LLM Backbone (divided into N Groups) -> VTA Module (at group boundaries) -> Aggregator (Graph Constructor -> Selector -> Aggregator -> Pruner) -> Output

- **Critical path:** The *Selector* logic is the most brittle part. It depends on extracting the attention map from the specific layers in the preceding group. If the attention hook fails or the tensor shapes change (e.g., different sequence lengths during batching), the selection index alignment will break.

- **Design tradeoffs:**
  - **Alpha ($\alpha$):** Controls how much "removed" token information leaks into the "kept" tokens. High $\alpha$ risks drowning out the original features of the kept tokens; low $\alpha$ approximates simple pruning.
  - **Group Size ($S$):** Smaller groups = more frequent compression = higher speedup but higher risk of early information loss.
  - **Retention Ratio ($p$):** Standard tradeoff. The paper suggests dynamic $p$ values per group to stabilize FLOPs reduction.

- **Failure signatures:**
  - **Performance collapse on TextVQA:** Indicates the aggregation mechanism (graph similarity) is merging text regions incorrectly (likely due to high visual similarity of text backgrounds), suggesting $\alpha$ is too high or graph edges should be sparser.
  - **Slower than baseline:** The overhead of constructing the graph ($O(n^2)$) and matrix multiplication for aggregation is higher than the savings from reduced sequence length (likely if $p$ is too high or sequence length is small).

- **First 3 experiments:**
  1. **Sanity Check (ToMe vs. VTA):** Implement the VTA module in isolation (layer-wise) on a small ViT/LLM block. Compare "averaging" vs. "graph aggregation" on a simple retrieval task to verify that graph aggregation preserves semantics better than averaging.
  2. **Ablation on GTS:** Run VISA on LLaVA-1.5-7B with two settings: (A) Single-layer pruning (FastV style) vs. (B) Group-wise selection. Keep FLOPs constant and measure accuracy delta on VQAv2.
  3. **Hyperparameter Sweep:** Fix the model and benchmark (MMBench). Vary $\alpha \in [0.0, 0.5]$ and Group Size $S \in [4, 8]$ to find the Pareto front of Speed vs. Accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does VISA perform compared to other leading token reduction methods (specifically SparseVLM or PyramidDrop) on video understanding tasks? [Basis: The authors explicitly limit video comparisons to FastV due to implementation gaps.] [Evidence needed: Benchmark results comparing VISA against SparseVLM and PyramidDrop on video datasets like TGIF-QA and ActivityNet-QA.]

- **Open Question 2:** Does the computational overhead of dynamic graph construction become a latency bottleneck when scaling to input sequences with significantly higher visual token counts (e.g., >3000 tokens) than tested? [Basis: Section 3.4 claims minimal overhead but acknowledges O((pn)²) complexity, tested only up to 2880 tokens.] [Evidence needed: Profiling data of VTA module execution time and memory consumption on inputs ranging from 3000 to 10,000+ visual tokens.]

- **Open Question 3:** Does the Group-wise Token Selection (GTS) strategy's reliance on the "last text token" for importance scoring degrade performance on multi-turn dialogues? [Basis: Section 3.3 defines importance indicator using "the attention score from the last text token to the visual token."] [Evidence needed: Ablation study on a multi-turn conversational benchmark comparing current "last token" strategy against aggregated attention score from all text tokens.]

## Limitations

- The similarity graph construction may cause "semantic bleeding" where distinct objects with similar visual features are incorrectly merged, with no thorough investigation of failure cases
- Reliance on text token attention for visual token selection may fail when user prompts are ambiguous or irrelevant to image content
- Computational overhead of constructing similarity graphs (O(n²)) and matrix multiplications may offset savings, particularly for smaller models or high retention ratios

## Confidence

**High Confidence:** The core architectural contribution (graph-based aggregation + group-wise progressive compression) is well-specified and the experimental results on standard benchmarks are reproducible. The comparison against baselines (ToMe, FastV, TokenMerge) is methodologically sound.

**Medium Confidence:** The claims about semantic preservation during aggregation are supported by benchmark results but lack detailed analysis of what specific information is lost during the process. The ablation studies provide reasonable evidence but don't exhaustively explore the design space.

**Low Confidence:** The scalability claims to Video-LLaVA and the generalization across diverse tasks are based on limited experiments. The paper doesn't provide sufficient analysis of failure modes or demonstrate effectiveness on challenging edge cases like text-heavy images or videos with rapid scene changes.

## Next Checks

**Check 1: Semantic Preservation Analysis**
Implement visualization tools to track which visual tokens are kept vs. removed across different image types (text-heavy, object-centric, scene understanding). Measure whether the aggregation mechanism correctly preserves semantic entities by computing object detection accuracy on retained tokens compared to ground truth bounding boxes.

**Check 2: Ablation on Graph Construction**
Compare VISA's graph-based aggregation against simpler alternatives: (A) No aggregation (pure pruning), (B) Feature averaging (ToMe), and (C) Random aggregation. Fix the token selection mechanism and measure the marginal benefit of the graph topology to isolate its contribution.

**Check 3: Robustness to Prompt Quality**
Test VISA on intentionally mismatched prompt-image pairs where the text query is unrelated to visual content. Measure whether the attention-based selection mechanism still preserves critical visual information or becomes biased toward the irrelevant prompt, and compare against a prompt-agnostic selection baseline.