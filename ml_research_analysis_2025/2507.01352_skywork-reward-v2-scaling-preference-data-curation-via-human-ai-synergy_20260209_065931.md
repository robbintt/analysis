---
ver: rpa2
title: 'Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy'
arxiv_id: '2507.01352'
source_url: https://arxiv.org/abs/2507.01352
tags:
- reward
- arxiv
- preference
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the brittleness of current open reward models
  in reinforcement learning from human feedback (RLHF), which often fail to capture
  nuanced and sophisticated human preferences due to limitations in preference datasets.
  To solve this, the authors introduce SynPref-40M, a large-scale preference dataset
  of 40 million pairs, and develop a human-AI synergistic two-stage pipeline for scalable
  data curation.
---

# Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy

## Quick Facts
- arXiv ID: 2507.01352
- Source URL: https://arxiv.org/abs/2507.01352
- Reference count: 36
- Achieves state-of-the-art performance across seven major benchmarks with human-AI synergistic curation pipeline

## Executive Summary
This paper addresses the brittleness of current open reward models in RLHF by introducing SynPref-40M, a 40 million-pair preference dataset curated through a human-AI synergistic pipeline. The authors develop a two-stage process: Stage 1 combines human verification with LLM annotation guided by adaptive retrieval of similar examples, while Stage 2 uses a gold reward model to automatically curate data at scale. Training eight reward models (0.6B-8B parameters) on a 26 million-pair curated subset, Skywork-Reward-V2 achieves state-of-the-art performance across seven major benchmarks, demonstrating that high-quality curation—not just data scale—drives reward model effectiveness.

## Method Summary
The method involves a two-stage human-AI synergistic curation pipeline. Stage 1 uses human annotators to verify quality while LLMs perform scalable annotation guided by human preferences and adaptive retrieval of similar verified examples. Stage 2 employs LLMs to curate data at scale using consistency checks with a gold reward model, without human involvement. The final models are trained on a curated subset of 26 million pairs using Bradley-Terry pairwise loss with Llama/Qwen backbones (0.6B-8B parameters), global batch size 10,240, and constant learning rate 3e-6 for one epoch.

## Key Results
- Achieves state-of-the-art performance across seven major benchmarks including RewardBench, PPE, RMB, and RM-Bench
- Demonstrates strong alignment with human preferences, objective correctness, safety, and resistance to stylistic biases
- Shows best-of-N scaling and confirms effectiveness stems from both data scale and high-quality curation through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Human-AI Synergistic Curation
Combining human verification for quality with LLM annotation for scalability produces higher-quality preference data than either approach alone. Human annotators with proper tools and structured guidelines can identify nuanced preference signals that LLMs systematically miss. When human annotators lack domain expertise or annotation costs become prohibitive at scale, this mechanism breaks down.

### Mechanism 2: Quality Enables Scaling
Preference data scaling yields performance improvements only when data is properly curated; uncurated data shows negligible gains. The discriminability of chosen vs. rejected responses matters more than raw data quantity. When curation becomes overly selective or introduces systematic annotation biases, this mechanism fails.

### Mechanism 3: Error-Driven Adaptive Retrieval
Focusing annotation on samples where current reward models perform poorly accelerates learning efficiency. Similarity in (conversation, attributes) embedding space correlates with similar reward model failure modes. When embedding similarity fails to capture true preference-equivalence or error patterns are inconsistent across iterations, this mechanism breaks.

## Foundational Learning

- **Concept: RLHF Pipeline and Reward Model Role**
  - Why needed here: Understanding where reward models fit in LLM post-training (between SFT and PPO/RL optimization) contextualizes why data quality matters.
  - Quick check question: Given a policy π and reward model r, what quantity does PPO maximize?

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: All Skywork-Reward-V2 models use this pairwise objective; understanding p(y_w ≻ y_l) = σ(r(x,y_w) − r(x,y_l)) is essential.
  - Quick check question: If r(x,y_w) = 2.5 and r(x,y_l) = 1.0, what probability does BT predict?

- **Concept: Best-of-N Evaluation**
  - Why needed here: RMB and PPE-Correctness use BoN to measure downstream utility; this is the practical test of reward model quality.
  - Quick check question: In BoN, you have N candidate responses and a reward model—how do you select the best?

## Architecture Onboarding

- **Component map:** Seed with human-verified preference pairs → Train RM on silver data, evaluate on gold → Retrieve similar samples to errors, re-annotate → Iterate Stage 1 until convergence → Run Stage 2 consistency filtering at scale → Train final models on curated 26M pairs

- **Critical path:** 1. Seed with human-verified preference pairs; 2. Train RM on silver data, evaluate on gold; 3. Retrieve similar samples to errors, re-annotate; 4. Iterate Stage 1 until convergence; 5. Run Stage 2 consistency filtering at scale; 6. Train final models on curated 26M pairs

- **Design tradeoffs:** Human annotation quality vs. scalability/cost; curation strictness vs. data retention (40M → 26M); 8B model performance vs. inference efficiency for RLHF deployment; multi-benchmark generalization vs. single-benchmark overfitting

- **Failure signatures:** High RewardBench but poor downstream BoN performance (over-optimization); large Normal→Hard gap on RM-Bench (style bias); inconsistent predictions on semantically similar pairs (embedding failure); safety/helpfulness tradeoff collapse

- **First 3 experiments:** 1. Reproduce Figure 8 ablation: no curation vs. LLM-only vs. human-only vs. human+adaptive retrieval (measure avg. score across 6 benchmarks); 2. Implement Stage 2 consistency filtering: train gold RM on human-verified data, apply to unverified pool, measure retention rate and quality improvement; 3. Small-scale scaling test: train on 300K curated vs. 300K random samples from the same pool, compare convergence and final performance

## Open Questions the Paper Calls Out

### Open Question 1
Can frontier LLMs enable fully automatic preference data curation without human verification? The pipeline required human-AI synergy because simple LLM curation failed to improve quality, but budget constraints prevented testing the latest reasoning models. Evaluation of a data curation pipeline run solely on frontier reasoning models (e.g., o-series, DeepSeek-R1) against human-verified gold standards would resolve this.

### Open Question 2
Does explicitly labeling the differences between response pairs improve reward model performance? Current datasets often ignore the specific indication of preference (the difference), potentially causing reward models to learn from underspecified responses. Training ablations comparing datasets with explicit difference annotations against standard chosen/rejected labels on identical base data would resolve this.

### Open Question 3
What is the true quality and utility of the preference pairs discarded during consistency filtering? The authors empirically observed gains from "recycling" this data by flipping labels, but did not verify if the noise outweighed the signal. Human evaluation of the discarded pool to identify error rates and distribution biases, followed by controlled re-training on verified subsets, would resolve this.

## Limitations
- The effectiveness of human-AI synergy heavily depends on the quality of human annotation protocols and tool usage, which are only partially specified in the appendix
- The adaptive retrieval mechanism's impact is based on internal ablation studies; independent validation would require reproducing the similarity embedding space and error-pattern detection logic
- The claim that "training on 1.8% of a 16M mixture outperforms previous SOTA" lacks comparison against other curated datasets of similar scale

## Confidence

- **High confidence**: Bradley-Terry loss implementation, training hyperparameters (batch size, LR, epochs), and the general two-stage pipeline structure are clearly specified and reproducible
- **Medium confidence**: The qualitative improvements from human-AI synergy and adaptive retrieval are supported by internal ablations, but external validation is limited by missing dataset details
- **Low confidence**: The exact composition and provenance of SynPref-40M, and the full human annotation protocol, are not publicly available, limiting full independent replication

## Next Checks
1. **Ablation replication**: Independently reproduce Figure 8—compare no curation vs. LLM-only vs. human-only vs. human+adaptive retrieval on a held-out gold set, measuring avg. score across 6 benchmarks
2. **Curation filtering test**: Implement Stage 2 consistency filtering with a gold RM on a small, publicly available preference dataset; measure retention rate and downstream quality improvement
3. **Scaling sanity check**: Train models on 300K curated vs. 300K random samples from the same pool; compare convergence speed and final performance to verify quality-over-quantity claims