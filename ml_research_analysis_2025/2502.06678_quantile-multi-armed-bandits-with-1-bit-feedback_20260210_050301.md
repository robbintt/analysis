---
ver: rpa2
title: Quantile Multi-Armed Bandits with 1-bit Feedback
arxiv_id: '2502.06678'
source_url: https://arxiv.org/abs/2502.06678
tags:
- algorithm
- quantile
- bound
- instance
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of identifying the arm with the
  highest quantile reward in a multi-armed bandit setting, under a 1-bit communication
  constraint where the agent can only send a single bit of feedback per arm pull.
  The proposed algorithm uses noisy binary search as a subroutine to estimate quantile
  rewards through threshold queries, enabling the learner to compute confidence intervals
  without direct reward observations.
---

# Quantile Multi-Armed Bandits with 1-bit Feedback

## Quick Facts
- arXiv ID: 2502.06678
- Source URL: https://arxiv.org/abs/2502.06678
- Reference count: 40
- Primary result: Achieves logarithmic sample complexity scaling in quantile bandit problems under 1-bit feedback, with minimal communication overhead compared to full-feedback setting.

## Executive Summary
This paper studies the problem of identifying the arm with the highest q-quantile reward in a multi-armed bandit setting under a severe 1-bit communication constraint. The authors propose an algorithm that uses noisy binary search as a subroutine to estimate quantile rewards through threshold queries, enabling the learner to compute confidence intervals without direct reward observations. The key theoretical contribution is showing that the sample complexity scales logarithmically with λ/ε (where λ bounds the quantile rewards and ε is the tolerance), which is tight to within logarithmic factors under mild conditions. The paper also characterizes which problem instances are solvable, showing that essentially all "solvable" instances have positive arm gaps under their novel definition.

## Method Summary
The algorithm operates in a fixed-confidence best arm identification setting where the learner can only receive 1-bit threshold feedback ("Is reward ≤ γ?") from an agent. It uses a successive elimination approach where each arm maintains lower and upper confidence bounds on its quantile value. The core subroutine, QuantEst, implements noisy binary search over a discretized grid [0, λ] to estimate these bounds. For each round, the algorithm calls QuantEst twice per active arm to update confidence bounds, eliminates arms whose upper confidence bounds fall below the maximum lower confidence bound of other arms, and terminates when some arm's lower confidence bound exceeds all others' upper confidence bounds by the tolerance threshold. The discretization parameter c controls the fineness of the search space and affects both the number of solvable instances and the sample complexity.

## Key Results
- Achieves sample complexity that scales logarithmically with λ/ε, a key improvement over prior linear scaling in similar settings
- Shows that 1-bit feedback has minimal impact on the fundamental scaling of sample complexity
- Proves a theorem of the alternative: instances with positive arm gaps are solvable with finite samples, while zero-gap instances are fundamentally unsolvable by any algorithm
- Provides matching upper and lower bounds showing logarithmic tightness (within factors of logarithmic or constant) under various conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An arm with the highest q-quantile reward can be identified using only 1-bit feedback per arm pull.
- Mechanism: The learner sends a threshold query ("Is reward $r \le \gamma$?") to an agent. The agent replies with a single bit. The learner uses these bits in a noisy binary search subroutine to estimate quantiles and build confidence intervals without ever observing the reward directly. This effectively turns a continuous-value estimation problem into a sequence of 1-bit decision problems.
- Core assumption: The $q$-quantile of each arm's reward distribution is bounded within a known range $[0, \lambda]$.
- Evidence anchors:
  - [abstract] "...the communication from an agent... is restricted to only one bit of feedback per arm pull. We propose an algorithm that utilizes noisy binary search as a subroutine..."
  - [PAGE 4, Section 2.1] "Specifically, the communication to the learner is restricted to one bit of feedback per arm pull... We will focus on the threshold query model..."
  - [corpus] The paper "Does Feedback Help in Bandits with Arm Erasures?" (arXiv:2504.20894) also explores a distributed MAB problem over constrained networks, confirming the relevance of communication constraints, though with a different model.
- Break condition: If an arm's true quantile lies outside the assumed range $[0, \lambda]$ or if the agent cannot execute the threshold comparison, the mechanism fails.

### Mechanism 2
- Claim: The sample complexity scales logarithmically with the ratio of the quantile bound to the tolerance ($\lambda/\epsilon$).
- Mechanism: The algorithm discretizes the search space $[0, \lambda]$. Noisy binary search over this space has a complexity that grows with the logarithm of the size of the search space. The tolerance $\epsilon$ helps define the grid's fineness. This is a key theoretical improvement over prior work on mean-based bandits which showed linear scaling with $\lambda$.
- Core assumption: The problem has a positive arm gap ($\Delta > 0$).
- Evidence anchors:
  - [abstract] "The authors derive an instance-dependent upper bound on the sample complexity, which scales logarithmically with $\lambda/\epsilon$..."
  - [PAGE 11, Theorem 14] The bound includes a term $O(\log(\frac{c\lambda K}{\epsilon}))$, demonstrating logarithmic scaling.
  - [corpus] Evidence is weak in the provided corpus for this specific logarithmic vs. linear comparison.
- Break condition: If the discretization parameter $c$ is set too low, it may negatively affect the arm gap definition, potentially invalidating the performance guarantees.

### Mechanism 3
- Claim: The set of solvable problem instances is characterized by having a positive "arm gap" ($\Delta > 0$).
- Mechanism: The paper defines a novel arm gap (Definition 10) which accounts for both the quantile structure and the $\epsilon$-relaxation. It then proves a theorem of the alternative: instances with a positive gap are solvable by their algorithm with finite samples, while instances with a zero gap are fundamentally unsolvable by any algorithm.
- Core assumption: The problem is in a fixed-confidence, pure exploration setting.
- Evidence anchors:
  - [abstract] "...and that essentially all 'solvable' instances have positive arm gaps under their definition."
  - [PAGE 13-14, Theorem 21] "Zero gap is unsolvable... positive gap is solvable."
  - [corpus] No direct parallels in the corpus.
- Break condition: This specific gap definition and solvability characterization applies only to the quantile-based objective with threshold queries.

## Foundational Learning
- Concept: **Noisy Binary Search**
  - Why needed here: It is the core subroutine (`QuantEst`) enabling quantile estimation from 1-bit threshold feedback.
  - Quick check question: How does the query complexity of noisy binary search depend on the size of the search space and the desired confidence?
- Concept: **Fixed-Confidence Best Arm Identification (BAI)**
  - Why needed here: This is the problem framework. The objective is to find a near-optimal arm with probability at least $1-\delta$, not to minimize regret.
  - Quick check question: What distinguishes the fixed-confidence setting from the fixed-budget setting in bandits?
- Concept: **Quantile-based Performance Measures**
  - Why needed here: The goal is to find the arm with the highest quantile (e.g., median), not the highest mean. This changes the definition of "best arm" and the structure of the arm gaps.
  - Quick check question: For what kind of reward distributions is a quantile-based objective preferable to a mean-based one?

## Architecture Onboarding
- Component map: **Learner** -> **QuantEst Subroutine** -> **Agent** (threshold query -> 1-bit feedback) -> **Learner**
- Critical path: For each round, the **Learner** calls **`QuantEst`** for every **Active Arm**. `QuantEst` sends a **threshold query** to the **Agent** and uses the 1-bit reply to update its search. The Learner uses the result to update the arm's confidence interval and then **eliminates** suboptimal arms. The loop continues until a termination condition is met.
- Design tradeoffs: **Discretization Fineness ($c$):** A larger $c$ improves the theoretical guarantees (makes more instances solvable) but increases the size of the search space for `QuantEst`, potentially increasing sample complexity.
- Failure signatures: **Non-termination** on instances with a zero arm gap. **Incorrect arm selection** (probability $\le \delta$) on solvable instances.
- First 3 experiments:
  1.  **Solvability Validation:** Run Algorithm 1 on a simple two-arm instance with a known positive gap. Plot sample complexity against $\log(1/\delta)$ to verify scaling.
  2.  **Unsolvable Instance Probe:** Construct an instance with a zero arm gap. Run the algorithm and confirm it fails to terminate or exceeds practical sample limits.
  3.  **Communication Cost Benchmark:** Compare the sample complexity of this 1-bit algorithm against a baseline that receives full reward feedback to quantify the overhead of the communication constraint.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the dependence on the arm gaps $\Delta_k$ in the sample complexity be improved from logarithmic to doubly-logarithmic, similar to results in unquantized quantile best-arm identification? The current algorithm uses a noisy binary search subroutine with logarithmic dependence, and it's unclear if the 1-bit constraint prevents achieving doubly-logarithmic scaling.
- **Open Question 2**: Are there specific regimes where the joint dependence on the arm gaps $\Delta$ and the relaxation parameters $(\lambda, \epsilon)$ can be improved to be tight beyond the "low error probability" regime? The current bounds are only tight within constant factors for specific parameter regimes.
- **Open Question 3**: Can general 1-bit quantization methods (beyond threshold queries) significantly reduce the sample complexity or remove the $\Omega(\log(\lambda/\epsilon))$ dependence? The paper focuses on threshold queries but notes this as an open direction.
- **Open Question 4**: How does the 1-bit feedback constraint impact the sample complexity and algorithm design in the fixed budget setting for quantile bandits? The paper explicitly focuses on the fixed confidence setting, leaving the fixed budget setting unaddressed.

## Limitations
- The lower bound analysis is incomplete, with tightness claims supported only for mild conditions or specific low error probability regimes
- The theoretical analysis leaves some constants and parameters (particularly the discretization parameter c) unspecified, affecting practical implementation
- Empirical validation is limited to synthetic examples without real-world applications or comparisons to alternative approaches

## Confidence
- **High**: The mechanism for using 1-bit threshold queries with noisy binary search is well-founded and the logarithmic scaling with λ/ε is convincingly demonstrated
- **Medium**: The characterization of solvable instances through the arm gap definition is novel and the theorem of the alternative is mathematically sound, though practical implications require further exploration
- **Low**: The tightness of the lower bound in the general case and the exact impact of parameter choices on practical performance remain uncertain

## Next Checks
1. **Gap Computation Validation**: Verify the arm gap computation (Definition 10) on a variety of synthetic instances, particularly those with satisfying arms, to ensure the max over subsets S ⊇ A_ε is computed correctly
2. **Parameter Sensitivity Analysis**: Systematically vary the discretization parameter c and observe its effect on both the number of solvable instances and the sample complexity to identify the practical tradeoff
3. **Baseline Comparison**: Implement a version of the algorithm that receives full reward feedback and compare its sample complexity to the 1-bit version on identical instances to quantify the communication overhead