---
ver: rpa2
title: Avoiding spurious sharpness minimization broadens applicability of SAM
arxiv_id: '2502.02407'
source_url: https://arxiv.org/abs/2502.02407
tags:
- sharpness
- minimization
- training
- logit
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why Sharpness-Aware Minimization (SAM) performs
  poorly in natural language processing (NLP) tasks, despite its success in computer
  vision. The authors find that in NLP settings, SAM predominantly reduces sharpness
  through logit statistics manipulation rather than improving the geometry of the
  function itself.
---

# Avoiding spurious sharpness minimization broadens applicability of SAM

## Quick Facts
- arXiv ID: 2502.02407
- Source URL: https://arxiv.org/abs/2502.02407
- Reference count: 39
- The paper shows SAM performs poorly in NLP due to spurious sharpness reduction through logit statistics manipulation, and proposes Functional-SAM and preconditioned SAM to address this.

## Executive Summary
The paper investigates why Sharpness-Aware Minimization (SAM), despite its success in computer vision, performs poorly in natural language processing tasks. The authors discover that in NLP settings, SAM predominantly reduces sharpness through logit statistics manipulation rather than improving the underlying function's geometry. To address this, they propose two algorithms: Functional-SAM, which regularizes curvature through modification of the statistics of the overall function, and preconditioned SAM, which uses the inverse of the second-moment statistics from AdamW to precondition the SAM perturbation. These algorithms significantly outperform standard SAM and AdamW baselines in both fixed-length and Chinchilla-style training settings across various model scales, including billion-parameter models.

## Method Summary
The paper proposes two modifications to SAM for NLP tasks. Functional-SAM computes gradients only through the perturbed Jacobian while keeping the loss-to-logits gradient fixed, isolating curvature effects arising from parameter-space geometry. Preconditioned SAM uses AdamW's second-moment statistics to precondition the SAM perturbation, reducing gradient-Hessian alignment and downweighting the logit path contribution. The authors test these methods across model scales (23.9M to 1208M parameters) using decoder-only Transformers on C4 dataset, comparing against AdamW baseline in both fixed-length and Chinchilla-style training regimes.

## Key Results
- Functional-SAM and preconditioned SAM variants significantly outperform standard SAM and AdamW baselines in NLP tasks
- Preconditioned Functional-SAM consistently achieves lower evaluation loss across model scales (23.9M to 1208M parameters)
- The proposed methods achieve better landscape properties with lower Hessian eigenvalues while maintaining or improving validation loss
- Functional-SAM remains stable without warmup at billion-parameter scale, unlike standard SAM which exhibits numerical instability

## Why This Works (Mechanism)

### Mechanism 1: Dual-Path Sharpness Decomposition
SAM's sharpness penalty gradient decomposes into two paths: (1) logit path δ_logit = HG·ε* that manipulates output statistics, and (2) functional path δ_func = HF·ε* that modifies the network's Jacobian geometry. In NLP, τ_logit ≫ τ_func after initial training steps, indicating SAM prioritizes logit-path effects.

### Mechanism 2: Spurious Minimization via Over-Confidence
The logit path reduces sharpness by making predictions more one-hot regardless of correctness. As softmax outputs approach one-hot vectors, logit sharpness decreases without improving underlying function, creating a spurious optimization signal more accessible in NLP due to larger output vocabularies.

### Mechanism 3: Functional-SAM Preserves Loss Gradient While Perturbing Jacobian
By computing ∇_θ F(θ + ρε*) · ∇_F L(θ) instead of ∇_θ L(θ + ρε*), Functional-SAM isolates curvature effects from parameter-space geometry while keeping the loss gradient fixed, preventing logit-path manipulation.

### Mechanism 4: Preconditioning Reduces Gradient-Hessian Alignment
Preconditioning ε* by M^(-1) (AdamW's second-moment statistics) approximates H_G^(-1), reducing amplification of logit-path effects since gradients align with top eigenspaces of HG, thereby promoting functional-path contributions.

## Foundational Learning

- **Gauss-Newton Decomposition of the Hessian (H_L = H_G + H_F)**: Understanding SAM's sharpness penalty decomposition requires knowing that loss Hessian splits into positive semi-definite GGN (H_G) and indefinite functional Hessian (H_F). *Quick check*: Can you explain why H_G is always PSD while H_F can have negative eigenvalues?

- **Vector-Jacobian Products (VJP) in Automatic Differentiation**: Functional-SAM requires computing ∇_θ F(θ + ρε*) · ∇_F L(θ) efficiently without materializing full Jacobian. *Quick check*: In JAX/PyTorch, how would you compute a vector-Jacobian product v^T · J without forming J explicitly?

- **Preconditioning in Adaptive Gradient Methods**: Understanding why AdamW's M^(-1) approximates H_G^(-1) requires knowing how second-moment statistics accumulate gradient curvature information. *Quick check*: Why does Adam use per-parameter learning rates inversely proportional to the root of accumulated squared gradients?

## Architecture Onboarding

- **Component map**: Loss/gradient computation -> Perturbation application -> VJP computation -> AdamW update

- **Critical path**:
  1. Forward pass computes loss L(θ) and logits F(θ)
  2. Backward pass obtains ∇_θ L(θ) and ∇_F L(θ)
  3. Normalize gradient to get ε*, apply preconditioning if enabled
  4. Perturb parameters: θ_pert = θ + ρ·M^(-1)·ε*
  5. Compute VJP at θ_pert using cached ∇_F L(θ) → functional gradient
  6. Pass functional gradient to AdamW update step

- **Design tradeoffs**:
  - **Perturbation radius ρ**: Smaller ρ ≈ 0.1 works for SAM; Functional-SAM tolerates larger ρ but requires tuning
  - **Preconditioning overhead**: Negligible additional cost, but introduces coupling between SAM and optimizer
  - **Gradient computation cost**: Both SAM and Functional-SAM require 2× gradient evaluations per step vs. AdamW alone

- **Failure signatures**:
  - **NaN loss at billion-parameter scale**: Standard SAM/preconditioned SAM exhibit instability; Functional-SAM remains stable without warmup
  - **Worse validation than AdamW**: Indicates τ_logit dominance; verify by measuring sharpness composition ratios
  - **Large optimal ρ (≥0.5)**: Suggests functional path is active and benefiting from stronger perturbations

- **First 3 experiments**:
  1. **Sharpness composition profiling**: On 2M parameter model, log τ_logit, τ_func, τ_cross throughout training. Confirm τ_logit ≫ τ_func for NLP.
  2. **Ablation of ρ sensitivity**: Compare validation loss vs. ρ ∈ {0.1, 0.25, 0.5, 0.75, 1.0} for SAM vs. Functional-SAM. Expect SAM to degrade with larger ρ, Functional-SAM to improve.
  3. **Scale-up to 23.9M with combined method**: Test preconditioned Functional-SAM against AdamW baseline in fixed-length regime. Target: ≥0.02 improvement in eval loss.

## Open Questions the Paper Calls Out

- **Downstream task generalization**: Does the flatter minimum solution found by Functional-SAM translate to improved performance on downstream tasks or better robustness to model compression techniques compared to AdamW? The paper notes this in Section 7 but leaves detailed study to future work.

- **Computational efficiency**: Can the 2x gradient computational overhead of Functional-SAM be reduced to make it efficient enough for large-scale training deployment? The authors highlight this as requiring additional future work to make it suitable for deployment.

- **Hyperparameter transfer across scales**: How can the perturbation radius hyperparameter (ρ) be optimally transferred or scaled across different model sizes and training regimes without requiring expensive re-tuning? The paper notes that extensive tuning of ρ is unfeasible at largest scales and would benefit from improved parameterizations.

## Limitations

- The paper's claims rely heavily on the logit/functional path decomposition, with confidence that this decomposition is the primary explanation for SAM's poor performance in NLP, though empirical evidence is primarily based on gradient alignment metrics rather than direct ablation studies.
- The assertion that the logit path is "spurious" assumes that one-hot prediction behavior is inherently undesirable, though this may not hold in all NLP contexts.
- The preconditioning mechanism's effectiveness depends on AdamW's second-moment statistics approximating the inverse of the GGN matrix, which is only validated through random matrix models rather than direct spectral analysis on trained models.

## Confidence

- **High Confidence**: The empirical observation that SAM underperforms AdamW in NLP tasks, and that Functional-SAM and preconditioned SAM variants improve upon both baselines across multiple scales and training regimes.
- **Medium Confidence**: The theoretical decomposition of SAM's sharpness penalty into logit and functional paths, and the characterization of the logit path as "spurious" due to over-confidence mechanisms.
- **Medium Confidence**: The claim that preconditioning by AdamW's second-moment statistics specifically reduces logit-path dominance and improves functional-path contribution.

## Next Checks

1. **Direct Path Ablation**: Implement a variant of SAM that explicitly disables either the logit or functional path contribution and compare performance against standard SAM and Functional-SAM to directly validate which path contributes positively/negatively to generalization.

2. **Over-Confidence Analysis**: Measure calibration metrics (e.g., expected calibration error, confidence histograms) on SAM, Functional-SAM, and preconditioned SAM models to empirically verify whether SAM's worse validation performance correlates with increased prediction over-confidence as claimed.

3. **Spectral Verification of Preconditioning**: Compute the top eigenvalues of the GGN matrix (H_G) during training and compare them with AdamW's accumulated second-moment statistics M to empirically validate whether M^(-1) approximates H_G^(-1) as the preconditioning theory suggests.