---
ver: rpa2
title: Cued Speech Generation Leveraging a Pre-trained Audiovisual Text-to-Speech
  Model
arxiv_id: '2501.04799'
source_url: https://arxiv.org/abs/2501.04799
tags:
- speech
- hand
- lips
- cued
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatically generating Cued
  Speech (CS) hand and lip movements from text input, a challenging task due to the
  scarcity of training data and the unique temporal dynamics of CS gestures. The authors
  propose leveraging a pre-trained audiovisual autoregressive text-to-speech model
  (AVTacotron2) and adapting it for CS generation through transfer learning.
---

# Cued Speech Generation Leveraging a Pre-trained Audiovisual Text-to-Speech Model

## Quick Facts
- arXiv ID: 2501.04799
- Source URL: https://arxiv.org/abs/2501.04799
- Reference count: 24
- Authors: Sanjana Sankar; Martin Lenglet; Gerard Bailly; Denis Beautemps; Thomas Hueber
- Primary result: 77% phonetic-level decoding accuracy using frozen-encoder transfer learning

## Executive Summary
This paper addresses automatic Cued Speech generation by adapting a pre-trained audiovisual text-to-speech model through transfer learning. The authors demonstrate that freezing the encoder weights during fine-tuning prevents catastrophic forgetting and enables effective transfer from standard speech data to Cued Speech landmark generation. The model generates synchronized 2D hand and lip movements from text, achieving 77% phonetic accuracy when evaluated by an automatic Cued Speech recognition system. The work introduces a new high-quality French Cued Speech dataset (CSF23) to support future research in this domain.

## Method Summary
The method repurposes an AVTacotron2 model pre-trained on audiovisual TTS data, fine-tuning it to generate 2D hand and lip landmarks from text input. Visual features are extracted using MediaPipe (42 lip landmarks, 21 hand landmarks), compressed via PCA to 10 components each, and synchronized with mel-spectrograms at 30fps. The best performance was achieved by freezing the pre-trained encoder and fine-tuning only the decoder and regressor components on the CSF23 dataset using an Adam optimizer with learning rate 0.001 and MSE loss.

## Key Results
- Frozen-encoder fine-tuning (Strategy S3) achieved 77.3% phonetic decoding accuracy, compared to 17.06% from scratch training
- Unfrozen fine-tuning (Strategy S2) failed due to catastrophic forgetting, producing "average lip shape for every frame"
- PCA compression retained 99% variance while reducing overfitting risk on limited CS data

## Why This Works (Mechanism)

### Mechanism 1
Freezing the pre-trained encoder during fine-tuning prevents catastrophic forgetting and enables effective transfer from audiovisual TTS to cued speech generation. The pre-trained encoder has learned robust text-to-phoneme representations from large-scale AV-TTS data (6,538 utterances). By freezing these weights, the model retains its ability to encode linguistic content while the decoder learns to map these representations to the new output modality (hand + lip landmarks). The encoder implicitly encodes phoneme durations, providing stable temporal scaffolding for the decoder.

### Mechanism 2
Autoregressive attention mechanisms learn to handle the inherent asynchrony between hand and lip movements in cued speech without requiring explicit temporal alignment. Tacotron2's location-sensitive attention learns soft alignments between input text and output frames. This allows the model to discover that hand movement onsets can precede lip movements by up to 120ms—without manual annotation. The attention weights are processed by a 1D CNN, enabling the model to track position in the output sequence and maintain temporal coherence across both visual streams.

### Mechanism 3
PCA-compressed 2D landmarks provide sufficient information for phonetic-level CS generation while reducing model complexity. Visual features (42 lip landmarks, 21 hand landmarks) extracted via MediaPipe are reduced to 10 principal components each (20D total), capturing 99% of variance. This compression reduces overfitting risk on limited CS data and forces the model to learn articulator-relevant dynamics rather than pixel-level noise.

## Foundational Learning

- **Encoder-Decoder with Attention (Seq2Seq)**: The entire AVTacotron2 architecture is built on this paradigm. Without understanding how attention learns soft alignments between input sequences (text/phonemes) and output sequences (spectrograms/landmarks), you cannot debug alignment failures or interpret attention weights.
  - Quick check: Given a text input of 5 phonemes producing 80 output frames, what shape would the attention matrix have, and what would a well-aligned attention pattern look like?

- **Transfer Learning & Catastrophic Forgetting**: The paper's core contribution is identifying that Strategy S2 (unfrozen fine-tuning) fails due to catastrophic forgetting, while S3 (frozen encoder) succeeds. Understanding why neural networks overwrite previously learned representations is essential for designing transfer protocols.
  - Quick check: If you freeze the encoder but fine-tune the decoder on a new dataset with different output statistics, what specific component of the model must adapt to bridge the distribution gap?

- **Cued Speech Phonetics**: Cued Speech uses hand position + handshape + lip shape to disambiguate phonemes that look identical on the lips. The model must learn this tripartite coding system. Without understanding that /p/, /b/, /m/ are visually identical on lips but distinguished by hand position, you cannot interpret error patterns.
  - Quick check: In French Cued Speech, if a model correctly generates lip movements but consistently produces incorrect hand positions, what phoneme-level confusions would you expect in the ACSR evaluation?

## Architecture Onboarding

- **Component map**: Text Input → [Encoder: 3 Conv1D + BiLSTM] → Encoded Representation → [Location-Sensitive Attention] → Context Vector → [Decoder: Prenet(256) + 2× UniLSTM(1024)] → [Regressor: Mel(80) + Lips(10) + Hand(10)] → [Postnet: 5 Conv1D] → Mel refinement

- **Critical path**: Text → Encoder → Attention → Decoder → Hand/Lip Regressor. The frozen encoder is the transfer bottleneck; the decoder+regressors are the adaptation targets.

- **Design tradeoffs**:
  - Frozen vs. unfrozen encoder: Frozen preserves linguistic knowledge but limits adaptation to CS-specific timing; unfrozen risks catastrophic forgetting (empirically validated: S2 failed).
  - Joint audio+visual vs. visual-only fine-tuning: Paper fine-tunes on both audio and visual CS data. Assumption: audio provides auxiliary signal for lip timing, even though final application may not need audio output.
  - 2D landmarks vs. 3D pose vs. pixels: Landmarks are compact and differentiable but lose appearance information needed for photorealistic rendering (acknowledged as future work).

- **Failure signatures**:
  - Catastrophic forgetting (S2): Model generates "average lip shape for every frame" while hand movements remain somewhat intelligible—encoder representations degraded.
  - Alignment drift: Generated sequences have correct phoneme content but wrong timing; attention fails to learn CS-specific durations.
  - Modality imbalance: One stream (e.g., lips) dominates training loss, causing underfitting on the other (hand).

- **First 3 experiments**:
  1. Reproduce S1 vs. S3 comparison: Train from scratch on CSF23 and compare to frozen-encoder fine-tuning. Verify 17% vs. 77% accuracy gap. If gap is smaller, investigate data augmentation or regularization differences.
  2. Ablate audio during fine-tuning: Train S3 with visual-only loss (no mel-spectrogram). Hypothesis: audio provides implicit timing supervision; removing it may hurt lip-sync quality even if hand generation is unaffected.
  3. Cross-dataset generalization test: Train on CSF23, evaluate on CSF22 (which has different sentence prompts and no linguistic overlap). Compare to reported 71% accuracy. If significantly lower, investigate domain shift in encoder representations.

## Open Questions the Paper Calls Out

- **How can the generated 2D landmarks be effectively rendered into photorealistic videos or avatar animations to facilitate human perception?**: The authors state that "further development is necessary to advance from 2D poses... to either avatar-based implementations or the synthesis of photo-realistic videos using, for instance, GAN or Diffusion Models." The current study focuses exclusively on generating 2D landmark coordinates (poses), lacking the visual texture required for a consumer-facing communication tool.

- **How does the proposed AVTacotron2-based approach compare performance-wise against recent diffusion-based models for Cued Speech generation?**: The authors identify a recent preprint using a diffusion model for Chinese CS but note, "we leave a comparison with our approach for future work." The paper only benchmarks against its own baselines (S1, S2) and does not contrast its autoregressive method with the emerging diffusion-based architecture.

- **Does the high automatic recognition accuracy (77%) correlate with high intelligibility and naturalness for human Cued Speech receivers?**: The paper evaluates performance using an automatic CS recognition (ACSR) system because "perceptual evaluation is not feasible" with raw landmarks. The 77% phonetic accuracy is a proxy metric; it remains unconfirmed if the generated "temporal dynamics" and "occasional offset of the features" degrade comprehension for actual human users.

## Limitations

- The CSF23 dataset, while introduced as higher quality than CSF22, is not yet publicly available for independent verification.
- The evaluation metric (ACSR decoding accuracy) depends on a pre-trained recognition model whose sensitivity to synthetic versus real Cued Speech data is not characterized.
- The choice of 2D landmarks with 10 PCA components per stream is presented as sufficient without direct empirical validation against 3D pose or raw pixel alternatives.

## Confidence

- **High Confidence**: The experimental comparison between strategies S1 (scratch), S2 (unfrozen fine-tuning), and S3 (frozen encoder) is internally consistent and reproducible.
- **Medium Confidence**: The claim that autoregressive attention can learn the asynchrony between hand and lip movements without explicit alignment is supported by the model's performance but lacks ablation studies isolating the attention mechanism's role.
- **Low Confidence**: The sufficiency of 2D PCA landmarks for phonetic-level CS generation is asserted but not rigorously tested.

## Next Checks

1. **Ablation on Landmark Dimensionality**: Train identical models using 2D landmarks with 5, 10, and 15 PCA components per stream. Compare ACSR accuracy to quantify the information loss threshold and validate whether 10 components truly capture sufficient phonetic detail.

2. **Cross-Modality Evaluation**: Generate Cued Speech from text, then render the landmarks as video frames and evaluate using a standard lip-reading model. This tests whether the generated visual features are interpretable beyond the specialized ACSR system and validates the claim that the model produces phonetically meaningful gestures.

3. **Encoder Representation Analysis**: Extract encoder activations from both frozen and unfrozen conditions, then perform nearest-neighbor search in phoneme embedding space between source AV-TTS data and target CS data. This would empirically verify whether freezing prevents catastrophic forgetting by preserving the original linguistic manifold structure.