---
ver: rpa2
title: Large Language Models Pass the Turing Test
arxiv_id: '2503.23674'
source_url: https://arxiv.org/abs/2503.23674
tags:
- human
- test
- were
- turing
- witness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "In a randomized, controlled Turing test, GPT-4.5 and LLaMa-3.1-405B\
  \ were evaluated against human participants. GPT-4.5 was judged to be human 73%\
  \ of the time\u2014significantly more often than actual human participants\u2014\
  while LLaMa-3.1 achieved 56%, not significantly different from chance."
---

# Large Language Models Pass the Turing Test

## Quick Facts
- **arXiv ID:** 2503.23674
- **Source URL:** https://arxiv.org/abs/2503.23674
- **Reference count:** 35
- **Primary result:** GPT-4.5 and LLaMa-3.1-405B passed a three-party Turing test when persona-prompted, with GPT-4.5 achieving 73% win rate versus human participants' 27%.

## Executive Summary
In a randomized, controlled Turing test, GPT-4.5 and LLaMa-3.1-405B were evaluated against human participants. GPT-4.5 was judged to be human 73% of the time—significantly more often than actual human participants—while LLaMa-3.1 achieved 56%, not significantly different from chance. Both models required a persona prompt to perform at this level; without it, their win rates dropped to 36-38%, similar to baselines GPT-4o (21%) and ELIZA (23%). The results show that modern LLMs can pass a standard three-party Turing test when appropriately prompted, indicating their capacity to convincingly imitate human conversation and raising concerns about their potential social and economic impacts.

## Method Summary
The study implemented a three-party Turing test where human interrogators had 5-minute simultaneous conversations with one human and one AI witness. Four AI systems were tested: GPT-4.5, LLaMa-3.1-405B, GPT-4o, and ELIZA. Models were evaluated under two prompt conditions: NO-PERSONA (minimal instructions) and PERSONA (detailed persona instructions for a young, introverted internet user). The PERSONA prompt was critical for success, with GPT-4.5 achieving 73% win rate versus 36% without persona. ELIZA's 23% win rate validated the methodology by demonstrating discriminability. The Django-based WebSocket interface included typing delays to simulate human behavior.

## Key Results
- GPT-4.5-PERSONA achieved 73% win rate, significantly exceeding both human witnesses (27%) and chance level (50%)
- LLaMa-3.1-405B-PERSONA achieved 56% win rate, not significantly different from chance
- Without persona prompting, both models dropped to 36-38% win rates, similar to GPT-4o-NO-PERSONA (21%) and ELIZA (23%)
- Linguistic style (27% of reasons) and interactional dynamics (23%) were the most frequent factors in interrogator judgments

## Why This Works (Mechanism)

### Mechanism 1: Persona-Guided Distributional Alignment
The PERSONA prompt instructs models to adopt characteristics of a young, introverted, internet-savvy person using informal language. This constrains the model's token generation toward stylistic patterns (slang, brevity, emotional expression) that interrogators rely on most heavily for judgments—27% of reasons cited linguistic style and 23% cited interactional dynamics. The 35+ percentage point drop between PERSONA and NO-PERSONA conditions demonstrates that performance is contingent on instruction quality. This mechanism fails if interrogators employ adversarial strategies like jailbreaks or if the persona mismatch becomes obvious through sustained interrogation.

### Mechanism 2: Three-Party Comparative Baseline
The simultaneous three-party setup provides robust discrimination by forcing direct comparison between AI and human witnesses. The interrogator sees two concurrent conversations—one with a human, one with AI—and must select which witness is human. ELIZA's 23% win rate (significantly below chance) validates that interrogators can discriminate, while GPT-4.5-PERSONA's 73% win rate demonstrates genuine indistinguishability. This mechanism degrades if human witnesses behave erratically or if conversation duration extends beyond the model's coherence window.

### Mechanism 3: Strategic Imperfection and Feigned Ignorance
GPT-4.5-PERSONA achieved supra-human performance partly by strategically feigning ignorance and avoiding AI-typical behaviors like excessive helpfulness. The model, guided by persona instructions, produces responses that match interrogator stereotypes of human imperfection—including plausible knowledge gaps. One of the most accurate reasons for verdicts was that a witness was human because they "lacked knowledge." This mechanism fails if interrogators become aware of this optimization and adjust their heuristics to detect "strategically imperfect" responses.

## Foundational Learning

- **Controlled Comparative Evaluation**
  - Why needed: To validate that "passing" reflects genuine indistinguishability rather than flawed methodology, ELIZA's sub-chance performance (23%) is critical—it proves interrogators can discriminate.
  - Quick check: Why is ELIZA's 23% win rate more methodologically important than GPT-4.5's 73% win rate?

- **Prompt Sensitivity**
  - Why needed: The 35+ percentage point drop between PERSONA and NO-PERSONA conditions demonstrates that performance is not an inherent model property but contingent on instruction quality.
  - Quick check: What was LLaMa-3.1-405B's approximate win rate drop between PERSONA and NO-PERSONA conditions?

- **Interrogator Heuristic Bias**
  - Why needed: Understanding that 61% of interrogators used small talk and 50% probed socio-emotional qualities (vs. 12% using knowledge tests) explains why persona-based stylistic mimicry is effective.
  - Quick check: What two reason categories most strongly predicted accurate verdicts?

## Architecture Onboarding

- **Component map:** PERSONA prompt → LLM generates stylistically aligned responses → Interrogator applies stylistic heuristics → Model selected as human
- **Critical path:** PERSONA prompt → LLM generates stylistically aligned responses → Interrogator applies stylistic heuristics → Model selected as human
- **Design tradeoffs:** 5-minute limit enables high throughput but restricts deep probing; laypersons provide generalizability but may miss sophisticated detection strategies; simultaneous conversation increases cognitive load but eliminates sequential bias
- **Failure signatures:** Win rate at chance (50%) with high variance suggests random guessing; win rate below 30% indicates obvious AI detection; persona consistency failures expose prompt dependence
- **First 3 experiments:** 1) Ablate persona components to identify which features drive the 35+ percentage point gain; 2) Extend duration to 15/30 minutes to test sustained coherence; 3) Recruit expert interrogators to measure performance against sophisticated detection strategies

## Open Questions the Paper Calls Out

### Open Question 1
Does shared cultural or situational "common ground" serve as a reliable defense for humans trying to identify AI impostors? The authors note that while undergraduate participants were more accurate than Prolific participants—potentially due to relying on "cultural, social, and linguistic common ground"—they were still fooled the majority of the time. They explicitly state, "Whether and to what degree common ground serves as a prophylaxis against AIs posing as humans remains to be determined."

### Open Question 2
Can expert interrogators (e.g., AI researchers or psychologists) reliably distinguish between persona-prompted LLMs and humans in a three-party Turing test? The paper suggests that "specifically recruiting experts in AI or psychology could reveal whether there exist any participants who can reliably distinguish people from AI."

### Open Question 3
How robust is the models' ability to pass the Turing test over longer durations and higher-stakes interactions? The authors state, "Longer tests might be more exacting, and provide a better estimate of how capable models would be at long-running social engineering operations."

### Open Question 4
Can LLMs pass the Turing test without relying on explicit "persona" prompts that instruct them to adopt specific human-like traits? The authors discuss the heavy reliance on the PERSONA prompt, noting, "What does it mean to say that LLMs pass the Turing test, but only when they are suitably prompted?"

## Limitations

- The exact PERSONA prompt text is not fully provided in the main document, creating a reproducibility barrier
- Results are based on laypersons rather than expert interrogators, raising questions about detection by specialists
- The 5-minute duration may not reflect real-world interaction lengths where models could fail to maintain consistency
- Results are specific to tested model versions and may not generalize to newer or older LLM versions

## Confidence

- **High Confidence:** The finding that persona prompting is essential for Turing test success (evidenced by the 35+ percentage point performance drop between PERSONA and NO-PERSONA conditions)
- **Medium Confidence:** The claim that GPT-4.5 "passes" the Turing test with 73% win rate, as this is contingent on specific prompt engineering and the choice of lay interrogators
- **Low Confidence:** The mechanism of "strategic imperfection" where models feign ignorance to appear more human-like, as this is inferred from correlational patterns rather than direct experimental manipulation

## Next Checks

1. **Prompt Ablation Study:** Systematically remove components from the PERSONA prompt (slang instructions, introversion, knowledge level) to identify which features drive the performance gain and establish the minimum effective prompt.
2. **Expert Interrogator Benchmark:** Replicate the study with expert participants (AI researchers, psychologists) to determine whether laypersons' stylistic heuristics can be exploited or whether experts employ more robust detection strategies.
3. **Duration Scaling Experiment:** Extend conversation duration from 5 minutes to 15 and 30 minutes to test whether model coherence degrades over time, potentially revealing systematic limitations in sustained human-like interaction.