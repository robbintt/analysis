---
ver: rpa2
title: 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs'
arxiv_id: '2510.09201'
source_url: https://arxiv.org/abs/2510.09201
tags:
- prompt
- image
- prompts
- multimodal
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces multimodal prompt optimization for Multimodal
  Large Language Models (MLLMs), addressing the limitation of existing text-only approaches
  by extending the search space to include both textual and non-textual prompts. The
  authors propose MPO, a framework that jointly refines multimodal prompts through
  alignment-preserving exploration using generation, edit, and mix operators, combined
  with prior-inherited Bayesian UCB for efficient candidate selection.
---

# Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs

## Quick Facts
- arXiv ID: 2510.09201
- Source URL: https://arxiv.org/abs/2510.09201
- Reference count: 40
- This paper introduces multimodal prompt optimization for MLLMs, achieving 6-8 percentage point accuracy gains and 42% evaluation budget reduction across 10 diverse datasets spanning images, videos, and molecules.

## Executive Summary
This paper addresses the limitation of existing text-only prompt optimization methods for Multimodal Large Language Models (MLLMs) by extending the search space to include both textual and non-textual prompts. The authors propose MPO, a framework that jointly refines multimodal prompts through alignment-preserving exploration using generation, edit, and mix operators, combined with prior-inherited Bayesian UCB for efficient candidate selection. Across 10 diverse datasets spanning images, videos, and molecules, MPO consistently outperforms leading text-only optimization methods, achieving average accuracy gains of 6-8 percentage points and reducing evaluation budget by 42%. The method demonstrates strong generalizability across different base models, optimizer models, and modality-specific generators, confirming the importance of expanding prompt optimization into the multimodal domain to fully exploit MLLM capabilities.

## Method Summary
MPO extends prompt optimization from text-only to multimodal space by jointly refining textual prompts and non-textual conditions (images, videos, molecules) through three operators: generation (create new multimodal prompts), edit (modify existing ones), and mix (combine elements from different prompts). The framework uses cohesive backpropagation to generate unified textual feedback from failure sets, then jointly updates both modalities. A key innovation is the Prior-Inherited Bayesian UCB algorithm, which initializes Beta priors for child prompts from parent posterior means scaled by a prior strength parameter, enabling efficient exploration while preserving promising candidates. The method maintains cross-modal alignment throughout optimization and demonstrates consistent performance improvements across diverse datasets and model architectures.

## Key Results
- MPO achieves 6-8 percentage point accuracy gains over text-only optimization methods across 10 diverse datasets
- The framework reduces evaluation budget by 42% compared to uniform sampling approaches
- MPO demonstrates strong generalizability across different base models (Qwen2.5-VL, Qwen3), optimizer models (GPT-4o mini), and modality-specific generators (GPT-Image, Wan2.1, GPT-4o mini)

## Why This Works (Mechanism)
MPO works by expanding the prompt optimization search space from purely textual to multimodal, allowing the framework to leverage the full capabilities of MLLMs that can process both text and non-text inputs. The joint optimization of text and non-text conditions through generation, edit, and mix operators enables more expressive and contextually appropriate prompts. The Prior-Inherited Bayesian UCB mechanism efficiently navigates this expanded search space by preserving promising candidates through informed prior distributions while maintaining exploration capability. This approach addresses the fundamental limitation of text-only methods that cannot fully exploit MLLM capabilities when the optimal prompt may require non-textual elements.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Models that can process and generate both textual and non-textual content. Why needed: MPO targets these models specifically, leveraging their dual modality processing capabilities. Quick check: Verify model can accept both text and image/video/molecule inputs.
- **Bayesian UCB with Prior Inheritance**: A bandit algorithm that uses Beta distribution posteriors with priors inherited from parent candidates. Why needed: Enables efficient exploration of multimodal prompt space while preserving promising candidates. Quick check: Confirm Beta posterior updates correctly after each evaluation round.
- **Cohesive Backpropagation for Textual Feedback**: Method to generate unified textual gradients from failure sets. Why needed: Provides direction for joint text and non-text prompt updates. Quick check: Validate textual feedback gradients improve text-only performance before multimodal optimization.
- **Cross-modal Alignment Preservation**: Maintaining consistency between textual and non-textual prompt components. Why needed: Ensures multimodal prompts work together effectively rather than contradicting each other. Quick check: Measure alignment scores between text and non-text components during optimization.
- **Modality-Specific Generators**: Different generators for images, videos, and molecules. Why needed: Each modality requires specialized generation approaches. Quick check: Verify each generator produces valid outputs for its respective modality.
- **Prompt Optimization Operators**: Generation, edit, and mix operations for exploring prompt space. Why needed: Provide diverse mechanisms for discovering effective multimodal prompts. Quick check: Confirm all three operators produce valid and varied prompt candidates.

## Architecture Onboarding

**Component Map**
MPO -> Base MLLM + Optimizer Model + Modality Generators -> Datasets -> Evaluation Metrics

**Critical Path**
Initialization -> Generation (first iteration) -> Joint Update (text + non-text) -> Operator Selection (gen/edit/mix) -> Bayesian UCB Selection -> Evaluation -> Posterior Update -> Repeat until budget exhausted

**Design Tradeoffs**
- Expanded search space vs. computational complexity
- Prior inheritance strength vs. exploration capability
- Joint optimization vs. sequential modality-specific optimization
- Operator diversity vs. convergence speed

**Failure Signatures**
- Cross-modal misalignment: Textual and non-textual prompts contradict each other
- Cold-start inefficiency: Without prior inheritance, wasted evaluations on unpromising candidates
- Operator bias: Over-reliance on one operator type leading to suboptimal exploration

**First Experiments**
1. Implement single-iteration MPO on CUB-200-2011 with only generation operator to verify basic functionality
2. Compare Bayesian UCB with uniform sampling on PlantVillage dataset to demonstrate budget efficiency
3. Run ablation study removing prior inheritance to quantify its contribution to performance

## Open Questions the Paper Calls Out
None

## Limitations
- Missing specification of batch size B for Bayesian UCB evaluation rounds, creating uncertainty in optimization dynamics
- Lack of exact random seeds and data splits for datasets without official splits introduces reproducibility concerns
- Incomplete specification of modality-specific meta-prompts for video and molecular domains requires assumptions

## Confidence
**High Confidence** in the core methodological contribution and claimed advantages (6-8 percentage point improvements, 42% budget reduction) due to logical framework design and alignment with established optimization principles.

**Medium Confidence** in reproducibility of specific results across all 10 datasets due to missing implementation details about batch sizes, random seeds, and exact prompt formulations.

**Low Confidence** in practical implementation details including Beta distribution updates, operator implementations, and modality-specific generators that require significant engineering effort to reproduce faithfully.

## Next Checks
1. Replicate the core MPO pipeline on CUB-200-2011 (image) and BBBP (molecular) datasets using reasonable assumptions for missing parameters, verifying the 6-8 percentage point accuracy improvement over text-only baselines.

2. Conduct ablation studies removing prior inheritance from Bayesian UCB, restricting to text-only optimization, and disabling joint text-non-text optimization to confirm each component's contribution to reported performance gains and budget savings.

3. Apply MPO to a new modality (e.g., audio classification) not covered in the original study to validate framework generalizability and confirm performance improvements when applied to unseen domains.