---
ver: rpa2
title: Slot Filling as a Reasoning Task for SpeechLLMs
arxiv_id: '2510.19326'
source_url: https://arxiv.org/abs/2510.19326
tags:
- reasoning
- llms
- slot
- speechllms
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes integrating chain-of-thought (CoT) reasoning
  into speech large language models (speechLLMs) for slot-filling tasks. The authors
  decompose slot-filling into multi-step reasoning, create a reasoning dataset with
  intermediate traces, and fine-tune speechLLMs via supervised learning.
---

# Slot Filling as a Reasoning Task for SpeechLLMs

## Quick Facts
- arXiv ID: 2510.19326
- Source URL: https://arxiv.org/abs/2510.19326
- Authors: Kadri Hacioglu; Manjunath K E; Andreas Stolcke
- Reference count: 0
- This paper proposes integrating chain-of-thought (CoT) reasoning into speech large language models (speechLLMs) for slot-filling tasks. The authors decompose slot-filling into multi-step reasoning, create a reasoning dataset with intermediate traces, and fine-tune speechLLMs via supervised learning. They experiment with various text foundation LLMs—regular, instruction-tuned, reasoning-specialized, and hybrid—and evaluate performance in regular and reasoning modes. Results show that medium-sized, balanced (hybrid) models perform best. Reasoning-specialized LLMs show large relative gains with reasoning supervision but still underperform due to domain overfitting. Smaller models struggle in reasoning mode but improve with hybrid fine-tuning. Hybrid speechLLMs fine-tuned on both direct and reasoning supervision achieve the best overall performance, demonstrating that balanced training improves generalization and flexibility in speech understanding tasks.

## Executive Summary
This paper introduces chain-of-thought reasoning into speechLLMs for slot-filling tasks by decomposing the problem into multi-step reasoning traces. The authors create a reasoning dataset with intermediate steps and fine-tune various text foundation LLMs (regular, instruction-tuned, reasoning-specialized, hybrid) under regular and reasoning supervision. Their results show that medium-sized hybrid models achieve the best performance, while reasoning-specialized LLMs gain the most from reasoning supervision but still underperform due to domain overfitting. Smaller models struggle with reasoning mode but improve with hybrid fine-tuning, demonstrating that balanced training enhances generalization and flexibility in speech understanding.

## Method Summary
The authors build speechLLMs using Whisper-base encoders (frozen) with 2-layer MLP modality adapters (8× downsampling) and text foundation LLMs fine-tuned with LoRA. They create a reasoning dataset with GPT-4o annotations containing transcription, mention identification, and slot justification steps. Three training modes are explored: regular (direct JSON output), reasoning (CoT with tags), and hybrid (merged data with mode-switching tokens). Models are evaluated using partial-match precision, recall, and F1 across various foundation LLM types and scales.

## Key Results
- Hybrid speechLLMs fine-tuned on both direct and reasoning supervision achieve the best overall performance
- Medium-sized hybrid models (Qwen3 4B) outperform single-mode training with F1 of 0.7988 vs. 0.7550 (regular-only) and 0.6338 (reasoning-only)
- Reasoning-specialized text LLMs show large relative gains with reasoning supervision (+22.72%) but underperform in absolute terms due to domain overfitting
- Smaller models (<4B parameters) degrade with reasoning-only supervision but improve with hybrid fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing slot filling into explicit multi-step reasoning traces improves performance for medium-scale speechLLMs by encouraging deeper semantic interpretation before label assignment.
- Mechanism: The chain-of-thought framework forces the model to (1) transcribe speech mentally, (2) identify entity mentions, (3) justify slot-value assignments semantically—mimicking human annotation workflows. This intermediate supervision may provide richer training signals than direct JSON output.
- Core assumption: Explicit reasoning traces create useful intermediate representations that improve slot-value mapping accuracy.
- Evidence anchors: [abstract] "We demonstrate performance improvements by introducing reasoning (intermediate) steps." [section 3.4] "the base model... exhibits notable improvements, and performs the best, indicating that models without post training may serve as more flexible candidates for incorporating reasoning capabilities" [corpus] Related work on SpeechLLMs for slot filling (arXiv:2510.15851) shows zero-shot approaches without explicit reasoning achieve competitive results, suggesting the mechanism may be task- and scale-dependent.
- Break condition: Smaller models (<4B parameters) show performance degradation with reasoning supervision—likely due to capacity constraints overwhelming the model with longer sequences.

### Mechanism 2
- Claim: Reasoning-specialized text foundation LLMs (trained primarily on math, logic, code) can degrade speechLLM performance for slot filling due to loss of general language comprehension.
- Mechanism: Domain overfitting during reasoning distillation may distort linguistic knowledge representations needed for spoken language understanding. When the foundation LLM is later fine-tuned with speech data, the preserved reasoning patterns conflict with language understanding requirements.
- Core assumption: The performance drop stems from representation drift during reasoning specialization, not from training hyperparameters or data distribution shifts.
- Evidence anchors: [abstract] "a reasoning textual LLM developed mainly for math, logic and coding domains might be inferior as a foundation model for a reasoning speechLLM" [section 3.3] "Deepseek R1 Distill Llama 3.1 8B, performs worse than all other family members... likely due to the model being over-specialized for abstract reasoning tasks" [corpus] No direct corpus evidence on this specific failure mode; this appears to be a novel finding in speechLLM literature.
- Break condition: When the same reasoning-specialized model receives explicit reasoning supervision during speechLLM fine-tuning, it shows the largest relative gain (+22.72%), suggesting the mechanism can partially recover with appropriate training signals.

### Mechanism 3
- Claim: Hybrid fine-tuning that preserves both direct and reasoning response modes in a single speechLLM yields better generalization than single-mode training.
- Mechanism: Training on merged datasets with mode-switching tokens (`\no think` / `\think`) provides regularization and prevents overfitting to either short-form direct outputs or long-form reasoning traces. The model learns when to apply each strategy.
- Core assumption: The improvement comes from training diversity rather than simply having more total training examples.
- Evidence anchors: [abstract] "hybrid speechLLMs... have better performance than those fine-tuned employing only one mode of operation" [section 3.5] Qwen3 4B hybrid speechLLM achieves 0.7988 F1 vs. 0.7550 (regular-only) and 0.6338 (reasoning-only) [corpus] Related work on dialogue state tracking via LLMs (arXiv:2503.08857) explores natural language summarization but does not compare hybrid vs. single-mode training.
- Break condition: Works best with hybrid text foundation LLMs; effectiveness with purely reasoning-specialized or purely base foundation models is not demonstrated.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper builds directly on CoT frameworks from reasoning LLMs; understanding how intermediate steps elicit reasoning is essential for interpreting the mechanism claims.
  - Quick check question: Can you explain why generating reasoning traces before final answers might improve task accuracy compared to direct prediction?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: All experiments use LoRA with specific hyperparameters (rank=32, α=128); understanding this is necessary for reproduction and interpreting which components are frozen vs. trainable.
  - Quick check question: Which components in the speechLLM architecture are fully fine-tuned, which use LoRA, and which remain frozen?

- Concept: **Slot Filling in Spoken Language Understanding**
  - Why needed here: The task formulation—extracting entity values from speech into structured slots—is the target application; understanding the partial-match evaluation metric is critical for interpreting F1 scores.
  - Quick check question: Why might partial-match scoring be preferred over exact match for generative speechLLM outputs?

## Architecture Onboarding

- Component map:
  Whisper-base encoder (frozen) -> 2-layer MLP modality adapter (8× downsampling) -> Text foundation LLM (LoRA fine-tuned)

- Critical path:
  1. Prepare instruction-based dataset with reasoning traces (transcription → mention identification → slot justification)
  2. Initialize speechLLM with frozen Whisper encoder + trainable adapter + LoRA-initialized LLM
  3. Fine-tune with regular, reasoning, or hybrid supervision
  4. Evaluate with partial-match precision/recall/F1

- Design tradeoffs:
  - Foundation LLM selection: Base models more flexible for reasoning integration; instruction-tuned models show moderate gains; reasoning-specialized models risk language comprehension degradation
  - Model scale vs. reasoning: 8B models benefit from reasoning supervision; smaller models (0.6B-4B) may degrade
  - Training mode: Hybrid > regular-only > reasoning-only for hybrid foundation LLMs; reasoning-only underperforms for smaller scales

- Failure signatures:
  - Large performance drop when switching regular speechLLM to reasoning mode with small models (Qwen3 4B: -16% F1)
  - Reasoning-specialized foundation LLMs (DeepSeek R1 Distill) underperform base models in absolute terms despite larger relative gains
  - Overfitting to reasoning traces indicated by degraded general language understanding

- First 3 experiments:
  1. **Baseline replication**: Train regular speechLLM with Llama 3.1 8B Instruct using non-reasoning data; verify F1 ~0.73 to confirm setup
  2. **Ablation on foundation type**: Compare base vs. instruction-tuned vs. reasoning-specialized variants at same scale (8B) under identical LoRA settings to isolate foundation model effects
  3. **Hybrid mode validation**: Train Qwen3 4B with hybrid supervision; confirm mode-switching (`\think`/`\no think`) produces distinct output patterns and achieves F1 improvement over single-mode training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent is model scale a prerequisite for reasoning capabilities to emerge in speechLLMs, and can smaller models bridge the gap with efficient fine-tuning?
- Basis in paper: [explicit] The authors explicitly ask "to what extent is model scale a prerequisite for reasoning capabilities to emerge?" while discussing the trade-off between model size and compute cost.
- Why unresolved: The experiments were limited to "tiny" (0.6B) and "medium" (8B) models due to resource constraints, leaving the scaling laws for this specific task undefined.
- What evidence would resolve it: A systematic evaluation of slot-filling performance across a wider range of parameter sizes (e.g., 13B, 70B) using the same reasoning supervision.

### Open Question 2
- Question: Does optimization for reasoning in text LLMs inherently degrade the general linguistic knowledge required for speech understanding tasks?
- Basis in paper: [explicit] The authors observe that DeepSeek R1 Distill performs worse and hypothesize it is "over-specialized for abstract reasoning tasks... which may cause it to distort linguistic knowledge."
- Why unresolved: The paper identifies the performance drop but does not isolate the specific mechanism of catastrophic forgetting or domain interference in the foundation model.
- What evidence would resolve it: Probing experiments on the foundation models to measure retention of general language benchmarks versus reasoning benchmarks after domain-specific optimization.

### Open Question 3
- Question: What is the optimal text foundation type—base, instruct, or reasoning-optimized—for maximizing performance in reasoning speechLLMs?
- Basis in paper: [explicit] The introduction poses the question: "What type of foundation LLM is most appropriate for this task: a base model, an instruction-tuned model, or one optimized for reasoning?"
- Why unresolved: Results show base models gain most from reasoning supervision, while hybrid models excel in regular settings; a definitive "best" type remains ambiguous due to varying results across model families.
- What evidence would resolve it: A controlled ablation study using a single model family (e.g., Llama 3.1) adapted separately into base, instruct, and reasoning variants.

### Open Question 4
- Question: Does hybrid fine-tuning (combining direct and reasoning supervision) scale effectively to larger parameter sizes beyond the 4B models tested?
- Basis in paper: [inferred] The hybrid method yielded the best results for Qwen 0.6B and 4B, but the authors note they avoided "large and very-large LLMs," leaving the efficacy of hybrid training for 8B+ models unconfirmed.
- Why unresolved: It is unclear if the benefits of hybrid training are specific to the capacity constraints of smaller models or if they represent a universal training paradigm.
- What evidence would resolve it: Applying the hybrid training protocol to the 8B Llama models and comparing results against the single-mode baselines established in the paper.

## Limitations
- Domain overfitting in reasoning-specialized text LLMs may conflict with spoken language understanding requirements, though this failure mode is not definitively proven.
- Hybrid training advantages could reflect dataset size rather than training diversity, as proper ablation studies were not conducted.
- Reasoning supervision benefits appear highly scale-dependent, with smaller models (<4B parameters) showing degradation, limiting generalizability.

## Confidence
- **High confidence**: The experimental methodology is sound, with clear ablation studies across foundation LLM types and training modes. The finding that hybrid training outperforms single-mode approaches is robust across multiple model scales and appears replicable.
- **Medium confidence**: The mechanism explaining why reasoning-specialized text LLMs underperform (domain overfitting) is plausible but not definitively proven. Alternative explanations include training hyperparameters or data distribution shifts not controlled for in the study.
- **Low confidence**: The claim that explicit reasoning traces create universally beneficial intermediate representations lacks sufficient evidence. The paper demonstrates correlation but does not establish causation at the representation level, and related work shows zero-shot approaches without explicit reasoning can achieve competitive results.

## Next Checks
1. **Scale-Agnostic Reasoning Evaluation**: Conduct controlled experiments varying model scale (0.6B to 34B parameters) while holding all other variables constant, to determine the precise threshold where reasoning supervision transitions from beneficial to harmful.

2. **Dataset Size Ablation for Hybrid Training**: Create matched-size datasets for regular-only, reasoning-only, and hybrid training modes to isolate whether hybrid advantages stem from training diversity versus simply having more total examples.

3. **Cross-Domain Generalization Test**: Evaluate speechLLMs trained with reasoning supervision on out-of-domain slot-filling tasks to assess whether the claimed intermediate representations generalize beyond the training distribution.