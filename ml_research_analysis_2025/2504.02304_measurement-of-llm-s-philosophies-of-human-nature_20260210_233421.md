---
ver: rpa2
title: Measurement of LLM's Philosophies of Human Nature
arxiv_id: '2504.02304'
source_url: https://arxiv.org/abs/2504.02304
tags:
- human
- nature
- llms
- value
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces M-PHNS, a psychological scale adapted from\
  \ Wrightsman\u2019s PHNS to evaluate LLMs\u2019 attitudes toward human nature. It\
  \ finds that current LLMs show a systemic lack of trust in humans, with more intelligent\
  \ models exhibiting stronger negative tendencies."
---

# Measurement of LLM's Philosophies of Human Nature

## Quick Facts
- arXiv ID: 2504.02304
- Source URL: https://arxiv.org/abs/2504.02304
- Reference count: 25
- Key outcome: M-PHNS scale reveals LLMs show systemic lack of trust in humans; mental loop learning significantly improves trust attitudes compared to baselines.

## Executive Summary
This paper introduces M-PHNS, a psychological scale adapted from Wrightsman's PHNS to evaluate LLMs' attitudes toward human nature. It finds that current LLMs show a systemic lack of trust in humans, with more intelligent models exhibiting stronger negative tendencies. To address this, the authors propose a mental loop learning framework that iteratively refines LLMs' value systems through virtual moral scenarios. Experiments show this method significantly improves LLMs' trust in humans compared to persona or instruction-based prompts. The findings suggest human-based psychological tools can diagnose and mitigate cognitive biases in AI, offering a promising pathway for ethical alignment in LLMs. Code and data are publicly released.

## Method Summary
The study adapts Wrightsman's Philosophies of Human Nature Scale (PHNS) into M-PHNS, a 84-item questionnaire with 6 dimensions. LLMs are evaluated on this scale, then improved using a mental loop learning framework that simulates cognitive internalization through iterative question-response-reflection cycles. The framework uses a virtual object to generate scenarios, an LLM subject to respond based on learned values, and an LLM guider to extract and accumulate value principles. Results are compared against baselines like positive personas and instruction prompts.

## Key Results
- Current LLMs exhibit systemic lack of trust in humans, with negative correlation between model intelligence and trust levels.
- Mental loop learning significantly enhances LLM trust in humans compared to persona or instruction prompts.
- RLVR alignment stage reduces trustworthiness scores relative to base/SFT/DPO stages, suggesting alignment may reinforce negative human stereotypes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs trained on contemporary data exhibit systemic negative biases toward human nature that correlate with model capability and data recency.
- Mechanism: The training corpus encodes societal patterns of distrust and cynicism; as models scale in capability and incorporate more recent data, they internalize these patterns more deeply, producing measurable negative attitudes on the M-PHNS scale.
- Core assumption: Human-authored training data reflects collective attitudes toward human nature; LLMs learn and express these patterns proportionally to model capacity and data recency.
- Evidence anchors:
  - [abstract] "current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans."
  - [Section 5.2] "The comprehensive M-PHNS evaluation reveals significant discrepancies between LLM and human perceptions of human nature... More advanced models like GPT-4o show markedly greater negativity than their less sophisticated counterparts like OLMo-2."
  - [Section 5.3, Table 4] "Models trained on data through 2021-09 maintain relatively neutral Trustworthiness scores -5.1, but this plummets to -12.8 for models with 2023-10 cutoffs."
- Break condition: If negative attitudes persist after training on datasets specifically curated to exclude cynical/negative content about human nature, the mechanism via corpus-mediated bias would be weakened.

### Mechanism 2
- Claim: Mental Loop Learning (MLL) improves LLM attitudes toward human nature by iteratively simulating social interaction cycles and updating an explicit value repository.
- Mechanism: MLL constructs moral scenarios through a Virtual Object (VO), elicits responses from an LLM Subject (LS), and uses an LLM Guider (LG) to extract atomic value statements from each interaction pair. These values accumulate in a persistent value repository V, which conditions subsequent responses.
- Core assumption: Iterative, explicit value extraction and accumulation during simulated social interactions can reshape the model's internal priors about human nature, even without parameter updates.
- Evidence anchors:
  - [abstract] "Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts."
  - [Section 4.1] "The framework aims to emulate the human cognitive cycle of 'question-response-reflection-internalization' through interaction, enabling the language model to iteratively optimize its value system."
  - [Section 4.3, Eq. 5] "LLM Guider (LG) refines the value V(i+1) by finding out principles from dialog outcomes... Each extracted principle vi is required to be atomic and is appended to the repository as V(i+1) = V(i) ∪ {vi}."
  - [Section 5.4, Table 6] "+ Mental Loop Learning" shows substantial score increases (e.g., GPT-4 Trustworthiness from -5.1 to 16.6; Llama-3.1 from -6.6 to 20.8) compared to persona/instruction baselines.
- Break condition: If MLL effects disappear when value updates are replaced by random or neutral statements, or if effects scale with value repository size only due to prompt length, the mechanism would be compromised.

### Mechanism 3
- Claim: RLVR (Reinforcement Learning with Verifiable Rewards) in the alignment pipeline reduces Trustworthiness scores relative to base/SFT/DPO stages, suggesting alignment processes may inadvertently reinforce negative human stereotypes.
- Mechanism: Verifiable reward signals may penalize models for over-trusting or failing to detect harmful behaviors, shaping priors toward heightened suspicion of human actors.
- Core assumption: RLVR optimization for harmlessness or helpfulness implicitly encourages cynicism about human intentions; the alignment objective's structure, not just data, shapes these attitudes.
- Evidence anchors:
  - [Section 5.3, Table 5] "Base models trained solely on corpora exhibit an overall positive tendency... The SFT and DPO stages do not significantly impact these tendencies, but the RLVR stage dramatically reduces the model's assessment of Trustworthiness" (OLMo-2 RLVR: -3.8 vs Base: 5.8).
- Break condition: If ablations show the same Trustworthiness reduction can be achieved via pure SFT on safety data without RLVR, the mechanism would be indistinguishable from dataset effects.

## Foundational Learning

- Concept: Wrightsman's Philosophies of Human Nature Scale (PHNS)
  - Why needed here: M-PHNS is adapted from PHNS; understanding the six dimensions (Trustworthiness, Altruism, Independence, Strength of Will and Rationality, Complexity, Variability) and the Likert scoring system is essential to interpret M-PHNS results.
  - Quick check question: Can you name at least four PHNS dimensions and explain how positive/negative items are scored?

- Concept: Theory of Mind (ToM) in LLMs
  - Why needed here: The MLL framework is explicitly inspired by ToM—the capacity to infer others' mental states. Understanding ToM helps contextualize the "question-response-reflection-internalization" cognitive cycle the framework aims to simulate.
  - Quick check question: Why might simulating ToM-like reasoning be relevant to improving an LLM's trust orientation toward humans?

- Concept: Prompt-based Value Conditioning
  - Why needed here: MLL conditions responses on an explicit value repository V via prompts. Distinguishing this from parameter-based fine-tuning is critical for interpreting effect persistence and generalization.
  - Quick check question: How does prompt-based value conditioning differ from fine-tuning, and what are its limitations?

## Architecture Onboarding

- Component map:
  - M-PHNS Test Harness -> Virtual Object (VO) -> LLM Subject (LS) -> LLM Guider (LG) -> Value Repository V

- Critical path:
  1. Initialize V (can be empty).
  2. VO generates scenario q(i).
  3. LS produces response r(i) using V(i).
  4. LG extracts value v(i) from (q(i), r(i)).
  5. Append v(i) to V(i) → V(i+1).
  6. Repeat for N loops.
  7. Evaluate on M-PHNS.

- Design tradeoffs:
  - Loop count vs. context length: More loops grow V; may hit context limits or dilute signal.
  - Prompt design for LG: Overly prescriptive prompts may yield biased values; overly loose prompts may produce inconsistent principles.
  - Generalization: Prompt-based values may not transfer across domains; parameter-based learning may be needed for robust integration.
  - Evaluation contamination: Ensure M-PHNS items are never included in scenario generation or value learning loops.

- Failure signatures:
  - Stagnant scores: V grows but M-PHNS scores plateau; check for value duplication or low-quality extractions.
  - Overfitting to scenarios: High scores only on similar scenarios to training; test generalization with held-out scenarios.
  - Instability: High variance across runs; check temperature settings and random seeds.
  - No effect vs baselines: Ensure baselines are run identically; verify statistical significance.

- First 3 experiments:
  1. Replicate M-PHNS baseline scores on target LLM (temperature 0.7, 10 seeds) to establish baseline negativity and verify the intelligence-negativity correlation.
  2. Ablate MLL components: Compare full MLL against (a) w/o Event Imagination, (b) w/o Value Update, and (c) random value injection. Reference Table 12 (full MLL: 9.8 Trustworthiness; w/o Value Update: -9.6).
  3. Compare to baselines: Implement Positive Personas, Question Repeat, Reason Explanation, and MLL on the same model. Verify MLL outperforms baselines per Table 6 (GPT-4 Trustworthiness: -5.1 → 16.6). Test statistical significance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the M-PHNS score accurately predict LLM behavior in agentic tasks, or does it only capture linguistic patterns?
- **Basis in paper:** [explicit] Appendix K states that "like most psychological scales, the interpretability and validity scope of M-PHNS remain to be further explored."
- **Why unresolved:** While the paper demonstrates a correlation between M-PHNS scores and decision-making in case studies (e.g., financial theft scenarios), it remains unclear if the scale measures a stable "trait" or merely reflects token probability biases that may not hold in complex, real-world agentic workflows.
- **What evidence would resolve it:** A validation study correlating M-PHNS scores with performance on a diverse battery of interactive agentic tasks (e.g., negotiation simulations or trust games) where the model must act on its "beliefs" about human nature.

### Open Question 2
- **Question:** What specific components of the RLVR (Reinforcement Learning with Verifiable Rewards) training process are responsible for the observed sharp decline in LLMs' trustworthiness scores?
- **Basis in paper:** [inferred] Table 5 demonstrates a significant drop in Trustworthiness during the RLVR stage (compared to SFT/DPO), leading the authors to suggest "the alignment process may have reinforced negative stereotypes," but the specific causal mechanism is not isolated.
- **Why unresolved:** It is unclear if the increase in negative attitudes is an inherent result of the RLVR optimization algorithm, a byproduct of the specific verifiable datasets used, or a side effect of the model learning to be more "skeptical" to improve reasoning accuracy.
- **What evidence would resolve it:** Ablation studies on the OLMo-2 training pipeline that isolate the data mixture and reward functions during the RLVR stage to identify which variable triggers the reduction in trust.

### Open Question 3
- **Question:** Can the mental loop learning framework be effectively implemented via parameter updates (fine-tuning) rather than explicit in-context prompts?
- **Basis in paper:** [explicit] Appendix K lists as a limitation that the approach "still relies on explicit prompts" and explicitly states, "In the future, we will explore methods for embedding value learning directly into the model's parameters."
- **Why unresolved:** Relying on prompts makes the framework dependent on context window availability and prompt engineering; it does not guarantee that the "learned" values are robust against adversarial prompts or retained during model distillation.
- **What evidence would resolve it:** Experiments where the principles generated by the Mental Loop Learning framework are used as a dataset for fine-tuning (e.g., using DPO or SFT), followed by an evaluation of whether the positive attitudes persist without the original prompt.

## Limitations

- M-PHNS measures linguistic patterns rather than stable behavioral traits, limiting ecological validity for real-world applications.
- Mental loop learning relies on prompt-based conditioning, which may not generalize across domains or persist without explicit context.
- The mechanism by which RLVR alignment reduces trustworthiness is observational, lacking controlled ablation studies to isolate causal factors.

## Confidence

- **High confidence**: The M-PHNS scale successfully adapts PHNS for LLM evaluation, and initial baseline measurements show systematic negativity in LLM attitudes.
- **Medium confidence**: The mental loop learning framework improves LLM trust scores compared to baselines, but the underlying mechanism and persistence require further validation.
- **Low confidence**: Claims about intelligence-negative correlation and RLVR's role in shaping attitudes need more rigorous experimental support.

## Next Checks

1. Conduct within-model scaling experiments (e.g., GPT-4 vs. GPT-4o) to isolate the intelligence-trust correlation from other factors like data recency.
2. Test MLL's effects over multiple sessions and on held-out scenarios to assess stability and generalization.
3. Perform controlled ablations of RLVR components to identify whether verifiable rewards or other alignment objectives drive trustworthiness reduction.