---
ver: rpa2
title: 'Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck'
arxiv_id: '2503.21757'
source_url: https://arxiv.org/abs/2503.21757
tags:
- tokens
- image
- visual
- vision
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to compress vision tokens in large
  vision-language models into a small number of summary tokens that work for both
  generative and discriminative tasks. The key idea is to use a "double-forward pass"
  training strategy where the model first condenses visual information into summary
  tokens, then uses those for both generation and contrastive learning.
---

# Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck

## Quick Facts
- **arXiv ID**: 2503.21757
- **Source URL**: https://arxiv.org/abs/2503.21757
- **Reference count**: 40
- **Primary result**: Achieves 2× higher compression rate without compromising performance for both generative and discriminative tasks

## Executive Summary
This work introduces Fwd2Bot, a method for compressing vision tokens in large vision-language models (LVLMs) into a small number of summary tokens that work effectively for both generative and discriminative tasks. The key innovation is a "double-forward pass" training strategy where the model first condenses visual information into summary tokens, then uses those for both generation and contrastive learning. This results in highly informative compressed representations. For generative tasks, the method achieves 2× higher compression rate without compromising performance, outperforming prior work. For discriminative tasks like image retrieval and compositionality, it sets new state-of-the-art results. The method works with different model sizes and shows strong performance even with very few summary tokens.

## Method Summary
Fwd2Bot compresses vision tokens in LVLMs using a double-forward pass training strategy. During the first forward pass, learnable summary tokens interact with all image tokens and a fixed summarization prompt, creating a compressed representation. The second forward pass uses only these compressed tokens with an instruction for generation. This forces the model to encode task-agnostic visual information efficiently. The method combines autoregressive loss for generation with contrastive loss for discrimination, applied after the first pass to boost representation strength. Stage-specific LoRA adapters enable dual-mode operation, allowing the model to specialize its attention patterns for compression versus generation without catastrophic interference.

## Key Results
- Achieves 2× higher compression rate without compromising performance on generative tasks compared to prior work
- Sets new state-of-the-art results for discriminative tasks like image retrieval and compositionality
- Maintains strong performance even with very few summary tokens (4-32 tokens)
- Outperforms generative-only training on retrieval tasks by 22.5 percentage points (83.8% vs 61.3% R@1 on Flickr30K)

## Why This Works (Mechanism)

### Mechanism 1: Double-Forward Pass Bottleneck Creates Task-Agnostic Compression
Passing learnable summary tokens through the LLM alongside image tokens in a first forward pass, then using only the compressed output in a second pass, forces the model to encode task-agnostic visual information efficiently. The first forward pass enables summary tokens to interact with all image tokens and a fixed summarization prompt. The output becomes the compressed representation used in the second pass with autoregressive loss, creating gradient pressure that forces the compressed representation to retain sufficient information for downstream tasks.

### Mechanism 2: Contrastive Loss Enhances Both Discrimination and Generation
Adding a contrastive loss on the compressed tokens after the first forward pass improves retrieval performance and also benefits generative tasks by enforcing better representation structure. The contrastive loss computes cosine similarity between pooled compressed visual embeddings and text embeddings, encouraging separable representations. This explicitly shapes the embedding space, which the autoregressive loss alone does not do. Generative-only training degrades retrieval (61.3% → 83.8% R@1 improvement with both losses).

### Mechanism 3: Stage-Specific LoRA Adapters Enable Dual-Mode Operation
Using separate LoRA adapters for the compression stage and generation stage allows the model to specialize its attention patterns without catastrophic interference. Stage-specific adapters mean different low-rank weight updates for compression vs. generation. Compression requires dense attention to all visual tokens to extract information, while generation attends selectively. Adapter norm divergence across layers confirms different attention patterns are learned for each stage.

## Foundational Learning

- **Concept**: LVLM architecture (vision encoder → projector → LLM)
  - **Why needed here**: Fwd2Bot builds directly on LLaVA-1.5, reusing its vision encoder and LLM without architectural changes. Understanding this pipeline is essential to see where compression is inserted.
  - **Quick check question**: Can you trace how a 336×336 image becomes 576 vision tokens in LLaVA, and where Fwd2Bot intervenes?

- **Concept**: LoRA (Low-Rank Adaptation)
  - **Why needed here**: All training uses LoRA adapters; stage-specific adapters are a core contribution. Understanding rank constraints and weight composition is critical.
  - **Quick check question**: If LoRA rank r = 8 and hidden dimension d = 4096, how many parameters does one LoRA module add to a weight matrix?

- **Concept**: Contrastive learning (CLIP-style)
  - **Why needed here**: The discriminative loss is a standard image-text contrastive objective. Understanding how cosine similarity and batch negatives shape the embedding space is necessary.
  - **Quick check question**: In a batch of 64 image-text pairs, how many negative pairs does each image use for contrastive loss?

## Architecture Onboarding

- **Component map**: Image → CLIP ViT-L/14 → Hv (576×d) → Projector → [Hv; Hp; Hr] → LLM (Pass 1) → Hc_v → Contrastive loss → [Hc_v; instruction] → LLM (Pass 2) → Answer → Autoregressive loss

- **Critical path**:
  1. **Training (offline)**: Image → CLIP → [Hv; Hp; Hr] → LLM (pass 1) → Hc_v → contrastive loss
  2. **Training (continued)**: [Hc_v; instruction] → LLM (pass 2) → autoregressive loss
  3. **Inference (offline)**: Image → CLIP → compression forward pass → store Hc_v
  4. **Inference (online)**: Load Hc_v + instruction → generation forward pass → output

- **Design tradeoffs**:
  - **Token count k'**: Lower = more compression but higher information loss. Paper shows 16 tokens nearly matches full performance; 4 tokens degrades VQAv2 by ~4 points
  - **Adapter rank**: Higher rank = more expressiveness but higher overfit risk. Paper uses r = 8
  - **Training data mix**: 1:1 ratio of discriminative (CC3M) to generative (LLaVA-665k). Imbalance may favor one task type
  - **Attention type**: Causal attention used throughout; bidirectional attention for compression helps retrieval but harms generation

- **Failure signatures**:
  - **Retrieval much worse than generation**: Check if contrastive loss is being applied (L_disc may be disabled or batch size too small for effective negatives)
  - **Hallucination increases with fewer tokens**: Expected; early summary tokens encode coarse information, later tokens encode finer details. Masking early tokens causes 10%+ drop
  - **Generation works but retrieval fails on new domains**: Contrastive training may have overfit to CC3M caption style; consider domain-specific fine-tuning
  - **Full fine-tuning underperforms LoRA**: Check for overfitting; use stage-specific LoRA instead

- **First 3 experiments**:
  1. **Sanity check**: Train with k' = 32 tokens and only autoregressive loss on LLaVA-665k. Verify generation works (MMB, VQAv2) but retrieval degrades (~60% Flickr30K R@1 expected). This confirms the baseline.
  2. **Add contrastive loss**: Enable L_disc with CC3M data, keeping 1:1 mix. Measure retrieval improvement (target: 80%+ Flickr30K R@1) and check if generation metrics hold or improve. Both should improve.
  3. **Ablate stage-specific adapters**: Compare single shared LoRA vs. stage-specific LoRA. Expect ~1-2 point gain on generation benchmarks and more stable training. Visualize adapter norms to confirm divergence across layers.

## Open Questions the Paper Calls Out

### Open Question 1
Can a specialized pre-alignment training phase enable the use of bidirectional attention during the compression pass to boost discriminative performance without degrading generative capabilities? The current method relies on causal attention to maintain generative performance. Directly switching to bidirectional attention disrupts the autoregressive optimization, creating a performance trade-off the authors did not solve.

### Open Question 2
How sensitive is the compression quality to the specific text prompt used during the first forward pass, and is the reliance on natural language instructions necessary? It is unclear if the prompt actively guides the compression semantics or if it acts as a mere anchor. If the prompt biases the tokens towards linguistic concepts, it might limit the representation of non-linguistic visual features.

### Open Question 3
Does the double-forward bottleneck generalize to LVLM architectures that use complex, non-linear vision-language connectors (e.g., Q-Formers) rather than simple linear projection layers? The method assumes the LLM can effectively "rephrase" visual features in its input space. Architectures with aggressive compression stages might produce features that are harder for the LLM to summarize further without significant information loss.

## Limitations

- **Training Configuration Ambiguity**: Key hyperparameters like LoRA rank, alpha, learning rate schedule, and exact batch size are unspecified, creating barriers to faithful reproduction
- **Generalization Scope**: Evaluation focuses heavily on LLaVA-1.5-based models (7B and 13B), with limited testing on other LVLM architectures
- **Marginal Adapter Contribution**: Stage-specific LoRA improvement shown in Table 6 is marginal (64.4 vs 64.3 MMB), making it difficult to assess practical significance

## Confidence

- **High Confidence**: Core double-forward pass mechanism and implementation are well-specified and directly supported by equations and experimental setup
- **Medium Confidence**: Reported performance improvements are well-documented across multiple benchmarks, but exact training conditions cannot be fully verified due to missing hyperparameter details
- **Low Confidence**: Claims about working "across different model sizes" are based on limited evidence (only LLaVA-7B and 13B tested)

## Next Checks

1. **Ablation of Contrastive Loss Components**: Replicate the generative-only vs. combined loss experiment to verify retrieval performance improves from ~60% to ~84% R@1 on Flickr30K when contrastive loss is added, while maintaining or improving generation metrics.

2. **Stage-Specific Adapter Impact Test**: Implement both single shared LoRA and stage-specific LoRA configurations to measure actual performance difference on both generation and retrieval tasks. Compare adapter norms across layers to confirm claimed divergence in attention patterns.

3. **Token Count Sensitivity Analysis**: Systematically evaluate performance trade-off across different summary token counts (4, 8, 16, 32) on both generation (MMB, VQAv2) and retrieval (Flickr30K) tasks to identify optimal compression point and verify claimed 2× compression rate.