---
ver: rpa2
title: Improving Recommendation Fairness via Graph Structure and Representation Augmentation
arxiv_id: '2508.19547'
source_url: https://arxiv.org/abs/2508.19547
tags:
- fairness
- sensitive
- data
- graph
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness issues in graph-based recommender
  systems, where Graph Convolutional Networks (GCNs) propagate sensitive user attributes
  through the graph structure, amplifying bias. Existing fairness-aware methods focus
  on representation learning but neglect the impact of biased data, leading to limited
  fairness improvement.
---

# Improving Recommendation Fairness via Graph Structure and Representation Augmentation

## Quick Facts
- arXiv ID: 2508.19547
- Source URL: https://arxiv.org/abs/2508.19547
- Reference count: 40
- Primary result: FairDDA significantly improves fairness (reduces DP by up to 17.5% and EO by up to 16.3%) while maintaining or slightly improving recommendation utility on MovieLens-1M and LastFM-360K

## Executive Summary
This paper addresses fairness issues in graph-based recommender systems where Graph Convolutional Networks propagate sensitive user attributes through the graph structure, amplifying bias. The authors propose FairDDA, a dual data augmentation framework that combines graph structure augmentation (pruning sensitive edges) and representation augmentation (masking sensitive features). By identifying sensitive edges and features through comparisons between performance-oriented and fairness-aware recommendations, FairDDA achieves significant fairness improvements while maintaining recommendation utility, outperforming state-of-the-art fairness-aware approaches.

## Method Summary
FairDDA operates in two phases: pre-training and main training. During pre-training, it trains a performance-oriented LightGCN (X^p) and a biased representation encoder (X^b) that predicts sensitive attributes. In the main training phase, it generates augmented graph structures by pruning sensitive edges identified through ranking differences, and creates augmented representations by masking sensitive feature dimensions. The method jointly optimizes multiple losses including BPR for recommendation, reconstruction for augmented representations, contrastive learning for preference preservation, and HSIC minimization to enforce independence from sensitive attributes.

## Key Results
- FairDDA reduces Demographic Parity (DP) by up to 17.5% and Equal Opportunity (EO) by up to 16.3% compared to baselines
- Recommendation utility is maintained or slightly improved, with NDCG and Recall showing comparable or better performance
- Edge pruning contributes more to fairness improvement than feature masking, particularly on dense graphs
- The method demonstrates effectiveness in multi-class sensitive attribute scenarios beyond binary gender
- Visualization confirms FairDDA better balances fairness and preference modeling compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning sensitive edges reduces bias propagation through the graph structure
- Mechanism: Edges where items rank higher in performance-oriented vs. fairness-aware outputs are identified as "sensitive" and receive higher pruning probabilities via Bernoulli sampling with Gumbel-softmax approximation
- Core assumption: Items ranked lower in fairness-aware recommendations contribute disproportionately to bias
- Evidence anchors: Abstract mentions edge pruning helps improve fairness; Section 4.4.1 describes ranking difference calculation for pruning probability
- Break condition: If edge pruning removes >30% of interactions or significantly reduces graph connectivity, utility degrades faster than fairness improves

### Mechanism 2
- Claim: Masking sensitive feature dimensions prevents the model from relying on biased representations
- Mechanism: A feature detector network computes similarity between biased representations (trained to predict sensitive attributes) and debiased representations, suppressing dimensions with high similarity
- Core assumption: Feature dimensions where biased and debiased representations align encode sensitive information that should be removed
- Evidence anchors: Abstract mentions masking sensitive features; Section 4.4.2 shows mask computation based on similarity between representations
- Break condition: If the biased representation classifier achieves <70% accuracy on attribute prediction, the feature detector lacks a reliable signal for masking

### Mechanism 3
- Claim: Minimizing HSIC between augmented and biased representations enforces independence from sensitive attributes
- Mechanism: HSIC measures statistical dependence between random variables, and minimizing it forces augmented representations to become independent from biased representations encoding sensitive information
- Core assumption: HSIC effectively captures and can minimize dependence between representations in high-dimensional embedding spaces
- Evidence anchors: Abstract mentions minimizing dependence between representations and sensitive information; Section 4.5 explains HSIC measures statistical independence
- Break condition: If λ_d (HSIC weight) is set too high, the model loses predictive signal and utility drops sharply

## Foundational Learning

- **Graph Convolutional Networks (GCNs) and message passing**
  - Why needed here: Understanding how GCNs propagate information across the user-item bipartite graph is essential to grasp why bias amplifies through structure
  - Quick check question: Can you explain why homophily (similar nodes connecting) amplifies sensitive attribute propagation in GCNs?

- **Group fairness metrics (Demographic Parity and Equal Opportunity)**
  - Why needed here: DP measures whether recommendation distributions differ across groups; EO measures whether users with similar interests receive equal treatment regardless of group
  - Quick check question: Given two demographic groups, what would a DP value of 0.30 vs. 0.10 indicate about recommendation fairness?

- **Hilbert-Schmidt Independence Criterion (HSIC)**
  - Why needed here: HSIC is the core debiasing mechanism; understanding kernel-based independence testing explains why minimizing HSIC between representations enforces fairness
  - Quick check question: Why does HSIC = 0 imply statistical independence, and what kernel function does the paper use?

## Architecture Onboarding

- **Component map:** Pre-training phase: (1) Performance-oriented encoder (LightGCN with BPR loss) → X^p; (2) Biased encoder (attribute prediction classifier) → X^b; Main training phase: Debiased encoder with three parallel streams: Augmented graph G^a (edge pruning via Hypothesis 1), Augmented representations X^a (feature masking via Hypothesis 2), Contrastive learning aligning X^d ↔ X^a

- **Critical path:** Pre-train X^p and X^b first; then in main training: (1) compute edge pruning probabilities, (2) compute feature masks, (3) pass through GCN to get augmented representations, (4) compute all losses, (5) backprop through Gumbel-softmax approximation

- **Design tradeoffs:** Higher λ_d → better fairness but potential utility loss; aggressive edge pruning → fairness gains but fragmented user behavior patterns; feature masking vs. edge pruning: ablation shows edge pruning has larger fairness impact on ML-1M; masking is dataset-dependent

- **Failure signatures:** Utility drops >5%: λ_d too high or pruning probability threshold too aggressive; Fairness metrics unchanged: biased representation classifier not learning (check cross-entropy loss); Training instability: Gumbel temperature τ too low; increase to smooth gradient approximation

- **First 3 experiments:** (1) Reproduce baseline comparison on ML-1M with LightGCN backbone; verify DP@10, EO@10, NDCG@10 match Table 2 within 1-2% tolerance; confirm pre-training converges before main training; (2) Ablation study: Run FairDDA-L_dl, FairDDA-EP, FairDDA-FM variants; expect largest fairness drop when debiasing loss is removed; verify edge pruning contributes more than feature masking on dense graphs; (3) Hyperparameter sweep on λ_d ∈ {10, 20, 30, 40, 50}; plot DP@10 vs. NDCG@10 tradeoff curve; identify knee point where fairness gains plateau while utility degrades

## Open Questions the Paper Calls Out

- **Question 1:** Can inserting fairness-enhancing edges into the user-item interaction graph yield better utility-fairness trade-offs than the current edge pruning strategy?
  - Basis: The conclusion states future work will explore additional data augmentation methods, such as inserting fairness-enhancing edges
  - Why unresolved: Current FairDDA exclusively focuses on removing (pruning) edges; potential benefit of actively adding edges remains unexplored
  - What evidence would resolve it: Experimental results from a modified FairDDA version that utilizes edge insertion, comparing its NDCG/Recall and DP/EO scores against pruning-only approach

- **Question 2:** Is the framework computationally feasible for industrial-scale graphs given the cubic complexity of the debiasing component?
  - Basis: Section 4.7.2 acknowledges the debiasing learning step involves constructing m×m kernel matrices with O(m²d + m³) complexity
  - Why unresolved: Experiments limited to relatively small academic datasets; does not demonstrate performance on graphs with billions of interactions
  - What evidence would resolve it: Complexity tests on datasets with significantly higher interaction densities, or integration of approximate HSIC computation methods

- **Question 3:** Does Hypothesis 1 inadvertently prune interactions that represent legitimate user preferences rather than bias amplification?
  - Basis: Hypothesis 1 identifies sensitive edges based on ranking differences, but doesn't verify if drops are strictly due to bias or genuine preference divergence
  - Why unresolved: While paper shows overall utility is maintained, doesn't analyze if "sensitive" interactions removed were actually biased or simply strong preferences penalized by fairness model
  - What evidence would resolve it: Qualitative or quantitative analysis of pruned edges to determine if they correlate with sensitive attributes or specific item genres legitimately preferred by demographic groups

## Limitations

- The paper's edge pruning mechanism assumes the fairness-aware baseline correctly identifies bias sources, which may not hold in all recommendation scenarios
- Feature masking effectiveness depends on the quality of the biased representation classifier, which may not generalize well to attributes beyond binary gender
- The computational complexity of HSIC calculation (O(m³)) could become prohibitive for industrial-scale recommendation systems
- The framework's performance on highly sparse graphs and multi-class sensitive attributes requires further validation

## Confidence

- **High confidence:** The core mechanism of using HSIC for independence enforcement and the experimental methodology (baseline comparisons, ablation studies)
- **Medium confidence:** The hypotheses for identifying sensitive edges/features work as intended across diverse recommendation scenarios
- **Low confidence:** The generalizability to multi-class sensitive attributes and highly sparse graphs

## Next Checks

1. **Ablation on edge pruning threshold:** Systematically vary the Gumbel-softmax temperature and pruning probability threshold to identify the optimal balance between fairness improvement and graph connectivity preservation

2. **Cross-attribute robustness:** Test FairDDA on occupation (multi-class) and age (ordinal) attributes to verify the feature masking mechanism generalizes beyond binary gender

3. **Sparsity stress test:** Evaluate FairDDA on ML-100K (much sparser) to assess whether the method maintains effectiveness when user-item interactions are limited