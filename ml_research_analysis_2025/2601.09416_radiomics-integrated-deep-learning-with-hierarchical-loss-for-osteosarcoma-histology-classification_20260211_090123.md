---
ver: rpa2
title: Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma
  Histology Classification
arxiv_id: '2601.09416'
source_url: https://arxiv.org/abs/2601.09416
tags:
- tumor
- hierarchical
- classification
- viable
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automated classification of osteosarcoma histopathology
  images into non-tumor, non-viable tumor, and viable tumor regions. The key methodological
  innovation is integrating radiomic features with deep learning using a hierarchical
  multi-task loss.
---

# Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification

## Quick Facts
- arXiv ID: 2601.09416
- Source URL: https://arxiv.org/abs/2601.09416
- Reference count: 0
- Three-class osteosarcoma histology classification with 0.86 accuracy and 0.96 AUC

## Executive Summary
This paper presents a multimodal deep learning approach for automated classification of osteosarcoma histopathology images into non-tumor, non-viable tumor, and viable tumor regions. The method integrates radiomic features with CNN image embeddings using a hierarchical multi-task loss framework. By decomposing the three-class problem into two coupled binary tasks (tumor vs. non-tumor, then viable vs. non-viable), and fusing radiomic descriptors with deep learning features via an attention mechanism, the approach achieves state-of-the-art performance on the TCIA osteosarcoma dataset.

## Method Summary
The proposed method uses InceptionV3 as a backbone to extract image embeddings, which are fused with 29 radiomic features (first-order statistics and 2D shape descriptors) through a learnable attention gate. Classification is performed using two coupled binary tasks with a homoscedastic uncertainty-weighted loss to balance task contributions. The model is trained end-to-end with patient-level splits to prevent data leakage, using AdamW optimizer and early stopping based on validation AUC. Extensive experiments demonstrate that the hierarchical loss formulation and multimodal fusion significantly outperform single-task and non-radiomics baselines.

## Key Results
- Overall accuracy of 0.86 and AUC of 0.96 on the TCIA osteosarcoma dataset
- Per-class performance significantly improved for non-viable tumors (Sen@Spe90: 0.83, AUC: 0.93)
- Hierarchical loss with radiomics integration outperformed single-task and non-radiomics baselines
- Patient-level evaluation confirms clinical relevance with no data leakage

## Why This Works (Mechanism)
The hierarchical loss formulation allows the model to learn tumor vs. non-tumor distinction first, then refine viable vs. non-viable classification only for tumor samples. The uncertainty-weighted loss automatically balances the contribution of each task during training, preventing one task from dominating. Fusing radiomic features with deep learning embeddings provides complementary information - radiomics captures quantitative morphology and texture that may be missed by CNN features alone.

## Foundational Learning

**Hierarchical Multi-Task Learning**: Decomposing a complex classification problem into simpler binary tasks, then combining predictions. Needed to handle the three-class problem more effectively than direct multi-class classification. Quick check: verify hierarchical probability computation follows Eq. (3) correctly.

**Homoscedastic Uncertainty Weighting**: Using task-specific uncertainty parameters to dynamically weight loss contributions during training. Needed to balance learning between the two binary tasks without manual tuning. Quick check: monitor uncertainty weights (e^(-λ)) to ensure they stabilize during training.

**Attention-Based Feature Fusion**: Learning how to combine multimodal features (CNN embeddings + radiomic descriptors) through a weighted gate mechanism. Needed to leverage complementary information from different feature types. Quick check: verify attention weights are properly normalized and learned.

## Architecture Onboarding

**Component Map**: Image tiles → InceptionV3 → CNN embedding → MLP projection → Attention gate → Classification heads
                           ↓
                     PyRadiomics → Radiomic features → MLP encoder → Attention gate

**Critical Path**: Image preprocessing → Feature extraction (CNN + radiomics) → Multimodal fusion → Hierarchical classification → Uncertainty-weighted loss

**Design Tradeoffs**: InceptionV3 vs. EfficientNet/ViT backbones (accuracy vs. parameter count), radiomic feature selection (comprehensive vs. task-specific), loss formulation (manual weighting vs. learned uncertainty)

**Failure Signatures**: 
- Performance collapse if patient-level splits not enforced (data leakage)
- Class imbalance causing biased predictions if inverse frequency weighting not applied
- Unstable training if uncertainty weights diverge (λ→∞ or λ→−∞)

**First Experiments**:
1. Verify patient-level train/val/test splits prevent any patient overlap
2. Check per-head class weights are computed correctly from inverse class counts
3. Monitor uncertainty weight evolution during training for stability

## Open Questions the Paper Calls Out
None

## Limitations
- Missing exact patient-to-tile mapping prevents verification of proper patient-level separation
- Unspecified architectural hyperparameters (embedding dimensions, encoder sizes) may affect reproducibility
- Unclear training duration and early stopping criteria could impact performance comparison

## Confidence

**High confidence**: Core experimental design, dataset composition, evaluation metrics, and overall model architecture are well-specified.

**Medium confidence**: Training procedure details (epochs, learning rate schedule, early stopping) and architectural hyperparameters require clarification.

**Medium confidence**: Patient-level splitting methodology and exact implementation of hierarchical probability computation need specification.

## Next Checks
1. Verify no patient appears in both training and test sets by implementing patient-level splits before tile sampling
2. Monitor uncertainty weight evolution (λA, λB) during training to ensure stable hierarchical loss weighting
3. Confirm radiomic feature extraction and normalization pipeline matches the reported 29 features and z-score standardization using training statistics only