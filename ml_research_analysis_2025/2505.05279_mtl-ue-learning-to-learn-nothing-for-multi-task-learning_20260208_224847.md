---
ver: rpa2
title: 'MTL-UE: Learning to Learn Nothing for Multi-Task Learning'
arxiv_id: '2505.05279'
source_url: https://arxiv.org/abs/2505.05279
tags:
- learning
- mtl-ue
- tasks
- data
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of protecting multi-task learning
  (MTL) data from unauthorized training. The authors propose MTL-UE, the first framework
  for generating unlearnable examples (UE) that prevent MTL models from learning useful
  features across multiple tasks.
---

# MTL-UE: Learning to Learn Nothing for Multi-Task Learning

## Quick Facts
- **arXiv ID:** 2505.05279
- **Source URL:** https://arxiv.org/abs/2505.05279
- **Authors:** Yi Yu; Song Xia; Siyuan Yang; Chenqi Kong; Wenhan Yang; Shijian Lu; Yap-Peng Tan; Alex C. Kot
- **Reference count:** 40
- **Primary result:** Introduces MTL-UE, a generator-based framework for creating unlearnable examples that protect multi-task datasets from unauthorized training, significantly outperforming prior methods.

## Executive Summary
This paper addresses the challenge of protecting multi-task learning (MTL) data from unauthorized training. The authors propose MTL-UE, the first framework for generating unlearnable examples (UE) that prevent MTL models from learning useful features across multiple tasks. Unlike previous methods that optimize perturbations per sample, MTL-UE introduces a generator-based structure with label priors and class-wise feature embeddings, which significantly reduces intra-class variance. The method also incorporates intra-task and inter-task embedding regularization to enhance inter-class separation and minimize feature redundancy. Extensive experiments across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5 task-weighting strategies demonstrate consistent improvements in attacking performance.

## Method Summary
MTL-UE is a generator-based framework that creates unlearnable examples for multi-task datasets. It uses an encoder-decoder architecture with learnable class-wise embeddings per task. The generator takes an input image, encodes it to a latent representation, concatenates this with task-specific class embeddings selected based on the labels, and decodes to produce a perturbation. The perturbation is clipped to a small range and added to the original image. The generator is trained using a combined loss that includes the base UE loss (from methods like EM, TAP, or SEP), intra-task embedding regularization to increase inter-class distance, and inter-task embedding regularization to reduce feature redundancy across tasks. This framework significantly reduces intra-class variance compared to traditional sample-wise perturbation methods.

## Key Results
- MTL-UE-TAP reduces STL model accuracy to approximately 68% on CelebA, significantly lower than baseline methods.
- The method shows consistent improvements across 4 MTL datasets (CelebA, ChestX-ray14, UTKFace, NYUv2) and 3 base UE methods.
- MTL-UE demonstrates robustness against defense mechanisms like ISS (Instance-Specific Smoothing).
- The framework supports partial protection of specific tasks while maintaining effectiveness on others.

## Why This Works (Mechanism)

### Mechanism 1: Label-Prior Injection via Class-wise Embeddings Reduces Intra-class Variance
The generator-based architecture uses an encoder to produce a latent z, which is concatenated with learnable class-wise embeddings selected based on task labels. A decoder then generates the perturbation. This shifts the optimization from a high-dimensional pixel-space search to a lower-dimensional decoder output space, structured by the embeddings. The core assumption is that lower intra-class variance in spurious features makes them easier for a victim model to learn as shortcuts, thereby displacing the learning of benign features. Evidence includes the abstract's claim of reduced intra-class variance and Table 3 showing MTL-UE methods significantly reduce average and maximum intra-class standard deviation compared to baselines.

### Mechanism 2: Embedding Regularization Enhances Inter-class Separation and Feature Independence
Two regularization terms are added to the loss: Intra-task ER minimizes cosine similarity between embeddings within each task, pushing them apart to increase inter-class distance. Inter-task ER promotes geometric independence across tasks by minimizing the absolute cosine similarity between embeddings from different tasks. The core assumption is that spurious features with greater inter-class distance and less cross-task redundancy create more distinct and learnable shortcuts for the victim model, preventing it from generalizing to clean test data. Evidence includes the abstract's claim of enhanced inter-class separation and Table 7's ablation study showing performance degradation when removing these regularizations.

### Mechanism 3: Plug-and-Play Framework Leverages Existing UE Objectives
The framework's generator architecture is a plug-and-play module that can integrate any existing surrogate-dependent UE loss function, adapting single-task strategies for multi-task data. The MTL-UE generator is trained by optimizing a combined loss: L = Lb + λ1·LIntra + λ2·LInter, where Lb is the loss from any base UE method. The core assumption is that a generator trained with a specific UE loss will produce perturbations that induce the same type of learning failure in the victim model, but with improved characteristics for multi-task scenarios. Evidence includes the abstract's statement about plug-and-play integration and Table 2's demonstration of consistent improvements over base UE methods when integrated into the MTL-UE framework.

## Foundational Learning
- **Concept: Multi-Task Learning (MTL)** - Needed to understand that the paper targets protecting data used for MTL models, which learn multiple tasks simultaneously via a shared encoder. Quick check: Why would a perturbation affecting a shared encoder's features potentially harm performance across multiple downstream tasks?
- **Concept: Unlearnable Examples (UE) / Clean-Label Data Poisoning** - Needed to understand the core problem: UE are a form of data poisoning where imperceptible perturbations are added to training data to degrade a model's performance on clean test data without changing the labels. Quick check: What is the primary difference in objective between an adversarial example and an unlearnable example?
- **Concept: Surrogate Models and Bi-level Optimization** - Needed to understand that many UE methods use a surrogate model to optimize perturbations, and the attack's success depends on how well this surrogate mimics the training process of a potential victim model. Quick check: When generating unlearnable examples, why is it necessary to use a surrogate model that represents a plausible victim architecture?

## Architecture Onboarding
- **Component map:** Input image x -> Encoder E (9 conv layers) -> Latent z -> Concatenation with {ek_yk}K_k=1 -> Decoder D (4 ConvTranspose2d) -> Perturbation δ (clipped) -> Poison image x' = x + δ
- **Critical path:** 1) Input a clean image batch (x, y). 2) Select the appropriate class embeddings {ek_yk}K_k=1 based on the labels y. 3) Encode the image: z = E(x). 4) Concatenate latent z with embeddings. 5) Decode to get perturbation: δ = Clip(D([z, embeddings]), -ε, ε). 6) Create the poisoned image: x' = x + δ. 7) Compute loss L = Lb + λ1·LIntra + λ2·LInter. 8) Update generator parameters via backpropagation.
- **Design tradeoffs:** Embedding Dimension (higher allows complex spurious features but increases memory/computation; paper uses 16×H/2×W/2). Base UE Method (EM vs TAP vs SEP affects training process and perturbation characteristics). Batch size configured identically to MTL models but unconfirmed.
- **Failure signatures:** Perturbations appear as random noise (generator fails to learn structured features). High intra-class variance (regularization fails, reducing attack power). No degradation of victim model accuracy (ultimate failure, indicating perturbations are not unlearnable).
- **First 3 experiments:** 1) Single-Task Baseline Reproduction: Train MTL-UE generator on a single-task dataset using base method TAP, compare against standard TAP. 2) Multi-Task Ablation: Train full MTL-UE on UTKFace, remove LIntra and LInter one by one, quantify attack performance drop and measure intra-class variance. 3) Backbone Transfer Test: Generate unlearnable examples using ResNet-18 surrogate, train new victim MTL models with different backbones (VGG-16, DenseNet-121) on poisoned data, report accuracy degradation.

## Open Questions the Paper Calls Out
- **Scalability to many tasks:** How does MTL-UE performance against STL models scale when the number of tasks increases significantly beyond the 40 tasks tested on CelebA? The paper notes performance under more tasks remains an area for future exploration.
- **Partial protection effectiveness:** Can the framework be adapted to remain effective when only a strict subset of the training data is protected, rather than requiring full dataset poisoning? The paper notes effectiveness drops quickly when data is not fully unlearnable.
- **Unified regularization for dense tasks:** Is the exclusion of embedding regularization necessary for dense prediction tasks, or can a unified regularization scheme be designed to support both classification and regression tasks simultaneously? The paper states embedding regularizations are not employed for dense tasks because redundancy in spurious features is minimal.

## Limitations
- Performance on truly large-scale multi-task problems (e.g., >50 tasks) remains untested and may degrade.
- Robustness claims against defense mechanisms like ISS may not generalize to more sophisticated countermeasures.
- Computational overhead introduced by the generator architecture compared to standard UE methods is not thoroughly characterized.

## Confidence
- **High confidence** in the core mechanism: The generator architecture with label priors and embedding regularization demonstrably reduces intra-class variance and improves attack effectiveness, as evidenced by controlled ablation studies and quantitative metrics.
- **Medium confidence** in generalization: While the framework shows good transfer across different backbones and datasets, the extent of this transferability to radically different model families or task types is not fully established.
- **Low confidence** in scalability: The paper does not provide evidence for the method's performance or computational feasibility on extremely large multi-task datasets or when protecting a very high number of tasks simultaneously.

## Next Checks
1. **Scale-up test:** Apply MTL-UE to a multi-task dataset with a significantly larger number of tasks (e.g., CelebA with all 40 tasks) and report both attack effectiveness and computational overhead.
2. **Defense robustness:** Evaluate MTL-UE's resilience against a broader range of data augmentation and preprocessing defenses beyond ISS, such as adversarial training on the poisoned data or differential privacy techniques.
3. **Architecture transfer:** Generate unlearnable examples using a CNN surrogate and attempt to train a Transformer-based MTL model on this data. Quantify the attack's success and analyze the failure cases to understand the limits of transferability.