---
ver: rpa2
title: 'Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate
  Presentation Content with Actionable Feedback'
arxiv_id: '2505.18240'
source_url: https://arxiv.org/abs/2505.18240
tags:
- presentation
- slides
- slide
- text
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating automatically generated
  presentation slides using a reference-free framework called REFLEX. The key innovation
  is generating negative samples with metric-specific perturbations (coverage, redundancy,
  text-image alignment, and flow) to train a contrastive learning model.
---

# Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback

## Quick Facts
- **arXiv ID:** 2505.18240
- **Source URL:** https://arxiv.org/abs/2505.18240
- **Reference count:** 30
- **Primary result:** REFLEX uses metric-specific perturbations to train a Phi3-Mini model to evaluate presentations with scores up to 0.59 correlation vs pseudo-ground truth, outperforming GPT-4o-based evaluators.

## Executive Summary
This paper introduces REFLEX, a reference-free framework for evaluating presentation slides with actionable feedback. The core innovation is generating negative samples via controlled metric-specific perturbations (coverage, redundancy, text-image alignment, flow) and using them to fine-tune a contrastive learning model. By training on these synthetic degradations, REFLEX can score presentations and provide improvement suggestions without requiring ground truth references during inference. The approach significantly outperforms both classical heuristic methods and state-of-the-art LLM evaluators like G-Eval and Phi3-Eval.

## Method Summary
REFLEX generates negative presentation samples by applying metric-specific perturbations (e.g., removing slides for Coverage, shuffling for Flow) to high-quality presentations, creating positive-negative pairs with pseudo-scores. A Phi-3-Mini LLM is fine-tuned using LoRA on these pairs, first for explanation generation and then for scoring. During inference, an explanation model generates feedback which is fed into a scoring model to produce the final score. The framework evaluates four metrics: Coverage, Redundancy, Text-Image Alignment, and Flow, using datasets RefSlides (8,111 presentations) and SciDuet (1,088 pairs).

## Key Results
- Achieves Spearman correlation up to 0.59 with pseudo-ground truth scores, significantly outperforming G-Eval (0.40) and Phi3-Eval (0.46)
- Generates more useful and actionable explanations compared to baseline methods
- Demonstrates strong performance in human evaluations with 50 samples from SciDuet dataset
- Successfully detects subtle quality degradations through the perturbation-based training approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning an LLM using synthetic negative samples generated via metric-specific perturbations enables reference-free evaluation of presentation quality.
- Mechanism: The system generates negative samples by applying controlled perturbations to high-quality presentations targeting specific metrics. A base LLM is fine-tuned using LoRA on these positive-negative pairs, teaching it to discriminate quality levels and generate scores and explanations.
- Core assumption: Synthetic perturbations accurately simulate real-world quality degradations.
- Evidence anchors: [abstract] proposes REFLEX using metric-specific perturbations; [section 5.2] describes applying controllable perturbations; [corpus] related works acknowledge automated slide evaluation challenges.
- Break condition: If perturbations don't generalize to real-world errors (e.g., semantic hallucinations), evaluator performance will degrade.

### Mechanism 2
- Claim: Providing a model with a structured explanation of quality before predicting a score improves score accuracy and robustness.
- Mechanism: Inference pipeline is sequential - first generate explanation, then use both presentation and explanation as input to scoring model. This forces the model to ground its score in specific reasoning about presentation flaws.
- Core assumption: Final score is causally dependent on intermediate explanation quality.
- Evidence anchors: [section 5.2.3] describes feeding presentation and explanation to scoring model; [figure 5] shows explanation-then-score chain.
- Break condition: If explanation model hallucinates flaws or becomes unreliable, scoring model will produce incorrect scores.

### Mechanism 3
- Claim: A small, fine-tuned model (Phi-3-Mini) can outperform larger LLMs (GPT-4o) on this specific task when trained with contrastive examples.
- Mechanism: General LLMs suffer from inconsistency and limited score ranges. Fine-tuning a smaller model with clear positive/negative labels creates a more reliable specialized judge.
- Core assumption: Correlation with pseudo-ground truth and human annotations will generalize to other domains.
- Evidence anchors: [section 6.3.1] shows REFLEX achieving 0.59 vs G-Eval's 0.40 correlation; [section 1] notes G-Eval's limitations.
- Break condition: Performance advantage may disappear on topics not represented in training datasets.

## Foundational Learning

- Concept: **Contrastive Learning**
  - Why needed here: Provides training signal to distinguish high-quality samples from synthetically degraded ones.
  - Quick check question: Can you explain how a perturbation of degree 4 (e.g., shuffling 80% of slides) provides different training signal than degree 1 (shuffling 20%)?

- Concept: **Reference-Free Evaluation**
  - Why needed here: Must evaluate intrinsic quality without gold standard reference during inference.
  - Quick check question: How does REFLEX substitute for lack of ground-truth reference during inference?

- Concept: **LoRA (Low-Rank Adaptation) Fine-tuning**
  - Why needed here: Enables efficient training of separate adapters for each metric rather than maintaining four full LLM copies.
  - Quick check question: Why is LoRA preferred over full fine-tuning for deploying multiple metric-specific evaluation models?

## Architecture Onboarding

- Component map: Preprocessing Module -> Perturbation Engine -> Explanation Model -> Scoring Model
- Critical path: Preprocessing Module is first critical path (errors propagate downstream); Inference Chain (Explanation -> Score) is second critical path (failure in first blocks second).
- Design tradeoffs: Phi-3-Mini balances efficiency with capability; four specific metrics are interpretable but not exhaustive.
- Failure signatures:
  1. Out-of-Domain Content: Presentations with complex charts or domain-specific jargon yield poor scores.
  2. Perturbation Mismatch: Model fails to catch subtle non-syntactic errors if perturbations only taught it to spot shuffled slides.
  3. Cascade Failure: Generic explanation output leads to default score output.
- First 3 experiments:
  1. Reproduce Preprocessing: Run Phi-3-Vision pipeline on 5 slides from different domains and inspect JSON output.
  2. Validate Perturbation Sensitivity: Apply perturbations of degree 1-4 to known high-quality presentation and plot predicted scores.
  3. Ablate Explanation Chain: Run inference with and without explanation model input to measure performance delta.

## Open Questions the Paper Calls Out

- **Scaling Performance**: The authors were unable to use larger models like LLaMA-70B due to computational constraints, leaving potential performance gains from scaling unexplored.
- **Design Evaluation**: The framework focuses solely on content without accounting for visual aesthetics like layout, typography, and color schemes.
- **Generalization to Real Errors**: While effective against synthetic perturbations, performance on complex unscripted errors in naturally generated presentations is unknown.

## Limitations
- Perturbations may not fully capture real-world errors produced by generative AI models, particularly semantic hallucinations
- Method validated only on RefSlides and SciDuet datasets; performance on other domains unknown
- Scoring model's performance is contingent on explanation model quality, creating potential cascade failures

## Confidence
- **High**: Method's effectiveness on validation datasets and core mechanism of using metric-specific perturbations for contrastive training
- **Medium**: Claim that Phi-3-Mini outperforms larger LLMs, as this is dataset-specific
- **Low**: Assumption that perturbations accurately reflect real-world presentation quality degradations

## Next Checks
1. **Perturbation Fidelity Test**: Apply perturbations to high-quality presentation and verify predicted scores decrease monotonically with perturbation degree
2. **Out-of-Domain Performance**: Evaluate REFLEX on presentations from different domain (e.g., business pitch decks) to assess domain transfer
3. **Ablation of Explanation Chain**: Compare scoring performance with and without explanation model's input to quantify chained inference contribution