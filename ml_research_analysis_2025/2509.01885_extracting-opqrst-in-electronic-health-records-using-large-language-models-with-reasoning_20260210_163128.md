---
ver: rpa2
title: Extracting OPQRST in Electronic Health Records using Large Language Models
  with Reasoning
arxiv_id: '2509.01885'
source_url: https://arxiv.org/abs/2509.01885
tags:
- entity
- reasoning
- chief
- extraction
- complaint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of extracting OPQRST patient\
  \ assessment data from unstructured Electronic Health Records (EHRs) using Large\
  \ Language Models (LLMs). The authors reframe the task from sequence labeling to\
  \ text generation, enabling LLMs to provide reasoning steps that mimic a physician\u2019\
  s cognitive process during patient evaluation."
---

# Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning

## Quick Facts
- **arXiv ID:** 2509.01885
- **Source URL:** https://arxiv.org/abs/2509.01885
- **Reference count:** 21
- **Primary result:** F1 scores ranging from 0.838 to 0.944 for OPQRST entity extraction from EHRs using reasoning-enhanced LLM prompting

## Executive Summary
This study addresses the challenge of extracting OPQRST patient assessment data from unstructured Electronic Health Records (EHRs) using Large Language Models (LLMs). The authors reframe the task from sequence labeling to text generation, enabling LLMs to provide reasoning steps that mimic a physician's cognitive process during patient evaluation. This approach improves interpretability and reduces the need for extensive labeled training data. The proposed method uses Llama-2-13B-chat with carefully engineered prompts that guide the model through extracting chief complaints and OPQRST entities.

## Method Summary
The method reframes OPQRST extraction from EHRs as a text generation task rather than sequence labeling. It uses Llama-2-13B-chat with a six-part prompt structure: task definition, entity definitions, physician simulation placeholders, reasoning steps with self-verification, few-shot examples, and formatting instructions. The approach leverages in-context learning without fine-tuning, using a small dataset of 85 annotated HPI samples. The reasoning steps decompose the extraction process to mimic physician cognition, first identifying the chief complaint before extracting OPQRST entities. A self-verification step helps reduce hallucinations by forcing the model to review its own reasoning before finalizing the output.

## Key Results
- F1 scores ranging from 0.838 to 0.944 across different OPQRST entities
- Full prompting method outperforms baseline approaches including prefix, cloze, anticipatory, chain-of-thought, and heuristic prompts
- Self-verification significantly improves performance by reducing consolidation failures
- Semantic similarity measures (BERTScore and PromptLLM) provide better evaluation than exact matches in clinical contexts

## Why This Works (Mechanism)

### Mechanism 1: Generative Reframing of Extraction
Reframing Named Entity Recognition from sequence labeling to text generation allows LLMs to leverage implicit clinical context that rigid classifiers miss. Instead of assigning tags to tokens, the model generates answers as text continuations, enabling normalization of verbose clinical language and handling linguistic variability through pre-trained generative capabilities.

### Mechanism 2: Cognitive Simulation via Dependency Decomposition
Decomposing the extraction prompt to mimic physician cognition—specifically identifying the Chief Complaint before identifying OPQRST entities—improves accuracy by resolving context. The prompt forces the model to resolve the "Chief Complaint" first, reducing the search space for subsequent entities and mirroring the human cognitive process of anchoring assessment details to the primary problem.

### Mechanism 3: Self-Verification for Output Consolidation
Instructing the model to verify its own reasoning steps before finalizing the output significantly reduces hallucinations and consolidates the final answer. A self-verification step forces the model to attend to its own generated logic one more time before printing the delimited answer, acting as an internal check against consolidation failures.

## Foundational Learning

- **In-Context Learning (Few-Shot Prompting):** Required because the study explicitly avoids fine-tuning due to data scarcity, relying entirely on the model's ability to learn extraction patterns from examples provided in the prompt.
  - Quick check: Can you identify the difference between updating model weights (fine-tuning) vs. updating the input context (in-context learning)?

- **Semantic Similarity Metrics (BERTScore):** Needed because traditional NER metrics rely on exact string matches while generative models often output synonyms or grammatical variations.
  - Quick check: If the ground truth is "chest pain" and the model outputs "chest discomfort," would an Exact Match F1 score capture this as correct?

- **OPQRST Clinical Mnemonic:** Essential to understand that these entities are interconnected attributes of a symptom (Onset, Provocation, Quality, etc.) rather than random keywords.
  - Quick check: Why is "Provocation" (what makes it worse) logically distinct from "Palliation" (what makes it better), and how might an LLM confuse them without clear definitions?

## Architecture Onboarding

- **Component map:** Unstructured HPI text -> Prompt Constructor (6-part template) -> Llama-2-13B-chat (Hugging Face Accelerate) -> Output Parser (regex for `@` tokens) -> Evaluator (BERTScore/PromptLLM vs Ground Truth)

- **Critical path:** 1) Load HPI note, 2) Inject into structured Prompt Template, 3) Generate response (temp=1.0, top_p=0.95), 4) Parse `@content@`, 5) Score against ground truth using semantic similarity

- **Design tradeoffs:** Temperature 1.0 allows flexibility in generating tokens for extracted entities and reasoning, trading raw precision for semantic recall; regex parsing relies on strict formatting instructions; semantic evaluation prioritizes clinical intent over verbatim extraction

- **Failure signatures:** Verbose Extraction (extra tokens like "to arrival"), Consolidation Failure (correct reasoning but wrong final output), Scope Drift (extracts data for secondary symptom)

- **First 3 experiments:** 1) Sanity Check (zero-shot vs few-shot on 5 known HPI notes), 2) Ablation of Reasoning (remove physician simulation and reasoning steps), 3) Metric Correlation (compare Exact Match F1 vs BERTScore/PromptLLM scores)

## Open Questions the Paper Calls Out
- **Question 1:** Does fine-tuning LLMs on clinical corpora provide significant performance gains over the in-context learning approach demonstrated? The study did not consider fine-tuned clinical LLMs as a primary limitation.
- **Question 2:** Can human-in-the-loop annotation, where annotators view model reasoning, improve ground truth label quality and subsequent model performance? The current annotation process did not expose annotators to model reasoning steps.
- **Question 3:** Is the reasoning-based prompting strategy robust across larger, multi-institutional datasets with varying documentation styles? The evaluation was conducted on a small test set of 85 samples from a single institution.

## Limitations
- Prompt template for the full method is incompletely specified in the paper
- Dataset of 85 annotated samples may not capture full linguistic diversity of real-world EHRs
- Approach depends on single-threaded dependency resolution that may struggle with multiple concurrent chief complaints

## Confidence
- **High Confidence (0.85-1.00):** Core finding that generative prompting with reasoning steps improves OPQRST extraction is well-supported by F1 scores (0.838-0.944) and ablation studies
- **Medium Confidence (0.60-0.84):** Semantic evaluation approach using BERTScore and PromptLLM is methodologically sound but exact PromptLLM prompt design is underspecified
- **Low Confidence (0.00-0.59):** Claim that model can handle all linguistic variations without fine-tuning is optimistic given limited dataset and lack of external validation

## Next Checks
1. **Prompt Template Reconstruction:** Reconstruct full prompt template from ablation variants and test on small validation set to verify self-verification step consistently reduces hallucination rates below 5%
2. **Multi-Chief Complaint Stress Test:** Evaluate system on HPI notes containing 2-3 concurrent chief complaints to measure performance degradation and identify failure modes in dependency resolution
3. **External Dataset Validation:** Test trained prompting approach on independent EHR dataset from different institution or clinical specialty to assess true generalization beyond original 85-sample corpus