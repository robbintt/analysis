---
ver: rpa2
title: 'Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators
  and Directionally Aligned Perturbations'
arxiv_id: '2510.19975'
source_url: https://arxiv.org/abs/2510.19975
tags:
- gradient
- random
- optimization
- pxtq
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting optimal random perturbations
  in two-point zeroth-order optimization methods to minimize the asymptotic variance
  of gradient estimators. The authors formulate a constrained functional optimization
  problem over perturbation distributions, revealing that minimum-variance perturbations
  must either have fixed length or align directionally with the true gradient.
---

# Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations

## Quick Facts
- arXiv ID: 2510.19975
- Source URL: https://arxiv.org/abs/2510.19975
- Authors: Shaocong Ma; Heng Huang
- Reference count: 40
- Primary result: Directionally Aligned Perturbations (DAPs) achieve minimum asymptotic variance in two-point gradient estimation by aligning perturbations with the true gradient direction.

## Executive Summary
This paper addresses the fundamental question of selecting optimal random perturbations in two-point zeroth-order optimization methods to minimize the asymptotic variance of gradient estimators. The authors formulate a constrained functional optimization problem over perturbation distributions and prove that minimum-variance perturbations must either have fixed length or align directionally with the true gradient. Based on these theoretical insights, they propose Directionally Aligned Perturbations (DAPs) that adaptively focus accuracy along critical gradient directions, achieving superior performance in both gradient estimation accuracy on synthetic problems and faster convergence in language model fine-tuning tasks compared to traditional methods like uniform and Gaussian perturbations.

## Method Summary
The method centers on a two-stage gradient estimation process. First, a rough gradient estimate is obtained using standard uniform perturbations on a sphere. This estimate serves as the vector a for the DAP sampling algorithm, which projects random vectors onto the hyperplane defined by a^T v = ξ√δ||a||. The final gradient estimator uses these DAPs for the remaining function evaluations. The approach maintains δ-unbiasedness (E[vv^T] = δI) while minimizing variance through directional alignment. The method is applied within standard SGD updates, using hyperparameters η=10⁻⁴, μ=10⁻⁵, and batch sizes of b=2 for ZO estimation.

## Key Results
- DAPs achieve minimum asymptotic variance by enforcing directional alignment with the true gradient (v^T ∇f)^2 = δ ||∇f||^2
- Theoretical analysis establishes convergence bounds showing optimal dimension dependence for SGD using δ-unbiased perturbations
- Empirical results demonstrate superior gradient estimation accuracy and faster LLM fine-tuning convergence compared to uniform and Gaussian perturbations
- Directional alignment property is particularly effective for sparse gradients with varying magnitudes across dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DAPs minimize asymptotic variance by correlating perturbation vectors with the true gradient.
- **Mechanism:** Standard isotropic sampling wastes queries on irrelevant directions. The paper proves minimum variance requires (v^T ∇f)^2 = δ ||∇f||^2, projecting search onto gradient-aligned hyperplane.
- **Core assumption:** Smooth objective (L-smooth) and δ-unbiased perturbations with μ → 0.
- **Evidence:** Theorem 2.2 proves variance bounds; [abstract] confirms directional alignment achieves minimum variance.
- **Break condition:** Fails when ∇f ≈ 0 (stationary point) as alignment condition becomes degenerate.

### Mechanism 2
- **Claim:** DAPs provide anisotropic error reduction for sparse or varying-magnitude gradients.
- **Mechanism:** Unlike uniform error distribution, DAPs adapt sampling density based on gradient vector a, reducing variance specifically along effective dimensions.
- **Core assumption:** Gradient has sparse or anisotropic structure.
- **Evidence:** [page 7] explains adaptive low-variance perturbation; [page 9, Figure 2] shows variance comparison.
- **Break condition:** For dense, uniform gradients, anisotropic advantage disappears and overhead dominates.

### Mechanism 3
- **Claim:** Two-stage process enables practical DAP implementation without true gradient access.
- **Mechanism:** First stage uses b/2 standard perturbations to obtain rough gradient estimate â, which defines alignment plane for DAP sampling in second stage.
- **Core assumption:** Initial estimate â sufficiently correlates with true gradient.
- **Evidence:** [page 8, Algorithm 2] details workflow; [page 10] shows DAP outperforms with estimated gradient.
- **Break condition:** Small initial batch yields orthogonal estimate, causing DAPs to align with noise.

## Foundational Learning

- **Concept: Two-Point Gradient Estimation**
  - **Why needed:** Understanding the basic estimator $\hat{\nabla}f(x) = \frac{f(x+\mu v) - f(x)}{\mu}v$ is essential to grasp why perturbation distribution matters.
  - **Quick check:** Doubling μ increases bias (due to higher-order terms) while potentially reducing variance.

- **Concept: δ-Unbiasedness (E[vv^T] = δI)**
  - **Why needed:** This constraint ensures asymptotic alignment with true gradient and is critical for theoretical guarantees.
  - **Quick check:** Coordinate-wise perturbation (sampling standard basis vectors) does satisfy δ-unbiasedness with δ=1/d.

- **Concept: Functional Variance Optimization**
  - **Why needed:** The paper's core contribution is formulating perturbation selection as optimization over distributions rather than heuristic choice.
  - **Quick check:** Minimizing estimator variance directly improves SGD convergence by reducing gradient noise in parameter updates.

## Architecture Onboarding

- **Component map:** Rough Estimator -> DAP Generator (Algorithm 1) -> Refined Estimator -> SGD Updater
- **Critical path:** The Rough Estimator - poor initial gradient estimate propagates through entire pipeline, producing misaligned DAPs.
- **Design tradeoffs:** Batch split between rough (alignment quality) and refined (actual descent) estimators; hyperparameter δ affects stability.
- **Failure signatures:** Gradient orthogonality (worst-case), dense gradient degradation, memory overhead in high dimensions.
- **First 3 experiments:**
  1. **Variance Benchmark:** Implement quadratic f(x)=x^T A x, compare DAP vs Uniform vs Gaussian τ-MSE (Fig 3).
  2. **Batch Split Ablation:** Test 10/90, 50/50, 90/10 ratios on sparse Product function to optimize allocation.
  3. **LLM Memory Wall:** Implement forward-pass only fine-tuning on OPT-1.3b, verify memory matches standard ZOO while tracking convergence (Fig 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the necessary and sufficient conditions for minimum-variance perturbations?
- **Basis:** Page 5 states extending current sufficient conditions to necessary and sufficient is "an interesting but challenging topic."
- **Why unresolved:** Counterexample exists (mixed DAP-uniform distribution) achieving minimum variance without satisfying current conditions exclusively.
- **Evidence:** Mathematical characterization of full solution space for constrained functional optimization (Eq. 3).

### Open Question 2
- **Question:** Does DAP performance degrade on dense gradients compared to sparse ones?
- **Basis:** Appendix G suggests DAPs may falter with dense gradients as gradient estimation errors could outweigh alignment benefits.
- **Why unresolved:** Method relies on anisotropy and sparse gradient assumptions; dense gradients may amplify bias from estimated alignment.
- **Evidence:** Theoretical analysis or empirical benchmarks on dense synthetic functions showing error vs variance trade-off.

### Open Question 3
- **Question:** Can DAP sampling be implemented without full gradient estimate memory overhead in high dimensions?
- **Basis:** Appendix G notes projection steps require storing full gradient estimates, conflicting with memory-efficiency motivation.
- **Why unresolved:** Current Algorithm 1 relies on full-dimensional vectors, problematic for very high-dimensional settings.
- **Evidence:** Algorithm using low-rank approximations or sparsification maintaining theoretical variance guarantees.

## Limitations
- Theoretical optimality assumes perfect δ-unbiasedness and infinitesimal step sizes, which may not hold practically
- Two-stage sampling overhead may not pay off for dense gradients or extremely constrained computational budgets
- Adaptive advantage diminishes in high dimensions as projection overhead increases while relative benefit decreases

## Confidence
- **High confidence:** Theoretical formulation of variance minimization via directional alignment and DAP sampling algorithm derivation
- **Medium confidence:** Empirical performance gains demonstrated but dependent on specific problem characteristics
- **Low confidence:** Scalability claims for high-dimensional problems and general superiority across diverse landscapes

## Next Checks
1. **Gradient Orthogonality Stress Test:** Generate orthogonal rough gradient estimate (small batch from N(0,I)) and measure DAP vs standard method performance degradation.
2. **Dense Gradient Baseline Comparison:** Construct dense, uniformly distributed gradient synthetic problems and compare DAP performance to quantify when anisotropic advantage disappears.
3. **Memory-Computation Tradeoff Analysis:** Implement DAPs with gradient checkpointing or low-rank approximations to assess viability in very high dimensions (d > 10⁴) where projection becomes expensive.