---
ver: rpa2
title: 'PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset'
arxiv_id: '2512.10148'
source_url: https://arxiv.org/abs/2512.10148
tags:
- persona
- review
- generation
- explicit
- paran
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PARAN, a two-stage prompting framework for personalized
  review response generation in food delivery platforms. It addresses the challenge
  of limited user information by inferring explicit (stated preferences) and implicit
  (demographic or stylistic cues) personas directly from short review texts.
---

# PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset

## Quick Facts
- arXiv ID: 2512.10148
- Source URL: https://arxiv.org/abs/2512.10148
- Reference count: 31
- Key outcome: Two-stage prompting framework improves personalized review response generation in food delivery platforms

## Executive Summary
PARAN addresses the challenge of generating personalized review responses in food delivery platforms where user information is limited. The framework infers explicit (stated preferences) and implicit (demographic/stylistic cues) personas directly from short review texts, then incorporates these inferred attributes into response generation prompts. Evaluated on a Korean food delivery dataset across six LLMs, PARAN improves response personalization with notable gains in precision and diversity, particularly for smaller models. The approach uses temperature scaling to balance exploration and faithfulness while maintaining semantic consistency.

## Method Summary
PARAN employs a two-stage prompting framework for personalized review response generation. In stage one, LLMs extract 14 predefined explicit persona attributes (food taste, portion size, freshness, pricing, etc.) and implicit persona traits (demographic cues, tone, dietary preferences) from review text. In stage two, these inferred personas are incorporated into a generation prompt to produce user-tailored responses. The framework requires no fine-tuning, operates through pure prompting, and includes temperature scaling to control the precision-diversity trade-off while preserving semantic consistency.

## Key Results
- Smaller models (8B) show significant precision gains (+18-25% on BLEU/Rouge-2) with persona conditioning
- Implicit persona inference increases diversity metrics (Distinct-2) by up to +35.9% in some models
- Temperature scaling maintains semantic consistency (BERTScore F1 stability) while increasing diversity
- Larger models (70B+) show degraded performance with persona conditioning, suggesting over-conditioning effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit persona extraction anchors responses in factual review content, improving lexical precision metrics.
- **Mechanism:** The framework prompts LLMs to extract 14 predefined attributes from review text before generation. These structured attributes serve as grounded context that constrains generation toward content-relevant outputs.
- **Core assumption:** LLMs can reliably extract stated preferences from short, sparse reviews without hallucinating attributes not present in the source text.
- **Evidence anchors:**
  - [abstract] "infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts"
  - [section III.A] "These attributes serve to anchor the generated response in the factual content of the review and promote empathetic alignment with the reviewer."
  - [corpus] Weak direct corpus support; related work on food delivery focuses on prediction tasks rather than persona extraction.
- **Break condition:** Reviews with fewer than 5 words removed during preprocessing; very sparse reviews may yield insufficient explicit signals.

### Mechanism 2
- **Claim:** Implicit persona inference introduces stylistic variation that increases lexical diversity without requiring user metadata.
- **Mechanism:** LLMs leverage pretrained associations between linguistic patterns and demographic/stylistic traits to infer latent user characteristics. These inferred traits modulate response style independently of review content.
- **Core assumption:** Pretrained LLMs encode valid correlations between text features and user characteristics that transfer to food delivery domain reviews in Korean.
- **Evidence anchors:**
  - [abstract] "implicit (e.g., demographic or stylistic cues) personas directly from short review texts"
  - [section III.A] "These latent attributes are automatically derived by the LLM without explicit supervision, leveraging its pretraining on large-scale human interaction data."
  - [section V.C] "Claude 3.5 Haiku achieves the highest Distinct-2 score (0.1041) when using only the implicit persona, a +35.9% gain over the baseline."
  - [corpus] No direct corpus validation of persona inference accuracy; corpus papers focus on delivery prediction, not user profiling.
- **Break condition:** High-capability models (70B+) may already encode sufficient stylistic variability, making external implicit signals redundant or interfering.

### Mechanism 3
- **Claim:** Temperature scaling controls the precision-diversity trade-off while preserving semantic consistency.
- **Mechanism:** Higher temperature increases sampling stochasticity, encouraging the model to explore diverse phrasings while persona context maintains topical relevance. Semantic similarity remains stable because persona constraints prevent semantic drift.
- **Core assumption:** Persona conditioning provides sufficient grounding to prevent semantic drift even at high temperatures.
- **Evidence anchors:**
  - [section III.B] "A higher τ encourages more exploratory responses, especially in the presence of ambiguous or sparse persona cues."
  - [section V.B] "BERTScore F1 of most models remains remarkably stable despite increasing temperature... semantic consistency is largely preserved."
  - [section V.B] "Llama 3.1 Instruct (8B) experiences a significant drop in BERTScore F1 at temperature 1.0... semantic drift at high temperature settings."
  - [corpus] No corpus evidence on temperature-persona interactions.
- **Break condition:** Smaller models (8B) show semantic drift at τ=1.0; temperature calibration is model-specific.

## Foundational Learning

- **Concept: Two-stage prompting cascades**
  - **Why needed here:** PARAN separates inference (persona extraction) from generation (response synthesis), allowing each stage to use different temperature settings and reducing task interference.
  - **Quick check question:** Can you explain why extracting persona attributes separately before generation might outperform a single combined prompt?

- **Concept: Precision-diversity trade-off in NLG evaluation**
  - **Why needed here:** The paper explicitly balances Rouge/BLEU (lexical overlap) against Distinct-2 (bigram uniqueness), and uses sum-of-ranks to find optimal temperature. Understanding this trade-off is essential for interpreting results.
  - **Quick check question:** Why might higher diversity scores correlate with more personalized responses even though Distinct-2 doesn't directly measure personalization?

- **Concept: Model capacity and prompting effectiveness**
  - **Why needed here:** Results show smaller models (8B) benefit significantly from persona conditioning (+18-25% on precision metrics) while larger models (70B) show degraded performance. This suggests external signals can be redundant or interfering for high-capacity models.
  - **Quick check question:** What factors might cause a 70B model to perform worse with additional persona conditioning compared to a no-persona baseline?

## Architecture Onboarding

- **Component map:** Review text → Explicit persona prompt → Implicit persona prompt → Combined generation prompt → Temperature-scaled decoding → Response

- **Critical path:** Review text → Explicit persona prompt → Implicit persona prompt → Combined generation prompt → Temperature-scaled decoding → Response

- **Design tradeoffs:**
  - Explicit-only: Best precision (GPT-3.5: +14.5% BLEU vs baseline), lower diversity
  - Implicit-only: Best diversity (Claude Haiku: +35.9% Distinct-2), lower precision
  - Combined (PARAN): Balanced performance; may underperform explicit-only for some models
  - Model selection: Smaller models benefit more; avoid PARAN for 70B+ models without validation

- **Failure signatures:**
  - Semantic drift at τ=1.0 for smaller models (Llama 8B: BERTScore drops)
  - Over-conditioning in large models (Llama 70B: -9.3% Rouge, -16.2% Distinct-2 vs baseline)
  - Persona interference (GPT-3.5: combined PARAN underperforms explicit-only)

- **First 3 experiments:**
  1. **Baseline calibration:** Run all 6 models with no persona, measure Rouge-2/BLEU/Distinct-2 to establish per-model baseline variance
  2. **Ablation grid:** For your target model, test all 4 conditions (no persona, explicit-only, implicit-only, combined) at 3 temperature settings (0.2, 0.6, 1.0) to identify optimal configuration
  3. **Temperature robustness check:** Plot BERTScore F1 across temperature range for your chosen model-persona combination to confirm semantic stability before deployment

## Open Questions the Paper Calls Out

- **Question:** How can direct personalization metrics be developed to supersede lexical diversity as a proxy for user alignment?
  - **Basis in paper:** [explicit] The authors state the need to "move beyond diversity as an indirect proxy by developing direct personalization metrics" in the conclusion.
  - **Why unresolved:** Current evaluation relies on Distinct-2, which measures lexical variety rather than the semantic accuracy of the personalized response relative to the specific user.
  - **What evidence would resolve it:** A new evaluation framework or metric validated via human annotation that correlates strongly with perceived user-specific resonance rather than just n-gram uniqueness.

- **Question:** In what specific operational regimes does persona-prompting outperform fine-tuning for response generation?
  - **Basis in paper:** [explicit] The paper lists "quantitatively compare PARAN against fine-tuned approaches to identify the regimes where persona-prompting is preferable" as a next step.
  - **Why unresolved:** The study only establishes the efficacy of a zero-shot prompting approach; the trade-offs regarding data scarcity, computational cost, and performance ceilings compared to fine-tuning remain unexplored.
  - **What evidence would resolve it:** A comparative benchmark study measuring performance gaps between PARAN and fine-tuned baselines across varying sizes of training data.

- **Question:** How robust is LLM-based persona inference when faced with noisy, sarcastic, or adversarial review texts?
  - **Basis in paper:** [explicit] The authors propose to "examine the robustness of persona inference under noisy or adversarial inputs" as future work.
  - **Why unresolved:** Real-world reviews often contain ungrammatical structures or irony; it is currently unclear if the inference mechanism hallucinates incorrect personas when linguistic cues are misleading.
  - **What evidence would resolve it:** Evaluation results on a perturbed dataset where reviews are intentionally manipulated to test the stability of the inferred persona attributes.

- **Question:** Why does explicit persona conditioning degrade performance in very large models (e.g., 70B parameters)?
  - **Basis in paper:** [inferred] The results show Llama 3.1 (70B) suffered performance drops with PARAN. The authors hypothesize "over-conditioning" or internal redundancy but do not offer a solution.
  - **Why unresolved:** It is uncertain if the additional persona instructions constrain the model's superior inherent reasoning or if the prompt format is sub-optimal for large parameter scales.
  - **What evidence would resolve it:** An analysis of attention mechanisms in large models to determine if persona tokens disproportionately overshadow the original review context.

## Limitations

- The framework lacks ground truth persona labels for quantitative validation of extraction accuracy; all persona quality assessments are indirect.
- Dataset size (1,110 reviews, 82 users) is relatively small for establishing robust generalization, though it meets k-core filtering requirements.
- Temperature sensitivity varies significantly across model sizes, requiring careful per-model calibration.

## Confidence

- **High confidence:** The two-stage prompting architecture is clearly described and the evaluation methodology (precision-diversity trade-off, sum-of-ranks temperature selection) is methodologically sound. The observation that smaller models benefit more from persona conditioning while larger models show diminishing returns is empirically supported by the reported metrics.
- **Medium confidence:** The mechanism by which implicit persona extraction improves diversity is supported by observed Distinct-2 increases but lacks direct validation that inferred demographic traits actually reflect user characteristics. The claim that persona conditioning prevents semantic drift at high temperatures is supported by BERTScore stability in most cases but contradicted by the Llama 8B outlier.
- **Low confidence:** The effectiveness of the 14-attribute explicit persona schema for food delivery reviews is asserted but not validated against alternative attribute sets. The paper provides no evidence that the specific attribute selection captures the most salient review dimensions for this domain.

## Next Checks

1. **Persona extraction accuracy validation:** Create a small labeled subset (50-100 reviews) with human-annotated ground truth personas to directly measure the accuracy of both explicit and implicit persona extraction modules before evaluating downstream response quality.

2. **Cross-domain generalization test:** Apply the framework to a different review domain (e.g., restaurant reviews, product reviews) to assess whether the persona extraction and response generation approach generalizes beyond food delivery, particularly testing if the 14-attribute schema remains relevant.

3. **Model capacity threshold identification:** Systematically test the framework across a broader range of model sizes (1B, 3B, 8B, 13B, 33B, 70B) to identify the precise model capacity threshold where persona conditioning transitions from beneficial to detrimental, and test whether this threshold varies by model architecture family.