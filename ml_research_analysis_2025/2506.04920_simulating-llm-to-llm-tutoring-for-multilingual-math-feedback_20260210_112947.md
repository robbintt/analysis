---
ver: rpa2
title: Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback
arxiv_id: '2506.04920'
source_url: https://arxiv.org/abs/2506.04920
tags:
- student
- language
- hint
- hints
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study is the first to simulate multilingual tutor-student
  interactions using LLMs, exploring 352 experimental settings across 11 languages,
  four state-of-the-art LLMs, and multiple prompting strategies. It investigates whether
  language-specific feedback leads to measurable learning gains, examining how student
  input language, teacher feedback language, model choice, and language resource level
  jointly influence performance.
---

# Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback

## Quick Facts
- **arXiv ID:** 2506.04920
- **Source URL:** https://arxiv.org/abs/2506.04920
- **Reference count:** 34
- **Primary result:** Multilingual hints significantly improve learning outcomes, especially for low-resource languages when feedback aligns with student's native language.

## Executive Summary
This study simulates multilingual tutor-student interactions using LLMs across 352 experimental settings spanning 11 languages, four state-of-the-art LLMs, and multiple prompting strategies. The research investigates whether language-specific feedback leads to measurable learning gains and how student input language, teacher feedback language, model choice, and language resource level jointly influence performance. Results demonstrate that multilingual hints can significantly improve learning outcomes, particularly in low-resource languages when feedback is aligned with the student's native language.

## Method Summary
The study uses MGSM dataset (250 grade-school math problems × 11 languages) to simulate LLM-to-LLM tutoring. A stronger "teacher" model generates hints while a weaker "student" model solves problems and revises answers based on hints. Student models include Mistral-7B (monolingual) and Aya-8B (multilingual), while teacher models include LLaMA-3.3-70B and LLaMA-3.1-8B. Four hint strategies are tested: EN→EN, EN→EN→L, L→L, and EN→L. GPT-4o evaluates correctness against gold answers, and Student Gain G = (A_after - A_before) / A_before × 100 measures improvement. The study uses temperature=0 for deterministic student outputs and temperature=1 for diverse teacher hints.

## Key Results
- Multilingual hints significantly improve learning outcomes, particularly for low-resource languages when feedback aligns with student's native language
- English-only prompting generally outperforms multilingual approaches across most hint types and student models
- Monolingual models excel in high-resource languages while multilingual models perform better in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1: Language Alignment Boosts Comprehension
Feedback in the student's native language improves learning gains by reducing cognitive load from language translation, allowing student models to focus reasoning resources on the mathematical task itself. This mirrors UNESCO findings on mother-tongue instruction improving comprehension.

### Mechanism 2: Teacher-Student Capability Gap Enables Pedagogical Guidance
A stronger teacher model generates hints that enable a weaker student model to correct errors it could not self-correct through superior reasoning and language capabilities that allow diagnosis of student errors and generation of targeted scaffolding hints.

### Mechanism 3: Prompting Strategy Modulates Cross-Lingual Transfer
English prompting (EN→EN) generally outperforms other strategies by leveraging the model's strongest training distribution, while native-language prompts reduce translation friction when student inputs are non-English.

## Foundational Learning

- **Concept: Relative Gain Metric (G)**
  - Why needed here: Normalizes improvement across languages and models using G = (A_after - A_before) / A_before × 100
  - Quick check: If a student model improves from 10% to 15% accuracy, what is the relative gain? (Answer: 50%)

- **Concept: High-Resource vs. Low-Resource Languages (HRL vs. LRL)**
  - Why needed here: Performance and optimal strategies differ by resource level (HRLs: en, zh, fr, de, ja, ru, es; LRLs: bn, th, te, sw)
  - Quick check: Would you expect Aya-8B or Mistral-7B to perform better on Swahili math problems? (Answer: Aya-8B, as multilingual models excel on LRLs)

- **Concept: Hint as Scaffolding (Not Answer-Giving)**
  - Why needed here: Teacher models are explicitly instructed to generate hints as questions without revealing answers
  - Quick check: If a hint contains the final answer verbatim, does it represent successful tutoring? (Answer: No, this violates the pedagogical design principle)

## Architecture Onboarding

- **Component map:** Student Model → Teacher Model → Hint Strategy Module → Evaluator (GPT-4o) → Revised Solution
- **Critical path:** Load MGSM exercise → Student generates candidate solution → GPT-4o evaluates correctness → If incorrect, Teacher generates hint → Student revises solution → GPT-4o evaluates revised solution → Compute relative gain G
- **Design tradeoffs:** Teacher size vs. cost (LLaMA-3.3-70B better but more expensive); English vs. native-language prompts (English safer but L→L outperforms for LRLs); single vs. multiple hints (N>1 improves gains but with diminishing returns)
- **Failure signatures:** Low language consistency (~91% LID accuracy for revised solutions); zero gains on specific languages (Mistral-7B shows 0% gain on Telugu); answer leakage (up to 8% for LLaMA-3.1-8B)
- **First 3 experiments:**
  1. Baseline calibration: Run zero-shot evaluation on MGSM for all candidate student models to establish A_before per language
  2. Hint strategy ablation: For one HRL and one LRL, run all four hint strategies with LLaMA-3.3-70B as teacher and compare gains
  3. Leakage audit: Apply regex-based detection to teacher-generated hints for LRL exercises; flag any hints containing gold answers verbatim

## Open Questions the Paper Calls Out

1. **Human Validation Gap:** Do multilingual LLM-to-LLM tutoring gains transfer to real human learners, and how do learner characteristics moderate these effects? The paper acknowledges LLM simulations cannot capture the full diversity of real learners' misconceptions, language proficiency, or problem-solving styles.

2. **Fine-Grained Evaluation Metrics:** What metrics can capture partial progress and hint quality beyond binary solution correctness? The study notes that correctness alone misses key pedagogical dimensions such as conceptual scaffolding, clarity, and alignment with learning objectives.

3. **Domain Generalization:** How do the multilingual feedback findings generalize to domains beyond mathematics? The paper states future work aims to extend research to subject areas beyond mathematics reasoning.

## Limitations
- LLM simulations cannot capture the full diversity of real learners' misconceptions, language proficiency, or problem-solving styles
- The study only measures whether the final solution becomes correct, ignoring cases where feedback addresses one error while others persist
- Answer leakage remains a concern, particularly for smaller teacher models (up to 8% for LLaMA-3.1-8B in LRLs)

## Confidence
- **High Confidence:** English-only prompting generally outperforms multilingual approaches; multilingual models excel in low-resource languages while monolingual models perform better in high-resource settings
- **Medium Confidence:** Language-aligned hints produce superior gains in low-resource languages, though highly model-dependent with some zero-gain cases
- **Low Confidence:** Answer leakage claims may underestimate the problem, particularly for smaller teacher models

## Next Checks
1. Conduct a small-scale human validation study with actual students in one high-resource and one low-resource language to validate whether LLM-simulated learning gains correlate with human learning outcomes.

2. Systematically test answer leakage across all teacher-student combinations using automated detection of gold answers in hints, focusing particularly on the boundary between pedagogical guidance and answer revelation.

3. Extend hint iterations beyond N=5 for select language-model pairs to determine whether observed diminishing returns represent fundamental limits or simply insufficient exploration of the learning space.