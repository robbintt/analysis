---
ver: rpa2
title: Evaluating Large Language Models on Non-Code Software Engineering Tasks
arxiv_id: '2506.10833'
source_url: https://arxiv.org/abs/2506.10833
tags:
- large
- tasks
- llms
- base
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SELU, the first comprehensive benchmark for
  evaluating large language models on non-code software engineering tasks. The authors
  collect 17 non-code tasks spanning classification, regression, Named Entity Recognition,
  and Masked Language Modeling from diverse SE sources like issue tracking systems
  and developer forums.
---

# Evaluating Large Language Models on Non-Code Software Engineering Tasks

## Quick Facts
- arXiv ID: 2506.10833
- Source URL: https://arxiv.org/abs/2506.10833
- Reference count: 40
- Primary result: Moderate-scale decoder-only LLMs (1–7B) consistently outperform other architectures on non-code SE tasks

## Executive Summary
This paper introduces SELU, the first comprehensive benchmark for evaluating large language models on non-code software engineering tasks. The authors collect 17 diverse tasks spanning classification, regression, Named Entity Recognition, and Masked Language Modeling from real SE sources like issue tracking systems and developer forums. They evaluate 22 open-source LLMs, 2 proprietary models, and 2 classical baselines, finding that moderate-scale decoder-only models consistently achieve the highest performance and lowest across-task variance. The results show that architectural choice and model scale matter more than code-focused domain adaptation for non-code SE tasks, providing actionable guidance for selecting LLMs in software engineering workflows.

## Method Summary
The SELU benchmark evaluates models on 17 non-code SE tasks from diverse sources including issue trackers, developer forums, and requirement specifications. Models are fine-tuned using task-specific heads with AdamW optimizer (lr=1e-5), BFloat16 precision, and early stopping. Evaluation uses F1-macro, SMAPE, and accuracy metrics, with Bayesian signed-rank tests determining practical equivalence between models. The study compares 22 open-source LLMs (encoder-only, decoder-only, encoder-decoder), 2 proprietary models (zero-shot prompting), and 2 classical ML baselines across classification, regression, NER, and MLM tasks.

## Key Results
- Moderate-scale decoder-only models (1–7B parameters) consistently outperform encoder-only and encoder-decoder architectures on non-code SE tasks
- Code-focused pre-training provides only modest improvements compared to architectural choice and model scale
- Scaling benefits are task-dependent, with strongest gains for multi-label classification and negligible effects for regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moderate-scale decoder-only LLMs consistently outperform other architectures
- Mechanism: Autoregressive pre-training develops inductive bias toward modeling sequential language flow and long-range dependencies
- Core assumption: Autoregressive objective captures generalizable linguistic patterns beyond pre-training distribution
- Evidence: "moderate-scale decoder-only models consistently form a top-tier, exhibiting high mean performance and low across-task variance"

### Mechanism 2
- Claim: Scaling benefits are task-type dependent
- Mechanism: Larger capacity discriminates fine-grained label combinations in multi-label tasks, while regression saturates with sufficient data
- Core assumption: Task complexity correlates with capacity requirements more than sample size
- Evidence: "Scaling leads to clear gains in multi-label classification but yields diminishing returns in other kind of tasks, particularly regression"

### Mechanism 3
- Claim: Code-focused pre-training provides modest improvements
- Mechanism: Code-adapted tokenizers and representations diverge from linguistic diversity of natural language SE artifacts
- Core assumption: Domain gap between code corpora and non-code SE text limits transfer
- Evidence: "domain adaptation via code-focused pre-training offers only modest improvements"

## Foundational Learning

- Concept: Task-specific heads (classification, regression, NER, MLM)
  - Why needed here: LLMs require architectural extensions to produce task-appropriate outputs
  - Quick check question: Which head requires token-level alignment between sub-word tokens and word-level labels?

- Concept: F1-macro vs F1-micro distinction
  - Why needed here: Classification tasks have varying class imbalance
  - Quick check question: Why is F1-micro preferred for NER while F1-macro is used for multi-class classification in SELU?

- Concept: Bayesian signed-rank test with ROPE
  - Why needed here: Determines whether model differences are practically meaningful
  - Quick check question: What decision rule determines if two models are "practically equivalent" versus comparison being inconclusive?

## Architecture Onboarding

- Component map: Datasets -> Preprocessing -> Task-specific head attachment -> Fine-tuning -> Evaluation -> Bayesian comparison
- Critical path: 1) Data preprocessing (mask URLs, hashes, code blocks; stratified 80/20 split) 2) Head selection per task type 3) Fine-tuning with early stopping 4) Metric aggregation and normality check 5) Bayesian signed-rank test for tier assignment
- Design tradeoffs:
  - Code masking removes potentially useful context to isolate natural language capabilities
  - Compute budget limited to ≤7B parameters; larger models may shift rankings
  - Zero-shot prompting for proprietary models uses simple templates without prompt engineering
- Failure signatures:
  - High across-task variance (>0.2 std. dev.) indicates model instability
  - Cohen's d > 0.8 with substantial ROPE overlap suggests large effect size but high uncertainty
  - Encoder-decoder models show lowest mean and highest variance—avoid for non-code SE
- First 3 experiments:
  1. Replicate Llama 3.2 3b vs. TFIDF+XGBoost on binary classification tasks
  2. Compare GPT-2 xl vs. CodeLlama 7b on multi-label classification
  3. Ablate code masking on comment_type tasks to quantify context loss

## Open Questions the Paper Calls Out

- Does including source code context improve LLM performance on non-code SE tasks compared to natural-language-only approach?
- Does pre-training on non-code SE natural language texts yield better performance than generalist or code-focused pre-training?
- How robust are performance rankings when varying random seeds, hyper-parameters, and prompting strategies?

## Limitations

- Benchmark scope limited to moderate-scale models (≤7B parameters), leaving questions about larger scale performance
- Code masking preprocessing may underestimate performance on tasks where code context provides valuable signal
- Zero-shot prompting uses simple templates without prompt engineering, potentially underrepresenting proprietary model capabilities

## Confidence

**High Confidence:**
- Moderate-scale decoder-only models consistently outperform other architectures on non-code SE tasks
- Architectural choice has more impact than code-focused domain adaptation
- Task-specific heads and proper preprocessing are essential for reliable benchmarking

**Medium Confidence:**
- Scaling benefits are strongest for multi-label classification and negligible for regression
- Moderate-scale models achieve best balance of performance and across-task variance
- 1-7B parameter range represents optimal operating point for non-code SE tasks

**Low Confidence:**
- Code masking improves isolation of natural language capabilities without significant performance trade-offs
- Bayesian signed-rank test provides definitive tier assignments across all model pairs
- Results generalize to tasks outside current 17-task benchmark suite

## Next Checks

1. **Scale-Expansion Validation**: Fine-tune top-performing decoder-only models (Llama 3.2 7B) alongside 13B-70B parameter models on SELU tasks to determine if optimal performance range extends to larger scales.

2. **Code Context Ablation Study**: Run comment_type tasks with and without code masking to quantify performance impact of removing code blocks on documentation tasks.

3. **Domain Adaptation Re-evaluation**: Pre-train moderate-scale decoder-only model on non-code SE artifacts (requirements, issue discussions, forum posts) and evaluate on SELU to measure benefits of SE-specific natural language pre-training.