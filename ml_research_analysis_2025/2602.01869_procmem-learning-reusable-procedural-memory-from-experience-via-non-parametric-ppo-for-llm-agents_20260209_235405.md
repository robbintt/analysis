---
ver: rpa2
title: 'ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric
  PPO for LLM Agents'
arxiv_id: '2602.01869'
source_url: https://arxiv.org/abs/2602.01869
tags:
- skill
- memory
- procedural
- procmem
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProcMEM introduces a framework enabling LLM agents to autonomously
  learn procedural memory from interaction experiences without parameter updates.
  It addresses the problem of insufficient experience reuse in LLM agents, which leads
  to computational redundancy and execution instability.
---

# ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents

## Quick Facts
- arXiv ID: 2602.01869
- Source URL: https://arxiv.org/abs/2602.01869
- Reference count: 40
- Key result: Achieves superior reuse rates (0.925 in-domain) and significant performance gains with extreme memory compression (816 tokens vs. hundreds of thousands for baselines)

## Executive Summary
ProcMEM introduces a framework enabling LLM agents to autonomously learn procedural memory from interaction experiences without parameter updates. It addresses the problem of insufficient experience reuse in LLM agents, which leads to computational redundancy and execution instability. The core idea is to formalize procedural units as executable Skills defined by activation, execution, and termination conditions, and evolve them through Non-Parametric PPO that uses semantic gradients for candidate generation and a PPO Gate for robust verification. Experiments demonstrate that ProcMEM achieves superior reuse rates (e.g., 0.925 in-domain reuse) and significant performance gains with extreme memory compression (only 816 tokens vs. hundreds of thousands for baselines) across in-domain, cross-task, and cross-agent scenarios. The framework's transparent Skill evolution and reuse validate its effectiveness in facilitating long-term autonomy.

## Method Summary
ProcMEM formalizes procedural memory as executable Skills (ω = ⟨I_ω, π_ω, β_ω⟩) defined by activation, execution, and termination conditions. The framework evolves these skills through Non-Parametric PPO: semantic gradients extracted from failed trajectories via hindsight attribution are aggregated and used to generate candidate skill updates, which are verified through a PPO Gate (clipped surrogate objective) before admission. The agent maintains a fixed-capacity skill pool, pruning skills based on online performance scores. This approach enables experience reuse without parameter updates, bypassing deliberative reasoning in recurring scenarios while ensuring stability through trust-region verification.

## Key Results
- Achieves 0.925 in-domain reuse rate and 0.640 cross-task reuse rate
- Reduces memory footprint from hundreds of thousands to only 816 tokens
- Demonstrates cross-agent transfer success from Gemma-2-9B to Qwen3-32B
- Ablation studies show -66.9% reuse rate drop without Semantic Gradients and performance instability without PPO Gate

## Why This Works (Mechanism)

### Mechanism 1: Procedural Abstraction for Inference Bypass
Formalizing memory as executable procedures rather than passive narratives reduces computational redundancy by bypassing deliberative reasoning in recurring scenarios. Skills act as temporally extended actions that execute directly when conditions match, avoiding re-deriving solutions from scratch.

### Mechanism 2: Semantic Gradient Descent
Natural language "gradients" derived from hindsight attribution allow effective skill refinement without differentiable parameters. Batch-level aggregation of per-trajectory gradients filters out noise to reveal systematic weaknesses.

### Mechanism 3: Trust-Region Verification (PPO Gate)
A PPO-style verification filter prevents capability degradation by rejecting unstable or hallucinated skill updates. Candidates are evaluated using clipped surrogate objectives against historical trajectories, mimicking PPO's trust-region constraint in text-edit space.

## Foundational Learning

- **Concept:** **Proximal Policy Optimization (PPO) & Clipping**
  - **Why needed here:** The paper adapts PPO's mathematical stability guarantees (specifically the clipped objective) to text-based skill evolution. Understanding why standard gradient ascent is unstable and why the "trust region" prevents catastrophic updates is critical.
  - **Quick check question:** Why does the PPO Gate compare the probability of historical actions under the *new* skill vs. the *old* skill?

- **Concept:** **Episodic vs. Procedural Memory**
  - **Why needed here:** The core distinction is that episodic memory is "what happened" (requiring inference) vs. procedural memory is "how to act" (executable). This is critical to grasping the efficiency gains.
  - **Quick check question:** Why does retrieving a raw trajectory (episodic) force the LLM to "re-reason," while a Skill does not?

- **Concept:** **Temporal Abstraction (Options/Skills)**
  - **Why needed here:** The paper operates on a "Skill-MDP," extending standard MDPs. Concepts like initiation sets and termination conditions are borrowed from the Options Framework in RL.
  - **Quick check question:** How does the **Skill-MDP** differ from a standard MDP in terms of decision frequency?

## Architecture Onboarding

- **Component map:** Trajectory Buffer -> Semantic Gradient Generator -> PPO Gate -> Skill Pool -> Online Scorer -> Skill Selection
- **Critical path:** The PPO Gate evaluation is the bottleneck. Ensuring the frozen LLM can efficiently process the likelihood of historical actions under a counterfactual skill description is essential.
- **Design tradeoffs:** Explicit vs. Implicit representation (current design uses readable text for transparency but consumes prompt tokens); Pool Size K (fixed capacity forces evolutionary pressure but may forget rare-but-critical skills)
- **Failure signatures:** FIFO Collapse (performance degradation from evicting good skills for bad ones); Hallucination Cascade (pool fills with nonsensical textual variations that look refined but fail execution)
- **First 3 experiments:** 1) Ablation on PPO Gate: Run "w/o PPO Gate" variant on deterministic environment to confirm training instability; 2) Token Efficiency Audit: Measure Δ Prompt Tokens/Step against RAG baseline; 3) Cross-Agent Transfer: Train skills on small model and test on larger model to verify model-agnostic procedural logic

## Open Questions the Paper Calls Out
1. Can explicit natural-language Skills be progressively compressed into implicit, directly executable modules to better emulate human procedural memory? (Section 6, Appendix F.1)
2. Does the Non-Parametric PPO mechanism scale effectively to open-ended environments with sparse rewards or noisy feedback? (Inferred from limited domain scope)
3. Does the capacity of the frozen base LLM impose a hard ceiling on the complexity of evolved Skills that cannot be overcome by non-parametric updates? (Inferred from fixed LLM assumption)

## Limitations
- Semantic gradient validity depends entirely on LLM judge capability, with no isolated validation of batch aggregation filtering noise
- PPO Gate generalizability limited to tested domains (ALFWorld, TextArena) with no validation for gradual probability changes
- Cross-agent assumptions unverified for encoder-decoder or structurally different models beyond decoder-only comparisons

## Confidence
- **High Confidence:** Procedural abstraction mechanism and efficiency benefits are well-supported by theoretical framing and empirical results
- **Medium Confidence:** Semantic Gradient descent shows strong ablation results but depends entirely on LLM judge capability
- **Medium Confidence:** PPO Gate provides stability as claimed, but PPO-style clipping for text is novel and may not generalize beyond tested domains

## Next Checks
1. **Gradient Quality Isolation:** Create synthetic task with known systematic error to measure whether Semantic Gradient extractor correctly identifies it versus generating trajectory-specific noise
2. **Model Architecture Transfer:** Test skill transfer from decoder-only (Gemma) to encoder-decoder (T5) model to verify model-agnostic procedural memory
3. **PPO Gate Robustness:** Evaluate PPO Gate on domain with continuous action spaces to test whether importance ratio-based verification remains effective with smoother action probabilities