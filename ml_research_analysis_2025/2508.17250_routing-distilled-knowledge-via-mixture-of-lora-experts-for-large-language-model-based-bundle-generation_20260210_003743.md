---
ver: rpa2
title: Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language
  Model based Bundle Generation
arxiv_id: '2508.17250'
source_url: https://arxiv.org/abs/2508.17250
tags:
- knowledge
- bundle
- generation
- experts
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses knowledge conflicts in LLM-based bundle generation
  when naively combining different types of distilled knowledge. The authors propose
  RouteDK, a framework that uses knowledge-specific LoRA experts (base, high-level,
  and fine-grained) and a dynamic fusion module with input-aware routing to effectively
  integrate complementary knowledge types.
---

# Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation

## Quick Facts
- **arXiv ID:** 2508.17250
- **Source URL:** https://arxiv.org/abs/2508.17250
- **Reference count:** 40
- **Primary result:** Dynamic fusion of LoRA experts with input-aware routing achieves 9.32% average gain over knowledge distillation baselines while maintaining computational efficiency.

## Executive Summary
This paper addresses knowledge conflicts that arise when integrating multiple types of distilled knowledge into student LLMs for bundle generation tasks. The authors propose RouteDK, a framework that separates different knowledge types (base, high-level rules, fine-grained reasoning) into specialized LoRA experts and uses a dynamic fusion module with input-aware routing to adaptively combine them. The approach resolves knowledge conflicts by preventing gradient interference during training and allows context-adaptive knowledge selection. An inference-time enhancement strategy based on self-consistency further improves robustness. Experiments on three public datasets demonstrate that RouteDK achieves accuracy comparable to or better than teacher LLMs while maintaining computational efficiency.

## Method Summary
RouteDK employs three LoRA experts trained separately on different knowledge types: Base Expert (raw item relationships), High-level Expert (general bundling rules distilled via self-reflection), and Fine-grained Expert (session-specific reasoning distilled via chain-of-thought). These experts are combined using a dynamic fusion module that performs input-aware routing at each transformer layer. The router uses context vectors derived from average pooled hidden states to produce expert weights, enabling context-adaptive knowledge selection. During inference, the model generates N parallel candidate bundles using temperature sampling and applies majority voting to select the most consistent solution. The framework is trained on three SIGIR 2022 datasets (Electronic, Clothing, Food) using a Llama3.1-8B-Instruct backbone with QLoRA optimization.

## Key Results
- RouteDK achieves 9.32% average gain over knowledge distillation baselines across three datasets
- Dynamic fusion outperforms static and averaged fusion strategies on Precision/Recall metrics
- Test-time scaling with N=8 improves accuracy by capturing model consensus while N=32 introduces noise
- Input-aware routing enables context-adaptive knowledge selection, with expert weights varying by session complexity

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Conflict Resolution via Specialized LoRA Experts
Separating different types of distilled knowledge into specialized LoRA experts mitigates knowledge conflicts that arise when naively concatenating all knowledge types. Each expert (Base, High-level, Fine-grained) learns to internalize a distinct type of knowledge through separate MLE objectives. This separation prevents gradient interference during training and allows each expert to specialize in different bundling knowledge.

### Mechanism 2: Input-Aware Dynamic Routing for Context-Adaptive Fusion
An input-aware router that dynamically assigns expert weights at each layer improves bundle generation over static or averaged fusion strategies. The router produces softmax-normalized weights for each expert based on layer-specific context vectors, enabling the model to emphasize different knowledge types based on session characteristics. This layer-wise routing allows granular control over knowledge contribution throughout the generation process.

### Mechanism 3: Test-Time Scaling via Self-Consistency for Variance Reduction
Majority voting over N parallel stochastic decoding runs reduces variance and improves inference reliability without architectural changes. At inference, the model generates N candidate bundle sequences using temperature sampling. Candidates are normalized by sorting item IDs to eliminate ordering variations. The most frequent sequence is selected via majority voting, effectively averaging out sampling noise and capturing the model's internal distributional consensus.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** RouteDK builds on LoRA as the base parameter-efficient fine-tuning mechanism. Understanding that LoRA adds trainable low-rank matrices A and B to frozen weights is prerequisite to grasping how multiple LoRA experts can be trained and combined.
  - **Quick check question:** If a LoRA module has rank r=16 and the frozen layer has dimensions [din=4096, dout=4096], how many trainable parameters does the LoRA module add?

- **Concept: Mixture of Experts (MoE) Routing**
  - **Why needed here:** The dynamic fusion strategy implements MoE-style routing where expert contributions are weighted per input. Distinguishing between token-level routing and sequence-level routing is critical.
  - **Quick check question:** In RouteDK, the router produces weights at each layer based on the pooled hidden state z^l. What would happen if routing used the full sequence H^{l-1}_dyn instead of z^l?

- **Concept: Knowledge Distillation Paradigms**
  - **Why needed here:** RouteDK distills two complementary knowledge types (rules vs. reasoning) from a teacher LLM. Understanding explicit vs. implicit distillation helps contextualize why rule distillation uses self-reflection and reasoning distillation uses chain-of-thought.
  - **Quick check question:** Why might distilling both high-level rules and fine-grained reasoning create conflicts when naively concatenated, even though both come from the same teacher?

## Architecture Onboarding

- **Component map:**
```
RouteDK Architecture:
├── Frozen Backbone LLM (Llama3.1-8B-Instruct, θ frozen)
│   └── Per-layer LoRA injection points
├── Three LoRA Experts (trained separately, ϕ_ei)
│   ├── Base Expert (eB): raw session input only
│   ├── High-level Expert (eH): input + distilled rules
│   └── Fine-grained Expert (eF): input + CoT reasoning
├── Dynamic Fusion Module (ψ trained separately)
│   ├── AvgPool → context vector z^l
│   ├── Linear + Softmax → expert weights α^l
│   └── Weighted sum of LoRA updates Δ^l_ei
└── Inference Enhancement (no training)
    └── N parallel samples → normalize → majority vote
```

- **Critical path:**
  1. **Knowledge distillation from teacher:** Teacher LLM generates rules (self-reflection) and reasoning (CoT) for each training session
  2. **Expert training:** Each LoRA expert trained independently with expert-specific inputs (raw, rules-augmented, or CoT-augmented)
  3. **Router training:** Freeze backbone and all experts; train only router ψ to maximize likelihood via dynamic fusion
  4. **Inference:** Raw input → router produces weights → weighted expert fusion → N samples → majority vote

- **Design tradeoffs:**
  - N sampling candidates: Higher N improves robustness but adds linear inference cost. Paper finds N=8 optimal; N=32 degrades due to noise.
  - Number of experts (K): K=1 (single LoRA) and K=2 (raw + merged knowledge) underperform K=3. Each expert adds training time and parameter overhead.
  - Routing granularity: Layer-wise routing provides fine-grained control but requires N_layers router parameters. A single global router would be simpler but less adaptive.
  - Router input: Average pooling over sequence loses token-level detail. Using full attention would be more expressive but computationally heavier.

- **Failure signatures:**
  - **Uniform routing:** If router weights converge to ~0.33 for all experts across all layers, the system degrades to average pooling. Check router weight distribution during training.
  - **Expert collapse:** If one expert's contribution dominates (>0.9 weight) across all inputs, the router failed to learn discriminative patterns. Verify expert weight variance across validation samples.
  - **TTS degradation:** If accuracy drops as N increases beyond a threshold, candidate diversity is too high or model lacks calibration. Monitor candidate overlap rates.
  - **Knowledge conflict persists:** If dynamic fusion underperforms static fusion, the router may be overfitting. Check validation vs. training router loss gap.

- **First 3 experiments:**
  1. **Expert necessity validation:** Train with K=1, K=2, K=3 experts. Measure Precision/Recall gap. Expected: K=3 > K=2 > K=1, with largest gain from K=2→K=3.
  2. **Routing strategy comparison:** Implement Average Pooling, TIES merging, Static (learned fixed coefficients), and Dynamic (input-aware) fusion. Expected: Dynamic > Static > TIES > Average on most datasets.
  3. **TTS sensitivity analysis:** Sweep N ∈ {1, 4, 8, 16, 32} with temperature T=0.7. Plot Precision/Recall vs. N. Expected: Peak at N=8, degradation at N=32.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of multimodal signals (e.g., images, item relationships) into the LoRA experts further enhance bundle generation performance compared to the text-only RouteDK framework?
- **Basis in paper:** [explicit] The authors state, "Future work will explore incorporating images and item relationships to enrich the knowledge space and enhance task understanding," identifying the reliance on textual knowledge as a limitation.
- **Why unresolved:** The current architecture and experiments are designed exclusively for text-based inputs, leaving the potential visual or structural signals unutilized.

### Open Question 2
- **Question:** Can advanced inference-time scaling strategies, such as reward-model-guided selection, outperform the current majority voting mechanism in identifying optimal bundle solutions?
- **Basis in paper:** [explicit] The authors note that the current test-time scaling "remains simplistic, relying on majority voting that may miss optimal solutions," suggesting the use of reward models as a promising future direction.
- **Why unresolved:** The current method relies on frequency-based consensus (self-consistency), which cannot verify the correctness of novel but accurate reasoning paths.

### Open Question 3
- **Question:** Can structural optimizations, such as sparse expert activation, provide further efficiency gains beyond the parameter reduction achieved by the current LoRA implementation?
- **Basis in paper:** [explicit] The conclusion states, "Future work will investigate expert activation mechanisms... or selectively activating subsets of experts during inference," noting that current gains rely on parameter reduction rather than structural efficiency.
- **Why unresolved:** The current "Dynamic Fusion" computes outputs for all experts (Base, High-level, Fine-grained) and combines them with weights, meaning all experts are computationally active during inference.

### Open Question 4
- **Question:** Under what specific data conditions does static fusion outperform dynamic, input-aware routing in bundle generation tasks?
- **Basis in paper:** [inferred] The ablation study on fusion strategies notes that in the Food domain, "Static surpasses Dynamic in Recall," suggesting that "globally learned coefficients can sometimes provide more stable guidance."
- **Why unresolved:** The paper speculates that "domain signals" might be the cause but does not validate if this is due to data consistency, session noise, or limitations in the router's training convergence for specific domains.

## Limitations

- **Knowledge conflict quantification:** The paper demonstrates knowledge conflict exists but doesn't measure the magnitude of gradient interference or parameter-space overlap that causes this conflict.
- **Router generalization:** The input-aware routing mechanism depends on the assumption that pooled hidden states contain discriminative signals for routing decisions, but this isn't empirically validated.
- **TTS calibration:** The self-consistency mechanism assumes correct predictions cluster consistently while errors are random, without measuring model calibration or candidate diversity metrics.

## Confidence

- **High confidence:** The effectiveness of separating high-level rules and fine-grained reasoning into different LoRA experts. The ablation showing K=3 > K=2 > K=1 is direct and compelling.
- **Medium confidence:** The superiority of dynamic over static routing. While Figure 4 shows dynamic fusion outperforming alternatives, the router weight patterns and whether they capture meaningful input characteristics need deeper analysis.
- **Low confidence:** The optimality of N=8 for test-time scaling. The degradation at N=32 suggests noise issues, but without candidate diversity metrics or calibration analysis, the true cause is unclear.

## Next Checks

1. **Router weight analysis:** Measure the entropy and variance of expert weights across validation samples. Low entropy (<0.5 nats) indicates uniform routing; high variance indicates successful input discrimination. Compare against a random routing baseline.

2. **Expert contribution ablation:** After training, freeze the router and systematically zero out one expert at a time during inference. Measure the drop in performance to quantify each expert's contribution and verify they are truly complementary.

3. **Knowledge conflict measurement:** Train a single LoRA expert with both knowledge types (DK4) and measure the cosine similarity between expert gradients and between expert parameters. Compare against the three-expert configuration to quantify conflict reduction.