---
ver: rpa2
title: 'RT-DEMT: A hybrid real-time acupoint detection model combining mamba and transformer'
arxiv_id: '2502.11179'
source_url: https://arxiv.org/abs/2502.11179
tags:
- points
- estimation
- detection
- acupuncture
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RT-DEMT, a hybrid real-time acupoint detection
  model combining Mamba and Transformer architectures to address the challenges of
  slow localization speed and low accuracy in intelligent acupuncture systems. The
  method leverages Mamba's excellent inference efficiency through state-space modeling
  while retaining the global modeling advantages of Transformer's attention mechanism,
  achieving efficient global information integration for acupoint localization tasks.
---

# RT-DEMT: A hybrid real-time acupoint detection model combining mamba and transformer

## Quick Facts
- **arXiv ID**: 2502.11179
- **Source URL**: https://arxiv.org/abs/2502.11179
- **Reference count**: 40
- **Primary result**: Achieved state-of-the-art performance with 7.792 EPE and 10.05ms inference time on private human back acupoint dataset

## Executive Summary
RT-DEMT addresses the dual challenges of slow localization speed and low accuracy in intelligent acupuncture systems by combining Mamba's inference efficiency with Transformer's global modeling capabilities. The hybrid architecture leverages State Space Models for efficient sequence processing while retaining attention mechanisms for complex structural relationships between acupoints. A key innovation is the Residual Likelihood Estimation (RLE) technique that bypasses expensive upsampling processes, enabling direct coordinate regression with competitive accuracy. On a private dataset of 84 back acupoints from nearly 200 participants, RT-DEMT achieved approximately 14% improvement in both accuracy and speed compared to the second-best algorithm.

## Method Summary
RT-DEMT employs a Mamba-based backbone with SS2D blocks to extract features from serialized image patches, followed by an Efficient Hybrid Encoder combining AIFI (intra-scale) and CCFM (cross-scale) for feature fusion. The model uses IOU-aware query selection and a classification head with RLE + DSNT for direct coordinate regression, bypassing traditional heatmap upsampling. Training employs MSE loss with multi-scale features at resolutions h/32, h/16, and h/8. The architecture processes images through a critical path of 2D to 1D sequence transformation, feature extraction, and numerical coordinate prediction.

## Key Results
- Achieved 7.792 average Euclidean distance pixel error (EPE) on private human back acupoint dataset
- Reached 10.05 milliseconds average inference time per localization task
- Demonstrated approximately 14% improvement in both accuracy and speed compared to second-best algorithm
- Validated on dataset with 84 acupoints from nearly 200 healthy participants

## Why This Works (Mechanism)

### Mechanism 1: Linear-Complexity Global Context via State Space Models
The model achieves efficient global feature extraction by replacing CNN backbone with Mamba (State Space Model), avoiding quadratic computational cost of Transformers while maintaining global receptive field. Mamba models sequential data using discretized SSMs with $O(N)$ inference scaling through RNN-like recurrence, allowing processing of high-resolution image sequences as long sequences without memory explosions. Core assumption: 2D spatial dependencies can be effectively serialized into 1D sequences without losing local geometric relations.

### Mechanism 2: Direct Coordinate Regression via Residual Likelihood Estimation (RLE)
Bypasses computational bottleneck of generating and upsampling heatmaps by directly regressing coordinates using DSNT (soft-argmax) augmented by RLE. Standard heatmap methods require expensive deconvolution for resolution restoration, introducing quantization errors. RT-DEMT uses DSNT to convert low-resolution feature maps directly into numerical coordinates, with RLE modeling error distribution between predicted and ground truth coordinates as flow-based generation model. Core assumption: Feature maps contain sufficient spatial resolution and semantic density for accurate keypoint localization without intermediate probability distribution modeling.

### Mechanism 3: Hybrid Global Aggregation (Mamba + Transformer)
Integration creates synergy where Mamba handles efficient sequence modeling while Transformer handles structural relationship modeling. Mamba backbone extracts features from serialized image, passed to Transformer-based hybrid encoder (AIFI/CCFM from RT-DETR). While Mamba is efficient, Transformer's attention mechanism explicitly models complex dependencies between acupoints better than SSMs alone. Core assumption: Mamba features are compatible with Transformer attention mechanisms without extensive domain adaptation.

## Foundational Learning

- **Concept**: State Space Models (SSMs) vs. Attention
  - **Why needed here**: Mamba is core innovation for speed; understanding SSMs approximate attention by compressing history into state vector rather than attending to all past tokens explains why faster but potentially harder to train for "retrieval" tasks
  - **Quick check question**: Can you explain why an SSM has $O(N)$ complexity during inference while a standard Transformer has $O(N^2)$?

- **Concept**: Coordinate Classification vs. Heatmap Regression
  - **Why needed here**: Paper pivots away from dominant heatmap paradigm; understanding "Quantization Error" in heatmaps (limited by image stride) vs. "instability" of direct regression is necessary to understand why RLE (flow-based loss) is required
  - **Quick check question**: Why does upsampling (deconvolution) in heatmap methods increase computational cost and introduce quantization error?

- **Concept**: Key Point Localization (Acupoints vs. Pose)
  - **Why needed here**: Acupoints lack high-contrast edges and skeletal rigidity of standard pose estimation keypoints; justifies need for "global information integration" because local patches look identical
  - **Quick check question**: Why would a local patch CNN struggle to distinguish between "Lingtai" (6th thoracic) and "Zhiyang" (7th thoracic) acupoints compared to distinguishing head from shoulder?

## Architecture Onboarding

- **Component map**: Image $I \in \mathbb{R}^{H \times W \times 3}$ -> Mamba-based encoder (SS2D blocks + Down Sample Layers) -> Efficient Hybrid Encoder (Transformer-based) -> IoU-aware Query Selection + RLE + DSNT Head
- **Critical path**: The transformation of 2D images $\to$ 1D sequences (Mamba Backbone) $\to$ Feature Maps $\to$ Numerical Coordinates; RLE component is critical
- **Design tradeoffs**: Trading proven stability of CNN backbones for Mamba's speed (requires specific CUDA kernels); bypassing upsampling saves Flops but forces backbone to encode precise location info in low resolution latent spaces
- **Failure signatures**: "Stacking" Error (Mamba fails to capture long-range dependencies, confusing upper-back and lower-back acupoints); Regression Drift (RLE head predicts coordinates significantly outside image bounds without heatmap constraints)
- **First 3 experiments**:
  1. Backbone Ablation: Replace Mamba with ResNet50 (keeping RLE head fixed) to isolate contribution of SSM backbone to accuracy vs. speed
  2. Head Ablation: Swap RLE+DSNT head for standard Heatmap head to verify "bypass upsampling" claim results in higher throughput without accuracy loss
  3. Inference Latency Test: Profile `ait` (average inference time) component by component to ensure Mamba serialization + Transformer encoder doesn't introduce hidden memory bottlenecks

## Open Questions the Paper Calls Out

- **Open Question 1**: Can RT-DEMT maintain performance when applied to acupoint localization on complex, non-planar body regions (e.g., limbs, face) or standard human pose estimation datasets?
  - **Basis**: Authors validate exclusively on "private dataset of acupoints on the human back" despite noting acupoints exist across whole body
  - **Why unresolved**: Back provides uniform surface compared to articulated joints or variable facial features, making generalization unclear
  - **What evidence would resolve**: Evaluation results on public multi-person pose datasets (e.g., COCO) or proprietary dataset with limb and facial acupoints

- **Open Question 2**: How robust is model when applied to subjects with spinal deformities or abnormal body mass indices (BMI)?
  - **Basis**: Dataset collected from "nearly 200 healthy participants with normal body types and no apparent spinal deformities"
  - **Why unresolved**: "Bone proportional measurement" method assumes standard anatomical proportions which shift in pathological cases or obesity
  - **What evidence would resolve**: Performance metrics (EPE and PCK) on test subset of patients with high BMI or diagnosed spinal conditions

- **Open Question 3**: Can 2D pixel-level predictions be reliably translated into 3D operational space required for physical acupuncture robots?
  - **Basis**: Claims "significant potential for automated acupuncture robot systems" but relies entirely on 2D Euclidean distance pixel error
  - **Why unresolved**: Acupuncture requires inserting needle to specific depth (3D) relative to skin surface; 2D model lacks depth perception critical for safety and efficacy
  - **What evidence would resolve**: Integration with depth sensors or stereo vision in robotic prototype, measuring 3D positional error relative to patient's skin surface

## Limitations
- Evaluation on private dataset prevents independent verification of claimed 14% improvement and generalization capability
- Computational efficiency gains may not translate to all hardware configurations due to SSM-specific CUDA kernel requirements
- Model trained exclusively on healthy participants with normal body types, limiting applicability to pathological cases

## Confidence
- **High Confidence**: Core architectural components (Mamba backbone, DSNT coordinate regression, hybrid encoder) are well-established; efficiency claims (10.05ms inference time) are plausible
- **Medium Confidence**: 14% improvement claim is reasonable but magnitude depends heavily on private dataset characteristics; RLE mechanism lacks independent validation in acupoint detection domain
- **Low Confidence**: Assertion that RLE "surpasses heatmap accuracy" is particularly uncertain without ablation studies on comparable datasets

## Next Checks
1. **Dataset Generalization Test**: Apply RT-DEMT to public keypoint detection dataset (e.g., COCO) to verify 14% improvement claim and assess performance across diverse body types and poses
2. **Ablation Study Replication**: Implement three ablation experiments (backbone replacement with ResNet50, head replacement with heatmap approach, component-wise latency profiling) to isolate contribution of each innovation
3. **Robustness Evaluation**: Test model performance on back images with occlusions (clothing, hair), varying lighting conditions, and different camera angles to assess real-world applicability in clinical settings