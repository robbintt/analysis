---
ver: rpa2
title: Validity Arguments For Constructed Response Scoring Using Generative Artificial
  Intelligence Applications
arxiv_id: '2501.02334'
source_url: https://arxiv.org/abs/2501.02334
tags:
- scores
- human
- scoring
- evidence
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for validating constructed response
  scoring using generative AI, addressing the gap in transparency and consistency
  compared to traditional NLP-based AI scoring. It outlines specific validity evidence
  needed, including documentation of LLM selection, prompting strategies, fine-tuning
  decisions, and reproducibility studies.
---

# Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications

## Quick Facts
- arXiv ID: 2501.02334
- Source URL: https://arxiv.org/abs/2501.02334
- Reference count: 7
- Primary result: Framework for validating generative AI scoring of constructed responses with moderate agreement to human ratings (QWK: .55-.77)

## Executive Summary
This paper addresses the critical gap in validity evidence for using generative AI, specifically large language models like GPT-4, to score constructed responses in educational assessment. Unlike traditional NLP-based scoring systems, generative AI models operate as black boxes without the transparency of feature-based scoring, necessitating new frameworks for establishing validity. The authors propose a comprehensive approach that extends beyond traditional validity frameworks to include documentation of LLM selection, prompting strategies, fine-tuning decisions, and reproducibility studies.

The study demonstrates this framework using GPT-4 on TOEFL, GRE, and Praxis datasets, finding moderate agreement with human ratings (QWK: .55-.77) but lower than traditional e-rater models. The authors emphasize that while generative AI offers potential benefits for scoring, rigorous validation is essential before operational use in high-stakes testing contexts. The paper outlines specific validity evidence needed, including fairness analyses and consistency checks, and highlights open questions about combining scores from multiple LLMs and adapting explainable AI tools for educational assessment.

## Method Summary
The study used GPT-4 (version 0311) with temperature=0 and zero-shot prompting to score constructed responses from TOEFL iBT, GRE, and Praxis tests. The dataset included 1,581 total responses across 14 items (GRE: 569 responses/5 items, Praxis: 357 responses/4 items, TOEFL: 246 responses/5 items). Prompts were structured with XML tags for question text, scoring rubric, and response text, requesting JSON output with score and three reasons. The primary metric was Quadratic Weighted Kappa (QWK) between GPT-4 and human ratings, with secondary analyses including partial/semi-partial correlations, exact agreement rates, and conditional score distributions.

## Key Results
- GPT-4 achieved moderate agreement with human ratings (QWK: .55-.77) across all three test datasets
- Agreement was lower than traditional e-rater models, which achieved QWK values of .70-.87 on similar datasets
- The framework successfully documented validity evidence including LLM selection, prompting strategies, and reproducibility considerations

## Why This Works (Mechanism)
Generative AI models can score constructed responses by leveraging their natural language understanding capabilities to interpret both the prompt instructions and the response content. The zero-shot prompting approach allows the model to apply its pre-trained knowledge to score responses according to provided rubrics without requiring additional training data. The structured XML-tagged prompts ensure the model can parse and differentiate between the question, rubric, and response components, while the JSON output format enables systematic score extraction and analysis.

## Foundational Learning
- **Quadratic Weighted Kappa (QWK)**: Measures agreement between two raters accounting for chance agreement and score proximity. Why needed: Standard metric for comparing machine and human scores in educational assessment. Quick check: QWK of 1 indicates perfect agreement, 0 indicates chance agreement.
- **Validity frameworks in educational measurement**: Established approaches for demonstrating that test scores measure what they claim to measure. Why needed: Provides foundation for extending validity arguments to AI scoring systems. Quick check: Five sources of validity evidence include test content, response processes, internal structure, relationships with other variables, and consequences.
- **Zero-shot prompting**: Providing task instructions without examples during inference. Why needed: Allows use of pre-trained models without fine-tuning on scoring tasks. Quick check: Temperature=0 ensures deterministic outputs within model limitations.
- **Constructed response scoring**: Evaluating open-ended responses rather than selected-response items. Why needed: More complex scoring task requiring understanding of content and argumentation. Quick check: Scores typically on ordinal scales (e.g., 1-6) reflecting quality levels.
- **Explainable AI (XAI)**: Tools and methods for understanding AI model decision-making. Why needed: Critical for establishing validity when models operate as black boxes. Quick check: Current XAI tools not validated for psychometric purposes.
- **Fairness and bias analysis**: Assessing differential performance across demographic groups. Why needed: Essential for high-stakes testing applications. Quick check: Differential item functioning (DIF) analysis can identify systematic biases.

## Architecture Onboarding
- **Component map**: Test dataset -> XML-tagged prompt -> GPT-4 API call -> JSON output -> Score extraction -> QWK calculation -> Validity analysis
- **Critical path**: Prompt design → API configuration → Score extraction → Agreement calculation
- **Design tradeoffs**: Zero-shot prompting preserves pre-trained knowledge but may underperform fine-tuned models; structured prompts ensure parsing but may constrain model flexibility
- **Failure signatures**: Non-deterministic outputs despite temperature=0; JSON parsing failures; score range violations; systematic bias across demographic groups
- **First experiments**: 1) Score extraction validation using regex fallback for malformed JSON; 2) Self-agreement testing by scoring same responses twice; 3) Sensitivity analysis of QWK scores to prompt design variations

## Open Questions the Paper Calls Out
**Open Question 1**: How can scores from multiple Large Language Models (LLMs) be effectively combined to maximize construct coverage in the absence of human ratings? The paper suggests factor analytic models might be an approach but lacks empirical data on combining multiple distinct LLMs.

**Open Question 2**: How can Explainable AI (XAI) tools be adapted to provide meaningful validity evidence for LLM scoring in educational assessment? Current XAI tools are not validated for psychometric purposes, making it difficult to connect LLM internal weights to scoring rubrics.

**Open Question 3**: What constitutes sufficient validity evidence for fully automated scoring systems that operate without expert-defined rubrics or human ratings? Current validity frameworks rely on a "chain of evidence" linking machine scores to human judgment, which breaks when removing the human element.

## Limitations
- Proprietary ETS datasets prevent independent verification of results
- Zero-shot prompting approach may not reflect optimal performance achievable with fine-tuning
- Focus on agreement metrics without detailed error analysis of systematic biases or differential performance across demographic groups

## Confidence
- **High confidence**: Framework for validity evidence documentation and general approach to prompt engineering
- **Medium confidence**: Reported QWK values (.55-.77) are reasonable but exact replication impossible without proprietary datasets
- **Low confidence**: Comparison between GPT-4 and traditional e-rater models may be confounded by differences in scoring scales and test formats

## Next Checks
1. **Replication with open datasets**: Apply the exact prompting framework to the ASAP Kaggle dataset to generate comparable QWK values and validate methodology independently
2. **Bias and fairness analysis**: Conduct differential item functioning (DIF) analysis across demographic subgroups using GPT-4 scores to assess potential fairness concerns
3. **Prompt engineering optimization**: Systematically vary prompt components to establish sensitivity of QWK scores to prompt design choices and identify optimal configurations