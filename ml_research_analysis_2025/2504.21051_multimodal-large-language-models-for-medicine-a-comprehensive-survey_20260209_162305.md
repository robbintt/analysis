---
ver: rpa2
title: 'Multimodal Large Language Models for Medicine: A Comprehensive Survey'
arxiv_id: '2504.21051'
source_url: https://arxiv.org/abs/2504.21051
tags:
- medical
- arxiv
- language
- data
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys medical multimodal large language models (MLLMs),
  reviewing 330 recent papers to identify their applications, data types, model characteristics,
  and challenges. MLLMs integrate large language models with modality encoders to
  handle complex medical tasks across imaging, text, and other data types.
---

# Multimodal Large Language Models for Medicine: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2504.21051
- Source URL: https://arxiv.org/abs/2504.21051
- Authors: Jiarui Ye; Hao Tang
- Reference count: 330
- Primary result: Survey identifies 330 papers on medical MLLMs, covering applications, data types, and challenges across imaging, text, and other modalities.

## Executive Summary
This survey comprehensively reviews medical multimodal large language models (MLLMs), analyzing their architecture, applications, and challenges across 330 recent papers. MLLMs integrate large language models with modality encoders to handle complex medical tasks across imaging, text, and other data types. Applications include medical report generation, professional medical communication, and clinical surgery assistance, each addressing critical healthcare needs. The authors analyze six data modalities and corresponding benchmarks, while discussing challenges such as data scarcity, hallucination, fairness, and bias. Proposed solutions include model optimization, data augmentation, edge deployment, and privacy-preserving techniques. Key requirements for clinical application include professionalism, accuracy, and empathy. The paper concludes that while MLLMs show significant potential, comprehensive evaluation frameworks and regulatory oversight are needed before widespread clinical implementation.

## Method Summary
The survey aggregates findings from 330 papers on medical MLLMs, analyzing their architecture (LLM + modality encoder + alignment module), applications (report generation, VQA, communication), data modalities (imaging, text, audio, genomics, structured data, sensors), and challenges (hallucination, bias, edge deployment). The authors categorize models by task, evaluate performance on medical benchmarks (VQA-RAD, SLAKE, USMLE), and propose solutions including LoRA optimization, data augmentation, and privacy-preserving techniques. The review synthesizes current state-of-the-art approaches and identifies gaps requiring further research.

## Key Results
- MLLMs achieve medical task performance by aligning modality-specific encoders with LLMs through learnable alignment modules
- Medical domain adaptation occurs primarily through supervised fine-tuning on curated image-text pairs and instruction-following datasets
- Hallucination arises from object misidentification, relationship hallucination, and over-reliance on LLM priors rather than visual evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs achieve medical task performance by aligning modality-specific encoders with a frozen or fine-tuned LLM through a learnable alignment module.
- Mechanism: A vision encoder (e.g., ViT) extracts image features; an alignment module (MLP or Q-Former) projects these into the LLM's embedding space; the LLM generates text conditioned on aligned multimodal representations.
- Core assumption: Pre-trained LLMs already encode sufficient language understanding; alignment quality determines cross-modal reasoning.
- Evidence anchors:
  - [section] "Most current MLLMs have similar structures. They put LLMs at their core, while incorporating an encoder and a diffusion generative model at the input and output" (Page 2, Section 2.2).
  - [section] "Between the encoder of the Large Scale Language Model (LLM) and the other modalities, there is an alignment module that aligns the text input with the input of the other modalities into the feature space" (Page 2).
  - [corpus] Related work on tool-augmented MLLMs confirms alignment modules as critical bottlenecks for cross-modal transfer.
- Break condition: If alignment module parameters are too small or training data insufficient, modalities fail to align, causing hallucination and degraded reasoning.

### Mechanism 2
- Claim: Medical domain adaptation occurs primarily through supervised fine-tuning on curated image-text pairs and instruction-following datasets.
- Mechanism: General-purpose MLLMs are fine-tuned on medical image-report pairs (e.g., chest X-rays with radiology reports) and medical VQA data; the model learns domain-specific vocabulary, report structure ("finding-impression"), and clinical reasoning patterns.
- Core assumption: High-quality paired medical data exists and can be standardized; pre-trained representations transfer across domains.
- Evidence anchors:
  - [section] "XrayGPT inputs X-ray images into a frozen vision encoder... utilizes the alignment module... to align the extracted image features with the caption text data" (Page 3).
  - [section] "LLaVA-Med training data consists of 600,000 biomedical image-text pairs extracted from PubMed Central, along with multi-turn dialogue data generated using GPT-4" (Table 1, Page 5).
  - [corpus] Corpus evidence on medical knowledge editing benchmarks (MedMKEB) confirms fine-tuning effectiveness but notes catastrophic forgetting risks.
- Break condition: If fine-tuning data is noisy, biased, or lacks reasoning chains, the model memorizes impressions without developing inferential capability.

### Mechanism 3
- Claim: Hallucination in medical MLLMs arises from object misidentification, relationship hallucination, and over-reliance on LLM priors rather than visual evidence.
- Mechanism: Insufficient visual encoder resolution, weak cross-modal alignment, or imbalanced training data causes the LLM to generate plausible but unsupported statements; mitigation requires visual supervision, segmentation grounding, and adversarial testing.
- Core assumption: Hallucinations can be detected and reduced through targeted training interventions and evaluation benchmarks.
- Evidence anchors:
  - [section] "Research has found that existing large language models focused on vision often overlook image information, relying excessively on the pre-existing knowledge of LLMs to make predictions" (Page 10).
  - [section] "Hallucinations caused by misidentification can be categorized into three types: type, property, and relationship" (Page 10).
  - [corpus] Related work on reasoning LLMs in medicine emphasizes explanation grounding as a hallucination mitigation strategy.
- Break condition: If evaluation benchmarks only measure surface similarity (BLEU, ROUGE) without grounding verification, hallucinations go undetected.

## Foundational Learning

- Concept: **Transformer self-attention and positional encoding**
  - Why needed here: All reviewed MLLMs use Transformer-based LLMs (LLaMA, Vicuna, PaLM) as backbones; understanding attention is prerequisite for debugging alignment failures.
  - Quick check question: Can you explain why self-attention enables variable-length input processing but positional encoding is required for sequence order?

- Concept: **Contrastive image-text pre-training (CLIP-style)**
  - Why needed here: Many medical MLLMs (Med-Flamingo, LLaVA-Med) build on CLIP-style pre-training; the ITC (Image-Text Contrastive) task is foundational for cross-modal alignment.
  - Quick check question: How does contrastive loss differ from generative loss in terms of what representations it encourages?

- Concept: **Catastrophic forgetting in continual learning**
  - Why needed here: The paper explicitly warns that dynamic fine-tuning on new medical knowledge can cause forgetting of prior knowledge; mitigation strategies (replay, EWC) are relevant.
  - Quick check question: Why does sequential fine-tuning on new data degrade performance on earlier tasks?

## Architecture Onboarding

- Component map:
  LLM backbone (LLaMA-2, Vicuna, PaLM) -> Modality encoder (ViT, Swin, RAD-DINO) -> Alignment module (MLP, Q-Former) -> Optional diffusion decoder

- Critical path:
  1. Freeze LLM backbone; train alignment module on image-text pairs (alignment phase)
  2. Unfreeze or use LoRA for efficient fine-tuning on medical instruction data (instruction-tuning phase)
  3. Evaluate on medical benchmarks (VQA-RAD, SLAKE, USMLE-style questions); iterate on data quality

- Design tradeoffs:
  - **Frozen vs. fine-tuned LLM**: Frozen preserves general capabilities, reduces compute; fine-tuned improves medical specificity but risks overfitting and forgetting
  - **MLP vs. Q-Former alignment**: MLP is simpler, faster; Q-Former extracts more fine-grained visual features but adds complexity
  - **2D vs. 3D imaging support**: 3D (CT, MRI) requires specialized encoders (ViT3D) and higher memory; most models focus on 2D

- Failure signatures:
  - **Hallucination**: Model generates findings not present in image (e.g., describing a fracture when none exists)
  - **Template rigidity**: Outputs generic report text regardless of input image specifics
  - **Modality imbalance**: Model ignores visual input and relies on text priors
  - **Demographic bias**: Performance degrades on underrepresented populations

- First 3 experiments:
  1. **Sanity check**: Load pre-trained LLaVA-Med or similar; run inference on sample chest X-ray with simple prompt ("Describe this image"). Verify output structure and relevance
  2. **Alignment ablation**: Replace Q-Former with linear MLP; measure performance delta on VQA-RAD benchmark to quantify alignment module contribution
  3. **Hallucination probe**: Run adversarial test with images containing no abnormalities; count rate of false positive findings in generated reports

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can medical MLLMs be continuously updated with evolving clinical knowledge without suffering from catastrophic forgetting of prior training?
- **Basis in paper:** [explicit] Section 6.1 states that dynamic training strategies are needed to maintain "freshness" of knowledge but notes continuous fine-tuning may lead to "catastrophic forgetting."
- **Why unresolved:** The tension between knowledge currency and retention is highlighted as a fundamental challenge; existing mitigation methods (e.g., regular reviews) are acknowledged but not proven for medical domain-specific stability.
- **What evidence would resolve it:** Empirical studies showing a medical MLLM can integrate new clinical guidelines or rare disease data over sequential updates while maintaining â‰¥95% accuracy on earlier benchmarks.

### Open Question 2
- **Question:** What lightweight architectures and optimization techniques enable effective deployment of medical MLLMs on edge devices in resource-constrained healthcare settings?
- **Basis in paper:** [explicit] Section 6.2 identifies that regions with scarce medical resources often lack computational infrastructure, and while techniques like LoRA and Med-MoE are proposed, it concludes these have "considerable room for growth."
- **Why unresolved:** The trade-off between model compression and diagnostic accuracy for edge deployment is not systematically evaluated across diverse medical tasks and hardware constraints.
- **What evidence would resolve it:** Comparative benchmarking of compressed medical MLLMs (e.g., via quantization, pruning) on standardized edge devices, measuring accuracy, latency, and energy consumption for tasks like report generation and VQA.

### Open Question 3
- **Question:** Can robust evaluation frameworks be developed to bridge the gap between high performance on medical licensing exams (e.g., USMLE) and reliability in actual clinical workflows?
- **Basis in paper:** [explicit] Section 5.1 notes models like GPT-4 and Med-PaLM achieve >86% on USMLE but still perform unsatisfactorily in clinical settings due to "over-dependence on memorization" and lack of inference. The conclusion calls for "comprehensive evaluation frameworks."
- **Why unresolved:** Current benchmarks are static and exam-oriented, failing to capture real-world complexity, patient interaction, and longitudinal reasoning.
- **What evidence would resolve it:** Creation and validation of a multi-modal, longitudinal clinical evaluation suite that correlates strongly (e.g., Pearson's r > 0.8) with clinician-assessed performance in prospective trials.

## Limitations
- Survey relies on reported metrics without independent replication; many models lack publicly available code or detailed hyperparameters
- Paper acknowledges but does not fully address potential dataset bias and domain shift when applying Western medical data to other populations
- Claims about clinical deployment readiness are limited by acknowledged regulatory and evaluation framework gaps

## Confidence
- **High confidence**: Core architecture descriptions (LLM + modality encoder + alignment module) and general application domains (report generation, VQA, communication)
- **Medium confidence**: Specific performance claims on medical benchmarks, as these depend on unreported implementation details and dataset quality
- **Low confidence**: Claims about clinical deployment readiness, as the paper explicitly notes regulatory and evaluation framework gaps

## Next Checks
1. **Independent benchmark replication**: Select 3-5 representative medical MLLMs from the survey and replicate their performance on VQA-RAD and SLAKE using provided checkpoints or re-implemented versions
2. **Hallucination detection validation**: Implement adversarial testing with normal/abnormal image pairs to measure false positive rates and compare against reported benchmarks
3. **Bias assessment across populations**: Evaluate top-performing models on Harvard-FairVLMed or similar fairness benchmarks to quantify demographic performance disparities