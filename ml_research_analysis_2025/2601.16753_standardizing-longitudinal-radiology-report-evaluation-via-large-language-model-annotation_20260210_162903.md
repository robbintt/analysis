---
ver: rpa2
title: Standardizing Longitudinal Radiology Report Evaluation via Large Language Model
  Annotation
arxiv_id: '2601.16753'
source_url: https://arxiv.org/abs/2601.16753
tags:
- longitudinal
- report
- reports
- annotation
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large language model (LLM)-based pipeline
  for automatically annotating longitudinal information in radiology reports. The
  method identifies longitudinal sentences and extracts disease progression labels,
  outperforming existing rule-based and lexicon-driven approaches.
---

# Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation

## Quick Facts
- arXiv ID: 2601.16753
- Source URL: https://arxiv.org/abs/2601.16753
- Authors: Xinyi Wang; Grazziela Figueredo; Ruizhe Li; Xin Chen
- Reference count: 40
- Primary result: LLM-based annotation pipeline achieves 11.3% higher F1-score for longitudinal sentence detection compared to rule-based approaches

## Executive Summary
This paper introduces a large language model (LLM)-based pipeline for automatically annotating longitudinal information in radiology reports. The method identifies longitudinal sentences and extracts disease progression labels, outperforming existing rule-based and lexicon-driven approaches. When evaluated on 500 manually annotated reports, the LLM-based approach achieved 11.3% and 5.3% higher F1-scores for longitudinal sentence detection and disease progression tracking, respectively. The pipeline was used to annotate 95,169 reports from the MIMIC-CXR dataset, creating a standardized benchmark for evaluating report generation models. Using this benchmark, seven state-of-the-art models were assessed, revealing that while recent models capture more longitudinal information than earlier baselines, substantial improvements are still needed.

## Method Summary
The method employs a two-stage LLM pipeline: first identifying longitudinal sentences containing temporal comparisons, then extracting disease progression labels (improved, no change, worsened, unmentioned). The approach uses Qwen2.5-32B with deterministic greedy decoding and specific prompt templates requiring structured output in angle brackets. The system processes raw radiology reports through Stanza sentence segmentation, applies text cleaning rules, and generates structured annotations for the L-MIMIC benchmark dataset. The pipeline was validated against 500 manually annotated reports from Chest ImaGenome and applied to 95,169 MIMIC-CXR follow-up reports.

## Key Results
- LLM-based approach achieved 11.3% and 5.3% higher F1-scores for longitudinal sentence detection and disease progression tracking, respectively, compared to rule-based baselines
- The benchmark (L-MIMIC) enabled standardized evaluation of seven state-of-the-art report generation models
- Recent models capture more longitudinal information than earlier baselines, but F1 scores for "improved" and "worsened" categories remain below 18%
- The pipeline demonstrates 15x faster inference speed compared to larger models while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1: Semantic Contextualization over Lexicon Matching
The LLM-based pipeline outperforms rule-based systems by interpreting semantic context rather than relying on rigid keyword presence. The model processes sentence embeddings to discern intent (e.g., comparison vs. static description) even when specific temporal lexicons are absent, reducing the false negative rate inherent in rule-based methods. The pre-trained LLM possesses sufficient embedded medical knowledge to distinguish clinical progression from general language use without domain-specific fine-tuning.

### Mechanism 2: Sequential Filtering via Conditional Prompts
Decomposing the annotation into a two-stage process (sentence detection followed by progression extraction) likely improves accuracy by narrowing the context window for complex reasoning. The pipeline first filters out cross-sectional noise, ensuring the progression labeler operates only on sentences with confirmed temporal signals, thereby reducing the search space for classification errors. The error propagation from the first stage to the second is lower than the error rate of a joint multi-task prompt.

### Mechanism 3: Deterministic Output Constraints
Enforcing structured output via greedy decoding likely stabilizes the benchmark generation by minimizing stochastic variance. By forcing the model to select the highest probability token within a constrained grammar (e.g., `<1>`, `<0>`), the system ensures reproducibility, which is critical for creating a standardized evaluation dataset. The model's probability distribution is well-calibrated enough that the highest probability token aligns with clinical truth.

## Foundational Learning

- **Concept:** Longitudinal vs. Cross-sectional Information
  - Why needed here: This is the primary classification task. One must distinguish between static findings (current state) and comparative findings (change over time) to correctly apply the annotation pipeline.
  - Quick check question: Does the sentence "The heart is enlarged" contain longitudinal information? (No, it is cross-sectional; "The heart has enlarged" is longitudinal).

- **Concept:** Hallucination in Generative Evaluation
  - Why needed here: The paper evaluates report generation models which are prone to "hallucinating" temporal comparisons. The LLM annotator must be robust enough to penalize this.
  - Quick check question: If a generated report says "Opacities improved" but the ground truth has no prior mention of opacities, is this a hallucination? (Yes, specifically a "redundant comparison").

- **Concept:** Token Efficiency vs. Model Scale
  - Why needed here: The paper selects Qwen2.5-32B over larger models based on efficiency. Understanding this tradeoff is crucial for architectural decisions.
  - Quick check question: Why might a smaller model (32B) be preferred over a larger one (72B) for annotation tasks? (Inference speed and cost, provided accuracy is sufficient).

## Architecture Onboarding

- **Component map:** Raw reports -> Stanza segmentation -> LLM prompts (Detection -> Keyword -> Progression) -> Structured L-MIMIC dataset
- **Critical path:** The Longitudinal Sentence Identification prompt is the primary gatekeeper. If this prompt misclassifies a sentence, the subsequent Disease Progression extraction is skipped or run on invalid data.
- **Design tradeoffs:** The authors traded the slightly higher theoretical performance of Llama3.3-70B for the 15x faster inference speed of Qwen2.5-32B. The system classifies at the sentence level, potentially missing inter-sentence dependencies or complex multi-finding sentence structures.
- **Failure signatures:** The model struggles to distinguish "increased [finding]" (worsened) from "increased [size description]" (cross-sectional). Report generation models often output "stable" findings without input priors; the evaluator flags these as false longitudinal claims.
- **First 3 experiments:**
  1. Run the exact prompts from Section 2.1.2 on a 50-sentence sample to verify the `<0>/<1>` constraint is reliably followed by your local LLM inference engine.
  2. Measure tokens/second for Qwen2.5-32B on your hardware against the paper's 18.45 tokens/s to estimate annotation time for the full 95k reports.
  3. Specifically test sentences with the word "increased" to quantify the misclassification rate between "cross-sectional" and "worsened" progression.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating prior reports into the prompt improve LLM identification of "worsened" disease progression cases?
- Basis in paper: Future improvements could involve more nuanced prompt engineering, such as integrating prior reports to provide essential clinical context
- Why unresolved: LLMs misinterpret 46/190 "worsened" sentences as cross-sectional due to ambiguous terms like "increased," missing only 9 "improved" cases.
- What evidence would resolve it: A controlled experiment comparing zero-context prompts vs. prompts augmented with prior reports, measuring F1 improvements specifically on the "worsened" class.

### Open Question 2
- Question: How do LLM-based longitudinal annotations perform across imaging modalities and anatomical regions beyond chest X-rays?
- Basis in paper: The validation in this study is limited to chest X-ray reports... constructing the necessary datasets and further evaluating the reliability of LLM-based annotations across a broader range of clinical scenarios are critical next steps
- Why unresolved: Expert-annotated longitudinal datasets for other modalities (CT, MRI) and body regions are scarce, limiting validation.
- What evidence would resolve it: Benchmarking the pipeline on multi-modal radiology datasets with ground-truth longitudinal annotations and reporting annotation accuracy metrics.

### Open Question 3
- Question: Can keyword-guided constraints during training improve report generation models' ability to capture longitudinal information?
- Basis in paper: Incorporating additional constraints, such as keywords, to better integrate prior images and reports into the training process represents a promising direction for future report generation research
- Why unresolved: Current models show substantial imbalance (F1 for "no change": 49.7-51.5% vs. "improved/worsened": <18%), suggesting they fail to recognize target diseases for comparison.
- What evidence would resolve it: Training report generation models with keyword-based attention or loss constraints, then evaluating progression class balance and longitudinal sentence coverage on L-MIMIC.

## Limitations
- Validation relies on a single annotated dataset (Chest ImaGenome) with 500 reports, limiting generalizability to other medical report corpora
- Systematic weaknesses in "worsened" classification category where the model struggles to distinguish true disease progression from cross-sectional descriptions
- Deterministic greedy decoding approach suppresses uncertainty signals, potentially introducing silent errors into the L-MIMIC benchmark

## Confidence

- **High confidence:** The comparative performance advantage of LLM over rule-based methods (11.3% F1 improvement) is well-supported by the experimental results on the 500-report validation set.
- **Medium confidence:** The scalability claims and benchmark creation are supported by the methodology, but the long-term stability of the LLM annotation quality hasn't been established through temporal validation.
- **Low confidence:** The generalizability of the approach to different medical domains or languages, as the evaluation is limited to chest X-ray reports in English.

## Next Checks

1. **Cross-dataset validation:** Apply the LLM annotation pipeline to a separate, independently annotated radiology report corpus to assess performance degradation and identify domain-specific failure modes.

2. **Uncertainty quantification test:** Modify the inference to use temperature sampling or top-k decoding on ambiguous cases, then compare the distribution of outputs against the greedy-decoded benchmark to identify systematic blind spots.

3. **Human-in-the-loop error analysis:** Have domain experts review a stratified sample of LLM annotations focusing on the "worsened" category and ambiguous temporal cases to determine if errors represent true negatives or model limitations.