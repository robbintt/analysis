---
ver: rpa2
title: Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs
  with Minimal Human Interventions
arxiv_id: '2502.08657'
source_url: https://arxiv.org/abs/2502.08657
tags:
- safety
- uni00000003
- samples
- uni00000013
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PT-ALIGN is a safety self-alignment method that minimizes human
  supervision by refining both positive and toxic samples for fine-grained dual instruction
  tuning of LLMs. The method generates instruction-response triplets with highly polarized
  safety contrasts, using LLM-generated constraints to synthesize both safe and severely
  harmful responses.
---

# Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions

## Quick Facts
- arXiv ID: 2502.08657
- Source URL: https://arxiv.org/abs/2502.08657
- Reference count: 40
- Key result: PT-ALIGN achieves 98.28% harmless accuracy on LLaMA2-13B with only 0.51% jailbreak success rate while maintaining 81.36% helpfulness

## Executive Summary
PT-ALIGN is a safety self-alignment method that minimizes human supervision by refining both positive and toxic samples for fine-grained dual instruction tuning of LLMs. The method generates instruction-response triplets with highly polarized safety contrasts, using LLM-generated constraints to synthesize both safe and severely harmful responses. Training employs maximum likelihood estimation for positive samples and fine-grained unlikelihood training at the token level for toxic samples, enabling the model to distinguish between safe and harmful content. Experiments on 9 popular open-source LLMs demonstrate significant safety improvements while maintaining helpfulness.

## Method Summary
PT-ALIGN refines positive and toxic samples for dual safety self-alignment of LLMs with minimal human interventions. The method starts with a brief human-written constraint seed, which the LLM expands into comprehensive constraint sets through text completion. These constraints guide the generation of instruction-response triplets where positive responses refuse harm and toxic responses provide harmful content. The model is then fine-tuned using maximum likelihood estimation for positive samples and fine-grained unlikelihood training for toxic samples, with a token-level indicator function to prevent penalizing shared safe prefixes. The approach achieves strong safety improvements while maintaining helpfulness with fewer than 50 human annotations.

## Key Results
- On LLaMA2-13B baseline: reduced harm scores from 2.21 to 0.30, increased harmless accuracy from 74.14% to 98.28%
- Maintained helpfulness at 81.36% compared to 81.36% baseline
- Achieved jailbreak attack success rates of only 0.51% versus 5.06% for the official chat version
- PT-Samples exhibit more pronounced semantic differences (cosine distance 0.3824) compared to preference-based pairs (0.2806)

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained token-level unlikelihood training prevents unintended penalization of safe tokens shared between positive and toxic responses. The method identifies γ* (the first mismatched token between positive response p and toxic response n) and applies UT loss only from that position onward, controlled by indicator function I(γ ≥ γ*). This preserves shared prefixes like "I'm sorry" while still pushing the model away from divergent harmful content.

### Mechanism 2
Self-generated constraints enable high-quality positive and toxic sample synthesis with minimal human supervision. From a brief human-written constraint seed (cp or cn), the LLM performs text completion to generate comprehensive constraint sets (Cp, Cn). These constraints then guide in-context generation of response pairs via inner-thought reasoning, ensuring contextually appropriate refusals (positive) or harmful content (toxic) rather than mechanical template responses.

### Mechanism 3
Highly polarized sample pairs (extreme positive vs. extreme toxic) create stronger safety gradients than preference-based pairs with minimal distinction. PT-ALIGN constructs deliberately contrasting pairs where positive responses refuse harm and toxic responses provide detailed harmful instructions. This large semantic gap provides clearer supervisory signal for the model to distinguish safe from unsafe outputs.

## Foundational Learning

**Maximum Likelihood Estimation (MLE) for Language Models**
- Why needed here: MLE loss (LMLE) trains the model to maximize log probability of positive (harmless) response tokens given an instruction. This is standard causal language modeling but applied specifically to safety-aligned responses.
- Quick check question: Can you explain why MLE alone cannot prevent harmful outputs—what distributional shift does it fail to address?

**Unlikelihood Training (UT)**
- Why needed here: UT loss (LUT) explicitly minimizes the probability of generating specific negative tokens, complementing MLE by pushing the model away from harmful outputs rather than only toward safe ones.
- Quick check question: Given LUT = −Σ log(1 − Prθ(nγ|...)), what happens when Prθ(nγ) approaches 1.0 versus 0.0?

**In-Context Learning (ICL) with Constraints**
- Why needed here: The method uses self-generated constraint texts as ICL examples to guide response generation without explicit fine-tuning at synthesis time. Inner thoughts are included to compel reasoning before responding.
- Quick check question: How does providing constraint examples in the prompt differ from fine-tuning on constraint-following data?

## Architecture Onboarding

**Component map:**
Safety Domains (D) → Topic Generation (T) → Instruction Synthesis (I)
                                                         ↓
Seed Constraints (cp, cn) → Constraint Completion (Cp, Cn) → Response Generation (P, N)
                                                         ↓
                                    Triplet Dataset S = {(I, P, N)}
                                                         ↓
                      Dual Tuning: MLE(P) + λ × fine-grained UT(N, γ*)

**Critical path:**
1. Topic subdivision must produce diverse, safety-relevant instructions (10 domains × 10 topics each)
2. Constraint auto-completion must generate coherent positive/negative rules
3. Fine-grained token alignment (finding γ*) must correctly identify first divergence point
4. Loss combination (λ = 0.4) must balance safety vs. helpfulness

**Design tradeoffs:**
- Sample count vs. efficiency: Paper shows scaling from 2K to 16K samples improves harmless score from 86% to 98% (Figure 4), but marginal returns diminish
- λ penalty coefficient: Values 0.1–1.0 tested; λ=0.4 optimal (Table IX). Lower λ under-weights safety; higher λ over-penalizes and degrades helpfulness
- LoRA vs. full fine-tuning: Paper uses LoRA (r=16, α=16) targeting {q_proj, k_proj, v_proj, o_proj} for efficiency—full fine-tuning may improve results but at much higher cost

**Failure signatures:**
- Mechanical refusals: If inner-thought ICL is omitted, model generates identical "I cannot help" responses to all instructions (Section III.B)
- Helpfulness collapse: If fine-grained indicator is removed, shared tokens are penalized, causing helpful score to drop (Table VIII: 81.36 → 77.97)
- Safety drift on small samples: With <2K samples, harmless score plateaus at ~86% (Figure 4)

**First 3 experiments:**
1. Ablate fine-grained indicator: Train with standard UT (no γ* masking) on LLaMA2-13B. Compare harmless/helpful scores to Table VIII baseline. Expected: harmless drops 1–2%, helpful drops 3–4%.
2. Vary λ sensitivity: Sweep λ ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0} on Alpaca-13B with fixed 8K samples. Plot harmless vs. helpful tradeoff curve to validate λ=0.4 as Pareto-optimal.
3. Sample scaling validation: Train PT-ALIGN on LLaMA2-7B-chat with {2K, 4K, 8K, 12K, 16K} samples. Reproduce Figure 4 curve to confirm harmless scaling behavior and identify saturation point for this model size.

## Open Questions the Paper Calls Out

### Open Question 1
How can the PT-ALIGN framework be effectively adapted for safety alignment in multi-modal large language models (MLLMs)? The authors state in the conclusion: "In the future, we plan to... extend its application to the safety alignment of multi-modal large language models." This remains unresolved as the current method relies on text-only token-level unlikelihood training which may not directly translate to cross-modal embeddings.

### Open Question 2
How does PT-ALIGN perform against more sophisticated and diverse attack scenarios beyond the currently tested methods like AutoDAN? While the paper demonstrates robustness against AutoDAN (0.51% ASR), the adversarial landscape is rapidly evolving and the token-level unlikelihood training may be susceptible to obfuscation or semantic attacks not yet tested.

### Open Question 3
Can the method for synthesizing corresponding adversarial samples be integrated into the PT-ALIGN pipeline to further enhance robustness? The authors explicitly list "investigat[ing] methods for synthesizing corresponding adversarial samples" as a future research direction. The current study focuses on refining static positive and toxic samples without exploring dynamic generation of adversarial examples during training.

### Open Question 4
Does the reliance on the base model to generate its own "severely toxic" samples introduce a risk of amplifying the model's inherent distributional biases? The method uses the LLM itself to synthesize toxic responses guided by self-constraints. While efficient, this assumes the base model possesses sufficient knowledge to generate diverse and representative toxic content without reinforcing specific biases present in its pre-training data.

## Limitations

- Constraint generation quality uncertainty: The method relies on LLM self-completion of constraint sets without validation that the generated constraints are semantically coherent, comprehensive, or correctly polarized.
- Generalization to unseen domains: The 9 safety domains are predefined and clustered. The method's ability to handle novel safety scenarios outside these domains is untested.
- False positive safety penalties: The fine-grained UT mechanism might incorrectly preserve harmful prefixes while penalizing legitimate safety refusals that happen to diverge early.

## Confidence

**High confidence**: The dual-training mechanism combining MLE for positive samples and fine-grained UT for toxic samples is technically sound with clear mathematical formulation and direct ablation evidence.

**Medium confidence**: The claim that self-generated constraints enable minimal human supervision is plausible but insufficiently validated with only "fewer than 50 human annotations" reported without characterizing constraint quality.

**Low confidence**: The assertion that highly polarized sample pairs create stronger safety gradients is supported by qualitative PCA analysis but lacks rigorous quantitative comparison between 0.3824 vs 0.2806 cosine distance differences.

## Next Checks

1. **Constraint quality audit**: Sample 100 constraint sets from Cp and Cn across multiple domains. Have safety experts rate each constraint for: (a) semantic coherence, (b) correct polarity (positive/negative), (c) completeness for the domain. Calculate inter-rater reliability and report proportion of correctly polarized, coherent constraints.

2. **Cross-domain generalization test**: After training PT-ALIGN on the 9 predefined domains, evaluate on a held-out "novel domain" set containing safety instructions from domains not in D. Compare performance drop to baseline to quantify domain generalization capability.

3. **Over-exposure analysis**: Train PT-ALIGN with increasing amounts of toxic samples (2K → 16K) while holding positive samples constant. Plot jailbreak success rate vs. toxic sample count to detect potential safety degradation from overexposure to harmful content.