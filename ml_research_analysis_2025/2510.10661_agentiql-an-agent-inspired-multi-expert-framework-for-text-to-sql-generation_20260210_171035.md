---
ver: rpa2
title: 'AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation'
arxiv_id: '2510.10661'
source_url: https://arxiv.org/abs/2510.10661
tags:
- qwen2
- query
- order
- customer
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGENTIQL improves text-to-SQL generation by decomposing complex
  queries into sub-questions, generating corresponding SQL sub-queries, and merging
  them with a refinement step for column selection. An adaptive router dynamically
  selects between this modular pipeline and a baseline parser based on query complexity.
---

# AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation

## Quick Facts
- **arXiv ID**: 2510.10661
- **Source URL**: https://arxiv.org/abs/2510.10661
- **Reference count**: 40
- **Key outcome**: AGENTIQL achieves up to 86.07% execution accuracy on Spider benchmark with 14B models using Planner&Executor merging strategy, narrowing gap to GPT-4-based state-of-the-art (89.65% EX).

## Executive Summary
AGENTIQL is a modular text-to-SQL framework that decomposes complex queries into sub-questions, generates corresponding SQL sub-queries, and merges them with refinement for column selection. The framework uses an adaptive router to dynamically select between this modular pipeline and a baseline parser based on query complexity. Evaluated on the Spider benchmark, AGENTIQL achieves strong performance with open-source LLMs, demonstrating up to 86.07% execution accuracy with 14B models while maintaining interpretability through exposed intermediate reasoning steps.

## Method Summary
AGENTIQL operates as a four-stage pipeline: (1) Division - Table Selection, Question Decomposition, and Query Generation with error-correction iterations; (2) Merge - Planner&Executor strategy using reasoning LLM to plan and coding LLM to execute; (3) Column Selection - reasoning LLM refines SELECT clause; (4) Routing - adaptive router selects between divide-and-merge pipeline and baseline parser. The framework uses Qwen2.5-Coder models and was trained on the Spider dataset across 8× A100 80GB GPUs for approximately 1450 GPU-hours.

## Key Results
- Achieves 86.07% execution accuracy with 14B models using Planner&Executor merging strategy
- Outperforms baseline few-shot prompting by 9.7% at 7B scale (75.85% vs 76.4%)
- Demonstrates 2-5% improvement from Column Selection refinement across configurations
- Narrows gap to GPT-4-based state-of-the-art (89.65% EX) while using smaller open-source models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing complex queries into sub-questions improves SQL generation accuracy for multi-step reasoning tasks.
- **Mechanism**: A reasoning agent breaks a natural language question into smaller sub-questions, each addressed by an SQL sub-query. These are then merged (via Planner&Executor or Last Sub-query strategy) to form the final SQL. This explicit step-by-step process exposes reasoning and handles complex operations like multi-joins and nested aggregations.
- **Core assumption**: Sub-questions can be accurately generated and mapped to correct sub-queries; the merging strategy reliably combines them without introducing errors.
- **Evidence anchors**: [abstract] "AGENTIQL improves text-to-SQL generation by decomposing complex queries into sub-questions, generating corresponding SQL sub-queries, and merging them..."; [section 3.1] Decomposition produces {x1, x2, ..., xk} = f_decomp(x, s̃); sub-queries {y1, y2, ..., yk} are generated and then merged; [corpus] Related work STaR-SQL (arXiv:2502.13550) demonstrates improved text-to-SQL via step-by-step rationale generation.
- **Break condition**: If the decomposition is flawed or sub-queries are incorrect, merging may produce invalid SQL, leading to failures (e.g., Figure 6: ambiguous column references due to flawed sub-query join).

### Mechanism 2
- **Claim**: Column Selection (CS) refinement aligns SQL outputs with user intent, boosting execution accuracy.
- **Mechanism**: After merging sub-queries into an intermediate SQL, a reasoning LLM reviews the SELECT clause against the original question and schema to adjust columns and their ordering. This corrects extraneous or misordered columns.
- **Core assumption**: The CS LLM can correctly interpret the original question's intent and map it to appropriate columns.
- **Evidence anchors**: [abstract] "...refinement step for column selection."; [section 3.3] Formal definition: y = f_col(x, s, ŷ); Table 1 shows CS improves EX by 2-5% across configurations.
- **Break condition**: If the CS LLM misinterprets the question or schema, it may introduce or fail to correct column errors (e.g., Figure 7: adding unrequested columns).

### Mechanism 3
- **Claim**: Adaptive routing optimally allocates queries to a baseline parser or the divide-and-merge pipeline based on complexity, improving efficiency and accuracy.
- **Mechanism**: A router (e.g., XGBoost classifier or reasoning-agent "judge") evaluates the query and schema to decide which path (baseline or modular) to use. Simple queries go to the baseline; complex ones use the full pipeline.
- **Core assumption**: The router can reliably assess complexity and predict which approach will perform better.
- **Evidence anchors**: [abstract] "An adaptive router dynamically selects between this modular pipeline and a baseline parser based on query complexity."; [section 3.4] Router exploits complementary strengths; Table 4 shows correlation between schema size (complexity proxy) and performance.
- **Break condition**: Incorrect routing decisions send queries to the wrong path, reducing overall accuracy (e.g., complex queries misrouted to baseline).

## Foundational Learning

- **Concept: Query Decomposition**
  - Why needed here: Breaking down complex NL questions into sub-questions is central to AGENTIQL's pipeline, enabling modular reasoning.
  - Quick check question: How would you decompose "Find customers with more than 2 orders who bought at least 3 distinct products" into sub-questions?

- **Concept: Schema Linking**
  - Why needed here: The framework must identify relevant tables and columns from the database schema to generate correct SQL.
  - Quick check question: For "average price of ordered products," which tables must be linked in a schema with Products, Orders, and Order_Items?

- **Concept: Execution Accuracy (EX)**
  - Why needed here: This is the primary evaluation metric, determining correctness based on whether the generated SQL produces the same result as the ground truth.
  - Quick check question: If a generated SQL returns correct rows but includes an extra column, is it considered correct under EX on Spider?

## Architecture Onboarding

- **Component map**: AGENTIQL consists of: (1) Adaptive Router, (2) Baseline Parser (direct LLM), (3) Divide-and-Merge Module (Reasoning Agent for decomposition + Coding Agent for sub-query generation + Merge Function), and (4) Column Selection Refinement. Inputs: NL question + schema; Output: final SQL query.
- **Critical path**: Router decision → either Baseline (direct) or Divide-and-Merge pipeline (decomposition → sub-query generation → merge → CS) → final SQL. The Planner&Executor merge strategy generally yields higher accuracy but more latency.
- **Design tradeoffs**: Planner&Executor merge vs Last Sub-query: higher accuracy vs lower latency. Routing can balance efficiency and accuracy but requires a reliable complexity signal.
- **Failure signatures**: 1) Decomposition errors lead to incorrect sub-questions (Figure 6: ambiguous column). 2) Column Selection may add extra or omit required columns (Figure 7: returns extra building names). 3) Routing errors send simple queries to complex pipeline (inefficient) or complex queries to baseline (inaccurate).
- **First 3 experiments**:
  1. **Ablation of CS**: Run the divide-and-merge pipeline with and without Column Selection on a held-out set. Measure EX difference to confirm 2-5% gain.
  2. **Router Analysis**: Implement a simple router based on table count (proxy for complexity). Evaluate routing accuracy (correct path selection) and overall EX on Spider test set. Compare to random routing.
  3. **Merge Strategy Comparison**: For the same model size (e.g., 14B), compare Last Sub-query vs Planner&Executor merge (both with CS) on a subset of complex queries. Measure EX and average latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AGENTIQL maintain its performance advantages on complex benchmarks like BIRD or SQL-Eval?
- Basis in paper: [explicit] The conclusion states that testing on additional benchmarks (e.g., BIRD or SQL-Eval) is necessary to assess generalizability.
- Why unresolved: Evaluation was restricted to the Spider dataset, limiting claims of robustness across diverse real-world database schemas.
- What evidence would resolve it: Reporting execution accuracy (EX) results on the BIRD and SQL-Eval test sets using the proposed modular pipeline.

### Open Question 2
- Question: Can reinforcement learning (RL) integration improve the quality of generated SQL queries?
- Basis in paper: [explicit] The conclusion suggests incorporating RL into the query generation stage to provide adaptive feedback.
- Why unresolved: The current framework relies on few-shot prompting without RL-based optimization for the coding agent.
- What evidence would resolve it: A comparative study measuring EX on Spider between the current generator and an RL-optimized version.

### Open Question 3
- Question: Can advanced routing mechanisms (e.g., XGBoost or RAG) outperform simple schema metrics in balancing efficiency and accuracy?
- Basis in paper: [explicit] The conclusion calls for investigating various routing options to adapt more effectively to diverse question complexities.
- Why unresolved: The experiments primarily analyzed simple metrics like table count rather than learned decision functions.
- What evidence would resolve it: Evaluation of routing accuracy and end-to-end latency using an XGBoost classifier or retrieval-augmented generation.

## Limitations

- The effectiveness of the adaptive router depends on the quality of the complexity assessment, which is not fully specified in the paper.
- The Column Selection refinement, while improving accuracy, introduces potential for adding unrequested columns, highlighting a tradeoff between completeness and precision.
- The merge strategies show significant accuracy differences, but the robustness of these strategies across diverse schema structures needs further validation.

## Confidence

- **High**: The divide-and-merge pipeline improves text-to-SQL generation accuracy, especially for complex queries. The Column Selection refinement consistently improves execution accuracy by 2-5%.
- **Medium**: The adaptive router improves efficiency by selecting the appropriate path (baseline vs. pipeline) based on query complexity, though the specific implementation details are unclear.
- **Low**: The Planner&Executor merge strategy consistently outperforms Last Sub-query across all model sizes and schema complexities.

## Next Checks

1. Implement and evaluate the adaptive router with a simple complexity heuristic (e.g., table count) to assess routing accuracy and its impact on overall execution accuracy.
2. Conduct an ablation study of the Column Selection refinement on a held-out set to quantify its contribution to execution accuracy and identify potential over-selection issues.
3. Analyze failure cases where the decomposition step produces incorrect sub-questions, focusing on ambiguous column references and incomplete coverage of required tables/joins.