---
ver: rpa2
title: 'MedForge: Building Medical Foundation Models Like Open Source Software Development'
arxiv_id: '2502.16055'
source_url: https://arxiv.org/abs/2502.16055
tags:
- data
- merging
- branch
- medforge
- plugin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedForge tackles the challenge of developing medical foundation
  models in a privacy-sensitive, multi-center environment where data silos hinder
  collaborative model development. It introduces a community-driven, asynchronous
  framework that allows distributed medical institutions to contribute task-specific
  knowledge via lightweight LoRA modules and distilled datasets, without sharing raw
  patient data.
---

# MedForge: Building Medical Foundation Models Like Open Source Software Development

## Quick Facts
- **arXiv ID**: 2502.16055
- **Source URL**: https://arxiv.org/abs/2502.16055
- **Reference count**: 19
- **Primary result**: Achieves up to 20% better accuracy and 8% better AUC than baseline collaborative methods through asynchronous, privacy-preserving LoRA module merging

## Executive Summary
MedForge addresses the challenge of developing medical foundation models in privacy-sensitive, multi-center environments where data silos hinder collaborative model development. The framework enables distributed medical institutions to contribute task-specific knowledge via lightweight LoRA modules and distilled datasets without sharing raw patient data. Through experiments on breast, lung, and colon cancer histopathology datasets, MedForge-Mixture achieves up to 20% better accuracy and 8% better AUC than baseline collaborative methods while maintaining robustness to task merging order.

## Method Summary
MedForge is a community-driven, asynchronous framework for privacy-preserving medical foundation model development. It uses LoRA modules as plugin units that can be trained locally on private data and merged through two strategies: Fusion (parameter averaging) and Mixture (weighted output aggregation). Dataset distillation via distribution matching enables knowledge transfer without exposing raw data. The framework supports continuous, decentralized contributions where institutions can asynchronously add task-specific modules that get merged into a growing foundation model.

## Key Results
- MedForge-Mixture achieves 0.909 ACC / 0.941 AUC on average across three tasks vs. 0.806 ACC / 0.892 AUC for Fusion
- Mixture strategy shows consistent performance across merge orders (0.907-0.931 AUC range) while Fusion varies significantly (0.846-0.892 AUC range)
- Distilled datasets with 20 images per class achieve 0.909 ACC vs. 0.875 for alternative distillation methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Distributed medical institutions can contribute specialized knowledge asynchronously via lightweight LoRA modules without real-time synchronization or raw data sharing.
- **Mechanism**: Each institution trains task-specific LoRA modules locally on private data using low-rank decomposition (B ∈ Rd×r, A ∈ Rr×k where r ≪ min(d,k)), then asynchronously commits them to a central repository.
- **Core assumption**: Task-specific LoRA modules trained independently can be combined without catastrophic forgetting.
- **Evidence anchors**: [abstract] mentions LoRA module merging; [section 3.2] details LoRA application to CLIP; limited corpus support for asynchronous LoRA merging specifically.

### Mechanism 2
- **Claim**: Dataset distillation via distribution matching enables privacy-preserving knowledge transfer for guiding merge coefficients without exposing raw patient data.
- **Mechanism**: Distribution matching (DM) synthesizes small-scale distilled datasets by minimizing Maximum Mean Discrepancy between real and synthetic feature distributions, with Differentiable Siamese Augmentation (DSA) enhancing robustness.
- **Core assumption**: Distilled datasets retain sufficient statistical information to optimize merging weights effectively.
- **Evidence anchors**: [section 3.1.2] describes MMD-based distillation; [table 3] shows DM-DSA's superior performance; no direct corpus validation of this specific approach.

### Mechanism 3
- **Claim**: Output-weighted mixture strategy preserves individual module integrity better than direct parameter fusion.
- **Mechanism**: Instead of fusing parameters, Mixture computes weighted output combinations while keeping each module's internal parameters unchanged.
- **Core assumption**: Output-level aggregation introduces less noise and interference than parameter-level modification.
- **Evidence anchors**: [table 1] shows 10%+ accuracy improvement for Mixture; [table 2] demonstrates reduced order sensitivity; related work focuses on different optimization strategies.

## Foundational Learning

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - **Why needed here**: Understanding low-rank decomposition is essential for implementing branch contributions
  - **Quick check question**: Can you explain why LoRA's low-rank constraint (r=16 in experiments) enables both efficient training and module merging?

- **Concept: Distribution Matching for Dataset Distillation**
  - **Why needed here**: The privacy-preserving knowledge transfer mechanism relies on creating representative synthetic datasets
  - **Quick check question**: How does minimizing Maximum Mean Discrepancy ensure distilled data represents the original distribution without memorizing individual samples?

- **Concept: Gradient-Free Optimization (Shiwa Algorithm)**
  - **Why needed here**: Coefficient optimization for merging uses gradient-free methods
  - **Quick check question**: Why might gradient-free optimization be preferred over gradient descent for learning merge coefficients across heterogeneous LoRA modules?

## Architecture Onboarding

- **Component map**: CLIP ViT-B/16 (frozen) -> LoRA modules (rank=16, alpha=16) -> Distilled dataset generator (DM-DSA) -> Merge controller (Shiwa/Cobyla) -> Inference layer (cosine similarity)
- **Critical path**: 1) Branch trains LoRA locally on private data, 2) Branch generates distilled dataset via DM-DSA, 3) Branch uploads LoRA + distilled data, 4) Merge controller optimizes coefficients, 5) Updated forge item integrated into main branch, 6) Inference uses merged model
- **Design tradeoffs**: Fusion vs. Mixture (storage vs. performance), distillation quality vs. privacy, CLIP lock-in vs. architecture flexibility
- **Failure signatures**: Order sensitivity in Fusion, distillation quality degradation (>5% ACC gap), catastrophic forgetting (>10% performance drop), coefficient instability (extreme values)
- **First 3 experiments**: 1) Single-task LoRA baseline reproduction on each dataset, 2) Distillation quality validation comparing distilled vs. raw data training, 3) Two-task merge order sensitivity testing Fusion vs. Mixture

## Open Questions the Paper Calls Out
- How does MedForge perform when extended to non-classification medical AI tasks such as semantic segmentation, object detection, or report generation?
- Can MedForge provide formal privacy guarantees (e.g., differential privacy bounds) rather than relying solely on heuristic privacy benefits of dataset distillation?
- How does MedForge scale when the number of contributing institutions and task-specific LoRA modules grows substantially?

## Limitations
- Dependency on high-quality dataset distillation for capturing rare pathology patterns
- CLIP base model lock-in restricts architecture choices and may not generalize to non-natural image modalities
- No exploration of scaling to hundreds of contributors where storage and computation costs could become prohibitive

## Confidence
- **High Confidence**: MedForge-Mixture's superior accuracy and order-robustness over Fusion
- **Medium Confidence**: Privacy guarantees of distillation—no empirical validation of membership inference resistance
- **Low Confidence**: Scalability to large contributor pools—only 3 datasets tested, no scaling analysis

## Next Checks
1. **Distillation fidelity test**: Generate distilled datasets with varying IPC (5, 10, 20, 50 images/class) and measure ACC degradation vs. raw data LoRA training
2. **Rare pattern preservation**: Introduce synthetic rare-class samples (1% prevalence) into source datasets and verify they appear in distilled data through nearest-neighbor analysis
3. **Contributor scaling simulation**: Simulate 10-50 contributor merges using existing datasets to measure Mixture's storage/compute scaling and identify inflection points where Fusion becomes preferable