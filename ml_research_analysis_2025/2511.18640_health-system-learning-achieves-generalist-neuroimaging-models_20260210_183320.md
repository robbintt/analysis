---
ver: rpa2
title: Health system learning achieves generalist neuroimaging models
arxiv_id: '2511.18640'
source_url: https://arxiv.org/abs/2511.18640
tags:
- neurovfm
- data
- findings
- brain
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A novel health system learning approach enables generalist neuroimaging
  models by training directly on uncurated clinical MRI and CT data. The NeuroVFM
  visual foundation model, trained on 5.24 million clinical volumes via scalable volumetric
  self-supervised learning, achieved state-of-the-art performance with mean AUROC
  of 92.7% for CT and 92.5% for MRI diagnoses.
---

# Health system learning achieves generalist neuroimaging models

## Quick Facts
- arXiv ID: 2511.18640
- Source URL: https://arxiv.org/abs/2511.18640
- Reference count: 40
- Primary result: Health system learning approach trained on 5.24 million clinical volumes achieved state-of-the-art performance with mean AUROC of 92.7% for CT and 92.5% for MRI diagnoses

## Executive Summary
A novel health system learning approach enables generalist neuroimaging models by training directly on uncurated clinical MRI and CT data. The NeuroVFM visual foundation model, trained on 5.24 million clinical volumes via scalable volumetric self-supervised learning, achieved state-of-the-art performance with mean AUROC of 92.7% for CT and 92.5% for MRI diagnoses. It exhibited emergent neuroanatomical understanding, zero-shot cross-modal transfer, and interpretable diagnostic grounding. When paired with open-source language models for report generation, NeuroVFM surpassed frontier models like GPT-5 and Claude Sonnet 4.5 in accuracy, clinical triage, and expert preference while reducing hallucinations and laterality errors. These results demonstrate that health system learning can produce safe, high-performance generalist medical AI models.

## Method Summary
NeuroVFM employs a volumetric Joint-Embedding Predictive Architecture (Vol-JEPA) trained on uncurated clinical neuroimaging data. The model uses a 3D ViT encoder with a predictor network and EMA-updated teacher encoder to predict masked volumetric regions in latent space. Training involves ~85% masking of foreground tokens with neuroanatomy-aware sampling, using smooth L1 loss between predicted and teacher latents. The model processes 4×16×16 voxel patches after intensity normalization and quantization. Downstream tasks use attentive pooling over volume tokens for classification and report generation through integration with open-source LLMs.

## Key Results
- NeuroVFM achieved state-of-the-art diagnostic performance with mean AUROC of 92.7% for CT and 92.5% for MRI across 82 and 74 diagnoses respectively
- Zero-shot cross-modal transfer enabled classifiers trained on CT to work on MRI and vice versa for conditions like Chiari malformation and ventriculomegaly
- When paired with open-source language models, NeuroVFM surpassed frontier models like GPT-5 and Claude Sonnet 4.5 in diagnostic accuracy, clinical triage, and expert preference while reducing hallucinations and laterality errors

## Why This Works (Mechanism)

### Mechanism 1: Health System Data Distributional Advantage
Training on uncurated clinical data from routine care produces representations that outperform internet-scale pretraining for neuroimaging tasks. Health system data captures the true distribution of disease presentations, imaging protocols, and edge cases that clinicians actually encounter, unlike curated public datasets that systematically underrepresent neuroimaging due to facial identifiability concerns.

### Mechanism 2: Volumetric Joint-Embedding Predictive Architecture (Vol-JEPA) Learns Semantic Invariants
Predicting masked volumetric regions in latent space encourages learning of anatomical and pathological semantics rather than pixel-level reconstruction. By predicting representations (not pixels) of masked brain regions from visible context, the model must infer underlying anatomical structure rather than memorize local textures.

### Mechanism 3: Cross-Modal Transfer Through Shared Anatomy Representation
Joint CT-MRI training produces a unified latent space where the same anatomical structures map to similar representations regardless of imaging modality. Since both modalities image the same underlying brain anatomy, forcing them into a shared representation space causes the model to disentangle modality-specific features from anatomy-specific features that are invariant across modalities.

## Foundational Learning

- **Self-Supervised Learning (SSL) Objectives**
  - Why needed here: Vol-JEPA is an SSL method; understanding the distinction between contrastive, reconstructive, and predictive objectives is essential for grasping why this approach scales without manual labels
  - Quick check question: Can you explain why predicting latents (JEPA) might learn different features than reconstructing pixels (MAE)?

- **Vision Transformer (ViT) Tokenization**
  - Why needed here: The model tokenizes 3D volumes into patches; understanding how spatial information is encoded in tokens and position embeddings is critical for interpreting the masking and attention mechanisms
  - Quick check question: How does the receptive field of a token in a 3D ViT compare to a 2D ViT given the 4×16×16 patch size?

- **Foundation Model Scaling Laws**
  - Why needed here: The paper explicitly investigates scaling behavior (data volume, model size); distinguishing between empirical observations and theoretically guaranteed scaling is necessary for extrapolation
  - Quick check question: What does the observed log-linear relationship between positive training examples and F1 score imply about data collection priorities?

## Architecture Onboarding

- **Component map:** Input preprocessing -> Vol-JEPA training (Student encoder -> Predictor -> Target latent prediction; Teacher encoder -> Target latent ground truth) -> Downstream probing (Study-level attentive pooling -> Classification head) -> Report generation (Frozen NeuroVFM encoder -> Perceiver resampler -> MLP connector -> Qwen3-14B LLM)

- **Critical path:** Data preprocessing quality (incorrect orientation handling or windowing will corrupt the entire pipeline) -> EMA decay rate in teacher encoder (too fast = unstable targets; too slow = slow learning) -> Context/target ratio (too little context = insufficient signal; too much = trivial prediction)

- **Design tradeoffs:** CT multi-window (brain/blood/bone windows increase storage but capture different pathologies; weighted sampling biases toward parenchymal findings) vs. 4mm slice thickness (preserves clinical acquisition axis but loses through-plane resolution) vs. Token truncation at 20 patches/axis (bounds memory but may truncate large lesions)

- **Failure signatures:** Representation collapse (all embeddings converge to similar values; check via variance of token embeddings across a batch) -> Modality shortcut (model distinguishes CT vs. MRI via background statistics rather than content; probe for modality classification accuracy) -> Attention to background (grounding attention maps highlight air/skull instead of pathology)

- **First 3 experiments:** Ablate masking strategy (compare uniform random masking vs. neuroanatomy-informed block masking on downstream AUROC for 5 representative diagnoses) -> Probe cross-modal transfer systematically (train classifier on CT for all 82 diagnoses, evaluate zero-shot on MRI) -> Scale data vs. model (train ViT-Small, ViT-Medium, ViT-Base on 5%, 25%, 100% of data in factorial design)

## Open Questions the Paper Calls Out

- **Integrating multimodal data:** Does integrating longitudinal clinical outcomes and genomic data into the NeuroVFM framework improve its ability to predict disease progression compared to imaging-only analysis? The current study focuses on cross-sectional imaging data, and constructing unified representations of disease progression requires modeling complex, heterogeneous data types.

- **Generalizing beyond neuroimaging:** Can the Vol-JEPA architecture maintain its state-of-the-art diagnostic accuracy when applied to non-neurological medical imaging modalities and other body regions? The current model was trained and evaluated exclusively on neuroimaging (CT/MRI of head/neck).

- **Prospective clinical deployment:** How does the performance and safety of NeuroVFM change when deployed prospectively in live clinical triage workflows compared to its retrospective evaluation? While the model reduced hallucinations in a retrospective test set, real-world clinical deployment introduces feedback loops and user interaction dynamics.

## Limitations

- **Data access restriction:** The UM-NeuroImages dataset is private, limiting independent verification of results and broader community evaluation. This creates a fundamental reproducibility barrier where performance claims cannot be externally validated.

- **Cross-modal transfer validity:** While the paper demonstrates zero-shot transfer between CT and MRI for 3 conditions, the corpus lacks evidence that this generalizes across the full diagnostic spectrum. The mechanism assumes anatomical invariance, but some pathologies may manifest modality-specifically.

- **Clinical impact extrapolation:** Superior performance on retrospective data does not guarantee real-world clinical utility. The paper demonstrates technical capability but lacks prospective deployment data or human-in-the-loop validation showing improved patient outcomes.

## Confidence

- Health system data advantage: High confidence
- Vol-JEPA architecture efficacy: Medium confidence
- Cross-modal transfer capability: Low-Medium confidence

## Next Checks

1. **Systematic cross-modal transfer analysis:** Evaluate NeuroVFM's zero-shot performance across all 82 CT diagnoses on MRI data (and vice versa), reporting transfer gaps per diagnosis category to identify modality-specific limitations.

2. **Independent replication with public data:** Reimplement Vol-JEPA architecture and train on a public neuroimaging dataset (e.g., Brain Tumor Segmentation challenge) to verify that the self-supervised approach works outside the proprietary UM-NeuroImages corpus.

3. **Clinical deployment pilot:** Conduct a prospective study where NeuroVFM generates preliminary interpretations for actual clinical cases, measuring inter-observer agreement with radiologists and assessing whether the system reduces diagnostic errors or improves workflow efficiency.