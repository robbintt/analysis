---
ver: rpa2
title: 'The Few-shot Dilemma: Over-prompting Large Language Models'
arxiv_id: '2509.13196'
source_url: https://arxiv.org/abs/2509.13196
tags:
- few-shot
- examples
- llms
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study identifies the \"over-prompting\" phenomenon, where\
  \ incorporating excessive domain-specific examples into prompts can paradoxically\
  \ degrade the performance of certain Large Language Models (LLMs) in few-shot learning.\
  \ The researchers evaluated three few-shot selection methods\u2014random sampling,\
  \ semantic embedding, and TF-IDF vectors\u2014across seven LLMs, including GPT-4o,\
  \ GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral."
---

# The Few-shot Dilemma: Over-prompting Large Language Models

## Quick Facts
- arXiv ID: 2509.13196
- Source URL: https://arxiv.org/abs/2509.13196
- Reference count: 40
- Key outcome: Incorporating excessive domain-specific examples in prompts can degrade LLM performance in few-shot learning, with TF-IDF outperforming other selection methods for filtering relevant examples

## Executive Summary
This study investigates the phenomenon of "over-prompting" in few-shot learning with Large Language Models (LLMs), where adding too many domain-specific examples to prompts can paradoxically reduce model performance. The researchers evaluated three few-shot selection methods—random sampling, semantic embedding, and TF-IDF vectors—across seven popular LLMs using software requirement classification datasets. Their findings reveal that TF-IDF consistently outperformed other methods in selecting the most relevant examples, and by identifying the optimal number of examples for each model, they achieved state-of-the-art performance with a 1% improvement in classifying functional and non-functional requirements.

## Method Summary
The researchers conducted experiments across seven LLMs (GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral) using two real-world software requirement classification datasets. They tested three few-shot selection methods: random sampling, semantic embedding-based selection, and TF-IDF vector-based selection. The study systematically varied the number of examples in prompts to identify the optimal quantity for each model while avoiding the over-prompting phenomenon. Performance was measured by accuracy in classifying functional versus non-functional software requirements, with results compared against existing state-of-the-art approaches.

## Key Results
- TF-IDF vector selection method consistently outperformed random sampling and semantic embedding methods in filtering relevant examples
- Optimal example count varies by model, with over-prompting occurring when exceeding model-specific thresholds
- Achieved 1% improvement over state-of-the-art performance in software requirement classification
- Demonstrated that example quality and quantity must be balanced to avoid performance degradation

## Why This Works (Mechanism)
Unknown: The paper does not explicitly detail the underlying mechanisms driving over-prompting. Assumption: The phenomenon may relate to attention limitations, context window effects, or interference between examples, but these hypotheses remain untested in this study.

## Foundational Learning
- Few-shot learning: Training LLMs with limited examples requires careful prompt engineering; quick check: can the model perform the task with 5-10 examples
- TF-IDF vectorization: Text representation method that captures term importance across documents; quick check: high TF-IDF scores indicate distinctive, relevant terms
- Over-prompting phenomenon: Performance degradation when adding excessive examples to prompts; quick check: monitor accuracy changes as example count increases
- Software requirement classification: Binary categorization of requirements as functional or non-functional; quick check: requires understanding of system behavior versus constraints
- Semantic embedding selection: Uses vector similarity to select relevant examples; quick check: cosine similarity between examples and target text
- Prompt engineering optimization: Systematic variation of prompt components to maximize performance; quick check: A/B testing different prompt structures

## Architecture Onboarding
**Component map:**
User query -> Few-shot example selector -> Prompt construction -> LLM inference -> Classification output

**Critical path:**
Example selection (TF-IDF) -> Optimal example count determination -> Prompt assembly -> Model inference

**Design tradeoffs:**
Quality vs. quantity of examples; computational cost of selection methods (TF-IDF is faster than semantic embedding); model-specific optimization vs. universal approach

**Failure signatures:**
Performance plateau or decline as example count increases; inconsistent classification across similar requirements; increased computational cost without accuracy gains

**First experiments:**
1. Test TF-IDF selection on a held-out validation set with varying example counts
2. Compare random vs. TF-IDF selection at the identified optimal example count
3. Measure inference latency across different selection methods at maximum performance

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions in the provided content. Assumption: Open questions likely include investigating underlying mechanisms of over-prompting, testing generalizability across domains, and exploring whether the phenomenon correlates with model scale or architecture.

## Limitations
- Evaluation limited to software requirement classification datasets, limiting generalizability to other domains
- Focus on seven specific LLMs primarily from OpenAI and Meta, not representing the full diversity of available models
- Does not investigate underlying mechanisms driving over-prompting (attention limitations, context window effects, etc.)
- Performance improvement of 1% over state-of-the-art, while meaningful, represents a modest absolute gain

## Confidence
- High confidence: The existence of over-prompting as a measurable phenomenon affecting LLM performance
- Medium confidence: TF-IDF as the optimal selection method for this specific task domain
- Medium confidence: The relationship between example quantity and performance degradation across models

## Next Checks
1. Replicate the study across diverse task domains (e.g., medical diagnosis, legal document classification) to test generalizability
2. Test additional LLM architectures including open-weight models with varying parameter counts to assess if over-prompting correlates with model scale
3. Conduct ablation studies varying prompt formatting, example ordering, and instruction phrasing while maintaining constant example quality to isolate effects of presentation from content