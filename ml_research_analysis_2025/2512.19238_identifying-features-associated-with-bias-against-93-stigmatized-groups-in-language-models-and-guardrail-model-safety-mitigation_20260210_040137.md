---
ver: rpa2
title: Identifying Features Associated with Bias Against 93 Stigmatized Groups in
  Language Models and Guardrail Model Safety Mitigation
arxiv_id: '2512.19238'
source_url: https://arxiv.org/abs/2512.19238
tags:
- bias
- stigma
- guardrail
- social
- stigmas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias against 93 stigmatized groups in large
  language models using SocialStigmaQA benchmark. It examines how social features
  of stigma (aesthetics, concealability, course, disruptiveness, origin, peril), stigma
  cluster types, and prompt styles affect biased outputs.
---

# Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation

## Quick Facts
- **arXiv ID**: 2512.19238
- **Source URL**: https://arxiv.org/abs/2512.19238
- **Authors**: Anna-Maria Gueorguieva; Aylin Caliskan
- **Reference count**: 18
- **Primary result**: LLM outputs show varying bias against 93 stigmatized groups, with perilous stigmas eliciting the most bias; guardrail models reduce bias by 10.4%, 1.4%, and 7.8% but fail to identify bias intent.

## Executive Summary
This study investigates bias against 93 stigmatized groups in large language models using the SocialStigmaQA benchmark. The researchers examine how social features of stigma (aesthetics, concealability, course, disruptiveness, origin, peril), stigma cluster types, and prompt styles affect biased outputs. They collect human and LLM ratings of stigma features and measure bias across three widely-used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B). Results show that LLM ratings of features only weakly to moderately correlate with human ratings. Stigmas rated as highly perilous (e.g., gang membership, HIV) show the most bias (60% biased outputs), while sociodemographic stigmas show the least (11%). Guardrail models reduce bias by 10.4%, 1.4%, and 7.8% respectively, but fail to identify the intent of bias-eliciting prompts, instead flagging inputs based on keywords. The study demonstrates that features associated with bias remain unchanged post-mitigation and suggests future work to improve guardrail models' ability to discern intent in bias-related prompts.

## Method Summary
The study collected human ratings of 93 stigmas across six features (peril, course, concealability, disruptiveness, origin, aesthetics) and five cluster types. These ratings were compared with LLM ratings from three models. The SocialStigmaQA benchmark containing 10,360 prompts across 37 scenarios was used to elicit biased outputs. Bias was measured by prompting LLMs with these questions and classifying responses as biased or unbiased based on a ground-truth rubric. Guardrail models (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API) were evaluated by measuring their ability to flag bias-eliciting prompts. Post-mitigation bias levels were compared to baseline using McNemar's test.

## Key Results
- LLM ratings of stigma features only weakly to moderately correlate with human ratings, particularly showing negative correlation for concealability
- Stigmas rated as highly perilous (e.g., gang membership, HIV) elicit the most bias (60% of outputs), while sociodemographic stigmas show the least bias (11%)
- Guardrail models reduce bias by 10.4%, 1.4%, and 7.8% respectively, but primarily flag inputs based on keywords rather than recognizing bias intent
- Post-mitigation analysis shows that features associated with bias remain unchanged, suggesting guardrails address symptoms rather than root causes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stigma features—particularly peril, course, and concealability—predict biased LLM outputs.
- Mechanism: Stigmas rated by humans as highly perilous (e.g., gang membership, HIV) or temporary (low course persistence) trigger more discriminatory model responses in social decision scenarios, likely because training data encodes societal associations between threat/contagion and devaluation. The model replicates these associations when generating recommendations about stigmatized individuals.
- Core assumption: Human-rated stigma features from psychology literature (Jones et al. 1984) map onto statistical patterns in LLM training corpora that influence generation.
- Evidence anchors:
  - [abstract] "stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%)."
  - [section 7.2] "human ratings of Peril (F = 41.8, p<.0001)... human ratings of Course (F = 29.5, p<.0001)... and LLM ratings of Concealability (F = 29.3, p<.0001)" are significant predictors of bias percentage.
  - [corpus] Related work on bias amplification in Stable Diffusion (arXiv:2508.17465) shows stigmatized identities systematically associated with darker skin tones, suggesting cross-modal stigma representation patterns—but corpus does not directly confirm the peril→bias mechanism.
- Break condition: If stigma features are orthogonal to training data distributions (e.g., if peril ratings reflect purely psychological constructs without corpus correlates), the mechanism would not hold.

### Mechanism 2
- Claim: Guardrail models reduce bias via keyword-based harm flagging but fail intent recognition for bias-eliciting prompts.
- Mechanism: Guardrails classify inputs as safe/unsafe across predefined categories. When inputs contain stigma-related keywords (e.g., "having sex for money"), they may be flagged under coarse categories (e.g., "sex crimes") rather than bias/discrimination, blocking some biased outputs incidentally without understanding the prompt's discriminatory intent.
- Core assumption: Flagging an input as unsafe prevents the downstream LLM from generating a biased response in typical deployment pipelines.
- Evidence anchors:
  - [abstract] "guardrail models often fail to recognize the intent of bias in prompts."
  - [section 7.3] "For Llama Guard... only 44 prompts out of the entire 10,360 bias-eliciting prompts in SocialStigmaQA were flagged because of the category 'Hate'... For Granite Guardian's 'Social Bias' category, the percentage of the SocialStigmaQA inputs identified as harmful due to social bias is 0.76%."
  - [corpus] No direct corpus evidence on guardrail intent recognition; related papers focus on bias detection, not mitigation mechanisms.
- Break condition: If guardrails were calibrated to recognize subtle bias-eliciting question structures (not just keywords), the incidental keyword-matching mechanism would be superseded by true intent detection.

### Mechanism 3
- Claim: Human-LLM misalignment in rating stigma features (especially concealability) contributes to biased outputs.
- Mechanism: LLMs rate stigmas as more visible/concealable differently than humans (negative correlation for concealability: r ≈ -0.28 to -0.02). This misalignment may cause models to inappropriately weight visibility in decision contexts—for instance, treating a concealable stigma as salient when humans would not, amplifying bias in scenarios where the model "over-perceives" stigma presence.
- Core assumption: LLM feature ratings reflect internal representations that influence downstream generation behavior.
- Evidence anchors:
  - [section 7.1] "All models have negative correlation of human to model concealability ratings" and Figure 2 shows this divergence.
  - [section 8.1] "models are not able to reason about physical manifestations of stigmas, but may replicate social biases based on the revulsion of a stigma."
  - [corpus] No corpus papers directly test the misalignment→bias mechanism; this is an original contribution.
- Break condition: If LLM feature ratings are epiphenomenal (generated post-hoc without causal connection to generation), the misalignment would not explain biased outputs.

## Foundational Learning

- Concept: **Six stigma features from psychology (aesthetics, concealability, course, disruptiveness, origin, peril)** (Jones et al. 1984)
  - Why needed here: The entire methodology depends on using these features as predictors. Without understanding what "peril" (threat/contagion potential) or "course" (persistence over time) mean, you cannot interpret the regression results or design new stigma-bias experiments.
  - Quick check question: If a stigma is rated high on "origin" (perceived as deliberate/controllable), would you expect more or less bias compared to a stigma rated low on origin? (Assumption: controllability typically increases blame and bias.)

- Concept: **Binomial/Linear Mixed Models with fixed and random effects**
  - Why needed here: The paper uses BLMMs and LMMs to isolate the effect of stigma features while controlling for model-to-model variation. Understanding fixed vs. random effects is necessary to replicate or extend the analysis.
  - Quick check question: In their LMM, why is "model type" a random effect rather than a fixed effect? (Answer: Because the three models are treated as a sample from a population of possible LLMs, not as the only models of interest.)

- Concept: **Guardrail model categories and intent detection**
  - Why needed here: Evaluating guardrails requires understanding that "Hate" or "Social Bias" categories are distinct from "Violence" or "Sexual Content." The paper's key finding is that guardrails misclassify bias-eliciting prompts into wrong categories.
  - Quick check question: If a prompt asks "Should I hire someone who is a gang member?" and the guardrail flags it as "Violent Crimes" rather than "Social Bias," what deployment problem does this create? (Answer: It may block the query for the wrong reason and fail to provide feedback that would improve bias-specific training.)

## Architecture Onboarding

- Component map:
  - **Stigma feature dataset** (Pachankis et al. 2018b): Human ratings of 93 stigmas × 6 features, plus 5 cluster assignments (Awkward, Threatening, Sociodemographic, Innocuous Persistent, Unappealing Persistent).
  - **SocialStigmaQA benchmark**: 10,360 prompts across 37 scenarios × 93 stigmas × 4 prompt styles (base, original, positive, doubt). Each prompt has a ground-truth "biased answer" label.
  - **LLM layer**: Granite 3.0-8B, Llama-3.1-8B, Mistral-7B (Instruct variants). Outputs are post-processed to extract yes/no and classify as biased/unbiased.
  - **Guardrail layer**: Granite Guardian 3.0-2B, Llama Guard 3-8B, Mistral Moderation API. Each has distinct category taxonomies; only Granite has an explicit "Social Bias" category.

- Critical path:
  1. Feature rating collection → 2. Bias benchmarking (LLM outputs on SocialStigmaQA) → 3. Correlation/LMM analysis → 4. Guardrail evaluation (input flagging rates, category distribution) → 5. Post-mitigation bias measurement (McNemar's test for significance).

- Design tradeoffs:
  - **Binary output requirement**: Prompting for yes/no sacrifices nuance but enables scalable benchmarking. Open-ended responses would be harder to classify but might reveal subtler bias expressions.
  - **Improper output handling**: Classifying non-yes/no responses as "unbiased" may undercount bias (if evasion is a bias strategy) or overcount unbiased outputs (if the model genuinely refuses to decide).
  - **Category selection for Granite**: Must make separate API calls for "Harm" (default) vs. "Social Bias"; this affects both computational cost and detection sensitivity.

- Failure signatures:
  - Low correlation between human and LLM ratings (especially negative for concealability) suggests models do not share human intuitions about stigma visibility.
  - Guardrails flagging <1% of bias-eliciting prompts under "Hate" or "Social Bias" categories indicates keyword-matching failure mode.
  - Post-mitigation feature effects remaining significant suggests guardrails do not address root causes, only symptoms.

- First 3 experiments:
  1. **Replicate on a different LLM family** (e.g., GPT-4, Claude) with the same SocialStigmaQA benchmark to test whether peril/course/concealability effects generalize beyond the three open-source models studied.
  2. **Ablate prompt styles**: Run the "base" prompts (no stigma mentioned) alongside "original" prompts across all 93 stigmas to quantify the incremental bias attributable to stigma presence alone vs. scenario-specific factors.
  3. **Guardrail category calibration**: Create a validation set where each prompt is manually labeled for intended bias type (e.g., hiring discrimination, service denial) and measure guardrail precision/recall per category to diagnose whether the failure is false negatives (missed bias) or category confusion (wrong flag).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can guardrail models be improved to discern the intent of bias-eliciting prompts without over-relying on keyword matching?
- Basis in paper: [explicit] Section 8.2 states, "Future work can isolate these effects by conducting experimental studies on changing intent behind statements while including the same keyword."
- Why unresolved: Current guardrails often flag inputs based on keywords (e.g., "having sex for money") rather than context, failing to distinguish between harmful prompts and earnest information seeking about stigmatized identities.
- What evidence would resolve it: Experimental results showing guardrails successfully distinguishing between different intents in prompts containing identical stigmatized keywords.

### Open Question 2
- Question: Do the six social features of stigma (aesthetics, concealability, etc.) correlate with LLM bias similarly in non-US cultural contexts?
- Basis in paper: [explicit] Section 8.3 notes the study focuses on "US-centric stigmas" and recommends future work "apply similar frameworks of analyzing bias via stigma features for other culturally specific stigmas."
- Why unresolved: Stigmas and their associated social features differ by culture; it is unknown if the high correlation between "peril" and bias holds globally.
- What evidence would resolve it: Replication of the SocialStigmaQA benchmark methodology using culturally specific stigmas and LLMs trained on non-Western data.

### Open Question 3
- Question: Does the misalignment between human and LLM ratings of stigma features directly cause biased outputs in decision-making scenarios?
- Basis in paper: [explicit] Section 8.1 states, "Future work is necessary to determine how (mis)alignment between humans and LLMs may lead to bias, especially in decision making scenarios."
- Why unresolved: While the study identifies weak correlations in ratings (e.g., concealability) and the presence of bias, it does not establish a causal link between this specific misalignment and the biased decision output.
- What evidence would resolve it: A causal analysis demonstrating that divergent ratings of specific features (like visibility) predict biased "yes/no" decisions in social scenarios.

## Limitations

- The study relies on binary yes/no outputs, which may undercount nuanced or evasive biased responses that don't directly answer the question
- Guardrail evaluation measures input flagging rates rather than actual output bias, making it unclear whether blocked prompts would have generated biased content
- The negative correlation between human and LLM ratings for concealability suggests models may not reliably replicate human stigma perceptions
- Post-mitigation analysis shows no significant change in feature effects despite claimed 10%+ bias reduction, suggesting potential statistical power issues

## Confidence

**High confidence**: The finding that perilous stigmas (gang membership, HIV) elicit the most bias (60%) while sociodemographic stigmas show least bias (11%) is robust across all three LLMs and supported by significant LMM coefficients. The observation that guardrails fail to identify bias-eliciting intent, instead flagging keywords under wrong categories, is clearly demonstrated by category distribution analysis.

**Medium confidence**: The predictive power of stigma features (peril, course, concealability) on bias output rates is statistically significant but the effect sizes are modest. The negative correlation between human and LLM concealability ratings is consistent but the mechanism linking this misalignment to biased outputs remains theoretical.

**Low confidence**: The claim that guardrails reduce bias by 10.4%, 1.4%, and 7.8% respectively is based on input flagging rates rather than actual output bias measurement. Without analyzing whether flagged prompts would have generated biased responses, these percentages may overstate or understate true mitigation effectiveness.

## Next Checks

1. **Output-based guardrail evaluation**: Instead of measuring input flagging rates, run the full pipeline (guardrail → LLM → bias classification) and compare bias percentages in actual outputs pre- and post-mitigation to validate the claimed 10%+ reduction rates.

2. **Cross-modal stigma feature alignment**: Test whether the misalignment between human and LLM ratings for concealability persists when using image-based stigma representations (e.g., Stable Diffusion outputs) to determine if this is a text-specific phenomenon.

3. **Intent recognition benchmark**: Create a validation set of bias-eliciting prompts manually labeled by bias type (hiring, housing, service denial) and measure guardrail precision/recall per category to quantify whether failures are false negatives or category confusion.