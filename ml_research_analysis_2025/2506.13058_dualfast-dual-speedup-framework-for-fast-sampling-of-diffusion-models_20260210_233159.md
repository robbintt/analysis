---
ver: rpa2
title: 'DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models'
arxiv_id: '2506.13058'
source_url: https://arxiv.org/abs/2506.13058
tags:
- sampling
- error
- dpm-solver
- diffusion
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DualFast, a training-free acceleration framework
  for diffusion models that addresses both discretization and approximation errors
  in the sampling process. The key insight is that sampling errors consist of two
  distinct components: discretization error (from approximating continuous integration)
  and approximation error (from the neural network''s imperfect score function estimation).'
---

# DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models

## Quick Facts
- **arXiv ID:** 2506.13058
- **Source URL:** https://arxiv.org/abs/2506.13058
- **Reference count:** 34
- **Key outcome:** DualFast is a training-free acceleration framework that reduces both discretization and approximation errors in diffusion sampling, achieving substantial FID improvements (e.g., -9.95 FID on ImageNet with 5 steps) by modifying noise predictions using weighted combinations from larger timesteps.

## Executive Summary
This paper introduces DualFast, a novel training-free acceleration framework for diffusion models that addresses both discretization and approximation errors in the sampling process. The key insight is that sampling errors consist of two distinct components: discretization error (from approximating continuous integration) and approximation error (from the neural network's imperfect score function estimation). Existing fast samplers primarily focus on reducing discretization error through high-order solvers, while neglecting the significant contribution of approximation error. DualFast introduces a dual-error disentanglement strategy to analyze and reduce both error types simultaneously. The framework is compatible with existing ODE-based samplers and incorporates a novel approximation error reduction strategy that leverages the observation that approximation error monotonically decreases with larger step sizes.

## Method Summary
DualFast is a training-free acceleration framework that modifies existing ODE-based diffusion samplers by reducing approximation error through a novel prediction substitution strategy. The method caches the network's noise prediction at the initial timestep T and uses it to modify predictions at subsequent steps. For each timestep t, DualFast computes a new noise prediction using the formula: ϵ_new(x_t, t) = (1 + c)ϵ(x_t, t) - cϵ(x_τ, τ), where c is a mixing coefficient that linearly decreases from 0.5 to 0.0 as the timestep decreases, and τ is typically set to the initial timestep T. This approach leverages the observation that approximation error decreases monotonically with larger step sizes, allowing the method to effectively reduce total sampling error while maintaining compatibility with any ODE-based sampler.

## Key Results
- **FID Improvements:** DualFast achieves substantial FID improvements across various models and datasets, with -9.95 FID on ImageNet with 5 steps and -10.38 FID with 6 steps
- **Cross-Modal Performance:** The method shows consistent gains across unconditional, class-conditional, and text-conditional generation tasks, with 64.8% HPD v2 human preference score for Stable Diffusion-XL
- **Solver Compatibility:** DualFast consistently outperforms baseline samplers when integrated with DDIM, DPM-Solver, and DPM-Solver++, demonstrating universal applicability across ODE-based methods

## Why This Works (Mechanism)

### Mechanism 1: Dual-Error Disentanglement
The paper decomposes total sampling error into discretization error (from approximating continuous integration) and approximation error (from imperfect neural network score estimation). By constructing three transition processes—exact (using true kernel), approximation-only (using infinitesimally small steps), and both errors (using large steps)—the magnitudes and trends of the two error sources can be isolated and analyzed using MSE between resulting distributions.

### Mechanism 2: Monotonic Approximation Error Reduction
The approximation error decreases monotonically as the diffusion timestep t increases (i.e., at higher noise levels/larger step sizes). This occurs because at higher noise levels, the noise pattern in x_t is more recognizable, making the network's denoising task easier and its prediction more accurate.

### Mechanism 3: Prediction Substitution via Weighted Combination
DualFast substitutes the network's noise prediction at the current small step t with a weighted combination of its prediction at t and a prediction from a larger timestep τ. The new prediction formula (1 + c)ϵ_θ(x_t, t) - cϵ_θ(x_τ, τ) effectively reduces approximation error by incorporating information from larger timesteps where error is lower.

## Foundational Learning

### Probability Flow ODEs and Exponential Integrators
**Why needed here:** DualFast builds upon existing ODE-based samplers which solve the diffusion reverse process using exponential integrators. Understanding how the continuous solution is approximated by discrete steps is essential to grasp where discretization error comes from.
**Quick check question:** Can you explain how a first-order solver like DDIM approximates the integral term in the diffusion ODE?

### Score Function and Neural Network Estimation
**Why needed here:** The core insight of DualFast is that the neural network ϵ_θ provides an imperfect estimate of the true score function (-σ_t∇log q_t(x_t)). Understanding this source of "approximation error" is central to the paper's contribution.
**Quick check question:** What is the score function in a diffusion model, and what is the network trained to predict?

### Noise vs. Data Prediction Modes
**Why needed here:** DualFast is applied to different solvers (DPM-Solver, DPM-Solver++) which may operate in noise-prediction (ϵ-prediction) or data-prediction (x-prediction) modes. The integration of the approximation error reduction strategy differs slightly between them.
**Quick check question:** In a diffusion model, how can you convert a predicted noise ϵ_θ(x_t, t) into a predicted clean sample x_θ(x_t, t)?

## Architecture Onboarding

### Component map
Base Solver -> DualFast Modifier -> Mixing Coefficient Scheduler -> Timestep Selector

### Critical path
1. Initialize the sampler. Run the first step (t=T) and cache the network's output ϵ_θ(x_T, T).
2. For each subsequent step t from T-1 down to 0:
   a. Calculate the base prediction D_t-1 using the chosen solver (e.g., DPM-Solver).
   b. Retrieve the cached initial noise prediction ϵ_θ(x_T, T).
   c. Compute the mixing coefficient c based on the current timestep t.
   d. Compute the modified prediction D'_t-1 = (1 + c)D_t-1 - c * ϵ_θ(x_T, T).
   e. Use D'_t-1 to update the sample x_t-1.

### Design tradeoffs
- **Choice of τ:** The paper defaults to τ=T for simplicity, but other larger timesteps could be used. Ablation shows performance is robust to this choice.
- **Choice of c schedule:** The paper uses a linearly decreasing schedule from 0.5 to 0.0, based on the assumption that approximation error linearly increases as step decreases.
- **Compatibility:** The method is training-free and compatible with any ODE solver that can be expressed in the general form of Eq. 6.

### Failure signatures
- **No improvement over base sampler:** This may occur if the base solver's discretization error is already very low (e.g., with a large number of steps), making the approximation error reduction less impactful.
- **Artifacts or instability:** If the mixing coefficient c is set too high, the modification could overpower the base solver's prediction, potentially introducing new errors or artifacts.
- **Inapplicability:** The method assumes an ODE-based formulation and may not apply directly to SDE-based samplers without modification.

### First 3 experiments
1. **Baseline Comparison (Few-Step Sampling):** Implement DualFast on top of DDIM and DPM-Solver++ and measure FID on a standard dataset like ImageNet or CIFAR-10 with NFEs in the range of 5-10. Compare against the base solvers to verify the reported performance gains.
2. **Ablation on Mixing Coefficient:** Test different schedules for c (e.g., constant values, different linear slopes) and different choices of τ (e.g., T, T-50, T-100) to understand the sensitivity of the method to these hyperparameters.
3. **Error Analysis via Disentanglement:** Replicate the dual-error disentanglement experiment by measuring the MSE between distributions generated with different step sizes to empirically confirm the claimed trends of discretization vs. approximation error.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical foundation for cleanly separating discretization and approximation errors using MSE between distributions is not rigorously established
- The claim that approximation error monotonically decreases with larger timesteps is primarily empirical and may not generalize to all model architectures
- The method assumes an ODE-based formulation and may not apply directly to SDE-based samplers without modification

## Confidence

### Confidence Labels
- **High confidence**: The empirical improvements in FID scores and qualitative visual enhancements are well-documented and reproducible
- **Medium confidence**: The theoretical justification for error disentanglement is plausible but not rigorously proven
- **Low confidence**: The claim that this framework works "across any ODE-based sampler" needs validation on a broader range of solvers

## Next Checks

1. **Error Decomposition Validation:** Replicate the dual-error disentanglement experiment by measuring MSE between distributions generated with different step sizes to empirically confirm the claimed trends of discretization vs. approximation error.

2. **Solver Compatibility Test:** Apply DualFast to a fourth ODE-based sampler (e.g., UniPC or DDIM++) not covered in the original experiments to verify the claimed universal compatibility with ODE solvers.

3. **Robustness to τ Selection:** Systematically vary the substitution timestep τ (not just T) across multiple models and datasets to determine if the default choice is truly optimal or if adaptive selection could yield further improvements.