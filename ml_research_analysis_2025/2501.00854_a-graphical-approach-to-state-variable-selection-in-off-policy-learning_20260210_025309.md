---
ver: rpa2
title: A Graphical Approach to State Variable Selection in Off-policy Learning
arxiv_id: '2501.00854'
source_url: https://arxiv.org/abs/2501.00854
tags:
- assumption
- state
- policy
- walk
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unifying graphical framework for causal identification
  in sequential decision-making problems, encompassing both Dynamic Treatment Regimes
  (DTRs) and Markov Decision Processes (MDPs). The key contribution is a set of graphical
  identification criteria based on acyclic directed mixed graphs (ADMGs) that extend
  the backdoor criterion to dynamic settings and relax the memorylessness assumption
  common in MDPs.
---

# A Graphical Approach to State Variable Selection in Off-policy Learning

## Quick Facts
- arXiv ID: 2501.00854
- Source URL: https://arxiv.org/abs/2501.00854
- Reference count: 6
- One-line primary result: Graphical identification criteria based on ADMGs extend the backdoor criterion to sequential settings and unify DTRs with MDPs.

## Executive Summary
This paper introduces a unifying graphical framework for causal identification in sequential decision-making, bridging Dynamic Treatment Regimes (DTRs) and Markov Decision Processes (MDPs). The core contribution is a set of graphical identification criteria based on acyclic directed mixed graphs (ADMGs) that relax memorylessness assumptions and extend the backdoor criterion to dynamic settings. Under three key assumptions—nested states, memorylessness, and dynamic back-door—the joint distribution of rewards, states, and actions under an adaptive policy can be identified from observational data.

## Method Summary
The paper establishes a theoretical framework for identifying causal effects in sequential decision-making using ADMGs. The method involves specifying a causal graph, selecting state variables that satisfy three assumptions (nested states, memorylessness, and dynamic back-door), and applying g-computation to identify the policy value. The framework is demonstrated through a synthetic container logistics simulation where different state selections lead to varying policy performance.

## Key Results
- Theorem 1 establishes identification conditions under three assumptions (nested states, memorylessness, dynamic back-door) for observational data
- Simulation study shows learned policies can perform worse than null policy when state selection violates identification assumptions
- Framework unifies DTR and RL literatures by clarifying implicit causal assumptions in each field
- Partially observed MDPs are generally not identifiable due to memorylessness violations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The joint distribution of rewards, states, and actions under a target policy can be identified from observational data if the state variables satisfy specific graphical criteria.
- **Mechanism:** The paper utilizes Acyclic Directed Mixed Graphs (ADMGs) to encode causal relationships. By ensuring the state set $S_t$ satisfies "Nestedness," "Memorylessness," and the "Dynamic Back-door" criteria, the framework extends Pearl's static back-door adjustment to sequential settings. This effectively blocks confounding paths and irrelevant history, allowing the g-computation formula (product of conditional densities) to validly estimate the policy value.
- **Core assumption:** The causal graph (ADMG) accurately reflects the data generating process, and the "Dynamic Consistency" and "Positivity" assumptions hold.
- **Evidence anchors:**
  - [abstract] Mentions providing "graphical identification criteria in general decision processes."
  - [section 1.1] Explicitly states Theorem 1: "Under Assumptions 1, 2, and 3, we have [identification formula]."
  - [corpus] Weak direct support; neighbor papers focus on variable selection in optimization or bandits, not this specific ADMG unification.
- **Break condition:** If the ADMG is misspecified (e.g., missing a latent confounder) or Positivity fails (certain actions are never taken in historical data), the mechanism breaks and identification is impossible.

### Mechanism 2
- **Claim:** The "Memorylessness" assumption (Assumption 2) allows the decision process to discard history without losing identifiability, provided the chosen state $S_t$ captures all relevant past information.
- **Mechanism:** This mechanism relies on m-separation in the ADMG. It requires that future innovations $N_{t+1}$ are independent of the discarded history $(S_{t-1} \cup A_{t-1}) \setminus S_t$ given the current state $S_t$. This generalizes the Markov property of MDPs to settings with latent confounders (bidirected edges), enabling valid policy evaluation without tracking the full history required by Dynamic Treatment Regimes (DTRs).
- **Core assumption:** The state $S_t$ is chosen such that it graphically separates the past from the future.
- **Evidence anchors:**
  - [section 1.1] Defines Assumption 2: "novel variables at time $t + 1$ must be m-separated from previous state and action variables given the current state."
  - [section 1.2] Figure 2b shows a violation where lagged price effects ($A_{t-1} \to B_{t+1}$) break this separation if not handled.
  - [corpus] "Trajectory-Aware Eligibility Traces" discusses off-policy bias, relevant to the consequences of breaking memory assumptions.
- **Break condition:** If a past variable excluded from $S_t$ directly causes a future outcome (e.g., lagged pricing effects on demand), Assumption 2 is violated, leading to biased value estimates.

### Mechanism 3
- **Claim:** Unmeasured confounding can be controlled in a sequential setting if the "Dynamic Back-door" criterion (Assumption 3) is met by the state variables.
- **Mechanism:** This criterion ensures that all "back-door" paths (confounding paths entering an action node $A_k$) are blocked by the state $S_k$. Unlike static confounding, this must hold dynamically across time steps. If satisfied, it implies "Dynamic Unconfoundedness" (Assumption 3*), allowing the causal effect of the policy to be isolated.
- **Core assumption:** Confounders are either observed and included in the state $S_t$ or blocked by other variables in $S_t$.
- **Evidence anchors:**
  - [section 1.1] Defines Assumption 3 as extending "the back-door criterion... to the dynamic setting."
  - [section 1.2] Figure 2a demonstrates an unidentifiable scenario where immediate confounding ($A_t \leftrightarrow B_{t+1}$) cannot be blocked.
  - [corpus] "The Minimal Search Space for Conditional Causal Bandits" relates to search spaces for causal variables but doesn't confirm this specific dynamic criterion.
- **Break condition:** If a latent variable causes both the action and the immediate outcome (simultaneous confounding), no state selection can satisfy Assumption 3, and the policy effect is not identifiable.

## Foundational Learning

- **Concept:** **m-separation in ADMGs**
  - **Why needed here:** This is the logical engine of the paper. Standard d-separation (used in DAGs) fails when latent confounders are present. You must understand m-separation to verify if a proposed state $S_t$ actually blocks confounding paths.
  - **Quick check question:** Given a path with a collider, does conditioning on the collider open or close the path?

- **Concept:** **g-formula (Robins' g-computation)**
  - **Why needed here:** Theorem 1's identification result is essentially a recursive application of the g-formula. Understanding this helps see how the paper converts the graphical conditions into a statistical estimand (a product of conditional probabilities).
  - **Quick check question:** If you intervene to set a policy $g$, how does the joint probability factorize differently than the observational distribution?

- **Concept:** **MDP vs. DTR (Dynamic Treatment Regime)**
  - **Why needed here:** The paper positions itself as a bridge between these two fields. MDPs assume "memorylessness" and often "randomized decisions," while DTRs assume "sequential ignorability" but require full history. Understanding the gaps in both helps contextualize why this unified framework is necessary.
  - **Quick check question:** Why is "full history" often impractical in Reinforcement Learning (MDP) settings?

## Architecture Onboarding

- **Component map:** Data $\to$ Causal Graph (ADMG) $\to$ State Selection ($S_t$) $\to$ Graphical Validator (Assumptions 1-3) $\to$ Policy Learner (e.g., Policy Iteration)
- **Critical path:** The manual or automated construction of the ADMG (Section 2.1) and the subsequent selection of $S_t$ (Section 3). If the graph is wrong or the state choice fails the validator, the downstream policy learner will optimize a biased objective.
- **Design tradeoffs:** Increasing the size of the state $S_t$ makes it easier to satisfy Assumptions 2 and 3 (blocking more paths) but increases the variance of the policy learner (curse of dimensionality). The "Nestedness" assumption restricts the state to grow monotonically with history.
- **Failure signatures:**
  - **Negative Regret:** As seen in Section 6 (Figure 3/Table 1), the learned policy performs worse than the null policy (negative regret % in Table 1 for $\vec{G}_2$).
  - **Colliders as Confounders:** Accidentally conditioning on a collider in the state $S_t$ opens a biasing path (Section 5.3 discusses "phantom confounding" in latent projections).
- **First 3 experiments:**
  1.  **Graph Validation:** Replicate the "Simple dynamic pricing" simulation (Section 1.2, Figure 1). Verify that a state satisfying all 3 assumptions recovers the optimal policy.
  2.  **Assumption Stress Test:** Introduce the "Retrospective price updates" scenario (Figure 2b/Section 6.2). Run policy iteration with the "naive" state (violating Assumption 2) vs. the "correct" state to observe the performance drop (negative regret).
  3.  **Confounding Audit:** Attempt to identify a policy in the "Competitor prices" scenario (Figure 2a). Demonstrate that no state selection can satisfy Assumption 3 due to the unobserved bidirected edge, confirming the system limits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can automated algorithms be developed to select state variables that satisfy the nested states, memorylessness, and dynamic back-door criteria?
- **Basis in paper:** [explicit] The authors state in the Discussion that a "possible avenue of future work is to develop algorithms for state variable selection in off-policy learning" analogous to confounder selection.
- **Why unresolved:** The paper provides the theoretical criteria (Theorem 1) to validate a given state selection but does not propose a computational method to search the variable space to find such a set.
- **What evidence would resolve it:** An algorithm capable of traversing observed variables to propose a valid $S_t$ satisfying Assumptions 1-3, validated on the container logistics simulation or similar benchmarks.

### Open Question 2
- **Question:** How can online reinforcement learning algorithms be designed to simultaneously select state variables and optimize policies?
- **Basis in paper:** [explicit] The authors suggest it is "particularly interesting to develop online RL algorithms... that can select state variables and improve the policy at the same time."
- **Why unresolved:** Standard online RL assumes a fixed state representation; integrating structural causal discovery (state selection) with policy optimization in an online feedback loop presents theoretical stability challenges.
- **What evidence would resolve it:** A convergence proof for an online algorithm that dynamically adjusts state variables during training, along with empirical results showing it outperforms fixed-state baselines.

### Open Question 3
- **Question:** Under what specific structural conditions can partially observed MDPs (POMDPs) be identified without requiring the full history as the state?
- **Basis in paper:** [inferred] The paper notes POMDPs are "generally not identified" due to memorylessness violations (Assumption 2), mentioning approximate inference only as a possibility for "sufficiently mixed" processes.
- **Why unresolved:** Requiring the full history negates the dimension reduction benefits of RL, yet the paper does not define the precise graphical or parametric relaxations needed for valid POMDP identification.
- **What evidence would resolve it:** A modified graphical criterion or theoretical bounds establishing identifiability for POMDPs with specific latent structures, rather than a blanket non-identification result.

## Limitations
- The validity of the entire framework hinges on correctly specifying the causal graph, which is often done from domain knowledge and cannot be empirically validated
- The container logistics simulation is highly stylized, and the robustness of the graphical criteria to noisy, high-dimensional data remains untested
- The paper assumes standard off-policy methods will work once identification is established but does not explore their variance or sensitivity to positivity violations

## Confidence
- **High Confidence:** The theoretical framework for graphical identification (Theorems 1-4, Assumptions 1-3) is logically sound and builds on established causal inference principles
- **Medium Confidence:** The simulation study demonstrates the core concepts (Assumptions 2 and 3 being violated leads to worse policies), though examples are simplified and performance gains are modest
- **Low Confidence:** The claim about POMDPs being generally unidentifiable is stated but not empirically demonstrated, and the assertion that "any policy that can be learned with an MDP is identifiable" is plausible but not proven for complex, non-linear policies

## Next Checks
1. **Empirical Stress Test:** Implement the framework on a more complex, noisy simulation (e.g., healthcare or recommendation systems) with known ground truth. Systematically vary the degree of confounding and measure the bias in the learned policy.
2. **ADMG Robustness Audit:** Given an observed dataset, use constraint-based causal discovery algorithms (e.g., FCI or RFCI) to estimate the ADMG. Compare the identification results under the true vs. estimated graphs to quantify the impact of graph misspecification.
3. **POMDP Counterexample:** Construct a simple POMDP (e.g., a partially observed contextual bandit) where the optimal policy requires conditioning on history in a way that violates the memorylessness assumption. Demonstrate that the proposed graphical criteria fail to identify this policy, validating the paper's claim.