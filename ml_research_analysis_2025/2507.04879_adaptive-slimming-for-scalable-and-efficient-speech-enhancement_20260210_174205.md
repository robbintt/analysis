---
ver: rpa2
title: Adaptive Slimming for Scalable and Efficient Speech Enhancement
arxiv_id: '2507.04879'
source_url: https://arxiv.org/abs/2507.04879
tags:
- speech
- input
- utilization
- enhancement
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic slimming approach for speech enhancement
  by extending the DEMUCS architecture with slimmable blocks and a lightweight routing
  network. The method dynamically adjusts model capacity based on input difficulty,
  achieving better performance-efficiency trade-offs than static baselines.
---

# Adaptive Slimming for Scalable and Efficient Speech Enhancement

## Quick Facts
- arXiv ID: 2507.04879
- Source URL: https://arxiv.org/abs/2507.04879
- Reference count: 40
- One-line result: Dynamic slimming with a routing network achieves better performance-efficiency trade-offs than static models by adapting capacity to input difficulty.

## Executive Summary
This paper introduces adaptive slimming for speech enhancement, extending the DEMUCS architecture with slimmable convolutional blocks and a lightweight routing network. The method dynamically adjusts model capacity based on input difficulty, achieving better performance-efficiency trade-offs than static baselines. When trained to use 10% of capacity on average, the dynamic model matches or exceeds the speech quality of a static model using 25% capacity while reducing MACs by 29%. The approach demonstrates Pareto optimality across multiple speech quality metrics and adapts resource allocation to input complexity, providing computational savings on easier inputs while maintaining quality on challenging ones.

## Method Summary
The method extends DEMUCS with slimmable convolutional blocks that can operate at different utilization factors (UF), each corresponding to a different performance/efficiency trade-off. A lightweight routing network determines the optimal UF for each input using a differentiable Gumbel-softmax trick. The system is trained in two stages: first pre-training the backbone at all UFs, then jointly training with the router using efficiency and balance regularization losses. The approach enables a single model to approximate multiple fixed-capacity models without extra storage costs while adapting to input difficulty.

## Key Results
- Dynamic model trained for 10% average utilization matches or exceeds quality of static model using 25% capacity (PESQ: 2.66 vs 2.65)
- 29% reduction in MACs compared to static UF=0.25 baseline
- Router overhead is negligible at 0.06% additional parameters
- Pareto-optimal trade-offs demonstrated across multiple speech quality metrics (PESQ, SI-SDR, STOI, DNSMOS-P808)

## Why This Works (Mechanism)

### Mechanism 1: Slimmable Convolutional Blocks
Slimmable convolutional blocks enable a single model to approximate multiple fixed-capacity models by selectively activating channel subsets. Each convolutional layer maps a utilization factor (UF) to a specific portion of filters. Given weight tensor W ∈ R^{C_out × C_in × K}, slimming selects the first ⌊C_out · υ_j⌋ output channels and/or ⌊C_in · υ_j⌋ input channels. Encoder blocks slim internal feature space while preserving input/output dimensions, avoiding GLU gating inconsistencies. Core assumption: Input difficulty correlates with required model capacity; easier inputs (high SNR, stationary noise) need fewer active channels.

### Mechanism 2: Lightweight Routing Network
A lightweight routing network enables input-adaptive capacity selection with negligible overhead. Router R comprises 1D convolution, ReLU, diagonal GRU, and pointwise convolution, computing score matrix r ∈ R^{J×T*} where J is the number of UFs. Gumbel-softmax trick enables differentiable discrete selection: forward pass uses arg max(r + G), backward pass uses softmax(r + G) where G is Gumbel noise. This ensures all UFs are explored during training while maintaining discrete decisions at inference. Core assumption: The router can learn meaningful mappings from acoustic features to optimal capacity without explicit difficulty labels.

### Mechanism 3: Two-Stage Training with Regularization
Two-stage training with regularization losses enforces utilization targets while maintaining speech quality. Stage 1 pre-trains backbone by minimizing sum of denoising losses across all UFs: L_slim = Σ_j L_SE(s, ŝ_j). Stage 2 adds router with efficiency loss L_Eff = (Σ_j Υ̃_j · υ_j - υ_trgt)² and balancing loss L_Bal = (1/(J-1)) · (Σ_j Υ̃²_j - 1) where Υ̃_j is relative UF occurrence. This prevents collapse to single UF while enforcing average utilization targets. Core assumption: Pre-training at all UFs before routing enables stable joint optimization.

## Foundational Learning

- **Width-wise dynamism (slimmable networks)**: Understanding how channel subsets can be switched at inference time without storing multiple models. Quick check: Can you explain why slimming along output channels only (vs. both input and output) preserves block interface compatibility?

- **Gumbel-softmax reparameterization**: Grasping how discrete decisions become differentiable for end-to-end training. Quick check: What happens to gradient flow if you use arg max directly without the softmax relaxation in the backward pass?

- **Encoder-decoder architectures with skip connections (U-Net)**: Understanding DEMUCS backbone structure where slimming is applied. Quick check: How do skip connections affect where you can safely apply channel slimming without breaking feature alignment?

## Architecture Onboarding

- **Component map**: Input x [1, T] → Encoder E_1 through E_L → Bottleneck (M=4 GRUs) → Decoder D_L through D_1 → Output ŝ. Router: Conv1D→ReLU→DiagonalGRU→PWConv→Gumbel-softmax.

- **Critical path**: Input passes through slimmable encoder blocks (each receiving UF from router), bottleneck with grouped GRUs, slimmable decoder blocks with skip connections, while router computes UF selection in parallel. Router output is upsampled to match input resolution for block selection.

- **Design tradeoffs**: Higher β (efficiency weight) → better target adherence but potential quality drop if υ_trgt too aggressive. Higher γ (balance weight) → more uniform UF usage but may waste capacity on easy inputs. Router pre-training + fine-tuning → better utilization targeting but worse PESQ than direct joint training.

- **Failure signatures**: UF collapse (model always selects υ_J=1.0) → check L_Bal coefficient, increase γ. Underutilization (actual utilization far below υ_trgt) → check L_Eff coefficient, increase β. Quality degradation at low UF (PESQ drops significantly for υ < 0.25) → verify pre-training converged for all UFs independently.

- **First 3 experiments**: 1) Static baseline validation: run backbone at each UF ∈ U without routing to verify L_slim training quality. 2) Router ablation: train with β=0 (no efficiency constraint) to verify router learns meaningful difficulty correlations. 3) Target sweep: train dynamic models with υ_trgt ∈ {0.1, 0.25, 0.5, 0.75, 0.9}. Measure actual utilization, MACs reduction, and PESQ.

## Open Questions the Paper Calls Out

- **Extending to recurrent bottleneck**: The authors plan to extend this approach to encompass the recurrent bottleneck. This remains unresolved as the current implementation applies slimming only to convolutional encoder and decoder blocks, leaving the GRU-based bottleneck static. A study demonstrating stable training and performance efficiency when applying utilization factors to the recurrent units' hidden states and weight matrices would resolve this.

- **Leveraging internal representations for routing**: The authors plan to leverage the model's internal representations for routing. This is unresolved because the current routing subnet operates on the input mixture; it is unknown if deeper, encoded features would provide better signals for estimating input difficulty. Comparative experiments where the router takes intermediate encoder features as input would resolve this.

- **Transferability to other architectures**: The authors aim to verify its applicability on alternative SE architectures. This remains unproven as the method is currently validated only on the U-Net-like DEMUCS structure; generalizability to attention-based or hybrid architectures is unknown. Successful integration into alternative models (e.g., Conv-TasNet or Transformers) would resolve this.

## Limitations

- Router overhead efficiency claims lack independent validation beyond the stated 0.06% parameter overhead, with no ablation studies on router complexity trade-offs
- Two-stage training benefits versus joint training are asserted but not empirically validated through direct comparison experiments
- Input difficulty correlation assumptions remain largely theoretical, with limited corpus evidence linking acoustic features to optimal capacity needs
- GLU-aware slimming implementation details in decoder blocks are underspecified, potentially affecting reproducibility

## Confidence

- **High**: Core mechanism of slimmable blocks with Gumbel-softmax routing, basic architecture specifications (L=5, K=8, U=4, H=32), and training pipeline (two-stage, loss formulations)
- **Medium**: Pareto optimality claims and relative performance comparisons, particularly regarding MACs reduction and speech quality metrics
- **Low**: Router overhead efficiency claims, optimal hyperparameter settings (β, γ values), and generalization to other speech enhancement architectures

## Next Checks

1. Implement router ablation studies varying complexity (GRU hidden size, convolutional kernel size) to quantify efficiency trade-offs and validate the 0.06% parameter overhead claim
2. Conduct systematic hyperparameter sweeps for β and γ to map the full Pareto frontier and verify the claimed robustness of default settings (β=1.0, γ=0.1)
3. Design controlled experiments testing the input difficulty correlation hypothesis by correlating router-selected UFs with input SNR and other acoustic features across the test set