---
ver: rpa2
title: On Benchmarking Human-Like Intelligence in Machines
arxiv_id: '2502.20502'
source_url: https://arxiv.org/abs/2502.20502
tags:
- human
- human-like
- cognitive
- tasks
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current AI benchmarks are insufficient for
  assessing human-like intelligence because they often lack human-validated labels,
  fail to capture human response variability, and rely on oversimplified tasks. The
  authors conducted a human evaluation study on ten existing AI benchmarks, revealing
  significant biases and flaws in task and label designs.
---

# On Benchmarking Human-Like Intelligence in Machines

## Quick Facts
- arXiv ID: 2502.20502
- Source URL: https://arxiv.org/abs/2502.20502
- Reference count: 40
- One-line primary result: Human evaluation reveals significant flaws in existing AI benchmarks, including invalid labels and oversimplified tasks that fail to capture human cognitive diversity.

## Executive Summary
This paper identifies critical shortcomings in current AI benchmarks for evaluating human-like intelligence. Through a human evaluation study of 10 existing benchmarks, the authors demonstrate that ground-truth labels frequently disagree with actual human judgments, and that oversimplified task designs fail to capture the complexity of human cognition. They propose five recommendations for future benchmarks: use actual human data as gold labels, evaluate against population-level distributions, assess gradedness and uncertainty, ground tasks in cognitive theory, and design ecologically valid tasks. These recommendations aim to enable more rigorous and meaningful evaluations of human-like cognitive capacities in AI systems.

## Method Summary
The authors conducted a human evaluation study using 240 participants recruited via Prolific, with 24 participants assigned to each of 10 benchmarks. They randomly sampled 30 stimuli per benchmark (300 total) and collected responses on 1-100 slider scales, converting multiple-choice options to graded ratings. The study assessed agreement rates between human responses and benchmark ground-truth labels, analyzing response distribution patterns including bimodality, uncertainty, and gradedness. Participants completed 30 trials in randomized order, and the researchers computed per-stimulus agreement rates and aggregate statistics to evaluate the validity of existing benchmark labels.

## Key Results
- Ground-truth labels in existing benchmarks disagree with human judgments, with only 63.51% average agreement and 26.67% of stimuli below 50% agreement
- Human responses show substantial gradedness (57.69% of ratings between 20-80), which binary labels fail to capture
- Current benchmarks lack human-validated labels and fail to preserve population-level response distributions
- Many benchmarks rely on oversimplified tasks that don't reflect real-world cognitive complexity

## Why This Works (Mechanism)

### Mechanism 1: Human-Validated Ground Truth Exposes Label Invalidity
Benchmark labels created without human validation frequently disagree with actual human judgments, undermining claims about "human-level" AI performance. When evaluated against actual human responses, these labels systematically diverge from population judgments, creating a false signal where models optimizing for invalid labels appear to achieve human-like performance while actually learning misaligned objectives.

### Mechanism 2: Distributional Evaluation Preserves Population-Level Cognitive Diversity
Collapsing human responses to single labels via majority voting discards signal about legitimate cognitive diversity, leading to models that overfit to majority patterns and fail on minority subpopulations. Human cognition varies systematically across individual differences, creating multi-modal response distributions that aggregated labels cannot capture.

### Mechanism 3: Graded Response Elicitation Captures Intra-Individual Uncertainty
Binary forced-choice formats conflate high-confidence and low-confidence responses, obscuring model calibration and human-like reasoning under uncertainty. Slider-based elicitation reveals uncertainty structure that binary choices force-thresholding loses, preventing assessment of human-like uncertainty calibration.

## Foundational Learning

- **Concept: Ecological Validity in Cognitive Task Design**
  - Why needed here: The paper argues simplified benchmark tasks fail to reflect real-world cognitive complexity. Understanding ecological validity helps distinguish tasks that probe genuine cognition vs. artificial test-taking.
  - Quick check question: Does your benchmark task require the same cognitive processes humans use in naturalistic settings, or does it simplify away the interesting structure?

- **Concept: Inter-Annotator Agreement vs. Ground Truth**
  - Why needed here: The paper shows low agreement rates don't necessarily indicate task failure—they may reveal legitimate ambiguity. Distinguishing "noisy labels" from "ambiguous stimuli" is critical.
  - Quick check question: When annotators disagree, is the stimulus genuinely ambiguous (like "The Dress" illusion) or is the annotation protocol underspecified?

- **Concept: Distributional vs. Point Evaluation Metrics**
  - Why needed here: The paper recommends KL divergence and Wasserstein distance for comparing model outputs to human distributions. Standard accuracy metrics cannot capture whether models replicate human response patterns.
  - Quick check question: Can your evaluation metric distinguish between a model that gets 60% accuracy with high confidence on all items vs. one that matches human uncertainty patterns?

## Architecture Onboarding

- **Component map:**
  ```
  Benchmark Design Pipeline:
  ├── Task Selection (cognitive theory grounding)
  │   └── Input: Meta-reviews of target construct (e.g., Beaudoin et al.'s 220 ToM tasks)
  ├── Stimulus Construction
  │   └── Include: Ecologically valid scenarios, structured ambiguity, systematic ablation
  ├── Human Data Collection
  │   ├── Sample size: Sufficient for population-level distribution estimation
  │   ├── Elicitation: Graded scales (sliders), not forced choice
  │   └── Demographics: Capture individual differences for subgroup analysis
  ├── Label Processing
  │   ├── Preserve: Full response distributions (soft labels)
  │   └── Optional: Bias-free external labels for dual-track evaluation
  └── Evaluation Metrics
      ├── Distribution matching: KL divergence, Wasserstein distance
      ├── Mode detection: Can model explain multi-modal patterns?
      └── Conditional analysis: Subgroup performance
  ```

- **Critical path:** Task design → Human data collection → Distribution preservation → Distributional evaluation. If you collapse to hard labels at any stage, downstream claims about "human-like" performance become unverifiable.

- **Design tradeoffs:**
  - Scale vs. granularity: Slider elicitation captures uncertainty but increases annotation time
  - Ecological validity vs. controllability: Naturalistic tasks are harder to systematically vary
  - Human data cost vs. benchmark validity: Paper argues quality > quantity for smaller, carefully curated datasets

- **Failure signatures:**
  - High benchmark accuracy + low human agreement on labels → likely optimizing for invalid objectives
  - Unimodal model outputs on multi-modal human distributions → failing to capture population diversity
  - Binary high-confidence predictions on uncertain human judgments → miscalibrated confidence

- **First 3 experiments:**
  1. Re-annotation audit: Take an existing benchmark, sample 30-50 stimuli, collect graded human responses, compute agreement rate with existing labels
  2. Distributional baseline: Compare model performance using accuracy on hard labels vs. KL divergence from human soft label distribution
  3. Uncertainty calibration check: Evaluate whether model confidence aligns with human uncertainty on ambiguous stimuli

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the resource-intensive process of collecting robust, population-level human data be scaled for cognitively rich and interactive AI benchmarks?
- Basis in paper: The authors state in Section 5.3 that they "urge substantial additional research into ways that we can make evaluation with humans more scalable especially as we consider human-likeness... in interactions."
- Why unresolved: Collecting dense, distributional data is expensive and current crowdsourcing tools struggle with the complexity of "cognitively rich" tasks.

### Open Question 2
- Question: Under what specific conditions should AI systems replicate human cognitive biases versus striving for objective, "bias-free" correctness?
- Basis in paper: Section 5.2 notes that "The extent to which AI should replicate human cognitive biases must be evaluated on a case-by-case basis," highlighting a tension between simulating human behavior and avoiding harmful prejudices.
- Why unresolved: It is unclear if replicating errors (like loss aversion) helps or hinders human-AI collaboration in different contexts (e.g., economic vs. safety-critical).

### Open Question 3
- Question: How can benchmark designers systematically map complex psychological constructs to specific tasks when existing literature offers hundreds of divergent definitions?
- Basis in paper: Section 4.4 highlights the "common pitfall" of using impoverished theory, citing that reviews of Theory of Mind have identified 220 distinct tasks, making it difficult to choose a comprehensive set for evaluation.
- Why unresolved: There is no standardized method for distilling massive, fragmented psychological literatures into a cohesive, finite benchmark suite.

## Limitations
- The study relies on Prolific participants whose representativeness across cultural contexts remains uncertain, particularly relevant for social reasoning and moral permissibility tasks
- The claim that human-validated ground truth is essential may overgeneralize from domains where human judgment is appropriate to those with objective external verification
- The proposed five recommendations create significant practical barriers in terms of resource requirements and implementation complexity

## Confidence

- **High confidence**: The empirical finding that ground truth labels frequently disagree with human judgments (63.51% average agreement, 26.67% below 50% agreement) is directly supported by the human evaluation data
- **Medium confidence**: The mechanisms explaining why distributional evaluation and graded elicitation improve human-likeness assessment are theoretically grounded but lack extensive empirical validation
- **Low confidence**: The claim that ecological validity and cognitive theory grounding are essential for all benchmarks lacks empirical support

## Next Checks

1. **Benchmark re-evaluation study**: Select 5 existing benchmarks from different cognitive domains and conduct the same human re-annotation protocol to establish whether the 63.51% average disagreement is representative or benchmark-specific.

2. **Distributional evaluation comparison**: For a task with human distribution data, train two models (accuracy-optimized vs. distribution-matched) and evaluate both on accuracy, distributional match, and subgroup performance to test whether distributional objectives improve human-likeness.

3. **Ecological validity assessment**: Select 3 benchmarks and conduct expert review (cognitive scientists) to assess whether task scenarios reflect real-world cognitive demands and whether response formats capture actual human judgment complexity.