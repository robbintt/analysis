---
ver: rpa2
title: Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression
  and diagonal linear networks under $\ell_p$ bias
arxiv_id: '2509.21181'
source_url: https://arxiv.org/abs/2509.21181
tags:
- norm
- equation
- bulk
- generalization
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how the family of \u2113r norms {\u2016wp\u2016\
  r}r\u2208[1,p] scales with sample size for minimum-\u2113p interpolators in overparameterized\
  \ linear regression with Gaussian design. Using a dual-ray analysis, the authors\
  \ reveal a competition between a signal spike and a bulk of null coordinates in\
  \ X^\u22A4Y, yielding a data-dependent transition n\u22C6 (the \"elbow\") and a\
  \ universal threshold r\u22C6 = 2(p-1) that separates norms that plateau from those\
  \ that continue to grow with explicit exponents."
---

# Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias

## Quick Facts
- **arXiv ID**: 2509.21181
- **Source URL**: https://arxiv.org/abs/2509.21181
- **Reference count**: 40
- **Primary result**: Unified closed-form solution for $\ell_r$ norm scaling in overparameterized linear regression, revealing a data-dependent transition $n_\star$ and universal threshold $r_\star = 2(p-1)$ that separates plateau vs. growth regimes.

## Executive Summary
This paper provides a comprehensive analysis of how parameter norms scale with sample size in overparameterized linear regression under $\ell_p$ bias. Using a dual-ray framework, the authors reveal a competition between signal spike and noise bulk in the correlations $X^\top Y$, leading to a data-dependent transition $n_\star$ (the "elbow") and a universal threshold $r_\star = 2(p-1)$ that separates norms that plateau from those that continue to grow. The analysis extends to Diagonal Linear Networks (DLNs), showing that their implicit bias inherits similar scaling laws via an effective exponent $p_{\text{eff}}(\alpha)$ calibrated from initialization scale.

## Method Summary
The paper analyzes minimum-$\ell_p$ interpolation in overparameterized linear regression with Gaussian design, using dual optimization to derive closed-form scaling laws. For DLNs, gradient descent dynamics are mapped to effective $\ell_p$ geometry through separable potential analysis. The key methodology involves: (1) solving constrained $\ell_p$ minimization problems, (2) analyzing dual-ray optimization to identify spike-bulk competition, and (3) calibrating DLN initialization scale to effective $p_{\text{eff}}$ via slope-matching of the separable potential.

## Key Results
- Reveals a universal threshold $r_\star = 2(p-1)$ separating $\ell_r$ norms that plateau vs. grow with sample size
- Identifies data-dependent transition scale $n_\star \propto (\kappa_{\text{bulk}} \tau_s^q / W_q)^{2/(q-2)}$ governing elbow behavior
- Shows DLNs inherit same scaling laws as explicit minimum-$\ell_p$ interpolation via effective $p_{\text{eff}}(\alpha)$
- Provides closed-form expressions for all $\ell_r$ norms within $r \in [1,p]$ under $\ell_p$-biased interpolation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The scaling of parameter norms is determined by a competition between the true signal ("spike") and the noise in the null coordinates ("bulk") within the correlations $X^\top Y$.
- **Mechanism**: The analysis reduces the high-dimensional interpolation problem to a one-dimensional "dual-ray" optimization. The dual scale $t^\star$ depends on the ratio $\|Y\|_2^2 / \|X^\top Y\|_q^q$. The denominator decomposes into a signal term $\propto n^q$ and a bulk term $\propto (d-s)n^{q/2}$. Whichever term dominates dictates the scaling regime.
- **Core assumption**: Isotropic Gaussian design ($X_{ij} \sim \mathcal{N}(0,1)$) and strictly convex $\ell_p$ penalties ($p > 1$).
- **Evidence anchors**:
  - [Abstract]: "reveals a competition between a signal spike and a bulk of null coordinates in $X^\top Y$"
  - [Section 3.1]: Eq. (1) shows the explicit decomposition of $t^\star$ into spike, bulk, and remainder terms.
  - [Corpus]: Paper 103103 discusses related implicit regularization but focuses on the $\ell^1$ limit; this paper generalizes the competition framework.
- **Break condition**: If the design matrix $X$ is highly anisotropic or heavy-tailed, the concentration of the bulk term ($m_q$) may fail, changing the crossover point $n_\star$.

### Mechanism 2
- **Claim**: A universal threshold $r_\star = 2(p-1)$ separates $\ell_r$ norms that plateau from those that grow indefinitely as sample size increases.
- **Mechanism**: In the spike-dominated regime ($n \gg n_\star$), the KKT map transforms dual correlations into primal weights via a power function. The contribution of the bulk to the $\ell_r$ norm scales as $n^{1/r - 1/2(p-1)}$. If $r > r_\star$, the exponent is negative (decay/plateau); if $r < r_\star$, the exponent is positive (growth).
- **Core assumption**: High-dimensional limit where $d/n \to \kappa > 1$.
- **Evidence anchors**:
  - [Abstract]: "universal threshold $r_\star = 2(p-1)$ that separates $\|\hat{w}_p\|_r$'s which plateau from those that continue to grow"
  - [Page 4, Theorem 3.1]: Eq. (4) explicitly defines the spike-dominated scaling with the split cases based on $r$ vs. $2(p-1)$.
  - [Corpus]: Paper 93535 (Early stopping) identifies implicit $\ell_2$ bias in logistic regression ($p=2 \implies r_\star=2$), aligning with this threshold logic.
- **Break condition**: At the boundary $p=2$ (Ridge), $r_\star=2$, and no $n$-driven transition exists in the proportional limit.

### Mechanism 3
- **Claim**: Diagonal Linear Networks (DLNs) trained by Gradient Descent (GD) inherit the same scaling laws as explicit minimum-$\ell_p$ interpolation via an effective exponent $p_{\text{eff}}(\alpha)$.
- **Mechanism**: The DLN dynamics are governed by a separable potential $Q_\alpha$. By fitting the log-log slope of $Q_\alpha$ on $k$-sparse probes against the known $\ell_p$ geometry ($k^{1-p/2}$), one maps the initialization scale $\alpha$ to an effective geometry $p_{\text{eff}}$.
- **Core assumption**: Gradient descent with small initialization (gradient flow limit); finite learning rate effects are treated as noise.
- **Evidence anchors**:
  - [Section 4.4]: "Calibrating the initialization scale $\alpha$ to an effective $p_{\text{eff}}(\alpha)$ via the DLN separable potential"
  - [Appendix B]: Algorithm 1 details the slope-matching map $\alpha \to p_{\text{eff}}$.
  - [Corpus]: Paper 103103 provides tight bounds on the $\ell^1$ implicit bias in similar DLN architectures, supporting the small-$\alpha$ limit.
- **Break condition**: Large learning rates with label noise introduce an "effective temperature," increasing $p_{\text{eff}}$ and shifting the predicted elbow $n_\star$ (see Appendix D).

## Foundational Learning

- **Concept: Convex Duality and KKT Conditions**
  - **Why needed here**: The entire scaling derivation relies on analyzing the dual problem $\max_\lambda \lambda^\top Y - \frac{1}{q}\|X^\top \lambda\|_q^q$ rather than the primal. The KKT conditions link the dual ray scale to the primal coordinates.
  - **Quick check question**: Given a dual variable $\lambda$, how does the KKT map define the primal weight $w_i$ in terms of the correlation $\langle X_i, \lambda \rangle$?

- **Concept: Gaussian Concentration Inequalities**
  - **Why needed here**: The "bulk" characterization depends on the precise concentration of $\sum \langle X_j, \lambda \rangle^q$ for null coordinates. Understanding the moment bounds $m_q$ is essential to verify the validity of the closed-form expressions.
  - **Quick check question**: Why does the bulk term scale as $\approx (d-s)n^{q/2}$ while the spike term scales as $\approx n^q$?

- **Concept: Implicit Regularization via Initialization**
  - **Why needed here**: To bridge the gap between explicit $\ell_p$ penalties and neural networks (DLNs), you must understand how initialization scale $\alpha$ acts as a hyperparameter controlling the geometry (sparse vs. dense) of the solution.
  - **Quick check question**: As initialization $\alpha \to 0$, does the DLN solution approach an $\ell_1$ or $\ell_2$ geometry?

## Architecture Onboarding

- **Component map**:
  - **Primal Solver**: `argmin ||w||_p` s.t. $Xw=Y$ (The object of study)
  - **Dual Ray**: The 1D scalar $t^\star$ derived from $X^\top Y$ (The diagnostic tool)
  - **DLN Trainer**: Gradient descent on a diagonal network parameterized by $\alpha$
  - **Calibrator**: Algorithm 1 & 2 to map $\alpha \to p_{\text{eff}}$

- **Critical path**:
  1. Compute correlations $X^\top Y$
  2. Estimate spike strength ($W_q$) and noise scale ($\tau_s$)
  3. Calculate the transition sample size $n_\star \approx (\kappa_{\text{bulk}} \tau_s^q / W_q)^{2/(q-2)}$
  4. Select the norm index $r$ relative to threshold $r_\star=2(p-1)$ to determine if you are in a growth or plateau regime

- **Design tradeoffs**:
  - **Choice of $r$**: If using norms as generalization proxies, $r > r_\star$ offers stability (plateau) but may be insensitive to data quality; $r < r_\star$ is sensitive to data growth but potentially noisy
  - **Initialization $\alpha$**: Small $\alpha$ favors sparsity (low $p$) but may require precise tuning to avoid noise amplification (bulk dominance) in early training

- **Failure signatures**:
  - **Unexpected Growth**: If $\|\hat{w}\|_r$ grows when it should plateau ($r > r_\star$), check if you are actually in the bulk-dominated regime ($n < n_\star$) or if high learning rate noise has inflated $p_{\text{eff}}$
  - **Missing Elbow**: If no transition is observed, the signal spike $W_q$ may be too weak or the dimension $d$ too small relative to $n$

- **First 3 experiments**:
  1. **Verify $n_\star$ Prediction**: Generate data with a single spike ($w^\star = e_1$), solve minimum $\ell_p$ interpolation for varying $n$, and plot $\|\hat{w}\|_r$ to locate the elbow (compare against Eq. 3)
  2. **Threshold Sweep**: Fix $p=1.5$ (so $r_\star=1.0$). Vary $r \in [0.5, 1.5]$ and confirm that $r < 1.0$ grows while $r > 1.0$ plateaus in the spike-dominated regime
  3. **DLN Calibration Check**: Train a DLN with specific $\alpha$, estimate $p_{\text{eff}}$ using Algorithm 1, and verify that the observed $\|\hat{w}\|_r$ scaling matches the explicit minimum-$\ell_{p_{\text{eff}}}$ interpolator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the transition scale $n_\star$ and the universal threshold $r_\star$ deform under anisotropic, sub-Gaussian, or heavy-tailed covariate designs?
- Basis in paper: [explicit] The "Future directions" section explicitly proposes extending the dual-ray analysis to "anisotropic/sub-Gaussian designs... and to heavy-tailed covariates" to characterize spectrum and tail effects.
- Why unresolved: The current proof relies heavily on isotropy and Gaussian assumptions (e.g., Lemmas A.5â€“A.8) to decompose $\mathbf{X}^\top\mathbf{Y}$ into spike and bulk terms with precise moments.
- What evidence would resolve it: Derivations of scaling laws where the covariance spectrum or tail parameters explicitly enter the expressions for $n_\star$ and $r_\star$.

### Open Question 2
- Question: Does the universal threshold $r_\star = 2(p-1)$ persist in deep nonlinear architectures when replacing power links with depth-dependent implicit links?
- Basis in paper: [explicit] The "Future directions" section asks to test "whether an $r_\star$-type threshold persists" when extending to "deep (nonlinear) architectures" and path-norm regimes.
- Why unresolved: The current theory is restricted to Diagonal Linear Networks (DLNs) where the implicit bias is characterized by a separable potential; nonlinearity introduces complex interactions not captured by this framework.
- What evidence would resolve it: Empirical scaling studies in ResNets or Transformers showing similar plateau vs. growth transitions governed by depth-dependent links.

### Open Question 3
- Question: Can a rigorous theory for $p_{\text{eff}}$ be developed that explicitly accounts for optimization hyperparameters (step size, momentum, noise) as geometric parameters?
- Basis in paper: [explicit] The "Future directions" section calls for a theory of $p_{\text{eff}}$ that accounts for "step size, batch size, momentum, and label noise."
- Why unresolved: The paper provides an empirical calibration $\alpha \to p_{\text{eff}}(\alpha)$ for DLNs and notes finite learning rates/noise act as an "effective temperature," but lacks a closed-form mapping.
- What evidence would resolve it: A quantitative function mapping hyperparameters (e.g., learning rate $\eta$, noise $\sigma^2$) to $p_{\text{eff}}$ that accurately predicts the shift in the elbow $n_\star$.

## Limitations
- The theoretical framework assumes isotropic Gaussian design and strictly convex $\ell_p$ penalties ($p > 1$), which may not hold in practice
- The universal threshold $r_\star = 2(p-1)$ is derived asymptotically and may exhibit finite-sample deviations
- The mapping from DLN initialization scale $\alpha$ to effective $p_{\text{eff}}$ is based on slope-matching that could be sensitive to the specific probing method

## Confidence
- **High Confidence**: The dual-ray analysis framework for deriving closed-form $\ell_r$ norm scaling (Section 3.1-3.3)
- **Medium Confidence**: The universal threshold $r_\star = 2(p-1)$ separating growth vs. plateau regimes
- **Medium Confidence**: The empirical mapping from DLN initialization $\alpha$ to effective $p_{\text{eff}}$ via separable potential

## Next Checks
1. **Finite-sample threshold validation**: Generate synthetic data with controlled spike/bulk ratios and measure the empirical $r_\star$ threshold across different sample sizes to quantify deviations from the theoretical $2(p-1)$ prediction.

2. **Design matrix robustness**: Test the scaling laws under non-Gaussian designs (e.g., correlated features, heavy-tailed entries) to identify when the spike/bulk competition mechanism breaks down.

3. **Label noise sensitivity**: Incorporate varying levels of label noise and measure its impact on the elbow transition $n_\star$ and the effective $p_{\text{eff}}$ mapping in DLNs, particularly in the large-$\alpha$ regime where noise effects may be amplified.