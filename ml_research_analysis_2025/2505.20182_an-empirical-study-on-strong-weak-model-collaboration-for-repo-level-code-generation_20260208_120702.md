---
ver: rpa2
title: An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation
arxiv_id: '2505.20182'
source_url: https://arxiv.org/abs/2505.20182
tags:
- strong
- weak
- code
- shot
- issue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates strong-weak LLM collaboration
  strategies for repository-level code generation. The core idea is to leverage a
  strong model for complex tasks while delegating simpler ones to a weaker, cheaper
  model to reduce cost.
---

# An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation

## Quick Facts
- arXiv ID: 2505.20182
- Source URL: https://arxiv.org/abs/2505.20182
- Reference count: 40
- This study systematically evaluates strong-weak LLM collaboration strategies for repository-level code generation.

## Executive Summary
This paper investigates how to optimally combine strong and weak large language models for repository-level code generation to reduce costs while maintaining performance. The authors evaluate 12 collaboration strategies across different model pairs on the SWE-Bench Lite benchmark, finding that pipeline-based methods like "Strong LM First" can achieve 92% of strong model performance at 60% of the cost. The study reveals surprising insights about routing decisions and context augmentation, demonstrating that weak routers often outperform strong routers and that instance-level context guidance is more effective than repository-level summaries.

## Method Summary
The study uses the Agentless Lite framework with voyage-code-3 retrieval to address GitHub issues through repository-level code generation. It evaluates 12 collaboration strategies across four categories: cost-equated weak-only methods, context-based augmentation, pipeline-based delegation, and dynamic routing. The methods use pairs of strong and weak LLMs (O3-mini, O4-mini, GPT-4o-mini with Qwen2.5-Coder variants) to generate SEARCH/REPLACE patches. Key metrics include Resolution Rate, Cost, Efficiency Score, Valid Patch Rate, and average iterations. Temperature starts at 0.0 and increases by 0.1 per retry up to 10 attempts maximum.

## Key Results
- The "Strong LM First" pipeline strategy achieves 41.67% resolution rate at $18.3, delivering 92% of strong model performance at 60% of the cost
- Weak-only self-consistency baselines consistently underperform collaboration strategies, with even 15+ samples from GPT-4o-mini achieving only ~16% resolution
- Weak routers frequently outperform strong routers by 6 percentage points in resolution while reducing cost by ~20%, suggesting strong models "overthink" routing decisions
- Instance-level context augmentation (plans, QA pairs) significantly boosts weak model performance, while repository-level context consistently fails to improve results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pipeline-based "Strong LM First" delegation achieves near-strong performance at reduced cost by leveraging complementary model strengths.
- Mechanism: The strong model generates an initial solution attempt, then the weak model iteratively refines format and details until a valid patch is produced. This exploits the strong model's superior reasoning while using the weak model for cheaper syntactic correction.
- Core assumption: Weak models can reliably perform format correction and minor refinements even when they cannot solve complex problems from scratch.
- Evidence anchors:
  - [abstract] "Our most effective collaborative strategy achieves equivalent performance to the strong model while reducing the cost by 40%."
  - [Section 5, Main Results] "Strong LM First achieves a 0.4167 resolution rate, which is ∼92% more than the best corresponding cost-equated baseline"
  - [corpus] Limited direct evidence; "Synergistic Weak-Strong Collaboration by Aligning Preferences" addresses preference alignment but not pipeline delegation specifically.
- Break condition: When weak model cannot reliably format-correct strong model output (e.g., very weak models like Qwen-7B show lower valid patch rates per Table 5).

### Mechanism 2
- Claim: Instance-level context augmentation (plans, QA pairs) outperforms repository-level context for weak model guidance.
- Mechanism: Strong model generates task-specific planning or FAQ documents tailored to the immediate problem, providing precise navigational cues without overwhelming the weak model with coarse repository-wide information.
- Core assumption: Weak models suffer more from attention dilution than from lack of general knowledge; focused context is more valuable than comprehensive context.
- Evidence anchors:
  - [Section 5.1, Interesting cases] "Instance-level help is better than repo-level help. Repo-level context (e.g., summaries, structure, QA pairs) consistently failed to improve weak LM performance."
  - [Section 5.1] "In contrast, instance-level augmentation (plans, QA pairs) significantly boosted resolution rates, justifying their higher cost."
  - [corpus] No direct corpus evidence for instance-level vs. repo-level context in code generation.
- Break condition: When instance-level augmentation cost approaches or exceeds the cost difference between weak and strong models, negating efficiency gains.

### Mechanism 3
- Claim: Weak routers can outperform strong routers in task delegation due to reduced overthinking.
- Mechanism: A weaker model acting as router makes simpler classification decisions (SIMPLE vs. COMPLEX) without excessive analysis, avoiding the "reasoning-action dilemma" where stronger models over-analyze routing decisions and misclassify.
- Core assumption: Routing is a classification task where calibration matters more than raw reasoning capability; strong models may be poorly calibrated for this specific decision boundary.
- Evidence anchors:
  - [Section 5.1] "Weak Router frequently outperformed Strong Router, both in accuracy and cost. For example, with O4-mini + Qwen2,5-Coder-32B, Weak Router achieved 6 percentage points higher resolution at ∼20% lower cost."
  - [Section 5.1] "We suspect that stronger models, while good at problem solving, may 'overthink' routing decisions"
  - [corpus] "The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks" (Cuadron et al., 2025) is cited in the paper as supporting evidence.
- Break condition: When weak routers lack sufficient competence to distinguish complexity levels (observed with very weak models like Qwen-7B where strong routing slightly edged ahead).

## Foundational Learning

- Concept: **Cost-performance Pareto curves for model selection**
  - Why needed here: The paper demonstrates that optimal strategy selection depends on budget and performance constraints, with curves intersecting at different cost thresholds (e.g., regime shift at $18.3 in Figure 2).
  - Quick check question: Given a budget of $15 and minimum 20% resolution requirement, which collaboration strategy from Figure 2 would you select?

- Concept: **Self-consistency vs. collaboration trade-offs**
  - Why needed here: The paper shows cost-equated weak-only methods (sampling n outputs from weak model to match strong model cost) consistently underperform collaboration strategies, challenging the assumption that more samples from weak models compensate for capability gaps.
  - Quick check question: Why does sampling 15+ outputs from GPT-4o-mini (to match O4-mini cost) achieve only ~16% resolution vs. 41.67% for Strong LM First at similar cost?

- Concept: **Valid patch rate vs. resolution rate distinction**
  - Why needed here: Prompt Reduction achieves higher resolution despite lower valid patch rate (~65% vs. ~95%), illustrating that turn-wise reliability and final correctness are distinct metrics.
  - Quick check question: If your deployment prioritizes API efficiency (fewer retries) over maximum resolution, which strategy would you avoid?

## Architecture Onboarding

- Component map: Retriever -> Router (optional) -> Context Augmentor (optional) -> Generator (strong/weak) -> Validator -> (retry loop if invalid, max 10 iterations)
- Critical path: Retriever → [Router decision] → Context Augmentor (if applicable) → Generator (strong/weak based on strategy) → Validator → (retry loop if invalid, max 10 iterations)
- Design tradeoffs:
  - Strong LM First: Higher upfront cost, highest resolution (~92% of strong model), best when budget allows
  - Weak LM First: Lowest cost, lower resolution, optimal under tight budgets
  - Prompt Reduction: Aggressive context pruning reduces valid patch rate but can improve resolution; high-variance strategy
  - Dynamic routing: Adaptive but requires router calibration; weak routers often outperform strong routers
- Failure signatures:
  - Low valid patch rate (<70%) indicates weak model struggles with format adherence (see Qwen-7B in Table 5)
  - Self-consistency methods showing resolution ≤ weak baseline indicates patch selection failure
  - Strong Router underperforming Weak Router suggests overthinking/classification miscalibration
- First 3 experiments:
  1. Establish baselines: Run strong-only and weak-only on your dataset to quantify the performance gap and cost ratio (essential for calculating cost-equated weak samples).
  2. Test Strong LM First with your strongest/cheapest pair: This consistently achieved best or near-best performance across all model pairs in the study; use it as your collaboration upper bound.
  3. Generate cost-performance curves for your domain: Plot resolution vs. cost for at least Weak LM First, Strong LM First, and Weak Router to identify regime boundaries specific to your budget constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do strong-weak collaboration strategies maintain their efficacy in complex, multi-turn agentic frameworks?
- Basis in paper: [explicit] Section 6 states "Future work can extend this taxonomy to more complex approaches," noting the current study restricted scope to the simpler "Agentless Lite" framework.
- Why unresolved: The study validates methods only on a two-step RAG framework; it is unknown if the overhead of agentic interactions negates the cost benefits seen in static pipelines.
- What evidence would resolve it: Evaluating the "Strong LM First" or routing strategies within an agentic system like SWE-Agent to compare cost-performance curves.

### Open Question 2
- Question: How does strong-weak collaboration impact latency and energy consumption relative to monetary cost?
- Basis in paper: [inferred] Section 7 explicitly notes the study measures cost via Token/API usage and "do not consider factors like latency or energy consumption."
- Why unresolved: Delegating to strong models or using pipeline methods may reduce API costs but increase total latency due to sequential processing or model switching.
- What evidence would resolve it: A benchmark measuring end-to-end wall-clock time and compute resource usage for "Prompt Reduction" vs. baseline approaches.

### Open Question 3
- Question: Why do strong models "overthink" routing decisions, and can this behavior be mitigated to improve dynamic collaboration?
- Basis in paper: [inferred] Section 5.1 observes that "Strong Router frequently underperformed," hypothesizing that stronger models may "overthink" routing decisions compared to weak routers.
- Why unresolved: The paper identifies the counter-intuitive phenomenon (Weak Router > Strong Router) but does not isolate the specific reasoning failure modes causing it.
- What evidence would resolve it: An analysis of strong router reasoning traces to determine if excessive caution or hallucinated complexity causes misclassification of simple tasks.

## Limitations

- The study focuses on a single benchmark (SWE-Bench Lite) with limited domain diversity, potentially limiting generalizability to different codebases and programming languages.
- The analysis doesn't fully explore edge cases where weak models might systematically fail on certain code patterns or syntactic structures.
- While high-resolution rates are reported for pipeline-based methods, the paper doesn't thoroughly examine the trade-offs between turn-wise reliability and final correctness.

## Confidence

- **High Confidence**: The cost-performance trade-off curves and the superiority of pipeline-based "Strong LM First" strategy are well-supported by quantitative results across multiple model pairs.
- **Medium Confidence**: The mechanism explaining why weak routers outperform strong routers (overthinking hypothesis) is plausible but relies on behavioral observations rather than direct measurement of router decision processes.
- **Medium Confidence**: The recommendation for instance-level context augmentation over repository-level context is supported by the data but may not generalize to domains where broader context provides more value than immediate task-specific guidance.

## Next Checks

1. **Domain Transfer Test**: Evaluate the top 3 collaboration strategies on a different code generation benchmark (e.g., HumanEval or a domain-specific repository) to assess generalization beyond SWE-Bench Lite.

2. **Router Calibration Analysis**: Instrument the routing mechanism to log classification confidence scores and analyze whether weak routers consistently show better calibration on the SIMPLE/COMPLEX boundary than strong routers.

3. **Failure Mode Analysis**: Conduct targeted testing on code patterns where weak models historically struggle (complex refactoring, unfamiliar APIs, multi-file dependencies) to quantify when collaboration breaks down versus when it succeeds.