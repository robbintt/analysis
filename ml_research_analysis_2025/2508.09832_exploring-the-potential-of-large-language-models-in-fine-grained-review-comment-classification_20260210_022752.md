---
ver: rpa2
title: Exploring the Potential of Large Language Models in Fine-Grained Review Comment
  Classification
arxiv_id: '2508.09832'
source_url: https://arxiv.org/abs/2508.09832
tags:
- code
- review
- llms
- comments
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explored the potential of Large Language Models (LLMs)
  for fine-grained classification of code review comments into 17 categories. We designed
  various prompt engineering approaches using different classification strategies
  (flat vs hierarchical) and input contexts (with or without code).
---

# Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification

## Quick Facts
- **arXiv ID:** 2508.09832
- **Source URL:** https://arxiv.org/abs/2508.09832
- **Reference count:** 40
- **Primary result:** LLMs classify code review comments into 17 categories with Llama 3.1-405B achieving 46.2% average F1 score, significantly outperforming supervised methods on rare, high-utility categories.

## Executive Summary
This paper investigates the use of Large Language Models (LLMs) for fine-grained classification of code review comments into 17 distinct categories. The authors evaluate various prompt engineering approaches, including flat and hierarchical classification strategies, with and without code context. Five LLMs are compared, with Llama 3.1-405B showing the best performance at 46.2% average F1 score. The study demonstrates that LLMs significantly outperform state-of-the-art supervised deep learning approaches, particularly on the five most useful but scarce categories. The findings suggest LLMs offer a scalable solution for code review analytics, addressing class imbalance issues common in traditional methods.

## Method Summary
The study employs zero-shot inference with five LLMs (Llama 3.1-405B via API, Llama 3-70B, Qwen 2-72B, Llama 3-8B, Qwen 2-7B on GPUs) to classify 1,828 annotated code review comments from OpenStack Nova. Two strategies are tested: flat (single-step 17-way classification) and hierarchical (5-group → subcategory selection). Prompts are structured as multiple-choice questions with category definitions, with code context optionally included. Classification effectiveness is measured using weighted average F1-score, precision, recall, and accuracy, compared against a CodeBERT+LSTM baseline using 10-fold cross-validation.

## Key Results
- Llama 3.1-405B achieved highest average F1 score of 46.2% using flat strategy with code context
- LLMs significantly outperformed supervised deep learning on five most useful categories (functional defect, logical, security, integration, architectural) which are scarce and challenging for traditional methods
- Hierarchical prompting improved small/medium model accuracy while flat prompting suited large models
- Code context improved classification for most models, particularly those with sufficient context capacity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs outperform supervised deep learning on low-frequency, high-utility review comment categories without task-specific training data.
- **Mechanism:** Pre-trained code and language knowledge enables zero-shot transfer to fine-grained classification, bypassing data scarcity bottleneck that limits supervised models trained from scratch.
- **Core assumption:** Semantic patterns distinguishing 17 comment types exist within LLM pre-training corpora (code repositories, review discussions).
- **Evidence anchors:** Llama 3.1-405B F1=10.4 vs. CodeBERT+LSTM F1=0.0 on functional defect (0.65% of data); Llama 3.1-405B F1=13.2 vs. 0.0 on logical (3% of data).
- **Break condition:** Target domain uses highly project-specific terminology or novel comment types absent from pre-training.

### Mechanism 2
- **Claim:** Hierarchical (two-step) prompting improves small/medium model accuracy; flat (single-step) prompting suits large models.
- **Mechanism:** Decomposing 17-way classification into 5-group → subcategory selection reduces reasoning complexity per step, aiding models with limited context capacity; large models process all categories holistically without error propagation.
- **Core assumption:** Error propagation cost in hierarchical prompting exceeds flat-context confusion cost only for smaller models.
- **Evidence anchors:** Qwen 2-7B hierarchical F1=29.2–31.9 vs. flat F1=19.2–21.7; Llama 3.1-405B flat F1=46.2 > hierarchical F1=42.8.
- **Break condition:** When high-level group classification accuracy drops below ~50%, hierarchical error propagation nullifies gains.

### Mechanism 3
- **Claim:** Adding code change context (old + new code) improves classification for models with sufficient context capacity.
- **Mechanism:** Code context lets LLMs infer comment intent by analyzing comment-code relationships (e.g., deprecated methods, interface changes), supplementing linguistic features.
- **Core assumption:** LLMs can jointly reason over code diffs and natural language comments within prompt context windows.
- **Evidence anchors:** Llama 3-70B: flat F1 improves 33.5→38.7 with code; Qwen 2-72B: hierarchical F1 improves 38.3→40.1.
- **Break condition:** When code-context length exceeds model context window or diff is irrelevant/noisy, added tokens add noise without signal.

## Foundational Learning

- **Concept:** Zero-shot classification via prompt engineering
  - **Why needed here:** The entire approach assumes LLMs can classify without fine-tuning; understanding prompt design (task framing, definitions, response formatting) is prerequisite to replication.
  - **Quick check question:** Can you explain why formatting the task as a multiple-choice question with structured definitions improves reliability over open-ended generation?

- **Concept:** Class imbalance in multi-class classification
  - **Why needed here:** 17 categories span 0.21% (Timing) to 21% (Documentation); weighted F1-score interpretation requires understanding how imbalance skews metrics.
  - **Quick check question:** Why might a model achieve high accuracy but near-zero F1 on minority classes?

- **Concept:** Hierarchical vs. flat classification strategies
  - **Why needed here:** Model-size-dependent strategy selection is central; choosing wrong strategy for your model size yields suboptimal results.
  - **Quick check question:** What is the trade-off between reduced per-step complexity and error propagation in hierarchical classification?

## Architecture Onboarding

- **Component map:** Input layer (code diff + comment text) → Prompt constructor (system prompt + user prompt + category options + code context) → Strategy selector (Flat vs Hierarchical) → LLM inference engine (Llama 3/Qwen 2 models) → Output parser (extract category label using "$" stopping criterion)

- **Critical path:** 1) Load comment + associated code diff; 2) Construct prompt with refined category definitions; 3) Route to flat (405B) or hierarchical (≤72B) based on model size; 4) Parse response; log F1/precision/recall per category; 5) Compare against CodeBERT+LSTM baseline using 10-fold cross-validation

- **Design tradeoffs:** Model size vs. inference cost (405B: 9 hrs, best F1 vs. 72B: 5 hrs, near-baseline vs. 8B: 1 hr, below baseline); Code context vs. prompt length (context helps large models but adds tokens; small models show negligible gain); Definition detail vs. context load (refined definitions help small models but can overwhelm context windows)

- **Failure signatures:** Non-standard responses (<5% of outputs): model generates text outside label list → mapped to "False Positive"; Near-zero F1 on rare categories (Timing, Support): suggests context insufficient or category definitions overlap; Hierarchical F1 < flat F1 on large model: signals unnecessary decomposition overhead

- **First 3 experiments:** 1) Replicate Llama 3.1-405B flat + code+comment on 100-comment sample; verify F1 ≈ 46 and identify high-error categories; 2) Ablate code context: run flat strategy comment-only vs. code+comment on Qwen 2-72B; quantify F1 delta; 3) Test definition sensitivity: swap refined definitions for brief definitions on Llama 3-8B hierarchical; measure F1 change direction (paper shows +20.2% possible)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced prompting strategies like Chain-of-Thought (CoT) or Retrieval-Augmented Generation (RAG) improve classification effectiveness?
- **Basis in paper:** The authors state in Section VI-C that future work should explore these advanced techniques to better leverage in-context learning.
- **Why unresolved:** The current study was restricted to zero-shot prompting because the limited and imbalanced benchmark dataset provided insufficient examples to construct reliable few-shot prompts.
- **What evidence would resolve it:** An evaluation of LLMs using few-shot examples or RAG pipelines on an expanded, balanced dataset of review comments.

### Open Question 2
- **Question:** Can knowledge distillation from large LLMs be used to generate synthetic training data for fine-tuning smaller, more efficient models?
- **Basis in paper:** Section VI-C suggests exploring knowledge distillation to automatically label more data, thereby addressing data scarcity and enabling the fine-tuning of smaller language models.
- **Why unresolved:** The current study focused on zero-shot inference to solve the data scarcity problem; the utility of LLMs as teachers for creating training sets for this specific taxonomy remains untested.
- **What evidence would resolve it:** Experiments where a large model (e.g., Llama 3.1-405B) labels raw data to train a smaller student model, comparing the student's performance and efficiency against the zero-shot baseline.

### Open Question 3
- **Question:** How can performance be improved for challenging, context-dependent categories like Support, Resource, and Interface?
- **Basis in paper:** Section VI-C identifies these categories as challenging due to the nuanced nature of comments and overlapping definitions (e.g., library configuration vs. interaction).
- **Why unresolved:** The current input context (comment + code diff) lacks the repository-level scope necessary to infer issues like API interactions, leading to lower F1-scores in these specific areas.
- **What evidence would resolve it:** A study testing enriched context windows (e.g., importing file structures or API definitions) or empirically refined category definitions to better distinguish these similar groups.

## Limitations

- **Single dataset limitation:** The study relies exclusively on code review comments from OpenStack Nova Project, limiting generalizability to other platforms or domains.
- **Context quality uncertainty:** While code context improves classification, the paper doesn't quantify how context quality (noise, relevance) affects accuracy across model sizes.
- **Reproducibility gaps:** Key hyperparameters (temperature, top_p, max_tokens) and precise parsing logic for non-standard LLM responses are unspecified, creating barriers to faithful reproduction.

## Confidence

- **High Confidence:** LLMs significantly outperform supervised deep learning on five most useful but rare categories (functional defect, logical, security, integration, architectural comments). Well-supported by internal comparisons showing non-zero F1 scores where baselines achieve zero.
- **Medium Confidence:** Hierarchical prompting improves small/medium model accuracy while flat prompting suits large models. Results show clear patterns within tested models, but mechanism lacks external validation and the 50% accuracy threshold for group classification is theoretical.
- **Medium Confidence:** Adding code context improves classification for models with sufficient context capacity. Evidence shows improvements for most models, but paper doesn't quantify context quality or address scenarios where context is noisy or irrelevant.

## Next Checks

1. **External Dataset Validation:** Test the proposed methodology on code review datasets from other platforms (e.g., GitHub pull requests, Gerrit reviews) to assess generalizability beyond OpenStack Nova. This addresses the single-dataset limitation and tests whether zero-shot transfer works across different codebases and review cultures.

2. **Context Quality Impact Analysis:** Systematically vary code context quality by introducing controlled noise or irrelevant diffs to quantify how context quality affects classification accuracy across model sizes. This would validate the assumption that context helps only when it's meaningful and within context windows.

3. **Hyperparameter Sensitivity Study:** Conduct ablation studies varying temperature, top_p, and max_tokens across all five LLMs to determine optimal settings and quantify their impact on F1 scores. This addresses the reproducibility gap created by unspecified inference hyperparameters.