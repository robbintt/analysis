---
ver: rpa2
title: Automatic Evaluation of Healthcare LLMs Beyond Question-Answering
arxiv_id: '2502.06666'
source_url: https://arxiv.org/abs/2502.06666
tags:
- open-ended
- perplexity
- metrics
- arxiv
- careqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the correlation between open-ended and close-ended
  evaluation methodologies for large language models (LLMs) in healthcare. The authors
  introduce a new medical benchmark, CareQA, available in both closed- and open-ended
  formats, and propose a novel metric, Relaxed Perplexity, to better assess factuality
  in open-ended evaluations.
---

# Automatic Evaluation of Healthcare LLMs Beyond Question-Answering

## Quick Facts
- arXiv ID: 2502.06666
- Source URL: https://arxiv.org/abs/2502.06666
- Reference count: 40
- Primary result: Weak correlations between open-ended and close-ended healthcare LLM evaluations suggest these approaches assess different model capabilities

## Executive Summary
This paper systematically examines the relationship between open-ended and close-ended evaluation methodologies for large language models in healthcare. The authors introduce CareQA, a new medical benchmark available in both closed- and open-ended formats, and propose a novel metric called Relaxed Perplexity to better assess factuality in open-ended evaluations. Through comprehensive experiments across multiple benchmarks and models, they demonstrate that open-ended and close-ended evaluations capture largely independent aspects of model performance, with correlations typically below 0.3. Within open-ended evaluations, metrics cluster into three distinct groups based on their information extraction methods: perplexity-based, n-gram/semantic similarity, and LLM-as-a-judge metrics.

## Method Summary
The authors created CareQA, a comprehensive medical benchmark derived from Spanish MIR exam materials (2020-2024), containing 5,621 close-ended MCQA pairs and 2,769 rephrased open-ended QA pairs. The open-ended versions were generated through automated translation (GPT-4) and rephrasing (Qwen2.5-72B-Instruct) followed by two iterations of human review with 10 annotators. They evaluated 12 open LLMs across 12 benchmarks using 11 open-ended metrics (BLEU, ROUGE, BERTScore, BLEURT, MoverScore, Prometheus, and perplexity variants) plus accuracy for close-ended tasks. The novel Relaxed Perplexity metric computes the probability of the target appearing anywhere in the completion, addressing standard perplexity's limitation with intermediate reasoning tokens.

## Key Results
- Weak correlations (typically <0.3) between open-ended and close-ended benchmarks indicate orthogonal evaluation dimensions
- Three distinct metric clusters identified in open-ended evaluations: perplexity-based (high correlation >0.96), n-gram/semantic similarity, and LLM-as-a-judge metrics
- CareQA shows varying difficulty across categories, with Dermatology being easiest and Ethics being most challenging
- Relaxed Perplexity better captures improvements in healthcare fine-tuned models compared to standard perplexity
- Within open-ended evaluations, BERTScore shows highest correlation with accuracy, while ROUGE-2 performs poorly on factuality tasks

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Evaluation Dimensions
Close-ended MCQA measures selection accuracy among fixed options (factual recall), while open-ended tasks measure generative capacity including discourse coherence, clinical reasoning articulation, and contextual appropriateness. The paper shows weak to negative correlations between these paradigms, suggesting medical competence involves both factual knowledge retrieval and discourse generation, which are separable cognitive functions in LLMs.

### Mechanism 2: Metric Clustering by Information Source
Open-ended evaluation metrics cluster into three groups based on what signal they extract from generated text. Perplexity metrics capture probabilistic prediction quality; n-gram and semantic similarity metrics capture surface and meaning overlap with references; LLM-judge metrics capture holistic quality judgments including formatting and reasoning structure. High intra-cluster correlation (>0.96 for perplexity metrics) indicates redundancy, while inter-cluster divergence indicates complementary information.

### Mechanism 3: Relaxed Perplexity for Factuality Without Formulation Dependence
Traditional perplexity fails to reward correct answers appearing after intermediate reasoning tokens; Relaxed Perplexity addresses this by computing the probability of the target appearing anywhere in the completion. This captures chain-of-thought benefits without requiring exact formulation matching, allowing evaluation of factuality independent of intermediate reasoning tokens.

## Foundational Learning

- **Concept:** Perplexity as a language model quality measure
  - **Why needed here:** The paper's novel Relaxed Perplexity metric modifies standard perplexity; understanding baseline perplexity (exp of cross-entropy measuring how well a model predicts a sequence) is prerequisite to grasping why the relaxation matters.
  - **Quick check question:** Given a model with perplexity 10 on a medical corpus, what does this intuitively mean about its prediction quality?

- **Concept:** N-gram overlap metrics (BLEU, ROUGE) and their limitations
  - **Why needed here:** The paper demonstrates these metrics cluster together and are resilient to rephrasing but may miss semantic correctness. Understanding their surface-level matching mechanism explains why they're insufficient alone.
  - **Quick check question:** Why would a medically accurate response using different terminology score poorly on ROUGE-1?

- **Concept:** LLM-as-a-judge evaluation paradigm
  - **Why needed here:** Prometheus represents a distinct metric cluster with unique properties (formatting sensitivity, low self-consistency). Understanding judge-based evaluation is critical for interpreting why it provides "unique perspective" but requires multiple runs.
  - **Quick check question:** What biases might an LLM judge introduce when evaluating responses from the same model family it was trained on?

## Architecture Onboarding

- **Component map:**
  ```
  Evaluation Suite
  ├── Close-ended tasks (MCQA, prescriptions, classification, relation extraction)
  │   └── Metric: Accuracy
  ├── Open-ended tasks (questions, diagnosis, note-taking, factuality, summarization, entailment)
  │   ├── Metric Cluster 1: Perplexity variants (Word, Byte, Bits-per-Byte)
  │   ├── Metric Cluster 2: Overlap/Similarity (BLEU, ROUGE, BERTScore, BLEURT, MoverScore)
  │   ├── Metric Cluster 3: LLM-judge (Prometheus)
  │   └── Novel: Relaxed Perplexity (factuality-focused, formulation-agnostic)
  └── CareQA benchmark (close + open variants, EN/ES, 2,769-5,621 QA pairs)
  ```

- **Critical path:**
  1. Start with CareQA-Close (MCQA accuracy) as baseline factuality check
  2. Run CareQA-Open with at least one metric from each cluster (e.g., Word Perplexity, BERTScore, Prometheus)
  3. If model shows weak close-open correlation (expected), add Relaxed Perplexity to assess whether factuality gap is real or formulation-dependent
  4. For production deployment decisions, include task-specific benchmarks (MTS-Dialog for note-taking, not just general medical QA)

- **Design tradeoffs:**
  - **Metric selection:** N-gram metrics are most self-consistent and rephrasing-resilient but miss semantic correctness; LLM-judge captures nuance but has highest variance across runs (Figure 2 shows BLEURT and Prometheus as least consistent)
  - **Relaxed Perplexity parameters:** ℓ (sequences to consider) and stride affect computation cost vs. accuracy; paper recommends ℓ=5, stride=8-16
  - **Benchmark contamination:** CareQA (2020-2024) addresses data drift from older benchmarks (HeadQA: 2013-2017), but translation/rephrasing quality introduces new noise (73.6% inter-annotator agreement on open-ended rephrasing)

- **Failure signatures:**
  - High MCQA accuracy + low open-ended scores: Model has factual knowledge but poor generative discourse
  - High BERTScore + low Prometheus: Semantic content correct but formatting/reasoning structure poor
  - Standard perplexity decreasing while Relaxed Perplexity increasing: Model producing verbose intermediate reasoning that standard metric penalizes
  - Negative correlation between summarization and MCQA (observed in paper): Task requires different capabilities than factual recall

- **First 3 experiments:**
  1. **Baseline correlation check:** Run target model on CareQA-Close (accuracy) and CareQA-Open (all metric clusters); compute Pearson correlation. Expect weak correlation (0.1-0.3 based on paper findings). This validates whether your model follows the orthogonality pattern.
  2. **Metric cluster redundancy test:** For a fixed benchmark (e.g., MedText), compute correlation matrix across all 11 open-ended metrics. Confirm three-cluster structure. If clusters merge for your model/task, reduce redundant metrics.
  3. **Relaxed vs. standard perplexity comparison:** Evaluate healthcare fine-tuned vs. base model on OLAPH factuality benchmark with both metrics. Per paper findings, Relaxed Perplexity should show larger score gap favoring the fine-tuned model. Implementation requires vllm for efficient inference, top-p sampling with ℓ=5, s=10 search space.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does implementing Relaxed Perplexity with beam search improve computational efficiency or correlation with expert judgments compared to the current top-p sampling implementation?
- **Basis in paper:** The authors state "It remains as future work to implement Relaxed Perplexity with beam search" when discussing the novel metric's implementation.
- **Why unresolved:** The current implementation uses top-p sampling, but beam search may provide more comprehensive coverage of likely token sequences when estimating P(An|Bn).
- **What evidence would resolve it:** A comparison study of Relaxed Perplexity computed via beam search versus top-p sampling, measuring both computational cost and correlation with human expert factuality ratings.

### Open Question 2
- **Question:** Do the observed weak correlations between open-ended and close-ended benchmarks generalize to non-English languages and culturally diverse healthcare contexts?
- **Basis in paper:** The Limitations section states "All experiments are conducted on English benchmarks (except for the Spanish version of CareQA), and generalization to other languages has not been considered."
- **Why unresolved:** Medical knowledge, terminology, and clinical communication norms vary across languages and cultures, potentially affecting how models perform on open-ended versus close-ended tasks.
- **What evidence would resolve it:** Replicating the correlation analysis across multilingual medical benchmarks covering diverse healthcare systems and languages.

### Open Question 3
- **Question:** What specific biases exist in CareQA, and how do they affect model evaluation across different patient demographics and medical specializations?
- **Basis in paper:** The authors note "CareQA has not been subjected to formal bias assessment. Consequently, it may not adequately represent the full spectrum of medical knowledge or encompass all possible patient demographics."
- **Why unresolved:** While the original MIR exam materials underwent public scrutiny, the translation and rephrasing processes using GPT-4 and Qwen2.5-72B-Instruct may have introduced systematic biases.
- **What evidence would resolve it:** A formal bias audit of CareQA examining representation across demographic groups, medical specialties, and question difficulty levels.

## Limitations
- CareQA benchmark creation introduces potential noise through automated translation and rephrasing, with only 73.6% inter-annotator agreement on rephrasing quality
- All experiments conducted on English benchmarks (except Spanish CareQA), limiting generalizability to other languages and cultural healthcare contexts
- Prometheus LLM-judge metric has high variance across runs and lacks transparency in its scoring criteria, affecting reproducibility

## Confidence
- **High Confidence:** The orthogonality of open vs. closed evaluation dimensions - supported by consistent weak correlations across multiple benchmarks and model families
- **Medium Confidence:** The metric clustering framework - the clustering pattern is robust for the tested datasets but may vary with different medical domains
- **Medium Confidence:** Relaxed Perplexity's effectiveness - demonstrated improvement over standard perplexity but limited to specific factuality benchmarks

## Next Checks
1. **Cross-domain correlation validation:** Test the open-closed correlation patterns on non-medical benchmarks (legal, financial, technical domains) to determine if orthogonality is healthcare-specific or a general LLM evaluation phenomenon
2. **Cluster stability analysis:** Evaluate whether the three metric clusters persist when testing with different model families (particularly smaller models or specialized medical models) and across diverse medical subdomains (radiology vs. general practice)
3. **Reproducibility stress test:** Conduct multiple independent evaluations of the same model/benchmark pairs with different random seeds to quantify variance, particularly for LLM-as-judge metrics like Prometheus where Figure 2 shows high inconsistency between runs