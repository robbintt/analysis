---
ver: rpa2
title: 'HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation'
arxiv_id: '2508.18118'
source_url: https://arxiv.org/abs/2508.18118
tags:
- user
- personalized
- title
- titles
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HLLM-Creator addresses the challenge of generating personalized
  ad content at scale in online advertising by modeling user interests and integrating
  them into creative generation while maintaining factual accuracy and efficiency.
  The proposed hierarchical LLM framework combines user clustering, ad-user matching
  prediction, and chain-of-thought-based data construction to enable high-quality,
  user-specific content generation with minimal hallucination.
---

# HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation

## Quick Facts
- **arXiv ID:** 2508.18118
- **Source URL:** https://arxiv.org/abs/2508.18118
- **Reference count:** 40
- **Primary result:** 0.476% increase in Adss and 1.789% improvement in CTR via A/B testing on Douyin Search Ads

## Executive Summary
HLLM-Creator addresses the challenge of generating personalized ad content at scale in online advertising by modeling user interests and integrating them into creative generation while maintaining factual accuracy and efficiency. The proposed hierarchical LLM framework combines user clustering, ad-user matching prediction, and chain-of-thought-based data construction to enable high-quality, user-specific content generation with minimal hallucination. In real-world deployment on Douyin Search Ads, the system achieved significant gains in both effectiveness and scalability for personalized advertising.

## Method Summary
HLLM-Creator is a hierarchical LLM framework that personalizes ad title generation by encoding user click history into embeddings, clustering users for scalable inference, and generating personalized titles via a chain-of-thought synthetic data pipeline. The system consists of three LLMs: Item LLM (TinyLlama-1.1B) encodes ad titles, User LLM (TinyLlama-1.1B) aggregates user click history into embeddings, and Creative LLM (Qwen3-8B) generates personalized titles. Training uses end-to-end optimization with auxiliary losses (classification, alignment, reconstruction) to prevent embedding collapse. Offline clustering reduces generation calls from |users| × |ads| to K × |ads|, enabling industrial deployment.

## Key Results
- Achieved 0.476% increase in Adss and 1.789% improvement in CTR in real-world A/B testing on Douyin Search Ads
- 4.2% advantage over Pigeon baseline using sequence length 500 while maintaining computational efficiency
- 43.6% advantage with two-step Chain-of-Thought vs. 7.4% for single-step generation
- Maintained 68% advantage even with 8 clusters, demonstrating robustness to coarsened personalization

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical User Interest Encoding via Late Fusion
Separating item encoding from user sequence aggregation improves both efficiency and representation quality compared to flattening raw text sequences into a single LLM context. Item LLM encodes each clicked ad title into a fixed embedding via a special `[item]` token, then User LLM aggregates these pre-computed embeddings into a single user embedding via a `[user]` token. This allows sequence lengths of up to 500 items without quadratic attention costs on raw text.

### Mechanism 2: Chain-of-Thought Synthetic Data Construction
Decomposing personalized title generation into explicit reasoning steps produces higher-quality training data than direct generation prompts. Three-stage pipeline: (1) User Interest Profiling extracts multi-dimensional interests from click history; (2) Interest-Driven Title Generation selects user-relevant selling points before writing; (3) Hallucination-Free Filtering removes fabricated content. This creates supervision signals unavailable in real advertiser data.

### Mechanism 3: User Clustering + Matching-Based Pruning for Scalable Inference
Trading individual-level personalization for cluster-level generation makes industrial deployment feasible with acceptable quality degradation. Users are clustered offline via k-means on embeddings, then at inference Item-User Predictor scores each ad against all K cluster centers, only generating titles for top-k clusters. This reduces generation calls from |users| × |ads| to K × |ads|.

## Foundational Learning

- **Late Fusion in Sequential Recommendation**
  - Why needed here: Understanding why HLLM separates item encoding from sequence aggregation is essential for debugging embedding quality and sequence length tradeoffs.
  - Quick check question: Can you explain why computing item embeddings separately before sequence modeling reduces computational complexity compared to processing raw text sequences end-to-end?

- **Chain-of-Thought Prompting for Structured Generation**
  - Why needed here: The data construction pipeline's effectiveness depends on understanding how intermediate reasoning steps improve output quality.
  - Quick check question: What types of errors would you expect if you skipped the "selling point selection" step and asked the LLM to generate titles directly from user profiles?

- **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The semantic alignment loss uses InfoNCE to align user embeddings with text-extracted interest features.
  - Quick check question: In the align loss formulation, what happens to gradient signals if all users in a batch have similar interest descriptions?

## Architecture Onboarding

- **Component map:**
  - Item LLM (TinyLlama-1.1B) -> Item embeddings E
  - User LLM (TinyLlama-1.1B) -> User embedding U
  - Creative LLM (Qwen3-8B) -> Personalized title
  - Item-User Predictor (8-layer MLP-Mixer) -> Matching scores
  - Auxiliary components: User Feature Extractor, User Interest Decoder

- **Critical path:**
  1. Offline: Train all components end-to-end with L_gen + λ_cls·L_cls + λ_align·L_align + λ_recon·L_recon
  2. Offline: Generate synthetic training data via CoT pipeline with hallucination filtering
  3. Offline: Cluster all user embeddings via k-means into K groups
  4. Serving: For each ad, predict top-k matching clusters → generate k personalized titles offline
  5. Online: At request time, select from pre-generated title candidates via existing creative selection pipeline

- **Design tradeoffs:**
  - Cluster count (K): Higher K → better personalization but more generation cost. Paper uses 256; diminishing returns observed.
  - Sequence length: 500 items used; longer sequences may improve modeling but increase User LLM computation.
  - Sharing decoder weights: Saves memory but may create gradient conflicts between recon loss and generation loss.
  - Query-aware vs. query-free generation: Query-aware targets top-1 cluster with higher precision; query-free covers top-5 clusters with broader reach.

- **Failure signatures:**
  - High hallucination rate (>25%): Check selling point coverage in input; verify filtering pipeline not skipping detection.
  - Low GSB advantage (<10%): Inspect auxiliary loss weights; verify User LLM not collapsing to uniform embeddings.
  - Cluster quality degradation: Monitor intra-cluster variance; re-cluster if user distribution shifts.
  - CTR not improving despite offline gains: Check online selection pipeline—personalized titles may lose to existing candidates in ranking.

- **First 3 experiments:**
  1. Baseline user modeling comparison: Replace HLLM with keyword-based user representation or ID-based sequential model; measure GSB advantage drop (Figure 9 shows 51% vs. 30.6% for keywords).
  2. Ablation of auxiliary losses: Train with each loss individually and combined; Table 4 shows combined achieves 42.6% advantage vs. 30.8% without any.
  3. Cluster count sensitivity: Sweep K ∈ {8, 16, 32, 64, 128, 256} and measure both GSB advantage and total generation latency; identify ROI-optimal point.

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact on online Return on Investment (ROI) when scaling the number of user clusters to balance the tradeoff between inference cost and personalization granularity? While offline metrics improve with more clusters, computational overhead for industrial-scale serving remains a barrier, and optimal operating point for ROI has not been established.

### Open Question 2
Can the model's inherent resistance to hallucination be improved beyond the reported 75% pass rate without sacrificing the diversity of generated creatives? Current approach relies heavily on post-hoc filtering rather than training the model to inherently avoid fabricating facts during generation.

### Open Question 3
How does the framework perform when explicit "selling points" are unavailable or of low quality? The method depends on structured, high-quality input data (selling points), but it's unclear if the CoT pipeline degrades significantly for products where features are ambiguous or purely visual.

## Limitations
- The clustering approach trades individual-level personalization for computational efficiency, with no cross-domain validation of quality degradation
- Synthetic data construction pipeline may not generalize to product categories with different selling point structures
- No validation of whether hierarchical embedding approach captures universal personalization patterns vs. domain-specific artifacts

## Confidence

- **High Confidence:** The hierarchical architecture's computational efficiency claims are well-supported by both theoretical analysis and empirical comparison with Pigeon baselines.
- **Medium Confidence:** The Chain-of-Thought data construction methodology shows strong offline performance improvements, but generalizability to real-world user preferences remains an assumption.
- **Medium Confidence:** The user clustering + matching pruning approach demonstrates practical deployment benefits, but the tradeoff needs more extensive cross-category validation.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply HLLM-Creator to a different advertising domain (e.g., e-commerce product ads vs. short-video ads) and measure whether the same K=256 cluster configuration maintains the 68% baseline advantage.

2. **Individual vs. Cluster-Level Personalization Gap Analysis:** For a subset of high-value users, generate both individual-level titles (without clustering) and cluster-level titles, then measure CTR differences to quantify actual personalization loss.

3. **Synthetic Data Distribution Shift Evaluation:** Compare the semantic distribution of CoT-generated selling points against real advertiser-provided selling points across multiple product categories to identify systematic biases.