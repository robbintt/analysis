---
ver: rpa2
title: 'The Art of Asking: Multilingual Prompt Optimization for Synthetic Data'
arxiv_id: '2510.19806'
source_url: https://arxiv.org/abs/2510.19806
tags:
- language
- prompt
- prompts
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces prompt-space optimization for multilingual\
  \ synthetic data generation, addressing the limitation of translation-based prompts\
  \ that inherit English-centric framing and cultural bias. The method applies three\
  \ targeted transformations\u2014Naturalness (removing translation artifacts), Cultural\
  \ Adaptation (recontextualizing for local relevance), and Difficulty Enhancement\
  \ (increasing task complexity)\u2014to translated prompts using a multilingual LLM."
---

# The Art of Asking: Multilingual Prompt Optimization for Synthetic Data

## Quick Facts
- **arXiv ID:** 2510.19806
- **Source URL:** https://arxiv.org/abs/2510.19806
- **Reference count:** 31
- **One-line primary result:** Prompt-space optimization for multilingual synthetic data generation improves downstream performance by up to +4.7% accuracy on Global-MMLU and +35.3% win-rates on mArenaHard over translation-only baselines.

## Executive Summary
This paper introduces prompt-space optimization for multilingual synthetic data generation, addressing the limitation of translation-based prompts that inherit English-centric framing and cultural bias. The method applies three targeted transformations—Naturalness (removing translation artifacts), Cultural Adaptation (recontextualizing for local relevance), and Difficulty Enhancement (increasing task complexity)—to translated prompts using a multilingual LLM. Evaluated across 12 languages spanning 7 families, the approach substantially improves downstream performance over translation-only baselines, with Cultural Adaptation and Difficulty Enhancement together yielding the most well-rounded improvements across all benchmarks.

## Method Summary
The approach translates English seed prompts to target languages, then applies three targeted transformations (Naturalness, Cultural Adaptation, Difficulty Enhancement) using Gemma3-27B-it. Naturalness restores idiomatic phrasing and removes translationese, Cultural Adaptation replaces English-centric references with locally relevant equivalents, and Difficulty Enhancement increases prompt complexity and length. Transformed prompts are filtered with FastText language ID, then used to generate completions with the same teacher model. The resulting data is mixed (typically 50% Cultural + 50% Difficulty) with standard instruction datasets and used to fine-tune a 7B base model via SFT with Adam optimizer, weight decay 0.1, and 2 epochs.

## Key Results
- +4.7% accuracy improvement on Global-MMLU over translation-only baseline
- +2.4% improvement on Flores XCometXL benchmark
- +35.3% win-rates on mArenaHard and PolyWrite benchmarks
- Cultural Adaptation yields 7% improvement on culturally-sensitive G-MMLU subset vs 2% for cultural-agnostic questions
- Naturalness transformation increases n-gram diversity from 0.88 to 0.90 and reduces perplexity from 15.34 to 14.06

## Why This Works (Mechanism)

### Mechanism 1
Optimizing the input prompt distribution P(x) directly shapes model behavior more effectively than optimizing only completions. By applying transformation operators T = {T_nat, T_cult, T_diff} to translated prompts, the approach shifts P_opt,ℓ closer to the true user distribution P*ℓ, thereby altering what the model learns to understand, not just what it learns to say. Core assumption: Prompt-space interventions carry through to completion quality and downstream generalization.

### Mechanism 2
Removing translation artifacts ("translationese") increases prompt naturalness and diversity, improving alignment with authentic language use. The Naturalness transformation (T_nat) restores idiomatic phrasing and removes lexical errors, increasing n-gram diversity (from 0.88 to 0.90) and reducing perplexity (from 15.34 to 14.06), indicating closer alignment with pretraining distributions. Core assumption: Lower perplexity and higher diversity in prompts correlate with better downstream generation quality.

### Mechanism 3
Cultural adaptation grounds prompts in local contexts, improving performance on culturally-sensitive tasks. The Cultural Adaptation transformation (T_cult) replaces English-centric references with locally relevant equivalents (e.g., "Thanksgiving" → "Erntedankfest" → "Christmas" for German), shifting prompts toward distributions users actually produce. Core assumption: Cultural grounding improves not just relevance but model calibration for cultural knowledge retrieval.

### Mechanism 4
Difficulty enhancement produces more informative training signals, particularly for reasoning and generative tasks. The Difficulty transformation (T_diff) increases prompt length by 4.8× and complexity, yielding completions 2.2× longer, providing more target-language tokens and richer supervision. Core assumption: Longer, more complex prompts induce more detailed reasoning in the teacher model.

## Foundational Learning

- **Concept: Synthetic Data Generation Pipelines**
  - **Why needed here:** The entire approach operates within a synthetic data paradigm where teacher models generate training data for student models. Understanding the generation-completion pipeline (P(x) → P(y|x)) is essential to see where prompt optimization fits.
  - **Quick check question:** Can you explain why synthetic data quality depends on both prompt distribution P(x) and completion distribution P(y|x)?

- **Concept: Distribution Shift and Inductive Bias**
  - **Why needed here:** The paper's core thesis is that intervening on P(x) directly shapes the model's inductive bias. Understanding how training distributions affect generalization is critical.
  - **Quick check question:** Why might a model trained on translated English prompts underperform on authentic target-language user queries?

- **Concept: Translationese and Cross-Lingual Artifacts**
  - **Why needed here:** The Naturalness transformation specifically targets translationese—systematic artifacts in translated text. Recognizing these artifacts helps evaluate whether transformations succeed.
  - **Quick check question:** What are three characteristics of "translationese" that might differ from naturally written text in a target language?

- **Concept: Multilingual LLM Evaluation**
  - **Why needed here:** The paper uses diverse benchmarks (Global-MMLU, Include44, Flores, MGSM++, mArenaHard, PolyWrite) measuring different capabilities. Understanding what each captures is essential for interpreting results.
  - **Quick check question:** Why might discriminative benchmarks (like MMLU) be insufficient for evaluating open-ended generation quality?

## Architecture Onboarding

- **Component map:**
```
English Seed Prompts (280k)
        ↓
Language Identification + Filtering
        ↓
Sub-sampling (10k per language)
        ↓
Translation (Expert LLM)
        ↓
Prompt Transformations (Gemma3-27B-it):
  ├─ Naturalness (T_nat) → P_nat,ℓ
  ├─ Cultural Adaptation (T_cult) → P_cult,ℓ
  └─ Difficulty Enhancement (T_diff) → P_diff,ℓ
        ↓
Language ID Filtering (FastText)
        ↓
Completion Generation (Gemma3-27B-it)
        ↓
Language ID Filtering (FastText)
        ↓
Data Mixing (e.g., 50% Cultural + 50% Difficulty)
        ↓
SFT Fine-tuning (CommandR7B base)
        ↓
Evaluation Suite
```

- **Critical path:**
  1. **Seed prompt quality:** Real user prompts provide authentic distributional coverage; noise filtering is essential.
  2. **Transformation execution:** Gemma3-27B-it must have sufficient multilingual capability; transformation prompts (Table 8) must be well-designed to avoid over-templating.
  3. **Language identification:** FastText LID prevents language confusion from propagating; critical for low-resource languages.
  4. **Data mixing strategy:** The 50/50 Cultural+Difficulty mix achieved best overall performance; individual transformations excel on specific tasks.

- **Design tradeoffs:**
  - **Transformation aggressiveness vs. diversity:** Difficulty transformation lowers diversity (0.77 vs. 0.90) while increasing quality; mixing strategies balance this.
  - **Teacher model size vs. feasibility:** Using a 27B teacher for both transformation and completion is computationally expensive but ensures consistency; smaller models may introduce quality degradation.
  - **Language filtering strictness vs. data yield:** Aggressive LID filtering reduces dataset size (Table 7 shows variation from 5,287 to 9,992 samples); looser filtering risks language confusion.
  - **Transformation specificity vs. generalization:** Overly rigid transformation guidelines risk reducing diversity; simple prompts (as chosen) allow more natural variation.

- **Failure signatures:**
  - **Low-resource language degradation:** Basque, Welsh, Latvian show inconsistent gains; Naturalness transformation sometimes underperforms translation baseline (Table 4).
  - **Language confusion in completions:** Without LID filtering, models may respond in wrong language (Line Pass Rate drops without filtering).
  - **Cultural substitution errors:** Aggressive cultural adaptation may introduce factual errors if transformation model lacks cultural knowledge.
  - **Difficulty amplification noise:** In languages where teacher struggles, difficulty-boosted prompts may produce incoherent or hallucinated completions.

- **First 3 experiments:**
  1. **Baseline establishment:** Translate 10k English prompts to target languages using your translation model; generate completions with your teacher model; fine-tune base model; evaluate on Global-MMLU, Flores, and mArenaHard. This establishes the translation-only baseline all improvements are measured against.
  2. **Single transformation ablation:** Apply each transformation (Naturalness, Cultural, Difficulty) independently to translated prompts; generate completions and fine-tune separate models; compare downstream performance to identify which transformation helps which tasks. Expect Naturalness to help open-ended generation, Cultural to help culturally-sensitive tasks, Difficulty to help reasoning.
  3. **Mixing strategy exploration:** Create data mixtures combining transformations (e.g., 50/50 Cultural+Difficulty); fine-tune and evaluate; assess whether combined approaches achieve more well-rounded performance. The paper's best results came from mixing, suggesting complementary strengths.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does model merging provide superior generalization compared to data mixing when combining cultural and difficulty transformations?
  - **Basis in paper:** [explicit] The authors state in Section 4.2: "Future work may explore combinations through model merging rather than data mixing."
  - **Why unresolved:** The current study relies on mixing datasets (50% Cultural, 50% Difficulty) before fine-tuning. It is unknown if merging the weights of models fine-tuned separately on these distinct distributions would better preserve the "orthogonal benefits" observed in tasks like G-MMLU and mArenaHard.
  - **What evidence would resolve it:** A direct comparison of downstream benchmark performance (e.g., mArenaHard win rates) between a single model trained on the mixed dataset versus a merged model derived from separate checkpoints.

- **Open Question 2:** To what extent does geographic proximity influence the transferability of prompt optimization strategies to low-resource languages?
  - **Basis in paper:** [explicit] Section 6.2 notes, "Further work needs to confirm if the observations transfer to similarly positioned languages. There might also be some benefits from geographic proximity that we have not controlled for."
  - **Why unresolved:** The study is restricted to 12 European languages, making it difficult to disentangle the success of the transformations from the underlying linguistic or geographic similarity of the target languages to the source (English).
  - **What evidence would resolve it:** Applying the transformation pipeline to non-European or linguistically distant languages (e.g., Asian or African language families) and measuring the delta against the translation-only baseline.

- **Open Question 3:** Do automated LLM judges bias preference rankings towards "translationese," and can human evaluation confirm the reported improvements in naturalness?
  - **Basis in paper:** [explicit] Section 6.3 states, "LLM judges might have been trained on translationese as well and therefore favor it in their preferences... Human evaluation would be needed to confirm model rankings eventually."
  - **Why unresolved:** The evaluation relies heavily on LLM-based win rates (e.g., using GPT-4.1). If the judge model has a pre-training bias towards translated text, the reported "naturalness" improvements (or lack thereof) might not align with human perception of fluency.
  - **What evidence would resolve it:** A human evaluation study where native speakers blindly rank model outputs against the automated judge scores to validate the correlation.

## Limitations

- **Teacher model dependency:** The approach relies entirely on Gemma3-27B-it's multilingual competence to execute transformations correctly, with limited validation of transformation quality in low-resource languages.
- **Data mixing composition:** The exact composition of the remaining ~52% of the dataset (standard instruction tuning datasets) is unspecified, affecting reproducibility.
- **Language ID filtering tradeoffs:** FastText LID filtering removes ~10-15% of samples on average, with some languages losing more, reducing dataset size and potentially affecting low-resource languages disproportionately.

## Confidence

- **High confidence** in the core mechanism that prompt-space optimization provides a more powerful lever than completion-space optimization alone, supported by consistent improvements across multiple benchmarks.
- **Medium confidence** in the specific transformation implementations, as evaluation shows clear improvements in targeted metrics but limited validation of linguistic and cultural accuracy.
- **Low confidence** in the generalizability to unsupported languages and extreme low-resource scenarios, as the paper only evaluates 12 languages with variable performance in Basque, Welsh, and Latvian.

## Next Checks

1. **Cross-linguistic transformation quality audit:** Manually evaluate a stratified sample of transformed prompts (5 per language, 3 languages representing high-resource, mid-resource, and low-resource) to verify that Naturalness removes translationese without introducing errors, Cultural Adaptation makes appropriate substitutions, and Difficulty enhancement maintains coherence.

2. **Teacher model size ablation:** Repeat the pipeline using progressively smaller multilingual models (e.g., Mistral-7B, Gemma2-9B) for transformations to assess whether the 27B teacher is truly necessary or whether smaller models can achieve comparable transformation quality with appropriate prompting.

3. **Zero-shot transfer to unsupported languages:** Apply the trained model to a held-out language family (e.g., Turkic or Dravidian) not present in the training data to test whether the prompt optimization generalizes to truly unseen languages, measuring performance drop and identifying which transformation types transfer best.