---
ver: rpa2
title: 'HEART: Achieving Timely Multi-Model Training for Vehicle-Edge-Cloud-Integrated
  Hierarchical Federated Learning'
arxiv_id: '2501.09934'
source_url: https://arxiv.org/abs/2501.09934
tags:
- task
- training
- time
- global
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficiently training multiple
  machine learning models in dynamic vehicle-edge-cloud-integrated hierarchical federated
  learning (VEC-HFL) systems. The authors introduce a hybrid synchronous-asynchronous
  aggregation rule that combines synchronous aggregation at the edge layer with asynchronous
  aggregation at the cloud layer, effectively balancing timeliness and model quality
  while mitigating model obsolescence risks.
---

# HEART: Achieving Timely Multi-Model Training for Vehicle-Edge-Cloud-Integrated Hierarchical Federated Learning

## Quick Facts
- arXiv ID: 2501.09934
- Source URL: https://arxiv.org/abs/2501.09934
- Reference count: 40
- Primary result: Achieves up to 36.76% reduction in overall time overhead for multi-model training in VEC-HFL systems while maintaining balanced task execution across vehicles.

## Executive Summary
This paper introduces HEART, a two-stage framework for efficient multi-model training in dynamic vehicle-edge-cloud-integrated hierarchical federated learning (VEC-HFL) systems. The authors propose a hybrid synchronous-asynchronous aggregation rule that combines synchronous edge aggregation with asynchronous cloud aggregation, effectively balancing timeliness and model quality while mitigating model obsolescence risks. To address the NP-hard optimization problem of minimizing global training latency while ensuring balanced training across diverse tasks, HEART employs a hybrid Particle Swarm Optimization-Genetic Algorithm for balanced task scheduling, followed by a greedy algorithm for optimizing task training sequences to maximize overlap and minimize upload time.

## Method Summary
HEART consists of two optimization stages. Stage 1 uses a hybrid PSO-GA algorithm with dynamic inertia weight decay and GA crossover/mutation to assign tasks to vehicles across heterogeneous edge servers, balancing workload while satisfying dwell-time constraints. Stage 2 employs a greedy algorithm that sequences tasks on each vehicle by maximizing an aggregate-score combining task overlap (enabling faster edge aggregation) and upload timing (minimizing transmission time). The framework operates with a hybrid aggregation rule where edge servers wait synchronously for all assigned vehicles but the cloud server aggregates asynchronously upon receiving a subset of edge models, reducing overall training latency while preserving model quality.

## Key Results
- Achieves up to 36.76% reduction in overall time overhead compared to baseline methods
- Maintains balanced task execution across vehicles with uniform assignment distribution
- Effectively reduces non-task-training time through optimized task sequencing
- Demonstrates superior performance under varying numbers of vehicles and tasks

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Synchronous-Asynchronous Aggregation
- Claim: Combining synchronous edge aggregation with asynchronous cloud aggregation reduces global training latency while preserving model quality.
- Mechanism: Edge servers wait for all assigned vehicles to upload local models before aggregation (synchronous), ensuring full data utilization and model consistency. The cloud server aggregates once it receives a subset of edge models (asynchronous), avoiding delays from slower edge servers. This hybrid rule mitigates model obsolescence by reducing wait time at the cloud while still leveraging all vehicle data at the edge.
- Core assumption: Vehicles under the same edge server have relatively comparable latencies, while edge servers exhibit heterogeneous completion times due to varying vehicle distributions and channel conditions.

### Mechanism 2: Hybrid PSO-GA for Balanced Task Scheduling
- Claim: Integrating GA crossover and mutation into PSO prevents premature convergence to imbalanced task assignments across heterogeneous vehicles.
- Mechanism: The hybrid PSO-GA introduces dynamic inertia weight decay to transition from exploration to exploitation, and GA crossover and adaptive mutation between PSO iterations to diversify the solution space. The fitness function penalizes imbalanced task allocation via absolute deviation terms and rewards longer-duration tasks via weight coefficients, ensuring no single task dominates vehicle assignments.
- Core assumption: The task scheduling optimization landscape contains many local optima where single-algorithm approaches get trapped; the problem is sufficiently structured that evolutionary exploration improves solutions.

### Mechanism 3: Aggregate-Score Greedy Sequencing
- Claim: A greedy algorithm maximizing an aggregate-score (combining task overlap and upload timing) reduces non-task-training time at edge servers.
- Mechanism: For each edge server, the algorithm sequentially assigns tasks to vehicle training sequences by computing an aggregate-score: the overlap score counts how many vehicles train the same task at the same rank position (enabling timely edge aggregation), while the upload score prioritizes transmitting larger models when vehicles are closer to the edge server (reducing upload time).
- Core assumption: Vehicle-to-edge server distance varies predictably during the edge iteration, allowing upload-time optimization; higher task overlap directly translates to faster edge aggregation cycles.

## Foundational Learning

- **Federated Learning with Local/Global Iterations**
  - Why needed here: The paper builds on standard FL with SGD updates, edge aggregation, and cloud aggregation. Understanding how local iterations compose into edge iterations, then global iterations, is essential for following the time-cost model.
  - Quick check question: Can you explain why the global model depends on weighted averaging of edge models rather than direct vehicle contributions?

- **Particle Swarm Optimization Basics**
  - Why needed here: Stage 1 extends PSO with velocity updates, inertia weight, and probabilistic position updates. Understanding swarm exploration vs. exploitation helps evaluate why GA integration is necessary.
  - Quick check question: In standard PSO, what role does the inertia weight play in balancing exploration vs. exploitation?

- **Min-Max Optimization and NP-Hardness**
  - Why needed here: The core problem is a min-max formulation minimizing the maximum task completion time across all edge servers. The paper states it is NP-hard (integer programming with sequence variables), motivating heuristic approaches.
  - Quick check question: Why does the min-max objective (minimizing worst-case completion time) differ from minimizing average completion time in multi-task scheduling?

## Architecture Onboarding

- **Component map:**
  - Cloud Server (CS) -> Edge Servers (ESs) -> Vehicles
  - CS maintains global models for each task and broadcasts to ESs
  - ESs perform synchronous aggregation per task and relay models between vehicles and CS
  - Vehicles execute local SGD iterations and hold heterogeneous datasets

- **Critical path:**
  1. CS broadcasts initial global models to all ESs → vehicles
  2. Stage 1: Hybrid PSO-GA assigns tasks to vehicles satisfying dwell-time and balance constraints
  3. Stage 2: Greedy algorithm determines training rank for each vehicle based on aggregate-score
  4. Vehicles execute H SGD iterations for each assigned task in sequence
  5. ES waits for all vehicles training task j, aggregates via weighted average, sends edge model to CS
  6. CS updates global model upon receiving Q edge models and broadcasts back
  7. Repeat until convergence threshold is reached for each task

- **Design tradeoffs:**
  - Sync vs. Async Aggregation: Synchronous edge aggregation maximizes data utilization but risks delays from slow vehicles; asynchronous cloud aggregation reduces latency but may introduce staleness in global updates
  - Balance vs. Speed: Strict task balance may increase overall latency if some tasks have longer training times; HEART uses flexible bounds to allow controlled imbalance
  - Exploration vs. Exploitation in PSO-GA: Higher initial inertia weight and mutation rate improve exploration but slow convergence
  - Overlap vs. Upload Priority in Sequencing: Higher priority on overlap (faster aggregation) versus upload timing depends on model size variability and channel conditions

- **Failure signatures:**
  - Task Starvation: One or more tasks consistently receive fewer than N/J assignments (constraint violated), slowing convergence
  - Model Obsolescence: Vehicles complete training but wait excessively for edge model broadcast, increasing inactive time
  - Dwell-Time Violation: Vehicle exits ES coverage before completing assigned tasks, causing model loss
  - PSO-GA Premature Convergence: Fitness plateaus early with suboptimal balance

- **First 3 experiments:**
  1. Implement fully synchronous aggregation vs. HEART's hybrid rule with 25-50 vehicles; measure total time overhead and non-task-training time to confirm 10-15% reduction
  2. Compare HEART vs. TSPSO, TSGA, TSGD with 4-9 heterogeneous tasks; verify more uniform task assignments, especially for longer-duration tasks
  3. Scale vehicle count from 25 to 100+; measure HEART's decision time (PSO-GA + greedy) and total training time to identify practical break points

## Open Questions the Paper Calls Out

- How can the HEART framework be extended to accommodate variable vehicle arrival and departure patterns alongside fluctuating vehicular computing availability? (Basis: Section VI on future work; unresolved due to heuristic algorithms not sufficiently adapting to highly stochastic real-time resource fluctuations)
- What is the impact of data storage constraints and data freshness (temporal variations) on the convergence and accuracy of the multi-model training process? (Basis: Section VI identifies these factors; unresolved as paper assumes static data and doesn't incorporate storage limits or Age of Information metrics)
- How can the VEC-HFL architecture be made robust against unexpected disconnections between vehicles and edge servers during the training process? (Basis: Section VI states this as future direction; unresolved as current synchronous edge aggregation requires all vehicles to upload, breaking with disconnections)

## Limitations

- The hybrid PSO-GA algorithm's performance on larger-scale problems (beyond 9 tasks) is unverified, with O(N × J × p* × τ*) complexity potentially becoming prohibitive
- Critical hyperparameters and the exact Driver Yawning dataset preprocessing are unspecified, making exact reproduction challenging
- Channel model details and precise mobility simulation parameters are missing, affecting upload time predictions
- The approach assumes predictable vehicle mobility patterns; highly erratic movements could invalidate upload-time optimizations

## Confidence

- **High confidence** in the hybrid synchronous-asynchronous aggregation mechanism and its ability to balance timeliness with model quality, supported by theoretical formulation and consistent experimental results
- **Medium confidence** in the hybrid PSO-GA's effectiveness for task scheduling, as the method is well-grounded but corpus lacks direct validation of this specific hybrid approach
- **Low confidence** in the greedy sequencing algorithm's aggregate-score formulation without independent validation of the overlap-upload scoring mechanism or sensitivity analysis on ξ7 weighting

## Next Checks

1. **Baseline aggregation comparison:** Implement fully synchronous vs. HEART's hybrid aggregation under 25-50 vehicles; measure total time overhead and non-task-training time to confirm 10-15% reduction
2. **Task balance validation:** Compare HEART vs. TSPSO, TSGA, TSGD with 4-9 heterogeneous tasks; verify more uniform task assignments, especially for longer-duration tasks
3. **Scalability stress test:** Scale vehicle count from 25 to 100+; measure HEART's decision time (PSO-GA + greedy) and total training time to identify practical break points for real-time IoV deployment