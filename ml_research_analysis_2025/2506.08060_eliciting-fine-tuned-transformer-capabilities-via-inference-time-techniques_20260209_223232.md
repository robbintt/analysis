---
ver: rpa2
title: Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques
arxiv_id: '2506.08060'
source_url: https://arxiv.org/abs/2506.08060
tags:
- arxiv
- prompt
- pfine
- examples
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves that supervised fine-tuning capabilities can\
  \ be approximated by a base transformer using inference-time techniques like in-context\
  \ learning, without parameter updates. Under idealized conditions, the base model\
  \ can approximate the fine-tuned model's output distribution within a quantifiable\
  \ error \u03B5."
---

# Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques

## Quick Facts
- arXiv ID: 2506.08060
- Source URL: https://arxiv.org/abs/2506.08060
- Reference count: 40
- This paper proves that supervised fine-tuning capabilities can be approximated by a base transformer using inference-time techniques like in-context learning, without parameter updates.

## Executive Summary
This paper establishes a theoretical framework proving that base transformer models can approximate supervised fine-tuned (SFT) capabilities using inference-time techniques, specifically in-context learning (ICL), without modifying model parameters. The key insight is that SFT primarily refines access to existing capabilities rather than implanting new knowledge. Under idealized conditions, the base model can approximate the fine-tuned model's output distribution within a quantifiable error ε, with dataset sizes scaling as O(mV/ε² log m/δ) for text generation or O(d/ε) for linear classification tasks.

## Method Summary
The method constructs ICL prompts by concatenating input-output example pairs from the fine-tuning dataset with separator tokens, followed by the query input. The base model processes this structured prompt to compute P_base(y|x, prompt), approximating P_fine(y|x). For tasks with bounded context windows, retrieval-augmented selection (e.g., k-nearest neighbors) identifies representative examples that preserve local decision boundaries. The theoretical framework establishes sample complexity bounds using Hoeffding's inequality and coreset theory, quantifying the trade-off between dataset size, approximation error ε, and failure probability δ.

## Key Results
- Base transformers can approximate SFT capabilities within error ε using ICL without parameter updates
- Dataset size requirements: O(mV/ε² log m/δ) for text generation, O(d/ε) for linear classification
- Bounded context windows enable approximation with O(1/ε² log 1/δ) examples using local selection strategies
- Results grounded in transformer Turing completeness provide theoretical foundation for resource-efficient LLM deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A base transformer can approximate fine-tuned output distributions using in-context learning without parameter updates.
- Mechanism: ICL conditions the model on input-output example pairs from dataset D, enabling Bayesian inference over task distributions. The model computes P_base(y|x, D) by processing a structured prompt [x₁,y₁,...,xₙ,yₙ,x], generalizing from examples to approximate P_fine(y|x).
- Core assumption: The fine-tuned behavior represents latent capabilities already present in the base model; SFT primarily refines access rather than implanting new knowledge.
- Evidence anchors:
  - [abstract]: "formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters"
  - [section 2.3]: "Recent work suggests SFT reformats outputs for task-specific styles [18], supporting our hypothesis that base models can approximate fine-tuned behaviors via inference"
  - [corpus]: No directly relevant corpus neighbors found; this theoretical framework appears novel
- Break condition: If the fine-tuned capability requires knowledge genuinely absent from the base model's pre-training (e.g., domain-specific facts post-dating pre-training), ICL cannot recover it.

### Mechanism 2
- Claim: The approximation error ε decreases proportionally to O(1/√N) where N is the number of in-context examples.
- Mechanism: Hoeffding's inequality bounds the deviation between empirical distributions (from sampled examples) and true fine-tuned distributions. For m contexts with vocabulary V, total dataset size O(mV/ε² log m/δ) ensures the L₁ distance between base and fine-tuned next-token distributions remains within ε with probability ≥1-δ.
- Core assumption: Examples are drawn i.i.d. from the fine-tuned distribution; text data correlations may weaken this.
- Evidence anchors:
  - [section 5.5.1]: "By Hoeffding's inequality [34], ||p̂(·|cᵢ) − p_fine(·|cᵢ)||₁ ≤ ε with probability 1−δᵢ requires: nᵢ = O(V/ε² log 1/δᵢ)"
  - [section A.2]: Detailed derivation showing the union bound over V tokens and m contexts
  - [corpus]: Weak corpus support; related work on query complexities exists but doesn't directly validate these bounds
- Break condition: If examples exhibit strong long-range dependencies (non-i.i.d.), the stated bounds underestimate required dataset sizes.

### Mechanism 3
- Claim: With bounded context windows, task-specific example selection (e.g., k-nearest neighbors) achieves comparable approximation with O(1/ε² log 1/δ) examples for linear classification.
- Mechanism: Local approximation leverages Lipschitz continuity of task distributions—nearby inputs share similar outputs. Retrieval-augmented selection identifies representative examples that capture local decision boundaries, trading global coverage for context efficiency.
- Core assumption: Task distributions are locally smooth; the base model can approximate simple functions (linear classifiers) via ICL with bounded error η.
- Evidence anchors:
  - [section 5.6.1]: "For a query x, select a subset Sₓ ⊆ D of size k = O(1/ε² log 1/δ) using, e.g., k-nearest neighbors"
  - [section 6.3]: "For linear classification, O(1/ε² log 1/δ) examples (e.g., 500 for ε=0.1, δ=0.01) fit modern context windows"
  - [corpus]: No direct validation; neighboring papers address differential privacy bounds, not ICL sample complexity
- Break condition: For highly irregular decision boundaries or out-of-distribution queries, local selection fails and η dominates error.

## Foundational Learning

- Concept: **Total Variation Distance**
  - Why needed here: The paper's core theorem uses TV distance to quantify how closely the base model approximates the fine-tuned model. Understanding that TV(P,Q) = ½∑|P(y)−Q(y)| measures distributional difference is essential for interpreting ε bounds.
  - Quick check question: If TV distance is 0.05, can you explain what this means about the maximum probability difference for any single token prediction?

- Concept: **Coreset Theory**
  - Why needed here: Theorem 3's linear classification result relies on coreset theory to prove that small subsets (O(d/ε)) preserve classifier performance. This underpins the practical claim that minimal datasets suffice.
  - Quick check question: For a binary classifier in 50 dimensions with ε=0.1, approximately how many examples does coreset theory suggest are needed?

- Concept: **Turing Completeness of Transformers**
  - Why needed here: The proof requires that transformers can simulate any computable function. This theoretical foundation (from Pérez et al., 2021) enables the claim that fine-tuned functions are simulatable via ICL.
  - Quick check question: What assumptions about transformer resources are required for Turing completeness, and how does this differ from practical deployed models?

## Architecture Onboarding

- Component map:
  - Prompt Constructor -> Example Selector -> Base Model (θ_base) -> Error Monitor

- Critical path:
  1. Identify task type (text generation vs. linear classification) → determines dataset size bounds
  2. Retrieve/select examples from fine-tuning dataset D based on query similarity
  3. Construct prompt with separator tokens ensuring clear example boundaries
  4. Forward pass through frozen base model → P_base(y|x, prompt)
  5. Compare against fine-tuned baseline (if available) to estimate realized error

- Design tradeoffs:
  - **Context length vs. example count**: More examples improve approximation but exceed context windows; mitigate with RAG selection
  - **Example diversity vs. relevance**: Diverse examples improve generalization; relevant examples reduce η for specific queries
  - **Separator choice**: [SEP] tokens clarify boundaries but consume tokens; natural delimiters may confuse attention patterns
  - **Model scale**: Larger models reduce η but increase inference cost

- Failure signatures:
  - **Recency bias**: Model overweights later examples; re-order prompts placing most relevant examples last
  - **Distribution shift**: Query x differs from D domain; η spikes, outputs become unreliable
  - **Token boundary confusion**: Poor separator choice causes attention to blend examples; increase separator prominence
  - **Context overflow**: Exceeded context window truncates examples unpredictably; implement explicit example prioritization

- First 3 experiments:
  1. **Baseline calibration**: On a held-out classification task with known fine-tuned accuracy, measure TV distance between base+ICL and fine-tuned outputs across example counts k∈{10,50,100,500} to empirically validate O(1/√k) scaling.
  2. **Ablation on example selection**: Compare random selection vs. k-NN retrieval vs. clustering-based selection for fixed k=50 examples, measuring accuracy and η on out-of-distribution queries.
  3. **Context window stress test**: For text generation with V=50k vocabulary, measure at what output length l the bounded-context bounds (Theorem 5) break down, comparing against unbounded theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the empirical sample complexity constants and ICL error (η) for real-world tasks, and how do they compare to the theoretical bounds?
- Basis in paper: [explicit] Section 6.7 and A.6: "Empirical validation is crucial to confirm these bounds... Future research should focus on empirical validation" and "experiments on MMLU's science questions could measure η by comparing ICL predictions to fine-tuned model outputs"
- Why unresolved: The paper provides theoretical bounds (O(mV/ε² log m/δ), O(d/ε), etc.) but does not empirically validate them; the ICL error η is characterized as dependent on model capacity and prompt design but not quantified experimentally
- What evidence would resolve it: Empirical measurements of sample complexity and η on benchmarks (GLUE, MMLU, BigBench, WikiText) comparing ICL approximation error against fine-tuned baselines across varying dataset sizes

### Open Question 2
- Question: How does ICL-based approximation generalize to out-of-distribution inputs compared to supervised fine-tuning?
- Basis in paper: [explicit] Section 6.7: "Empirical studies on out-of-distribution inputs are needed to validate ICL's generalization compared to SFT" and Section A.6: "If the query x differs significantly from the dataset D, ICL may fail to generalize, increasing η"
- Why unresolved: SFT learns global patterns via parameter updates while ICL infers locally from examples; the paper notes this makes ICL more sensitive to prompt diversity but does not characterize OOD failure modes
- What evidence would resolve it: Controlled experiments measuring TV(Pbase, Pfine) on inputs drawn from distributions that differ from the fine-tuning dataset, with varying degrees of distribution shift

### Open Question 3
- Question: What prompting strategies minimize ICL approximation error η under bounded context constraints?
- Basis in paper: [explicit] Section 6.4: "Future work should explore robust prompting strategies to minimize variability and optimize performance" and Section A.3: "Empirical studies show that optimal ordering can reduce η by up to 10% in some tasks"
- Why unresolved: Prompt sensitivity (example order, selection, separator tokens) contributes to η but systematic strategies for error minimization remain unexplored; the paper notes "suboptimal prompts may increase approximation errors in Theorems 1–5"
- What evidence would resolve it: Ablation studies varying example selection (k-NN, clustering), ordering heuristics, and separator choices on ICL approximation error across tasks

## Limitations
- Theoretical bounds assume i.i.d. sampling from fine-tuned distributions, but real-world text data exhibits strong correlations and long-range dependencies that may require larger datasets
- ICL approximation error η is treated as a constant but is highly model-dependent and varies with prompt construction, example selection strategy, and task complexity
- Practical feasibility of retrieving representative examples within bounded context windows for complex tasks remains unproven, particularly for tasks requiring diverse or domain-specific knowledge absent from the base model's pre-training

## Confidence

**High Confidence:** The core mathematical proofs establishing sample complexity bounds (Theorems 2-5) are rigorous and follow standard techniques from information theory and statistical learning. The application of Hoeffding's inequality and coreset theory to ICL is methodologically sound.

**Medium Confidence:** The claim that base models can approximate fine-tuned capabilities through ICL is supported by recent empirical observations (e.g., [18]) and the theoretical framework, but direct empirical validation on specific model pairs is limited. The mechanism by which SFT "refines access" rather than "implants knowledge" is plausible but not definitively proven.

**Low Confidence:** The practical effectiveness of the bounded-context bounds (Theorems 4-5) for real-world applications, particularly for tasks requiring diverse knowledge beyond local decision boundaries. The paper's suggestion that O(1/ε² log 1/δ) examples suffice for linear classification within context windows needs empirical validation.

## Next Checks

1. **Empirical Scaling Validation:** Measure total variation distance between base+ICL and fine-tuned models across example counts k∈{10,50,100,500} on a classification task with known fine-tuned accuracy to empirically verify the predicted O(1/√k) scaling relationship.

2. **Example Selection Ablation:** Compare random selection vs. k-NN retrieval vs. clustering-based selection for fixed k=50 examples, measuring accuracy and η on out-of-distribution queries to determine the practical impact of selection strategy on approximation quality.

3. **Context Window Stress Test:** For text generation with V=50k vocabulary, measure at what output length l the bounded-context bounds break down by comparing against unbounded theoretical predictions, identifying the practical limits of context-efficient ICL.