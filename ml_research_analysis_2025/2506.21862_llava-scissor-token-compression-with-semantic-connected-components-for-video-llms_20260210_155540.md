---
ver: rpa2
title: 'LLaVA-Scissor: Token Compression with Semantic Connected Components for Video
  LLMs'
arxiv_id: '2506.21862'
source_url: https://arxiv.org/abs/2506.21862
tags:
- token
- video
- arxiv
- tokens
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LLaVA-Scissor, a training-free token compression
  method for video multimodal large language models that leverages Semantic Connected
  Components (SCC) to identify and represent distinct semantic regions in video tokens.
  Unlike attention-based approaches that often miss semantic regions or introduce
  redundancy, SCC partitions tokens into non-overlapping regions by measuring pairwise
  similarity and finding connected components, then compresses tokens through two
  steps: spatial compression within frames and temporal compression across frames.'
---

# LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs

## Quick Facts
- arXiv ID: 2506.21862
- Source URL: https://arxiv.org/abs/2506.21862
- Authors: Boyuan Sun; Jiaxing Zhao; Xihan Wei; Qibin Hou
- Reference count: 40
- Key outcome: LLaVA-Scissor achieves up to 6.1% higher performance than FastV at 3% token retention on MVBench and VideoMME while reducing FLOPs by up to 55%.

## Executive Summary
This paper introduces LLaVA-Scissor, a training-free token compression method for video multimodal large language models that leverages Semantic Connected Components (SCC) to identify and represent distinct semantic regions in video tokens. Unlike attention-based approaches that often miss semantic regions or introduce redundancy, SCC partitions tokens into non-overlapping regions by measuring pairwise similarity and finding connected components, then compresses tokens through two steps: spatial compression within frames and temporal compression across frames. Evaluated on diverse video understanding benchmarks including video question answering, long video understanding, and multi-choice tasks, LLaVA-Scissor consistently outperforms existing compression methods, particularly at low token retention ratios.

## Method Summary
LLaVA-Scissor uses Semantic Connected Components (SCC) to compress video tokens without training. The method computes pairwise cosine similarity between tokens, creates a binary adjacency matrix using a threshold τ, and applies Union-Find to find connected components representing distinct semantic regions. This process occurs in two steps: first spatially within each frame to identify unique regions, then temporally across frames to remove recurring semantics. Finally, all original tokens are merged into the retained representatives through weighted averaging based on similarity, preserving information density while achieving aggressive compression.

## Key Results
- Achieves up to 6.1% higher performance than FastV at 3% token retention on MVBench and VideoMME
- Reduces FLOPs by up to 55% while maintaining higher average performance across benchmarks
- Demonstrates consistent superiority over existing compression methods across diverse video understanding tasks including video QA, long video understanding, and multi-choice benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Token clustering via graph connectivity prevents semantic gaps better than attention-based scoring.
- **Mechanism**: Constructs a graph where tokens are vertices, forms an adjacency matrix using binary similarity threshold τ, and finds Semantic Connected Components (SCC)—disjoint subgraphs where all tokens are similar—to identify unique semantic regions. Tokens within a component are averaged to form a single representative.
- **Core assumption**: Pairwise cosine similarity in the feature space correlates strongly with semantic distinctness, and high similarity implies redundancy.
- **Evidence anchors**: [abstract] states SCC partitions tokens into non-overlapping regions by measuring pairwise similarity and finding connected components; [Section 3.1] explains this property suggests the token set can be partitioned into distinct semantic regions through connected components.

### Mechanism 2
- **Claim**: Decoupling spatial and temporal compression minimizes redundancy across the video timeline.
- **Mechanism**: Applies SCC spatially within each frame to identify unique regions, then concatenates spatial tokens and applies SCC temporally across the sequence to remove recurring semantics (e.g., static background appearing in multiple frames).
- **Core assumption**: Semantically similar regions are not necessarily spatially adjacent in the token grid or temporally adjacent in the frame sequence.
- **Evidence anchors**: [abstract] describes two-step spatio-temporal token compression strategy; [Section 3.2] explains similar to spatial fusion, SCC is applied again temporally to further compress similar representative tokens across the sequence.

### Mechanism 3
- **Claim**: Merging discarded tokens into retained ones preserves information density.
- **Mechanism**: Rather than dropping tokens that fall outside retention criteria, assigns every original token to the most similar retained "target" token and performs a weighted average merge.
- **Core assumption**: Retained "target" tokens form a sufficient basis set to represent the feature space of all "source" tokens.
- **Evidence anchors**: [Section 3.2] describes assigning each source token to the most similar target token and performing an average merge; [Table 4a] shows performance drop from 61.98 to 60.80 on MVBench when merge operation is removed.

## Foundational Learning

- **Concept**: **Union-Find (Disjoint Set Union)**
  - **Why needed here**: This data structure is the computational engine for finding SCCs efficiently. Understanding path compression and union-by-rank is required to implement the O(N²) avoidance strategy mentioned in Appendix C.
  - **Quick check question**: Can you explain why union-by-rank prevents the tree height in the connected components graph from growing excessively?

- **Concept**: **Token Pruning vs. Token Merging**
  - **Why needed here**: The paper explicitly contrasts "selection" (pruning) with "merging." Understanding this distinction is vital because LLaVA-Scissor's success relies on preserving information from discarded tokens via merging, rather than hard pruning used in methods like FastV.
  - **Quick check question**: If you have two tokens representing the same object but with different lighting, does averaging them (merging) discard more or less information than keeping only one (pruning)?

- **Concept**: **Visual Encoder Redundancy**
  - **Why needed here**: The method assumes visual tokens are highly redundant. Knowing why vision transformers (like SigLIP used here) produce redundant patches (e.g., background textures) helps in tuning the threshold τ.
  - **Quick check question**: Why does the paper suggest that attention-based methods (like FastV) might fail to capture "all semantic regions"?

## Architecture Onboarding

- **Component map**: Input (Video frames) -> Visual Encoder (SigLIP) -> Visual Projector -> Spatial SCC (per frame) -> Temporal SCC (across frames) -> Merge Layer -> Compressed tokens -> LLM (Qwen)

- **Critical path**: The construction of the binary adjacency matrix (Eq. 1) and the subsequent Union-Find operation (Appendix Algorithm 2). This is where the hyperparameter τ (threshold) and ε (error tolerance) directly control the compression ratio.

- **Design tradeoffs**:
  - **Threshold τ vs. Accuracy**: Higher τ (stricter similarity) leads to more components and less compression; lower τ leads to aggressive merging.
  - **Approximation ε**: Lowering ε increases sampling size N', improving accuracy but increasing computation.
  - **Efficiency**: The method trades off upfront FLOPs (calculating N² similarity) for reduced LLM prefilling cost. Note: The paper uses an approximation to avoid full N² complexity.

- **Failure signatures**:
  - **"Speckled" Output**: If spatial SCC is too aggressive (low τ), fine-grained details (like text or small objects) may be merged into background blobs.
  - **Context Loss**: At retention ratios < 5%, the model may hallucinate, as indicated by the performance cliff in Section 5.2.

- **First 3 experiments**:
  1. **Threshold Sweep**: Run LLaVA-Scissor on a single benchmark (e.g., MVBench) varying τ from 0.80 to 0.99 to map the curve of "Token Count vs. Accuracy."
  2. **Component Analysis**: Visualize the "connected components" on a sample frame. Color-code tokens belonging to the same component to verify if SCC respects object boundaries or ignores them.
  3. **Ablation on Merge**: Implement a version that hard-prunes instead of merges (as in Table 4a) to quantify the information loss on a long-video benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can a theoretical "reducing law" be formalized to predict the specific performance tipping point (e.g., the observed 35% retention threshold) for video token compression across diverse architectures?
- **Basis in paper**: [explicit] The authors state in Section 5: "we attempt to analyze the reducing law in video token compression," observing a sharp decline below 35% retention, but present this as an empirical finding rather than a proven theoretical law.
- **Why unresolved**: The paper provides empirical evidence of a degradation threshold on specific benchmarks (MVBench, VideoMME) but does not derive a mathematical formula to predict this limit for different video complexities or model scales.
- **What evidence would resolve it**: A theoretical formulation that correlates video information density or temporal redundancy with the optimal token retention ratio, validated across varied model architectures.

### Open Question 2
- **Question**: Can the two-step spatio-temporal compression strategy be adapted for online or streaming video processing where future frames are unavailable for temporal SCC calculation?
- **Basis in paper**: [inferred] The method requires concatenating representative tokens from all frames (T' = concat[t'_1, ..., t'_n]) before performing temporal compression (Section 3.2), implying a requirement for access to the full video sequence.
- **Why unresolved**: The current algorithm depends on global temporal connectivity to eliminate redundancy, which introduces latency incompatible with real-time causal processing.
- **What evidence would resolve it**: A modified SCC approach that processes temporal chunks incrementally (sliding window) while maintaining the non-overlapping semantic coverage described in the paper.

### Open Question 3
- **Question**: Does the reliance on raw token cosine similarity (SCC) fail to capture high-level semantic equivalence between visually distinct regions (e.g., different angles of the same object)?
- **Basis in paper**: [inferred] Section 3.1 defines semantic regions based on pairwise similarity A = (K · K^T > τ) using token embeddings, assuming that semantic similarity correlates directly with feature vector proximity.
- **Why unresolved**: Visual tokens representing the same object from different viewpoints may have low cosine similarity, potentially causing the SCC algorithm to treat them as distinct semantic regions and retaining unnecessary redundancy.
- **What evidence would resolve it**: A comparative analysis of clustering quality using "ground truth" semantic segments versus SCC clusters in multi-view video datasets.

## Limitations

- **Threshold sensitivity and reproducibility**: The method's performance hinges critically on the binary similarity threshold τ, but the paper provides only coarse guidance without specifying how to systematically select τ for arbitrary videos, introducing significant variability.
- **Generalizability beyond curated benchmarks**: While LLaVA-Scissor shows consistent gains across 8 benchmarks, these datasets share common characteristics (relatively short videos, clean content, clear semantic boundaries), and performance on naturalistic, noisy, or extremely long videos remains untested.
- **Computational cost characterization**: The paper claims improved efficiency but doesn't provide comprehensive runtime comparisons. The SCC algorithm requires computing pairwise similarity matrices (O(N²) complexity before approximation), which could become prohibitive for high-resolution or long-duration videos.

## Confidence

- **High confidence**: The core claim that SCC-based compression outperforms attention-based methods (FastV) is well-supported by direct benchmark comparisons across multiple retention ratios. The ablation study (Table 4a) demonstrating the importance of the merge operation provides strong evidence for this mechanism.
- **Medium confidence**: The claim of "training-free" token compression is technically accurate but potentially misleading. The method requires a pre-trained visual encoder (SigLIP) and LLM (Qwen) that were themselves trained on extensive data. The enhanced LLaVA-OneVision model also requires retraining, which the paper glosses over.
- **Low confidence**: The assertion that SCC "consistently outperforms" all baselines across all scenarios needs qualification. While superior at low retention ratios (3-7%), the performance advantage diminishes at higher ratios (35-50%), and the paper doesn't adequately address scenarios where attention-based methods might be preferable.

## Next Checks

1. **Threshold calibration study**: Run LLaVA-Scissor on a diverse set of 50 videos spanning different domains (sports, surveillance, educational content) and systematically vary τ from 0.80 to 0.99. Plot retention ratio vs. accuracy for each domain to identify whether a universal threshold exists or if domain-specific calibration is required.

2. **Long-duration video stress test**: Evaluate LLaVA-Scissor on videos exceeding 5 minutes in length with significant semantic evolution (e.g., cooking tutorials, conference talks). Measure performance degradation over time and compare against attention-based baselines to verify the claimed 55% FLOP reduction holds for extended sequences.

3. **Semantic boundary preservation analysis**: Select videos containing objects with gradual transitions (e.g., sunrise, melting ice) and visualize the SCC clustering results. Quantify how often semantically distinct regions are incorrectly merged versus how often semantically similar regions are unnecessarily split, using human annotations as ground truth.