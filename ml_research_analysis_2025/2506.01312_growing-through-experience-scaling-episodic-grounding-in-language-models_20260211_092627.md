---
ver: rpa2
title: 'Growing Through Experience: Scaling Episodic Grounding in Language Models'
arxiv_id: '2506.01312'
source_url: https://arxiv.org/abs/2506.01312
tags:
- episodic
- tasks
- grounding
- planning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  to effectively learn from and apply past experiences (episodic grounding) in physical
  planning tasks. The core problem is that while large models possess strong hierarchical
  representations, they lack efficient mechanisms to leverage experience streams.
---

# Growing Through Experience: Scaling Episodic Grounding in Language Models

## Quick Facts
- arXiv ID: 2506.01312
- Source URL: https://arxiv.org/abs/2506.01312
- Authors: Chunhui Zhang; Sirui; Wang; Zhongyu Ouyang; Xiangchi Yuan; Soroush Vosoughi
- Reference count: 16
- Primary result: Surpasses GPT-4o by 3.45% in physical planning tasks through weak-to-strong episodic grounding transfer

## Executive Summary
This paper addresses the challenge of enabling large language models to learn from and apply past experiences (episodic grounding) in physical planning tasks. While large models possess strong hierarchical representations, they lack efficient mechanisms to leverage experience streams. The authors propose a weak-to-strong episodic learning framework that transfers episodic behaviors from smaller to larger models using Monte Carlo Tree Search for structured experience collection and a novel distillation method. Experiments show their approach surpasses state-of-the-art proprietary models by 3.45% across planning and question-answering tasks, with layer-wise probing revealing significant improvements in task alignment.

## Method Summary
The method employs Monte Carlo Tree Search to collect both successful and failed episodic trajectories in a physical simulator, generating structured preference data. A small LM (≤8B) is trained on this data to establish episodic grounding, from which behavior ratios are computed. These ratios are then used to guide a large LM (70B-405B) through reverse KL-divergence minimization, inducing mode-seeking behavior that produces more goal-aligned action sequences. The approach combines weak-to-strong distillation with Direct Preference Optimization, preserving inherent LM capabilities while embedding episodic memory. For 405B models, inference-time output adjustment is used instead of full fine-tuning due to computational constraints.

## Key Results
- Outperforms GPT-4o by 3.45% on RobotHow benchmarks (74.34% vs 70.89% overall accuracy)
- Later layers achieve up to 90% accuracy in episodic reasoning tasks vs ~20-40% in early layers
- Maintains stable generalization for unseen scenarios with increased planning complexity
- Shows 3.2% improvement on plan generation tasks and 3.5% on question-answering tasks

## Why This Works (Mechanism)

### Mechanism 1: Weak-to-Strong Behavior Ratio Distillation
The method transfers episodic grounding from small to large LMs via behavior ratios (πE/πN), adjusting the large LM's output distribution via reverse KL-divergence minimization. This induces mode-seeking behavior that produces more goal-aligned action sequences. The core assumption is that behavior ratios capture generalizable episodic grounding principles rather than small-model-specific artifacts.

### Mechanism 2: MCTS-Generated Structured Preference Data
Monte Carlo Tree Search explores the action space to collect both successful and failed episodic trajectories, providing structured preference data. Successful explorations become positive samples while failed or redundant plans become negative samples for DPO training. The MCTS reward function (+2 for goal satisfaction, -0.1/timestep for irrelevant actions) reflects physical environment constraints.

### Mechanism 3: Hierarchical Processing Alignment in Later Layers
The training framework aligns episodic reasoning capabilities in deeper layers, with layer-wise probing showing later layers achieving up to ~90% accuracy. Early layers show lower accuracy (~20-40%), while later layers (35-70 in Llama2-70B) show substantial boost. This suggests task-relevant features for episodic grounding are pushed toward output layers.

## Foundational Learning

**Monte Carlo Tree Search (MCTS)**
- Why needed here: Core experience collection mechanism; understanding selection, expansion, rollout, and backpropagation is essential to grasp how positive/negative samples are generated.
- Quick check question: Can you explain how the UCB formula balances exploration vs. exploitation in the selection step?

**Direct Preference Optimization (DPO)**
- Why needed here: The method extends DPO to physical planning by incorporating structured contrasts between successful and failed experiences.
- Quick check question: How does reverse KL-divergence differ from forward KL in terms of mode-seeking vs. mode-covering behavior?

**Knowledge Distillation (Weak-to-Strong)**
- Why needed here: The paper's central contribution is transferring episodic behaviors from smaller to larger models, which is an unconventional distillation direction.
- Quick check question: Why might transferring from a weaker (smaller) model to a stronger (larger) model be more challenging than traditional strong-to-weak distillation?

## Architecture Onboarding

**Component map:** Physical Simulator (VirtualHome) -> MCTS Module -> Small LM Post-Training -> Behavior Ratio Calculator -> Large LM Distillation -> Preference Optimizer

**Critical path:** MCTS data collection → Small LM instruction tuning → Behavior ratio extraction → Large LM distillation (70B) → DPO on preference pairs → (Optional) Inference-time adjustment for 405B models

**Design tradeoffs:**
- Computational efficiency vs. direct training: 405B model uses inference-time output adjustment rather than weight optimization to avoid prohibitive costs
- Generalization vs. overfitting: Including failed attempts as negative examples mitigates overfitting to successful trajectories only
- Probe accuracy vs. generation quality: Layer-wise probing reveals potential but may not directly translate to long-form plan generation improvement

**Failure signatures:**
- Early-layer representations show ~50% accuracy drop vs. fine-tuned models → suggests weaker initial task alignment (expected, not a bug)
- Rapid accuracy degradation beyond 4 reasoning steps in baseline few-shot methods → indicates capability boundary that this method is designed to address
- Probing performance doesn't match generation performance on long-form tasks → known limitation per Section 4.5

**First 3 experiments:**
1. Reproduce MCTS data collection on a simple VirtualHome task: Verify trajectory generation, reward assignment, and positive/negative labeling logic
2. Train a small LM (e.g., 1.3B or 8B) on collected instruction data: Confirm episodic grounding capability emerges in the smaller model before distillation
3. Compare layer-wise probing accuracy between fine-tuned and post-trained LMs: Reproduce Figure 4 curve to validate the hierarchical alignment hypothesis before scaling to larger models

## Open Questions the Paper Calls Out
None

## Limitations
- The weak-to-strong distillation approach's effectiveness is not well-supported by corpus evidence, and it's unclear how well behavior ratios generalize across different model scales or task domains
- Layer-wise probing accuracy doesn't translate directly to improved long-form generation performance, creating a disconnect between classification and generative capabilities
- The 405B model uses only inference-time adjustment rather than full fine-tuning, potentially limiting the depth of episodic grounding integration

## Confidence

**High Confidence:** MCTS-based experience collection methodology and basic DPO framework; these are well-established techniques with clear mathematical formulations in the paper.

**Medium Confidence:** The weak-to-strong distillation approach's effectiveness; while the mechanism is clearly described, limited corpus evidence exists for this specific transfer direction.

**Low Confidence:** The relationship between layer-wise probing accuracy and actual generation performance; the paper acknowledges this gap, and high probing accuracy doesn't guarantee improved long-form planning outputs.

## Next Checks

1. **Behavior Ratio Transferability Test**: Train the same small LM architecture (e.g., 8B) on a different domain (non-planning task), extract behavior ratios, and apply to the same large LM. Measure whether the distilled model shows domain-specific improvements or degrades in unrelated capabilities.

2. **Layer-wise Probing vs. Generation Correlation**: Systematically vary the layer ranges used for episodic reasoning (e.g., only early layers 10-20, middle layers 30-50, late layers 60-70) and measure both probing accuracy and actual plan generation quality.

3. **Scaling Behavior Analysis**: Apply the full pipeline (MCTS → small LM → behavior ratio distillation → large LM) across multiple small LM sizes (1.3B, 3B, 8B) and measure how the distilled large LM performance varies with small model capacity.