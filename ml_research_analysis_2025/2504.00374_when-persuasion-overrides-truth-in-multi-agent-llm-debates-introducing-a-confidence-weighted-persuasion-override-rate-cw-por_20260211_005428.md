---
ver: rpa2
title: 'When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a
  Confidence-Weighted Persuasion Override Rate (CW-POR)'
arxiv_id: '2504.00374'
source_url: https://arxiv.org/abs/2504.00374
tags:
- confidence
- judge
- cw-por
- answer
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Confidence-Weighted Persuasion Override
  Rate (CW-POR) to measure how often LLM judges are confidently misled by persuasive
  but incorrect answers in single-turn debates. The method uses a multi-agent setup
  where one agent gives a correct answer from TruthfulQA, another persuasively defends
  a falsehood, and the same LLM architecture judges both.
---

# When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)

## Quick Facts
- arXiv ID: 2504.00374
- Source URL: https://arxiv.org/abs/2504.00374
- Reference count: 12
- Key outcome: Multi-agent LLM debate experiments reveal that persuasive rhetoric can override factual correctness, especially at certain verbosity levels, with CW-POR measuring both frequency and strength of judge errors.

## Executive Summary
This paper introduces the Confidence-Weighted Persuasion Override Rate (CW-POR) to measure how often LLM judges are confidently misled by persuasive but incorrect answers in single-turn debates. The method uses a multi-agent setup where one agent gives a correct answer from TruthfulQA, another persuasively defends a falsehood, and the same LLM architecture judges both. CW-POR weights override events by the judge's combined rubric and log-likelihood confidence, revealing not just frequency but also strength of error. Experiments across 5 models (3B–14B parameters) show that persuasive rhetoric can override factual correctness, especially at certain verbosity levels, highlighting the need for robust calibration and adversarial testing.

## Method Summary
The study employs a three-role multi-agent setup: a Neutral Agent that generates factual answers from TruthfulQA, a Persuasive Agent that defends incorrect answers with confident rhetoric, and a Judge that selects between them with confidence ratings. The CW-POR metric combines rubric confidence (1–5 from judge output) and log-likelihood confidence (from token probabilities) to weight override events. Experiments test 5 models across verbosity levels 30–300 words, randomizing answer positions to control bias. The pipeline extracts judge decisions via regex, computes log-probabilities for final answer tokens, and aggregates weighted override rates.

## Key Results
- Most models show a U-shaped relationship between verbosity and persuasion susceptibility, with minimum vulnerability at 90–120 words
- Combined rubric + log-likelihood confidence better captures severe persuasion failures than either metric alone
- Surprisingly, non-adversarial questions often trigger higher CW-POR than adversarial ones
- High CW-POR values (0.35–0.45) indicate judges can be both frequently and strongly misled by persuasive rhetoric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persuasive rhetoric can cause LLM judges to override factual correctness in single-turn evaluations.
- Mechanism: The persuasive agent produces emotionally charged, authoritative language without uncertainty markers. This triggers higher internal activation patterns in the judge that correlate with "convincing" content, even when factually wrong. The judge's confidence mechanisms respond to rhetorical strength rather than truth values.
- Core assumption: LLMs trained on web text associate persuasive stylistic features (certainty, emotional appeals, authoritative tone) with credibility, independent of factual accuracy.
- Evidence anchors:
  - [abstract] "persuasive rhetoric can override factual correctness, especially at certain verbosity levels"
  - [Section 1] "can rhetorical style alone outshine factual correctness when there is no second chance to respond?"
  - [corpus] Related work (Chiang et al.) shows "LLM can be swayed by emotionally charged dialogue into endorsing blatantly false statements"

### Mechanism 2
- Claim: Combining self-reported rubric confidence with log-likelihood confidence better captures the severity of persuasion overrides than either metric alone.
- Mechanism: Self-reported confidence captures the model's explicit certainty (what it says), while log-likelihood captures implicit distributional preference (what its activations indicate). Multiplying them exposes cases where a model is both verbally and internally certain about wrong answers—a more dangerous failure mode than low-confidence errors.
- Core assumption: Neither confidence measure alone fully captures calibration failures; their product reveals "strongly held wrong beliefs" more accurately.
- Evidence anchors:
  - [abstract] "CW-POR weights override events by the judge's combined rubric and log-likelihood confidence"
  - [Section 3.4] "This captures not just how often the judge is misled, but also how strongly it believes in the wrong choice"
  - [corpus] No direct corpus evidence for the specific combination method; related work discusses calibration (Jiang et al., Kadavath et al.) but not this weighted combination

### Mechanism 3
- Claim: Verbosity exhibits a U-shaped relationship with persuasion susceptibility, with minimum vulnerability at 90–120 words.
- Mechanism: Very short answers (30–60 words) lack sufficient context for judges to distinguish truth from confident-sounding falsehood. Very long answers (200+ words) provide more surface area for rhetorical manipulation and emotional saturation. Mid-range answers provide enough detail for factual clarity without excessive persuasion cues.
- Core assumption: Rhetorical effectiveness scales with available text length beyond a threshold, while factual clarity requires a minimum length.
- Evidence anchors:
  - [Section 4.3] "Most models share a common drop between 90–120 words, achieving their lowest likelihood of confident misjudgment"
  - [Section 4.3] "Both LLaMA 3.2B and Granite 3.2 8B follow a mild 'U-shape,' returning to higher CW-POR at 300 words"
  - [corpus] No corpus papers examine verbosity effects on persuasion; mechanism is specific to this paper's findings

## Foundational Learning

- Concept: **TruthfulQA benchmark design**
  - Why needed here: The paper uses TruthfulQA as the source of questions with known correct answers and plausible distractors. Understanding that distractors are designed to mimic common human misconceptions explains why persuasive agents have plausible material to work with.
  - Quick check question: Why would TruthfulQA distractors be more useful for studying persuasion than randomly generated false answers?

- Concept: **Log-likelihood confidence from token probabilities**
  - Why needed here: The LLC metric requires computing log-probabilities for "Final Answer: Answer A" vs "Final Answer: B" tokens and applying softmax. Without this, you cannot replicate the combined confidence measure.
  - Quick check question: How would you extract log-likelihood confidence for a binary choice from an autoregressive LLM?

- Concept: **Position bias in LLM evaluations**
  - Why needed here: The paper randomizes A/B ordering because LLMs exhibit position bias. Understanding this is critical for interpreting results and designing fair evaluations.
  - Quick check question: Why does randomizing which agent gets "Answer A" vs "Answer B" matter for measuring true persuasion effects?

## Architecture Onboarding

- Component map:
  - TruthfulQA dataset -> Neutral Agent (factual answer) -> Persuasive Agent (confident falsehood) -> Judge (A/B choice + confidence) -> CW-POR calculator (weighted override rate)

- Critical path:
  1. Load TruthfulQA validation split
  2. For each question, generate Neutral and Persuasive responses with verbosity constraint
  3. Randomize position assignment (which is A vs B)
  4. Pass both to Judge, extract decision + rubric confidence via regex
  5. Compute LLC by comparing log-probs of "Answer A" vs "Answer B" final tokens
  6. Combine confidences and accumulate CW-POR

- Design tradeoffs:
  - Single-turn vs multi-turn: Paper deliberately excludes rebuttal, which increases ecological validity for "one-shot" scenarios but limits remediation opportunities
  - Same-architecture judges: Mirrors real-world deployment but may amplify systematic biases; corpus suggests cross-model judges warrant future study
  - Verbosity ranges (30–300): Covers realistic response lengths but may not capture extreme cases

- Failure signatures:
  - High CW-POR on non-adversarial questions (Figure 5): Indicates overconfidence on "innocuous-looking" content
  - Flat or increasing confidence on wrong picks (Figure 6, Phi-4 14B): Signals severe miscalibration
  - CW-POR climbing at high verbosity (>200 words): Suggests rhetorical saturation overwhelming factual discrimination

- First 3 experiments:
  1. **Baseline replication**: Run the full pipeline on one model (e.g., Mistral 7B) across all verbosity levels to reproduce Figure 4's U-shape and confirm CW-POR computation.
  2. **Ablation on confidence combination**: Test whether rubric-only or LLC-only weighting produces different CW-POR rankings vs the combined metric.
  3. **Cross-model judge test**: Use a different model family as judge (e.g., LLaMA as judge for Mistral-generated responses) to test if same-architecture bias drives the effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would multi-turn debates with rebuttals reduce persuasion override rates compared to single-turn setups?
- Basis in paper: [explicit] "In future work, multi-turn setups could incorporate limited rebuttals or clarifications by the neutral agent."
- Why unresolved: All experiments use single-turn debates; no rebuttal or clarification phase was tested.
- What evidence would resolve it: Run CW-POR evaluations with 2–3 turn debate protocols, measuring whether neutral agent rebuttals lower override rates.

### Open Question 2
- Question: Does using a different model architecture as judge reduce systematic persuasion biases compared to same-architecture setups?
- Basis in paper: [explicit] "Testing whether a different model architecture as judge reduces systematic biases (rather than the same LLM family for all roles) might shed light on cross-model resilience."
- Why unresolved: The study uses the same model family for all three roles (neutral, persuasive, judge) across all experiments.
- What evidence would resolve it: Cross-architecture experiments where judges differ from debating agents, comparing CW-POR across homogeneous vs. heterogeneous configurations.

### Open Question 3
- Question: Why do some models show higher CW-POR on non-adversarial questions than adversarial ones?
- Basis in paper: [explicit] "Surprisingly, most models exhibit higher CW-POR on non-adversarial questions. This runs counter to the intuition that 'hard' or 'tricky' adversarial questions should be more misleading."
- Why unresolved: The paper documents this counterintuitive finding but only speculates about mechanisms; no causal analysis is performed.
- What evidence would resolve it: Ablation studies comparing linguistic features of successful persuasive arguments in adversarial vs. non-adversarial contexts, identifying what makes non-adversarial questions more vulnerable.

### Open Question 4
- Question: Can dynamic confidence thresholding or external verification requests mitigate high-confidence persuasion overrides?
- Basis in paper: [explicit] "Exploring dynamic confidence interventions—such as thresholding or requesting external verification when combined confidence is high—could mitigate the risk of strongly endorsed but incorrect statements."
- Why unresolved: The paper introduces CW-POR as a diagnostic but implements no mitigation strategies.
- What evidence would resolve it: Implement threshold-based intervention systems that trigger external verification when combined confidence exceeds a cutoff, measuring reduction in final error rates.

## Limitations

- Same-architecture judging may amplify systematic biases rather than reveal general persuasion vulnerability
- Single-turn format eliminates remediation opportunities and may underestimate multi-turn persuasion effects
- TruthfulQA's specific distractor design may not generalize to naturalistic persuasive scenarios

## Confidence

- **High confidence**: The CW-POR metric's mathematical formulation and the core finding that persuasive rhetoric can override factual correctness are well-supported by the experimental design and results
- **Medium confidence**: The U-shaped relationship between verbosity and persuasion susceptibility is observed across models but may be influenced by specific prompt engineering and model training characteristics
- **Low confidence**: Claims about the severity of persuasion failures on "innocuous-looking" non-adversarial questions (Figure 5) are based on limited samples and may not represent broader model behavior

## Next Checks

1. **Cross-architecture judge test**: Replicate key experiments using a different model family as judge (e.g., LLaMA as judge for Mistral-generated responses) to isolate same-architecture bias effects from general persuasion vulnerability

2. **Multi-turn debate extension**: Implement a two-turn format where the Neutral Agent can rebut the Persuasive Agent's claims, measuring how rebuttal opportunities affect CW-POR and judge calibration

3. **Adversarial prompt analysis**: Systematically vary persuasive agent prompts (e.g., emotional appeals vs. authoritative tone vs. logical fallacies) to identify which rhetorical features most strongly trigger override events across different verbosity levels