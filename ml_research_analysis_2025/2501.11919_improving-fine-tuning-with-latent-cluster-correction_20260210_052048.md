---
ver: rpa2
title: Improving Fine-Tuning with Latent Cluster Correction
arxiv_id: '2501.11919'
source_url: https://arxiv.org/abs/2501.11919
tags:
- clustering
- clusters
- 'true'
- samples
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving fine-tuning performance
  in neural networks by optimizing the formation of latent semantic clusters in hidden
  layers. The authors propose a novel method called latent cluster correction (LCC)
  that leverages the Louvain community detection algorithm and a specifically designed
  clustering loss function.
---

# Improving Fine-Tuning with Latent Cluster Correction

## Quick Facts
- arXiv ID: 2501.11919
- Source URL: https://arxiv.org/abs/2501.11919
- Reference count: 38
- Key outcome: LCC achieves up to 1.02% accuracy improvement on ResNet-18/CIFAR-100 fine-tuning, with architecture-dependent performance

## Executive Summary
This paper addresses the challenge of improving fine-tuning performance in neural networks by optimizing the formation of latent semantic clusters in hidden layers. The authors propose a novel method called latent cluster correction (LCC) that leverages the Louvain community detection algorithm and a specifically designed clustering loss function. The method identifies clusters in latent spaces using a k-nearest neighbors graph and Louvain-Leiden community detection, then applies a loss function that encourages misclustered samples to move toward their target clusters. When applied to fine-tuning ResNet-18 on CIFAR-100, LCC achieved accuracy gains of up to 1.02% compared to baseline fine-tuning. However, results varied significantly across architectures, with negligible gains for AlexNet.

## Method Summary
LCC operates by first extracting latent representations from the target layer of a pre-trained model, then constructing a k-NN graph of these representations. The Louvain-Leiden algorithm identifies communities in this graph, which are then matched to ground-truth labels using a max-flow optimization. Samples whose community assignment disagrees with their true label are identified as "misclustered," and a clustering loss is applied to pull these samples toward the centroid of their k nearest correctly-clustered neighbors from the same class. This clustering loss is combined with cross-entropy loss during fine-tuning, with the weight of the clustering term being a key hyperparameter. The process repeats at the start of each epoch with updated clusters based on the current latent representations.

## Key Results
- LCC achieved up to 1.02% accuracy improvement on ResNet-18 fine-tuned on CIFAR-100
- Accuracy gains were architecture-dependent, with minimal improvement for AlexNet
- The method showed significant computational overhead due to Louvain community detection
- Optimal performance required careful tuning of clustering loss weight and neighbor count parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Community detection on k-NN graphs of latent representations can discover semantic clusters without requiring prior knowledge of cluster count or geometry.
- **Mechanism:** The Louvain-Leiden algorithm maximizes modularity on the k-NN graph, iteratively merging nodes into communities that maximize within-community connectivity. This produces clusters that reflect the intrinsic structure of the latent space, including cases where a single class splits into multiple clusters.
- **Core assumption:** Latent representations of same-class samples form dense regions that are discoverable via nearest-neighbor connectivity patterns.
- **Evidence anchors:** [abstract] "...using the Louvain community detection algorithm...demonstrate the viability of this process" [Section II, Page 2-3] "Our method, k-NN-Louvain, first constructs the k-nearest neighbors (k-NN) graph of the dataset of LRs, and then applies the Louvain method to find communities"
- **Break condition:** If latent representations are uniformly distributed or lack local density structure, k-NN graphs will not yield meaningful communities.

### Mechanism 2
- **Claim:** Misclustered samples have lower classification accuracy than correctly clustered samples, and correcting their latent positions can improve overall accuracy.
- **Mechanism:** After cluster-to-label assignment via max-flow optimization, samples whose cluster assignment disagrees with their true label are identified as "misclustered." The clustering loss pulls these samples toward the centroid of their k nearest correctly-clustered neighbors from the same class.
- **Core assumption:** Correctly clustered neighbors exist and their centroid represents a desirable target position in latent space.
- **Evidence anchors:** [Section IV.C, Page 5] "acc(X_k,MC, y_k,MC) < acc(X,y) < acc(X_k,CC, y_k,CC)...The discrepency increases with k" [Section III.D, Page 4] "The clustering loss...is the mean of all these terms" where terms are distances to correction targets
- **Break condition:** If no correctly clustered neighbors exist within a class (N_corr = 0), the loss is undefined for that class.

### Mechanism 3
- **Claim:** The clustering loss weight and layer selection determine whether LCC improves or degrades performance, with architecture-dependent optima.
- **Mechanism:** The total loss combines cross-entropy with weighted clustering loss (w × L_clst). Applied at the classifier head or penultimate layer. Higher weights (10^-2) and more neighbors (500) improved ResNet-18 but had mixed effects on AlexNet.
- **Core assumption:** The chosen latent layer has formed class-relevant cluster structure during pretraining.
- **Evidence anchors:** [Section IV.B, Page 5] ResNet-18 best: Head, w=10^-4, k=500 → +1.02%; AlexNet best: Head, w=10^-2, k=50 → +0.23% [Section V.B, Page 6] "The efficacy of LCC is expected to be highly dependent on the choice of LS"
- **Break condition:** Applying LCC to early feature extraction layers where clusters have not yet formed will likely fail or harm performance.

## Foundational Learning

- **Concept: k-Nearest Neighbor Graphs**
  - **Why needed here:** The method constructs k-NN graphs on latent representations as the substrate for community detection. Understanding graph construction, edge weighting (Gaussian RBF), and complexity (O(N) with KD-tree for construction) is essential.
  - **Quick check question:** Given N samples in d dimensions, what is the time complexity of constructing a k-NN graph using a ball-tree index?

- **Concept: Modularity-Based Community Detection (Louvain Algorithm)**
  - **Why needed here:** LCC relies on the Louvain-Leiden algorithm to partition the k-NN graph. Understanding modularity (Q), the iterative merge process, and worst-case complexity (O(kN²)) helps anticipate computational bottlenecks.
  - **Quick check question:** Why does the Louvain algorithm sometimes produce sparse communities, and how does Leiden address this?

- **Concept: Max-Flow / Assignment Problems**
  - **Why needed here:** Matching discovered clusters to true labels uses a discrete max-flow-max-weight formulation. The supersource/supersink construction and capacity constraints enforce one-to-many matching.
  - **Quick check question:** In the flow network F = (V', E'), what does a capacity of 1 on edges from cluster nodes c to the supersink ⊥ enforce?

## Architecture Onboarding

- **Component map:**
  Input Batch X → Forward Pass → Latent Representations Z (at layer k)
  ↓
  Build k-NN Graph on Z
  ↓
  Louvain Community Detection → y_clst
  ↓
  Max-Flow Cluster-Label Assignment → α
  ↓
  Identify Misclustered (MC) & Correctible Samples
  ↓
  For each correctible z_i: compute centroid z̄_i from k CC neighbors
  ↓
  L_clst = (1/√d·N_corr) Σ ||z_i - z̄_i||
  ↓
  Total Loss = L_CE + w · L_clst

- **Critical path:** The per-epoch overhead is dominated by: (1) full dataset forward pass to extract Z, (2) k-NN index construction O(N), (3) Louvain iterations O(kN²) worst-case, (4) per-class k-NN indices for CC samples O(N/n_true). Memory constraint: all Z must fit in memory.

- **Design tradeoffs:**
  - **Exact vs. Approximate Louvain Loss:** Section VI.B proposes using one random CC sample per cluster instead of k neighbors, reducing index size and query cost but potentially less accurate correction targets.
  - **k-NN Pressure vs. Louvain:** Section VI.A proposes a conductance-based alternative with better sparse matrix performance (O(kN) per iteration vs. O(kN²)).
  - **Layer depth:** Deeper layers show better cluster formation (Fig. 1) but also larger discrepancy between MC/CC accuracy (Fig. 3-4), suggesting higher correction potential but also higher risk.

- **Failure signatures:**
  - Accuracy degradation (AlexNet, some configurations): suggests clusters are not well-formed or loss weight is too aggressive.
  - Memory exhaustion: storing full Z dataset in memory for large N or high d.
  - Non-termination: Louvain iterations may not converge quickly on poorly structured graphs.
  - Empty loss (N_corr = 0): no correctible samples found, clustering loss is undefined.

- **First 3 experiments:**
  1. **Baseline validation on ResNet-18/CIFAR-100:** Replicate the best reported configuration (Head layer, w=10^-4, k=500, 1 warmup epoch) to verify +1% accuracy gain. Log MC/CC accuracy curves to confirm inequality pattern from Section IV.C.
  2. **Ablation on layer depth:** Compare LCC applied at Head vs. 2nd-to-last vs. 3rd-to-last layer on ResNet-18. Expect diminishing returns or negative impact at shallower layers. Measure per-epoch Louvain runtime to quantify computational cost.
  3. **Approximate loss comparison:** Implement the approximate Louvain loss (Section VI.B, single random CC sample per cluster) and compare accuracy vs. speed tradeoff against exact LCC on the same ResNet-18 setup. Target: <10% accuracy degradation for >50% speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does LCC substantially improve ResNet-18 accuracy but show negligible impact on AlexNet?
- Basis in paper: [explicit] "We plan to... investigate the fundamental reason of the lukewarm benchmark results of AlexNet. This latter task is expected to provide further insights into the latent space structures of deep neural networks."
- Why unresolved: The authors observe the architectural dependency but do not investigate the underlying causes, noting only that "the impact of LCC is highly model-dependent."
- What evidence would resolve it: Comparative analysis of latent space structures (cluster compactness, manifold geometry) between ResNet-18 and AlexNet; ablation studies varying architectural components.

### Open Question 2
- Question: Can the proposed k-NN pressure clustering algorithm replace Louvain while maintaining accuracy gains and reducing computational cost?
- Basis in paper: [inferred] Section VI.A proposes k-NN pressure as "a cheaper method to cluster" with advantages for sparse graphs, but provides no empirical validation against Louvain.
- Why unresolved: The algorithm is fully specified mathematically but no experiments compare its clustering quality or downstream classification accuracy to the Louvain-based approach.
- What evidence would resolve it: Benchmarks comparing k-NN pressure vs. Louvain on clustering metrics, computational time/memory, and final classification accuracy on CIFAR-100/ResNet-18.

### Open Question 3
- Question: Does orthogonal correction (projecting correction vectors onto the tangent space orthogonal complement) accelerate or improve cluster separation?
- Basis in paper: [explicit] "It could be advantageous to first project v onto T_z M^⊥, the subspace orthogonal to the tangent space of M at z, in order to accelerate the separation of z from M."
- Why unresolved: The approach is proposed conceptually with a method for approximating tangent spaces via PCA, but never implemented or tested.
- What evidence would resolve it: Experiments comparing standard LCC vs. orthogonal correction LCC in terms of convergence speed, cluster quality metrics, and final accuracy.

### Open Question 4
- Question: Can LCC scale effectively to larger datasets (ImageNet) and modern architectures (Vision Transformers)?
- Basis in paper: [explicit] "We expect ongoing optimization efforts to reduce this cost and bring models with larger latent dimensions (such as vision transformers) and larger datasets (such as ImageNet) within reach."
- Why unresolved: Computational constraints prevented testing beyond CIFAR-100; the O(kN²) worst-case Louvain complexity and memory requirements for storing all latent representations are prohibitive at scale.
- What evidence would resolve it: Implementation of proposed optimizations (k-NN pressure, approximate Louvain loss) followed by experiments on ImageNet with ViT or larger ResNet variants.

## Limitations
- Substantial computational overhead due to Louvain community detection with O(kN²) worst-case complexity
- Architecture-dependent performance with minimal gains on AlexNet despite success on ResNet-18
- No specification of batch size or image preprocessing details limiting reproducibility
- Results heavily dependent on careful hyperparameter tuning of layer selection and loss weights

## Confidence

- **High Confidence:** Basic mechanism of using Louvain clustering on k-NN graphs is sound; community detection theory is well-established
- **Medium Confidence:** Claims about accuracy improvements on ResNet-18 are supported by reported results but lack ablation studies for hyperparameter sensitivity
- **Low Confidence:** Generalization claims across architectures are weak due to only two models tested with inconsistent results

## Next Checks

1. **Reproduce baseline results:** Implement exact LCC configuration on ResNet-18/CIFAR-100 and verify +1% accuracy gain. Monitor MC/CC accuracy patterns to confirm theoretical claims.
2. **Layer depth ablation study:** Systematically test LCC at different latent layers on ResNet-18 to identify optimal layer and quantify computational tradeoffs.
3. **Approximate method validation:** Implement Section VI.B's approximate Louvain loss and measure accuracy-speed tradeoff compared to exact method under identical conditions.