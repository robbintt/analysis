---
ver: rpa2
title: Hallucination, reliability, and the role of generative AI in science
arxiv_id: '2504.08526'
source_url: https://arxiv.org/abs/2504.08526
tags:
- generative
- data
- training
- hallucination
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the epistemic threat posed by hallucinations
  in generative AI models used for scientific inference. It argues that hallucinations,
  while inevitable due to the high-dimensional nature of generative outputs, can be
  effectively managed by embedding models in theory-informed workflows rather than
  relying on brute inductivism.
---

# Hallucination, reliability, and the role of generative AI in science

## Quick Facts
- arXiv ID: 2504.08526
- Source URL: https://arxiv.org/abs/2504.08526
- Authors: Charles Rathkopf
- Reference count: 22
- Primary result: Scientific generative AI can be reliable when embedded in theory-guided workflows that use violation loss functions and confidence-based error screening to manage hallucinations

## Executive Summary
This paper argues that hallucinations in generative AI models pose an epistemic threat to scientific inference, but can be effectively managed through a phenomenon-centric framework rather than brute data-centric filtering. The key insight is that hallucinations are inevitable in high-dimensional generative outputs, but mature scientific domains with robust theoretical frameworks can discipline these models through theory-guided training and confidence-based error screening. When generative AI models are embedded in such workflows—exemplified by AlphaFold for protein structure prediction and GenCast for weather forecasting—they can support reliable scientific inference despite their opacity, as errors are converted into manageable uncertainties rather than sources of false belief.

## Method Summary
The paper synthesizes a framework for managing hallucinations in scientific generative AI through three core mechanisms: theory-guided training with violation loss functions that penalize physically implausible outputs, confidence-based error screening using internal consistency metrics like pLDDT scores or ensemble dispersion, and embedding models in mature scientific domains with robust theoretical frameworks. The method involves curating training data with domain theory, designing loss functions that encode physical/biological constraints, training generative models with stochastic sampling, computing confidence scores or ensemble spread at inference, filtering outputs below confidence thresholds, and validating retained outputs experimentally. The framework emphasizes that successful implementation requires both mature domain theory to constrain valid solutions and calibration procedures to ensure confidence metrics track actual reliability.

## Key Results
- Theory-guided training with violation loss functions significantly reduces physically implausible outputs in protein structure prediction
- Confidence-based screening via pLDDT scores or ensemble dispersion effectively identifies unreliable outputs without ground-truth access
- GenCast achieves well-calibrated probabilistic forecasts with spread-skill ratio near 1, demonstrating reliable uncertainty estimation in weather prediction

## Why This Works (Mechanism)

### Mechanism 1: Theory-guided Training
- Claim: Embedding domain-specific theoretical constraints into training reduces hallucination frequency and severity
- Mechanism: Violation loss functions penalize outputs that breach known physical laws (steric clashes, implausible bond angles, conservation violations)
- Core assumption: Domain theory is sufficiently mature, quantitative, and correct to constrain valid solutions
- Evidence anchors: AlphaFold's use of violation loss functions to penalize physically unrealistic torsional angles; related work on physics-informed ML mentioned but not tested
- Break condition: Domain theory is qualitative, incomplete, or incorrect

### Mechanism 2: Confidence-based Error Screening
- Claim: Internal consistency metrics can identify unreliable outputs without ground-truth access at inference time
- Mechanism: Generate multiple stochastic samples from same input; measure local agreement (pLDDT) or ensemble dispersion
- Core assumption: Hallucinations are sample-dependent rather than systematic across stochastic runs
- Evidence anchors: pLDDT scores reflect degree of local agreement among samples; hallucinations not uniform across stochastic samples
- Break condition: Model produces systematically wrong high-confidence outputs

### Mechanism 3: Ensemble-based Uncertainty Calibration
- Claim: Forecast dispersion correlates with empirical error, enabling calibrated probabilistic inference
- Mechanism: Generate ensemble of trajectories via stochastic sampling; compute spread-skill ratio
- Core assumption: Chaotic systems exhibit predictable relationship between ensemble variance and actual forecast error
- Evidence anchors: GenCast's spread-skill ratio near 1 indicates well-matched uncertainty estimates; related work on ML surrogates for reliability analysis
- Break condition: Training distribution lacks rare events; validation feedback cycles too long for calibration

## Foundational Learning

- **Computational Reliabilism**:
  - Why needed: Provides epistemic framework for trusting opaque models based on workflow reliability rather than parameter interpretability
  - Quick check: Can you explain why AlphaFold's trustworthiness depends on its workflow design, not its internal transparency?

- **Spread-Skill Ratio (SSR)**:
  - Why needed: Core metric for validating whether ensemble uncertainty estimates are calibrated
  - Quick check: What does SSR = 1 indicate? What would SSR >> 1 or SSR << 1 suggest about model calibration?

- **Phenomenon-centric vs. Data-centric Assessment**:
  - Why needed: Conceptual shift that enables discovery while managing hallucination; deviations from training data may signal discoveries, not errors
  - Quick check: Why would filtering all outputs that deviate from training data eliminate scientific discoveries?

## Architecture Onboarding

- **Component map**: Training data (domain-theory curated) -> Loss functions (task loss + violation penalties) -> Model (diffusion-based generative) -> Inference (stochastic sampling) -> Output (prediction + confidence signal) -> Post-processing (threshold filtering + refinement) -> Workflow (experimental validation + feedback)

- **Critical path**:
  1. Curate training corpus with domain theory (experimentally validated structures, reanalysis data)
  2. Design violation loss terms encoding physical/biological constraints
  3. Train generative model with constrained optimization
  4. At inference, generate N stochastic samples per input
  5. Compute internal consistency (pLDDT) or ensemble spread
  6. Filter outputs below confidence threshold before downstream use
  7. Validate retained outputs experimentally; feed results back into refinement

- **Design tradeoffs**:
  - Higher confidence threshold → fewer hallucinations but more missed novel discoveries
  - Larger ensembles → better uncertainty estimation but higher compute cost
  - Stronger violation penalties → more physically plausible outputs but risk of over-constraining
  - Cross-distillation → improved error profiles but dependency on auxiliary model quality

- **Failure signatures**:
  - High-confidence hallucination: pLDDT/spread looks good but output fails experimental validation (miscalibration)
  - Consistent low confidence on valid regions: model over-cautious, flags genuine uncertainty as unreliability
  - Ensemble mode collapse: insufficient diversity in samples, spread underestimates true uncertainty
  - Physical violation in final output: violation losses insufficient, post-processing refinement required

- **First 3 experiments**:
  1. **Calibration curve**: Plot pLDDT/spread against experimental validation rate across threshold bins
  2. **Ablation study**: Remove violation loss terms and measure hallucination rate increase
  3. **SSR stability**: Compute spread-skill ratio across forecast horizons and input regimes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can general-purpose architectures achieve the same scientific reliability as domain-specific models when embedded in theory-guided workflows?
- Basis in paper: The author asks whether models must be domain-specific or whether general-purpose architectures could achieve similar reliability when embedded in appropriate workflows
- Why unresolved: Current successful implementations like AlphaFold are domain-specific, and it remains unproven whether general-purpose models can effectively integrate necessary theoretical scaffolding
- What evidence would resolve it: Comparative studies evaluating general-purpose models against specialized scientific models on identical inference tasks

### Open Question 2
- Question: Can chemical or physical representations be effectively treated as additional modalities within multi-modal generative models?
- Basis in paper: Page 25 states this remains an open question worth exploring
- Why unresolved: Unknown if integrating physical constraints into general latent space yields same epistemic benefits as hard-coded architectural constraints
- What evidence would resolve it: Demonstrations of multi-modal models successfully synthesizing physical laws with other data types without violating conservation principles

### Open Question 3
- Question: What alternative strategies are required to ensure reliability in domains where background theory is absent or immature?
- Basis in paper: The framework depends on "theoretically mature domains," suggesting comparable reliability may be unattainable elsewhere
- Why unresolved: Proposed framework relies on theory to discipline training and interpret outputs; this mechanism fails where theoretical knowledge is qualitative or sparse
- What evidence would resolve it: Successful deployment of generative models in scientific fields lacking strong theoretical foundations using novel error-screening methods

## Limitations
- Domain transferability uncertainty: Success relies heavily on mature theoretical frameworks, which most scientific domains lack
- Calibration assumption may break down in chaotic or rare-event regimes where training data is sparse
- Confidence-based screening assumes hallucinations are idiosyncratic across samples, but systematic biases could evade detection

## Confidence

**High Confidence**: Theory-guided training with violation loss functions reduces physically implausible outputs; ensemble-based uncertainty calibration works in domains with sufficient historical data

**Medium Confidence**: Confidence-based screening reliably identifies hallucinations in novel domains; internal consistency metrics correlate with experimental validation rates

**Low Confidence**: Framework scales to domains lacking mature quantitative theories; confidence-based screening prevents all harmful hallucinations without excessive false positives; ensemble methods remain calibrated in long-range or rare-event forecasting

## Next Checks
1. **Domain Transferability Test**: Apply workflow to a scientific domain with moderate theoretical maturity and measure hallucination reduction vs baseline models

2. **Calibration Robustness Analysis**: Systematically degrade training data quality/sparsity and measure how quickly ensemble calibration degrades

3. **Novelty-Preservation Experiment**: Design benchmark where some test inputs represent valid but rare phenomena outside training distribution; measure false negative rate when confidence thresholds filter outputs