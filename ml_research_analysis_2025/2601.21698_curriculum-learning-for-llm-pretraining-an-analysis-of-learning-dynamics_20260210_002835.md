---
ver: rpa2
title: 'Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics'
arxiv_id: '2601.21698'
source_url: https://arxiv.org/abs/2601.21698
tags:
- training
- learning
- curriculum
- random
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how pretraining data order affects LLM\
  \ learning dynamics and stability. Using Pythia models, we combine HMM-based phase\
  \ analysis with gradient noise scale and singular entropy diagnostics to compare\
  \ three curricula\u2014Age-of-Acquisition, word frequency, and Verb Variation (VV)\u2014\
  against Random ordering."
---

# Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics

## Quick Facts
- **arXiv ID**: 2601.21698
- **Source URL**: https://arxiv.org/abs/2601.21698
- **Reference count**: 40
- **One-line primary result**: Curriculum orderings reduce gradient noise scale and singular entropy in smaller models, improving late-phase optimization stability.

## Executive Summary
This work investigates how pretraining data order affects LLM learning dynamics and stability. Using Pythia models, we combine HMM-based phase analysis with gradient noise scale and singular entropy diagnostics to compare three curricula—Age-of-Acquisition, word frequency, and Verb Variation (VV)—against Random ordering. The joint HMM analysis confirms that all orderings share latent training phases; differences arise from which data appears within these phases. For smaller models (14M–70M), Random ordering produces higher gradient noise scale; for models up to 160M, Random also produces higher singular entropy, corresponding to noisier optimization and late-stage spectral collapse in the language modeling head. Curriculum-based orderings reduce both quantities and are associated with less late-stage degradation consistent with softmax-bottleneck saturation. At larger scales, these gaps narrow and curriculum gains shrink. Our theoretical framework formalizes a mechanism linking difficulty pacing to optimization stability: by controlling stochastic-gradient variance through structured data exposure, curricula can maintain tighter stability bounds than uniform sampling. The empirical results support this view—curricula reduce gradient noise scale and singular entropy in smaller models, and these reductions coincide with improved late-phase accuracy. This suggests a practical guideline: curricula are most valuable when model capacity is constrained and late-stage saturation is a concern; at larger scales where capacity suffices, random ordering performs comparably.

## Method Summary
The study compares four pretraining data orderings—Age-of-Acquisition (AoA), word frequency, Verb Variation (VV), and Random—using Pythia architectures (14M–410M parameters) trained on The Pile corpus for 300B tokens. Curriculum samples are sorted by difficulty metrics and packed into 2048-token sequences. Learning dynamics are analyzed using a joint 5-state Gaussian HMM on 14 weight metrics, gradient noise scale (GNS), and singular entropy of the output head. Zero-shot benchmarks (ARC, PIQA, SciQ, LogiQA, LAMBADA, WinoGrande, WSC) and BLiMP probes measure downstream performance. The theoretical framework links curriculum difficulty pacing to optimization stability via controlled stochastic-gradient variance.

## Key Results
- Smaller models (14M–70M) under Random ordering exhibit higher gradient noise scale and singular entropy than curriculum orderings.
- Curriculum orderings reduce both metrics and correlate with less late-phase spectral collapse in the language modeling head.
- For models up to 160M, curriculum orderings improve late-phase accuracy by mitigating softmax-bottleneck saturation.
- At larger scales (≥160M), curriculum benefits diminish and gaps narrow compared to Random ordering.

## Why This Works (Mechanism)
Curriculum learning improves optimization stability by controlling stochastic-gradient variance through structured data exposure. By pacing difficulty from easy to hard, curricula maintain tighter stability bounds than uniform sampling, reducing gradient noise scale and singular entropy. This mechanism is particularly effective in smaller models where capacity constraints make late-stage saturation a concern.

## Foundational Learning
- **Gradient noise scale (GNS)**: Measures batch gradient variance relative to full-batch gradient; needed to diagnose optimization stability. Quick check: Compute GNS at multiple checkpoints to track variance trends.
- **Singular entropy**: Quantifies spectral collapse in weight matrices; needed to detect late-stage optimization degradation. Quick check: Monitor singular entropy of output head across training phases.
- **Hidden Markov Models (HMMs)**: Identify latent training phases from weight metric trajectories; needed to compare learning dynamics across orderings. Quick check: Fit joint 5-state HMM to ensure phase alignment across orderings.
- **Curriculum learning**: Orders training data by difficulty to improve optimization; needed to control gradient variance and stability. Quick check: Verify curriculum scores correlate with sample loss.
- **Softmax-bottleneck saturation**: Occurs when output layer capacity limits model expressiveness; needed to explain late-stage degradation. Quick check: Track output head spectral properties for collapse signs.
- **Sequence packing**: Groups variable-length samples into fixed-size batches; needed for efficient training on The Pile. Quick check: Confirm all orderings use identical packing procedure.

## Architecture Onboarding
- **Component map**: Data → Curriculum Scoring → Sequence Packing → Model → Training → Checkpoints → Diagnostics (GNS, Singular Entropy, HMM) → Benchmarks
- **Critical path**: Curriculum scoring → sequence packing → training → diagnostics extraction → joint HMM fitting → benchmark evaluation
- **Design tradeoffs**: Curriculum ordering reduces gradient variance but requires difficulty scoring overhead; Random ordering is simpler but noisier for small models.
- **Failure signatures**: No GNS/singular entropy differences across orderings → likely model scale too large or training insufficient; HMM phases misaligned → inconsistent observation sets or fitting parameters.
- **First experiments**: (1) Compute curriculum scores and verify correlation with sample loss; (2) Train 14M model with Random and AoA orderings, extract GNS at checkpoints; (3) Fit joint HMM to confirm phase alignment across orderings.

## Open Questions the Paper Calls Out
None

## Limitations
- GNS and singular entropy measurements rely on specific checkpoint intervals not fully specified, creating potential variation in diagnostic trends.
- The theoretical stability bound linking curriculum difficulty pacing to GNS is qualitative, lacking exact mathematical derivation.
- Model scale progression beyond 160M is sparsely reported; claims of vanishing curriculum benefits are extrapolated from limited data.
- Seed variation is minimal, raising concerns about statistical robustness of observed differences.

## Confidence
- **High**: Empirical observation that smaller models (14M-70M) under Random ordering exhibit higher GNS and singular entropy; curriculum orderings reduce these metrics and correlate with less late-phase spectral collapse.
- **Medium**: Interpretation that curriculum orderings improve late-phase accuracy by mitigating softmax-bottleneck saturation; this mechanism is plausible but not directly measured.
- **Medium**: Claim that curriculum benefits are model-capacity-dependent, with diminishing returns above 160M; supported by data but with limited scale coverage.

## Next Checks
1. Replicate GNS and singular entropy trends for 70M and 160M models across all four orderings with at least 3 seeds to confirm statistical significance.
2. Verify the joint HMM phase alignment by reproducing the 5-state fit and checking that phases remain consistent across orderings with identical 14-metric observation sets.
3. Test the theoretical stability bound by measuring actual optimization step variance (‖G‖) alongside GNS to confirm curriculum-induced reductions in stochastic gradient variance.