---
ver: rpa2
title: 'Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media
  Chatter Use Case'
arxiv_id: '2507.10803'
source_url: https://arxiv.org/abs/2507.10803
tags:
- xylazine
- posts
- thematic
- themes
- post
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case

## Quick Facts
- arXiv ID: 2507.10803
- Source URL: https://arxiv.org/abs/2507.10803
- Reference count: 26
- Primary result: GPT-4o with two-shot prompting achieved 90.9% accuracy and F1-score of 0.71 for binary classification of 13 xylazine wound management themes

## Executive Summary
This study demonstrates that large language models can automate thematic analysis of social media content about xylazine-associated wounds through binary classification decomposition and few-shot calibration. The approach improves upon traditional multi-label classification by modeling each of 13 themes as independent binary tasks, reducing cognitive load on the model. When calibrated with two labeled examples per theme, GPT-4o achieved high accuracy while maintaining moderate precision and recall, showing promise for scaling expert-driven thematic analysis in public health surveillance.

## Method Summary
The study collected Reddit posts using PRAW with xylazine-related keywords, creating two datasets: DS1 (n=286, 2014-2023) for development and DS2 (n=236, 2024-Mar 2025) for validation. Domain experts first identified 12 themes through manual analysis of a small sample, then annotated the full datasets. The authors modeled thematic analysis as 13 binary classification tasks rather than multi-label classification, testing zero-shot, single-shot, and few-shot prompting across five LLMs. The final prompt template asked for binary outputs (relevant=1, not relevant=0) for each theme, optimized through iterative refinement on DS1 before validation on DS2.

## Key Results
- GPT-4o with two-shot prompting achieved the best performance: accuracy 90.9%, F1-score 0.71
- Binary classification decomposition outperformed multi-label approaches by reducing cognitive load on models
- Zero-shot prompting consistently over-predicted high-frequency themes and missed rare categories like "Stigma"
- Open-source model DeepSeek-V3 showed competitive performance (F1: 0.671) but lagged behind GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: Binary Classification Decomposition
Decomposing multi-theme classification into independent binary tasks improves LLM performance by reducing cognitive load and preventing cascade errors. Instead of simultaneous multi-label output, each theme is evaluated separately as binary classification. This works when themes are sufficiently independent that treating them as separate decisions does not lose important cross-theme correlations.

### Mechanism 2: Few-Shot Calibration Against Over-Prediction
Zero-shot prompting produces inflated theme prevalence estimates due to overly broad relevance criteria. Providing 1-2 labeled examples per theme anchors the model to expert-labeled boundaries, reducing false positive rates. This calibration is effective when few-shot examples are representative of the true distribution and do not introduce new biases.

### Mechanism 3: Human-in-the-Loop Theme Definition
LLMs can reliably classify pre-defined themes but cannot perform purely inductive theme discovery without human specification. Domain experts first identify themes through manual analysis of a small sample, then LLMs scale classification across larger corpora. This works when expert-defined themes remain stable and applicable across the full dataset.

## Foundational Learning

- **Thematic Analysis vs. Topic Modeling**: Thematic analysis is interpretive and domain-informed; topic modeling is statistical and unsupervised. The paper positions LLMs as tools for scaling expert-driven thematic analysis, not replacing it. Quick check: Can you explain why the authors chose binary classification over topic modeling approaches like LDA?

- **Zero-Shot vs. Few-Shot Prompting**: The performance gap between zero-shot and few-shot is central to the paper's findings. Zero-shot prompts lack reference points, causing models to apply overly broad relevance criteria. Quick check: What specific failure mode did zero-shot prompting exhibit in this study?

- **Precision-Recall Tradeoffs in Qualitative Coding**: The paper reports moderate precision (0.76) and recall (0.66), meaning false positives and false negatives persist. Downstream use cases must account for this uncertainty. Quick check: If your use case requires catching all mentions of "wound management" (high recall) but can tolerate some false positives, how would you adjust the classification threshold?

## Architecture Onboarding

- **Component map**: Data extraction (Reddit API → keyword filtering → deduplication) → Expert annotation (manual theme identification → consolidation → full annotation) → Prompt engineering (binary classification template → iterative refinement → few-shot example selection) → Model evaluation (run 5 LLMs → compute metrics) → Validation (apply best model to DS2 → compare distributions)

- **Critical path**: Expert theme definition → prompt template design → few-shot example selection → model selection → validation on held-out data

- **Design tradeoffs**: Closed-source (GPT-4o) vs. open-source (DeepSeekV3): GPT-4o performed best but open-source enables local hosting; Single binary prompt vs. batched prompts: final prompt asks for all 13 binary outputs in one call vs. 13 separate API calls; Number of few-shot examples: two-shot performed best; more examples may over-constrain or exceed context limits

- **Failure signatures**: Zero-shot over-predicts high-frequency themes; rare themes (e.g., "Stigma") may be completely missed; adding contextual information reduced performance; theme distributions drift across time periods

- **First 3 experiments**: 1) Baseline replication: re-run binary classification prompt on DS1 with GPT-4o zero-shot vs. two-shot; 2) Threshold tuning: sweep classification thresholds to optimize for recall on high-priority themes; 3) Temporal stability test: train on DS1, test on DS2; measure per-theme F1 degradation

## Open Questions the Paper Calls Out

- Can frameworks be designed for purely unsupervised thematic categorization that identify emergent themes without predetermined categories? The study required human experts to define 12 themes upfront; LLMs only classified posts into these pre-existing categories.

- Can iterative feedback mechanisms meaningfully improve LLM agreement with human experts in thematic analysis? The study tested only zero-, single-, and two-shot prompting without feedback loops; precision and recall remained moderate.

- Can refined prompt engineering close the performance gap between open-source and closed-source models for thematic analysis? GPT-4o outperformed DeepSeek-V3, but both models used identical prompt structures; open-source models may benefit from prompt strategies tailored to their architectures.

## Limitations

- Binary classification decomposition assumes themes are sufficiently independent, but co-occurrence patterns are not explicitly modeled or evaluated
- Few-shot calibration depends on representative examples, but the paper does not report how examples were selected or whether they introduced new biases
- Temporal stability of themes is not rigorously tested—DS1 and DS2 show different top themes, but no statistical analysis of drift is provided

## Confidence

- **High confidence**: Binary classification decomposition improves LLM performance compared to multi-label framing
- **Medium confidence**: Few-shot prompting calibrates outputs toward expert distributions
- **Medium confidence**: LLMs cannot perform purely inductive theme discovery

## Next Checks

1. **Co-occurrence validation**: Analyze the correlation matrix of theme predictions to determine whether binary decomposition misses important thematic relationships

2. **Example selection audit**: Conduct ablation studies varying the few-shot examples per theme to quantify how selection methodology affects calibration quality

3. **Temporal drift analysis**: Apply statistical tests comparing theme distributions across DS1, DS2, and DS2F to quantify when re-annotation becomes necessary