---
ver: rpa2
title: 'mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced
  Languages'
arxiv_id: '2507.16011'
source_url: https://arxiv.org/abs/2507.16011
tags:
- context
- languages
- tail
- entity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses multilingual knowledge graph construction (mKGC)
  for low-resourced languages (Tigrinya, Amharic) by reformulating it as a multilingual
  question-answering task. The proposed method, mRAKL, uses a retrieval-augmented
  generation pipeline that retrieves context from monolingual Wikipedia articles and
  generates answers with a fine-tuned multilingual language model (AfriTeVa).
---

# mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages

## Quick Facts
- **arXiv ID**: 2507.16011
- **Source URL**: https://arxiv.org/abs/2507.16011
- **Reference count**: 34
- **Primary result**: mRAKL improves mKGC accuracy by 4.92% for Tigrinya and 8.79% for Amharic using retrieval-augmented generation from monolingual Wikipedia

## Executive Summary
This work addresses the challenge of constructing knowledge graphs for low-resourced languages (Tigrinya, Amharic) by reformulating multilingual knowledge graph construction as a multilingual question-answering task. The proposed mRAKL system uses a retrieval-augmented generation pipeline that retrieves context from monolingual Wikipedia articles and generates answers using a fine-tuned multilingual language model (AfriTeVa). By leveraging cross-lingual entity alignment and unstructured data, mRAKL achieves significant accuracy improvements over baselines. The approach demonstrates that multilingual and cross-lingual contexts can substantially benefit low-resourced languages where labeled structured data is scarce.

## Method Summary
mRAKL reformulates multilingual knowledge graph construction (mKGC) as a multilingual question-answering task. The system retrieves relevant context from monolingual Wikipedia articles for each entity, then uses a fine-tuned multilingual language model (AfriTeVa) to generate answers. This retrieval-augmented generation pipeline leverages cross-lingual entity alignment to connect information across languages. The approach is specifically designed for low-resourced languages where structured metadata is limited, using unstructured Wikipedia data as the primary knowledge source. The method combines retrieval quality with generation capabilities to construct knowledge graphs without requiring extensive labeled data in the target languages.

## Key Results
- mRAKL achieves 4.92 percentage point improvement over baselines for Tigrinya
- mRAKL achieves 8.79 percentage point improvement over baselines for Amharic
- Outperforms closed-domain approaches that rely on structured metadata

## Why This Works (Mechanism)
mRAKL works by addressing the fundamental challenge of low-resourced languages: the scarcity of labeled structured data. By reformulating mKGC as a question-answering task, the system can leverage the vast amount of unstructured information available in Wikipedia articles. The retrieval component ensures that the generation model has access to relevant context, while the fine-tuned multilingual model can generate accurate answers even for languages with limited training data. The cross-lingual entity alignment allows the system to connect information across languages, effectively expanding the knowledge base available for low-resourced languages.

## Foundational Learning
- **Multilingual Knowledge Graph Construction**: The process of creating structured knowledge representations that span multiple languages, essential for global information systems
  - *Why needed*: Enables cross-lingual knowledge sharing and access
  - *Quick check*: Can the system handle entities with names in multiple scripts/languages?
- **Retrieval-Augmented Generation**: A pipeline that first retrieves relevant context then uses it to inform generation, combining information retrieval with language modeling
  - *Why needed*: Provides necessary context for accurate generation in data-scarce scenarios
  - *Quick check*: Does retrieval quality correlate with final generation accuracy?
- **Cross-Lingual Entity Alignment**: The technique of linking equivalent entities across different language knowledge bases
  - *Why needed*: Connects fragmented knowledge across language boundaries
  - *Quick check*: Are entity alignments consistent across different language pairs?
- **Monolingual Wikipedia Utilization**: Using language-specific Wikipedia articles as knowledge sources for that language
  - *Why needed*: Leverages existing high-quality monolingual content
  - *Quick check*: Does Wikipedia coverage affect performance across different topics?
- **Fine-Tuned Multilingual Models**: Adapting pre-trained multilingual models to specific low-resource language tasks
  - *Why needed*: Improves performance on languages with limited training data
  - *Quick check*: How does fine-tuning affect performance on the source languages?

## Architecture Onboarding

**Component Map**: Wikipedia Retrieval -> Context Selection -> Fine-tuned AfriTeVa Generation -> Knowledge Graph Construction

**Critical Path**: The retrieval component is critical - if relevant Wikipedia articles cannot be found, the generation quality degrades significantly. The fine-tuned AfriTeVa model is the second critical component, as it must accurately generate answers from the retrieved context.

**Design Tradeoffs**: The system trades computational complexity (retrieval + generation) for improved accuracy on low-resourced languages. It prioritizes leveraging existing unstructured data over creating new labeled datasets, which is more scalable but introduces retrieval dependencies.

**Failure Signatures**: 
- Poor retrieval quality manifests as irrelevant or missing context, leading to generation errors
- Model limitations appear as systematic errors on certain entity types or relation patterns
- Cross-lingual alignment failures result in disconnected or duplicated entities in the knowledge graph

**3 First Experiments**:
1. Measure retrieval accuracy on a held-out set of entities to establish baseline retrieval quality
2. Test generation quality with and without retrieved context to quantify retrieval contribution
3. Evaluate cross-lingual alignment accuracy to ensure entity connections are correct

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may not generalize to languages where Wikipedia coverage is more limited than Tigrinya and Amharic
- The system introduces an additional point of failure through dependency on retrieval quality
- Does not explicitly handle entity disambiguation across languages, which could lead to alignment errors

## Confidence

**High confidence**: The retrieval-augmented methodology is sound and the reported improvements over baselines are statistically significant

**Medium confidence**: The generalizability to other low-resourced languages beyond Tigrinya and Amharic

**Low confidence**: The scalability of the approach to industrial-scale knowledge graph construction with millions of entities

## Next Checks
1. Evaluate mRAKL on a third low-resourced language (e.g., Oromo or Somali) with minimal Wikipedia coverage to test robustness across different linguistic families
2. Conduct ablation studies removing the retrieval component to quantify the exact contribution of retrieved context versus the generation model alone
3. Test the system on out-of-domain knowledge to assess performance degradation when Wikipedia articles are sparse or highly specialized