---
ver: rpa2
title: Reverse Engineering Human Preferences with Reinforcement Learning
arxiv_id: '2505.15795'
source_url: https://arxiv.org/abs/2505.15795
tags:
- attack
- preambles
- command
- specific
- preamble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates adversarial attacks on LLM-as-a-judge evaluation
  frameworks. The authors propose Reinforcement Learning for Reverse Engineering (RLRE),
  a method that adversarially tunes a preamble generator to optimize upstream text
  instructions that maximize downstream evaluation scores from judge-LLMs.
---

# Reverse Engineering Human Preferences with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.15795
- **Source URL:** https://arxiv.org/abs/2505.15795
- **Reference count:** 40
- **Primary result:** RLRE adversarially tunes preamble generators to boost judge-LLM scores while evading detection

## Executive Summary
This study introduces Reinforcement Learning for Reverse Engineering (RLRE), a method that adversarially tunes preamble generators to optimize upstream text instructions that maximize downstream evaluation scores from judge-LLMs. Unlike post-hoc response editing attacks, RLRE generates natural language preambles injected into frozen candidate-LLMs before inference. Experiments demonstrate that pipelining candidate-LLMs with adversarially tuned preambles substantially increases judge-LLM scores (e.g., +0.64 points on MT-Bench for Command R7B) while remaining virtually undetectable via perplexity analysis and human inspection. The attack successfully transfers across candidate-LLMs, judge-LLMs, and evaluation benchmarks.

## Method Summary
RLRE formulates preamble generation as a reinforcement learning problem where a small trainable generator produces context-specific preambles conditioned on questions and instructions. These preambles are pipelined with a frozen candidate LLM, whose responses are evaluated by a judge LLM providing scalar rewards. The generator is updated using Contrastive Policy Gradient (CoPG), comparing preamble pairs based on reward differences to learn optimal preambles. The method is trained on UltraFeedback (~60k samples) and evaluated on MT-Bench across different model pipelines, demonstrating effectiveness and transferability while maintaining natural language fluency that evades detection.

## Key Results
- Adversarially tuned preambles increase MT-Bench scores by +0.64 points for Command R7B candidate-LLM
- Attack transfers effectively across different candidate-LLMs and judge-LLMs
- Preamble-based attack remains virtually undetectable with human detection rate only 14% vs 12% for unattacked
- Non-fluent preambles can achieve high scores, raising questions about natural language constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small preamble generator can indirectly optimize a frozen candidate LLM's outputs to maximize a judge LLM's scores by learning to produce context-specific preambles that shift the candidate's behavior.
- Mechanism: RLRE treats preamble generation as a policy optimization problem using Contrastive Policy Gradient to compare preamble pairs based on reward differences.
- Core assumption: The candidate LLM's behavior is significantly modifiable by upstream textual conditioning, and the judge's scalar reward provides sufficient gradient information.
- Evidence anchors: Abstract states RLRE uses judge-LLM signals as rewards to adversarially tune preamble generators. Section 3 formulates the RL problem with expected reward maximization. No direct corpus evidence exists for this RL-based preamble optimization pipeline.

### Mechanism 2
- Claim: Preambles tuned on one judge-candidate pair transfer effectively to unseen judges and candidates because LLMs share common representational structures and evaluation biases.
- Mechanism: The generator learns to exploit generalizable features that influence LLM behavior across families, capturing universal patterns of "preferred" outputs.
- Core assumption: LLMs share convergent behavioral responses to certain textual patterns and stylistic cues, reflecting common human preference signals in their training.
- Evidence anchors: Abstract demonstrates effectiveness transfers when candidate-LLM and judge-LLM are replaced with untrained models. Section 5.1 shows attack remains effective with unknown target judge-LLM. Corpus paper on guardrail attacks suggests transferability of attack patterns across models but doesn't address preamble transfer specifically.

### Mechanism 3
- Claim: The preamble-based attack evades detection because it does not alter the final response text, only the conditioning context.
- Mechanism: RLRE modifies only the input context (preamble), leaving the candidate's output distribution within normal operating range, producing fluent, coherent responses.
- Core assumption: Detection methods focus on response-text anomalies rather than input-context manipulation, and humans/automated systems evaluate final output quality rather than generation process.
- Evidence anchors: Abstract states the method is virtually undetectable unlike frameworks intervening directly on responses. Section 5.2 shows attacked responses are labelled as "attacked" nearly as rarely (14%) as non-attacked (12%). Corpus paper on guardrail attacks focuses on evading safety filters, not evaluation manipulation.

## Foundational Learning

- Concept: Reinforcement Learning with Black-Box Rewards
  - Why needed here: RLRE optimizes a preamble generator using rewards from an opaque judge LLM without access to internal gradients. Understanding how to estimate gradients from scalar rewards and manage exploration-exploitation in black-box settings is essential.
  - Quick check question: Given only scalar reward R(c) for response c, how would you estimate a gradient for updating the preamble generator's policy πθ?

- Concept: Contrastive Learning for Sequence-Level Optimization
  - Why needed here: The paper uses Contrastive Policy Gradient (CoPG), comparing pairs of generations to estimate policy updates rather than traditional actor-critic methods. Understanding how contrastive objectives enable efficient learning from relative preferences is critical.
  - Quick check question: How does CoPG loss L(p, p'; π) = ||R(q, c) - R(q, c') - β[ln π(p|i,q)/πref(p|i,q) - ln π(p'|i,q)/πref(p'|i,q)]||² differ from REINFORCE?

- Concept: Prompt Engineering and LLM Conditioning
  - Why needed here: The attack operates through textual preambles injected as system prompts. Grasping how preambles influence LLM behavior—via attention patterns, context utilization, instruction following—is foundational.
  - Quick check question: What mechanisms allow a preamble injected before a user query to systematically shift an LLM's output distribution without modifying weights?

## Architecture Onboarding

- Component map: Preamble Generator (πθ) -> Candidate LLM (LLMC) -> Judge LLM -> Contrastive Policy Gradient Update -> Preamble Generator (πθ)
- Critical path: Sample question → Generator produces preamble pair (p, p') → Each preamble + question fed to candidate LLM → Responses (c, c') → Judge evaluates → Rewards → Compute CoPG loss → Update generator (repeat until validation plateaus)
- Design tradeoffs:
  - KL coefficient (β): Low β (0.03) allows greater deviation from reference policy, producing effective but potentially non-fluent preambles. Higher β improves fluency but may reduce reward gains.
  - Generator size vs. cost: Smaller generators (7B-8B) are cheaper but may produce less sophisticated preambles.
  - Question-conditioning: Ablation shows removing question context slightly reduces but doesn't eliminate effectiveness—tradeoff between specificity and generality.
- Failure signatures:
  - Reward plateau early: Insufficient exploration (increase temperature) or KL penalty too high (reduce β).
  - High response perplexity: Generator producing overly unusual preambles; increase β or add fluency constraints.
  - Poor judge transfer: Overfitting to training judge's biases; consider training with a jury of judges.
  - Human detection spike: Preambles inducing detectable stylistic shifts; consider detection-loss term.
- First 3 experiments:
  1. Reproduce baseline comparison: Train generator on Command R7B using UltraFeedback, evaluate on MT-Bench with Command R+ judge. Compare against no-attack, verbosity, bandwagon, authority, refinement, and universal attack baselines to validate implementation.
  2. Ablate preamble conditioning: Train three variants—(a) question-conditioned, (b) generic instruction only, (c) no instruction. Evaluate on MT-Bench to quantify each information source's contribution.
  3. Test judge transferability: Train on Command R+ judge, evaluate same generator on MT-Bench using GPT-4o-mini, Claude Haiku, and an open-source judge (Llama 3.1 70B). Measure score improvements and variance to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-as-a-judge evaluation frameworks be redesigned to be robust against preamble-based adversarial attacks while maintaining scalability?
- Basis in paper: The paper states "These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings" and demonstrates current safeguards fail to detect this attack.
- Why unresolved: The attack operates upstream at preamble level rather than modifying responses post-hoc, rendering perplexity filters and human inspection ineffective.
- What evidence would resolve it: Development of evaluation methods invariant to preamble injection, or novel detection mechanisms that identify semantic manipulation in system prompts.

### Open Question 2
- Question: Does constraining conditioning tokens (preambles, reasoning traces) to natural language unnecessarily limit model performance?
- Basis in paper: The authors state findings "raise important questions about whether constraining conditioning tokens—such as preambles or reasoning tokens—to the manifold of natural language may inadvertently limit model capabilities."
- Why unresolved: Non-fluent preambles from the Llama pipeline achieved high scores, suggesting optimal conditioning may extend beyond human-readable text, but the interpretability-performance trade-off remains unexplored.
- What evidence would resolve it: Systematic comparison across fluent, optimized non-fluent, and intermediate representations, measuring both task performance and interpretability.

### Open Question 3
- Question: Can training preamble generators with ensembles of multiple judge-LLMs improve attack transferability to unseen evaluators?
- Basis in paper: Appendix B.4 states "We postulate that our training strategy could similarly benefit from an ensemble of multiple judge-LLMs, potentially improving the transferability of RLRE to new, unseen LLM-judges at inference" and calls this "an avenue worth exploring in future work."
- Why unresolved: Single-judge training shows some transferability, but ensemble effects on robustness and generalization remain untested.
- What evidence would resolve it: Experiments with varying ensemble sizes, measuring transfer performance across diverse unseen judge architectures.

### Open Question 4
- Question: Can RLRE be effectively applied to positive objectives like toxicity reduction or bias mitigation?
- Basis in paper: The paper states the method "could be paired with different downstream rewards to optimise preambles for a variety of applications beyond just adversarial attacks—including but not limited to meaningful tasks such as reducing toxicity or mitigating bias."
- Why unresolved: Adversarial reward maximization is demonstrated, but effectiveness for positive behavioral constraints through upstream optimization is unknown.
- What evidence would resolve it: Applying RLRE with toxicity classifiers or bias detectors as reward signals, measuring behavioral change effectiveness and cross-model transferability.

## Limitations
- Transferability claims rest on limited judge diversity (three proprietary models plus one open-source), raising questions about generalizability to smaller or differently trained judges
- Paper does not report runtime latency or computational costs for training preamble generators, which could limit practical applicability
- Detection evasion results are based on two specific methods (PPL-W and human annotation) with limited sample sizes, leaving open the possibility that other detection approaches might succeed

## Confidence
- **High Confidence:** The fundamental RLRE mechanism and the fact that adversarial preambles increase judge scores
- **Medium Confidence:** The claim that preambles transfer across different candidate-LLMs and judge-LLMs (demonstrated but based on limited set of models)
- **Medium Confidence:** The detection evasion claims (supported by reported results but evaluation methods are relatively narrow)

## Next Checks
1. **Transferability Stress Test:** Train preamble generators on a diverse set of judge-LLMs (including smaller, open-source models) and evaluate on a wide range of candidate-LLMs across different model families and sizes to quantify the limits of cross-model transfer.
2. **Robust Detection Analysis:** Investigate the effectiveness of alternative detection methods, including input-side preamble profiling, multi-stage evaluation that considers generation context, and detection models trained specifically to identify adversarial preambles.
3. **Human Evaluation Validation:** Conduct a more rigorous human evaluation study with multiple, trained annotators, including inter-annotator agreement metrics and clear instructions, to validate the reported 14% detection rate and assess the robustness of the detection evasion claims.