---
ver: rpa2
title: 'ADO: Automatic Data Optimization for Inputs in LLM Prompts'
arxiv_id: '2502.11436'
source_url: https://arxiv.org/abs/2502.11436
tags:
- data
- optimization
- prompt
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Automatic Data Optimization (ADO), a framework
  that enhances LLM performance by optimizing input data within prompts through content
  engineering and structural reformulation. Content engineering involves imputing
  missing values, removing irrelevant attributes, and enriching profiles by generating
  additional information inferred from existing attributes.
---

# ADO: Automatic Data Optimization for Inputs in LLM Prompts

## Quick Facts
- **arXiv ID:** 2502.11436
- **Source URL:** https://arxiv.org/abs/2502.11436
- **Reference count:** 39
- **Primary result:** A framework that significantly improves LLM performance by optimizing input data within prompts through content engineering and structural reformulation, with DPS outperforming existing optimization algorithms in 8 out of 9 datasets.

## Executive Summary
This paper introduces Automatic Data Optimization (ADO), a framework that enhances LLM performance by optimizing input data within prompts through content engineering and structural reformulation. The framework applies two sequential transformations: content engineering (imputing missing values, removing irrelevant attributes, enriching profiles) and structural reformulation (changing data presentation format). A Diverse Prompt Search (DPS) algorithm with Bayesian optimization automatically determines optimal diversity constraints, ensuring semantic and lexical diversity among candidate prompts. Experimental results across nine real-world datasets show that ADO significantly improves LLM performance.

## Method Summary
ADO is a multi-agent framework that automatically optimizes input data for LLM inference. It consists of three LLM instances: a Prompt-Generation LLM (LLMG) proposes data-optimization prompts, a Data Optimization LLM (LLMO) executes these prompts to transform raw data, and a Task Inference LLM (LLMI) makes predictions using the optimized data. The framework employs a Diverse Prompt Search (DPS) algorithm that uses Bayesian optimization to tune diversity hyperparameters (number of candidates, cosine similarity constraint, METEOR score constraint) while generating multiple diverse candidate prompts per iteration. This diversity enforcement prevents convergence to semantically similar, suboptimal solutions. The system also includes a factual-validation LLM to check for hallucinations in the optimized data.

## Key Results
- DPS algorithm outperforms existing optimization algorithms (APE, OPRO, etc.) in 8 out of 9 tested datasets
- ADO improves LLM performance by systematically cleaning, enriching, and reformatting input data
- Integrating ADO with Chain-of-Thought reasoning and In-Context Learning further enhances performance
- The framework successfully handles diverse real-world datasets including StrategyQA, Fraudulent Job Detection, and various recommendation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing the content and structure of input data within a prompt can improve LLM inference performance.
- **Mechanism:** ADO applies two sequential transformations to input data: content engineering and structural reformulation. Content engineering reduces noise (removing irrelevant attributes, imputing missing values) and enriches the signal (inferring new attributes from existing ones). Structural reformulation then changes the data's presentation (e.g., to XML, tables) to better match the LLM's processing preferences. The composite function transforms raw data $D$ into optimized data $D'$.
- **Core assumption:** LLM performance is hindered by noisy, incomplete, or poorly formatted input data, and these issues can be systematically corrected via text-based transformations.
- **Evidence anchors:**
  - [abstract] "...optimizing input data within prompts through content engineering and structural reformulation... significantly improves LLM performance..."
  - [section 2.1] "Content optimization emphasizes enhancing the saliency of features... Format optimization focuses on structuring the data in an optimal format..."
  - [corpus] The paper "Prompt-in-Content Attacks" (arXiv:2508.19287) demonstrates that LLMs are highly sensitive to input content, which can even contain embedded instructions, reinforcing the importance of input quality.
- **Break condition:** If the original input data is already clean, complete, and formatted optimally for the specific LLM being used, ADO may offer little to no benefit and could introduce unnecessary processing overhead.

### Mechanism 2
- **Claim:** A Diverse Prompt Search (DPS) algorithm using Bayesian optimization finds better data-optimization procedures than standard search methods.
- **Mechanism:** DPS improves upon standard iterative optimizers (like APE or OPRO) by enforcing diversity in the candidate optimization prompts. It uses Bayesian search to tune hyperparameters ($k$ candidates per iteration, cosine similarity constraint $c_1$, METEOR score constraint $c_2$). This forces the search to explore a broader space of optimization strategies, preventing convergence to semantically similar, suboptimal solutions.
- **Core assumption:** Standard optimization algorithms suffer from mode collapse or local optima because they generate minor variations of prior candidates; enforcing lexical and semantic diversity promotes exploration.
- **Evidence anchors:**
  - [abstract] "The framework employs a Diverse Prompt Search (DPS) algorithm... DPS outperforming existing optimization algorithms in eight out of nine datasets."
  - [section 2.3] "...optimizing by augmenting a single candidate prompt as context... may hinder the discovery of an optimal prompt."
  - [corpus] "LLM-AutoDiff" (arXiv:2501.16673) notes prompt engineering remains labor-intensive, suggesting that more effective automated search strategies like DPS are valuable.
- **Break condition:** If the optimal data-optimization strategy is extremely similar to an initial prompt or if the diversity constraints are poorly tuned (e.g., too strict), DPS might discard good candidates or fail to converge efficiently.

### Mechanism 3
- **Claim:** ADO is complementary to other prompt engineering techniques like Chain-of-Thought (CoT) and In-Context Learning (ICL).
- **Mechanism:** ADO targets a different part of the problem than traditional methods. While CoT improves the model's reasoning process and ICL provides task examples, ADO improves the *input data* upon which that reasoning is based. By first cleaning and structuring the data, ADO reduces ambiguity, allowing CoT and ICL to operate more effectively.
- **Core assumption:** The efficacy of reasoning and in-context examples is bounded by the quality and clarity of the input data they reference.
- **Evidence anchors:**
  - [abstract] "...integrating ADO with other prompt engineering techniques like Chain-of-Thought reasoning and In-Context Learning further enhances performance."
  - [section 4.2] Table 2 shows that applying "CoT + ADO" yields higher performance than "CoT" alone across all tested datasets.
  - [corpus] "Prompt Alchemy" (arXiv:2503.11085) and other works focus on refining task instructions, a separate vector from ADO's input data optimization.
- **Break condition:** If a task's primary bottleneck is not data clarity but rather the model's fundamental lack of domain knowledge or reasoning capacity, combining ADO with other techniques may yield diminishing returns.

## Foundational Learning

- **Concept: Bayesian Optimization**
  - **Why needed here:** The DPS algorithm uses Bayesian optimization to tune its own search parameters ($k$, $c_1$, $c_2$). Understanding this is necessary to configure DPS and interpret how it explores the space of possible optimization prompts.
  - **Quick check question:** In the context of DPS, what are the hyperparameters that Bayesian optimization is trying to tune, and what metric is it trying to maximize?

- **Concept: LLM-based Agents and Workflows**
  - **Why needed here:** The ADO framework is not a single model but a multi-agent system with distinct roles: a generator agent ($LLM_G$), an optimizer agent ($LLM_O$), and an inference agent ($LLM_I$).
  - **Quick check question:** What is the specific role of the Data Optimization LLM ($LLM_O$)? Does it generate the final answer for the user task?

- **Concept: Prompt Engineering vs. Feature Engineering**
  - **Why needed here:** ADO effectively automates "feature engineering" using natural language prompts. It bridges the gap between traditional ML data preparation and modern LLM prompting, requiring an understanding of both.
  - **Quick check question:** How is the "content engineering" step in ADO conceptually similar to traditional feature engineering in machine learning, and how is it different in its implementation (tool-based vs. LLM-based)?

## Architecture Onboarding

- **Component map:** Prompt-Generation LLM ($LLM_G$) -> Data Optimization LLM ($LLM_O$) -> Task Inference LLM ($LLM_I$)
- **Critical path:**
  1. **Configuration:** Define the meta-prompt and validation set.
  2. **Search Loop (DPS):**
     a. Bayesian search selects diversity hyperparameters.
     b. $LLM_G$ generates $k$ diverse optimization prompts.
  3. **Evaluation:** For each prompt, $LLM_O$ optimizes validation data, and $LLM_I$ makes predictions. Performance is scored.
  4. **Feedback:** Prompt-score pairs are fed back to $LLM_G$.
  5. **Deployment:** The best-performing $P_o$ is used to process data for the live task.

- **Design tradeoffs:**
  - **Performance vs. Latency/Cost:** The multi-step process with multiple LLM calls adds significant latency and cost compared to a single prompt.
  - **Automation vs. Control:** Automated imputation and enrichment can introduce errors (hallucinations) if not properly validated.
  - **Diversity vs. Coherence:** Enforcing too much diversity in DPS might lead to incoherent or irrelevant optimization strategies.

- **Failure signatures:**
  - **Hallucinated Data:** $LLM_O$ enriches profiles with plausible but incorrect information, leading $LLM_I$ to make wrong inferences.
  - **Procedural Error:** $LLM_G$ proposes a procedure $LLM_O$ cannot perform textually (e.g., "perform PCA on the data").
  - **Negative Optimization:** The "optimized" format confuses $LLM_I$, performing worse than raw data.

- **First 3 experiments:**
  1.  **Baseline Check:** Measure your task performance using raw, unoptimized data.
  2.  **Simple ADO Test:** Use a basic search (like OPRO) to find a data-optimization prompt. Compare the results using optimized data vs. raw data.
  3.  **Hallucination Audit:** Run ADO's content engineering on a few samples and manually inspect the enriched data. Check for factual correctness before deploying the full system.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can optimal task instructions and data optimization procedures be searched for concurrently rather than sequentially? The current ADO framework optimizes data representation first, followed by instruction optimization, which is less efficient than a unified search.
- **Open Question 2:** Is the computational overhead of the Diverse Prompt Search (DPS) and multi-LLM workflow justified for low-complexity tasks? The paper doesn't quantify the cumulative token cost or latency trade-offs compared to the performance gains achieved.
- **Open Question 3:** How robust is the hallucination mitigation strategy (LLMF) when applied to highly specialized or technical domains? The effectiveness depends on the reviewer model's domain expertise, which may falter in niche fields.

## Limitations
- **Hyperparameter Tuning Complexity:** DPS relies on Bayesian optimization to tune diversity constraints, but the paper doesn't specify search space bounds or sensitivity analysis, which could lead to excessive computational cost or suboptimal solutions.
- **Hallucination Risk:** The content engineering component may introduce factually incorrect information when imputing missing values or inferring new attributes, with unclear effectiveness of the proposed mitigation strategy.
- **Computational Overhead:** The multi-stage pipeline with multiple LLM calls significantly increases latency and cost compared to single-prompt approaches, though runtime comparisons are not provided.

## Confidence
- **High Confidence:** The core mechanism that optimizing input data quality (content and structure) improves LLM performance is well-supported by empirical results across nine datasets.
- **Medium Confidence:** The claim that DPS outperforms existing optimization algorithms is supported by the results (8/9 datasets), but the statistical significance and practical impact magnitude are not fully quantified.
- **Medium Confidence:** The complementarity claim with Chain-of-Thought and In-Context Learning is demonstrated, but the interaction effects and when this combination provides diminishing returns are not fully explored.

## Next Checks
1. **Hallucination Audit:** Implement the factual-validation step and systematically measure the rate of incorrect imputations/enrichments across different dataset types and LLM models.
2. **Computational Efficiency Analysis:** Measure the runtime and cost per query for ADO versus baseline approaches, including sensitivity analysis for the DPS hyperparameter tuning iterations.
3. **Transferability Test:** Evaluate whether optimization prompts discovered on one dataset type transfer to improve performance on structurally similar but distinct datasets, or if prompt optimization must be dataset-specific.