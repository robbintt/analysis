---
ver: rpa2
title: Understanding and Mitigating Risks of Generative AI in Financial Services
arxiv_id: '2504.20086'
source_url: https://arxiv.org/abs/2504.20086
tags:
- risk
- https
- services
- system
- taxonomy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We demonstrate empirically that existing general-purpose guardrail
  systems fail to identify risks in financial services applications. Our domain-specific
  risk taxonomy for financial services covers risks related to confidentiality, counterfactual
  narratives, defamation, discrimination, financial services impartiality, financial
  services misconduct, irrelevance, non-financial advice, offensive language, personally
  identifiable information, prompt injection and jailbreaking, social media headline
  risk, and jurisdiction-specific and product-specific risks.
---

# Understanding and Mitigating Risks of Generative AI in Financial Services

## Quick Facts
- arXiv ID: 2504.20086
- Source URL: https://arxiv.org/abs/2504.20086
- Authors: Sebastian Gehrmann; Claire Huang; Xian Teng; Sergei Yurovski; Iyanuoluwa Shode; Chirag S. Patel; Arjun Bhorkar; Naveen Thomas; John Doucette; David Rosenberg; Mark Dredze; David Rabinowitz
- Reference count: 40
- We demonstrate empirically that existing general-purpose guardrail systems fail to identify risks in financial services applications.

## Executive Summary
This paper reveals a critical safety gap in current Generative AI guardrail systems when applied to financial services. The authors develop a domain-specific risk taxonomy for financial services and demonstrate that general-purpose guardrails like Llama Guard, AEGIS, and ShieldGemma achieve low recall on both inputs and outputs, even when prompted to cover the new taxonomy. For example, Llama Guard 3 detects only 12% of social media headline risk violations and less than 1% of confidential disclosure violations. The models also suffer from high false positive rates when expanded, making them infeasible for production use in regulated industries.

The research introduces a comprehensive 13-category financial services risk taxonomy and shows that safety must be achieved through a multi-layer governance process rather than relying on a single guardrail system. The paper emphasizes that while technical guardrails are imperfect, they can play an instrumental role as triggers within a broader sociotechnical system that includes human review and escalation policies.

## Method Summary
The paper evaluates existing LLM guardrails (Llama Guard, AEGIS, ShieldGemma) on a proprietary red-teaming dataset of 10,400 system inputs and 7,340 outputs collected for financial services applications. Two evaluation setups are used: (1) Default mode using original prompts, and (2) Expanded mode where new taxonomy definitions are injected into the guardrail model's prompt. The study measures precision, recall, and F1 score across binary, lenient, and strict per-category metrics, with particular focus on false positive rates on "normal course of business" queries.

## Key Results
- General-purpose guardrails achieve low recall on financial services risks, with Llama Guard 3 detecting only 12% of social media headline risk violations
- Expanding guardrails via prompt engineering leads to significantly increased false positive rates (up to 32.8%) and lower precision
- Technical guardrails act as triggers within a broader sociotechnical system rather than as standalone safety solutions

## Why This Works (Mechanism)

### Mechanism 1: The Safety Gap via Taxonomy Misalignment
General-purpose guardrail systems fail to identify financial risks because their underlying training taxonomies do not align with domain-specific regulatory definitions. Guardrail models are fine-tuned on general safety categories like violence and hate speech, lacking the semantic representations to distinguish between benign business queries and regulatory violations, resulting in low recall.

### Mechanism 2: Instability of Prompt-Based Adaptation
Expanding guardrail coverage via prompt engineering leads to high false positive rates and precision degradation. When users attempt to expand coverage by listing new risks in the prompt, the model struggles to map these new labels to its existing weights, either missing violations or overcorrecting and flagging benign queries.

### Mechanism 3: Governance as a Structural Safety Layer
Because technical guardrails are imperfect (low recall), safety is achieved through a multi-layer governance process rather than a single model filter. Technical guardrails act as triggers within a broader sociotechnical system where human review, escalation policies, and logging catch edge cases that automated filters miss.

## Foundational Learning

- **Concept: Sociotechnical Safety Assessment**
  - Why needed here: The paper argues that evaluating a model in isolation creates a "Framing Trap." Learners must understand that risk is a function of the specific domain, user intent, and applicable regulations, not just the model's output.
  - Quick check question: Why does the paper argue that a "safe" response in a general chatbot might be "unsafe" in a financial services context?

- **Concept: Guardrails vs. Base Models**
  - Why needed here: The paper evaluates separate "guardrail systems" distinct from the underlying LLM. Understanding this separation is key to the proposed architecture where a smaller, specialized model audits a larger one.
  - Quick check question: According to the paper, what is the primary role of a "guardrail" in the GenAI stack?

- **Concept: The Precision/Recall Trade-off in Safety**
  - Why needed here: The empirical results show that trying to catch more bad actors often blocks legitimate business users. Learners need to grasp why a 32.8% false positive rate renders a system "infeasible" for business use.
  - Quick check question: Why is high precision critical for "normal course of business" queries in a professional setting?

## Architecture Onboarding

- **Component map:** User Interface -> Guardrail Layer -> LLM Core -> Output Filter -> Governance/Routing
- **Critical path:** 1. Adopt the domain-specific risk categories detailed in Table 1. 2. Run red-teaming exercises to build a dataset of "unsafe" inputs and "normal course of business" inputs. 3. Test off-the-shelf guardrails on this specific data to identify the "Safety Gap."
- **Design tradeoffs:** Using general guardrails "out of the box" has high precision but misses domain risks. Expanding the prompt catches more risks but increases false positives. Blocking content automatically is safer but frustrating; routing flagged content to humans increases safety coverage but adds latency and cost.
- **Failure signatures:** High False Positives: The system blocks legitimate queries like "Should I buy SPY today?" Low Recall on Confidentiality: The system allows leaks of Material Non-Public Information because the guardrail conflates it with generic privacy.
- **First 3 experiments:** 1. Construct a Proxy Dataset using examples from Table 4 to test current guardrails. 2. Compile 100 legitimate, tricky business queries and run them through your current guardrail to measure False Positive rate. 3. Implement the "Expanded" prompt strategy and measure degradation in precision vs. gain in recall.

## Open Questions the Paper Calls Out

### Open Question 1
How can systemic risks in financial markets, such as instabilities caused by shared inductive biases among multiple market actors, be identified and mitigated given that current taxonomies only cover content-level risks? The paper states this key limitation in Section 4, noting that the proposed taxonomy does not capture systemic risk from many actors relying on the same model.

### Open Question 2
What technical approaches beyond prompt engineering are required to successfully adapt general-purpose guardrail models to domain-specific risk taxonomies without incurring high false positive rates? Section 7.2 notes this is a field of ongoing research, as simply expanding prompts leads to significantly increased false positive rates and low recall.

### Open Question 3
How can domain-specific risk taxonomies and guardrails be effectively developed and validated for languages and jurisdictional contexts beyond English? Section 7.2 explicitly lists this as a gap, noting that ensuring risk coverage in languages beyond English remains an open challenge.

## Limitations

- Dataset Access Barrier: The primary limitation is reliance on a proprietary red-teaming dataset (10,400 inputs, 7,340 outputs) that cannot be publicly shared, requiring creation of high-quality substitutes for reproduction.
- Guardrail Configuration Specificity: The "Expanded" prompt mode results depend on specific prompt engineering details that are not fully specified in the text, creating uncertainty in faithful reproduction.
- Generalization Beyond Financial Services: While methodology is presented as broadly applicable, empirical validation is specific to financial services, and confidence in translation to other domains is not established.

## Confidence

- **High Confidence**: The fundamental mechanism that general-purpose guardrails fail to detect domain-specific risks is well-supported by empirical data and logical reasoning.
- **Medium Confidence**: The precision/recall trade-off findings are specific to the experimental setup and guardrail configurations tested, though the phenomenon is likely general.
- **Low Confidence**: The governance layer's effectiveness as a structural safety solution assumes sufficient human review capacity, which is not empirically validated in the paper.

## Next Checks

1. **Taxonomy-to-Production Audit**: Take the 13-category financial services taxonomy and systematically map it against your organization's current production logs and incident reports to identify which risk categories are currently being missed.

2. **Normal Course Stress Test**: Compile a representative set of 50-100 legitimate business queries that push the boundaries of financial advice and run these through your current guardrail system to measure the actual false positive rate.

3. **Guardrail Substitution Experiment**: Implement a controlled experiment comparing your current guardrail system against a simple rule-based filter using the paper's taxonomy definitions to measure which approach catches more true violations while maintaining acceptable precision.