---
ver: rpa2
title: What Would an LLM Do? Evaluating Large Language Models for Policymaking to
  Alleviate Homelessness
arxiv_id: '2509.03827'
source_url: https://arxiv.org/abs/2509.03827
tags:
- policy
- llms
- experts
- social
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether large language models (LLMs) can effectively
  support policymaking to alleviate homelessness, a global challenge affecting over
  150 million people. Using a novel benchmark grounded in the Capability Approach,
  the research compares LLM-generated policy recommendations with those of domain
  experts across four cities.
---

# What Would an LLM Do? Evaluating Large Language Models for Policymaking to Alleviate Homelessness

## Quick Facts
- arXiv ID: 2509.03827
- Source URL: https://arxiv.org/abs/2509.03827
- Reference count: 40
- Primary result: LLMs align with expert homelessness policy choices but prioritize immediate safety over long-term capability restoration and show context sensitivity gaps

## Executive Summary
This study evaluates whether large language models (LLMs) can effectively support policymaking to alleviate homelessness, a global challenge affecting over 150 million people. Using a novel benchmark grounded in the Capability Approach, the research compares LLM-generated policy recommendations with those of domain experts across four cities. The study also introduces a pipeline linking LLM-selected policies to an agent-based model to simulate social impact. Results show that while LLMs often align with expert choices and produce plausible policies, they tend to prioritize immediate safety over long-term capability restoration and are less sensitive to local contexts. When tested in simulations, LLM-recommended policies slightly outperformed expert choices in meeting the needs of people experiencing homelessness, suggesting potential value if paired with contextual calibration and expert oversight.

## Method Summary
The study creates a benchmark of 170 homelessness policy scenarios across four cities (Barcelona, Johannesburg, South Bend, Macau), each with four policy options annotated with Martha Nussbaum's ten central capabilities. Domain experts in three cities rank 20 scenarios each (10 local, 10 universal). Six LLMs are prompted to select and rank policies, with results compared to expert choices using top-choice agreement, Kendall tau correlation, and Sentence-BERT similarity. A Deepseek-R1 LLM translates selected policies into State-Action-Transition matrix modifications for an existing Barcelona agent-based model, which simulates outcomes for 80 agents over 1,450 steps, tracking physiological, safety, belonging, and self-esteem needs.

## Key Results
- LLMs achieve 70-80% top-choice agreement with experts and moderate correlation (0.31-0.65 Kendall tau) in full rankings
- LLMs systematically prioritize immediate safety capabilities (Life, Bodily Health, Bodily Integrity) over affiliation and practical reason compared to experts
- LLM-recommended policies slightly outperform expert choices in ABM simulations, particularly for physiological and safety needs
- Context sensitivity gaps emerge, with LLMs showing stronger alignment with US experts and diverging on localized scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policies annotated with the Capability Approach provide a normative framework for comparing LLM and expert prioritization patterns.
- Mechanism: Each of the 680 policy options is tagged with one or more of Nussbaum's ten central capabilities (e.g., Life, Affiliation, Practical Reason). This transforms unstructured policy text into a structured, ethically grounded representation that enables quantitative comparison of what LLMs vs. experts prioritize.
- Core assumption: The Capability Approach's ten capabilities capture the relevant dimensions of human dignity for homelessness policymaking across diverse urban contexts.
- Evidence anchors:
  - [abstract] "policy choices that are grounded in the conceptual framework of the Capability Approach for human development"
  - [section 3.1] "annotated with one or more of Martha Nussbaum's ten central human capabilities"
  - [corpus] Weak/no direct corpus support for this specific mechanism; related work focuses on LLMs for surveys and social science evaluation, not Capability Approach grounding.
- Break condition: If the ten capabilities fail to capture context-specific values (e.g., local religious or ethnic dimensions of belonging), the comparison framework becomes misaligned with local ethical realities.

### Mechanism 2
- Claim: LLMs exhibit systematic prioritization of immediate safety capabilities over affiliation and practical reason compared to domain experts.
- Mechanism: LLMs are prompted to rank policies; their top choices are mapped to capability tags. The distribution of prioritized capabilities is compared against expert rankings. A consistent pattern emerges: LLMs favor policies tagged Life, Bodily Health, Bodily Integrity; experts more often select Affiliation, Practical Reason.
- Core assumption: The observed divergence reflects training data composition (e.g., over-representation of U.S.-centric safety-first narratives) rather than random variation.
- Evidence anchors:
  - [abstract] "LLMs often agree with experts on top choices but prioritize immediate safety over social affiliation"
  - [section 4.2] "GPT-4.1 demonstrated a stronger tendency to select policies that protect against immediate physical threats, prioritizing life, bodily health, and bodily integrity"
  - [corpus] Corpus papers discuss LLM alignment and bias but do not directly address safety vs. affiliation trade-offs in social policy.
- Break condition: If contextual prompting (explicitly instructing LLMs to consider local context) eliminates the divergence, the mechanism would shift to "context-blindness" rather than inherent prioritization bias.

### Mechanism 3
- Claim: An LLM-mediated pipeline can convert natural-language policies into executable behavioral adjustments in an ABM, enabling simulated impact comparison.
- Mechanism: An LLM is prompted with a policy description, the ABM's State-Action-Transition (SAT) matrix structure, and modification rules. The LLM outputs a JSON with small, bounded adjustments (Δ ≤ 0.03) to specific matrix cells. The modified ABM runs, tracking need-satisfaction vectors for agents representing people experiencing homelessness.
- Core assumption: The LLM's proposed SAT adjustments faithfully represent the policy's intent and maintain behavioral realism.
- Evidence anchors:
  - [abstract] "automated pipeline that connects the policies to an agent-based model"
  - [section 3.3] "LLMs convert the natural language policy proposals they have previously selected into agents' behavioral adjustments in the ABM"
  - [corpus] Corpus papers discuss LLM-ABM integration conceptually but do not provide empirical validation of this specific translation pipeline.
- Break condition: If LLM-generated adjustments violate ABM constraints or produce logically incoherent behavior (e.g., increasing "steal" actions for policies aiming to reduce crime), the pipeline's validity collapses.

## Foundational Learning

- Concept: **Capability Approach (Nussbaum)**
  - Why needed here: Provides the ethical annotation vocabulary for all 680 policies; essential for interpreting LLM vs. expert divergence.
  - Quick check question: Can you name three of the ten central capabilities and explain why "Affiliation" differs from "Bodily Integrity" in a homelessness policy context?

- Concept: **Agent-Based Modeling (ABM) State-Action-Transition Matrices**
  - Why needed here: The ABM impact evaluation hinges on understanding how SAT matrix modifications propagate to agent behavior and need satisfaction.
  - Quick check question: Given a 14×11 SAT matrix mapping 14 needs to 11 actions, what does modifying matrix["shelter"]["go reception center"] represent behaviorally?

- Concept: **LLM Evaluation in High-Stakes Domains**
  - Why needed here: The paper's central claim is about LLM alignment; understanding hallucination, bias amplification, and context-blindness is critical for interpreting results.
  - Quick check question: What specific risks does the paper cite for deploying LLMs in homelessness policymaking, and how does the benchmark design attempt to mitigate them?

## Architecture Onboarding

- Component map:
  Benchmark Generator -> Expert Interface -> LLM Evaluator -> ABM Translator -> ABM Simulator

- Critical path:
  Scenario generation → Expert ranking → LLM evaluation → Policy selection for ABM → SAT matrix translation → Simulation runs (80 agents, 1,450 steps) → Statistical comparison (t-test on need satisfaction)

- Design tradeoffs:
  - Synthetic vs. expert-generated scenarios: LLM-generated scenarios scale efficiently but risk generic patterns; one baseline per city was expert-crafted.
  - Single-city ABM: Only Barcelona's ABM is integrated; generalization to other cities requires new model calibration.
  - Translation fidelity vs. automation: Fully automated policy-to-SAT translation is scalable but may misrepresent nuanced policies; manual override is possible but costly.

- Failure signatures:
  - **Context blindness**: LLM recommendations align with experts on universal scenarios but diverge on localized ones, especially in under-represented regions (e.g., Johannesburg).
  - **Capability imbalance**: LLM policies improve safety and self-esteem but neglect belonging, producing simulation improvements that miss socially critical outcomes.
  - **Translation artifacts**: ABM results show inconsistent or extreme SAT adjustments (e.g., values exceeding 0.03 bounds), indicating LLM misunderstanding of matrix structure.

- First 3 experiments:
  1. **Single-city validation**: Run the full pipeline for Barcelona only, comparing LLM vs. expert policies across all 40 scenarios (not just 10) to test consistency.
  2. **Capability distribution analysis**: For each LLM, compute the distribution of prioritized capabilities across all top choices; compare statistically against expert distributions using chi-square tests.
  3. **Contextual prompting ablation**: Repeat LLM evaluation with and without explicit local-context prompts for Johannesburg; quantify the alignment shift to test the context-blindness hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the over-representation of US-centric data in LLM training corpora drive their preference for immediate safety interventions over social affiliation?
- Basis in paper: [explicit] The authors hypothesize that the strong alignment with US experts and preference for safety-first policies "could potentially be illustrating a bias... stemming from the over-representation of US-centric published discourse."
- Why unresolved: The study evaluates model outputs but does not audit training data compositions or perform controlled ablations on data sources.
- What evidence would resolve it: Comparative analysis of policy rankings between standard LLMs and models fine-tuned on geographically diverse, non-Western social policy texts.

### Open Question 2
- Question: Does constructing the benchmark in the local languages of the target cities improve LLM alignment with domain expert policy preferences?
- Basis in paper: [explicit] The authors list a key limitation: "benchmark scenarios and policies are currently all in English. We recommend that future studies build the entire benchmark... with realistic policies proposed by domain experts in the local languages."
- Why unresolved: Current results may be artifacts of English-language reasoning applied to non-Western contexts, failing to capture local nuance.
- What evidence would resolve it: A replication of the ranking experiment where scenarios are originally authored and presented to the LLM in Spanish, Chinese, and Portuguese.

### Open Question 3
- Question: Can the social impact of LLM-recommended policies observed in Barcelona be replicated in the agent-based models for the other three cities?
- Basis in paper: [explicit] The methodology notes the simulation pipeline "currently handles the policy scenarios for one of the locations in scope only (Barcelona)" and lists extending this to "three additional cities" as future work.
- Why unresolved: The superior performance of LLM policies in satisfying physiological and safety needs is currently validated only within the specific socio-economic simulation of Barcelona.
- What evidence would resolve it: Implementation of the State-Action-Transition (SAT) matrix pipeline and ABM environments for Johannesburg, South Bend, and Macau.

## Limitations
- Benchmark dataset not publicly released, preventing independent validation
- ABM integration limited to single city (Barcelona), with unclear generalization
- Translation fidelity from policy text to SAT matrix adjustments lacks empirical validation

## Confidence
- **High**: LLM-expert top-choice agreement rates (70-80%) and Kendall tau correlations are reliably measured
- **Medium**: Capability distribution findings are internally consistent but depend on correct annotation framework
- **Low**: ABM simulation results are difficult to validate without access to the underlying model implementation

## Next Checks
1. **Benchmark Release**: Request and validate the complete 170-scenario dataset with capability annotations and expert rankings to enable independent replication

2. **Cross-City ABM Testing**: Extend the SAT matrix translation and simulation pipeline to Johannesburg and South Bend using locally-calibrated ABMs to test generalizability

3. **Context Prompting A/B Test**: Systematically test the impact of explicit local context instructions on LLM policy selection across all cities, measuring both top-choice agreement and capability distribution shifts