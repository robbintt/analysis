---
ver: rpa2
title: Does the Prompt-based Large Language Model Recognize Students' Demographics
  and Introduce Bias in Essay Scoring?
arxiv_id: '2504.21330'
source_url: https://arxiv.org/abs/2504.21330
tags:
- scoring
- essay
- llms
- students
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias in Large Language Models (LLMs) when
  scoring student essays, focusing on how well LLMs infer demographic attributes (gender
  and first-language background) and whether this ability influences scoring fairness.
  Using a dataset of over 25,000 argumentative essays, the authors prompted GPT-4o
  to predict demographics and assign scores, then analyzed bias using multiple fairness
  metrics and multivariate regression.
---

# Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?

## Quick Facts
- **arXiv ID**: 2504.21330
- **Source URL**: https://arxiv.org/abs/2504.21330
- **Reference count**: 40
- **Primary result**: Prompt-based LLMs can infer student demographics (first-language background) and introduce scoring bias, especially against non-native English speakers when correctly identified.

## Executive Summary
This study investigates whether prompt-based Large Language Models (LLMs) can infer student demographics—specifically gender and first-language background—and whether this ability introduces bias in essay scoring. Using over 25,000 argumentative essays, the authors prompted GPT-4o to predict demographics and assign scores, then analyzed bias using multiple fairness metrics and multivariate regression. Results showed that LLMs can accurately infer first-language background (accuracy 0.75–0.87) but are less accurate for gender (accuracy 0.86–0.96 with low coverage). Scoring bias was more pronounced when LLMs correctly identified students’ first-language background, with non-native English speakers experiencing increased scoring errors in such cases. No significant gender bias was observed. These findings highlight that prompt-based LLMs can perpetuate bias, especially related to language background, suggesting a need for debiasing strategies tailored to these models.

## Method Summary
The study used a dataset of over 25,000 argumentative essays from students with known demographic attributes. GPT-4o was prompted to predict both the demographic attributes (gender and first-language background) and assign essay scores. The authors then applied multiple fairness metrics (e.g., demographic parity, equal opportunity) and multivariate regression to analyze whether the model's ability to infer demographics influenced scoring fairness. They compared scoring accuracy and bias across different demographic groups, focusing on cases where the LLM correctly versus incorrectly inferred demographics.

## Key Results
- LLMs accurately inferred first-language background (accuracy 0.75–0.87) but were less accurate for gender (accuracy 0.86–0.96 with low coverage).
- Scoring bias was more pronounced when LLMs correctly identified students’ first-language background, with non-native English speakers experiencing increased scoring errors.
- No significant gender bias was observed in the scoring results.

## Why This Works (Mechanism)
The study demonstrates that LLMs can leverage subtle linguistic cues in student essays to infer demographic attributes, particularly first-language background. This inference ability interacts with the scoring process, leading to differential treatment of students based on their inferred demographics. The mechanism appears to be that when the LLM correctly identifies a student as a non-native English speaker, it may unconsciously adjust the score downward, introducing bias.

## Foundational Learning
- **Demographic Inference**: The ability of LLMs to infer student attributes from text is foundational to understanding how bias can be introduced. This is needed to assess whether LLMs can "recognize" demographics before scoring. Quick check: Verify inference accuracy across multiple trials and datasets.
- **Fairness Metrics**: Understanding metrics like demographic parity and equal opportunity is essential to quantify bias. This is needed to rigorously evaluate whether scoring is fair across groups. Quick check: Ensure metrics are applied consistently and interpreted correctly.
- **Multivariate Regression**: Used to isolate the effect of demographic inference on scoring while controlling for other variables. This is needed to establish causality between inference and bias. Quick check: Confirm model assumptions and robustness of results.

## Architecture Onboarding
- **Component Map**: Student essays -> LLM inference (demographics) -> LLM scoring -> Fairness analysis (metrics + regression)
- **Critical Path**: Essay text is processed by LLM to infer demographics, then scored; fairness is assessed by comparing scores across demographic groups and analyzing the impact of correct/incorrect inference.
- **Design Tradeoffs**: Using a single LLM (GPT-4o) ensures consistency but limits generalizability; focusing on two demographics simplifies analysis but may miss intersectional effects.
- **Failure Signatures**: Low coverage for gender predictions suggests incomplete representation; high accuracy for first-language inference but increased bias when correct indicates a systematic issue.
- **First Experiments**:
  1. Replicate demographic inference and scoring with a different LLM (e.g., Claude) to test generalizability.
  2. Systematically vary demographic cues in essays to isolate their causal effect on scoring.
  3. Apply intersectional analysis to uncover potential biases not captured by current metrics.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- The study relies on a single essay scoring task and a specific LLM (GPT-4o), which may not represent how bias manifests across different educational contexts or other LLMs.
- The demographic inference accuracy was based on a subset of essays with clear demographic indicators, and low gender prediction coverage suggests incomplete representation.
- The study does not account for potential confounding factors such as essay topic, prompt specificity, or variations in student writing style.

## Confidence
- **High**: Confidence in the finding that LLMs can accurately infer first-language background, supported by strong statistical results across multiple trials.
- **Medium**: Confidence in the claim that this inference leads to scoring bias, as the effect was observed but may be influenced by unmeasured variables.
- **Medium**: Confidence in the lack of gender bias, given the low coverage and potential sampling issues in the gender prediction data.

## Next Checks
1. Replicate the study using a broader range of LLMs (e.g., Claude, Llama) and essay types (narrative, persuasive) to assess generalizability.
2. Conduct a controlled experiment where demographic cues are systematically varied in essays to isolate their causal effect on scoring.
3. Apply additional fairness metrics, such as intersectional analysis or counterfactual fairness, to uncover potential biases not captured by the current approach.