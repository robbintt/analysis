---
ver: rpa2
title: An Empirical Study of Group Conformity in Multi-Agent Systems
arxiv_id: '2506.01332'
source_url: https://arxiv.org/abs/2506.01332
tags:
- agents
- conformity
- agent
- intelligence
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how multi-agent LLM systems exhibit group
  conformity dynamics similar to human behavior. Using over 2,500 debate simulations
  across five contentious topics, the research finds that neutral agents align with
  numerically dominant groups and higher-intelligence agents.
---

# An Empirical Study of Group Conformity in Multi-Agent Systems

## Quick Facts
- **arXiv ID**: 2506.01332
- **Source URL**: https://arxiv.org/abs/2506.01332
- **Reference count**: 31
- **One-line result**: Neutral LLM agents align with numerically dominant groups and higher-intelligence agents, with intelligence exerting stronger influence than numerical advantage.

## Executive Summary
This study investigates how multi-agent LLM systems exhibit group conformity dynamics similar to human behavior. Using over 2,500 debate simulations across five contentious topics, the research finds that neutral agents align with numerically dominant groups and higher-intelligence agents. Statistical analysis reveals a majority effect, with single high-intelligence agents exerting more influence than larger groups of lower-intelligence agents. The findings show conformity rates increase with numerical advantage and model intelligence, with effect sizes indicating intelligence has the strongest impact on conformity.

## Method Summary
The study uses paired comparison design with 2,000 simulations across five debate topics and ten scenarios manipulating agent count (1 vs 2) and model size (large vs small). Each debate involves 3 turns with 3 speaking opportunities per side per turn. The neutral agent (GPT-4o) evaluates persuasiveness after each turn and selects which side was more persuasive. Conformity Rate (CR) measures the proportion of proponent-supporting turns, while Full Conformity Ratio (FCR) measures complete alignment across all turns. Statistical validation uses Chi-square tests and Welch's ANOVA with effect sizes.

## Key Results
- Neutral agents show significant majority conformity, with CR reaching 72.11% when facing a 2:1 numerical disadvantage
- Intelligence has larger effect size than majority advantage (η²p ≈ 0.1665 vs. 0.068)
- Single high-intelligence agents influence neutral agents more effectively than groups of lower-intelligence agents
- Conformity rates increase with numerical advantage and model intelligence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Neutral LLM agents align with numerically dominant groups due to repeated exposure to majority-consistent arguments.
- **Mechanism**: Cumulative rhetorical weight from repeated majority arguments increases alignment probability.
- **Core assumption**: Agents process arguments independently and aggregate persuasiveness across turns.
- **Evidence anchors**: Significant majority conformity (χ² = 164.839, p < 0.001); CR reaches 72.11% with 2:1 majority.
- **Break condition**: Matched quality and quantity from both sides should approach 50% conformity.

### Mechanism 2
- **Claim**: Higher-capability models produce more persuasive arguments, leading to greater conformity even when outnumbered.
- **Mechanism**: Larger models generate more coherent, logically structured responses perceived as higher-quality.
- **Core assumption**: Parameter size is a valid proxy for persuasion quality.
- **Evidence anchors**: Intelligence shows larger effect size (η²p ≈ 0.1665); scenario (j) shows 66.33% CR with 1 superior agent vs 2 inferior opponents.
- **Break condition**: Shared architecture between neutral and inferior models might reduce superiority conformity.

### Mechanism 3
- **Claim**: Majority groups exhibit argument escalation while minority groups concede entirely.
- **Mechanism**: Imbalanced debates lead to majority reinforcement and minority spiral of silence.
- **Core assumption**: Agents interpret debate dynamics socially, not just as argument evaluation.
- **Evidence anchors**: Qualitative examples show polarization and spiral of silence patterns.
- **Break condition**: Fewer turns or position-maintenance instructions should attenuate these dynamics.

## Foundational Learning

- **Concept: Group Conformity (Social Psychology)**
  - Why needed here: The paper frames LLM behavior in terms of classic conformity experiments. Without this background, you won't recognize borrowed constructs like "majority effect" and "spiral of silence."
  - Quick check question: What is the difference between normative social influence (conforming to fit in) and informational social influence (conforming because you believe others are correct)?

- **Concept: Proxy Metrics for Intelligence**
  - Why needed here: The study uses model parameter size as a proxy for "intelligence" based on MMLU correlations. This is a simplifying assumption, not a direct measure of reasoning ability.
  - Quick check question: If two models have identical parameter counts but different training data compositions, would you expect identical conformity effects? Why or why not?

- **Concept: Effect Size Interpretation (η²p)**
  - Why needed here: The paper reports partial eta-squared values to compare majority vs. intelligence effects. You need to know that η²p ~0.01 is small, ~0.06 is medium, and >0.14 is large.
  - Quick check question: A statistically significant p-value with η²p = 0.02 indicates what about practical importance?

## Architecture Onboarding

- **Component map**: Debate orchestrator -> Agent pool (GPT-4o-mini, GPT-3.5-turbo, Claude-3-Sonnet/Haiku, Qwen-2.5) -> Turn manager -> Conformity logger

- **Critical path**: 1) Load topic and assign agent roles. 2) Run debate loop with randomized speaking order. 3) Neutral agent evaluates and selects most persuasive side. 4) Log conformity decision. 5) Aggregate across repetitions and apply statistical tests.

- **Design tradeoffs**: Paired comparison design controls for topic bias but limits generalizability. Parameter size as intelligence proxy is pragmatic but conflates scale with reasoning quality. Temperature = 0.7 introduces variability but may increase noise.

- **Failure signatures**: Low-quality debates with small models produce erratic conformity. Neutral agent bias can confound measurements. Prompt framing effects may introduce artifacts.

- **First 3 experiments**: 1) Reproduce majority effect (scenarios a-d): Fix model family, vary proponent/opponent count (1 vs 2). Expect CR ~60-70%. 2) Reproduce intelligence effect (scenarios e-f): Fix agent count at 1:1, vary model size. Expect CR ~70% for superior agent. 3) Test interaction boundary (scenario j): 1 superior agent vs 2 inferior agents. If CR > 50%, intelligence outweighs majority.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do conformity dynamics change when human participants interact with LLM agents, compared to purely agent-to-agent interactions?
  - Basis: Limitations section states future studies should explore human-AI interactions.
  - Why unresolved: Study only examined LLM-LLM interactions.
  - Resolution: Experiments with human participants as neutral evaluators or debaters.

- **Open Question 2**: To what extent do conformity patterns generalize across languages and cultural contexts beyond English?
  - Basis: Limitations note conducting debates exclusively in English may introduce biases.
  - Why unresolved: All simulations were in English using Western-centric topics.
  - Resolution: Cross-linguistic experiments with translated topics and cultural adaptations.

- **Open Question 3**: How robust are conformity effects when the neutral agent uses different models?
  - Basis: Methodology always used GPT-4o as neutral agent; dependence on this choice is unstated.
  - Why unresolved: Neutral agent's model characteristics may influence conformity susceptibility.
  - Resolution: Re-running experiments with different models as neutral evaluator.

- **Open Question 4**: What specific interventions effectively mitigate bias amplification in multi-agent LLM discourse?
  - Basis: Abstract calls for policy measures promoting diversity and transparency but doesn't propose or test any.
  - Why unresolved: Study identifies problem but doesn't evaluate solutions.
  - Resolution: Experiments testing minority-view protection prompts, rotating neutral agents, or transparency disclosures.

## Limitations
- Model capability proxy using parameter count assumes monotonic improvement in persuasion quality without controlling for training data differences
- Only five contentious topics tested, limiting generalization to less polarized or culturally specific issues
- Single neutral agent (GPT-4o) may exhibit inherent biases toward certain argument styles or model families

## Confidence
- **High**: Majority effect exists (CR increases with numerical advantage) — strong statistical evidence (χ² = 164.839, p < 0.001)
- **Medium**: Intelligence effect dominates majority effect in 1:1 comparisons — supported by effect size differences but not tested across all model pairs
- **Low**: Emergent dynamics (polarization, spiral of silence) are reliably reproducible — based on qualitative examples only

## Next Checks
1. Replicate with alternative intelligence proxies using model-specific benchmark scores rather than parameter count
2. Expand topic diversity to neutral policy topics to assess generalization beyond contentious issues
3. Introduce additional neutral agents from different model families to test whether conformity patterns hold across evaluators