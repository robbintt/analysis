---
ver: rpa2
title: 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and
  Question Decomposition'
arxiv_id: '2510.08818'
source_url: https://arxiv.org/abs/2510.08818
tags:
- video
- question
- visual
- compression
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of adapting image-pretrained
  vision-language models (VLMs) to video understanding, specifically the perception
  bottleneck and token overload. The perception bottleneck arises from static compression
  strategies that fail to retain salient information distributed unevenly across temporal
  and spatial dimensions.
---

# D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition

## Quick Facts
- **arXiv ID**: 2510.08818
- **Source URL**: https://arxiv.org/abs/2510.08818
- **Reference count**: 27
- **Primary result**: First training-free method to surpass training-required models on EgoSchema (58.0% vs 51.7%)

## Executive Summary
This paper addresses the challenges of adapting image-pretrained vision-language models (VLMs) to video understanding, specifically the perception bottleneck and token overload. The perception bottleneck arises from static compression strategies that fail to retain salient information distributed unevenly across temporal and spatial dimensions. Token overload occurs when compressed video inputs still exceed the processing capacity of image-pretrained VLMs. To address these issues, the authors propose D-CoDe, a training-free framework that combines dynamic compression with question decomposition. Dynamic compression adaptively selects representative frames and performs content-aware spatial token pruning and merging to preserve essential visual information while reducing redundancy. Question decomposition reformulates complex queries into focused sub-questions, guiding the model to attend to distinct aspects of the video and enabling comprehensive understanding. Experiments show that D-CoDe consistently improves performance across multiple-choice and open-ended VideoQA benchmarks.

## Method Summary
D-CoDe is a training-free framework that adapts image-pretrained VLMs to video understanding through two key mechanisms: dynamic compression and question decomposition. Dynamic compression operates in both temporal and spatial dimensions - temporally by selecting semantically diverse frames through uniform sampling augmented with dissimilarity-based supplementary frame selection, and spatially by pruning low-activation tokens and merging semantically similar ones. Question decomposition reformulates complex queries into focused sub-questions using an LLM, processes each sub-question independently with the compressed visual tokens, and aggregates the sub-answers to provide comprehensive context for final inference. The framework uses LLaVA-NeXT-7B with RoPE scaling for 8192 tokens and CLIP visual features for semantic encoding.

## Key Results
- Achieves 58.0% accuracy on EgoSchema, surpassing the best training-required model (51.7%)
- Outperforms baseline by 6.2 points on EgoSchema using only spatial compression
- Shows consistent improvements across multiple-choice and open-ended VideoQA benchmarks
- First training-free method to surpass training-required models on challenging long-video benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Dissimilarity Frame Selection
- Claim: Augmenting uniform sampling with semantically diverse supplementary frames captures informative content that fixed-interval sampling misses.
- Mechanism: After uniform sampling (⌊α·N⌋ frames), iteratively select frames with lowest average cosine similarity to already-selected frames using CLIP global features (Eq. 3-4). This prioritizes segments with higher semantic variation.
- Core assumption: Salient events are non-uniformly distributed in time; semantic dissimilarity proxies informativeness.
- Evidence anchors:
  - [section 3.2.2] "supplementary frames are selected based on their semantic dissimilarity to uniformly sampled ones"
  - [table 5] Supplementary frame selection (51.8%) outperforms both uniform (50.6%) and question-aware sampling (51.4%) on EgoSchema
  - [corpus] Weak direct evidence; related work (LongVU) uses query-guided selection but different criterion
- Break condition: Videos where key events are temporally uniform (e.g., continuous surveillance) would not benefit; α too low removes global coverage (Table 9 shows α=0.80 drops to 57.2%).

### Mechanism 2: Activation-Guided Token Pruning and Merging
- Claim: Filtering low-activation tokens and merging semantically similar ones reduces spatial redundancy while preserving discriminative features.
- Mechanism: (1) Compute L2 norm per token as salience proxy; retain top-⌊β·M⌋ (Eq. 5-6). (2) Greedily merge tokens exceeding cosine similarity threshold τ via mean pooling (Eq. 7-9).
- Core assumption: Token activation magnitude correlates with information content; high cosine similarity indicates semantic redundancy.
- Evidence anchors:
  - [section 3.2.2] "token compression is applied to each selected frame, which involves pruning uninformative visual tokens based on their activation magnitudes"
  - [figure 5] Visualizes pruning and clustering pipeline
  - [table 6] No spatial constraint (51.8%) vs ≤4 neighbors (51.4%) shows broader merge ranges help
  - [corpus] VideoNSA applies sparse attention to video; similar token-reduction intent but architectural approach differs
- Break condition: Scenes requiring fine-grained spatial detail (e.g., counting small objects) may lose critical information if β or τ too aggressive.

### Mechanism 3: Sub-Question Answer Aggregation
- Claim: Reformulating complex queries into focused sub-questions with intermediate answers enables progressive, multi-aspect video understanding.
- Mechanism: LLM generates n sub-questions targeting temporal/dynamic aspects; each answered independently over shared visual tokens; answers concatenated as auxiliary context for final inference (Eq. 11-13).
- Core assumption: Decomposed answers provide diverse supporting evidence; LLM can integrate multi-perspective context better than processing all information in single forward pass.
- Evidence anchors:
  - [section 3.2.3] "question decomposition reformulates the original query into a sequence of focused sub-questions"
  - [table 8] Sub-answers (58.0%) substantially outperform sub-questions alone (50.4%) and no decomposition (51.8%)
  - [figure 2b] Accuracy gap widens with more tokens, showing scalability benefit
  - [corpus] Entailment tree reasoning (Commonsense VQA) uses decomposition but with different aggregation logic
- Break condition: Simple questions (e.g., "what color is the car?") decompose into unnecessary complexity, hurting performance (Table 13 shows MSVD drops 80.0→72.4 with decomposition).

## Foundational Learning

- **CLIP Visual Features and Cosine Similarity**
  - Why needed here: Frame selection and token merging both rely on CLIP embeddings and cosine similarity as the semantic distance metric.
  - Quick check question: Given two CLIP image embeddings [0.6, 0.8] and [0.8, 0.6], compute their cosine similarity. (Answer: 0.96)

- **Token Activation as Salience Proxy**
  - Why needed here: Spatial pruning uses L2 norm of visual tokens to rank informativeness before merging.
  - Quick check question: Why might L2 norm correlate with token importance in vision transformers? (Answer: Higher activations often indicate stronger response to discriminative features; low activations may correspond to background/uninformative regions.)

- **In-Context Learning and Context Window Limits**
  - Why needed here: Question decomposition addresses token overload by distributing reasoning across multiple LLM calls rather than exceeding context capacity.
  - Quick check question: If a video produces 7200 visual tokens but your LLM has 4096 token context window, what failure mode occurs? (Answer: Truncation or attention degradation; model cannot process full input effectively.)

## Architecture Onboarding

- **Component map:**
  Video → Frame extraction → CLIP encoding → Temporal selection (Eq. 3-4) → Per-frame spatial compression (Eq. 5-9) → Token concatenation (Eq. 10) → Question decomposition (Eq. 11) → Sub-answer generation (Eq. 12) → Final inference (Eq. 13)

- **Critical path:**
  Video → Frame extraction → CLIP encoding → Temporal selection (Eq. 3-4) → Per-frame spatial compression (Eq. 5-9) → Token concatenation (Eq. 10) → Question decomposition (Eq. 11) → Sub-answer generation (Eq. 12) → Final inference (Eq. 13)

- **Design tradeoffs:**
  - **α (uniform ratio)**: Lower = more diverse frames but weaker global coverage; optimal at 0.85 (Table 9)
  - **β (retention ratio)**: Higher = more tokens retained; optimal at 0.625 (Table 10)
  - **τ (merge threshold)**: Lower = more aggressive merging; optimal at 0.9 (Table 11)
  - **Latency vs. accuracy**: Question decomposition adds ~6x latency (3.9s→37.4s per sample); limiting sub-questions to 5 reduces to 26.3s with minor accuracy drop (Table 17)

- **Failure signatures:**
  - **Frequent scene transitions**: Accuracy drops sharply on MSRVTT scene-change samples (64.2→56.0 vs. SF-LLaVA's 65.8→64.0; Table 15)
  - **Simple spatial queries**: Over-decomposition hurts (MSVD: "what is man sitting on?" decomposed into 6 sub-questions; Table 14)
  - **Duration/timestamp understanding**: Common Vid-LLM limitation; D-CoDe does not address explicitly (Limitations section)

- **First 3 experiments:**
  1. **Ablate spatial vs. temporal compression**: Run baseline, +spatial only, +temporal only, +both on EgoSchema with 10 frames to isolate contribution (expect ~6pt gain from spatial, ~1pt from temporal per Table 4)
  2. **Token count scaling test**: Vary β from 0.5 to 0.8 and measure accuracy vs. latency tradeoff on a long-video benchmark; identify knee point
  3. **Decomposition applicability threshold**: Test question decomposition on simple vs. complex QA datasets; determine heuristic for when to skip decomposition (e.g., question length < 10 tokens or single-clause structure)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating a slow-fast architecture into D-CoDe improve handling of videos with frequent scene transitions?
- Basis in paper: [explicit] The authors state in the Limitations section: "future work could explore integrating a slow-fast architecture into D-CoDe to better balance temporal and spatial modeling."
- Why unresolved: D-CoDe underperforms on MSRVTT-QA (Table 15 shows accuracy drops from 64.2% to 56.0% on scene-change samples), and the current uniform sampling with supplementary frames may not capture rapid transitions effectively.
- What evidence would resolve it: A modified D-CoDe with dual-stream processing evaluated on a benchmark of scene-transition-heavy videos, showing reduced accuracy gaps compared to methods like SF-LLaVA.

### Open Question 2
- Question: Would incorporating a memory bank enhance D-CoDe's long-range temporal reasoning and context retention?
- Basis in paper: [explicit] The Limitations section notes: "incorporating a memory bank...may further improve the model's ability to handle complex video inputs."
- Why unresolved: While question decomposition helps guide attention, the framework lacks explicit mechanisms to maintain persistent temporal state across extended video sequences, particularly for long-form benchmarks like EgoSchema.
- What evidence would resolve it: Ablation experiments comparing D-CoDe with and without memory bank integration on long-video benchmarks (e.g., EgoSchema, ActivityNet-QA), measuring both accuracy and the model's ability to reference earlier frames when answering later questions.

### Open Question 3
- Question: Under what conditions does question decomposition improve versus harm performance, and can adaptive decomposition strategies be developed?
- Basis in paper: [inferred] Table 13 shows question decomposition hurts open-ended VideoQA accuracy (MSVD: 80.0%→72.4%), and Table 14 shows over-decomposition of simple queries.
- Why unresolved: The current approach applies decomposition uniformly regardless of question complexity, suggesting a need for criteria or a classifier to determine when decomposition is beneficial.
- What evidence would resolve it: Analysis correlating question complexity metrics (e.g., number of reasoning steps, temporal scope) with decomposition effectiveness, followed by experiments with adaptive decomposition strategies.

### Open Question 4
- Question: Can training-free approaches achieve precise timestamp and duration understanding, or does this require architectural modifications or task-specific training?
- Basis in paper: [explicit] The Limitations section states: "understanding durations and timestamps...Addressing this may require task-specific training or architectural modifications, as shown in LLaVA-ST."
- Why unresolved: D-CoDe handles relative temporal reasoning through frame selection but lacks explicit temporal encoding mechanisms for absolute time references.
- What evidence would resolve it: Evaluation on temporal grounding benchmarks with timestamp-based questions, comparing D-CoDe against methods with explicit temporal encoding.

## Limitations

- Question decomposition shows inconsistent performance, significantly harming accuracy on simple open-ended questions (MSVD drops from 80.0% to 72.4%)
- Struggles with videos containing frequent scene transitions, where accuracy drops sharply compared to specialized models
- Significant latency increase from decomposition (3.9s to 37.4s per sample) presents practical deployment challenges

## Confidence

- **High Confidence**: The core claim that dynamic compression effectively addresses token overload is well-supported by ablation studies (Table 4 shows 7.2-point improvement from spatial compression alone). The finding that D-CoDe is the first training-free method to surpass training-required models on EgoSchema (58.0% vs 51.7%) is directly verifiable from Table 3.

- **Medium Confidence**: The effectiveness of question decomposition shows mixed evidence. While it provides substantial gains on complex benchmarks (EgoSchema, IntentQA), it harms performance on simple open-ended questions (MSVD). The claim that decomposition enables comprehensive multi-aspect understanding is supported but requires careful application heuristics.

- **Medium Confidence**: The semantic-dissimilarity frame selection mechanism shows promising results (51.8% vs 50.6% for uniform sampling on EgoSchema), but the assumption that CLIP-based semantic dissimilarity perfectly proxies informativeness needs further validation, particularly for domains where CLIP embeddings may not capture task-relevant semantics.

## Next Checks

1. **Ablation study on simple vs. complex questions**: Systematically test question decomposition on a spectrum of question complexities (from single-object queries to multi-step reasoning) to develop a reliable heuristic for when to apply decomposition versus using direct processing.

2. **Cross-dataset hyperparameter validation**: Evaluate the robustness of the fixed hyperparameters (α=0.85, β=0.625, τ=0.9) across diverse video domains including surveillance footage, sports, and educational content to identify domain-specific tuning requirements.

3. **Temporal understanding benchmark**: Create a controlled benchmark specifically testing timestamp and duration comprehension to quantify D-CoDe's performance on this common Vid-LLM limitation and identify whether the current framework can be extended to address temporal reasoning gaps.