---
ver: rpa2
title: 'Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback'
arxiv_id: '2508.08486'
source_url: https://arxiv.org/abs/2508.08486
tags:
- data
- cardinal
- cdpo
- preference
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper shows that preference-based alignment methods like
  RLHF and DPO cannot reliably select the best model because they only use ordinal
  feedback (A B). The authors prove an impossibility result: without knowing the strength
  of preferences, algorithms cannot correctly resolve tradeoffs across prompts.'
---

# Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback

## Quick Facts
- arXiv ID: 2508.08486
- Source URL: https://arxiv.org/abs/2508.08486
- Reference count: 40
- Key outcome: Cardinal feedback via willingness-to-pay improves alignment by 50% over ordinal methods and wins 55% more Arena-Hard battles

## Executive Summary
This paper demonstrates that preference-based alignment methods like RLHF and DPO are fundamentally limited because they only capture ordinal preferences (A > B) without measuring preference strength. The authors prove that without knowing how much better one option is than another, alignment algorithms cannot reliably resolve tradeoffs across different prompts. To address this, they introduce cardinal feedback collection using willingness-to-pay elicitation, creating the CARDINAL PREFS dataset of 25,000 judgments. Their CDPO method, trained with cardinal feedback, outperforms standard DPO by 50% on ground-truth reward and wins significantly more human preference battles.

## Method Summary
The authors develop a novel approach to collect cardinal human feedback through willingness-to-pay (WTP) elicitation, where annotators specify how much they would pay for one model's output over another. This creates a dataset of 25,000 judgments across 32 prompts and 8 models. They then implement CDPO (Cardinal DPO), a variant of DPO that incorporates preference strength information. The method uses WTP values to weight preference comparisons during training, allowing the model to learn not just which responses are preferred but by how much. The CARDINAL PREFS dataset is released alongside the method to enable further research.

## Key Results
- CDPO achieves 50% higher ground-truth reward than DPO when trained on the CARDINAL PREFS dataset
- CDPO wins 55% more Arena-Hard battles compared to DPO-trained models
- CDPO better prioritizes substantive improvements over stylistic ones compared to DPO

## Why This Works (Mechanism)
The mechanism leverages willingness-to-pay values as a proxy for preference strength, enabling the alignment algorithm to distinguish between strong and weak preferences. This additional information helps resolve the tradeoff problem where models must balance competing preferences across different prompts. By incorporating cardinal information into the training objective, CDPO can more accurately optimize for the underlying reward function that generated the preferences.

## Foundational Learning
None

## Architecture Onboarding
None

## Open Questions the Paper Calls Out
None

## Limitations
- The CARDINAL PREFS dataset is relatively small (25,000 judgments) and may not be fully representative of broader user preferences
- Empirical superiority claims are based on model-based reward estimates and crowdsourced preference battles rather than real-world task performance
- The approach focuses on short-form generation tasks and may not scale to complex reasoning or planning scenarios
- The willingness-to-pay elicitation protocol may introduce bias based on how participants interpret monetary values
- The theoretical impossibility result assumes preferences satisfy certain axioms that may not hold in practice

## Confidence
- Theoretical impossibility result: High confidence
- Dataset representativeness: Medium confidence
- Empirical superiority claims: Medium confidence

## Next Checks
1. Test CDPO on longer-form generation tasks (500+ tokens) to validate whether cardinal feedback scales to complex reasoning and planning scenarios
2. Conduct real-world deployment studies measuring CDPO's impact on downstream task performance rather than just preference battles
3. Evaluate robustness of CDPO to different WTP elicitation protocols and pricing mechanisms to ensure the approach isn't overly sensitive to specific implementation details
4. Investigate whether the theoretical assumptions underlying the impossibility result hold in practice across diverse preference distributions