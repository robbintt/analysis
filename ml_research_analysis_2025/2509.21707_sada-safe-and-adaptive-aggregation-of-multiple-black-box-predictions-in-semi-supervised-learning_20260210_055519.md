---
ver: rpa2
title: 'SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised
  Learning'
arxiv_id: '2509.21707'
source_url: https://arxiv.org/abs/2509.21707
tags:
- sada
- estimator
- data
- predictions
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SADA, a method for safely and adaptively aggregating
  multiple black-box predictions in semi-supervised learning for both inference and
  prediction tasks. SADA guarantees that its estimator never performs worse than using
  labeled data alone, regardless of prediction quality, and adaptively leverages accurate
  predictions to achieve faster convergence rates or the semiparametric efficiency
  bound when any prediction perfectly fits the ground truth.
---

# SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2509.21707
- Source URL: https://arxiv.org/abs/2509.21707
- Reference count: 40
- Guarantees estimator never performs worse than labeled-only baseline while adapting to accurate predictions

## Executive Summary
SADA introduces a method for safely and adaptively aggregating multiple black-box predictions in semi-supervised learning for both inference and prediction tasks. The method guarantees that its estimator never performs worse than using labeled data alone, regardless of prediction quality, while adaptively leveraging accurate predictions to achieve faster convergence rates or the semiparametric efficiency bound when any prediction perfectly fits the ground truth. Theoretical analysis establishes safety (guaranteed efficiency gains over labeled-only methods) and adaptivity (automatic identification and exploitation of accurate predictions). The method is evaluated through simulations showing superior performance to PPI and PPI++ methods, and two real-data applications: regression on politeness scores from online requests (where SADA consistently outperforms other methods) and image classification on ImageNet-5 (where SADA achieves the best Top-1 accuracy across all labeled-unlabeled ratios). An R package is provided for implementation.

## Method Summary
SADA extends prediction-powered inference to multiple black-box predictors for both inference and prediction tasks. For inference, it computes optimal weights that minimize asymptotic variance of an augmented estimating equation, guaranteeing safety (never worse than labeled-only baseline) and adaptivity (automatic efficiency gains from accurate predictors). For prediction, SADA minimizes an adaptively weighted empirical loss that reduces variance of the risk estimator. The method uses optimal weight matrices derived from variance-covariance structure of estimating equations and prediction losses, enabling it to automatically identify and exploit accurate predictions while remaining robust to poor ones. Implementation involves computing optimal weights from combined labeled and unlabeled data, then solving weighted estimating equations (inference) or iteratively optimizing weighted loss functions (prediction).

## Key Results
- SADA guarantees asymptotic variance never exceeds labeled-only baseline, with theoretical proof showing positive semi-definite efficiency gain term
- When any prediction perfectly matches the ground truth, SADA achieves the semiparametric efficiency bound with N^(-1/2) convergence rate
- Empirical evaluation on politeness regression shows SADA consistently outperforms PPI and PPI++ across all sample sizes
- On ImageNet-5 image classification, SADA achieves best Top-1 accuracy across all labeled-unlabeled ratios compared to standard semi-supervised methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SADA guarantees its estimator never has larger asymptotic variance than the labeled-only baseline.
- Mechanism: The optimal weight matrix W_opt is derived by minimizing mean squared error within a family of unbiased estimators. The resulting variance formula Σ_opt = Σ_nv − (N−n)/N · Σ_g contains a positive semi-definite "efficiency gain" term Σ_g that is always ≥ 0 by construction, regardless of prediction quality.
- Core assumption: The estimating equation s(X,Y;θ*) has bounded moments; labeled and unlabeled data share the same marginal distribution of X and Ŷ.
- Evidence anchors:
  - [abstract] "it never performs worse than using the labeled data alone, regardless of the quality of the predictions"
  - [Section 3, Theorem 1] Shows Σ_opt = Σ_nv − (N−n)/N · Σ_g with Σ_g positive semi-definite
  - [corpus] CaliMatch paper addresses "safe semi-supervised learning" but with different mechanisms for label distribution mismatch
- Break condition: If cov(Ŷ, Y) = 0 for all predictions, the gain term vanishes and SADA reduces exactly to the naive estimator—safety is preserved but no benefit gained.

### Mechanism 2
- Claim: When any prediction perfectly matches Y (or E[Y|X] for deterministic predictions), SADA automatically achieves faster convergence (n^−½ → N^−½) or the semiparametric efficiency bound.
- Mechanism: The optimal weight formula includes var{S(X,Ŷ;θ*)}^−1 · E{S(X,Ŷ;θ*)s(X,Y;θ*)^T}. When Ŷ_k ≡ Y, this matrix algebra collapses to assign weight (N−n)/N to the perfect predictor and 0 to others, effectively converting unlabeled data into "labeled" data.
- Core assumption: At least one Ŷ_k has non-zero covariance with Y; the variance matrix var{S(X,Ŷ;θ*)} is invertible.
- Evidence anchors:
  - [Section 3.1] Mean estimation example shows ω_opt = (N−n)/N · (1,0,...,0)^T when Ŷ_1 ≡ Y
  - [Section 3, Theorem 2] Formalizes both the oracle case and semiparametric efficiency case
  - [corpus] Corpus papers focus on SSL efficiency but don't address multi-predictor aggregation
- Break condition: If all predictions are independent of Y, the covariance term is zero and weights collapse to zero.

### Mechanism 3
- Claim: For prediction tasks, SADA minimizes an adaptively weighted empirical loss that reduces variance of the risk estimator.
- Mechanism: Rather than directly aggregating predictions, SADA constructs a family of unbiased loss estimators R̂(θ,ω) with data-dependent weights ω_opt(θ) that minimize E[{R̂(θ,ω) − R(θ)}²]. Lower variance in the empirical loss translates to tighter uniform deviation bounds and lower excess risk.
- Core assumption: Loss function is bounded; Dudley's entropy integral is finite (Assumption 2).
- Evidence anchors:
  - [Section 4, Equation 7] Defines the weighted loss function family
  - [Section 4, Theorem 3] Shows excess risk bound of O(√(N⁻¹)) when a perfect predictor exists
  - [corpus] Corpus lacks comparable prediction-focused SSL methods with theoretical guarantees
- Break condition: If the loss landscape has high complexity (large covering number), the uniform deviation bound may dominate.

## Foundational Learning

- Concept: **Estimating equations and M-estimation**
  - Why needed here: SADA defines θ* through E{s(X,Y;θ*)} = 0; understanding how solving empirical estimating equations yields consistent estimators is essential.
  - Quick check question: Given s(x,y;θ) = y − θ, what parameter θ* does the estimating equation define?

- Concept: **Semiparametric efficiency bounds**
  - Why needed here: Theorem 2(ii) claims SADA attains the semiparametric efficiency bound when Ŷ_k ≡ E[Y|X]; this requires understanding efficient influence functions.
  - Quick check question: In a model where only E[Y|X] is observed for unlabeled data, what is the efficient influence function for estimating E[Y]?

- Concept: **Prediction-powered inference (PPI)**
  - Why needed here: SADA extends PPI to multiple predictors; knowing PPI's single-predictor framework helps understand what SADA generalizes.
  - Quick check question: Why might PPI have worse efficiency than labeled-only estimation when predictions are poor?

## Architecture Onboarding

- Component map:
  - Input data (labeled and unlabeled) -> Weight estimation -> Aggregation (inference) or iterative optimization (prediction) -> Output θ̂_sada

- Critical path:
  1. Compute naive estimator θ̂_nv from labeled data only
  2. Estimate variance var{S(X,Ŷ;θ̂_nv)} and covariance E{S(X,Ŷ;θ̂_nv)s(X,Y;θ̂_nv)^T}
  3. Compute optimal weights Ŵ_opt = (N−n)/N · var{S(X,Ŷ;θ*)}^−1 · E{S(X,Ŷ;θ*)s(X,Y;θ*)^T}
  4. Solve the augmented estimating equation (3) to obtain θ̂_sada
  5. For prediction: iterate between weight updates and parameter updates (Algorithm 1)

- Design tradeoffs:
  - Weight estimation stability: Small n leads to noisy variance estimates; consider regularization or shrinkage toward zero
  - Multiple predictors vs. selection: SADA aggregates all predictors rather than selecting one; this adds robustness but may dilute signal if one predictor dominates
  - Inference vs. prediction mode: Inference mode optimizes parameter MSE; prediction mode optimizes risk variance—they differ when the loss landscape is non-convex

- Failure signatures:
  - Variance matrix singularity: If predictions are highly collinear, var{S(X,Ŷ;θ*)} may be poorly conditioned; add ridge regularization
  - No efficiency gain: If relative efficiency ≈ 1 across all methods, predictions are uninformative—verify prediction quality first
  - Divergent weights: If |Ŵ_opt| >> 1, numerical instability may occur; check data preprocessing

- First 3 experiments:
  1. Sanity check: Apply SADA to mean estimation with synthetic data where Ŷ_1 = Y (oracle) and Ŷ_2 = noise; verify weights are (1, 0) and variance equals N^−1·var(Y)
  2. Robustness test: Vary prediction quality γ from 0 to 1 in the simulation setup; confirm relative efficiency is always ≤ 1 and improves with better predictions
  3. Real data baseline: On a labeled dataset, mask labels to create semi-supervised setting; compare SADA against PPI, PPI++, and naive using multiple pretrained models as predictors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SADA framework be extended to handle distribution shifts, such as covariate or label shift, between the labeled and unlabeled datasets?
- Basis in paper: [explicit] The Discussion section explicitly states, "In settings with distribution shift, the current method should be adapted accordingly... We are going to investigate these extensions in future work."
- Why unresolved: The theoretical guarantees (Theorems 1 and 2) rely on the assumption that labeled and unlabeled data share the same marginal distribution; under shift, the augmentation terms in equation (3) are no longer unbiased.
- What evidence would resolve it: Derivation of modified weights that incorporate importance sampling or distributional adjustments, along with proofs maintaining safety and adaptivity guarantees under shift.

### Open Question 2
- Question: What are the theoretical convergence guarantees for the iterative optimization procedure (Algorithm 1) used for prediction tasks?
- Basis in paper: [inferred] Section 4 recommends an iterative algorithm to handle the interdependence of the parameter θ and the weights ω, but provides no formal analysis of its convergence properties or stability.
- Why unresolved: The optimization landscape is non-convex because the optimal weights ω_opt(θ) depend on the unknown parameter θ, potentially leading to local minima or oscillation.
- What evidence would resolve it: A theoretical proof establishing convergence conditions or empirical simulations analyzing the stability of the iterative updates against a joint optimization baseline.

### Open Question 3
- Question: How does the estimation stability and computational complexity of SADA scale as the number of black-box predictors (K) increases to be comparable with or larger than the sample size?
- Basis in paper: [inferred] The computation of the optimal weight W_opt in Proposition 1 involves inverting a covariance matrix of dimension Kp × Kp, which becomes unstable or computationally prohibitive when K is large.
- Why unresolved: The simulations and real-data examples only test small K (e.g., 2 or 3), leaving the method's behavior in high-dimensional aggregation settings (e.g., aggregating hundreds of models) uncharacterized.
- What evidence would resolve it: Introduction of regularization techniques for the covariance inversion and simulation studies analyzing estimation variance and runtime as K grows.

## Limitations
- Performance may degrade if predictions are highly collinear or when variance-covariance matrix is ill-conditioned
- Algorithm 1 convergence is not rigorously established for non-convex loss landscapes
- Real-data results depend on specific LLM prediction quality that may not transfer to other domains

## Confidence
- Safety guarantee: Medium confidence - supported by rigorous theoretical analysis but relies on well-conditioned variance-covariance matrix
- Adaptivity claim: Medium confidence - demonstrated both theoretically and empirically with clean perfect-predictor case
- Empirical evaluation: Low confidence for broad applicability - ImageNet-5 results depend on specific model choices and training procedures

## Next Checks
1. Test SADA with synthetic data where all predictions are highly correlated but only one is accurate to check robustness to collinearity
2. Apply SADA to a regression problem with continuous predictions to verify safety guarantee holds beyond binary classification
3. Compare SADA against simple prediction selection strategies (e.g., pick best single predictor) on datasets with heterogeneous prediction quality