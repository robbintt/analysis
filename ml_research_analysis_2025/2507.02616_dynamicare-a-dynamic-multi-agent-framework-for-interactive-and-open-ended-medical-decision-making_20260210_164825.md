---
ver: rpa2
title: 'DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended
  Medical Decision-Making'
arxiv_id: '2507.02616'
source_url: https://arxiv.org/abs/2507.02616
tags:
- patient
- system
- clinical
- information
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DynamiCare, a dynamic multi-agent framework
  for interactive, open-ended medical decision-making. It addresses the limitation
  of existing static multi-agent systems by proposing a dynamic doctor system that
  iteratively recruits, coordinates, and adjusts a specialist team based on evolving
  patient information.
---

# DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making

## Quick Facts
- arXiv ID: 2507.02616
- Source URL: https://arxiv.org/abs/2507.02616
- Reference count: 20
- Achieves up to 71.6% top-10 hit rate on clinical diagnosis benchmarks

## Executive Summary
DynamiCare introduces a dynamic multi-agent framework for interactive, open-ended medical decision-making. It addresses the limitations of static multi-agent systems by iteratively recruiting, coordinating, and adjusting a specialist team based on evolving patient information. The framework uses a patient system built on MIMIC-Patient, a structured dataset derived from MIMIC-III, to simulate realistic clinical interactions. Experiments show DynamiCare outperforms single-agent variants and achieves strong performance on MIMIC-Patient and MEDIQ benchmarks.

## Method Summary
DynamiCare models clinical diagnosis as a multi-round loop where a central agent dynamically adjusts team composition. The framework uses a patient system built on MIMIC-Patient, a structured dataset derived from MIMIC-III. The doctor system employs a Central Agent that reviews visit logs and issues JSON commands to add or remove specialists (max 5). Multi-specialist consensus with confidence-weighted voting yields more robust decisions. The system uses GPT-4.1 and GPT-4o-mini as backbone models, with evaluation based on ICD-9 code mapping and top-K hit/recall metrics.

## Key Results
- DynamiCare achieves 71.6% top-10 hit rate on MEDIQ benchmark
- Multi-agent GPT-4.1 achieves 63.4% Hit@5 vs. 58.0% for single-agent
- Patient system demonstrates high truthfulness and relevance in responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic team composition adjustment improves diagnostic accuracy over static multi-agent configurations
- Mechanism: Central Agent reviews visit log after each interaction and issues JSON commands to add/remove specialists (max 5), aligning team expertise with evolving clinical cues
- Core assumption: Correct specialist mix evolves as patient information accumulates
- Evidence: Abstract states framework "iteratively recruits, coordinates, and adjusts a specialist team based on evolving patient information"; section 4.2 describes continuous review of visit log
- Break condition: If Central Agent recruiting logic becomes noisy, latency and coordination overhead may exceed diagnostic benefit

### Mechanism 2
- Claim: Two-stage patient response pipeline reduces hallucination while preserving flexibility
- Mechanism: Queries undergo regex-based keyword extraction mapped to JSON sections; if unmatched or ambiguous, system falls back to GPT-4.1 with full patient context
- Core assumption: Structured fields can be reliably retrieved via keyword matching; only unstructured synthesis requires LLM inference
- Evidence: Section 4.1 describes two-stage answering process combining rule-based keyword matching with fallback LLM inference
- Break condition: If keyword dictionary coverage is sparse, false negatives will over-trigger fallback, increasing hallucination risk

### Mechanism 3
- Claim: Multi-specialist consensus with confidence-weighted voting yields more robust decisions
- Mechanism: Each specialist independently proposes response with confidence (1-5); peers vote; first proposal meeting agreement threshold is accepted, otherwise highest-confidence response selected
- Core assumption: Aggregated specialist opinions reduce individual model bias and reasoning gaps
- Evidence: Section 4.2 describes voting mechanism; Table 2 shows multi-agent GPT-4.1 achieves 63.4% Hit@5 vs. 58.0% for single-agent
- Break condition: If specialists are redundant or prompts lack diversity, voting collapses into self-agreement without true deliberation

## Foundational Learning

- Concept: ICD-9 Code Mapping & Top-K Hit/Recall Metrics
  - Why needed: Diagnoses evaluated by mapping predictions to 3-digit ICD-9 codes; Hit@K and Rec@K measure ranking quality across large label space
  - Quick check: Can you explain why matching only first 3 digits of ICD-9 codes relaxes evaluation while still being clinically meaningful?

- Concept: Multi-Round Dialogue State Management
  - Why needed: 6-step loop requires maintaining visit log (Q&A history) that grows each round and conditions subsequent agent decisions
  - Quick check: How would you design data structure to append Q&A pairs while keeping context within LLM token limits?

- Concept: Prompt Chaining with Structured JSON Outputs
  - Why needed: All coordination (triage, adjustment, voting) relies on LLMs emitting valid JSON for downstream parsing
  - Quick check: What failure modes occur if LLM outputs malformed JSON in Central Agent coordination prompt?

## Architecture Onboarding

- Component map: Patient System (keyword mapper → section retriever → GPT-4.1 answer generator) → Doctor System interface → Central Agent (triage + adjustment prompts) → Specialist Team (1-5 LLM agents) → consensus/voting module → diagnosis or follow-up question

- Critical path: 1. Initialize visit log from patient demographics + chief complaint 2. Central Agent triage → initial specialist list 3. Specialists analyze → propose question OR diagnosis 4. Patient System answers question via two-stage pipeline 5. Log update → Central Agent re-triggers adjustment 6. Repeat until diagnosis or max rounds

- Design tradeoffs: Keyword matching vs. full LLM retrieval (faster, more deterministic but brittle); Max 5 specialists (caps coordination cost but may truncate expertise); 3-digit ICD-9 matching (forgiving evaluation but may over-credit coarse guesses)

- Failure signatures: Infinite loop (specialists keep asking questions without reaching diagnosis); JSON parse errors (agent outputs invalid JSON); Hallucinated patient answers (fallback LLM invents facts not in patient JSON)

- First 3 experiments: 1. Ablate dynamic adjustment vs. full DynamiCare on Hit@5/10 to quantify marginal gain from dynamism 2. Replace keyword-based patient retrieval with pure LLM inference; measure truthfulness/relevance score degradation 3. Vary max specialists (1, 3, 5) and max interaction rounds (5, 10, 15) to identify saturation points in accuracy vs. latency

## Open Questions the Paper Calls Out

- Question: Can the framework effectively integrate non-textual clinical modalities (medical imaging, genomics, sensor data) to improve diagnostic accuracy?
- Basis: Section 7 states system currently operates only on textual and tabular data and suggests incorporating these modalities could enable richer reasoning
- Why unresolved: Current architecture and dataset construction focus exclusively on structured JSON and text extraction
- What evidence would resolve: Extension with multi-modal analysis specialist agents evaluated on dataset containing linked imaging and genetic data

- Question: Does simulating proactive patient behavior (volunteering information not directly asked for) improve diagnostic efficiency or accuracy?
- Basis: Section 7 notes real patients often volunteer important information and suggests future versions could simulate this behavior
- Why unresolved: Current Patient System is purely reactive, designed only to answer specific queries
- What evidence would resolve: Comparative study measuring diagnostic hit rates and interaction rounds between reactive and proactive patient agents

- Question: To what extent does integrating RAG or external medical knowledge bases enhance specialist team reliability?
- Basis: Section 7 suggests performance enhancements may come from integrating RAG or modular expert components
- Why unresolved: Current system relies solely on internal parametric knowledge of backbone LLMs
- What evidence would resolve: Ablation studies comparing baseline DynamiCare against RAG-equipped variant, specifically evaluating performance on rare diseases or recent clinical guidelines

## Limitations

- The framework currently operates only on textual and tabular data, lacking integration of non-textual clinical modalities
- The current patient system is purely reactive, unable to simulate proactive patient behavior where patients volunteer information
- The system relies solely on internal parametric knowledge of backbone LLMs, without integration of external knowledge bases or RAG

## Confidence

- **High confidence**: General architecture and workflow clearly specified and supported by experimental results showing improved Hit@5/10 metrics over single-agent baselines
- **Medium confidence**: Core claim that dynamic specialist team composition improves diagnostic accuracy is plausible and supported by relative performance gains, but exact contribution of dynamic adjustment is not isolated in ablation studies
- **Low confidence**: Robustness of two-stage patient response pipeline to novel or ambiguous queries is not empirically validated; reliance on keyword matching introduces brittleness only partially mitigated by fallback LLM inference

## Next Checks

1. Isolate dynamic adjustment benefit: Run ablation experiment comparing fixed specialist team versus full dynamic adjustment across same cases to quantify marginal diagnostic accuracy gain from dynamism

2. Test keyword coverage robustness: Systematically evaluate Patient System on held-out set of queries designed to stress-test keyword-to-section mapping, measuring fallback frequency and hallucination rate

3. Explore specialist redundancy: Vary maximum number of specialists (1, 3, 5) in controlled experiments to determine point of diminishing returns in accuracy versus coordination overhead