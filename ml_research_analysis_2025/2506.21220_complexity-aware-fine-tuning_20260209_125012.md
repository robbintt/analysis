---
ver: rpa2
title: Complexity-aware fine-tuning
arxiv_id: '2506.21220'
source_url: https://arxiv.org/abs/2506.21220
tags:
- data
- entropy
- answer
- complexity
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses efficient fine-tuning of small language models
  by identifying complex questions using entropy-based uncertainty estimation and
  applying different training strategies accordingly. The method splits data into
  complexity categories using token-wise answer entropy, applies standard supervised
  fine-tuning to easy/medium questions, and uses distilled chain-of-thought reasoning
  for hard questions from a larger teacher model.
---

# Complexity-aware fine-tuning

## Quick Facts
- arXiv ID: 2506.21220
- Source URL: https://arxiv.org/abs/2506.21220
- Authors: Andrey Goncharov; Daniil Vyazhev; Petr Sychev; Edvard Khalafyan; Alexey Zaytsev
- Reference count: 16
- Key outcome: Achieved 0.52 vs 0.39 accuracy on MMLU-Pro and 0.64 vs 0.51 on Phi-4-mini using 81% less data than full distillation, with entropy-based complexity estimation (ROC AUC 0.73).

## Executive Summary
This paper addresses the efficiency challenge of fine-tuning small language models by introducing a complexity-aware approach that routes training data based on question difficulty. The method uses token-level answer entropy to automatically categorize questions into easy, medium, and hard groups, then applies standard supervised fine-tuning to the easier questions while using distilled chain-of-thought reasoning for the hardest questions from a larger teacher model. This hybrid strategy achieves superior performance compared to both standard fine-tuning and full distillation approaches while significantly reducing computational costs.

## Method Summary
The pipeline first estimates question complexity by computing single-token answer entropy from the student model, achieving ROC AUC of 0.73 for detecting correctly answered vs incorrectly answered questions. Training data is then split into three equal parts (easy: 0-25% entropy, medium: 25-75%, hard: 75-100%). Standard supervised fine-tuning is applied to the combined easy+medium data for 5 epochs, followed by chain-of-thought distillation from a teacher model on the hard questions for another 5 epochs. This approach uses only 19% of the data required for full CoT distillation while maintaining or improving accuracy across MMLU-Pro, GSM8K, and MedMCQA benchmarks.

## Key Results
- Achieved 0.52 accuracy on MMLU-Pro for Qwen2.5-3B versus 0.39 for standard SFT
- Reduced data usage by 81% compared to full distillation while maintaining comparable or better accuracy
- ROC AUC of 0.73 for entropy-based complexity estimation, outperforming MASJ-based methods
- Consistent improvements across diverse domains: academic (MMLU-Pro), mathematical (GSM8K), and medical (MedMCQA)

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Token-level answer entropy from the student model provides a reliable, unsupervised signal for estimating question complexity, achieving a ROC AUC of 0.73.
**Mechanism:** A model's internal probability distribution over answer tokens reflects its uncertainty. High entropy indicates the model is torn between competing options, which correlates with the question being objectively harder or requiring knowledge the model lacks. This allows for automatic data categorization without manual labeling.
**Core assumption:** A model's token probability is a calibrated measure of its epistemic uncertainty and correlates with ground-truth question difficulty, not just miscalibration.
**Evidence anchors:**
- [abstract] "...split the training data into complexity categories by a single token answer entropy (ROC AUC 0.73)..."
- [section] "We confirm that entropy works as a difficulty estimation... Single-token answer entropy reaches ROC AUC values up to 0.8, clearly beating MASJ-based estimates" (Conclusion).
- [corpus] Supported by findings in Fadeeva et al. (2023) and Sychev et al. (2025) on entropy-based uncertainty estimation.
**Break condition:** If the model is systematically overconfident on hard questions (hallucination) or underconfident on easy ones, the entropy signal will be misleading, leading to suboptimal data splitting.

### Mechanism 2
**Claim:** A hybrid training strategy is optimal: standard SFT suffices for low-uncertainty data, while CoT distillation is necessary to unlock performance on high-uncertainty data.
**Mechanism:** Easy questions (low entropy) can be learned via direct pattern matching from SFT. Hard questions (high entropy) require explicit reasoning chains. The student model cannot generate these chains on its own, so distilling them from a capable teacher provides the necessary scaffolding for the student to learn the multi-step reasoning process.
**Core assumption:** The benefit of CoT distillation is not uniform; it provides marginal or no value on simple queries while being critical for complex ones, and the student model has sufficient capacity to learn the distilled reasoning.
**Evidence anchors:**
- [abstract] "...fine-tune LLMs via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach..."
- [section] "Standard supervised fine-tuning (SFT) is enough for the easy and medium bands, but lags on the hard band. For hard questions, adding a distilled chain of thought... unlocks further gains" (Conclusion).
- [corpus] General distillation principles are supported, e.g., in "Distill Not Only Data but Also Rewards."
**Break condition:** The mechanism fails if the "hard" data contains questions that are unsolvable by the student model's architecture, regardless of the training signal, or if the teacher model provides flawed reasoning.

### Mechanism 3
**Claim:** Applying CoT distillation only to the high-entropy data subset achieves performance comparable or superior to full distillation while using ~81% fewer tokens.
**Mechanism:** Full CoT distillation is expensive due to long sequences. By restricting it to the ~25% of data identified as "hard," the pipeline drastically cuts the token count for both teacher inference and student training, focusing computational resources where they yield the highest marginal return.
**Core assumption:** The computational cost of the initial entropy estimation pass is negligible compared to the savings from avoiding CoT generation on 75% of the dataset.
**Evidence anchors:**
- [abstract] "...outperforms the distillation approach (0.58 vs 0.56 average accuracy) while using 81% less data."
- [section] Table 1 and Table 2 show token savings of 79-82% with comparable or superior accuracy (e.g., Qwen 3B: 0.50 accuracy vs. 0.49 for full distillation with 79% token savings).
- [corpus] Related work (e.g., "SOAEsV2-7B/72B") emphasizes efficiency in domain adaptation.
**Break condition:** The efficiency claim breaks if the "hard" subset is very large (e.g., >50% of data) or if the overhead of the two-stage pipeline (multiple training runs, data handling) outweighs the token-computation benefits.

## Foundational Learning

**Concept: Entropy as a measure of uncertainty**
- **Why needed here:** The entire pipeline hinges on using answer token entropy. A high-entropy distribution (e.g., [0.33, 0.33, 0.33]) indicates high uncertainty, while a low-entropy distribution (e.g., [0.9, 0.05, 0.05]) indicates confidence.
- **Quick check question:** For a multiple-choice question with 4 options, which model output indicates higher uncertainty: a) token probabilities of [0.7, 0.1, 0.1, 0.1] or b) [0.25, 0.25, 0.25, 0.25]?

**Concept: Knowledge Distillation (Chain-of-Thought)**
- **Why needed here:** You must understand that the pipeline doesn't just copy answers; it transfers the teacher's *reasoning process*. This teaches the student "how to think" for complex problems.
- **Quick check question:** What is the fundamental difference between standard SFT (training on question-answer pairs) and CoT distillation (training on question-reasoning-answer triplets)?

**Concept: ROC AUC for evaluating a classifier**
- **Why needed here:** The paper uses ROC AUC to quantify how well entropy separates "correctly answered" from "incorrectly answered" questions. An AUC of 0.73 is better than random (0.5) but not perfect.
- **Quick check question:** If the entropy-based splitter had an ROC AUC of 0.55, would you expect the complexity-aware pipeline to outperform the baselines? Why or why not?

## Architecture Onboarding

**Component map:**
Uncertainty Estimator -> Data Splitter -> Multi-Stage Trainer

**Critical path:**
1. Verify the student model outputs log-probabilities for entropy calculation.
2. Validate the splitter: Check ROC AUC on a validation set to ensure entropy correlates with error.
3. Ensure the teacher model can reliably generate high-quality CoT for the hard subset.

**Design tradeoffs:**
- **Simplicity vs. Signal:** The paper uses single-token entropy over more complex CoT-aggregated methods. This is cheaper and empirically better but may fail if a model is confidently wrong (low entropy on a hard question).
- **Data Budget:** The evaluation balances groups, limiting training data. A production system might forgo balancing to use all available easy/medium data, changing the efficiency profile.
- **Thresholds:** The 25/50/25 split is a fixed heuristic. Optimal thresholds likely vary by model size and domain.

**Failure signatures:**
- **No Separation:** ROC AUC ≈ 0.5. Entropy is random; the pipeline degrades to random data selection.
- **Teacher Failure:** The teacher model produces flawed or unhelpful reasoning for the hard questions, causing the second training phase to add noise instead of signal.
- **Capacity Mismatch:** The student model plateaus on hard questions even with CoT distillation, indicating the task is fundamentally beyond its capacity.

**First 3 experiments:**
1. **Metric Validation:** Run the uncertainty estimator on a benchmark (e.g., MMLU-Pro). Calculate the ROC AUC to confirm entropy is a valid difficulty proxy for your specific model-domain pair.
2. **Ablation on Split Ratio:** Run the pipeline with different hard-set definitions (e.g., top 10%, top 25%, top 40% by entropy) to find the optimal balance between data efficiency and accuracy.
3. **Generalization Test:** Apply the full pipeline to a different domain (e.g., GSM8K for math or MedMCQA for medicine). Compare its performance against standard SFT and full distillation baselines to test domain robustness.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does the complexity-aware fine-tuning pipeline maintain its efficiency and performance advantages when applied to significantly larger student models (e.g., 7B–70B parameters) and open-ended generative tasks?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that the "pipeline is tested only on small models" (≈3B) and "results may differ... on larger LLMs" or "open-ended tasks."
- Why unresolved: The current study restricts validation to small models and multiple-choice formats where single-token entropy is easily calculable; it is unclear if entropy remains a reliable complexity proxy for generative tasks or larger architectures.
- What evidence would resolve it: Applying the identical pipeline to models like Llama-3-70B and open-ended benchmarks (e.g., GSM8K with open generation), comparing accuracy and data efficiency against the 3B baselines.

**Open Question 2**
- Question: Is the arbitrary division of data into three equal parts (easy/medium/hard) optimal for maximizing accuracy and data efficiency, or would adaptive thresholding yield better results?
- Basis in paper: [explicit] The Limitations section notes that the authors "split the data into 3 equal parts and did not explore other possible boundaries."
- Why unresolved: An even split assumes a uniform distribution of complexity which may not hold, potentially misclassifying "medium" questions that would benefit from reasoning or wasting CoT resources on "hard" questions that are actually unsolvable.
- What evidence would resolve it: An ablation study varying the percentile thresholds (e.g., 10/80/10 splits vs. 33/33/33 splits) and analyzing the resulting impact on accuracy and token usage.

**Open Question 3**
- Question: How does the reliability of single-token entropy as a complexity metric degrade in the presence of confident hallucinations, and can the metric be robustified?
- Basis in paper: [explicit] The paper acknowledges in the Limitations that "Low entropy can still correspond to hallucinations, which leads to imperfect identification of the question complexity."
- Why unresolved: High confidence (low entropy) does not guarantee correctness; the pipeline currently treats high-confidence errors as "easy" data suitable for standard SFT, potentially reinforcing errors.
- What evidence would resolve it: A quantitative analysis of the "false positive" rate of the entropy metric (instances of low-entropy but incorrect answers) and testing hybrid metrics that combine entropy with self-consistency checks.

## Limitations

**Model Size Restriction**: The pipeline is validated only on small models (~3B parameters), with explicit acknowledgment that results may differ significantly for larger LLMs.

**Fixed Data Split**: The authors use a rigid 25/50/25 split without exploring alternative boundaries that might be optimal for different domains or model sizes.

**Hallucination Vulnerability**: The entropy metric can be misled by confident but incorrect answers, potentially routing difficult hallucinated questions to the wrong training strategy.

## Confidence

**High Confidence**: The entropy-based complexity detection mechanism works as described (ROC AUC 0.73 confirmed in paper, supported by related work on uncertainty estimation). The general hybrid training strategy (SFT for easy/medium, CoT distillation for hard) is well-established in the literature.

**Medium Confidence**: The specific 25/50/25 split ratio for complexity bands is empirically effective but likely not optimal across all model sizes and domains. The 81% token savings claim depends on assumptions about dataset composition that may not hold universally.

**Low Confidence**: The generalization of these results to models outside the ~3B parameter range, domains with substantially different characteristics, or scenarios where the teacher model's reasoning quality is uncertain.

## Next Checks

1. **Cross-Domain Robustness Test**: Apply the complete pipeline to a diverse set of domains (e.g., code generation, commonsense reasoning, legal analysis) and systematically measure how the entropy-based complexity estimator's ROC AUC and the overall performance gains vary across domains. This would reveal whether the method's effectiveness is domain-specific or broadly applicable.

2. **Model Size Scaling Study**: Implement the pipeline for a range of student model sizes (e.g., 1B, 3B, 7B, 13B parameters) on the same datasets to identify the minimum model capacity threshold where the hybrid approach provides meaningful improvements over standard SFT. This would clarify whether the method is universally applicable or has strict model size requirements.

3. **Teacher Quality Sensitivity Analysis**: Conduct controlled experiments varying the teacher model's capability (e.g., using different sized teacher models or introducing controlled noise into the CoT generation) to quantify how sensitive the student's performance is to teacher quality. This would establish whether the method requires state-of-the-art teachers or can work with more modest teacher models.