---
ver: rpa2
title: 'RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback'
arxiv_id: '2507.15024'
source_url: https://arxiv.org/abs/2507.15024
tags:
- critic
- refcritic
- critique
- performance
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RefCritic, a critic model trained via reinforcement
  learning with dual rewards to improve both critique quality and policy model refinement.
  It addresses the limitation of supervised fine-tuning in producing deep, actionable
  critiques for complex reasoning tasks.
---

# RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback

## Quick Facts
- arXiv ID: 2507.15024
- Source URL: https://arxiv.org/abs/2507.15024
- Reference count: 13
- This paper introduces RefCritic, a critic model trained via reinforcement learning with dual rewards to improve both critique quality and policy model refinement, achieving 6.8–7.2% gains in pass@1 after refinement and 3.6% gains in majority voting with 64 samples.

## Executive Summary
This paper addresses the limitation of supervised fine-tuning in producing deep, actionable critiques for complex reasoning tasks by introducing RefCritic, a critic model trained via reinforcement learning with dual rewards to improve both critique quality and policy model refinement. The approach uses instance-level correctness judgments and refinement accuracy as rewards, training on 10K filtered samples from NumimaMath. Evaluated on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across AIME24/25 and Olympiad, it achieves significant gains in pass@1 after refinement and majority voting with 64 samples. It also outperforms baselines on ProcessBench for step-level error identification and generalizes to coding and science tasks, demonstrating strong scaling and cross-model supervision capabilities.

## Method Summary
RefCritic uses a two-stage training approach: first, a cold-start SFT phase to teach formatting and basic reasoning using high-quality DeepSeek-R1-Distill-32B generated critiques; then a two-stage RL phase using GRPO with dual rewards (judgment accuracy and refinement accuracy). The refinement reward is only calculated when the critic correctly flags an incorrect solution, requiring the policy model to attempt refinements. The training uses ~10K filtered samples from NumimaMath-1.5, with a staged approach (λ=0 for 600 steps, then λ=1 for 300 steps) to balance computational costs and training stability.

## Key Results
- 6.8–7.2% gains in pass@1 after refinement on AIME24/25 and Olympiad
- 3.6% gains in majority voting with 64 samples
- Outperforms step-level supervised approaches on ProcessBench for step-level error identification
- Generalizes to coding and science tasks, demonstrating strong scaling and cross-model supervision capabilities

## Why This Works (Mechanism)

### Mechanism 1: Outcome-Aligned Feedback Loop
Conditioning the critic's reward on the policy model's successful refinement encourages generation of actionable feedback over superficial binary labeling. The Refinement Reward provides a sparse, binary signal based on whether the policy model produces a correct solution after receiving the critique. To maximize this reward, the critic is pressured to produce feedback specific enough to alter the policy's reasoning trajectory, resolving the alignment problem identified in SFT baselines where "correct judgments emerge from flawed reasoning processes."

### Mechanism 2: Implicit Process Supervision via Solution-Level Rewards
Optimizing for final refinement success forces the critic to learn fine-grained error localization without requiring explicit step-level labels during training. To guide a policy model from an incorrect to a correct solution, the critic must identify the specific step where the derivation diverges. The paper notes that during RL, critic output length increases significantly (e.g., from 500 to 3500 tokens), suggesting the model learns to "trace" the solution step-by-step internally to find the intervention point required to secure the refinement reward.

### Mechanism 3: Dual-Reward Regularization
Separating judgment accuracy and refinement success prevents the "degenerate critic" failure mode where a critic might recommend incorrect changes that coincidentally match the ground truth. By weighting the objective with both rewards, the model is prevented from sacrificing fundamental judgment accuracy solely to maximize "helpfulness." The staged approach (λ=0 then λ=1) suggests stable convergence requires establishing judgment capability first.

## Foundational Learning

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR)**
  - Why needed here: RefCritic uses GRPO with non-differentiable, rule-based rewards (math equality). Understanding how to estimate gradients or advantages from sparse outcome signals is required to implement the training loop.
  - Quick check question: Can you explain why a "rule-based reward" (0 or 1) is harder to optimize than a continuous loss function, and how sampling multiple outputs ($k=8$) per prompt helps mitigate this?

- **Concept: The Critique-Repair Cycle**
  - Why needed here: The core innovation is treating the critique as an intermediate step in a repair process, not just an evaluation. You must understand the data flow: Solution → Critique → Refinement → Reward.
  - Quick check question: In the RefCritic architecture, does the Critic model update its weights based on its own judgment of the solution, or based on whether the *Policy* model subsequently succeeds?

- **Concept: Majority Voting (Self-Consistency)**
  - Why needed here: A key evaluation metric is `Maj@N` with critic filtering. The paper argues RefCritic improves scaling by filtering out "bad" solutions before the vote.
  - Quick check question: Why might filtering out incorrect solutions before a majority vote improve performance more than simply increasing the number of sampled solutions (standard scaling)?

## Architecture Onboarding

- **Component map:** Policy Model ($P_\phi$) -> Critic Model ($C_\theta$) -> Reward Engine -> Data Filter

- **Critical path:**
  1. Data Curation: Sample 8 solutions per problem → Filter to keep 1 correct/1 incorrect per prompt (balanced data)
  2. Cold Start (SFT): Fine-tune Critic on high-quality DeepSeek-R1-Distill-32B generated critiques to teach formatting and basic reasoning
  3. RL Phase 1 ($\lambda=0$): Optimize only for Judgment Accuracy ($R_j$) to create a reliable "error detector" (Cost: Low)
  4. RL Phase 2 ($\lambda=1$): Enable Refinement Reward ($R_r$). For every critique generated, run the Policy Model to attempt a fix. Reward the Critic only if the fix is correct (Cost: High)

- **Design tradeoffs:**
  - Refinement Sampling Cost: Calculating $R_r$ requires generating $m$ refinements (paper uses $m=8$) *per critique step* during RL training, making training roughly $m \times$ more expensive than standard RL
  - Base Model Selection: The paper uses "Instruct" and "Distill" models that already possess strong reasoning (Long CoT) capabilities; applying this to a weaker base model might fail to produce the requisite reasoning traces

- **Failure signatures:**
  - Superficial Agreement: The critic agrees with the solution's conclusion but for the wrong reasons (hallucinated verification)
  - Answer Leakage: The critique explicitly states the final answer rather than the derivation logic
  - Format Collapse: During RL, the model might forget the structured output format

- **First 3 experiments:**
  1. Sanity Check (Table 1 Reproduction): Train a critic via SFT only. Verify that while Critique Accuracy rises, Pass@1 after Refinement remains flat or drops compared to Self-Critique
  2. Reward Isolation (Ablation): Train RefCritic using *only* $R_r$ (Refinement Reward) without $R_j$ (Judgment Reward). Check if the critic learns to "gaslight" the model
  3. Scaling Analysis (Figure 3): Evaluate `Maj@N` performance as $N$ increases from 8 to 64. Verify that the slope of improvement for RefCritic is steeper than the baseline

## Open Questions the Paper Calls Out

- **Question:** Can the dual-reward reinforcement learning framework scale efficiently to models significantly larger than 14B parameters without prohibitive computational costs?
  - Basis in paper: The authors state in the Limitations section that "The dual-reward reinforcement learning framework requires significant computational resources, which may limit its scalability for very large models."
  - Why unresolved: The experiments were limited to 14B parameter models; the cost of sampling 8 refinements per training sample during RL becomes increasingly expensive at larger scales
  - What evidence would resolve it: Successful training and evaluation of RefCritic on models with 70B+ parameters, with reported training costs and comparison to baseline methods

- **Question:** How effectively does RefCritic generalize to domains beyond mathematical and logical reasoning, such as commonsense reasoning or specialized professional contexts?
  - Basis in paper: The Limitations section notes "its generalizability to domains like commonsense reasoning or specialized professional contexts remains to be thoroughly investigated."
  - Why unresolved: While OOD tests on LiveCodeBench and GPQA showed modest improvements, systematic evaluation across diverse non-mathematical domains was not conducted
  - What evidence would resolve it: Comprehensive evaluation on benchmarks for commonsense reasoning, legal reasoning, medical diagnosis, or other professional domains, comparing RefCritic against domain-specific baselines

- **Question:** What is the optimal curriculum or scheduling strategy for balancing the judgment reward and refinement reward during training?
  - Basis in paper: The authors use a staged approach (λ=0 for 600 steps, then λ=1 for 300 steps), but state "Considering the cost of sampling refinements, we initially set λ=0 to achieve rapid improvement in critic performance for 600 steps" without exploring alternative schedules
  - Why unresolved: The ablation study only compares λ=0→λ=0 versus λ=0→λ=1, leaving open whether gradual schedules, adaptive λ, or different stage durations would yield better results
  - What evidence would resolve it: A systematic study varying λ schedules and their impact on final critique quality and refinement effectiveness

## Limitations
- The dual-reward reinforcement learning framework requires significant computational resources, limiting scalability for very large models
- Generalizability to domains like commonsense reasoning or specialized professional contexts remains to be thoroughly investigated
- The staged RL approach suggests delicate training dynamics that may not transfer to other model architectures or problem domains

## Confidence

- **High Confidence:** Dual-reward mechanism effectiveness (supported by ablation showing staged training superiority), ProcessBench generalization (cross-benchmark validation), and refinement sampling methodology (explicit cost-benefit discussion)
- **Medium Confidence:** Claims about implicit process supervision (inferred from output length increases rather than direct step-by-step evaluation), and the necessity of the SFT cold-start phase (plausible but not rigorously isolated)
- **Low Confidence:** The 10K sample size sufficiency claim (only compared to 4K vs 10K within SFT, not against much larger datasets), and the assertion that "weakening" base models is the primary bottleneck (no experiments with significantly weaker models)

## Next Checks

1. **Generalization Stress Test:** Apply RefCritic to a non-mathematical domain (e.g., code debugging or scientific reasoning) with minimal architecture changes to verify cross-domain capability
2. **Data Efficiency Analysis:** Systematically vary training data size (1K, 5K, 20K) to identify diminishing returns and establish minimum viable dataset requirements
3. **Computational Overhead Measurement:** Profile the exact wall-clock time and GPU memory consumption for RL Phase 2 (λ=1) to quantify the refinement sampling cost and identify optimization opportunities