---
ver: rpa2
title: Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis
arxiv_id: '2512.00214'
source_url: https://arxiv.org/abs/2512.00214
tags:
- order
- language
- grammatical
- linguistic
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for corpus-grounded grammatical
  analysis using agentic large language models. The system, UDagent, translates linguistic
  questions into executable code for analyzing Universal Dependencies corpora, producing
  structured answers about word-order patterns.
---

# Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis

## Quick Facts
- arXiv ID: 2512.00214
- Source URL: https://arxiv.org/abs/2512.00214
- Reference count: 0
- Primary result: UDagent achieved dominant-order prediction accuracy above 0.9 on 9 out of 13 typological tasks across 175+ languages

## Executive Summary
This paper introduces UDagent, a framework that bridges natural language grammatical queries with empirical corpus evidence through agentic large language models. The system translates linguistic questions into executable Python code for analyzing Universal Dependencies corpora, enabling structured analysis of word-order patterns across 175+ languages. Evaluated on 13 typological tasks, UDagent demonstrated dominant-order prediction accuracy above 0.9 on 9 tasks, with coverage F1 scores over 0.9 in 10 tasks and Hellinger distances below 0.1 in 9 tasks, outperforming baselines and demonstrating the viability of LLM-based agents for scalable multilingual grammatical analysis.

## Method Summary
UDagent follows a fixed-plan agent architecture that converts corpus-level questions into executable code through a four-stage pipeline: task conversion (reformulating questions into sentence-level subtasks), code generation (LLM produces Python functions for analyzing CONLL-U formatted dependency trees), resource analysis (executing code on UD treebanks), and result aggregation (computing frequency distributions and dominant patterns). The system requires precise Universal Dependencies schema knowledge and operates on gold-standard annotated corpora, producing structured outputs including dominant order, attested orders, and full distributional estimates. Evaluation used 13 typological features from WALS, comparing results against ground truth derived from manual corpus analysis.

## Key Results
- Dominant-order prediction accuracy above 0.9 on 9 out of 13 typological tasks
- Coverage F1 scores exceeding 0.9 in 10 tasks, indicating reliable detection of relevant constructions
- Hellinger distances below 0.1 in 9 tasks, demonstrating high distributional fidelity to ground truth
- Outperformed GPT-5 baseline on all 13 tasks by leveraging empirical corpus evidence rather than parametric knowledge alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating linguistic questions into executable code enables precise, reproducible corpus analysis that exceeds parametric LLM knowledge alone
- Mechanism: An LLM generates Python functions that parse CONLL-U formatted dependency trees, extracting grammatical relations (e.g., `nsubj`, `obj`) to compute word-order patterns sentence-by-sentence. This code-mediated approach grounds reasoning in actual data rather than memorized patterns
- Core assumption: The LLM can correctly map natural-language grammatical concepts to the Universal Dependencies annotation schema and produce syntactically valid, semantically appropriate code
- Evidence anchors:
  - [abstract] "translates linguistic questions into executable code for analyzing Universal Dependencies corpora"
  - [section 4] "the task solution code is generated using an LLM... [with] a description of the data format in UD (CONLL-U) is provided along with the instructions to generate a Python function"
  - [corpus] Related work on LLMs for corpus query translation (Lu et al., 2024) supports feasibility, though corpus evidence specifically for grammatical analysis agents is nascent
- Break condition: Code generation failures—logical errors in data access (e.g., incorrect feature parsing) or conceptual mismatches in linguistic interpretation—propagate through the pipeline, producing systematically wrong outputs

### Mechanism 2
- Claim: Aggregating sentence-level analyses into distributions captures within-language variation better than single-label classification
- Mechanism: Each sentence yields a categorical label (e.g., "SVO" or "None" if inapplicable). The system computes proportional frequencies across all valid outputs, returning both the dominant pattern and full distribution—reflecting that real usage exhibits gradient variation
- Core assumption: Corpus data adequately samples the target linguistic phenomena, and frequency reflects genuine grammatical patterns rather than genre or register artifacts
- Evidence anchors:
  - [abstract] "producing structured answers about word-order patterns" with "dominant-order prediction accuracy" and "distributional fidelity" as separate metrics
  - [section 3.1] Output includes three layers: "Dominant order," "Attested orders," and "Order distribution" (e.g., "SVO: 92%, SOV: 6%, VSO: 2%")
  - [corpus] Gradient approaches to word-order typology (Levshina et al., 2023; Baylor et al., 2024) provide theoretical grounding, though direct corpus evidence for this specific aggregation method is limited to this work
- Break condition: Small treebanks (e.g., Buryat, Komi Zyrian) produce unreliable distributional estimates; near-equal distributions (Dutch features 83A/84A) make dominant-order predictions unstable

### Mechanism 3
- Claim: Separating code generation from execution allows interpretable verification of analysis logic
- Mechanism: The generated Python function is inspectable—researchers can examine how the agent operationalized a linguistic question (e.g., what counts as a "genitive construction"). This transparency supports debugging and methodological critique
- Core assumption: Users have sufficient programming and linguistic expertise to interpret generated code and identify conceptual mismatches
- Evidence anchors:
  - [section 5.3] "Examining the intermediate scripts provided insight into how the agent interprets linguistic instructions and where the process fails"
  - [section 8] The Dutch example shows divergence: ground truth considered only nominal arguments, while the agent included pronouns—"not necessarily an error but rather a broader interpretation"
  - [corpus] No direct corpus evidence on inspection utility; this is an architectural claim requiring user studies
- Break condition: If code becomes complex or poorly structured, inspection burden increases; if users lack expertise, transparency doesn't translate to actionable verification

## Foundational Learning

- Concept: **Universal Dependencies annotation schema**
  - Why needed here: The code generation module requires precise knowledge of CONLL-U format, dependency relations (`nsubj`, `obj`, `nmod`, `amod`, etc.), and morphological features (`Case=Gen`, `Polarity=Neg`). Misunderstanding these leads to incorrect code
  - Quick check question: Given a CONLL-U sentence, can you identify which token is the direct object and which relation links it to the verb?

- Concept: **Agentic LLM architectures (planning, tool use, reasoning)**
  - Why needed here: UDagent follows a fixed-plan architecture with explicit stages (task conversion → code generation → execution → summarization). Understanding this decomposition helps diagnose where failures originate
  - Quick check question: What is the difference between a fixed-plan agent and a dynamic planning agent like ReAct, and why might fixed plans be preferred for structured analysis tasks?

- Concept: **Linguistic typology and word-order features**
  - Why needed here: The evaluation tasks are drawn from WALS typological features. Interpreting results requires understanding what "VO order" or "adposition-noun order" means linguistically
  - Quick check question: For the feature "Order of subject, object, and verb," what are the six logically possible orders, and which are attested cross-linguistically as dominant patterns?

## Architecture Onboarding

- Component map: Question → Task conversion → Code generation → Resource analyzer → Result aggregator

- Critical path:
  Question → Task conversion → Code generation → [highest-risk: code correctness] → Corpus execution → Distribution computation → Answer

  Code generation is the bottleneck; syntactically valid but logically incorrect code silently produces wrong results

- Design tradeoffs:
  - **Fixed plan vs. dynamic planning**: Simpler to debug but cannot adapt to unexpected data formats or task variations
  - **Sentence-level processing vs. corpus-level queries**: Enables fine-grained distributional analysis but requires more compute
  - **Predefined answer options vs. open-ended**: Constrains output space for reliable evaluation but may miss unexpected patterns

- Failure signatures:
  - Zero or near-zero valid answers: Code likely has a data-access bug (see 144A/144B where `Polarity=Neg` was incorrectly accessed)
  - Syntactically correct code with systematically wrong answers: Conceptual mismatch in linguistic interpretation (see 86A genitive-noun task)
  - High variance in dominant-order predictions across similar languages: May indicate underspecified prompts or corpus sparsity

- First 3 experiments:
  1. Run UDagent on a high-resource language (e.g., English or Czech) with manual code inspection—verify the generated function correctly identifies subject, object, and verb tokens and computes their order
  2. Ablate corpus grounding: Compare UDagent against the GPT-5 baseline on a subset of features, quantifying accuracy gain attributable to data access vs. parametric knowledge
  3. Inject controlled errors: Modify the prompt to omit UD format documentation or provide incorrect relation names; measure performance degradation to isolate the importance of precise schema specification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source or smaller language models maintain the reasoning and code generation capabilities required for corpus-grounded grammatical analysis without relying on proprietary systems?
- Basis in paper: [explicit] The authors note the evaluation relied on a single proprietary model and state, "Future work should test open and smaller models."
- Why unresolved: The UDagent prototype relied on GPT-5's specific ability to interpret tasks and generate valid Python code; it is unproven whether smaller models possess sufficient syntactic and semantic understanding to avoid the "fragility of automatic code generation" cited in the error analysis
- What evidence would resolve it: Re-running the evaluation framework using open-weight models (e.g., Llama, Mistral) on the 13 word-order tasks to compare dominant order accuracy and code execution success rates against the GPT-5 baseline

### Open Question 2
- Question: Can the agentic architecture scale to multi-step reasoning tasks that require synthesizing diverse aggregation strategies beyond single-feature frequency counts?
- Basis in paper: [explicit] The conclusion identifies the need to "expand to multi-step reasoning tasks requiring diverse aggregation strategies" as a necessary extension beyond the current elementary tasks
- Why unresolved: The current system uses a fixed plan (task conversion -> code generation -> summarization) optimized for isolating single grammatical features. It is unclear if this workflow can chain multiple operations or handle queries requiring complex logic (e.g., conditional probabilities or cross-feature correlations)
- What evidence would resolve it: Designing a benchmark of complex linguistic queries (e.g., "Does the order of object and verb correlate with the presence of case markers?") and measuring the agent's ability to decompose and solve them

### Open Question 3
- Question: What mechanisms can effectively mitigate "conceptual mismatches" where the generated code is syntactically valid but misinterprets the linguistic definition of the task?
- Basis in paper: [explicit] The error analysis highlights "conceptual mismatches" (e.g., differing interpretations of genitive relations) and explicitly calls for "incorporating safeguards and clearer linguistic specifications"
- Why unresolved: The LLM currently operationalizes linguistic concepts without external verification. As noted in the Dutch examples, the model may implement a technically valid but linguistically distinct interpretation of the prompt, leading to incorrect distributional estimates
- What evidence would resolve it: Implementing a verification loop where the agent generates test cases or seeks human feedback on the linguistic logic before full corpus execution, and evaluating the reduction in conceptual errors

### Open Question 4
- Question: Does integrating automatic parsing tools allow the system to perform accurate grammatical analysis on raw text despite potential error propagation?
- Basis in paper: [explicit] The authors propose generalizing the architecture by "incorporating parsers as tools to process raw text directly"
- Why unresolved: The current framework operates exclusively on gold-standard Universal Dependencies corpora. Processing raw text would introduce noise from the parser (e.g., incorrect dependency heads), which could compound the system's existing reasoning and code generation errors
- What evidence would resolve it: Extending UDagent to accept raw text input via an integrated parser and comparing the resulting word-order distributions against the gold-standard UD-derived ground truth

## Limitations

- Code generation reliability is limited by the fragility of automatic code generation, where syntactically valid but semantically incorrect code can silently produce wrong results
- Low-resource language performance suffers from unreliable distributional estimates when treebanks are small (e.g., Buryat, Komi Zyrian)
- Task specification sensitivity can lead to inconsistent interpretations when prompts are underspecified (e.g., "negative morpheme" vs. specific linguistic features)

## Confidence

- **High confidence**: The core mechanism of translating linguistic questions into executable code for corpus analysis is well-demonstrated and reproducible
- **Medium confidence**: The claim that code generation is the primary bottleneck is supported but not exhaustively validated
- **Low confidence**: The assertion that this approach enables "scalable, interpretable multilingual grammatical analysis" overstates current capabilities

## Next Checks

1. **Treebank size sensitivity analysis**: Systematically evaluate UDagent performance as a function of treebank token count across all 175+ languages, identifying the minimum viable corpus size for reliable distributional estimates on each typological feature

2. **Controlled prompt ablation study**: Create variants of the same linguistic question with systematically varied specificity (e.g., "negative morpheme" vs. "negative particle vs. negative affix") and measure how answer consistency varies across languages and features

3. **Expert code review protocol**: Have linguists and computational linguists independently inspect generated code for a representative sample of tasks, rating semantic correctness and identifying patterns in where linguistic interpretation diverges from UD schema conventions