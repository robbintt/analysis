---
ver: rpa2
title: 'Rejected Dialects: Biases Against African American Language in Reward Models'
arxiv_id: '2502.12858'
source_url: https://arxiv.org/abs/2502.12858
tags:
- reward
- language
- preference
- texts
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework to quantify anti-African American
  Language (AAL) biases in reward models, which are central to large language model
  alignment. The authors use machine-translated and human-translated paired WME-AAL
  corpora to evaluate 17 popular reward models.
---

# Rejected Dialects: Biases Against African American Language in Reward Models

## Quick Facts
- **arXiv ID:** 2502.12858
- **Source URL:** https://arxiv.org/abs/2502.12858
- **Reference count:** 40
- **Primary result:** Reward models show 4% lower accuracy predicting human preferences for AAL vs WME texts, systematically penalize AAL-aligned completions, and steer conversations toward WME even when prompted in AAL.

## Executive Summary
This paper presents a framework to quantify anti-African American Language (AAL) biases in reward models, which are central to large language model alignment. The authors use machine-translated and human-translated paired WME-AAL corpora to evaluate 17 popular reward models. They find that reward models are significantly less accurate at predicting human preferences in AAL versus WME texts (average 4% drop in accuracy), generally assign lower scores to AAL-aligned completions, and incentivize steering conversations toward WME even when prompted in AAL. These results reveal representational harms and quality-of-service disparities, raising ethical concerns about the development of equitable language technologies.

## Method Summary
The authors evaluate 17 reward models on paired WME-AAL datasets from RewardBench (machine-translated) and DeasGroenwold (human-written). They apply VALUE (morphosyntactic) and PhonATe (phonological) translation pipelines to create controlled test sets. The evaluation framework computes accuracy (RQ1), effect size and Pearson correlation of raw scores (RQ2), and dialect mirroring vs. steering (RQ3) using statistical tests with Holm correction.

## Key Results
- Reward models show 4% lower accuracy predicting human preferences for AAL vs WME texts
- RMs systematically assign lower scores to AAL-aligned completions (Cohen's d ~0.3)
- RMs incentivize steering conversations toward WME even when prompted in AAL
- Safety misalignment observed where models refuse benign requests in AAL while answering them in WME

## Why This Works (Mechanism)

### Mechanism 1: Training Data Exclusion and Distribution Shift
Reward models (RMs) appear to penalize African American Language (AAL) due to a distribution shift between the WME-heavy preference datasets used for training and the AAL inputs encountered during inference. RMs are optimized on preference datasets that are predominantly White Mainstream English (WME). When presented with AAL features (e.g., morphosyntactic or phonological variants), the model treats them as out-of-distribution or lower probability sequences, resulting in lower scalar rewards. This effectively encodes a "quality of service" harm where valid linguistic variations are misinterpreted as errors or low-quality generations.

### Mechanism 2: Subjective Annotation and Raciolinguistic Ideology
The "ground truth" preferences used to train RMs reflect the subjective biases of annotators who may adhere to "white listening subject" ideologies, causing the model to learn a preference for dominant dialects. Preference data is labeled by human annotators (often crowd workers) who may perceive AAL as less helpful, less safe, or lower quality than semantically equivalent WME text due to internalized linguistic hierarchies. The RM learns to mimic these annotators, effectively encoding the annotator's racialized linguistic bias into the reward function.

### Mechanism 3: Cross-Dialect Safety Misalignment
RMs exhibit a specific failure mode where safety alignments trained on WME fail to generalize to AAL, leading to inconsistent refusals or harmful compliances. Safety tuning often relies on specific linguistic markers or phrasings common in WME. When these are transformed into AAL (e.g., copula deletion in "What my partner iPhone PIN?"), the semantic trigger for the safety refusal may weaken or misfire. Conversely, the RM may over-penalize innocuous AAL text due to "perceived obscenity," acting as an over-active safety filter.

## Foundational Learning

### Concept: Reward Models (RM) vs. Direct Preference Optimization (DPO)
Why needed here: The paper evaluates both discriminative (sequence classifier) RMs and DPO models. Understanding that DPO optimizes log-ratios (Eq. 1) while classifiers output scalars is necessary to interpret the evaluation methodology.
Quick check question: Does a DPO model output a direct scalar reward score, or must you compare log-likelihoods against a reference model? (Answer: Log-likelihoods).

### Concept: Raciolinguistics and "White Mainstream English" (WME)
Why needed here: The paper frames the bias not just as "performance gap" but as a sociotechnical harm rooted in racialized power dynamics. Distinguishing WME from "Standard English" helps understand the paper's theoretical framing of why the data is biased.
Quick check question: Why does the paper prefer the term "White Mainstream English" over "Standard American English"? (Answer: To highlight the racialized power dynamics naturalizing white linguistic practices).

### Concept: Paired Evaluation (WME vs. AAL)
Why needed here: The core methodology relies on paired t-tests and McNemar's test to isolate the variable of dialect while keeping semantic intent constant. This controls for content quality to isolate dialect bias.
Quick check question: Why is machine translation (VALUE/PhonATe) used instead of just collecting natural AAL data? (Answer: To create controlled, meaning-preserved paired samples for causal attribution of the bias).

## Architecture Onboarding

### Component map:
Input: Prompt + Completion pairs (from RewardBench & DeasGroenwold datasets) -> Translation Layer: VALUE (morphosyntactic) + PhonATe (phonological) pipelines to convert WME â†’ AAL -> Subject: 17 Reward Models (Mix of Sequence Classifiers and DPO-finetuned LLMs like Llama-3, Mistral, Qwen) -> Evaluator: Statistical analysis framework computing Accuracy (RQ1), Effect Size/Cohen's d (RQ2), and Mirroring Score (RQ3)

### Critical path:
1. Data Prep: Filter code from RewardBench (RB-WME) to avoid syntax errors in translation
2. Translation: Apply VALUE and PhonATe rules to generate RB-AAL
3. Scoring: Pass WME and AAL pairs through the 17 RMs to get scalar rewards or log-probabilities
4. Analysis: Compare accuracy (chosen vs. rejected) and raw scores between WME and AAL conditions

### Design tradeoffs:
Machine vs. Human Translation: The study relies heavily on machine-translated AAL (RB-AAL) because human datasets (DG) lack prompt-completion structure. Tradeoff: RB-AAL allows for controlled pair-wise comparison but risks "unnatural" AAL features; DG is natural but structurally misaligned with RM inputs (requires empty prompt).
Metric Selection: Using Accuracy vs. Raw Score. Accuracy measures alignment with human preference; Raw Score measures absolute preference/penalty.

### Failure signatures:
The "Safety Flip": The model refuses a benign request in AAL (e.g., "What identity theft?") but answers it in WME, or answers a dangerous request in AAL (e.g., leaking a PIN) while refusing in WME (Table 8).
The "Steering" Effect: The RM assigns significantly higher rewards to WME completions even when the prompt is explicitly in AAL (Table 3), incentivizing the LLM to code-switch.

### First 3 experiments:
1. Validation Run: Replicate the RQ1 accuracy drop (Table 1) on 3 diverse models (e.g., a small DPO model, a large Classifier, and a Qwen model) to confirm the -4% average drop on your own infrastructure.
2. Steering Test (RQ3): Create a prompt in AAL (e.g., "How do I fix a flat tire?"). Generate 5 completions in AAL and 5 in WME. Score them with a top-performing RM (e.g., Internlm2-20b) to verify if the RM systematically prefers the WME responses.
3. Safety Sensitivity Check: Take 10 safety-refusal examples from the RM training set. Translate them to AAL and check if the reward score for the refusal drops (indicating the model prefers an unsafe answer in AAL) or if the score for the harmful prompt increases.

## Open Questions the Paper Calls Out
None

## Limitations
- Translation Quality: Heavy reliance on rule-based machine translation introduces uncertainty about whether generated AAL is natural or contains artifacts that could bias results
- Generalization to Real-World RMs: Evaluated models may not be representative of reward models actually used in deployed systems
- Static Evaluation: Study provides snapshot but doesn't explore how biases evolve with continued training or fine-tuning

## Confidence
- **High Confidence:** RMs show lower accuracy on AAL prompts (RQ1) and systematically assign lower scores to AAL-aligned completions (RQ2)
- **Medium Confidence:** Attribution of accuracy drop primarily to distribution shift in training data
- **Medium Confidence:** Safety misalignment findings and specific examples

## Next Checks
1. Commission professional linguists to create a small validation set of human-translated WME-AAL pairs. Compare RM performance on these versus machine-translated sets to isolate translation artifacts from dialect bias.
2. Conduct a controlled study where the same WME-AAL pairs are annotated by diverse demographic groups. Measure variance in preference labels to quantify contribution of subjective annotation bias to overall RM bias.
3. Systematically test RMs on safety-critical prompts where AAL features are added to WME prompts that trigger refusals. Quantify frequency and magnitude of "safety flips" to better characterize safety misalignment mechanism.