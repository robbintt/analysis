---
ver: rpa2
title: 'Neuroplasticity and Corruption in Model Mechanisms: A Case Study Of Indirect
  Object Identification'
arxiv_id: '2503.01896'
source_url: https://arxiv.org/abs/2503.01896
tags:
- circuit
- heads
- figure
- name
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates how fine-tuning language models affects
  their internal mechanisms, focusing on a case study of the Indirect Object Identification
  (IOI) task. Key findings include: (1) Task-specific fine-tuning enhances existing
  mechanisms through "circuit amplification" without introducing new mechanisms, even
  over long training durations; (2) Model poisoning via toxic fine-tuning locally
  corrupts specific circuit components rather than introducing novel mechanisms; and
  (3) Models exhibit "neuroplasticity" by relearning original mechanisms after retraining
  on clean data, reforming the original model circuits.'
---

# Neuroplasticity and Corruption in Model Mechanisms: A Case Study Of Indirect Object Identification

## Quick Facts
- arXiv ID: 2503.01896
- Source URL: https://arxiv.org/abs/2503.01896
- Reference count: 40
- Key outcome: Fine-tuning enhances existing mechanisms without introducing new ones; toxic fine-tuning locally corrupts specific components; models can relearn original mechanisms after retraining on clean data

## Executive Summary
This paper investigates how fine-tuning affects the internal mechanisms of language models, focusing on the Indirect Object Identification (IOI) task in GPT-2-small. The authors demonstrate that task-specific fine-tuning amplifies existing circuit components rather than introducing novel mechanisms, even over extended training durations. They show that toxic fine-tuning locally corrupts specific circuit components based on the type of corruption, and that models exhibit "neuroplasticity" by recovering original mechanisms when retrained on clean data. The study uses path patching, knockout, and cross-model activation patching techniques to systematically analyze these phenomena.

## Method Summary
The authors fine-tune GPT-2-small (12 layers, 12 heads/layer, 80M parameters) on IOI datasets of 6,360 samples with 15 templates, 100 English names, and 20 places/objects. Three scenarios are tested: (1) clean task-specific fine-tuning for 1-100 epochs, (2) toxic fine-tuning on corrupted datasets (Name Moving, Subject Duplication, Duplication) for 3-5 epochs, and (3) retraining corrupted models on clean data for 3-5 epochs. They use path patching and cross-model activation patching to analyze circuit components including Name Mover Heads, S-Inhibition Heads, Negative Name Mover Heads, and Backup Name Mover Heads. Faithfulness scores measure circuit performance against full model performance.

## Key Results
- Task-specific fine-tuning amplifies existing circuit components, increasing faithfulness from 87% baseline to 99.89% at epoch 25
- Toxic fine-tuning locally corrupts specific circuit components (Name Mover Heads, S-Inhibition Heads) rather than introducing novel mechanisms
- Corrupted models relearn original task mechanisms when fine-tuned on clean data, achieving 95-96% faithfulness
- MLP layers show minimal contribution to IOI mechanisms across all fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1: Circuit Amplification
Fine-tuning enhances existing circuit components without introducing novel mechanisms through three pathways: existing attention heads increase their capacity to perform assigned mechanisms (e.g., Name Mover Heads show increased attention probability toward IO token), logit attribution per head increases significantly, and some Backup Name Mover Heads transition to performing Name Mover functionality. The pre-trained model contains functional circuits for the task, and fine-tuning optimizes rather than replaces these mechanisms.

### Mechanism 2: Localized Corruption via Attention Head Disruption
Toxic fine-tuning corrupts task performance by altering mechanisms of specific circuit components rather than adding novel corruption mechanisms. Corruption targets distinct heads based on dataset design: Name Moving dataset causes S-Inhibition Heads to suppress both IO and S token queries, while Subject Duplication dataset causes Name Mover and Negative Name Mover Heads to invert their behavior. MLP layers show minimal contribution changes across corruption scenarios.

### Mechanism 3: Neuroplasticity Through Mechanism Recovery
Corrupted models relearn original task mechanisms when fine-tuned on clean data, often with amplified versions of original circuits. Post-reversal models recover attention head functionality with enhanced projection magnitudes, including both original components and newly emerging backup components that replicate original mechanisms. Pre-trained representations retain sufficient structure to support mechanism reformation given clean supervision.

## Foundational Learning

- **Attention head decomposition (QK and OV matrices)**: Understanding W_OV determines what writes to residual stream; W_QK determines attention patterns. Quick check: Can you explain why an attention head could show unchanged attention patterns but changed output contributions?

- **Circuit faithfulness, minimality, and completeness evaluation**: Faithfulness measures |F(M) - F(C)| (circuit vs. full model performance). Minimality checks for unnecessary components. Completeness ensures all necessary components are included. Quick check: If a circuit achieves 100% faithfulness but low minimality, what does this indicate?

- **Path patching and activation patching**: Core methodology for circuit discovery and cross-model analysis. Path patching isolates direct vs. indirect effects. Cross-Model Activation Patching (CMAP) transfers activations between model variants to localize corruption/amplification effects. Quick check: How does cross-model patching differ from within-model activation patching?

## Architecture Onboarding

- **Component map**: S-Inhibition Heads → Name Mover Heads → final logits. S-Inhibition Heads bias Name Mover queries; Name Mover Heads write IO token to residual stream. MLP layers contribute minimally except Layer 0 (extended embedding).

- **Critical path**: The circuit follows S-Inhibition Heads biasing queries, then Name Mover Heads copying IO token to residual stream, with Negative Name Mover Heads providing suppression.

- **Design tradeoffs**: Small dataset size (6,360 samples) allows observing head-level changes without overfitting. Learning rate (1e-5) with weight decay (0.1) preserves pre-trained structure. Manual path patching chosen over ACDC/EAP for fidelity to original IOI work.

- **Failure signatures**: Over-amplification shows Negative Name Mover Heads amplifying alongside Name Mover Heads. Incomplete corruption preserves IOI functionality initially. Duplication dataset shows no statistically significant mechanism changes, indicating ineffective corruption strategy.

- **First 3 experiments**: 1) Reproduce baseline IOI circuit discovery using path patching on original GPT-2-small; verify faithfulness ~87%. 2) Fine-tune on clean IOI dataset for 3 epochs; use Cross-Model Pattern Patching to verify amplification localizes to original circuit heads. 3) Fine-tune on Subject Duplication dataset for 3-5 epochs; apply Cross-Model Output Patching to confirm corruption is localized to Name Mover and Negative Name Mover heads.

## Open Questions the Paper Calls Out

- **Cross-Dataset Generalization**: Do circuit amplification, localized corruption, and neuroplasticity generalize to larger model architectures and more complex tasks? Section 9 states the work focuses on GPT-2-small and requires additional work to scale/generalize findings.

- **Twinning Behavior**: Does the "twinning" behavior between Name Mover and Negative Name Mover heads occur in other tasks? Section 4.2 observes this unexpected phenomenon during Subject Duplication experiment and suggests further investigation.

- **Mechanism-Based Poisoning**: Can mechanistic knowledge enable the design of precise model poisoning attacks or backdoor triggers that alter specific mechanisms without compromising general model utility? Section 4.2 hypothesizes this could be a future research direction.

## Limitations
- Analysis limited to GPT-2-small architecture and specific IOI task, requiring additional work to scale/generalize to larger models
- Small dataset size (6,360 samples) may not generalize to larger, more diverse corpora
- Mechanistic analysis depends heavily on path patching methodology, which requires careful hyperparameter tuning

## Confidence
- **High Confidence**: Circuit amplification findings (faithfulness improvements from 87% to 99.89%) are well-supported by quantitative metrics and cross-model patching experiments.
- **Medium Confidence**: Neuroplasticity recovery claims (95-96% faithfulness post-reversal) are statistically significant but limited to single corruption-recovery cycle.
- **Low Confidence**: Assertion that MLP layers contribute minimally to IOI mechanisms across all fine-tuning scenarios is based on limited analysis.

## Next Checks
1. **Cross-Dataset Generalization Test**: Validate amplification and corruption localization findings on a 10x larger IOI dataset (63,600 samples) with more diverse sentence structures.

2. **Multi-Task Neuroplasticity Experiment**: Fine-tune the corrupted IOI model on a different structured task (e.g., subject-verb agreement) for 10 epochs, then retrain on clean IOI data.

3. **Attention Pattern Dynamics Analysis**: For amplified heads, measure how QK attention patterns evolve over fine-tuning epochs to distinguish between capacity enhancement versus mechanism diversification.