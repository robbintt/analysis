---
ver: rpa2
title: A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship
  Attribution on Chinese Lyrics
arxiv_id: '2511.21930'
source_url: https://arxiv.org/abs/2511.21930
tags:
- genre
- genres
- lyrics
- cross-genre
- test2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates authorship attribution for Chinese lyrics,
  a domain lacking clean, public datasets. The authors create a new, balanced dataset
  of Chinese lyrics spanning five thematic genres and develop a domain-specific model,
  comparing its performance against zero-shot inference using DeepSeek LLM.
---

# A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics

## Quick Facts
- arXiv ID: 2511.21930
- Source URL: https://arxiv.org/abs/2511.21930
- Authors: Yuxin Li; Lorraine Xu; Meng Fan Wang
- Reference count: 8
- This study investigates authorship attribution for Chinese lyrics, a domain lacking clean, public datasets.

## Executive Summary
This study investigates authorship attribution for Chinese lyrics, a domain lacking clean, public datasets. The authors create a new, balanced dataset of Chinese lyrics spanning five thematic genres and develop a domain-specific model, comparing its performance against zero-shot inference using DeepSeek LLM. Two hypotheses are tested: (1) a fine-tuned model will outperform a zero-shot LLM baseline, and (2) attribution performance will vary across genres. Results strongly support hypothesis 2: structured genres like Folklore & Tradition yield significantly higher accuracy than abstract genres like Love & Romance. Hypothesis 1 receives partial support: fine-tuning improves robustness and generalization in real-world test data but offers limited gains in a smaller, synthetically-augmented set due to design limitations such as label imbalance and shallow lexical differences. The study establishes the first benchmark for cross-genre Chinese lyric attribution and highlights the importance of genre-sensitive evaluation.

## Method Summary
The study constructs a new dataset of 1,055 Chinese songs (post-2000, ≥80% Chinese content) across five thematic genres, using DeepSeek-LLaMA-70B for genre labeling followed by manual verification. Lyrics are cleaned (removing timestamps, special characters) and filtered for length outliers using Tukey's Fences. Pairwise authorship attribution data is created (same-author vs different-author pairs) in both per-genre and cross-genre settings. The model uses hfl/chinese-roberta-wwm-ext with Sentence Transformers' ContrastiveLoss for fine-tuning, while zero-shot inference employs DeepSeek-LLaMA-70B with linguistically-informed Chinese prompts. Evaluation uses F1-Micro, F1-Macro, F1-Weighted, Accuracy, Precision, and Recall, with F1-Macro as the primary metric due to label imbalance concerns.

## Key Results
- Structured genres (Folklore & Tradition) yield significantly higher attribution accuracy than abstract genres (Love & Romance)
- Fine-tuning improves robustness and generalization in real-world test data but offers limited gains in synthetically-augmented datasets
- The study establishes the first benchmark for cross-genre Chinese lyric attribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured genres yield higher attribution accuracy than abstract genres due to distinct stylistic markers.
- **Mechanism:** Genres like Folklore & Tradition contain domain-specific vocabulary (regional storytelling, historical motifs) and constrained rhetorical patterns that create low intra-author variance and high inter-author separability. Abstract genres like Love & Romance share generic emotional language across authors, reducing discriminative signal.
- **Core assumption:** Stylistic distinctiveness correlates with genre structure; authors in structured genres maintain consistent stylistic signatures.
- **Evidence anchors:**
  - [abstract] "structured genres (e.g., Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g., Love & Romance)"
  - [section 5.1] Folklore & Tradition achieves perfect or near-perfect scores across settings; Love & Romance shows "emotional and generic language prevalent in the genre, which introduces semantic overlap across authors"
  - [corpus] Weak direct evidence—neighbor papers focus on cross-genre attribution methods but do not systematically compare genre structure effects on attribution difficulty
- **Break condition:** If authors in structured genres vary their style significantly across works (high intra-author variance), the mechanism fails.

### Mechanism 2
- **Claim:** Fine-tuning improves robustness by learning author-specific embeddings that generalize beyond genre cues.
- **Mechanism:** Contrastive loss (from Sentence Transformers) optimizes cosine similarity—pushing same-author pairs closer in embedding space and different-author pairs apart. This forces the model to capture stylistic traits independent of topical content, reducing "genre shortcut" reliance.
- **Core assumption:** The training pairs contain sufficient stylistic signal not confounded by topic-author correlations; hard positives (same author, different genres) are crucial.
- **Evidence anchors:**
  - [abstract] "fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres)"
  - [section 4.5] "We fine-tune the pre-trained Chinese RoBERTa base model using contrastive learning...optimizes the model to increase cosine similarity for positive pairs"
  - [section 5.1] "finetuning may equalize the model's reliance on genre cues by reinforcing author-specific traits independent of genre"
  - [corpus] Neighbor paper "LLM one-shot style transfer for Authorship Attribution" notes supervised approaches often conflate style with topic
- **Break condition:** If training data has topic-author confounds (e.g., one author dominates a genre), the model learns topic shortcuts instead of style.

### Mechanism 3
- **Claim:** Token-level synthetic augmentation (e.g., inserting [SYN0]) can artificially inflate class separability and obscure true generalization.
- **Mechanism:** Synthetic tokens create predictable lexical artifacts that models can exploit as classification shortcuts, leading to inflated scores on augmented test sets that do not reflect real-world performance.
- **Core assumption:** The model learns to detect augmentation artifacts rather than genuine stylistic features.
- **Evidence anchors:**
  - [abstract] "design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning"
  - [section 5.2] "This process may artificially inflate class separability by introducing predictable lexical artifacts, leading to overly optimistic scores"
  - [corpus] Neighbor paper by Şahin (2021, cited in paper) suggests token-level augmentation helps sparsity but this study shows its limitations
- **Break condition:** If augmentation tokens are semantically integrated or randomized effectively, they may not create detectable shortcuts.

## Foundational Learning

- **Concept: Contrastive Learning for Stylistic Embeddings**
  - **Why needed here:** The core model uses contrastive loss to learn embeddings where same-author lyrics are close and different-author lyrics are distant, regardless of genre.
  - **Quick check question:** Can you explain why contrastive loss is preferred over cross-entropy for this pairwise attribution task?

- **Concept: Hard Positive/Negative Sampling**
  - **Why needed here:** The dataset design uses "hard positives" (same author, different genres) and "hard negatives" (different authors, similar themes) to force the model to learn style rather than topic shortcuts.
  - **Quick check question:** What would happen if all training pairs were from the same genre?

- **Concept: F1 Macro vs. F1 Micro for Imbalanced Data**
  - **Why needed here:** Test2 shows label imbalance; F1 Macro was chosen as the primary metric because it treats all classes equally rather than being dominated by the majority class.
  - **Quick check question:** Why does the paper report F1 Macro dropping significantly in Test2 while recall stays at 1.0?

## Architecture Onboarding

- **Component map:** Raw lyrics → Cleaning (remove timestamps, special characters) → Length filtering (Tukey's Fences) → Genre labeling (DeepSeek + human verification) → Pair construction (per-genre/cross-genre, same-author/different-author) → Model: Chinese-RoBERTa-wwm-ext → Contrastive loss → EmbeddingSimilarityEvaluator → Evaluation: Accuracy, F1 Micro/Macro/Weighted, per-genre and per-mode breakdown

- **Critical path:** Dataset construction quality (especially hard positive/negative curation) → Training pair balance → Threshold tuning during fine-tuning → Test set composition (real vs. synthetic) determines whether gains are observable

- **Design tradeoffs:**
  - Smaller real dataset (Test1) vs. synthetic augmentation (Test2): Real data shows fine-tuning gains; synthetic data obscures them
  - Per-genre vs. cross-genre evaluation: Per-genre easier but tests genre-internal style; cross-genre harder but tests true stylistic generalization
  - Zero-shot LLM vs. fine-tuned encoder: LLM may have seen training data; fine-tuned model has limited generalization to unseen authors

- **Failure signatures:**
  - Perfect recall (~1.0) with near-zero precision and F1 → threshold collapse (predicting all positive)
  - Large gap between F1 Micro and F1 Macro → label imbalance
  - Perfect scores on Test2 per-genre for some genres → likely augmentation artifacts
  - Performance drops sharply from per-genre to cross-genre → model relying on genre shortcuts

- **First 3 experiments:**
  1. **Baseline replication:** Run zero-shot DeepSeek on Test1/Test2 with the provided Chinese prompt; verify F1 Macro values (0.5637 per-genre, 0.4563 cross-genre on Test1)
  2. **Ablation on augmentation:** Train on original pairs only (no [SYN0] tokens) and compare Test2 performance to isolate augmentation artifact effects
  3. **Genre balance analysis:** Compute per-genre F1 Macro separately; confirm Folklore & Tradition > Life & Reflection > Love & Romance hierarchy, then investigate what lexical features distinguish Folklore & Tradition lyrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does domain-adaptive pretraining on a large corpus of Chinese lyrics significantly improve generalization to unseen authors compared to standard fine-tuning?
- Basis in paper: [explicit] The authors explicitly recommend "investigate domain-adaptive pretraining as a pathway for improved attribution performance" in the Conclusion.
- Why unresolved: The current study utilized a general Chinese-RoBERTa model, which showed limited generalization capabilities (F1 Macro of 0.2917) on the unseen-author Test2 set.
- What evidence would resolve it: A comparative experiment showing a domain-adapted model outperforming the baseline Chinese-RoBERTa on a hold-out set of entirely new authors.

### Open Question 2
- Question: To what extent does the removal of token-level data augmentation and the use of natural, balanced datasets improve the reliability of authorship attribution evaluation?
- Basis in paper: [explicit] The authors conclude that design limitations like "label imbalance, shallow lexical differences" in Test2 obscured results, advising to "reduce reliance on token-level data augmentation."
- Why unresolved: It is unclear if the poor performance on Test2 was caused by the model's inability to generalize or by artifacts introduced by synthetic token injection ([SYN0]).
- What evidence would resolve it: Re-evaluating the model on a new test set constructed solely from natural lyrics without synthetic augmentation, ensuring balanced author representation.

### Open Question 3
- Question: Which specific linguistic features allow models to succeed in structured genres (e.g., Folklore & Tradition) while failing in abstract ones (e.g., Love & Romance)?
- Basis in paper: [inferred] While the paper confirms performance varies by genre (Hypothesis 2), it does not isolate the specific stylistic markers driving the accuracy gap.
- Why unresolved: The authors note the gap but rely on high-level assumptions (e.g., "emotional and generic language" in Romance) rather than providing a feature importance analysis.
- What evidence would resolve it: An interpretability study (e.g., using SHAP or attention visualization) mapping specific lexical or syntactic features to attribution confidence across different genres.

## Limitations
- Synthetic token augmentation may artificially inflate performance metrics by creating predictable classification shortcuts
- Limited generalization to unseen authors (F1 Macro of 0.2917 on Test2) suggests model relies on topic rather than pure stylistic features
- Performance differences across genres may reflect dataset imbalances rather than fundamental attribution difficulty

## Confidence

- **High confidence** in the finding that structured genres (Folklore & Tradition) yield significantly higher attribution accuracy than abstract genres (Love & Romance)
- **Medium confidence** in the claim that fine-tuning improves robustness and generalization on real-world data (Test1)
- **Low confidence** in the conclusion that fine-tuning is generally superior to zero-shot LLM inference

## Next Checks

1. **Ablation on synthetic tokens:** Train the fine-tuned model on the original (non-augmented) Test2 pairs only and compare performance. If scores drop significantly from 0.7628 to near zero-shot levels (0.7396), this would confirm that synthetic tokens create artificial separability rather than measuring true stylistic generalization.

2. **Hard negative sampling verification:** Analyze the distribution of genre pairs in training hard negatives. If certain genre combinations (e.g., Love & Romance paired with Life & Reflection) dominate, the model may learn topic shortcuts. Re-balance these pairs and measure changes in cross-genre performance.

3. **Author-wise performance breakdown:** Compute per-author F1-Macro scores in Test1. If a few prolific authors dominate high accuracy while others show poor performance, this would indicate the model relies on author-specific quirks rather than generalizable stylistic features. Identify the minimum number of works per author needed for reliable attribution.