---
ver: rpa2
title: Joint Embeddings Go Temporal
arxiv_id: '2509.25449'
source_url: https://arxiv.org/abs/2509.25449
tags:
- learning
- time
- series
- ts-jepa
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Time Series Joint Embedding Predictive Architecture
  (TS-JEPA), a self-supervised learning approach for time series that leverages joint
  embedding to predict masked portions of sequences in latent space rather than input
  space. The method addresses limitations of autoregressive and masked reconstruction
  approaches that are sensitive to noise and confounding variables.
---

# Joint Embeddings Go Temporal

## Quick Facts
- arXiv ID: 2509.25449
- Source URL: https://arxiv.org/abs/2509.25449
- Reference count: 35
- Primary result: TS-JEPA achieves competitive or superior performance to state-of-the-art baselines on both classification and forecasting tasks through latent space masked prediction

## Executive Summary
This paper introduces Time Series Joint Embedding Predictive Architecture (TS-JEPA), a self-supervised learning approach for time series that leverages joint embedding to predict masked portions of sequences in latent space rather than input space. The method addresses limitations of autoregressive and masked reconstruction approaches that are sensitive to noise and confounding variables. TS-JEPA uses a tokenizer, transformer encoder, predictor, and EMA-encoder to learn representations by minimizing L1 reconstruction loss in latent space. Experiments on multiple datasets demonstrate that TS-JEPA achieves competitive or superior performance to state-of-the-art baselines on both classification and forecasting tasks, with particularly strong results on classification.

## Method Summary
TS-JEPA operates by tokenizing time series into patches, masking 70% of them, and predicting the latent representations of masked patches using a transformer predictor. The model learns to reconstruct masked information in the latent space by minimizing L1 distance between predictions and targets from an EMA-encoder. The EMA-encoder provides slowly moving targets that stabilize training and prevent representation collapse. The architecture consists of a 1D-CNN tokenizer with positional embeddings, a transformer encoder for non-masked patches, a predictor that estimates masked patch representations, and an EMA-encoder that generates targets. Downstream tasks use a frozen evaluation protocol where the encoder is kept fixed and only a lightweight task-specific head is trained.

## Key Results
- TS-JEPA matches or exceeds MAE and autoregressive baselines on most classification datasets (FordA 91.5%, ECG5000 89.5%)
- The approach shows good sample efficiency when labeled data is limited
- Long-term forecasting results are mixed, with TS-JEPA accumulating less error on ETT-Small and Electricity but more on Weather
- TS-JEPA demonstrates robustness to noise through latent space prediction rather than input-space reconstruction

## Why This Works (Mechanism)

### Mechanism 1
Latent space prediction filters out noise and irrelevant details that input-space reconstruction would otherwise model. By predicting the EMA-encoded representation of masked patches rather than raw values, the predictor must learn which latent features are predictable from context, discarding high-frequency noise and confounding factors that do not have consistent representations. This assumes the EMA-encoder learns stable, semantically meaningful representations where predictable structure is preserved and noise is suppressed.

### Mechanism 2
The EMA-encoder provides slowly moving targets that stabilize training and prevent representation collapse. The predictor chases a target that evolves gradually as an exponential moving average of the encoder weights, providing a stable learning signal while avoiding trivial constant solutions. This assumes a slow-moving target encourages meaningful representations rather than collapse to constants, though direct corpus validation for EMA specifically in time series is limited.

### Mechanism 3
Masked prediction in latent space yields representations that transfer better to classification than autoregressive objectives, at some cost to short-horizon forecasting. Classification benefits from representations encoding context-aware structure rather than precise next-step values, whereas autoregressive training specializes for sequential prediction at the expense of global semantics. This assumes downstream tasks differ in whether they require local predictive precision versus global semantic abstraction.

## Foundational Learning

- **Joint Embedding Architectures (Encoder + Target Encoder)**: TS-JEPA relies on two encoders where the target encoder provides latent targets for prediction; understanding BYOL-style asymmetry and EMA updates is essential. Quick check: Can you explain why using two identical encoders without EMA or asymmetry leads to collapse?
- **Masked Modeling vs Contrastive Learning**: The paper positions TS-JEPA against both contrastive (TS2Vec) and masked reconstruction (MAE) baselines; understanding these paradigms clarifies tradeoffs. Quick check: What is the difference in supervision signal between predicting masked tokens and contrasting positive/negative pairs?
- **Positional Encoding for Temporal Order**: The tokenizer uses absolute sin-cos positional embeddings; temporal order is critical for the predictor to learn meaningful dynamics. Quick check: Why must positional information be preserved when patchifying a time series for a transformer encoder?

## Architecture Onboarding

- **Component map**: Tokenizer (1D-CNN + positional embeddings) -> Patch splitting (70% masked, 30% non-masked) -> Encoder processes non-masked patches -> Predictor estimates masked representations -> EMA-Encoder generates targets -> L1 loss computation
- **Critical path**: Pretrain: Forward pass through tokenizer → split into masked (70%) and non-masked (30%) patches → encoder processes non-masked → predictor outputs predictions → EMA-encoder processes masked → compute L1 loss. Downstream: Freeze encoder, train lightweight classification/regression head on labeled data (frozen evaluation protocol).
- **Design tradeoffs**: Masking ratio: 70% for TS-JEPA vs 75% for MAE; higher masking increases prediction difficulty but may improve generalization if model capacity is sufficient. Latent vs input prediction: Robustness to noise vs loss of fine-grained signal detail. EMA momentum: Higher stability (m ≈ 0.998) vs slower adaptation to improved representations.
- **Failure signatures**: Collapse: Constant encoder outputs regardless of input; monitor representation variance across batches. Overfitting to noise: If tokenizer is too high-capacity without regularization, latent space may encode noise; check reconstruction loss on held-out noise-augmented data. Poor transfer: Large gap between pretraining and downstream performance may indicate insufficient encoder capacity or mismatched masking strategy.
- **First 3 experiments**: 1) Reproduce classification results on FordA with frozen encoder + linear head; confirm accuracy within reported confidence intervals. 2) Ablate masking ratio (50%, 70%, 90%) on a validation set to confirm 70% is near-optimal for this architecture. 3) Test EMA momentum values (0.99, 0.996, 0.999) and monitor representation variance to verify collapse avoidance and stable training curves.

## Open Questions the Paper Calls Out

- **Scaling strategies for foundation models**: Next steps will include exploration of scaling strategies for TS-JEPA, with the ultimate goal of establishing a new paradigm for time series foundation models. This remains unresolved as current experiments use fixed small architecture and moderate dataset sizes.
- **Architectural choices impact**: While not conducting an ablation study on configuration, the paper defers investigation of architectural choices (attention heads, embedding dimension, patch count, masking ratio) to future work, leaving their impact on representation quality unexplored.
- **Multivariate time series performance**: The architecture is claimed to be easily adaptable to multivariate time series, but all experiments use univariate datasets, leaving multivariate capability unvalidated.
- **Cross-domain representation transfer**: Transfer experiments only test on similar datasets (FordA→FordB), leaving whether representations generalize across fundamentally different data generating processes (sensor to financial, medical to climate) unexplored.

## Limitations

- Critical architectural details like transformer depth, MLP hidden dimensions, and exact patch sizes are underspecified, limiting faithful reproduction
- Ablation studies focus primarily on masking ratio and EMA momentum without exploring transformer architecture variations or downstream head design
- The frozen evaluation protocol may underestimate true transfer potential compared to fine-tuning approaches
- Dataset-specific biases and extensive generalization testing across diverse domains are not addressed

## Confidence

- **High confidence**: TS-JEPA outperforms MAE and autoregressive baselines on most classification tasks; masked latent space prediction is more robust to noise than input-space reconstruction; EMA stabilization prevents representation collapse
- **Medium confidence**: TS-JEPA achieves competitive forecasting performance with specific advantages in long-term horizons on certain datasets; 70% masking ratio is optimal for this architecture
- **Low confidence**: TS-JEPA will generalize to multimodal time series or very long sequences without architectural modifications; the specific EMA momentum value (0.998) is optimal across all time series domains

## Next Checks

1. **Architecture sensitivity analysis**: Systematically vary transformer depth (1-4 layers), MLP hidden dimensions, and dropout rates to identify performance sensitivity and potential overfitting points. Compare full fine-tuning vs frozen evaluation protocols on classification tasks.
2. **Cross-dataset generalization test**: Train TS-JEPA on one dataset family (e.g., UCR classification) and evaluate on out-of-distribution datasets (e.g., wearable sensor data or medical time series) to assess true representation transferability.
3. **Ablation of masking strategies**: Compare uniform masking (70%) against block-wise masking and stochastic block size variations to determine if localized temporal context is more valuable than dispersed context for different downstream tasks.