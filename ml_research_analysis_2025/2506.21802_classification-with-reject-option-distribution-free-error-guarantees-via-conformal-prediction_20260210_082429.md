---
ver: rpa2
title: 'Classification with Reject Option: Distribution-free Error Guarantees via
  Conformal Prediction'
arxiv_id: '2506.21802'
source_url: https://arxiv.org/abs/2506.21802
tags:
- prediction
- error
- conformal
- reject
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the connection between conformal prediction
  and machine learning with reject option in binary classification. By accepting only
  singleton predictions from conformal predictors, the authors create a binary classifier
  with reject option that provides distribution-free error guarantees.
---

# Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction

## Quick Facts
- **arXiv ID:** 2506.21802
- **Source URL:** https://arxiv.org/abs/2506.21802
- **Reference count:** 6
- **Primary result:** Formalizes connection between conformal prediction and reject option in binary classification, providing distribution-free error guarantees.

## Executive Summary
This paper establishes a theoretical foundation for using conformal prediction (CP) to implement reject option in binary classification. By accepting only singleton prediction sets from CP, the method creates a binary classifier with provable distribution-free error guarantees. The core contribution is a theoretical error rate formula for singleton predictions, resolving inaccuracies in prior works. The method works in both online and offline settings with finite sample estimates, and numerical experiments demonstrate the error-reject tradeoff across multiple datasets.

## Method Summary
The method converts conformal predictors into binary classifiers with reject option by accepting only singleton prediction sets (those containing exactly one label) and rejecting all others. The approach uses three datasets: qsar-biodeg, spambase, and California-Housing-Classification. Three experimental settings are employed: full conformal prediction with 1-NN nonconformity measure, offline inductive conformal prediction using the Crepes package with Random Forest, and batch offline ICP with incremental updates. The singleton error rate œÉ is estimated as (nŒµ - e)/s, where n is total predictions, Œµ is significance level, e is empty predictions, and s is singleton predictions.

## Key Results
- Singleton error rate formula œÉ = (Œµ - P(E))/P(S) is theoretically proven and corrects prior work inaccuracies
- Practical approximation œÉÃÇ = (nŒµ - e)/s works well across datasets but becomes noisy at large Œµ values
- Error-reject curves demonstrate the fundamental tradeoff between error rate and reject rate
- Method works in both online (full CP) and offline (inductive CP) settings with appropriate validity guarantees
- One-against-all approach enables extension to multiclass problems, though not theoretically analyzed in this work

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Accepting only singleton prediction sets from conformal predictors creates a binary classifier with reject option that maintains distribution-free error guarantees.
- **Mechanism:** Conformal prediction outputs prediction sets ŒìŒµ containing 0, 1, or 2 labels in binary classification. By mapping |ŒìŒµ| = 1 to "accept" and |ŒìŒµ| ‚àà {0, 2} to "reject," the method distinguishes novelty rejection (empty sets - insufficient confidence) from ambiguity rejection (double sets - overlapping classes).
- **Core assumption:** Data sequence is exchangeable (weaker than i.i.d.).
- **Evidence anchors:**
  - [abstract] "By accepting only the singleton predictions, we turn CP into a binary classifier with reject option."
  - [Section 4, Eq. 14] Formal definition of CP rejector mapping to ‚ìá‚àÖ, ‚ìáùíú, or singleton.
  - [corpus] Related work (Distribution-informed Online Conformal Prediction) confirms CP provides "distribution-free statistical guarantees" under exchangeability.
- **Break condition:** If exchangeability fails (e.g., strong distribution shift), validity guarantees degrade.

### Mechanism 2
- **Claim:** The singleton error rate œÉ is provably bounded by (Œµ - P(E)) / P(S), where Œµ is the significance level, P(E) is the empty prediction probability, and P(S) is the singleton probability.
- **Mechanism:** By the law of total probability: Œµ = P(err) = P(err|E)P(E) + P(err|S)P(S) + P(err|D)P(D). Since empty predictions are always errors (P(err|E)=1) and double predictions are never errors (P(err|D)=0), rearranging yields the singleton error formula.
- **Core assumption:** Smoothed conformal predictor with exact validity (errors are independent Bernoulli(Œµ)).
- **Evidence anchors:**
  - [Section 5.1, Eq. 15-16] Full derivation of œÉ = (Œµ - P(E)) / P(S).
  - [Proposition 2, page 6] "The probability of a singleton prediction making an error is œÉ := (Œµ - P(E)) / P(S)."
  - [corpus] Weak direct corpus support for this specific formula; related works focus on set prediction validity, not singleton analysis.
- **Break condition:** Formula assumes online smoothed CP; offline ICP requires adjustment via training-conditional validity (see Appendix B.2).

### Mechanism 3
- **Claim:** The theoretical singleton error rate can be practically approximated by (nŒµ - e) / s, where n is total predictions, e is empty predictions, and s is singleton predictions.
- **Mechanism:** By the law of large numbers, empirical frequencies converge to probabilities: lim(n‚Üí‚àû) e/n = P(E) and s/n = P(S). Substituting into the theoretical formula yields the practical estimator.
- **Core assumption:** Sufficiently large sample size for convergence; same calibration/test distribution.
- **Evidence anchors:**
  - [Section 5.1, Eq. 17] "œÉ = lim(n‚Üí‚àû) (nŒµ - e) / s"
  - [Section 7] "In our numerical illustrations, our approximation of œÉ is (nŒµ - e) / s."
  - [corpus] Conformal calibration standards paper mentions finite-sample guarantees but doesn't address singleton-specific estimation.
- **Break condition:** Approximation becomes noisy for large Œµ (many empty predictions, few singletons); small s amplifies estimation variance.

## Foundational Learning

- **Concept: Exchangeability**
  - Why needed here: All validity guarantees derive from this assumption; it's weaker than i.i.d. and permits dependence structure that preserves joint distribution under permutation.
  - Quick check question: If you shuffle the order of your training and test data, should prediction sets change? (Answer: No, under exchangeability.)

- **Concept: Nonconformity measures**
  - Why needed here: These functions (e.g., distance to nearest neighbor of same class) convert raw predictions into comparable scores; choice of measure affects efficiency but not validity.
  - Quick check question: What happens to validity if you use a "bad" nonconformity measure? (Answer: Still valid, but prediction sets may be larger/less efficient.)

- **Concept: Error-reject tradeoff**
  - Why needed here: Setting Œµ controls the balance; lower Œµ ‚Üí higher reject rate but lower error rate on accepted predictions.
  - Quick check question: If you want error rate < 5% on accepted predictions, can you always achieve it? (Answer: Only if reject rate is acceptable; some reject rates may be unachievable depending on data and predictor.)

## Architecture Onboarding

- **Component map:** Base model (any classifier) ‚Üí Nonconformity scorer ‚Üí Calibration set scoring ‚Üí p-value computation per label ‚Üí Prediction set ŒìŒµ ‚Üí Singleton filter ‚Üí Accept/Reject decision; For offline ICP: Split into proper training set (train base model) + calibration set (compute nonconformity scores)

- **Critical path:**
  1. Train base classifier on proper training set
  2. Compute nonconformity scores on calibration set (O(m log m) for m calibration samples)
  3. For each test object: compute p-values for both possible labels ‚Üí form ŒìŒµ
  4. Count singletons (s), empties (e), doubles
  5. Estimate singleton error rate: œÉÃÇ = (nŒµ - e) / s

- **Design tradeoffs:**
  - Full CP (online) ‚Üí strongest validity, computationally expensive O(n¬≤) per prediction
  - Offline ICP ‚Üí O(m log m) after training, but validity is (Œµ, Œ¥)-valid rather than exact
  - Larger calibration set ‚Üí tighter bounds, but less data for training
  - Significance level Œµ ‚Üí lower Œµ = lower error rate but higher reject rate

- **Failure signatures:**
  - Very high reject rate (>80%): Nonconformity measure poorly matched to problem, or Œµ set too low
  - œÉÃÇ > Œµ: Indicates calibration set distribution differs from test distribution, or implementation error
  - Negative œÉÃÇ values: Can occur when e > nŒµ; suggests miscalibration or insufficient calibration set size
  - Same reject rate, different error rates (Figure 1b): Multiple Œµ values yield same s; choose smaller Œµ for lower error

- **First 3 experiments:**
  1. **Baseline validation:** Implement offline ICP with Random Forest on a binary dataset (e.g., spambase). Split: 500 proper training, 500 calibration, 100 test. Compute œÉÃÇ across multiple Œµ values [0.01, 0.05, 0.1, 0.2]. Plot error-reject curve. Verify œÉÃÇ stays bounded by theoretical prediction.
  2. **Calibration set sensitivity:** Fix proper training size, vary calibration set size [100, 300, 500, 1000]. For each, compute training-conditional validity bound (Appendix A.5, Eq. 48: ŒµÃÉ = Œµ - ‚àö(ln(1/Œ¥) / 2h)). Observe how estimated singleton error rate converges.
  3. **Nonconformity measure comparison:** Compare two nonconformity measures (e.g., 1-nearest-neighbor distance vs. probability-based from Random Forest). Plot error-reject curves side-by-side. Expect same validity, different efficiency (achievable reject rates).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the singleton error rate guarantees be extended to multiclass classification beyond the one-against-all approach sketched?
- Basis in paper: [explicit] "In this work, we have only considered the case of binary classification. This is no fundamental restriction, as multiclass problems (with more than two possible labels) can be handled by building several models in a one-against-all fashion."
- Why unresolved: The one-against-all approach is mentioned but not theoretically analyzed or empirically validated; the singleton error rate formula derivation depends on prediction sets having cardinality 0, 1, or 2, which no longer holds for multiclass settings.
- What evidence would resolve it: A theoretical extension of the singleton error rate formula (Proposition 2) to general k-class problems, with empirical validation across multiclass datasets.

### Open Question 2
- Question: What causes the noisiness in the singleton error rate approximation œÉÃÇ at large significance levels Œµ, and how can it be mitigated?
- Basis in paper: [explicit] "The plots indicate that this approximation is noisy for large Œµ, which likely is caused by a large number of traditional CP empty predictions in combination with a small number of accepted predictions."
- Why unresolved: The authors propose a likely cause but do not characterize the conditions under which this noisiness becomes problematic, nor do they offer remedies beyond the observation.
- What evidence would resolve it: Theoretical bounds on approximation variance as a function of Œµ, e, and s; or alternative estimators with reduced variance at large Œµ.

### Open Question 3
- Question: How can users systematically select the significance level Œµ when the error-reject curve exhibits non-monotonic behavior (multiple error rates for the same reject rate)?
- Basis in paper: [explicit] "the curve in Figure 1b indicates two different error rates for some reject rates. This happens because some Œµ yield the same number of singleton predictions in this case. In this situation, it is preferable to choose the Œµ that leads to the lower error rate, which will always be the smallest one."
- Why unresolved: While the authors provide a rule of thumb (choose smaller Œµ), they do not provide a principled decision framework, especially when the trade-off between error rate and reject rate must meet application-specific constraints.
- What evidence would resolve it: A decision-theoretic framework or algorithm for selecting Œµ given constraints on tolerable error and reject rates, validated across diverse datasets and conformal predictors.

## Limitations

- The approximation œÉÃÇ = (nŒµ - e)/s can produce negative or unstable estimates when reject rate is high or singleton count is small
- Validity guarantees depend on exchangeability assumptions that may fail under distribution shift or concept drift
- The method only theoretically analyzes binary classification, with multiclass extension mentioned but not validated
- High reject rates (>80%) suggest poor nonconformity measure choice or inappropriately low Œµ setting

## Confidence

- **High:** The core theoretical framework linking conformal prediction to reject option classification is well-established and rigorously proven.
- **Medium:** The practical approximation formula and its numerical stability across diverse datasets and parameter settings.
- **Low:** The performance implications when exchangeability assumptions are violated (distribution shift, concept drift).

## Next Checks

1. **Robustness under distribution shift:** Evaluate œÉÃÇ estimation when test data distribution differs from calibration data (e.g., covariate shift experiments).
2. **High-reject-rate regime analysis:** Systematically study when and why œÉÃÇ becomes negative or highly variable, particularly for Œµ > 0.1.
3. **Alternative nonconformity measures:** Compare 1-NN, probability-based, and margin-based measures to quantify efficiency vs. stability tradeoffs.