---
ver: rpa2
title: 'Social Caption: Evaluating Social Understanding in Multimodal Models'
arxiv_id: '2601.14569'
source_url: https://arxiv.org/abs/2601.14569
tags:
- social
- video
- context
- human
- mllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SOCIALCAPTION is a framework for evaluating multimodal models''
  social understanding across three dimensions: Social Inference (QA accuracy), Holistic
  Social Analysis (comprehensive video descriptions), and Directed Social Analysis
  (targeted information extraction). Experiments with state-of-the-art models reveal
  that smaller open-source models can match or exceed larger closed-source models
  on certain dimensions, and that performance in one dimension does not guarantee
  strong performance in others.'
---

# Social Caption: Evaluating Social Understanding in Multimodal Models

## Quick Facts
- arXiv ID: 2601.14569
- Source URL: https://arxiv.org/abs/2601.14569
- Reference count: 40
- Social understanding evaluation framework with 3 dimensions: Social Inference, Holistic Social Analysis, Directed Social Analysis

## Executive Summary
SOCIALCAPTION introduces a framework for evaluating multimodal models' social understanding across three dimensions: Social Inference (multiple-choice QA), Holistic Social Analysis (comprehensive video descriptions), and Directed Social Analysis (targeted information extraction). Experiments with state-of-the-art models reveal that smaller open-source models can match or exceed larger closed-source models on certain dimensions, and that performance in one dimension does not guarantee strong performance in others. MLLM judges show strong alignment with human ratings, demonstrating the viability of scalable, automated evaluation.

## Method Summary
The framework evaluates MLLMs on SOCIAL-IQ 2.0 validation set (145 videos, 943 questions) using three task types. Social Inference uses multiple-choice questions answered with or without transcriptions. Holistic Social Analysis and Directed Social Analysis require open-ended video descriptions evaluated by human annotators using structured rubrics and MLLM judges. Videos are processed at 1 FPS with WhisperX transcriptions. Human-annotator agreement is measured via binary F1 scores, and MLLM judge alignment with human scores is assessed.

## Key Results
- Smaller open-source models (InternVL3) match or exceed larger closed-source models on HSA/DSA dimensions
- Social Inference accuracy improves up to 13% with transcription inclusion across all models
- MLLM judges show strong alignment with human ratings (binary F1 > 90%) for HSA/DSA evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spoken context (transcriptions) provides complementary signals that improve social inference accuracy by revealing intent, emotion, and relationships not always visible in visual cues alone.
- **Mechanism:** Transcriptions supply explicit verbal and prosodic information (dialogue content, tone indicators via timestamps) that allows models to ground social inferences in both visual and linguistic modalities, reducing ambiguity in non-verbal signals.
- **Core assumption:** Social interactions encode information across modalities; verbal content carries semantic information about mental states and relationships that visual behavior alone may not fully convey.
- **Evidence anchors:**
  - Providing transcriptions consistently improved SI performance across all models, with accuracy gains up to 13%... demonstrating the importance of spoken context in MLLM social reasoning, as this modality captures information about intent, emotion, and relationships that informs accurate social inferences.
  - The text-only baseline achieves 64.64% indicating that spoken context and LLM backbone priors within MLLM architectures provide strong signals for accurate SI performance.
  - Related work (Social Genome, SIV-Bench) similarly emphasizes multimodal cues—verbal, visual, vocal—for social reasoning, supporting the claim that language is a critical modality.
- **Break condition:** If visual cues are sufficient (e.g., purely physical interactions with minimal dialogue), transcription gains may diminish. If transcriptions are noisy or mistranscribed, performance may degrade.

### Mechanism 2
- **Claim:** Social understanding dimensions (SI, HSA, DSA) involve distinct capabilities that do not automatically transfer; architectural choices and training regimes create specialization in specific dimensions.
- **Mechanism:** SI relies on discriminative selection among options (focused inference), while HSA requires generative synthesis of comprehensive descriptions across multiple sub-dimensions (scene, individuals, socio-emotional dynamics), and DSA requires targeted extraction. Models optimized for one task (e.g., QA accuracy) may not develop the generative or integrative capacities needed for others.
- **Core assumption:** Social understanding is not monolithic; it comprises separable cognitive processes (inference, holistic synthesis, directed extraction) that may rely on different model capabilities.
- **Evidence anchors:**
  - Models with strong SI performance can struggle to comprehensively describe social information... Gemini-2.5-Pro (the highest-performing model per SI) demonstrates uneven performance along sub-dimensions... improvements in visual processing or SI reasoning ability do not necessarily result in stronger abilities to describe social scenes.
  - Qwen2.5-Omni demonstrated strong performance in the discriminative SI task (68.42%) but struggled across all sub-dimensions in both HSA and DSA... This model contains identical MLLM backbones to Qwen2.5-VL, yet Qwen2.5-VL did not struggle... Qwen2.5-Omni introduces a low-latency streaming architecture that reduced its ability to synthesize coherent social narratives.
  - Related benchmarks (SIV-Bench, Social Genome) focus on inference tasks, not cross-dimensional comparison.
- **Break condition:** If models are trained with multi-task objectives explicitly spanning inference, description, and extraction, transfer may improve. Architectures that sacrifice synthesis for streaming/low-latency may underperform on HSA/DSA regardless of backbone quality.

### Mechanism 3
- **Claim:** MLLM judges can approximate human evaluation for social understanding because they have internalized similar social reasoning patterns during pretraining and can apply structured rubrics consistently.
- **Mechanism:** Judge models receive the same evaluation instructions and rubrics as humans; their pretraining on large-scale multimodal and social text enables them to assess whether descriptions capture relevant social information. Strong human-judge alignment suggests judges have learned to distinguish high-quality from low-quality social descriptions.
- **Core assumption:** The judge models' pretraining corpora include sufficient examples of social interaction descriptions and evaluations for them to internalize human-like judgment criteria.
- **Evidence anchors:**
  - We find that Gemini-2.5-Pro exhibits a weaker alignment with human scores, compared to InternVL3 models which exhibit higher alignment with humans across nearly all HSA and DSA sub-dimensions... the distinction between high-quality and low-quality HSA and DSA generations strongly aligns with human judgments.
  - InternVL3-78B achieves the highest alignment with human ratings, exhibiting strong binary F1 and low average absolute difference values.
  - Related work (MLLM-as-a-Judge, JudgeAnything) explores LLM/MLLM judges for multimodal evaluation, supporting the general approach but not specific to social understanding.
- **Break condition:** Judges show self-preference bias and tend to assign higher absolute scores; alignment is better for relative ranking than absolute scoring. Judges may fail on edge cases or culturally specific social norms underrepresented in training data.

## Foundational Learning

- **Concept: Multimodal Fusion**
  - **Why needed here:** SOCIALCAPTION evaluates models that must integrate visual, verbal (transcription), and temporal information to understand social interactions. Understanding how modalities complement each other is prerequisite.
  - **Quick check question:** Given a video of a tense conversation, what unique information does the visual modality provide versus the transcription? How would their fusion improve inference over either alone?

- **Concept: Social Cognition Taxonomy (APRACE: Actors, Partners, Relations, Activities, Context, Evaluation)**
  - **Why needed here:** HSA and DSA evaluation criteria are grounded in this taxonomy. Understanding what components constitute social interactions is necessary to interpret rubrics and model outputs.
  - **Quick check question:** For a video of two colleagues debating, identify each APRACE component. Which components would a model need to capture for a high HSA score?

- **Concept: Discriminative vs. Generative Evaluation Tasks**
  - **Why needed here:** SI (multiple-choice QA) is discriminative; HSA/DSA (open-ended descriptions) are generative. The paper shows these tap different capabilities, explaining why models may excel at one but not others.
  - **Quick check question:** Why might a model achieve 80% accuracy on social inference QA but score poorly on generating a comprehensive social scene description? What architectural or training factors could cause this divergence?

## Architecture Onboarding

- **Component map:** Video input (1 FPS) -> Vision encoder -> Temporal modeling (3D conv or frame aggregation) -> Alignment module -> LLM backbone -> Output (SI: answer selection; HSA/DSA: generated description). Transcriptions (from WhisperX) concatenated with visual tokens as additional input context.

- **Critical path:** Video input (1 FPS) → Vision encoder → Temporal modeling (3D conv or frame aggregation) → Alignment module → LLM backbone → Output (SI: answer selection; HSA/DSA: generated description). Transcriptions (from WhisperX) concatenated with visual tokens as additional input context.

- **Design tradeoffs:**
  - **Scale vs. Efficiency:** Larger models (InternVL3-78B, Qwen2VL-72B) approach closed-source SI performance but require more compute; smaller models (7-8B) can match or exceed closed-source on HSA/DSA with better architectural choices.
  - **Pretraining vs. Fine-tuning:** Including video data in pretraining was not found necessary for strong SI (InternVL3, Qwen2-VL use video fine-tuning without video pretraining); fine-tuning strategy matters more.
  - **Streaming vs. Synthesis:** Streaming architectures (Qwen2.5-Omni) enable low latency but may impair ability to synthesize coherent social narratives for HSA/DSA.

- **Failure signatures:**
  - **Poor temporal modeling:** LongVA treats video as extended images; achieves only marginal improvement over text-only baseline (65.43% vs. 64.64%), indicating failure to capture temporal social signals.
  - **Streaming-optimized architectures:** Qwen2.5-Omni matches backbone on SI but fails on HSA/DSA (lowest scores across all sub-dimensions), suggesting synthesis capability is sacrificed for latency.
  - **Self-preference in judges:** MLLM judges (especially Gemini-2.5-Pro) assign higher scores to their own outputs; use relative ranking rather than absolute scores.
  - **Cross-dimensional non-transfer:** High SI accuracy does not predict HSA/DSA performance; must evaluate all dimensions separately.

- **First 3 experiments:**
  1. **Baseline SI evaluation:** Run SI prompts (with and without transcriptions) on a subset of videos using a standard-scale model (e.g., InternVL3-8B). Compare accuracy to paper benchmarks (73.83% w/ trans.) to validate your setup and prompt formatting.
  2. **HSA generation and human evaluation:** Generate HSA descriptions for 5 videos using two contrasting architectures (e.g., InternVL3-8B vs. Qwen2.5-Omni). Have human annotators score outputs using the 6-criterion rubric. Verify that InternVL3-8B outperforms the streaming model, confirming architectural impact.
  3. **MLLM judge alignment test:** Use InternVL3-8B and Gemini-2.5-Pro as judges to evaluate a subset of HSA/DSA outputs. Compute binary F1 and agreement with human scores. Confirm InternVL3 models show stronger alignment (F1 > 90%) than Gemini-2.5-Pro, validating judge selection for scalable evaluation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MLLM performance on Holistic and Directed Social Analysis dimensions change when applied to long-context video interactions extending beyond the 1-minute duration used in this study? [explicit] The authors explicitly state in the Limitations section that all videos are 1 minute long and note, "There remains an open challenge for the research community to study long-context social understanding." Why unresolved: Current experiments rely on short-term, micro-social interactions, leaving the models' ability to synthesize coherent social narratives over extended periods (e.g., 10+ minutes) unknown. What evidence would resolve it: An evaluation of MLLLMs on the SOCIALCAPTION framework using a curated dataset of longer-form social interaction videos (e.g., 5 to 60 minutes).

- **Open Question 2:** Do the specific social understanding capabilities identified in this study generalize to non-English and non-US-centric cultural contexts? [explicit] The paper notes that the data is restricted to English and US annotators, and explicitly motivates "future community research that builds upon SOCIALCAPTION to evaluate social understanding abilities in broader contexts." Why unresolved: Social norms, non-verbal cues, and interaction dynamics differ significantly across cultures, and current benchmarks do not test if models can handle this diversity. What evidence would resolve it: Results from applying the SOCIALCAPTION framework to a dataset of multilingual and multicultural social interactions, comparing model performance against the original English/US-centric baseline.

- **Open Question 3:** Why does strong performance in Social Inference (SI) fail to guarantee high performance in Socio-Emotional Analysis (a sub-component of Holistic Social Analysis)? [explicit] The authors observe that Gemini-2.5-Pro achieved top SI performance but "substantially lower performance in capturing Socio-Emotional information," concluding that "improvements in visual processing or SI reasoning ability do not necessarily result in stronger abilities to describe social scenes." Why unresolved: The paper identifies the correlation failure but does not investigate the specific architectural or training bottlenecks that cause generative social description to lag behind discriminative inference. What evidence would resolve it: Ablation studies analyzing whether specific visual encoders or fine-tuning datasets are responsible for the disconnect between recognizing social cues (SI) and synthesizing them into descriptions (HSA).

- **Open Question 4:** What specific factors enable smaller open-source models (like InternVL3) to align more closely with human judges than larger closed-source models? [inferred] The results show that InternVL3-78B and InternVL3-8B achieved higher binary F1 scores with human annotators than Gemini-2.5-Pro, despite Gemini's superior inference capabilities. Why unresolved: The paper validates the viability of MLLM judges but does not explain the mechanism behind the superior human-alignment of specific model families over others. What evidence would resolve it: An analysis of the training distributions and reward models of InternVL3 versus Gemini to identify if specific data alignment techniques lead to more "human-like" evaluation judgments.

## Limitations
- Cross-cultural generalizability: The SOCIAL-IQ 2.0 dataset and evaluation criteria are likely biased toward Western social norms and communication styles.
- Temporal resolution constraints: The study uses 1 FPS video processing, which may miss critical social cues that occur between frames.
- Judge model limitations: MLLM judges exhibit self-preference bias and perform better at relative ranking than absolute scoring.

## Confidence
- High Confidence: The finding that smaller open-source models can match or exceed larger closed-source models on HSA/DSA dimensions is well-supported by direct experimental comparisons across multiple model families with clear performance metrics.
- Medium Confidence: The claim about distinct capabilities required for different social understanding dimensions is supported by cross-dimensional performance analysis, though could benefit from additional ablation studies isolating specific architectural factors.
- Medium Confidence: The assertion that MLLM judges can approximate human evaluation is supported by strong alignment metrics, but the self-preference bias and limitations in absolute scoring suggest this approach requires careful calibration for production use.

## Next Checks
1. **Cross-cultural validation:** Test model performance on social understanding tasks from non-Western cultural contexts to assess generalizability. Use videos from diverse cultural sources and evaluate whether models maintain consistent performance across cultural boundaries.

2. **Temporal resolution ablation:** Systematically evaluate model performance at multiple frame rates (0.5 FPS, 1 FPS, 2 FPS, 5 FPS) to quantify the impact of temporal granularity on social understanding accuracy, particularly for fast-paced social interactions.

3. **Judge calibration study:** Conduct a comprehensive analysis of MLLM judge biases by testing their performance on synthetic outputs spanning the full quality spectrum. Measure self-preference bias magnitude and develop calibration techniques to improve absolute scoring reliability.