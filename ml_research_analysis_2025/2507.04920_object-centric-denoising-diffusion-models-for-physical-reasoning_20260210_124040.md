---
ver: rpa2
title: Object-centric Denoising Diffusion Models for Physical Reasoning
arxiv_id: '2507.04920'
source_url: https://arxiv.org/abs/2507.04920
tags:
- objects
- object
- diffusion
- denoising
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an object-centric denoising diffusion model
  for physical reasoning that generates object trajectories conditioned on multiple
  states at arbitrary time steps. The model is translation equivariant over time and
  permutation equivariant over objects, enabling flexible inference with varying numbers
  of objects and trajectory lengths.
---

# Object-centric Denoising Diffusion Models for Physical Reasoning

## Quick Facts
- arXiv ID: 2507.04920
- Source URL: https://arxiv.org/abs/2507.04920
- Authors: Moritz Lange; Raphael C. Engelhardt; Wolfgang Konen; Andrew Melnik; Laurenz Wiskott
- Reference count: 10
- Primary result: Object-centric denoising diffusion model for physical reasoning with translation equivariance over time and permutation equivariance over objects, evaluated on PHYRE benchmark

## Executive Summary
This paper introduces an object-centric denoising diffusion model for generating object trajectories conditioned on multiple states at arbitrary time steps. The model achieves translation equivariance over time and permutation equivariance over objects through a novel Attention-Convolution block architecture. The approach outperforms autoregressive methods on physical reasoning tasks where objects must connect initial and goal states, demonstrating the ability to solve PHYRE benchmark tasks through flexible conditioning mechanisms combining soft (FiLM modulation) and hard (trajectory shifting) approaches.

## Method Summary
The model uses a U-Net architecture built from Attention-Convolution (AC) blocks, where each block processes object trajectories through MLP projection, Multi-Head Attention for object interactions, and 1D temporal convolution for temporal evolution. Soft conditioning is implemented via FiLM modulation with a mask, while hard conditioning enforces constraints through trajectory shifting. The model is trained on PHYRE object feature vectors with cosine noise schedule and MSE loss, augmented with random offsets and boundary bars.

## Key Results
- Median RMSE of approximately 0.034 on PHYRE template 0
- Demonstrates flexible inference with varying numbers of objects and trajectory lengths
- Outperforms autoregressive approaches on physical reasoning tasks
- Ablation studies confirm importance of both architecture and conditioning mechanisms

## Why This Works (Mechanism)
The model works by combining diffusion generative modeling with equivariant architectures. The AC blocks enable the model to learn temporal evolution and object interactions while maintaining desired symmetries. Soft conditioning via FiLM provides flexible conditioning, while hard conditioning via shifting guarantees constraint satisfaction. The translation equivariance over time allows flexible trajectory lengths, and permutation equivariance over objects enables generalization to varying object counts.

## Foundational Learning
- **Denoising Diffusion Probabilistic Models (DDPMs):** The core generative engine using forward noise addition and reverse denoising processes. *Why needed:* Provides framework for generating trajectories conditioned on multiple states. *Quick check:* Can you explain how a U-Net predicts the noise at each timestep t in the reverse process?
- **Equivariance in Neural Networks:** The model maintains specific symmetries - permutation equivariance for objects and translation equivariance for time. *Why needed:* Enables generalization to varying object counts and trajectory lengths. *Quick check:* If you permute the order of objects in the input, does an equivariant model's output for each object change, or just the order of the outputs?
- **FiLM (Feature-wise Linear Modulation):** Conditioning mechanism that scales and shifts intermediate feature maps. *Why needed:* Provides flexible soft conditioning for arbitrary state constraints. *Quick check:* In FiLM, what are the two operations typically applied to the feature maps based on the conditioning signal?

## Architecture Onboarding

**Component Map:**
Input Trajectory Tensor (Objects × Time × Features) → U-Net → Attention-Convolution Blocks → Predicted Trajectory → Conditioning Module (FiLM + Mask) → Soft Conditioned Output → Hard Conditioning (Trajectory Shifting)

**Critical Path:**
1. Input: Concatenate noisy trajectory X[t] with time embedding t
2. U-Net Forward: Signal passes through residual blocks containing AC blocks with FiLM modulation
3. Prediction: U-Net outputs predicted clean trajectory X̂[0]
4. Hard Conditioning: X̂[0] is shifted to satisfy constraints
5. Next Step: Result used to sample X[t-1] for next denoising iteration

**Design Tradeoffs:**
- Diffusion vs. Autoregressive: Diffusion allows future goal conditioning but is slower; autoregressive is fast but cannot condition on future
- Hard vs. Soft Conditioning: Soft conditioning is flexible but doesn't guarantee exact satisfaction; hard conditioning guarantees it but can create artifacts

**Failure Signatures:**
- Inconsistent Physics: Objects passing through each other or defying gravity
- Drift at Longer Trajectories: Performance degrades for lengths far from training data (L≠64)
- Generalization Issues: Struggles with novel relative object positions not well-covered in training

**First 3 Experiments:**
1. Ablate Conditioning: Train soft-only vs. soft+hard models, compare RMSE on generated trajectories
2. Vary Object Count: Train on 3 objects, test with object removal or addition to test equivariance
3. Vary Trajectory Length: Train on L=64, generate L=50, 64, 100, plot RMSE vs. length

## Open Questions the Paper Calls Out
- **Physical correction steps:** Can overlap penalties or collision resolution be integrated into denoising without convergence failure? The authors attempted but failed to converge.
- **Interaction coverage impact:** How does limited coverage of relative object positions in training data affect generalization? PHYRE benchmark lacks sufficient diversity to determine this.
- **Temporal convolution boundaries:** How to modify Conv1D layers to eliminate boundary effects for unseen trajectory lengths? Performance degrades for L≠64 due to boundary effects.
- **Raw visual input:** Can the equivariant architecture process images directly instead of pre-extracted object features? Currently depends on ground-truth object states.

## Limitations
- Narrow interaction coverage in PHYRE limits validation of general physical dynamics
- Performance degradation for trajectory lengths far from training length (L≠64)
- Hard conditioning can introduce physically implausible artifacts when predictions are far from constraints

## Confidence

**High Confidence:**
- Architectural contribution of AC blocks with equivariance properties
- General approach of diffusion models for multi-state conditioning
- Performance advantage over autoregressive methods

**Medium Confidence:**
- Effectiveness of FiLM-based soft conditioning vs alternatives
- Claim about solving physical reasoning tasks on PHYRE subset
- Generalizability across 25 PHYRE templates given high variance

**Low Confidence:**
- Claims about handling arbitrary object numbers and trajectory lengths
- Model's capture of general physical interaction dynamics

## Next Checks

1. **Interaction Coverage Analysis**: Quantify object distance distributions in PHYRE training data; generate synthetic test cases with varied interaction patterns to assess generalization beyond narrow training distribution.

2. **Conditioning Mechanism Ablation**: Systematically compare soft-only, hard-only, and combined conditioning across multiple templates; measure both trajectory accuracy (RMSE) and physical plausibility (object penetration depth).

3. **Trajectory Length Robustness**: Conduct systematic study across L=32 to L=128; plot performance degradation to identify failure points and correlate with U-Net receptive field limitations.