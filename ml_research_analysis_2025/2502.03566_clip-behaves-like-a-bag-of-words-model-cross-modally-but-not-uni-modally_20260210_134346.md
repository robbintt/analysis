---
ver: rpa2
title: CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally
arxiv_id: '2502.03566'
source_url: https://arxiv.org/abs/2502.03566
tags:
- clip
- text
- binding
- objects
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why CLIP behaves like a bag-of-words (BoW)
  model when binding attributes to objects across modalities. The authors first demonstrate
  that while CLIP is BoW-like in cross-modal binding (e.g., failing to distinguish
  between "a blue square and an orange triangle" vs "a blue triangle and an orange
  square"), it actually encodes attribute-object binding information within each modality.
---

# CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally

## Quick Facts
- arXiv ID: 2502.03566
- Source URL: https://arxiv.org/abs/2502.03566
- Reference count: 40
- This paper demonstrates that CLIP's cross-modal bag-of-words behavior stems from poor alignment rather than missing binding information in individual modalities, and proposes LABCLIP to fix this with a simple linear transformation.

## Executive Summary
This paper investigates why CLIP fails at cross-modal attribute-object binding despite encoding this information within each modality. The authors show through linear probing that CLIP embeddings for images and text can separately predict which attributes belong to which objects with high accuracy, demonstrating that binding information exists. The failure occurs only when matching across modalities under cosine similarity. They propose LABCLIP, which applies a learned linear transformation to text embeddings before cross-modal comparison, significantly improving binding accuracy on both synthetic and real-world datasets without requiring full model fine-tuning.

## Method Summary
LABCLIP addresses CLIP's cross-modal bag-of-words behavior by adding a learned linear transformation matrix A to text embeddings before cosine similarity computation. The method freezes CLIP's image and text encoders and trains only A using contrastive loss with synthetic hard negatives (attribute permutations). Two training modes are used: Standard Batch (SB) and Hard Negative Batch (HNB). The transformation remaps text embeddings into the image-embedding structure, enabling proper cross-modal binding without modifying the original CLIP weights. LABCLIP is evaluated on synthetic datasets (CLEVR, PUG:SPAR, PUG:SPARE) and real-world benchmarks (ARO, SugarCrepe, COCO).

## Key Results
- Linear probes on frozen CLIP embeddings achieve 0.96 (image) and 1.00 (text) accuracy on CLEVR, vs. 0.12 random baseline, demonstrating binding information exists within each modality
- Cross-modal accuracy with vanilla CLIP is near chance (0.50-0.56) on binding tasks, confirming bag-of-words behavior
- LABCLIP improves cross-modal binding accuracy to 0.94-0.95 on synthetic datasets and shows consistent gains on real-world benchmarks without full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained CLIP encoders already encode attribute-object binding within each modality; the failure manifests only at cross-modal matching
- Linear probes trained on frozen CLIP embeddings can predict which attribute belongs to which object with near-ceiling accuracy (0.95-1.00), implying the embeddings carry structured binding features rather than pure bag-of-words
- If binding information were not linearly accessible, probe accuracy would collapse to random; empirically not observed

### Mechanism 2
- Cross-modal BoW behavior stems from misalignment under cosine similarity, not missing binding features
- Cosine similarity compares global embeddings without explicit alignment of binding-features across modalities, collapsing distinctions like "blue square + orange triangle" vs. "orange square + blue triangle"
- If the misalignment were highly nonlinear, a linear matrix would fail to recover binding; LABCLIP's success suggests this is not the case (but limited to tested datasets)

### Mechanism 3
- A learned linear transformation on text embeddings can recover cross-modal binding without modifying CLIP weights
- LABCLIP trains a D×D matrix A via contrastive loss with hard negative captions (permuted attribute-object associations). A remaps text embeddings so cosine similarity with image embeddings respects binding structure
- If distribution shift or more complex bindings (3+ objects, relations, negation) require nonlinear transformations, LABCLIP gains may degrade

## Foundational Learning

- **Contrastive Language-Image Pretraining (CLIP)**: Understanding the dual-encoder structure, embedding spaces, and training objective is critical for reasoning about interventions like LABCLIP. Quick check: Can you explain why image and text encoders share a D-dimensional space and how InfoNCE loss shapes their alignment?

- **Linear Probing**: The paper's core diagnostic uses linear probes to demonstrate binding information exists. Understanding what probes reveal (and what they don't) is critical for interpreting results. Quick check: If a linear probe on frozen features achieves 95% accuracy, what can you conclude? What can you NOT conclude?

- **Cosine Similarity for Cross-Modal Retrieval**: The identified failure mode is specific to cosine similarity over global embeddings. Understanding its behavior helps assess why a linear remap helps. Quick check: Why might cosine similarity conflate "blue square + red circle" and "red square + blue circle" even if both modalities encode binding?

## Architecture Onboarding

- **Component map**: CLIP Image Encoder (frozen) -> CLIP Text Encoder (frozen) -> LABCLIP Transformation Matrix A (trained) -> Cosine Similarity Computation
- **Critical path**: Precompute CLIP embeddings for all image-text pairs → Initialize A as identity → Compute transformed text embeddings A·t → Compute contrastive loss over positive + hard negative pairs → Update A only
- **Design tradeoffs**: HNB generally performs better but requires generating negatives; text-only transformation is cheaper than bilateral modifications; frozen encoders preserve original capabilities but limit expressivity
- **Failure signatures**: Near-chance cross-modal accuracy indicates unmitigated BoW behavior; significant gap between linear probe accuracy and cross-modal retrieval accuracy indicates alignment problem; no gain on ARO/SugarCrepe suggests mismatched negative generation strategy
- **First 3 experiments**: (1) Linear probe baseline on CLEVR/PUG:SPARE to confirm binding information exists (expect 0.90+ accuracy vs. 0.12 random), (2) Cross-modal BoW diagnosis with vanilla CLIP cosine similarity (expect ~0.50-0.56), (3) LABCLIP training on COCO with HNB mode and evaluation on ARO/SugarCrepe (target improved VG-R/PRC metrics and comparable Recall@1 to NegCLIP)

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several implications arise from the work:

## Limitations
- The linear transformation assumption may not generalize to more complex compositional structures (3+ objects, relations, negation)
- Synthetic negative generation strategy differences across datasets are not fully characterized, potentially affecting generalization
- Alternative mechanisms for cross-modal BoW behavior (attention patterns, embedding nonlinearities) are not systematically ruled out

## Confidence
- **High Confidence**: Linear probes demonstrate binding information exists in each modality (tested across CLEVR, PUG:SPAR, PUG:SPARE with consistent 0.95-1.00 accuracy)
- **Medium Confidence**: Cross-modal BoW behavior stems from cosine similarity misalignment (supported by chance-level accuracy and LABCLIP success, but alternative mechanisms not ruled out)
- **Medium Confidence**: LABCLIP's linear transformation generalizes across datasets (validated on ARO, SugarCrepe, COCO, but synthetic negative generation strategy differences not fully characterized)

## Next Checks
1. Evaluate LABCLIP on a dataset with 3+ objects and explicit relational descriptions to test if linear misalignment assumptions break down for complex compositions
2. Apply attention visualization to CLIP's cross-attention layers when processing binding-critical pairs to determine if BoW behavior is attention-driven
3. Train LABCLIP variants with different negative generation strategies (random word shuffling, attribute-only permutation, object-only permutation) and compare generalization to real-world datasets