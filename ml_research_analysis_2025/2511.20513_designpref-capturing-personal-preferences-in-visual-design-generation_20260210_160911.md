---
ver: rpa2
title: 'DesignPref: Capturing Personal Preferences in Visual Design Generation'
arxiv_id: '2511.20513'
source_url: https://arxiv.org/abs/2511.20513
tags:
- design
- preference
- designers
- each
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DesignPref, a dataset of 12,000 pairwise
  UI design comparisons labeled by 20 professional designers with multi-level preference
  ratings and rationales. The dataset captures substantial disagreement among designers
  (Krippendorff's alpha = 0.25 for binary preferences), revealing that 28.5% of comparisons
  show 96%+ pairwise disagreement.
---

# DesignPref: Capturing Personal Preferences in Visual Design Generation

## Quick Facts
- arXiv ID: 2511.20513
- Source URL: https://arxiv.org/abs/2511.20513
- Reference count: 40
- Primary result: 12,000 pairwise UI design comparisons labeled by 20 professional designers with multi-level preference ratings and rationales

## Executive Summary
This paper introduces DesignPref, a dataset capturing individual design preferences among professional UI designers. The dataset reveals substantial disagreement among designers, with 28.5% of comparisons showing near-complete pairwise disagreement. The authors demonstrate that modeling individual designer preferences outperforms population-level aggregation approaches, both in finetuning UI assessment models and in personalized retrieval-augmented generation pipelines.

## Method Summary
The authors created DesignPref by collecting pairwise UI design comparisons from 20 professional designers, who provided multi-level preference ratings and rationales for each comparison. They analyzed inter-rater reliability using Krippendorff's alpha, finding low agreement (0.25) that indicates genuine individual differences in design taste. The dataset was used to finetune UI assessment models on individual designer labels and to develop personalized RAG pipelines for design preference prediction.

## Key Results
- Krippendorff's alpha of 0.25 for binary preferences indicates substantial designer disagreement
- 28.5% of comparisons show 96%+ pairwise disagreement among designers
- Finetuning on individual designer labels outperforms models trained on 20× more aggregated data
- Personalized RAG pipelines show improved performance over population-level approaches

## Why This Works (Mechanism)
The paper demonstrates that individual design preferences capture meaningful variation in aesthetic judgment that population-level aggregation fails to represent. By treating design taste as a personalized attribute rather than a universal standard, the approach can better predict individual designer preferences and generate more satisfying design recommendations for specific users.

## Foundational Learning
- Inter-rater reliability metrics (Krippendorff's alpha) - why needed: to quantify agreement/disagreement among raters; quick check: values < 0.3 indicate poor reliability
- Pairwise comparison methodology - why needed: standard approach for preference elicitation; quick check: ensure stimulus presentation is consistent across raters
- Multi-level rating scales - why needed: capture nuance beyond binary preferences; quick check: verify scale anchors are meaningful to participants
- UI assessment model finetuning - why needed: adapt pre-trained models to domain-specific tasks; quick check: monitor overfitting with validation sets
- Retrieval-augmented generation - why needed: incorporate external knowledge into generation tasks; quick check: verify retrieved examples are relevant and diverse

## Architecture Onboarding

Component map: Dataset collection -> Preference modeling -> RAG pipeline -> Evaluation

Critical path: Designer comparisons → Individual preference modeling → Personalized generation → Preference prediction

Design tradeoffs: The dataset focuses on professional designers rather than general users, prioritizing expert judgment over broad representativeness. This choice enables studying sophisticated design preferences but limits generalizability to non-professional contexts.

Failure signatures: Low inter-rater reliability could indicate either genuine individual differences or problems with the comparison task design. Poor finetuning performance might suggest insufficient model capacity or suboptimal hyperparameter choices.

First experiments:
1. Replicate the finetuning comparison between individual vs. aggregated training approaches
2. Test the personalized RAG pipeline on held-out designer preferences
3. Conduct ablation studies varying the number of designer examples per individual

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Low inter-rater reliability (Krippendorff's alpha = 0.25) raises questions about dataset consistency
- Focus on professional designers limits external validity to broader populations
- Evaluation methodology for RAG pipeline improvements lacks full specification
- Generalizability to other design domains and model architectures remains uncertain

## Confidence
- Dataset construction and characteristics: Medium
- Preference modeling superiority claims: High (for demonstrated methods)
- RAG pipeline improvements: Medium
- Generalizability across design domains: Low

## Next Checks
1. Conduct cross-validation with additional designer populations (different experience levels, cultural backgrounds) to assess the robustness of individual preference patterns and inter-rater reliability metrics.

2. Perform ablation studies varying the amount of individual vs. aggregated training data to establish the precise conditions under which personalized modeling provides benefits over population-level approaches.

3. Test the finetuned models on held-out real-world design tasks (not just the pairwise comparison format) to validate practical utility beyond the controlled experimental setting.