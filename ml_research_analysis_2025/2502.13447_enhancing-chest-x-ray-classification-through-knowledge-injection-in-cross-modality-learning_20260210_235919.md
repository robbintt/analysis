---
ver: rpa2
title: Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality
  Learning
arxiv_id: '2502.13447'
source_url: https://arxiv.org/abs/2502.13447
tags:
- knowledge
- medical
- captions
- learning
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of knowledge injection on medical
  cross-modality learning, focusing on CXR image classification. We introduce a novel
  Set Theory-based knowledge injection framework to generate captions with varying
  levels of medical knowledge granularity.
---

# Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning

## Quick Facts
- **arXiv ID**: 2502.13447
- **Source URL**: https://arxiv.org/abs/2502.13447
- **Reference count**: 24
- **Primary result**: Knowledge injection with fine-grained medical captions improves zero-shot CXR classification accuracy from 49.9% to 72.5% on CheXpert

## Executive Summary
This study introduces a Set Theory-based knowledge injection framework to enhance cross-modality learning for Chest X-ray classification. By generating captions with varying levels of medical knowledge granularity—coarse, medium, and fine—the framework demonstrates that incorporating detailed medical knowledge, especially excluded phenotypes, substantially improves CLIP-based zero-shot classification. Fine-grained captions outperform human-generated captions and models without explicit medical knowledge, highlighting the importance of domain expertise in medical vision-language tasks.

## Method Summary
The framework defines disease-to-phenotype mappings using expert knowledge, generating captions at three granularity levels via LLM paraphrasing. These captions fine-tune CLIP's dual-encoder on MIMIC-CXR images, then evaluate zero-shot classification on CheXpert using cosine similarity between image and disease prompt embeddings. The approach systematically increases knowledge density from coarse (disease labels only) to fine (typical and excluded phenotypes), leveraging domain-specific LLMs for improved caption quality.

## Key Results
- Fine-grained knowledge injection achieves 72.5% average accuracy vs 49.9% with human captions
- Including excluded phenotypes (fine-grained) yields 12.36% average accuracy gain over medium-grained
- Domain-specific LLMs (BioMistral) outperform general LLMs by 7.77% with fine-grained knowledge
- Knowledge injection outperforms vLLM integration, suggesting structured knowledge > raw visual description

## Why This Works (Mechanism)

### Mechanism 1: Negative Contrast via Excluded Phenotypes
- Claim: Explicitly stating absent phenotypes improves classification by providing discriminative boundaries.
- Mechanism: Fine-grained captions include P_exc (excluded phenotypes), creating negative contrast signals that help the model distinguish between visually similar conditions.
- Core assumption: Medical images of different diseases share overlapping visual features; exclusion information reduces false positives.
- Evidence anchors:
  - [section III-B2]: "The inclusion of excluded phenotypes (fine-grained) results in a substantial 12.36% average accuracy gain compared to the medium-grained level (p < 0.01)"
- Break condition: If diseases share few visual features, or if excluded phenotypes are irrelevant to differential diagnosis, this mechanism would not yield gains.

### Mechanism 2: Knowledge Density Gradient
- Claim: Performance improves monotonically with increasing medical knowledge density in captions.
- Mechanism: Three granularity levels (coarse → medium → fine) encode progressively richer semantic information, enabling better image-text alignment during contrastive pre-training.
- Core assumption: CLIP's contrastive objective can exploit finer semantic distinctions when they are explicitly verbalized.
- Evidence anchors:
  - [abstract]: "injecting fine-grained medical knowledge substantially improves classification accuracy, achieving 72.5% compared to 49.9% when using human-generated captions"
  - [section III-B2]: "incorporating disease phenotypes (medium-grained) consistently yields an average 4.52% accuracy improvement over using only disease labels (coarse-grained) (p < 0.05)"
- Break condition: If caption length or noise dominates signal at higher densities, returns may diminish or reverse.

### Mechanism 3: Domain-Specific LLM Paraphrasing Quality
- Claim: Medical-domain LLMs generate captions that more accurately verbalize fine-grained knowledge than general-purpose LLMs.
- Mechanism: Domain-specific LLMs (BioMistral, Med42) have better medical concept understanding, producing more clinically coherent paraphrases that preserve semantic precision.
- Core assumption: Caption quality, not just content, affects cross-modality learning; clinical coherence matters.
- Evidence anchors:
  - [section III-C1]: "BioMistral with fine-grained knowledge demonstrates a 7.77% accuracy improvement over Mistral"
  - [section III-C1]: "domain expertise is most beneficial when handling complex information like fine-grained knowledge"
- Break condition: If domain-specific LLMs introduce systematic biases or hallucinations, gains may not generalize.

## Foundational Learning

- Concept: **Contrastive Learning (CLIP-style)**
  - Why needed here: The entire method fine-tunes CLIP using contrastive loss; understanding image-text alignment is essential.
  - Quick check question: Can you explain why CLIP can perform zero-shot classification without task-specific training data?

- Concept: **Zero-Shot Classification**
  - Why needed here: Evaluation uses zero-shot classification on CheXpert; understanding how text prompts encode class labels is critical.
  - Quick check question: How does zero-shot classification differ from standard supervised classification in terms of label requirements?

- Concept: **Medical Phenotypes and Differential Diagnosis**
  - Why needed here: The Set Theory framework relies on mapping diseases to typical/excluded phenotypes; clinical knowledge structures the injection.
  - Quick check question: Why might two different lung diseases share overlapping phenotypes, and how does this affect model confusion?

## Architecture Onboarding

- Component map:
  - Knowledge Definition Layer -> Caption Generation Layer -> Cross-Modality Encoder -> Zero-Shot Classifier

- Critical path: Knowledge definition → Caption generation (LLM) → CLIP fine-tuning (MIMIC-CXR) → Zero-shot evaluation (CheXpert)

- Design tradeoffs:
  - Caption granularity vs. noise: Finer captions add signal but may introduce LLM hallucinations
  - Domain-specific LLM vs. general LLM: Domain models improve accuracy but require additional resources
  - vLLM integration: Direct visual captioning models (Gemini, ChexAgent) underperformed knowledge injection, suggesting structured knowledge > raw visual description

- Failure signatures:
  - Coarse-grained captions achieve similar accuracy to human captions (~49-52%), indicating insufficient discriminative information
  - vLLM integration reduces accuracy, suggesting visual captions may conflict with injected knowledge
  - Performance drops on rare conditions (e.g., Pneumothorax at 3.4% with some models), indicating class imbalance sensitivity

- First 3 experiments:
  1. **Baseline comparison**: Evaluate vanilla CLIP, PubmedCLIP, and human-caption fine-tuned CLIP on CheXpert zero-shot classification to establish knowledge sufficiency gap.
  2. **Granularity ablation**: Fine-tune CLIP on coarse/medium/fine captions using multiple 7B LLMs (llama2, vicuna, zephyr) to isolate knowledge density effects.
  3. **LLM comparison**: Compare domain-specific LLMs (BioMistral, Med42) against base models (Mistral, Llama2) with vLLM integration to assess caption generation quality factors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Set Theory-based knowledge injection framework be effectively generalized to non-standardized medical imaging modalities, such as CT or MRI, where phenotypic descriptions are less rigid than in CXR?
- **Basis in paper:** [explicit] The Conclusion states, "Future research can explore the generalizability of this framework to other medical imaging tasks..."
- **Why unresolved:** The study relied on Chest X-rays (CXR) specifically because of their standardized acquisition protocols and well-defined textual phenotype descriptions, conditions which may not hold for more complex 3D modalities.
- **What evidence would resolve it:** Successful application of the framework to a CT or MRI dataset, demonstrating that injected fine-grained knowledge yields similar performance improvements over baselines in those modalities.

### Open Question 2
- **Question:** Can alternative knowledge representation methods, such as structured embeddings or knowledge graphs, provide more efficient or robust signals for cross-modality learning than natural language captions?
- **Basis in paper:** [explicit] The Conclusion suggests, "investigate alternative knowledge representation methods" as a direction for future work.
- **Why unresolved:** The current framework relies on LLMs to paraphrase set-theoretic knowledge into natural language text; it remains unknown if the model could learn more effectively from a non-textual, purely structural representation of the same data.
- **What evidence would resolve it:** A comparative study where CLIP is fine-tuned using structured knowledge embeddings versus the proposed text captions, evaluating the trade-offs between classification accuracy and training efficiency.

### Open Question 3
- **Question:** How can the visual grounding capabilities of Vision-LLMs (vLLMs) be integrated with explicit knowledge injection without introducing the noise or informational inconsistencies that currently degrade performance?
- **Basis in paper:** [inferred] Section III.C.2 notes that integrating vLLM-generated captions "generally reduces accuracy compared to using BioMistral alone," suggesting the visual features currently introduce noise rather than clarity.
- **Why unresolved:** The paper demonstrates that simply concatenating vLLM outputs with expert knowledge is detrimental, but implies that the visual information *should* be useful if the "biases" or "inconsistencies" could be isolated and removed.
- **What evidence would resolve it:** The development of a filtering mechanism or attention module that aligns vLLM visual descriptions with the injected set-theory knowledge, resulting in performance that exceeds the text-only baseline.

## Limitations
- Knowledge base construction relies on expert-defined phenotype mappings that are not publicly available, making direct reproduction challenging
- Evaluation focuses on a single dataset (CheXpert) and single downstream task (zero-shot classification), limiting generalizability
- Performance gap between human captions and generated captions may partly reflect differences in caption length and detail rather than pure knowledge quality

## Confidence
- **High confidence**: The monotonic improvement across granularity levels and the consistent advantage of domain-specific LLMs are well-supported by statistical significance tests (p < 0.01 and p < 0.05)
- **Medium confidence**: The mechanism that negative contrast via excluded phenotypes drives performance gains is plausible but not directly validated through ablation studies isolating P_exc contributions
- **Low confidence**: The claim that knowledge injection outperforms visual description models (vLLM integration) is based on negative results without exploring hybrid approaches that might combine both signals

## Next Checks
1. **Ablation study on phenotype components**: Evaluate models using only P_typ, only P_exc, and their combination to isolate which knowledge type drives performance gains
2. **Cross-dataset generalization**: Test the knowledge-injected models on MIMIC-CXR or other CXR datasets to assess whether gains transfer beyond CheXpert
3. **Caption length and noise control**: Generate medium-grained captions with matched length to fine-grained captions while preserving medical content to separate knowledge effects from text complexity