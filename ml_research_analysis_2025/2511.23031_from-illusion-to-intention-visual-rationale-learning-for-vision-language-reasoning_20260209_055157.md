---
ver: rpa2
title: 'From Illusion to Intention: Visual Rationale Learning for Vision-Language
  Reasoning'
arxiv_id: '2511.23031'
source_url: https://arxiv.org/abs/2511.23031
tags:
- visual
- reasoning
- answer
- rationale
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of models appearing to engage
  in visual reasoning while actually relying on superficial or spurious visual actions
  that do not genuinely ground their reasoning. The core method, Visual Rationale
  Learning (ViRL), reframes visual actions as core reasoning primitives rather than
  optional tools, and introduces process supervision to directly optimize the fidelity
  and utility of visual reasoning chains.
---

# From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning

## Quick Facts
- arXiv ID: 2511.23031
- Source URL: https://arxiv.org/abs/2511.23031
- Authors: Changpeng Wang; Haozhe Wang; Xi Chen; Junhan Liu; Taofeng Xue; Chong Peng; Donglian Qi; Fangzhen Lin; Yunfeng Yan
- Reference count: 40
- One-line primary result: Achieves 90.1% accuracy on V*, 75.3% on HRBench, and 88.7% on POPE through process supervision that ensures visual reasoning traces are both accurate and faithful.

## Executive Summary
This paper addresses the problem of vision-language models appearing to engage in visual reasoning while actually relying on superficial or spurious visual actions. The core method, Visual Rationale Learning (ViRL), reframes visual actions as core reasoning primitives and introduces process supervision to directly optimize the fidelity and utility of visual reasoning chains. By combining rationale fidelity rewards with fine-grained credit assignment, ViRL ensures each visual action meaningfully contributes to the reasoning trajectory. Trained with end-to-end RL, ViRL achieves state-of-the-art performance across multiple benchmarks and produces models with higher answer accuracy and stronger rationale fidelity compared to outcome-only baselines.

## Method Summary
ViRL introduces a process-level supervision framework that treats visual actions (zoom-ins) as core reasoning primitives rather than optional tools. The method combines three key innovations: process supervision with ground-truth rationales, objective alignment via step-level reward shaping, and fine-grained credit assignment to distinguish correct, redundant, and erroneous actions. The framework fine-tunes Qwen2.5-VL-7B with PPO-style RL, using a rationale fidelity reward that computes IoU between predicted and ground-truth bounding boxes. A bi-level credit assignment mechanism modulates trajectory-level advantages based on the quality of individual visual actions, amplifying credit for high-fidelity rationales and attenuating it for poor ones. The training data is curated through a three-stage pipeline that ensures samples genuinely require visual grounding.

## Key Results
- Achieves 90.1% accuracy on V*, 75.3% on HRBench, and 88.7% on POPE
- Demonstrates 90.4% answer accuracy vs 87.6% for outcome-only baselines
- Shows 87.3% rationale fidelity vs 78.2% for outcome-only baselines
- Prevents "visual thinking collapse" where models learn to answer without grounding

## Why This Works (Mechanism)

### Mechanism 1
Step-level fidelity rewards prevent "visual thinking collapse" by creating dense per-action feedback rather than sparse outcome-only signals. The reward function combines a signed correctness signal with a progressive refinement bonus that increases as IoU with ground-truth improves. This creates incentives for each zoom-in to align with ground-truth rationale regions.

### Mechanism 2
Bi-level credit assignment distinguishes useful visual actions from erroneous ones within the same trajectory. Coarse trajectory-level advantage is modulated per-action via a modulator that amplifies high-fidelity visual rationales and attenuates low-fidelity ones, ensuring the model learns to associate specific action quality with trajectory success.

### Mechanism 3
Reasoning-centric data filtering ensures training samples genuinely require visual grounding rather than being solvable via text shortcuts. A three-stage pipeline removes samples with overly large rationale regions and those solvable without zoom-ins, creating a distribution where visual actions are necessary for correct reasoning.

## Foundational Learning

**Chain-of-Thought (CoT) Reasoning**: ViRL explicitly frames visual rationalization as "the visual analogue of textual Chain-of-Thought." Understanding sequential textual reasoning decomposition is prerequisite for understanding the extension to visual actions. Quick check: Can you explain why outcome-only supervision fails to produce faithful CoT traces even when final answers are correct?

**Proximal Policy Optimization (PPO) and Advantage Estimation**: The framework uses PPO-style policy optimization with modified advantage computation. Understanding baseline advantage calculation is necessary to understand how credit assignment modulates it. Quick check: How does the advantage function A(s,a) differ from the raw reward R(s,a), and why does this distinction matter for credit assignment?

**Vision-Language Model (VLM) Architecture**: The paper builds on Qwen2.5-VL and addresses the "information bottleneck" in visual encoders. Understanding how VLMs process images is necessary to understand why zoom-in actions are needed. Quick check: What is the resolution-information tradeoff in standard VLM visual encoders, and how does "thinking with images" attempt to address it?

## Architecture Onboarding

**Component map**: Data pipeline → Ground-truth rationales → Rollout generation → Rationale fidelity scoring → Bi-level advantage computation → Policy update

**Critical path**: The reward signal depends entirely on ground-truth rationale quality, flowing from curated data through IoU computation to policy gradient updates.

**Design tradeoffs**: 
- Threshold h₀: Low threshold (0.2) over-rewards weak alignments; high threshold (0.5) over-penalizes early exploration
- Redundancy penalty ρ: Controls trajectory length vs. thoroughness tradeoff
- Data filtering aggressiveness: 200k → 8k samples retained; stronger filtering improves grounding but may reduce task diversity
- h_good/h_bad values: Calibration affects how strongly the model amplifies/blames specific actions

**Failure signatures**:
- Visual thinking collapse: High answer accuracy with near-zero rationale count
- Invocation explosion: Rapid spike in zoom-in calls with accuracy drop
- Superficial grounding: High rationale count but low per-invocation accuracy

**First 3 experiments**:
1. Reproduce ablation (Table 2): Train with and without rationale fidelity reward on a small subset. Verify that removing fidelity rewards causes visual thinking collapse while maintaining answer accuracy.
2. Threshold sensitivity analysis: Vary h₀ across {0.2, 0.3, 0.4, 0.5} and plot rationale accuracy vs. answer accuracy. The paper claims mid-range thresholds balance exploration vs. precision.
3. Data filtering validation: Train on unfiltered vs. filtered data. Measure both Acc_ans and Acc_rat to confirm that reasoning-centric filtering improves grounding quality rather than just reducing dataset size.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the performance gains from process-level supervision be maintained when scaling the visual reasoning chain length significantly beyond the 6-round maximum used in experiments? The paper states that "current supervision mainly targets short reasoning chains; extending it to deeper, longer-horizon reasoning remains an open challenge." The Fine-Grained Credit Assignment averages rewards over trajectory length, which may dilute the learning signal as the chain grows.

**Open Question 2**: Does the Reasoning-Centric Filtering pipeline introduce a distributional bias that limits generalization to real-world tasks where global context is sufficient? The data pipeline explicitly filters out samples solvable without localized visual evidence, which may artificially constrain the training distribution to scenarios requiring fine-grained inspection.

**Open Question 3**: Is the Rationale Fidelity Reward (IoU-based) adaptable to other visual actions like drawing or segmentation, and does it maintain fidelity in those modalities? The paper frames visual actions as "core reasoning primitives" but the fidelity reward is explicitly defined using spatial IoU, with no evidence provided for non-spatial visual tools.

**Open Question 4**: To what extent does the "Visual Thinking Collapse" phenomenon depend on the base model's inherent capabilities versus the design of the prompt and reward signal? The paper analyzes "visual thinking collapse" but uses a single, strong base model, making it unclear if a weaker base model would fail to overcome the collapse despite the proposed interventions.

## Limitations
- The method relies on high-quality ground-truth rationales, which may not be available in many real-world applications
- The three-stage filtering process is computationally expensive and may limit scalability to larger datasets
- The approach focuses on short reasoning chains and may not generalize to deeper, multi-hop reasoning tasks
- Critical hyperparameters for reward functions and credit assignment are underspecified, making faithful reproduction difficult

## Confidence
- **High Confidence**: The core observation that outcome-only supervision leads to visual thinking collapse is well-supported by ablation results showing degraded rationale fidelity without process supervision.
- **Medium Confidence**: The claim that bi-level credit assignment significantly improves grounding quality is supported by Table 2, though the lack of detailed implementation parameters introduces uncertainty about the robustness of this improvement.
- **Low Confidence**: The generalization of the 200k→8k filtering pipeline's effectiveness across different vision-language tasks is uncertain, as the paper does not validate the filtering criteria beyond the reported benchmarks.

## Next Checks
1. Reproduce core ablation with controlled hyperparameters: Train the model with and without rationale fidelity rewards on a small, well-documented dataset using the paper's equations but with clearly specified hyperparameters. Verify that the outcome-only baseline shows high answer accuracy with near-zero rationale count, confirming the "visual thinking collapse" phenomenon.

2. Validate credit assignment sensitivity: Systematically vary h_good and h_bad values across a range and measure the impact on rationale fidelity and answer accuracy. This will test whether the reported improvement from bi-level credit assignment is robust to hyperparameter choices.

3. Test data filtering transferability: Apply the three-stage filtering pipeline to a different vision-language dataset and measure whether the filtered data consistently requires visual grounding. Compare the grounding quality of models trained on filtered vs. unfiltered data to assess the pipeline's generalizability.