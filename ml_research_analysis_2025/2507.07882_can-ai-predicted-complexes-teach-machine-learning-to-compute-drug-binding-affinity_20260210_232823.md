---
ver: rpa2
title: Can AI-predicted complexes teach machine learning to compute drug binding affinity?
arxiv_id: '2507.07882'
source_url: https://arxiv.org/abs/2507.07882
tags:
- training
- binding
- structures
- dence
- bindingnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether co-folding models can generate synthetic
  data to train machine learning scoring functions for drug binding affinity prediction.
  The researchers trained AEV-PLIG models using experimental structures, BindingNet
  datasets, and Boltz-1x co-folding predictions.
---

# Can AI-predicted complexes teach machine learning to compute drug binding affinity?

## Quick Facts
- **arXiv ID**: 2507.07882
- **Source URL**: https://arxiv.org/abs/2507.07882
- **Reference count**: 24
- **Key outcome**: Synthetic data augmentation only improves ML scoring function performance when high-quality structures are included, with proper filtering enabling synthetic data to substitute for experimental structures

## Executive Summary
This study investigates whether co-folding models can generate synthetic data to train machine learning scoring functions for drug binding affinity prediction. The researchers trained AEV-PLIG models using experimental structures, BindingNet datasets, and Boltz-1x co-folding predictions, finding that synthetic data augmentation only improved model performance when high-quality structures were included. By applying simple heuristics like confidence score thresholds and train-test similarity filters, they successfully identified reliable co-folding predictions without reference structures. Models trained on filtered co-folding predictions performed statistically equivalent to those trained on experimental structures, demonstrating that properly curated synthetic data can effectively substitute for scarce experimental data in machine learning-based scoring function training.

## Method Summary
The study employed a systematic approach to evaluate synthetic data for ML scoring function training. Researchers trained AEV-PLIG models using three data sources: experimental structures, BindingNet datasets, and Boltz-1x co-folding predictions. They implemented quality assessment methods including confidence score thresholds and train-test similarity filters to identify reliable synthetic structures. The evaluation compared model performance across different training set compositions, measuring how synthetic data augmentation affected predictive accuracy when combined with experimental structures versus used alone.

## Key Results
- Synthetic data augmentation improved model performance only when high-quality structures were included
- Low-quality synthetic examples provided minimal benefit despite larger training set sizes
- Filtered co-folding predictions (using confidence scores and similarity thresholds) performed statistically equivalent to experimental structures

## Why This Works (Mechanism)
The success of synthetic data in ML training depends critically on data quality rather than quantity. High-quality co-folding predictions capture essential structural features and binding interactions that ML models need to learn accurate scoring functions. The quality filtering heuristics (confidence scores and similarity thresholds) effectively identify synthetic structures that preserve the key characteristics of experimentally-determined complexes while eliminating poor predictions that would introduce noise and degrade model performance.

## Foundational Learning
- **Co-folding prediction quality assessment**: Essential for distinguishing reliable synthetic structures from poor predictions that would harm model training. Quick check: Compare confidence score distributions between high and low performing synthetic structures.
- **Train-test similarity filtering**: Prevents data leakage and ensures models generalize to new systems rather than memorizing training examples. Quick check: Measure pairwise similarity between training and test sets to confirm adequate separation.
- **Quality-over-quantity principle in synthetic data**: Demonstrates that curated synthetic data can match or exceed the value of larger, unfiltered datasets. Quick check: Compare model performance on filtered versus unfiltered synthetic training sets.

## Architecture Onboarding
**Component Map**: Experimental structures -> AEV-PLIG model training -> Performance evaluation; Synthetic structures (Boltz-1x) -> Quality filtering (confidence scores, similarity thresholds) -> AEV-PLIG model training -> Performance comparison

**Critical Path**: Data quality assessment → Filtered synthetic data → Model training → Performance validation → Equivalence testing with experimental data

**Design Tradeoffs**: Larger training sets with mixed quality versus smaller, high-quality curated datasets. The study shows that quality filtering provides better performance than raw data volume, but requires computational overhead for assessment.

**Failure Signatures**: Poor model performance when synthetic data contains excessive low-quality examples, data leakage from inadequate train-test separation, and overfitting when similarity thresholds are too permissive.

**First Experiments**: 1) Compare model performance using confidence score threshold sweeps to identify optimal filtering levels, 2) Test different similarity threshold values to balance data quantity against quality, 3) Validate quality filtering heuristics on completely independent test sets.

## Open Questions the Paper Calls Out
The study acknowledges uncertainty about whether similar benefits would be observed with other co-folding or generative models beyond Boltz-1x. Additionally, the optimal balance between synthetic and experimental data across different protein families remains unexplored.

## Limitations
- Results may not generalize to other co-folding models or different protein-ligand systems
- Limited to single ML architecture (AEV-PLIG), uncertain if results extend to other model types
- Quality assessment methods require validation on independent datasets to confirm robustness

## Confidence
- **Primary conclusion validity**: Medium - promising results but specific to experimental conditions
- **Generalizability to other models**: Low - only tested with Boltz-1x and AEV-PLIG
- **Long-term reliability**: Medium - synthetic data performance equivalent but requires ongoing validation

## Next Checks
1. Test the same methodology with alternative co-folding models and generative approaches to verify benefits are not specific to Boltz-1x
2. Apply quality filtering heuristics to entirely independent test sets with no overlap to validate generalizability
3. Conduct time-series experiments where models are periodically retrained with newly generated synthetic data to assess long-term performance stability