---
ver: rpa2
title: 'Binary Token-Level Classification with DeBERTa for All-Type MWE Identification:
  A Lightweight Approach with Linguistic Enhancement'
arxiv_id: '2601.19360'
source_url: https://arxiv.org/abs/2601.19360
tags:
- mwes
- coam
- discontinuous
- streusle
- expressions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates multiword expression (MWE) identification
  as binary token-level classification, predicting START/END/INSIDE labels for each
  token rather than using multi-class sequence labeling. The approach integrates linguistic
  features (NP chunking and dependency paths) and data augmentation to address class
  imbalance.
---

# Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement

## Quick Facts
- arXiv ID: 2601.19360
- Source URL: https://arxiv.org/abs/2601.19360
- Reference count: 13
- Primary result: 69.8% F1 on CoAM using DeBERTa-v3-large, outperforming Qwen-72B (57.8% F1) by 12 points while using 165× fewer parameters

## Executive Summary
This paper reformulates multiword expression (MWE) identification as binary token-level classification, predicting START/END/INSIDE labels for each token rather than using multi-class sequence labeling. The approach integrates linguistic features (NP chunking and dependency paths) and data augmentation to address class imbalance. Using DeBERTa-v3-large, the model achieves 69.8% F1 on CoAM, outperforming the previous state-of-the-art (Qwen-72B, 57.8% F1) by 12 points while using 165× fewer parameters. The method also generalizes to STREUSLE with 78.9% F1, demonstrating that carefully designed smaller models with linguistic enhancements can substantially outperform large language models on structured NLP tasks.

## Method Summary
The method reformulates MWE detection as three independent binary classifications per token (START, END, INSIDE) rather than span-based or sequence labeling approaches. It uses DeBERTa-v3-large as the encoder with 16-dimensional NP chunk embeddings and 32-dimensional dependency distance embeddings. The model employs dataset-specific augmentation: oversampling for the smaller CoAM dataset and lexical substitution for the larger STREUSLE dataset. At inference, it reconstructs spans from token pairs satisfying probability thresholds and dependency distance constraints (≤4), rejecting spans longer than 13 tokens.

## Key Results
- 69.8% F1 on CoAM, outperforming Qwen-72B (57.8% F1) by 12 points
- 78.9% F1 on STREUSLE, demonstrating generalization across datasets
- Binary token-level formulation yields +5.2 F1 improvement over span-based prediction (53.7% → 58.9%)
- Disentangled attention mechanisms achieve 63.7% F1, +4.8 points over standard BERT (58.9%)

## Why This Works (Mechanism)

### Mechanism 1
Three independent binary predictions per token provide richer gradient signals than single multi-class labels. The reformulation decouples boundary detection into separate learnable tasks, each receiving dedicated gradient feedback rather than competing within a shared softmax.

### Mechanism 2
Disentangled attention in DeBERTa captures discontinuous patterns by separating content and position encoding. This allows the model to learn relationships between separated tokens based on their content similarity even when positionally distant—critical for discontinuous MWEs.

### Mechanism 3
Dataset size determines optimal augmentation strategy: oversampling for small curated datasets, lexical substitution for larger ones. Small datasets benefit from exact pattern preservation while larger datasets can generalize from lexical variation.

## Foundational Learning

- **BIO/BILOU sequence labeling schemes:** Understanding these traditional tagging schemes is necessary to grasp why decoupling into independent binary decisions changes the learning dynamics. *Quick check:* In BIO tagging for "stock market crash" as a single MWE, what labels would each token receive? How would the binary approach represent this differently?

- **Class imbalance in token classification:** MWE tokens are rare (11.3% discontinuous, specific types even sparser). The paper's oversampling and threshold tuning directly address severe imbalance. *Quick check:* If 95% of tokens are "not MWE" and the model predicts "not MWE" for everything, what accuracy does it achieve? What F1?

- **Dependency parse paths and graph traversal:** The linguistic feature integration computes shortest dependency path lengths between token pairs using NetworkX graph traversal. *Quick check:* In "looked the information up," what is the dependency relationship between "looked" and "up"? Why might a short path length indicate they form a discontinuous MWE?

## Architecture Onboarding

```
Input tokens → DeBERTa-v3-large encoder → [contextualized representations]
                        ↓
              ┌───────────┼───────────┐
              ↓           ↓           ↓
         NP chunk     Dependency   Token states
         embeddings   embeddings   (concatenated)
              ↓           ↓           ↓
              └───────────┼───────────┘
                        ↓
            3 parallel binary classifiers
            (START / END / INSIDE heads)
                        ↓
            Threshold filtering (τstart, τend, τinside)
                        ↓
            Span reconstruction algorithm
                        ↓
            Dependency distance constraint (≤4)
                        ↓
            Final MWE spans output
```

Critical path: The span reconstruction algorithm forms candidate spans from token pairs (s, e) where `pstart_s ≥ τstart` and `pend_e ≥ τend`, then validates intermediate tokens via INSIDE probabilities and dependency constraints. Incorrect thresholds here cascade into massive over/under-prediction.

Design tradeoffs:
- Binary formulation: Enables discontinuous detection but loses type prediction
- Linguistic features: +2.2 F1 on base model but caused -2.8 F1 drop on large model without augmentation (overfitting on small data)
- Dependency constraint (≤4): Improves precision but may reject valid long-range discontinuous MWEs

Failure signatures:
- Model predicts START but no matching END → incomplete spans
- Low τ values → massive over-prediction (baseline BBS: 11.1% precision on STREUSLE)
- High train-test F1 gap (>25%) on small datasets → overfitting indicator
- Discontinuous F1 near 0% with non-zero continuous F1 → reconstruction algorithm not activating for non-adjacent pairs

First 3 experiments:
1. Reproduce BBS vs BBT comparison: Train BERT-base with span-based loss, then identical model with binary token-level heads, confirm ~5 F1 improvement on a held validation split
2. Threshold sensitivity analysis: Grid search τstart, τend, τinside in [0.2, 0.4, 0.6] on development set, measure precision-recall tradeoff curves
3. Ablate linguistic features with augmentation: Train DLT with (+l), (+o), and (+lo) to reproduce the interaction effect where features hurt without augmentation on small data but help with it

## Open Questions the Paper Calls Out

Can a two-stage pipeline combining binary boundary detection with explicit type classification outperform unified approaches for typed MWE identification? The authors state type classification could be added as a second-stage task but is beyond the scope of this work.

To what extent do the NP chunking and dependency path features transfer to morphologically rich languages with different syntactic structures? The authors note these syntactic features may transfer poorly to languages with different syntactic structures or morphological complexity.

Would replacing hard dependency distance constraints (≤4) with learned soft constraints improve recall on long-range discontinuous MWEs? The authors report 29.7% discontinuous F1 and note the constraints may be overly restrictive for certain discontinuous patterns.

Does dataset size systematically determine whether oversampling or lexical substitution is the optimal augmentation strategy for MWE detection? The authors observe different strategies work best on different sized datasets but only test two datasets.

## Limitations

The binary token-level reformulation lacks complete specification of feature integration points and initialization strategies, making faithful reproduction uncertain. The lexical substitution augmentation procedure lacks implementation details including embedding source and candidate selection criteria. The CoAM dataset appears to be a derivative work that retypes STREUSLE annotations, raising questions about whether the 12-point improvement reflects genuine architectural advantage or annotation methodology differences.

## Confidence

**High Confidence**: The three core mechanisms (binary token-level reformulation, disentangled attention benefits, and dataset-dependent augmentation strategies) are directly supported by ablation studies in the paper's own experiments.

**Medium Confidence**: While the reported improvements are substantial, they depend on several implementation choices that are not fully specified, and the 12-point improvement over Qwen-72B requires careful consideration of dataset preprocessing differences.

**Low Confidence**: The paper demonstrates strong performance on discontinuous structures in CoAM, but the mechanism (disentangled attention) lacks direct corpus evidence for this specific application, and the 29.7% discontinuous F1 improvement could reflect architectural advantages specific to the CoAM data distribution.

## Next Checks

1. Reproduce the BBS→BBT improvement: Train BERT-base with span-based loss and identical architecture with binary token-level heads on a held-out validation split, confirming the ~5 F1 improvement before proceeding with full pipeline.

2. Characterize the augmentation-overfitting interaction: Systematically ablate linguistic features with and without augmentation across different dataset sizes to reproduce the reported interaction effect (features hurt without augmentation on small data but help with it).

3. Test dependency constraint sensitivity: Experiment with relaxing the distance threshold from 4 to 5 and measure impact on discontinuous MWE recall, particularly for known long-range cases (>10 tokens).