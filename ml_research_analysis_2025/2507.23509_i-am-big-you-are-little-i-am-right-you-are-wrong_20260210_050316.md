---
ver: rpa2
title: I Am Big, You Are Little; I Am Right, You Are Wrong
arxiv_id: '2507.23509'
source_url: https://arxiv.org/abs/2507.23509
tags:
- image
- different
- size
- classification
- mpss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the decision-making processes of different
  image classification models by analyzing their minimal sufficient pixel sets (MPSs),
  which represent the smallest pixel sets required for classification. Using the ReX
  tool, the authors compare 15 models across five architectures (ConvNext, EV A, Inception,
  ResNet, and ViT) on ImageNet-1k.
---

# I Am Big, You Are Little; I Am Right, You Are Wrong

## Quick Facts
- arXiv ID: 2507.23509
- Source URL: https://arxiv.org/abs/2507.23509
- Reference count: 40
- Key outcome: Study finds that larger, modernized models produce smaller Minimal Sufficient Pixel Sets (MPSs) and that incorrect classifications are associated with larger MPSs than correct ones (2.6% increase on average).

## Executive Summary
This study investigates how different image classification models make decisions by analyzing their Minimal Sufficient Pixel Sets (MPSs) - the smallest pixel sets required for classification. Using the ReX tool, the authors compare 15 models across five architectures (ConvNext, EVA, Inception, ResNet, and ViT) on ImageNet-1k. The analysis reveals statistically significant differences in MPS size and location across architectures, with larger models (particularly ConvNext and EVA) producing smaller, more spatially distinct MPSs. The study finds that incorrect classifications are associated with larger MPSs than correct classifications, suggesting these models may be more prone to overfitting by relying on very few pixels.

## Method Summary
The study employs the ReX tool to analyze 15 different image classification models across five architectures. ReX generates image mutants by masking superpixels and iteratively refines pixel sets while maintaining classification. The method converts models to ONNX format for standardized inference, creates satisfying and failing mutant sets based on classification outcomes, and extracts the minimal sufficient pixel sets. The analysis compares MPS characteristics (size, location, Dice coefficient) across models, examines differences between correct and incorrect classifications, and investigates architectural variations in feature reliance.

## Key Results
- Larger models (ConvNext, EVA) produce significantly smaller MPSs (5.4% area for EVA Giant vs 23-25% for Inception) with distinct spatial patterns
- Incorrect classifications are associated with 2.6% larger MPSs than correct classifications on average
- Different architectures show distinct spatial preferences for MPS locations, with Inception V4 focusing on different regions than other models
- The study provides evidence that high-capacity models may rely on sparse, discriminative features rather than full object structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger, modernized models (specifically ConvNext and EVA) tend to rely on significantly smaller subsets of pixels (MPS) to classify images compared to older architectures like Inception.
- **Mechanism:** High-capacity models may learn highly sparse, discriminative features that trigger confident classifications with minimal evidence. The paper suggests this "myopic" behavior indicates overfitting, where the model relies on specific textures or shortcuts (e.g., a small patch of fur) rather than the full object structure.
- **Core assumption:** The size of the Minimal Sufficient Pixel Set (MPS) acts as a proxy for the model's "concentration" or dependency on sparse features.
- **Evidence anchors:**
  - [abstract]: "larger models (particularly ConvNext and EVA) producing smaller, more spatially distinct MPSs."
  - [section 4.4]: "These very small MPSs... may indicate overfitting."
  - [corpus]: Corpus signals regarding "Right Looks, Wrong Reasons" support the general concept of compositional fidelity issues.

### Mechanism 2
- **Claim:** Model misclassifications (infidelity to ground truth) are statistically associated with larger Minimal Sufficient Pixel Sets.
- **Mechanism:** When a model misclassifies an image, it appears to require a slightly larger "field of view" or more context to maintain that incorrect prediction compared to correct ones. This contradicts the intuition that errors might come from single-pixel noise; rather, they seem to involve slightly more complex (but wrong) feature assemblies.
- **Core assumption:** Ground truth labels in ImageNet-1k are correct.
- **Evidence anchors:**
  - [abstract]: "incorrect classifications are associated with larger MPSs than correct classifications (2.6% increase on average)."
  - [section 4.3 (RQ3)]: "A mixed linear model... demonstrated that having the incorrect classification increased the explanation size by 2.6%."
  - [corpus]: "Murphy's Laws of AI Alignment" generally supports the difficulty of alignment.

### Mechanism 3
- **Claim:** Different architectures inherently "look" at different spatial locations and require different amounts of information density to reach a classification decision.
- **Mechanism:** Architectural inductive biases (e.g., convolutional windows vs. global attention) distribute "causal responsibility" differently across an image. For instance, Inception models (wider) require significantly more pixels (higher MPS size) than Transformer-based or modernized ConvNet models.
- **Core assumption:** The ReX responsibility landscape accurately captures the primary causal region utilized by the model.
- **Evidence anchors:**
  - [section 4.3 (RQ2)]: "Inception V4 consistently finds pixel sets in different locations... The closest to these of Inception V4 are produced by ResNet152."
  - [table 1]: Shows Inception models average ~23-25% MPS area, while EVA Giant is ~5.4%.
  - [corpus]: "Out-of-the-box: Black-box Causal Attacks" relates to identifying causal vulnerabilities.

## Foundational Learning

- **Concept: Actual Causality (Halpern-Pearl)**
  - **Why needed here:** The paper uses a specific definition of "explanation" derived from actual causality (Definition 1), where a pixel set is an explanation if it is *minimal* and *sufficient* to cause the classification, not just correlated with it.
  - **Quick check question:** Can you explain why a pixel set being "sufficient" for a classification is different from it merely having "high attention weight"?

- **Concept: Minimal Sufficient Pixel Sets (MPS)**
  - **Why needed here:** This is the core metric used to compare models. It defines the lower bound of information a model needs, acting as a measure of model "myopia" or robustness.
  - **Quick check question:** If you mask all pixels *except* the MPS, what must the model output be for the set to be considered "sufficient"?

- **Concept: Out-of-Distribution (OOD) Sensitivity**
  - **Why needed here:** The ReX tool generates "mutants" (images with masked pixels) which are effectively OOD data. Understanding how models handle these unnatural inputs is key to interpreting why some models need smaller MPSs.
  - **Quick check question:** Why might a model that tolerates "unnatural" masked images (OOD mutants) be considered more prone to overfitting according to the paper?

## Architecture Onboarding

- **Component map:** Input Image -> ReX Perturbation Engine -> Model (Black Box) -> Responsibility Calculator -> Output MPS
- **Critical path:**
  1. Model Conversion: Convert PyTorch models to ONNX (opset 20) for standardized inference
  2. Mutant Generation: Create image mutants by masking random superpixel combinations
  3. Inference Loop: Query the model with mutants; sort into "satisfying" (same class as original) and "failing"
  4. Refinement: Iteratively subdivide satisfying superpixels to shrink the pixel set while maintaining classification
  5. Extraction: Select the top-ranked pixels that form the minimal sufficient set
- **Design tradeoffs:**
  - Patch-based vs. Pixel-based: ReX is patch-free, allowing for tighter MPS boundaries than tools like SAG, but requires more iterations to resolve exact pixel boundaries
  - Baseline Value: ReX uses 0 (black) masking. This creates OOD artifacts; blurring (used by others) might preserve local statistics better but changes the causal query
- **Failure signatures:**
  - High Hausdorff Distance: Indicates a model focusing on a completely different region (e.g., background vs. foreground) compared to other models
  - Tiny MPS / High Confidence: A "myopic" model (e.g., EVA) that classifies correctly using <6% of pixels, potentially relying on shortcuts
- **First 3 experiments:**
  1. Calibration Run: Run ReX on a ResNet152 vs. EVA-Giant on a single image. Visualize the MPS to confirm EVA produces a physically smaller pixel set
  2. Validation of Sufficiency: Manually mask an image to show *only* the MPS pixels. Feed it to the model to verify it outputs the original class
  3. Dice Coefficient Comparison: Calculate the overlap (Sørensen–Dice) between the MPS of a ConvNext and a ViT model on 10 images to quantify spatial distinctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model confidence correlate with the size and composition of Minimal Sufficient Pixel Sets (MPSs) across different architectures?
- Basis in paper: [explicit] The conclusion states, "We will extend this investigation with the effect of model confidence on the size of MPSs," noting that current results show models have different degrees of confidence even with small MPSs.
- Why unresolved: The current study focuses primarily on the binary distinction between correct and incorrect classifications rather than the continuous variable of confidence scores.
- What evidence would resolve it: A regression analysis or correlation study comparing model output confidence scores against the relative pixel area of the MPS for each image.

### Open Question 2
- Question: To what extent does incorrect human labeling (noise) in datasets like ImageNet-1k confound the observed relationship between classification fidelity and MPS size?
- Basis in paper: [explicit] The discussion notes that "ImageNet-1K labels are slightly 'noisy'" and states, "more work is needed to discover the impact of incorrect human labeling on the size of MPSs."
- Why unresolved: The study measures "correctness" based on ground truth labels; however, if the ground truth is wrong, the "incorrect" classifications attributed to the model might actually be correct, skewing the MPS size analysis.
- What evidence would resolve it: Re-evaluating the statistical difference in MPS size between "correct" and "incorrect" sets using a verified, noise-free dataset or manually vetted labels.

### Open Question 3
- Question: Do the identified Minimal Sufficient Pixel Sets align with human semantic understanding enough to serve as useful explanations?
- Basis in paper: [explicit] The authors state, "the question remains as to whether an MPS is a good explanation for a human," observing that MPSs are often "myopic" and surprisingly small.
- Why unresolved: The study evaluates MPSs based on model-centric metrics (size, location) but lacks a qualitative or quantitative comparison against human-annotated explanations.
- What evidence would resolve it: A user study or benchmark comparison measuring the overlap between ReX MPSs and human-generated segmentation masks or feature importance maps.

### Open Question 4
- Question: Are the observed architectural differences in "concentration" robust when using alternative baseline masking strategies (e.g., blurring) instead of zero-masking?
- Basis in paper: [inferred] The authors use a default baseline of 0 but note that "different models are more sensitive to different types of ood images," suggesting that the specific nature of the mask could influence the resulting MPS.
- Why unresolved: The paper establishes that MPSs are effectively out-of-distribution (OOD) images; however, it does not test if the architectural ranking of concentration holds when the OOD perturbation method changes.
- What evidence would resolve it: Running the ReX algorithm using alternative baselines (e.g., blurred versions of the image) on the same model set to see if the relative MPS sizes remain consistent.

## Limitations
- The analysis assumes ImageNet-1k ground truth labels are correct, which may not hold for all images
- The study uses only top-1 classification without considering confidence scores or probability distributions
- The focus on a single explanation method (ReX) limits generalizability to other attribution techniques
- Reliance on ReX's causal definition of explanations assumes zeroing pixels to 0 is a valid causal intervention

## Confidence
- **High:** The comparative ranking of MPS sizes across architectures is robust (p < 0.001), based on statistical tests across 15 models and 50,000 images
- **Medium:** The claim that smaller MPS indicates overfitting is supported but requires additional validation, as it's based on proxy metrics rather than direct generalization tests
- **Low:** The assertion that incorrect classifications require 2.6% larger MPS than correct ones, while statistically significant, may be influenced by dataset noise or label ambiguity

## Next Checks
1. Cross-method validation: Replicate MPS analysis using a different explanation method (e.g., Grad-CAM or Integrated Gradients) to verify if ConvNext consistently produces smaller MPSs
2. Generalization correlation: Measure the correlation between MPS size and out-of-distribution performance (e.g., on ObjectNet or corrupted ImageNet variants) to directly test the overfitting hypothesis
3. Label noise robustness: Analyze MPS characteristics on a subset of ImageNet images with verified ground truth labels to ensure the 2.6% difference isn't driven by label errors