---
ver: rpa2
title: Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce
  Learning-to-Rank
arxiv_id: '2507.20753'
source_url: https://arxiv.org/abs/2507.20753
tags:
- learning
- e-commerce
- otto
- systems
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses whether deep neural networks (DNNs) can outperform
  gradient-boosted decision trees (GBDTs) for learning-to-rank tasks in e-commerce.
  The authors benchmark multiple DNN architectures (Two-Tower, Cross-Encoder, Transformer)
  and loss functions (RankNet, Softmax Cross-Entropy) against a production-grade LambdaMART
  model on a large proprietary dataset from OTTO.
---

# Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank

## Quick Facts
- arXiv ID: 2507.20753
- Source URL: https://arxiv.org/abs/2507.20753
- Authors: Yunus Lutz; Timo Wilm; Philipp Duwe
- Reference count: 25
- Key outcome: Simple Two-Tower DNN with Softmax Cross-Entropy loss outperforms production GBDT baseline, achieving 1.86% increase in total clicks and 0.56% increase in revenue

## Executive Summary
This study benchmarks multiple deep neural network architectures against a production-grade LambdaMART model for e-commerce learning-to-rank tasks. Using a large proprietary dataset from OTTO, the authors evaluate Two-Tower, Cross-Encoder, and Transformer architectures with different loss functions. Through extensive offline evaluation and an 8-week online A/B test, they find that a simple Two-Tower architecture with Softmax Cross-Entropy loss delivers superior performance, achieving significant increases in both engagement and revenue metrics while maintaining parity in units sold.

## Method Summary
The paper evaluates learning-to-rank models on a large e-commerce dataset with 43M training samples and 700k test samples. They compare a LightGBM LambdaMART baseline against three DNN architectures (Two-Tower, Cross-Encoder, Transformer) using two loss functions (Modified RankNet, Softmax Cross-Entropy). The Two-Tower architecture features separate encoders for context and product features with dot-product scoring. Training uses Adam optimizer with learning rate 0.001, batch size 1000, and dropout 0.0. Features include normalized numerical values, categorical embeddings (128-dim), and bag-of-words text embeddings (512-dim). The combined loss function weights clicks and orders with α=0.5.

## Key Results
- Two-Tower DNN with Softmax CE loss outperforms LambdaMART baseline on offline NDCG metrics
- Online A/B test shows 1.86% increase in total clicks and 0.56% increase in revenue
- Units sold remained stable, indicating revenue gains came from higher-value items
- Transformer architecture underperformed on order prediction despite higher click performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Two-Tower architecture with dot-product scoring outperforms non-linear baselines when feature embeddings are expressive and normalized
- **Mechanism**: Independent dense representations for context and product enable pre-computation of product embeddings, reducing inference latency while maintaining ranking quality through angular distance measurement
- **Core assumption**: Relevance relationships can be captured by linear interaction in learned embedding space rather than complex feature crossing at inference time
- **Evidence anchors**: Abstract confirms Two-Tower outperforms GBDT; section 5.1 describes pre-computation and dot-product scoring; corpus neighbors highlight industry standard status but potential confounding issues
- **Break condition**: Performance plateaus if relevance relies heavily on complex feature interactions that cannot be separated

### Mechanism 2
- **Claim**: Softmax Cross-Entropy loss provides more stable gradient signals than pairwise RankNet losses for multi-interaction lists
- **Mechanism**: Treats ranking as multi-class classification over candidate list, normalizing labels to force probability distribution prediction, which regularizes Two-Tower more effectively than pairwise loss
- **Core assumption**: Listwise context provides better training signal than pairwise comparisons for this data distribution
- **Evidence anchors**: Section 5.2 details CE loss normalization; section 6.1 shows CE performs best for Two-Tower; corpus lacks direct comparisons of CE vs RankNet in Two-Tower contexts
- **Break condition**: Computationally unstable if list size varies drastically or contains thousands of items

### Mechanism 3
- **Claim**: DNNs capture semantic similarity via text embeddings, providing distinct signal from exact matching or engineered statistics
- **Mechanism**: Processes textual features through embedding layers, allowing generalization across synonyms or related concepts rather than treating text as categorical
- **Core assumption**: Previous GBDT baseline under-utilized textual semantic signals, and DNN's ability to learn embeddings drove incremental click increases
- **Evidence anchors**: Section 4.1 describes bag-of-words text representation; section 6.2 reports +1.86% clicks; corpus neighbors focus on GBDT robustness or Two-Tower bias, not text-embedding contributions
- **Break condition**: No gain if product catalog is static and text is noisy or irrelevant

## Foundational Learning

- **Concept: Two-Tower Architecture**
  - **Why needed here**: This is the winning architecture; understand how query/user and item features project into shared space for fast retrieval
  - **Quick check question**: Can you explain why calculating a dot product is faster at inference time than running a full Cross-Encoder network for every product?

- **Concept: Listwise vs. Pairwise Loss**
  - **Why needed here**: Paper distinguishes between RankNet (pairwise) and Softmax CE (listwise); understanding gradient difference explains performance gap
  - **Quick check question**: Does RankNet optimize the order of a pair of items, or the probability distribution of the entire list?

- **Concept: Offline vs. Online Metric Correlation**
  - **Why needed here**: Paper validates offline NDCG gains with online Revenue/Clicks; practitioners struggle when offline gains don't translate online
  - **Quick check question**: Why might an increase in NDCGc correlate with Revenue even if NDCGo stays flat?

## Architecture Onboarding

- **Component map**: Input Layers (Numerical → Power-law/Z-score, Categorical → Embeddings, Text → BoW Embeddings) → Backbone (3-layer MLP: LayerNorm → ReLU → Dropout with Skip Connections) → Towers (Separate encoders for Context h_c and Product h_p) → Head (Dot Product Scoring s = h_c^T h_p) → Loss (Weighted Softmax Cross-Entropy: Clicks + Orders)

- **Critical path**:
  1. **Data Prep**: Normalize numerical features (Power-law for skewed, Z-score for others)
  2. **Training**: Implement Modified RankNet and Softmax CE losses; tune α (click/order weighting)
  3. **Validation**: Check NDCGc AND NDCGo; paper warns optimizing only clicks can degrade order performance

- **Design tradeoffs**:
  - **Two-Tower vs. Transformer**: Two-Tower won on ROI; simpler and faster but theoretically less expressive than Transformer (which failed to converge well on orders)
  - **CE vs. RankNet**: CE better for Two-Tower; RankNet better for Cross-Encoder/Transformer (per Table 1); don't default to one loss for all architectures

- **Failure signatures**:
  - **Transformer Degradation**: Watch for NDCGo dropping (-1.48% in paper); authors attribute to over-focusing on dominant click signal
  - **Serving Cost**: DNNs have higher training/serving costs than GBDTs; paper claims "negligible" but verify inference latency (ms) for your infrastructure

- **First 3 experiments**:
  1. **Baseline Reproduction**: Train LightGBM LambdaMART on your data to establish NDCG@15 bar
  2. **Two-Tower + CE**: Implement Two-Tower with Softmax Cross-Entropy; use paper's hyperparams (3 layers, 1024 hidden size) as starting point
  3. **Ablation on Alpha**: Sweep α parameter (weighting clicks vs. orders); paper used 0.5, but business logic may differ

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Transformer-based architectures outperform Two-Tower models for e-commerce LTR when specifically tuned to handle extreme class imbalance between clicks and orders?
- **Basis in paper**: Explicit statement that Transformer underperformance in NDCGo "might be attributed to choice of α=0.5 and imbalanced nature of our dataset"
- **Why unresolved**: Study used fixed hyperparameter search without isolating impact of loss weighting or sampling strategies for Transformer on order prediction
- **What evidence would resolve it**: Ablation study varying loss weighting α or employing sampling techniques for Transformer architecture to specifically target order optimization

### Open Question 2
- **Question**: What are specific computational costs and inference latencies associated with proposed DNN architectures compared to GBDT baseline in production environment?
- **Basis in paper**: Inferred from section 6.2 stating DNN has "higher training and serving costs" dismissed as "negligible" without quantitative latency data
- **Why unresolved**: Industry practitioners face strict latency budgets; without quantitative latency comparisons, viability for lower-latency environments remains unverified
- **What evidence would resolve it**: Comparative analysis reporting inference time in milliseconds, memory footprint, and throughput (QPS) for all models against LightGBM

### Open Question 3
- **Question**: Why does proposed DNN model result in statistically significant revenue increase (+0.56%) while achieving only parity in units sold?
- **Basis in paper**: Inferred from section 6.2 reporting divergence where clicks and revenue increased significantly but "units sold remained stable"
- **Why unresolved**: Pattern suggests favoring higher-priced items, but paper doesn't analyze underlying cause of decoupling between revenue and volume
- **What evidence would resolve it**: Analysis of price distribution of purchased items in A/B test groups to determine if revenue lift driven by higher AOV rather than conversion volume

## Limitations

- **Dataset Generalization**: Results based on single proprietary OTTO dataset; may not generalize to different product domains, user behaviors, or market contexts
- **Architectural Exploration**: Only one specific Two-Tower configuration tested; didn't investigate deeper towers, attention mechanisms, or different scoring functions
- **Feature Engineering Impact**: Paper mentions extensive feature engineering but lacks ablation studies to determine which feature types drive performance improvements

## Confidence

- **High Confidence**: Empirical finding that Two-Tower DNN with Softmax CE loss outperforms LambdaMART baseline on both offline and online metrics; 8-week A/B test provides strong evidence
- **Medium Confidence**: Mechanistic explanations for why Two-Tower + CE works better; plausible reasoning about dot-product efficiency and listwise regularization but could benefit from more theoretical analysis
- **Low Confidence**: Claim that approach generalizes to other e-commerce contexts without modification; paper provides no evidence about transfer learning or domain adaptation capabilities

## Next Checks

1. **Cross-Domain Validation**: Implement Two-Tower + CE architecture on publicly available e-commerce dataset (e.g., Amazon product data) to verify performance gains replicate outside OTTO environment

2. **Architectural Ablation**: Systematically vary Two-Tower architecture—test deeper towers, alternative scoring functions (cosine vs. dot product), and different backbone configurations to identify essential vs. incidental components

3. **Feature Attribution Analysis**: Conduct SHAP or similar feature importance analysis to determine whether DNN gains come from better utilization of existing features or capturing entirely new signals that GBDTs couldn't access