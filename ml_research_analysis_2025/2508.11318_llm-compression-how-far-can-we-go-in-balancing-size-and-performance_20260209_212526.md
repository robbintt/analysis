---
ver: rpa2
title: 'LLM Compression: How Far Can We Go in Balancing Size and Performance?'
arxiv_id: '2508.11318'
source_url: https://arxiv.org/abs/2508.11318
tags:
- quantization
- gptq
- accuracy
- memory
- boolq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 4-bit quantization techniques (GSQ and GPTQ)
  on three small language models (LLaMA 1B, Qwen 0.5B, and Phi 1.5B) across multiple
  NLP tasks. The primary goal is to balance model compression with maintained performance,
  assessing accuracy, inference latency, throughput, and memory usage.
---

# LLM Compression: How Far Can We Go in Balancing Size and Performance?

## Quick Facts
- arXiv ID: 2508.11318
- Source URL: https://arxiv.org/abs/2508.11318
- Reference count: 17
- Primary result: 4-bit quantization achieves up to 13× size reduction with minimal accuracy drops in some cases

## Executive Summary
This study evaluates 4-bit quantization techniques (GSQ and GPTQ) on three small language models (LLaMA 1B, Qwen 0.5B, and Phi 1.5B) across multiple NLP tasks. The primary goal is to balance model compression with maintained performance, assessing accuracy, inference latency, throughput, and memory usage. Results show that while GSQ and GPTQ achieve up to 13-fold model size reduction with minimal accuracy drops in some cases, performance trade-offs vary by model and task. LLaMA 1B shows improved MS MARCO accuracy post-quantization, whereas Qwen 0.5B experiences significant accuracy loss. Latency and throughput are largely unaffected, supporting practical deployment. The study identifies quantization as a viable compression method, though sensitivity to model-task combinations and group size constraints remain challenges.

## Method Summary
The study applies post-training quantization to three pre-trained small language models using two 4-bit techniques: GPTQ (layer-wise quantization with Hessian-based error correction) and GSQ (activation-aware group scaling). Models are evaluated on three tasks: MS MARCO (information retrieval), BoolQ (boolean QA), and GSM8K (mathematical reasoning). The quantization process uses group size 16 by default, with some tests using group size 64. Evaluation metrics include accuracy, perplexity, inference latency, throughput, and memory usage on NVIDIA A100 SXM4 40GB hardware.

## Key Results
- GSQ and GPTQ achieve up to 13-fold model size reduction with minimal accuracy drops in some cases
- LLaMA 1B improves MS MARCO accuracy post-quantization (81.12% → 99.86% with GPTQ)
- Qwen 0.5B experiences significant accuracy loss across all tasks, particularly on GSM8K (15.74% → 0.50% with GPTQ)
- Latency and throughput are largely unaffected by quantization

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Quantization with Error Compensation (GPTQ)
Converting model weights from FP32/FP64 to 4-bit representations while propagating quantization error corrections across layers can preserve task performance with significant size reduction. GPTQ quantizes one layer at a time, computing the inverse Hessian to determine optimal weight adjustments. When quantizing weights in row $i$, it compensates by adjusting weights in columns $j > i$ to minimize output perturbation. Quantization error can be approximated as locally linear and corrected through Hessian-weighted updates without destabilizing later layers. GSM8K math tasks degrade sharply—GPTQ shows near-zero accuracy on GSM8K for all three models, suggesting numerical precision requirements exceed error correction capacity.

### Mechanism 2: Activation-Aware Group Scaling (GSQ)
Partitioning weights into small groups with per-group scaling factors preserves important weights better than magnitude-based selection. GSQ divides weight matrices into groups (e.g., 16 or 64 elements) and assigns each a shared scale factor. Critically, it identifies "important" weights by measuring their impact on activation magnitudes, not absolute weight values, then applies per-group INT4 quantization. Weights that disproportionately affect activations encode task-critical information; preserving their relative precision within groups maintains downstream performance. Tensor dimension divisibility—GSQ requires the last dimension to be divisible by group size. A (256, 100) tensor fails with group size 32. Fine-tuned models with sparse structures exacerbate this constraint.

### Mechanism 3: Task-Specific Quantization Sensitivity
Different NLP tasks exhibit quantization tolerance inversely correlated with numerical precision requirements. BoolQ relies on semantic similarity matching tolerable to weight perturbation. Mathematical reasoning (GSM8K) requires precise numerical manipulation; small precision losses compound through multi-step arithmetic. Information retrieval (MS MARCO) lies between—embedding-based ranking degrades gracefully. Model-task pairs have inherent sensitivity profiles determined by whether the task requires categorical vs. continuous numerical reasoning. Smaller models (Qwen 0.5B) show higher sensitivity across all tasks, indicating capacity constraints interact with quantization—insufficient parameter redundancy for error absorption.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: GSQ and GPTQ are PTQ methods applied after training without re-training. Understanding this distinction explains why error correction (not gradient-based adaptation) is the primary mechanism.
  - Quick check question: Why would a one-shot PTQ method require no training data beyond a small calibration set?

- **Concept: Group Size Trade-off in Quantization**
  - Why needed here: The paper identifies group size (16 vs. 64) as a critical hyperparameter affecting accuracy, latency, and throughput. Smaller groups preserve accuracy but increase overhead.
  - Quick check question: If you decrease group size from 64 to 16, would you expect latency to increase or decrease? Why?

- **Concept: Perplexity as Degradation Signal**
  - Why needed here: The paper reports perplexity increases post-quantization, particularly for GPTQ (up to 2×). This serves as an early warning metric before task-specific evaluation.
  - Quick check question: Why might perplexity increase even when accuracy remains stable on a specific benchmark?

## Architecture Onboarding

- **Component map**: Pre-trained Model (FP16/FP32) → Calibration Data → [GSQ: Group partitioning + activation-aware scaling] → [GPTQ: Layer-wise Hessian inversion + error correction] → Quantized Model (INT4 weights + FP16 activations) → Inference: Dequantize → Compute → (optional) Requantize

- **Critical path**: Select group size → Verify tensor divisibility → Run calibration (100-512 samples typical) → Apply quantization → Validate perplexity < 1.5× baseline → Benchmark on task-specific accuracy

- **Design tradeoffs**:
  - Group size 16: Higher accuracy retention (+2-5% on some tasks), but +10-30% latency overhead and potential memory paradox (quantized model uses more memory due to scale factor storage)
  - Group size 64: Faster inference, lower memory, but accuracy drops on precision-sensitive tasks (math, code)
  - GPTQ vs. GSQ: GPTQ excels on semantic tasks (BoolQ, MS MARCO), GSQ provides more stable runtime metrics but underperforms on GSM8K

- **Failure signatures**:
  - Math task collapse: GPTQ achieves near-zero on GSM8K across all models—immediate signal of numerical precision failure
  - Memory paradox: Quantized models using MORE memory than baseline (Table 1 shows +2.7GB for some runs)—indicates inefficient dequantization caching or scale factor overhead
  - Group size runtime error: Tensor dimension not divisible by group size—GSQ throws shape mismatch without fallback
  - Perplexity spike: >2× increase suggests catastrophic information loss; GPTQ shows this in edge cases

- **First 3 experiments**:
  1. Baseline calibration sweep: Run both GSQ and GPTQ on your target model with group sizes [16, 32, 64], measuring perplexity on Wikitext. Flag any configuration with >1.5× perplexity increase.
  2. Task sensitivity probe: Before full quantization, test a 100-sample subset of your target task with both methods. If accuracy drops >5%, consider hybrid approaches (layer-wise mixed precision) or larger group sizes.
  3. Memory-latency validation: On your deployment hardware, measure actual (not theoretical) memory footprint and tokens/second. The paper shows latency/throughput are "largely unaffected" on A100, but edge devices may show different profiles due to memory bandwidth constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the impact of 4-bit quantization vary across individual transformer layers in small language models?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work includes a layer-wise analysis of the effects of 4-bit quantization."
- Why unresolved: The current study aggregates performance metrics (accuracy, latency) across the entire model, masking potential variations in sensitivity or error accumulation in specific layers.
- What evidence would resolve it: A breakdown of perplexity and accuracy metrics assessed after quantizing individual layers or blocks independently.

### Open Question 2
- Question: Why do specific quantized model configurations (e.g., LLaMA 1B on GSM8K) report higher memory usage than the unquantized baseline?
- Basis in paper: [inferred] Table 1 notes a "Memory Paradox" where quantized models often use MORE memory (marked with †), contradicting the general finding of reduced footprints.
- Why unresolved: While the paper suggests this implies experimental issues or inefficient implementation, the specific mechanism (e.g., overhead from de-quantization buffers or KV-cache handling) is not isolated.
- What evidence would resolve it: Memory profiling that separates weight storage from runtime activation memory and implementation overhead during inference.

### Open Question 3
- Question: Can variable group sizing or fallback handling mechanisms be implemented to prevent runtime failures when tensor dimensions are not divisible by the group size?
- Basis in paper: [explicit] The limitations section notes GSQ lacks fallback handling and requires the last tensor dimension to be divisible by the group size (e.g., tensor dim 100 vs group size 32).
- Why unresolved: The current GSQ implementation fails on incompatible shapes, restricting its applicability to models with varying layer dimensions or sparsity patterns introduced by fine-tuning.
- What evidence would resolve it: A modified algorithm that successfully quantizes tensors with non-standard dimensions without inducing runtime errors or significant accuracy loss.

## Limitations
- Strong model-task interaction effects suggest results may not generalize beyond the three tested models
- GSQ's group size constraints create potential failure modes not addressed in the paper
- Memory paradox where quantized models use more memory than baselines indicates potential inefficiencies

## Confidence
- **High confidence**: Compression ratios and memory usage measurements - deterministic calculations from weight counts and reported GPU usage
- **Medium confidence**: Accuracy trends on BoolQ and MS MARCO - consistent measurements but model-task interaction effects raise generalizability questions
- **Low confidence**: GSM8K accuracy results - GPTQ achieving near-zero accuracy across all models suggests either fundamental limitations or potential evaluation issues

## Next Checks
1. Test GPTQ and GSQ performance across 5 different calibration set sizes (32, 128, 512, 2048, 8192 samples) on a single model-task pair to quantify the impact of calibration quality on final accuracy.
2. Implement layer-wise mixed precision quantization (FP16 for first/last layers, INT4 for middle layers) on GSM8K to test whether mathematical reasoning tasks can be rescued by preserving higher precision in critical layers.
3. Apply the best-performing quantization configuration (GSQ group_size=16 for LLaMA 1B) to two additional small models (e.g., Gemma 2B, Mistral 7B) on the same task suite to test whether the observed model-task interactions are specific to the original three models or represent broader patterns.