---
ver: rpa2
title: 'FastTTS: Accelerating Test-Time Scaling for Edge LLM Reasoning'
arxiv_id: '2509.00195'
source_url: https://arxiv.org/abs/2509.00195
tags:
- memory
- reasoning
- arxiv
- cache
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FastTTS addresses the challenge of deploying test-time scaling
  (TTS) for edge LLM reasoning under strict memory constraints. It introduces three
  synergistic techniques: Speculative Beam Extension to hide straggler latency, Dynamic
  Prefix-Aware Scheduling to maximize KV cache reuse, and Asymmetric Multi-Model Memory
  Allocation to optimize memory usage between generator and verifier models.'
---

# FastTTS: Accelerating Test-Time Scaling for Edge LLM Reasoning

## Quick Facts
- arXiv ID: 2509.00195
- Source URL: https://arxiv.org/abs/2509.00195
- Reference count: 40
- Primary result: Achieves 2.2x higher goodput and 38%-68% latency reduction vs. vLLM baseline on edge LLM reasoning

## Executive Summary
FastTTS is a system optimization framework designed to deploy test-time scaling (TTS) for edge LLM reasoning under strict memory constraints. It addresses the challenge of irregular reasoning paths and memory bottlenecks in verifier-guided search by introducing three synergistic techniques: Speculative Beam Extension, Dynamic Prefix-Aware Scheduling, and Asymmetric Multi-Model Memory Allocation. The system enables edge LLMs (â‰¤7B) on a single 24GB GPU to match cloud-model accuracy and latency.

## Method Summary
FastTTS implements three core optimizations on top of vLLM v0.9.2 to enable efficient edge deployment of verifier-guided beam search. Speculative Beam Extension hides straggler latency by generating tokens for completed beams, Dynamic Prefix-Aware Scheduling maximizes KV cache reuse through beam reordering, and Asymmetric Multi-Model Memory Allocation optimizes memory distribution between generator and verifier based on their compute-memory profiles. The system uses beam search with a Process Reward Model (PRM) and is evaluated on mathematical reasoning and code generation tasks.

## Key Results
- Achieves average 2.2x higher goodput compared to vLLM baseline
- Reduces latency by 38%-68% across different model configurations
- Maintains Top-1 accuracy within 1-2% of baseline across beam counts (n=8 to 512)
- Successfully runs 7B+1.5B model pairs on single 24GB GPU

## Why This Works (Mechanism)

### Mechanism 1: Speculative Beam Extension (SBE)
SBE reduces latency by speculatively generating tokens for completed beams during GPU idle cycles. The core assumption is that previous verifier scores correlate with future selection probability, allowing the system to guess which beams are worth speculating on. The technique hides straggler latency in verifier-guided search where beams finish at different times.

### Mechanism 2: Dynamic Prefix-Aware Scheduling (DPAS)
DPAS maximizes KV cache reuse by reordering beam execution to process paths with shared prefixes consecutively. This transforms random eviction patterns into sequential access patterns, reducing memory bandwidth pressure. The optimization assumes the cost of scheduling reordering is lower than KV cache recompression or eviction costs.

### Mechanism 3: Asymmetric Multi-Model Memory Allocation (AMMA)
AMMA improves throughput by dynamically partitioning GPU memory between generator and verifier based on their distinct profiles. The verifier (prefill-heavy) is compute-bound while the generator (decoding-heavy) is memory-bandwidth-bound. The allocation assumes standard roofline model characteristics for these components.

## Foundational Learning

- **Test-Time Scaling & Verifier-Guided Search**: Needed to understand FastTTS as a systems optimization for existing TTS methods. Quick check: In beam search, why does generation have irregular token counts while verification is more uniform?

- **KV Cache & PagedAttention**: Required to understand Dynamic Prefix-Aware Scheduling's cache management. Quick check: When a beam is duplicated, does its KV cache need immediate copying? How does prefix sharing affect this?

- **Roofline Model**: Essential for understanding Asymmetric Multi-Model Memory Allocation. Quick check: Why does increasing batch size help memory-bound decoding more than compute-bound prefill?

## Architecture Onboarding

- **Component map**: Scheduler -> Generator Worker -> Verifier Worker -> Memory Allocator
- **Critical path**: 1) Generation: Generator produces tokens for active beams. 2) Straggler Handling: Scheduler fills GPU slots with speculative extensions. 3) Verification: Verifier scores completed paths. 4) Selection & Branching: Top paths are selected and grouped. 5) Scheduling: Queues are reordered for prefix optimization.
- **Design tradeoffs**: Speculation aggressiveness risks distribution deviation vs. latency improvement; memory split affects decoding speed vs. verifier throughput.
- **Failure signatures**: Goodput degradation indicates poor speculation hit rate or DPAS overhead; stalling suggests under-allocated verifier memory.
- **First 3 experiments**: 1) Profile baseline Beam Search on vLLM to observe straggler patterns. 2) Vary KV cache split to find roofline inflection points. 3) Measure speculation hit rate with Speculative Beam Extension enabled.

## Open Questions the Paper Calls Out

### Open Question 1
Can FastTTS be adapted to optimize Monte Carlo Tree Search (MCTS) based reasoning methods? FastTTS is designed for verifier-guided beam search; MCTS involves different execution dynamics that may not map directly to the proposed techniques.

### Open Question 2
How does FastTTS perform when using Generative Process Reward Models (PRMs) instead of discriminative ones? The current memory allocation is tuned for discriminative verifiers' compute/memory profiles.

### Open Question 3
What are the performance gains when integrating FastTTS with algorithmic speculative decoding techniques? The paper mentions seamless integration is possible but hasn't been evaluated.

## Limitations
- Assumes specific memory-bound vs compute-bound profiles that may not hold for all model pairs
- Speculative beam extension relies on correlation between verifier scores and selection probability that could break with different PRM architectures
- Dynamic prefix-aware scheduling assumes sufficient prefix overlap in reasoning trees

## Confidence

**High confidence**: Quantitative results showing 2.2x goodput improvement and 38-68% latency reduction are well-supported by ablation studies across three model pairs and three datasets.

**Medium confidence**: Mechanism explanations for Speculative Beam Extension and Dynamic Prefix-Aware Scheduling are logically sound but rely on ideal conditions that may not generalize.

**Medium confidence**: Asymmetric Multi-Model Memory Allocation is theoretically justified through roofline analysis, but real-world performance could vary.

## Next Checks

1. **Speculation accuracy validation**: Measure speculative token acceptance rates across different reasoning tasks to verify hit rate exceeds compute cost.

2. **Memory allocation sensitivity analysis**: Systematically vary KV cache split across different model sizes to map actual throughput inflection points versus theoretical predictions.

3. **Prefix overlap measurement**: Quantify actual prefix sharing in reasoning trees to determine if Dynamic Prefix-Aware Scheduling provides consistent benefits.