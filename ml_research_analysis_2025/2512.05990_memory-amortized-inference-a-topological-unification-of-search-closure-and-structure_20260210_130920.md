---
ver: rpa2
title: 'Memory-Amortized Inference: A Topological Unification of Search, Closure,
  and Structure'
arxiv_id: '2512.05990'
source_url: https://arxiv.org/abs/2512.05990
tags:
- topological
- memory
- context
- system
- cycle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Memory-Amortized Inference (MAI), a topological
  framework that unifies learning and memory as phase transitions of a single geometric
  substrate. The key idea is the Homological Parity Principle, which posits that even-dimensional
  homology (Heven) represents stable content (what), while odd-dimensional homology
  (Hodd) represents dynamic context (where).
---

# Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure

## Quick Facts
- arXiv ID: 2512.05990
- Source URL: https://arxiv.org/abs/2512.05990
- Authors: Xin Li
- Reference count: 40
- Primary result: Introduces a topological framework unifying learning and memory as phase transitions of a single geometric substrate

## Executive Summary
Memory-Amortized Inference (MAI) proposes that information naturally partitions into two conjugate phases: even-dimensional homology (Heven) encodes stable content ("what"), while odd-dimensional homology (Hodd) encodes dynamic context ("where"). The framework unifies learning and memory as phase transitions of a single geometric substrate governed by the Homological Parity Principle. The core mechanism converts high-complexity recursive search into low-complexity lookup via topological cycle closure, implemented through a three-stage transformation: Search → Closure → Structure. This process is governed by a topological generalization of the Wake-Sleep algorithm that alternates between optimizing the Hodd flow (inference/wake) and condensing persistent cycles into the Heven scaffold (learning/sleep).

## Method Summary
The method implements a three-stage transformation: (1) Search—high-complexity recursive simulation via Savitch's Theorem explores state space through transient Hodd chains; (2) Closure—successful trajectories close into persistent cycles marking topological phase transitions; (3) Structure—persistent cycles condense into Heven scaffolds via memoization for future O(1) retrieval. Learning and inference temporally alternate as parity-specific optimization phases: wake/e-step fixes scaffold to optimize flow via recursive simulation, while sleep/m-step fixes flow to condense persistent cycles into scaffold. The system minimizes topological free energy $L(\Psi, \Phi) = |\chi|$ to achieve parity balance between content and context.

## Key Results
- Information naturally partitions into two conjugate phases: Heven (stable content) and Hodd (dynamic context)
- High-complexity recursive search can be converted to low-complexity lookup via topological cycle closure (NPSPACE → P complexity reduction)
- Learning and inference must temporally alternate as parity-specific optimization phases—topological generalization of Wake-Sleep algorithm

## Why This Works (Mechanism)

### Mechanism 1
Information naturally partitions into two conjugate phases: even-dimensional homology (Heven) encodes stable content ("what"), while odd-dimensional homology (Hodd) encodes dynamic context ("where"). The boundary conservation law ∂² = 0 creates structural asymmetry: closed chains that are boundaries vanish (trivial cycles), while nontrivial cycles persist. Semantic memory maps to Heven scaffolds (β₀ components, β₂ cavities) and episodic memory to Hodd flows (β₁ cycles), with consolidation as phase transition between them.

### Mechanism 2
High-complexity recursive search converts to low-complexity lookup via topological cycle closure—computational complexity reduction from NPSPACE to P. Three-stage transformation: (1) Search—Savitch-style recursive simulation explores state space via transient Hodd chains (∂c ≠ 0), trading time for space; (2) Closure—successful trajectory closes into persistent cycle (∂(Ψ_trace) = 0), marking "insight" as topological phase transition; (3) Structure—persistent cycle condenses into Heven scaffold via memoization, enabling future O(1) retrieval.

### Mechanism 3
Learning and inference must temporally alternate as parity-specific optimization phases—topological generalization of Wake-Sleep. Wake/E-step (inference): Fix scaffold Φ, optimize flow Ψ via recursive simulation—context-driven. Sleep/M-step (learning): Fix flow Ψ via replay, condense persistent cycles into Φ—content-driven. This implements "half-step trick" as time-reversed RL: propagate completed trace backwards to update generative model.

## Foundational Learning

- **Homology groups and boundary operators (Hₖ, ∂, cycles vs. boundaries)**: The framework rests on distinguishing trivial cycles (boundaries that vanish) from nontrivial cycles (holes that persist). Understanding ker(∂)/im(∂) is essential for the parity distinction. Quick check: Given a simplicial complex, can you explain why ∂² = 0 implies that all boundaries are cycles, but not all cycles are boundaries?

- **Savitch's Theorem (NPSPACE ⊆ PSPACE)**: The paper claims cognition "trades time for space" via recursive bisection. Understanding why nondeterministic space collapses to deterministic space (with quadratic time blowup) clarifies the "slow thinking" mechanism. Quick check: Why does Savitch's construction require O(log²n) space but exponential time?

- **Wake-Sleep algorithm (Hinton et al., 1995)**: MAI generalizes this to topological domain. The original alternates between recognition training (wake) and generative training (sleep)—understanding this pattern reveals how MAI extends it. Quick check: In Wake-Sleep, why can't the recognition and generative networks be trained simultaneously without alternating phases?

## Architecture Onboarding

- **Component map:**
  ```
  Context Ψ_t → Retrieval R(Φ_{t+1}, Ψ_t) → Candidate ̂Φ_t
                                              ↓
  Memory M = {(Ψ⁽ⁱ⁾, Φ⁽ⁱ⁾)} ←──────── Bootstrapping F(̂Φ_t, Ψ_t) → Φ_{t+1}
  ```
  - **R operator**: Retrieval + lightweight adaptation (gradient-free, cheap)
  - **F operator**: Bootstrapping update with cycle-consistency constraint
  - **CCUP regulator**: Minimizes |χ| while maximizing CH (capacity)
  - **Synchronization filter (S_Ψ)**: Context gate—phase-aligned evidences only
  - **Recurrence filter (R_Φ)**: Content gate—validates nontrivial H₁ cycles

- **Critical path:**
  1. Implement R via topological proximity search (not just vector similarity)
  2. Implement F with cycle-consistency loss: if (Φ_t, Ψ_t) ∈ γ, then Φ_{t+T} ≈ Φ_t
  3. Ensure G_δ = S_Ψ ∧ R_Φ admits only contextually coherent, topologically persistent traces

- **Design tradeoffs:**
  - **Capacity vs. coherence**: Maximizing CH grows memory; minimizing |χ| enforces balance. Paper claims Φ_Topo = 2·min(dim(Φ), dim(Ψ))—bottlenecked by weaker parity.
  - **Retrieval precision vs. adaptation cost**: R can be cheap (kernel attention) or expensive (topological search). Paper suggests homological constraints improve precision.
  - **Assumption:** Hierarchical condensation (H_odd^(k) → H_even^(k+1)) is automatic; no implementation detail provided.

- **Failure signatures:**
  - **Decoherence**: When recursion depth exceeds phase-locking capacity (≈7 per Miller's law), Hodd cycles lose distinguishability—working memory collapse.
  - **Parity imbalance**: Rich scaffold but no flow (dim(Φ) ≫ dim(Ψ)) yields high complexity but low Φ_Topo—system cannot adapt.
  - **Catastrophic interference**: Attempting simultaneous Φ and Ψ optimization violates CCUP—breaks the half-step trick.

- **First 3 experiments:**
  1. **Parity probe**: On a simple maze/task, measure whether learned representations naturally split into Heven-like (position-invariant) and Hodd-like (trajectory-specific) components. Compute Betti numbers of activation space.
  2. **Closure timing**: Instrument the "aha moment"—does prediction error drop coincide with detectable cycle closure in latent space? Test whether insight correlates with β₁ → β₁ + 1 transition.
  3. **Amortization validation**: Compare inference cost on repeated vs. novel contexts. If MAI works, repeated contexts should show O(1) retrieval cost while novel contexts show Savitch-like recursive search scaling.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Homological Parity Principle be empirically validated by demonstrating a strict dichotomy between rate coding (Heven) and phase coding (Hodd) in the ventral and dorsal streams? The paper provides theoretical mapping but acknowledges no specific experimental protocol or existing data that definitively proves this topological parity is the mechanism for the What/Where pathway distinction.

### Open Question 2
What defines the specific threshold or "surprisal" condition that triggers the "Topological Cycle Closure" phase transition to prevent premature consolidation? The paper asserts closure happens upon "success" or "stability," but in a continuous learning system, a specific boundary condition is needed to separate high-entropy exploration from low-entropy storage.

### Open Question 3
How can the proposed "time-reversed Reinforcement Learning" process be physically implemented without violating causality? While "hindsight experience replay" exists in RL, the paper frames this as a fundamental physical dynamic of the substrate, leaving the mechanism for backward propagation in a forward-time system ambiguous.

### Open Question 4
What specific physical substrates are candidates for building "Topological Resonance Engines" that compute via the proposed resonance? The paper provides a theoretical blueprint but stops short of identifying materials (e.g., photonic crystals, spin glasses) or hardware architectures capable of natively instantiating the ∂² = 0 conservation law.

## Limitations
- **Theoretical framework without empirical validation**: Claims about homology-based information partitioning lack direct neurophysiological evidence
- **Complexity of implementation**: Abstract operators R and F lack concrete algorithmic specification
- **Physical substrate ambiguity**: No identified materials or architectures for building proposed topological resonance engines

## Confidence
- **Homological Parity Principle**: Medium - Theoretically sound but empirically unverified
- **Complexity reduction via cycle closure**: Low - Claims NPSPACE→P reduction lack practical demonstration
- **Topological Wake-Sleep generalization**: Medium - Conceptually valid but implementation details missing

## Next Checks
1. **Parity probe experiment**: Test whether learned representations in a simple navigation task naturally split into position-invariant (Heven-like) and trajectory-specific (Hodd-like) components by computing Betti numbers of activation space.

2. **Closure timing validation**: Instrument "aha moment" in learning tasks to determine if prediction error drops coincide with detectable cycle closure events in latent space, testing whether insight correlates with β₁ → β₁ + 1 transitions.

3. **Amortization gap measurement**: Compare inference costs on repeated versus novel contexts in navigation or reasoning tasks, expecting O(1) retrieval cost for repeated contexts versus Savitch-like recursive search scaling for novel contexts.