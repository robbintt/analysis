---
ver: rpa2
title: 'LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video
  Generation'
arxiv_id: '2502.12945'
source_url: https://arxiv.org/abs/2502.12945
tags:
- video
- generation
- llms
- prompt
- popularity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  as assistants for generating popular micro-videos. The authors propose a pipeline
  (LLMPopcorn) that uses LLMs to generate prompts for video generation models, then
  evaluates the resulting videos' predicted popularity.
---

# LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation

## Quick Facts
- arXiv ID: 2502.12945
- Source URL: https://arxiv.org/abs/2502.12945
- Reference count: 0
- Large language models can generate prompts that produce more popular micro-videos than human-written prompts

## Executive Summary
This paper investigates the use of large language models (LLMs) as assistants for generating popular micro-videos. The authors propose a pipeline (LLMPopcorn) that uses LLMs to generate prompts for video generation models, then evaluates the resulting videos' predicted popularity. They introduce a prompt enhancement technique combining retrieval-augmented generation and chain-of-thought prompting. Experiments comparing five LLMs and three video generation models show that DeepSeek-V3 and DeepSeek-R1 achieve the highest performance in generating popular micro-videos.

## Method Summary
The study introduces a pipeline where LLMs generate prompts for video generation models, which are then evaluated using an automated popularity prediction model (RAAC). The method involves two phases: (1) retrieval-augmented generation to enhance prompts by searching for similar videos and (2) chain-of-thought prompting to generate improved prompts. The system compares five LLMs (Llama-3.3-70B, Qwen-2.5-72B, ChatGPT-4o, DeepSeek-V3, and DeepSeek-R1) against human-written prompts across multiple video generation models.

## Key Results
- DeepSeek-V3 and DeepSeek-R1 outperform other LLMs in generating popular micro-videos
- DeepSeek-V3 achieves higher popularity scores than human-created content (0.56 vs 0.44)
- Prompt enhancement technique significantly improves results, with DeepSeek-R1 achieving a peak win rate of 66% for enhanced prompts

## Why This Works (Mechanism)
The paper doesn't explicitly explain the underlying mechanisms, but the approach leverages LLMs' ability to understand patterns in popular content and generate contextually appropriate prompts. The combination of retrieval-augmented generation and chain-of-thought prompting allows the models to build upon existing successful content while adding creative variations.

## Foundational Learning
- **Prompt Engineering**: Critical for guiding video generation models; quick check: test different prompt structures to see impact on output quality
- **Retrieval-Augmented Generation**: Enhances prompts by incorporating relevant context; quick check: measure improvement in generated content quality with and without retrieval
- **Popularity Prediction Models**: Automated evaluation of content appeal; quick check: validate predicted popularity against actual user engagement metrics
- **Zero-Shot Learning**: Testing models without fine-tuning; quick check: compare performance with fine-tuned models on same task
- **Chain-of-Thought Prompting**: Enables step-by-step reasoning in prompt generation; quick check: evaluate impact on logical consistency of generated content

## Architecture Onboarding

**Component Map**: User Query -> LLM Prompt Generator -> Video Generation Model -> RAAC Popularity Predictor -> Evaluation Metrics

**Critical Path**: The core workflow flows from user query through LLM prompt generation to video generation and popularity prediction. The prompt enhancement technique (retrieval-augmented generation + chain-of-thought) represents the critical innovation that improves outcomes.

**Design Tradeoffs**: The study prioritizes automation and scalability over real-world validation, using predicted popularity rather than actual user engagement. This enables large-scale testing but may not reflect true user preferences.

**Failure Signatures**: Poor performance may manifest as low predicted popularity scores, irrelevant or nonsensical video content, or prompts that don't translate well to visual generation. Failure could also occur if the retrieval system returns irrelevant content or if the chain-of-thought reasoning breaks down.

**First 3 Experiments**:
1. Compare LLM-generated prompts against human-written prompts using RAAC popularity prediction
2. Evaluate the impact of prompt enhancement technique on generated video popularity
3. Test different LLMs (DeepSeek-V3, DeepSeek-R1, Llama-3.3-70B, Qwen-2.5-72B, ChatGPT-4o) to identify top performers

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on automated popularity prediction rather than actual user engagement metrics
- Single text-to-video model (RunwayML Gen-2) limits generalizability
- Zero-shot setting without fine-tuning may not represent optimal performance
- Dataset derived from TikTok may have platform-specific characteristics
- Focus on Chinese content with English translations affects cross-cultural generalizability

## Confidence

**High Confidence**: The experimental methodology for comparing different LLMs and prompt enhancement techniques is rigorous and well-documented. The observation that DeepSeek models outperform other tested LLMs in the given experimental conditions is well-supported by the data.

**Medium Confidence**: The conclusion that LLMs can effectively assist in creating popular micro-videos is supported, but the reliance on automated popularity prediction rather than real user engagement introduces uncertainty. The comparative advantage of DeepSeek-V3 over human-created content is based on predicted popularity scores that may not translate to actual user preferences.

**Low Confidence**: The generalizability of findings to other video generation models, platforms beyond TikTok, and different cultural contexts remains uncertain due to the study's specific experimental conditions and dataset characteristics.

## Next Checks
1. Conduct A/B testing with actual users to validate whether videos predicted as popular by RAAC actually achieve higher engagement metrics (likes, shares, watch time) compared to human-created content

2. Test the LLMPopcorn pipeline on multiple micro-video platforms (e.g., Instagram Reels, YouTube Shorts) to assess generalizability across different user demographics and platform algorithms

3. Repeat experiments using multiple text-to-video generation models (e.g., Pika, Stable Video Diffusion) to determine whether the observed LLM performance patterns hold across different generation architectures