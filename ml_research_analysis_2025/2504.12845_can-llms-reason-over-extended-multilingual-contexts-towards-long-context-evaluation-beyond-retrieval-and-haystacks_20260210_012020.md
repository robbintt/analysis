---
ver: rpa2
title: Can LLMs reason over extended multilingual contexts? Towards long-context evaluation
  beyond retrieval and haystacks
arxiv_id: '2504.12845'
source_url: https://arxiv.org/abs/2504.12845
tags:
- context
- multilingual
- evaluation
- languages
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in multilingual long-context evaluation
  by introducing MLRBench, a synthetic benchmark that goes beyond retrieval-centric
  tasks to assess reasoning, multi-hop inference, and epistemic reasoning across seven
  languages. Unlike existing benchmarks, MLRBench is designed to be parallel, resistant
  to data leakage, and scalable to arbitrary context lengths.
---

# Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks

## Quick Facts
- **arXiv ID:** 2504.12845
- **Source URL:** https://arxiv.org/abs/2504.12845
- **Reference count:** 12
- **Primary result:** Introduces MLRBench, a multilingual long-context evaluation benchmark revealing significant underutilization of context windows and performance gaps across languages

## Executive Summary
This paper addresses a critical gap in multilingual long-context evaluation by introducing MLRBench, a synthetic benchmark that assesses reasoning, multi-hop inference, and epistemic reasoning across seven languages. Unlike existing benchmarks focused primarily on retrieval tasks, MLRBench is designed to be parallel, resistant to data leakage, and scalable to arbitrary context lengths. The evaluation of Llama-3.1-Instruct reveals that multilingual LLMs effectively utilize less than 30% of their claimed context window, with significant performance disparities between high- and low-resource languages, particularly for complex reasoning tasks. While Retrieval Augmented Generation (RAG) improves stability, it does not fully solve the long-context problem.

## Method Summary
The authors developed MLRBench as a synthetic benchmark for evaluating multilingual long-context reasoning capabilities. The benchmark generates parallel documents across seven languages and evaluates models on retrieval, reasoning, multi-hop inference, and epistemic reasoning tasks. Experiments were conducted using Llama-3.1-Instruct across various context lengths and languages. The study also implemented RAG-based approaches to assess their impact on long-context performance. The evaluation framework measures both accuracy and context utilization rates to provide a comprehensive assessment of multilingual LLM capabilities.

## Key Results
- Multilingual LLMs utilize less than 30% of their claimed context window during reasoning tasks
- Significant performance gaps exist between high- and low-resource languages, particularly for complex reasoning
- RAG improves stability but does not fully address long-context limitations
- Current benchmarks may overestimate LLM reasoning capabilities due to their focus on retrieval-centric tasks

## Why This Works (Mechanism)
MLRBench works by creating controlled, synthetic multilingual environments that isolate long-context reasoning capabilities from retrieval performance. The parallel document generation ensures language consistency while allowing systematic variation of context length and complexity. By focusing on reasoning tasks beyond simple retrieval, the benchmark exposes fundamental limitations in how LLMs process and utilize extended contexts across different languages.

## Foundational Learning

**Multilingual LLM Architecture**
- *Why needed:* Understanding how multilingual models process different languages simultaneously is crucial for evaluating cross-lingual reasoning capabilities
- *Quick check:* Verify model tokenizer handles all seven languages efficiently without significant vocabulary overlap issues

**Context Window Utilization**
- *Why needed:* Measuring actual versus claimed context usage reveals architectural limitations in information processing
- *Quick check:* Track attention patterns across different context lengths to identify utilization bottlenecks

**Synthetic Data Generation**
- *Why needed:* Controlled generation ensures benchmark reproducibility and eliminates data leakage concerns
- *Quick check:* Validate generated documents maintain linguistic coherence across languages while varying complexity

**Retrieval Augmented Generation**
- *Why needed:* RAG represents a common approach to managing long contexts; understanding its limitations is crucial
- *Quick check:* Compare RAG performance against direct context processing to identify specific failure modes

## Architecture Onboarding

**Component Map**
MLRBench Generator -> Synthetic Document Corpus -> Task Definition Engine -> Evaluation Pipeline -> Performance Metrics

**Critical Path**
Document generation → Task formulation → Context window scaling → Model evaluation → Performance analysis

**Design Tradeoffs**
The synthetic nature provides control and reproducibility but may miss real-world complexity; parallel generation ensures comparability but may oversimplify language-specific nuances.

**Failure Signatures**
Underutilization below 30% indicates architectural constraints; performance drops in low-resource languages suggest data imbalance effects; RAG stability improvements without accuracy gains indicate superficial rather than fundamental solutions.

**First Experiments**
1. Measure context utilization rates across different percentage windows (10%, 30%, 50%, 70%, 100%)
2. Compare performance between high-resource (English, Spanish) and low-resource (Bengali, Swahili) languages
3. Evaluate RAG versus direct context processing for multi-hop reasoning tasks

## Open Questions the Paper Calls Out

None

## Limitations

The synthetic nature of MLRBench may not fully capture real-world multilingual complexity and variability. The study's focus on Llama-3.1-Instruct limits generalizability to other multilingual LLMs. Observed underutilization patterns could be influenced by factors beyond pure model capability, including prompt formulation differences and evaluation design choices.

## Confidence

- **High:** The finding that multilingual LLMs underutilize their claimed context windows is robust, supported by systematic experiments across multiple languages and tasks.
- **Medium:** The performance gaps between high- and low-resource languages are well-documented but may be influenced by factors beyond pure model capability, such as prompt formulation or evaluation design.
- **Medium:** The conclusion that RAG improves stability but doesn't fully solve long-context problems is supported, though the extent of improvement and its generalizability across different RAG implementations remains to be fully explored.

## Next Checks

1. Conduct real-world multilingual long-context evaluation using authentic multilingual documents to validate MLRBench findings and assess ecological validity.
2. Test additional multilingual LLMs beyond Llama-3.1-Instruct to determine if the observed underutilization pattern is consistent across different model architectures and training approaches.
3. Investigate the impact of context window utilization on task-specific performance by varying the percentage of context used and measuring corresponding changes in accuracy and reasoning quality across different languages.