---
ver: rpa2
title: Capturing Conditional Dependence via Auto-regressive Diffusion Models
arxiv_id: '2504.21314'
source_url: https://arxiv.org/abs/2504.21314
tags:
- diffusion
- training
- ddpm
- have
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes AR diffusion models to better capture conditional
  dependencies in data compared to vanilla diffusion models. It provides the first
  theoretical results on AR diffusion sampling error, showing that AR diffusion reduces
  the gap in approximating data conditional distributions and only increases inference
  time moderately compared to DDPM.
---

# Capturing Conditional Dependence via Auto-regressive Diffusion Models

## Quick Facts
- **arXiv ID**: 2504.21314
- **Source URL**: https://arxiv.org/abs/2504.21314
- **Reference count**: 40
- **Primary result**: AR diffusion models achieve O(KL²d/ε²) gradient complexity while better capturing conditional dependencies in data compared to vanilla DDPM

## Executive Summary
This paper introduces Auto-regressive Diffusion Models (AR diffusion) that generate data sequentially in patches, conditioning each new patch on previously generated ones. The key insight is that this approach better captures conditional dependencies in data compared to standard diffusion models. Under mild assumptions, AR diffusion provides theoretical guarantees for KL convergence with only a moderate increase in inference time. The paper provides the first theoretical analysis of AR diffusion sampling error and validates its effectiveness on synthetic image generation tasks where geometric features exhibit strict conditional relationships.

## Method Summary
The core innovation is a sequential patch generation strategy where an image is divided into K patches (e.g., 2x2 for 32x32 images). Patches are generated one at a time, with each new patch conditioned on the previous ones through a latent representation z = g^{ar}_θ(x_{1:k}). This contrasts with vanilla DDPM which generates all pixels simultaneously. The model trains using noise prediction (MSE) on each patch independently, while inference proceeds sequentially. The approach theoretically achieves O(KL²d/ε²) gradient complexity with KL convergence, where K is the number of patches, L is diffusion steps, d is data dimension, and ε is the error tolerance.

## Key Results
- AR diffusion achieves O(KL²d/ε²) gradient complexity with KL convergence under mild assumptions
- On synthetic Sun/Shadow task, AR diffusion achieves R² > 0.95 capturing the l₁h₁ = l₂h₂ constraint, while DDPM fails (R² ≈ 0.01)
- On Squares task with l₂ = 1.5l₁ constraint, AR diffusion maintains better R² performance than DDPM when patches preserve feature integrity
- AR diffusion training loss predicts inference performance on conditional dependency capture

## Why This Works (Mechanism)
The mechanism relies on sequential conditioning: by generating patches in order and conditioning each new patch on previous ones through a learned latent z, the model can explicitly model dependencies that exist between different parts of the data. This contrasts with vanilla diffusion which treats all pixels independently during generation. The AR approach effectively implements a form of conditional generation where the model learns to predict how one region should look given what came before, naturally capturing relationships like geometric constraints in images.

## Foundational Learning
- **Patch-based sequential generation**: Why needed: To break down complex generation into manageable conditional steps. Quick check: Verify patch size doesn't split semantic features.
- **Condition encoding g^{ar}_θ**: Why needed: To compress previous patches into a latent that captures their combined information. Quick check: Test different architectures (MLP vs transformer) for context encoding.
- **Teacher forcing during training**: Why needed: To train the model on ground truth context rather than generated context, preventing error accumulation. Quick check: Monitor training stability with ground truth vs generated context.
- **KL convergence analysis**: Why needed: To provide theoretical guarantees on the quality of conditional distributions. Quick check: Verify the bounded Hessian assumption holds for synthetic data.

## Architecture Onboarding

**Component Map:**
Data → Patch Splitter → Sequential Generator (patch 1, patch 2, ...) → Image Reconstructor

**Critical Path:**
1. Split image into K patches in raster order
2. For patch k: encode previous patches → predict noise → denoise → append to context
3. Repeat until all patches generated

**Design Tradeoffs:**
- **Patch size vs dependency capture**: Smaller patches increase K (computation) but may preserve dependencies better; larger patches reduce K but risk splitting features
- **Context encoder architecture**: MLP is simpler but may struggle with spatial patterns vs U-Net which has inductive bias for image structure
- **Order of generation**: Raster scan assumes dependencies align with spatial order; other orders may be needed for different dependency structures

**Failure Signatures:**
- Random R² scores indicate patches split semantic features or incorrect ordering
- Degraded quality from patch 1 to K indicates error accumulation in context encoding
- Training instability suggests context encoder cannot handle generated (noisy) data

**3 First Experiments:**
1. Generate synthetic Sun/Shadow data with constraint l₁h₁ = l₂h₂ and verify geometric feature extraction works
2. Train AR-MLP with ground truth context on single patch to validate basic noise prediction
3. Run sequential inference with teacher forcing on 4 patches to verify context encoding and generation pipeline

## Open Questions the Paper Calls Out

**Open Question 1**: Can optimal patch size K and sequential generation order be theoretically determined or learned to align with intrinsic conditional dependencies of unknown data?
- Basis: Paper notes performance depends on "appropriate patch partitioning" and degrades when patching disrupts dependencies
- Why unresolved: Authors manually select patch sizes based on known synthetic rules with no method for real-world data
- Resolution evidence: Adaptive algorithm clustering features into patches based on statistical dependence, outperforming fixed orders

**Open Question 2**: Is it possible to achieve conditional dependence guarantees without linear increase in inference gradient complexity (O(K)) relative to vanilla DDPM?
- Basis: Theorem 4.3 establishes gradient complexity as O(KL²dε⁻²), introducing factor of K
- Why unresolved: Analysis assumes sequential generation; authors don't explore parallel decoding or consistency distillation
- Resolution evidence: Modified sampling scheme maintaining KL convergence with sub-linear or constant gradient complexity

**Open Question 3**: Do theoretical convergence guarantees hold if data distribution lies on low-dimensional manifold, violating bounded Hessian assumption?
- Basis: Analysis relies on Assumption [A2] (bounded Hessian/gradient), a strong smoothness requirement
- Why unresolved: Real-world image data often lies on lower-dimensional manifolds incompatible with smoothness
- Resolution evidence: Convergence bounds under manifold assumptions without requiring Lipschitz continuity in ambient space

## Limitations
- Synthetic tasks are highly simplified and may not reflect real-world conditional dependency complexity
- AR strategy's effectiveness heavily depends on proper patch ordering and context encoding, which are underspecified
- Assumption that g^{ar}_θ can effectively compress x_{1:k} into latent z is not validated for complex data distributions
- Theoretical analysis assumes data smoothness that may not hold for real-world image manifolds

## Confidence

**High**: Theoretical complexity analysis and KL convergence guarantees
**Medium**: Experimental results on synthetic tasks showing AR diffusion captures conditional dependencies
**Low**: Claims about AR diffusion's effectiveness on real-world data and prediction of inference performance from training loss

## Next Checks
1. Implement and validate the condition encoder g^{ar}_θ architecture with multiple design choices (transformer vs MLP) to confirm which best captures conditional dependencies
2. Test AR diffusion on more complex synthetic tasks with multiple interdependent features to assess scalability
3. Conduct ablation studies comparing AR diffusion with varying patch sizes and orderings to identify optimal configurations for different dependency structures