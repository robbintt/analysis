---
ver: rpa2
title: Compact Neural TTS Voices for Accessibility
arxiv_id: '2501.17332'
source_url: https://arxiv.org/abs/2501.17332
tags:
- neural
- speech
- system
- latency
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a compact neural TTS system optimized for
  accessibility applications on low-power devices. The approach applies aggressive
  compression techniques to each component of a cascaded TTS pipeline: a Transformer-based
  frontend, FastSpeech2 acoustic model, and WaveRNN vocoder.'
---

# Compact Neural TTS Voices for Accessibility

## Quick Facts
- arXiv ID: 2501.17332
- Source URL: https://arxiv.org/abs/2501.17332
- Reference count: 24
- Primary result: 15ms latency, 18MB footprint, 4.09 MOS for low-power TTS

## Executive Summary
This paper presents a compact neural TTS system optimized for accessibility applications on low-power devices. The approach applies aggressive compression techniques to each component of a cascaded TTS pipeline: a Transformer-based frontend, FastSpeech2 acoustic model, and WaveRNN vocoder. The resulting system achieves 15ms latency, 18MB disk footprint, and 4.09 MOS score, enabling high-quality speech synthesis suitable for pre-installation across multiple voices while meeting strict resource constraints for accessibility use cases.

## Method Summary
The proposed system applies hardware-aware optimizations to a cascaded TTS pipeline consisting of three components: a Transformer-based frontend for text normalization, FastSpeech2 for acoustic modeling, and WaveRNN for neural vocoding. Key compression techniques include INT8 quantization and filter reduction in the acoustic model, sparsity and subscale generation in the vocoder, and parameter sharing with KV-caching in the frontend. The optimizations target a single-threaded CPU execution model on low-power devices, achieving a 15ms real-time factor with an 18MB total footprint for four voices.

## Key Results
- 15ms latency on Snapdragon 888 at 16kHz, single-threaded
- 18MB total footprint across four voices (5.4MB each for 8kHz, 8.1MB each for 16kHz)
- 4.09 MOS score, matching uncompressed baseline quality
- Single-threaded execution suitable for low-power CPU deployment

## Why This Works (Mechanism)
The system achieves compact deployment through targeted compression of each TTS component. The frontend benefits from parameter sharing and KV-caching to reduce memory usage during inference. The acoustic model uses INT8 quantization and reduces convolutional filters to decrease model size and computation. The vocoder applies structured sparsity to weight matrices and generates waveforms at reduced sample rates (subscale generation) before upsampling. These techniques collectively reduce computational requirements while maintaining speech quality through careful calibration of compression parameters.

## Foundational Learning

1. **Transformer KV-Caching**: Stores key-value pairs during attention computation to avoid redundant calculations
   - Why needed: Reduces repeated computation in autoregressive text normalization
   - Quick check: Verify cache hit rates during inference

2. **INT8 Quantization**: Reduces 32-bit floating-point weights to 8-bit integers
   - Why needed: Decreases memory footprint and accelerates matrix operations
   - Quick check: Compare inference speed and accuracy with INT8 vs FP32

3. **Structured Sparsity**: Applies predefined patterns to zero out weight matrix elements
   - Why needed: Reduces computation while maintaining model capacity
   - Quick check: Verify sparsity ratio and impact on model accuracy

4. **Subscale Generation**: Generates waveform at lower sample rate then upsamples
   - Why needed: Reduces vocoder computational load by factor of 2-4
   - Quick check: Measure quality degradation with different subsample factors

## Architecture Onboarding

**Component Map**: Text -> Frontend (Transformer) -> FastSpeech2 -> WaveRNN -> Speech

**Critical Path**: The acoustic model (FastSpeech2) represents the primary computational bottleneck, with the frontend and vocoder providing parallel optimizations.

**Design Tradeoffs**: Single-threaded execution maximizes compatibility with low-power CPUs but sacrifices potential throughput gains from parallelization. The compressed model size enables pre-installation but may limit voice variety.

**Failure Signatures**: Quality degradation typically manifests as robotic artifacts from aggressive quantization or dropout in the vocoder. Latency spikes indicate cache misses or inefficient memory access patterns.

**First 3 Experiments**:
1. Measure baseline inference latency and memory usage on target hardware
2. Apply INT8 quantization to acoustic model and evaluate quality impact
3. Test KV-caching effectiveness by comparing with and without caching

## Open Questions the Paper Calls Out
None

## Limitations
- Results are hardware-specific to Snapdragon 888, limiting cross-platform generalizability
- Quality degradation from compression remains unquantified due to lack of baseline comparison
- Voice scalability untested beyond four speakers, raising concerns about performance with diverse characteristics

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical implementation details | High |
| 18MB footprint and 4.09 MOS score | Medium |
| 15ms latency across hardware | Medium |

## Next Checks
1. Conduct cross-platform validation testing on at least three different low-power CPU architectures to verify consistent latency and quality metrics.
2. Perform A/B testing comparing compressed versus uncompressed model quality using the same evaluation methodology to quantify exact quality trade-offs.
3. Test the compressed system with at least 10 additional voices spanning diverse speaker characteristics and languages to evaluate scalability limits.