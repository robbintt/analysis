---
ver: rpa2
title: An Analysis of Datasets, Metrics and Models in Keyphrase Generation
arxiv_id: '2506.10346'
source_url: https://arxiv.org/abs/2506.10346
tags:
- keyphrase
- association
- linguistics
- computational
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# An Analysis of Datasets, Metrics and Models in Keyphrase Generation

## Quick Facts
- arXiv ID: 2506.10346
- Source URL: https://arxiv.org/abs/2506.10346
- Reference count: 40
- Key outcome: None

## Executive Summary
This paper presents a comprehensive analysis of keyphrase generation, identifying significant issues in evaluation practices and dataset redundancy. The authors find that commonly used datasets are highly similar, leading to overestimated performance due to normalization artifacts in evaluation metrics. They propose a unified BART-large baseline that outperforms 19 previous models and emphasize the need for diverse, large-scale benchmarks with proper metadata and licensing.

## Method Summary
The authors fine-tune a BART-large model using the ONE2MANY paradigm on the KP20k dataset, training for 9-10 epochs with AdamW optimizer (lr=1e-5, batch size=4). The model generates keyphrases as a single delimited string sequence, with gold keyphrases arranged in present-absent order. Evaluation uses F1@M, F1@5, and F1@10 metrics, with careful attention to avoiding normalization artifacts that artificially inflate scores. The approach deliberately avoids the Meng et al. (2017) preprocessing pipeline to provide a more accurate baseline.

## Key Results
- Removal of standard normalization reduces F1 scores by 2-3 absolute points, revealing evaluation inflation
- Five commonly used keyphrase generation datasets show high correlation, offering no practical benefit when reported together
- BART-large fine-tuning consistently outperforms 19 previous models, establishing a new strong baseline
- Present keyphrase performance remains significantly higher than absent keyphrase performance (~40% vs ~11% F1)

## Why This Works (Mechanism)

### Mechanism 1: Evaluation Inflation via Normalization Artifacts
Standard keyphrase normalization (unifying digits, stripping acronyms) applied to ground-truth data artificially inflates performance metrics by reducing lexical diversity, increasing partial match probabilities without improving semantic understanding.

### Mechanism 2: Benchmark Redundancy and Data Leakage
Keyphrase generation benchmarks share common source domains and citation structures, creating high cross-dataset correlation that masks overfitting to the academic domain and provides diminishing returns for generalization assessment.

### Mechanism 3: PLM Fine-Tuning as a Convergence Point
Fine-tuning Pre-trained Language Models, specifically BART, subsumes the need for specialized RNN mechanisms like pointer networks and coverage models, providing superior performance through strong semantic priors and attention mechanisms.

## Foundational Learning

- **Concept: Sequence-to-Sequence (Seq2Seq) with Attention**
  - Why needed here: Keyphrase generation is framed as a Seq2Seq task where the input is a document and the output is a sequence of phrases.
  - Quick check question: How does the ONE2MANY paradigm (generating a single delimited string) differ from traditional multi-label classification?

- **Concept: Evaluation Metrics (F1@k vs. F1@M)**
  - Why needed here: The paper critiques how metrics are calculated, requiring understanding of Precision/Recall to diagnose why normalization changes scores.
  - Quick check question: If a model predicts 3 keyphrases but k=5 in F1@5, how does padding with "dummy" phrases penalize the score compared to F1@M?

- **Concept: Present vs. Absent Keyphrases**
  - Why needed here: The paper analyzes these categories separately; "Present" appears in text (extraction-like) while "Absent" requires inference (generation-like).
  - Quick check question: Why is strict matching against ground truth particularly problematic for evaluating "Absent" keyphrases?

## Architecture Onboarding

- **Component map:** Scientific abstract + Title -> BART-large encoder -> Cross-attention -> BART-large decoder -> Delimiter-separated keyphrase string
- **Critical path:** 1) Data Cleaning: Do not apply Meng et al. normalization to ground truth 2) Training: Fine-tune BART-large on KP20k for 9-10 epochs using ONE2MANY objective 3) Decoding: Use Beam Search (k=20) to generate keyphrase string
- **Design tradeoffs:** ONE2MANY vs. ONE2SET (sequence vs. set generation), with ONE2MANY being easier to implement but potentially struggling with duplicates
- **Failure signatures:** Overestimation from legacy evaluation scripts using dummy phrase padding or normalization; low Absent scores (~11% F1) are expected behavior
- **First 3 experiments:** 1) Baseline Verification: Reproduce F1@M scores ensuring no normalization is applied 2) Metric Ablation: Compare F1@5 with dummy padding vs. F1@M 3) Domain Shift Test: Evaluate on non-CS dataset (e.g., KPTimes) to observe performance drop

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can Large Language Models (LLMs) serve as a robust automatic evaluation method for keyphrase generation, overcoming limitations of exact matching metrics?
**Basis in paper:** [explicit] Section 5.2 suggests testing LLM ability to evaluate generated keyphrases, as this approach has proven successful in several tasks.
**Why unresolved:** Current metrics rely on exact matching against single ground truth, penalizing valid lexical variations and failing to account for task-dependent relevance.
**What evidence would resolve it:** Experiments demonstrating LLM-based evaluation scores correlate more strongly with human judgments or extrinsic downstream task performance than traditional F1 scores.

### Open Question 2
**Question:** How can the field construct a large-scale benchmark dataset that mitigates redundancy and metadata issues present in current standards?
**Basis in paper:** [explicit] Section 5.1 identifies lack of diverse, sizeable benchmark datasets and suggests leveraging arXiv for proper licensing and metadata.
**Why unresolved:** Current benchmarks are highly correlated and lack essential metadata like DOIs or authorship, leading to data leakage and copyright concerns.
**What evidence would resolve it:** Release of large-scale dataset with clear provenance and metadata showing low performance correlation with existing scientific abstract datasets.

### Open Question 3
**Question:** To what extent does data contamination affect evaluation of LLMs on existing keyphrase generation benchmarks?
**Basis in paper:** [inferred] Section 5.3 mentions contamination is likely because benchmarks are widely available but solutions are not straightforward.
**Why unresolved:** LLMs pre-trained on massive corpora likely include source documents of current test sets, artificially inflating performance metrics.
**What evidence would resolve it:** Studies applying pre-training data detection methods to identify contaminated benchmark samples and reporting performance drops when those samples are excluded.

## Limitations
- The paper identifies significant evaluation inflation through normalization artifacts but doesn't provide a complete solution for all evaluation scenarios
- Dataset redundancy analysis shows high correlation but doesn't offer concrete alternatives beyond suggesting arXiv as a potential source
- The BART-large baseline, while strong, may not be accessible to all researchers due to computational constraints

## Confidence
- High: Evaluation normalization artifacts causing inflated scores (directly measured and replicated)
- High: Dataset redundancy and lack of diversity in current benchmarks (statistically demonstrated through correlation analysis)
- Medium: BART-large as optimal architecture (strong performance but limited comparison to other PLM architectures)

## Next Checks
1. Verify baseline reproduction by running inference on KP20k test set without applying Meng et al. normalization
2. Test the correlation claim by evaluating the BART baseline across multiple existing datasets (KP20k, SemEval, Krapivin, etc.)
3. Implement and compare F1@5 with dummy padding versus F1@M to quantify the penalty of variable-length predictions