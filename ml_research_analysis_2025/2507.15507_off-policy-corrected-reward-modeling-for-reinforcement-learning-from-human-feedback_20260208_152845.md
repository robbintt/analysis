---
ver: rpa2
title: Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human
  Feedback
arxiv_id: '2507.15507'
source_url: https://arxiv.org/abs/2507.15507
tags:
- policy
- reward
- training
- learning
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of overoptimization in Reinforcement
  Learning from Human Feedback (RLHF), where the reward model (RM) becomes inaccurate
  as the policy diverges from the training distribution, leading to suboptimal policy
  updates. The core method, Off-Policy Corrected Reward Modeling (OCRM), iteratively
  corrects the RM using importance weighting to maintain accuracy on the current policy
  distribution without requiring new labels or samples.
---

# Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2507.15507
- Source URL: https://arxiv.org/abs/2507.15507
- Reference count: 40
- Primary result: Iterative importance weighting corrects reward model distribution shift, achieving 73.6% win rate vs. 62.9% for PPO on TL;DR summarization

## Executive Summary
The paper addresses overoptimization in Reinforcement Learning from Human Feedback (RLHF), where the reward model (RM) becomes inaccurate as the policy diverges from the training distribution, leading to suboptimal policy updates. The core method, Off-Policy Corrected Reward Modeling (OCRM), iteratively corrects the RM using importance weighting to maintain accuracy on the current policy distribution without requiring new labels or samples. Experiments on summarization and chatbot tasks show OCRM outperforms standard RLHF methods, achieving higher win rates (e.g., 73.6% vs. 62.9% for PPO) and better alignment with human preferences. The approach is validated with both gold-model and GPT-4.1 Nano feedback, demonstrating consistent improvements across models and tasks.

## Method Summary
OCRM iteratively retrains the reward model using importance-weighted loss on fixed preference data to correct for distribution shift as the policy diverges from the SFT model. The method alternates between running PPO for a large number of steps (k=100,000) with the current RM and a KL penalty against the previous policy, then retraining a corrected RM using importance weights computed from the current and SFT policy distributions. The importance weights are flattened (η=0.001) and relative weighted (α=0.9) to control variance. This process repeats for m iterations, with value function and optimizer state reset after each RM update.

## Key Results
- OCRM achieves 73.6% win rate vs. 62.9% for standard PPO on TL;DR summarization task
- Consistent improvements across model sizes (Pythia 1B, Qwen 2.5 1.5B) and tasks
- Ablation shows both importance weighting and shifted KL-penalty contribute to performance gains
- Results validated with both synthetic gold-model and GPT-4.1 Nano feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overoptimization in RLHF is fundamentally a distribution shift problem. As the policy is updated, it diverges from the initial Supervised Fine-Tuned (SFT) policy that generated the preference data, causing the reward model (RM) to become inaccurate and the policy gradient estimate to become inconsistent.
- Mechanism: The RM is trained on data from the SFT policy (π₁). As the RL policy (πi) optimizes reward, it shifts to a new distribution. The RM parameters (θ̂) that were optimal for π₁ are an inconsistent estimator for the optimal parameters (θ*) on the new distribution πi. This inconsistency propagates to the policy gradient, leading to suboptimal updates and reward hacking.
- Core assumption: The true human reward function is not perfectly specified by the model class (θH ∉ Θ), making the RM susceptible to covariate shift.
- Evidence anchors:
  - [abstract]: "...the shift results in an inconsistent estimate of the RM parameters, leading to an inconsistent estimate of the policy gradient."
  - [section 4]: "As dataset size NRM → ∞... θ̂ achieves a worse loss given unlimited training data and is an inconsistent estimator of θ*."
  - [corpus]: Corpus evidence for this specific mechanism is weak/missing in the provided neighbors, though related work like "Mitigating Length Bias in RLHF" addresses other RM biases.
- Break condition: This mechanism would not cause overoptimization if the RM were perfectly specified (θH ∈ Θ) and could generalize perfectly to any distribution, or if the policy never diverged from π₁.

### Mechanism 2
- Claim: Importance Weighting (IW) provides a consistent estimate of the RM parameters for the current policy without requiring new human labels.
- Mechanism: The expected RM loss under the current policy (πi) can be estimated using data from the original policy (π₁) by weighting each sample by the probability ratio of the two policies: w(s, aw, al) = πi(aw|s)πi(al|s)/π₁(aw|s)π₁(al|s). Minimizing this weighted loss on the fixed dataset yields a corrected RM that is accurate on the current policy's distribution.
- Core assumption: The support of the current policy πi is a subset of the SFT policy π₁. For LLMs with softmax outputs, π(a|s) > 0 for all a, s, satisfying this condition.
- Evidence anchors:
  - [abstract]: "...off-policy corrects the RM using importance weighting... without requiring new labels or samples."
  - [section 5]: "We will thus explain how we can obtain a consistent estimate of θ*... using importance weight w(s, aw, al) = πi(aw|s)πi(al|s)/π₁(aw|s)π₁(al|s)."
  - [corpus]: Corpus evidence on importance weighting for RLHF is weak/missing.
- Break condition: The mechanism fails if the density of the SFT policy π₁ is unknown for the training samples, or if the importance weights have extreme variance, which can destabilize training.

### Mechanism 3
- Claim: An iterative process of retraining the RM with IW and updating the policy with a shifted KL-penalty yields a superior final policy compared to standard RLHF.
- Mechanism: The OCRM method alternates between: (1) training a corrected RM Rθᵢ using IW for the current policy πᵢ, and (2) performing k PPO updates to get πᵢ₊₁, using Rθᵢ as reward and a KL-penalty against the previous policy πᵢ. This iterative correction allows the policy to explore further from the SFT model while the RM remains accurate, avoiding stagnation.
- Core assumption: The computational burden of retraining the RM can be managed by using a large k (e.g., 100k), and the variance of IW can be controlled with flattening (η) and relative weighting (α).
- Evidence anchors:
  - [abstract]: "Experiments... show OCRM outperforms standard RLHF methods, achieving higher win rates (e.g., 73.6% vs. 62.9% for PPO)."
  - [section 5.2]: "We repeat this m times and denote the policy after iteration i ∈ 1, 2,..., m as πᵢ₊₁..."
  - [section 6.2.2, Table 2]: Ablations show that both the IW-corrected RM ("Ours") and the shifted KL-penalty ("PPO + New KL") contribute to the final performance gain.
- Break condition: The benefit is reduced if the number of PPO steps k is too small, leading to poor value function estimation, or if the KL-penalty reference is not updated, restricting exploration.

## Foundational Learning

### Reinforcement Learning from Human Feedback (RLHF)
- Why needed here: The entire paper is a modification of the standard RLHF pipeline. Understanding its three stages (SFT, Reward Modeling, RL) is prerequisite to grasping the problem of overoptimization.
- Quick check question: Can you describe the three main stages of the standard RLHF pipeline?

### Distribution/Covariate Shift
- Why needed here: The paper's central thesis is that overoptimization is a form of distribution shift. One must understand how a change in data distribution (π₁ → πᵢ) can cause a model (the RM) to fail.
- Quick check question: Under what condition does a model trained on one distribution fail to generalize to a new distribution?

### Importance Sampling/Weighting
- Why needed here: This is the core mathematical tool for the proposed solution. The method's name (OCRM) and mechanism depend on re-weighting the loss to correct for distribution shift.
- Quick check question: How does importance weighting allow you to estimate an expectation under one distribution using samples from another?

## Architecture Onboarding

### Component map
SFT model -> Initial RM (Rθ₁) -> PPO (k steps) -> π₂ -> Importance-weighted RM retraining (Rθ₂) -> PPO (k steps) -> π₃ -> ... -> Final policy πₘ₊₁

### Critical path
1. Initialize policy π₁ via SFT and train initial RM Rθ₁ on D_RM
2. Run PPO for k steps to get π₂, using reward Rθ₁ and KL vs π₁
3. For each of m-1 iterations:
   a. Compute importance weights for all samples in D_RM using the current policy πᵢ
   b. Retrain a corrected RM Rθᵢ using the weighted loss. Reset value function/optimizer
   c. Run PPO for k steps to get πᵢ₊₁, using reward Rθᵢ and KL vs πᵢ

### Design tradeoffs
- **PPO steps (k)**: A high k (e.g., 100k) reduces retraining overhead and allows the value function to converge, but lets distribution shift accumulate. A low k is more responsive but computationally expensive and can be unstable.
- **KL-penalty reference**: Using πᵢ₋₁ allows the policy to move further from π₁ (better exploration) but risks drift. Using π₁ is more stable but limits improvement.
- **IW parameters (η, α)**: Flattening (η=0.001) and relative weighting (α=0.9) are crucial to control variance. Tuning them trades off bias and variance in the RM estimate.

### Failure signatures
- **Stagnant reward / instability**: Often due to too small a k, causing inaccurate value function bootstrapping
- **High variance in RM loss**: Caused by extreme importance weights. Check IW hyperparameters (η, α)
- **No improvement over PPO**: May indicate the KL-penalty reference wasn't updated, or the IW correction isn't being applied correctly

### First 3 experiments
1. **Ablation Study**: Compare (a) standard PPO, (b) PPO + shifted KL, (c) PPO + RM reset, (d) Full OCRM. This isolates the contribution of each component.
2. **Hyperparameter Search for k**: Test values like 10k, 50k, and 100k on a summarization task. Plot the KL-reward frontier to find a practical balance.
3. **Baseline Comparison**: Replicate the main results by comparing OCRM (m=3) against PPO and DPO on the TL;DR summarization task with a model like Pythia 1B, measuring win rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method be adapted for scenarios where the SFT policy density is unavailable or where preference data is collected from a mixture of policies?
- Basis in paper: [explicit] The Conclusion states that a limitation is the requirement of the density of the SFT policy π₁ and suggests it would be interesting to consider cases with data from other policies.
- Why unresolved: The current importance weighting derivation strictly relies on the ratio πᵢ / π₁, requiring explicit knowledge of π₁'s density.
- What evidence: A proposed extension using density ratio estimation or mixture models that operates without π₁'s explicit weights, validated on datasets aggregated from diverse models.

### Open Question 2
- Question: Can the instability caused by value function resets be mitigated to allow for more frequent reward model corrections (smaller k)?
- Basis in paper: [inferred] The authors note in Section 5.2 and Appendix E.3 that small k causes poor gradient estimates due to value function inaccuracy after resets, forcing them to use large k.
- Why unresolved: There is a trade-off between the accuracy of the reward model (favors small k) and the stability of the value function (favors large k) that has not been resolved.
- What evidence: Introduction of a warm-starting mechanism for the value function or optimizer that allows stable training with k << 100,000.

### Open Question 3
- Question: Does ensuring the consistency of the reward model estimator through importance weighting guarantee convergence to the global optimal policy?
- Basis in paper: [explicit] Section 5.1 states that while the method addresses the cause of inconsistency, "this does not mean that RLHF with off-policy corrected RMs will necessarily converge to an optimal policy."
- Why unresolved: The paper proves the RM loss converges to the optimal loss, but does not theoretically analyze the convergence of the resulting policy gradient updates in the non-convex RL setting.
- What evidence: A theoretical proof extending the analysis to the policy gradient or empirical convergence to known global optima in a controlled synthetic environment.

## Limitations
- Relies heavily on synthetic gold-model feedback rather than direct human evaluation for primary results
- Critical assumption that SFT policy density is known for all training samples is not explicitly verified
- Trade-off between RM accuracy (favors small k) and value function stability (favors large k) remains unresolved

## Confidence
- **High**: 73.6% win rate vs. 62.9% for PPO claim validated across multiple methods (GPT-4.1 Nano, AlpacaEval)
- **Medium**: Theoretical framework for importance weighting is sound but ablation study doesn't fully disentangle IW vs KL contributions
- **Low**: All primary results use proxy evaluators; direct human judgment validation is missing

## Next Checks
1. **Human Evaluation Validation**: Conduct direct pairwise comparisons with human raters on the TL;DR summarization task to verify the GPT-4.1 Nano and gold-model win rates translate to human preferences, particularly for the edge cases where the policy has diverged furthest from SFT.

2. **IW Variance Analysis**: Systematically vary the importance weight hyperparameters (η, α) and measure their effect on reward variance and final policy quality. Plot the trade-off curve between IW variance and KL divergence to identify optimal settings.

3. **Distribution Shift Quantification**: Measure the KL divergence between π₁ and πᵢ at each iteration and correlate it with reward model accuracy degradation and win rate improvements to empirically validate the distribution shift hypothesis.