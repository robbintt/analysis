---
ver: rpa2
title: ChatGPT for President! Presupposed content in politicians versus GPT-generated
  texts
arxiv_id: '2503.01269'
source_url: https://arxiv.org/abs/2503.01269
tags:
- politicians
- texts
- https
- discourse
- presupposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines GPT-4's ability to replicate political discourse
  strategies, focusing on presuppositions as a manipulative linguistic device. A corpus-based
  pragmatic analysis compared French and Italian political speeches with GPT-4-generated
  texts.
---

# ChatGPT for President! Presupposed content in politicians versus GPT-generated texts

## Quick Facts
- arXiv ID: 2503.01269
- Source URL: https://arxiv.org/abs/2503.01269
- Authors: Davide Garassino; Nicola Brocca; Viviana Masia
- Reference count: 21
- Key outcome: GPT-4 generates political texts with frequent but formulaic presuppositions, differing from politicians' more varied and creative use of manipulative linguistic devices.

## Executive Summary
This study compares presuppositions—linguistic devices that presuppose shared knowledge—in French and Italian political speeches versus GPT-4-generated texts. While GPT-4 uses presuppositions frequently, key differences emerge in their frequency, form, and function. Politicians employ a wider variety of presupposition triggers and discourse functions, while GPT-4 relies heavily on change-of-state verbs for stance-taking. These subtle distinctions are challenging to detect without specialized tools, highlighting potential risks of LLMs in political communication.

## Method Summary
The study conducted a corpus-based pragmatic analysis comparing 8 human political speeches (2 each from Macron, Le Pen, Meloni, Schlein) with 8 GPT-4-generated counterpart texts. Texts were annotated for potentially manipulative presuppositions (PMPs), classified by trigger type (definite descriptions, change-of-state verbs, etc.) and discourse function (criticism, self-praise, praise of others, stance-taking). The authors used zero-shot prompting via ChatGPT web interface, manual annotation with interrater reliability measurement, and statistical analysis of frequency and collocation patterns.

## Key Results
- GPT-4 generates presuppositions more frequently than politicians (0.8‰-1.7‰ vs 0.3‰-0.9‰ of words)
- GPT-4 overuses change-of-state verbs in fixed phrases, while politicians use more varied triggers
- GPT-4's presuppositions are functionally narrower, primarily serving stance-taking versus politicians' balanced use of criticism, self-praise, and praise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 over-represents change-of-state verbs (CSVs) in political text generation due to statistical salience of certain n-grams in training data.
- Mechanism: LLMs amplify patterns that appear frequently in their training corpus—specifically, verb-object combinations like "defend borders" or "build Europe" that are common in political discourse. These combinations get reproduced at higher rates than humans would naturally use them.
- Core assumption: The frequency of CSV+object n-grams in training data directly influences GPT's tendency to use them preferentially over other presupposition triggers.
- Evidence anchors:
  - [abstract] "ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians use presupposition triggers in more varied and creative ways"
  - [section 5.2] Table 6 shows CSV frequency in GPT corpus ranges from 0.9‰ to 2.5‰ versus 0.1‰ to 0.9‰ in politician speeches
  - [corpus] Neighboring papers on political bias detection in LLMs suggest similar pattern-reproduction behaviors, but no direct corpus evidence confirms the training data hypothesis
- Break condition: If CSV overuse disappears when prompting with temperature=0 or with chain-of-thought prompting, the mechanism may be prompt-sensitive rather than architecture-inherent

### Mechanism 2
- Claim: LLMs process presuppositions via surface-level lexical cues rather than modeling genuine pragmatic inference.
- Mechanism: GPT identifies presupposition triggers through pattern matching on trigger words (e.g., "the," "stop," "continue") without building a mental model of what content should be taken as shared knowledge. This explains formulaic output lacking the strategic creativity humans display.
- Core assumption: LLM success on presupposition-related tasks correlates with lexical cue recognition, not contextual appropriateness.
- Evidence anchors:
  - [section 2.3] Reviews Kabbara & Cheung (2022): models relied on surface-level lexical cues like "no one" rather than "pragmatic understanding"
  - [section 2.3] Sravanthi et al. (2024) found even best models "tended to rely on superficial linguistic forms instead of genuine pragmatic reasoning"
  - [corpus] Related work on LLM pragmatic understanding shows similar surface-level processing patterns
- Break condition: If fine-tuning on explicit pragmatic reasoning tasks (e.g., chain-of-thought prompting) changes CSV distribution patterns, the mechanism may be training-data addressable

### Mechanism 3
- Claim: GPT narrows the functional range of presuppositions to stance-taking (STK), missing the broader communicative functions politicians employ.
- Mechanism: Because CSV-heavy phrases are statistically associated with STK in training data, GPT's pattern-matching approach produces functionally narrower output—primarily statements of values and positions—while politicians also use presuppositions for criticism, self-praise, and praise of others.
- Core assumption: The STK function is over-represented in whatever political training data GPT absorbed, or STK-linked phrases have stronger statistical associations.
- Evidence anchors:
  - [section 5.1] Fig. 4-5 show STK dominates GPT data (~60-70%) while politician data shows balanced CRT/STK distribution
  - [abstract] "key differences emerged in their frequency, form, and function"
  - [corpus] No direct corpus evidence on functional distribution in training data—this remains an inference
- Break condition: If explicit functional-diversity prompts (e.g., "include criticism of opponents") broaden the functional range, the limitation is prompt-addressable rather than architectural

## Foundational Learning

- Concept: **Presupposition triggers** (definite descriptions, change-of-state verbs, iterative adverbs)
  - Why needed here: The entire analysis framework depends on identifying specific lexical items that signal presupposed content. Without understanding that "the Euromonster" presupposes existence, or "continue" presupposes prior action, you cannot interpret the study's coding scheme or results.
  - Quick check question: In "We must rebuild our economy," what is presupposed and what trigger signals it?

- Concept: **Corpus-based pragmatic annotation with interrater reliability**
  - Why needed here: The study's claims rest on human annotators agreeing on what counts as a potentially manipulative presupposition and its discourse function. Cohen's k values of 0.43-0.97 show where subjectivity creeps in—you need to understand why "discourse function" is harder to agree on than "trigger type."
  - Quick check question: Why would two annotators agree on a trigger (e.g., "definite description") but disagree on its function (e.g., criticism vs. stance-taking)?

- Concept: **N-gram statistical amplification in LLMs**
  - Why needed here: The core finding—that GPT overuses CSVs in formulaic combinations—requires understanding that LLMs are probabilistic next-token predictors that amplify frequent patterns. This explains why GPT's output feels "political" but lacks strategic variation.
  - Quick check question: If "defend our values" appears 1000x more often than "defend the proposition" in political training data, what would you expect GPT to generate when prompted to write a conservative speech?

## Architecture Onboarding

- Component map: Prompt (persona + topic + language) → GPT-4 web interface → Raw output → Manual annotation (PMP identification → trigger coding → function coding) → Consensus negotiation → Final dataset → Frequency/functional analysis

- Critical path:
  1. **Prompt design**—persona pattern with specific politician, context, topic, and language determines output baseline
  2. **Length matching**—politician texts trimmed to GPT output length to enable fair frequency comparison
  3. **Annotation protocol**—two-phase process: first identify PMPs (consensus), then code triggers/functions (measure k)
  4. **Qualitative pattern extraction**—manual analysis of verb-argument associations per politician persona

- Design tradeoffs:
  - **Web interface vs. API**: Authors chose web interface for ecological validity (how users actually interact), but lost control of temperature/parameters—replicability suffers
  - **Zero-shot prompting**: Simple but may underperform chain-of-thought; authors note this limitation explicitly
  - **Small corpus (16 texts)**: Enables deep pragmatic annotation but limits statistical power; normalized frequencies help but n remains low
  - **English prompts for French/Italian output**: Avoids introducing prompt-language as confound, but may influence output quality in lower-resource languages

- Failure signatures:
  - **CSV overuse without functional diversity**: If your GPT output shows 60%+ STK function with CSV triggers clustered in formulaic phrases, you're seeing the pattern this paper documents
  - **Disagreement on discourse function**: If annotators achieve k < 0.6 on function coding, the functional categories may need refinement or examples
  - **English interference**: Watch for calques or unnatural phrasing in non-English outputs—mentioned in §2.1 literature review as known issue

- First 3 experiments:
  1. **Replicate with temperature control**: Run the same prompts via API with temperature settings [0.0, 0.7, 1.0] to test whether CSV overuse is robust or prompt-sensitive
  2. **Functional-diversity prompting**: Add explicit instructions ("Include criticism of political opponents") to test whether functional narrowing is addressable via prompt engineering
  3. **Cross-model comparison**: Run identical prompts through GPT-4, Claude, and open-source models (Llama-2-70B) to determine if CSV amplification is GPT-specific or a general LLM property in political text generation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs be effectively fine-tuned or prompted to automatically detect and mitigate potentially manipulative presuppositions (PMPs) in political discourse?
- **Basis in paper:** [explicit] The abstract and conclusion explicitly call for "research into LLMs' ability to detect and mitigate manipulative language" to address the risks identified.
- **Why unresolved:** While the study establishes that LLMs generate manipulative content, the reverse capability—using LLMs as defensive tools to identify such content—remains untested for this specific pragmatic phenomenon.
- **What evidence would resolve it:** Experiments testing LLM performance on identifying and flagging PMPs within human and AI-generated political corpora.

### Open Question 2
- **Question:** How reliable are LLMs in automating the complex, labor-intensive manual annotation of pragmatic categories like presupposition triggers and discourse functions?
- **Basis in paper:** [explicit] The conclusion identifies reducing the annotation workload via AI assistance as a "key desideratum" to overcome the data size limitations faced in this study.
- **Why unresolved:** The current study relied on manual annotation ("consensus through negotiation") due to the subjectivity of pragmatic categories; the feasibility of automating this specific nuanced task is not yet confirmed.
- **What evidence would resolve it:** Comparative benchmarking of LLM annotation accuracy against the human interrater reliability scores (Cohen's k) established in this paper.

### Open Question 3
- **Question:** Do alternative prompting strategies, such as chain-of-thought prompting, reduce the formulaic reliance on change-of-state verbs and increase functional variety in generated political texts?
- **Basis in paper:** [inferred] The limitations section notes the study relied on zero-shot prompting, while the literature review suggests advanced strategies like chain-of-thought prompting yield higher-quality outputs.
- **Why unresolved:** It is unclear if the observed repetition of specific n-grams (e.g., "defend values") is an inherent constraint of the model's architecture or a byproduct of the specific zero-shot prompt design used.
- **What evidence would resolve it:** A comparative analysis of presupposition triggers and discourse functions in texts generated via zero-shot versus chain-of-thought prompts.

## Limitations
- Small corpus size (16 texts) limits generalizability to broader political discourse
- Use of ChatGPT web interface prevents precise control over sampling parameters
- Moderate to low interrater reliability (k = 0.43-0.64) for discourse function annotation
- Mechanism behind CSV overuse is inferential rather than empirically confirmed

## Confidence
- **High confidence**: The core empirical finding that GPT-4 and politicians differ in PMP frequency, form, and function is well-supported by the annotated corpus data and statistical analysis.
- **Medium confidence**: The interpretation that GPT-4's CSV overuse stems from statistical amplification in training data is plausible but not directly verified.
- **Medium confidence**: The claim that functional narrowing to STK reflects LLM pattern-matching rather than pragmatic reasoning is consistent with related literature but requires further testing.

## Next Checks
1. **Prompt-sensitivity test**: Replicate the study using GPT-4 API with controlled temperature and sampling parameters to determine whether CSV overuse persists across settings or is prompt-dependent.
2. **Fine-tuning intervention**: Test whether explicit chain-of-thought prompting or pragmatic reasoning instructions alter the distribution of presupposition triggers and functions in GPT-4 output.
3. **Cross-linguistic robustness**: Expand the corpus to include additional languages and political contexts to assess whether observed differences generalize beyond French and Italian discourse on European issues.