---
ver: rpa2
title: 'DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging'
arxiv_id: '2508.01148'
source_url: https://arxiv.org/abs/2508.01148
tags:
- task
- merging
- vector
- proj
- distac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two failure modes in model merging: (1)
  differences in task vector norms and (2) low confidence of source models. To address
  these issues, the authors propose DisTaC, a knowledge distillation-based pre-conditioning
  method that harmonizes task vector norms and increases source model confidence before
  merging.'
---

# DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging

## Quick Facts
- arXiv ID: 2508.01148
- Source URL: https://arxiv.org/abs/2508.01148
- Reference count: 40
- Primary result: Knowledge distillation-based pre-conditioning method that significantly improves model merging robustness by addressing task vector norm disparities and low source confidence

## Executive Summary
DisTaC addresses two key failure modes in model merging: task vector norm disparities and low source model confidence. The method uses knowledge distillation with unlabeled data to harmonize vector norms and increase source model confidence before merging. By shrinking high-norm vectors and using asymmetric temperature scaling during distillation, DisTaC prevents dominant vectors from distorting the merged direction while improving merge robustness even with low-confidence sources.

## Method Summary
DisTaC operates by first analyzing task vector norms and scaling down large vectors to match smaller ones. It then initializes student models at these scaled vectors and performs knowledge distillation using unlabeled data, with higher student temperature than teacher temperature to increase confidence. An ℓ2 regularization term keeps the student close to the scaled anchor point. The conditioned models are then merged using standard arithmetic operations, resulting in significantly improved accuracy and normalized accuracy compared to unconditioned merging.

## Key Results
- Up to 20.8 percentage points absolute accuracy gain on challenging merging scenarios
- Normalized accuracy restored from 68% to 92% under low-confidence conditions
- Significant improvements across eight vision tasks using ViT-B-32/L-14 backbones
- Effective in scenarios where state-of-the-art merging techniques fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Harmonizing task vector norms prevents high-magnitude vectors from dominating the merged direction, recovering performance lost during naive scaling.
- **Mechanism:** DisTaC rescales large task vectors to match the norm of smaller vectors, then uses knowledge distillation to recover lost accuracy. The KD process re-learns task knowledge into the smaller vector footprint without re-introducing destructive interference scale.
- **Core assumption:** The merged model performance relies on directional alignment of task vectors; high-norm vectors distort this alignment, and shrinking is less destructive than stretching.
- **Evidence anchors:**
  - [Section 3.1]: Shows that if ||τ₂|| >> ||τ₁||, the cosine similarity of the merged vector to τ₂ approaches 1, ignoring the smaller task.
  - [Section 6.1]: Empirical analysis demonstrates that shrinking vectors retains performance better than stretching them to match a larger norm.
  - [corpus]: Related work (e.g., CAT Merging) addresses conflicts during merge; DisTaC differs by pre-conditioning source norms to prevent scale-based dominance.
- **Break condition:** If task vectors are orthogonal and norms are balanced, this mechanism provides minimal gain and adds unnecessary compute.

### Mechanism 2
- **Claim:** Increasing source model confidence via temperature scaling in KD improves merge robustness, even if it temporarily hurts calibration.
- **Mechanism:** DisTaC sets student temperature higher than teacher temperature during distillation, forcing sharper probability distributions. The paper argues that current merging methods are brittle to low-confidence sources and perform best when sources are over-confident.
- **Core assumption:** Post-hoc calibration can effectively fix over-confidence in the final merged model, but merging cannot fix under-confidence.
- **Evidence anchors:**
  - [Section 3.2]: Identifies "Low Confidence" as a failure mode where normalized accuracy drops by up to 24%.
  - [Section 4.2]: Describes asymmetric temperature setup to push student toward lower-entropy outputs.
  - [Figure 4]: Demonstrates inverse correlation between label smoothing strength and merge performance.
- **Break condition:** If downstream application strictly prohibits over-confident models and post-hoc calibration is infeasible, this mechanism may introduce undesirable behavioral traits.

### Mechanism 3
- **Claim:** Unlabeled data is sufficient to condition task vectors because optimization is anchored to weight initialization rather than ground truth labels.
- **Mechanism:** The method initializes student model at rescaled vector θ_pre + κτ and uses ℓ2 regularization to keep student close to this anchor while minimizing KL divergence against teacher on unlabeled data.
- **Core assumption:** Essential task-specific knowledge is already encoded in weights and can be redistributed via soft targets without requiring hard labels.
- **Evidence anchors:**
  - [Algorithm 1]: Explicitly lists θ₀ ← θ_pre + κτ as anchor and includes ℓ2 penalty.
  - [Section 4.1]: States method relies solely on unlabeled data, using soft-target distillation only (ζ=1).
  - [corpus]: Weak direct evidence in neighbors for this specific unlabeled constraint, though standard KD often uses labels; DisTaC's specific contribution is anchor-and-distill loop without them.
- **Break condition:** If unlabeled data distribution differs significantly from task training distribution, KL divergence may distort task vector rather than condition it.

## Foundational Learning

- **Concept:** Task Arithmetic & Vector Norms
  - **Why needed here:** DisTaC operates entirely on premise of "Task Vectors" (τ = θ_tuned - θ_pre). Understanding that magnitude of this vector affects interference is prerequisite to grasping why DisTaC rescales them.
  - **Quick check question:** If you double the learning rate during fine-tuning, how does the task vector norm typically change, and why might that harm a linear merge?

- **Concept:** Knowledge Distillation (KD) Temperature
  - **Why needed here:** Core innovation uses asymmetric temperatures (T_stu ≠ T_tcr). Must understand that higher temperatures soften probability distributions (increase entropy), while lower temperatures sharpen them, to see why T_stu > T_tcr makes student "more confident."
  - **Quick check question:** Does raising the temperature T in a softmax make the output distribution flatter or sharper?

- **Concept:** Model Calibration vs. Confidence
  - **Why needed here:** Paper highlights trade-off: well-calibrated models (probabilities match accuracy) often merge poorly. DisTaC deliberately breaks calibration (making models over-confident) to save merge, relying on post-hoc fixes.
  - **Quick check question:** If a model predicts "Cat" with 90% probability but is only correct 50% of the time, is it over-confident or under-confident?

## Architecture Onboarding

- **Component map:** Input: Pre-trained model (θ_pre), Fine-tuned models (θ_t), Unlabeled proxy data (D̃_u) -> Pre-processor: Norm Analyzer (calculates κ scaling factors) -> Core Engine (DisTaC Loop) -> Output: Conditioned models -> Standard Merger (e.g., TSVM, TIES)

- **Critical path:**
  1. Calculate norms of all input task vectors
  2. Identify target norm (paper suggests shrinking large vectors to mean of rest)
  3. **Anchor:** Initialize student with scaled weights
  4. **Distill:** Run KD loop (e.g., 500 steps) with T_stu > T_tcr to sharpen confidence and recover accuracy
  5. Merge conditioned models using standard arithmetic

- **Design tradeoffs:**
  - **Shrinking vs. Stretching:** DisTaC prefers shrinking large vectors. Stretching small vectors often degrades performance more than shrinking large ones (Section 6.1)
  - **Calibration vs. Accuracy:** DisTaC sacrifices calibration of source models to ensure high merge accuracy. Must plan to recalibrate final merged model if reliable probabilities are needed

- **Failure signatures:**
  - **Entropy Drift:** If KD steps are too high without ℓ2 regularization, task vector norm may drift back to harmful magnitudes
  - **Unlabeled Skew:** If unlabeled data is not representative, "dark knowledge" distilled may be irrelevant, degrading single-task accuracy
  - **Dominant Tasks:** Even after conditioning, if one task is fundamentally more complex, "consensus" masking in downstream mergers might still suppress it

- **First 3 experiments:**
  1. **Baseline Failure Reproduction:** Train two CLIP models on different tasks with vastly different learning rates (e.g., 1e-5 vs 1e-4). Verify that simple merging fails (performance drop)
  2. **DisTaC Norm Recovery:** Apply DisTaC only to high-norm model from Experiment 1. Verify if post-merge accuracy recovers to "Original" baseline level (Table 1)
  3. **Confidence Ablation:** Train models with Label Smoothing (low confidence). Apply DisTaC with high student temperature. Plot shift in prediction entropy (Figure 2b) and check merge accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond identifying failure modes and proposing solutions. The identified failure modes (norm disparities and low confidence) are presented as problems that DisTaC addresses rather than open research questions.

## Limitations

- **Unlabeled Data Dependency:** Method's effectiveness relies on availability of unlabeled data representative of task distribution; significant distribution shift can degrade knowledge preservation during conditioning
- **Calibration Trade-off:** Intentionally produces over-confident source models requiring post-hoc calibration, adding deployment complexity when reliable probability estimates are needed throughout
- **Computational Overhead:** While cheaper than full fine-tuning, still requires additional distillation steps (e.g., 500 steps) per task vector, adding overhead compared to direct merging

## Confidence

**High Confidence:** Identification of norm disparities as failure mode and basic mechanism of vector scaling have strong empirical support from Section 6.1. Asymmetric temperature approach for improving confidence is well-grounded in knowledge distillation literature.

**Medium Confidence:** Claim that unlabeled data alone is sufficient for effective conditioning assumes soft targets contain all necessary task knowledge. While plausible, this is less directly validated than norm-scaling claims.

**Medium Confidence:** Specific hyperparameters (500 steps, β=0.5, temperature settings) are presented as effective but may require tuning for different model architectures or task domains.

## Next Checks

1. **Distribution Shift Sensitivity:** Test DisTaC when unlabeled proxy data distribution differs from actual task distribution (e.g., using CIFAR-10 unlabeled data for conditioning models trained on ImageNet tasks). Measure degradation in single-task accuracy post-conditioning.

2. **Calibration Impact Assessment:** After applying DisTaC and merging, measure calibration error (e.g., Expected Calibration Error) of final model. Verify whether post-hoc temperature scaling can effectively restore calibration without significantly degrading accuracy.

3. **Computational Efficiency Comparison:** Benchmark DisTaC's total runtime (including distillation steps) against alternative approaches like re-fine-tuning or using more sophisticated merging methods like CAT Merging. Quantify trade-off between accuracy gain and computational cost.