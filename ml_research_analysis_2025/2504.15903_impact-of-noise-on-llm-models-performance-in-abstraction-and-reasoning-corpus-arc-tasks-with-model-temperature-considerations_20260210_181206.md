---
ver: rpa2
title: Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus
  (ARC) Tasks with Model Temperature Considerations
arxiv_id: '2504.15903'
source_url: https://arxiv.org/abs/2504.15903
tags:
- uni00000003
- uni00000013
- uni00000048
- uni00000057
- uni00000053
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically investigates the impact of noise on\
  \ the performance of large language models (LLMs) in solving tasks from the Abstraction\
  \ and Reasoning Corpus (ARC). Three models\u2014GPT-4o, DeepSeek R1, and LLaMA 3.2\u2014\
  were evaluated under varying noise levels and temperature settings using a controlled\
  \ experimental design that introduced noise into input and output grids while measuring\
  \ model accuracy across 2-shot and 3-shot learning examples."
---

# Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations

## Quick Facts
- arXiv ID: 2504.15903
- Source URL: https://arxiv.org/abs/2504.15903
- Reference count: 40
- Three models (GPT-4o, DeepSeek R1, LLaMA 3.2) show significant performance degradation with increased noise levels

## Executive Summary
This study systematically investigates how noise impacts large language model performance on ARC tasks, revealing critical vulnerabilities in current architectures. The research demonstrates that even minimal noise (0.05%) causes substantial accuracy drops across GPT-4o, DeepSeek R1, and LLaMA 3.2, with GPT-4o showing extreme sensitivity. Model temperature settings significantly influence robustness, where lower temperatures (0.0) provide more stable predictions while higher temperatures (1.0) increase variability and reduce accuracy. These findings highlight the gap between language and abstract reasoning capabilities in LLMs, emphasizing the need for architectures that can handle uncertainty and noise while maintaining pattern abstraction.

## Method Summary
The study evaluates three LLMs on ARC tasks using controlled noise injection into input and output grids at varying levels (0.05% to 50%). Models are tested with 2-shot and 3-shot prompting using both "Original Prompt" and "New Prompt" variations that explicitly warn about noise presence. Performance is measured through accuracy comparisons between predicted and ground truth output grids. The experimental design systematically varies noise levels and temperature settings (0.0 to 1.0) to assess their combined impact on model robustness, with particular attention to how models handle structured reasoning tasks under imperfect conditions.

## Key Results
- All three models show significant accuracy degradation as noise levels increase from 0% to 50%
- GPT-4o exhibits extreme sensitivity, with performance disruption occurring even at 0.05% noise levels
- Lower temperature settings (0.0) produce more stable and accurate predictions compared to higher temperatures (1.0)
- Performance varies substantially across models, with DeepSeek R1 showing better noise resilience than GPT-4o
- Model temperature critically affects noise sensitivity, with higher temperatures amplifying the negative impact of noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs treat random noise in few-shot examples as relevant features for rule induction
- Mechanism: Noise disrupts attention mechanisms by causing models to attend to random pixels as potential "exceptions" or "special rules," breaking extraction of general grid transformation logic
- Core assumption: Models lack inherent noise filters to distinguish core structural patterns from random pixel artifacts
- Evidence anchors: High sensitivity to noise in input-output pairs [abstract], performance degradation with imperfect data [section 6], robustness as key developmental challenge [corpus]

### Mechanism 2
- Claim: Explicit prompt instructions can mitigate noise sensitivity by contextualizing noise as distractors
- Mechanism: Warning prompts ("Note that random noise has been added...") prime models to gate attention and look for invariance rather than pixel-perfect matches
- Core assumption: Models have sufficient instruction-following fine-tuning to apply "ignore noise" as logical constraint
- Evidence anchors: "New Prompt" section explicitly adds noise warning [appendix], discrepancy in structured reasoning capabilities [section 6], reasoning-oriented LLMs handling constraints [corpus]

### Mechanism 3
- Claim: Training with structured noise can improve model resilience by forcing invariant feature learning
- Mechanism: Multiple noisy inputs mapping to same clean output forces network representations to collapse variations into single latent representation
- Core assumption: Architectures can learn invariance through gradient descent without overfitting to specific noise patterns
- Evidence anchors: Proposing multiple noisy outputs per input [section 7], advancing AI models through this approach [abstract], efficient program synthesis requiring detail filtering [corpus]

## Foundational Learning

Concept: Few-Shot In-Context Learning (ICL)
- Why needed here: Entire evaluation relies on 2-shot and 3-shot prompting without weight updates
- Quick check question: How does performance scale as shots increase from 2 to 3, and how does noise disrupt this scaling?

Concept: Grid-Based Visual Reasoning
- Why needed here: ARC tasks involve spatial relationships, objectness, and geometry encoded as text/arrays, distinct from natural language
- Quick check question: Does model treat grid as sequence of tokens or 2D spatial structure, and how does noise break spatial coherence?

Concept: Robustness vs. Sensitivity
- Why needed here: Study measures sensitivity; must distinguish "sensitivity to noise" (bad) from "sensitivity to pixel changes" (necessary for detecting transformations)
- Quick check question: If task involves "changing one pixel," how distinguish signal detection from noise reaction?

## Architecture Onboarding

Component map: Noise Injector -> Prompt Constructor -> Evaluator
Critical path: Noise Injector logic is most critical experimental variable; noise must be structured enough to simulate real-world dirt but random enough not to form secondary patterns
Design tradeoffs: Text-based representation (used here) vs. Vision-based input; text tokens might fragment grid structure making noise more disruptive to local attention than Vision Transformer
Failure signatures: Model generates verbose explanations justifying random noise pixels as part of rule (e.g., "The rule applies to red pixel except when next to random blue dot")
First 3 experiments:
1. Baseline Sweep: Run 2-shot clean vs. 2-shot noisy (10% noise) on subset of ARC tasks to confirm performance drop
2. Prompt Ablation: Compare "New Prompt" (warning about noise) vs. "Old Prompt" (no warning) on same noisy data
3. Noise Scaling: Plot performance curves as noise levels increase from 0% to 50% to identify breaking point of reasoning capability

## Open Questions the Paper Calls Out

Open Question 1: What specific architectural or reasoning mechanisms allow Claude 3.5 Sonnet to solve ARC tasks while GPT-4o and Gemini 1.5 Pro fail completely under identical zero-noise 3-shot conditions?
- Basis in paper: Study notes Claude 3.5 Sonnet solved tasks while "GPT-4o and Gemini 1.5 Pro failed to solve any tasks" in 3-shot zero-noise scenarios
- Why unresolved: Paper identifies performance discrepancy but doesn't analyze internal reasoning traces or architectural differences
- What evidence would resolve it: Comparative study of attention heads and intermediate reasoning states across models during identical task evaluations

Open Question 2: How does structured noise propagate through specific activation layers to distort pattern recognition in LLMs?
- Basis in paper: Authors state future goal to "explore its impact on internal representations" by analyzing "how noise propagates through different activation layers"
- Why unresolved: Current research evaluates models as black boxes regarding internal noise handling, focusing only on input-output accuracy
- What evidence would resolve it: Layer-wise relevance propagation (LRP) or activation patching experiments visualizing noise distortion in hidden states

Open Question 3: Can systematic exposure to structured noise in k-shot examples fundamentally improve model resilience to uncertainty in reasoning tasks?
- Basis in paper: "Future Work" section proposes assessing "whether controlled exposure to such noise can improve the model's resilience in uncertain environments"
- Why unresolved: This is hypothesis for future research; current study only observes negative impact of noise, not potential for training adaptation
- What evidence would resolve it: Comparing performance metrics of models fine-tuned with noisy few-shot examples against baseline models on new, noisy tasks

## Limitations

- Noise injection methodology lacks precise mathematical specification, making exact replication challenging
- Evaluation methodology for "multiple noisy outputs" corresponding to single inputs remains underspecified
- Proposed architectural solutions (structured noise training, instruction-following mitigation) lack empirical validation within this study

## Confidence

- **High Confidence**: Core finding that GPT-4o shows extreme sensitivity to minimal noise (0.05%) disrupting pattern abstraction is well-supported
- **Medium Confidence**: Temperature sensitivity results (lower temperatures yielding more stable predictions) are robust but mechanism requires further investigation
- **Low Confidence**: Proposed architectural solutions are conceptually sound but remain speculative directions without empirical validation

## Next Checks

1. **Noise Specification Validation**: Implement and test three different noise injection strategies (per-cell probability, fixed count, region-specific) to determine which best reproduces reported performance degradation patterns
2. **Temperature Interaction Analysis**: Conduct factorial experiment varying both temperature (0.0, 0.5, 1.0) and noise levels (0%, 5%, 10%, 20%) to map interaction surface and identify optimal operating conditions
3. **Cross-Model Generalization**: Test same noise and temperature protocol across additional LLM architectures (Claude, Gemini, open-source alternatives) to determine whether GPT-4o's extreme sensitivity is architecture-specific or represents broader class vulnerability