---
ver: rpa2
title: 'From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based
  Diffusion Models through Oracle Velocity'
arxiv_id: '2512.02826'
source_url: https://arxiv.org/abs/2512.02826
tags:
- training
- stage
- diffusion
- data
- oracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the memorization-generalization trade-off
  in flow-based diffusion models by deriving a closed-form oracle velocity field under
  Gaussian priors and finite datasets. The analysis reveals that the training target
  inherently exhibits a two-stage structure: an early navigation stage guided by a
  mixture of data modes and a later refinement stage dominated by the nearest data
  sample.'
---

# From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity

## Quick Facts
- **arXiv ID**: 2512.02826
- **Source URL**: https://arxiv.org/abs/2512.02826
- **Reference count**: 40
- **Key outcome**: Flow-based diffusion models exhibit a two-stage training structure where early navigation stage drives generalization while late refinement stage drives memorization, with model capacity primarily improving refinement performance.

## Executive Summary
This paper investigates the memorization-generalization trade-off in flow-based diffusion models by deriving a closed-form oracle velocity field under Gaussian priors and finite datasets. The analysis reveals that the training target inherently exhibits a two-stage structure: an early navigation stage guided by a mixture of data modes and a later refinement stage dominated by the nearest data sample. Empirical results show that generalization primarily stems from the navigation stage while memorization arises in the refinement stage. The study also demonstrates that model capacity and training compute mainly benefit refinement performance, with navigation stage performance remaining largely unchanged across model sizes.

## Method Summary
The authors derive a closed-form oracle velocity field u*_t(x_t,t) = A_t·Σ_i γ_i(x_t,t)·x^(i)_1 + B_t·x_t, where γ_i represents posterior weights computed via softmax over squared distances in latent space. They train LightningDiT models on ImageNet latents (VA-VAE or SD-VAE encoded) using rectified flow with (α_t, σ_t) = (t, 1-t). The training uses AdamW optimizer with EMA, batch size 512, and 100-800 epochs. They evaluate performance using gFID@50K and DINO loss for structural similarity to nearest training samples. Mixed sampling experiments combine oracle velocity (early stages) with model velocity (later stages) to validate the two-stage hypothesis.

## Key Results
- The oracle velocity naturally exhibits a two-stage structure: navigation stage (t < 0.1) dominated by mixture of modes, refinement stage (t > 0.1) dominated by nearest sample
- Model capacity and training compute primarily improve refinement stage performance, with navigation stage performance remaining constant across model sizes
- Early switching to oracle velocity during sampling produces novel samples, while late switching reproduces training images, validating the stage characterization
- Generalization primarily arises from the navigation stage, while memorization is an inherent property of the refinement stage

## Why This Works (Mechanism)
The closed-form oracle velocity reveals that flow-based diffusion models must navigate between data modes early in sampling before refining toward specific training samples. This creates an inherent two-stage optimization problem where the early stage requires balancing multiple modes while the late stage focuses on precise reconstruction. The navigation stage benefits from larger model capacity to represent complex mode structures, while the refinement stage benefits from additional training compute to improve reconstruction accuracy of individual samples.

## Foundational Learning
- **Rectified Flow**: A training framework that optimizes flow matching between source and target distributions by parameterizing the flow as a time-dependent velocity field. Needed to understand how the oracle velocity relates to practical training objectives. Quick check: Verify that (α_t, σ_t) = (t, 1-t) satisfies the rectified flow parameterization.
- **Flow Matching Loss**: The objective function that minimizes the expected squared difference between model and oracle velocity fields across timesteps. Needed to understand how models learn to approximate the oracle velocity. Quick check: Confirm that the CFM loss implementation matches the theoretical formulation.
- **Posterior Weight Computation**: The softmax-normalized weights γ_i that determine the contribution of each training sample to the oracle velocity at a given point. Needed to understand how the oracle velocity represents the data distribution. Quick check: Validate that Σ_i γ_i = 1 for all (x_t, t).
- **Latent Space Encoding**: The transformation of images into latent representations using VA-VAE or SD-VAE before training flow models. Needed to understand the dimensionality and structure assumptions in the oracle derivation. Quick check: Verify latent dimensions (16×16) and normalization in preprocessing.
- **Velocity Field Interpolation**: The mixed sampling approach that combines oracle and model velocities at different timesteps. Needed to experimentally validate the two-stage hypothesis. Quick check: Confirm smooth transition between oracle and model velocities in mixed sampling.
- **Structural Similarity Metrics**: DINO loss and gFID used to evaluate sample quality and memorization. Needed to quantify the difference between novel and training-like samples. Quick check: Compare DINO loss distributions between training samples and generated samples.

## Architecture Onboarding

**Component Map:**
ImageNet → VA-VAE/SD-VAE Encoder → LightningDiT (Flow Model) → Velocity Field → Sample Decoder → Generated Images

**Critical Path:**
Training: Data Encoding → Oracle Velocity Computation → CFM Loss → Model Update → Inference: Sampling (Euler, 50 steps) → Latent Decoding → Image Output

**Design Tradeoffs:**
- Latent space dimensionality vs. computational efficiency: Higher dimensions enable more expressive navigation but increase memory requirements
- Timestep resolution vs. training stability: Finer timesteps capture more detail but may require smaller learning rates
- Model depth vs. refinement capability: Deeper models improve reconstruction accuracy but don't affect navigation stage performance

**Failure Signatures:**
- Oracle velocity computation OOM: Reduce class subset size or use approximate nearest neighbors
- Stage transition not observed at t≈0.1: Check latent space normalization and sample size effects
- Mixed sampling produces artifacts: Verify proper interpolation between oracle and model velocities

**First Experiments:**
1. Implement oracle velocity computation and validate MSE vs CFM target shows divergence in [0.0,0.1]
2. Train LightningDiT-B on ImageNet latents and log oracle MSE by timestep
3. Run mixed sampling with multiple switch points and quantify DINO loss vs nearest neighbor

## Open Questions the Paper Calls Out
None

## Limitations
- The closed-form oracle derivation assumes Gaussian priors and finite datasets, which may not generalize to complex real-world distributions
- The analysis relies on specific architectural choices (LightningDiT) and latent space encodings whose properties may differ from other implementations
- The stage transition point (t≈0.1) appears sensitive to latent dimensionality and sample size, but this dependency is not systematically explored

## Confidence

**High confidence:**
- The existence of two distinct stages in oracle velocity (navigation vs refinement) is well-supported by both theoretical derivation and empirical validation

**Medium confidence:**
- The generalization-memorization attribution to specific stages is logically sound but relies on DINO loss as a proxy for "novelty"
- The explanation for why timestep shifting and CFG intervals work is compelling but could benefit from additional ablation studies

**Low confidence:**
- The universality of t≈0.1 as the transition point across different latent spaces and data complexities is not thoroughly validated

## Next Checks

1. Replicate the stage transition observation across multiple latent space configurations (different VA-VAE/SD-VAE architectures and dimensionalities) to verify that t≈0.1 is a robust threshold
2. Conduct controlled experiments varying dataset size N while holding D constant to quantify how the transition point and stage characteristics scale with data complexity
3. Implement the mixed sampling protocol (oracle → model velocity) with multiple switch points and validate that early switches consistently produce novel samples while late switches reproduce training data, using both DINO loss and human perceptual evaluation