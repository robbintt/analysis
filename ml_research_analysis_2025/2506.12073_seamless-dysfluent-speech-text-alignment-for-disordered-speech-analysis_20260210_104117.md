---
ver: rpa2
title: Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis
arxiv_id: '2506.12073'
source_url: https://arxiv.org/abs/2506.12073
tags:
- speech
- alignment
- dysfluent
- neural
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately aligning dysfluent
  speech with intended text for automated diagnosis of neurodegenerative speech disorders
  like primary progressive aphasia (PPA). The proposed Neural LCS method improves
  upon traditional alignment approaches by leveraging robust phoneme-level modeling
  and context-aware similarity mapping.
---

# Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis

## Quick Facts
- arXiv ID: 2506.12073
- Source URL: https://arxiv.org/abs/2506.12073
- Reference count: 0
- Primary result: Neural LCS improves dysfluent speech alignment accuracy to 72.55% (phoneme level) and 68.44% (word level), outperforming DTW and Hard LCS baselines

## Executive Summary
This paper addresses the challenge of accurately aligning dysfluent speech with intended text for automated diagnosis of neurodegenerative speech disorders like primary progressive aphasia (PPA). The proposed Neural LCS method improves upon traditional alignment approaches by leveraging robust phoneme-level modeling and context-aware similarity mapping. Neural LCS uses a siamese neural network architecture with T5-based feature encoding and focal loss training to handle partial alignments and similar phoneme mapping. The method was evaluated on both simulated text-text data and LLM-generated text-speech data, showing significant improvements over DTW and Hard LCS baselines with alignment accuracy reaching 72.55% (phoneme level) and 68.44% (word level) on text-text data. For speech-text alignment, Neural LCS achieved boundary loss metrics of 10ms (repetition), 8ms (substitution), and 23ms (insertion) on simulated data, outperforming YOLO-Stutter baseline. The method successfully detects and segments various dysfluency types including repetition, deletion, substitution, and insertion, providing a more accurate and linguistically grounded solution for dysfluent speech alignment.

## Method Summary
Neural LCS addresses dysfluent speech alignment through a two-stage pipeline. First, text-to-text alignment uses a siamese neural network with shared T5-based feature encoding to map dysfluent text to intended reference text at phoneme or word level. The model employs focal loss to handle class imbalance between aligned and dysfluent tokens. Second, for speech-to-text alignment, a cascaded approach uses wav2vec 2.0 feature extraction with CTC decoding to obtain dysfluent phoneme sequences, followed by Neural LCS alignment to reference text. The method was trained on simulated data generated by injecting dysfluencies into VCTK text corpus, with evaluation on both simulated text-text data and LLM-generated text-speech data.

## Key Results
- Neural LCS achieves 72.55% phoneme-level alignment accuracy and 68.44% word-level accuracy on text-text data
- Outperforms DTW and Hard LCS baselines by ~48 percentage points on text-text alignment
- Speech-text alignment shows boundary loss of 10ms (repetition), 8ms (substitution), and 23ms (insertion) on simulated data
- Successfully detects and segments repetition, deletion, substitution, and insertion dysfluency types
- Deletion detection underperforms other types (27ms boundary loss vs 8-10ms for others)

## Why This Works (Mechanism)

### Mechanism 1: Siamese T5 Encoding for Soft Phoneme Alignment
The siamese architecture with T5 encoding enables context-aware similarity mapping, allowing phonetically similar sounds to align even when not identical. Dual sequence inputs pass through a shared T5-based feature encoder with fully-visible attention mask (all tokens attend to each other). Representations are concatenated and processed through 1D CNN and MLPs, producing alignment labels (0=within dysfluent part, 1=aligned boundaries, 2=missing phonemes). Core assumption: Phoneme-level representations can capture acoustic-phonetic similarity patterns that hard matching cannot detect.

### Mechanism 2: Focal Loss for Imbalanced Dysfluency Detection
Focal loss training mitigates class imbalance in alignment labels, improving detection of underrepresented dysfluency types. Uses focal loss LFL = -α(1-pt)^γ × log(pt) with [α0, α1, α2] = [0.5, 0.1, 0.8] and γ=3. Higher α for class 2 (missing phonemes) focuses learning on rare dysfluency instances. Core assumption: Dysfluent speech contains fewer aligned tokens than dysfluent tokens; standard cross-entropy would bias toward majority class.

### Mechanism 3: Cascaded Speech-Text Alignment via CTC + Neural LCS
The STA model combines wav2vec 2.0 feature extraction with CTC decoding and Neural LCS for end-to-end dysfluency segmentation. wav2vec 2.0 extracts features projected to CMU phoneme vocabulary+1 dimension, trained with CTC loss. Inference uses greedy decoding for audio-to-dysfluent alignment, then Neural LCS aligns dysfluent sequence to reference text. Core assumption: CTC can produce meaningful phoneme sequences from dysfluent speech despite SOTA systems struggling with atypical speech.

## Foundational Learning

- **Concept: Longest Common Subsequence (LCS) Algorithm**
  - Why needed here: Neural LCS is a differentiable neural variant designed for local (partial) alignment rather than global alignment like DTW.
  - Quick check question: Given "PEN" and "PKEN", what would hard LCS return vs. Neural LCS if /K/ is phonetically similar to /G/?

- **Concept: Siamese Neural Networks**
  - Why needed here: Core architecture uses shared-weight twin branches ensuring consistent representation learning for similarity comparison.
  - Quick check question: Why must siamese networks use shared weights rather than separate encoders?

- **Concept: CTC (Connectionist Temporal Classification)**
  - Why needed here: STA model uses CTC loss for audio-to-phoneme mapping; understanding CTC collapse is essential for debugging.
  - Quick check question: How does CTC handle repeated phonemes (e.g., "P P P" in stuttering)?

## Architecture Onboarding

- **Component map**: VCTK text → dysfluency injection → alignment labels (0/1/2) → T5 encoder (shared) → concat → 1D CNN → MLP → softmax → Neural LCS
- **Critical path**: Tokenization → T5 encoding → sequence concatenation → CNN+MLP → softmax → alignment labels
- **Design tradeoffs**:
  - Phoneme-level (72.55%) outperforms word-level (68.44%) due to morphological complexity
  - LLM+TTS simulation provides scale but may not capture all real PPA acoustic characteristics
  - Deletion detection underperforms (BL=27ms) vs. repetition/substitution (8-10ms)
- **Failure signatures**:
  - Low substitution accuracy when training data has low substitution proportion
  - High deletion boundary loss relative to other dysfluency types
  - Assumption: Simulated-to-real domain gap not fully characterized
- **First 3 experiments**:
  1. Run DTW, Hard LCS, Neural LCS on text-text test set; expect ~48pp improvement at phoneme level
  2. Compare STA vs. YOLO-Stutter boundary loss on LLM disorder data; focus on repetition/insertion
  3. Ablate dysfluency proportions (P1-P4); validate substitution-insertion accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating articulatory kinematic or gestural representations improve Neural LCS's ability to model phoneme similarities in dysfluent speech?
- Basis in paper: [explicit] The conclusion states "it would be helpful to also explore better decoder... or phoneme similarity models either in kinematics systems [18, 14] or gestural systems [5, 13, 12]."
- Why unresolved: Neural LCS currently relies on T5 text embeddings and heuristically defined phoneme categories (plosive, fricative, etc.) for similarity, which may not capture articulatory dynamics.
- What evidence would resolve it: Experiments integrating articulatory feature representations (e.g., vocal tract kinematics from MRI or gesture-based features) as additional inputs, compared against the current text-only encoding on dysfluency detection accuracy.

### Open Question 2
- Question: Why does the STA model underperform on deletion detection (27ms boundary loss) compared to YOLO-Stutter (13ms), and how can this be remedied?
- Basis in paper: [inferred] Table 4 shows deletion is the only dysfluency type where STA model performs worse than baseline; this anomaly is reported but not explained or addressed.
- Why unresolved: The paper does not analyze why the alignment-based approach struggles specifically with detecting absent speech segments versus other dysfluency types.
- What evidence would resolve it: Ablation studies isolating deletion detection, analysis of failure cases, and architectural modifications (e.g., explicit absence modeling) with resulting boundary loss comparisons.

### Open Question 3
- Question: To what extent does the performance gap between simulated (LLM-generated) data and real PPA speech limit clinical applicability?
- Basis in paper: [inferred] The paper acknowledges simulated data "may not be natural enough" and real PPA evaluation shows only a single aggregate boundary loss (17ms) without per-dysfluency-type breakdowns or alignment accuracy metrics.
- Why unresolved: The 72.55%-90.96% alignment accuracy on simulated data lacks a comparable real-world benchmark, leaving the domain shift unquantified.
- What evidence would resolve it: Full alignment accuracy evaluation on real PPA data (including per-type metrics) and correlation analysis between simulated and real speech performance.

## Limitations
- Domain generalization uncertainty between simulated and real clinical dysfluency patterns
- Critical architectural details remain underspecified (CNN kernel sizes, MLP dimensions)
- Substantial computational requirements (50+ hours on RTX A6000) limit clinical deployment

## Confidence
- **High Confidence**: Text-text alignment improvements over DTW and Hard LCS baselines; Focal loss effectiveness; Basic siamese T5 architecture
- **Medium Confidence**: Speech-text alignment boundary loss metrics; Phoneme-level modeling superiority; Real clinical data performance
- **Low Confidence**: Cross-dataset generalization claims; Computational efficiency assertions; Complete characterization of dysfluency detection across PPA subtypes

## Next Checks
1. **Cross-Dataset Validation**: Test Neural LCS on additional PPA datasets with varying severity levels and linguistic backgrounds to validate generalization claims beyond the single PPA Speech dataset.
2. **Ablation on Architectural Components**: Systematically remove T5 encoding, focal loss, and siamese structure to quantify individual contributions to performance gains and identify critical components.
3. **Real-Time Performance Evaluation**: Measure inference latency and memory requirements for clinical deployment scenarios to assess practical viability beyond laboratory conditions.