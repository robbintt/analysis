---
ver: rpa2
title: Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence
arxiv_id: '2504.20059'
source_url: https://arxiv.org/abs/2504.20059
tags:
- clinical
- patient
- trial
- trials
- trialgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated TrialGPT, an AI-driven clinical trial matching
  framework, using 50 real-world online patient cases sourced from PubMed case reports
  and Reddit health communities. The framework leverages a large language model to
  match patients to clinical trials and outperforms traditional keyword-based searches
  by 46% in identifying eligible trials, with patients being eligible for an average
  of 7 out of the top 10 recommended trials.
---

# Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence

## Quick Facts
- arXiv ID: 2504.20059
- Source URL: https://arxiv.org/abs/2504.20059
- Reference count: 40
- Key outcome: TrialGPT outperforms traditional keyword-based searches by 46% in identifying eligible clinical trials, with patients being eligible for an average of 7 out of the top 10 recommended trials.

## Executive Summary
This study introduces TrialGPT, an AI-driven framework that matches patients to clinical trials using large language models (LLMs). Evaluated on 50 real-world online patient cases from PubMed and Reddit health communities, TrialGPT demonstrates a 46% improvement over traditional keyword-based searches in identifying eligible trials. The framework's ability to handle semantic complexity in eligibility criteria and provide interpretable explanations for matches highlights its potential to expand access to clinical trials for patients with complex or rare conditions through non-traditional online sources.

## Method Summary
TrialGPT employs a three-stage pipeline: (1) Retrieval - extracts up to 32 keywords from patient notes and uses hybrid BM25 and MedCPT semantic search to retrieve top 1,000 trials; (2) Matching - conducts criterion-by-criterion analysis comparing patient summaries against inclusion/exclusion criteria with sentence-level evidence extraction; (3) Ranking - aggregates scores from matching, relevance, and eligibility confidence into a unified ranking to output top 10 trials. The system was compared against a baseline using GPT-4o to generate Boolean queries for ClinicalTrials.gov searches, with performance measured through Precision@k, Mean Reciprocal Rank (MRR), and Hit Rate metrics.

## Key Results
- TrialGPT outperforms traditional keyword-based searches by 46% in identifying eligible trials
- Patients are eligible for an average of 7 out of the top 10 recommended trials
- Outreach to case authors and trial organizers yielded positive feedback confirming TrialGPT's effectiveness and interpretability

## Why This Works (Mechanism)

### Mechanism 1
LLM-based criterion-level analysis improves matching accuracy by handling semantic complexity in eligibility criteria. TrialGPT decomposes criteria into individual components, identifies relevant sentences, and generates explanations for matches, enabling nuanced reasoning about medical concepts that keyword systems miss.

### Mechanism 2
Hybrid retrieval combining lexical and semantic search broadens the candidate pool. The system extracts keywords and uses both BM25 (exact term frequency) and MedCPT (semantic embeddings) to retrieve top 1,000 trials, capturing both exact matches and conceptually related trials.

### Mechanism 3
Aggregated scoring from multiple signals improves ranking quality. TrialGPT combines criterion-level eligibility scores, relevance scores, and LLM-assigned eligibility confidence into a unified ranking, prioritizing trials where patients meet more criteria with higher confidence.

## Foundational Learning

**Concept: Precision@k and Mean Reciprocal Rank (MRR)**
- Why needed here: These metrics quantify how many recommended trials are actually eligible (Precision@k) and how early the first eligible trial appears (MRR)
- Quick check question: If a system recommends 10 trials and the patient is eligible for 3, what is Precision@10? (Answer: 0.30)

**Concept: Inclusion/Exclusion Criteria Structure**
- Why needed here: Clinical trials define eligibility through structured criteria that must be evaluated independently
- Quick check question: Why might a patient meet all inclusion criteria but still be ineligible? (Answer: They may fail one or more exclusion criteria)

**Concept: Hybrid Information Retrieval**
- Why needed here: TrialGPT combines BM25 with MedCPT to balance precision and recall
- Quick check question: What advantage does semantic search provide over pure keyword matching for rare diseases? (Answer: Captures conceptually related terms even if exact keywords differ)

## Architecture Onboarding

**Component map:**
Patient summary -> TrialGPT-Retrieval (keyword extraction + hybrid search) -> TrialGPT-Matching (criterion-by-criterion analysis) -> TrialGPT-Ranking (aggregated scoring) -> Top 10 ranked trials

**Critical path:** Retrieval quality determines the ceiling for matching; if relevant trials aren't retrieved, matching cannot recover them. Focus validation on retrieval recall before optimizing ranking.

**Design tradeoffs:**
- Top 1,000 retrieval vs. full database: Reduces compute cost but may miss edge-case trials
- Top 10 output: Balances usability with screening burden; paper shows 75.7% average eligibility at this cutoff
- GPT-4o vs. smaller models: Higher accuracy but increased cost and latency

**Failure signatures:**
- Empty retrieval: Patient conditions too rare or keywords poorly extracted
- High false positives: Patient summary lacks detail to disconfirm exclusion criteria
- Low inter-rater agreement: Ambiguous criteria or subjective medical judgment

**First 3 experiments:**
1. Retrieval recall audit: Verify known eligible patient-trial pairs appear in top 1,000 retrieval
2. Criterion-level error analysis: Review cases where TrialGPT and keyword search disagree to identify systematic patterns
3. Missing data robustness test: Measure performance degradation when ablating sections of patient summaries

## Open Questions the Paper Calls Out

**Open Question 1:** Can TrialGPT effectively match patients across a wider variety of online platforms such as Facebook, X, or PatientsLikeMe?
- Basis: Authors state future research should investigate engagement patterns across broader online health communities
- Why unresolved: Study limited to PubMed case reports and three specific Reddit communities
- What evidence would resolve it: Performance metrics from patient cases sourced from alternative social media platforms

**Open Question 2:** Does integrating direct follow-up with patients or healthcare institutions improve the accuracy of eligibility assessments when online data is incomplete?
- Basis: Paper notes future work could explore collaborations with healthcare institutions for comprehensive clinical details
- Why unresolved: Online cases frequently lack critical medical details limiting eligibility assessment
- What evidence would resolve it: Comparative study measuring matching accuracy between standard summaries and institution-enriched summaries

**Open Question 3:** How can the outreach and validation process be optimized to overcome low response rates?
- Basis: Authors report 8-15% response rates from clinicians and trial organizers made comprehensive assessment challenging
- Why unresolved: Reliance on busy stakeholders limited generalizability of feedback
- What evidence would resolve it: Implementation of alternative validation workflows resulting in statistically significant response rates

## Limitations

- Patient summary completeness: Online cases frequently lack critical medical details, potentially leading to false positive eligibility predictions
- Retrieval recall verification: No evidence that all truly eligible trials are captured in the top 1,000 retrieval pool
- Manual annotation reliability: Annotation process details are not fully specified, raising questions about consistency

## Confidence

- **High confidence**: 46% improvement claim well-supported by reported metrics and clearly specified architecture
- **Medium confidence**: Mechanism explanations for LLM advantages are reasonable but not empirically validated through ablation studies
- **Low confidence**: Claims about real-world applicability limited by synthetic nature of test cases and lack of longitudinal follow-up

## Next Checks

1. **Retrieval recall audit**: For a held-out set of known eligible patient-trial pairs, verify they appear in the top 1,000 retrieval results before matching occurs
2. **Missing data robustness test**: Systematically ablate sections of patient summaries to measure performance degradation and establish minimum information requirements
3. **Longitudinal patient follow-up**: Track whether patients who receive TrialGPT recommendations actually attempt enrollment and identify barriers not captured in current evaluation