---
ver: rpa2
title: 'Mercury: Ultra-Fast Language Models Based on Diffusion'
arxiv_id: '2506.17298'
source_url: https://arxiv.org/abs/2506.17298
tags:
- mercury
- wang
- zhang
- coder
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Inception Labs presents Mercury, a family of large-scale diffusion-based\
  \ language models that achieve up to 10\xD7 faster inference than existing speed-optimized\
  \ models while maintaining comparable quality. Mercury Coder models, optimized for\
  \ coding, deliver 1109 tokens/sec (Mini) and 737 tokens/sec (Small) on NVIDIA H100\
  \ GPUs, outperforming frontier models in throughput."
---

# Mercury: Ultra-Fast Language Models Based on Diffusion

## Quick Facts
- arXiv ID: 2506.17298
- Source URL: https://arxiv.org/abs/2506.17298
- Reference count: 40
- Primary result: Mercury Coder models achieve up to 10× faster inference (1109-737 tokens/sec) than autoregressive models while matching quality on coding benchmarks

## Executive Summary
Mercury is a family of diffusion-based language models that achieve up to 10× faster inference than existing speed-optimized models while maintaining comparable quality. The models use a Transformer architecture with diffusion-based parallel token generation, enabling efficient GPU utilization. Mercury Coder models, optimized for coding tasks, deliver exceptional throughput on NVIDIA H100 GPUs and match or exceed baseline performance on coding benchmarks including HumanEval, MultiPL-E, and fill-in-the-middle tasks.

## Method Summary
Mercury extends discrete diffusion language models to large-scale code generation. The models use a Transformer backbone trained with a diffusion loss to predict clean sequences from progressively noised versions. At inference, the model refines entire blocks of tokens simultaneously across multiple diffusion steps instead of one token per forward pass. This parallelism amortizes the Transformer's attention overhead over many tokens per step, better saturating GPU compute and increasing arithmetic intensity. The models support any-order and infilling generation by treating known context as conditioning within the denoising process.

## Key Results
- Mercury Coder Mini: 1109 tokens/sec on H100 GPU, outperforming all other models in throughput
- Mercury Coder Small: 737 tokens/sec on H100 GPU, ranking second on quality and fastest overall on Copilot Arena
- Match or exceed baseline performance on HumanEval (71.8%), MultiPL-E (44.5%), and fill-in-the-middle tasks (93.1% single-line FIM)
- Second place on quality and fastest overall on Copilot Arena

## Why This Works (Mechanism)

### Mechanism 1: Parallel diffusion denoising
Replacing sequential token prediction with parallel diffusion denoising yields up to 10× higher throughput with comparable quality on coding tasks. A Transformer-based denoising network is trained to predict clean sequences from progressively noised versions. At inference, the model refines entire blocks of tokens simultaneously across multiple diffusion steps instead of one token per forward pass. This parallelism amortizes the Transformer's attention overhead over many tokens per step, better saturating GPU compute and increasing arithmetic intensity.

### Mechanism 2: Coarse-to-fine refinement
Coarse-to-fine refinement enables controllable, high-quality generation suitable for fill-in-the-middle and autocomplete workloads. Diffusion denoising operates progressively from high-noise (global structure) to low-noise (local detail), allowing the model to globally adjust multiple tokens before finalizing. This supports any-order and infilling generation by treating known context as conditioning within the denoising process.

### Mechanism 3: Transformer compatibility
Retaining a Transformer architecture preserves compatibility with existing training infrastructure, kernel optimizations, and alignment pipelines. Mercury uses a standard Transformer backbone, differing from autoregressive LLMs only in the training objective (diffusion loss) and parallel sampling procedure. This orthogonality allows reuse of attention kernels, mixed-precision training, and RLHF/DPO alignment by swapping the loss term.

## Foundational Learning

### Concept: Discrete diffusion processes
- Why needed here: Mercury applies forward noising and learned reverse denoising to token sequences, a core departure from autoregressive factorization
- Quick check question: Can you write the forward noising transition for a simple masking schedule on a 5-token sequence?

### Concept: Parallel decoding and arithmetic intensity
- Why needed here: The 10× throughput claim hinges on converting sequential decode-bound inference into compute-bound parallel updates across diffusion steps
- Quick check question: For a fixed FLOP budget, how does generating 10 tokens per step over 5 steps compare to generating 1 token per step over 50 steps in terms of memory bandwidth pressure?

### Concept: Conditional generation and prompting in dLLMs
- Why needed here: Mercury exposes an OpenAI-compatible API and supports zero-shot/few-shot prompting; understanding how prompts are encoded as conditioning is essential for integration
- Quick check question: Given a prompt "def sort(arr):" and a masked completion span, how would you construct the initial noisy state for infilling?

## Architecture Onboarding

### Component map:
Tokenizer and embedding layer -> Transformer backbone (denoising network) -> Noise scheduler -> Custom inference engine (dynamic batching, paging, custom kernels) -> Optional alignment stages

### Critical path:
Training: Data curation → tokenizer → forward noising → Transformer forward → diffusion loss → backprop
Inference: Prompt encoding → sample from prior → iterate denoising network for T steps → detokenize output

### Design tradeoffs:
- Fewer diffusion steps: Lower latency but risk of under-refined outputs
- Batching and paging: Higher throughput but increased memory pressure; custom kernels mitigate overhead
- Context length extension to 128k: Higher quality on long-context tasks but may stress attention kernels

### Failure signatures:
- Unexpectedly low throughput despite parallel decoding (kernel inefficiency, poor batching)
- Degraded coherence or increased syntax errors (diffusion steps insufficient, noise schedule misconfigured)
- Alignment failures (RLHF/DPO applied without adapting to diffusion loss)
- API latency spikes under load (paging or batching bottlenecks)

### First 3 experiments:
1. Reproduce throughput benchmarks on H100 GPU and compare against reported 1109/737 tokens/sec
2. Sweep diffusion steps T ∈ {2,4,8,16} on MultiPL-E and FIM tasks to characterize speed/quality tradeoffs
3. Integrate Mercury via OpenAI-compatible API into a code assistant workflow, measuring end-to-end latency and correctness on infilling tasks versus an autoregressive baseline

## Open Questions the Paper Calls Out
None

## Limitations

- Critical architectural details (exact parameters, layer counts, noise schedule) are not disclosed, limiting reproducibility
- The source of the 10× speedup (algorithmic vs. engineering) is unclear due to proprietary inference engine
- Performance on non-coding language tasks is not reported, leaving generalization uncertainty

## Confidence

- High confidence: The core mechanism (parallel diffusion denoising in a Transformer backbone) is sound and supported by related work
- Medium confidence: Quality claims are supported by reported results but need independent verification
- Low confidence: The exact source of the 10× speedup and robustness of infilling generation remain uncertain

## Next Checks

1. Reproduce throughput and quality benchmarks on H100 GPU using HumanEval and FIM tasks, comparing against autoregressive baselines
2. Systematically vary the number of diffusion steps T and measure impact on throughput and quality to clarify speed/quality tradeoffs
3. Test Mercury on diverse infilling tasks beyond single-line FIM, including multi-span and long-context scenarios, comparing against autoregressive models on correctness and API latency