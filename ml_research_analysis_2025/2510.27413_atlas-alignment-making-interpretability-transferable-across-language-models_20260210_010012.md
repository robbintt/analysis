---
ver: rpa2
title: 'Atlas-Alignment: Making Interpretability Transferable Across Language Models'
arxiv_id: '2510.27413'
source_url: https://arxiv.org/abs/2510.27413
tags:
- concept
- features
- atlas
- subject
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Atlas-Alignment, a framework for transferring
  interpretability across language models by aligning unknown latent spaces to a labeled,
  human-interpretable Concept Atlas. The approach leverages lightweight representational
  alignment techniques to map subject model features into a foundation model's interpretable
  latent space, enabling semantic feature search and steerable generation without
  costly training or labeled concept data.
---

# Atlas-Alignment: Making Interpretability Transferable Across Language Models

## Quick Facts
- arXiv ID: 2510.27413
- Source URL: https://arxiv.org/abs/2510.27413
- Reference count: 32
- Primary result: Achieves up to 0.97 MRR for semantic retrieval and 41.8% faithfulness for concept steering by aligning subject model latent spaces to a pre-labeled Concept Atlas.

## Executive Summary
Atlas-Alignment introduces a framework for transferring interpretability across language models by aligning unknown latent spaces to a labeled, human-interpretable Concept Atlas. The approach leverages lightweight representational alignment techniques to map subject model features into a foundation model's interpretable latent space, enabling semantic feature search and steerable generation without costly training or labeled concept data. Through quantitative and qualitative evaluations, the method demonstrates robust semantic retrieval with Orthogonal Procrustes achieving up to 0.97 MRR and 0.21 MPP, and controllable steering with over 30% faithfulness in top layers. The work shows that interpretability can be amortized by investing in a high-quality atlas, making many new models transparent at minimal marginal cost.

## Method Summary
The method extracts max-pooled activations from both a subject model (e.g., Llama 3.1 8B variants) and a Concept Atlas (Gemma 2 2B with Gemma Scope SAE) from shared input sequences. It learns a translation matrix T using Orthogonal Procrustes on 500k samples: min ||A_s - A_c T^T||_F s.t. T T^T = I, with row-wise L2 normalization. This alignment enables semantic retrieval by mapping atlas concept queries into subject feature space and concept steering by injecting similarity vectors into subject activations at inference. The approach avoids per-model SAE training by amortizing interpretability costs across models through a shared atlas.

## Key Results
- Orthogonal Procrustes achieves AUROC 0.83-0.86 and AP 0.43-0.49 for translation quality, significantly outperforming alternatives.
- Semantic retrieval reaches MRR 0.97 and MPP 0.21 using 454 validated features and synthetic concept queries.
- Concept steering shows faithfulness up to 41.8% increase in Llama-IT layer 30, with minimal effect in early layers (3-25).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal Procrustes alignment preserves semantic structure across model latent spaces.
- Mechanism: Given paired activations (A_s, A_c) from shared inputs, Procrustes finds an orthogonal rotation T that minimizes ||A_s - A_c T^T||_F. After L2-normalization, this equivalently minimizes cosine distance, aligning semantically similar activation patterns across spaces.
- Core assumption: The Platonic Representation Hypothesis holds sufficiently—models trained on similar data converge on comparable latent structures.
- Evidence anchors:
  - [Section 3.3] Equation 2 explicitly defines the orthogonal constraint T·T^T = I.
  - [Table 3] Orthogonal Procrustes achieves AUROC 0.83–0.86 vs. 0.74–0.82 for alternatives.
  - [corpus] Related work on cross-model alignment (Jha et al., 2025) requires expensive training; this method avoids that.
- Break condition: If models diverge significantly in architecture or training data, latent spaces may not share sufficient structure for linear rotation to capture.

### Mechanism 2
- Claim: Translated concept queries enable semantic retrieval without labeled data.
- Mechanism: A concept query q_c (one-hot over atlas features) is mapped via cosine similarity to subject features: s_c = T̄_s→c · q_c / ||q_c||. Top-scoring subject features are retrieved, inheriting atlas labels.
- Core assumption: The Linear Representation Hypothesis—concepts are encoded as linear directions.
- Evidence anchors:
  - [Section 3.4, Eq. 3] Defines the similarity scoring mechanism.
  - [Table 4] MRR = 0.97 for Orthogonal Procrustes on 454 validated features (near-perfect retrieval).
  - [Table 1] Qualitative examples show correct feature identification across Llama variants.
- Break condition: If subject model features are polysemantic or the atlas lacks coverage, similarity scores become ambiguous.

### Mechanism 3
- Claim: Steering vectors derived from translated queries modify generation along concept directions.
- Mechanism: The similarity vector s_c is added to activations: a_modified = (a + λs_c) · ||a|| / ||a + λs_c||. This shifts the residual stream toward atlas-aligned directions before layer output.
- Core assumption: Activation-space interventions propagate causally to token probabilities (not proven in this paper).
- Evidence anchors:
  - [Section 3.4, Eq. 4] Steering formula with normalization.
  - [Table 5] Faithfulness up to 41.8% increase in Llama-IT layer 30; minimal effect in early layers.
  - [Table 2] Qualitative steering examples (e.g., "dogs and cats" jokes).
- Break condition: Steering in layers 3–25 shows near-random effects; success concentrates in final layers. Non-linear interactions may cause incoherent outputs.

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs)**
  - Why needed here: The Concept Atlas is built from SAE features; understanding sparsity and monosemanticity is prerequisite.
  - Quick check question: Can you explain why SAE features are more interpretable than raw neuron activations?

- Concept: **Linear Representation Hypothesis**
  - Why needed here: Justifies why linear rotation suffices for cross-model semantic transfer.
  - Quick check question: What does it mean for a concept to be "linearly encoded" in activation space?

- Concept: **Orthogonal Procrustes Problem**
  - Why needed here: The primary translation method; understanding rotation vs. scaling matters for interpreting results.
  - Quick check question: Why constrain T to be orthogonal rather than allowing arbitrary linear transforms?

## Architecture Onboarding

- Component map: Concept Atlas -> Translation Function -> Query Interface -> Steering Module
- Critical path:
  1. Collect shared inputs X (500k+ samples recommended; Section 4.1).
  2. Forward through both models; max-pool activations to (N × d) matrices.
  3. Fit T_s→c on training split; evaluate retrieval/steering on held-out split.
  4. Validate atlas coverage—insufficient concept variety in X degrades translation.
- Design tradeoffs:
  - Orthogonal Procrustes: Best retrieval/steering, but requires d_s = d_c or dimensionality matching.
  - Covariance/Correlation: Faster, no optimization, but lower AP (0.16–0.24 vs. 0.43–0.49).
  - Linear Regression: Near-random retrieval (MRR ≈ 0.03); not recommended.
- Failure signatures:
  - Low AP (< 0.2): Translation dataset lacks atlas concept coverage.
  - Steering has no effect: Intervening in early/mid layers (3–25); try layer 30.
  - Incoherent generations: Single-feature steering causes repetition; combine 5+ related features.
- First 3 experiments:
  1. Replicate Table 3 on your subject model: measure AUROC/AP for 100 random atlas features.
  2. Test retrieval (Table 4 protocol): generate synthetic sequences for 50 subject features, measure MRR.
  3. Steer with a multi-feature query at layers 3, 12, 19, 25, 30; compare faithfulness scores.

## Open Questions the Paper Calls Out

- **Attention Head Transferability**: Can Atlas-Alignment effectively translate specialized attention head functions, such as in-context retrieval, across models? The introduction states that a promising application lies in translating attention heads, which perform specialized tasks, though the current study restricted evaluation to MLP and residual stream features. High-quality labeled latent spaces for attention heads were not available for this study, leaving the transferability of their specialized functions unverified.

- **Positional Information Loss**: How does the discarding of positional information via max-pooling limit the precision of localized semantic interventions? The limitations section notes that the current design discards positional information through max-pooling of activations. Aggregating sequence data prevents the framework from localizing concepts to specific tokens or sequence positions, potentially limiting fine-grained control.

- **Layer-Specific Steering Efficacy**: Why is concept steering efficacy concentrated in the final layers (Layer 30) rather than early-to-mid layers? Table 5 shows Orthogonal Procrustes achieves >30% faithfulness in Layer 30 but only ~6-9% in earlier layers (Layers 3-25), a discrepancy the authors report but do not explain. The linear alignment hypothesis may hold differently across processing stages, or early layers may require different translation methods to influence output behavior.

## Limitations

- **Atlas Coverage Dependency**: The method fails when subject models encode concepts not represented in the Concept Atlas, limiting applicability to models with similar concept distributions.
- **Causal Steering Uncertainty**: While activation changes correlate with concept expression, the paper does not establish whether these changes directly cause the observed effects rather than being downstream consequences.
- **Early Layer Ineffectiveness**: Steering interventions show near-random effects in layers 3-25, concentrating success only in final layers where the alignment hypothesis may hold differently.

## Confidence

- **High confidence**: The orthogonal Procrustes alignment method works robustly for semantic retrieval (MRR 0.97, backed by quantitative metrics)
- **Medium confidence**: Steering effectiveness is real but context-dependent (works well at layer 30, poorly at layers 3-25)
- **Medium confidence**: Interpretability transfer is practical for models with similar training distributions to the atlas

## Next Checks

1. Test atlas-alignment on models trained on distinctly different data (e.g., medical vs. general text) to measure degradation in translation quality
2. Perform ablation studies varying translation dataset size (below 500k samples) to identify minimum viable corpus requirements
3. Verify steering causality through ablation - remove subject features orthogonal to the target concept and measure whether steering effects persist