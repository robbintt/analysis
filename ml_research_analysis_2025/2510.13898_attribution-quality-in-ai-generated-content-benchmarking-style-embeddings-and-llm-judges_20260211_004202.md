---
ver: rpa2
title: Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and
  LLM Judges
arxiv_id: '2510.13898'
source_url: https://arxiv.org/abs/2510.13898
tags:
- style
- text
- human
- continuations
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks two complementary attribution mechanisms\u2014\
  fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o)\u2014on the Human\
  \ AI Parallel Corpus, a balanced dataset of 600 instances spanning six domains.\
  \ The Style Embedding baseline outperforms the LLM judge overall on GPT-generated\
  \ continuations (82% vs."
---

# Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges

## Quick Facts
- arXiv ID: 2510.13898
- Source URL: https://arxiv.org/abs/2510.13898
- Authors: Misam Abbas
- Reference count: 15
- Key outcome: Style embeddings outperform LLM judge overall on GPT continuations (82% vs. 68% accuracy), while LLM judge slightly edges out embeddings on Llama continuations (85% vs. 81%), though differences are not statistically significant.

## Executive Summary
This paper benchmarks two complementary attribution mechanisms—fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o)—on the Human AI Parallel Corpus, a balanced dataset of 600 instances spanning six domains. The Style Embedding baseline outperforms the LLM judge overall on GPT-generated continuations (82% vs. 68% accuracy), while the LLM judge slightly edges out embeddings on Llama continuations (85% vs. 81%), though differences are not statistically significant. Notably, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. The study provides a reproducible benchmark with open-source code and data under the MIT license.

## Method Summary
The study evaluates binary classification to identify human-authored vs LLM-generated text continuations. Using the Human AI Parallel Corpus (600 balanced instances across six domains), it compares two approaches: (1) Style Embedding baseline using cosine similarity between prompt and continuations via pretrained BERT-based siamese network, and (2) LLM Judge using GPT-4o with zero-shot binary prompt. Accuracy and McNemar's test for statistical significance serve as evaluation metrics. The methodology includes randomizing continuation positions to prevent bias and provides exact prompt templates for reproducibility.

## Key Results
- Style embeddings achieve 82% accuracy vs. 68% for LLM judge on GPT continuations
- LLM judge achieves 85% accuracy vs. 81% for embeddings on Llama continuations
- LLM judge significantly outperforms embeddings in fiction (96% vs. 70%) and academic prose domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Style embeddings detect human authorship by measuring stylistic continuity between prompt and continuation via cosine similarity.
- Mechanism: A pretrained BERT-based encoder produces fixed-dimensional embeddings for T1 (prompt) and both continuations. The continuation with higher cosine similarity to T1 is predicted as human-authored, under the assumption that human continuations preserve stylistic registers more consistently than LLM-generated ones.
- Core assumption: Stylistic coherence is a reliable signal of human authorship; LLMs introduce stylistic drift detectable via embedding distance.
- Evidence anchors:
  - [abstract] "Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82% vs. 68%)"
  - [Section V.B.3] "Style Embeddings excel at capturing structural and register-specific linguistic features (particularly evident in spoken language and scripted dialogue)"
  - [corpus] Related work on content-independent style representations (Wegmann et al., 2022) demonstrates that contrastive training can separate style from topic—weak direct evidence for this specific benchmark.
- Break condition: If LLMs are fine-tuned on domain-specific corpora to match stylistic registers closely, embedding-based similarity may no longer discriminate effectively.

### Mechanism 2
- Claim: LLM judges attribute authorship by evaluating semantic coherence and narrative plausibility, outperforming embeddings in content-heavy domains.
- Mechanism: GPT-4o is prompted with T1 and two continuations, then asked to identify the human-authored one. The model applies learned priors about discourse structure, citation conventions, and narrative continuity—capabilities emerging from instruction tuning and broad pretraining.
- Core assumption: LLMs have sufficient internal representation of "human-like" vs. "machine-like" text to serve as reliable classifiers when prompted appropriately.
- Evidence anchors:
  - [abstract] "LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity"
  - [Section V.B.1] "Fiction: The LLM judge significantly outperforms the Base model for both GPT (96% vs. 70%) and Llama (100% vs. 67%)"
  - [corpus] "Quantifying Label-Induced Bias in LLM Evaluations" suggests systematic biases in LLM self-evaluation—corpus evidence cautions against assuming neutrality.
- Break condition: When evaluating self-generated text (GPT-4o judging GPT-4o), the judge may conflate its own output with human text, reducing accuracy to 67.7%.

### Mechanism 3
- Claim: Domain structure determines which attribution signal—stylistic or semantic—is more discriminative.
- Mechanism: Spoken dialogue and TV scripts exhibit irregular turn-taking, fillers, and register shifts that create distinctive embedding signatures. Fiction and academic prose require evaluating argument structure and narrative coherence, where LLM judges excel.
- Core assumption: No single detection approach generalizes across all domains; optimal attribution requires matching mechanism to domain characteristics.
- Evidence anchors:
  - [Section V.B.1] "Spoken: Base model achieves perfect accuracy (100%) while LLM judge performs poorly (33% for GPT...)"
  - [Section VI] "Provenance detection will benefit from hybrid approaches that fuse structural style signals with the semantic reasoning inherent to LLMs"
  - [corpus] "Atomic Literary Styling" identifies neurons discriminating literary prose from rigid AI text—supporting domain-specific signal variation.
- Break condition: Hybrid approaches add complexity; naive ensembling may amplify errors if both mechanisms fail on edge cases (e.g., heavily edited AI text).

## Foundational Learning

- **Concept: Cosine similarity in embedding space**
  - Why needed here: The baseline classifier relies entirely on comparing embedding vectors; understanding geometric similarity is prerequisite to interpreting why embeddings excel on structured domains.
  - Quick check question: If two texts have cosine similarity of 0.95, what does that imply about their stylistic relationship?

- **Concept: LLM-as-a-Judge evaluation paradigm**
  - Why needed here: The study positions GPT-4o as an evaluator; understanding prompt design, position bias, and self-evaluation limitations is critical for reproducibility.
  - Quick check question: Why does the protocol randomize the order of continuations A and B?

- **Concept: McNemar's test for paired classifiers**
  - Why needed here: The paper uses McNemar's test to determine if accuracy differences are statistically significant; understanding paired nominal data testing is required to interpret Tables I–IV.
  - Quick check question: What type of error does McNemar's test evaluate—discordant or concordant predictions?

## Architecture Onboarding

- **Component map:** Data layer -> Style Embedding encoder -> LLM Judge -> Evaluation
- **Critical path:**
  1. Sample and preprocess T1, human continuation, LLM continuation
  2. Compute style embeddings → cosine similarities → predictions
  3. Issue prompts to GPT-4o → collect judgments
  4. Compare predictions to ground truth → compute accuracy
  5. Run McNemar's test on paired predictions per segment

- **Design tradeoffs:**
  - Embedding baseline: Fast, interpretable, domain-agnostic training—but blind to semantic coherence
  - LLM judge: Semantically rich, adapts to context—but slower, API-dependent, vulnerable to self-recognition bias
  - Single judge (GPT-4o) limits generalizability; alternative LLMs/reasoning models not evaluated

- **Failure signatures:**
  - Embeddings fail on academic/fiction (55% accuracy) → semantic drift undetected
  - LLM judge fails on spoken (33% for GPT) → conversational markers misinterpreted
  - LLM judge underperforms on self-generated text (67.7% vs. 84.7% on Llama) → self-attribution bias

- **First 3 experiments:**
  1. Reproduce benchmark on local data split to validate pipeline integrity (accuracy ±2% of reported)
  2. Ablate prompt design: test chain-of-thought rationales vs. zero-shot binary to measure prompt sensitivity
  3. Implement naive ensemble (vote on domains where each excels) to quantify hybrid gains vs. complexity cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a hybrid ensemble combining style embeddings and LLM judges outperform either method alone, and what weighting scheme would optimize performance across domains?
- Basis in paper: [explicit] The authors state these "complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies" and call for "ensemble frameworks that dynamically weight stylistic and semantic evidence."
- Why unresolved: The paper benchmarks each method independently but does not implement or test any fusion strategy.
- What evidence would resolve it: Experiments combining both approaches with domain-adaptive weighting, reporting accuracy gains over individual methods on the same corpus.

### Open Question 2
- Question: Does GPT-4o exhibit a systematic blind spot for its own generated text, and does this self-detection deficit generalize across LLM families?
- Basis in paper: [inferred] The authors note GPT-4o performs worse distinguishing human vs. GPT continuations (67.7%) than human vs. Llama continuations (84.7%), suggesting "GPT-4o is more likely to conflate the text that it has generated itself with human generated text."
- Why unresolved: Only two generator families were tested, and no other LLMs served as judges.
- What evidence would resolve it: A cross-product study with multiple LLM judges evaluating multiple generator families, controlling for model lineage.

### Open Question 3
- Question: Can chain-of-thought prompting or reasoning-based LLM judges improve attribution accuracy, particularly in domains where zero-shot prompting currently fails?
- Basis in paper: [explicit] The authors list as future work: "explore prompt engineering and chain-of-thought rationales for LLM judges."
- Why unresolved: Only zero-shot binary prompts were evaluated; no ablation on prompt design was conducted.
- What evidence would resolve it: Comparative experiments using CoT, few-shot, and reasoning-augmented prompts on the same benchmark.

## Limitations
- Single LLM judge (GPT-4o) limits generalizability to other model families
- No hybrid ensemble methods tested despite complementary performance patterns
- Self-evaluation bias revealed: GPT-4o underperforms on its own generated text

## Confidence
- **High confidence**: Style embeddings' superior performance on structured domains (spoken, scripts) and LLM judge's dominance in semantic-rich domains (fiction, academic)
- **Medium confidence**: Overall accuracy differences between approaches (not statistically significant at p<0.05)
- **Low confidence**: Generalizability to other LLM architectures beyond GPT-4o and Llama-70B-Instruct

## Next Checks
1. Test attribution accuracy when evaluating text generated by smaller, open-weight models (e.g., Mistral-7B, Phi-3) to assess scalability
2. Implement and benchmark hybrid approaches that combine style embeddings and LLM judgments through weighted voting or learned fusion
3. Conduct ablation studies varying prompt design (zero-shot vs. chain-of-thought) for the LLM judge to quantify sensitivity to instruction format