---
ver: rpa2
title: 'SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical
  Specialties'
arxiv_id: '2511.03542'
source_url: https://arxiv.org/abs/2511.03542
tags:
- medical
- solve-med
- agent
- specialists
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties

## Quick Facts
- arXiv ID: 2511.03542
- Source URL: https://arxiv.org/abs/2511.03542
- Reference count: 27
- Primary result: Specialized orchestration of 10 LoRA-fine-tuned 1B SLMs and a 9B orchestrator outperforms larger general-purpose models on Italian medical QA (ROUGE-1=0.301, BERTScore F1=0.697).

## Executive Summary
SOLVE-Med is a multi-agent system for medical question answering that routes queries to multiple specialized SLMs and synthesizes responses via a larger orchestrator. It leverages domain-specific fine-tuning and recall-prioritized routing to improve coverage while maintaining answer quality. The architecture combines lightweight, specialized models for efficiency with a larger orchestrator for coherent synthesis.

## Method Summary
The system uses DistilBERT for multi-label routing to 10 LoRA-fine-tuned LLaMA-3.2-1B-Instruct specialists, each trained on a distinct medical specialty. A Gemma-2-9B-IT orchestrator synthesizes responses from selected specialists. Data comes from Italian medical forums (Medicitalia, Dica33), consolidated into 10 macro-categories with clustering-based sampling. The pipeline uses quantized models for deployment efficiency and evaluates with automated metrics (ROUGE, BERTScore, etc.).

## Key Results
- F3 threshold routing achieves recall=0.890 with 2.277 avg specialists vs. Top-2's recall=0.804
- Orchestrator synthesis achieves ROUGE-1=0.301 and BERTScore F1=0.697
- System outperforms LLaMA-3.1-8B-Instruct, Gemma-2-9B-IT, and Velvet-14B baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Routing queries to multiple specialists with recall-prioritized selection improves coverage without sacrificing final answer quality.
- Mechanism: The Router Agent uses multi-label classification (DistilBERT) with threshold-based or top-n selection. Thresholds tuned for Fβ (β=2,3) explicitly prioritize recall, delegating precision recovery to the Orchestrator Agent downstream.
- Core assumption: Medical queries often span multiple specialties; including more specialists provides perspectives the orchestrator can filter.
- Evidence anchors:
  - Table 2 shows F3 threshold yields recall=0.890 with 2.277 avg specialists vs. Top-2's recall=0.804, confirming recall-precision tradeoff is intentional.
  - "This emphasis on recall reflects the system's design: the Orchestrator Agent subsequently refines the response by filtering out incorrect or irrelevant contributions."
  - Weak direct corpus validation—neighbor papers discuss multi-agent medical systems but do not evaluate this specific recall-prioritized routing strategy.
- Break condition: If specialist responses are consistently noisy or contradictory beyond orchestrator capacity, higher recall may degrade rather than improve outputs.

### Mechanism 2
- Claim: A larger orchestrator model can effectively synthesize multiple specialist outputs into coherent responses.
- Mechanism: The Orchestrator Agent (Gemma-2-9B-IT, quantized) receives the original query plus all selected specialist responses and produces a unified answer via structured prompting that instructs it to merge evidence-based contributions.
- Core assumption: A 9B model has sufficient capacity to integrate, filter, and reconcile outputs from multiple 1B specialists.
- Evidence anchors:
  - "Its larger parameter count w.r.t. medical specialists enables effective integration of their contributions, helping to mitigate issues such as omissions or oversimplification."
  - Reports ROUGE-1=0.301 and BERTScore F1=0.697, outperforming standalone models up to 14B.
  - "Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation" supports multi-agent synthesis viability but in a different medical subdomain.
- Break condition: If specialist outputs conflict substantially without clear resolution criteria, orchestrator may produce inconsistent or hedged responses.

### Mechanism 3
- Claim: Domain-specialized SLMs fine-tuned with LoRA can outperform larger general-purpose models on niche medical QA.
- Mechanism: Ten 1B-parameter LLaMA-3.2-1B-Instruct models are fine-tuned via LoRA on Italian medical forum data, each on a distinct specialty. Clustering (UMAP + HDBSCAN) selects representative training samples per domain.
- Core assumption: Specialty-specific fine-tuning on forum data transfers to real medical question answering despite potential distribution shift.
- Evidence anchors:
  - "Each SLM was trained to generate coherent, domain-specific answers with consistent style and terminology."
  - Table 1 shows SOLVE-Med variants outperform LLaMA-3.1-8B-Instruct, Gemma-2-9B-IT, and Velvet-14B on ROUGE and BERTScore metrics.
  - "Small language models learn enhanced reasoning skills from medical textbooks" (cited as [11]) provides indirect support for SLM medical fine-tuning efficacy.
- Break condition: If evaluation queries differ substantially from forum data distribution, fine-tuning benefits may not generalize.

## Foundational Learning

- Concept: Multi-label classification with threshold tuning
  - Why needed here: Router Agent must assign queries to multiple relevant specialists, not just one.
  - Quick check question: Can you explain why β>1 in Fβ prioritizes recall over precision?

- Concept: Parameter-Efficient Fine-Tuning (LoRA)
  - Why needed here: Enables fine-tuning multiple SLMs without prohibitive memory/compute costs.
  - Quick check question: How does LoRA reduce trainable parameters while preserving model capacity?

- Concept: Model quantization for deployment
  - Why needed here: Both specialists and orchestrator use quantized versions to enable local, resource-constrained deployment.
  - Quick check question: What is the primary trade-off when applying 4-bit or 8-bit quantization to LLMs?

## Architecture Onboarding

- Component map: User Query → Context-aware Reformulation → Router Agent (DistilBERT multi-label classifier) → Selected Medical Specialists (1-3 of 10 SLMs) → Orchestrator Agent (Gemma-2-9B-IT quantized) → Final Response

- Critical path: Router classification accuracy → specialist response quality → orchestrator synthesis. The router's recall setting directly controls specialist count, which affects downstream load and output diversity.

- Design tradeoffs:
  - Top-n vs. threshold-based routing: Top-n guarantees fixed compute; threshold adapts to query ambiguity but introduces variance.
  - Orchestrator size: 9B enables synthesis but increases latency vs. using a smaller aggregator.
  - Specialist count (10): Covers major domains but may miss subspecialties; consolidating 102 fine-grained categories into 10 macro-categories loses granularity.

- Failure signatures:
  - Router consistently selects wrong specialists → check class balance in training data; verify threshold calibration.
  - Orchestrator produces generic or ungrounded responses → inspect prompt template; verify specialist inputs are being passed correctly.
  - Specialist outputs off-topic → verify LoRA adapters are loaded correctly per domain; check training data quality.

- First 3 experiments:
  1. Reproduce router evaluation: Train DistilBERT on provided splits; measure precision/recall under Top-2, Top-3, F2, F3 settings. Compare to Table 2.
  2. Ablate specialist count: Run inference with 1, 2, 3, 5 specialists per query; measure ROUGE/BERTScore trajectory to verify diminishing returns point.
  3. Swap orchestrator model: Replace Gemma-2-9B-IT with a smaller model (e.g., 3B) and compare synthesis quality to isolate the 9B model's contribution.

## Open Questions the Paper Calls Out
- How does SOLVE-Med perform regarding clinical accuracy and safety under human expert evaluation? The study relies exclusively on automated lexical and semantic metrics which do not capture medical correctness or safety.
- How effectively does the system manage context retention and query reformulation in multi-turn interactions? Experimental validation is restricted to single-turn QA pairs despite a context-aware reformulation module.
- To what extent does the multi-agent architecture reduce versus redistribute the risk of factual hallucinations? The Orchestrator Agent might generate plausible but incorrect inferences when merging outputs from selected specialists.

## Limitations
- Relies solely on automated metrics, lacking expert evaluation of clinical accuracy and safety.
- Does not validate multi-turn context handling despite claiming it as a future improvement.
- May redistribute hallucination risk through orchestrator synthesis of specialist outputs.

## Confidence
- Method description clarity: High
- Reproducibility with provided details: Medium (key hyperparameters and prompts unspecified)
- Clinical applicability without expert validation: Low

## Next Checks
1. Verify router recall-precision tradeoff by reproducing Table 2 with different Fβ thresholds
2. Test specialist count ablation to identify optimal number per query
3. Validate orchestrator contribution by comparing with smaller model alternatives