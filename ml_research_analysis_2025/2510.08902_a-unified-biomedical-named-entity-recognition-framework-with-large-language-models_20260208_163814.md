---
ver: rpa2
title: A Unified Biomedical Named Entity Recognition Framework with Large Language
  Models
arxiv_id: '2510.08902'
source_url: https://arxiv.org/abs/2510.08902
tags:
- entity
- bioner
- datasets
- recognition
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unified biomedical named entity recognition
  framework using large language models to address the challenges of nested entities,
  boundary ambiguity, and cross-lingual generalization. The method reformulates BioNER
  as a text generation task with a symbolic tagging strategy for explicit boundary
  annotation, and performs bilingual joint fine-tuning across multiple Chinese and
  English datasets.
---

# A Unified Biomedical Named Entity Recognition Framework with Large Language Models

## Quick Facts
- arXiv ID: 2510.08902
- Source URL: https://arxiv.org/abs/2510.08902
- Reference count: 32
- One-line primary result: State-of-the-art BioNER performance on four benchmark datasets with robust zero-shot generalization across Chinese and English

## Executive Summary
This paper introduces a unified biomedical named entity recognition framework that reformulates BioNER as a text generation task using large language models. The approach employs a symbolic tagging strategy with explicit boundary markers to improve entity extraction, combined with bilingual joint fine-tuning across multiple Chinese and English datasets. A contrastive learning-based entity selector is introduced to filter incorrect predictions. The method achieves state-of-the-art performance on four benchmark datasets and demonstrates strong zero-shot generalization on unseen corpora.

## Method Summary
The framework reformulates BioNER as text generation using symbolic tagging with brackets to explicitly mark entity boundaries. It performs multi-dataset bilingual joint fine-tuning with QLoRA on GLM4-9B-Chat using four biomedical datasets (CMeEE-V2, GENIA, BioRED, BC5CDR-Chem). A contrastive learning-based entity selector filters spurious predictions by training on perturbed negative samples. The approach handles both flat and nested entities while maintaining cross-lingual generalization through dataset-level prompt shuffling and shared entity type definitions.

## Key Results
- Symbolic tagging achieves F1=76.15 vs JSON 73.92 vs HTML 70.30 on CMeEE-V2
- 12-13% error reduction in Type, Span, and Spurious categories with contrastive selector
- State-of-the-art performance on four benchmark datasets with robust zero-shot generalization

## Why This Works (Mechanism)

### Mechanism 1
Symbolic tagging with explicit boundary markers improves LLM-based entity extraction by aligning with autoregressive generation patterns. The lightweight symbols ("[" and "]") reduce structural ambiguity during decoding compared to numerical indices or nested tags.

### Mechanism 2
Multi-dataset bilingual joint fine-tuning improves generalization across languages and entity structures by helping the model learn transferable boundary detection patterns rather than dataset-specific heuristics.

### Mechanism 3
Contrastive learning-based entity selector reduces boundary-related prediction errors by training on perturbed negative samples (±1-2 token shifts and type substitutions) to distinguish valid from invalid candidates.

## Foundational Learning

- **Named Entity Recognition (NER) Reformulation**: Why needed - Traditional NER uses sequence labeling, but this approach requires understanding text generation differences and why explicit span annotation matters. Quick check - Can you explain why generating "TYPE: [entity text]" differs from predicting BIO tags per token?

- **Parameter-Efficient Fine-Tuning (QLoRA)**: Why needed - The method uses QLoRA for multi-dataset training; understanding quantization and adapter mechanics is essential. Quick check - What is the trade-off between full fine-tuning and QLoRA in terms of memory vs performance preservation?

- **Contrastive Learning with Hard Negatives**: Why needed - The entity selector relies on constructing meaningful negative samples; understanding positive/negative pair design is critical. Quick check - How would you construct negative samples for an entity selector, and why might random negatives fail?

## Architecture Onboarding

- **Component map**: Tagging Strategy Module → Multi-Dataset Trainer → Contrastive Selector
- **Critical path**: 1) Choose base LLM (GLM4-9B-Chat recommended) 2) Implement symbolic tagging prompt template 3) Prepare multi-dataset training data with sentence-level shuffling 4) Fine-tune with QLoRA (15 epochs, lr=2e-4, batch=6) 5) Generate candidate entities 6) Train selector on 10K positive/negative samples 7) Filter candidates with selector threshold
- **Design tradeoffs**: Symbolic vs JSON/HTML (cleaner generation vs structured metadata); Single vs joint training (improved generalization vs potential interference); Selector overhead (~1.1% F1 gain vs additional inference pass)
- **Failure signatures**: HTML parsing errors on nested entities >2-3 levels; zero-shot collapse on unseen datasets; selector over-filtering causing high precision but low recall; long-range entity misses for complex phrases
- **First 3 experiments**: 1) Tagging strategy ablation on single dataset (GENIA) 2) Selector threshold sweep at [0.3, 0.5, 0.7, 0.9] 3) Zero-shot probe on held-out dataset with entity type definition changes

## Open Questions the Paper Calls Out

- **Open Question 1**: Can incorporating domain-specific medical knowledge graphs effectively resolve the framework's limitation in recognizing complex composite entities involving long-range dependencies? The current framework struggles with semantic integration across sentence fragments, and knowledge graph integration is proposed but not yet implemented or tested.

- **Open Question 2**: Why do reasoning-oriented LLMs (e.g., DeepSeek-R1) underperform general-purpose LLMs after task-specific fine-tuning for BioNER? The paper observes performance drops but doesn't conduct mechanistic analysis of why reasoning capabilities inhibit boundary precision.

- **Open Question 3**: Does the proposed symbolic tagging strategy and bilingual joint training maintain efficacy when extended to low-resource languages with different morphological structures? The study is limited to Chinese and English; it's unclear if the approach generalizes to languages with different tokenization or syntax.

## Limitations
- Symbolic tagging may not generalize to entity types requiring rich attribute representation beyond type and text span
- Zero-shot generalization claims rely on only two unseen datasets without comparison to specialized zero-shot methods
- Performance on highly complex nested structures (beyond 2-3 levels) remains untested
- Selector overhead may not justify additional inference pass in latency-sensitive deployments

## Confidence

- **High confidence**: Symbolic tagging outperforming JSON/HTML alternatives (directly measured in Table II)
- **Medium confidence**: Multi-dataset bilingual joint fine-tuning improves generalization (supported by Table V comparisons)
- **Medium confidence**: Contrastive selector reduces boundary errors (12-13% improvement in Table IV)
- **Low confidence**: Zero-shot generalization claims (based on limited dataset testing)

## Next Checks

1. **Tagging strategy ablation study**: Implement all three strategies (JSON, HTML, Symbolic) on a single dataset (GENIA) and measure parsing error rates and generation consistency to validate symbolic superiority.

2. **Selector threshold optimization**: Train the entity selector and systematically evaluate precision-recall tradeoffs at thresholds [0.3, 0.5, 0.7, 0.9] to determine optimal operating point.

3. **Cross-domain zero-shot probe**: After joint training, evaluate on a held-out biomedical dataset (e.g., NCBI-Disease) with only entity type definitions changed in the prompt, measuring generalization gap versus supervised baseline.