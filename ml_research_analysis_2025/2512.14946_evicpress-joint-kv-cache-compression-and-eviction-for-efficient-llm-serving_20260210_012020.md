---
ver: rpa2
title: 'EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving'
arxiv_id: '2512.14946'
source_url: https://arxiv.org/abs/2512.14946
tags:
- cache
- compression
- quality
- eviction
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EVICPRESS jointly optimizes KV cache eviction and compression across
  multi-tier storage to reduce LLM inference latency while maintaining quality. It
  introduces a utility function to quantify the trade-off between generation quality
  and loading delay for each cache placement configuration, enabling per-context adaptation
  based on compression sensitivity.
---

# EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving

## Quick Facts
- arXiv ID: 2512.14946
- Source URL: https://arxiv.org/abs/2512.14946
- Reference count: 40
- Primary result: Reduces time-to-first-token by up to 2.19x and achieves 2.0–3.6x higher throughput at 80% quality target

## Executive Summary
EVICPRESS addresses the challenge of efficient KV-cache management in LLM serving by jointly optimizing compression and eviction across multi-tier storage. The system introduces a unified utility function that quantifies the trade-off between generation quality and loading delay for each cache placement configuration, enabling per-context adaptation based on compression sensitivity. By profiling each context to determine optimal eviction-compression decisions and periodically updating them, EVICPRESS achieves significant reductions in inference latency while maintaining quality targets.

## Method Summary
EVICPRESS implements a unified utility function that combines quality degradation and loading delay into a single score for each (compression method, ratio, storage tier) configuration. The system profiles each context's compression sensitivity using synthetic queries generated by GPT-5, then applies a greedy heuristic to solve the multi-choice knapsack problem of placing KV caches across storage tiers when capacity constraints arise. Periodic re-profiling adapts to changing workloads, while the utility function enables direct comparison of all possible decisions to maximize total system utility.

## Key Results
- Reduces time-to-first-token by up to 2.19x compared to baselines
- Achieves 2.0–3.6x higher throughput at 80% quality target
- Shows 1.43–3.77× TTFT reduction across 12 datasets and 5 models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly optimizing compression and eviction decisions across all KV caches yields better quality-delay trade-offs than treating them as independent problems.
- Mechanism: A unified utility function `Util(method, ratio, device) = (α·quality - TTFT) · frequency` converts each (compression_method, compression_ratio, storage_tier) configuration into a single score, enabling direct comparison across all possible decisions. The system maximizes total utility by selecting configurations that preserve quality for sensitive contexts while aggressively compressing insensitive ones.
- Core assumption: Quality and loading delay can be meaningfully combined via linear weighting (α) and that frequency predicts future access patterns sufficiently well.
- Evidence anchors:
  - [abstract]: "proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction"
  - [section 4.2]: Explicit formula shows how quality (from compression), TTFT (from loading bandwidth and compressed size), and frequency combine into one score
  - [corpus]: AdaptCache addresses similar storage hierarchy optimization but without the unified utility formulation

### Mechanism 2
- Claim: Per-context profiling of compression sensitivity outperforms uniform compression policies because contexts vary significantly in their tolerance for lossy compression.
- Mechanism: The profiler generates synthetic queries per context using GPT-5, measures quality degradation under each compression configuration, and stores utility scores. Periodic re-profiling adapts to query distribution drift when quality drops below threshold X%.
- Core assumption: Profiling queries are representative of actual queries; compression sensitivity is quasi-stable between re-profiling intervals.
- Evidence anchors:
  - [section 3.1]: Shows coefficient of variation for compression sensitivity ranges 0.078–0.394 across datasets, demonstrating significant per-context variation
  - [section 4.3]: Describes profiling with training queries and periodic re-profiling trigger conditions
  - [corpus]: Weak corpus evidence—EvolKV and SWAN use layer-specific or static heuristics rather than context-level profiling

### Mechanism 3
- Claim: A greedy heuristic provides practical near-optimal solutions to the NP-hard multi-choice knapsack problem of placing KV caches across tiers.
- Mechanism: When a storage tier fills, the algorithm iteratively selects the configuration change (more aggressive compression or eviction to lower tier) with the minimum utility drop until capacity constraints are satisfied. This cascades recursively through lower tiers.
- Core assumption: Local greedy decisions approximately optimize global utility; utility drops are quasi-independent across contexts.
- Evidence anchors:
  - [section 4.4]: Explicitly identifies the problem as "Multi-Choice Knapsack Problem, which is NP-hard" and describes the greedy solution
  - [Figure 9]: Shows EVICPRESS achieves higher CPU hit rates (faster tier) while maintaining quality, validating placement decisions
  - [corpus]: No direct corpus precedent for greedy knapsack approach in related KV cache management systems

## Foundational Learning

- Concept: KV Cache Fundamentals (Prefill vs Decode Phases)
  - Why needed here: Understanding that KV cache is the intermediate state from the compute-intensive prefill phase—and that reusing it skips expensive recomputation—is essential to grasping why caching/compression matters.
  - Quick check question: Why does the prefill phase have super-linear computational complexity with context length, and how does KV cache reuse address this?

- Concept: Storage Tier Hierarchy (GPU → CPU DRAM → SSD → Remote Disk)
  - Why needed here: EVICPRESS's core contribution depends on managing trade-offs across devices with different capacity/bandwidth characteristics (e.g., 20GB/s fast storage vs 2GB/s slow storage in Figure 2).
  - Quick check question: If CPU DRAM bandwidth is ~10× higher than SSD, why does compressing a KV cache before storing on SSD reduce overall TTFT?

- Concept: Lossy Compression Quality-Rate Trade-off
  - Why needed here: The utility function fundamentally balances compression-induced quality degradation against reduced data transfer time.
  - Quick check question: For a 4GB KV cache, what are the two opposing effects of compressing to 0.2GB on the system's utility score?

## Architecture Onboarding

- Component map:
  Lookup Module -> Profiler Module -> Configuration Selection Module -> Retriever -> Storage Backend

- Critical path:
  1. Request arrives → Lookup checks cache existence
  2. Cache hit → Retriever loads from tier, decodes continue
  3. Cache miss → Full prefill runs, store triggers Configuration Selection
  4. Storage full → Greedy eviction/compression cascades through tiers

- Design tradeoffs:
  - Higher α prioritizes quality; lower α prioritizes TTFT (user-tunable)
  - Re-profiling frequency trades GPU overhead against adaptation speed
  - Context-level granularity chosen over chunk-level for global token importance view

- Failure signatures:
  - Quality dropping below X% threshold indicates stale profiles needing re-profiling
  - CPU hit rate declining while SSD hit rate rising suggests suboptimal utility scoring
  - Brief latency spikes during re-profiling intervals (expected, documented in §6.3)

- First 3 experiments:
  1. **Single-context sensitivity validation**: Take one context from LongBench, apply keydiff compression at ratios 0.2–0.9, measure quality drop. Verify variation matches paper's coefficient of variation range (0.078–0.394).
  2. **Two-tier greedy placement simulation**: Implement utility function and greedy algorithm with 50 contexts under constrained CPU DRAM. Confirm sensitive contexts get evicted to SSD while insensitive contexts are compressed and retained on CPU.
  3. **End-to-end TTFT benchmark**: Deploy on vLLM + LMCache with EVICPRESS policies, measure TTFT at varying QPS against keydiff+LRU baseline. Target: reproduce 1.43–3.77× TTFT reduction at equivalent quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can EVICPRESS be adapted to manage KV-cache placement and coherence efficiently in distributed, disaggregated memory architectures?
- Basis in paper: [explicit] Section 8 states, "Extending EVICPRESS to multi-node deployments, cross-GPU cache sharing... is an important direction for future work."
- Why unresolved: The current prototype and evaluation are confined to a single-node setup; distributed systems introduce network latency and consistency challenges not addressed by the current single-node utility function.
- What evidence would resolve it: A demonstration of EVICPRESS running on a multi-node cluster, showing cache coherency overhead and hit rates under distributed workloads.

### Open Question 2
- Question: How can the system maintain optimal utility when storage bandwidth fluctuates more rapidly than the re-profiling frequency?
- Basis in paper: [explicit] Section 8 notes, "When the available bandwidth changes more rapidly than our re-profiling frequency... precomputed utility function scores could temporarily deviate... [future work includes] bandwidth-aware profiling."
- Why unresolved: The current methodology profiles utility based on bandwidth assumptions that may not hold during bursts or network contention, potentially leading to suboptimal eviction decisions.
- What evidence would resolve it: Evaluation results under simulated network contention or "bursty" bandwidth conditions showing the system's ability to dynamically adjust utility scores without waiting for full re-profiling cycles.

### Open Question 3
- Question: Can the profiling module remain efficient if the configuration search space expands to include KV-cache quantization methods alongside token dropping?
- Basis in paper: [explicit] Section 8 discusses future work: "EVICPRESS can be easily extended to choose from KV cache quantization methods... [by] adapting the fraction of tokens to be dropped and the number of bits."
- Why unresolved: The paper currently evaluates token-dropping methods; integrating quantization adds a dimension to the configuration space (bits per element), which could increase profiling overhead significantly.
- What evidence would resolve it: Profiling latency and system overhead measurements when the configuration pool includes both varying compression ratios and quantization bit-widths.

## Limitations

- The synthetic query generation for profiling may not perfectly represent real user queries, potentially leading to suboptimal utility scores
- The greedy heuristic's effectiveness assumes utility drops are quasi-independent across contexts, but the NP-hard nature of the problem suggests potential for suboptimal decisions
- The quality metrics (accuracy, F1, BLEU) may not fully capture user-perceived degradation from lossy compression

## Confidence

- **High confidence**: The core mechanism of unified utility function combining quality and latency is well-justified by the mathematical formulation and experimental results. The profiling approach showing context-level variation in compression sensitivity (CV 0.078-0.394) is empirically validated.

- **Medium confidence**: The greedy heuristic's effectiveness assumes utility drops are quasi-independent across contexts. While experimental results support this, the NP-hard nature of the underlying problem suggests potential for suboptimal decisions in complex scenarios.

- **Low confidence**: The synthetic query generation for profiling may not perfectly represent real user queries, potentially leading to suboptimal utility scores. The frequency-based weighting assumes stationary access patterns that may not hold in dynamic workloads.

## Next Checks

1. **Robustness to query drift**: Implement EVICPRESS with varying re-profiling intervals (5s, 30s, 5min) on a workload with gradually shifting topic distribution. Measure quality degradation over time to quantify how quickly profiles become stale.

2. **Greedy vs optimal comparison**: On a small-scale simulation with 20 contexts and 3 storage tiers, enumerate all possible configurations to find optimal utility. Compare against greedy heuristic's solution quality and runtime overhead to establish approximation ratio bounds.

3. **User perception study**: Deploy EVICPRESS on a real LLM serving system and conduct A/B testing comparing compressed vs uncompressed outputs. Measure not just automated metrics but also human ratings of output quality, particularly for sensitive contexts that profiling identified as quality-critical.