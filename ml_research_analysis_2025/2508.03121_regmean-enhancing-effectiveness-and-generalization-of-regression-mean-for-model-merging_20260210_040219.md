---
ver: rpa2
title: 'RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for
  Model Merging'
arxiv_id: '2508.03121'
source_url: https://arxiv.org/abs/2508.03121
tags:
- regmean
- merging
- tasks
- task
- vit-b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of RegMean, a model merging
  method that independently processes each linear layer, ignoring the propagation
  of features and information through the network. The proposed RegMean++ extends
  RegMean by explicitly incorporating intra- and cross-layer dependencies into the
  merging objective.
---

# RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging

## Quick Facts
- **arXiv ID**: 2508.03121
- **Source URL**: https://arxiv.org/abs/2508.03121
- **Reference count**: 40
- **Primary result**: RegMean++ achieves competitive or state-of-the-art performance across various settings including in-domain/out-of-domain generalization, sequential merging, large-scale tasks, and robustness under distribution shifts.

## Executive Summary
This paper addresses a fundamental limitation of RegMean, a model merging method that processes each linear layer independently, ignoring the propagation of features and information through the network. The proposed RegMean++ extends RegMean by explicitly incorporating intra- and cross-layer dependencies into the merging objective. This is achieved by using the representations from the merge model itself to compute the input features for the linear layers, rather than relying on the candidate models. Extensive experiments demonstrate that RegMean++ consistently outperforms RegMean across various settings, achieving competitive or state-of-the-art performance compared to recent advanced model merging methods.

## Method Summary
RegMean++ modifies the standard RegMean closed-form solution by changing how input features are computed during the merging process. While RegMean uses input features from candidate models, RegMean++ computes these features by passing calibration data through the current state of the merge model itself. Specifically, for each layer, it uses the output of the previous merged layer as input features, creating a sequential dependency where each layer's merge depends on all previous layers. This approach incorporates intra- and cross-layer dependencies into the merging objective, ensuring that weights are optimal for the actual input distribution the merged model will see. The method maintains the closed-form solution advantage (no gradient descent) but introduces computational overhead due to sequential forward passes.

## Key Results
- RegMean++ achieves 84.4% average accuracy on 8 ViT tasks, outperforming standard RegMean (82.4%)
- Maintains >98% accuracy when merging only middle and deep layers, demonstrating layer importance hierarchy
- Shows consistent improvements across in-domain and out-of-domain generalization settings
- Excels in sequential merging scenarios while maintaining competitive performance against state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1: Inference-Aware Input Statistics
RegMean++ improves merging accuracy by replacing candidate model activations with merge model activations in the regression objective. The key difference lies in how input feature X is obtained: RegMean++ computes X based on the activations produced by the previous merge layer. This explicitly incorporates intra- and cross-layer dependencies into the merging objective, ensuring the weights W_M are optimal for the actual input distribution the merged model will see, rather than the distribution of the disconnected candidate models.

### Mechanism 2: Representation Bias Reduction in Non-Linearities
Aligning merge statistics with the merge model's forward pass reduces error accumulation across non-linear components (LayerNorm, GELU). Deep networks interleave linear layers with non-linearities. RegMean treats linear layers independently, ignoring how non-linear transformations distort features between layers. By using f^(l-1)_merge, RegMean++ captures the "true" propagated features, leading to reduced representation bias where the merged model's hidden states align better with the targets than the independent layer approximation.

### Mechanism 3: Layer-wise Knowledge Localization
Merging effectiveness is unevenly distributed across transformer layers; middle and deep layers (specifically MLPs) contribute more to task performance than early layers or attention heads. The paper empirically finds that linear layers in MLPs act as "key-value memories" storing task-relevant knowledge. Merging these specific components using the inference-aware statistics preserves >98% of accuracy even when ignoring early layers.

## Foundational Learning

- **Concept**: Linear Regression for Merging (RegMean)
  - **Why needed here**: RegMean++ is a direct modification of the RegMean closed-form solution. You must understand that RegMean treats weight merging as a least-squares problem minimizing ||XW_merge - XW_candidate||^2.
  - **Quick check question**: Why does the original RegMean formula require the input matrix X (or X^T X) rather than just the weights W?

- **Concept**: Forward Propagation & Activation Statistics
  - **Why needed here**: The core innovation is where the input statistics X come from. Understanding how activations transform layer-by-layer is required to grasp why candidate-model statistics fail to predict merge-model behavior.
  - **Quick check question**: In RegMean++, to compute the weights for Layer 5, do you use the activations from the Candidate Model's Layer 4 or the Merge Model's Layer 4?

- **Concept**: Closed-Form Solutions vs. Optimization
  - **Why needed here**: RegMean++ maintains the "closed-form" benefit (no gradient descent/training) but introduces a computational dependency (sequential forward passes).
  - **Quick check question**: Does RegMean++ require backpropagation/gradients? (No, only forward passes for statistics).

## Architecture Onboarding

- **Component map**: Data Loader -> Merge Model (evolving) -> Candidates (frozen) -> Solver
- **Critical path**:
  1. Initialize Merge Model with weights from a base model (or average of candidates)
  2. Iterate layers l=1 → L:
     - Forward Pass: Pass calibration batch X through the current state of the Merge Model up to layer l-1 to get input features X^(l)_merge
     - Stat Collection: Compute inner-product matrices G^(l)_i using X^(l)_merge
     - Merge: Calculate W^(l)_M using the RegMean closed-form equation
     - Update: Replace W^(l) in Merge Model
  3. Average non-linear/other weights (embeddings, biases) as per standard RegMean

- **Design tradeoffs**:
  - Sequential vs. Parallel: Unlike RegMean (which can be parallelized layer-wise if statistics are pre-computed), RegMean++ is inherently sequential because the input to layer l depends on the merged weights of layer l-1
  - Accuracy vs. Speed: The paper notes a ~2x increase in merging time due to repeated forward passes, but no increase in inference cost

- **Failure signatures**:
  - Matrix Singularity: If X^T X is singular (rare with reasonable batch size), the inverse fails. Mitigation: The α parameter (diagonal reduction) is crucial here
  - OOD Data Mismatch: If the calibration dataset X is vastly different from the training distribution, performance degrades significantly

- **First 3 experiments**:
  1. Sanity Check (RegMean vs. RegMean++): Merge 2 distinct ViT tasks. Verify that X_merge diverges from X_candidate and check if accuracy improves
  2. Ablation on α: Sweep α ∈ [0.1, 0.9]. The paper suggests high α (e.g., 0.95) is optimal. Verify if low α (heavy diagonal regularization) negates the benefits of RegMean++
  3. Layer-wise Isolation: Attempt merging only the MLP layers while averaging Attention layers. Compare this against merging everything to validate the paper's claim that MLPs are the critical path

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does RegMean++ perform on model scales significantly larger than 8 billion parameters?
- **Basis in paper**: The authors note in the Limitations section that due to computational constraints, experiments were capped at 8 billion parameters, risking the overlooking of interesting generalization aspects, and suggest exploring higher model scales as a promising direction.
- **Why unresolved**: The computational cost and potential for representation divergence in ultra-large models remain untested.
- **What evidence would resolve it**: Empirical benchmarks of RegMean++ on frontier models (e.g., Llama-3-70B) comparing accuracy and merging time against baseline methods.

### Open Question 2
- **Question**: What causes the performance degradation of RegMean++ on smaller models (e.g., Llama-3.2-3B) and instruction-following tasks compared to standard RegMean?
- **Basis in paper**: Section 5.8 reports that RegMean++ "underperforms when merging Llama-3.2-3B candidates" and specifically "underperforms RegMean in the instruction following task," though it excels on the larger 8B model.
- **Why unresolved**: The paper acknowledges the performance gap but does not provide an ablation or theoretical explanation for why the cross-layer dependency mechanism fails in these specific instances.
- **What evidence would resolve it**: An ablation study analyzing the layer-wise representation alignment on smaller models versus larger ones, specifically for instruction-following data distributions.

### Open Question 3
- **Question**: Can the computational overhead caused by the sequential dependency in feature collection be reduced or parallelized?
- **Basis in paper**: The authors identify the "increased computational overhead" as a primary limitation, noting that the need for sequential forward passes results in an approximate 2x increase in total merging time compared to RegMean.
- **Why unresolved**: While the accuracy gains are established, the efficiency trade-off limits practical application in rapid prototyping scenarios.
- **What evidence would resolve it**: A modified RegMean++ implementation using caching strategies or approximate parallel feature estimation that maintains accuracy while reducing the time complexity to near-linear.

## Limitations
- Computational overhead: Sequential forward passes result in approximately 2x increase in total merging time compared to RegMean
- Limited ablation studies on the critical hyperparameter α, which affects both performance and numerical stability
- Performance degradation on smaller models (e.g., Llama-3.2-3B) and instruction-following tasks where it underperforms standard RegMean

## Confidence
- **High confidence**: The core mechanism of using merge-model activations for input features is clearly specified and empirically validated across multiple experiments
- **Medium confidence**: The claim about middle/deep layers being more important for merging performance, while supported by Table 6, lacks detailed ablation studies showing why early layers contribute less
- **Low confidence**: The paper's explanation of why RegMean++ works (representation bias reduction) is somewhat hand-wavy and could benefit from more rigorous theoretical justification

## Next Checks
1. **Ablation study on α**: Systematically vary α from 0.1 to 0.99 to determine the sensitivity of RegMean++ to this hyperparameter and identify optimal values across different tasks
2. **Layer-wise performance isolation**: Merge only MLP layers while averaging all other components to quantitatively verify the claim that MLPs contain the majority of task-specific knowledge
3. **Computation complexity analysis**: Measure wall-clock time and memory usage across model sizes (B/16, L/14) to validate the claimed 2x increase and assess practical scalability limits