---
ver: rpa2
title: 'Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities'
arxiv_id: '2512.02973'
source_url: https://arxiv.org/abs/2512.02973
tags:
- image
- text
- visual
- attack
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses jailbreak attacks on multimodal large language
  models (MLLMs), which have been less explored than those on text-only models. The
  proposed method, Contextual Image Attack (CIA), embeds harmful queries into visually
  coherent contexts using a multi-agent system, combining four visualization strategies,
  contextual element enhancements, and toxicity obfuscation.
---

# Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities

## Quick Facts
- **arXiv ID:** 2512.02973
- **Source URL:** https://arxiv.org/abs/2512.02973
- **Authors:** Yuan Xiong; Ziqi Miao; Lijun Li; Chen Qian; Jie Li; Jing Shao
- **Reference count:** 40
- **Primary result:** CIA achieves 86.31% ASR against GPT-4o and 91.07% ASR against Qwen2.5-VL-72B by embedding harmful queries into visually coherent contexts.

## Executive Summary
This paper addresses jailbreak attacks on multimodal large language models (MLLMs), which have been less explored than those on text-only models. The proposed method, Contextual Image Attack (CIA), embeds harmful queries into visually coherent contexts using a multi-agent system, combining four visualization strategies, contextual element enhancements, and toxicity obfuscation. Experiments on MMSafetyBench-tiny and SafeBench-tiny datasets show that CIA significantly outperforms prior baselines, achieving attack success rates of 86.31% and 91.07% against GPT-4o and Qwen2.5-VL-72B respectively, with high toxicity scores, demonstrating the vulnerability of MLLMs to image-centric jailbreak attacks.

## Method Summary
The Contextual Image Attack (CIA) employs a four-agent system to jailbreak MLLMs by embedding harmful queries into visual contexts. The pipeline consists of: (1) a Parser agent that extracts semantic information from the original harmful query, (2) a Text Refiner agent that ensures consistency between the extracted semantics and visualization strategy, (3) an Image Generator agent that creates the visual context using one of four strategies (Demonstration, Sequential Path, Structured Content, Dialogue Layout), and (4) an Image Refiner agent that enhances the image with contextual elements. The attack uses uncensored Qwen models for text generation and Qwen-Image models for image creation, iterating through refinement steps to maximize toxicity while maintaining visual coherence.

## Key Results
- CIA achieves attack success rates of 86.31% against GPT-4o and 91.07% against Qwen2.5-VL-72B on benchmark datasets
- The attack demonstrates high toxicity scores while maintaining visually coherent contexts
- Ablation studies show significant performance degradation (ASR drops to 9.52% for GPT-4o) when using aligned models instead of uncensored ones
- Visual context collapses the separability between benign and harmful representations in MLLMs

## Why This Works (Mechanism)
The attack exploits the vulnerability of MLLMs to semantic embedding within visual contexts. By distributing harmful content across both image and text modalities while maintaining contextual coherence, the attack bypasses standard safety mechanisms that typically focus on explicit textual harm indicators. The multi-agent refinement process iteratively optimizes the visual-textual alignment to maximize toxicity while preserving the appearance of legitimate content.

## Foundational Learning
- **Semantic Embedding in Visual Context:** The technique of hiding harmful instructions within seemingly benign visual layouts - needed to understand how attacks bypass content filters; quick check: can you identify where harmful content is distributed across image/text components?
- **Multi-Agent Iterative Refinement:** The system of coordinated agents that progressively enhance attack effectiveness - needed to grasp the attack's optimization process; quick check: can you trace how each agent modifies the output of the previous one?
- **Visualization Strategy Selection:** The four distinct approaches (Demonstration, Sequential Path, Structured Content, Dialogue Layout) for presenting harmful content - needed to understand the attack's versatility; quick check: can you match each strategy to its visual presentation format?
- **Contextual Element Augmentation:** Techniques like emoji insertion and noise injection to enhance attack effectiveness - needed to understand how minor modifications impact success rates; quick check: can you predict which augmentations work best for each visualization strategy?
- **Toxicity Score Evaluation:** The 1-5 scale metric used to quantify attack success - needed to interpret experimental results; quick check: can you explain what score threshold constitutes a successful attack?
- **Cross-Modal Safety Boundary Collapse:** How visual context eliminates the distinction between safe and harmful representations - needed to understand the fundamental vulnerability; quick check: can you explain why visual context makes detection harder?

## Architecture Onboarding

**Component Map:** Parser -> Text Refiner -> Image Generator -> Image Refiner -> Target Model (GPT-4o/Qwen2.5-VL-72B) -> Judge (GPT-4o)

**Critical Path:** The core attack pipeline flows from semantic extraction through iterative refinement to final evaluation. The most critical components are the uncensored text model (for generating harmful "visual text") and the Image Generator (for creating the contextual layout).

**Design Tradeoffs:** The system trades computational overhead (multiple refinement iterations) for attack effectiveness. Using uncensored models enables the attack but limits reproducibility. The four visualization strategies provide versatility but require careful selection based on the target context.

**Failure Signatures:** 
- Image generation refusal (model blocks harmful content rendering)
- Semantic drift during refinement (output deviates from original harmful intent)
- Low toxicity scores despite technical attack success
- Target model refusing to process the generated image-text pair

**3 First Experiments:**
1. Implement and test the Parser and Text Refiner agents on a small subset of MMSafetyBench-tiny samples
2. Run the Image Generator using the "Demonstration" strategy and evaluate output coherence
3. Test the complete pipeline (all 4 agents) on 5 samples and measure initial ASR and toxicity scores

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can safety alignment mechanisms be adapted to distinguish between benign visual context and adversarial semantic embedding without compromising the model's ability to follow legitimate visual instructions?
**Basis in paper:** The Conclusion states the findings "underscore the need for vastly more robust safety alignment for visual modalities," and Section 4.4 shows that visual context collapses the separability of benign and harmful representations.
**Why unresolved:** The paper demonstrates the failure of current alignment (visual context suppresses safety boundaries) but does not propose or test specific defense strategies to restore this separability.
**What evidence would resolve it:** A demonstration of a training technique or inference-time filter that maintains high benign/harmful embedding separability (as measured by Fisher Ratio) even when visual context is present.

### Open Question 2
**Question:** What specific visual or interaction features determine why certain contextual element augmentations (e.g., emoji insertion, noise injection) succeed in specific visualization strategies (e.g., Demonstration) but fail in others (e.g., Sequential Path)?
**Basis in paper:** Table 3 and the accompanying text note that "the effectiveness of contextual elements varies across scenario strategies," citing conflicting success rates for different augmentations, but provide no causal explanation.
**Why unresolved:** The authors report the empirical variance (e.g., noise improves Sequential Path but hinders Demonstration) but do not analyze the underlying mechanism or feature interaction causing this discrepancy.
**What evidence would resolve it:** An ablation study using attention visualization or feature attribution to correlate specific visual perturbations with token-level attention shifts in different layout contexts.

### Open Question 3
**Question:** Can existing multimodal defense mechanisms (e.g., LlamaGuard, perplexity filters, or cross-modal similarity checks) effectively detect Contextual Image Attacks where the harmful intent is semantically embedded in the image rather than the text?
**Basis in paper:** Related works (Section 2.2 and Appendix A) discuss existing defenses, but the experimental evaluation focuses solely on attack success rates against base models, omitting any evaluation against these specific defense architectures.
**Why unresolved:** It remains untested whether standard detectors, which often rely on textual anomaly detection or explicit image matching, can flag the "seemingly benign visual contexts" constructed by the CIA multi-agent system.
**What evidence would resolve it:** Evaluation of CIA-generated images and text prompts against state-of-the-art defense baselines (e.g., JailGuard, LlamaGuard) to determine detection rates and false positive ratios.

## Limitations
- The attack fundamentally requires uncensored text and image models, making faithful reproduction difficult
- Prompt templates contain placeholders that require manual reconstruction, introducing variability
- Results are specific to the tested MLLM architectures and may not generalize to others
- The attack does not evaluate effectiveness against existing multimodal defense mechanisms

## Confidence
- **High confidence:** The fundamental vulnerability exists - MLLMs can be jailbroken through image-based contextual injection
- **Medium confidence:** The specific ASR and Toxicity Score values reported for CIA (86.31% and 91.07% respectively) against the target models
- **Low confidence:** The generalizability of CIA to other MLLM architectures beyond GPT-4o and Qwen2.5-VL-72B

## Next Checks
1. **Model Access Verification:** Attempt to access or obtain the exact `Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-Abliterated-Uncensored` model weights or equivalent uncensored Qwen text model through available channels to confirm the attack is technically feasible.
2. **Prompt Reconstruction Validation:** Implement the Parser and Text Refiner agents using the provided templates, manually reconstructing the placeholder examples and strategy lists from the paper's methodology description, then test on a small subset of MMSafetyBench-tiny samples.
3. **Cross-Model Generalization Test:** After establishing CIA functionality on the specified models, test the attack against at least one additional MLLM (e.g., Gemini, Claude 3) to assess whether the visual context vulnerability extends beyond the tested architectures.