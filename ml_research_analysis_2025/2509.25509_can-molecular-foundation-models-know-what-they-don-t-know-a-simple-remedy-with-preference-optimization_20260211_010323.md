---
ver: rpa2
title: Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy
  with Preference Optimization
arxiv_id: '2509.25509'
source_url: https://arxiv.org/abs/2509.25509
tags:
- molecular
- detection
- auroc
- scaffold
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting out-of-distribution
  (OOD) molecular samples to prevent "chemical hallucination" where models make high-confidence
  yet incorrect predictions for unknown molecules. The authors propose Mole-PAIR,
  a plug-and-play post-training framework that formulates OOD detection as preference
  optimization using pairwise learning.
---

# Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization

## Quick Facts
- arXiv ID: 2509.25509
- Source URL: https://arxiv.org/abs/2509.25509
- Reference count: 31
- Key outcome: Mole-PAIR improves OOD detection in molecular models with up to 45.8% AUROC gains and FPR95 reductions to zero

## Executive Summary
This paper addresses the critical challenge of out-of-distribution (OOD) detection in molecular foundation models, which can make high-confidence yet incorrect predictions for unknown molecules—a phenomenon termed "chemical hallucination." The authors propose Mole-PAIR, a post-training framework that formulates OOD detection as preference optimization using pairwise learning. By optimizing the relative ranking between in-distribution (ID) and OOD samples through logistic loss, the method aligns with maximizing AUROC, a key metric for OOD detection.

Experimental results across five molecular datasets demonstrate consistent improvements under size, scaffold, and assay distribution shifts. Mole-PAIR achieves up to 45.8% increases in AUROC and reduces FPR95 to zero in many cases, significantly outperforming existing baselines. The approach is described as a plug-and-play solution that requires minimal modifications to existing models while providing substantial gains in reliability for molecular property prediction tasks.

## Method Summary
Mole-PAIR addresses OOD detection by reformulating it as a preference optimization problem using pairwise learning. The framework takes a pre-trained molecular foundation model and fine-tunes it using a logistic loss that compares the relative scores of ID and OOD samples. Instead of predicting absolute probabilities, the model learns to rank ID samples higher than OOD samples, which naturally aligns with maximizing AUROC for OOD detection.

The method requires access to a small set of OOD samples during training, which are used to create pairwise comparisons. During inference, the model computes logits for both the input molecule and OOD samples, and the difference in these logits serves as the OOD score. This approach leverages the model's existing knowledge while teaching it to recognize distributional shifts, making it applicable to various foundation models without requiring architectural changes.

## Key Results
- Mole-PAIR achieves up to 45.8% improvements in AUROC for OOD detection across molecular datasets
- FPR95 (false positive rate at 95% recall) is reduced to zero in many cases, indicating near-perfect specificity
- Consistent performance gains observed across three types of distribution shifts: size, scaffold, and assay variations

## Why This Works (Mechanism)
Mole-PAIR works by transforming OOD detection from a point-wise classification problem into a pairwise ranking problem. Traditional approaches attempt to estimate absolute uncertainty or confidence scores, which can be unreliable for foundation models that tend to produce overconfident predictions even for OOD samples. By contrast, preference optimization directly optimizes the relative ordering between ID and OOD samples.

The logistic loss used in pairwise learning ensures that the model learns to assign higher scores to ID samples compared to OOD samples. This ranking objective is mathematically aligned with AUROC maximization, which measures the model's ability to discriminate between ID and OOD samples across all possible thresholds. The pairwise formulation is particularly effective because it leverages the model's discriminative capabilities rather than requiring it to estimate absolute uncertainty, which is a more challenging task for complex foundation models.

## Foundational Learning
**AUROC (Area Under the ROC Curve)**: Measures the model's ability to discriminate between classes across all thresholds. Needed because it provides a threshold-independent evaluation of OOD detection performance. Quick check: AUROC of 1.0 indicates perfect separation between ID and OOD samples.

**Preference Optimization**: A learning paradigm that optimizes relative comparisons between samples rather than absolute predictions. Needed because it aligns the training objective with the evaluation metric (AUROC) for OOD detection. Quick check: The model should consistently rank ID samples higher than OOD samples during pairwise comparisons.

**Distribution Shift**: The phenomenon where test data differs from training data in terms of underlying distribution. Needed because molecular discovery involves encountering molecules with different scaffolds, sizes, or assay conditions than those seen during training. Quick check: OOD detection should remain effective across various types of distribution shifts (size, scaffold, assay).

**Pairwise Learning**: A machine learning approach that learns from relative comparisons between samples rather than individual labels. Needed because it naturally captures the ordinal nature of OOD detection (ID vs OOD). Quick check: The pairwise loss should converge and produce meaningful rankings between ID and OOD samples.

## Architecture Onboarding

**Component Map**: Foundation Model -> Mole-PAIR Fine-tuning Module -> OOD Score Generator

**Critical Path**: Input Molecule → Foundation Model → Logits → Pairwise Comparison with OOD Samples → OOD Score (Logit Difference)

**Design Tradeoffs**: The method trades computational overhead of fine-tuning for improved OOD detection performance. It requires access to OOD samples and model logits, which may not be available for all models or deployment scenarios.

**Failure Signatures**: Poor performance when OOD samples are not representative of the actual distribution shift encountered. Overfitting to the specific OOD samples used during training rather than learning generalizable OOD detection patterns.

**First Experiments**:
1. Baseline comparison: Evaluate foundation model OOD detection performance without Mole-PAIR fine-tuning
2. Ablation study: Compare pairwise logistic loss vs point-wise loss for OOD detection
3. Sensitivity analysis: Test performance with varying quantities of OOD samples during fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Requires access to a small set of OOD samples during training, which may not always be available
- Effectiveness depends on the quality and representativeness of OOD samples used
- Computational overhead of fine-tuning may be prohibitive for some deployment scenarios
- Focus on molecular property prediction may limit generalizability to other domains

## Confidence
**High Confidence**: Experimental methodology and implementation details are clearly described with reproducible results across multiple datasets. Pairwise preference optimization formulation is theoretically sound with substantial and consistent AUROC improvements.

**Medium Confidence**: "Plug-and-play" characterization is supported by results, but real-world applicability varies with OOD sample availability and computational constraints. Molecular focus may limit broader generalization.

**Low Confidence**: Long-term stability in dynamic molecular discovery environments is unaddressed. Computational overhead relative to practical benefits remains unclear.

## Next Checks
1. Conduct systematic sensitivity analysis of Mole-PAIR's performance when varying quantity and quality of OOD samples during training
2. Test Mole-PAIR on non-molecular datasets (e.g., image or text classification) to assess broader applicability
3. Perform pilot study in actual drug discovery pipeline to measure practical impact on downstream tasks and decision-making processes