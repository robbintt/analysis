---
ver: rpa2
title: 'Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation
  via Language Model Arithmetic'
arxiv_id: '2512.15765'
source_url: https://arxiv.org/abs/2512.15765
tags:
- data
- shapley
- value
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational challenge of computing Shapley
  values for data valuation in large language model (LLM) fine-tuning, where traditional
  approaches require an exponential number of model retrainings. The authors propose
  a novel method that leverages the mathematical structure of Direct Preference Optimization
  (DPO) combined with language model arithmetic to reduce the computational cost from
  exponential to linear in the number of data sources.
---

# Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic

## Quick Facts
- arXiv ID: 2512.15765
- Source URL: https://arxiv.org/abs/2512.15765
- Authors: Mélissa Tamine; Otmane Sakhi; Benjamin Heymann
- Reference count: 40
- Primary result: Reduces Shapley value computation from exponential to linear cost by leveraging DPO's additive reward structure with language model arithmetic

## Executive Summary
This paper addresses the computational challenge of computing Shapley values for data valuation in LLM fine-tuning, where traditional approaches require an exponential number of model retrainings. The authors propose a novel method that leverages the mathematical structure of Direct Preference Optimization (DPO) combined with language model arithmetic to reduce the computational cost from exponential to linear in the number of data sources. By fine-tuning one model per data source and using arithmetic combinations of these models, they can approximate coalition utilities without coalition-specific fine-tuning. The method is demonstrated on SmolLM-135M-Instruct fine-tuned on UltraFeedback data, showing heterogeneous contributions of different data sources across helpfulness and harmlessness dimensions.

## Method Summary
The method trains n LoRA adapters (one per data source) using DPO with β=0.1, then constructs coalition models at inference by combining log-probabilities: s_S(x,y) = Σℓ∈S sℓ(x,y) + (1−|S|)s₀(x,y). This arithmetic combination exploits DPO's additive reward structure where coalition utilities equal the sum of individually trained models' utilities. Shapley values are computed by evaluating all 2^n coalition models on held-out prompts and aggregating marginal contributions. For large n, Monte Carlo permutation sampling reduces inference calls while maintaining accuracy.

## Key Results
- Achieves O(n) fine-tuning cost versus O(2^n) for exact Shapley computation
- Demonstrates heterogeneous data source contributions across helpfulness and harmlessness dimensions
- Enables practical multi-objective data valuation for LLMs at scale
- Shows that coalition models can be exactly reconstructed from individually trained models via log-probability arithmetic

## Why This Works (Mechanism)

### Mechanism 1: Additive Reward Structure in Sequential DPO
Sequential DPO training creates a semi-group structure where coalition utilities can be computed without coalition-specific training. Under DPO's KL-regularized objective, the optimal policy satisfies π*(y|x) ∝ exp(r(x,y)/β)π₀(y|x). When training sequentially across datasets, the implicit reward functions sum: log π*_S(y|x) = (1/β)Σℓ∈S r̂ℓ(x,y) + log π₀(y|x) + C(x). This means the coalition model can be reconstructed from individually trained models using only log-probability arithmetic. Core assumption: Sequential DPO converges to the same solution regardless of dataset ordering (commutativity holds at convergence).

### Mechanism 2: Language Model Arithmetic for Coalition Approximation
Coalition models can be constructed at inference time by combining output probabilities of independently fine-tuned models. Train one LoRA adapter per data source. At inference, combine log-probabilities: s_S(x,y) = Σℓ∈S sℓ(x,y) + (1−|S|)s₀(x,y). The normalization constant C(x) cancels when computing action probabilities. Core assumption: Independently trained adapters can be combined as if they were trained sequentially—the decomposition holds without actual sequential training.

### Mechanism 3: Linear-Cost Shapley via Coalition Approximation
Shapley values can be computed with O(n) fine-tunings instead of O(2^n) by constructing coalition models via arithmetic. Train n models (one per source), construct all 2^n coalition models at inference via log-prob combinations, evaluate utilities using reward models, compute Shapley values. For large n, Monte Carlo permutation sampling further reduces inference calls. Core assumption: Approximated coalition utilities are sufficiently accurate for Shapley estimation.

## Foundational Learning

### Concept 1: Shapley Value Axioms (Cooperative Game Theory)
Why needed: Understanding why Shapley is the "right" solution for fair data valuation—unique value satisfying efficiency, symmetry, dummy, and linearity axioms.
Quick check: Can you explain why efficiency (values sum to total utility) and symmetry (identical marginal contributions get identical values) matter for data market pricing?

### Concept 2: Direct Preference Optimization (DPO) and KL-Regularized RL
Why needed: The entire method relies on DPO's implicit reward structure; PPO/RLHF don't have the same additive properties.
Quick check: What is the closed-form relationship between the optimal policy π* and implicit reward r in DPO? How does β control the strength of preference signals?

### Concept 3: LoRA and Parameter-Efficient Fine-Tuning
Why needed: Implementation uses LoRA adapters for efficient storage and combination. Understanding weight decomposition is critical for correct coalition construction.
Quick check: How does combining LoRA adapter weights at inference differ from weight averaging? Why can we combine log-probabilities instead?

## Architecture Onboarding

### Component Map:
Base Model (frozen) → LoRA Adapters (n total) → Coalition Builder → Utility Evaluator → Shapley Computer

### Critical Path:
1. Partition data into n disjoint sources with preference pairs (x, y⁺, y⁻)
2. Train n LoRA adapters with identical DPO hyperparameters
3. For each coalition S ⊆ N: build coalition model, sample responses, compute utility v(π̂_S)
4. Compute Shapley values: φᵢ = Σ_{S⊆N\{i}} (|S|!(n−|S|−1)!)/n! × [v(π̂_{S∪{i}}) − v(π̂_S)]

### Design Tradeoffs:
- Exact vs. sampled Shapley: Exact feasible for n≤15; Monte Carlo for larger n
- Sequential vs. independent training: Paper claims equivalence at convergence; validate empirically
- Reward model selection: Different rewards reveal different value profiles (Figure 2 shows helpfulness/harmlessness trade-offs)
- Assumption: Consistent β across all adapters required for algebraic validity

### Failure Signatures:
- Inconsistent β values: Breaks additive reward structure
- Under-trained adapters: Convergence issues violate commutativity
- Negative Shapley values: Informative—source hurts average coalition performance
- High variance across coalition orderings: Signals inadequate convergence

### First 3 Experiments:
1. Reproducibility check: Replicate SmolLM-135M + 4 UltraFeedback sources; compare Figure 2 profiles (flan_v2_niv2, sharegpt, evol_instruct, ultrachat)
2. Scaling test: Increase to n=8 sources; compare exact Shapley (2⁸=256 coalitions) vs. Monte Carlo with 100/500/1000 permutation samples
3. Convergence ablation: Train adapters with 1/2/4/8 epochs; measure how coalition approximation accuracy degrades with under-training

## Open Questions the Paper Calls Out
None

## Limitations
- Core mechanism relies on DPO's additive reward structure, which may not hold for all training objectives
- Demonstrated only on SmolLM-135M with 4 data sources; scalability to larger models and more sources needs validation
- Shapley values depend on external reward models, making valuations sensitive to reward model quality and calibration

## Confidence

**High Confidence**: The mathematical framework connecting DPO's KL-regularized objective to additive reward structures is well-established in the RL literature. The linear reduction in fine-tuning cost (from exponential to linear) is a direct consequence of the arithmetic combination approach, assuming the underlying mechanism holds.

**Medium Confidence**: The empirical demonstration on SmolLM-135M shows heterogeneous data source contributions, but with only 4 sources and limited model scales, the generalizability to broader applications (data markets, incentive design) requires additional validation. The convergence requirements for the sequential equivalence assumption are stated but not empirically characterized.

**Low Confidence**: Claims about "unlocking applications in data markets" and "interpretable training" are aspirational and not directly supported by the presented experiments. The sensitivity of Shapley values to reward model choice and the method's performance with highly heterogeneous data sources remain unvalidated.

## Next Checks
1. **Convergence characterization**: Systematically vary training epochs (1/2/4/8) and measure how coalition approximation accuracy degrades. Quantify the minimum effective training required for the arithmetic combination to produce accurate coalition utilities.

2. **Order invariance test**: Train the same set of adapters using different dataset orderings (random permutations) and measure variation in coalition utilities. Characterize the relationship between convergence quality and order invariance.

3. **Scaling experiment**: Apply the method to a larger model (e.g., 1B+ parameters) and increase the number of data sources to 8-12. Compare exact Shapley computation (feasible for n=8) with Monte Carlo approximations to validate the sampling approach for larger coalition spaces.