---
ver: rpa2
title: 'Reasoning about Affordances: Causal and Compositional Reasoning in LLMs'
arxiv_id: '2502.16606'
source_url: https://arxiv.org/abs/2502.16606
tags:
- llms
- claude
- gpt-4o
- humans
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the causal and compositional reasoning abilities
  of Large Language Models (LLMs) in object affordances tasks. Humans and LLMs were
  asked to select unconventional objects to replace typical tools for specific purposes.
---

# Reasoning about Affordances: Causal and Compositional Reasoning in LLMs

## Quick Facts
- arXiv ID: 2502.16606
- Source URL: https://arxiv.org/abs/2502.16606
- Reference count: 26
- Key outcome: GPT-4o with chain-of-thought prompting achieved near-human performance (86% accuracy) in tool innovation tasks, significantly outperforming GPT-3.5 (47%) and Claude 3.5.

## Executive Summary
This study examined causal and compositional reasoning in Large Language Models through tool innovation tasks where models must select unconventional objects to replace typical tools. The research compared GPT-4o, Claude 3.5, and GPT-3.5 across three conditions: Standard (4 options), Distractor (9 options), and Image (visual presentation). Results showed GPT-4o with chain-of-thought prompting matched human performance in the Standard condition, while both GPT-4o and Claude 3.5 demonstrated superior ability to identify and flexibly apply causally relevant object properties compared to their predecessors. The study revealed that increasing option complexity and adding visual information affected model performance differently, suggesting variations in reasoning robustness across architectures.

## Method Summary
The study used 20 tool innovation questions with three conditions: Standard (4 options: 2 associated-but-unsuitable, 1 correct, 1 irrelevant), Distractor (9 options: 4 associated, 4 irrelevant, 1 correct), and Image (visual presentation of 4 options). LLMs were prompted with standard questions ("Which one would you use?") and chain-of-thought variants ("Evaluate each option separately before specifying your choice"). Performance was measured by accuracy across multiple runs, with results compared to human baselines (100-300 participants). Statistical analysis used ANOVA and t-tests with Bonferroni correction, treating model runs as pseudo-participants for comparison.

## Key Results
- GPT-4o with chain-of-thought prompting achieved 86% accuracy in Standard condition, matching human performance (88%) and significantly outperforming GPT-3.5 (47%)
- Claude 3.5 showed strong performance but lagged behind GPT-4o across all conditions
- The Distractor condition (9 options) significantly reduced performance across all models and humans, with GPT-4o and Claude 3.5 degrading ~10-11% versus humans' 9.1%
- Claude 3.5 experienced a 28% accuracy drop in Image condition, while GPT-4o and humans showed minimal impact

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-thought prompting enables systematic property evaluation, improving compositional reasoning performance by forcing explicit evaluation of each option's properties against task requirements, reducing reliance on associative retrieval from training data.

### Mechanism 2
Advanced LLMs (GPT-4o, Claude 3.5) encode flexible, decomposable representations of object properties that generalize beyond training co-occurrences by decomposing objects into abstract properties (hardness, flatness, waterproofness) and recombining them for novel tool-use scenarios.

### Mechanism 3
Increasing option complexity exposes reasoning robustness differences across model generations, with the Distractor condition (9 options vs. 4) forcing models to maintain reasoning over longer evaluation sequences, revealing whether performance stems from genuine capability or task-specific shortcuts.

## Foundational Learning

- **Object Affordances**: Understanding what actions objects enable based on their properties, independent of typical use cases. *Quick check*: Can you explain why a swimming cap might serve as a water container despite being designed to keep water out?

- **Causal vs. Associative Reasoning**: Distinguishing between statistical associations (screwdriver ~ hammer) and causal necessity (flat, heavy surface needed). *Quick check*: Why might selecting a screwdriver to replace a hammer reflect associative retrieval rather than causal reasoning?

- **Compositional Generalization**: Decomposing tools into abstract properties and recomposing them onto novel objects. *Quick check*: What properties must be extracted from "broom" to recognize "wig" as a valid substitute for sweeping?

## Architecture Onboarding

- **Component map**: Task generator (20 questions × 3 conditions) → Model interface (API calls with temperature control, prompt variants) → Evaluation (per-question accuracy, aggregated by model run) → Qualitative analysis (manual review of reasoning traces)

- **Critical path**: 1. Define task with goal, typical tool, afforded object, associated objects, distractors; 2. Construct prompt (standard: "Which one?"; CoT: "Evaluate each option separately"); 3. Collect N responses per configuration (N=25 for temperatures, N=10 for CoT); 4. Aggregate responses by model run, compare to human baseline (N=100-300)

- **Design tradeoffs**: Temperature 0 vs. 1 (deterministic outputs vs. variance capture); Standard vs. CoT prompt (comparability vs. eliciting reasoning); Aggregation method (treating model runs as "participants" enables human comparison but violates independence assumptions)

- **Failure signatures**: Selecting associated objects over afforded objects (associative retrieval dominance); Contradictory property evaluations within same response (inconsistent world model); Large accuracy drops in Image condition (Claude 3.5: -28%) (modality integration weakness); Inability to recognize functional analogies (e.g., wig ~ broom bristles) (limited compositional flexibility)

- **First 3 experiments**: 1. Replicate Standard condition with your target model using both prompt types; verify CoT improvement magnitude; 2. Test Distractor condition to assess reasoning robustness; compare degradation slope to reported baselines (human: -9.1%, GPT-4o: -10%, Claude 3.5: -10%); 3. Add adversarial variant where multiple options share correct causal properties but differ on secondary constraints (tests over-reliance on single-property matching)

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs solve tool innovation tasks using robust causal algorithms or a "bag of heuristics"? The study relies on behavioral output which cannot distinguish between coherent causal world models and sophisticated pattern matching. Evidence that would resolve this includes mechanistic interpretability studies identifying specific internal circuits that represent abstract causal variables.

### Open Question 2
Do multimodal LLMs reason in a shared amodal representational format, or do they translate visual inputs into text-based descriptions? GPT-4o performed similarly on text and image tasks, which supports both "translation" and "amodal reasoning" hypotheses equally. Evidence that would resolve this includes analysis of internal activation patterns showing whether visual and textual inputs map to the same neural circuits.

### Open Question 3
Can causal knowledge of object affordances acquired from text transfer to real-world physical manipulation? The experiments tested decision-making in a static, virtual context rather than physical execution. Evidence that would resolve this includes evaluating LLMs embodied in robots or agents within physics simulators to determine if they can manipulate objects as successfully as they select them.

## Limitations
- Synthetic task design limits ecological validity as real-world tool innovation involves physical interaction and feedback loops absent from text/image prompts
- Aggregation of model runs as pseudo-participants for statistical comparison violates independence assumptions, potentially inflating significance metrics
- Cannot distinguish whether CoT prompting improves genuine reasoning versus surface-level response structuring

## Confidence
- **High Confidence**: GPT-4o with CoT prompting achieving near-human performance (86% vs 88%) in Standard condition; Claude 3.5 showing intermediate performance
- **Medium Confidence**: Causal property identification mechanisms; compositional generalization claims require stronger corpus validation
- **Low Confidence**: Image condition results (particularly Claude 3.5's 28% drop); generalizability of task design to broader reasoning domains

## Next Checks
1. Test whether CoT benefits persist when prompts explicitly forbid "thinking step-by-step" language but require systematic evaluation
2. Compare model performance on tasks where multiple options share correct causal properties but differ on secondary constraints
3. Validate that performance differences reflect reasoning quality rather than instruction-following capability by using reverse prompts ("Explain why option X is wrong")