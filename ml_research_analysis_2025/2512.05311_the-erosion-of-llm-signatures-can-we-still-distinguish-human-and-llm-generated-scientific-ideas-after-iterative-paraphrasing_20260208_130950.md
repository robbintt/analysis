---
ver: rpa2
title: 'The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated
  Scientific Ideas After Iterative Paraphrasing?'
arxiv_id: '2512.05311'
source_url: https://arxiv.org/abs/2512.05311
tags:
- research
- idea
- ideas
- stages
- paraphrasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether LLM-generated scientific ideas
  can be reliably distinguished from human-generated ones after multiple paraphrasing
  stages. It constructs a dataset from 846 papers and uses LLMs to generate and paraphrase
  ideas through five iterative stages using four strategies: general paraphrase, simplified
  summary, brief summary, and detailed technical paraphrase.'
---

# The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?

## Quick Facts
- **arXiv ID**: 2512.05311
- **Source URL**: https://arxiv.org/abs/2512.05311
- **Reference count**: 21
- **Key outcome**: Iterative paraphrasing erodes LLM signatures, reducing detection accuracy by 25.4% F1-score drop across 5 stages

## Executive Summary
This paper investigates whether LLM-generated scientific ideas can be reliably distinguished from human-generated ones after multiple paraphrasing stages. The authors construct a dataset from 846 papers and use LLMs to generate and paraphrase ideas through five iterative stages using four strategies. Detection performance declines significantly across stages, with an average 25.4% drop in F1-score after five paraphrases. Notably, simplified paraphrasing intended for non-expert audiences reduces detectability the most. These findings show that iterative paraphrasing erodes characteristic LLM signatures, challenging the reliability of existing classifiers that rely heavily on linguistic patterns rather than deeper conceptual structures.

## Method Summary
The study uses 846 papers from five A* CS conferences (2017-2021) to create a dataset of human-written research ideas. LLMs (GPT-4o, GPT-4o-mini, O3-mini, Claude-3.5 models) extract research problems and generate ideas, which are then iteratively paraphrased through five stages using four strategies: general paraphrase, simplified summary, brief summary, and detailed technical paraphrase. The resulting 28,764 paraphrased versions are classified using fine-tuned transformers (BERT, RoBERTa, BigBird, T5) and embedding models (MiniLM, GIST, GTE, Stella) with 2-layer FFNN heads. The classification task distinguishes human-written from LLM-generated ideas, with and without research problem context.

## Key Results
- Detection performance declines by an average of 25.4% F1-score after five consecutive paraphrasing stages
- Simplified paraphrasing for non-expert audiences causes the most significant degradation in detection accuracy
- Incorporating research problem context as input improves detection performance by up to 2.97%
- Technical terminology distributions serve as partial detection signals that are removed during simplification

## Why This Works (Mechanism)

### Mechanism 1
Iterative paraphrasing reduces the separability of human and LLM-generated ideas in embedding space by smoothing distributional differences through repeated surface-level modifications.

**Core assumption**: Initial distinguishability stems primarily from surface-level stylistic markers rather than deeper conceptual patterns.

**Evidence anchors**: 25.4% average detection drop; declining Fisher's Discriminant Ratio; declining Word Mover's Distance.

**Break condition**: If detection relied on semantic or reasoning-level features invariant to paraphrasing, performance would remain stable.

### Mechanism 2
Removing technical terminology through simplification disproportionately degrades detection accuracy by eliminating domain-specific vocabulary that serves as a proxy signal.

**Core assumption**: Technical terminology distributions differ between human and LLM-generated scientific writing in detectable ways.

**Evidence anchors**: Consistent deterioration under simplified paraphrasing; correlation of specific words with human vs LLM classification.

**Break condition**: If classifiers learned robust semantic features independent of register and terminology, simplification would not selectively harm performance.

### Mechanism 3
Providing research problem context improves detection by enabling models to learn problem-solution coherence patterns that may differ between human and LLM outputs.

**Core assumption**: Humans and LLMs exhibit systematically different patterns in how their proposed solutions relate to stated research problems.

**Evidence anchors**: Up to 2.97% improvement with RP+idea; consistent performance advantage across embedding models.

**Break condition**: If the problem-idea relationship is semantically equivalent across human and LLM outputs, context would provide no discriminative benefit.

## Foundational Learning

- **Fisher's Discriminant Ratio (FDR)**: Measures how separable two classes are in feature space. Understanding FDR decline explains why detection becomes harder—distributions overlap more after paraphrasing. *Quick check*: If FDR is near zero, what does that imply about classifier performance?

- **Multi-stage cascade paraphrasing**: The core experimental intervention where each stage applies one of four paraphrase types, avoiding consecutive repeats to prevent compression artifacts. *Quick check*: Why might consecutive identical paraphrase operations cause data quality issues?

- **Macro F1-score**: Primary evaluation metric for detection. Macro averaging gives equal weight to both classes, appropriate for potentially imbalanced detection scenarios. *Quick check*: Why is F1 preferred over accuracy when classes may be imbalanced or the cost of false positives/negatives differs?

## Architecture Onboarding

- **Component map**: Problem extraction → Idea generation → 5-stage paraphrasing (4 strategies) → 28,764 total variants → Fine-tuned classifiers + embedding models → Classification

- **Critical path**: Problem extraction quality → Idea generation fidelity → Paraphrasing consistency → Train-test split integrity → Stage-wise evaluation

- **Design tradeoffs**: Dataset scope limited to CS conferences pre-2021; 5 paraphrasing stages chosen pragmatically without determining chance-level performance; black-box LLMs prevent attribution of detection failures to specific behaviors

- **Failure signatures**: Early plateau at higher loss in later stages; validation loss upward drift in later stages; combined-stage training harms early-stage performance

- **First 3 experiments**:
  1. **Baseline verification**: Train BERT on Stage 1 data only. Verify ~85-90% F1 on held-out Stage 1 test set to confirm initial separability.
  2. **Cross-stage generalization**: Train on Stage 1, evaluate on Stages 2-5. Quantify degradation curve to confirm the 25.4% average drop.
  3. **Context ablation**: Compare idea-only vs. RP+idea for Stella+FFNN. Verify 2-3% improvement with context.

## Open Questions the Paper Calls Out

### Open Question 1
Do the challenges in distinguishing human and LLM-generated ideas persist across non-CS scientific disciplines? The study is restricted to CS conferences, limiting assessment of LLM idea-generation capacity in fields like Psychology or Economics. *Evidence*: Evaluation of detection performance using datasets from non-CS conferences or journals.

### Open Question 2
Can incorporating the LLM's reasoning trajectory during idea generation provide a more robust signal for detection than the final text alone? Current detection failures stem from classifiers relying on surface-level linguistic patterns. *Evidence*: Comparing classifiers trained on final output versus step-by-step reasoning traces.

### Open Question 3
At what specific iterative paraphrasing stage does detection performance drop to chance levels? While the study observes a 25.4% drop by Stage 5, the progression to complete indistinguishability is not mapped. *Evidence*: Extending the pipeline beyond five stages and monitoring when macro F1-score approaches random guessing.

### Open Question 4
Does a cross-attention modeling structure improve the integration of research problem context compared to simple concatenation? The current method provides only modest performance gains, suggesting the interaction between context and idea is not fully captured. *Evidence*: Comparative study of classification accuracy between FFNN with concatenated inputs and a model utilizing cross-attention.

## Limitations
- Dataset narrowly focused on CS conference papers (2017-2021), limiting generalizability to other scientific domains
- Study doesn't establish whether performance degradation reaches chance level at any stage, only quantifying the 25.4% average drop
- LLM black-box nature prevents attributing detection failures to specific model behaviors versus linguistic transformations
- Dataset and full prompt details are not publicly available, hindering independent verification

## Confidence

- **High**: Iterative paraphrasing demonstrably erodes detection performance (25.4% F1 drop verified through multiple embedding models and classifiers)
- **Medium**: Technical terminology serves as a detection signal that simplification removes (supported by consistent simplified-paraphrase degradation patterns)
- **Medium**: Problem-solution coherence patterns differ systematically between humans and LLMs (2-3% detection improvement with context is measurable but relatively modest)

## Next Checks

1. **Domain Transfer Test**: Evaluate the same detection pipeline on non-CS domains (e.g., biomedical or social science papers) to assess generalizability beyond the current dataset

2. **Chance-Level Determination**: Identify the specific paraphrasing stage where detection performance reaches random chance to understand the practical limits of current approaches

3. **Feature Ablation Study**: Conduct controlled experiments isolating technical terminology's contribution by systematically adding/removing domain-specific vocabulary while holding other factors constant