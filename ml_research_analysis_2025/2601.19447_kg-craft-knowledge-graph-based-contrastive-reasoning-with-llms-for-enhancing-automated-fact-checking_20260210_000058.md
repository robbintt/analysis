---
ver: rpa2
title: 'KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing
  Automated Fact-checking'
arxiv_id: '2601.19447'
source_url: https://arxiv.org/abs/2601.19447
tags:
- contrastive
- claim
- questions
- knowledge
- kg-craft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KG-CRAFT introduces a knowledge graph-based contrastive reasoning
  framework for automated fact-checking, addressing the challenge of verifying claims
  by constructing structured evidence from claims and reports. The method first builds
  a knowledge graph from textual input, then formulates contrastive questions grounded
  in this structure, generates evidence-based answers, and synthesizes a concise summary
  for veracity assessment by LLMs.
---

# KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking

## Quick Facts
- arXiv ID: 2601.19447
- Source URL: https://arxiv.org/abs/2601.19447
- Reference count: 34
- Primary result: Achieves 73.87% F1-score on LIAR-RAW, outperforming baselines by up to 30.2 points

## Executive Summary
KG-CRAFT introduces a knowledge graph-based contrastive reasoning framework for automated fact-checking that constructs structured evidence from claims and reports. The method builds a knowledge graph from textual input, generates contrastive questions grounded in this structure, answers them using reports, and synthesizes a concise summary for veracity assessment by LLMs. Evaluations on LIAR-RAW and RAWFC datasets show state-of-the-art performance, with KG-CRAFT achieving up to 73.87% F1-score on LIAR-RAW. The framework demonstrates particular effectiveness in enabling smaller language models to perform verification tasks typically requiring larger models.

## Method Summary
KG-CRAFT operates through a four-stage pipeline: first, it constructs a knowledge graph from claims and reports using LLM-based entity extraction and relation identification; second, it formulates contrastive questions by swapping entities of the same semantic class to create "Why X rather than Y?" queries; third, it answers these questions using the report context and summarizes the answers; finally, it classifies the claim's veracity using the generated summary. The framework employs Maximal Marginal Relevance to select the most informative questions and can use various LLMs at different stages, with Claude 3 Haiku for extraction and Claude 3.7 Sonnet for verification showing strong performance.

## Key Results
- Achieves 73.87% F1-score on LIAR-RAW (6-class classification), outperforming traditional models by 30.2 points and LLM-only baselines by 13.6 points
- Demonstrates robust performance across different numbers of contrastive questions, with K=5 providing optimal balance between accuracy and cost
- Enables smaller language models to achieve competitive results, with SmolLM2 1.7B reaching 73.40% F1-score
- Shows consistent improvements on RAWFC dataset (2-class classification) with 79.92% F1-score

## Why This Works (Mechanism)

### Mechanism 1: Structured Semantic Contrast
Grounding contrastive reasoning in a Knowledge Graph forces the model to distinguish between semantically similar entities rather than relying on superficial text patterns. The framework extracts entities and classifies them (e.g., Person, Organization), then generates "Why X rather than Y?" questions by swapping entities of the same class. This forces the reasoning process to look for specific evidence differentiating the claim entity from its peers, reducing the chance of the model hallucinating distinctions based on wording alone.

### Mechanism 2: Evidence Distillation via Contrastive QA
Answering contrastive questions acts as a filter that distills relevant evidence from noisy reports, improving the signal-to-noise ratio for the final verifier. Instead of feeding raw reports directly to the classifier, KG-CRAFT uses the questions to extract specific answers, which are then summarized. This intermediate step forces the model to retrieve only information pertinent to the contrast, effectively discarding unrelated narrative details in the source documents.

### Mechanism 3: Small Language Model (SLM) Scaffolding
Externalizing the reasoning structure (KG + Questions) allows smaller models to perform verification tasks typically requiring larger models. The heavy lifting of structuring the problem (graph extraction, question generation) is handled by the framework/larger models. The final verification step becomes a classification task on a clean, structured summary, which is within the capabilities of SLMs.

## Foundational Learning

- **Concept: Contrastive Explanations**
  - Why needed here: The core logic of the paper relies on "Why X rather than Y?" questioning. Understanding that explanations are often comparative (counterfactual) rather than purely causal is essential to grasping the method's design.
  - Quick check: Can you distinguish between asking "Why did the stock market crash?" vs. "Why did the stock market crash rather than just correct?"

- **Concept: Knowledge Graph Construction (Triple Extraction)**
  - Why needed here: The first phase of KG-CRAFT depends entirely on converting unstructured text into (Head, Relation, Tail) triples.
  - Quick check: Given the sentence "Apple CEO Tim Cook announced the new iPhone," what are the likely triples and entity types?

- **Concept: Maximal Marginal Relevance (MMR)**
  - Why needed here: The paper uses MMR to select the top K questions. You need to know that MMR balances relevance (to the claim) with diversity (avoiding redundant questions).
  - Quick check: Why would a system pick a question that is slightly less semantically similar to the claim than another, if the second question is very similar to an already selected one?

## Architecture Onboarding

- **Component map:** KG Extractor -> Question Generator -> QA Engine -> Summarizer -> Verifier
- **Critical path:** The Entity Typing in the KG Extractor is the most fragile step. If entities are misclassified (e.g., a generic noun typed as a specific entity), the Question Generator creates invalid contrasts (e.g., "Why did *the economy* do X rather than *John*?"), cascading into irrelevant answers and failed verification.
- **Design tradeoffs:**
  - Cost vs. Accuracy: The pipeline requires multiple LLM calls (Extraction -> Answering -> Summarizing -> Verifying). This increases latency and cost compared to single-shot prompting but significantly boosts accuracy (as shown in ablation studies).
  - K (Number of Questions): The paper finds K=5 is a sweet spot. Higher K increases cost with diminishing returns; lower K risks missing critical evidence.
- **Failure signatures:**
  - Entity Hallucination: The KG Extractor creates relations not present in the text.
  - Empty Answers: The QA engine cannot answer generated questions because the "contrast" was artificial and not addressed in the reports.
  - Summary Drift: The Summarizer omits the specific contrasts found in the Q&A step, reverting to a generic summary that loses the "contrastive" reasoning.
- **First 3 experiments:**
  1. KG Quality Audit: Run the extraction prompt on 10 random samples and manually verify if the extracted triples and entity types make sense. Check specifically for "Type Errors."
  2. Ablation on K: Replicate the K experiment (Figure 2) on a small validation set to see if the "sweet spot" of K=5 holds for your specific domain or if you need more questions for complex reports.
  3. LLM vs. KG Questions: Compare the quality of questions generated by Algorithm 1 (KG-based) vs. a zero-shot prompt asking the LLM to "generate contrastive questions." Measure the "relevance" of the resulting answers to see if the structural constraint actually helps your model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the contrastive answer summaries generated by KG-CRAFT be effectively leveraged to produce coherent, human-readable explanations for claim veracity?
- Basis in paper: The Conclusion states: "For future work, we propose... the generation of explanations for full AFC based on contrastive answer summaries."
- Why unresolved: The current work focuses on predicting the veracity label (V_C) using the summary (A_C), but does not explicitly format this summary into a final justification for the end user.

### Open Question 2
- Question: To what extent does KG-CRAFT maintain performance across non-English languages and domains outside of political and public health fact-checking?
- Basis in paper: The Conclusion proposes "an extended evaluation with other domains," and the Limitations section notes that future work should "explore broader language coverage."
- Why unresolved: The paper acknowledges all experiments were conducted in English, and performance may vary due to limitations in intermediate components like entity linking for other languages.

### Open Question 3
- Question: How sensitive is the final veracity prediction to errors or noise in the initial LLM-based Knowledge Graph extraction step?
- Basis in paper: The authors admit they "do not qualitatively verify the intermediate components," specifically noting that KG construction is "particularly sensitive and central to the method's overall performance."
- Why unresolved: The pipeline assumes the LLM correctly extracts entities and relations, but an analysis of how extraction errors propagate to contrastive question generation and final verification is missing.

## Limitations
- The framework relies heavily on LLM-based extraction accuracy, but prompt examples are abbreviated, making exact replication difficult
- Performance may degrade with sparse or unrelated source documents where contrastive questions cannot be meaningfully answered
- The framework has only been evaluated in English and on political/public health domains, limiting generalizability

## Confidence

| Assessment Area | Confidence Level | Justification |
|-----------------|------------------|---------------|
| Core pipeline structure | High | Well-defined architecture with convincing ablation results |
| Quantitative results | Medium | F1-scores appear solid but lack of detailed prompt examples introduces uncertainty |
| Generalizability | Medium | Results supported but applicability beyond tested datasets remains to be validated |

## Next Checks

1. Implement KG extraction with full prompt examples and audit 10 random samples for entity typing accuracy - identify if Type Errors occur frequently enough to break the contrastive question logic.
2. Replicate the MMR question selection process with different embedding models to assess sensitivity to the unspecified embedding choice.
3. Test the framework on a dataset with known sparse evidence (similar to the SciFact ablation) to measure performance degradation and identify the extraction/summary thresholds where the pipeline fails.