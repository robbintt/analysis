---
ver: rpa2
title: Language Models reach higher Agreement than Humans in Historical Interpretation
arxiv_id: '2504.02572'
source_url: https://arxiv.org/abs/2504.02572
tags:
- historical
- llms
- phase
- humans
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares historical annotations by humans and LLMs on
  the same tasks: annotating historical decades with labels representing either cultural/social
  (Structural Demographic Theory) or economic (Big Cycle) cycles. Three LLMs (GPT-4,
  Claude 3.5, Gemini 1.5) and three human annotators labeled the same set of historical
  descriptions.'
---

# Language Models reach higher Agreement than Humans in Historical Interpretation

## Quick Facts
- **arXiv ID:** 2504.02572
- **Source URL:** https://arxiv.org/abs/2504.02572
- **Reference count:** 22
- **Primary result:** LLMs achieved higher inter-annotator agreement than humans on historical interpretation tasks (Fleiss' Kappa 0.33 vs 0.09 for 5-label task)

## Executive Summary
This study compares historical annotations between three large language models (GPT-4, Claude 3.5, Gemini 1.5) and three human annotators on the same historical texts. The task involved labeling historical decades using two different cycle theories: Structural Demographic Theory (5 labels) and Big Cycle (3 labels). The researchers found that LLMs exhibited significantly higher agreement among themselves than human annotators, particularly on the more complex 5-label task. The study also identified cultural bias patterns, with only one LLM showing consistent Eurocentric bias while all human annotators displayed this tendency. The findings suggest LLMs could enable large-scale, consistent historical annotation for digital humanities research.

## Method Summary
The researchers used a subset of 106 examples from the Chronos dataset containing historical descriptions from various polities and time periods. Three LLMs were prompted using specific instructions that included role-playing as expert historians, few-shot examples, and sequential constraints (e.g., "A cycle cannot turn back"). Three human annotators (students without history expertise) performed the same annotation task. Agreement was measured using Fleiss' Kappa for inter-annotator consistency. The study compared agreement on two theoretical frameworks: Structural Demographic Theory (5 labels) and Big Cycle (3 labels).

## Key Results
- LLMs showed significantly higher agreement than humans on the 5-label SDT task (Fleiss' Kappa 0.33 vs 0.09)
- Agreement was comparable on the 3-label Big Cycle task (0.23 vs 0.25)
- All human annotators displayed consistent Eurocentric bias, while only one LLM did
- LLM disagreements stemmed primarily from skipping phases or constraint violations, while human disagreements reflected personal biases
- The study demonstrates potential for LLM-based large-scale historical annotation

## Why This Works (Mechanism)

### Mechanism 1: Consensus via Statistical Regularization
LLMs converge on the most statistically probable interpretation of text, filtering out idiosyncratic subjective variance. The study found humans disagree based on personal biases (political views, background), leading to low Fleiss' Kappa scores (0.091). In contrast, LLMs share a common knowledge base from training data and tend to cluster around the same statistical mode when given clear guidelines, resulting in higher consensus (Fleiss' K 0.330).

### Mechanism 2: Cultural Bias as a Retrieval Function
Cultural bias in LLMs reflects retrieval failure where models prioritize dominant Western narratives from training data over prompt instructions. The paper showed Gemini 1.5 labeling China's 1960s as "Crisis" rather than "Growth" (SDT theory), suggesting the model retrieved the semantic association "Cultural Revolution = Violence = Crisis" from pre-training weights, overriding the prompt's instruction to look for "fresh culture that creates social cohesion."

### Mechanism 3: Logic Errors as Context Window Drift
LLMs make structural "skipping" errors due to attention mechanisms failing to maintain strict sequential constraints over longer contexts. The paper notes LLMs tend to "skip phases" (e.g., jumping from phase 1 to 3) while humans violate "repeated crises" constraints. This suggests LLMs process text locally or fail to maintain the "no skipping" rule in working memory across the sequence.

## Foundational Learning

- **Concept: Fleiss' Kappa (Inter-annotator Agreement)**
  - Why needed: The paper relies entirely on Kappa scores to claim LLMs are "better" (more consistent) than humans
  - Quick check: If three humans annotate a text as "Crisis" and three LLMs annotate it as "Growth," which group has higher agreement, and does Kappa tell us who is "correct"?

- **Concept: Structural-Demographic Theory (SDT)**
  - Why needed: The experiment forces history into a rigid sequence (Growth -> Immiseration -> Elite Overproduction -> State Stress -> Crisis)
  - Quick check: According to the prompt constraints used in the paper, why is annotating a decade as "Growth" immediately after "Crisis" logically valid, but annotating "State Stress" followed by "Growth" invalid?

- **Concept: Prompt Engineering (Role & Constraints)**
  - Why needed: LLM performance is conditional on the "Act as an expert historian" persona and explicit constraints
  - Quick check: The paper added a "standard pattern" sequence to the prompt to improve human agreement. Why did this addition decrease LLM consensus (from 0.330 to 0.274 in Table V)?

## Architecture Onboarding

- **Component map:** Chronos Dataset -> Prompt Template -> LLM Triad (Claude 3.5, Gemini 1.5, GPT-4) -> Agreement Engine
- **Critical path:** The construction of Prompt 1 and Prompt 2 (Figures 3 & 4). These prompts serve as the "API" to the history interpretation logic.
- **Design tradeoffs:**
  - Standard Sequence Schema: Using a default sequence (1,1,2,2...) significantly boosts human agreement (+0.25 Kappa) but slightly reduces LLM agreement
  - Label Granularity: The 3-label "Big Cycle" task had comparable human/LLM agreement, while the 5-label "SDT" task showed LLMs dominating
- **Failure signatures:**
  - Model Bias (Gemini-type): labeling "violence" as "crisis" regardless of context
  - Human Bias (Student-type): labeling based on political view, violating "repeated crises" constraints
  - Architecture Failure (LLM-type): "Skipping" phases (1 -> 3) or hallucinating non-existent events
- **First 3 experiments:**
  1. Run the "1960s China" example against 10 different LLMs to confirm if "Crisis" vs "Growth" correlates with geographic origin of training data
  2. Remove the "A cycle cannot turn back" constraint from the prompt and measure increase in "Backward phase" errors
  3. Increase character count of historical descriptions from 300 to 1000 to test if "skipping" is caused by attention/context window issues

## Open Questions the Paper Calls Out

- Would expert historians achieve higher consensus levels comparable to LLMs, or does the higher machine agreement persist against expert baselines? The study used students with "no expertise in history," limiting generalizability to professional historical analysis.

- Is the observed Eurocentric bias in specific LLMs (e.g., Gemini) consistent across a wide range of non-Western historical contexts? The assessment relied on a single "1960s China" test case, which is insufficient to determine if the bias is systemic.

- Can specific prompting strategies effectively mitigate the tendency of LLMs to "skip phases" in sequential historical modeling? The paper identifies this error type but does not test interventions to enforce temporal logic.

## Limitations
- Results depend heavily on specific prompt engineering that may not generalize to other historical interpretation tasks
- Human annotator expertise level was minimal (students), potentially inflating LLM agreement by lowering the human performance baseline
- Only three LLMs from major vendors were tested, limiting generalizability to the broader LLM ecosystem

## Confidence
- **High confidence:** LLMs show higher inter-annotator agreement than humans on the 5-label task under tested conditions
- **Medium confidence:** Cultural bias patterns reflect training data distributions rather than inherent model architecture
- **Low confidence:** "Statistical regularization" fully explains LLM agreement superiority, as this requires assumptions about model convergence behavior not directly measured

## Next Checks
1. Test whether removing the "Act as an expert historian" persona and few-shot examples changes agreement patterns to isolate the effect of instruction tuning
2. Repeat the experiment with trained historians as annotators to establish whether the LLM advantage persists against domain experts
3. Apply the same methodology to contemporary political events to test whether cultural bias patterns shift based on recency and geopolitical relevance in training data