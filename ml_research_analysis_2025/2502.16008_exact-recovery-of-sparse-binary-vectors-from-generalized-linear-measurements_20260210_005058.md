---
ver: rpa2
title: Exact Recovery of Sparse Binary Vectors from Generalized Linear Measurements
arxiv_id: '2502.16008'
source_url: https://arxiv.org/abs/2502.16008
tags:
- where
- algorithm
- bound
- sensing
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes exact recovery of a k-sparse binary vector\
  \ from generalized linear measurements, such as logistic regression and noisy one-bit\
  \ quantized linear measurements. The authors analyze the linear estimation algorithm\
  \ followed by a top-k selection step, showing it recovers the unknown signal with\
  \ O((k + \u03C3\xB2) log n) measurements, where \u03C3\xB2 is the noise variance."
---

# Exact Recovery of Sparse Binary Vectors from Generalized Linear Measurements

## Quick Facts
- **arXiv ID**: 2502.16008
- **Source URL**: https://arxiv.org/abs/2502.16008
- **Reference count**: 40
- **Primary result**: Achieves O((k + σ²) log n) sample complexity for exact recovery of k-sparse binary vectors using linear estimation plus top-k selection

## Executive Summary
This paper establishes fundamental limits on exact recovery of k-sparse binary vectors from generalized linear measurements, including logistic regression and noisy one-bit quantized measurements. The authors analyze a simple linear estimation algorithm followed by top-k selection, proving it achieves optimal sample complexity of O((k + σ²) log n) for recovery with Gaussian noise, and O((k + 1/β²) log n) for logistic regression where β controls noise. The theoretical guarantees match information-theoretic lower bounds up to constant factors, demonstrating that only sign information from measurements suffices for recovery when measurements exceed critical thresholds.

## Method Summary
The proposed method consists of two stages: first, linear estimation of the sparse binary vector from generalized linear measurements, followed by a top-k selection step that identifies the k largest magnitude entries. For noisy linear measurements, the algorithm exploits the fact that sign information alone is sufficient for exact recovery when the number of measurements exceeds (k + σ²) log n. The analysis employs maximum-likelihood estimation for upper bounds and conditional Fano's inequality for matching lower bounds. The approach handles various measurement models including logistic regression and one-bit quantization, with computational complexity O((k + σ²)n log n) due to the sorting required in the top-k selection.

## Key Results
- Achieves O((k + σ²) log n) sample complexity for exact recovery of k-sparse binary vectors with Gaussian noise
- For logistic regression, requires O((k + 1/β²) log n) samples where β controls noise level
- Proves optimality by matching information-theoretic lower bounds up to constant factors
- Shows sign information alone suffices for recovery in noisy linear measurements above critical thresholds
- Demonstrates computational efficiency with O((k + σ²)n log n) complexity compared to alternatives like Lasso

## Why This Works (Mechanism)
The mechanism relies on the linear estimation stage providing accurate enough estimates of the sparse vector components, followed by the top-k selection exploiting the signal's sparsity structure. The key insight is that when measurements exceed (k + σ²) log n, the accumulated information from sign patterns is sufficient to distinguish the k non-zero entries from noise. The optimality follows from information-theoretic limits showing that fewer measurements cannot provide enough information to guarantee exact recovery with high probability.

## Foundational Learning

**Generalized Linear Models**: Framework extending linear regression to non-Gaussian noise and link functions (why needed: measurement models in the paper; quick check: verify logistic regression and quantized measurements fit GLMs)

**Fano's Inequality**: Information-theoretic tool bounding probability of error based on mutual information between signal and measurements (why needed: proving lower bounds; quick check: confirm conditional Fano's application to sparse recovery)

**Top-k Selection**: Algorithm selecting k largest magnitude entries from a vector (why needed: final recovery step; quick check: verify stability under noise)

**Maximum Likelihood Estimation**: Statistical method finding parameters maximizing likelihood of observed data (why needed: establishing upper bounds; quick check: confirm MLE consistency for sparse vectors)

**Sparse Recovery**: Problem of identifying sparse signals from underdetermined systems (why needed: problem context; quick check: verify k-sparsity assumption)

## Architecture Onboarding

**Component Map**: Measurement Model -> Linear Estimator -> Top-k Selector -> Recovered Vector

**Critical Path**: The linear estimation stage followed immediately by top-k selection represents the critical path for recovery. The measurement model quality directly impacts estimator performance, which in turn determines the success probability of top-k selection.

**Design Tradeoffs**: Simplicity vs. robustness - the linear estimator is computationally efficient but may be suboptimal for heavy-tailed noise; top-k selection is optimal for exactly k-sparse vectors but may fail with approximate sparsity or model mismatch.

**Failure Signatures**: Recovery failure occurs when noise variance exceeds thresholds, measurements fall below (k + σ²) log n, or the linear estimator produces biased estimates that cause top-k to select wrong entries.

**First Experiments**: 1) Test recovery probability vs. measurement count for varying k and σ²; 2) Compare performance against Lasso and OMP under different noise levels; 3) Validate sign-information sufficiency by reconstructing from quantized measurements only.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results assume noiseless or Gaussian noise settings, potentially limiting real-world applicability
- Binary entries assumption (0 or 1) is restrictive compared to continuous or other discrete values in practice
- Computational complexity claims based on theoretical analysis rather than empirical runtime measurements

## Confidence
- **Sample complexity bounds**: High - matched upper and lower bounds with rigorous proofs
- **Computational complexity**: Medium - based on theoretical assumptions about implementation efficiency
- **Sign information sufficiency**: Medium - depends on specific threshold conditions that may not generalize

## Next Checks
1. Empirical validation of the linear estimation + top-k algorithm on synthetic data with varying noise levels and sparsity patterns
2. Extension of the analysis to non-Gaussian noise distributions and measurement models beyond logistic regression
3. Numerical comparison of recovery performance against other sparse recovery algorithms like Lasso, OMP, and AMP under realistic conditions