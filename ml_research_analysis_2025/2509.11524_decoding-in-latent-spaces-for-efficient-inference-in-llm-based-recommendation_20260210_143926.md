---
ver: rpa2
title: Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation
arxiv_id: '2509.11524'
source_url: https://arxiv.org/abs/2509.11524
tags:
- item
- decoding
- hidden
- items
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high inference overhead of autoregressive
  decoding in LLM-based recommendation systems by proposing a novel latent-space decoding
  method called Light Latent-space Decoding (L2D). The core idea is to bypass language-space
  decoding by directly matching candidate items with the LLM's internal thought representations
  in the latent space, eliminating the time-consuming autoregressive process.
---

# Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation

## Quick Facts
- arXiv ID: 2509.11524
- Source URL: https://arxiv.org/abs/2509.11524
- Authors: Chengbing Wang; Yang Zhang; Zhicheng Wang; Tianhao Shi; Keqin Bao; Fuli Feng; Tat-Seng Chua
- Reference count: 23
- One-line primary result: >10x faster inference than autoregressive decoding while maintaining or enhancing recommendation performance

## Executive Summary
This paper addresses the high inference overhead of autoregressive decoding in LLM-based recommendation systems by proposing Light Latent-space Decoding (L2D), a method that bypasses language-space decoding by directly matching candidate items with the LLM's internal thought representations in the latent space. L2D eliminates the time-consuming autoregressive process by using hidden states from test sequences to represent user-preferred items and aggregating hidden states from training sequences to form candidate item representations. The method offers two aggregation strategies: global aggregation (averaging all associated hidden states) and local aggregation (using top-M most relevant samples based on test sample similarity).

## Method Summary
L2D fine-tunes a base LLM (Llama3.2-1B) on instruction data with prompts like "A user has interacted with the following items: <history>; which item would the user like next?" After fine-tuning, it extracts final-layer hidden states from all training samples and stores them in memory as (hidden_state, ground_truth_item) pairs. At inference, L2D computes the test sample's hidden state and retrieves top-K candidate items by L2 similarity to either globally-aggregated (average of all item-associated hidden states) or locally-aggregated (average of top-M most similar states) item representations. The method maintains the generative training objective while eliminating sequential token-by-token generation.

## Key Results
- L2D achieves >10x faster inference than autoregressive decoding (beam sizes 1, 5, 10) on Amazon CDs and Games datasets
- L2D-G (global aggregation) and L2D-L (local aggregation) outperform baseline methods on both datasets, with L2D-G showing more consistent performance
- Local aggregation with M=100 achieves the best results on sparse items (e.g., CDs dataset), while global aggregation performs better on dense datasets (e.g., Games dataset)
- Memory subsampling experiments show that storing only 30% of training hidden states retains most performance while significantly reducing storage requirements

## Why This Works (Mechanism)

### Mechanism 1
Bypassing autoregressive language-space decoding with latent-space matching reduces inference latency by >10x while preserving recommendation quality. During standard autoregressive decoding, each token generation waits for all preceding tokens, creating sequential dependency. L2D extracts the final-layer hidden state from the prompt encoding (a single forward pass) and matches it against pre-computed item representations, eliminating the token-by-token generation loop entirely. The hidden state at the final layer encodes sufficient information about the model's "intent" or preferred item, such that matching in latent space approximates what autoregressive decoding would produce.

### Mechanism 2
Aggregating hidden states from training samples labeled with each candidate item produces effective item representations without additional training. Each training sample's hidden state captures one contextualized view of the ground-truth item. By grouping and averaging these states (global aggregation), the method constructs a comprehensive item representation. Local aggregation further refines this by selecting only top-M most similar states to the test sample. Hidden states from different contexts but same target item share a common "item subspace" that averaging or selective aggregation can isolate.

### Mechanism 3
L2 distance-based similarity matching in the shared latent space preserves the generative training benefits while enabling efficient decoding. Both test hidden states and candidate item representations exist in the same LLM latent space (from the same model layer). L2 distance measures proximity, and top-K items are retrieved—no projection or alignment training is required. The latent space geometry learned during generative fine-tuning already positions similar items closer together, such that simple distance metrics suffice.

## Foundational Learning

- **Autoregressive decoding in transformers**: Why needed - L2D's value proposition hinges on understanding why token-by-token generation is slow (sequential dependency, KV-cache growth) and what's being bypassed. Quick check - Explain why generating a 10-token item title requires 10 sequential forward passes in standard autoregressive decoding.

- **Hidden states as semantic representations**: Why needed - The method treats the final-layer hidden state as a "thought vector" encoding user preferences and candidate item identity. Quick check - What does the hidden state at position i in a transformer represent, and why might the last position be most informative for next-item prediction?

- **Embedding space similarity and nearest-neighbor retrieval**: Why needed - The decoding step is essentially k-NN search in latent space using L2 distance. Quick check - Given query vector q and item vectors {v₁, v₂, ..., vₙ}, write the condition for vᵢ being the top-1 retrieval under L2 distance.

## Architecture Onboarding

- **Component map**: Base LLM -> Memory Module -> Aggregation Engine -> Retrieval Layer
- **Critical path**: 1) Fine-tune base LLM on (history, target_item) instruction pairs 2) Pre-compute: Run forward pass per training sample, store (hⱼ, vⱼ) pairs in memory 3) At inference: Encode test prompt → extract h_test → compute L2 distances to candidate representations → return top-K
- **Design tradeoffs**: Global vs. Local Aggregation (global robust for sparse items; local more personalized but degrades when few associations), Memory Size (full training set storage vs. subsampling), Layer Selection (only final-layer hidden states used)
- **Failure signatures**: Cold-start items with zero training associations cannot form representations; dense scenarios with local aggregation and small M may exclude relevant hidden states; corrupted or misaligned fine-tuning yields random results
- **First 3 experiments**: 1) Latency benchmark: Compare inference time for L2D vs. autoregressive decoding on CDs and Games datasets 2) Aggregation ablation: Compare L2D-G vs. L2D-L across varying M 3) Sparse-item analysis: Stratify test set by item frequency; measure Recall@50 for both aggregation strategies

## Open Questions the Paper Calls Out
- How can the L2D memory module be efficiently updated in real-time as user interaction data accumulates?
- Can interpolation techniques or auxiliary models successfully extend L2D to handle fully cold-start items with zero interaction history?
- What optimizations can reduce the computational overhead of the memory pre-construction phase?

## Limitations
- Cold-start items with zero training associations cannot be handled by the method
- Memory storage requirement for large-scale datasets (up to 2TB for 10⁹ samples) may be prohibitive
- The optimal hyperparameter M for local aggregation varies by dataset without clear selection criteria

## Confidence
- High confidence in the core mechanism (>10x speedup via latent-space matching)
- Medium confidence in the aggregation strategies' effectiveness (M varies significantly across datasets)
- Medium confidence in generalization across different datasets (consistent improvements on both CDs and Games)

## Next Checks
1. **Layer Sensitivity Analysis**: Test whether intermediate transformer layers can serve as effective latent representations for L2D
2. **Dynamic Aggregation Strategy**: Implement an adaptive method that automatically selects between global and local aggregation based on item sparsity
3. **Memory Efficiency Benchmarking**: Conduct systematic experiments on memory-subsampling strategies to identify optimal storage-retention tradeoffs