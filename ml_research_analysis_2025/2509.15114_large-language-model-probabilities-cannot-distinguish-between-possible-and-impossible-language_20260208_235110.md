---
ver: rpa2
title: Large Language Model probabilities cannot distinguish between possible and
  impossible language
arxiv_id: '2509.15114'
source_url: https://arxiv.org/abs/2509.15114
tags:
- language
- llms
- impossible
- surprisal
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the claim that Large Language Models (LLMs)
  can distinguish between grammatically possible and impossible language by using
  model-internal probability estimates. The authors test four LLMs (Gemma-2b, Gemma-7b,
  Pythia-1b, and Mistral-7b) using a novel minimal-pair benchmark that compares grammatical
  sentences to lower-frequency grammatical, ungrammatical, pragmatically odd, and
  semantically odd sentences.
---

# Large Language Model probabilities cannot distinguish between possible and impossible language

## Quick Facts
- arXiv ID: 2509.15114
- Source URL: https://arxiv.org/abs/2509.15114
- Reference count: 40
- Primary result: LLM surprisal values cannot reliably distinguish syntactically impossible from possible language

## Executive Summary
This paper challenges the widespread assumption that LLM probability estimates can distinguish grammatically possible from impossible language. Using a novel minimal-pair benchmark with four conditions (grammatical vs frequency-matched, grammatical vs ungrammatical, grammatical vs pragmatically odd, and grammatical vs semantically odd), the authors test four LLMs (Gemma-2b, Gemma-7b, Pythia-1b, and Mistral-7b). Contrary to expectations, ungrammatical prompts do not consistently show highest surprisal compared to other violation types. Instead, semantically odd prompts consistently produce highest surprisal values across all models, suggesting probabilities are unreliable proxies for grammaticality.

## Method Summary
The study uses minimal-pair sentences from four conditions, comparing each violation type against grammatical baselines. For each model, token-level probabilities are extracted and converted to surprisal values (negative log-probabilities), which are then aggregated to sentence-level surprisal. Minimal-pair differences are computed (violation sentence minus grammatical baseline), and statistical significance is assessed using linear mixed-effects models with random intercepts for test items. The key metric is whether ungrammatical conditions show significantly higher surprisal than other violation types.

## Key Results
- Ungrammatical prompts do not consistently show highest surprisal across models
- Semantically odd prompts consistently produce highest surprisal values
- No clear cut-off point exists for distinguishing ungrammaticality based on surprisal alone
- All tested models show greater surprisal for semantic violations than syntactic violations (all bs > 3.7, all ps < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surprisal is a multi-source signal that cannot uniquely identify grammaticality violations.
- Mechanism: Surprisal reflects composite signals including word frequency, ambiguity, idiomaticity, collocability, and semantic-pragmatic soundnessâ€”not just syntactic well-formedness. Binary designs mask this confound; multi-condition designs reveal overlap.
- Core assumption: If surprisal were unique to ungrammaticality, ungrammatical sentences would show distinct, non-overlapping surprisal signatures.
- Evidence: Semantically odd prompts consistently show highest surprisal; C1 frequency-matched controls show no significant differences (all ps > 0.4).

### Mechanism 2
- Claim: High surprisal on impossible patterns reflects weak training-data match, not internalized grammatical rules.
- Mechanism: LLMs detect low-probability forms via statistical pattern matching without understanding violation nature (Searle's Chinese Room analogy).
- Core assumption: Surprisal indicates deviation from training distribution, not abstract rule competence.
- Evidence: Leet decoding experiment produced nonsensical outputs (e.g., "Eight eighteen resigned seventeen...") absent from human responses, showing pattern-match failure without generalization.

### Mechanism 3
- Claim: LLMs show greater surprisal for semantic violations than syntactic violations.
- Mechanism: "Impossible concepts" violating semantic selectional requirements produce consistently higher surprisal than syntactic word-order reversals with intact semantics.
- Core assumption: If LLMs had autonomous syntactic competence, syntactic violations should yield highest surprisal.
- Evidence: All models show significantly higher surprisal for semantic violations (C4) than syntactic violations (C2), with all bs > 3.7 and all ps < 0.001.

## Foundational Learning

- Concept: **Surprisal (negative log-probability)**
  - Why needed here: The paper's central metric measuring processing difficulty proportional to unpredictability in context.
  - Quick check question: Given a token with probability 0.125 in context, what is its surprisal in bits? (Answer: 3 bits)

- Concept: **Minimal-pair experimental design**
  - Why needed here: Methodology comparing sentences differing in one dimension to isolate specific linguistic factors; the paper critiques designs that restrict pairs to grammatical/ungrammatical only.
  - Quick check question: Why might a minimal-pair design comparing only grammatical vs. ungrammatical sentences yield misleading conclusions about grammaticality detection?

- Concept: **Token probability elicitation from LLMs**
  - Why needed here: Extracting model-internal probability estimates to probe "competence" directly rather than relying on verbal judgments.
  - Quick check question: How does token-level probability aggregation yield sentence-level surprisal?

## Architecture Onboarding

- Component map: Probability extraction layer -> Surprisal computation -> Minimal-pair differencing -> Statistical analysis layer
- Critical path: 1) Define minimal-pair conditions (C1-C4) with controlled frequency baselines. 2) Extract token probabilities for all sentences. 3) Compute surprisal differences relative to shared grammatical baseline. 4) Fit mixed-effects model: `diff_surprisal ~ condition + (1 | test_item)`.
- Design tradeoffs: Binary vs. multi-condition designs (binary masks confounds; multi-condition reveals overlap); frequency-matched controls essential to isolate violation effects; testing multiple model sizes/families checks robustness.
- Failure signatures: Overlapping surprisal distributions (ungrammatical condition not significantly higher than pragmatic oddness); inverted patterns (semantic violations show higher surprisal than syntactic violations); no significant intercept (frequency confounds not controlled).
- First 3 experiments: 1) Replicate design on different model family (e.g., Llama-3) to verify cross-architecture generalization. 2) Add fifth condition combining semantic + syntactic violations to test additive vs. interactive effects. 3) Run human acceptability judgments on same stimuli to compare with model patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative methodology can reliably verify if LLMs distinguish possible from impossible language, given that probability measurements are unreliable proxies?
- Basis: The authors conclude that "claims about models being able to distinguish possible from impossible language need verification through a different methodology."
- Why unresolved: Surprisal values conflate syntactic violations with semantic/pragmatic oddness and frequency effects, making them "too blunt a measure" to isolate grammaticality.
- What evidence would resolve it: A novel testing framework that successfully decouples grammatical competence from statistical probability and semantic plausibility.

### Open Question 2
- Question: Are LLMs capable of drawing finer distinctions between violations of syntax versus semantics versus pragmatics?
- Basis: The paper states, "It remains to be demonstrated whether models are capable of drawing finer distinctions, within the realm of impossible, of what defies syntax vs. semantics vs. pragmatics."
- Why unresolved: The study found semantic oddness yielded higher surprisal than ungrammaticality, suggesting models do not process these linguistic levels independently.
- What evidence would resolve it: Experiments dissociating these factors using internal representational analysis to see if distinct neural patterns emerge.

### Open Question 3
- Question: Can a theoretically grounded hierarchy of grammatical impossibility be constructed that is valid for both humans and LLMs?
- Basis: The authors argue that "it remains to be established whether a hierarchy of grammatical impossibility can be construed at all," noting current hierarchies are arbitrary.
- Why unresolved: No solid theory defines what makes a language "impossible" (vs. just difficult to process), and current hierarchies lack empirical human baselines.
- What evidence would resolve it: A formal metric for "impossibility" based on linguistic principles rather than word-salad complexity, validated against human processing data.

## Limitations

- The study establishes negative results (probisal cannot uniquely identify ungrammaticality) but cannot definitively prove LLMs lack syntactic competence entirely.
- The semantic dominance effect could potentially be architecture-specific to decoder-only models of this scale.
- The findings, while robust across four models, require direct replication across different transformer variants (encoder-decoder, different attention mechanisms).

## Confidence

**High Confidence**: The core empirical finding that ungrammatical sentences do not consistently show highest surprisal compared to semantically odd sentences. Results are statistically significant with substantial effect sizes across all tested models.

**Medium Confidence**: The mechanistic interpretation that surprisal reflects multi-source signals rather than grammaticality-specific information. Data supports this, but alternative explanations cannot be fully ruled out without additional experiments.

**Medium Confidence**: The broader theoretical claim that current probability-based testing methods may overestimate LLMs' syntactic competence. This follows logically from empirical results but requires further validation.

## Next Checks

1. **Cross-architecture replication**: Test the minimal-pair benchmark on encoder-decoder models (e.g., BERT variants) and larger models (Llama-3, GPT-4) to determine whether the semantic dominance pattern generalizes across transformer architectures and scales.

2. **Human correlation study**: Conduct parallel human acceptability judgments on the same minimal-pair stimuli to compare whether human grammaticality/oddness ratings correlate with model surprisal patterns or diverge.

3. **Violation interaction experiment**: Add a fifth condition combining syntactic and semantic violations to test whether surprisal effects are additive or show interaction patterns, revealing whether LLMs process these violations through shared or distinct mechanisms.