---
ver: rpa2
title: Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation
arxiv_id: '2512.17673'
source_url: https://arxiv.org/abs/2512.17673
tags:
- gaze
- performance
- error
- estimation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce ST-Gaze, a video-based gaze estimation model that
  addresses the challenge of feature representation in appearance-based methods. Our
  key innovation is a novel spatio-temporal recurrence that first models intra-frame
  spatial context using a Gated Recurrent Unit (GRU) before propagating the context
  through time to capture inter-frame dynamics.
---

# Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation

## Quick Facts
- arXiv ID: 2512.17673
- Source URL: https://arxiv.org/abs/2512.17673
- Authors: Alexandre Personnic; Mihai Bâce
- Reference count: 40
- Primary result: ST-Gaze achieves 2.58° angular error on EVE benchmark, reducing error by 0.61° compared to prior state-of-the-art

## Executive Summary
This paper introduces ST-Gaze, a video-based gaze estimation model that addresses the challenge of feature representation in appearance-based methods. The key innovation is a novel spatio-temporal recurrence that first models intra-frame spatial context using a Gated Recurrent Unit (GRU) before propagating the context through time to capture inter-frame dynamics. This design overcomes the limitations of premature spatial pooling found in prior methods. By combining an EfficientNet-B3 backbone with attention-based feature fusion, ST-Gaze achieves state-of-the-art performance on the EVE benchmark, reducing the angular error to 2.58° compared to previous best of 3.19°. Furthermore, when used as a backbone for person-specific adaptation, ST-Gaze reduces the error to 1.87°. The ablation study confirms that the proposed recurrence structure and self-attention module are critical for achieving strong generalization performance.

## Method Summary
ST-Gaze is a dual-stream architecture that processes left eye, right eye (horizontally flipped), and face patches (128×128) through two EfficientNet-B3 backbones (ImageNet pretrained, modified to output 128×8×8 and 32×8×8 features). The features are concatenated (160×8×8), processed by an Efficient Channel Attention (ECA) module, then reshaped to a sequence of 64 spatial patches for processing by a Self-Attention Module (SAM) with 3 transformer blocks. A 2-layer GRU scans this sequence spatially, with its hidden state propagated across video frames. The final hidden state passes through average pooling and a 2-layer MLP with Tanh activation (scaled by π/2) to predict 3D gaze vectors. The model is trained end-to-end with a combined loss (L_total = λ_ang·L_ang + λ_cm·L_PoG,cm + λ_px·L_PoG,px) using Adam optimizer, cosine annealing, and 3° offset augmentation.

## Key Results
- Achieves 2.58° angular error on EVE benchmark, a 0.61° improvement over previous state-of-the-art (3.19°)
- When used as backbone for person-specific adaptation, reduces error to 1.87°
- Ablation study shows SAM module is critical for generalization (2.58° → 4.84° without SAM)
- ECA module improves downstream adaptation performance despite slight raw generalization cost

## Why This Works (Mechanism)

### Mechanism 1: Intra-Frame Spatial Context Preservation
Preserving the 2D spatial arrangement of features before temporal modeling improves gaze estimation accuracy compared to early vectorization. The model treats the 2D feature map as a sequence of 64 spatial patches. A Gated Recurrent Unit (GRU) scans this "intra-frame" sequence to build a context-aware hidden state, which is then propagated to the next time step ("inter-frame"). This prevents the loss of structural facial relationships that occurs when pooling features into a single vector immediately after convolution. The core assumption is that spatial relationships within a single frame (e.g., the relative position of the eye to the face) provide necessary context that purely temporal models or early-pooling architectures discard. Evidence includes the paper's claim that this is the key innovation and the ablation showing spatial recurrence improves performance.

### Mechanism 2: Global Spatial Dependencies via Self-Attention
Self-attention is the primary driver of generalization to unseen subjects (test sets). The Self-Attention Module (SAM) applies Transformer-style blocks to the feature sequence, allowing the model to weigh the importance of different spatial regions globally and learn long-range dependencies between eye and face features that local convolutions might miss. The core assumption is that global spatial context within the feature map is necessary to disambiguate gaze direction across diverse appearances and head poses. Evidence includes the ablation showing removing SAM causes performance to collapse from 2.58° to 4.84°, and the description of SAM as a method to "learn long-range spatial dependencies across the feature map."

### Mechanism 3: Dynamic Channel Reweighting for Adaptation
Channel-wise attention (ECA) acts as a regularizer that specifically benefits downstream person-specific adaptation, even if it doesn't always boost raw generalization. The Efficient Channel Attention (ECA) module re-weights the 160-channel fused feature map (Eye + Face) without dimensionality reduction, allowing the model to dynamically prioritize either eye details or face context. The core assumption is that the relative importance of eye vs. face features varies by input, and learning this weighting creates a more robust feature manifold for later fine-tuning. Evidence includes the ablation showing removing ECA improves raw test error slightly but degrades performance when used as a backbone for adaptation.

## Foundational Learning

- **Concept:** Recurrent Hidden State Propagation
  - Why needed here: The core of ST-Gaze relies on a GRU not just for time, but for space. You must understand how a hidden state $h_t$ carries information from step $t$ to $t+1$ to grasp how spatial context (64 steps) is compressed and then passed to the next frame.
  - Quick check question: If you process a 64-patch spatial sequence with a GRU, which output state is used to initialize the temporal recurrence for the next video frame?

- **Concept:** Self-Attention vs. Convolution Inductive Bias
  - Why needed here: The paper argues that the SAM (Transformer) is critical for generalization. Understanding that convolutions focus on local features while attention focuses on global relationships explains why the SAM ablation caused such a catastrophic failure on the test set.
  - Quick check question: Why would a local convolution fail to capture the relationship between a specific eye region and a distant face contour in a way that self-attention would not?

- **Concept:** Feature Map Flattening vs. Preservation
  - Why needed here: The central critique of prior work is "premature spatial pooling." You need to distinguish between preserving a tensor shape $(C, H, W)$ versus flattening it to a vector $(C)$ to understand the architectural flow.
  - Quick check question: Does the ST-Gaze architecture flatten the feature map before or after the Spatio-Temporal Recurrence module?

## Architecture Onboarding

- **Component map:** Inputs (Eye 128x128, Face 128x128) → Dual EfficientNet-B3 → ECA → Reshape to Sequence (64x160) → SAM (3 Transformer blocks) → GRU (2 layers, processes 64 patches) → Average Pool → MLP(Tanh) → 3D Gaze Vector

- **Critical path:** The flow from the SAM output (Sequence) to the GRU spatial scan. If the reshaping or positional encoding is misaligned, the spatial recurrence will receive garbage data, and the temporal propagation will fail.

- **Design tradeoffs:** EfficientNet-B3 vs. ResNet: The paper explicitly notes ResNet-18 caused a collapse in test performance (4.22°). Stick to EfficientNet-B3 for the backbone. Accuracy vs. Speed: The dual-stream setup + SAM achieves high accuracy but requires 6.39 GFLOPs. Early fusion (combining inputs before backbone) is faster but failed the ablation (3.26° error).

- **Failure signatures:** Validation vs. Test Gap: If you see good validation scores but >4.0° test scores, check the SAM module. The paper notes validation failed to predict the critical importance of SAM for "in-the-wild" generalization. Person-Specific Drift: If the model fails on specific users (e.g., those with glasses causing reflections), this is a known limitation. Temporal Instability: If gaze flickers, ensure the Inter-frame propagation ($h_{t-1} \to h_t$) is correctly implemented.

- **First 3 experiments:**
  1. Sanity Check (Ablation): Remove the SAM module and verify the angular error increases significantly (>4.0°) on the test set to confirm the attention implementation is working.
  2. Recurrence Validation: Compare "Spatial Pooling pre-GRU" (baseline) vs. "ST-Gaze Recurrence" (proposed). You should see a consistent delta of ~0.2° improvement favoring the proposed method.
  3. Backbone Stress Test: Train a version with ResNet-18. Observe the generalization gap to confirm the paper's finding that EfficientNet features are superior for this specific task.

## Open Questions the Paper Calls Out

### Open Question 1
Can noise-aware training or generative image reconstruction effectively mitigate the high gaze estimation error caused by visual artifacts such as eyeglass reflections? The conclusion explicitly identifies robustness to reflections as a "persistent challenge" and suggests developing noise-aware strategies or generative models as future work. This remains unresolved because the qualitative analysis shows that for outlier participants (e.g., test03), neither the base model nor the person-specific adaptation (SCPT) could correct errors caused by severe occlusions. Demonstrated reduction in angular error on the EVE test set when applying augmentation or in-painting modules specifically to frames containing strong corneal reflections would resolve this question.

### Open Question 2
Do the learned spatio-temporal feature representations transfer effectively to predicting higher-level cognitive states, such as cognitive load or confusion? The authors state that moving beyond gaze direction to predict cognitive states using the model's subtle temporal dynamics is a "compelling avenue for future research." This remains unresolved because the current work focuses exclusively on regressing 3D gaze vectors rather than classifying internal user states. Transfer learning experiments where ST-Gaze features are fine-tuned on datasets with cognitive state labels, achieving competitive classification performance compared to specialized models, would resolve this question.

### Open Question 3
Does the spatio-temporal recurrence mechanism maintain its superiority over premature spatial pooling in "in-the-wild" environments with extreme head poses or unconstrained movement? The authors limit evaluation to the EVE dataset, explicitly noting they excluded "in-the-wild" datasets like GAZE360 or ETH-XGaze to focus on desktop scenarios. This remains unresolved because the paper validates the architecture on EVE (constrained head movement), but it remains untested whether the spatial recurrence is robust to the extreme pose variations found in excluded benchmarks. A comparison of ST-Gaze against baseline methods on the GAZE360 or ETH-XGaze test sets, showing that the recurrence gap persists under extreme head pose variations, would resolve this question.

## Limitations
- Vulnerability to eyeglasses reflections and occlusions, causing large errors for specific participants
- Dependency on large-scale training data and potential struggle with extreme head poses or lighting conditions not well-represented in EVE
- Computational overhead from SAM module (~3.8% parameter increase) may limit real-time applications

## Confidence
- **High Confidence:** The core architectural claim that spatio-temporal recurrence outperforms premature spatial pooling is well-supported by ablation study and error reduction (2.58° vs. 3.19° baseline)
- **Medium Confidence:** The claim that self-attention is the primary driver of generalization to unseen subjects is supported by dramatic ablation results (2.58° to 4.84° without SAM), but paper doesn't explore alternative attention mechanisms
- **Low Confidence:** The assertion that ECA specifically benefits downstream person-specific adaptation is based on limited ablation data and lacks direct evidence in corpus

## Next Checks
1. Component Isolation Test: Systematically ablate the SAM module on the validation set first to confirm the dramatic generalization gap (2.02° vs. 4.84°) before running the more expensive test set evaluation.
2. Adaptation Benchmark: Implement the person-specific adaptation protocol described in the paper and measure the angular error drop (1.87°) to validate the ECA module's claimed benefit for downstream personalization.
3. Failure Mode Analysis: Identify and analyze the specific participants (e.g., test03 with glasses) where ST-Gaze performance degrades significantly to quantify the model's robustness limits and inform future improvements.