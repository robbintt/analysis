---
ver: rpa2
title: Pulling Back the Curtain on Deep Networks
arxiv_id: '2507.22832'
source_url: https://arxiv.org/abs/2507.22832
tags:
- pullback
- networks
- pullbacks
- deep
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semantic Pullbacks address the instability and lack of perceptual
  alignment in gradient-based explanations by approximating a locally expected pullback
  of a neuron's action, using softened backward transport through hard-gated layers
  and a double pullback formulation for self-attention. This produces class-conditional,
  perceptually coherent explanations that are more faithful to the network's internal
  computation.
---

# Pulling Back the Curtain on Deep Networks

## Quick Facts
- **arXiv ID**: 2507.22832
- **Source URL**: https://arxiv.org/abs/2507.22832
- **Reference count**: 40
- **One-line result**: Semantic Pullbacks produce perceptually coherent explanations with orders-of-magnitude lower infidelity than established baselines.

## Executive Summary
Semantic Pullbacks address the instability and lack of perceptual alignment in gradient-based explanations by approximating a locally expected pullback of a neuron's action. The method uses softened backward transport through hard-gated layers and a double pullback formulation for self-attention, producing class-conditional, perceptually coherent explanations that are more faithful to the network's internal computation. Across ResNet50, VGG11, and PVT models, Semantic Pullbacks significantly outperform established baselines in faithfulness and robustness metrics while enabling compelling counterfactual perturbations without heavy regularization.

## Method Summary
Semantic Pullbacks compute locally expected pullbacks via backward-only soft adjoints, replacing hard gates with temperature-controlled soft versions (ReLU → Φ(z/τ), SiLU → σ(z/τ), GELU → Φ(z/τ), MaxPool → softmax routing). For attention models, a double pullback formulation improves alignment by taking a small step toward the first pullback direction before the second backward pass. Pullback ascent iteratively applies the pullback vector field to generate counterfactuals, using K small steps with projection to valid pixel bounds. The method is implemented via Forward Gradient Injection and detach-based blocking, preserving the original forward pass while modifying backward rules.

## Key Results
- Orders-of-magnitude lower infidelity than Integrated Gradients, Gradient SHAP, DeepLift, and Guided GradCam across all tested architectures
- Improved faithfulness correlations (0.91 vs 0.80 for Gradient) and robustness (Max Sensitivity 0.15 vs 0.25-0.30 for baselines)
- Double Pullback specifically addresses attention model limitations, reducing background attention in explanations
- Counterfactual generation without heavy regularization produces semantically coherent images in 3-10 steps

## Why This Works (Mechanism)

### Mechanism 1: Softened Backward Transport Through Hard-Gated Layers
Replaces sharp backward gating with temperature-controlled soft gates (Φ(z/τ) for ReLU, σ(z/τ) for SiLU/GELU) to recover weakly active but semantically coherent feature components that standard gradients suppress. Assumes features are partially expressed on most inputs, with a single gradient sampling a fragmented realization of a feature that exists "in expectation" over the data distribution.

### Mechanism 2: Double Pullback for Self-Attention Routing
A second pullback after a small step toward the first pullback direction yields more coherent explanations for attention-based architectures. The first pullback shifts attention patterns toward the target neuron's locally preferred direction, while the second pullback flows through improved attention alignment, capturing value transport and attention-based routing jointly.

### Mechanism 3: Pullback Ascent for Counterfactual Perturbations
Iterative constrained ascent along the soft pullback vector field produces semantically coherent counterfactuals without heavy regularization. Small steps (K ∈ {3,5,10}) along the normalized pullback direction amplify target-aligned structures while suppressing negatively-weighted features, remaining near the original input.

## Foundational Learning

- **Concept: Pullback (Adjoint Action) vs. Gradient**
  - Why needed: Pullbacks—adjoints of dynamic linear operators—differ from gradients in layers where differentiating through gates/statistics introduces extra terms
  - Quick check: For a ReLU layer, does the pullback equal the gradient? For SiLU, why do they differ?

- **Concept: Dynamic Linear Operators**
  - Why needed: Layers like ReLU, MaxPool, and Attention can be expressed as x_ℓ = W_ℓ(x_ℓ₋₁)·x_ℓ₋₁, where W depends on input but acts linearly once fixed
  - Quick check: Write the pullback for a network f(x) = W_2(ReLU(W_1 x)). How does it differ from ∇f?

- **Concept: Vector-Jacobian Products (VJP) and Autograd**
  - Why needed: Implementation modifies VJPs (backward rules) without changing forward computation using Forward Gradient Injection and stop-gradient
  - Quick check: In PyTorch, how would you modify the backward pass of nn.ReLU without changing its forward output?

## Architecture Onboarding

- **Component map:** Pretrained model → soft adjoint wrappers → forward pass (unchanged) → backprop with modified VJPs → soft pullback → (optional) double pullback or pullback ascent
- **Critical path:** Traverse model tree and replace layers with soft-adjoint wrappers → run standard forward pass, cache activations → set target score s = logits[c], call torch.autograd.grad(s, x) → (for attention) compute second pullback at x + α·ñᵤ(x) → (for counterfactuals) iterate K steps with projection
- **Design tradeoffs:** Temperature τ: Higher τ = softer gates = more coherent but less selective explanations. Double Pullback vs. Soft Pullback: Double Pullback superior for attention architectures. K for Pullback Ascent: K=5-10 for counterfactuals; K=3 for subtle feature accentuation.
- **Failure signatures:** Explanations attending to background → try Double Pullback or increase α. Explanations too smooth/diffuse → decrease temperature τ. Counterfactuals producing noise → reduce K or α, check constraint projection. Random Logit score high → verify target-specific selector u = e_c is used correctly.
- **First 3 experiments:** 1) Compare standard gradient vs. soft pullback for ReLU-only MLP on synthetic data. 2) Replicate Infidelity and Max Sensitivity metrics on ResNet50/Imagenette for Soft Pullback vs. baselines. 3) Run Pullback Ascent (α=20, K=5) on 10 diverse images targeting 3 classes each; assess class-conditional coherence.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Semantic Pullback framework be effectively extended to text and multimodal architectures while maintaining high faithfulness? The formulation is domain-agnostic but empirical validation is restricted to vision models.
- **Open Question 2:** Does explicit incorporation of bias terms into pullback formulation significantly improve explanation alignment or faithfulness? Current method omits explicit treatment of bias terms, treating them as virtual gating shifts.
- **Open Question 3:** Can models be trained using Semantic Pullbacks or softened backward rules to simultaneously improve performance and intrinsic interpretability? Method is currently evaluated only as post-hoc explanation technique.

## Limitations

- **Architecture dependence:** Method claims to be architecture-agnostic but double pullback specifically targets attention models, with performance gains most pronounced in transformer-based architectures.
- **Counterfactual generation:** Pullback ascent method for generating counterfactuals lacks rigorous quantitative validation and robustness analysis against adversarial inputs.
- **Parameter sensitivity:** Temperature parameter τ is chosen heuristically without systematic exploration of its impact across different architectures and input types.

## Confidence

- **High:** Overall improvement in faithfulness metrics (Infidelity, Faithfulness Correlation) and robustness (Max Sensitivity) compared to established baselines is well-supported by presented results.
- **Medium:** Softened backward transport mechanism is theoretically grounded and supported by performance gains, but temperature parameter choice requires further investigation.
- **Medium:** Double pullback formulation for attention models is empirically validated but theoretical justification for why second pullback improves alignment is not fully elaborated.
- **Low:** Effectiveness of pullback ascent for generating perceptually coherent counterfactuals is demonstrated visually but lacks rigorous quantitative evaluation.

## Next Checks

1. **Temperature sensitivity analysis:** Systematically vary temperature τ across range (e.g., τ ∈ [0.1, 0.3, 0.5, 0.7, 1.0, 2.0]) for ReLU and attention models, evaluate impact on faithfulness metrics and visual quality, identify optimal τ per architecture.

2. **Robustness to adversarial inputs:** Evaluate Semantic Pullbacks on adversarially perturbed images (PGD/FGSM attacks) and compare to baselines, assess whether soft pullback provides inherent robustness and maintains perceptual alignment under attack.

3. **Quantitative counterfactual evaluation:** Develop metrics to assess counterfactual quality including class-conditional changes (classifier probability increase), perceptual similarity (LPIPS/FID), and semantic coherence (human evaluation/CLIP similarity), compare across different K and α values.