---
ver: rpa2
title: Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency
  for Language Models
arxiv_id: '2602.01428'
source_url: https://arxiv.org/abs/2602.01428
tags:
- watermark
- sampling
- strength
- token
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the trade-off between watermark strength and
  sampling efficiency in speculative decoding for large language models (LLMs). The
  key insight is that maximal watermark strength is achieved when token generation
  is a deterministic function of pseudorandom numbers, but prior approaches weakened
  this by introducing randomness in token acceptance.
---

# Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models

## Quick Facts
- arXiv ID: 2602.01428
- Source URL: https://arxiv.org/abs/2602.01428
- Authors: Weiqing He; Xiang Li; Li Shen; Weijie Su; Qi Long
- Reference count: 40
- Primary result: Proposed method breaks trade-off between watermark strength and sampling efficiency by injecting pseudorandomness into draft-token acceptance, achieving maximal watermark strength (entropy of target distribution) while preserving unbiasedness and maximal efficiency (1−TV(Q,P)).

## Executive Summary
This work addresses the fundamental trade-off between watermark strength and speculative sampling efficiency in large language models (LLMs). Prior approaches weakened watermark strength by introducing randomness in token acceptance during speculative decoding. The authors propose a principled method that replaces truly random acceptance decisions with pseudorandom ones, making the entire token generation process a deterministic function of pseudorandom variables. This approach preserves unbiasedness, attains maximal speculative sampling efficiency (1−TV(Q,P)), and achieves maximum watermark strength (entropy of the target distribution). Experiments on ELI5 and C4 datasets with Llama and Gemma model pairs demonstrate improved watermark detectability (TPR@FPR=1% up to 0.8 vs 0.6 for baselines) while maintaining sampling efficiency (AATPS ≈ 1.8-3.3 across settings).

## Method Summary
The method injects pseudorandomness into the draft-token acceptance decision of speculative sampling. In standard speculative sampling, the decision to accept a draft token is made by sampling a uniform random variable. This work replaces this step with a deterministic mapping from a pseudorandom variable (ζR) to an acceptance value, making the final token completely determined by (ζD, ζT, ζR). This preserves the correct acceptance probability while making the generation process a deterministic function of pseudorandom variables, thereby maximizing watermark strength. The detection process is modified to leverage the acceptance variable u, using either threshold-based selection (Ars-τ) or an MLP to estimate the token's source (Bayes-MLP).

## Key Results
- Improved watermark detectability: TPR@FPR=1% up to 0.8 vs 0.6 for baseline methods
- Maintained sampling efficiency: AATPS ≈ 1.8-3.3 across settings, matching standard speculative sampling
- Achieved maximum watermark strength: WS(P'ζ) = Ent(P) (entropy of target distribution)
- Verified unbiasedness: No significant degradation in output distribution quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing truly random acceptance decisions with pseudorandom ones makes the entire token generation process a deterministic function of the pseudorandom seed, thereby achieving maximal watermark strength.
- Mechanism: In standard speculative sampling, the decision to accept a draft token is made by sampling a uniform random variable. This introduces external randomness not tied to the watermark seed. By replacing this step with a deterministic mapping from a pseudorandom variable (ζR) to an acceptance value, the final token becomes completely determined by (ζD, ζT, ζR). Since maximal watermark strength (KL divergence between watermarked and original distributions) is achieved when the watermarked distribution is degenerate (i.e., a point mass), this determinism ensures each pseudorandom seed maps to exactly one token, maximizing strength.
- Core assumption: A watermarking scheme attains maximal strength if and only if its output token is a deterministic function of the pseudorandom variable (Theorem 3.2). The pseudorandom generator G produces outputs indistinguishable from uniform random variables for the purposes of sampling correctness.
- Evidence anchors:
  - [abstract] "...injects pseudorandomness into the draft-token acceptance decision itself, making the entire generation process a deterministic function of pseudorandom variables."
  - [Section 4.1] "The key difference from (Dathathri et al., 2024) is that the acceptance variable u is now pseudorandom rather than truly random... As a result, Alg. 1 becomes a fully deterministic function of pseudorandom variables."
  - [Theorem 4.1(c)] "Maximum watermark strength: WS(P'ζ) = Ent(P)."
  - [corpus] Related work in corpus focuses on watermarking robustness and alignment but does not specifically analyze pseudorandom acceptance; no direct corpus evidence on this mechanism.
- Break condition: The mechanism fails if the acceptance variable ζR does not produce outputs that preserve the correct acceptance probability distribution, or if the combined process (ζD, ζT, ζR) → token does not yield a degenerate distribution for every seed.

### Mechanism 2
- Claim: Using pseudorandom acceptance preserves the theoretical maximum speculative sampling efficiency (acceptance rate), maintaining the same speedup as standard speculative sampling.
- Mechanism: The acceptance decision uses a pseudorandom variable u that is uniform in [0,1]. The probability that u < min{1, P(w)/Q(w)} is exactly min{1, P(w)/Q(w)}, which matches the acceptance probability in standard speculative sampling. Since the acceptance rate depends only on the distribution of u (not on whether it is truly random or pseudorandom), the efficiency is preserved.
- Core assumption: The pseudorandom generator produces samples that are uniformly distributed in [0,1], ensuring the acceptance probability is identical to the true random case. The draft and target models Q and P are fixed and the total variation distance TV(Q,P) remains unchanged.
- Evidence anchors:
  - [abstract] "...preserves unbiasedness, attains maximal speculative sampling efficiency (1−TV(Q,P))..."
  - [Theorem 4.1(b)] "Maximum sampling efficiency: SE(Q'ζD, A'ζ) = 1−TV(Q,P)."
  - [Section 5, Table 1] AATPS values for the proposed method closely match the "Std. SpecSampl." baseline across all settings.
  - [corpus] Corpus papers do not address sampling efficiency tradeoffs in speculative decoding; no direct corpus evidence.
- Break condition: The mechanism breaks if the pseudorandom generator has poor statistical quality, causing the empirical acceptance probability to deviate from the theoretical min{1, P(w)/Q(w)}.

### Mechanism 3
- Claim: Incorporating the pseudorandom acceptance variable ζR into the detection process improves watermark detectability by disambiguating whether a token originated from the draft or target model.
- Mechanism: During detection, the watermark signal (e.g., Gumbel-max score or SynthID tournament bits) can come from either the draft or target distribution. Knowing the acceptance variable u helps select the correct statistic. For Gumbel-max, this is done via a threshold τ on u; for SynthID, an MLP is trained to estimate P(token from draft | u). This targeted selection yields a stronger signal than averaging or random selection.
- Core assumption: The acceptance variable u carries exploitable information about the token's source (draft vs. target). The detection algorithm can be trained or calibrated to use u effectively.
- Evidence anchors:
  - [abstract] "...experiments demonstrate that this method improves watermark detectability (e.g., TPR@FPR=1% up to 0.8 vs 0.6 for baselines)..."
  - [Section 4.2] Describes Ars-τ and Bayes-MLP detection methods that leverage u.
  - [Figure 2 (middle & right)] Shows higher TPR for the proposed method (orange curves) compared to prior-based methods (blue curves).
  - [corpus] Corpus papers discuss watermark detection challenges but not the use of acceptance variables; no direct corpus evidence.
- Break condition: The mechanism fails if u is uncorrelated with the token's source, or if the detection model cannot learn the relationship between u and the correct statistic.

## Foundational Learning

- Concept: **Speculative Sampling (Speculative Decoding)**
  - Why needed here: The entire work builds on speculative sampling as the acceleration framework. You must understand the draft-then-verify loop, the acceptance probability formula, and how efficiency is measured by the acceptance rate.
  - Quick check question: Given draft distribution Q and target distribution P, what is the probability that a token w is accepted? What distribution is sampled from if w is rejected?

- Concept: **Unbiased Watermarking**
  - Why needed here: The paper focuses on unbiased watermarks where Eζ[P'ζ] = P. Understanding this property is essential to grasp how the proposed method maintains output quality.
  - Quick check question: Define unbiasedness for a watermarking decoder. Why is it important for preserving language model quality?

- Concept: **Pseudorandom Number Generators (PRNGs)**
  - Why needed here: The core innovation is using pseudorandomness in acceptance. You need to know how PRNGs produce deterministic sequences that approximate randomness.
  - Quick check question: How does a PRNG differ from a true random number generator? What properties must a PRNG have to replace true randomness in sampling?

## Architecture Onboarding

- Component map:
  - Prompt & Context -> Draft Model (Q) -> Target Model (P) -> Pseudorandom Generator (G) -> Acceptance Logic -> Residual Sampler -> Bonus Sampler -> Detection Module

- Critical path: Prompt → Draft Model (generates K tokens with ζD) → Target Model (computes logits in parallel) → For each draft token: compute u=G(ζR), compare to threshold → if accept, output token; else sample from residual and break. After loop, if all accepted, sample bonus token. Detection uses stored u and pseudorandom values.

- Design tradeoffs:
  - **Watermark Strength vs. Sampling Efficiency**: The paper's method aims to achieve both simultaneously, but in practice, choices of watermark decoder (S), lookahead (K), and PRNG quality affect the balance.
  - **Detection Complexity vs. Accuracy**: Using MLP for SynthID detection (Bayes-MLP) is more complex than threshold-based (Ars-τ) but may yield better TPR.
  - **PRNG Period vs. Sequence Length**: A PRNG with too short a period may repeat values in long generations, potentially affecting watermark patterns.

- Failure signatures:
  - **Unbiasedness violation**: Log perplexity (LOGPPL) increases significantly compared to unwatermarked baseline, indicating output distribution shift.
  - **Efficiency drop**: Average Accepted Tokens Per Step (AATPS) falls noticeably below standard speculative sampling baseline.
  - **Detection degradation**: True Positive Rate (TPR) at low False Positive Rate (FPR) decreases compared to prior methods, suggesting acceptance variable is not informative.

- First 3 experiments:
  1. **Verify unbiasedness**: Generate text with and without watermarking (using the proposed method), compute log perplexity on a validation set, and check for statistically significant differences (per Table 1/2 in paper).
  2. **Measure sampling efficiency**: Run generation with the proposed method and standard speculative sampling, record AATPS across different lookahead K values, and confirm they match within confidence intervals (per Table 1/2).
  3. **Assess detection improvement**: Train detection models (Ars-τ and Bayes-MLP) on a held-out set, evaluate TPR at FPR=1% on a test set, and compare against baseline detection methods (Ars-Prior, Bayes-Prior) to quantify improvement (per Figure 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed pseudorandom acceptance mechanism be effectively extended to advanced variants of speculative sampling, such as tree-based decoding methods?
- Basis in paper: [explicit] The Conclusion states that while the work focuses on standard speculative sampling, "the framework points toward potential extensions to variants such as tree-based methods (Miao et al., 2024; Cai et al., 2024), which could further accelerate generation."
- Why unresolved: The current theoretical proofs and Algorithm 1 are designed for the standard linear draft-then-verify setting, whereas tree-based methods require verifying multiple branching candidate sequences simultaneously.
- What evidence would resolve it: A modified algorithm and theoretical proof showing that pseudorandom acceptance preserves unbiasedness and maximal efficiency in a tree-structured verification setting, along with empirical benchmarks showing speedups over standard tree-based speculative decoding.

### Open Question 2
- Question: How can the framework be adapted to support non-degenerate (soft) or biased watermarks without sacrificing the established trade-off benefits?
- Basis in paper: [explicit] The Conclusion notes, "our current work directly applies to unbiased degenerate watermarks, but it is an open and interesting direction to investigate how to extend our framework and establish similar improvements to non-degenerate watermarks and even biased ones."
- Why unresolved: The current proof of maximal watermark strength relies on the condition that the watermarked distribution is degenerate (deterministic given the seed), a property that does not hold for soft watermarks or biased schemes that trade quality for detectability.
- What evidence would resolve it: A derivation of the trade-off curve for non-degenerate distributions or a modified acceptance mechanism that maintains maximal efficiency while maximizing the distinct definition of strength applicable to soft/biased watermarks.

### Open Question 3
- Question: Does the injection of pseudorandomness into the acceptance step alter the robustness of the watermark to post-hoc editing or adversarial perturbation?
- Basis in paper: [explicit] The Conclusion states, "given that human edits can weaken watermark signals..., investigating the impact of pseudorandom acceptance on robustness to editing is an open direction for future work."
- Why unresolved: While the paper demonstrates improved detectability under the FPR/TPR metrics, it does not evaluate whether the deterministic coupling of acceptance variables makes the watermark more brittle or more resilient to token substitution/paraphrasing attacks compared to baselines.
- What evidence would resolve it: Experiments measuring the decay of detectability (TPR) as the watermarked text undergoes varying levels of simulated human editing or adversarial corruption.

### Open Question 4
- Question: What are the theoretical and empirical impacts of using asymmetric watermarking decoder classes for the draft and target models within this framework?
- Basis in paper: [explicit] The Conclusion suggests, "future work can explore broader (Qdraft, Qtarget) choices—for example, using different decoders for the draft and target models."
- Why unresolved: The paper primarily analyzes linearly watermarked classes or specific existing schemes (Hu's, Google's) where the structure is often consistent; the interaction between heterogeneous decoder logic and the pseudorandom acceptance kernel is not characterized.
- What evidence would resolve it: A theoretical analysis of the resulting trade-off curves when S_draft and S_target belong to different function classes, and empirical results showing whether such asymmetry can offer better practical utility (e.g., lower latency) than homogeneous pairings.

## Limitations

- The method's theoretical guarantees depend on the pseudorandom generator producing outputs indistinguishable from true randomness, which may not hold for all PRNGs or in the presence of statistical anomalies.
- Detection improvements rely on the acceptance variable carrying meaningful information about token origin, but this correlation may weaken with different model architectures or training dynamics.
- The watermark strength maximization assumes determinism with respect to the seed is sufficient for maximal detectability, but real-world detectors may face additional confounding factors like adversarial post-processing.

## Confidence

- **High confidence**: The theoretical framework for unbiasedness preservation and efficiency maintenance (Mechanisms 1 and 2).
- **Medium confidence**: The detection improvement claims (Mechanism 3), as effectiveness depends on watermark signal strength and detection model quality.
- **Medium confidence**: The assumption that PRNG outputs are indistinguishable from true randomness in the context of sampling, though standard PRNGs are generally adequate.

## Next Checks

1. **PRNG Sensitivity Analysis**: Systematically evaluate the impact of different PRNGs (e.g., Mersenne Twister, PCG, Xoshiro) and seed qualities on sampling efficiency and watermark detectability. Measure acceptance rate variance and TPR degradation across PRNG choices to quantify robustness.

2. **Long-Generation Stress Test**: Generate sequences of varying lengths (e.g., 100, 1000, 10000 tokens) and monitor for statistical anomalies in acceptance patterns or watermark detectability decay. This will reveal whether PRNG period or cumulative rounding errors affect the method's guarantees over extended use.

3. **Cross-Model Transferability**: Test the proposed method across diverse model pairs (e.g., different architectures, sizes, training datasets) to assess whether detection improvements generalize or are specific to the Llama/Gemma settings used in the paper. Compare TPR gains relative to baseline methods across this expanded set.