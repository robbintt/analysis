---
ver: rpa2
title: 'Fragments to Facts: Partial-Information Fragment Inference from LLMs'
arxiv_id: '2505.13819'
source_url: https://arxiv.org/abs/2505.13819
tags:
- attacks
- medical
- fragments
- fine-tuned
- prism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can leak sensitive training data through
  memorization and membership inference attacks. Prior work has primarily focused
  on strong adversarial assumptions, including attacker access to entire samples or
  long, ordered prefixes, leaving open the question of how vulnerable LLMs are when
  adversaries have only partial, unordered sample information.
---

# Fragments to Facts: Partial-Information Fragment Inference from LLMs

## Quick Facts
- **arXiv ID**: 2505.13819
- **Source URL**: https://arxiv.org/abs/2505.13819
- **Reference count**: 40
- **Primary result**: Data-blind methods (LR-Attack and PRISM) for PIFI attacks are competitive with data-aware baselines, revealing fine-tuned LLMs are susceptible to partial-information fragment inference.

## Executive Summary
This paper introduces Partial-Information Fragment Inference (PIFI), a threat model where an attacker knows only a subset of fragments (facts) about an individual and aims to infer additional private fragments from a fine-tuned LLM. The authors propose two data-blind methods: a likelihood ratio attack (LR-Attack) and a prior-regularized variant (PRISM). Experiments on medical and legal datasets show these attacks are effective, with larger models and more fine-tuning epochs increasing vulnerability. The work demonstrates that fine-tuned LLMs can leak sensitive training data through fragment-level memorization, even under weaker adversarial assumptions than prior work.

## Method Summary
The paper proposes PIFI attacks that leverage the higher conditional probability assigned by a fine-tuned LLM to a target fragment given a set of known fragments, compared to a shadow model trained without that specific sample. The LR-Attack computes the ratio of probabilities from the target and shadow models, while PRISM incorporates a prior from a "world model" ensemble to regularize the score. The authors evaluate these methods on medical (MTS-Dialog) and legal datasets using fine-tuned LLMs (Llama-3-8B, Qwen-2-7B, Mistral-7B) and compare them against a data-aware baseline. Fragments are extracted using SciSpacy NER, and attacks are tested across different model sizes, fine-tuning epochs, and regularization methods (LoRA, DP).

## Key Results
- Data-blind methods (LR-Attack and PRISM) achieve competitive performance against a data-aware baseline classifier.
- Larger models and more fine-tuning epochs significantly increase vulnerability to PIFI attacks.
- PRISM outperforms LR-Attack for common fragments by reducing false positives through prior regularization.
- LoRA and DP provide only modest protection against PIFI attacks compared to full fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A fine-tuned LLM assigns a higher likelihood to a target fragment $y^*$ given a set of known fragments $S$ if the specific sample $s$ containing both was present in the training data $D$, compared to a model trained without $s$.
- **Mechanism**: This relies on extractable memorization. When a model fine-tunes on a specific sample $s$ (e.g., a patient record with both "hypertension" and "osteoarthritis"), it updates weights to increase the conditional probability of the sample's tokens. By comparing the probability $p(y^*|S)$ from the target model $f_{\theta,D}$ against a "shadow model" $f_{\theta,D'}$ (trained without $s$), the attacker identifies a likelihood ratio spike indicating that the association between $S$ and $y^*$ is learned specific knowledge, not general population statistics.
- **Core assumption**: The association between the known fragments $S$ and the target fragment $y^*$ is specific to the training sample $s$ and not purely a result of general correlation in the broader data distribution $\mathcal{D}$.
- **Evidence anchors**: [abstract]: "We show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks... likelihood ratio attack inspired by methods from membership inference." [section 4.2]: "Large $\hat{\ell}$ indicates that the target model assigns an unusually high probability to $y^*$ relative to the shadow model... distinguishing between $\{s \in D\}$ and $\{s \notin D\}$."

### Mechanism 2
- **Claim**: Incorporating a "world model" probability $p_{world}$ acts as a prior to regularize the likelihood ratio, reducing false positives caused by general co-occurrences in the real world.
- **Mechanism**: This mechanism uses Bayesian updating. The PRISM method acknowledges that a high likelihood ratio might occur simply because $S$ and $y^*$ are generally correlated in the world (e.g., "cough" and "cold"), not because the specific individual's data was memorized. By querying an ensemble of general models ($f_{\theta,world}$) that did not see the fine-tuning data, PRISM estimates the general association. If $p_{world}$ is high, it suppresses the attack score, filtering out generic correlations to isolate specific memorization.
- **Core assumption**: The ensemble of "world models" accurately captures the general distribution of associations between fragments independent of the private training set $D$.
- **Evidence anchors**: [abstract]: "PRISM... regularizes the ratio by leveraging an external prior." [section 4.3]: "PRISM... uses the LR-Attack score to update a posterior likelihood by using a prior, ultimately reducing false positives." [section 8.7]: "PRISM performs best for more common fragments... incorporating a prior from the world model helps to curb false positives."

### Mechanism 3
- **Claim**: Vulnerability to Partial-Information Fragment Inference (PIFI) scales positively with model size and repeated data exposure (epochs).
- **Mechanism**: This is driven by capacity-dependent overfitting. Larger models (more parameters) and repeated passes over the same data (more epochs) increase the model's capacity to memorize specific, idiosyncratic associations within the training data. As the model transitions from learning generalizable patterns to memorizing specific instances, the gap between $p_D$ and $p_{D'}$ widens, creating a stronger signal for the LR-Attack.
- **Core assumption**: The model has sufficient capacity to store specific instance-level associations without saturating or generalizing them away.
- **Evidence anchors**: [section 8.2]: "More fine-tuning consistently increases the success rates of PIFI attacks." [section 8.4]: "Larger parameter counts confer greater capacity for memorizing training examples."

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here**: PIFI is a generalization of MIA. While MIA asks "Is this *exact* document in the training set?", PIFI asks "Is this *fact* in the training set given I know these other *facts*?" Understanding MIA is prerequisite to understanding how the LR-Attack leverages comparative probabilities.
  - **Quick check question**: How does PIFI differ from standard MIA regarding the adversary's knowledge of the target sample?

- **Concept: Likelihood Ratio Test (Neyman-Pearson)**
  - **Why needed here**: The theoretical justification for the LR-Attack. It explains why comparing $p_D$ and $p_{D'}$ is the optimal way to distinguish between the hypothesis "sample is in data" vs. "sample is not in data" at a fixed false positive rate.
  - **Quick check question**: In the context of this paper, what two probabilities form the ratio $\Lambda$, and what does a high ratio imply?

- **Concept: Shadow Models**
  - **Why needed here**: The attacker cannot know the true distribution of models trained *without* the target data without simulation. Shadow models are the control group in this experiment, providing the baseline probability $p_{D'}$ required to detect the "signal" of the target model.
  - **Quick check question**: What data distribution is used to train the shadow model $f_{\theta, D'}$ in this paper?

## Architecture Onboarding

- **Component map**: Target Model ($f_{\theta,D}$) -> Shadow Model ($f_{\theta,D'}$) -> World Model ($f_{\theta,world}$) -> Prompt Constructor -> Scoring Engine
- **Critical path**:
  1. **Fragment Extraction**: Identify known public fragments $S$ for a target individual.
  2. **Candidate Selection**: Select a private target fragment $y^*$ to test.
  3. **Probability Query**: Pass the prompt containing $S$ and $y^*$ to the Target, Shadow, and World models to retrieve $p_D$, $p_{D'}$, and $p_{world}$.
  4. **Inference**: Calculate the LR-Attack statistic ($p_D / p_{D'}$) or PRISM score (Bayesian update using $p_{world}$). If score $> \tau$, infer presence.

- **Design tradeoffs**:
  - **LR-Attack vs. PRISM**: Use LR-Attack for "rare" or idiosyncratic fragments (e.g., rare diseases) where the likelihood ratio spikes sharply. Use PRISM for "common" fragments (e.g., "pain") where general correlations might cause false positives.
  - **Full Fine-tuning vs. LoRA**: Full fine-tuning creates a stronger attack surface (higher TPR) than LoRA, as LoRA constrains weight updates and appears to mitigate some memorization.
  - **Data-aware vs. Data-blind**: The paper uses a LightGBM classifier as a theoretical upper bound (Data-aware). Real-world deployment requires Data-blind methods (LR/PRISM), which the paper shows are competitive.

- **Failure signatures**:
  - **High False Positive Rate (FPR) on generic terms**: The model infers a connection because terms are linguistically common, not because of specific patient data. (Mitigation: Use PRISM).
  - **Low TPR on Single-Epoch models**: The signal is too weak if the model hasn't seen the data enough times to memorize the specific association.
  - **Legal Domain Failure**: Attacks perform worse in legal contexts because the language overlaps heavily with general pre-training data, diluting the specific "fine-tuning" signal.

- **First 3 experiments**:
  1.  **Sanity Check (LR-Attack)**: Fine-tune a small model (e.g., Llama-3-8B) on the MTS-Dialog dataset for 10 epochs. Verify that $p_D$ is consistently higher for training samples than the shadow model.
  2.  **Frequency Ablation**: Stratify test fragments by frequency (rare vs. common). Compare the ROC curves of LR-Attack vs. PRISM to confirm that LR excels on rare items while PRISM excels on common items (See Table 3).
  3.  **Defense Evaluation**: Fine-tune a model using LoRA or Differential Privacy (DP-SGD) and measure the drop in TPR at 2% FPR to evaluate the effectiveness of these defenses against PIFI.

## Open Questions the Paper Calls Out
- How does the efficacy of differential privacy (DP) fine-tuning as a defense against PIFI attacks vary across different privacy budgets ($\epsilon$) and model scales?
- Can specialized regularization techniques or perturbation methods effectively mitigate partial-information leakage without the utility degradation associated with differential privacy?
- To what extent does the performance of prior-regularized attacks like PRISM depend on the inclusion of state-of-the-art "world models" that are excluded from the current open-source ensemble?

## Limitations
- The exact PRISM scoring formula is not explicitly specified, leaving ambiguity about implementation.
- The paper does not explore the effectiveness of PIFI attacks on frontier models (e.g., GPT-4, Claude) or provide comprehensive defense evaluations across privacy budgets.
- The threat model assumes the attacker knows which fragments belong to a specific individual, but does not address how an attacker would reliably identify this set of public fragments in real-world scenarios.

## Confidence
**High Confidence**: The core mechanism of the likelihood ratio attack is well-established from membership inference literature. The observation that larger models and more fine-tuning epochs increase vulnerability is consistent with extractable memorization research. The distinction between rare and common fragments requiring different attack strategies (LR vs. PRISM) is supported by the empirical results.

**Medium Confidence**: The competitive performance of data-blind methods against the data-aware baseline suggests robustness, but the exact magnitude of this advantage depends on implementation details not fully specified. The effectiveness of LoRA and DP as defenses is demonstrated but not exhaustively explored across parameter settings.

**Low Confidence**: The exact PRISM scoring formula and its hyperparameter sensitivity remain unclear. The paper's assumption about the world model ensemble providing accurate priors for all fragment types, especially domain-specific medical terminology, is not fully validated.

## Next Checks
1. **PRISM Formula Verification**: Implement and test multiple plausible formulations of the PRISM score based on the described Bayesian framework. Compare results to ensure the method's effectiveness is not dependent on a specific mathematical interpretation.
2. **Real-World Fragment Identification**: Design a proof-of-concept attack where an attacker must first identify which fragments belong to a target individual using only public sources (social media, public health records). Measure how fragment identification accuracy affects overall attack success.
3. **Defense Parameter Sensitivity**: Systematically vary LoRA rank, alpha, and DP noise parameters to map the full defense landscape. Identify the minimum parameter settings that provide meaningful protection without severely degrading model utility.