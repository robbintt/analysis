---
ver: rpa2
title: Everyone prefers human writers, including AI
arxiv_id: '2510.08831'
source_url: https://arxiv.org/abs/2510.08831
tags:
- human
- attribution
- bias
- style
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AI evaluators show 2.5\xD7 stronger preference for content labeled\
  \ as human-authored compared to humans, with attribution bias of +34.3pp versus\
  \ +13.7pp for humans. Both humans and AI models devalue AI-generated content when\
  \ correctly labeled, but AI systems exhibit dramatically greater susceptibility\
  \ to attribution cues."
---

# Everyone prefers human writers, including AI

## Quick Facts
- arXiv ID: 2510.08831
- Source URL: https://arxiv.org/abs/2510.08831
- Authors: Wouter Haverals; Meredith Martin
- Reference count: 40
- Primary result: AI evaluators show 2.5× stronger preference for human-labeled content compared to humans

## Executive Summary
This study reveals a fundamental attribution bias in both human and AI evaluators, where content labeled as AI-generated receives systematically lower aesthetic ratings regardless of actual authorship. The bias operates across multiple AI architectures and is substantially stronger in AI evaluators (+34.3pp) than humans (+13.7pp). Using a three-condition experimental design (blind/open-label/counterfactual) on literary style exercises, the research demonstrates that attribution cues trigger criterion inversions that privilege perceived human authorship, with AI systems showing dramatically greater susceptibility to these cues.

## Method Summary
The study employed a between-subjects experimental design comparing 30 pairs of literary exercises (human-written originals vs GPT-4 variants) under three attribution conditions. Participants (humans via Prolific, AI models via OpenRouter API) made binary forced-choice decisions about aesthetic preference. Study 1 tested single-generator variants (GPT-4) with 13 AI evaluators; Study 2 expanded to 14×14 cross-evaluations. Attribution bias was measured as the percentage point difference between counterfactual and open-label conditions, with mixed-effects modeling and leave-one-out robustness checks.

## Key Results
- AI evaluators show 2.5× stronger preference for human-labeled content (+34.3pp vs +13.7pp for humans)
- Attribution bias operates across 14×14 combinations of AI evaluators and creators (+25.8pp, 95% CI: 24.1-27.6%)
- Content labeled as AI-generated is systematically devalued even when quality-matched to human content
- Between-model variance was moderate (Cohen's h range: 0.40-1.50)

## Why This Works (Mechanism)

### Mechanism 1: Attribution Label as Learned Token Association Trigger
- Claim: Attribution labels activate learned associations from training data that bias evaluation before content analysis begins
- Core assumption: Effect operates through token-level statistical associations rather than explicit reasoning about authorship
- Evidence: Models internalize patterns where "AI-generated" tokens co-occur with critique, while "human-authored" tokens associate with praise

### Mechanism 2: Criterion Inversion Through Deference-to-Human Script
- Claim: Attribution cues activate a "deference-to-human" evaluation schema that reweights which features count as evidence of quality
- Core assumption: Models have internalized evaluative hierarchies through reward signal internalization
- Evidence: RLHF alignment training installs learned reliability prior where deferring to human preferences is rewarded

### Mechanism 3: Cross-Architecture Generalization of Attribution Sensitivity
- Claim: Attribution bias is a fundamental property of transformer-based LLMs rather than architecture-specific
- Core assumption: Bias represents convergent behavior across independent training runs
- Evidence: 14×14 cross-evaluation design shows universal directional effects across diverse model families

## Foundational Learning

- Concept: Attribution bias in social psychology
  - Why needed: Experimental design builds on decades of research showing source cues shape interpretation
  - Quick check: Can you explain why three-condition design isolates attribution bias from content quality?

- Concept: RLHF and alignment training
  - Why needed: Primary mechanism explanation depends on understanding how preference training installs evaluative priors
  - Quick check: How would removing RLHF from training pipeline change predicted attribution bias magnitude?

- Concept: Cohen's h effect size for proportions
  - Why needed: Paper uses Cohen's h rather than Cohen's d because outcome is binary preference
  - Quick check: Why is h=0.28 considered "moderate" while h=0.70 is "large"?

## Architecture Onboarding

- Component map: Stimulus generation (Queneau styles) -> Attribution manipulation (three conditions) -> Evaluation interface (binary choice) -> Analysis pipeline (mixed-effects modeling)

- Critical path: Generate matched content pairs -> Apply attribution labels consistently -> Measure preference shifts as counterfactual minus open-label difference

- Design tradeoffs: Between-subjects vs within-subjects design; single vs multiple generators; temperature settings for naturalistic vs deterministic comparison

- Failure signatures: Blind-condition preferences differ from 50% (content confounds); counterfactual and open-label show similar preferences (manipulation failed)

- First 3 experiments: 1) Replicate three-condition design with your target model; 2) Test neutral attribution phrasing persistence; 3) Add fourth condition with attribution after evaluation

## Open Questions the Paper Calls Out

- Do AI model explanations for aesthetic judgments reflect underlying causal mechanisms or learned interpretive schemas?
- What specific training mechanisms cause attribution bias to emerge more strongly in some AI architectures than others?
- Can attribution bias in AI evaluators be effectively mitigated through debiasing interventions?

## Limitations

- Stimulus material consists of short literary exercises rather than naturalistic content, limiting ecological validity
- Binary forced-choice paradigm may not capture continuous nature of aesthetic judgment
- Study focuses exclusively on text-based creative content, leaving open whether similar biases exist for other modalities

## Confidence

- High confidence: Directional finding that both humans and AI devalue AI-labeled content is robust
- Medium confidence: Specific magnitude differences between AI and human evaluators may vary with content types
- Low confidence: Mechanism explanations remain speculative without direct experimental validation

## Next Checks

1. Test attribution bias persistence with novel attribution phrasings that had no training co-occurrence
2. Implement attention-check validation system with clear criteria and rerun human evaluations
3. Conduct cross-cultural validation by replicating study with evaluators from different linguistic backgrounds