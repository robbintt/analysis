---
ver: rpa2
title: Analysis of the vulnerability of machine learning regression models to adversarial
  attacks using data from 5G wireless networks
arxiv_id: '2505.00487'
source_url: https://arxiv.org/abs/2505.00487
tags:
- adversarial
- data
- regression
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies how regression models in 5G wireless networks
  can be attacked using adversarial examples. A DeepMIMO dataset is generated to simulate
  massive MIMO scenarios, focusing on predicting combined path loss between base stations
  and users.
---

# Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks

## Quick Facts
- arXiv ID: 2505.00487
- Source URL: https://arxiv.org/abs/2505.00487
- Authors: Leonid Legashev; Artur Zhigalov; Denis Parfenov
- Reference count: 13
- One-line primary result: Adversarial poisoning attacks can increase regression MSE by 33% and reduce R² by 10%, but a LightGBM classifier can detect poisoned samples with 98% accuracy

## Executive Summary
This paper investigates the vulnerability of machine learning regression models to adversarial attacks in 5G wireless networks. Using the DeepMIMO framework to simulate massive MIMO scenarios, the authors demonstrate that a regression model predicting combined path loss can be significantly degraded through FGSM-based poisoning attacks. The study shows that adversarial examples increase prediction error metrics while simultaneously creating distinguishable feature signatures that enable high-accuracy detection. By removing detected poisoned samples, the original model performance can be restored, demonstrating both the attack surface and potential defensive strategies for real-world 5G deployments.

## Method Summary
The study employs the DeepMIMO framework to generate a dataset simulating 5G massive MIMO scenarios, specifically the Boston5G_28 configuration with 105,842 samples. A LinearRegressor is trained on clean data and then converted to a PyTorch neural network for adversarial example generation using FGSM with ε=1e-10. The poisoning attack targets the training phase to maximize prediction errors. To detect adversarial samples, three binary classifiers (LightGBM, CatBoost, XGBoost) are trained on labeled benign vs poisoned data. The LightGBM classifier achieves 98% F1-score, enabling the removal of poisoned samples and restoration of original regression performance. The experimental pipeline includes data generation, model training, attack implementation, detection, and performance recovery evaluation.

## Key Results
- Adversarial FGSM poisoning increases regression MSE by 33% and reduces R² by 10%
- LightGBM binary classifier achieves 98% F1-score in detecting poisoned vs benign samples
- Removing detected adversarial anomalies restores regression model performance to baseline levels
- The defense mechanism successfully identifies malicious network activity in 5G wireless scenarios

## Why This Works (Mechanism)
The mechanism exploits the gradient-based nature of FGSM attacks, which create adversarial examples by adding small perturbations in the direction that maximizes prediction error. In regression tasks predicting path loss in 5G networks, these perturbations cause significant MSE increases while simultaneously creating detectable statistical signatures in the feature space. The binary classifier leverages these signatures by learning the distributional differences between clean and poisoned data. When poisoned samples are removed, the regression model is retrained on clean data, effectively restoring its original predictive capabilities.

## Foundational Learning
- **DeepMIMO Framework**: 5G wireless channel simulator for generating realistic massive MIMO data - needed for creating reproducible 5G network scenarios with specific path loss characteristics
- **FGSM Attack**: Fast Gradient Sign Method for generating adversarial examples - needed to demonstrate vulnerability of regression models to gradient-based poisoning
- **Binary Classification for Anomaly Detection**: Using supervised learning to distinguish between benign and adversarial data - needed to implement the detection mechanism that enables defense
- **Gradient-Based Optimization**: Computing gradients of loss with respect to inputs for adversarial perturbation generation - needed to understand how FGSM maximizes prediction errors
- **Feature Engineering in Wireless Networks**: Extracting relevant features like DoA, DoD, and phase information - needed to create meaningful input representations for both regression and classification tasks
- **Model Transfer Learning**: Converting trained sklearn model to PyTorch for adversarial example generation - needed to bridge different ML frameworks in the experimental pipeline

## Architecture Onboarding

**Component Map**
DeepMIMO -> Data Preprocessing -> LinearRegression -> PyTorch Conversion -> FGSM Attack -> Binary Classifier Training -> Performance Evaluation -> Sample Removal -> Regression Recovery

**Critical Path**
Data generation → Model training → Adversarial attack → Detection training → Performance validation → Recovery verification

**Design Tradeoffs**
The choice of LinearRegressor (simple, interpretable) vs. deep neural networks (potentially more vulnerable but complex) represents a key tradeoff. FGSM with ε=1e-10 balances attack effectiveness with detection difficulty, while the 40:40:20 data split optimizes for both attack training and detection model validation.

**Failure Signatures**
- MSE increase significantly below 33% indicates FGSM implementation issues or insufficient perturbation magnitude
- Binary classifier F1-score below 90% suggests feature distribution overlap between benign and poisoned samples
- Failure to recover baseline performance indicates either incomplete poisoning detection or regression model sensitivity to data filtering

**3 First Experiments**
1. Generate DeepMIMO Boston5G_28 dataset and verify feature distributions match massive MIMO characteristics
2. Implement FGSM attack and validate 33% MSE increase target is achievable with specified parameters
3. Train LightGBM classifier and confirm 98% F1-score on balanced benign vs poisoned classification task

## Open Questions the Paper Calls Out

### Open Question 1
Does the vulnerability of the regression model and the effectiveness of the LightGBM defense persist when applied to non-linear deep learning architectures? The study explicitly limits the victim model to a LinearRegressor initialized as a single-layer neural network without an activation function. The paper does not test complex architectures (e.g., LSTM, CNN) which may react differently to gradient-based perturbations or exhibit different feature signatures for the detector. Replicating the FGSM attack and detection pipeline using deep non-linear regression models on the same DeepMIMO dataset would resolve this.

### Open Question 2
Can the binary classifier maintain 98% accuracy against more sophisticated adversarial attacks designed specifically to bypass detection? The authors note that "many current studies... are devoted to the problem of classification," but this study only implements the basic Fast Gradient Sign Method (FGSM). The defense is trained on FGSM examples; it is unknown if the classifier is robust against iterative attacks (like Projected Gradient Descent) or "Low Profile" attacks mentioned in the literature review. Testing the trained LightGBM classifier against iterative or constrained optimization attacks to measure detection recall would resolve this.

### Open Question 3
Is the proposed defense mechanism effective against "imperceptible" perturbations where the noise magnitude (ε) is significantly reduced? The paper selects a specific ε to maximize error, noting that larger perturbations are easily identified as anomalies, but does not explore the lower bound of detectability. An attacker could use smaller perturbations that degrade model performance over time without triggering the anomaly detector. Evaluating the trade-off between perturbation size (ε), regression error increase (MSE), and detection accuracy (F1-score) would resolve this.

## Limitations
- Exact DeepMIMO configuration parameters are unspecified, making exact reproduction difficult
- The extremely small FGSM perturbation magnitude (ε=1e-10) combined with fract=0.99999 creates near-total poisoning that may not reflect realistic attack scenarios
- Baseline regression performance metrics are not clearly specified, making quantitative validation of recovery difficult
- Feature scaling/normalization approach is not mentioned, which could significantly impact FGSM effectiveness
- Only tests FGSM attack, not more sophisticated adversarial techniques designed to evade detection

## Confidence

**High Confidence**: The general methodology of applying FGSM to poison regression training data and using LightGBM for binary detection is clearly specified and reproducible.

**Medium Confidence**: The reported performance metrics (MSE increase of 33%, R² decrease of 10%, 98% F1-score) can be approximately reproduced but may vary based on implementation details.

**Low Confidence**: The exact quantitative recovery of regression performance to "original levels" cannot be verified without knowing the baseline metrics.

## Next Checks

1. Generate the DeepMIMO Boston5G_28 dataset with the specified parameters and verify feature distributions match expected massive MIMO characteristics.

2. Implement FGSM with ε=1e-10 and validate that adversarial perturbations produce visually distinct feature patterns while maintaining the 33% MSE increase target.

3. Train and evaluate all three classifiers (LightGBM, CatBoost, XGBoost) on the poisoned vs benign classification task to confirm LightGBM achieves the reported 98% F1-score performance.