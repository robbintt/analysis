---
ver: rpa2
title: Second-order Theory of Mind for Human Teachers and Robot Learners
arxiv_id: '2503.16524'
source_url: https://arxiv.org/abs/2503.16524
tags:
- learner
- teacher
- feedback
- robot
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of learner feedback that creates
  or perpetuates erroneous beliefs between human teachers and robot learners, thereby
  increasing the cognitive burden on teachers. The core method introduces a Second-order
  Theory of Mind (ToM-2) framework that enables robot learners to model and account
  for their teacher's beliefs about the learner's task knowledge and learning processes.
---

# Second-order Theory of Mind for Human Teachers and Robot Learners

## Quick Facts
- **arXiv ID**: 2503.16524
- **Source URL**: https://arxiv.org/abs/2503.16524
- **Reference count**: 6
- **Primary result**: Introduces Second-order Theory of Mind (ToM-2) framework for robot learners to model teacher's beliefs about learner's knowledge, reducing cognitive burden on teachers

## Executive Summary
This work addresses the problem of learner feedback that creates or perpetuates erroneous beliefs between human teachers and robot learners, thereby increasing the cognitive burden on teachers. The core method introduces a Second-order Theory of Mind (ToM-2) framework that enables robot learners to model and account for their teacher's beliefs about the learner's task knowledge and learning processes. The approach uses an Interactive Partially Observable Markov Decision Process (I-POMDP) augmented with learnable observation functions to model sources of perceived rationality, and incorporates Confidence Expressions (CEs) into feedback to convey the learner's certainty about stated features. The research evaluates this framework through both simulated interactions and a user study where human participants teach a robot rules of varied complexities, measuring teaching efficacy through the number of rounds needed to learn rules and the frequency of teacher misunderstandings.

## Method Summary
The proposed method employs an I-POMDP framework to model nested beliefs between teacher and learner, augmented with learnable observation functions that capture the teacher's degree of rationality through a temperature parameter β. The learner maintains beliefs over both the hidden rule and the teacher's rationality, updating these beliefs through observation of teacher card placements. Confidence Expressions ("I know," "I think," "I'm unsure if") are prepended to feedback utterances to explicitly communicate the learner's certainty level. The feedback selection mechanism chooses utterances that optimize learning while accounting for the modeled teacher beliefs about the learner's knowledge state.

## Key Results
- The ToM-2 framework enables robot learners to model teacher's beliefs about learner's task knowledge and learning processes
- Learnable observation functions allow inference of teacher's degree of rationality during interaction
- Confidence Expressions reduce teacher misunderstanding by making learner certainty explicit rather than implied
- User study planned to measure teaching efficacy through rounds to learn and frequency of teacher misunderstandings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling the teacher's beliefs about the learner reduces misaligned feedback and improves teaching efficiency.
- Mechanism: The I-POMDP framework extends standard POMDP states to "interactive states" that include both environment states and models of other agents. These nested models allow the learner to represent what the teacher believes about the learner's task knowledge and learning processes, enabling feedback selection that accounts for potential misunderstandings.
- Core assumption: Erroneous beliefs stem primarily from mismatches between actual and perceived rationality in how agents interpret each other's actions.
- Evidence anchors: [abstract] "This work endows an AI learner with a Second-order Theory of Mind that models perceived rationality as a source for the erroneous beliefs a teacher and learner may have of one another." [section] "With this framework, the learner can model the teacher's beliefs about the learner's task knowledge and learning processes through the components which comprise an I-POMDP." [corpus] "Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch" directly addresses mental model alignment in human-robot teaching contexts.
- Break condition: If teacher behavior is inconsistent or non-stationary, the learned belief model may fail to converge, producing misleading feedback.

### Mechanism 2
- Claim: Learnable observation functions enable the learner to infer the teacher's degree of rationality and adjust feedback accordingly.
- Mechanism: The I-POMDP is augmented with a discrete set of observation functions, each parameterized by a temperature β that controls noisily-rational behavior. By observing the teacher's card plays, the learner infers both the rule and the teacher's rationality level simultaneously.
- Core assumption: Teachers interpret learner feedback along a spectrum from perfectly-rational to perfectly-irrational, and this interpretation is stable within a session.
- Evidence anchors: [section] "This work augments the I-POMDP with a discrete set of learnable observation functions, each of which is a noisily-rational model whose rationality is inversely-proportional to a temperature parameter β." [section] "By observing the teacher's card plays, the learner will be able to learn the teacher's degree of rationality as it also learns the rule governing the teacher's actions." [corpus] Weak direct evidence; related work on ToM in embodied agents (MindPower) addresses belief inference but not this specific rationality-learning mechanism.
- Break condition: If the teacher's rationality varies across contexts or features, a single global β estimate will mischaracterize their behavior.

### Mechanism 3
- Claim: Confidence Expressions reduce teacher misunderstanding by making learner certainty explicit rather than implied.
- Mechanism: Feedback utterances are prepended with one of three confidence levels ("I know," "I think," "I'm unsure if") selected to match the learner's actual belief certainty. This prevents teachers from assuming 100% confidence when the learner is uncertain.
- Core assumption: Teachers default to interpreting categorical statements as high-confidence assertions unless explicitly told otherwise.
- Evidence anchors: [section] "Without CEs, feedback can express confidence far greater than the learner's actual confidence about the features it addresses, and this ambiguous communication can lead the teacher to misunderstand the learner's task knowledge." [section] "CEs are meant to convey the learner's certainty about the stated features." [corpus] Weak corpus support for this specific mechanism; related work on interpretable knowledge tracing models addresses transparency but not confidence expression in interactive feedback.
- Break condition: If confidence calibration is poor (e.g., overconfident beliefs), CEs will communicate incorrect certainty levels, potentially misleading teachers.

## Foundational Learning

- Concept: **Interactive POMDPs (I-POMDPs)**
  - Why needed here: The ToM-2 framework is built on I-POMDPs, which extend POMDPs to multi-agent settings by including models of other agents in the state representation.
  - Quick check question: Can you explain how an I-POMDP's "interactive state" differs from a standard POMDP state?

- Concept: **Theory of Mind (ToM) vs. Second-order ToM**
  - Why needed here: The paper distinguishes first-order ToM (inferring another's beliefs) from second-order ToM (awareness that others have ToM about you), which is the core innovation.
  - Quick check question: What is the difference between "I believe you want X" (ToM-1) and "I believe you think I want X" (ToM-2)?

- Concept: **Noisily-rational choice models ( Boltzmann/softmax)**
  - Why needed here: The learnable observation functions use temperature-parameterized rationality models, a standard approach in behavioral game theory.
  - Quick check question: How does increasing the temperature parameter β affect the distribution over actions in a softmax choice model?

## Architecture Onboarding

- Component map: **I-POMDP Core** -> **Belief Update Module** -> **Observation Function Bank** -> **Feedback Selector** -> **Confidence Expression Module**

- Critical path:
  1. Teacher plays a card → Learner observes
  2. Belief update over both rule hypothesis space and teacher rationality (β)
  3. Feedback selector generates candidate utterances
  4. CE module attaches appropriate confidence level
  5. Feedback utterance presented to teacher

- Design tradeoffs:
  - **Discrete vs. continuous β**: Paper uses discrete set of observation functions; continuous would allow finer-grained rationality modeling but increase computational cost
  - **CE granularity**: Three levels may be insufficient for nuanced belief states; more levels increase cognitive load for teachers
  - **Computational complexity**: Nested I-POMDPs scale poorly with number of agents and model depth; approximations likely required for real-time interaction

- Failure signatures:
  - Teacher continues providing redundant examples despite learner claiming high confidence → CE not influencing teacher behavior
  - Learner feedback oscillates between contradictory statements → Belief over rule or β failing to converge
  - Teacher ignores feedback entirely → Learner may have incorrectly inferred high β (perceived irrationality)

- First 3 experiments:
  1. **Validate observation function learning**: Run simulated teachers with known β values; measure correlation between inferred and ground-truth rationality over interaction rounds
  2. **Ablate Confidence Expressions**: Compare learning rounds and teacher misunderstanding rates between CE-enabled and CE-disabled conditions in simulation
  3. **Pilot user study with simple rules**: Deploy full system with 5-10 participants teaching 2-feature rules; measure rounds to convergence and collect NASA-TLX scores to verify cognitive burden reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ToM-2 framework significantly reduce teacher cognitive burden and teaching time compared to baseline learners?
- Basis in paper: [explicit] The paper states the "evaluations will investigate" these benefits and explicitly lists measuring "cognitive burden" (via NASA-TLX) and "teaching efficacy" (via rounds to learn) as primary goals of the proposed user study.
- Why unresolved: The paper outlines the methodology and hypotheses but presents the work as a proposal ("will evaluate," "will be undertaken"), implying the empirical results are not yet available.
- What evidence would resolve it: Quantitative results from the user study showing statistically significant reductions in NASA-TLX scores and the number of rounds required to learn the rule in the ToM-2 condition versus control conditions.

### Open Question 2
- Question: Can the learner accurately disentangle and infer the teacher's actual rationality level from the teacher's belief about the learner's rationality?
- Basis in paper: [explicit] The simulation experiments are designed to "investigate the learner’s ability to identify the teacher’s rationality and the teacher’s perceived learner rationality."
- Why unresolved: Inferring nested mental models (modeling the teacher's model of the learner) is a difficult inverse problem; it is unclear if the card-play domain provides sufficient observability to separate these two variables.
- What evidence would resolve it: Simulation results demonstrating that the learner’s inferred model parameters converge to the ground truth values for both the teacher’s rationality and the teacher’s belief regarding the learner’s rationality.

### Open Question 3
- Question: Is the Interactive POMDP (I-POMDP) approach computationally tractable for real-time interaction given the complexity of nested belief updates?
- Basis in paper: [inferred] The paper proposes using I-POMDPs to model nested beliefs but does not discuss computational complexity, solving algorithms, or latency constraints, despite the requirement for the robot to respond to human actions in real-time.
- Why unresolved: I-POMDPs are generally PSPACE-hard and computationally expensive; without specific approximation techniques or timing analysis, it is unknown if the system can generate feedback fast enough to maintain fluent interaction.
- What evidence would resolve it: An analysis of the system's computation time per decision step during the user study, confirming that feedback generation latency remains below the threshold of human perception (e.g., < 1 second).

## Limitations
- The computational complexity of nested I-POMDPs may limit real-time applicability without approximation techniques
- The framework's performance depends on accurate inference of teacher rationality, which may fail with inconsistent or non-stationary teacher behavior
- The three-level Confidence Expression system may be insufficient for nuanced belief states, potentially limiting communication precision

## Confidence

- **High confidence**: The theoretical framework of using I-POMDPs to model teacher beliefs about learner knowledge is well-grounded in existing literature on multi-agent planning and theory of mind.
- **Medium confidence**: The specific mechanism of learning teacher rationality through observation functions is novel but plausible, though direct empirical validation is limited.
- **Medium confidence**: The Confidence Expression mechanism addresses a real communication problem in human-robot interaction, but the corpus provides weak direct evidence for this specific approach.

## Next Checks

1. **Computational feasibility validation**: Measure belief update computation time and state-space size as interaction length increases to establish practical limits of the I-POMDP approach.

2. **Teacher rationality inference robustness**: Test the observation function learning mechanism with simulated teachers exhibiting varying levels of consistency and rationality, measuring inference accuracy across different interaction patterns.

3. **Confidence expression impact**: Conduct a controlled experiment comparing learning efficiency and teacher misunderstanding rates between CE-enabled and CE-disabled conditions with human teachers teaching simple rules to a simulated learner.