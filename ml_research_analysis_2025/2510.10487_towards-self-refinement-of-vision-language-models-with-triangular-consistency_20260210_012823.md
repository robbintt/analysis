---
ver: rpa2
title: Towards Self-Refinement of Vision-Language Models with Triangular Consistency
arxiv_id: '2510.10487'
source_url: https://arxiv.org/abs/2510.10487
tags:
- data
- llav
- arxiv
- latexit
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of Vision-Language Models
  (VLMs) to refine themselves without external supervision. The authors propose a
  self-refinement framework based on a Triangular Consistency principle, where a masked
  element within an image-question-answer triplet should be consistently and accurately
  reconstructed.
---

# Towards Self-Refinement of Vision-Language Models with Triangular Consistency

## Quick Facts
- arXiv ID: 2510.10487
- Source URL: https://arxiv.org/abs/2510.10487
- Reference count: 40
- Key result: Self-refinement framework achieves 2.1% average accuracy improvement on visual perception tasks and 1.7% on visual dialogue tasks

## Executive Summary
This study investigates whether Vision-Language Models can improve themselves without external supervision through a self-refinement framework based on Triangular Consistency. The approach generates synthetic instruction triplets from unlabeled images, filters them using reconstruction consistency scores, and retrains the VLM on the filtered data. Theoretical analysis from a causal perspective supports the framework's validity. Experimental results using LLaVA-1.5 demonstrate consistent improvements across multiple benchmarks, with gains generalizing across different VLM architectures and scales. The method shows potential as a pathway toward autonomous and continually improving multimodal intelligence.

## Method Summary
The self-refinement framework consists of three stages: (1) Multi-task training enhances the VLM's instruction generation capability by training on three tasks with different masking strategies; (2) Triangular Consistency filtering generates synthetic IQA triplets from unlabeled images and retains only those with high reconstruction consistency scores; (3) Instruction tuning merges filtered synthetic data with the original dataset to refine the VLM. The triangular consistency principle states that any masked element within an image-question-answer triplet should be consistently and accurately reconstructed, providing a self-generated quality signal for filtering synthetic data.

## Key Results
- 2.1% average accuracy increase on visual perception tasks (LLaVA-Wild, MM-Vet, MME-C, MMBench, ScienceQA-IMG)
- 1.7% average accuracy increase on visual dialogue tasks (MMBench-CN, VQAv2, GQA)
- Round 2 refinement provides additional gains of approximately 0.4% on average
- Method generalizes across different VLM architectures and scales
- Retained subset achieves 85.3% GPT-4o accuracy vs 59.9% for excluded triplets

## Why This Works (Mechanism)

### Mechanism 1: Triangular Consistency as Self-Generated Quality Signal
VLMs can self-evaluate instruction triplet quality by testing whether masked elements can be consistently reconstructed from remaining components. For each (Image, Question, Answer) triplet, the model masks A, predicts A' from (I,Q), masks Q, predicts Q' from (I,A), then computes consistency score S = √(Sim(Q,Q') × Sim(A,A')). High-scoring triplets are retained. Core assumption: Triplet quality correlates with reconstruction consistency—inconsistency indicates hallucination or misalignment. Break condition: If consistency scores don't correlate with downstream task improvement or human quality judgments, the signal is spurious.

### Mechanism 2: Multi-Task Training Unlocks Generation Capability
Standard instruction-tuned VLMs cannot reliably generate diverse, high-quality QA pairs; explicit multi-task training with bidirectional objectives is required. The approach combines three tasks: I→QA teaches holistic instruction synthesis, IA→Q teaches reverse inference, and standard IQ→A maintains answering capability. Combined loss trains a unified model capable of both answering and generating questions. Core assumption: Answering and generating questions are distinct capabilities requiring explicit training. Break condition: If generated QA pairs show low diversity or fail to cover salient image content, the multi-task training hasn't successfully transferred generation capability.

### Mechanism 3: Causal Signal Extraction from Unlabeled Images
Under Additive Noise Model assumptions, unlabeled images improve P(X|Y) estimation by providing better P(Y) observations, enabling self-refinement without external supervision. VLMs model P(X|Y) ∝ P(Y|X)P(X)/P(Y). Adding unlabeled images refines P(Y) estimate, which through Bayes' rule improves P(X|Y). Theoretical decomposition allows noise identification via deconvolution. Core assumption: ANM holds and forward mechanism P(Y|X) is invariant across distribution shifts. Break condition: If Round 2 shows no improvement or degradation, theoretical assumptions may be violated for real VLMs.

## Foundational Learning

- **Semi-supervised learning and pseudo-labeling:** The entire framework is self-training with pseudo-labels—using labeled seed data to train a generator, generating pseudo-labels for unlabeled images, filtering by consistency, then retraining. Understanding the bias-variance tradeoff in pseudo-labeling is essential for diagnosing failure modes. *Quick check question:* Why might consistency-based filtering preferentially select pseudo-labels that match the model's current distribution rather than expanding its capabilities?

- **Multi-task learning and negative transfer:** Stage 1 combines three tasks with shared parameters. Task interference could degrade original VQA capability while improving generation—need to monitor all task performance. *Quick check question:* If IQ→A accuracy drops significantly after multi-task training, what does this suggest about task compatibility and how might you diagnose it?

- **VLM architecture components (encoder-projector-LLM):** The method freezes the vision encoder and trains projector + LLM. This constrains what visual features can be refined—if the encoder misrepresents fine-grained visual details, self-refinement cannot fix it. *Quick check question:* Would you expect this method to improve OCR capability if the frozen vision encoder has poor text recognition? Why or why not?

## Architecture Onboarding

- **Component map:**
Stage 1: Enhancing Instruction Generation
Input: Original supervised dataset (e.g., llava_v1_5_mix665k)
Transform: Mask Q, A, or both (50%/30%/20% split) → create three task types
Train: Multi-task cross-entropy (L_qa + L_a + L_q)
Output: M_gen (generation-capable VLM)

Stage 2: Triangular Consistency Filtering
Input: Unlabeled images (e.g., 1M from LAION) + M_gen
Process: Generate (I,Q,A) → Reconstruct Q', A' → Compute S = √(Sim(Q,Q')×Sim(A,A'))
Filter: Retain top 20% per data type
Output: D_filtered (~200K triplets from 1M images)

Stage 3: Instruction Tuning
Input: Original dataset + D_filtered (merged)
Train: Standard instruction tuning (encoder frozen, projector+LLM trained)
Output: Refined VLM M_(k+1)

- **Critical path:**
1. Multi-task data preparation with correct masking ratios and prompt templates
2. Stage 1 training: ~20 hours on 8×A100
3. Similarity metric selection per data type
4. Filtering threshold calibration: 20% optimal in experiments
5. Stage 3 training: identical schedule to Stage 1
6. Optional: Round 2 iteration with new unlabeled images

- **Design tradeoffs:**
- Filtering threshold: Top 20% balances quality vs quantity; 5% too restrictive, 50%+ includes excessive noise
- Per-category filtering: Select top 20% within each data type to prevent domination by majority categories
- Masking ratios: 50/30/20 (Q&A/Q/A) chosen for balance; alternatives show comparable results
- Iteration count: Round 2 provides ~0.4% average gain—may not justify ~2× compute

- **Failure signatures:**
- Recap baseline matches SRF: Indicates filtering provides no signal
- Bottom 20% performs similarly to top 20%: Consistency score not discriminative
- Stage 1 degrades IQ→A performance: Multi-task training causing negative transfer
- Low diversity (TTR < 0.09, Distinct-2 < 0.35): Generation capability not successfully trained
- No Round 2 improvement: Theoretical assumptions violated or signal exhausted

- **First 3 experiments:**
1. Validate generation quality lift: Generate QA pairs for 1000 held-out images using M_gen vs baseline LLaVA-1.5; evaluate via GPT-4o accuracy and diversity metrics
2. Calibrate filtering threshold on your data: Train 5 models using top 5%/20%/50%/80%/100% filtered data; evaluate on your target benchmarks to find optimal threshold
3. Ablate data type contribution: Train separate models using only VQA data, only Caption data, etc.; map which data types improve which benchmark capabilities

## Open Questions the Paper Calls Out

- **Theoretical principles for filtering threshold selection:** The 20% threshold was empirically determined through ablation studies, but no theoretical justification exists for this choice. The relationship between filtering threshold, model capacity, and data characteristics remains unexplored beyond empirical observations.

- **Bias and hallucination amplification:** Learning from model-generated synthetic data risks reinforcing or amplifying biases and hallucinations present in the pretrained model. The paper demonstrates performance improvements but does not include bias auditing or hallucination frequency analysis.

- **Open-ended dialogue scalability:** The triangular consistency principle may not scale to open-ended, multi-turn dialogue scenarios where ground-truth consistency becomes ill-defined. The method uses similarity metrics suited to structured QA types but may be less discriminative for nuanced semantic consistency in free-form conversations.

## Limitations

- Triangular consistency assumes high reconstruction consistency correlates with genuine instruction quality rather than model-internal coherence patterns, which needs independent validation
- Theoretical causal framework relies on Additive Noise Model assumptions that may not hold for complex VLMs in practice
- Method effectiveness depends heavily on quality of unlabeled image sources and generation capability of multi-task trained model, which could vary across domains

## Confidence

**High Confidence:** Experimental results on 8 benchmark tasks show consistent improvements across multiple VLM architectures and scales, with quantitative gains of 2.1% on visual perception and 1.7% on visual dialogue tasks.

**Medium Confidence:** Triangular consistency filtering mechanism's effectiveness depends on the assumption that consistency scores reliably identify high-quality synthetic instructions, supported by correlation with human judgments but not independently validated across diverse domains.

**Low Confidence:** Theoretical causal framework's assumptions (ANM conditions, stable forward models) have not been empirically validated for real VLMs, and method performance may degrade if these assumptions are violated.

## Next Checks

1. **Ablation of Consistency Signal:** Generate synthetic triplets using M_gen, then randomly shuffle A or Q answers to create inconsistent triplets. Test whether filtering correctly identifies and excludes these while retaining consistent ones, confirming the discriminative power of the triangular consistency score.

2. **Cross-Domain Generalization:** Apply the complete pipeline (Stages 1-3) to a different domain (e.g., medical imaging, satellite imagery) using domain-specific unlabeled images and evaluate whether similar performance gains are observed, testing the method's generalizability beyond tested visual domains.

3. **Iteration Stability Analysis:** Run Round 2 refinement using the Round 1 refined model as the generator, tracking performance changes and consistency score distributions. Verify whether theoretical predictions about diminishing returns hold and whether any degradation occurs, which would indicate violation of causal assumptions.