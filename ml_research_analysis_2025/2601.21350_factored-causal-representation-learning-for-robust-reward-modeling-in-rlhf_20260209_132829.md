---
ver: rpa2
title: Factored Causal Representation Learning for Robust Reward Modeling in RLHF
arxiv_id: '2601.21350'
source_url: https://arxiv.org/abs/2601.21350
tags:
- reward
- causal
- arxiv
- rlhf
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CausalRM improves reward model robustness in RLHF by decomposing
  the latent representation into causal and non-causal factors. The causal component
  is used for reward prediction, while an adversarial head with gradient reversal
  suppresses reward-relevant information in the non-causal component.
---

# Factored Causal Representation Learning for Robust Reward Modeling in RLHF

## Quick Facts
- arXiv ID: 2601.21350
- Source URL: https://arxiv.org/abs/2601.21350
- Reference count: 35
- Primary result: CausalRM improves reward model robustness by decomposing latent representations into causal and non-causal factors, achieving higher pairwise accuracy and reduced sensitivity to spurious attributes like response length and sycophancy bias.

## Executive Summary
CausalRM addresses reward model brittleness in RLHF by decomposing the LLM embedding into causal and non-causal latent factors. The causal component predicts rewards while an adversarial head with gradient reversal suppresses reward-relevant information in the non-causal component. This structural approach achieves higher pairwise accuracy on both in-distribution and out-of-distribution preference data, better downstream RLHF performance, and robustness to spurious attributes like response length and sycophantic bias.

## Method Summary
CausalRM trains a reward model that decomposes the backbone embedding h into two latent variables z_c (causal) and z_nc (non-causal) using a variational encoder. The reward head depends only on z_c, while an adversarial head with gradient reversal actively removes reward-predictive information from z_nc. Training jointly optimizes a Bradley-Terry preference loss on z_c, KL regularization to standard normal priors, reconstruction loss, and adversarial loss. The method uses OpenRLHF v0.8.5 with Qwen2.5-7B base models, trained for 1 epoch with specific hyperparameters for math and dialogue tasks.

## Key Results
- Achieved up to 70.1% pairwise accuracy on math and 72.3% on dialogue benchmarks
- Demonstrated reduced sensitivity to spurious attributes: σ_len ≈ 0.03 vs. 0.12-0.22 for baselines on length bias
- Showed improved downstream RLHF performance with better final-answer accuracy and win rates
- Maintained robustness under hacked preference data (e.g., <2% accuracy drop vs. >10% for baselines on sycophancy)

## Why This Works (Mechanism)

### Mechanism 1: Factored Latent Decomposition via Variational Encoding
- **Claim:** Separating the backbone embedding into distinct causal and non-causal latent channels creates structural inductive bias against spurious reward correlations.
- **Mechanism:** A variational encoder maps h to two independent diagonal-Gaussian posteriors q_α(z_c|h) and q_α(z_nc|h), each with standard normal priors. This factorization forces the model to decide which channel encodes each type of information.
- **Core assumption:** The backbone embedding contains both reward-causal and reward-spurious signals that are approximately separable in latent space.
- **Evidence anchors:** "decomposes the latent representation into causal and non-causal factors" (abstract), VAE encoding described in section 3.2.

### Mechanism 2: Structural Invariance via Restricted Reward Head
- **Claim:** Constraining reward prediction to depend exclusively on causal factors implements invariance structurally.
- **Mechanism:** The reward head g_ψ computes r̂ = g_ψ(z_c) with no architectural path from z_nc, enforcing r ⊥⊥ z_nc by construction.
- **Core assumption:** The causal latent z_c contains sufficient information to predict human preferences with acceptable accuracy.
- **Evidence anchors:** "The reward head is then constrained to depend only on the causal component" (abstract), structural bias described in section 3.2.

### Mechanism 3: Adversarial Gradient Reversal for Active Leakage Suppression
- **Claim:** Adversarial training with gradient reversal actively removes residual reward-predictive information from the non-causal channel.
- **Mechanism:** An adversarial head a_ω predicts reward from z_nc, with gradients reversed during backpropagation to create a minimax game that suppresses reward signals.
- **Core assumption:** Reward-relevant information in z_nc can be detected by a linear probe and removed via gradient-based optimization.
- **Evidence anchors:** "an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal" (abstract), section 3.3 describes the minimax optimization.

## Foundational Learning

- **Variational Autoencoders and Information Bottlenecks**
  - Why needed here: CausalRM uses KL-regularized posteriors to enforce information bottlenecks on both latent factors.
  - Quick check question: Why does KL(q(z|x) || p(z)) upper-bound I(x; z), and what happens if this term goes to zero?

- **Gradient Reversal Layer (GRL)**
  - Why needed here: Understanding how GRL enables adversarial domain-invariant learning by reversing gradients.
  - Quick check question: If a feature extractor f feeds a classifier d through a GRL, what gradient does f receive from d's loss?

- **Bradley-Terry Preference Model**
  - Why needed here: This defines the pairwise preference loss used for reward model training.
  - Quick check question: Given rewards r_w and r_l, what's the Bradley-Terry probability that y_w ≻ y_l?

## Architecture Onboarding

- **Component map:** Input (x, y) → LLM Backbone f_φ → h → VAE Encoder → z_c, z_nc → Reward Head g_ψ → L_pref and Adversarial Head a_ω → L_adv → [z_c; z_nc] → Reconstruction Decoder d_η → L_rec

- **Critical path:** Input (x, y) → backbone → h → encoder → z_c → reward head → L_pref

- **Design tradeoffs:** d_c = 128, d_nc = 512 (larger z_nc preserves more information but requires stronger adversarial pressure); λ_c_KL = λ_nc_KL = 0.001 (higher values enforce tighter bottlenecks but risk posterior collapse); λ_adv = 0.05 (controls invariance strength vs. training stability); λ_rec = 0.001 (prevents degenerate factorization but adds reconstruction overhead)

- **Failure signatures:** Posterior collapse (KL divergences → 0, latents become uninformative); Reward leakage (adversarial head achieves >60% pairwise accuracy from z_nc); Reconstruction failure (||h - d_η([z_c; z_nc])||² remains high)

- **First 3 experiments:** 1) Ablation sweep: Remove each component and report pairwise accuracy delta on ID/OOD benchmarks per Table 9; 2) Length sensitivity: Plot reward vs. normalized response length; CausalRM should show σ_len ≈ 0.03 vs. baseline σ_len ≈ 0.12–0.22; 3) Sycophancy robustness: Train on hacked Anthropic-HH, evaluate on perturbed test—CausalRM should show <2% accuracy drop vs. >10% for baselines

## Open Questions the Paper Calls Out
- **Multi-turn dialogues with dynamic confounders:** Exploring application in multi-turn dialogues where confounders change dynamically over conversation history (explicitly listed as future work in conclusion)
- **True causal mechanism recovery:** Whether the method guarantees that learned causal factors uniquely recover true causal drivers of human preferences without interventional data (appendix E notes reliance on invariance principle rather than formal guarantee)
- **Process reward modeling compatibility:** Extending CausalRM to process reward modeling scenarios where rewards are assigned to intermediate reasoning steps (explicitly mentioned as future work in conclusion)

## Limitations
- Latent separability assumption may fail when causal and spurious attributes are mechanistically entangled (e.g., length correlates with reasoning quality)
- Adversarial training stability is under-validated with no sensitivity analysis to λ_adv or convergence diagnostics
- Claims about robustness to unspecified spurious attributes exceed empirical validation, which focuses on length and sycophancy bias specifically

## Confidence
- **High confidence:** Architectural decomposition and training objectives are clearly specified and reproducible; pairwise accuracy improvements are directly measurable
- **Medium confidence:** Causal invariance mechanism is sound but empirical evidence for component contributions is limited to ablation at epoch 3
- **Low confidence:** Claims about robustness to unspecified spurious attributes exceed validation focused on length and sycophancy

## Next Checks
1. **Latent disentanglement quantification:** Measure mutual information I(z_c; reward) and I(z_nc; reward) to verify adversarial component successfully reduces spurious correlations while maintaining causal signal sufficiency
2. **Adversarial stability analysis:** Run sensitivity analysis across λ_adv ∈ [0.01, 0.1] and report training curves for L_adv, L_pref, and reconstruction loss to identify optimal operating points
3. **Arbitrary bias generalization test:** Introduce novel spurious attribute (e.g., response formatting style) during training and measure whether CausalRM maintains accuracy while baseline models degrade