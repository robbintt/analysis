---
ver: rpa2
title: Efficient Continual Learning in Keyword Spotting using Binary Neural Networks
arxiv_id: '2505.02469'
source_url: https://arxiv.org/abs/2505.02469
tags:
- classes
- algorithms
- training
- accuracy
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how to efficiently add new keywords to small-footprint
  KWS models deployed on resource-constrained devices like MCUs. The authors combine
  Binary Neural Networks with Continual Learning algorithms, aiming to reduce both
  computation and memory while retaining performance on previously learned keywords.
---

# Efficient Continual Learning in Keyword Spotting using Binary Neural Networks

## Quick Facts
- arXiv ID: 2505.02469
- Source URL: https://arxiv.org/abs/2505.02469
- Reference count: 29
- This paper explores how to efficiently add new keywords to small-footprint KWS models deployed on resource-constrained devices like MCUs.

## Executive Summary
This paper investigates continual learning for keyword spotting on Binary Neural Networks (BNNs) deployed on tinyML devices. The authors evaluate seven CL algorithms on a 16-class KWS model using Google Speech Commands dataset, testing scenarios where 1-4 new keywords are added to an initial 12-class model. The study demonstrates that CL on BNNs is feasible with minimal computational overhead while maintaining accuracy above 82% even under worst-case conditions.

## Method Summary
The approach combines BNN compression with exemplar-free continual learning. A pre-trained BNN feature extractor (with frozen convolutional layers) is augmented with a trainable final classification layer. During CL, only the last layer is updated using seven different algorithms: TinyOL, TinyOL with batches, TinyOL v2, TinyOL v2 with batches, LwF, LwF with batches, and CWR. The model uses log-mel spectrograms as input and employs binary weights/activations in middle layers while keeping first and last layers in full precision.

## Key Results
- All CL algorithms maintain accuracy above 88% on the original 12 classes when adding new keywords
- TinyOL and LwF variants achieve highest accuracy (>95%) when adding a single new keyword
- Batch-based methods become more robust as class count increases but require more training samples
- Computational overhead for weight updates is negligible compared to the 291 MFLOP forward pass

## Why This Works (Mechanism)

### Mechanism 1: Last-Layer-Only Continual Learning
Freezing the BNN feature extractor and updating only the final fully-connected layer enables resource-efficient adaptation to new keywords while preserving prior knowledge. The pre-trained BNN backbone serves as a fixed feature extractor. During continual learning, backpropagation updates only weights and biases in the last classification layer using categorical cross-entropy loss. This prevents catastrophic forgetting by isolating parameter changes to the output space.

### Mechanism 2: Binary Neural Network Compression
Binarizing weights and activations to single-bit values dramatically reduces memory and computation while incurring only ~2% accuracy loss. First and last convolutional layers retain full precision; intermediate layers constrain weights and activations to ±1. This replaces multiply-accumulate operations with XNOR-popcount, reducing energy by 71× on RISC-V MCUs.

### Mechanism 3: Sample-Based vs. Batch-Based Update Trade-offs
Sample-based algorithms (e.g., TinyOL) achieve higher accuracy with few new classes but degrade faster as class count increases; batch-based algorithms are more stable across class counts but require significantly more training data. Sample-based methods update parameters after each inference; batch methods accumulate gradients over 32 samples. Batch methods smooth gradient noise but need more data to converge.

## Foundational Learning

- **Catastrophic Forgetting**: The core problem CL solves; neural networks overwrite previously learned weights when trained on new data. *Why needed*: Without CL techniques, accuracy on class A would drop when training on class B. *Quick check*: If you train a model on class A, then train on class B, what happens to accuracy on class A without CL techniques?

- **Binary Neural Networks (BNNs)**: Understanding binarization is essential for grasping the efficiency gains and accuracy trade-offs. *Why needed*: The efficiency gains and accuracy trade-offs depend on understanding how binarization works. *Quick check*: How does constraining weights to ±1 change the computational operations from standard floating-point multiplication?

- **Class-Incremental vs. Domain-Incremental Learning**: This paper uses "new instances and classes" scenario; distinguishing CL settings helps select appropriate algorithms. *Why needed*: The paper adds entirely new class labels rather than just new samples of known classes. *Quick check*: What's the difference between adding new samples of known classes vs. adding entirely new class labels?

## Architecture Onboarding

- **Component map**: Audio → Log-mel spectrogram (25ms windows, 10ms hop, 64 Mel filters, 50–7500 Hz) → BNN feature extractor (frozen) → Classifier (trainable FC layer) → 7 CL algorithms

- **Critical path**: 1) Pre-train BNN on 12 classes using 40% of dataset (~91% accuracy) 2) Freeze all layers except final FC layer 3) Initialize CL algorithm with learning rate 0.05, batch size 32 4) Stream CL data (new instances + new classes) and update last layer via backpropagation 5) Evaluate on held-out test set (3% of data)

- **Design tradeoffs**: TinyOL/LwF best for 1–2 new classes; TinyOL v2/CWR better for 3–4 new classes; batch methods need ≥2048 samples; sample-based methods work with ~512; LwF variants require ~55% more FLOPs than others

- **Failure signatures**: Accuracy drops below 85% on initial classes (hyperparameter issue, reduce learning rate or switch to CWR); CWR underperforms with limited data (expected behavior, switch to TinyOL or TinyOL v2); LwF with batches degrades initial class accuracy (copy layer may need more frequent synchronization)

- **First 3 experiments**: 1) Baseline validation: Pre-train BNN on 12 classes, confirm ≥90% accuracy on held-out test set before any CL 2) Single-class addition: Add one numeric keyword using all 7 CL algorithms; compare accuracy retention on initial 12 classes 3) Data sensitivity sweep: Fix to 4 new classes, vary training samples from 64 to 16,384; plot convergence for batch vs. sample-based algorithms

## Open Questions the Paper Calls Out

1. What is the minimum dataset size required for effective continual learning in BNN-based KWS systems? The authors state in the conclusion: "A more detailed investigation on the minimum size of the CL dataset may provide additional knowledge on effective training with less data."

2. How would the proposed continual learning approach perform when deployed on resource-constrained MCUs in real-world scenarios? The conclusion states: "Moreover, deployment of the proposed approach on resource-constrained MCUs should be evaluated."

## Limitations

- Exact BNN architecture details (layer counts, filter sizes, channels) are referenced but not fully specified in the paper
- CL algorithm implementations (loss formulations, copy layer update mechanics for LwF batches, consolidation weight scheme for CWR) are not detailed
- Number of training epochs/samples per CL run is not specified (only sample counts given)

## Confidence

- BNN efficiency claims (71× energy reduction, 2% accuracy loss): **High** - well-established from cited Cerutti et al. work
- CL algorithm performance rankings (TinyOL/LwF for 1-2 classes, batch methods for 4 classes): **Medium** - supported by experiments but dependent on dataset distribution
- Negligible computational overhead claim: **High** - FLOPs analysis is straightforward and data is provided

## Next Checks

1. **Architecture verification**: Implement the exact BNN architecture from [9] and confirm pre-training achieves ~91% accuracy before CL experiments
2. **Sample count threshold**: Systematically test batch-based methods below 2048 samples to confirm the documented failure point
3. **Generalization stress test**: Evaluate the best-performing algorithms (TinyOL, LwF) on a second keyword spotting dataset with different acoustic characteristics to validate frozen feature extractor assumptions