---
ver: rpa2
title: Effects of Robot Competency and Motion Legibility on Human Correction Feedback
arxiv_id: '2501.03515'
source_url: https://arxiv.org/abs/2501.03515
tags:
- robot
- learning
- people
- human
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how robot competency and motion legibility
  affect human correction feedback during learning from corrections (LfC) scenarios.
  A user study with 60 participants supervised a robot performing pick-and-place tasks
  under different competency (25% vs 75% success rate) and legibility conditions (predictable,
  legible, illegible motions).
---

# Effects of Robot Competency and Motion Legibility on Human Correction Feedback

## Quick Facts
- arXiv ID: 2501.03515
- Source URL: https://arxiv.org/abs/2501.03515
- Reference count: 40
- Primary result: Human sensitivity to robot errors depends on both robot competency and motion legibility, with high-competency robots receiving earlier corrections but also more unnecessary ones

## Executive Summary
This paper investigates how robot competency and motion legibility affect human correction feedback during Learning from Corrections (LfC) scenarios. A user study with 60 participants supervised a robot performing pick-and-place tasks under different competency (25% vs 75% success rate) and legibility conditions (predictable, legible, illegible motions). The study examined three assumptions about human corrections: that people correct only for significant task divergence, can accurately predict corrections, and trade off precision for effort. Results showed people were more sensitive to errors by highly competent robots, especially with legible and predictable motions, and were more likely to miss necessary corrections with incompetent robots. Physical effort positively correlated with correction precision, though this relationship was weaker for incompetent robots with legible motions.

## Method Summary
The study used a 2x3 between-subjects design with 60 participants (10 per condition) supervising a 7-DoF Kinova Gen3 robot arm performing pick-and-place tasks. Competency was manipulated at 25% vs 75% intended success rates, while motion types included predictable, legible, and illegible trajectories. Participants guided the robot using admittance control when corrections were needed. The experiment collected timing metrics (Task Objective Divergence via KLD, Time Until Correction, Trajectory Untraveled), accuracy metrics (Missed/Incorrect Correction Rates), and trade-off metrics (Precision vs Physical Effort). Data was logged at 10Hz during 64 trials per participant.

## Key Results
- People are more sensitive to suboptimal behavior by highly competent robots, correcting earlier for smaller task divergences
- Participants were more likely to miss necessary corrections with incompetent robots across all motion types
- Physical effort showed positive correlation with correction precision, but this relationship weakened significantly for incompetent robots with legible motions

## Why This Works (Mechanism)

### Mechanism 1: Expectation-Based Sensitivity Calibration
When a robot demonstrates high competency (75% success), supervisors develop strong expectations of success. Any deviation creates a salient prediction error, triggering immediate intervention. Conversely, low competency (25%) lowers expectations, causing users to tolerate larger divergences before intervening.

### Mechanism 2: Legibility-Enhanced Prediction Accuracy
Legible motions explicitly convey intent by exaggerating movement toward goals, allowing faster human inference of the robot's objective. This primarily accelerates intervention for high-performing robots where errors are unexpected, but doesn't rescue missed corrections for low-performing agents.

### Mechanism 3: The Precision-Effort Decoupling
Humans typically apply physical effort proportional to precision required. However, when supervising incompetent robots with legible motions, users may exert effort to "fix" the agent's general behavior rather than finely tune trajectories, decoupling the precision-effort correlation.

## Foundational Learning

- **Learning from Corrections (LfC)**: Unlike Learning from Demonstrations, LfC assumes the robot initiates motion and humans only intervene to correct errors. Understanding this is vital because the "no intervention" signal is implicit data. Quick check: If a human doesn't touch the robot during a trajectory, does LfC interpret that as (A) "The robot is wrong, but I'm lazy," or (B) "The robot is correct"?

- **Motion Legibility vs. Predictability**: Predictable motion is the most efficient path to a known goal; legible motion is the path that best conveys the goal to an observer who doesn't know it. Confusing these leads to misinterpreting results. Quick check: Which trajectory type typically involves "exaggerating" the movement toward the goal to help an observer infer intent faster?

- **Kullback-Leibler Divergence (KLD)**: Used to quantify "Task Objective Divergence" - how different the robot's planned goal distribution is from the correct goal. Low KLD at correction time means the user intervened early upon detecting slight drift. Quick check: If a user intervenes when KLD is low, are they being more or less sensitive to the robot's error than if they intervene when KLD is high?

## Architecture Onboarding

- **Component map**: RRT* Planner -> Legibility Optimizer -> Trajectory Execution -> User Intervention Detection -> Admittance Control -> Correction Logging
- **Critical path**: Set Context (define competency) -> Generate Motion (RRT* + legibility optimization) -> Execute & Monitor (robot moves, detect force) -> Feedback Loop (if force detected, switch to admittance control, record correction)
- **Design tradeoffs**: Legibility increases user awareness but creates longer, less efficient paths. High competency generates better data if users pay attention but risks unnecessary corrections. Low competency risks missed corrections as users tune out.
- **Failure signatures**: "Helicopter" Supervisor (constant intervention on high competency), "Neglect" Mode (ignoring incompetent robot), Effort-Precision Decoupling (sloppy corrections for incompetent+legible)
- **First 3 experiments**: 1) Calibration Run: 10 trials without feedback to establish baseline correction timing, 2) Noise Injection: Inject 25% failure rate to observe threshold changes, 3) Legibility A/B Test: Compare "Predictable" vs "Legible" paths for same goal to validate intent inference improvement

## Open Questions the Paper Calls Out
- Does the tendency to correct high-competency robots early stem from violated performance expectations rather than a lack of trust?
- Can a robot intentionally introducing low-stakes mistakes reduce the rate of unnecessary corrections during long-term supervision?
- Does down-weighting pre-correction trajectories from low-competency robots improve the accuracy of Learning from Corrections (LfC) algorithms?

## Limitations
- The artificial manipulation of success rates (25% vs 75%) may not fully capture real-world trust dynamics
- The study's controlled laboratory setting limits generalizability to safety-critical or real-world supervisory scenarios
- Specific implementation details for illegible motion generation and Task Objective Divergence computation remain partially unspecified

## Confidence
- **High Confidence**: Different correction patterns across competency levels (more sensitive to competent robots, more missed errors with incompetent ones)
- **Medium Confidence**: Interaction between motion legibility and competency affecting correction timing, and weakening of precision-effort correlation for incompetent robots with legible motions
- **Low Confidence**: Generalizability to safety-critical applications or real-world continuous learning scenarios

## Next Checks
1. Test whether humans naturally calibrate correction sensitivity based on observed robot performance in long-term interactions, rather than artificial success rate manipulation
2. Determine if the "benefit of the doubt" effect for incompetent robots disappears when tasks involve safety hazards or expensive equipment
3. Compare correction patterns between domain experts and naive users to validate whether precision-effort decoupling holds across experience levels