---
ver: rpa2
title: Text-Enhanced Panoptic Symbol Spotting in CAD Drawings
arxiv_id: '2510.11091'
source_url: https://arxiv.org/abs/2510.11091
tags:
- primitives
- symbol
- drawings
- spotting
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of panoptic symbol spotting in CAD
  drawings by incorporating textual annotations into the recognition pipeline. The
  proposed method constructs a unified graph representation that jointly models geometric
  primitives and textual elements, and uses a Transformer-based backbone enhanced
  with a type-aware attention mechanism to explicitly capture spatial dependencies
  across different primitive types.
---

# Text-Enhanced Panoptic Symbol Spotting in CAD Drawings

## Quick Facts
- arXiv ID: 2510.11091
- Source URL: https://arxiv.org/abs/2510.11091
- Reference count: 33
- Primary result: Panoptic quality (PQ) of 0.7371 on FloorPlanCAD dataset

## Executive Summary
This paper addresses panoptic symbol spotting in CAD drawings by incorporating textual annotations into the recognition pipeline. The proposed method constructs a unified graph representation that jointly models geometric primitives and textual elements, and uses a Transformer-based backbone enhanced with a type-aware attention mechanism to explicitly capture spatial dependencies across different primitive types. Experimental results on the FloorPlanCAD dataset show that the method achieves state-of-the-art performance with a panoptic quality (PQ) of 0.7371, compared to 0.7152 for prior approaches, and demonstrates robustness in complex CAD scenarios. The inclusion of textual annotations significantly improves symbol spotting accuracy, particularly for classes where semantic context is critical.

## Method Summary
The method decomposes CAD drawings into five primitive types (line, arc, circle, ellipse, text) and constructs a unified graph where each primitive is a node initialized with CNN-extracted visual features. The framework integrates textual annotations as distinct primitive nodes, filtering low-frequency text via a corpus-based threshold. A Transformer backbone with type-aware attention mechanism captures spatial relationships between different primitive types, using edge features that encode both primitive type indicators and geometric relations. The model jointly optimizes classification and instance clustering losses, achieving improved panoptic symbol spotting performance.

## Key Results
- Achieves panoptic quality (PQ) of 0.7371 on FloorPlanCAD dataset
- Outperforms prior state-of-the-art (CADTransformer) by 0.0219 PQ points
- Text integration improves PQ from 0.7152 to 0.7352, with further gains to 0.7371 using type-aware attention

## Why This Works (Mechanism)

### Mechanism 1
Incorporating textual annotations as a distinct primitive type improves symbol spotting accuracy for classes where semantic context is critical. Text primitives are filtered via a frequency threshold to remove noise (low-frequency annotations), then embedded using the same CNN-based feature extraction pipeline as geometric primitives. This allows the model to leverage semantic cues (e.g., dimension labels, symbol names) that complement geometric structure. The core assumption is that text annotations in CAD drawings are correlated with nearby geometric primitives and provide discriminative signal for classification. This mechanism may break down if text annotations are sparse, inconsistent across drawings, or weakly correlated with symbol classes.

### Mechanism 2
Type-aware attention captures implicit spatial dependencies between different primitive types, improving representation discriminability. Edge features encode (1) a type indicator (graphic-graphic, graphic-text, text-text) and (2) a 7D geometric relation vector (distance, position, angle). These are passed through an MLP to produce a structural embedding Ts, which is added as a bias term to the standard attention scores before softmax. The core assumption is that different primitive-type pairs have distinct spatial relationship patterns that are informative for symbol recognition. This mechanism may not provide discriminative benefit if spatial relationships are highly variable or uniform across types.

### Mechanism 3
Unified graph representation with joint modeling of geometric and textual primitives enables comprehensive scene understanding. CAD drawings are decomposed into primitives, each represented as a graph node initialized with CNN-extracted visual features. The Transformer backbone updates node features via multi-head attention, jointly optimizing classification (L_sem) and instance clustering (L_ins) losses. The core assumption is that primitives belonging to the same symbol share spatial proximity and feature similarity, enabling clustering. This mechanism may degrade performance if primitive decomposition is noisy or over-segmented, introducing spurious edges that disrupt clustering.

## Foundational Learning

- **Vision Transformer (ViT) for structured inputs**: The backbone is a standard ViT extended with type-aware attention. Understanding self-attention, multi-head attention, and residual connections is prerequisite. Quick check: Can you explain how attention scores are computed from query, key, and value vectors in a Transformer layer?

- **Graph neural network basics (node/edge representations)**: Primitives are nodes; edge features encode spatial relations. The model uses k-nearest neighbors to construct sparse graphs. Quick check: Given a set of nodes with features, how would you construct edge features based on spatial relationships?

- **Panoptic segmentation metrics (PQ, RQ, SQ)**: The task unifies instance detection (things) and semantic labeling (stuff). PQ = RQ × SQ, where RQ is recognition quality (F1-like) and SQ is segmentation quality (IoU-based). Quick check: For a predicted symbol matching a ground-truth symbol with IoU = 0.6, what is its contribution to SQ?

## Architecture Onboarding

- **Component map**: Input preprocessing (CAD → primitive decomposition + rasterization) -> Feature initialization (CNN extracts feature map) -> Graph construction (k-NN with edge features) -> Backbone (Transformer with type-aware attention) -> Heads (classification + clustering).

- **Critical path**: Primitive decomposition → feature initialization → graph construction → type-aware attention → classification/clustering heads. Errors in decomposition or feature sampling propagate directly.

- **Design tradeoffs**: k=16 neighbors balances computational cost and receptive field; larger k increases memory quadratically. Frequency-based text filtering removes noise but may discard rare but informative annotations. HRNetV2-W48 is high-capacity but computationally heavy; smaller backbones may trade accuracy for speed.

- **Failure signatures**: Low PQ with high SQ indicates recognition failures (misclassification); check L_sem weighting and class imbalance. Low SQ with high RQ indicates segmentation boundary errors; check instance offset loss (L_ins) and clustering head. Performance drop on specific classes (e.g., bay window) may be due to inconsistent text annotations or geometric complexity.

- **First 3 experiments**: 1) Ablation on text integration: Train with and without text primitives; measure ΔPQ per class to identify which classes benefit most. 2) Ablation on type-aware attention: Compare standard attention vs. type-aware attention; isolate contribution of edge-type bias. 3) Sensitivity to k and text frequency threshold: Vary k ∈ {8, 16, 32} and frequency threshold; plot PQ vs. hyperparameters to identify stable operating range.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can explicit semantic text encoders (e.g., BERT, OCR-specific models) improve performance over the current visual feature extraction for textual primitives? The methodology extracts features for text primitives solely via a pretrained CNN on rasterized images, potentially losing the semantic meaning of the character strings. Comparative experiments substituting CNN visual features for text primitives with linguistic embeddings while keeping the type-aware attention mechanism fixed would resolve this.

- **Open Question 2**: How can the framework be adapted to prevent performance degradation in symbol classes where text annotations are inconsistent or noisy? The authors note that "textual annotations are highly diverse and sometimes inconsistent," which caused a performance decline in classes like "bay window" due to noise sensitivity. A dynamic filtering mechanism or attention masking that improves PQ specifically for "bay window" and "opening symbol" classes would resolve this.

- **Open Question 3**: Is the a priori frequency threshold used for text filtering robust across diverse CAD domains (e.g., mechanical vs. architectural) without manual retuning? The paper states the threshold for eliminating low-frequency annotations is "determined a priori based on corpus statistics," suggesting potential rigidity. Evaluation of the model on a distinct CAD dataset (e.g., mechanical drawings) showing stable performance without recalibrating the frequency threshold would resolve this.

## Limitations

- Text filtering threshold is determined "a priori based on corpus statistics" but exact values are not disclosed, limiting reproducibility of text integration gains.
- Edge feature encoding details (7D geometric relations) are unspecified, which may affect type-aware attention performance.
- Transformer architecture specifics (depth, hidden dimension, MLP layer sizes) are omitted, preventing exact reproduction of the 0.7371 PQ result.

## Confidence

- High confidence: PQ improvement (0.7371) over baseline (0.7152) is well-supported by ablation studies and results in Table I.
- Medium confidence: Type-aware attention contribution (PQ boost from 0.7352 to 0.7371) is supported by section IV-B but lacks external validation.
- Medium confidence: Text integration benefit is supported by reported PQ gains but depends on undisclosed text filtering parameters.

## Next Checks

1. **Text ablation study**: Train without text primitives and measure per-class PQ changes to validate which categories benefit from semantic context.
2. **Attention mechanism isolation**: Compare standard vs. type-aware attention while keeping other components constant to isolate the edge-type bias contribution.
3. **Hyperparameter sensitivity analysis**: Vary k (8, 16, 32) and text frequency threshold to identify stable operating ranges and potential overfitting to default values.