---
ver: rpa2
title: Injecting Imbalance Sensitivity for Multi-Task Learning
arxiv_id: '2503.08006'
source_url: https://arxiv.org/abs/2503.08006
tags:
- imbalance
- learning
- issue
- pareto
- cagrad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that imbalance among task gradients, rather than
  conflicts, is the primary challenge in multi-task learning (MTL). The authors propose
  IMGrad, a method that injects imbalance sensitivity by adaptively constraining projected
  norms to achieve a dynamic equilibrium between Pareto optimality and convergence.
---

# Injecting Imbalance Sensitivity for Multi-Task Learning

## Quick Facts
- **arXiv ID**: 2503.08006
- **Source URL**: https://arxiv.org/abs/2503.08006
- **Reference count**: 8
- **Primary result**: IMGrad improves MTL performance by adaptively injecting imbalance sensitivity, outperforming state-of-the-art gradient manipulation methods on supervised and RL benchmarks.

## Executive Summary
This paper argues that gradient imbalance, rather than conflicts, is the primary challenge in multi-task learning (MTL). The authors propose IMGrad, a method that adaptively constrains projected norms to achieve a dynamic equilibrium between Pareto optimality and convergence. By incorporating a cosine-similarity-based adaptive weight, IMGrad enhances the CAGrad baseline to handle imbalance-sensitive scenarios. Experiments on NYUv2, CityScapes, CelebA, and MT10 benchmarks show consistent improvements over state-of-the-art methods.

## Method Summary
IMGrad extends CAGrad by injecting imbalance sensitivity through adaptive weighting. The method computes per-task gradients, estimates imbalance via cosine similarity between mean gradient and MGDA gradient, and solves a dual-objective optimization problem using cvxpy. The combined gradient is constructed by interpolating between CAGrad and MGDA directions based on the adaptive weight. This approach dynamically balances conflict avoidance with convergence while addressing gradient magnitude disparities across tasks.

## Key Results
- Achieves best performance on multi-task objective metrics and individual task progress across supervised and RL benchmarks
- Consistently improves upon state-of-the-art gradient manipulation methods including PCGrad, CAGrad, and Nash-MTL
- Demonstrates superior handling of gradient imbalance through adaptive weighting mechanism

## Why This Works (Mechanism)
IMGrad works by recognizing that gradient imbalance (magnitude disparities) rather than conflicts (directional disagreements) is the primary MTL challenge. The method adaptively weights between CAGrad's ball constraint and MGDA's Pareto direction based on cosine similarity between gradients. When imbalance is high, it shifts toward MGDA to address magnitude disparities; when low, it maintains CAGrad's convergence properties. This dynamic equilibrium allows IMGrad to achieve both Pareto optimality and stable convergence.

## Foundational Learning
- **Concept: Pareto Optimality and Pareto Stationary in Multi-Objective Optimization**
  - Why needed here: IMGrad explicitly optimizes for Pareto properties while avoiding Pareto failures. Understanding that Pareto optimal means no task can improve without harming another is essential to grasp why the method balances conflict-avoidance with convergence.
  - Quick check question: Given two loss functions L₁ and L₂, if point A has L₁=0.5, L₂=0.3 and point B has L₁=0.4, L₂=0.3, does A Pareto-dominate B, does B Pareto-dominate A, or neither?

- **Concept: Gradient Conflict vs. Gradient Imbalance (with Norm Ratio)**
  - Why needed here: The paper's central claim is that imbalance (ratio r = ||g_max||/||g_min||) matters more than conflict (cos φ < 0). Distinguishing these is critical: conflict is about direction, imbalance is about magnitude disparities.
  - Quick check question: If g₁ = [10, 0] and g₂ = [0, 1], are they conflicting? What is the imbalance ratio r?

- **Concept: Constrained Optimization with Lagrangian Dual Formulation**
  - Why needed here: IMGrad derives from adding a projected norm constraint to CAGrad's objective, then solving via Lagrangian duality. Following the derivation requires comfort with swapping min/max under strong duality.
  - Quick check question: For a problem min_x max_y f(x,y) subject to g(x) ≤ 0, if strong duality holds, can you swap to max_y min_x f(x,y) + λg(x)? What role does λ play?

## Architecture Onboarding
- **Component map**: Per-task gradient computation -> Imbalance estimator (cos θ) -> Dual-objective optimizer (cvxpy) -> Combined gradient construction -> Backward pass
- **Critical path**: Forward pass through shared backbone → task-specific heads → per-task losses → backward to get gradient matrix → compute g₀ and g_m → calculate cos θ → solve optimization → construct combined gradient → update shared parameters
- **Design tradeoffs**: CAGrad's c parameter controls ball constraint radius; adaptive μ = cos θ vs. fixed μ introduces coupling between imbalance and objective weighting; IMGrad interpolates between CAGrad (μ→0) and MGDA (μ→1)
- **Failure signatures**: Pareto failure (combined gradient has negative cosine similarity with task gradients), dominance/hijacking (one task's gradient norm vastly exceeds others), cvxpy infeasibility, stagnant ∆m% improvement
- **First 3 experiments**:
  1. **Toy 2D reproduction**: Replicate Figure 2 on synthetic imbalanced objectives (0.9·L₁, 0.1·L₂). Verify IMGrad reaches global optimum from multiple initial points while CAGrad/PCGrad/Nash-MTL fail or stall at Pareto front.
  2. **CityScapes imbalance ratio monitoring**: Train MTAN with LS, CAGrad, and IMGrad. Log per-step imbalance ratio r = ||g_max||/||g_min|| and cos θ. Confirm IMGrad maintains lower variance in individual progress and achieves lower ∆m%.
  3. **Pareto failure rate analysis**: On NYUv2 (3 tasks), count steps where cos φ_ωi < 0 for any task i. Compare failure rates across methods and correlate with ∆m% to validate that reducing Pareto failures improves multi-task performance.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does performance change if alternative metrics for gradient imbalance are used instead of cosine similarity? The authors state multiple alternatives exist but exclusively select cos θ without comparative ablations.
- **Open Question 2**: Why does IMGrad fail to surpass FAMO on the large-scale CelebA (40-task) benchmark despite showing gains on smaller datasets? The paper does not analyze the interaction between adaptive weight calculation and high-dimensional task spaces.
- **Open Question 3**: Can the IMGrad optimization strategy be successfully combined with architecture-based MTL methods? The introduction explicitly excludes architecture-based approaches, leaving unknown if injecting imbalance sensitivity conflicts with soft parameter sharing methods.

## Limitations
- The claim that imbalance is the primary MTL challenge relies on a specific definition of imbalance that may not generalize to all MTL settings
- Adaptive weighting scheme's robustness across diverse datasets and architectures remains unproven beyond evaluated benchmarks
- Computational overhead from cvxpy optimization could be prohibitive for very large-scale MTL applications

## Confidence
- **High**: Empirical performance improvements over baselines (∆m% reductions, individual task metrics) on evaluated datasets (NYUv2, CityScapes, CelebA, MT10)
- **Medium**: Theoretical framing of imbalance as primary challenge; mathematical derivation of IMGrad from CAGrad via Lagrangian duality
- **Low**: Generalization to unseen architectures (e.g., Transformers) and tasks (e.g., NLP, audio); long-term stability beyond 200 epochs

## Next Checks
1. **Ablation on adaptive vs. fixed μ**: Train IMGrad with cosθ replaced by fixed μ ∈ {0.2, 0.5, 0.8} on CityScapes. Compare ∆m% and individual task progress variance to isolate impact of adaptive weighting.
2. **Pareto failure rate correlation**: On NYUv2, log cos φ_ωi < 0 occurrences per step across all methods. Correlate failure rate with ∆m% to quantify Pareto failure's contribution to MTL degradation.
3. **Scalability test**: Apply IMGrad to a 10-task synthetic MTL problem with varying imbalance ratios (r = 1 to 100). Measure computational overhead (seconds per step) and performance stability as r increases.