---
ver: rpa2
title: 'Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy
  Prefixes'
arxiv_id: '2601.18795'
source_url: https://arxiv.org/abs/2601.18795
tags:
- off-policy
- prefixrl
- problems
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PrefixRL addresses the challenge of training reinforcement learning\
  \ (RL) policies on hard problems where correct on-policy traces are rare, causing\
  \ learning to stall. The core method idea is to condition the policy on off-policy\
  \ prefixes\u2014partial traces from previously successful attempts\u2014rather than\
  \ directly supervising on them."
---

# Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes

## Quick Facts
- arXiv ID: 2601.18795
- Source URL: https://arxiv.org/abs/2601.18795
- Reference count: 40
- Primary result: PrefixRL reaches the same training reward 2× faster than SFT+RL baseline and increases final reward by 3× on hard reasoning tasks.

## Executive Summary
PrefixRL addresses the challenge of training reinforcement learning (RL) policies on hard problems where correct on-policy traces are rare, causing learning to stall. The core method idea is to condition the policy on off-policy prefixes—partial traces from previously successful attempts—rather than directly supervising on them. This allows the RL policy to start from higher-rewarding states and continue training on-policy, sidestepping the instability of off-policy RL. The PrefixRL objective is theoretically consistent with and more sample-efficient than standard RL, and empirically, the model exhibits back-generalization: training on prefixed problems improves performance on original unprefixed problems. In experiments on hard reasoning tasks, PrefixRL reaches the same training reward 2× faster than the strongest baseline (SFT on off-policy data then RL) and increases final reward by 3×, with gains transferring to held-out benchmarks. PrefixRL remains effective even when off-policy traces come from a different model family, validating its flexibility in practical settings.

## Method Summary
PrefixRL is an RL algorithm that leverages off-policy data through conditioning rather than direct supervision. The method generates one correct off-policy trace per hard problem via rejection sampling, then creates multiple prefixed problems by taking random prefixes (40-80% of trace length) from each correct trace. During training, PrefixRL runs on-policy RL on a mixture of original and prefixed problems, but crucially masks gradients on the off-policy prefix tokens so updates occur only on on-policy completions. This design avoids the instability of off-policy RL while benefiting from the higher-rewarding starting states provided by prefixes. The algorithm uses REINFORCE with batch size 128, learning rate 1e-6, and gradient clipping 1.0, running 8 rollouts per prompt at temperature 1.0.

## Key Results
- PrefixRL reaches the same training reward 2× faster than SFT+RL baseline on hard reasoning problems
- PrefixRL increases final reward by 3× compared to standard RL baselines
- Training on prefixed problems transfers learning to improve performance on original unprefixed problems (back-generalization)
- PrefixRL maintains effectiveness even when off-policy traces come from different model families

## Why This Works (Mechanism)

### Mechanism 1
Off-policy prefixes place the policy in higher-rewarding states, reducing gradient variance and increasing learning signal. Correct off-policy prefixes move the starting state closer to successful outcomes, reducing exploration burden. With prefixes, the model encounters non-zero advantages more frequently, yielding more informative policy gradients rather than the all-zero gradients typical of hard problems. The prefix reveals strategy-relevant states that the current policy is unlikely to reach on its own (pass@k ≈ 0). Evidence shows PrefixRL boosts learning signal on hard problems by modulating difficulty through off-policy prefix length, and that prefixes extracted from correct off-policy traces place the current RL policy in states more likely to lead to correct answers. This mechanism fails if prefixes are uninformative or place the policy in states orthogonal to the solution manifold.

### Mechanism 2
Back-generalization transfers learning from prefixed problems to no-prefix problems via shared internal representations. Training on prefixed problems updates model weights that affect next-token distributions on unseen no-prefix states. The coupling between prefixed and no-prefix responses suggests representations learned for completing prefixes generalize backward to earlier states. This relies on function approximation in LLMs creating shared representations across context windows, not purely tabular state-to-action mappings. Evidence shows training on longer prefixes improves performance on shorter prefixes and eventually improves no-prefix performance, with tight coupling between strategy use on prefixed and no-prefix responses. The mechanism may fail if prefixed and no-prefix problems activate disjoint representations or when cross-family prefixes show weaker transfer due to mismatched representations.

### Mechanism 3
PrefixRL avoids off-policy instabilities by masking gradients on the off-policy prefix and only training on on-policy completions. Unlike SFT+RL (entropy collapse) or importance-weighted off-policy RL (high-variance gradients), PrefixRL uses off-policy data only as conditioning context. Gradients are computed only on tokens sampled from the current policy, avoiding distribution shift penalties. This assumes the off-policy prefix is sufficiently likely under the base model that conditioning does not induce pathological behavior. Evidence shows PrefixRL never trains the RL policy on off-policy data and runs on-policy RL conditioned on prefixed problems with gradients masked on the off-policy prefix. PrefixRL shows higher gradient norm and lower gradient variance than off-policy RL. The mechanism may fail if the prefix is extremely unlikely under the current policy, introducing train/test distribution mismatch.

## Foundational Learning

**On-policy vs. off-policy RL in LLMs**: PrefixRL intentionally keeps updates on-policy while leveraging off-policy data for conditioning. Understanding why importance-weighted off-policy RL is unstable clarifies the design motivation. Quick check: Why does importance weighting cause gradient variance spikes when off-policy traces are very unlikely under the current policy?

**Gradient variance and exploration bottlenecks**: On hard problems, zero-reward rollouts yield zero gradients. PrefixRL reduces the fraction of all-negative batches, improving learning signal. Quick check: What is the expected gradient when all n samples for a problem receive zero reward in REINFORCE?

**State reset and distribution shaping in RL**: PrefixRL resets the policy to off-policy states (prefixes) rather than the initial problem state. This is a form of start-state distribution shaping. Quick check: How does changing the start-state distribution affect the suboptimality bound in natural policy gradient analysis?

## Architecture Onboarding

**Component map**: Off-policy trace collector -> Prefix extractor -> Prefixed problem constructor -> On-policy RL engine -> Back-generalization evaluator

**Critical path**: Collect one correct trace per problem via rejection sampling (upfront cost) -> Extract multiple prefixes per trace to create D_pre -> Run on-policy RL on mixture of prefixed (≈3:1 ratio) and no-prefix problems -> Mask gradients on off-policy prefix tokens during backward pass -> Evaluate on no-prefix problems and held-out benchmarks to measure back-generalization.

**Design tradeoffs**: Prefix length (shorter improves back-generalization but may lack guidance; longer reduces train/test mismatch but may slow transfer), mixture ratio (more prefixed increases efficiency but reduces no-prefix coverage), off-policy source (same-family gives stronger back-generalization; cross-family offers flexibility but may be less effective).

**Failure signatures**: High all-negative ratio on no-prefix problems (prefixes not informative or too long), gradient norm spike (off-policy tokens accidentally included in gradient computation), entropy collapse (SFT warm-start used instead of pure PrefixRL).

**First 3 experiments**: Ablation on prefix length distribution (compare uniform 40-80% vs fixed 80% to measure back-generalization speed), cross-family transfer (source prefixes from Qwen-3-4B, train Llama-3.1-8B, measure accuracy gap vs same-family), compute-matched comparison (run PrefixRL vs SFT+RL vs standard RL for equal FLOPs including rejection sampling cost on hard problems with pass@512 ≈ 0).

## Open Questions the Paper Calls Out

**Open Question 1**: What are the precise theoretical mechanisms underlying back-generalization, and can we formalize when it transfers strategies beyond simple stitching? The authors state "the mechanisms (and full potential) of back-generalization aren't fully understood" and that it "cannot be explained by some of the 'stitching' arguments made in prior work." This remains unresolved as back-generalization is discovered empirically without a theoretical framework explaining why learning on prefixed states alters next-token distributions on unseen no-prefix states.

**Open Question 2**: How should prefix lengths and positions be optimally selected to maximize back-generalization and sample efficiency? The paper notes "prefixes...typically these are states revealing a high-level problem-solving strategy" but uses random cut points between 40-80% without systematic optimization. Prefix selection is heuristic and the relationship between prefix content, position, and transfer effectiveness remains unclear.

**Open Question 3**: When does cross-family prefix transfer succeed or fail, and what makes prefix states "compatible" with a target model's representations? The paper shows asymmetric transfer (Qwen→Llama works better than Llama→Qwen) and notes effectiveness depends on "how informative and 'compatible' those prefix states are with the target model's internal representations." The conditions for successful cross-model prefix reuse are characterized qualitatively but not formally.

## Limitations

- The exact mechanism and strength of back-generalization remains theoretically unclear, with limited corpus evidence beyond the paper's own experiments
- Prefix selection (40-80% random truncation) appears somewhat arbitrary with sensitivity to this hyperparameter not thoroughly explored
- Dependence on rejection sampling introduces upfront computational cost that may be prohibitive for some applications
- Evaluation focuses primarily on mathematical reasoning tasks, leaving generalizability to other domains untested

## Confidence

**High Confidence**: The claim that PrefixRL achieves faster convergence and higher final reward than baselines on hard reasoning problems is well-supported by experimental results with clear quantitative comparisons and statistical significance.

**Medium Confidence**: The mechanism explaining why PrefixRL works (reduced gradient variance through higher-rewarding starting states) is plausible and supported by evidence, but the exact quantitative relationship between prefix informativeness and learning signal strength is not established.

**Low Confidence**: The theoretical explanation for back-generalization remains underdeveloped, with limited corpus evidence supporting this mechanism beyond the paper's own experiments.

## Next Checks

1. **Prefix Length Sensitivity Analysis**: Systematically vary prefix lengths (20-30%, 40-80%, 60-90%) and measure both training efficiency and back-generalization strength to identify optimal ranges and understand the tradeoff between immediate performance gains and transfer capability.

2. **Cross-Domain Transfer Experiment**: Apply PrefixRL to non-mathematical domains (e.g., code generation or natural language reasoning) where the base model has near-zero pass rates, to test whether the back-generalization mechanism generalizes beyond mathematical reasoning tasks.

3. **Computation-Optimal Comparison**: Conduct a head-to-head FLOPs-matched comparison including rejection sampling costs, running PrefixRL, SFT+RL, and standard RL for equal total compute budgets to definitively establish whether the speedup is due to better sample efficiency or simply additional compute.