---
ver: rpa2
title: 'SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning'
arxiv_id: '2511.22367'
source_url: https://arxiv.org/abs/2511.22367
tags:
- replay
- surprise
- learning
- slow
- buffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work formalizes catastrophic forgetting in continual learning
  as the sum of two complementary errors: selection (imperfect replay distribution)
  and integration (variance in how new updates are consolidated). The authors propose
  Surprise-prioritized Replay (SuRe), a simple buffer update rule that retains the
  most surprising (high-NLL) sequences, and combine it with a dual LoRA learner using
  EMA for stable consolidation.'
---

# SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning

## Quick Facts
- arXiv ID: 2511.22367
- Source URL: https://arxiv.org/abs/2511.22367
- Reference count: 40
- Primary result: SuRe achieves state-of-the-art performance in Large Number of Tasks (LNT) continual learning with up to +5 accuracy points over prior SOTA

## Executive Summary
SuRe (Surprise-prioritized Replay) is a continual learning method for LLMs that combines surprise-based buffer selection with a dual LoRA learner using EMA consolidation. The method formalizes catastrophic forgetting as the sum of selection error (imperfect replay distribution) and integration error (variance in consolidation), addressing both through prioritized replay of high-NLL sequences and stable EMA-based parameter updates. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting with up to +5 accuracy points over prior SOTA, while demonstrating sample efficiency and robustness under reduced replay frequency and small buffer sizes.

## Method Summary
SuRe uses a dual LoRA adapter architecture where a fast learner is updated via SGD on mixed batches of current and replay data, while a slow learner tracks the fast weights via EMA (β=0.995). Buffer selection prioritizes sequences by their NLL under the fast model, maintaining equal per-task quotas. Training uses 1 epoch per task, batch size 64, learning rate 1e-3, and a replay ratio of 1:2. The method assumes known task boundaries for balanced buffer allocation and uses EMA to reduce integration variance while surprise ranking reduces selection error.

## Key Results
- SuRe achieves state-of-the-art performance in LNT setting with up to +5 accuracy points over prior SOTA
- Delivers best overall average across both Standard CL and LNT benchmarks
- Robust under reduced replay frequency and small buffer size (300-500 samples capture most gains)
- Buffer timing "before update after" shows +2-3% advantage over alternatives

## Why This Works (Mechanism)

### Mechanism 1: Surprise-Based Buffer Selection Reduces Selection Error
High-NLL sequences have large gradient norms (∥∇ℓ(θ; z)∥), contributing disproportionately to the true gradient E_P[∇ℓ]. Retaining these samples reduces the integral probability metric D_Floc(P, q) between replay distribution q and true past distribution P. This works when high-loss sequences represent task boundary regions where interference is highest. Break condition: If tasks have homogeneous difficulty or high NLL reflects outliers/noise rather than learnable boundaries, selection degrades to noisy prioritization.

### Mechanism 2: EMA Slow Learner Reduces Integration Variance
EMA parameters θ_slow = (1-β)Σβ^k θ_fast form a low-pass filter on parameter trajectories. For β=0.995, effective variance reduction is ~200x versus raw SGD iterates. This stabilizes consolidation when task optima drift is bounded (∥θ*_{k+1} - θ*_k∥ ≤ δ). Break condition: If task optima drift rapidly, EMA lag causes systematic bias that dominates variance reduction benefits.

### Mechanism 3: Complementary Error Decomposition (Selection + Integration)
Forgetting = Selection Error × Integration Error; addressing both is necessary because neither can drive total error to zero alone. Finite memory constrains D_Floc(P,q) > 0; finite steps constrain variance term > 0. Theorem 1 shows additive bound with independent controls. Break condition: If errors are not additive (e.g., selection and integration interact non-linearly), optimizing each independently may be suboptimal.

## Foundational Learning

- **Integral Probability Metrics (IPMs) / Maximum Mean Discrepancy**
  - Why needed here: Core theoretical tool bounding replay distribution mismatch (Lemma 1)
  - Quick check question: Can you explain why MMD measures distribution distance via witness functions in an RKHS?

- **Exponential Moving Average (Polyak-Ruppert Averaging)**
  - Why needed here: Underpins slow learner consolidation and variance reduction analysis
  - Quick check question: Why does EMA reduce variance by factor ~(1-β)⁻¹ while introducing tracking lag?

- **PL (Polyak-Łojasiewicz) Condition**
  - Why needed here: Required for convergence guarantees in non-convex LoRA subspace
  - Quick check question: How does PL condition (½∥∇f∥² ≥ μ(f - f*)) enable optimization bounds without convexity?

## Architecture Onboarding

- **Component map**: Base LLM (frozen) → LoRA adapters (Q, V projections) → Fast LoRA: SGD updates on current + replay batch → Slow LoRA: EMA of fast weights (β=0.995) → Buffer (2% dataset) → Per-task quota → Surprise-ranked selection (NLL under fast model) → Inference: Base + Slow LoRA only (fast discarded)

- **Critical path**: Buffer selection (compute NLL for all sequences before training) → Fast LoRA update (mixed batch) → EMA merge → Buffer update (post-training variant works best per ablation)

- **Design tradeoffs**:
  - Buffer timing: "Before" favors plasticity; "After" favors stability (Table 3 shows +2-3% difference)
  - β value: 0.995 optimal; 0.999 degrades sharply to ~57% accuracy (Table 10)
  - Buffer size: 300-500 samples capture most gains; 150 can underperform random baseline

- **Failure signatures**:
  - Label-level surprise (not sequence-level): 64.9% vs 77.2%—class imbalance dominates
  - Dynamic surprise updates during replay: Degrades vs static ranking (aging mechanism unnecessary)
  - Unknown task boundaries: Selection degrades; requires task-identity assumption

- **First 3 experiments**:
  1. Replicate SuRe vs Reservoir on Standard CL (4 tasks) with buffer size 300, replay ratio 1:2—verify ~+0.5-1% gap
  2. Ablate β ∈ {0.985, 0.99, 0.995, 0.999} on LNT Order-4—confirm collapse at 0.999
  3. Test buffer timing variants (SB-UB, SB-UA, SA-UA) on LNT—verify post-training update advantage under limited replay budget (1:8 ratio)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SuRe be adapted for fully online, task-agnostic continual learning where task boundaries are unknown?
- Basis in paper: [explicit] The Discussion section states that extending to fully online settings "would require additional mechanisms to prevent early-task dominance and remains future work," noting the current reliance on known boundaries for balanced buffer allocation.
- Why unresolved: The current implementation assumes known task boundaries to maintain equal per-task quotas in the buffer ($m_i^{(d)} = \lfloor S/d \rfloor$), which is not possible in boundary-free streams.
- What evidence would resolve it: A variant of SuRe that utilizes distribution shift detection or adaptive buffer rebalancing to maintain performance without task identity labels.

### Open Question 2
- Question: Does the surprise-based selection mechanism generalize effectively to non-NLP modalities, such as vision or vision-language models?
- Basis in paper: [explicit] The authors explicitly propose that "Further evaluation across... new modalities, Vision, Vision-Language Models... would strengthen our findings."
- Why unresolved: The paper validates SuRe primarily on text classification benchmarks (Standard CL and LNT) using T5 and Llama models, leaving its efficacy on high-dimensional visual data unproven.
- What evidence would resolve it: Empirical results applying SuRe to standard CL vision benchmarks (e.g., Split CIFAR-100) or vision-language tasks, showing comparable gains over reservoir sampling.

### Open Question 3
- Question: Can the computational overhead of the extra forward pass required for surprise computation be eliminated or amortized?
- Basis in paper: [inferred] The paper notes in the Discussion that "computing surprise requires an extra forward pass across datasets" and suggests adapting the approach to online settings would address this limitation.
- Why unresolved: While theoretically motivated, the current method requires calculating NLL for buffer selection, adding computational cost compared to purely online methods like Reservoir sampling.
- What evidence would resolve it: An algorithm that estimates surprise "for free" during training or uses a proxy that does not require a separate evaluation pass over the dataset.

### Open Question 4
- Question: What alternative consolidation operators or architectures could replace EMA to further reduce integration error?
- Basis in paper: [explicit] The Discussion mentions that the dual-learner architecture "merits deeper investigation of alternative designs and training objectives."
- Why unresolved: The theoretical analysis (Remark in Appendix H) suggests other operators like SWA or model soups fit the bound, but the experiments rely exclusively on EMA.
- What evidence would resolve it: Comparative studies showing superior performance or stability using alternative consolidation mechanisms within the SuRe framework.

## Limitations

- Theoretical gaps in selection error bound rely on idealized IPM assumptions that may not hold in high-dimensional LoRA subspaces
- 2% buffer assumption limits applicability to resource-constrained scenarios and depends on known task boundaries
- Neuroscientific inspiration unverified—no empirical validation connects neural surprise signals to LLM forgetting patterns
- Computational overhead from extra forward pass for surprise computation adds cost vs purely online methods

## Confidence

**High confidence**:
- Dual-LoRA + EMA architecture works as described
- Surprise-based selection improves over uniform replay when task boundaries are known
- β=0.995 EMA value is optimal for the tested range

**Medium confidence**:
- The complementary error decomposition (selection + integration) holds generally
- Buffer timing "before update after" is consistently optimal
- NLL-based surprise generalizes beyond text classification

**Low confidence**:
- Neuroscientific analogy between neural surprise and LLM NLL is valid
- PL conditions hold in LoRA subspace for these tasks
- Integration variance reduction scales linearly with (1-β)⁻¹

## Next Checks

1. **Test PL condition validity**: Empirically verify Polyak-Łojasiewicz condition holds in the LoRA subspace for each task by measuring gradient geometry (½∥∇f∥² vs f - f*)

2. **Validate forgetting curve predictions**: Compare SuRe's surprise ranking against actual forgetting curves measured by FOREVER-style probes to test if high-NLL truly predicts catastrophic forgetting

3. **Test boundary-free generalization**: Run SuRe without task-identity information using domain clustering or meta-learning to predict task switches, measuring performance degradation from the ideal case