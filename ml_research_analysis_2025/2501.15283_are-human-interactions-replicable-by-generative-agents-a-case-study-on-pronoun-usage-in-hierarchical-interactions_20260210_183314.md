---
ver: rpa2
title: Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun
  Usage in Hierarchical Interactions
arxiv_id: '2501.15283'
source_url: https://arxiv.org/abs/2501.15283
tags:
- prompt
- human
- llms
- pronoun
- pronouns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM agents can replicate human-like
  pronoun usage patterns in hierarchical interactions. The authors simulate task-oriented
  conversations between leader and non-leader agents using various LLM families and
  persona prompts, then compare pronoun usage differences to established human patterns.
---

# Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions

## Quick Facts
- arXiv ID: 2501.15283
- Source URL: https://arxiv.org/abs/2501.15283
- Reference count: 21
- Key outcome: LLM agents consistently fail to exhibit human-like pronoun usage patterns in hierarchical interactions, with most showing statistically significant differences opposite to human behavior.

## Executive Summary
This paper investigates whether LLM agents can replicate human-like pronoun usage patterns in hierarchical interactions. The authors simulate task-oriented conversations between leader and non-leader agents using various LLM families and persona prompts, then compare pronoun usage differences to established human patterns. Results show that LLM agents consistently fail to exhibit human-like patterns, with most showing statistically significant differences opposite to human behavior. Even when LLMs demonstrate understanding of these patterns through prompting, they fail to exhibit them during actual interactions. The study highlights fundamental limitations in using LLM agents for social simulation and cautions against relying on such simulations for modeling human social dynamics.

## Method Summary
The study simulates hierarchical group conversations (1 leader + 3 non-leaders) where agents rank customer service improvement items over three rounds of round-robin discussion. The authors test multiple LLM families (GPT-3.5/4/4o, Llama 3.1, Mistral, QWen) using four different persona prompts. They compare pronoun usage patterns (first-person singular I/me and plural we/us) between leader and non-leader roles against established human baselines from Kacewicz et al. (2014). Two agent types are tested: a simple agent that concatenates persona prompt with conversation history, and a specialized agent with reflection and planning modules. Pronoun frequencies are calculated as percentages of total words, and statistical significance is determined via t-tests.

## Key Results
- All tested LLM models showed statistically significant differences from human pronoun usage patterns, with most exhibiting opposite trends
- GPT-4o failed with one persona prompt (P1) but succeeded with others for first-person singular, showing prompt sensitivity
- The specialized agent with reflection and planning modules performed worse than the simple agent, producing zero human-like patterns
- Larger models within families did not systematically improve human-likeness, suggesting behavior is model-family-specific

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can encode social knowledge without exhibiting it in interaction outputs
- Evidence: Llama 70B scores 6/6 on first-person singular knowledge queries but 0/4 on demonstrating the pattern; GPT-4o scores 5/6 on knowledge but only 3/4 on demonstration

### Mechanism 2
- Claim: Adding reflection and planning modules disrupts rather than improves human-like interaction patterns
- Evidence: Specialized agent produces zero human-like patterns across all prompts, while simple agent produces 3/4 for first-person singular

### Mechanism 3
- Claim: Pronoun patterns are model-family-specific, suggesting training data or architecture—not scale—drive behavioral tendencies
- Evidence: All Llama 3.1 models show opposite trends regardless of prompt; QWen models show varying patterns within the family

## Foundational Learning

- Concept: LIWC pronoun analysis and social hierarchy
  - Why needed: The paper operationalizes "human-like" against Kacewicz et al. (2014), which established specific frequency baselines for non-leaders vs. leaders
  - Quick check: If f_non-leader – f_leader = -0.93 for first-person singular, what does this mean about the model's behavior relative to humans?

- Concept: Persona prompt fragility
  - Why needed: GPT-4o fails with P1 but succeeds with P2-P4, showing non-robustness critical for social simulation deployment
  - Quick check: Why is prompt-sensitivity more concerning for social simulation than for task-oriented prompting?

- Concept: Statistical significance in behavioral simulation
  - Why needed: The paper runs 41 group simulations and uses t-tests to determine if leader/non-leader differences are significant
  - Quick check: A model shows f_non-leader – f_leader = 0.10 for first-person plural with p=0.12. How do you interpret this relative to the human baseline of -0.50?

## Architecture Onboarding

- Component map: Simple Agent: Persona Prompt ⊕ Conversation History → LLM → Response
- Critical path: Assign roles → 3 rounds of round-robin discussion → Extract pronouns → Compute frequencies → T-test leader vs. non-leader distributions
- Design tradeoffs: Round-robin vs. natural conversation; three rounds vs. longer duration; truncation vs. retrieval
- Failure signatures: Opposite pattern (leader uses more I/me), no differentiation, role collapse
- First 3 experiments:
  1. Baseline replication with GPT-4o on 10 groups to verify P1-failure/P2-4-success pattern
  2. Ablation on reflection timing to isolate disruption causes
  3. Cross-prompt consistency test for "knows but doesn't demonstrate" pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do explicit cognitive modules like reflection and planning degrade an LLM agent's ability to replicate human-like pronoun usage?
- Why unresolved: The authors hypothesize that explicit reasoning disrupts "intuitive processing" but do not isolate the specific mechanism
- What evidence would resolve it: Ablation studies removing individual cognitive components combined with attention shift analysis

### Open Question 2
- Question: Does the inability to replicate pronoun usage patterns extend to other unconscious linguistic markers of hierarchy?
- Why unresolved: The paper focuses exclusively on pronouns, which are described as "unconscious" behaviors
- What evidence would resolve it: Comprehensive linguistic analysis of existing transcripts for other hierarchy markers

### Open Question 3
- Question: Can prompt optimization successfully elicit human-like pronoun usage without creating negative trade-offs?
- Why unresolved: The study tests existing prompts but does not attempt targeted optimization
- What evidence would resolve it: Experiments using reinforcement learning to improve pronoun fidelity while measuring trade-offs

## Limitations
- Models can correctly identify pronoun patterns when prompted but fail to apply this knowledge during generation
- Round-robin speaking order may not fully capture natural conversation dynamics that produce pronoun differences
- The study focuses exclusively on pronoun usage without examining other unconscious linguistic markers

## Confidence
- High Confidence: LLM agents systematically fail to replicate human pronoun patterns (most showing opposite trends)
- Medium Confidence: Mechanism explanation (declarative knowledge not automatically activated) is plausible but needs validation
- Medium Confidence: Reflection/planning modules disrupt natural patterns, but additional ablation studies needed

## Next Checks
1. Test whether knowledge representations accessed during prompting are identical to those used during generation
2. Systematically remove individual components (reflection only, planning only) from specialized agent to isolate disruption causes
3. Replicate study with non-English models to determine if failure pattern is language-specific or fundamental