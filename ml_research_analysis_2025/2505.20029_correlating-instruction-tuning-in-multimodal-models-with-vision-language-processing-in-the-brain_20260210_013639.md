---
ver: rpa2
title: Correlating instruction-tuning (in multimodal models) with vision-language
  processing (in the brain)
arxiv_id: '2505.20029'
source_url: https://arxiv.org/abs/2505.20029
tags:
- brain
- visual
- alignment
- image
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether instruction-tuned multimodal large
  language models (MLLMs) can better predict human brain activity when processing
  visual scenes compared to non-instruction-tuned or unimodal models. The authors
  use representations from three instruction-tuned MLLMs (InstructBLIP, mPLUG-Owl,
  and IDEFICS) when prompted with 10 different task-specific instructions (e.g., image
  captioning, visual question answering, color recognition).
---

# Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)

## Quick Facts
- arXiv ID: 2505.20029
- Source URL: https://arxiv.org/abs/2505.20029
- Authors: Subba Reddy Oota; Akshett Jindal; Ishani Mondal; Khushbu Pahwa; Satya Sai Srinath Namburi; Manish Shrivastava; Maneesh Singh; Bapi S. Raju; Manish Gupta
- Reference count: 40
- Key outcome: Instruction-tuned multimodal models (MLLMs) better predict human brain activity than non-instruction-tuned or unimodal models, especially in higher-level visual regions, with task-specific instructions (e.g., image captioning, recognition) showing the strongest alignment.

## Executive Summary
This study investigates how instruction-tuned multimodal large language models (MLLMs) align with human brain activity during visual scene processing, using fMRI data. By comparing representations from instruction-tuned models (InstructBLIP, mPLUG-Owl, IDEFICS) against non-tuned or unimodal baselines, the authors demonstrate that instruction tuning enhances brain-model alignment, particularly in higher-level visual cortex. Not all instructions yield equal alignment—captioning and recognition prompts perform best—and most neural variance is shared across tasks. The findings suggest that instruction tuning enables richer, task-specific visual understanding that mirrors brain function.

## Method Summary
The authors use three instruction-tuned MLLMs—InstructBLIP, mPLUG-Owl, and IDEFICS—and prompt each with 10 task-specific instructions (e.g., image captioning, visual question answering, color recognition). Representations from these prompts are used as input features in voxelwise encoding models to predict fMRI responses from participants viewing natural scenes. Brain alignment is measured by comparing model-predicted and actual voxel responses, and results are contrasted with those from vision-only and non-instruction-tuned multimodal models. Layer-wise and variance-partitioning analyses further dissect where and how well models align with brain activity.

## Key Results
- MLLMs significantly outperform vision-only models and match or exceed multimodal models like CLIP in brain alignment, especially in higher-level visual regions.
- Not all instructions yield equal alignment—image captioning and recognition-related prompts align better with brain activity than others.
- Variance partitioning reveals that most neural variance is shared across tasks, with image captioning overlapping highly with other instructions; middle layers of some MLLMs align best with high-level visual cortex, while others peak in later layers.

## Why This Works (Mechanism)
Assumption: Instruction tuning may enhance alignment by forcing models to extract and represent task-relevant visual features that overlap with how the human brain processes and categorizes visual information during specific cognitive tasks.

## Foundational Learning
- **fMRI voxel encoding**: Measures how well model features predict brain activity at the voxel level. Needed to quantify brain-model alignment; check by verifying encoding model R² values.
- **Instruction tuning**: Fine-tuning models on task-specific instructions to improve multimodal understanding. Needed to enable richer visual-language representations; check by comparing tuned vs. non-tuned model performance.
- **Multimodal representations**: Joint modeling of visual and linguistic inputs. Needed to capture task-relevant features for brain alignment; check by confirming multimodal input handling in models.

## Architecture Onboarding
- **Component map**: Input images and instructions -> MLLM encoder (multiple layers) -> feature extraction -> voxelwise encoding model -> predicted brain activity -> comparison with actual fMRI.
- **Critical path**: Image/instruction prompt → MLLM representation → voxelwise encoding → brain alignment score.
- **Design tradeoffs**: Instruction tuning boosts task-specific alignment but may reduce generalization; model depth affects layer-wise alignment (middle vs. later layers).
- **Failure signatures**: Low alignment in early visual areas, high variance overlap across tasks, inconsistent layer-wise peaks across models.
- **First experiments**: (1) Compare brain alignment of instruction-tuned vs. non-tuned models on same dataset. (2) Test additional instructions beyond the 10 used. (3) Perform cross-dataset validation with held-out fMRI data.

## Open Questions the Paper Calls Out
None.

## Limitations
- The study uses fMRI data from a single dataset with unspecified sample size and demographics, limiting external validity.
- Focus on 10 predefined task instructions may not capture the full range of human visual processing strategies.
- High overlap in neural variance across instructions suggests task-specific alignment may be more limited than implied.

## Confidence
- **High**: MLLMs outperform vision-only models in brain alignment; specific instructions (captioning/recognition) align better than others.
- **Medium**: Instruction tuning causally improves brain alignment; higher visual areas show strongest alignment.
- **Low**: All models fail to differentiate instructions precisely; middle/later layers universally align best (model-dependent).

## Next Checks
1. Test model-brain alignment using held-out brain datasets (e.g., HBN, individual-subject fMRI) to assess generalizability.
2. Conduct controlled ablation experiments comparing instruction-tuned versus non-tuned versions of the same model architectures.
3. Expand instruction diversity and quantify task-specific alignment using within-subject, cross-task variance decomposition.