---
ver: rpa2
title: 'LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning'
arxiv_id: '2510.09189'
source_url: https://arxiv.org/abs/2510.09189
tags:
- translation
- multilingual
- languages
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen3-XPlus addresses the challenge of maintaining reasoning capabilities
  in translation-enhanced LLMs by introducing a layer-selective tuning approach. Unlike
  existing methods that rely on large-scale training data, this work uses a small
  amount of parallel data and applies selective fine-tuning on an instruct model,
  avoiding catastrophic forgetting.
---

# LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning

## Quick Facts
- arXiv ID: 2510.09189
- Source URL: https://arxiv.org/abs/2510.09189
- Reference count: 40
- Qwen3-XPlus achieves 15+ spBLEU and 40+ xComet on low-resource languages while maintaining reasoning performance

## Executive Summary
Qwen3-XPlus addresses the challenge of maintaining reasoning capabilities in translation-enhanced LLMs by introducing a layer-selective tuning approach. Unlike existing methods that rely on large-scale training data, this work uses a small amount of parallel data and applies selective fine-tuning on an instruct model, avoiding catastrophic forgetting. The proposed two-stage training process tunes bottom and top layers while freezing middle layers, preserving general reasoning abilities. Qwen3-XPlus achieves over 15+ spBLEU and 40+ xComet points in low-resource languages such as Swahili, while maintaining reasoning performance comparable to the base Qwen3 model on 15 reasoning benchmarks. This approach significantly reduces complexity and enhances accessibility for multilingual enhancement.

## Method Summary
The method employs layer-selective tuning on Qwen3-Instruct using a two-stage training process. Stage 1 fine-tunes the bottom 4 layers, while Stage 2 fine-tunes the top 15 layers (skipping layer ~20). This selective approach avoids catastrophic forgetting and preserves reasoning capabilities. The training uses 0.8B tokens from NLLB and OPUS-100 datasets, processed through a 6-step pipeline including JSONL formatting, SimHash deduplication, language ID filtering, and quality estimation. Hyperparameters include 1 epoch, learning rate of 1e-5 with cosine scheduler, warmup of 0.03, and batch size of 1 with gradient accumulation of 2.

## Key Results
- Achieves over 15+ spBLEU and 40+ xComet points on FLORES-101 for low-resource languages
- Maintains reasoning performance comparable to base Qwen3 model across 15 reasoning benchmarks
- Demonstrates effectiveness with only 0.8B tokens compared to large-scale training approaches

## Why This Works (Mechanism)
The layer-selective tuning approach works by strategically choosing which layers to fine-tune based on their sensitivity and role in the model. By freezing middle layers (particularly around layer 20), the model preserves its general reasoning capabilities while adapting the bottom layers for translation understanding and top layers for translation generation. The two-stage training provides a smoother adaptation process, allowing the model to first establish translation foundations before refining the generation capabilities.

## Foundational Learning

**Layer freezing and selective tuning**
- Why needed: Prevents catastrophic forgetting while allowing targeted capability enhancement
- Quick check: Verify that unfrozen layers show higher gradient magnitudes during training

**Nuclear norm sensitivity analysis**
- Why needed: Identifies which layers are most sensitive to perturbations and should be treated carefully
- Quick check: Compare layer-wise gradient norms between frozen and unfrozen parameters

**Two-stage training methodology**
- Why needed: Provides smoother adaptation compared to simultaneous multi-layer training
- Quick check: Monitor validation metrics after each stage to ensure monotonic improvement

## Architecture Onboarding

**Component map**: Data preprocessing (6 steps) -> Layer-selective fine-tuning (2 stages) -> Evaluation (translation + reasoning)

**Critical path**: The most sensitive aspect is the layer selection strategy - particularly the decision to skip layer ~20 and the choice of bottom 4/top 15 layers for tuning.

**Design tradeoffs**: 
- Small parallel data vs. comprehensive training data
- Layer selectivity vs. full fine-tuning coverage
- Two-stage process vs. single-stage efficiency

**Failure signatures**: 
- Catastrophic forgetting manifests as reasoning performance collapse while translation improves
- Training middle layers shows spBLEU degradation, particularly at layer 20
- Single-layer ablations reveal performance drops when training excluded layers

**First experiments**:
1. Test layer 20 in isolation to confirm its negative impact on translation performance
2. Compare two-stage vs. one-stage training on a subset of languages
3. Run full ablation study on layer selection strategy (vary number of bottom/top layers)

## Open Questions the Paper Calls Out

**Open Question 1**: What is the theoretical mechanism that makes layer 20 particularly detrimental to translation performance when fine-tuned, and does this pattern generalize across different model architectures beyond Qwen3? The paper identifies layer 20 as problematic through empirical observation and gradient sensitivity analysis but does not provide a theoretical explanation for why this specific layer degrades performance or whether this finding is specific to the Qwen3 architecture.

**Open Question 2**: Can the two-stage training approach be further optimized by adjusting the order, timing, or data composition between stages, and does sequential training offer advantages beyond smoother adaptation? The paper notes advantages of sequential fine-tuning over simultaneous training but does not explore variations such as reversing the order or using different data for each stage.

**Open Question 3**: What are the theoretical limits of using small parallel datasets with instruct models, and at what point does the approach fail to provide meaningful translation improvements for truly low-resource languages without parallel corpora? While the paper demonstrates effectiveness with 0.8B tokens, it does not establish the lower bound of data requirements or whether the approach can work for languages where even limited parallel data is unavailable.

## Limitations
- Missing exact instruction prompt templates and SimHash implementation details
- No statistical significance testing for reasoning benchmark comparisons
- Limited exploration of minimum data requirements for different language families

## Confidence
- Medium: The conceptual approach is sound but critical implementation details are missing
- The layer selection strategy is empirically derived without theoretical justification
- Without exact data pipeline and model checkpoints, independent verification is challenging

## Next Checks
1. Implement the full 6-step data preprocessing pipeline with the same filtering thresholds and verify the 0.8B token count
2. Conduct controlled ablation studies on the layer selection strategy, specifically testing training middle layers versus the proposed bottom-4/top-15 approach
3. Run statistical significance tests comparing Qwen3-XPlus reasoning performance against Qwen3-Instruct on at least 5 reasoning benchmarks using paired t-tests or bootstrap confidence intervals