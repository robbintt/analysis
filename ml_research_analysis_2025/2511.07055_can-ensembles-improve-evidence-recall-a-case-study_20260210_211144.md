---
ver: rpa2
title: Can ensembles improve evidence recall? A case study
arxiv_id: '2511.07055'
source_url: https://arxiv.org/abs/2511.07055
tags:
- evidence
- recall
- ensemble
- complete
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether ensemble methods can improve evidence\
  \ recall in feature attribution tasks. The authors focus on a medical coding task\
  \ where complete evidence\u2014identifying all relevant input tokens\u2014is required\
  \ rather than just minimal sufficient justification."
---

# Can ensembles improve evidence recall? A case study

## Quick Facts
- arXiv ID: 2511.07055
- Source URL: https://arxiv.org/abs/2511.07055
- Authors: Katharina Beckh; Sven Heuser; Stefan Rüping
- Reference count: 18
- Primary result: Ensembles significantly improve evidence recall (0.87) over single models (0.60-0.63) for complete evidence extraction, though precision decreases

## Executive Summary
This paper investigates whether ensemble methods can improve evidence recall in feature attribution tasks for medical coding. The authors focus on a complete evidence extraction task where all relevant input tokens must be identified, rather than just minimal sufficient justification. Using the MDACE dataset with human-annotated complete evidence, they compare single models to ensembles aggregating evidence from multiple models. Results show ensembles significantly improve recall over individual models (0.87 vs 0.60-0.63 for singles), though precision decreases due to additional evidence tokens. The study concludes that ensembles are valuable when exhaustive coverage is needed but require handling increased false positives.

## Method Summary
The study uses encoder-based transformers fine-tuned on MDACE medical text with two training regimes: input gradient regularization (IGR, unsupervised) and evidence-guided training (EGT, supervised). Ten random seeds per regime (20 total models) were trained, with evidence extracted via AttInGrad feature attribution and thresholded on validation data. Ensembles were formed by union aggregation of tokens from all models in each group. The approach was evaluated against human-annotated complete evidence using recall and precision metrics.

## Key Results
- Ensemble recall (0.87) significantly exceeds single-model recall (0.60-0.63) for complete evidence extraction
- Evidence-guided training improves precision (0.74 single, 0.57 ensemble) over IGR (0.70 single, 0.49 ensemble) without affecting recall
- Evidence overlap between models is moderate (Jaccard similarity 0.52-0.57), indicating complementary information
- Marginal utility gains decline after 3-4 models, with diminishing returns for larger ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating evidence from multiple models trained with different random initializations improves recall of complete evidence.
- Mechanism: Models with similar classification performance rely on distinct solution strategies (Rashomon effect), causing each to identify different valid evidence tokens. Union aggregation captures complementary evidence across models.
- Core assumption: Different random initializations yield meaningfully diverse attention patterns; models do not converge to identical evidence selections.
- Evidence anchors:
  - [abstract] "an ensemble approach, aggregating evidence from several models, improves evidence recall over individual models"
  - [section 3] "Evidence similarity is between 0.52-0.57 indicating a substantial evidence overlap between the models" with "unique tokens range between 10 and 11"
  - [corpus] Related work on ensemble diversity (Pang et al.) suggests promoting diversity improves robustness; weak direct evidence for evidence extraction specifically.
- Break condition: If model diversity is insufficient (e.g., identical architectures with highly correlated attention), marginal gains diminish. Paper shows declining marginal utility after 3-4 models.

### Mechanism 2
- Claim: Evidence-guided training (EGT) improves precision without affecting recall compared to unsupervised input gradient regularization.
- Mechanism: Human-annotated evidence spans during training steer model attention toward clinically relevant tokens, reducing spurious highlights (function words, punctuation) that IGR models produce.
- Core assumption: Human annotations accurately represent valid evidence; annotation quality is sufficient to guide training.
- Evidence anchors:
  - [section 3] "EGT has higher precision scores for both, the average model and the ensemble" (0.74 vs 0.70 single; 0.57 vs 0.49 ensemble)
  - [section 3] "EGT models extract more clinically relevant tokens, whereas IGR models more frequently highlight function words and punctuation"
  - [corpus] No directly comparable corpus evidence on supervised evidence extraction.
- Break condition: If human annotations are incomplete or noisy, EGT may learn to miss valid evidence. Paper acknowledges under-annotation in human spans.

### Mechanism 3
- Claim: Ensemble size of 3-4 models captures most achievable recall gains; larger ensembles yield diminishing returns.
- Mechanism: Initial models contribute the most distinct evidence; subsequent models increasingly overlap with existing evidence, providing fewer new tokens per model added.
- Core assumption: Models are drawn from the same training distribution with only random seed variation.
- Evidence anchors:
  - [section 3] "From 3 models onwards (4 in the case of IGR), even the lowest performing combination has higher recall than the best single model"
  - [section 3] "The marginal utility gains of added models are declining, i.e., it is most pronounced for the leap from 1 to 2, and 2 to 3"
  - [corpus] Ensemble literature generally supports diminishing returns with size; no contradictory evidence found.
- Break condition: If model diversity is increased through different architectures or training regimes, optimal ensemble size may differ.

## Foundational Learning

- Concept: **Feature attribution vs. explanation**
  - Why needed here: Paper distinguishes evidence (which tokens supported prediction) from explanation (how prediction was reached). Evidence extraction methods like AttInGrad provide token importance scores, not causal reasoning.
  - Quick check question: Can you explain why AttInGrad scores indicate importance but not mechanism?

- Concept: **Complete vs. sufficient evidence**
  - Why needed here: Most prior work optimizes for minimal sufficient evidence (smallest set that justifies prediction). This task requires complete evidence (all supporting tokens), which is harder and requires different evaluation (recall-focused).
  - Quick check question: In a compliance scenario, why might missing evidence be more costly than false positives?

- Concept: **Jaccard similarity for set overlap**
  - Why needed here: Paper uses Jaccard similarity to quantify evidence overlap between models. Understanding this metric is essential to interpret the moderate (0.52-0.57) overlap meaning.
  - Quick check question: If two models have Jaccard similarity of 0.5 on their evidence sets, what fraction of tokens is unique to each model?

## Architecture Onboarding

- Component map: Pretrained medical transformer -> Fine-tune with IGR/EGT -> AttInGrad attribution -> Threshold validation -> Ensemble union aggregation -> Recall/precision evaluation

- Critical path:
  1. Load pre-trained medical transformer
  2. Fine-tune with either IGR or EGT using MDACE sufficient-evidence split
  3. Extract AttInGrad scores on test documents
  4. Apply validation-tuned threshold to obtain binary evidence tokens
  5. Aggregate across models via set union
  6. Evaluate against complete-evidence annotations using recall/precision

- Design tradeoffs:
  - Recall vs. precision: Ensemble union maximizes recall (0.87) but reduces precision (0.49-0.57)
  - Training complexity: EGT requires annotated evidence; IGR does not but yields lower precision
  - Ensemble size: 3-4 models capture most gains; 10 models used in study but marginal benefit diminishes

- Failure signatures:
  - Low recall on single models (~0.2 instead of ~0.6). Diagnostic: Check evidence threshold too aggressive; verify attribution scores computed correctly.
  - Ensemble recall not improving with size. Diagnostic: Verify models have meaningful diversity (check Jaccard similarity ~0.5); if near 1.0, models may be too similar (check initialization/training variance).
  - IGR highlighting function words/punctuation instead of clinical terms. Diagnostic: Evidence threshold may need adjustment or switch to EGT training.

- First 3 experiments:
  1. Replicate single-model recall baseline with IGR and EGT on MDACE complete-evidence subset to validate setup.
  2. Test ensemble sizes 2-5 to find optimal recall/precision tradeoff for your use case.
  3. Spot-check false positives from ensemble against domain expert to assess whether "errors" are actually under-annotated ground truth (paper suggests this occurs).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can increasing model diversity beyond different random initializations further improve evidence recall?
- Basis in paper: [explicit] The conclusion states that "increasing model diversity during training is a promising direction" as the current models only derive from different random seeds.
- Why unresolved: The study was limited to ensembles of models with identical architectures, differing only by initialization seed.
- What evidence would resolve it: Experiments comparing the recall performance of seed-based ensembles against ensembles of heterogeneous architectures or models trained on distinct data subsets.

### Open Question 2
- Question: How can the significant loss of precision in ensemble evidence extraction be effectively mitigated?
- Basis in paper: [inferred] The results show that aggregating evidence improves recall (0.60 to 0.87) but drastically reduces precision (0.70 to 0.49), yet no method was tested to filter this noise.
- Why unresolved: The authors use a union aggregation method which inherently lowers precision, and while they note the need to handle false positives, they do not propose a solution.
- What evidence would resolve it: A technique (e.g., weighted voting, intersection thresholds, or a secondary classifier) that filters aggregated tokens to restore precision without significantly sacrificing recall.

### Open Question 3
- Question: Do ensemble benefits for complete evidence extraction generalize to larger datasets and non-medical domains?
- Basis in paper: [inferred] The paper is framed as a "case study" and relies on a very small filtered test set of only 17 cases from the medical domain.
- Why unresolved: The small sample size limits the statistical certainty of the findings, and the unique linguistic characteristics of clinical notes may not apply to other text forms.
- What evidence would resolve it: Replication of the ensemble extraction methodology on a large-scale, multi-domain dataset featuring complete human annotations.

## Limitations

- Small evaluation dataset: Only 17 test cases met the multi-span criteria for complete evidence, constraining statistical power
- Precision-recall tradeoff: Union aggregation inherently trades precision for recall, limiting practical utility in precision-critical applications
- Training complexity difference: EGT requires annotated evidence while IGR does not, making direct comparison somewhat confounded

## Confidence

- **High confidence**: Ensemble methods improve recall over single models (0.87 vs 0.60-0.63) - supported by multiple statistical tests and consistent across different ensemble sizes
- **Medium confidence**: Evidence-guided training improves precision without affecting recall - results show clear precision gains, but the recall stability is observed on a small test set
- **Medium confidence**: Moderate evidence overlap (Jaccard 0.52-0.57) indicates complementary information - while measured, the practical significance of this overlap for different ensemble sizes remains uncertain

## Next Checks

1. **Statistical significance validation**: Replicate the recall/precision comparisons across multiple random seeds to establish confidence intervals. The paper reports high variance in single-model recall (±0.25), suggesting the need for more rigorous statistical testing of ensemble benefits.

2. **Domain expert validation of false positives**: Conduct blinded review of ensemble-generated evidence tokens classified as false positives to determine if these represent actual under-annotation in the human labels rather than model errors, as the authors suggest may be occurring.

3. **Ensemble size optimization study**: Systematically evaluate ensemble sizes beyond 10 models (e.g., 20-50) to determine if the diminishing returns pattern continues or if larger ensembles capture additional evidence diversity, particularly when using models with more diverse architectures or training objectives.