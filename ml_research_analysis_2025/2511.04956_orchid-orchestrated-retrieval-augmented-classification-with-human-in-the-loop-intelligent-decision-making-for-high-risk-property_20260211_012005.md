---
ver: rpa2
title: 'ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop
  Intelligent Decision-Making for High-Risk Property'
arxiv_id: '2511.04956'
source_url: https://arxiv.org/abs/2511.04956
tags:
- orchid
- classification
- feedback
- item
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ORCHID improves HRP classification accuracy to 63.12% weighted
  average and 70.37% binary accuracy by integrating retrieval-augmented generation
  with human-in-the-loop validation over a policy corpus, producing auditable, evidence-based
  decisions while deferring uncertain items to SMEs.
---

# ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property

## Quick Facts
- arXiv ID: 2511.04956
- Source URL: https://arxiv.org/abs/2511.04956
- Reference count: 15
- Primary result: Improves HRP classification accuracy to 63.12% weighted average and 70.37% binary accuracy

## Executive Summary
ORCHID is a five-agent system that combines retrieval-augmented generation with human oversight to classify procurement items into HRP categories (USML, NRC, CCL, EAR99) with auditable, policy-based outputs. The system uses hybrid retrieval (BM25 + embeddings + RRF) over a curated policy corpus and routes uncertain items to SMEs for review. By integrating validator agents and structured feedback capture, ORCHID produces evidence-backed decisions while maintaining reproducibility through comprehensive audit logging.

## Method Summary
ORCHID employs a thin Orchestrator coordinating five specialized agents via typed A2A messages: Information Retrieval (IR) for hybrid policy corpus search, Description Refiner (DR) for clarifying item descriptions, HRP Classifier (HRP) for label assignment with citations, Validator (VR) for coverage/conflict checks, and Feedback Logger (FL) for audit trail maintenance. The system uses mxbai-embed-large-v1 for embeddings and enforces on-policy grounding through citation requirements. Human-in-the-loop validation occurs when VR returns REVIEW/CONFLICT or confidence falls below thresholds, with SME feedback logged for future reuse.

## Key Results
- Achieves 63.12% weighted average accuracy across all HRP categories
- Binary accuracy reaches 70.37% when grouping high-risk (USML/NRC) vs low-risk (CCL/EAR99)
- Per-class accuracy varies significantly: USML 88%, NRC 90%, CCL 56%, EAR99 40%

## Why This Works (Mechanism)

### Mechanism 1
Hybrid retrieval over a curated policy corpus improves classification grounding and reduces hallucinations for regulatory compliance tasks. ORCHID uses BM25 (lexical) + semantic embeddings + Reciprocal Rank Fusion (RRF) over a whitelisted, versioned corpus (USML, NRC, CCL, EAR99). The Vector Store MCP tool returns ranked snippets with section IDs, and a citation packer filters to minimally sufficient spans that the classifier must cite verbatim. This restricts the model to on-policy sources.

### Mechanism 2
Agentic decomposition with a validator gate increases reliability by separating evidence retrieval, classification, and conflict detection. A thin Orchestrator coordinates IR (retrieval), DR (description refinement), HRP (classifier), VR (validator), and FL (feedback logger) via typed A2A messages. The VR agent checks coverage and contradiction, emitting AGREE/REVIEW/CONFLICT. Only AGREE proceeds automatically; REVIEW/CONFLICT routes to humans.

### Mechanism 3
Human-in-the-loop deferral with structured feedback capture creates an auditable decision trail and supports iterative improvement. When confidence is below threshold or VR returns REVIEW/CONFLICT, items are deferred to SMEs. SMEs provide Accept/Override decisions with rationale, logged by FL to an append-only audit store. Feedback is tagged to items for future "similar" item matching.

## Foundational Learning

- Concept: Hybrid retrieval (BM25 + dense embeddings + RRF)
  - Why needed here: ORCHID's grounding depends on combining exact phrase matching (BM25) with semantic similarity (embeddings). Understanding RRF fusion weights and re-ranking is essential for tuning retrieval quality.
  - Quick check question: Given a query with technical jargon, which retrieval method (BM25 vs. embedding) would you expect to perform better, and why might fusing both help?

- Concept: Agentic orchestration with typed message contracts
  - Why needed here: The Orchestrator enforces reproducibility via typed A2A messages (item_id, state, context, citations, decision, validator, provenance). Understanding these contracts is critical for debugging agent interactions.
  - Quick check question: If the IR agent returns empty citations, what should the Orchestrator do next, and how does the A2A schema handle missing fields?

- Concept: Auditability and provenance in compliance systems
  - Why needed here: ORCHID's value proposition includes exportable audit bundles (run-cards, prompts, evidence). Understanding what must be logged for replay and governance is essential for deployment.
  - Quick check question: List three artifacts that must be preserved to replay a classification decision in a regulatory audit.

## Architecture Onboarding

- Component map: Orchestrator -> IR -> DR -> HRP -> VR -> (Final Response / SME review) -> FL
- Critical path: Item submission → IR retrieval → DR refinement (optional) → HRP classification → VR validation → (if AGREE) Final Response / (if REVIEW/CONFLICT) SME review → FL logging → Audit bundle export
- Design tradeoffs:
  - Accuracy vs. automation: Higher deferral rates to SMEs improve accuracy (70.37% binary) but reduce throughput; EAR99 at 40% shows boundary ambiguity
  - On-prem security vs. model access: No external egress ensures data security but limits model choice to locally hosted options
  - Single-item UI vs. bulk processing: Current UI supports single-item flows; bulk requires scripts without UI
- Failure signatures:
  - Empty or low-quality citations from IR (sparse descriptions trigger fields-only fallback)
  - VR returns CONFLICT for CCL/EAR99 boundary items (ongoing calibration issue)
  - Audit bundle missing run-card or prompt versions (breaks reproducibility)
  - SME feedback not surfaced for similar items (similarity matching gap)
- First 3 experiments:
  1. Retrieval ablation: Disable BM25 or embeddings separately and measure impact on citation coverage and classification accuracy per category (USML, NRC, CCL, EAR99)
  2. Validator threshold sweep: Vary confidence thresholds for AGREE/REVIEW/CONFLICT and plot deferral rate vs. accuracy to find acceptable operating points
  3. Description quality audit: Run classification with and without operator-provided descriptions; compare accuracy and citation quality to quantify description dependency

## Open Questions the Paper Calls Out

### Open Question 1
How can validator thresholds be optimally calibrated to reduce boundary ambiguity between CCL and EAR99 classifications, which currently show the lowest per-category accuracy (56% and 40%)? The validator emits AGREE/REVIEW/CONFLICT signals but the paper provides no systematic method for setting coverage or conflict thresholds to address this specific ambiguity.

### Open Question 2
Does real-time SME feedback capture via the Feedback Logger agent measurably improve classification accuracy on similar items over time? The Feedback Logger was disabled during preliminary testing "due to data sensitivity," so the claimed benefit of feedback caching for "similar" items remains unevaluated.

### Open Question 3
What retrieval augmentation strategies can maintain classification reliability when item descriptions are sparse, missing, or poorly written? The fallback to fields-only retrieval is mentioned but not quantified; no mitigation strategies for description poverty are proposed.

## Limitations

- The policy corpus may not capture emerging dual-use technologies or evolving regulatory frameworks, creating potential coverage gaps
- Effectiveness of the validator agent in detecting contradictory citations remains unproven, particularly for boundary cases between CCL and EAR99
- System's dependence on operator-provided descriptions creates significant vulnerability when descriptions are sparse or absent

## Confidence

- High Confidence: Core architecture design and reported accuracy metrics are well-specified and methodologically sound
- Medium Confidence: Hybrid retrieval mechanism is supported by related work but lacks direct validation of ORCHID's specific RRF configuration
- Low Confidence: Validator agent effectiveness and SME feedback caching benefits are largely theoretical with minimal empirical support

## Next Checks

1. Implement retrieval ablation study disabling BM25 or embeddings separately to measure individual contributions to citation coverage and classification accuracy
2. Systematically vary validator confidence thresholds for AGREE/REVIEW/CONFLICT decisions and plot deferral rate vs. accuracy tradeoffs
3. Conduct ablation test comparing classification performance with and without operator-provided descriptions to quantify dependency on description quality