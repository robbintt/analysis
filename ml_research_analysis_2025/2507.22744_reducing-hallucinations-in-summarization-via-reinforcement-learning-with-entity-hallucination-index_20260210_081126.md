---
ver: rpa2
title: Reducing Hallucinations in Summarization via Reinforcement Learning with Entity
  Hallucination Index
arxiv_id: '2507.22744'
source_url: https://arxiv.org/abs/2507.22744
tags:
- entity
- hallucination
- fine-tuning
- summaries
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a reinforcement learning approach to reduce
  hallucinations in abstractive summarization by optimizing for Entity Hallucination
  Index (EHI), a lightweight metric measuring entity correctness and grounding. Given
  meeting transcripts, the method first generates baseline summaries and computes
  EHI scores by comparing extracted entities from model outputs to those in the input.
---

# Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index

## Quick Facts
- arXiv ID: 2507.22744
- Source URL: https://arxiv.org/abs/2507.22744
- Reference count: 4
- Method achieves significant EHI score improvements on ELITR datasets through RL fine-tuning with entity-level rewards

## Executive Summary
This paper introduces a reinforcement learning approach to reduce hallucinations in abstractive summarization by optimizing for Entity Hallucination Index (EHI), a lightweight metric measuring entity correctness and grounding. The method fine-tunes a pre-trained summarization model using REINFORCE with EHI as the reward signal, encouraging entity-faithful outputs without requiring human-written factuality annotations. Experiments demonstrate consistent improvements in EHI scores across datasets, with significant reductions in entity-level hallucinations while maintaining fluency and informativeness. The authors also release a reproducible Colab pipeline to facilitate further research on hallucination-aware model fine-tuning.

## Method Summary
The approach involves fine-tuning a pre-trained Flan-T5-Large model for meeting transcript summarization using reinforcement learning with EHI as the reward signal. The process begins with generating baseline summaries and computing EHI scores by comparing extracted entities from model outputs to those in the input. The model is then fine-tuned using REINFORCE with EHI rewards, with per-batch normalization to stabilize training. Summaries are regenerated periodically to reflect model improvements. The method uses spaCy NER for entity extraction and handles long transcripts through chunking (950 tokens with 200 stride overlap).

## Key Results
- RL-EHI fine-tuning achieves significant improvements in Entity Hallucination Index scores across ELITR datasets
- Entity-level hallucination reductions observed while maintaining summary fluency and informativeness
- The method demonstrates consistent EHI score improvements without requiring human factuality annotations

## Why This Works (Mechanism)

### Mechanism 1: EHI as Differentiable Reward Signal for Entity Grounding
- Claim: Optimizing for EHI via policy gradients biases the model toward entity-faithful generation without requiring human factuality annotations.
- Mechanism: EHI functions as a precision-weighted reward that increases when entities are correctly extracted (EF) or beneficial additions (PH), and decreases when hallucinated entities (NH), overfocus (OF), or lost entities (LF) are detected. The REINFORCE gradient estimator ∇θJ(θ) = E[EHI(y,x)∇θ log pθ(y|x)] updates model weights to maximize expected reward.
- Core assumption: NER extraction from spaCy (en_core_web_sm) accurately captures the entity universe relevant to meeting transcripts, and string-level matching suffices for entity alignment.
- Evidence anchors:
  - [abstract] "using EHI as a reward signal to bias generation toward entity-faithful outputs"
  - [section 3.5] Formal objective J(θ) = E_y∼pθ(y|x)[EHI(y, x)] and gradient estimation via REINFORCE
  - [corpus] Weak corpus support—related work on entity-focused summarization exists (InforME addresses named entity salience), but no direct evidence that RL-based entity rewards transfer across domains.
- Break condition: If NER fails on domain-specific entities (rare names, ambiguous mentions), EHI scores become unreliable, causing reward misspecification.

### Mechanism 2: In-Batch Reward Normalization Stabilizes Policy Updates
- Claim: Normalizing EHI rewards within each training batch prevents gradient explosion and enables stable convergence despite reward variance.
- Mechanism: Raw EHI scores can range from 0.0 to 1.0 with high volatility. Per-batch normalization centers rewards around zero and reduces variance, allowing the Adam optimizer with small learning rate (5×10⁻⁶) to make controlled parameter updates without destabilizing the pretrained model.
- Core assumption: Batch-level statistics remain representative of overall reward distribution throughout training.
- Evidence anchors:
  - [section 3.5] "To stabilize training, we normalize EHI rewards within each batch"
  - [table 2] "Green-marked improvements are attributed to reward normalization and convergence effects of the Adam optimizer"
  - [corpus] No direct corpus evidence for this specific normalization strategy in summarization RL.
- Break condition: If batch size is too small or reward distribution shifts during training, normalization statistics become noisy, destabilizing learning.

### Mechanism 3: Periodic Summary Regeneration Reflects Model Improvement
- Claim: Regenerating summaries every 500 updates allows EHI scoring to track model improvements rather than rewarding stale outputs.
- Mechanism: The policy gradient requires samples from the current policy. Periodic regeneration ensures EHI(y,x) is computed on outputs from updated parameters, preventing distribution mismatch between sampling and evaluation.
- Core assumption: 500-update intervals balance computational cost with distributional shift.
- Evidence anchors:
  - [section 3.7] "Summaries are regenerated periodically every 500 updates to reflect model improvements"
  - [section 3.5] "Summaries are regenerated periodically to reflect model improvements during fine-tuning"
  - [corpus] No corpus evidence on optimal regeneration frequency for summarization RL.
- Break condition: If regeneration interval is too long, reward estimates become stale; if too short, computational overhead dominates.

## Foundational Learning

- Concept: **REINFORCE / Policy Gradient**
  - Why needed here: The core fine-tuning mechanism uses REINFORCE to estimate gradients from sampled summaries. Understanding the log-derivative trick and baseline subtraction is essential for debugging reward variance.
  - Quick check question: Can you explain why REINFORCE requires sampling from the current policy rather than using fixed demonstration outputs?

- Concept: **Named Entity Recognition (NER) Evaluation**
  - Why needed here: EHI computation depends entirely on NER quality. Understanding NER limitations (entity boundaries, type errors, domain mismatch) is critical for interpreting EHI scores.
  - Quick check question: What types of entity errors would spaCy's en_core_web_sm likely make on meeting transcripts with internal acronyms?

- Concept: **Encoder-Decoder Summarization with Chunking**
  - Why needed here: Meeting transcripts exceed Flan-T5-Large's 1024-token context, requiring chunking strategies (950 tokens, 200 stride) that affect entity continuity across segments.
  - Quick check question: How might overlapping chunks with 200-token stride affect entity coverage at chunk boundaries?

## Architecture Onboarding

- Component map:
  Input layer -> Base model -> NER module -> EHI scorer -> RL optimizer -> Regeneration loop

- Critical path:
  1. Chunk transcript -> generate summary per chunk -> aggregate
  2. Extract entities from source chunks and summary
  3. Compute EHI via entity matching (case-insensitive string level)
  4. Normalize rewards within batch
  5. Compute REINFORCE gradient, update parameters
  6. Every 500 steps: regenerate summaries, repeat

- Design tradeoffs:
  - String-level matching vs. semantic matching: Fast but misses paraphrased entities (e.g., "CEO" vs. "Chief Executive")
  - spaCy NER vs. larger models: Lightweight but may miss domain entities; trade-off between speed and coverage
  - Chunking vs. long-context models: Preserves compatibility with Flan-T5 but risks entity fragmentation across chunks

- Failure signatures:
  - EHI stuck near 0.0: NER failing to extract entities from transcript or summary; check entity coverage
  - EHI volatile across batches: Reward normalization unstable; increase batch size or check for outlier samples
  - Model copies entities verbatim without abstraction: Over-optimization for EF penalizes paraphrasing; may need to weight PH higher
  - Meaningful entities missing from summaries but EHI high: EHI may not capture all important entities (LF penalty insufficient)

- First 3 experiments:
  1. Baseline EHI profiling: Before RL, compute EHI on held-out test set; identify which error type (NH, OF, LF) dominates to prioritize reward shaping.
  2. NER coverage audit: Extract entities from gold summaries vs. source transcripts using spaCy; quantify missed entities to assess EHI ceiling.
  3. Ablation: remove reward normalization: Run RL without per-batch normalization; compare EHI convergence stability and final scores to validate Section 3.5 claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EHI-based fine-tuning be extended to capture relation-level hallucinations, complementing entity-level faithfulness?
- Basis in paper: [explicit] "The hallucination index improves the quality focusing merely on entities and a subsequent method of Relation Hallucination is needed to complement it."
- Why unresolved: EHI quantifies entity presence and grounding but does not assess whether relationships between correctly extracted entities are accurate.
- What evidence would resolve it: A Relation Hallucination metric validated against human judgments of factual consistency in entity-pair relationships.

### Open Question 2
- Question: What is the trade-off between strict entity grounding and paraphrasing/abstraction quality under EHI optimization?
- Basis in paper: [explicit] "In some cases, the model overly prioritized exact entity copying at the expense of paraphrasing or abstraction quality. This suggests a trade-off between strict entity grounding and higher-level semantic fluency that merits further investigation."
- Why unresolved: The reward signal may incentivize entity verbatim copying, potentially degrading summary abstraction.
- What evidence would resolve it: Controlled experiments varying EHI reward weight while measuring semantic abstraction via lexical diversity and human ratings.

### Open Question 3
- Question: How does EHI-guided fine-tuning generalize to multi-document summarization?
- Basis in paper: [explicit] "In future work, we plan to extend EHI-based fine-tuning to multi-document summarization and explore its integration with controllable generation frameworks."
- Why unresolved: Current experiments are limited to single meeting transcripts; multi-document scenarios introduce cross-document entity grounding challenges.
- What evidence would resolve it: Benchmarks on multi-document datasets (e.g., Multi-News) showing EHI improvements without fluency loss.

### Open Question 4
- Question: Can EHI thresholds calibrated on reference data enable reference-free hallucination prediction for zero-shot summaries?
- Basis in paper: [explicit] "EHI has a scope to be used readily in reference free context by combining with Relative Abstractiveness and generating EHI scores on reference data itself and use it as threshold for zero shot reference less summaries to predict hallucination."
- Why unresolved: The proposed reference-free application is conceptual and untested.
- What evidence would resolve it: Experiments demonstrating calibrated EHI thresholds predicting hallucinations in unseen zero-shot outputs with high precision/recall.

## Limitations
- Method's heavy dependence on spaCy NER creates fragility—domain-specific entities may be systematically missed, causing reward misspecification
- String-level entity matching cannot capture paraphrased entities, potentially over-penalizing meaningful abstractions
- Lack of ablation studies comparing RL-EHI to simpler EHI-aware decoding or other factuality-aware fine-tuning methods

## Confidence

**High confidence**: The EHI metric formulation and its mathematical definition are clearly specified and reproducible. The chunking strategy for handling long transcripts is explicitly detailed.

**Medium confidence**: The RL fine-tuning procedure using REINFORCE with batch-normalized rewards is described in sufficient detail for replication, though exact variance reduction techniques beyond normalization are unspecified. The claim of improved EHI scores on ELITR datasets is supported by reported metrics.

**Low confidence**: The claim that RL-EHI maintains fluency and informativeness is weakly supported—only aggregate metrics (ROUGE, BERTScore) are reported without qualitative analysis of abstraction quality or entity paraphrasing. The assertion that EHI is "lightweight" and doesn't require human annotations is technically true but ignores the heavy dependence on NER quality.

## Next Checks

1. **NER Coverage Audit**: Run spaCy en_core_web_sm on gold summaries and source transcripts from the ELITR test set; quantify entity extraction precision/recall to establish EHI's upper bound and identify systematic failures on meeting-specific entities.

2. **Cross-Chunk Entity Coherence**: Analyze entity continuity across chunked inputs—measure LF penalties specifically for entities split between chunks versus those fully contained within single chunks to assess chunking's impact on entity grounding.

3. **Ablation: Reward Normalization**: Implement RL fine-tuning without batch-normalized rewards; compare EHI convergence curves and final scores to test the critical claim that normalization stabilizes training and prevents gradient explosion.