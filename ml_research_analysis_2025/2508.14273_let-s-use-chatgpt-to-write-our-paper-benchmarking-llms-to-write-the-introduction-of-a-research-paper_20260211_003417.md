---
ver: rpa2
title: Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction
  of a Research Paper
arxiv_id: '2508.14273'
source_url: https://arxiv.org/abs/2508.14273
tags:
- generation
- introduction
- llms
- research
- citation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-assisted academic writing shows promise but requires careful
  prompting and post-editing. LLaMA-4-Maverick outperforms other models in semantic
  similarity and faithfulness, while three-shot prompting consistently improves results.
---

# Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper

## Quick Facts
- arXiv ID: 2508.14273
- Source URL: https://arxiv.org/abs/2508.14273
- Authors: Krishna Garg; Firoz Shaik; Sambaran Bandyopadhyay; Cornelia Caragea
- Reference count: 40
- Primary result: LLaMA-4-Maverick achieves highest semantic similarity and faithfulness in automated introduction generation

## Executive Summary
This paper benchmarks large language models for writing research paper introductions, finding that structured prompting strategies significantly improve output quality. LLaMA-4-Maverick consistently outperforms other models across multiple evaluation metrics, while three-shot prompting provides reliable improvements in lexical overlap and faithfulness. The study reveals that generated introductions are coherent and contextually relevant but often miss fine-grained technical details and accurate citation integration. Despite high-quality drafts, human expertise remains essential for finalizing research paper introductions.

## Method Summary
The authors evaluate LLMs on the SciIG task, generating introductions from structured inputs (title, abstract, related work) using various prompting strategies including zero-shot, few-shot, and AutoCoT approaches. They use a fresh NAACL/ICLR 2025 dataset to avoid training contamination and employ automated metrics (ROUGE, BERTScore, citation quality) alongside LLM-as-Judge and human evaluations. Citations are enriched with SemanticScholar API lookups to improve grounding.

## Key Results
- LLaMA-4-Maverick achieves highest semantic similarity and faithfulness scores
- Three-shot prompting consistently improves lexical overlap and reference-free content coverage
- Adding Related Work context dramatically improves citation precision (0.36→0.94) and recall (0.12→0.56)
- Human evaluation shows generated introductions are coherent but miss technical details and citation accuracy

## Why This Works (Mechanism)

### Mechanism 1: Three-Shot Prompting
Three-shot prompting consistently improves lexical overlap and faithfulness compared to zero/one/two-shot approaches by providing structural templates and citation patterns that guide models toward academic rhetorical conventions without parameter updates. The model learns to emulate the CARS model structure from exemplars. Core assumption: Models can generalize formatting and citation patterns from 3 diverse examples. Evidence: Table 3 shows ROUGE-1 improving from 0.4402 (zero-shot) to 0.4460 (three-shot); Section 5.1 confirms consistent outperformance. Break condition: Diminishing returns after two-shot; mismatched domain examples may reduce quality.

### Mechanism 2: Citation Grounding with Related Work
Adding Related Work context dramatically improves citation precision (0.36→0.94) and recall (0.12→0.56) by grounding generation in factual anchors and reducing hallucination pressure. Providing explicit citation metadata allows the model to retrieve from context rather than parametric memory. Core assumption: Models will use provided context over internal knowledge when citations are explicitly available. Evidence: Table 4 ablation shows citation precision jumping from 0.3610 to 0.9357 with Related Work; LLM-as-Judge citation quality improves from 0.5184 to 0.8051. Break condition: Noisy or irrelevant related work degrades output.

### Mechanism 3: Structured Prompt Constraints
Structured prompt constraints (four-paragraph ELABORATE format) outperform unconstrained approaches on human-evaluated coherence and faithfulness by aligning generation with the CARS rhetorical model. Explicit structural constraints force sequential argumentation. Core assumption: Models can follow complex structural instructions without quality degradation. Evidence: Human evaluation (Table 5) shows ELABORATE 0-shot scores 4.20 on faithfulness vs. AutoCoT at 4.11; Section 5.2 notes trade-offs with AutoCoT's increased perplexity. Break condition: Over-rigid constraints may conflict with content requirements.

## Foundational Learning

- **Conditional Text Generation (f: T, A, R → I)**
  - Why needed here: The SciIG task formulates introduction generation as mapping structured inputs to coherent outputs
  - Quick check question: Given a paper's title, abstract, and 10 related papers, what information should the model use to establish the "research gap" in the introduction?

- **In-Context Learning vs. Fine-Tuning**
  - Why needed here: All experiments use frozen models with prompting strategies
  - Quick check question: If three-shot prompting improves ROUGE-1 by 0.006, what would you expect if you fine-tuned the same model on 800 NAACL introductions instead?

- **Faithfulness Metrics (QA-based, Entailment-based, Keyphrase-based)**
  - Why needed here: The paper introduces three complementary faithfulness measures
  - Quick check question: Why might a generated introduction score high on QA-based faithfulness (0.78) but near-zero on entailment-based faithfulness (-0.03)?

## Architecture Onboarding

- **Component map:** grobid2json PDF parser → structured JSON (title/abstract/introduction/authors/references) → Citation enrichment (LLaMA-4 mapping + SemanticScholar API) → Prompt assembly (template + examples + context) → Frozen LLM inference → Evaluation (automated + LLM-as-Judge + human annotation)

- **Critical path:** 1) Curate fresh dataset (NAACL/ICLR 2025) to avoid training contamination 2) Enrich citations with external metadata 3) Select prompting strategy (three-shot + ELABORATE recommended) 4) Generate and evaluate across multiple dimensions

- **Design tradeoffs:** More context (full related work) vs. token limits for long papers; few-shot examples improve quality but consume context window; AutoCoT improves keyphrase faithfulness (0.36) but increases perplexity (21→19); human evaluation is gold standard but doesn't scale beyond ~80 samples

- **Failure signatures:** Low citation recall (<0.50): Related work context missing or incomplete; high perplexity (>30): Prompt conflicts or insufficient structural guidance; negative entailment scores: Model hallucinating beyond source context; low LLM-as-Judge faithfulness (<0.70): Generated claims not grounded in abstract/title

- **First 3 experiments:** 1) Baseline establishment: Run ELABORATE prompt zero-shot on 50 NAACL samples; record ROUGE-1/2/L, BERTScore, and citation precision/recall as reference points 2) Few-shot validation: Add 3 examples from held-out set; measure delta on lexical overlap and faithfulness metrics 3) Context ablation: Run Title-only vs. Title+Abstract vs. Full context on same 50 samples; plot citation precision/recall curves

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on English-language NLP conference papers, limiting generalizability to other domains or languages
- Citation enrichment relies on SemanticScholar API coverage, which may be incomplete for newer or less-cited works
- Human evaluation sample size (80 papers) provides directional insights but lacks statistical power for fine-grained comparisons

## Confidence
- High confidence: LLaMA-4-Maverick outperforming other models in semantic similarity and faithfulness
- Medium confidence: Three-shot prompting consistently improving results
- Medium confidence: Citation quality improvements with Related Work context

## Next Checks
1. Cross-domain generalization test: Apply three-shot ELABORATE approach to biomedical and social science papers; measure whether ROUGE improvements persist outside NLP domain
2. Citation robustness evaluation: Systematically remove 20% of related work abstracts from enrichment pipeline; quantify degradation in citation precision/recall
3. Long-context stress test: Generate introductions for papers exceeding 8,000 tokens; measure how token limits affect few-shot example retention and overall quality degradation