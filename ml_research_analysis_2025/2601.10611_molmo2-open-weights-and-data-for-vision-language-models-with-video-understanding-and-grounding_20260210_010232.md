---
ver: rpa2
title: 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding
  and Grounding'
arxiv_id: '2601.10611'
source_url: https://arxiv.org/abs/2601.10611
tags:
- video
- data
- image
- training
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Molmo2 addresses the challenge of creating open, state-of-the-art
  vision-language models capable of video understanding and grounding, a capability
  lacking in current open-source models. The core method introduces a suite of nine
  novel datasets for video captioning, QA, pointing, and tracking, all collected without
  relying on closed models, alongside an efficient training recipe using sequence
  packing, message-tree encoding, bidirectional attention, and token-weighting strategies.
---

# Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding

## Quick Facts
- arXiv ID: 2601.10611
- Source URL: https://arxiv.org/abs/2601.10611
- Authors: Christopher Clark; Jieyu Zhang; Zixian Ma; Jae Sung Park; Mohammadreza Salehi; Rohun Tripathi; Sangho Lee; Zhongzheng Ren; Chris Dongjoo Kim; Yinuo Yang; Vincent Shao; Yue Yang; Weikai Huang; Ziqi Gao; Taira Anderson; Jianrui Zhang; Jitesh Jain; George Stoica; Winson Han; Ali Farhadi; Ranjay Krishna
- Reference count: 40
- Primary result: Molmo2-8B achieves state-of-the-art performance among open-weight models on video understanding, counting, and captioning, while significantly outperforming existing open models on video grounding tasks.

## Executive Summary
Molmo2 addresses the challenge of creating open, state-of-the-art vision-language models capable of video understanding and grounding, a capability lacking in current open-source models. The core method introduces a suite of nine novel datasets for video captioning, QA, pointing, and tracking, all collected without relying on closed models, alongside an efficient training recipe using sequence packing, message-tree encoding, bidirectional attention, and token-weighting strategies. The primary result shows that Molmo2-8B achieves state-of-the-art performance among open-weight models on video understanding, counting, and captioning, while significantly outperforming existing open models on video grounding tasks (e.g., 35.5 vs 29.6 accuracy on video counting, and 38.4 vs 20.0 F1 on video pointing), and matches or exceeds proprietary models on several benchmarks.

## Method Summary
Molmo2 uses a three-stage training pipeline: (1) pre-training on image captioning, pointing, and NLP tasks for 32k steps, (2) supervised fine-tuning on a mixture of 9 new Molmo2 datasets and 40+ academic datasets for 30k steps, and (3) long-context fine-tuning for 2k steps. The model architecture combines a SigLIP 2 vision encoder with Qwen3/OLMo LLM backbone, using attention pooling connectors (3x3 window for video) and bidirectional attention on visual tokens. A novel token-weighting strategy balances long and short outputs, while message-tree encoding enables efficient processing of multiple annotations per video.

## Key Results
- Molmo2-8B achieves 35.5 accuracy on video counting, outperforming existing open models (29.6) and proprietary models (33.0)
- Video pointing F1 of 38.4, significantly above open models (20.0) and approaching proprietary models (43.2)
- Video QA accuracy of 64.8 on MVBench, leading among open-weight models and competitive with proprietary models (68.5)
- Video captioning F1 of 71.7, outperforming open models (63.7) and matching proprietary models (72.0)

## Why This Works (Mechanism)

### Mechanism 1: Non-Distilled Grounding Data Generation
High-performance video grounding requires training data not synthetically distilled from proprietary models, which often carry their biases and hallucinations. The authors employ a human-and-LLM collaboration pipeline where annotators speak detailed descriptions (transcribed via Whisper-1) and click spatial-temporal points, while an LLM acts as a drafting assistant or query generator. This creates datasets that combine human verification with scalable generation, effectively removing hallucinations while retaining linguistic complexity.

### Mechanism 2: Token-Weighting for Multi-Task Balance
In mixed-dataset training, balancing the loss contribution of short-answer QA and long-form captions via specific token weights prevents the model from optimizing only for long-sequence generation. The paper applies fixed weights (e.g., 0.1 for captions, 0.2 for pointing) and a $4\sqrt{n}$ heuristic for other tasks, effectively up-weighting the importance of short, precise outputs relative to verbose ones.

### Mechanism 3: Bi-Directional Attention on Vision Tokens
Enabling bi-directional attention among visual tokens (even across frames or crops) improves the model's ability to integrate spatial and temporal context before generating text. Allowing vision tokens to attend to each other creates a richer latent representation of the visual scene, particularly useful for "grounding" where the model must locate a target relative to the whole scene.

## Foundational Learning

- **Concept:** Sequence Packing and Message Trees
  - **Why needed here:** The paper trains on inputs ranging from short text to 16k+ token videos with multiple annotations. Standard padding is wasteful. "Message trees" linearize multiple annotations into a single sequence with custom attention masks to prevent leakage.
  - **Quick check question:** Can you design an attention mask that allows a model to process "Video + QA_1" and "Video + QA_2" in parallel without QA_1 attending to QA_2?

- **Concept:** Spatio-Temporal Pointing Formats
  - **Why needed here:** The model outputs points in a compressed text format (HTML-like) including coordinates and object indices (IDs) for tracking.
  - **Quick check question:** How do you represent an object that disappears and reappears in a video using only normalized coordinates and integer indices in a text stream?

- **Concept:** Vision-Language Connectors (Pooling)
  - **Why needed here:** The connector maps ViT patch features to LLM tokens. Molmo2 uses attention pooling with different window sizes (2x2 for image vs 3x3 for video) to manage token count vs. spatial resolution.
  - **Quick check question:** What is the trade-off between using a 3x3 pooling window vs. a 4x4 window for video captioning? (Paper notes 4x4 degrades performance).

## Architecture Onboarding

- **Component map:** Video Frames (2fps) + Text (Queries/Subtitles) -> SigLIP 2 ViT -> Connector (3x3 pooling) -> Visual Tokens -> Interleaved with Text Tokens -> LLM Forward Pass (Bi-directional attention on Vision, Causal on Text) -> Text or `<points>` formatted strings

- **Critical path:**
  1. Frame extraction (Scene detection/Uniform sampling)
  2. ViT encoding -> Connector Pooling -> Visual Tokens
  3. Interleaving Visual Tokens + Text Tokens (with Time Stamps)
  4. LLM Forward Pass (Bi-directional attention on Vision part, Causal on Text)
  5. Loss Calculation (Apply Token Weighting)

- **Design tradeoffs:**
  - **Token Efficiency:** 3x3 pooling reduces video tokens (good for speed) but loses spatial detail (bad for fine-grained pointing). The paper suggests 3x3 is optimal for captioning/QA balance; 4x4 hurts captioning.
  - **Context Window:** Standard training uses 16k context; Long-context training uses 36k (expensive, requires Context Parallelism).

- **Failure signatures:**
  - **Degenerate Tracks:** Model outputs a long line of points on one frame or repeats the same point for every frame
  - **Repetitive Captioning:** Model generates repeating text when generating very long video captions using greedy decoding

- **First 3 experiments:**
  1. **Overfit Single Video Task:** Take one video with "VideoPoint" annotations. Train the model on just this sample to ensure the `<points>` coordinate format is parsed and generated correctly.
  2. **Ablate Token Weights:** Train two small models on a 10% data subset with and without the $0.1$ caption weighting. Compare the degradation rate on short-form QA benchmarks vs. caption quality.
  3. **Validate Message Masking:** Construct a batch with "packed" examples. Verify manually that gradients from "QA_1" do not affect weights contributing to "QA_2" in the same packed sequence.

## Open Questions the Paper Calls Out

- Can the performance gap on complex reasoning benchmarks (e.g., MathVista) be closed solely by augmenting the training mixture with domain-specific multi-modal reasoning data?
- Can the alignment discrepancy between low frame-rate sampling (for long context) and high frame-rate annotations be resolved to enable robust grounding in videos exceeding 3 minutes?
- Is the "degenerate output" behavior in video grounding (e.g., repeated points) caused primarily by task interference in the joint training mixture?

## Limitations
- Degenerate grounding outputs occur where the model may produce a "long line of points" or repeat the same point across frames, especially for high-frequency objects
- The model sometimes generates repetitive text in long video captions
- Grounding has limited support for long (3 minutes+) videos due to frame-rate misalignment between sampling and annotations

## Confidence

**High Confidence:**
- The technical implementation of sequence packing and message-tree encoding is sound and well-documented
- The overall training recipe (3-stage pipeline, connector architecture) is reproducible
- The claim that Molmo2 achieves state-of-the-art performance among open-weight models is supported by benchmark results

**Medium Confidence:**
- The superiority of non-distilled data over distilled data for grounding tasks
- The effectiveness of the specific token-weighting values (0.1, 0.2, 4âˆšn) for multi-task balance
- The contribution of bidirectional attention on vision tokens to overall performance

**Low Confidence:**
- The scalability of the human-LLM annotation pipeline for future dataset expansion
- The long-term generalization of the model to domains not represented in the training mixture
- The robustness of grounding outputs in scenarios with very high object counts or complex occlusion patterns

## Next Checks

1. **Isolate the Human Verification Effect**: Train two models on identical datasets where one uses fully manual annotations and the other uses human-verified LLM drafts. Compare grounding performance to quantify the actual contribution of the human verification step versus the LLM drafting capability.

2. **Token Weight Sensitivity Analysis**: Systematically vary the token weights (0.05, 0.1, 0.2, 0.5 for captions; 0.1, 0.2, 0.5, 1.0 for pointing) and measure the trade-off curve between long-form captioning quality and short-answer QA/grounding accuracy. Identify if the current values represent optimal balance or a local minimum.

3. **Grounding Robustness Benchmark**: Create a focused evaluation set with controlled variables: (a) videos with increasing object counts (1-50), (b) varying occlusion levels, and (c) rapid motion scenarios. Measure pointing accuracy and track stability across these conditions to identify failure thresholds and potential architectural improvements.