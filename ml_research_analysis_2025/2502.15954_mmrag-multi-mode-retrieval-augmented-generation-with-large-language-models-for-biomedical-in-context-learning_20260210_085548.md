---
ver: rpa2
title: 'MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models
  for Biomedical In-Context Learning'
arxiv_id: '2502.15954'
source_url: https://arxiv.org/abs/2502.15954
tags:
- mode
- examples
- biomedical
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a multi-mode retrieval-augmented generation\
  \ framework to improve in-context learning for biomedical NLP tasks. The framework\
  \ employs four retrieval strategies\u2014Random, Top, Diversity, and Class Mode\u2014\
  to optimize example selection for large language models across named entity recognition,\
  \ relation extraction, and text classification tasks."
---

# MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning

## Quick Facts
- arXiv ID: 2502.15954
- Source URL: https://arxiv.org/abs/2502.15954
- Reference count: 40
- Multi-mode retrieval-augmented generation framework improves biomedical NLP performance, with Top Mode achieving 26.4% F1 improvement on drug-drug interaction extraction.

## Executive Summary
This study introduces MMRAG, a multi-mode retrieval-augmented generation framework designed to enhance in-context learning for biomedical natural language processing tasks. The framework employs four retrieval strategies—Random, Top, Diversity, and Class Mode—to optimize example selection for large language models across named entity recognition, relation extraction, and text classification tasks. Experiments with Llama-2-7B and Llama-3-8B models on four biomedical datasets demonstrate that Top Mode significantly outperforms Random Mode, achieving a 26.4% improvement in F1 score on drug-drug interaction extraction. The framework shows enhanced adaptability and performance, addressing data scarcity challenges in biomedical NLP by refining example selection strategies.

## Method Summary
MMRAG implements a retrieval-augmented generation framework that optimizes example selection for in-context learning in biomedical NLP. The system retrieves semantically similar training examples using embedding-based retrievers (Contriever, MedCPT, or BGE-Large), then applies one of four selection modes: Random (baseline), Top (highest similarity), Diversity (skip-step selection for variety), or Class (category-based coverage). Retrieved examples are formatted into prompts with templates and fed to Llama-2-7B or Llama-3-8B models. Performance is evaluated using exact match metrics (precision, recall, F1) across NER, RE, and TC tasks on four biomedical datasets. The framework systematically explores retriever effectiveness, example quantity effects, and mode-task interactions.

## Key Results
- Top Mode retrieval achieved 26.4% F1 improvement (0.9669) over Random Mode on drug-drug interaction extraction tasks
- Llama-3-8B consistently outperformed Llama-2-7B, particularly in named entity recognition tasks (F1 0.9782 vs 0.9172 with 10 random examples)
- Contriever retriever outperformed MedCPT and BGE-Large on most experiments, though MedCPT excelled on GIT relation extraction in Class Mode
- Diminishing returns observed when increasing examples from 5 to 10 in Random Mode, while Top/Diversity modes maintained performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving semantically similar examples for in-context learning significantly improves biomedical NLP performance compared to random selection.
- Mechanism: Top Mode retrieval identifies training examples with high semantic similarity to the input query, which activates relevant domain patterns in the LLM's parametric knowledge, enabling more accurate entity recognition and relation extraction without weight updates.
- Core assumption: The retriever's similarity metric aligns with task-relevant semantic features (e.g., entity types, relation patterns) rather than surface-level lexical overlap.
- Evidence anchors:
  - [abstract] "Top Mode significantly outperforms Random Mode, achieving a 26.4% improvement in F1 score (0.9669) on drug-drug interaction extraction."
  - [section 3.2] "For RE on the DDI dataset, Contriever retrieval led to the highest F1 score of 0.9669 for Llama-2-7B."
  - [corpus] Related work (arXiv:2508.06504) confirms "RAG-based dynamic prompting" improves few-shot biomedical NER, suggesting the mechanism generalizes across biomedical tasks.
- Break condition: If retriever similarity scores do not correlate with task performance (e.g., high-similarity examples contain contradictory labels), the mechanism degrades to random baseline.

### Mechanism 2
- Claim: Introducing controlled diversity in retrieved examples balances relevance with coverage, reducing overfitting to narrow patterns.
- Mechanism: Diversity Mode's skip-step selection (picking examples at intervals from ranked lists) maintains semantic relevance while exposing the model to varied lexical contexts and relation types, improving generalization on noisy biomedical text.
- Core assumption: The top-k most similar examples are not uniformly optimal; some redundancy exists that can be exchanged for diversity without substantial relevance loss.
- Evidence anchors:
  - [abstract] Four modes introduced; Diversity Mode is one of two strategies that "significantly outperform Random mode on the RE (DDI) task."
  - [section 2.2] "Diversity Mode introduces a skip-step selection mechanism... ensuring that the retrieved examples maintain relevance while increasing the diversity of information."
  - [corpus] Weak corpus evidence—neighbor papers do not explicitly test diversity vs. top-k tradeoffs in biomedical ICL.
- Break condition: If gap parameter is too large (skipping too many relevant examples), performance converges toward Random Mode; if too small, converges toward Top Mode with its redundancy issues.

### Mechanism 3
- Claim: Task-specific retriever selection amplifies ICL effectiveness, but optimal pairing depends on task type and LLM generation.
- Mechanism: Contriever (contrastive-trained general retriever) outperformed MedCPT (domain-specific) on most experiments, suggesting general semantic matching may be more robust than domain fine-tuning for ICL example retrieval—though this varies by task (MedCPT excelled on GIT relation extraction in Class Mode).
- Core assumption: Retriever effectiveness for ICL is not simply a function of domain specialization but depends on how embedding space structure aligns with in-context example utility.
- Evidence anchors:
  - [abstract] "Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments."
  - [section 3.4] Class Mode shows MedCPT achieved highest F1 (0.6129) for Llama-3 on GIT dataset, outperforming Contriever (0.5978).
  - [corpus] No corpus papers directly compare retriever architectures for biomedical ICL; this remains underexplored.
- Break condition: If task requires highly specialized vocabulary (e.g., rare disease terms outside retriever's pre-training), general retrievers may fail regardless of embedding quality.

### Mechanism 4
- Claim: LLM generation capabilities interact with example selection strategy—newer models leverage retrieved context more effectively for structured extraction.
- Mechanism: Llama-3-8B consistently outperformed Llama-2-7B on NER (F1 0.9782 vs. 0.9172 with 10 random examples), suggesting improved instruction-following and structured output generation, though Llama-2 showed advantages on some TC/RE tasks.
- Core assumption: Generation quality improvements (boundary detection, format adherence) compound with retrieval quality.
- Evidence anchors:
  - [abstract] "Llama-3-8B consistently outperformed Llama-2-7B, particularly in named entity recognition tasks."
  - [section 4.1] Error analysis identifies "Over-Extraction" and boundary errors (e.g., tokenizing "CRE-binding-protein") as key failure modes, suggesting generation quality remains a bottleneck.
  - [corpus] Corpus evidence limited—neighbor papers do not systematically compare LLM generations for biomedical ICL.
- Break condition: If LLM has weak instruction-following or generates malformed outputs (invalid JSON, boundary errors), better retrieval provides diminishing returns.

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: The entire framework operates on ICL—understanding that LLMs can perform tasks from prompt examples without gradient updates is foundational to interpreting why retrieval strategy matters.
  - Quick check question: Can you explain why adding 5 examples improves performance but 5→10 shows diminishing returns in Random Mode? (Answer: Marginal information decreases; irrelevant examples can mislead.)

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: MMRAG extends RAG from document retrieval to example retrieval for ICL; understanding embedding-based semantic search is prerequisite.
  - Quick check question: What is the difference between using RAG for knowledge retrieval vs. example retrieval for ICL? (Answer: Knowledge retrieval provides facts to ground answers; example retrieval provides pattern demonstrations for task learning.)

- **Biomedical NLP Task Taxonomy (NER, RE, TC)**
  - Why needed here: The framework optimizes differently across task types—NER benefits most from Llama-3 and Top Mode; RE shows largest gains from retrieval optimization.
  - Quick check question: Why might relation extraction benefit more from retrieval optimization than text classification? (Answer: RE requires identifying entity pairs and relation types, which depend heavily on contextual patterns from similar examples.)

## Architecture Onboarding

- **Component map:** Training Corpus → Embedder/Retriever → Similarity Ranking → Selection Mode: Random/Top/Diversity/Class → Prompt Constructor ← Template + Retrieved Examples → LLM (Llama-2/3) → Structured Output → Exact Match Evaluator → Precision/Recall/F1

- **Critical path:** Retriever quality → Selection mode → Prompt construction (example ordering/format) → LLM generation → Exact match evaluation. The 26.4% F1 improvement is driven primarily by retriever-mode alignment.

- **Design tradeoffs:**
  | Mode | Strength | Weakness | Best For |
  |------|----------|----------|----------|
  | Random | Maximum diversity baseline | No relevance guarantee | Control experiments |
  | Top | Highest similarity | Redundancy, narrow patterns | Precision-critical (NER, RE) |
  | Diversity | Relevance + variation | Tuning gap parameter | Noisy/variable inputs |
  | Class | Category coverage | Lower per-example similarity | Multi-class classification |

- **Failure signatures:**
  - Low F1 with high retrieval scores: Retriever similarity doesn't align with task labels; try alternative retriever.
  - Performance degrades 5→10 examples: Random mode selecting misleading examples; switch to Top/Diversity.
  - Llama-3 underperforms Llama-2 on TC: Task-specific generation patterns; may need prompt format adjustment.
  - Boundary errors in entity spans: Tokenization mismatch between retriever examples and LLM; normalize example formatting.

- **First 3 experiments:**
  1. Baseline establishment: Run Random Mode with 1/5/10 examples on your target dataset to establish F1 baseline and variance (expect high std dev per Table 1).
  2. Retriever ablation: Compare Contriever vs. MedCPT vs. BGE-Large in Top Mode with 5 examples—expect Contriever to win on RE/TC, MedCPT competitive on domain-specific tasks.
  3. Mode comparison at fixed budget: Compare Top vs. Diversity vs. Class Mode with 5 examples using best retriever from experiment 2—quantify diversity-relevance tradeoff for your task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MMRAG framework perform when applied to a wider range of large language model architectures, particularly domain-specific biomedical models?
- Basis in paper: [explicit] The authors state that "Future work should explore the effectiveness of retrieval strategies across a wider range of models, including newer architectures and domain-specific LLMs."
- Why unresolved: The current study restricts its evaluation to only two general-purpose models (Llama-2-7B and Llama-3-8B), leaving the framework's generalizability to specialized medical architectures untested.
- What evidence would resolve it: Experimental results from MMRAG using domain-specific models (e.g., Meditron, BioMistral) or proprietary models (e.g., GPT-4) on the same datasets.

### Open Question 2
- Question: Can more efficient retrieval algorithms be developed to optimize MMRAG performance while maintaining computational feasibility on massive datasets?
- Basis in paper: [explicit] The limitations section notes that "to fully leverage big data, future research should focus on developing more efficient retrieval algorithms that optimize performance while maintaining computational feasibility."
- Why unresolved: The current framework may face computational bottlenecks when scaling up to extremely large corpora, which is necessary for broader application.
- What evidence would resolve it: A study demonstrating maintained or improved F1 scores with reduced latency and memory usage when applied to datasets significantly larger than those used in the current study.

### Open Question 3
- Question: Does enhancing LLMs to better mimic human cognitive patterns significantly improve the effectiveness of Class Mode retrieval strategies?
- Basis in paper: [inferred] The discussion posits that current LLMs are limited by next-word prediction training, whereas "strategies like Class Mode could further enhance their effectiveness" if "future LLMs evolve to better mimic human cognitive patterns."
- Why unresolved: It is unclear if the observed limitations of Class Mode are due to the retrieval strategy itself or the underlying reasoning limitations of current LLM architectures.
- What evidence would resolve it: Comparative experiments showing Class Mode performance gains in models fine-tuned for reasoning versus standard auto-regressive models.

## Limitations

- Evaluation confined to four specific biomedical datasets, limiting generalizability to other biomedical text types
- Comparison limited to two model generations (Llama-2-7B and Llama-3-8B) without exploring intermediate parameter sizes or other architectural variants
- Framework assumes exact match evaluation is appropriate for all tasks, which may not capture clinically relevant performance in real-world applications

## Confidence

- **High Confidence**: Claims about Top Mode outperforming Random Mode on RE tasks (DDI F1 0.9669), based on statistically significant improvements across multiple experiments with consistent methodology.
- **Medium Confidence**: Claims about Llama-3-8B's superiority on NER tasks, supported by consistent results but limited to two model generations without ablation studies on architectural differences.
- **Low Confidence**: Claims about retriever effectiveness (Contriever vs. MedCPT) due to limited dataset diversity and lack of evaluation on truly rare biomedical concepts outside pre-training distributions.

## Next Checks

1. **Dataset Generalization Test**: Evaluate MMRAG on three additional biomedical datasets representing underrepresented domains (e.g., clinical narratives, pathology reports) to assess performance stability across biomedical text types.

2. **Retriever Robustness Analysis**: Systematically degrade retriever performance (e.g., by introducing noise in embeddings or using retrievers trained on non-biomedical data) and measure downstream ICL performance to establish failure thresholds.

3. **Prompt Format Sensitivity Study**: Vary example ordering, formatting, and instruction phrasing within MMRAG to determine whether the observed performance gains are attributable to retrieval strategy or prompt engineering effects.