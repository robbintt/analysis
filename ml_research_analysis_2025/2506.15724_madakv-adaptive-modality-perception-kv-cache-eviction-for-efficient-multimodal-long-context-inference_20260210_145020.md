---
ver: rpa2
title: 'MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal
  Long-Context Inference'
arxiv_id: '2506.15724'
source_url: https://arxiv.org/abs/2506.15724
tags:
- cache
- attention
- layer
- arxiv
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MadaKV introduces a modality-adaptive key-value (KV) cache eviction
  strategy for multimodal large language models (MLLMs) to improve efficiency in long-context
  inference. Unlike traditional unimodal methods, MadaKV uses Modality Preference
  Adaptation to dynamically sense modality importance across attention heads and Hierarchical
  Compression Compensation to adjust eviction ratios across layers based on modality
  complexity.
---

# MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference

## Quick Facts
- arXiv ID: 2506.15724
- Source URL: https://arxiv.org/abs/2506.15724
- Reference count: 21
- Reduces KV cache memory footprint by 80-95% and improves inference decoding latency by 1.3-1.5x while maintaining high accuracy across multimodal tasks

## Executive Summary
MadaKV introduces a modality-adaptive key-value (KV) cache eviction strategy for multimodal large language models (MLLMs) to improve efficiency in long-context inference. Unlike traditional unimodal methods, MadaKV uses Modality Preference Adaptation to dynamically sense modality importance across attention heads and Hierarchical Compression Compensation to adjust eviction ratios across layers based on modality complexity. This approach significantly reduces memory usage and inference latency while preserving accuracy across multimodal tasks, particularly under tight cache budgets.

## Method Summary
MadaKV introduces two plug-and-play components to optimize KV cache eviction in MLLMs. The Modality Preference Adaptation (MPA) module computes per-head modality importance using proxy token attention scores from the final prompt tokens, dynamically allocating cache budgets to visual and textual modalities. The Hierarchical Compression Compensation (HCC) mechanism adjusts eviction ratios across layers by tracking cumulative budget deviation and enforcing stricter compression in subsequent layers when necessary. The method was evaluated on LLaVA-v1.5 and Qwen2.5-VL models using the MileBench benchmark, demonstrating 80-95% cache reduction and 1.3-1.5x decoding latency improvement.

## Key Results
- Reduces KV cache memory footprint by 80-95% compared to baseline
- Improves inference decoding latency by 1.3-1.5x while maintaining high accuracy
- Outperforms existing methods on MileBench benchmark tasks (TN, IEdit, MMCoQA, STD, ALFRED, CLEVR-C, DocVQA, ST, OI)
- Particularly effective under tight cache budgets with minimal accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1: Modality-Aware Budget Allocation (MPA)
- **Claim:** Allocating cache budgets based on the specific modality preferences of individual attention heads preserves more critical context than uniform eviction.
- **Mechanism:** During the prefill phase, MadaKV calculates a "preference metric" ($w_v, w_t$) for each attention head by summing attention scores directed at visual versus textual tokens from a set of "proxy tokens" (typically the final tokens of the prompt). It dynamically assigns a cache budget $\phi$ to each modality proportional to this preference, ensuring heads retain tokens of the modality they "specialize" in.
- **Core assumption:** The attention patterns of recent "proxy tokens" accurately reflect the global importance of visual vs. textual context for the entire sequence.
- **Evidence anchors:**
  - [Abstract]: "attention heads exhibit varying preferences for different modalities... MadaKV uses Modality Preference Adaptation to dynamically sense modality importance."
  - [Section 3.3]: Eq. (6-8) define the preference metric and budget allocation $\phi_{v,t} \propto w / (w_v+w_t)$.
  - [Corpus]: Neighbor paper "Hierarchical Adaptive Eviction..." suggests similar heterogeneous attention distribution in MLLMs, supporting the premise of non-uniform sparsity.
- **Break condition:** If attention heads do not exhibit stable preferences (e.g., random distribution), the adaptive budgeting reduces to noise, performing no better than static allocation.

### Mechanism 2: Inter-Layer Budget Compensation (HCC)
- **Claim:** Balancing the cache budget hierarchically across layers prevents error propagation caused by aggressive compression in sensitive layers.
- **Mechanism:** The Hierarchical Compression Compensation (HCC) mechanism monitors the cumulative budget deviation ($K_l$) across layers. If a specific layer retains more tokens than its target (due to high modality complexity), HCC enforces stricter eviction in subsequent layers to maintain the global memory cap. Conversely, savings in simple layers allow for larger caches in complex ones.
- **Core assumption:** Modality information density varies significantly by layer, and a global budget constraint is more important than per-layer uniformity.
- **Evidence anchors:**
  - [Abstract]: "Hierarchical Compression Compensation to adjust eviction ratios across layers based on modality complexity."
  - [Section 3.3]: Eq. (10-11) define the budget compensation $K_l$ and the adaptive update for the next layer $\phi_{l+1}$.
- **Break condition:** If early layers consistently overspend their budget, later layers may be starved of tokens, causing a collapse in reasoning coherence for complex tasks.

## Foundational Learning

- **Concept: KV Cache Sparsity**
  - **Why needed here:** MadaKV relies on the observation that MLLM attention is sparse (few tokens matter). Without this baseline, eviction would always degrade accuracy.
  - **Quick check question:** Can you explain why retaining only 20% of tokens often preserves 90% of attention mass in LLMs?

- **Concept: Cross-Modal Attention**
  - **Why needed here:** The paper's core thesis is that visual and text tokens have different "information densities." You must understand that visual tokens often require more spatial redundancy than concise text tokens.
  - **Quick check question:** Why might a model attend differently to a visual patch versus a function word like "the"?

- **Concept: Attention Sink / Proxy Tokens**
  - **Why needed here:** MadaKV uses specific tokens to calculate importance. Understanding that not all tokens are suitable "judges" of importance is critical.
  - **Quick check question:** Why does the paper select tokens from the end of the prompt as "proxy tokens" rather than the beginning?

## Architecture Onboarding

- **Component map:** Multimodal Prompt (Text + Visuals) -> MPA Module (computes per-head modality preference $w$) -> Eviction Policy (retains top-$k$ tokens per modality based on MPA budget $\phi$) -> HCC Controller (tracks cumulative token count vs. global target; adjusts $\phi$ for next layer) -> Compressed KV Cache
- **Critical path:** The calculation of the preference metric (Eq. 6) must occur efficiently during the prefill phase to avoid adding latency that negates the benefits of cache reduction.
- **Design tradeoffs:** The method trades off per-layer cache uniformity for global efficiency. While this improves average performance, it may introduce latency spikes if the HCC calculation is complex or if dynamic memory allocation overhead exists.
- **Failure signatures:**
  - **Modality Hallucination:** If MPA incorrectly prioritizes text in a visual-heavy task (e.g., Image Retrieval), the model may ignore the images entirely.
  - **Context Amnesia:** If HCC is too aggressive in compensating for early layers, later layers may have zero cache budget, resulting in incoherent generation.
- **First 3 experiments:**
  1. **Budget vs. Accuracy Sweep:** Run MileBench tasks (TN, MMCoQA) with cache budgets ranging from 5% to 100% to replicate the "knee" in the efficiency curve (Fig 4).
  2. **MPA Ablation:** Disable modality preference (force 50/50 split) to verify the performance drop shown in Table 3.
  3. **Needle-in-a-Haystack (Visual vs. Text):** Specifically test the "Text Needle" vs. "Image Retrieval" tasks to confirm that MadaKV correctly shifts preference to the relevant modality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MadaKV's Modality Preference Adaptation be effectively extended to temporal modalities like video and audio, where information density and attention sparsity patterns differ significantly from static images?
- **Basis in paper:** [explicit] The "Limitation" section explicitly states the study focused on visual and textual modalities and has not yet explored other modalities like video or audio.
- **Why unresolved:** The current method calculates importance based on static visual token sparsity; video tokens introduce temporal redundancy that might require a different eviction logic to prevent losing critical temporal continuity.
- **What evidence would resolve it:** Successful application of MadaKV to video-LLMs (e.g., Video-LLaVA) on long-video benchmarks (e.g., EgoSchema) without significant degradation in temporal reasoning.

### Open Question 2
- **Question:** Does the performance of MadaKV scale to significantly larger model parameters (e.g., 70B+) and extreme context windows (e.g., 100k+ tokens) without introducing computational bottlenecks?
- **Basis in paper:** [explicit] The authors note in the "Limitation" section that due to resource constraints, they have not conducted experiments on larger parameter sizes (e.g., 34B, 70B) or datasets with extremely long contexts.
- **Why unresolved:** The dynamic calculation of Hierarchical Compression Compensation (HCC) across layers might incur latency overheads that negate the inference speed benefits in larger, deeper architectures.
- **What evidence would resolve it:** Benchmark results on models like LLaMA-3-70B demonstrating that MadaKV maintains its 1.3-1.5x decoding latency improvement even as the KV cache scale increases.

### Open Question 3
- **Question:** Can MadaKV be combined with orthogonal inference optimization techniques, such as quantization, to achieve multiplicative efficiency gains?
- **Basis in paper:** [explicit] The "Conclusion" states: "Looking ahead, we plan to explore the integration of MadaKV with other MLLMs inference acceleration techniques to further enhance efficiency and performance."
- **Why unresolved:** Eviction methods drop tokens entirely, while quantization reduces precision; combining them could amplify information loss, potentially causing the Modality Preference Adaptation to misjudge token importance.
- **What evidence would resolve it:** Experiments showing that MadaKV remains robust when applied to a KV cache that has already been quantized to INT4 or INT8, maintaining accuracy on the MileBench benchmark.

## Limitations

- The effectiveness of MadaKV's MPA mechanism depends on the assumption that proxy token attention scores accurately represent global modality importance, which remains untested for very long contexts (>4096 tokens)
- The HCC mechanism assumes linear degradation across layers, but this may not hold for tasks requiring deep reasoning where early-layer compression could be more catastrophic than later-layer compression
- Critical hyperparameters including proxy token count, threshold θ, and initial budget allocation φ_l are not specified in the paper, making exact reproduction difficult

## Confidence

**High Confidence (Mechanism Validity):** The core premise that MLLM attention heads exhibit heterogeneous modality preferences is well-supported by existing literature and the ablation studies showing MPA's contribution to performance. The mathematical framework for preference metric calculation and budget allocation is internally consistent.

**Medium Confidence (Generalization):** While results are strong on MileBench (showing 80-95% cache reduction with 1.3-1.5x latency improvement), the benchmark's diversity in task types and visual complexity may not capture all real-world scenarios. The paper does not address performance on proprietary datasets or domain-specific applications.

**Low Confidence (Implementation Details):** Critical hyperparameters including proxy token count, threshold θ, and initial budget allocation φ_l are not specified, making exact reproduction difficult. The paper also lacks runtime overhead measurements for the MPA and HCC computations during prefill.

## Next Checks

1. **Layer-wise Accuracy Sensitivity Analysis:** Run MadaKV on TN and MMCoQA tasks with per-layer cache budget logging to identify which layers are most sensitive to compression. Compare the actual accuracy degradation pattern against the HCC's compensation strategy to validate whether it correctly prioritizes cache allocation.

2. **Cross-Domain Generalization Test:** Apply MadaKV to a medical imaging QA dataset (e.g., VQA-RAD) and a document understanding dataset (e.g., FUNSD) beyond MileBench. Measure whether the modality preferences learned on MileBench transfer to domains with different visual-textual ratios and complexity patterns.

3. **Long-Context Boundary Test:** Evaluate MadaKV on sequences exceeding 8192 tokens with mixed modality density (e.g., textbook-chapter length documents with interspersed figures). Track how the proxy token-based preference metric holds up as the relative position of "final tokens" becomes a smaller fraction of the total context, potentially invalidating the attention proxy assumption.