---
ver: rpa2
title: 'DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph
  Contrastive and Reinforcement Learning'
arxiv_id: '2512.11342'
source_url: https://arxiv.org/abs/2512.11342
tags:
- loop
- learning
- pass
- optimization
- cycles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing high-level synthesis
  (HLS) compiler pass ordering, which remains largely unexplored despite its critical
  impact on FPGA accelerator design. Current HLS tools rely on fixed optimization
  strategies inherited from software compilation, limiting their effectiveness across
  diverse designs.
---

# DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.11342
- Source URL: https://arxiv.org/abs/2512.11342
- Reference count: 34
- Achieves 2.36x speedup over Vitis HLS on average HLS benchmarks

## Executive Summary
This paper addresses the challenge of optimizing high-level synthesis (HLS) compiler pass ordering, which remains largely unexplored despite its critical impact on FPGA accelerator design. Current HLS tools rely on fixed optimization strategies inherited from software compilation, limiting their effectiveness across diverse designs. The authors propose DAPO, a design structure-aware pass ordering framework that leverages graph contrastive learning and reinforcement learning. DAPO extracts heterogeneous program representations from compilation IRs, employs contrastive learning to generate rich embeddings, and uses an analytical model for hardware metric estimation to guide an RL agent in discovering design-specific optimization strategies. Evaluated on classic HLS benchmarks spanning multiple application domains, DAPO achieves a 2.36x speedup over Vitis HLS on average, with improvements of 1.67x on designs without pragmas and 2.36x on designs with pragmas, while simultaneously reducing resource usage.

## Method Summary
DAPO uses a two-phase approach to optimize LLVM pass ordering for HLS compilation. First, it extracts heterogeneous graphs from LLVM IR, separating control flow, data flow, and hierarchical relationships using RGCNs. These graphs are pre-trained via contrastive learning using Heterogeneous Graph Edit Distance to create structure-aware embeddings. Second, a PPO reinforcement learning agent uses these frozen embeddings as states to explore pass sequences, receiving rewards from an analytical model (Light-HLS) that estimates hardware performance metrics. The framework evaluates on 100 HLS designs from multiple benchmark suites, optimizing sequences of 45 general-purpose LLVM transformation passes while keeping pragma-specific passes in fixed order.

## Key Results
- Achieves 2.36x average speedup over Vitis HLS across all benchmarks
- Improves designs without pragmas by 1.67x and with pragmas by 2.36x
- Reduces resource usage to 0.80x LUT and 0.93x DSP compared to baseline
- Kendall's Tau correlation with Light-HLS estimator â‰¥ 0.884 confirms reliable performance prediction

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Relation Decoupling
Separating program representations into distinct control, data, and hierarchical subgraphs allows the model to capture non-commutative transformation dependencies that homogeneous graphs miss. The framework constructs a heterogeneous graph from LLVM IR and processes it using Relational Graph Convolutional Networks (RGCNs). RGCNs apply separate weight matrices for different edge types (control vs. data vs. hierarchy), effectively treating them as distinct communication channels before merging them into a unified embedding. Core assumption: Optimization passes target specific graph structures (e.g., loop-simplify targets control flow, gvn targets data flow), and these structures require distinct feature extraction pathways.

### Mechanism 2: Structure-Aware Contrastive Pre-training
Pre-training embeddings using Heterogeneous Graph Edit Distance (HGED) enables the model to generalize optimization strategies to unseen designs without requiring expensive labeled optimal sequences. The model is trained via contrastive learning to minimize/maximize embedding distance based on HGED. This enforces that graphs with similar structures (low edit distance) have similar embeddings, effectively creating a "design similarity" space that the RL agent can navigate. Core assumption: Program structural similarity (graph topology) correlates with the effectiveness of specific pass sequences.

### Mechanism 3: RL Policy Optimization via Fast Analytical Feedback
Decoupling representation learning from policy optimization and using a fast analytical model (Light-HLS) for reward allows efficient exploration of the combinatorial pass sequence space. A Proximal Policy Optimization (PPO) agent receives the pre-trained graph embedding as the "state." It selects passes (actions) and receives a reward based on latency/resource estimates from Light-HLS, avoiding the bottleneck of actual synthesis. Core assumption: The analytical model (Light-HLS) provides a reward signal that correlates strongly enough with actual hardware performance to guide the RL agent.

## Foundational Learning

- **Concept: LLVM Intermediate Representation (IR)**
  - Why needed here: DAPO operates entirely on LLVM IR, extracting Control and Data Flow Graphs (CDFGs) from this lower-level code rather than the original C++ source.
  - Quick check question: Can you identify a basic block and a phi-node in LLVM IR?

- **Concept: Graph Edit Distance (GED)**
  - Why needed here: This metric defines "similarity" for the contrastive learning phase. Understanding it is crucial to knowing what the model considers "similar designs."
  - Quick check question: If you add a loop to a function, does the GED increase or decrease relative to the original?

- **Concept: Reinforcement Learning (RL) Policy vs. Value Functions**
  - Why needed here: The system uses an Actor-Critic (PPO) architecture. Understanding that the "Actor" suggests passes while the "Critic" evaluates potential performance is key to debugging training instability.
  - Quick check question: In this context, what represents the "Action" and what represents the "Reward"?

## Architecture Onboarding

- **Component map:** HLS C++ Code -> LLVM IR -> Heterogeneous Graph (Nodes: Instructions/Blocks; Edges: Control/Data/Hierarchy) -> RGCN -> Fixed-size Vector (Pre-trained via Contrastive Learning) -> PPO Actor-Critic (Input: Embedding Vector; Output: Probability distribution over 45 passes) -> Light-HLS (Analytical Estimator) -> Latency/Resources

- **Critical path:** The Embedding Quality is the critical path. If the RGCN fails to differentiate between a loop-heavy design and a pipeline design (conceptually), the RL agent receives a garbage state and cannot learn a coherent policy.

- **Design tradeoffs:**
  - Speed vs. Accuracy: Using Light-HLS (analytical) instead of Vivado/Vitis synthesis reduces evaluation time from hours/days to seconds, but introduces approximation error.
  - Generalization vs. Specialization: Contrastive learning forces generalization (structural similarity), potentially sacrificing performance on "outlier" designs that require unique, non-structural optimizations.

- **Failure signatures:**
  - Reward Hacking: The agent finds a pass sequence that minimizes the analytical latency estimate but fails timing or resources in actual synthesis (Proxy divergence).
  - Embedding Collapse: Contrastive loss drops to zero, but all graphs map to the same vector; RL fails to improve (Check: Variance of embeddings in a batch).
  - Pass Oscillation: RL agent cycles between passes (e.g., unroll -> simplify -> unroll) without convergence.

- **First 3 experiments:**
  1. Sanity Check (Random vs. Fixed): Run DAPO on a single simple design (e.g., Matrix Multiply) vs. Vitis HLS default -O3 to verify the pipeline executes and Light-HLS correlates with synthesis.
  2. Ablation (Embeddings): Train the RL agent with random embeddings (frozen) vs. pre-trained embeddings to quantify the contribution of contrastive learning (referencing Section V.C results).
  3. Generalization Test: Train on PolyBench suite, then test on a completely different domain (e.g., a Rosetta design) to verify zero-shot transfer capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DAPO framework be extended to jointly optimize pragma settings and general-purpose pass sequences, rather than treating the order of pragma-specific passes as a fixed constraint?
- Basis in paper: Section II-A states that "Given the topological order of pragma-specific passes, DAPO tends to focus on optimizing the order of general-purpose passes."
- Why unresolved: The authors explicitly decouple pragma transformations from the RL search space to manage complexity, leaving the interaction between dynamic pragma configuration and pass ordering unexplored.
- What evidence would resolve it: An evaluation of an expanded RL action space that includes pragma insertion/modification, showing convergence and QoR results compared to the current fixed-pragma approach.

### Open Question 2
- Question: How does the framework scale to optimization passes with parameters (e.g., specific unroll factors) where the action space becomes continuous or significantly larger?
- Basis in paper: Section IV-E defines the action space as "45 carefully selected LLVM transformation passes," treating them as discrete atomic actions without tunable parameters.
- Why unresolved: While the discrete approach works for on/off passes, many HLS optimizations (like loop unrolling) benefit from specific numeric factors which are currently excluded from the search space.
- What evidence would resolve it: Results from an experiment where the action space includes parameterized transformations, showing the RL agent's ability to navigate the increased dimensionality.

### Open Question 3
- Question: How robust are the learned embeddings and RL policy when applied to industrial-scale designs with significantly deeper hierarchy and complexity than the academic benchmarks used for training?
- Basis in paper: Section V-A evaluates the method on "100 representative HLS designs" derived from classic academic suites (PolyBench, CHStone, etc.).
- Why unresolved: Graph Neural Networks often struggle to generalize to graphs that are orders of magnitude larger or structurally different from the training distribution, raising questions about viability for commercial applications.
- What evidence would resolve it: Zero-shot inference performance results on a suite of large, real-world industrial HLS designs that were not part of the training or validation sets.

## Limitations
- Light-HLS analytical estimator correlation with actual hardware synthesis remains unproven, creating uncertainty about real-world performance.
- HGED-based contrastive learning assumes structural similarity implies optimization similarity, which may fail for designs where semantic differences dominate.
- The framework currently treats optimization passes as discrete atomic actions without parameters, limiting its ability to optimize specific transformation factors.

## Confidence

- **High Confidence:** The heterogeneous graph representation and RGCN architecture - supported by ablation showing RGCN outperforms homogeneous GNNs.
- **Medium Confidence:** The overall 2.36x speedup claim - strong correlation with Light-HLS but lacks validation against actual hardware synthesis results.
- **Medium Confidence:** The contrastive learning contribution - demonstrated through embedding quality metrics, but the specific HGED implementation details are sparse.

## Next Checks

1. **Hardware Validation:** Synthesize top-3 DAPO-optimized designs from each benchmark suite to actual FPGA hardware, measuring real latency and resource usage against Vitis HLS baseline.
2. **Cross-Architecture Transfer:** Train DAPO on Xilinx Vitis HLS benchmarks, then evaluate on Intel FPGA HLS tools to test architectural generalization of the learned pass ordering policies.
3. **Extreme Design Test:** Apply DAPO to a design outside the training distribution (e.g., a deep neural network with irregular sparsity patterns) to evaluate zero-shot transfer capability and identify potential failure modes.