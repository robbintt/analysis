---
ver: rpa2
title: Is Q-learning an Ill-posed Problem?
arxiv_id: '2502.14365'
source_url: https://arxiv.org/abs/2502.14365
tags:
- policy
- learning
- q-learning
- iteration
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the instability of Q-learning in continuous
  environments, a critical challenge in reinforcement learning. Using the cart-pole
  benchmark, the research systematically examines the effects of bootstrapping and
  model inaccuracies by progressively eliminating these potential error sources.
---

# Is Q-learning an Ill-posed Problem?

## Quick Facts
- **arXiv ID:** 2502.14365
- **Source URL:** https://arxiv.org/abs/2502.14365
- **Reference count:** 6
- **Primary result:** Q-learning instability in continuous environments persists even when eliminating bootstrapping and model errors, suggesting fundamental ill-posedness due to Q-function discontinuities.

## Executive Summary
This study investigates Q-learning instability in continuous-state reinforcement learning using the cart-pole benchmark. Through systematic elimination of potential error sources, the authors demonstrate that Q-learning remains unstable even when using ground-truth Q-values computed from true environment dynamics. The research reveals that the true Q-function exhibits inherent discontinuities that cannot be accurately approximated by standard neural network function approximators, making the supervised learning problem ill-posed. This fundamental limitation affects not only Q-learning but potentially other methods relying on sample-based Q-value evaluation.

## Method Summary
The study uses the cart-pole benchmark with a four-dimensional state space (position, velocity, angle, angular velocity) and trains Q-functions using neural networks with mean squared error loss. Three variants are compared: standard NFQ (with bootstrapping), BSF-NFQ (bootstrapping-free with learned model), and BSF-NFQ-real-dyn (using true environment dynamics). The authors progressively eliminate potential error sources to isolate the root cause of instability. Performance is measured by average return over 1,000 episodes, with success defined as reaching 5,000 steps.

## Key Results
- Q-learning success rate: 3.8% ± 0.4% (baseline NFQ)
- Bootstrapping-free method improves stability: 23.1% ± 1.1% (BSF-NFQ)
- True dynamics further improves but doesn't eliminate instability: 28% success (BSF-NFQ-real-dyn)
- Significant performance variance persists even with identical target values across random seeds
- True Q-function exhibits discontinuities that cannot be accurately approximated by neural networks

## Why This Works (Mechanism)

### Mechanism 1: Bootstrapping Error Propagation
Traditional NFQ computes targets via $Q_{i+1}(s_t, a_t) \leftarrow r_t + \gamma \max_{a_{t+1}} Q_i(s_{t+1}, a_{t+1})$, propagating approximation errors across iterations. BSF-NFQ replaces this with finite-horizon rollouts $\tilde{Q}^{\pi}_{MB}(s,a) = \sum_{k=0}^{K-1} \gamma^k R(\tilde{s}_k, \pi(\tilde{s}_k), \tilde{s}_{k+1})$, breaking the error propagation chain. Core assumption: The transition model $M$ and reward model $R$ can be learned sufficiently accurately from the offline dataset.

### Mechanism 2: Transition Model Inaccuracy as Secondary Error Source
Using the benchmark's analytical transition equations (BSF-NFQ-real-dyn) computes ground-truth Q-value targets, removing model bias. This isolates whether instability stems from model approximation. Core assumption: The environment has known or recoverable analytical dynamics.

### Mechanism 3: Discontinuous Q-Functions as Root Cause of Ill-Posedness
The true Q-function in continuous-state MDPs exhibits inherent discontinuities that cannot be accurately approximated by smooth neural network function approximators, rendering the supervised learning problem ill-posed. In cart-pole, Q-values for adjacent pole angles differ substantially due to deterministic rollout divergence near decision boundaries. Neural networks with ReLU/continuous activations learn smoothed averages, losing critical boundary information.

## Foundational Learning

- **Concept: Q-learning and the Bellman Equation**
  - Why needed here: The paper's entire diagnostic process assumes understanding how Q-learning iteratively refines value estimates via bootstrapped targets.
  - Quick check question: Can you explain why $\max_{a_{t+1}} Q(s_{t+1}, a_{t+1})$ creates error propagation across iterations?

- **Concept: Function Approximation with Neural Networks**
  - Why needed here: The core finding is that neural networks impose smoothness assumptions incompatible with discontinuous true Q-functions.
  - Quick check question: Why would a neural network with ReLU activations struggle to fit a function with sharp discontinuities?

- **Concept: The Deadly Triad (Bootstrapping, Off-Policy, Function Approximation)**
  - Why needed here: The paper's motivation is isolating which components of the deadly triad cause instability in continuous domains.
  - Quick check question: Which element of the deadly triad does BSF-NFQ attempt to eliminate?

## Architecture Onboarding

- **Component map:** NFQ (baseline) → BSF-NFQ (model-based rollouts) → BSF-NFQ-real-dyn (true dynamics) → Fundamental discontinuity problem

- **Critical path:** The diagnostic progression is the key contribution. Engineers should understand that improving algorithm components improves success rates (3.8% → 23.1% → 28%), but a fundamental ceiling exists due to MDP structure.

- **Design tradeoffs:**
  - Smooth function approximators (stable but biased) vs. discontinuous true Q-functions (accurate but unlearnable)
  - ε-greedy policy rollouts reduce discontinuity magnitude but do not eliminate them—trading variance for bias

- **Failure signatures:**
  - Good policy in iteration $i$ followed by bad policy in iteration $i+1$ despite same target computation procedure
  - High variance across random seeds when retraining on identical Q-value targets
  - No correlation between training/validation error and policy performance

- **First 3 experiments:**
  1. Replicate Figure 2: Train Q-functions on saved targets with 100 different seeds, observe performance variance distribution
  2. Replicate Figure 3: Visualize true Q-values along a single state dimension to identify discontinuity locations
  3. Test ε-greedy rollouts: Compare discontinuity magnitude between deterministic and stochastic (ε=0.05) policy rollouts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative function approximators (e.g., piecewise models, decision trees, or discontinuity-aware architectures) better approximate Q-functions with inherent discontinuities than standard neural networks?
- Basis in paper: [explicit] The authors state the true Q-function has "a structure that cannot be accurately approximated using a neural network" and that "fitting an NN with samples from a discontinuous function makes the problem ill-posed."
- Why unresolved: The paper demonstrates the problem exists but does not explore alternative approximators beyond standard feedforward NNs with ReLU activations.
- What evidence would resolve it: Empirical comparison showing improved stability and policy consistency using discontinuity-aware function approximators on the same benchmarks.

### Open Question 2
- Question: Do similar Q-function discontinuities emerge in other continuous MDPs (e.g., hopper, walker, robotic manipulation) beyond the cart-pole benchmark studied here?
- Basis in paper: [explicit] The authors note they explore stability issues "commonly encountered in continuous state MDPPs like inverted pendulum, acrobot and hopper" but conduct experiments only on cart-pole "due to illustration purposes."
- Why unresolved: The generalization claim to other benchmarks remains untested; cart-pole may have particularly severe discontinuity structures.
- What evidence would resolve it: Systematic analysis of true Q-function structure across multiple continuous control benchmarks, quantifying discontinuity prevalence and severity.

### Open Question 3
- Question: Can reward shaping or MDP design principles prevent Q-function discontinuities while preserving task objectives?
- Basis in paper: [inferred] The paper concludes that "the problem is induced by the definition of the MDP and not the algorithm itself," suggesting MDP modification as a potential remedy, but does not investigate this avenue.
- Why unresolved: The relationship between MDP specification (reward structure, termination conditions, transition dynamics) and Q-function smoothness remains uncharacterized.
- What evidence would resolve it: Theoretical or empirical demonstration that modified reward functions or MDP formulations yield smoother Q-functions while producing equivalent optimal policies.

### Open Question 4
- Question: Can theoretical conditions be established to identify a priori whether a given continuous MDP will produce a discontinuous Q-function?
- Basis in paper: [inferred] The authors show that "discontinuities can already occur if the state space of the MDP is continuous" but provide no predictive framework for determining when this will happen.
- Why unresolved: The paper demonstrates the phenomenon empirically but offers no theoretical characterization of discontinuity conditions.
- What evidence would resolve it: Formal theorem specifying sufficient or necessary conditions on MDP structure (transitions, rewards, policy space) that guarantee or preclude Q-function discontinuities.

## Limitations

- Findings are based on a single continuous control benchmark (cart-pole), which may not generalize to all continuous MDPs
- Analysis focuses on specific architecture (5-64-1 NN with ReLU) and hyperparameter settings
- Does not explore whether discontinuities can be mitigated through architectural changes (ensemble methods, specialized activation functions, adaptive discretization)

## Confidence

- **High Confidence**: The diagnostic methodology (progressively eliminating bootstrapping and model errors) is sound and the empirical results (success rates 3.8% → 23.1% → 28%) are well-supported by the data
- **Medium Confidence**: The claim that discontinuities in true Q-functions are a fundamental, MDP-structural problem. While Figure 3 convincingly shows discontinuities in cart-pole, the paper does not prove this is universal across continuous MDPs
- **Medium Confidence**: The assertion that discontinuities cause the observed variance across random seeds (Figure 2). The correlation is strong, but a causal mechanism linking specific discontinuity locations to policy performance drops is not explicitly tested

## Next Checks

1. **Cross-MDP Discontinuity Analysis**: Apply the Q-value visualization technique (Figure 3) to other continuous control benchmarks (e.g., Pendulum, Acrobot) to test whether discontinuities are a general phenomenon
2. **Policy Transfer Stability Test**: Train a successful policy, save its Q-function, and retrain from scratch on the same targets (as in Figure 2) but with different architectures (e.g., wider networks, different activations). Compare variance to assess if architecture affects sensitivity to discontinuities
3. **Discontinuity Localization Experiment**: Identify specific state regions where Q-value discontinuities occur. Systematically perturb the NN to force a discontinuity at a known location and observe if policy performance drops, establishing a causal link