---
ver: rpa2
title: Scale Up Composed Image Retrieval Learning via Modification Text Generation
arxiv_id: '2504.05316'
source_url: https://arxiv.org/abs/2504.05316
tags:
- image
- text
- retrieval
- images
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of composed image retrieval
  (CIR), which aims to search for an image of interest using a combination of a reference
  image and modification text. The main issue is the limited availability of training
  data and the laborious process of annotating triplets.
---

# Scale Up Composed Image Retrieval Learning via Modification Text Generation

## Quick Facts
- arXiv ID: 2504.05316
- Source URL: https://arxiv.org/abs/2504.05316
- Reference count: 40
- Primary result: Synthesizing modification text from image pairs improves CIR training data efficiency, achieving +2.39 Avg. recall on CIRR and +1.57 Avg. on FashionIQ benchmarks.

## Executive Summary
This paper addresses the challenge of composed image retrieval (CIR), which aims to search for an image of interest using a combination of a reference image and modification text. The main issue is the limited availability of training data and the laborious process of annotating triplets. To tackle this, the authors propose synthesizing training triplets to augment the training resource for CIR. Specifically, they train a modification text generator using large-scale multimodal models and scale up CIR learning through both pretraining and fine-tuning stages. During pretraining, they create Modification Text-oriented Synthetic Triplets (MTST) conditioned on pairs of images. For fine-tuning, they synthesize reverse modification text to connect the target image back to the reference image and employ a two-hop alignment strategy to bridge the semantic gap between the multimodal pair and the target image. The proposed methodology achieves competitive recall on both the CIRR and FashionIQ benchmarks.

## Method Summary
The approach involves three main stages: (1) Train an MTST generator by fine-tuning InstructBLIP-Vicuna-7b on annotated CIR triplets, freezing the image encoder and LLM while updating only the Q-Former; (2) Generate large-scale synthetic triplets (800K for CIRR, 580K for FashionIQ) by mining image pairs from source datasets and generating modification text via the trained MTST generator; (3) Pre-train a PTHA backbone on synthetic triplets with contrastive loss, then fine-tune with a two-hop alignment strategy that includes forward/reverse multimodal feature alignment (L_p2p) along with standard contrastive losses.

## Key Results
- Pre-training on CIRRMTST improves TIRG by +13.65 Avg. recall, ARTEMIS by +14.26, CLIP4CIR by +1.77, and SPRC by +1.15
- Two-hop alignment with L_p2p set to 0.5 provides optimal performance across models
- Generated text outperforms original text on validation sets (ROUGE-1: 0.248 vs 0.210, METEOR: 0.194 vs 0.186)
- Competitive performance on CIRR (+2.39 Avg.) and FashionIQ (+1.57 Avg.) benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Modification Text-oriented Synthetic Triplet (MTST) Generation
Generating modification text directly from image pairs produces higher-quality training triplets than caption-based or image-synthesis approaches. A fine-tuned InstructBLIP-Vicuna-7b model takes paired reference and target images, extracts visual features via a trainable Q-Former, and generates expressive modification text autoregressively. The generator learns from limited annotated triplets and then scales to large unlabeled image pairs. Text generation from image pairs has a smaller domain gap than image synthesis, and LLMs can capture fine-grained visual differences when conditioned on both images directly.

### Mechanism 2: Prototypical Two-Hop Alignment (PTHA)
Decomposing CIR into prototype learning plus modifier fusion improves alignment over single-hop fusion. First hop learns an implicit prototype by pulling reference→target and target→reference multimodal features together via L_p2p loss. Second hop fuses this prototype with modification text and aligns to target via contrastive loss. This captures what to preserve (implicit) before applying what to change (explicit). Modification text contains two separable signals—preserved features (implicit prototype) and changes—and modeling them sequentially reduces semantic gap.

### Mechanism 3: Two-Stage Pre-training → Fine-tuning with Synthetic Data
Pre-training on large-scale MTST before fine-tuning on benchmark data provides better initialization than fine-tuning alone. Pre-train on 800K (CIRR) or 580K (FashionIQ) synthetic triplets with contrastive loss. Fine-tune with PTHA on original benchmark. Synthetic data follows benchmark distribution by mining image pairs from same source datasets. Synthetic triplets approximate benchmark distribution sufficiently to transfer representations, without introducing systematic biases.

## Foundational Learning

- **Concept: Composed Image Retrieval (CIR)** - Why needed here: This is the core task—retrieving target images given (reference image + modification text). Understanding that modification text is relative, not absolute, is critical. Quick check question: Given a reference image of a red shirt and modification "make it blue," what should the target image contain?

- **Concept: Contrastive Learning with Triplet Loss** - Why needed here: All training objectives (L_q2t, L_t2t, L_p2p) are contrastive. Understanding positive/negative sampling in embedding space is essential. Quick check question: In a batch of 128 triplets, how many negative samples does each query implicitly compare against?

- **Concept: Q-Former / Query Transformer** - Why needed here: The MTST generator and PTHA both use Q-Former to bridge frozen vision encoders and LLMs. This is the trainable bottleneck. Quick check question: Why freeze the vision encoder and LLM while only training the Q-Former?

## Architecture Onboarding

- **Component map:** ViT-g/14 (frozen) → Q-Former (trainable, 32 tokens) → FC projection → Vicuna-7b (frozen) for MTST generator; ViT-g/14 (frozen) → BLIP-2 Q-Former (multimodal encoder) → BLIP-2 text encoder → contrastive alignment for PTHA

- **Critical path:** 1. Train MTST generator on annotated CIRR/FashionIQ triplets (only Q-Former updated) 2. Generate synthetic triplets by mining image pairs from same category/label 3. Pre-train PTHA backbone on synthetic triplets with L_q2t 4. Fine-tune with full PTHA loss (L_q2t + L_t2t + α·L_p2p, α=0.5)

- **Design tradeoffs:** Text generation vs. image synthesis: Text avoids visual domain gap but may hallucinate; image synthesis is grounded but visually inconsistent. Two-hop vs. single-hop: PTHA adds complexity but improves R@1 by ~1.5-2 points; may not justify cost for resource-constrained deployment. Domain-specific pre-training: CIRRMTST/FashionIQMTST boost in-domain performance but limit zero-shot generalization (Table IVf shows weaker transfer to CIRCO).

- **Failure signatures:** Low ROUGE/METEOR scores for generated text but high downstream performance → metrics don't capture CIR-relevant quality. Pre-training hurts fine-tuning → synthetic distribution mismatch; reduce pair sampling threshold or filter generated text. L_p2p causes collapse → gradient detachment on f_t2r may be broken; verify detach() is applied.

- **First 3 experiments:** 1. Train MTST generator on CIRR train set, generate 1K triplets, manually inspect for hallucinations and relevance. 2. Pre-train baseline (BLIP4CIR or SPRC) on CIRRMTST without PTHA; measure gain vs. no pre-training to isolate data contribution. 3. End-to-end PTHA with pre-training on FashionIQMTST; compare R@10/50 vs. SPRC baseline to validate domain transfer.

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: How can the MTST framework be adapted to improve cross-domain generalization and zero-shot performance on diverse datasets? The current method optimizes for specific benchmarks, creating a domain bias that hinders zero-shot transfer to out-of-domain datasets like CIRCO.

- **MLLM integration**: Can the modification text generation strategy be effectively integrated with powerful MLLMs without degrading their generalization capability? The specific fine-tuning for triplet generation appears to conflict with the pre-trained broad capabilities of these large models.

- **Fine-grained generation**: What architectural or training improvements are required to generate finer-grained modification texts while reducing factual errors? Current generation relies on standard instruction tuning which may lack the precision to capture subtle visual details without hallucination.

## Limitations

- Text generation quality vs. downstream impact: Low ROUGE/METEOR scores may not fully capture CIR-relevant text quality despite reported 4% hallucination rate in user studies
- Domain transfer constraints: Synthetic data improves in-domain performance but shows weaker zero-shot generalization to CIRCO
- Computational overhead: Two-hop alignment adds complexity with marginal gains, increasing training time and resource requirements

## Confidence

- **High Confidence**: The core claim that synthetic triplet generation via text improves CIR training data efficiency is well-supported by the 800K/580K triplet generation and downstream performance gains
- **Medium Confidence**: The two-hop alignment mechanism shows consistent improvements but lacks ablation studies isolating PTHA contribution from pre-training benefits
- **Medium Confidence**: The comparison to existing methods is valid within benchmark constraints, but the paper doesn't address potential biases introduced by synthetic data generation

## Next Checks

1. **Ablation Study**: Train a baseline PTHA model with pre-training on CIRRMTST but without the two-hop alignment (single-hop contrastive loss only). Measure whether gains are primarily from data scale or architectural innovation.

2. **Cross-Domain Transfer**: Evaluate the pre-trained PTHA model on a held-out CIR dataset (e.g., CIRCO) without fine-tuning to quantify zero-shot generalization. Compare against models trained only on benchmark data.

3. **Generated Text Quality Audit**: Sample 100 generated modification texts from CIRRMTST, have human annotators rate relevance and hallucination rate, and correlate with retrieval performance degradation in the corresponding triplets.