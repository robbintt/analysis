---
ver: rpa2
title: 'MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security'
arxiv_id: '2509.06807'
source_url: https://arxiv.org/abs/2509.06807
tags:
- mogu
- llms
- security
- arxiv
- moguv2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoGU V2, a framework designed to enhance
  the security of large language models (LLMs) while preserving their usability. The
  method employs a dynamic routing mechanism that balances the contributions of usability-optimized
  and security-optimized variants of the LLM.
---

# MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security

## Quick Facts
- arXiv ID: 2509.06807
- Source URL: https://arxiv.org/abs/2509.06807
- Reference count: 40
- MoGU V2 reduces average harmfulness score by 0.62-1.77 across mainstream, on-device, and reasoning LLMs without degrading usability

## Executive Summary
MoGU V2 is a framework that enhances LLM security by dynamically routing between security-optimized and usability-optimized model variants using intra-layer MLP routers. The method identifies layers encoding security features, embeds routers only in those locations, and jointly optimizes with backbone Q/K modules for bidirectional adaptation. Experiments show significant security improvements (up to 1.77 HS reduction) while maintaining task performance, with effectiveness across diverse LLM types from mainstream to on-device to reasoning models.

## Method Summary
MoGU V2 creates two LoRA variants of an LLM: Gladresp (trained to respond helpfully to all inputs) and Unwillresp (trained to reject all inputs). Intra-layer MLP routers are embedded in layers identified as encoding security features through probing classifiers. During training, routers learn to weigh outputs from both variants based on input intent, with joint optimization involving Q/K backbone modules. At inference, routing applies only to the first 5 tokens, balancing security gains with efficiency. The framework also introduces a data-mix strategy to mitigate security risks from instruction fine-tuning.

## Key Results
- Achieves average harmfulness score reduction of 0.62 for on-device LLMs and 1.77 for reasoning LLMs
- Maintains usability scores across all tested models, with only minor degradation on sub-1.5B parameter models (~0.2 US drop)
- Router placement in latter half layers reduces parameters by ~50% without performance loss
- Effectively mitigates security degradation from instruction fine-tuning while preserving task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intra-layer routers can dynamically sense input intent from hidden states and allocate weights between security-optimized and usability-optimized model variants.
- Mechanism: An MLP-based router reads hidden states at each target layer and outputs weights (w_glad, w_unwill) that blend outputs from Gladresp and Unwillresp variants. For benign inputs, higher weight flows to Gladresp; for malicious inputs, weight shifts to Unwillresp.
- Core assumption: Hidden states encode classifiable security features that correlate with input intent. Assumption: The router can learn this mapping from limited training data (600 examples).
- Evidence anchors:
  - [abstract] "routers are embedded only in layers encoding highly classifiable security features"
  - [Section III-A] Router function formalized with low-rank decomposition and sigmoid activation
  - [corpus] "Bleeding Pathways" paper confirms hidden state discriminability is central to jailbreak vulnerability—but also notes this discriminability can vanish, suggesting router effectiveness may degrade on certain attack types
- Break condition: If hidden states fail to encode distinguishable security features (e.g., adversarial perturbations collapse representations), routing becomes unreliable.

### Mechanism 2
- Claim: Security features concentrate in deeper layers, enabling selective router placement that reduces parameters by ~50% without performance loss.
- Mechanism: Probing classifiers (Eq. 2) evaluate security feature salience per layer. MoGU V2 embeds routers only in the latter half of layers where classification accuracy exceeds threshold.
- Core assumption: The layer-wise security feature distribution observed on probe data generalizes to unseen attacks.
- Evidence anchors:
  - [Section IV-A] "as the layer depth increases, security features become more prominent"
  - [Figure 4] Classification performance rises sharply in layers 20+ across Llama2, Vicuna, Qwen2, Mistral
  - [corpus] No direct corpus evidence on selective layer embedding for security; this appears novel
- Break condition: If security features shift to earlier layers under distribution shift (e.g., new attack families), router coverage becomes insufficient.

### Mechanism 3
- Claim: Joint optimization of routers with backbone Q/K modules enables bidirectional adaptation, improving both security and usability over router-only training.
- Mechanism: During router training, LoRA modules on Q and K are activated. This allows hidden states to adapt to router needs while routers adapt to hidden states.
- Core assumption: Q/K modules are the right targets for joint optimization—O module alone (used for variants) is insufficient.
- Evidence anchors:
  - [Section IV-A] "activating the Q and K modules within the LLM backbone...enabling joint optimization with routers"
  - [Table VIII] Ablation shows removing Q/K activation degrades ASR on 4/5 tested LLMs
  - [corpus] Multi-objective alignment papers (Hierarchical Experts, Pareto Optimality) emphasize tradeoff management but don't address architectural coupling specifically
- Break condition: If Q/K adaptation corrupts base model capabilities on non-security tasks, usability degrades.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) routing**
  - Why needed here: MoGU adapts MoE gating to blend security vs. utility experts dynamically.
  - Quick check question: Can you explain how MoE routing differs from simple ensemble averaging?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Used to create Gladresp/Unwillresp variants and activate Q/K modules without full fine-tuning.
  - Quick check question: What is the rank hyperparameter d_lora typically set to, and what does it control?

- Concept: **Probing classifiers**
  - Why needed here: Used to identify which layers encode security features for selective router placement.
  - Quick check question: What does a high probing accuracy at layer i indicate about that layer's hidden states?

## Architecture Onboarding

- Component map:
  Base LLM -> (Q/K modules activated with LoRA) -> (routers in latter half layers) -> Gladresp/Unwillresp variants -> blended output

- Critical path:
  1. Create variants: Train Gladresp on (X_m, Y_g) with contrastive loss; train Unwillresp on (X_b, Y_r) with contrastive loss
  2. Identify layers: Run probing classifiers to find high-accuracy layers (typically latter half)
  3. Train routers: Joint optimization with Q/K LoRA, using global-local loss (Eq. 1 + L1 regularization)
  4. Inference: First m tokens (default 5) use MoGU routing; remaining tokens use base LLM

- Design tradeoffs:
  - Router placement: Latter half layers → better security, but may miss early-layer attack signatures
  - Inference scope: Only first 5 tokens use MoGU → efficiency, but attacks emerging mid-generation are unchecked
  - Data-mix for IFT: Preserves task performance but slightly reduces security gains (Section VI-B)

- Failure signatures:
  - High rejection rate on benign inputs → router over-weighting Unwillresp; check L1 regularization weight λ
  - ASR unchanged on new attack types → security features may not transfer; consider expanding probe dataset
  - Usability drops on smaller LLMs (<1.5B) → limited capacity; paper notes ~0.2 US drop

- First 3 experiments:
  1. Replicate probing analysis (Section IV-A, Figure 4) on your target LLM to identify optimal router layers before full implementation.
  2. Ablate Q/K activation (Table VIII) to confirm its necessity for your model family.
  3. Test inference token budget: compare m=5 vs. m=10 to assess whether extended MoGU coverage improves security without excessive latency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the usability degradation observed in smaller-scale LLMs (e.g., sub-1.5B parameters) be mitigated when applying dynamic security routing?
- **Basis in paper:** [Explicit] Section V-C notes that for smaller models like Qwen2.5 0.5B, "a slight decline in usability can be observed... This underscores the greater challenge... and points to the need for further research in this area."
- **Why unresolved:** The current MoGU v2 framework optimizes for a Pareto frontier effectively in 7B models, but the constrained capacity of smaller models forces a trade-off that the current architecture does not fully resolve.
- **What evidence would resolve it:** Architectural modifications or loss functions specifically tailored for low-parameter models that maintain usability scores (US) comparable to the base model while retaining security gains.

### Open Question 2
- **Question:** Why does the relative importance of optimization components (contrastive loss, L1 norm, Q/K activation) vary inconsistently across different LLM families?
- **Basis in paper:** [Explicit] Section VII-B states, "We did not observe a consistent ranking of component importance across LLMs," noting that L1 norm is critical for Llama2 while contrastive loss is key for Vicuna.
- **Why unresolved:** The study empirically validates the utility of these components but lacks a theoretical explanation for why certain optimization strategies align better with specific model architectures or pre-training data distributions.
- **What evidence would resolve it:** A comparative analysis linking specific architectural features (e.g., attention head patterns) to the effectiveness of specific loss components (e.g., L1 norm regularization).

### Open Question 3
- **Question:** Does the reliance on a small, static dataset (600 instructions) limit the router's ability to generalize to novel or out-of-distribution adversarial attack vectors?
- **Basis in paper:** [Inferred] Section III-B highlights the use of "only 600 instructions" as a training mechanism. While efficient, it creates an assumption that the learned routing weights generalize beyond the specific malicious/benign examples provided in AdvBench and Alpaca.
- **Why unresolved:** The paper demonstrates robustness against known attack types (GCG, PAIR), but the router's decision boundary is derived from a fixed, limited sample of the malicious input space.
- **What evidence would resolve it:** Evaluation results against zero-day jailbreak prompts or adversarial examples structurally dissimilar to the 300 malicious instructions used for training.

## Limitations
- Security feature generalization may fail against adaptive attacks that collapse hidden state discriminability
- Only first 5 tokens receive security routing, creating vulnerability window for mid-generation attacks
- Selective layer placement assumes security features don't manifest in earlier layers, which may not hold for novel attack patterns

## Confidence
**High Confidence:**
- Router placement in latter half layers improves security while maintaining efficiency
- Q/K activation during joint optimization is necessary for bidirectional adaptation
- MoGU V2 achieves significant harmfulness reduction (HS drops of 0.62-1.77) across diverse LLM types

**Medium Confidence:**
- Data-mix strategy effectively mitigates IFT security degradation while preserving task performance
- Rule-based rejection works well on Llama2 but underperforms on larger models (Qwen2.5-72B)
- Security improvements transfer across mainstream, on-device, and reasoning LLMs

**Low Confidence:**
- Router effectiveness against adversarial perturbations that collapse hidden state discriminability
- Long-term generalization to attack families not represented in the 600-example training set
- Performance consistency on LLMs smaller than 1.5B parameters where paper notes ~0.2 US degradation

## Next Checks
1. **Adversarial Robustness Test**: Apply "vanishing discriminability" style attacks (hidden state perturbations) to verify router weight stability. If weights become uniform across benign/malicious inputs, security mechanism fails.

2. **Extended Inference Scope Experiment**: Compare m=5 vs m=10 token routing to quantify security-usability tradeoff from extending MoGU coverage. Measure ASR reduction vs US degradation.

3. **Cross-Domain Security Transfer**: Test MoGU V2 trained on Alpaca/AdvBench against completely different malicious instruction datasets (e.g., generated via different paradigms) to assess feature generalization limits.