---
ver: rpa2
title: 'VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making'
arxiv_id: '2505.03181'
source_url: https://arxiv.org/abs/2505.03181
tags:
- action
- arxiv
- learning
- agent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLM Q-Learning aligns vision-language models (VLMs) with interactive
  decision-making tasks by treating the model as an actor-critic RL agent that operates
  on token-level action spaces. It uses a learned critic to filter suboptimal actions
  during training, enabling VLMs to improve beyond their demonstrations while handling
  large token vocabularies and noisy syntax.
---

# VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making

## Quick Facts
- arXiv ID: 2505.03181
- Source URL: https://arxiv.org/abs/2505.03181
- Authors: Jake Grigsby; Yuke Zhu; Michael Ryoo; Juan Carlos Niebles
- Reference count: 40
- One-line primary result: Aligns VLMs with interactive decision-making using token-level RL and advantage-filtered fine-tuning

## Executive Summary
VLM Q-Learning aligns vision-language models with interactive decision-making by treating them as actor-critic RL agents operating on token-level action spaces. The method uses a learned critic to filter suboptimal actions during training, enabling VLMs to improve beyond their demonstrations while handling large token vocabularies and noisy syntax. Applied to two open-weight VLMs across three domains (Gym Cards, BabyAI, and MiniWoB), the method achieved significant gains in task success rates and action syntax accuracy compared to prompting alone.

## Method Summary
VLM Q-Learning extends VLMs with a critic head to perform offline-to-online RL on token-level action spaces. The approach converts turn-based interactions into token-level RL transitions, allowing granular credit assignment and syntax-aware filtering. During training, an advantage filter uses the critic's Q-value estimates to selectively imitate only high-value token actions, enabling the policy to exceed the performance of its training data. The method employs LoRA adapters for efficient fine-tuning and includes a gradual transition from offline demonstrations to online interaction.

## Key Results
- MoonDream2 and xGen-MM VLMs achieved significant gains in task success rates and action syntax accuracy compared to prompting alone
- In Gym Cards Blackjack, VLM Q-Learning improved from random agent baseline (0.2 success rate) to reference score (0.8) and achieved 100% valid action syntax
- The method matched or exceeded specialized models while maintaining the base VLM's general capabilities through LoRA adapters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Advantage-filtered supervised fine-tuning (AFSFT) enables VLMs to exceed the performance of their training demonstrations by selectively filtering suboptimal token-level actions.
- **Mechanism:** A learned Q-function (critic) estimates the expected return for each token in the vocabulary. During training, token actions are only imitated if their Q-value exceeds the weighted average Q-value by a margin β (the advantage threshold). This effectively converts standard SFT into an offline RL method that can learn from suboptimal or mixed-quality datasets.
- **Core assumption:** The Q-function can be trained to produce meaningful value estimates over large token vocabularies (|V| ≈ 25k–257k), and the advantage threshold β can be scheduled appropriately to transition from broad imitation to selective filtering.
- **Evidence anchors:** [abstract] "It uses a learned critic to filter suboptimal actions during training, enabling VLMs to improve beyond their demonstrations" [Section 5.2] "When Q-value estimation is successful, Eq. 4 creates a policy that outperforms its training data by declining to imitate action token outputs thought to be sub-optimal"
- **Break condition:** If the critic's Q-values remain miscalibrated for tokens outside the training dataset (due to the large vocabulary), the filter may pass random noise or reject genuinely good actions, degrading performance below SFT baseline.

### Mechanism 2
- **Claim:** Token-level action decomposition allows granular credit assignment within multi-turn agent interactions, enabling the critic to identify and reinforce syntactically correct action prefixes even when the complete turn is suboptimal.
- **Mechanism:** Turn-based transitions are converted to token-level RL transitions by extracting hidden representations for each action token. The critic learns to predict returns at each token position, allowing it to differentiate between syntactically correct prefixes (which may lead to valid actions) and syntactically incorrect paths. This addresses the exploration challenge where VLMs struggle to output valid action syntax.
- **Core assumption:** Hidden representations from the VLM contain sufficient information to predict future returns, and the TD target (bootstrapped from the next turn's state) provides a stable learning signal across the sparse-reward, multi-turn structure.
- **Evidence anchors:** [Section 5.2] "Converting from Turns to Tokens... We will think of VLMθ(oimgt, otextt, at) = ht as a sequence of RL states" [Section 3] "This 'token-based view' most closely aligns with how sequence models are trained"
- **Break condition:** If token-level rewards remain sparse (most intermediate tokens receive r=0) and the task horizon is long, TD learning may suffer from high variance, preventing the critic from learning useful value estimates.

### Mechanism 3
- **Claim:** The offline-to-online transition works because the filtered SFT objective maintains behavioral consistency with the dataset while progressively allowing the policy to deviate toward higher-value actions, avoiding the distribution-shift instability common in standard offline RL.
- **Mechanism:** Unlike conservative Q-learning methods that explicitly constrain the policy, AFSFT implicitly regularizes by only training on actions present in the dataset. When transitioning online, the replay buffer is filled with the policy's own experience, but the advantage filter continues to reject actions predicted to be low-value. This avoids the "maximization bias" problem where standard Q-learning overestimates Q-values for out-of-distribution actions.
- **Core assumption:** The token-level action space is large enough that random exploration rarely discovers high-value actions, making dataset-constrained learning a necessity rather than a limitation; and the VLM's existing language capabilities provide a reasonable prior over syntactically valid outputs.
- **Evidence anchors:** [Section 5.2] "Token action spaces are so large that they create a unique situation where this constraint is necessary even when we do allow for online interaction" [Section 6] "In RL, it is not uncommon to see a dip in performance during the transition between offline and online learning... but Eq. 4 avoids many of these obstacles, and we do not observe instability"
- **Break condition:** If online exploration discovers high-value actions that were absent from the offline dataset and the critic's Q-values for those actions remain untrained (effectively random), the advantage filter will incorrectly reject them, preventing the policy from benefiting from online learning.

## Foundational Learning

- **Concept:** Q-learning and temporal difference (TD) learning
  - **Why needed here:** The critic is trained with one-step TD loss to estimate Q-values. Understanding bootstrapping, the discount factor γ, and the bias-variance tradeoff in TD targets is essential to diagnose critic convergence issues.
  - **Quick check question:** Given a trajectory with rewards [0, 0, 0, +1], what Q-value should the critic predict for the first state if γ=0.99?

- **Concept:** Actor-critic architecture
  - **Why needed here:** The method adds a critic head to the VLM and trains both actor (language head) and critic jointly. Understanding the separation of concerns—critic estimates value, actor maximizes it—is necessary to debug training dynamics.
  - **Quick check question:** If the critic is overestimating Q-values for all actions, how would this affect the advantage filter and the resulting policy?

- **Concept:** LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
  - **Why needed here:** LoRA is used not just for efficiency but to preserve the base VLM's general capabilities. Understanding how LoRA adapters work and how they can be discarded to recover the base model is operationally important.
  - **Quick check question:** If you want to switch between a task-specific RL-finetuned VLM and the original base VLM at inference time, what would you need to store and swap?

## Architecture Onboarding

- **Component map:**
  - VLM backbone (frozen or LoRA-adapted) -> processes image + text observations, outputs hidden representations
  - LoRA adapters -> trainable low-rank matrices applied to attention layers
  - Actor head (L_φ) -> existing language modeling head, outputs token probability distribution over vocabulary |V|
  - Critic head (Q_ψ) -> new MLP output head, predicts Q-values for each token in vocabulary |V|
  - Token-level transition converter -> extracts hidden states for each action token, constructs RL transitions with sparse intermediate rewards
  - Advantage filter -> compares Q(s,a) to weighted average Q(s,·) to determine which tokens to include in actor loss

- **Critical path:**
  1. Initialize dataset D with offline demonstrations (can include random, suboptimal, or mixed-quality data)
  2. Forward pass through VLM to get hidden representations for observation + action sequence
  3. Compute critic loss (TD error) and actor loss (filtered cross-entropy) jointly
  4. Schedule β from -∞ (imitate all tokens) to 0 (filter suboptimal tokens) over training
  5. Optionally: transition to online data collection by sampling from current policy with ε-greedy exploration

- **Design tradeoffs:**
  - **Token-level vs. turn-based actions:** Token-level enables granular filtering but requires critic to output |V|-dimensional values; turn-based would be simpler but cannot filter within a response
  - **LoRA vs. full fine-tuning:** LoRA preserves base model and enables task switching but may limit expressivity for learning new action patterns
  - **Offline-first vs. online-only:** Offline-first provides stability and addresses syntax learning; online-only risks instability in large token action spaces
  - **Critic MLP size:** Paper uses hidden dim = d_model (VLM hidden dimension) for Gym Cards/BabyAI, 400 for MiniWoB; larger critic may improve value estimation but increases memory

- **Failure signatures:**
  - **Critic loss plateaus but actor performance degrades:** Advantage filter may be too aggressive (β too high) or Q-values are miscalibrated for out-of-dataset tokens
  - **Action syntax accuracy remains low after training:** Offline dataset may lack sufficient syntax diversity; consider increasing ε-greedy exploration or mixing in syntax-only demonstrations
  - **Performance drops during offline-to-online transition:** Replay buffer may be dominated by early, low-quality online samples; consider buffer prioritization or slower β schedule

- **First 3 experiments:**
  1. **Replicate Gym Cards Blackjack baseline:** Train MoonDream2 or xGen-MM on the offline dataset (28.8k transitions) with β schedule from -∞ to 0 over 75 epochs. Verify that task success rate approaches reference score (~0.8) and action syntax accuracy reaches ~100%. This validates the core AFSFT mechanism.
  2. **Ablate the advantage filter:** Compare AFSFT (β=0) against: (a) standard SFT (β=-∞, λ=0), (b) EPG objective (direct Q-maximization), (c) random filter. Expected result: AFSFT > SFT on suboptimal datasets; EPG fails due to Q-miscalibration; random filter ≈ SFT subset.
  3. **Test offline-to-online transition stability:** After offline training converges, enable online data collection with ε-greedy exploration (ε decaying from 1.0 to 0.1 over 300 epochs). Monitor for performance dips and recovery. Expected result: no significant instability (as reported); if instability occurs, investigate replay buffer composition and β re-scheduling.

## Open Questions the Paper Calls Out

- **Question:** Does VLM Q-Learning scale effectively to VLMs larger than 4.6B parameters?
- **Basis in paper:** [explicit] The authors limit experiments to "small enough" models (MoonDream2, xGen-MM) due to GPU constraints (Section 2).
- **Why unresolved:** The computational overhead of the critic head and memory requirements for larger architectures (e.g., 7B-70B) were not tested.
- **Evidence:** Evaluating success rates and training stability on larger open-weight VLMs (e.g., LLaVA-Next).

## Limitations

- **Token-level RL generalization uncertainty:** The method's success relies on the critic learning meaningful Q-values across a vocabulary of 25k-257k tokens, but this is not validated for out-of-distribution tokens.
- **Syntax learning attribution unclear:** The paper does not demonstrate whether syntax learning comes from the advantage filter or simply from the initial SFT warmup phase.
- **Offline-to-online stability claim under-supported:** The assertion of inherent stability is based on limited evidence (Gym Cards only) and contradicts common RL experience with distribution shift.

## Confidence

- **High confidence:** The core AFSFT mechanism is well-specified and the training procedure is reproducible.
- **Medium confidence:** The claim that VLM Q-Learning enables VLMs to "improve beyond their demonstrations" is supported by Blackjack results but generalizability is uncertain.
- **Low confidence:** The assertion that AFSFT inherently avoids offline-to-online instability is based on limited evidence and contradicts common RL experience.

## Next Checks

1. **Critic Q-value calibration test:** Extract the critic's Q-value predictions for all tokens in the vocabulary on a held-out validation set. Plot the distribution of Q-values for tokens that appear in the dataset versus tokens that don't. If the distributions are indistinguishable, the critic is not learning meaningful value estimates, undermining the entire method.

2. **Syntax learning ablation:** Train the same VLM using only the initial SFT phase (β=-∞ for all epochs) on the mixed-quality dataset. Compare action syntax accuracy to the full AFSFT method. If syntax accuracy is similar, the advantage filter is not contributing to syntax learning as claimed.

3. **Offline-to-online instability stress test:** After training on the Gym Cards offline dataset, transition to online learning with high exploration (ε=1.0) for 10,000 steps. Monitor task success rate and Q-value statistics. If performance drops significantly and recovers slowly, the claim of inherent stability is false and the method requires additional stabilization techniques.