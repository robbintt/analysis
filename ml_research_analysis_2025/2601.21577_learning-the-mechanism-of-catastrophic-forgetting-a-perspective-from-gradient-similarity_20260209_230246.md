---
ver: rpa2
title: 'Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient
  Similarity'
arxiv_id: '2601.21577'
source_url: https://arxiv.org/abs/2601.21577
tags:
- forgetting
- knowledge
- neurons
- catastrophic
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for catastrophic
  forgetting in LLMs during knowledge injection. The authors prove that strongly negative
  gradient similarity between new and previously learned knowledge is the root cause
  of forgetting.
---

# Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity

## Quick Facts
- **arXiv ID**: 2601.21577
- **Source URL**: https://arxiv.org/abs/2601.21577
- **Reference count**: 40
- **Primary result**: Gradient similarity analysis reveals conflicting neurons cause catastrophic forgetting; CNL reduces forgetting by 59.1%-81.7% in practice

## Executive Summary
This paper provides a theoretical explanation for catastrophic forgetting in LLMs during knowledge injection, proving that strongly negative gradient similarity between new and previously learned knowledge is the root cause. Through gradient analysis, they identify two types of neurons: conflicting neurons (50%-75%) that induce forgetting and collaborative neurons (25%-50%) that mitigate it. Based on this insight, they propose Collaborative Neural Learning (CNL), which freezes conflicting neurons and updates only collaborative neurons. Theoretically, CNL prevents catastrophic forgetting under infinitesimal learning rates and known mastered sets. Experiments on five LLMs, four datasets, and four optimizers show CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings, while maintaining comparable learning efficiency to standard fine-tuning.

## Method Summary
The method involves computing per-neuron gradient similarity between Mastered Set (M) and Injection Set (I) to classify neurons as conflicting (s<0) or collaborative (s≥0). CNL then freezes conflicting neurons (50-75% of parameters) and updates only collaborative neurons via masked gradient descent. The theoretical guarantee of zero forgetting requires infinitesimal learning rates and exact knowledge of the Mastered Set. In practice, out-of-set settings approximate M, achieving 59.1%-81.7% forgetting reduction. The approach is evaluated across Qwen2.5 (1.5B/3B/7B) and LLaMA3.2 (1B/3B) models on MMLU, MedQA, ARC-C, and CSQA datasets using FP32 training over 25 epochs.

## Key Results
- Gradient similarity analysis identifies 50-75% of neurons as "conflicting" (causing forgetting) and 25-50% as "collaborative" (mitigating forgetting)
- CNL theoretically eliminates catastrophic forgetting under infinitesimal learning rates and known Mastered Sets
- In out-of-set settings, CNL reduces forgetting by 59.1%-81.7% compared to standard fine-tuning
- Maintains comparable learning efficiency to standard fine-tuning while preventing catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Gradient Similarity as a Forgetting Predictor
The core claim is that catastrophic forgetting is fundamentally driven by strongly negative gradient similarity between the Injection Set (I) and the Mastered Set (M). Under first-order Taylor expansion, the loss change on Mastered Set is defined as ΔL_M = -η S(M, I). If gradient similarity S(M, I) is negative, the optimization step taken to minimize loss on I inherently increases loss on M. This holds when learning rate η is infinitesimal, ensuring the linear approximation is accurate. The "Mechanistic Analysis of Catastrophic Forgetting..." neighbor paper supports the general premise of analyzing forgetting via optimization dynamics, though the specific gradient similarity metric is unique to this work. If the learning rate is too large, the linear approximation fails, and the correlation between negative similarity and immediate forgetting may weaken due to higher-order curvature effects.

### Mechanism 2: Neuron Role Decomposition (Conflicting vs. Collaborative)
Forgetting is not uniform across parameters but is localized to specific "Conflicting Neurons" (50%-75% of the network), while "Collaborative Neurons" (25%-50%) naturally resist it. Global gradient similarity decomposes into the sum of per-neuron similarities s_θj(M, I). Conflicting neurons have s_θj < 0 (pushing mastery loss up), while collaborative neurons have s_θj ≥ 0 (pushing mastery loss down). The sheer volume of conflicting neurons overpowers the collaborative ones in standard fine-tuning. The paper treats individual parameters as the unit of analysis for gradient contribution. Neighbor papers generally discuss parameter isolation (e.g., LoRA) but do not explicitly categorize neurons by gradient similarity sign. If gradients are noisy or batch size is too small, the estimated role of a neuron might fluctuate, leading to unstable classification.

### Mechanism 3: Selective Plasticity via Masking (CNL)
Forgetting can be theoretically eliminated by freezing conflicting neurons and updating only collaborative ones. The method applies an indicator mask I[S(M,I) ≥ 0] to the gradient update. This ensures that the summation in the loss change equation contains only non-negative terms, mathematically guaranteeing ΔL_M ≤ 0. The theoretical proof relies on the assumption that the gradient sign observed at the start of the update remains valid throughout the update step (requiring small η). The method theoretically eliminates catastrophic forgetting under an infinitesimal learning rate and an exactly known mastered set. KORE mentions constraints for knowledge injection, but CNL's specific masking mechanism is distinct. If the Mastered Set M is unknown or estimated poorly (out-of-set setting), the mask is calculated on proxy data, introducing estimation error that reduces efficacy (forgetting is reduced by 59.1%-81.7% rather than eliminated).

## Foundational Learning

- **Concept: First-Order Taylor Expansion**
  - **Why needed here:** The entire theoretical proof relies on approximating the change in loss (ΔL) as a linear function of the gradient. Without this, the link between gradient similarity and immediate loss increase is not provable.
  - **Quick check question:** How does increasing the learning rate affect the validity of the Taylor approximation used to derive ΔL_M?

- **Concept: Gradient Inner Product (Similarity)**
  - **Why needed here:** This is the metric used to classify neurons. One must understand that a positive inner product implies alignment (mutual benefit), while a negative one implies opposition (interference).
  - **Quick check question:** If the gradients for Task A and Task B are orthogonal (S=0), does CNL predict forgetting or retention?

- **Concept: Hadamard (Element-wise) Product**
  - **Why needed here:** The CNL update rule (Eq. 19) applies a binary mask to the gradient vector. Understanding element-wise multiplication is required to implement the selective freezing mechanism.
  - **Quick check question:** In the CNL update rule θ ← θ - η I ⊙ ∇L, what happens to a parameter θ_j if its similarity score is negative?

## Architecture Onboarding

- **Component map:** Mastered Set (M) Buffer -> Dual-Gradient Engine -> Neuron Classifier -> Masked Optimizer -> Injection Set (I) Loader

- **Critical path:** The bottleneck is the Dual-Gradient Engine. Standard training computes one backward pass. CNL requires gradients w.r.t M to calculate the mask before the optimizer step for I. If M is large, this is computationally expensive.

- **Design tradeoffs:**
  - In-set vs. Out-of-set: Strict CNL requires knowing M perfectly. In practice (Out-of-set), you must approximate M with a subset, trading theoretical guarantees (zero forgetting) for empirical reduction (59%-81% reduction).
  - Plasticity Loss: By freezing ~50-75% of neurons, the model's capacity to learn complex new features in I might be constrained if the "Collaborative" neurons are insufficient for the task.

- **Failure signatures:**
  - Mask Flapping: If a neuron oscillates between "Conflicting" and "Collaborative" across steps, training becomes unstable.
  - Stagnation: If the network has too few collaborative neurons for the new task, the loss on I stops decreasing.
  - Memory Overrun: Storing the full Mastered Set M for gradient calculation is often infeasible; subsampling M poorly leads to higher forgetting than predicted.

- **First 3 experiments:**
  1. Baseline Gradient Correlation: Fine-tune on I and plot the histogram of per-neuron gradient similarities (s_θj) using a small held-out set M. Verify the 50%/50% split claimed in Table 2.
  2. Ideal CNL (In-Set): Implement the mask using the exact test set M (cheating scenario) to verify if "Zero Forgetting" is achievable on a small model (e.g., Qwen 1.5B).
  3. Optimizer Ablation: Implement CNL with Adam vs. SGD. Check if the adaptive learning rates in Adam require a modified definition of "similarity" or if the standard projection works as per Appendix G.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical guarantee of zero forgetting be extended to finite learning rates η, rather than relying on the assumption of an infinitesimal η?
- **Basis in paper:** The authors state in Section 5.1 that CNL "theoretically eliminates catastrophic forgetting under the assumption of an infinitesimal learning rate η," and highlight this as a restrictive condition in the conclusion.
- **Why unresolved:** The theoretical proof relies on a first-order Taylor expansion (Eq. 4-5), which is valid only for infinitesimal steps. Standard training uses finite steps, introducing approximation errors not covered by the current theory.
- **What evidence would resolve it:** A theoretical derivation providing bounds on forgetting for finite η, or empirical analysis showing the threshold learning rate beyond which CNL fails to prevent forgetting.

### Open Question 2
- **Question:** How can zero forgetting be achieved in out-of-set settings where the Mastered Set M is not explicitly known or cannot be fully computed?
- **Basis in paper:** The paper notes in Section 5.4 that in out-of-set settings (where M is estimated), CNL reduces forgetting by 59.1%-81.7% but "zero forgetting cannot be fully achieved" due to estimation error.
- **Why unresolved:** The method currently requires exact gradient calculations on the Mastered Set to identify conflicting neurons. When M is unknown or approximated by a subset, the gradient similarity calculation is noisy, failing to guarantee loss reduction.
- **What evidence would resolve it:** A method to approximate the Mastered Set or its gradients that preserves the theoretical guarantee of monotonic loss decrease (Eq. 21).

### Open Question 3
- **Question:** Is the binary classification of neurons into "conflicting" and "collaborative" types stable over sequential training steps?
- **Basis in paper:** The method relies on the indicator function I (Eq. 17) calculated at the current time step. It assumes that freezing a neuron currently identified as "conflicting" remains beneficial as the collaborative neurons continue to update and shift the loss landscape.
- **Why unresolved:** As collaborative neurons are updated, the gradient ∇L_M(θ) changes. A neuron identified as conflicting at step t might become collaborative at step t+1, potentially hindering learning efficiency if permanently frozen during a session.
- **What evidence would resolve it:** Analysis of the temporal consistency of neuron types across epochs, specifically measuring the rate at which neurons flip classification categories.

## Limitations

- The theoretical guarantee of zero forgetting depends critically on knowing the exact Mastered Set and using infinitesimal learning rates, creating a significant gap between theory and practice.
- The computational overhead of dual-gradient computation and the potential loss of plasticity from freezing 50-75% of neurons remain practical concerns not fully addressed.
- In out-of-set settings where M must be approximated, CNL achieves substantial but incomplete forgetting reduction (59.1%-81.7%), falling short of the theoretical guarantee.

## Confidence

- **High Confidence**: The core theoretical framework linking negative gradient similarity to forgetting is internally consistent and mathematically sound under stated assumptions (Assumption 1: infinitesimal learning rate; Assumption 2: known Mastered Set).
- **Medium Confidence**: The empirical claim of 59.1%-81.7% forgetting reduction in out-of-set settings is supported by experiments, but the exact conditions and baselines for comparison require verification.
- **Low Confidence**: The practical impact of freezing 50-75% of neurons on learning efficiency and the generalizability of the neuron decomposition (conflicting vs. collaborative) across diverse tasks remain underexplored.

## Next Checks

1. **Taylor Approximation Validity**: Systematically vary the learning rate (e.g., 1e-5, 1e-3, 1e-2) and measure the correlation between gradient similarity and immediate forgetting. This will quantify when the first-order approximation breaks down and explain the performance gap between in-set (theoretical zero) and out-of-set (partial reduction) settings.

2. **Neuron Classification Stability**: Track the fraction of neurons that flip between conflicting and collaborative roles across training epochs. High instability would invalidate the static masking strategy and suggest a need for dynamic re-classification.

3. **Baseline Ablation Study**: Compare CNL against simpler methods like rehearsal (replaying M during I training) and EWC (elastic weight consolidation). This will isolate whether the gradient similarity mechanism provides unique benefits or if existing methods achieve similar results with less complexity.