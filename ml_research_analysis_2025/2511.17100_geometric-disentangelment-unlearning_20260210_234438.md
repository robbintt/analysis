---
ver: rpa2
title: Geometric-disentangelment Unlearning
arxiv_id: '2511.17100'
source_url: https://arxiv.org/abs/2511.17100
tags:
- unlearning
- retain
- forget
- forgetting
- simnpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the persistent trade-off in machine unlearning
  between effective forgetting of sensitive data and preservation of useful retained
  knowledge. Existing methods often suffer from heuristic designs and collateral degradation
  of retained capabilities.
---

# Geometric-disentangelment Unlearning

## Quick Facts
- arXiv ID: 2511.17100
- Source URL: https://arxiv.org/abs/2511.17100
- Reference count: 40
- This work introduces Geometric-disentanglement Unlearning (GU), achieving up to 62% improved forgetting and 31% higher retention while maintaining model utility across TOFU, MUSE, and WMDP-cyber benchmarks.

## Executive Summary
Geometric-disentanglement Unlearning (GU) addresses the persistent trade-off in machine unlearning between effective forgetting of sensitive data and preservation of useful retained knowledge. Existing methods often suffer from heuristic designs and collateral degradation of retained capabilities. GU introduces a theoretically grounded projection method that enforces local retain-invariance by constraining forgetting updates to be orthogonal to the retain-gradient subspace under the optimizer's geometry. This approach provably prevents first-order harm to retained data while preserving strong forgetting. When added to SimNPO, GU achieves up to 62% improved forgetting Extraction Strength and 31% higher retention Extraction Strength across TOFU, MUSE, and WMDP-cyber benchmarks, while maintaining or improving model utility and privacy. GU provides a lightweight, plug-and-play framework with formal guarantees for controlled unlearning.

## Method Summary
GU projects forget gradients onto the H-orthogonal complement of the retain-gradient subspace under the optimizer's metric H. At each training step, it computes forget gradient g_f and retain gradient g_r from a KL-based retain anchor L_r = E_{x∈D_r}[KL(π_θ(·|x)||π_ref(·|x))]. The gradients are whitened using the optimizer's second-moment state (W = diag(1/√(ṽ+ε))), then g_f is decomposed into tangential and normal components relative to the retain subspace U. Only the normal component P_⊥^H g_f is used for updates, with an optional sign-aware gate to preserve beneficial tangential components. A trust-region constraint κ limits the energy of kept tangential components. The method maintains a low-rank orthonormal basis U for the retain subspace using streaming Gram-Schmidt updates from retain-gradient passes.

## Key Results
- Up to 62% improved forgetting Extraction Strength when added to SimNPO
- 31% higher retention Extraction Strength while maintaining model utility
- Formal theoretical guarantees on local retain-invariance under optimizer geometry

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Updates that are H-orthogonal to the retain-gradient subspace cause zero first-order change to the retain loss.
- **Mechanism:** The paper proves that for any update direction Δθ, the first-order retain loss change Δ⁽¹⁾Lᵣ = ⟨∇ᵧLᵣ, Δθ⟩_H equals zero if and only if Δθ lies in the H-orthogonal complement of Tᵣ = span{∇ℓᵣ(xᵣ; θ)}. By projecting forget gradients onto this complement, the method removes the "retain-harmful" tangential component while preserving the "retain-invariant" normal component.
- **Core assumption:** The first-order Taylor approximation holds for small parameter updates, and the optimizer-induced metric H remains approximately constant within an optimization step.
- **Evidence anchors:**
  - [abstract] "the retain loss is locally invariant if and only if the update direction is orthogonal to the subspace spanned by retain gradients"
  - [Proposition 3.1, Page 3] Formal equivalence between local retain invariance and H-orthogonality to Tᵣ
  - [corpus] Related work OrthoGrad enforces Euclidean orthogonality; GU differs by using optimizer-induced H-geometry and operates as a lightweight plug-in for LLM-scale unlearning
- **Break condition:** If forget gradient g_f lies entirely within Tᵣ (g_f ∈ Tᵣ), then the projected component is zero and no retain-invariant descent direction exists—reflecting a fundamental local trade-off rather than method failure.

### Mechanism 2
- **Claim:** Within the retain-invariant feasible set, the projected forget gradient P⊥_H g_f achieves the steepest descent for both the forget and joint objectives.
- **Mechanism:** Lemma 3.2 shows that minimizing ⟨g_f, Δθ⟩_H subject to Δθ ∈ Tᵣ⊥ yields Δθ* = −P⊥_H g_f / ||P⊥_H g_f||_H. Since Tᵣ⊥ and Tᵣ are H-orthogonal, the retain term in the joint objective vanishes, making the forget-only steepest direction also optimal for L_joint.
- **Core assumption:** A trust-region constraint limits step magnitude, and P⊥_H g_f ≠ 0 (non-degenerate case).
- **Evidence anchors:**
  - [Lemma 3.2, Page 4] "the direction achieving the largest first-order decrease of L_f over C is −P⊥_H g_f / ||P⊥_H g_f||_H"
  - [Corollary 3.4, Page 5] Descent guarantee under H-smoothness with explicit step-size condition
  - [corpus] Weak direct corpus comparison; GU's optimality proof is specific to this paper's theoretical framework
- **Break condition:** If smoothness assumption is violated (e.g., pathological loss landscape), the second-order O(ρ²) term may dominate and actual Lᵣ descent is not guaranteed.

### Mechanism 3
- **Claim:** Using the optimizer's preconditioning metric (Adam's diagonal Hessian approximation) rather than Euclidean geometry better aligns projection with actual parameter dynamics.
- **Mechanism:** Adam's second-moment accumulator ṽ defines W = diag(1/√(ṽ+ε)), yielding H = WᵀW. Projecting in this whitened space accounts for per-parameter scaling, so orthogonality matches the optimizer's effective step directions rather than raw gradient magnitudes.
- **Core assumption:** The diagonal approximation captures the dominant curvature; off-diagonal correlations are secondary.
- **Evidence anchors:**
  - [Page 3, footnote 1] "adaptive methods such as AdaGrad and Adam act as diagonal preconditioners"
  - [Appendix A.1, Page 13] Explicit construction of H from Adam's v̂
  - [corpus] No direct corpus comparison on optimizer-aware geometry in unlearning; this appears novel to GU
- **Break condition:** If true curvature has strong off-diagonal structure (e.g., correlated parameter groups), the diagonal approximation may misalign the retain subspace, reducing effectiveness.

## Foundational Learning

- **Concept: First-order Taylor expansion and directional derivatives**
  - Why needed here: The core guarantee relies on Δ⁽¹⁾Lᵣ = ⟨∇Lᵣ, Δθ⟩_H = 0 as the condition for "no first-order harm"
  - Quick check question: Given gradient g and direction d, what is the first-order change in loss L along d?

- **Concept: Orthogonal projection onto subspaces**
  - Why needed here: Computing P⊥_H g_f requires understanding how to decompose vectors into tangent and normal components relative to a subspace spanned by retain gradients
  - Quick check question: Given basis U spanning subspace T, write the projector onto T and its orthogonal complement.

- **Concept: Preconditioned gradient descent (Adam/AdaGrad intuition)**
  - Why needed here: The H-metric comes from optimizer state; understanding why this matters for step direction orthogonality
  - Quick check question: Why does Adam scale gradients differently per-parameter, and how does this affect what "orthogonal" means?

## Architecture Onboarding

- **Component map:** Retain basis maintainer (U) -> Projection layer -> Sign-aware gate -> Trust-region controller -> Optimizer shim

- **Critical path:** Backward pass → recover g_f from total gradient → whiten using Adam's v̂ → project onto Tᵣ⊥ → de-whiten → overwrite p.grad → optimizer.step()

- **Design tradeoffs:**
  - Basis rank k: Higher k captures more retain directions but increases O(kd) cost; k=8–16 sufficient in experiments
  - Refresh frequency: More frequent basis updates improve subspace accuracy but require extra backward passes
  - Tensor selection: Projecting only last K blocks reduces cost but may miss early-layer entanglement; paper uses last K + final norm + output head

- **Failure signatures:**
  1. ESᵤₙ stays near baseline but ESᵣᵉ degrades → subspace Tᵣ poorly estimated (increase k or refresh frequency)
  2. Model utility collapses → trust-region κ too loose; tighten or reduce learning rate
  3. Privacy leakage remains high → forget objective insufficiently aggressive; GU only removes retain-interfering component
  4. NaN/inf in whitening → Adam v̂ contains zeros/near-zeros; check ε parameter

- **First 3 experiments:**
  1. Reproduce TOFU forget-5% with SimNPO+GU on Llama-3.2-1B: verify ESᵤₙ ↓ while ESᵣᵉ ↑ compared to SimNPO baseline
  2. Ablate metric H: Compare Adam-diagonal vs. Euclidean (H=I) projection; expect Adam-diagonal to outperform on retention
  3. Sweep basis rank k ∈ {4, 8, 16, 32} while fixing other hyperparameters; expect diminishing returns beyond k=16

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating higher-order parameter correlations into the optimizer metric $H$ improve disentanglement fidelity compared to the standard diagonal approximation?
  - Basis in paper: [Explicit] Page 8 states the method approximates the Riemannian metric using diagonal information from optimizers, which "ignores higher-order parameter correlations that could theoretically offer finer-grained control."
  - Why unresolved: Computational constraints on LLMs necessitated the diagonal approximation, leaving the theoretical benefit of full curvature information unverified.
  - What evidence would resolve it: Experiments on smaller models comparing GU performance using diagonal Adam preconditioners versus block-diagonal or K-FAC approximations.

- **Open Question 2:** How can the retain-gradient subspace be accurately constructed when representative retain data is unavailable?
  - Basis in paper: [Explicit] Page 8 lists as a limitation: "In scenarios where retain data is unavailable, constructing an accurate $T_r$ becomes challenging."
  - Why unresolved: The core theoretical guarantee relies on orthogonality to the subspace spanned by retain gradients; without $D_r$, the projection target is undefined.
  - What evidence would resolve it: A method for synthesizing or approximating the retain subspace (e.g., using activation statistics or synthetic prompts) that maintains retention performance without access to ground-truth retain samples.

- **Open Question 3:** Does the first-order retain-invariance guarantee persist under the large parameter shifts required for aggressive unlearning?
  - Basis in paper: [Inferred] The theoretical analysis (Proposition 3.1) relies on a first-order approximation $\Delta^{(1)}L_r$ which assumes "small parameter updates."
  - Why unresolved: While the paper tests forgetting 10% of data, it is unclear if the linear approximation holds for larger, more disruptive unlearning operations where second-order effects might dominate.
  - What evidence would resolve it: Analysis of second-order Taylor expansion terms during unlearning to verify that the projected update effectively suppresses higher-order drift on the retain set.

## Limitations

- The theoretical guarantees rely on first-order Taylor approximations that may not hold for large parameter updates in non-convex loss landscapes.
- The Adam diagonal preconditioning assumption ignores off-diagonal curvature, potentially limiting effectiveness when parameters exhibit strong correlations.
- The method requires maintaining a retain-basis through additional backward passes, introducing computational overhead that scales with basis rank and refresh frequency.

## Confidence

- **High Confidence:** The geometric formulation and projection mechanics are mathematically sound, with clear derivations for H-orthogonality and retain-invariance conditions.
- **Medium Confidence:** The empirical improvements on benchmark tasks are well-documented, but the comparison set is limited to specific model sizes and datasets.
- **Medium Confidence:** The claim of being a "plug-and-play" addition to existing unlearning frameworks is supported by the lightweight nature of the projection, though integration complexity may vary.

## Next Checks

1. **Robustness to Basis Construction:** Systematically vary basis rank (k ∈ {4, 8, 16, 32}) and refresh frequency across multiple model sizes to quantify the trade-off between computational cost and unlearning performance, particularly examining failure modes when the retain subspace becomes poorly estimated.

2. **Generalization to Non-English Benchmarks:** Apply GU to multilingual unlearning tasks (e.g., cross-lingual author removal or topic forgetting) to test whether the geometric approach generalizes beyond the English-centric TOFU, MUSE, and WMDP-cyber benchmarks.

3. **Large-Scale Model Validation:** Evaluate GU on frontier models (>30B parameters) to verify that the diagonal preconditioning assumption remains valid and that the computational overhead remains manageable at scale, checking for any degradation in theoretical guarantees with increased model complexity.