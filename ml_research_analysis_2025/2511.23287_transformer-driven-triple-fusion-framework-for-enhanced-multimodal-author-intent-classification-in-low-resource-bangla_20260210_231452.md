---
ver: rpa2
title: Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent
  Classification in Low-Resource Bangla
arxiv_id: '2511.23287'
source_url: https://arxiv.org/abs/2511.23287
tags:
- intent
- fusion
- bangla
- multimodal
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces BangACMM, a multimodal framework for Bangla
  author intent classification leveraging transformer-based text models (mBERT, DistilBERT,
  XLM-RoBERTa) and vision transformers (ViT, Swin, SwiftFormer). The framework introduces
  an intermediate fusion strategy that concatenates text and image features before
  classification, outperforming traditional early and late fusion approaches.
---

# Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla

## Quick Facts
- **arXiv ID:** 2511.23287
- **Source URL:** https://arxiv.org/abs/2511.23287
- **Reference count:** 19
- **Primary result:** BangACMM framework achieves 84.11% macro-F1 on Bangla multimodal author intent classification, outperforming prior approaches by 8.4 percentage points

## Executive Summary
This study introduces BangACMM, a multimodal framework for Bangla author intent classification that leverages transformer-based text models (mBERT, DistilBERT, XLM-RoBERTa) and vision transformers (ViT, Swin, SwiftFormer). The framework introduces an intermediate fusion strategy that concatenates text and image features before classification, outperforming traditional early and late fusion approaches. Experiments on the Uddessho dataset (3,048 posts, six intent categories) demonstrate that multimodal fusion significantly improves performance, with mBERT+Swin achieving 84.11% macro-F1 score—an 8.4 percentage-point improvement over prior Bangla multimodal approaches. The results establish new benchmarks for low-resource language multimodal intent classification, showing that intermediate-level feature integration optimally balances modality-specific and cross-modal learning.

## Method Summary
The framework combines transformer-based text encoders (mBERT, DistilBERT, XLM-RoBERTa) with vision transformers (ViT, Swin, SwiftFormer) for multimodal author intent classification in Bangla. Text features are extracted using the [CLS] token embedding (768 dimensions), while images are processed through Swin Transformer. The key innovation is intermediate fusion—concatenating text and image features before passing them through a classification head. The model is trained on the Uddessho dataset with 3,048 paired text-image posts across six intent categories. Training uses AdamW optimizer with 2e-5 learning rate, linear warmup, and early stopping with patience of 10 epochs. Image augmentation includes rotation, horizontal flipping, and brightness adjustments. The approach demonstrates that intermediate fusion outperforms both early and late fusion strategies for this low-resource language task.

## Key Results
- BangACMM achieves 84.11% macro-F1 score on the Uddessho test set, setting a new benchmark for Bangla multimodal intent classification
- Intermediate fusion (concatenation of text and image features) outperforms both early fusion (addition) and late fusion (ensemble) approaches
- Class-wise performance varies significantly, with Expressive achieving 86.4% F1 and Controversial at 78.0% F1, suggesting class imbalance impacts certain categories
- The mBERT+Swin combination yields the best performance, demonstrating the effectiveness of Swin Transformer for image feature extraction in this multimodal context

## Why This Works (Mechanism)
The intermediate fusion approach works by preserving modality-specific information while enabling cross-modal interaction at the feature level. By concatenating the 768-dim text embeddings from mBERT with image features from Swin Transformer, the model maintains distinct representations of each modality before the classification head learns to integrate them. This contrasts with early fusion (element-wise addition) which may lose modality-specific nuances, and late fusion (ensemble) which processes modalities independently without cross-modal learning. The Swin Transformer's hierarchical feature extraction captures spatial relationships in images that complement the semantic information from mBERT's contextualized text embeddings, creating a rich multimodal representation space for intent classification.

## Foundational Learning
- **Multimodal Fusion Strategies**: Why needed - Different fusion methods impact cross-modal learning and performance. Quick check - Compare macro-F1 scores across early, late, and intermediate fusion on validation set.
- **Transformer Architecture**: Why needed - Provides contextualized representations for both text and images. Quick check - Verify [CLS] embedding dimensionality matches between text and image encoders.
- **Class Imbalance Handling**: Why needed - Significant performance variance across classes (78.0%-86.4% F1) suggests imbalance effects. Quick check - Analyze class distribution in training data and apply weighted loss if needed.
- **Low-Resource Language Processing**: Why needed - Bangla has limited annotated multimodal data compared to high-resource languages. Quick check - Evaluate performance drop when reducing training data size.
- **Feature Dimensionality Compatibility**: Why needed - Fusion requires matching or compatible feature dimensions. Quick check - Print shapes before concatenation to ensure 768+768 compatibility.

## Architecture Onboarding

**Component Map**: Text Encoder (mBERT) -> Feature Extraction -> Concatenation -> Classification Head <- Image Encoder (Swin) -> Feature Extraction

**Critical Path**: Text input → mBERT → [CLS] embedding (768-dim) → Concatenation → Classification head → Intent prediction

**Design Tradeoffs**: Intermediate fusion trades computational efficiency for better cross-modal learning compared to late fusion, while avoiding the potential information loss of early fusion. The choice of Swin over simpler CNN architectures provides better spatial feature extraction at the cost of increased parameters.

**Failure Signatures**: Poor macro-F1 performance with significant variance across classes indicates class imbalance. Dimension mismatch errors during concatenation suggest feature projection issues. Performance similar to unimodal baselines indicates ineffective fusion.

**First Experiments**: 1) Test text-only baseline with mBERT to establish unimodal performance (expected ~75-78% macro-F1). 2) Test image-only baseline with Swin to verify visual modality contribution. 3) Implement intermediate fusion with dummy data to verify concatenation dimensions work correctly before full training.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset accessibility is restricted—Uddessho requires direct author contact, preventing independent validation of the reported 84.11% macro-F1 score
- English-to-Bangla transliteration process for ~5% of posts is underspecified, potentially affecting reproducibility
- Class imbalance appears significant (78.0% vs 86.4% F1 across classes) but its impact on macro-F1 metrics is not discussed

## Confidence

**High Confidence**: The general methodological approach (transformer-based multimodal fusion with intermediate concatenation) is well-established and technically sound. The experimental design using standard metrics and controlled comparisons to early/late fusion baselines is rigorous.

**Medium Confidence**: The reported performance improvements over prior work are plausible given the architecture choices, but verification requires dataset access. The claim of "significant improvements" (8.4pp over baselines) is reasonable but unverified.

**Low Confidence**: The absolute performance numbers (84.11% macro-F1) cannot be independently assessed without the dataset. The handling of class imbalance and its impact on macro-F1 scores is unclear.

## Next Checks

1. **Dataset Access Verification**: Contact authors to obtain Uddessho dataset and verify class distribution, preprocessing steps (especially English→Bangla transliteration), and whether the 3,048 posts split matches the reported 2,423/313/312 configuration.

2. **Implementation Fidelity Check**: Reproduce the fusion mechanism by confirming mBERT's [CLS] embeddings (768-dim) and Swin's image features are compatible for concatenation. Verify any dimensionality projection steps and test with a small subset before full training.

3. **Baseline Reproducibility**: Implement the cited prior approaches (MFNN, Ensemble) on the same dataset to verify the claimed 8.4pp improvement. Test different fusion strategies (early, late, intermediate) to confirm the reported performance ranking.