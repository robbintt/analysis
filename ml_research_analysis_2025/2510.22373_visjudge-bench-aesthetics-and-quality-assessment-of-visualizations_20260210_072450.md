---
ver: rpa2
title: 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations'
arxiv_id: '2510.22373'
source_url: https://arxiv.org/abs/2510.22373
tags:
- data
- visualization
- color
- design
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISJUDGE-BENCH, the first benchmark for evaluating
  MLLMs' capabilities in assessing visualization aesthetics and quality. The benchmark
  is based on a three-dimensional framework of Fidelity, Expressiveness, and Aesthetics,
  containing 3,090 expert-annotated samples across 32 chart types.
---

# VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations

## Quick Facts
- arXiv ID: 2510.22373
- Source URL: https://arxiv.org/abs/2510.22373
- Authors: Yupeng Xie; Zhiyang Zhang; Yifan Wu; Sirong Lu; Jiayi Zhang; Zhaoyang Yu; Jinlin Wang; Sirui Hong; Bang Liu; Chenglin Wu; Yuyu Luo
- Reference count: 40
- Primary result: Introduces VISJUDGE-BENCH, the first benchmark for evaluating MLLMs' capabilities in assessing visualization aesthetics and quality, showing significant performance gaps between even advanced MLLMs and human experts.

## Executive Summary
This paper introduces VISJUDGE-BENCH, the first benchmark for evaluating MLLMs' capabilities in assessing visualization aesthetics and quality. The benchmark is based on a three-dimensional framework of Fidelity, Expressiveness, and Aesthetics, containing 3,090 expert-annotated samples across 32 chart types. Systematic testing reveals significant gaps between even the most advanced MLLMs (including GPT-5) and human experts, with MAE of 0.553 and correlation of only 0.428. To address this, the authors propose VISJUDGE, a model specifically designed for visualization quality assessment. Experimental results demonstrate substantial improvements over baseline models, reducing MAE to 0.421 (23.9% reduction) and increasing consistency with human experts to 0.687 (60.5% improvement). The model also generalizes effectively to real-world applications in visualization generation and recommendation systems.

## Method Summary
The authors create VISJUDGE-BENCH by crawling ~300K visualization images, deduplicating them using perceptual hashing, classifying chart types with GPT-4o, and sampling for expert annotation. Each sample is evaluated across 6 dimensions (Data Fidelity, Semantic Readability, Insight Discovery, Design Style, Visual Composition, Color Harmony) using adaptive questions. They train VISJUDGE using GRPO reinforcement learning with composite rewards on Qwen2.5-VL-7B with LoRA fine-tuning (rank=128), optimizing for both accuracy and valid JSON output format.

## Key Results
- VISJUDGE reduces MAE from 0.553 to 0.421 (23.9% improvement) and increases correlation from 0.428 to 0.687 (60.5% improvement) over baseline MLLMs
- VISJUDGE achieves near-perfect alignment with human scoring patterns (μ=3.11) compared to base models' score inflation (μ=3.89)
- The model generalizes effectively to real-world applications, improving visualization generation quality by +6.07 average across 7 generation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning reduces systematic evaluation biases in MLLMs
- Mechanism: Base MLLMs exhibit "score inflation" (mean 3.89 vs. human 3.13) and over-concentration around 4.0. Fine-tuning on expert-annotated data redistributes scores toward human distributions through reward signals that penalize deviation from ground truth.
- Core assumption: Human expert ratings represent valid ground truth for visualization quality.
- Evidence anchors:
  - [abstract] "VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.421 (a 23.9% reduction) and increasing the consistency with human experts to 0.687 (a 60.5% improvement)"
  - [section 5.2.2] "VISJUDGE achieves near-perfect alignment with human scoring patterns (μ= 3.11)"
  - [corpus] Related work on visualization literacy barriers (arXiv:2601.12585) confirms MLLMs struggle with visualization interpretation tasks.

### Mechanism 2
- Claim: Multi-dimensional evaluation framework enables diagnostic capability across heterogeneous visualization types
- Mechanism: The "Fidelity, Expressiveness, Aesthetics" framework with six sub-dimensions allows models to learn domain-specific assessment patterns rather than surface-level visual aesthetics alone. Adaptive question generation customizes evaluation criteria per chart type.
- Core assumption: Visualization quality decomposes into orthogonal dimensions that can be scored independently.
- Evidence anchors:
  - [abstract] "evaluating visualization quality requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics"
  - [section 3.1.2] "The 'Fidelity, Expressiveness, and Aesthetics' Framework Design... operationalizes this core concept into six measurable evaluation dimensions"
  - [corpus] Limited corpus evidence on multi-dimensional evaluation; related work focuses on single-aspect assessment.

### Mechanism 3
- Claim: GRPO-based reinforcement learning with composite reward aligns model outputs to structured evaluation format
- Mechanism: The composite reward combines accuracy reward (exponential decay from prediction error) and format reward (binary check for valid JSON structure). GRPO samples multiple outputs per input and optimizes relative to group baseline.
- Core assumption: The reward function adequately captures both quality alignment and output usability.
- Evidence anchors:
  - [section 4] "We employ reinforcement learning with the GRPO algorithm, using a composite reward function combining accuracy reward and format reward"
  - [appendix D.2] "R_acc_single = exp(−|score_predicted − score_ground-truth| / 0.5)"
  - [corpus] No direct corpus evidence on GRPO for visualization tasks; mechanism extrapolated from paper's description.

## Foundational Learning

- Concept: MLLM-as-a-Judge paradigm
  - Why needed here: The entire approach assumes MLLMs can serve as automated evaluators; understanding this paradigm explains why visualization-specific fine-tuning is necessary.
  - Quick check question: Can you explain why applying natural-image aesthetic assessment models to visualization fails to capture data fidelity issues?

- Concept: Mean Absolute Error (MAE) vs. correlation for evaluation alignment
  - Why needed here: The paper reports both MAE reduction (0.553→0.421) and correlation improvement (0.428→0.687) as separate metrics; they measure different aspects of alignment.
  - Quick check question: If a model has low MAE but low correlation, what does that indicate about its scoring behavior?

- Concept: Perceptual Hashing for Deduplication
  - Why needed here: The benchmark construction filters ~300K images to 3,090 samples; understanding deduplication ensures you appreciate the data quality pipeline.
  - Quick check question: Why is Hamming distance on perceptual hash more appropriate than exact pixel matching for visualization deduplication?

## Architecture Onboarding

- Component map: Web crawling -> Perceptual hash dedup -> GPT-4o classification -> Stratified sampling -> Expert annotation (3 annotators × 3,090 samples) -> Quality control (outlier removal, malicious scoring filter) -> Evaluation framework: Chart image -> GPT-4o metadata extraction -> Adaptive question generation -> 6-dimensional scoring (1-5 scale) -> Training pipeline: Base MLLM (Qwen2.5-VL-7B) -> LoRA adapters (rank=128) -> GRPO fine-tuning -> Composite reward

- Critical path: Annotation quality directly determines model alignment. If expert validation stage is skipped or rushed, the entire fine-tuning signal degrades.

- Design tradeoffs:
  - 32 chart types ensure diversity but create sparse samples for rare categories (e.g., violin plots: 1 sample)
  - 6 dimensions provide granularity but introduce annotation complexity (90 questions per task)
  - LoRA enables efficient training but limits adaptation depth compared to full fine-tuning

- Failure signatures:
  - Score inflation (μ > 3.5): Model learned surface aesthetics without domain knowledge
  - Sharp peak at 4.0: Model collapsed to non-discriminative behavior
  - Negative correlation on dashboards: Model lacks multi-view reasoning capability

- First 3 experiments:
  1. Ablation on training data scale: Train VisJudge with 100, 500, 1000, 2000 samples to verify scaling behavior (paper shows logarithmic correlation growth, R²=0.678)
  2. Cross-architecture transfer: Apply same LoRA+GRPO training to InternVL3-8B and LLaVA-1.6-7B to confirm methodology generalization (paper reports 31-40% MAE reduction across architectures)
  3. Real-world integration test: Integrate VisJudge as feedback model in MatPlotAgent and verify generation quality improves (paper shows +6.07 average improvement across 7 generation models)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific fine-tuning enable MLLMs to achieve robust cross-view semantic understanding for complex multi-visualization compositions and dashboards?
- Basis in paper: [explicit] Section 5.2.3 notes "performance degradation with complexity" and Appendix B.6 attributes this to "the lack of cross-view visual understanding" where models "tend to process each view as an isolated visualization" rather than constructing cross-view semantic connections.
- Why unresolved: While VisJudge improves overall performance, correlation on dashboards remains only 0.375 (vs. 0.577 for single visualizations), indicating cross-view understanding remains fundamentally challenging.
- What evidence would resolve it: Architecture modifications or training paradigms specifically designed for multi-view reasoning that achieve comparable dashboard-to-single-vis performance ratios, along with ablation studies isolating cross-view attention mechanisms.

### Open Question 2
- Question: How can data fidelity assessment be validated without access to source data underlying web-collected visualizations?
- Basis in paper: [explicit] Limitations section states "since samples are mainly collected from the web where raw data is often unavailable, our evaluation framework...primarily relying on visual presentation," limiting fidelity assessment to "visually detectable distortions."
- Why unresolved: Visual-only fidelity cannot detect semantic data errors (wrong values, fabricated data) that appear visually plausible, creating a fundamental gap between visual assessment and true data accuracy.
- What evidence would resolve it: A parallel benchmark where visualizations have known ground-truth data, enabling comparison between visual-only and data-informed fidelity assessments to quantify the assessment gap.

### Open Question 3
- Question: Can distribution-based evaluation methods better capture the inherent subjectivity and cultural diversity in human visualization preferences than single-score annotations?
- Basis in paper: [explicit] Future work section proposes "investigating distribution-based evaluation methods to better capture the diversity of human preferences" as an explicit direction, noting current single-score approach may not reflect preference heterogeneity.
- Why unresolved: Crowdsourced annotations show notable variance (crowd-crowd MAE 0.64–0.70), and annotators were primarily US-based (88.7%), potentially missing cross-cultural aesthetic preferences and domain-specific evaluation standards.
- What evidence would resolve it: Multi-annotator distribution profiles per sample, cross-cultural annotation studies, and models trained to predict preference distributions rather than point estimates, evaluated against diverse human preference distributions.

## Limitations

- Expert annotation bias: Human expert ratings serve as ground truth but may reflect subjective preferences rather than objective quality standards
- Limited generalization: The benchmark covers 32 chart types but with highly imbalanced representation - rare visualizations like violin plots have only 1 sample
- Static focus: The evaluation focuses on static visualization assessment without considering interactive or dynamic visualization scenarios common in real-world applications

## Confidence

- **High Confidence**: The empirical results showing VISJUDGE outperforming baseline MLLMs (MAE reduction 23.9%, correlation improvement 60.5%) are directly measurable from the presented experiments.
- **Medium Confidence**: The claim that domain-specific fine-tuning addresses systematic MLLM biases relies on the assumption that expert annotations represent valid ground truth, which cannot be independently verified without understanding annotator selection criteria.
- **Low Confidence**: The generalization claims to real-world applications (visualization generation and recommendation systems) are demonstrated through limited integration tests without ablation studies isolating VISJUDGE's specific contribution.

## Next Checks

1. **Inter-annotator agreement analysis**: Calculate Krippendorff's alpha across the three expert annotators to quantify annotation consistency and identify potential bias sources.
2. **Cross-dataset generalization**: Evaluate VISJUDGE on independent visualization datasets (e.g., FigureQA, DVQA) to test whether improvements transfer beyond the training distribution.
3. **Ablation on evaluation dimensions**: Systematically remove each of the six evaluation dimensions to determine which contribute most to model performance and whether the full framework provides diagnostic value.