---
ver: rpa2
title: 'Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic
  Decision-making'
arxiv_id: '2506.09390'
source_url: https://arxiv.org/abs/2506.09390
tags:
- llms
- strategic
- human
- each
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares the strategic decision-making of large language
  models (LLMs) and humans using two classic games from behavioral game theory: Rock-Paper-Scissors
  and the Prisoner''s Dilemma. The experiments directly replicate human-subject protocols,
  placing LLMs in identical decision-making environments with the same payoff structures
  and instructions.'
---

# Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making

## Quick Facts
- arXiv ID: 2506.09390
- Source URL: https://arxiv.org/abs/2506.09390
- Reference count: 23
- LLMs reproduce human-like bounded rationality heuristics in strategic games but apply them more rigidly and with weaker environmental sensitivity.

## Executive Summary
This study systematically compares large language models and humans in strategic decision-making using Rock-Paper-Scissors and Prisoner's Dilemma games. The experiments directly replicate human-subject protocols, placing LLMs in identical decision-making environments with the same payoff structures and instructions. Results show that LLMs reproduce human-like heuristics, such as outcome-based strategy switching and increased cooperation when future interaction is possible, but apply these rules more rigidly and demonstrate weaker sensitivity to dynamic environmental changes. Model-level analyses reveal distinctive architectural signatures in strategic behavior, and even reasoning-enhanced models struggle in scenarios requiring theory-of-mind inference. The findings indicate that current LLMs capture only a partial form of human-like bounded rationality, suggesting the need for training methods that encourage flexible opponent modeling and stronger context awareness.

## Method Summary
The study employs two classic games from behavioral game theory: Rock-Paper-Scissors (RPS) and Prisoner's Dilemma (PD). Six LLMs (GPT-4o, O1, Claude-3.5, Claude-3.7, DeepSeek-V3, DeepSeek-R1) are tested against each other and against deterministic bot opponents using modified payoff matrices. RPS experiments involve 45 pairwise matches (including self-play) with 50 rounds each, analyzing choice distributions and outcome-conditioned strategy transitions. PD experiments use rotation matching with 24 agents per session across Dice (δ∈{0,0.5,0.75}) and Finite (H∈{1,2,4}) treatments, comparing cooperation rates to human baselines. All API calls use temperature=1.0, and prompts are modeled after human subject instructions.

## Key Results
- LLMs exhibit human-like outcome-based strategy switching in RPS but with more stereotyped patterns (e.g., fixed lose-downgrade biases).
- Cooperation rates in PD increase with future interaction opportunities, mirroring human behavior but with less sensitivity to contextual changes.
- Model-level analyses reveal distinctive behavioral signatures: GPT-family shows stronger randomness-seeking, while Claude models exhibit more structured switching patterns.
- Even reasoning-enhanced models struggle in PD scenarios requiring theory-of-mind inference about opponent strategies.

## Why This Works (Mechanism)
The study demonstrates that LLMs can capture bounded rationality patterns through pattern matching and heuristic application, but lack the flexible contextual reasoning that characterizes human strategic thinking. The mechanism involves models recognizing game structures and applying learned response patterns based on immediate outcomes, but failing to dynamically adjust to opponent modeling or complex strategic inference.

## Foundational Learning
- **Bounded rationality**: Why needed - To understand human-like decision-making under cognitive constraints. Quick check - Compare model behavior against Nash equilibrium predictions.
- **Game theory payoffs**: Why needed - To establish the strategic environment and incentives. Quick check - Verify payoff matrix implementation matches human study specifications.
- **Outcome-conditioned transitions**: Why needed - To analyze how past results influence future choices. Quick check - Compute transition matrices and compare to human baseline distributions.

## Architecture Onboarding
- **Component map**: LLM API calls -> Game environment -> Outcome logging -> Statistical analysis -> Behavioral pattern comparison
- **Critical path**: Prompt generation → Model response → Game state update → Outcome evaluation → Strategy analysis
- **Design tradeoffs**: Temperature=1.0 maximizes behavioral diversity but may obscure deterministic patterns; API-based testing limits control over model internals.
- **Failure signatures**: Uniform choice distributions suggest insufficient context; systematic bot exploitation indicates prompt misalignment with strategic objectives.
- **First experiments**: 1) Validate bot opponent behavior independently before LLM integration; 2) Test single-model self-play to establish baseline choice distributions; 3) Compare chi-square significance thresholds for outcome-based strategy identification.

## Open Questions the Paper Calls Out
- Do the observed "strategic signatures" and rigid application of heuristics persist in more complex strategic environments, such as games with more than two players or imperfect information? The current study restricted its scope to two-player, perfect-information games, leaving complex multi-agent interactions untested.
- Can integrating opponent-aware fine-tuning objectives or reinforcement learning on human gameplay traces successfully mitigate the rigidity of LLM heuristics? The paper identifies the rigidity as a problem but does not implement or test the proposed training interventions.
- Does the explicit inclusion of theory-of-mind (ToM) scaffolds in reasoning pipelines enable LLMs to overcome their current limitations in adaptive, rule-based adversarial settings? The results show reasoning models fail when adaptation is required, but the study does not test if specific architectural changes would solve this.

## Limitations
- Study relies on fixed set of six frontier LLMs accessed through APIs at temperature=1.0, limiting generalizability.
- Prompt templates were not fully specified across all experimental conditions, introducing potential variability.
- Identification of outcome-based strategies used unspecified chi-square p-value thresholds and undocumented clustering methods.

## Confidence
- LLMs reproduce human-like bounded rationality patterns: High
- Model-level behavioral signatures are distinctive: Medium
- Reasoning models struggle with theory-of-mind inference: Medium

## Next Checks
- Systematically vary temperature and prompt engineering across the full experimental suite to test sensitivity of bounded rationality patterns.
- Implement the exact chi-square testing and clustering methodology for strategy identification to enable precise replication of the RPS behavioral categorization.
- Extend the PD experiments to include additional session structures that isolate specific theory-of-mind components, such as second-order belief attribution tasks.