---
ver: rpa2
title: High-Fidelity Pseudo-label Generation by Large Language Models for Training
  Robust Radiology Report Classifiers
arxiv_id: '2505.01693'
source_url: https://arxiv.org/abs/2505.01693
tags:
- deberta-rad
- language
- reports
- large
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeBERTa-RAD, a two-stage framework for high-fidelity
  chest X-ray report labeling. It leverages a powerful large language model (LLM)
  to generate high-quality pseudo-labels for a large corpus of reports, then trains
  a DeBERTa-Base model using knowledge distillation on these pseudo-labels.
---

# High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers

## Quick Facts
- arXiv ID: 2505.01693
- Source URL: https://arxiv.org/abs/2505.01693
- Reference count: 31
- Key outcome: DeBERTa-RAD achieves 0.9120 Macro F1 on MIMIC-500, outperforming prior methods by 1.3-4.6 points.

## Executive Summary
This paper introduces DeBERTa-RAD, a two-stage framework for high-fidelity chest X-ray report labeling. It leverages a powerful large language model (LLM) to generate high-quality pseudo-labels for a large corpus of reports, then trains a DeBERTa-Base model using knowledge distillation on these pseudo-labels. Evaluated on the MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming rule-based systems, fine-tuned transformer models, and direct LLM inference. The method excels at handling uncertain findings and maintains practical inference speed for high-throughput applications, demonstrating a viable path to overcome data annotation bottlenecks in medical text processing.

## Method Summary
The DeBERTa-RAD framework operates in two stages: (1) An LLM processes each radiology report with a structured prompt to identify 13 radiographic findings and their certainty status (Present/Absent/Uncertain), generating high-quality pseudo-labels for a large corpus; (2) A DeBERTa-Base model is trained using knowledge distillation on these pseudo-labels, combining hard label cross-entropy with temperature-scaled soft target distillation. The approach aims to capture linguistic nuances in radiology reports that rule-based systems miss while maintaining practical inference speed for high-throughput clinical applications.

## Key Results
- DeBERTa-RAD achieves 0.9120 Macro F1 on MIMIC-500, outperforming CheXbert (0.9047) and CheXpert (0.8864).
- Particularly strong performance on uncertain findings (0.852 F1 vs. 0.798 for CheXbert).
- Maintains practical inference speed (~750 reports/second on V100 GPU) suitable for clinical deployment.
- Framework generalizes to institutional reports beyond MIMIC-CXR corpus.

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated pseudo-labels capture linguistic nuances in radiology reports that rule-based systems miss. A large language model processes each report with a structured prompt that explicitly instructs it to identify 13 radiographic findings and classify certainty status (Present/Absent/Uncertain). The LLM's pre-trained understanding of negation, hedging language, and complex sentence structures enables higher-fidelity labels than hand-crafted rules. The core assumption is that the LLM's pseudo-labels are sufficiently accurate to serve as ground truth for training a student model. Errors or systematic biases in the teacher will transfer to the student. The paper validates pseudo-label quality against MIMIC-500 gold standard (Macro F1 0.9014), but this may not generalize to out-of-distribution report styles.

### Mechanism 2
Knowledge distillation with temperature-scaled softmax transfers the teacher's decision patterns more effectively than hard-label training alone. The student model is trained with a combined loss: (1-α)L_hard (standard cross-entropy on hard pseudo-labels) + αL_distill (cross-entropy with temperature T_distill > 1). The softened probability distribution encourages the student to learn relative class relationships rather than just memorizing hard labels. The core assumption is that the teacher's implicit confidence margins—captured through the hard label choice across diverse examples—contain learnable structure that temperature scaling can extract. The paper does not provide the teacher's soft probabilities, only hard labels. If α or T_distill are poorly tuned (e.g., too much weight on distillation with insufficient label diversity), the student may underfit hard labels.

### Mechanism 3
DeBERTa's disentangled attention mechanism better captures position-content dependencies in complex medical sentences. DeBERTa separates content and position embeddings in its attention mechanism, allowing more precise modeling of how token positions affect semantic relationships. This is hypothesized to help with long, syntactically complex radiology sentences where finding mentions may be distant from their certainty modifiers. The core assumption is that the performance gain from DeBERTa-Base over BERT-Base is attributable to the architecture rather than differences in training data or hyperparameters. If the performance difference is primarily due to pseudo-label quality rather than architecture, switching to DeBERTa may not yield gains.

## Foundational Learning

- Concept: **Knowledge Distillation**
  - Why needed here: The entire framework depends on transferring LLM knowledge to a smaller model. Without understanding distillation mechanics (temperature scaling, soft vs. hard targets), you cannot debug training issues or tune hyperparameters α and T_distill.
  - Quick check question: If the teacher model only outputs hard labels (not soft probabilities), what information is actually being distilled to the student, and what role does temperature play?

- Concept: **Multi-label Multi-class Classification with Class Imbalance**
  - Why needed here: The task involves 13 independent findings, each with 3 classes (Present/Absent/Uncertain), where "Uncertain" is underrepresented. Understanding why Macro F1 (not accuracy or Micro F1) is the primary metric is essential for interpreting results.
  - Quick check question: Why would a model that achieves high overall accuracy still perform poorly on the "Uncertain" class, and why does Macro F1 expose this failure?

- Concept: **Pseudo-labeling and Weak Supervision Risks**
  - Why needed here: Training on LLM-generated labels introduces systematic risks. If you cannot identify when pseudo-labels are wrong (e.g., by spot-checking against human labels or analyzing error patterns), you may deploy a model with inherited biases.
  - Quick check question: What validation steps would you add before trusting pseudo-labels for a new report style or institution not represented in the pseudo-labeling corpus?

## Architecture Onboarding

- Component map: LLM Pseudo-Labeler + Prompt P(R) → generates L_LLM(R) → DeBERTa-Base encoder → [CLS] token → 13 independent 3-way classification heads → combined distillation loss

- Critical path:
  1. Validate pseudo-label quality on a held-out human-annotated subset before full training (paper shows 0.9014 F1 vs. gold standard)
  2. Tune α and T_distill on validation split of pseudo-labeled data
  3. Monitor per-class F1 during training, especially for "Uncertain" class

- Design tradeoffs:
  - **Teacher quality vs. labeling cost**: A more capable LLM produces better pseudo-labels but increases Stage 1 cost. The paper uses an "advanced LLM" (likely GPT-4 class) but does not report labeling cost.
  - **Student size vs. inference speed**: DeBERTa-Base (~750 reports/sec) is slightly slower than CheXbert (~800) but more accurate. Larger DeBERTa variants would increase capacity but reduce throughput.
  - **Distillation emphasis vs. hard-label accuracy**: Higher α emphasizes distillation behavior (softer predictions) but may reduce confidence on correct hard labels.

- Failure signatures:
  - **Uncertain class F1 << Present/Absent F1**: Indicates pseudo-labels or student model struggle with hedging language; review prompt design and uncertain-labeled examples
  - **Performance drops on out-of-distribution report styles**: Pseudo-labels may not generalize; consider institution-specific fine-tuning or active learning on uncertain cases
  - **Student overfits pseudo-labels but underperforms on human labels**: Check pseudo-label quality and consider adding small human-annotated validation set for early stopping

- First 3 experiments:
  1. **Reproduce baseline comparison**: Run DeBERTa-RAD vs. CheXbert vs. CheXpert on MIMIC-500 to verify reported Macro F1 gains (0.9120 vs. 0.9047 vs. 0.8864)
  2. **Architecture ablation**: Train BERT-Base and DeBERTa-Base on the same pseudo-labels with identical hyperparameters to isolate architectural contribution
  3. **Uncertainty analysis**: Compute per-class F1 for Present/Absent/Uncertain across all 13 findings to verify the reported "Uncertain" improvement (0.852 vs. 0.798 for CheXbert) and identify any findings where uncertainty handling remains weak

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can active learning strategies involving minimal human review effectively correct LLM hallucinations or biases in the pseudo-labels to improve student model accuracy?
- Basis in paper: The Conclusion states, "Future work could explore methods to validate and potentially refine the LLM-generated pseudo-labels, perhaps through active learning strategies involving minimal human review of uncertain cases."
- Why unresolved: The current framework treats LLM pseudo-labels as ground truth, and the authors acknowledge that "errors or biases in the LLM's output can be transferred to the student."
- What evidence would resolve it: Experiments demonstrating performance gains where a small percentage of low-confidence pseudo-labels are corrected by human experts before distillation.

### Open Question 2
- Question: Does integrating image-based features with the text-based DeBERTa-RAD model improve the classification of uncertain findings?
- Basis in paper: The Conclusion identifies "integrating multimodal information (e.g., combining report text with image features)" as a promising direction for future research.
- Why unresolved: The current architecture is purely text-based and cannot leverage visual cues that might resolve textual ambiguity or uncertainty.
- What evidence would resolve it: Ablation studies on a multimodal variant of the model showing improved F1 scores for the "Uncertain" class compared to the text-only baseline.

### Open Question 3
- Question: To what extent does DeBERTa-RAD generalize to chest X-ray reports from institutions with significantly different reporting styles or non-English languages?
- Basis in paper: The authors note in the Conclusion that "performance might degrade on reports with vastly different structures or styles from the corpus used for pseudo-labeling."
- Why unresolved: The evaluation was restricted to the MIMIC-CXR dataset, and the paper notes potential brittleness to domain shift without testing on external datasets.
- What evidence would resolve it: Evaluation of the trained model on external datasets like CheXpert or Open-i without further fine-tuning to assess robustness.

## Limitations

- **Prompt and Teacher Model Dependency**: The framework's performance critically depends on the quality of the LLM-generated pseudo-labels, which in turn depends on the prompt design and the specific LLM used. The paper does not provide the exact prompt or confirm whether GPT-4 was used, making exact reproduction challenging.
- **Hyperparameter Sensitivity**: Key hyperparameters for knowledge distillation (α, T_distill) and training (learning rate, batch size, epochs) are not reported. The performance gains may be sensitive to these values, and suboptimal settings could reduce the reported F1 score improvements.
- **Generalizability to New Report Styles**: The pseudo-labels are generated from a large corpus of MIMIC-CXR reports. The framework's performance on reports from different institutions or report styles is not evaluated.

## Confidence

- **High Confidence**: The overall framework design (LLM pseudo-labeling + DeBERTa + knowledge distillation) is sound and the reported performance on MIMIC-500 (0.9120 Macro F1) is likely achievable with the described approach.
- **Medium Confidence**: The reported improvements over prior methods (CheXbert: 0.9047, CheXpert: 0.8864) are credible, but the exact magnitude may depend on unreported hyperparameters and the specific LLM used for pseudo-labeling.
- **Low Confidence**: Claims about the specific contribution of DeBERTa's disentangled attention mechanism are not directly supported, as no ablation study isolates the architectural effect from the pseudo-label quality effect.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Perform a grid search over T_distill ∈ {2,4,6,8} and α ∈ {0.1,0.3,0.5,0.7} to identify the optimal combination and quantify the impact on Macro F1. Report the final values used and their sensitivity.

2. **Architecture Ablation Study**: Train BERT-Base and DeBERTa-Base models using the *same* LLM-generated pseudo-labels and identical hyperparameters (α, T_distill, LR, batch size, epochs). Compare Macro F1 to isolate the contribution of the DeBERTa architecture.

3. **Out-of-Distribution Generalization Test**: Evaluate DeBERTa-RAD on a held-out set of radiology reports from a different institution or report style than MIMIC-CXR. Report per-class F1 for Present/Absent/Uncertain to assess whether uncertainty handling degrades on new data.