---
ver: rpa2
title: 'UbiQVision: Quantifying Uncertainty in XAI for Image Recognition'
arxiv_id: '2512.20288'
source_url: https://arxiv.org/abs/2512.20288
tags:
- uncertainty
- medical
- shap
- belief
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UbiQVision integrates Bayesian meta-learning, Shapley Additive
  exPlanations (SHAP), and Dempster-Shafer theory to quantify uncertainty in deep
  learning explanations for medical imaging. By treating model reliability as a Dirichlet
  random variable, the framework assigns dynamic weights to ensemble members based
  on validation performance, then maps SHAP-derived pixel contributions into evidential
  mass functions.
---

# UbiQVision: Quantifying Uncertainty in XAI for Image Recognition

## Quick Facts
- arXiv ID: 2512.20288
- Source URL: https://arxiv.org/abs/2512.20288
- Reference count: 40
- Primary result: UbiQVision integrates Bayesian meta-learning, SHAP, and Dempster-Shafer theory to quantify uncertainty in XAI for medical imaging

## Executive Summary
UbiQVision addresses the critical challenge of uncertainty quantification in explainable AI for medical image recognition. The framework combines Bayesian meta-learning for model reliability assessment, SHAP for pixel-level explanations, and Dempster-Shafer theory for evidential fusion. By treating model reliability as a Dirichlet random variable and mapping SHAP contributions into belief functions, UbiQVision generates both belief maps highlighting diagnostic regions and uncertainty maps distinguishing confident predictions from ambiguous cases. Evaluated across three medical imaging datasets, the approach successfully localized pathological biomarkers while providing clinically actionable uncertainty estimates.

## Method Summary
UbiQVision operates through a three-stage pipeline that quantifies uncertainty in XAI explanations for medical imaging. First, Bayesian meta-learning evaluates individual model reliability by treating performance on validation data as draws from a Dirichlet distribution, producing weights that dynamically adjust each model's influence. Second, SHAP values quantify pixel-level feature contributions, which are then converted into evidential mass functions representing belief, plausibility, and total ignorance. Finally, Dempster-Shafer theory fuses these mass functions across the ensemble, explicitly modeling both aleatoric uncertainty (data noise) and epistemic uncertainty (model ignorance). The framework outputs belief maps highlighting diagnostically relevant regions and uncertainty maps identifying areas requiring expert review.

## Key Results
- Belief maps precisely localized pathological biomarkers across three medical datasets (malaria parasites, Alzheimer's markers, diabetic retinopathy hemorrhages)
- Uncertainty maps effectively separated diagnostic regions from uninformative background, enabling clinical triage
- Bayesian weighting consistently identified the most robust model per dataset, reducing spurious attributions from weaker models
- The framework aligned predictions with anatomical evidence, enhancing interpretability in safety-critical AI diagnostics

## Why This Works (Mechanism)
UbiQVision works by explicitly modeling the dual nature of uncertainty in medical imaging through a principled evidential framework. The Bayesian meta-learning component captures epistemic uncertainty by treating model reliability as a random variable that adapts to dataset characteristics, while the Dempster-Shafer combination explicitly models total ignorance for ambiguous regions. By converting SHAP explanations into mass functions, the framework bridges local feature importance with global evidential reasoning. The dynamic weighting ensures that reliable models dominate the fusion process while still incorporating diverse perspectives from the ensemble. This combination allows the system to provide both confident diagnoses where evidence is strong and clear uncertainty signals where expert review is warranted.

## Foundational Learning

**Bayesian Meta-Learning**: A framework for adapting model behavior to new tasks using limited data by treating model parameters as distributions rather than point estimates. Needed because medical imaging tasks vary significantly across pathologies and modalities, requiring adaptive reliability assessment. Quick check: Verify that Dirichlet posteriors properly capture uncertainty in model performance across validation folds.

**Shapley Additive exPlanations (SHAP)**: A game-theoretic approach for attributing model predictions to input features by calculating fair contribution values based on all possible feature coalitions. Needed to provide interpretable pixel-level explanations that can be mapped to evidential mass functions. Quick check: Confirm SHAP values sum to the difference between model prediction and baseline.

**Dempster-Shafer Theory**: A mathematical framework for reasoning with evidence that allows explicit modeling of belief, plausibility, and ignorance by assigning mass to sets rather than individual hypotheses. Needed because traditional probability cannot represent complete ignorance or partial conflict between evidence sources. Quick check: Verify that mass assignments sum to 1 across the power set of hypotheses.

**Aleatoric vs Epistemic Uncertainty**: Aleatoric uncertainty represents inherent data noise and randomness, while epistemic uncertainty captures model ignorance and lack of knowledge. Needed to distinguish between uncertainty that can be reduced with more data versus uncertainty inherent to the problem. Quick check: Confirm that epistemic uncertainty decreases with more training data while aleatoric uncertainty remains relatively stable.

**Evidence Theory vs Probability Theory**: Evidence theory allows assignment of belief to sets of hypotheses and explicit modeling of ignorance, whereas probability theory requires precise belief in individual outcomes. Needed because medical diagnosis often involves incomplete evidence and multiple competing hypotheses. Quick check: Verify that belief and plausibility intervals properly capture uncertainty when combining conflicting evidence sources.

## Architecture Onboarding

Component map: Input Images -> Ensemble Models -> Bayesian Meta-Learning -> Reliability Weights -> SHAP Explanations -> Mass Functions -> Dempster-Shafer Fusion -> Belief/Uncertainty Maps

Critical path: The core inference pipeline runs models in parallel, computes Bayesian weights, generates SHAP values, converts to mass functions, and performs evidential fusion. The most timing-critical component is SHAP explanation generation, which scales poorly with input size and feature count.

Design tradeoffs: The framework trades computational efficiency for uncertainty quantification - SHAP generation and Dempster-Shafer fusion add significant overhead compared to simple ensemble averaging. The choice of Dirichlet priors for reliability modeling assumes exchangeability that may not hold for highly heterogeneous model families.

Failure signatures: Correlated model errors can violate the heterogeneity assumption, leading to overconfident uncertainty estimates. Poor SHAP convergence on high-resolution images may produce noisy mass functions. The Dempster-Shafer combination can yield counterintuitive results when evidence is highly conflicting.

First experiments: 1) Test framework on synthetic data with known uncertainty patterns to validate belief/uncertainty separation. 2) Evaluate sensitivity to temperature and sensitivity parameters across datasets with varying characteristics. 3) Compare Dempster-Shafer fusion against alternative combination methods like Monte Carlo dropout ensembles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Dempster-Shafer fusion be extended to multimodal medical imaging settings and longitudinal patient data?
- Basis in paper: [explicit] "Subsequent research could extend the Dempster-Shafer fusion to multimodal settings and to longitudinal data. In this manner, belief and ignorance could be tracked over time at the patient level, not just per image."
- Why unresolved: Current framework operates on single-modality, single-timepoint images; temporal dynamics and cross-modal evidence fusion remain unaddressed.
- What evidence would resolve it: Demonstrated belief/uncertainty tracking across patient visits and integration of multi-modal inputs (e.g., PET-CT, MRI sequences) with maintained epistemic rigor.

### Open Question 2
- Question: Can causal modeling and counterfactual reasoning be integrated to ensure belief masses reflect disease mechanisms rather than spurious correlates?
- Basis in paper: [explicit] "integrating causal modeling and counterfactual reasoning would help ensure that belief masses more faithfully capture disease mechanisms rather than stable but spurious correlates."
- Why unresolved: SHAP attributions may capture distributional correlations rather than causal relationships; current framework lacks mechanism-level validation.
- What evidence would resolve it: Framework modifications that distinguish causal biomarkers from confounders, validated against known disease mechanisms.

### Open Question 3
- Question: Is there a principled, data-driven method for setting the temperature (T) and sensitivity (λ) parameters across diverse datasets?
- Basis in paper: [inferred] Authors state T=5.0 and λ=100 were "selected empirically" based on balancing model contributions and signal-to-noise ratios, but no systematic calibration procedure is provided.
- Why unresolved: Manual tuning may not generalize; optimal values likely depend on dataset-specific characteristics like class balance, image resolution, and feature sparsity.
- What evidence would resolve it: An adaptive calibration algorithm validated across datasets with varying characteristics, demonstrating stable performance without manual hyperparameter search.

### Open Question 4
- Question: How robust is the framework when ensemble members violate the heterogeneity assumption and exhibit correlated errors?
- Basis in paper: [inferred] Assumption 2 states models are heterogeneous with uncorrelated errors, but the paper does not empirically test sensitivity to correlated model failures or provide diagnostics for detecting such violations.
- Why unresolved: Real-world ensemble models trained on similar data may develop correlated failure modes, potentially undermining the Dempster-Shafer fusion's conflict-resolution benefits.
- What evidence would resolve it: Systematic experiments with artificially induced error correlations, measuring degradation in belief map quality and uncertainty calibration.

## Limitations
- The assumption that model reliability can be fully captured by a Dirichlet random variable may not account for complex dependencies between models or data characteristics
- Reliance on SHAP values as the sole source of evidence for pixel contributions may miss other relevant features or interactions not captured by Shapley values
- The Dempster-Shafer combination rule can produce counterintuitive results when combining highly conflicting evidence, potentially affecting uncertainty estimates in ambiguous cases

## Confidence

| Claim | Confidence |
|-------|------------|
| Bayesian weighting consistently identifies the most robust model | Medium |
| Uncertainty maps clearly separate diagnostic regions from background | High |
| Clinical impact: quantified uncertainty enables better triage decisions | Medium |

## Next Checks

1. Evaluate UbiQVision on non-medical imaging tasks to assess generalizability of the uncertainty quantification approach across different domains and problem types.

2. Compare Dempster-Shafer fusion against alternative uncertainty combination methods like Monte Carlo dropout or deep ensembles to quantify the benefits of evidential reasoning.

3. Conduct controlled user studies with radiologists to quantify how uncertainty information affects diagnostic decision-making and trust calibration in clinical practice.