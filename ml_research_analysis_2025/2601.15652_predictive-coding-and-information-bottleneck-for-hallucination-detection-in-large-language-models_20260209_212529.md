---
ver: rpa2
title: Predictive Coding and Information Bottleneck for Hallucination Detection in
  Large Language Models
arxiv_id: '2601.15652'
source_url: https://arxiv.org/abs/2601.15652
tags:
- pcib
- auroc
- context
- hallucination
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pcib, a hybrid framework for hallucination
  detection in Large Language Models (LLMs) that combines neuroscience-inspired signal
  design with supervised machine learning. The core method extracts interpretable
  signals based on Predictive Coding (measuring surprisal against internal priors)
  and the Information Bottleneck (measuring signal retention under perturbation),
  enhanced by Entity-Focused Uptake, Context Adherence, and Falsifiability Score.
---

# Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models

## Quick Facts
- arXiv ID: 2601.15652
- Source URL: https://arxiv.org/abs/2601.15652
- Authors: Manish Bhatt
- Reference count: 25
- Primary result: Pcib achieves 0.8669 AUROC for hallucination detection using 75x less data and 1000x faster inference than LLM judges

## Executive Summary
This paper introduces Pcib, a hybrid framework for hallucination detection in Large Language Models that combines neuroscience-inspired signal design with supervised machine learning. The core method extracts interpretable signals based on Predictive Coding (measuring surprisal against internal priors) and the Information Bottleneck (measuring signal retention under perturbation), enhanced by Entity-Focused Uptake, Context Adherence, and Falsifiability Score. Evaluating on HaluBench (200 samples), the theory-guided baseline achieves 0.8017 AUROC, while BASE supervised models reach 0.8274 AUROC, and IMPROVED features boost performance to 0.8669 AUROC (+4.95% gain). The approach achieves competitive performance using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remains fully interpretable. Crucially, the paper reports a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting LLMs generate coherent reasoning for false premises ("Sycophancy").

## Method Summary
Pcib combines predictive coding and information bottleneck principles to detect hallucinations in RAG systems. The method extracts four base signals: Uptake (KL divergence between answer distributions with and without context), Stress (JS divergence of perturbed claims), Conflict (NLI contradiction scores), and Rationalization (reasoning trace overlap). Three enhancements amplify these: Entity-Focused Uptake (weighting high-value tokens), Context Adherence (coherence with context), and Falsifiability Score (claim testability). Signals aggregate via harmonic mean (ESI) for unsupervised detection or Random Forest for supervised classification. The framework operates on (Question, Context, Answer) triples, extracting interpretable features that diagnose hallucination presence without requiring massive training data or LLM judges.

## Key Results
- AUROC of 0.8669 achieved with IMPROVED features on HaluBench 200-sample benchmark
- 75x data efficiency: 200 training samples vs 15,000 for comparable Lynx system
- 1000x inference speedup: 5ms vs 5s compared to 70B LLM judges
- Entity-Focused Uptake consistently ranks as top contributor across models
- Rationalization signal shows no discriminative power (sycophancy: coherent reasoning for false premises)

## Why This Works (Mechanism)

### Mechanism 1: Uptake Signal (Predictive Coding)
- Claim: Hallucinations manifest as "low-uptake" states where the model's generated answer contradicts latent priors established by context.
- Mechanism: Computes KL divergence between P(A|Q,C) and P(A|Q)—measuring how much the context shifted the answer distribution. High divergence implies factual grounding; low divergence suggests prior-driven generation.
- Core assumption: Factual answers are context-dependent; hallucinations persist even when context is withheld because they derive from parametric memory.

### Mechanism 2: Stress Signal (Information Bottleneck)
- Claim: Hallucinated content degrades faster under semantic perturbation than factual knowledge.
- Mechanism: Perturbs extracted claims via paraphrasing, then measures Jensen-Shannon divergence of entailment probability distributions. High stress indicates the model "waffles" on claim truth under minor phrasing changes.
- Core assumption: Factual statements occupy smoother manifolds with dense connectivity; hallucinations are "stitched together" from disparate latent regions and lack robustness.

### Mechanism 3: Entity-Focused Uptake Enhancement
- Claim: Hallucinations concentrate in high-value tokens (entities, numbers, dates) rather than stopwords, and weighting signals accordingly improves detection.
- Mechanism: Amplifies base Uptake by entity density: U_entity = U_base × (1 + α × |entities|/|tokens|). This prevents low-information tokens from diluting the signal.
- Core assumption: Standard KL divergence treats all tokens equally, obscuring where factual errors actually occur.

## Foundational Learning

- **KL Divergence**
  - Why needed here: Core mathematical operation for Uptake signal; measures how much one probability distribution differs from another.
  - Quick check question: Can you explain why KL divergence is asymmetric and why that matters for comparing P(A|Q,C) vs P(A|Q)?

- **Jensen-Shannon Divergence**
  - Why needed here: Used for Stress signal; symmetric alternative to KL that handles distribution comparison more stably.
  - Quick check question: What property does JS divergence have that KL lacks, and why does the paper prefer it for entailment distribution comparison?

- **Natural Language Inference (NLI)**
  - Why needed here: Powers Conflict signal; determines whether perturbed claims contradict original answers.
  - Quick check question: Given two sentences, how would an NLI model classify their relationship as entailment, contradiction, or neutral?

## Architecture Onboarding

- **Component map:** Signal Extraction Layer (Uptake → KL, Stress → JS, Conflict → NLI, Rationalization → Jaccard) → Enhancement Layer (Entity-Focused Uptake, Context Adherence, Falsifiability) → Aggregation Layer (ESI harmonic mean or Random Forest) → Binary classification + decomposable signal diagnostics

- **Critical path:** 1) Compute P(A|Q) and P(A|Q,C) → Uptake via log-likelihood differences 2) Extract atomic claims from answer → perturb via paraphrasing 3) Run NLI on original vs perturbed claims → Stress and Conflict 4) Stack features → classify or aggregate via ESI

- **Design tradeoffs:** Theory-guided unsupervised (0.8017 AUROC, fully interpretable, zero training data) vs supervised (0.8669 AUROC, requires labeled data); Random Forest (best AUROC 0.8669) vs Gradient Boosting (BASE mode degrades to 0.7630); 5ms inference vs 5s for 70B LLM judges—1000x speed at cost of ~1% accuracy gap vs Lynx

- **Failure signatures:** Rationalization signal shows no discriminative power (sycophancy: models generate coherent reasoning for false premises); short contexts reduce Context Adherence reliability; Gradient Boosting highly sensitive to BASE features; requires IMPROVED feature set

- **First 3 experiments:** 1) Replicate theory-guided baseline (0.8017 AUROC) on HaluBench 200-sample subset to validate signal extraction pipeline 2) Ablate each enhanced signal individually (Entity-Focused, Context Adherence, Falsifiability) to confirm +4.95% gain is distributed as reported 3) Test on domain-shifted data (e.g., medical or legal RAG) to assess whether the "smoother manifold" assumption for facts holds outside benchmark distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PCIB signals generalize to multilingual contexts and abstractive summarization tasks?
- Basis in paper: [explicit] The conclusion states: "Future work includes extending PCIB signals to multilingual contexts, abstractive summarization tasks, and exploring the relationship between our hallucination detection signals and broader Natural Language Inference benchmarks."
- Why unresolved: Current evaluation is limited to English RAG question-answering on HaluBench. Entity extraction (via spaCy) and perturbation strategies may not transfer directly to morphologically rich languages or abstractive tasks.
- What evidence would resolve it: Evaluation of PCIB on multilingual hallucination benchmarks and summarization datasets with appropriate language-specific NLI models.

### Open Question 2
- Question: What is the optimal hybrid architecture combining PCIB with LLM judges for production deployment?
- Basis in paper: [explicit] The paper proposes: "For enterprise deployments, a hybrid architecture combining both approaches may be optimal: PCIB provides fast, interpretable first-pass filtering (eliminating 80% of queries), while Lynx handles edge cases requiring deep semantic reasoning on the remaining 20%."
- Why unresolved: This is a design proposal without empirical validation. The 80/20 split is stated as a heuristic without calibration experiments.
- What evidence would resolve it: A study measuring the trade-off frontier between PCIB threshold settings, Lynx invocation rates, overall accuracy, and latency/cost on held-out data.

### Open Question 3
- Question: Why does the Rationalization signal fail, and can sycophancy be mitigated in reasoning-based verification?
- Basis in paper: [explicit] The paper reports a negative result: "the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ('Sycophancy')." It further states: "Our findings suggest that LLMs exhibit 'sycophancy,' generating coherent but unfaithful reasoning traces that support false premises."
- Why unresolved: The mechanism is hypothesized but not experimentally isolated. It remains unclear whether this is intrinsic to autoregressive generation, a training data artifact, or prompt-dependent.
- What evidence would resolve it: Controlled experiments varying reasoning trace generation conditions (e.g., counterfactual prompts, debate-style verification) to determine if sycophancy can be reduced or if reasoning-based verification is fundamentally limited.

## Limitations

- The core mechanism relies on theoretical assumptions about "smoother latent manifolds" that are plausible but not empirically proven for LLMs specifically
- The entity-focused enhancement assumes hallucinations concentrate in entities, which may not hold for logical or verb-based hallucinations
- The claim of 1000x faster inference conflates different computational paradigms by comparing a 5ms classifier to 5s inference of an entirely different model class

## Confidence

- **High confidence**: AUROC performance metrics (0.8669 achieved, 0.8017 baseline), data efficiency claims (200 vs 15,000 samples), inference speed measurements (5ms vs 5s)
- **Medium confidence**: Signal mechanism explanations (KL divergence for uptake, JS divergence for stress), entity-focused enhancement effectiveness (+4.95% gain), negative rationalization result
- **Low confidence**: Underlying neuroscience assumptions about "smoother manifolds," generalizability beyond HaluBench benchmark, cross-domain robustness

## Next Checks

1. **Manifold validation**: Test whether factual answers consistently show higher stress robustness than hallucinations across diverse domains (medical, legal, technical) to validate the core "smoother manifold" assumption
2. **Entity assumption test**: Systematically generate hallucinations focused on logical connectors and verbs (not entities) to verify entity-weighting doesn't miss critical hallucination types
3. **Cross-architecture generalization**: Evaluate Pcib signals on non-RAG LLMs (e.g., direct question-answering) to determine if context adherence and uptake signals degrade without explicit context-document pairs