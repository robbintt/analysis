---
ver: rpa2
title: 'Executable Governance for AI: Translating Policies into Rules Using LLMs'
arxiv_id: '2512.04408'
source_url: https://arxiv.org/abs/2512.04408
tags:
- rules
- policy
- rule
- span
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Policy-to-Tests (P2T), a framework that converts
  natural-language policy documents into machine-readable rules for AI governance.
  The core method uses a pipeline with structured extraction, LLM-based judging, and
  semantic deduplication to produce actionable rules.
---

# Executable Governance for AI: Translating Policies into Rules Using LLMs

## Quick Facts
- arXiv ID: 2512.04408
- Source URL: https://arxiv.org/abs/2512.04408
- Reference count: 37
- Primary result: Framework converts natural-language policy documents into machine-readable rules, reducing AI agent violations from 34% to 5% using HIPAA-derived rules.

## Executive Summary
The paper introduces Policy-to-Tests (P2T), a framework that converts natural-language policy documents into machine-readable rules for AI governance. The core method uses a pipeline with structured extraction, LLM-based judging, and semantic deduplication to produce actionable rules. Evaluation shows the framework achieves span-level F1 scores up to 0.97 and rule-level coverage up to 94%, with strong alignment to human baselines. Rules generated from HIPAA policies reduced AI agent violations from 34% to 5% overall, with robust performance on obfuscated and compositional prompts. The approach generalizes across domains and enables scalable, automated policy compliance for AI systems.

## Method Summary
The framework uses a 6-stage pipeline: ingestion/chunking of policy documents into paragraph-level spans, optional clause mining with deontic filtering, structured extraction using LLMs under a strict JSON DSL schema with judge-repair validation, structural and semantic deduplication, testability tagging with evidence signal classification, and example generation for I/O-testable rules. The method was evaluated on three policy sets (EU AI Act, NIST AI RMF, HIPAA) using GPT-4o-mini, achieving high span F1 scores and demonstrating practical enforcement through reduced violation rates in AI agent evaluations.

## Key Results
- Span-level F1 scores up to 0.97 and rule-level coverage up to 94% on HIPAA policy extraction
- Judge+Repair loop improves coverage from 0.88 to 0.94 and span F1 from 0.83 to 0.88
- HIPAA-derived rules reduced AI agent violations from 34% to 5% overall in 60-prompt evaluation
- Semantic deduplication reduced unique rule count by 15-20% with minimal recall loss

## Why This Works (Mechanism)

### Mechanism 1
Schema-guarded LLM extraction with a judge-repair loop improves rule quality over naive few-shot extraction. An LLM extracts atomic rules constrained to a strict JSON DSL (fixed fields: scope, hazard, conditions, exceptions, requirement, testability). A second LLM ("judge") flags missing or malformed fields; a repair model applies minimal edits that preserve provenance. This iterative correction reduces structural errors and field omissions.

Core assumption: The DSL schema can adequately represent the policy's semantic content, and LLMs can reliably judge and repair without introducing new errors.

Evidence anchors:
- [abstract] "The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence."
- [section] Table 3 shows Judge+Repair improves coverage (0.88→0.94), span F1 (0.83→0.88), and slot similarity across GPT-4o-mini variants.
- [corpus] Related work (Policy-as-Prompt) similarly uses structured prompts to operationalize governance but lacks the judge-repair validation loop.

Break condition: If source clauses contain deeply ambiguous or context-dependent language that the DSL cannot capture, the judge may accept incomplete rules or hallucinate plausible but incorrect fields.

### Mechanism 2
Semantic deduplication consolidates paraphrased rules, reducing reviewer burden with controlled recall loss. After structural deduplication (canonical signature over scope/hazard/conditions), rules are embedded and high-similarity pairs within the same document and scope are merged. This collapses restatements of the same obligation from different clause spans.

Core assumption: Embedding similarity correlates with semantic equivalence of obligations, and merging does not discard distinct but superficially similar rules.

Evidence anchors:
- [abstract] "The core method uses a pipeline with structured extraction, LLM-based judging, and semantic deduplication."
- [section] Table 3 shows adding deduplication increases the duplicate index (0.15→0.20) but slightly reduces coverage (0.94→0.89), indicating a precision-recall trade-off.
- [corpus] No direct corpus evidence for this specific deduplication approach; related governance work focuses on extraction rather than consolidation.

Break condition: If policies use subtle qualifier differences ("may" vs. "shall") that embedding similarity misses, semantically distinct rules may be incorrectly merged.

### Mechanism 3
Evidence-gated testability tagging enables downstream enforcement by identifying operationally verifiable rules. An LLM assesses each rule against a rubric to determine if an objective pass-fail oracle exists, then assigns evidence signals from a closed vocabulary (io_check, log_check, config_check, ci_gate, data_check, repo_check, access_check, attest_check). Only rules with viable evidence channels are marked testable.

Core assumption: The evidence signal taxonomy is sufficiently comprehensive, and LLMs can reliably map abstract requirements to concrete verification channels.

Evidence anchors:
- [abstract] "Rules generated from HIPAA policies reduced AI agent violations from 34% to 5% overall."
- [section] Table 4 shows guarded agents (using 3 HIPAA-derived, I/O-testable rules) reduce violation rates from 0.34 to 0.05 overall, with robustness to obfuscated and compositional prompts.
- [corpus] Governance-as-a-Service (neighbor paper) similarly proposes multi-agent enforcement but does not address evidence channel classification.

Break condition: If rules require evidence channels outside the closed vocabulary (e.g., third-party audits, human attestations), they may be incorrectly marked untestable.

## Foundational Learning

**Deontic Logic Markers (obligation, prohibition, permission)**
- Why needed here: Clause mining relies on detecting deontic cues ("shall," "must," "may not") to filter obligation-bearing spans from definitions and boilerplate.
- Quick check question: Can you identify the deontic marker in: "Providers shall ensure that records of processing activities state why special-category data were necessary"?

**Satisfiability Modulo Theories (SMT)**
- Why needed here: The pipeline optionally uses SMT solvers to detect logical contradictions where overlapping scopes require and forbid the same predicate.
- Quick check question: If Rule A says "PHI disclosure requires patient consent" and Rule B says "Emergency PHI disclosure does not require consent," would an SMT solver flag a conflict? Why or why not?

**Provenance Tracking**
- Why needed here: Every extracted rule carries source.doc, source.citation, and source.span_id so downstream enforcement can trace rules back to original clauses for auditing and updates.
- Quick check question: Why is paragraph-level chunking preferred over single-sentence spans for provenance in policy documents?

## Architecture Onboarding

**Component map:**
Ingestion & Chunking -> Clause Mining (optional) -> Structured Extraction -> Deduplication -> Testability Tagging -> Example Generation

**Critical path:** Extraction (Step 3) dominates quality; judge-repair loop is the highest-leverage control. Deduplication trades recall for precision—enable if reviewer capacity is limited.

**Design tradeoffs:**
- Paragraph vs. sentence chunking: Paragraph preserves context but increases duplicates; sentence minimizes duplicates but under-specifies rules.
- Judge+Repair adds ~30% runtime cost but improves slot similarity by 5–10 points.
- Semantic deduplication reduces coverage by ~5% but removes 15–20% of duplicate rules.

**Failure signatures:**
- Softened qualifiers: "shall not" → "should avoid"
- Scope misassignment: cross-referenced conditions missed
- Over-normalization: "may," "should," "shall" collapsed to same requirement strength
- Nested exceptions: multiple negations incorrectly simplified

**First 3 experiments:**
1. Run P2T on a single policy section with and without the judge-repair loop; compare span F1 and slot similarity against human gold annotations.
2. Enable/disable semantic deduplication on a multi-document corpus; measure unique rule count reduction and spot-check merged rules for semantic equivalence.
3. Export testable HIPAA rules to NeMo Guardrails or Guardrails AI; run the 60-prompt evaluation (clean/obfuscated/compositional) and compare violation rates against the baseline assistant.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can the DSL be extended to capture temporal and probabilistic constraints in policy language?
- Basis in paper: [explicit] Conclusion states "a compact DSL might not capture all nuances (e.g., temporal or probabilistic constraints)."
- Why unresolved: The current schema encodes hazards, scope, conditions, and exceptions as natural-language strings or simple enums, which cannot formally represent deadlines, conditional probabilities, or time-bounded obligations.
- What evidence would resolve it: A schema extension that parses temporal markers ("within 30 days") and probabilistic qualifiers ("where feasible") into machine-interpretable fields, validated against a gold set of time-sensitive clauses.

**Open Question 2**
- Question: What interactive validation mechanisms most effectively reduce LLM hallucination in rule extraction?
- Basis in paper: [explicit] Conclusion notes models "can hallucinate or generate plausible but incorrect rules" and calls for "interactive validation loops (e.g., user feedback)."
- Why unresolved: The current pipeline uses a Judge+Repair module but lacks human-in-the-loop feedback that could catch fabricated obligations before downstream enforcement.
- What evidence would resolve it: A user study comparing hallucination rates between fully automated extraction and variants with targeted human review at high-uncertainty checkpoints.

**Open Question 3**
- Question: Can standardized benchmarks be developed to enable direct cross-method comparisons for policy-to-rule translation?
- Basis in paper: [explicit] Evaluation section states "there are no established benchmarks for policy-to-rule translation" and the authors rely on internal consistency and human agreement instead.
- Why unresolved: Without shared test sets and metrics, different approaches cannot be fairly compared, slowing progress in the field.
- What evidence would resolve it: A public benchmark suite with diverse policy documents, gold rule annotations, and standardized span-level and rule-level metrics adopted by multiple independent research groups.

## Limitations

- The DSL may not capture temporal or probabilistic constraints in policy language, limiting expressiveness for time-sensitive or conditional obligations.
- Semantic deduplication risks merging distinct rules with subtle qualifier differences, particularly when policies use nuanced language like "may" versus "shall."
- The evidence taxonomy may be insufficient for policies requiring verification channels outside the predefined vocabulary, leading to false-negative testability tagging.

## Confidence

- **High Confidence:** Span-level F1 scores (up to 0.97) and violation rate reductions (34%→5%) - these are directly measured and reproducible.
- **Medium Confidence:** Rule coverage improvements (0.83→0.94) - depends on annotation quality and schema completeness.
- **Medium Confidence:** Generalization across domains - demonstrated on three policy sets but limited diversity of source material.

## Next Checks

1. Run the judge-repair loop on policies with nested exceptions and multi-sentence deontic cues; measure slot similarity degradation and analyze specific failure patterns.
2. Apply semantic deduplication to a diverse corpus including policies with subtle qualifier differences; manually verify merged rules for semantic equivalence.
3. Test the framework on policies requiring evidence channels outside the current taxonomy; measure false-negative rates in testability tagging.