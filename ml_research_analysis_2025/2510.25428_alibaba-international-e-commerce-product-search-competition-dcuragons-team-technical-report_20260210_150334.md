---
ver: rpa2
title: Alibaba International E-commerce Product Search Competition DcuRAGONs Team
  Technical Report
arxiv_id: '2510.25428'
source_url: https://arxiv.org/abs/2510.25428
tags:
- multilingual
- e-commerce
- performance
- tasks
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report details the approach and results of the DcuRAGONs team
  in the Alibaba International E-commerce Product Search Competition, focusing on
  multilingual relevance prediction between user queries and product items. The key
  innovation lies in a data-centric methodology that leverages Large Language Models
  (LLMs) enhanced with translation augmentation and task-adaptive pre-training (TAPT)
  to handle noisy, multilingual search queries.
---

# Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report

## Quick Facts
- arXiv ID: 2510.25428
- Source URL: https://arxiv.org/abs/2510.25428
- Authors: Thang-Long Nguyen-Ho; Minh-Khoi Pham; Hoang-Bao Le
- Reference count: 1
- Primary result: Achieved average F1 score of 89.36% on private test set

## Executive Summary
This report details the DcuRAGONs team's approach to the Alibaba International E-commerce Product Search Competition, focusing on multilingual relevance prediction between user queries and product items. The team developed a data-centric methodology leveraging Large Language Models (LLMs) enhanced with translation augmentation and task-adaptive pre-training (TAPT) to handle noisy, multilingual search queries. A category-aware, query-grouped cross-validation strategy was introduced to ensure robust generalization. Experiments across various model scales—from XLM-Roberta to Gemma-3-12B—demonstrate that larger, multilingual LLMs with TAPT yield the best performance, achieving an average F1 score of 89.36% on the private test set.

## Method Summary
The approach centers on a two-stage supervised training protocol combining translation augmentation with task-adaptive pre-training. Input sequences concatenate original multilingual queries with their English translations, followed by target text (category or item). Models are trained using LoRA adapters on a Gemma-3-12B backbone, with validation employing a custom category-aware, query-grouped cross-validation strategy to prevent data leakage. The methodology emphasizes careful data handling, model architecture choice, and adaptive fine-tuning to build effective multilingual e-commerce search systems.

## Key Results
- Gemma-3-12B with translation augmentation and TAPT achieved the highest performance (89.36% average F1)
- Translation augmentation provided consistent gains across all model scales tested
- Task-adaptive pre-training improved generalization, with QC F1 improving by +0.14 and QI F1 by +0.02
- Larger multilingual models consistently outperformed smaller models, with XLM-Roberta showing ~7 F1 point deficit

## Why This Works (Mechanism)

### Mechanism 1: Translation-Augmented Input Representation
- **Claim:** Concatenating original multilingual queries with their English translations improves cross-lingual relevance prediction, particularly for low-resource languages.
- **Mechanism:** The English translation serves as a "universal pivot" that grounds ambiguous or noisy queries in a semantically richer representation, while retaining the original query preserves language-specific cues. The model learns to align both representations with target product text.
- **Core assumption:** The machine translation model produces sufficiently accurate translations; the encoder can effectively fuse both representations without introducing confusion.
- **Evidence anchors:**
  - [abstract]: "data-centric methodology that leverages Large Language Models (LLMs) enhanced with translation augmentation"
  - [section 2.1]: "qen serves as a universal pivot, guiding the model when it encounters unseen or low-resource languages by grounding the meaning in a high-resource, semantically rich representation"
  - [corpus]: Related competition reports (DILAB, Tredence_AICOE) similarly emphasize multilingual handling, though specific augmentation strategies vary.
- **Break condition:** Systematically poor translations for very low-resource languages could introduce noise that degrades performance on well-represented languages.

### Mechanism 2: Task-Adaptive Pre-Training (TAPT)
- **Claim:** Two-stage training—supervised pre-training on an auxiliary relevance task followed by fine-tuning on the target task—yields better generalization than direct fine-tuning.
- **Mechanism:** The auxiliary task (either Query-Category or Query-Item) exposes the model to domain-specific terminology, abbreviations, and semantic patterns in e-commerce search logs before specialization on the final task.
- **Core assumption:** The two tasks share transferable semantic features; the auxiliary task has sufficient labeled data without introducing distribution shift.
- **Evidence anchors:**
  - [abstract]: "task-adaptive pre-training (TAPT) to handle noisy, multilingual search queries"
  - [section 2.2]: "This two-stage process effectively specializes the model to the e-commerce domain, mitigating overfitting, improving generalization on low-resource languages"
  - [section 4.2.1, Table 3]: TAPT improved QC F1 by +0.14 and QI F1 by +0.02 on the private leaderboard.
  - [corpus]: Weak direct evidence—corpus papers focus on data-centric approaches but don't systematically compare TAPT.
- **Break condition:** If the auxiliary and target tasks have conflicting label distributions or semantic patterns, negative transfer could occur.

### Mechanism 3: Category-Aware Query-Grouped Cross-Validation (CA Split)
- **Claim:** A custom cross-validation strategy that groups identical queries and category prefixes prevents data leakage and provides more realistic performance estimates.
- **Mechanism:** For Query-Item tasks, all samples sharing the same query text are forced into the same fold. For Query-Category tasks, categories sharing a common prefix are grouped together, preventing the model from learning shallow string-matching patterns.
- **Core assumption:** Real-world deployment requires generalization to genuinely unseen queries and category structures; query identity or category prefix overlap enables shortcut learning.
- **Evidence anchors:**
  - [section 2.3]: "naive random or even stratified split... can lead to data leakage, where the model learns superficial correlations rather than generalizable patterns"
  - [section 2.3.1]: "the same query often appears multiple times with different items and relevance labels"
  - [section 2.3.2]: "category paths sharing a common prefix... contain transferable semantic features"
  - [corpus]: Competition context implies this was a shared concern, but no systematic comparison of splitting strategies in corpus.
- **Break condition:** If the test set has a different distribution of query duplicates or category hierarchies than the CA split assumes, validation scores may not correlate with test performance.

## Foundational Learning

- **Concept: Cross-lingual Transfer Learning**
  - **Why needed here:** The dataset spans 20+ languages including low-resource ones with little labeled data. Models must generalize zero-shot or few-shot to unseen languages.
  - **Quick check question:** Can you explain why a model pretrained on many languages might still fail on a specific low-resource language not in its pretraining corpus?

- **Concept: Data Leakage in Evaluation**
  - **Why needed here:** Query duplication and hierarchical category structures create leakage risks where models memorize patterns rather than learn semantics.
  - **Quick check question:** If the same query appears in both training and validation with different labels, what artifact might the model learn?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** Models up to 12B parameters must be fine-tuned efficiently on limited GPU memory while preserving pretrained multilingual knowledge.
  - **Quick check question:** Why might full fine-tuning of a multilingual LLM cause catastrophic forgetting of low-resource language capabilities?

## Architecture Onboarding

- **Component map:** Raw Query → Translation Model → [q_orig + q_en] → Multilingual Encoder (Gemma-3-12B) → LoRA Adapters → Target Text (Category/Item) → [SEP] token → Classification Head → Relevance Score

- **Critical path:**
  1. Implement CA split cross-validation first—this defines your evaluation integrity.
  2. Set up translation augmentation pipeline with a high-quality translation model.
  3. Run Stage 1 baseline experiments to select the best foundational model.
  4. Implement TAPT by pre-training on the auxiliary task before target task fine-tuning.

- **Design tradeoffs:**
  - **Model scale vs. efficiency:** Gemma-3-12B achieved best results but requires 4× A100 GPUs. XLM-Roberta is 20× smaller but loses ~7 F1 points.
  - **Translation quality vs. noise:** Poor translations for very low-resource languages may hurt more than help—consider ablation per language.
  - **TAPT data source:** Using QC data for QI task (or vice versa) may help or hurt depending on task similarity.

- **Failure signatures:**
  - Validation F1 high but test F1 drops significantly → likely data leakage from improper splits.
  - Performance degrades on high-resource languages after translation augmentation → original cues being overshadowed.
  - TAPT reduces performance → auxiliary task has conflicting semantics with target task.

- **First 3 experiments:**
  1. **Baseline comparison:** Run XLM-Roberta, Qwen3-7B, and Gemma-3-12B with vanilla fine-tuning (no TA, no TAPT, random split) to establish raw architectural differences.
  2. **Translation augmentation ablation:** Compare [q_orig only] vs. [q_orig + q_en] vs. [q_en only] to quantify contribution of each representation.
  3. **CA split validation:** Compare random stratified split vs. CA split on the same model to measure the gap between optimistic and realistic performance estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a unified multitask framework jointly learning Query-Category (QC) and Query-Item (QI) tasks yield better generalization than the separate modeling approaches used in this study?
- Basis in paper: [explicit] The authors identify developing a "unified framework for joint modeling" as a primary direction for future work to leverage shared representations.
- Why unresolved: The current study trained tasks separately due to resource constraints, leaving the potential cross-task transfer benefits unexplored.
- What evidence would resolve it: Experiments demonstrating that a multi-head model trained on both tasks simultaneously outperforms the single-task baselines on the private test set.

### Open Question 2
- Question: Can explicitly encoding the hierarchical structure of product categories improve prediction accuracy compared to the current method of treating them as flat text strings?
- Basis in paper: [explicit] The limitations section notes that the models were not optimized to exploit the "hierarchical structure of category paths," which could provide necessary inductive bias.
- Why unresolved: The team relied solely on text concatenation for category representation, potentially missing structural relationships that could prevent invalid predictions.
- What evidence would resolve it: Ablation studies comparing the current architecture against models with hierarchical encoders or specialized loss functions for taxonomy.

### Open Question 3
- Question: To what degree does noise from machine translation degrade the Translation Augmentation (TA) strategy, particularly for low-resource languages?
- Basis in paper: [explicit] The authors acknowledge that the augmentation strategy "may introduce noise in low-quality translations," which was not "systematically evaluate[d]."
- Why unresolved: While TA improved overall performance, the specific trade-off between semantic grounding and translation error remains unknown.
- What evidence would resolve it: An error analysis correlating translation quality scores (e.g., BLEU) with relevance prediction accuracy across different languages.

## Limitations

- The translation augmentation approach depends heavily on the quality and characteristics of the specific machine translation model used, which is not disclosed.
- The category-aware cross-validation strategy requires assumptions about category prefix depth that may not generalize to all e-commerce taxonomies.
- The auxiliary tasks used for TAPT may have different semantic alignment with the target tasks, creating potential negative transfer risks that were not systematically explored.

## Confidence

**High Confidence** - The core architectural claims regarding multilingual LLM scaling benefits and the effectiveness of translation augmentation have strong empirical support from the reported experiments, with clear performance differences across model sizes and augmentation conditions.

**Medium Confidence** - The TAPT methodology and its contribution to performance gains are reasonably supported by the reported improvements, though the lack of detailed hyperparameter information and the absence of ablation studies comparing different auxiliary tasks reduce confidence in the specific implementation choices.

**Low Confidence** - The custom CA split validation strategy's superiority over standard approaches is primarily supported by theoretical reasoning rather than systematic empirical comparison, and the exact implementation details needed for faithful reproduction are incomplete.

## Next Checks

1. **Translation Quality Ablation Study**: Systematically evaluate the contribution of translation augmentation by testing performance across different MT models (NLLB-200, Google Translate API) and language pairs, particularly focusing on low-resource languages where translation quality may vary significantly.

2. **Cross-Validation Strategy Comparison**: Conduct a controlled experiment comparing the CA split against standard random and stratified splits on the same model architecture to quantify the empirical impact of the custom validation strategy on generalization estimates.

3. **TAPT Task Transfer Analysis**: Perform systematic ablations testing different auxiliary task choices (QC→QI vs QI→QC) and timing strategies (early vs late TAPT) to understand the conditions under which task-adaptive pre-training provides maximum benefit versus potential negative transfer.