---
ver: rpa2
title: 'Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous
  GUI Agents'
arxiv_id: '2511.10705'
source_url: https://arxiv.org/abs/2511.10705
tags:
- arxiv
- grounding
- training
- planning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Co-EPG, a self-iterative training framework
  for co-evolution of planning and grounding in autonomous GUI agents. The core idea
  is to establish an iterative positive feedback loop where the planning model explores
  strategies under grounding-based reward guidance via GRPO, generating diverse data
  to optimize the grounding model, which in turn provides more effective rewards for
  the planning model.
---

# Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents

## Quick Facts
- arXiv ID: 2511.10705
- Source URL: https://arxiv.org/abs/2511.10705
- Reference count: 22
- Primary result: Co-EPG achieves 58.4% average Step Success Rate on Multimodal-Mind2Web and 83.1% average Step Accuracy on AndroidControl without external data

## Executive Summary
This paper proposes Co-EPG, a self-iterative training framework for co-evolution of planning and grounding in autonomous GUI agents. The core idea is to establish an iterative positive feedback loop where the planning model explores strategies under grounding-based reward guidance via GRPO, generating diverse data to optimize the grounding model, which in turn provides more effective rewards for the planning model. A confidence-based dynamic reward ensemble mechanism (C-DREM) is introduced to reduce reward noise by aggregating rewards from multiple grounding models with confidence-based weighting. Experiments on Multimodal-Mind2Web and AndroidControl benchmarks show that Co-EPG outperforms existing state-of-the-art methods after just three iterations without requiring external data, achieving 58.4% average Step Success Rate on Multimodal-Mind2Web and 83.1% average Step Accuracy on AndroidControl. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities.

## Method Summary
Co-EPG uses a P-G dual-model architecture where the planning model outputs structured tuples (plan description, action type, action value) and the grounding model predicts coordinates from (screenshot, plan). The iterative co-evolution loop consists of three phases per iteration: (1) SFT on dataset D_{k-1} to produce updated models π_k and ϕ_k; (2) GRPO training for π_k using C-DREM rewards; (3) Data Enhancement using Planner pool Π and Verifier pool Φ to build D_k. C-DREM aggregates rewards from three grounding models with static prior weights and dynamic confidence-based weights derived from token log-likelihoods. The framework is trained on Multimodal-Mind2Web and AndroidControl benchmarks using Qwen2.5-VL backbones with MS-SWIFT implementation.

## Key Results
- Co-EPG achieves 58.4% average Step Success Rate on Multimodal-Mind2Web benchmark
- Co-EPG achieves 83.1% average Step Accuracy on AndroidControl benchmark
- Performance improves consistently across three iterations without external data

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Co-Evolution Feedback Loop
- Claim: Jointly training planning and grounding models through mutual reinforcement improves both capabilities more effectively than isolated optimization.
- Mechanism: The planning model generates diverse action strategies guided by grounding-based rewards via Group Relative Policy Optimization (GRPO). Successful plans distill into training data that improves the grounding model. The refined grounding model then provides higher-quality reward signals for subsequent planning iterations, creating a virtuous cycle.
- Core assumption: Plan executability (whether grounding can locate the target) is a valid proxy for plan quality.
- Evidence anchors:
  - [abstract] "Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model."
  - [section 4.2] "This process iteratively aligns the capabilities of the planning and grounding models."
  - [corpus] Related work (ScaleTrack, UI-AGILE) explores iterative/refinement approaches for GUI agents, suggesting this is an active research direction, though no direct comparison to co-evolution is available.
- Break condition: If the grounding model fails to provide meaningful reward gradients (e.g., always succeeds or always fails regardless of plan quality), the feedback loop collapses into random exploration.

### Mechanism 2: Confidence-Based Dynamic Reward Ensemble (C-DREM)
- Claim: Aggregating rewards from multiple grounding models weighted by their confidence reduces reward noise and improves GRPO training stability.
- Mechanism: Instead of using a single grounding model as the reward signal, C-DREM computes a weighted sum of rewards from N grounding models. Weights combine a static prior (favoring the trained production model) with a dynamic confidence score based on the log-likelihood of predicted coordinate tokens.
- Core assumption: Model confidence (token log-likelihood) correlates with prediction reliability across diverse inputs.
- Evidence anchors:
  - [section 4.3] "To address the inherent bias and poor performance of single reward model on out-of-distribution data, we propose C-DREM, a confidence-based dynamic reward ensemble mechanism."
  - [section 5.4, Table 4] Ablation shows C-DREM achieves 58.41% vs. 56.50% without ensemble (1.91% improvement).
  - [corpus] No direct corpus evidence on reward ensembling for GUI agents; this appears novel to this framework.
- Break condition: If all grounding models are miscalibrated (high confidence on wrong predictions), weighting amplifies rather than reduces noise.

### Mechanism 3: Decoupled Planning-Grounding Architecture with Explicit Plan Interface
- Claim: Separating planning and grounding into specialized models with plans as the interface improves compositional generalization over end-to-end models.
- Mechanism: The planning model outputs a structured tuple (plan description, action type, action value). The grounding model takes the plan description + visual input to predict coordinates. This explicit interface forces the planning model to produce interpretable, actionable descriptions rather than implicit representations.
- Core assumption: Natural language plans are a sufficient communication medium between high-level reasoning and low-level visual grounding.
- Evidence anchors:
  - [section 4.1] "The decoupled design allows each model to specialize in its respective function, thereby enabling the GUI agent to efficiently manage complex multi-step tasks."
  - [section 5.4, Table 3] Decoupled P-G architecture achieves 53.5% vs. 50.1% for end-to-end (3.4% improvement).
  - [corpus] Multiple related works (Mobile-Agent-v3, UI-AGILE) explore modular architectures, supporting the general direction.
- Break condition: If plan descriptions become too ambiguous or too specific, the grounding model cannot reliably map them to coordinates, breaking the action pipeline.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**:
  - Why needed here: Core RL algorithm for training the planning model. Must understand how advantages are computed from group-wise reward normalization.
  - Quick check question: Can you explain why GRPO normalizes rewards across multiple rollouts (group computation) rather than using absolute rewards?

- **Vision-Language Model (VLM) Grounding**:
  - Why needed here: The grounding model maps textual plans to screen coordinates. Understanding how VLMs process multimodal inputs is essential.
  - Quick check question: How would you tokenize a screenshot and a text plan jointly for a VLM?

- **Self-Supervised Iterative Refinement**:
  - Why needed here: The framework's data enhancement relies on model-generated plans verified by other models, not human labels.
  - Quick check question: What failure modes could emerge from training on model-generated data without ground truth verification?

## Architecture Onboarding

- Component map:
  - Planner pool (Π) -> Planning model (π_k) -> Grounding model (ϕ_k) -> Verifier pool (Φ)

- Critical path:
  1. Initialize planner/verifier pools with open-source VLMs
  2. Generate initial dataset D₀ by filtering verified plans
  3. For each iteration k: SFT both models → GRPO training on planning model with C-DREM rewards → Data enhancement with updated models → Repeat

- Design tradeoffs:
  - Ensemble size vs. compute: Paper uses 3 grounding models; larger ensembles may have diminishing returns (see Table 5 on prior weight sensitivity)
  - Decoupled vs. end-to-end: 3.4% accuracy gain but requires coordinating two models at inference
  - Self-generated vs. external data: Data efficiency (2.42% of labeled data) vs. potential distribution drift

- Failure signatures:
  - Planning collapse: Plans become generic/uninformative (case study M₁ output merging multiple UI targets)
  - Grounding-reward mismatch: C-DREM weights favor consistently wrong models
  - Iteration divergence: Performance plateaus or degrades after M₃ (paper shows continued improvement, but extrapolation uncertain)

- First 3 experiments:
  1. Replicate the P-G dual-model vs. end-to-end ablation (Table 3) on a held-out benchmark to validate decoupled architecture benefit.
  2. Ablate C-DREM components (remove confidence, remove prior, single model) to understand reward quality contribution (replicate Table 4).
  3. Run a single iteration of the co-evolution loop and plot data quality metrics (purity, diversity) to verify the self-enhancement mechanism before committing to full multi-iteration training.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements rely heavily on the quality of the iterative data generation loop, which may amplify initial weaknesses rather than correct them
- The framework's effectiveness depends on the assumption that token log-likelihood correlates with grounding reliability, which lacks direct validation
- The sensitivity to initial Planner pool composition is not extensively validated

## Confidence
- **High Confidence:** The decoupled P-G architecture provides measurable gains over end-to-end approaches (3.4% improvement on Multimodal-Mind2Web), supported by direct ablation experiments
- **Medium Confidence:** The C-DREM mechanism reduces reward noise and improves stability, but the underlying assumption about confidence-score correlation with prediction quality lacks direct validation
- **Medium Confidence:** The co-evolution loop demonstrates consistent improvement over three iterations, though extrapolation beyond M₃ is uncertain without additional empirical evidence

## Next Checks
1. Ablate on Initial Model Quality: Run Co-EPG starting from weaker Planner pool members to test whether the framework can recover from poor initial data, or if it amplifies early weaknesses
2. Cross-Domain Transfer Test: Evaluate whether grounding models trained via Co-EPG generalize to unseen app domains (e.g., train on AndroidControl, test on iOS or web interfaces) to validate compositional generalization
3. Reward Calibration Analysis: Measure the correlation between C-DREM confidence scores and actual grounding accuracy on a held-out validation set to verify the validity of the confidence-based weighting assumption