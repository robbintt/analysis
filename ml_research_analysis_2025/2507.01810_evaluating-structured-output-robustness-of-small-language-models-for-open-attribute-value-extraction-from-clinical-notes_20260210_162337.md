---
ver: rpa2
title: Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value
  Extraction from Clinical Notes
arxiv_id: '2507.01810'
source_url: https://arxiv.org/abs/2507.01810
tags:
- parseability
- extraction
- clinical
- format
- json
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the parseability of structured outputs from\
  \ small language models (SLMs) performing open attribute-value extraction from clinical\
  \ notes. Three serialization formats\u2014JSON, YAML, and XML\u2014were compared\
  \ across seven open-weight models, varying in size (3\u201314B parameters) and prompting\
  \ conditions (open vs."
---

# Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes

## Quick Facts
- arXiv ID: 2507.01810
- Source URL: https://arxiv.org/abs/2507.01810
- Authors: Nikita Neveditsin; Pawan Lingras; Vijay Mago
- Reference count: 10
- Key outcome: JSON serialization format achieved the highest parseability for structured output from small language models performing open attribute-value extraction from clinical notes, with parseability improving with model size and prompt specificity.

## Executive Summary
This study systematically evaluates the parseability of structured outputs generated by small language models (SLMs) performing open attribute-value extraction from clinical notes. Across seven open-weight models ranging from 3B to 14B parameters, JSON consistently outperformed YAML and XML in terms of syntactic validity, particularly when paired with targeted prompting strategies. The research identifies key failure modes including infinite repetition and format-specific syntactic malformations, highlighting the importance of both model selection and prompt engineering for reliable clinical data extraction.

## Method Summary
The evaluation framework assessed seven open-weight language models (3-14B parameters) across three serialization formats (JSON, YAML, XML) for open attribute-value extraction from clinical notes. Models were tested under two prompting conditions: open-ended and targeted. Parseability was measured using a normalized syntactic validity metric ρ(D), calculated as the proportion of successfully parsed documents to total attempts. The study examined various document types including discharge summaries and physician notes, analyzing parseability across different document lengths and content structures.

## Key Results
- JSON serialization format achieved consistently higher parseability than YAML and XML across all model sizes and prompting conditions
- Parseability increased with model size and prompt specificity, demonstrating the importance of both architectural capacity and instruction design
- Clinical physician notes showed lower parseability rates compared to discharge summaries, attributed to structural complexity and dense formatting

## Why This Works (Mechanism)
None

## Foundational Learning
- Parseability metric (ρ(D)) - why needed: quantifies syntactic validity of structured outputs; quick check: ensure understanding of ratio calculation (parsed documents / total attempts)
- Serialization formats (JSON, YAML, XML) - why needed: different syntactic structures affect parseability; quick check: know key structural differences (brackets, indentation, tags)
- Attribute-value extraction - why needed: core task of identifying clinical entities and their values; quick check: understand difference between open vs. targeted extraction
- Greedy decoding - why needed: decoding strategy used in evaluation; quick check: know it selects highest probability token at each step
- Structured output - why needed: ensures machine-readable format for downstream processing; quick check: recognize importance of schema adherence

## Architecture Onboarding

**Component Map:**
Clinical notes → SLM (3-14B params) → Prompt (open/targeted) → Token generation → Serialization (JSON/YAML/XML) → Parseability validation

**Critical Path:**
Document input → Prompt engineering → Model inference → Token generation → Serialization formatting → Syntax validation

**Design Tradeoffs:**
Model size vs. computational efficiency, prompt specificity vs. flexibility, format simplicity vs. expressivity, parseability vs. semantic accuracy

**Failure Signatures:**
Infinite repetition in generation, unquoted numerals in non-numeric fields, mismatched brackets/tags, incorrect indentation in YAML, incomplete document structures

**First Experiments:**
1. Compare parseability rates across all three serialization formats using identical prompts and model configurations
2. Test impact of prompt specificity (open vs. targeted) on parseability for the same document types
3. Evaluate parseability degradation across increasing document lengths to identify thresholds

## Open Questions the Paper Calls Out
### Open Question 1
- Question: To what extent does syntactic parseability correlate with semantic accuracy in clinical attribute-value extraction?
- Basis in paper: [explicit] The Conclusion states future work should "support joint analysis of syntactic and semantic validity," and the Limitations section notes the study focused "exclusively on syntactic parseability."
- Why unresolved: High parseability (valid JSON/XML) does not guarantee that the extracted clinical values are correct or relevant; a model could produce structurally perfect but clinically hallucinated output.
- What evidence would resolve it: A study measuring both structural parseability rates and semantic F1 scores against gold-standard annotations simultaneously on the same dataset.

### Open Question 2
- Question: Can constrained decoding strategies or automatic post-processing effectively mitigate the specific failure modes identified, such as infinite repetition and unquoted numerals?
- Basis in paper: [explicit] The Conclusion suggests exploring "targeted prompt design and decoding strategies" and "automatic post-processing techniques" based on the error analysis which identified infinite repetition and syntactic malformations as key failure modes.
- Why unresolved: This study utilized greedy decoding; it is unknown if grammar-constrained decoding would suppress these structural errors without negatively impacting the model's ability to extract complex clinical information.
- What evidence would resolve it: Comparative experiments using grammar-constrained decoding (e.g., GBNF) vs. standard sampling on the identified "infinite repetition" error cases.

### Open Question 3
- Question: Do domain-specific clinical language models demonstrate superior structural robustness compared to the general open-weight models evaluated in this study?
- Basis in paper: [explicit] The Limitations section states that the evaluation was limited to a set of open-weight models and that "Future work should include domain-specific clinical language models."
- Why unresolved: General instruction-tuned models may lack the specific discipline to handle the dense, heterogeneous formatting of clinical text (e.g., vitals in physician notes) compared to models fine-tuned on medical corpora.
- What evidence would resolve it: Benchmarking clinical-specific SLMs against general models (e.g., Llama-3) of similar parameter sizes using the same parseability metrics (ρ(D)).

## Limitations
- Limited to seven open-weight SLMs within 3–14B parameter range, excluding larger frontier and proprietary models
- Evaluation restricted to synthetic and physician-annotated clinical notes, limiting generalizability to broader clinical contexts
- Error analysis is descriptive rather than systematically quantified, with failure modes identified qualitatively
- Does not address downstream task performance or real-world deployment constraints such as latency, cost, or workflow integration

## Confidence
- **High confidence**: JSON's superior parseability across models and formats is well-supported by quantitative results
- **Medium confidence**: Decline in parseability with document length and physician note complexity is observed but may be influenced by dataset-specific characteristics
- **Low confidence**: Claims about the necessity of targeted prompt design and decoding strategies are inferred rather than experimentally validated

## Next Checks
1. Replicate the parseability evaluation across a broader set of clinical note types (e.g., discharge summaries, radiology reports) to test generalizability beyond the current corpus
2. Conduct ablation studies isolating the impact of model size, prompt engineering, and serialization format on parseability, using statistical significance testing
3. Implement and benchmark error recovery strategies (e.g., schema validation, retry with constrained decoding) to quantify their effectiveness in mitigating identified failure modes