---
ver: rpa2
title: Evaluating Multimodal Generative AI with Korean Educational Standards
arxiv_id: '2502.15422'
source_url: https://arxiv.org/abs/2502.15422
tags:
- konet
- korean
- arxiv
- error
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KoNET, a benchmark for evaluating multimodal\
  \ generative AI models using Korean national educational tests. KoNET leverages\
  \ four Korean educational exams\u2014elementary, middle, high school, and college\u2014\
  to assess AI performance across increasing levels of difficulty."
---

# Evaluating Multimodal Generative AI with Korean Educational Standards

## Quick Facts
- arXiv ID: 2502.15422
- Source URL: https://arxiv.org/abs/2502.15422
- Reference count: 24
- Key outcome: KoNET benchmark reveals significant performance gaps in multimodal AI models on Korean educational tests, particularly highlighting challenges with Korean language and cultural contexts

## Executive Summary
This paper introduces KoNET, a benchmark for evaluating multimodal generative AI models using Korean national educational tests. KoNET leverages four Korean educational exams—elementary, middle, high school, and college—to assess AI performance across increasing levels of difficulty. The benchmark provides a unique evaluation framework by comparing AI performance with human error rates derived from real student data, particularly from the College Scholastic Ability Test (CSAT). Experiments across 46 models reveal significant performance gaps, especially in open-source models, highlighting challenges in Korean language and cultural contexts. The study also demonstrates the value of Chain-of-Thought prompting and highlights the need for advancements in non-English language models.

## Method Summary
The KoNET benchmark evaluates multimodal generative AI models on Korean national educational tests, comprising 2,377 multiple-choice questions across four educational levels (elementary, middle, high school, and college). The benchmark uses official PDFs from Korea Institute of Curriculum and Evaluation (KICE), parsing each question into a single grayscale image containing the question and answer choices. Models are evaluated using LLM-as-a-Judge (GPT-4o) with both Direct and Chain-of-Thought prompting formats. OCR pipelines using Naver Cloud OCR API provide text tokens for text-based models and supplementary input for MLLMs. Performance is measured as accuracy percentage across the four educational subsets, with correlation analysis against human error rates from the CSAT.

## Key Results
- EXAONE-3.0-7.8B-Instruct achieved 45.5% on KoNET, significantly outperforming other 7-8B models
- Chain-of-Thought prompting showed more pronounced improvements in high-performing closed-source models compared to open-source models
- Open-source MLLMs performed significantly worse than LLM+OCR approaches on Korean text recognition tasks
- Performance gaps between English-centric benchmarks (MMLU) and KoNET highlight the need for non-English language model evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistically and culturally targeted training data improves model performance on non-English benchmarks beyond what model scaling alone achieves.
- Mechanism: The EXAONE-3.0-7.8B-Instruct model, specifically designed for Korean (bilingual English/Korean), achieved a KoNET score of 45.5, significantly outperforming similar-sized models (7-8B range). This suggests training data composition, not just scale, drives cross-linguistic transfer.
- Core assumption: Performance gains reflect training data quality and language-specific optimization rather than benchmark overfitting.
- Evidence anchors:
  - "EXAONE-3.0-7.8B-Instruct... achieved a KoNET score of 45.5, significantly outperforming other models of similar size (7-8B)"
  - Cultural example: models lacking understanding of Yongbieocheonga (1445 Korean text) failed, while EXAONE succeeded
  - KMMLU-Redux/Pro benchmark paper confirms need for Korean professional evaluation benchmarks
- Break condition: If performance gains disappear on held-out Korean tasks or translate to mere memorization of training examples.

### Mechanism 2
- Claim: Chain-of-Thought prompting provides greater relative improvement for higher-performing models than lower-performing ones.
- Mechanism: CoT prompting "generally enhances performance across all models" but "improvement is more pronounced in high-performing closed-source models compared to open-source models" (Table 4), suggesting reasoning capabilities may scaffold on foundational language competence.
- Core assumption: CoT effectiveness depends on baseline model capabilities; weaker models cannot fully exploit reasoning chains.
- Evidence anchors:
  - "Notably, this improvement is more pronounced in high-performing closed-source models compared to open-source models"
  - Table 4 shows GPT-4o improving from 74.9 (Direct, w/ OCR) to 83.4 (CoT, w/ OCR) vs. smaller models showing smaller gains
  - V-Math paper on Vietnamese exams also uses agentic reasoning approaches for mathematical problem-solving
- Break condition: If CoT gains are primarily from prompt engineering artifacts rather than genuine reasoning improvements.

### Mechanism 3
- Claim: OCR-assisted text-only models can outperform some multimodal models on non-English visual question answering tasks.
- Mechanism: Many open-source MLLMs "do not perform as effectively" as LLMs with OCR support on Korean content. LLM+OCR pipeline provides explicit text extraction while MLLMs must jointly learn vision and language representations, which appears underdeveloped for Korean in open-source models.
- Core assumption: The text recognition capability for Korean characters in open-source MLLMs lags behind specialized OCR APIs.
- Evidence anchors:
  - "many open-source MLLMs do not perform as effectively, revealing a specific challenge with text recognition in the Korean context"
  - Table 4 shows Qwen2-1.5B (Text + OCR) achieving 19.2 vs. Qwen2-VL-2B (Vision) achieving 11.0
  - Limited direct corpus evidence on OCR vs. MLLM tradeoffs for low-resource languages
- Break condition: If MLLM underperformance stems from other factors (training data quality, model capacity) rather than Korean text recognition specifically.

## Foundational Learning

- **Multimodal Learning Fundamentals**
  - Why needed here: KoNET requires understanding how vision encoders, projection layers, and language models interact to process image+text exam questions.
  - Quick check question: Can you explain why an MLLM might fail on Korean text recognition even if it performs well on English visual QA?

- **Evaluation Methodology for LLMs**
  - Why needed here: The paper uses LLM-as-a-Judge (GPT-4o) to evaluate responses; understanding bias, consistency, and limitations of this approach is critical.
  - Quick check question: What are two potential failure modes when using an LLM to grade another LLM's exam responses?

- **Cross-Lingual Transfer in Foundation Models**
  - Why needed here: Performance gaps between English-centric and Korean benchmarks reveal how training data composition affects multilingual capabilities.
  - Quick check question: Why might a model achieve high accuracy on MMLU (English) but low accuracy on KoNET despite similar task structures?

## Architecture Onboarding

- **Component map:**
  - PDF parsing and image extraction (single image per question containing all necessary information) -> OCR token generation for text-based models -> Model inference with Direct or CoT prompting -> Response evaluation via LLM-as-a-Judge comparing against ground-truth answers

- **Critical path:**
  1. PDF parsing and image extraction (single image per question containing all necessary information)
  2. OCR token generation for text-based models
  3. Model inference with Direct or CoT prompting
  4. Response evaluation via LLM-as-a-Judge (GPT-4o) with the Judge prompt template

- **Design tradeoffs:**
  - Single-image format (question + choices embedded) vs. separated text/image inputs: chose embedded to force MLLMs to demonstrate genuine visual comprehension
  - Multiple-choice only vs. open-ended: chose MC for automated evaluation, but limits assessment of reasoning articulation
  - LLM-as-a-Judge vs. exact string matching: chose LLM judge for flexibility with varied response formats, accepts ~2-3% evaluation variance

- **Failure signatures:**
  - MLLMs achieving <5% on KoNET but >60% on MathVista: indicates Korean text recognition failure
  - Large performance gap between Direct and CoT prompting: suggests model lacks coherent reasoning capabilities for Korean
  - High variance across repeated evaluations with same prompt: indicates temperature/sampling instability

- **First 3 experiments:**
  1. Establish baseline with GPT-4o on KoCSAT subset with and without OCR to isolate visual vs. textual reasoning contribution
  2. Compare Qwen2-7B-Instruct (LLM+OCR) vs. Qwen2-VL-7B-Instruct (MLLM) to quantify OCR vs. native vision tradeoff
  3. Run ablation on CoT vs. Direct prompting across model size tiers (2B, 7B, 70B) to verify scaling-dependency of reasoning gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can KoNET be extended to evaluate the reasoning process and rationale generation of models, rather than just the final answer selection?
- Basis in paper: [explicit] The authors state in the Limitations section that the multiple-choice format "may not fully capture a model’s capacity to articulate problem-solving processes," suggesting future work should focus on "incorporating rationales behind their answers."
- Why unresolved: Evaluating open-ended rationales requires developing comprehensive reference answers and robust semantic evaluation metrics, which involves significantly higher computational costs than the current multiple-choice setup.
- What evidence would resolve it: The release of a human-annotated dataset of reasoning chains for KoNET questions and a validated evaluation protocol (e.g., an LLM-as-a-Judge metric) for grading these rationales.

### Open Question 2
- Question: What specific architectural or training deficiencies cause open-source MLLMs to underperform compared to OCR-augmented LLMs on Korean text recognition?
- Basis in paper: [inferred] The analysis in Section 4.3 notes that while closed-source MLLMs are competitive, "many open-source MLLMs do not perform as effectively, revealing a specific challenge with text recognition in the Korean context."
- Why unresolved: The paper identifies the performance gap but does not isolate whether the failure stems from the visual encoder's inability to parse Hangul or the projection layer's failure to align Korean visual tokens with the LLM.
- What evidence would resolve it: An ablation study comparing the performance of open-source MLLM visual encoders on isolated Korean OCR tasks versus their end-to-end reasoning performance.

### Open Question 3
- Question: How can the benchmark effectively assess the subjective, short-response questions found in the KoCSAT subset?
- Basis in paper: [explicit] The paper acknowledges that "a small proportion of the questions are subjective" but notes that evaluating reasoning abilities requires "consideration of the increased computational costs involved."
- Why unresolved: The current evaluation relies on a binary "Correct/Incorrect" judgment by an LLM, which may lack the nuance required to grade partial credit or short-form constructed responses accurately.
- What evidence would resolve it: A study demonstrating a high correlation between LLM-as-a-Judge scores and human expert grading on the subjective short-answer sections of the KoCSAT.

## Limitations

- Dataset Availability and Reproducibility: The KoNET benchmark dataset is not yet publicly available as of the paper's publication, requiring manual PDF parsing for reproduction.
- Evaluation Methodology Reliability: The LLM-as-a-Judge approach shows high agreement (97-98%) with human annotators, but lacks extensive inter-annotator reliability analysis for Korean content.
- Benchmark Representativeness: The single-image format per question may not fully capture real-world multimodal reasoning scenarios, and multiple-choice format limits assessment of open-ended reasoning.

## Confidence

**High Confidence**: Claims about KoNET's construction methodology, dataset composition (2,377 questions across 4 educational levels), and the performance rankings of tested models on the benchmark.

**Medium Confidence**: Claims regarding Chain-of-Thought prompting effectiveness, performance gaps between open-source and closed-source models, and the specific challenges of Korean text recognition in MLLMs.

**Low Confidence**: Claims about the universal superiority of LLM+OCR approaches over MLLMs, the complete absence of Korean cultural bias in the benchmark, and the long-term stability of evaluation results given potential LLM-as-a-Judge drift.

## Next Checks

1. **Dataset Builder Validation**: Test the released KoNET dataset builder on a held-out set of Korean educational PDFs to verify parsing accuracy, OCR quality, and question extraction consistency across different exam formats.

2. **Evaluation Reliability Assessment**: Conduct independent human evaluation of LLM-as-a-Judge outputs on 100+ KoNET samples, measuring inter-annotator agreement and identifying systematic biases in the current evaluation approach.

3. **Cross-Lingual Transfer Analysis**: Test the same 46 models on KoNET and comparable English benchmarks (e.g., MMLU) to quantify performance gaps and validate claims about Korean-specific training data requirements.