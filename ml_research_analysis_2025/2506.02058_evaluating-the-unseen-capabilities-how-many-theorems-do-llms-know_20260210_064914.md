---
ver: rpa2
title: 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?'
arxiv_id: '2506.02058'
source_url: https://arxiv.org/abs/2506.02058
tags:
- knowledge
- disease
- theorem
- unseen
- theorems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem that current LLM evaluations often
  fail to capture the full extent of models' internal knowledge, focusing only on
  observable outputs while missing substantial unseen knowledge. The authors introduce
  KnowSum, a statistical framework that estimates the amount of knowledge an LLM possesses
  beyond what is directly expressed.
---

# Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?

## Quick Facts
- **arXiv ID:** 2506.02058
- **Source URL:** https://arxiv.org/abs/2506.02058
- **Reference count:** 40
- **Primary result:** Introduces KnowSum framework to estimate unseen knowledge in LLMs using statistical extrapolation from observed frequency distributions

## Executive Summary
This paper addresses a fundamental limitation in LLM evaluation: current benchmarks only measure what models explicitly output, missing potentially vast amounts of internal knowledge that remains unexpressed. The authors introduce KnowSum, a statistical framework that estimates how much knowledge an LLM possesses beyond what is directly observable. Using a smoothed Good-Turing estimator, KnowSum extrapolates from observed frequency distributions to quantify unseen knowledge items that would appear with more queries. The framework reveals that LLMs typically express only 20-50% of their estimated internal knowledge, fundamentally changing how we should evaluate and compare these models.

The implications are significant: standard evaluations may dramatically underestimate model capabilities by focusing only on surface-level outputs. When accounting for unseen knowledge, model rankings can shift substantially - for instance, ChatGPT-3.5-turbo-chat surpasses DeepSeek-V3 in biomedical retrieval when total knowledge estimates include unexpressed items. The framework is computationally efficient, requiring no additional inference cost, and provides a complementary perspective to traditional evaluation by revealing latent capabilities that standard metrics miss.

## Method Summary
The KnowSum framework employs a smoothed Good-Turing estimator to extrapolate from observed frequency distributions of knowledge items generated by LLMs. By analyzing how often different knowledge items appear in model outputs, the framework estimates how many additional items would emerge with more queries. The approach treats knowledge as discrete items (theorems, diseases, or retrieval results) and assumes that the frequency distribution of observed outputs reflects the underlying knowledge space. The statistical methodology quantifies both seen and unseen knowledge, providing a total knowledge estimate that includes items the model "knows" but doesn't express in given queries. This allows for more comprehensive evaluation of LLM capabilities beyond just what is directly observable in standard benchmarks.

## Key Results
- LLMs express only 20-50% of their estimated internal knowledge across tested domains
- Accounting for unseen knowledge significantly changes model rankings in evaluation tasks
- In biomedical retrieval, ChatGPT-3.5-turbo-chat's total knowledge estimate increases from 2190 to 10367 when including unseen items, surpassing DeepSeek-V3
- The framework requires no additional computational cost and is modular for different applications

## Why This Works (Mechanism)
The framework works by leveraging statistical properties of frequency distributions to infer unseen categories. When an LLM generates knowledge items repeatedly, the pattern of frequencies (how many items appear once, twice, etc.) contains information about the total size of the underlying knowledge space. The Good-Turing estimator mathematically captures this relationship, allowing extrapolation beyond observed data. This approach assumes that the way items are distributed in outputs reflects the model's internal knowledge organization, enabling estimation of what remains hidden from direct observation.

## Foundational Learning
- **Good-Turing estimation**: Statistical method for estimating unseen categories from frequency data; needed to extrapolate beyond observed knowledge items
- **Frequency distribution analysis**: Understanding how often items appear in outputs; required to identify patterns in knowledge expression
- **Knowledge item discretrization**: Treating knowledge as countable units; necessary for applying statistical estimators
- **Extrapolation methodology**: Mathematical techniques to predict beyond observed data; essential for estimating unseen knowledge
- **LLM evaluation frameworks**: Standard approaches to measuring model capabilities; context for identifying evaluation limitations
- **Knowledge representation**: How models store and organize information internally; relevant for interpreting what "unseen knowledge" means

## Architecture Onboarding

**Component Map:**
Input Prompts -> LLM Generation -> Knowledge Item Extraction -> Frequency Counting -> Smoothed Good-Turing Estimation -> Unseen Knowledge Quantification -> Total Knowledge Estimation

**Critical Path:**
Prompt design → Knowledge extraction → Frequency analysis → Statistical extrapolation → Knowledge estimation

**Design Tradeoffs:**
- Statistical rigor vs. computational efficiency: Framework balances mathematical soundness with practical implementation
- Granularity vs. generalizability: Discrete knowledge items enable estimation but may oversimplify complex knowledge structures
- Domain specificity vs. broad applicability: Framework works across domains but requires domain-appropriate knowledge item definitions

**Failure Signatures:**
- Overfitting to specific prompt templates leading to biased frequency distributions
- Insufficient observations causing unreliable Good-Turing estimates
- Poor knowledge item discretization failing to capture meaningful distinctions
- Assumption violations when frequency distributions don't follow expected patterns

**First 3 Experiments to Run:**
1. Apply framework to a simple domain with known ground truth to validate estimation accuracy
2. Test sensitivity to different prompt engineering approaches on the same knowledge domain
3. Compare estimates across multiple model sizes within the same family to identify scaling patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes frequency distribution of outputs adequately represents underlying knowledge space
- Cannot distinguish between "known but unexpressed" items versus items that don't exist in model's knowledge base
- Reliability depends on sufficient observations to capture tail of frequency distribution

## Confidence
- **High confidence**: Statistical methodology using smoothed Good-Turing estimation is mathematically sound
- **Medium confidence**: Empirical findings about 20-50% expression rates are robust within tested domains
- **Low confidence**: Absolute magnitude of unseen knowledge estimates depends heavily on distributional assumptions

## Next Checks
1. Test KnowSum across diverse knowledge domains beyond theorems and diseases to verify generalizability of 20-50% expression rate
2. Validate framework's sensitivity to prompt engineering by systematically varying prompt templates and measuring estimate changes
3. Compare KnowSum estimates against external knowledge bases to assess correspondence with verifiable facts not present in model outputs