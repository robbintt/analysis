---
ver: rpa2
title: 'None To Optima in Few Shots: Bayesian Optimization with MDP Priors'
arxiv_id: '2511.01006'
source_url: https://arxiv.org/abs/2511.01006
tags:
- optimization
- learning
- each
- bayesian
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Bayesian optimization with very few function
  evaluations (fewer than 20) by introducing a method called Procedure-inFormed Bayesian
  Optimization (ProfBO). The key innovation is using Markov Decision Process (MDP)
  priors to model optimization trajectories from related source tasks, capturing procedural
  knowledge of efficient optimization.
---

# None To Optima in Few Shots: Bayesian Optimization with MDP Priors

## Quick Facts
- arXiv ID: 2511.01006
- Source URL: https://arxiv.org/abs/2511.01006
- Authors: Diantong Li; Kyunghyun Cho; Chong Liu
- Reference count: 40
- Primary result: ProfBO achieves state-of-the-art performance on few-shot Bayesian optimization with <20 function evaluations using MDP trajectory priors

## Executive Summary
This paper addresses the challenge of Bayesian optimization with very few function evaluations (fewer than 20) by introducing Procedure-inFormed Bayesian Optimization (ProfBO). The key innovation is using Markov Decision Process (MDP) priors to model optimization trajectories from related source tasks, capturing procedural knowledge of efficient optimization. These MDP priors are embedded into a prior-fitted neural network and trained using model-agnostic meta-learning (MAML) for fast adaptation to new target tasks. Experiments on real-world Covid and Cancer benchmarks, as well as hyperparameter tuning tasks, demonstrate that ProfBO consistently outperforms state-of-the-art methods by achieving high-quality solutions with significantly fewer evaluations.

## Method Summary
ProfBO introduces a two-stage training approach that combines MDP priors with prior-fitted neural networks (PFNs). First, DQN agents learn optimization policies for each source task, generating trajectory priors that capture procedural knowledge. These trajectories are then used to train a Transformer-based PFN through a combination of GP pre-training and MAML fine-tuning with positional encoding. The resulting model serves as a surrogate for Bayesian optimization, enabling efficient search with limited evaluations. The method is particularly effective for few-shot scenarios where traditional Bayesian optimization methods fail to converge within practical evaluation limits.

## Key Results
- ProfBO consistently outperforms state-of-the-art methods (FSBO, Meta-GP, NAP) on Covid-B and Cancer-B benchmarks with <20 evaluations
- The method achieves high-quality solutions in few-shot scenarios where traditional BO methods fail to converge
- ProfBO's modular design allows easy adaptation to various configurations and input types across scientific and engineering applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MDP priors transfer procedural knowledge from source tasks to accelerate target optimization by encoding how efficient optimization trajectories behave, not just what the response surface looks like.
- Mechanism: For each source task f^(i), a DQN agent learns a policy π_η^(i) that generates optimization trajectories. The state includes the previous k evaluations {x_τ, y_τ, τ/T}, action is the next query x_t, and reward is negative simple regret. These trajectories form p(T^(i)), which becomes the prior distribution for PFN training. This captures sequential decision patterns rather than static function shapes.
- Core assumption: Source tasks share procedural structure with target tasks (similar optimization dynamics, not necessarily similar functions).
- Evidence anchors:
  - [abstract] "MDP priors that model optimization trajectories from related source tasks, thereby capturing procedural knowledge on efficient optimization"
  - [section 4.2] "We propose a novel prior for Bayesian optimization that models p(T^(i)), the prior distribution of optimization trajectories, with MDP"
  - [corpus] Weak direct support; neighbor papers discuss multi-task BO but not MDP-based trajectory priors specifically.
- Break condition: Source and target tasks have fundamentally different optimization landscapes (e.g., convex source tasks, highly multimodal target tasks) such that learned policies transfer negatively.

### Mechanism 2
- Claim: PFNs enable tractable Bayesian inference with arbitrary priors (including MDP trajectory priors) by replacing GP posterior computation with a single Transformer forward pass.
- Mechanism: The PFN q_θ(·|x, D) approximates p(·|x, D) through supervised learning on samples from p(D). Training minimizes KL divergence via negative log-likelihood (Eq. 2). The Transformer attention mechanism allows context points to attend to each other and be attended by test locations. This avoids O(n³) matrix inversion in GP while supporting non-i.i.d. priors like trajectory distributions.
- Core assumption: The Transformer can learn to approximate Bayesian posterior inference through sufficient training samples from the prior.
- Evidence anchors:
  - [section 4.1] "PFN conducts simulation-based inference through the forward pass of a Transformer model"
  - [section 4.1] "The flexible choice of p(D) provides the possibility to infer any complicated while easy-to-generate prior"
  - [corpus] No direct corpus validation for PFN specifically; this is a specialized technique.
- Break condition: Prior distribution p(D) is too complex or high-dimensional for the Transformer capacity to approximate posterior accurately.

### Mechanism 3
- Claim: MAML + positional encoding enables the PFN to learn transferable trajectory patterns while preventing overfitting to task-specific spurious correlations.
- Mechanism: Positional encoding re-introduces sequence order information (removed in vanilla PFN for permutation invariance). MAML meta-optimizes θ to minimize L_meta(θ) = Σ L^(i)(θ - β·∂_θL^(i)(θ)), learning parameters that adapt quickly to new trajectory distributions. This extracts shared optimization patterns across tasks rather than memorizing specific trajectories.
- Core assumption: A gradient-based meta-learning objective can capture commonalities across optimization trajectories from different tasks.
- Evidence anchors:
  - [section 4.3] "MAML in our method is creatively repurposed to extract features shared across trajectories"
  - [section 5.4 ablation] "figures show that the performances of ProfBO indeed benefit from MAML... vanilla PFN removes the positional encoding... in our task we introduce the positional encoding back"
  - [corpus] No corpus evidence for this specific combination.
- Break condition: Source tasks are too diverse, causing MAML to find a poor initialization that doesn't specialize well to any target task.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) and Q-Learning**
  - Why needed here: Understanding how DQN generates trajectory priors—state/action/reward definitions, epsilon-greedy sampling, Bellman updates.
  - Quick check question: Can you explain why the reward is defined as negative simple regret rather than cumulative discounted reward over the full horizon?

- Concept: **Transformer Attention and In-Context Learning**
  - Why needed here: PFN uses attention to aggregate context (past evaluations) for posterior inference at test locations.
  - Quick check question: How does the attention mask in PFN differ from standard causal masking in autoregressive language models?

- Concept: **Meta-Learning (MAML specifically)**
  - Why needed here: ProfBO uses MAML not for fast adaptation at test time, but for training a model that extracts shared patterns across trajectory distributions.
  - Quick check question: Why does ProfBO use MAML's inner loop gradient step during training but not during test-time inference?

## Architecture Onboarding

- Component map:
  Source Task Datasets D^(i) -> DQN Training per task -> MDP Prior p(T^(i)) -> Trajectory Sampling -> PFN Pre-training <- GP/Synthetic Prior p(D) -> PFN Fine-tuning -> Trained PFN q_θ -> BO Loop: Acquire -> Evaluate -> Update Context

- Critical path:
  1. Verify source task data quality and coverage (must share procedural structure with target)
  2. Train DQN agents until they outperform random search (Appendix A.2: ~250 epochs)
  3. Generate trajectory samples (sequence length 40, batch size 128)
  4. Pre-train PFN on GP prior (stabilizes unexplored regions)
  5. Fine-tune with MAML on MDP priors (2 epochs, learning rate 3e-4)
  6. Deploy in BO loop with EI/UCB acquisition

- Design tradeoffs:
  - **Lightweight DQN vs. Transformer policy**: ProfBO uses MLP (200-200-200-200) for trajectory generation—faster but less expressive than NAP's Transformer policy. Training is 3.34x faster.
  - **Positional encoding vs. permutation invariance**: Re-introducing position helps trajectory learning but risks overfitting; MAML mitigates this.
  - **Pre-training stage cost**: One-time cost amortized across future problems; only fine-tuning needed for new task families.

- Failure signatures:
  - Regret doesn't decrease after 10-15 iterations → MDP prior may not match target task structure; check source/target task similarity
  - PFN outputs unrealistic uncertainty (all high or all low) → Pre-training may be insufficient; increase K_1 iterations
  - MAML fine-tuning destabilizes training → Reduce inner step size β or increase pre-training coverage
  - Performance worse than random search → DQN agents haven't converged; verify they beat random baseline first

- First 3 experiments:
  1. **Ablation on single benchmark**: Run ProfBO on one HPO-B problem with/without MDP prior (compare to TNP+), with/without MAML, with/without positional encoding. Replicate Figure 4 right panel to validate setup.
  2. **Source task sensitivity**: Vary the number of source tasks N used for MDP prior training (e.g., N=1, 3, 5) and measure target task regret at T=20. Identify minimum source tasks for effective transfer.
  3. **Dimension scaling test**: Test on synthetic problems with increasing dimensions (d=5, 10, 26) while keeping evaluation budget fixed at T=20. Characterize where ProfBO's advantage degrades relative to FSBO/GP baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProfBO perform when source tasks have low similarity to target tasks, and can negative transfer occur where meta-learned priors harm optimization performance?
- Basis in paper: [inferred] The method assumes access to "related source tasks" with existing evaluations, but the paper does not analyze performance degradation when this assumption is violated or quantify the degree of task similarity required.
- Why unresolved: All experiments use carefully curated benchmarks where source and target tasks share structural properties (same receptors with different binding conditions, same algorithms on different datasets).
- What evidence would resolve it: Experiments measuring ProfBO performance across controlled levels of task dissimilarity, comparing against non-meta-learned baselines to detect negative transfer regimes.

### Open Question 2
- Question: Can theoretical regret bounds or convergence guarantees be established for ProfBO, particularly given the use of learned MDP priors within the Bayesian framework?
- Basis in paper: [inferred] The introduction notes that standard BO theoretical guarantees "typically hold in the asymptotic regime," but ProfBO's combination of MDP priors, PFNs, and MAML lacks theoretical analysis despite its empirical success.
- Why unresolved: The paper focuses entirely on empirical validation across Covid-B, Cancer-B, and HPO-B benchmarks without providing formal analysis of the meta-learning procedure's impact on optimization guarantees.
- What evidence would resolve it: Derivation of regret bounds for the meta-learned trajectory surrogate, or analysis of how MDP prior quality affects convergence properties.

### Open Question 3
- Question: How does ProfBO scale to higher-dimensional optimization problems beyond the 26D tested, and does the DQN-based trajectory generation remain tractable?
- Basis in paper: [inferred] The paper acknowledges that NAP "required over 200 iterations to dominate in the 11D antibody design task" while testing on 26D problems with fewer iterations, leaving high-dimensional scalability unclear.
- Why unresolved: Drug discovery benchmarks use PCA-reduced 26D embeddings from 42D molecular features, but many real-world optimization problems involve hundreds of dimensions.
- What evidence would resolve it: ProfBO evaluation on standard high-dimensional BO benchmarks (e.g., 50-100+ dimensions) with analysis of computational costs for both DQN training and PFN fine-tuning.

## Limitations
- The method relies heavily on the assumption that source and target tasks share procedural optimization dynamics, which may not hold in practice
- MDP prior generation requires computationally intensive training of separate DQN agents for each source task
- The two-stage training approach adds complexity and requires careful hyperparameter tuning, particularly for the unspecified MAML inner step size

## Confidence
- **High Confidence**: The core ProfBO architecture and its integration of MDP priors with PFNs is technically sound and well-supported by the experimental results
- **Medium Confidence**: The claim that ProfBO consistently outperforms state-of-the-art methods across all tested scenarios, given the limited number of benchmark problems
- **Low Confidence**: The assertion that ProfBO is broadly applicable to various scientific and engineering applications without further validation on diverse problem domains

## Next Checks
1. **Statistical Significance Testing**: Perform paired t-tests or Wilcoxon signed-rank tests on the benchmark results to determine if ProfBO's performance improvements are statistically significant compared to baselines
2. **Source Task Sensitivity Analysis**: Systematically vary the number and diversity of source tasks to quantify how ProfBO's performance scales with different MDP prior qualities and coverage
3. **Cross-Domain Generalization**: Test ProfBO on optimization problems from domains not used in the paper (e.g., robotics control, material design) to validate its broader applicability claims