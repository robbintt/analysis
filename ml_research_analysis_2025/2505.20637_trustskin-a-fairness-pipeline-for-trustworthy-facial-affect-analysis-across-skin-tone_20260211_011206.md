---
ver: rpa2
title: 'TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across
  Skin Tone'
arxiv_id: '2505.20637'
source_url: https://arxiv.org/abs/2505.20637
tags:
- skin
- tone
- fairness
- across
- dark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses fairness in facial affect analysis (FAA)
  systems by investigating how skin tone measurement methods affect performance across
  demographic groups. The authors compare two skin tone classification approaches:
  the widely used Individual Typology Angle (ITA) and a perceptually grounded Lightness-Hue
  (L-H) method.'
---

# TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone

## Quick Facts
- arXiv ID: 2505.20637
- Source URL: https://arxiv.org/abs/2505.20637
- Authors: Ana M. Cabanas; Alma Pedro; Domingo Mery
- Reference count: 40
- Primary result: ITA underestimates fairness disparities for darker skin tones; L*-H* method reveals more accurate subgrouping and significant emotion classification gaps.

## Executive Summary
This paper addresses fairness in facial affect analysis (FAA) systems by investigating how skin tone measurement methods affect performance across demographic groups. The authors compare the widely used Individual Typology Angle (ITA) with a perceptually grounded Lightness-Hue (L*-H*) method. Using the AffectNet dataset and a MobileNet-based classifier, they evaluate fairness through metrics such as F1-score disparity, True Positive Rate (TPR) disparity, and Equal Opportunity. Results show that ITA misclassifies darker skin tones, leading to inflated performance metrics, while the L*-H* method yields more accurate subgrouping and reveals significant disparities—particularly for emotions like Anger, Fear, and Contempt—in darker skin tones. Grad-CAM analysis further shows variation in model attention across skin tones, with misclassifications linked to inconsistent feature localization. The authors propose a modular fairness-aware FAA pipeline integrating perceptual skin tone estimation, fairness evaluation, and explainability.

## Method Summary
The study uses the AffectNet dataset with ~450K manually annotated images for 8-class emotion classification. Faces are detected using MTCNN, and skin segmentation is performed using Otsu's thresholding combined with chrominance filtering in YCrCb and HSV color spaces. Two skin tone classification methods are compared: ITA (thresholds: Light >55°, Medium 30–55°, Dark <30°) and L*-H* (Light: L*>67, Medium: 37≤L*≤67, Dark: L*<37 with brown-tone override rule). A MobileNet classifier is trained on the dataset, and fairness metrics (F1-score disparity, TPR disparity, Equal Opportunity Difference) are computed across skin tone groups. Grad-CAM is used for explainability analysis to visualize model attention patterns.

## Key Results
- ITA misclassifies some Medium-skinned individuals as Dark, artificially inflating Dark group performance metrics.
- L*-H* method reveals significant F1-score and TPR disparities across skin tones, particularly for emotions like Anger, Fear, and Contempt in darker skin tones.
- Grad-CAM analysis shows that misclassifications correlate with inconsistent spatial attention patterns that vary by skin tone, with diffuse or irrelevant activation in misclassified cases.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating hue information (H*) alongside lightness (L*) improves skin tone classification robustness under variable illumination compared to ITA alone.
- Mechanism: ITA computes `arctan((L* - 50)/b*)`, which omits the a* (green-red) chromatic axis. The L*-H* method derives Hue as `arctan(b*/a*)`, capturing both chromaticity dimensions. This allows finer differentiation of undertones (reddish vs. yellowish) that ITA cannot distinguish, particularly for darker complexions where hue variation is visually salient.
- Core assumption: Perceptual color spaces (CIE Lab) provide a more stable representation of skin tone than ITA's simplified angle formulation under real-world lighting variability.
- Evidence anchors:
  - [abstract] "ITA shows limitations due to its sensitivity to lighting, the H*-L* method yields more consistent subgrouping"
  - [Section II-B] "ITA entirely omits the a* component that encodes the green-red axis... fails to capture hue information"
  - [corpus] Limited direct corpus validation for L*-H* specifically; neighbor papers focus on continuous skin tone variables but not this exact metric
- Break condition: If images contain extreme illumination artifacts (saturation, heavy shadows), both methods may fail; L*-H* thresholds may require dataset-specific tuning.

### Mechanism 2
- Claim: More accurate subgrouping reveals previously masked fairness disparities rather than creating them.
- Mechanism: ITA's misclassification of darker skin tones as Medium inflates the "Dark" group's apparent performance by including easier-to-classify lighter samples. The L*-H* method's more accurate subgroup boundaries expose the true performance gap for genuinely dark skin tones.
- Core assumption: Higher measured disparity with better measurement reflects more accurate auditing, not worse model behavior.
- Evidence anchors:
  - [abstract] "ITA-based evaluations may overlook disparities affecting darker-skinned individuals"
  - [Section IV-A] "ITA's higher recall for Dark skin could be partially attributed to incorrectly including Medium-skinned individuals"
  - [corpus] "Fairness-Aware Grouping for Continuous Sensitive Variables" notes that continuous attributes poorly discretized can obscure true disparities
- Break condition: If the corrected subgrouping produces groups too small for statistical power (<50 samples), fairness metrics become unreliable (paper notes Dark group had only 52 test images).

### Mechanism 3
- Claim: Misclassifications correlate with inconsistent spatial attention patterns that vary by skin tone.
- Mechanism: Grad-CAM visualization traces gradient contributions to the final convolutional layer. Correct predictions show focused attention on expression-relevant regions (mouth for disgust, eyes for fear); misclassifications show diffuse or irrelevant activation (forehead, nose for sadness errors in Dark group).
- Core assumption: Saliency map patterns indicate causal feature reliance rather than post-hoc correlation.
- Evidence anchors:
  - [abstract] "Grad-CAM analysis further highlights differences in model attention patterns by skin tone"
  - [Section IV-B] "accurate predictions tend to correspond to localized, expression-relevant regions, while misclassifications show more dispersed or inconsistent patterns"
  - [corpus] No direct corpus validation of Grad-CAM skin tone differences; neighbor papers do not address this XAI dimension
- Break condition: Grad-CAM provides coarse attribution and may not capture true decision logic; paper explicitly notes limitations and suggests LRP or MinPlus for deeper analysis.

## Foundational Learning

- Concept: **Individual Typology Angle (ITA)**
  - Why needed here: ITA is the de facto standard for objective skin tone classification in computer vision fairness research. Understanding its formula, limitations (lighting sensitivity, omission of a*), and mapping to Fitzpatrick types is essential for interpreting prior work.
  - Quick check question: Why does ITA fail to distinguish two individuals with the same lightness but different red vs. yellow undertones?

- Concept: **Equal Opportunity Difference (EOD)**
  - Why needed here: This fairness metric measures whether true positive rates are equal across groups for each class. Unlike aggregate accuracy, EOD reveals class-specific disparities (e.g., anger recognized well for Light but poorly for Dark).
  - Quick check question: If a model has 90% TPR for "happiness" across all skin tones but 80%/60%/40% TPR for "fear" across Light/Medium/Dark, is Equal Opportunity satisfied?

- Concept: **CIE Lab Color Space**
  - Why needed here: The L*-H* method operates in this perceptually uniform color space. L* represents lightness (0-100), a* green-red, b* blue-yellow. Converting from RGB requires understanding this transformation.
  - Quick check question: Why is CIE Lab preferred over RGB for skin tone measurement when analyzing images captured under different lighting?

## Architecture Onboarding

- Component map: Input Image → Face Detection (MTCNN) → Skin Segmentation (Otsu + YCrCb/HSV thresholds) → Color Space Conversion (RGB → CIE Lab) → L*-H* Classification → MobileNet Classifier → Fairness Metrics (F1-gap, TPR disparity, EOD) → Explainability (Grad-CAM)

- Critical path: The skin tone classification stage (L*-H*) determines all downstream fairness analysis. Errors here propagate to metrics and can mask or exaggerate disparities. The brown-tone override rule (RGB ranges + Hue angle 20°-50°) is a critical edge-case handler.

- Design tradeoffs:
  - **ITA vs. L*-H***: ITA is computationally simpler and widely adopted; L*-H* provides better subgroup fidelity but requires empirically tuned thresholds that may not generalize.
  - **Three-group vs. six-group discretization**: Paper collapses Fitzpatrick I-VI into Light/Medium/Dark for statistical power; this reduces granularity but enables meaningful comparison.
  - **Balanced test set vs. realistic distribution**: Paper uses unbalanced test set to reflect real-world deployment; this limits statistical power for Dark group analysis.

- Failure signatures:
  - **High variance in Dark group metrics**: Likely due to small sample size (<50); consider confidence intervals or bootstrapping.
  - **ITA shows lower disparity than L*-H***: Suspect ITA is misclassifying Medium samples as Dark, artificially improving Dark group metrics.
  - **Grad-CAM shows diffuse attention for correct predictions**: May indicate model is using non-facial features (background, hair) or dataset has annotation noise.

- First 3 experiments:
  1. **Validate L*-H* classification accuracy**: Manually annotate a stratified sample (n=100-200) across skin tones with ground-truth labels; compute classification agreement rate for both ITA and L*-H*.
  2. **Cross-dataset threshold calibration**: Apply the L*-H* method to a different FAA dataset (e.g., RAF-DB, FER2013) to assess whether thresholds (L*=67, L*=37, brown-tone override) require re-tuning.
  3. **Fairness comparison across architectures**: Train ResNet-50 and ViT on same data; compare fairness metrics (F1-gap, EOD) to determine whether model capacity affects skin tone disparities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do increased model capacity or self-attention mechanisms in architectures like ResNet and ViT mitigate or exacerbate skin tone disparities compared to lightweight CNNs?
- Basis in paper: [explicit] The authors state, "Specifically, we plan to extend the proposed fairness pipeline... to assess whether increased capacity or self-attention mechanisms mitigate or exacerbate the skin tone disparities observed in lightweight CNNs."
- Why unresolved: The current study isolated measurement variables by using only a MobileNet-based classifier, leaving the interaction between architectural complexity and the proposed skin tone grouping unexplored.
- What evidence would resolve it: Comparative fairness metrics (F1-score gap, EOD) derived from training ResNet and ViT models on the same H*-L* stratified dataset.

### Open Question 2
- Question: Do explainability techniques like MinPlus or Layer-wise Relevance Propagation (LRP) reveal specific bias mechanisms in FAA that Grad-CAM fails to capture?
- Basis in paper: [explicit] The paper notes that Grad-CAM "provides only coarse attribution" and suggests "future work could integrate more robust explainability techniques, such as MinPlus or Layer-wise Relevance Propagation".
- Why unresolved: Grad-CAM analysis in the study showed diffuse attention for misclassifications in darker skin tones, but lacked the granularity to determine if this was due to feature encoding failure or lack of data.
- What evidence would resolve it: Qualitative and quantitative comparisons of attribution maps identifying distinct failure modes in dark skin tones using LRP versus Grad-CAM.

### Open Question 3
- Question: Can the empirically defined H*-L* thresholds and "brown-tone override" heuristics generalize effectively to external datasets without manual retuning?
- Basis in paper: [inferred] The authors acknowledge the method "depends on empirically defined thresholds that may require tuning across datasets" and that the override rules are "dataset-specific heuristics."
- Why unresolved: The classification rules were optimized for the specific lighting conditions and color distributions present in the AffectNet dataset.
- What evidence would resolve it: Validation of the classification pipeline's accuracy and consistency when applied without modification to diverse datasets like FairFace or Casual Conversations.

## Limitations

- The L*-H* classification thresholds are empirically derived and may not generalize to other datasets without manual recalibration.
- The Dark skin tone group has very few samples (n=52), limiting statistical power and increasing variance in fairness metrics.
- Grad-CAM provides only coarse attribution and may not capture the true decision logic behind misclassifications.

## Confidence

- **High**: That skin tone measurement methodology affects subgroup classification accuracy, and that measurement choice matters for fairness auditing.
- **Medium**: That ITA misclassifies Medium-skinned individuals as Dark, inflating Dark group metrics. This is supported by the recall data but requires validation of the misclassification mechanism.
- **Low**: That Grad-CAM patterns causally explain misclassification differences. The paper explicitly notes this is exploratory and recommends deeper analysis (e.g., LRP, MinPlus).

## Next Checks

1. **Ground-truth validation of skin tone classification**: Manually annotate a stratified random sample (n=200) with expert skin tone labels to measure agreement rates for both ITA and L*-H* methods.
2. **Cross-dataset generalization**: Apply L*-H* thresholds to RAF-DB or FER2013 datasets to assess whether the same groupings hold and whether disparity patterns replicate.
3. **Statistical robustness analysis**: Use bootstrapping or confidence intervals to quantify uncertainty in fairness metrics, particularly for the Dark group with n=52 samples.