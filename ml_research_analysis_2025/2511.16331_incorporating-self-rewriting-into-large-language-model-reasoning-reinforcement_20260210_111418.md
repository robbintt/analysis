---
ver: rpa2
title: Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement
arxiv_id: '2511.16331'
source_url: https://arxiv.org/abs/2511.16331
tags:
- reasoning
- arxiv
- rewriting
- length
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces self-rewriting, a novel approach that integrates
  reasoning rewriting into reinforcement learning to improve internal reasoning quality
  in large reasoning models. The method selectively rewrites "simple" samples with
  consistent correctness, learning from these rewrites to address common reasoning
  flaws like over-thinking, under-thinking, redundant-thinking, and disordered-thinking.
---

# Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement

## Quick Facts
- arXiv ID: 2511.16331
- Source URL: https://arxiv.org/abs/2511.16331
- Reference count: 20
- Key result: +0.6 accuracy improvement while reducing reasoning length by -46%

## Executive Summary
This paper introduces self-rewriting, a novel approach that integrates reasoning rewriting into reinforcement learning to improve internal reasoning quality in large reasoning models. The method selectively rewrites "simple" samples with consistent correctness, learning from these rewrites to address common reasoning flaws like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. The approach achieves a favorable accuracy-length tradeoff and significantly outperforms strong baselines in internal reasoning quality according to LLM-as-a-judge metrics.

## Method Summary
Self-rewriting works by identifying simple samples where the model consistently produces correct answers, then selectively rewriting these samples to create improved reasoning paths. The method learns from these rewrites to correct four types of reasoning flaws: over-thinking (excessive reasoning steps), under-thinking (insufficient reasoning), redundant-thinking (repetitive reasoning), and disordered-thinking (illogical reasoning order). During reinforcement learning, the model is trained on both original and rewritten samples, with the rewrites serving as high-quality examples of efficient reasoning. This selective approach focuses on improving reasoning quality rather than just accuracy, targeting the internal structure of the reasoning process itself.

## Key Results
- +0.6 absolute accuracy improvement on benchmark datasets
- -46% reduction in reasoning length while maintaining accuracy gains
- +7.2 improvement in internal reasoning quality according to LLM-as-a-judge metrics
- Outperforms strong baseline approaches in both accuracy and reasoning efficiency

## Why This Works (Mechanism)
Self-rewriting improves reasoning quality by providing the model with explicit examples of optimal reasoning paths for problems it can already solve correctly. By focusing on "simple" samples where the model demonstrates consistent correctness, the method ensures that rewrites target genuine reasoning inefficiencies rather than fundamental understanding gaps. The approach leverages the model's existing capabilities to bootstrap improvements in reasoning structure, teaching it to arrive at correct answers through more efficient and logical reasoning paths. This targeted intervention addresses specific flaws in reasoning patterns that would be difficult to correct through standard supervised learning or reinforcement learning alone.

## Foundational Learning

**Reinforcement Learning with Reasoning Models**
- Why needed: Standard RL approaches optimize for final answer accuracy but don't address internal reasoning quality
- Quick check: Can the model improve reasoning structure while maintaining or improving accuracy?

**Selective Data Curation**
- Why needed: Not all samples are equally valuable for improving reasoning quality; focusing on consistently correct samples ensures targeted improvements
- Quick check: Does the selection criteria reliably identify samples where reasoning improvements are possible?

**Reasoning Quality Metrics**
- Why needed: Need quantitative measures of reasoning efficiency and logical structure beyond just answer correctness
- Quick check: Do LLM-as-a-judge metrics correlate with human assessments of reasoning quality?

## Architecture Onboarding

**Component Map:**
Self-Rewriting Module -> Reasoning Quality Classifier -> Reinforcement Learning Pipeline -> Output Model

**Critical Path:**
1. Sample selection (identify consistently correct "simple" samples)
2. Reasoning rewriting (apply transformations to correct specific flaws)
3. Quality assessment (evaluate rewritten reasoning)
4. RL training (incorporate original and rewritten samples)
5. Performance evaluation (measure accuracy and reasoning quality)

**Design Tradeoffs:**
The method trades computational overhead of the rewriting process for improved reasoning quality. Selective rewriting focuses resources on samples where improvements are most likely, rather than attempting wholesale rewriting of all data. The approach balances between preserving the model's demonstrated capabilities and introducing more efficient reasoning patterns.

**Failure Signatures:**
- Over-reliance on rewriting could lead to overfitting to specific reasoning patterns
- Poor sample selection might result in learning incorrect reasoning shortcuts
- LLM-as-a-judge metrics may not align with human judgment of reasoning quality
- The method assumes that shorter reasoning paths are always better, which may not hold for all problem types

**3 First Experiments:**
1. Ablation study removing each rewriting type (over-thinking, under-thinking, etc.) to quantify individual contributions
2. Comparison of self-rewriting vs. standard RL on reasoning efficiency metrics
3. Human evaluation study validating LLM-as-a-judge assessments of reasoning quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on LLM-as-a-judge metrics, which may not fully capture human-perceived reasoning quality
- The accuracy-length tradeoff was tested on specific datasets and may not generalize across all reasoning tasks
- Selective rewriting approach assumes reliable identification of "simple" samples, which may be challenging in practice
- The method's effectiveness for complex reasoning tasks requiring longer chains of thought remains unclear

## Confidence

**Self-rewriting approach validity: Medium**
- The core methodology appears sound, but limited evaluation scope reduces confidence

**Reported performance improvements: Medium**
- Results are positive but rely heavily on LLM evaluation

**Generalizability across tasks: Low**
- Limited testing suggests uncertainty about broader applicability

## Next Checks
1. Conduct human evaluation studies to validate LLM-as-a-judge assessments of reasoning quality improvements
2. Test the approach across diverse reasoning task types (mathematical, logical, commonsense) to assess generalizability
3. Implement ablation studies to quantify the contribution of each rewriting type (over-thinking, under-thinking, etc.) to overall performance gains