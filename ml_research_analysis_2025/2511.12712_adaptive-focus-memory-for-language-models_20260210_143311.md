---
ver: rpa2
title: Adaptive Focus Memory for Language Models
arxiv_id: '2511.12712'
source_url: https://arxiv.org/abs/2511.12712
tags:
- context
- memory
- constraint
- language
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AFM addresses the problem of long-horizon constraint preservation
  in multi-turn dialogue by dynamically allocating message fidelity (Full, Compressed,
  or Placeholder) based on semantic relevance, recency, and importance classification.
  This selective memory management maintains critical constraints without full context
  replay.
---

# Adaptive Focus Memory for Language Models

## Quick Facts
- arXiv ID: 2511.12712
- Source URL: https://arxiv.org/abs/2511.12712
- Reference count: 13
- AFM achieves 83.3% pass rate on allergy safety benchmark while all baselines fail completely

## Executive Summary
Adaptive Focus Memory (AFM) addresses long-horizon constraint preservation in multi-turn dialogue by dynamically allocating message fidelity (Full, Compressed, or Placeholder) based on semantic relevance, recency, and importance classification. This selective memory management maintains critical constraints without full context replay. On a safety-critical allergy benchmark, AFM achieves 83.3% pass rate under strict evaluation, while all baselines fail completely. On a tax compliance benchmark, AFM maintains 100% refusal of illegal requests. The approach reduces token usage by roughly two-thirds while improving constraint retention through explicit salience management rather than pure retention.

## Method Summary
AFM is a model-agnostic context management system that maintains conversation history through adaptive fidelity representation. Messages are scored using a combination of semantic similarity to the current query, exponential recency decay, and importance classification. Scores determine intended fidelity levels (Full, Compressed, or Placeholder), which are then packed chronologically under a fixed token budget using a greedy algorithm that degrades fidelity when necessary. The system uses separate LLM classifiers for importance labeling and compression, with a fallback heuristic compressor for efficiency.

## Key Results
- 83.3% pass rate on peanut allergy travel scenario (30 seeds), versus 0% for all baselines
- 100% pass rate on tax compliance scenario (30 seeds), maintaining refusal of illegal requests
- Token usage reduced by roughly two-thirds compared to full context replay
- AFM adds 3-20s of system overhead for classification and embedding

## Why This Works (Mechanism)

### Mechanism 1: Importance-Driven Fidelity Elevation
If a message is explicitly classified as "CRITICAL" (e.g., a safety constraint), assigning it a maximum score forces it into high-fidelity retention, preventing the "illusion of memory" where text is present but inert. An auxiliary LLM labels messages as CRITICAL, RELEVANT, or TRIVIAL. The scoring function assigns a fixed maximum score (1.0) to CRITICAL items, overriding semantic or recency decay. During packing, these items are prioritized for Full fidelity. The importance classifier reliably identifies safety-critical constraints with few false negatives. Disabling importance classification collapses performance to 0%, indicating that which messages receive Full fidelity is the dominant driver of constraint preservation.

### Mechanism 2: Budget-Aware Fidelity Degradation
Dynamically downgrading message fidelity (Full → Compressed → Placeholder) allows the system to fit a long history into a fixed token budget while preserving chronological structure. A greedy packing algorithm iterates through history. If a message at its intended fidelity exceeds the remaining budget, the system attempts a lower-fidelity form (e.g., summary or stub) rather than dropping it entirely. A "Placeholder" or "Compressed" stub provides sufficient context trace to prevent hallucination, though insufficient for strict constraint following. If the summary still does not fit, it falls back to a stub. AFM attempts to maintain a cheap trace of low-importance turns.

### Mechanism 3: Semantic-Recency Weighting
Combining embedding similarity with exponential temporal decay ensures that relevant older messages are retained while irrelevant recent noise is suppressed. A message's base score is derived from cosine similarity between the current query and the message embedding. This is weighted by a recency factor (0.5^(k/h)). The resulting score determines the "intended" fidelity tier before budget constraints are applied. The semantic content of the current query correlates strongly with the historical turns required to answer it safely. A user query about "restaurants" might fail to semantically match a past message about "anaphylactic shock" if standard embedding distances are used.

## Foundational Learning

- **Context Window & Token Budgeting**: Understanding that LLMs have hard limits on input tokens (and cost/latency scales with them) is prerequisite to understanding why "naive replay" fails and why AFM packs messages. If you double the conversation history length in a standard LLM API call, what happens to the inference latency and cost?

- **Prompt Engineering vs. Fine-Tuning**: AFM is explicitly a "model-agnostic, plug-and-play design" operating at the "prompt layer." It does not modify weights. Distinguishing inference-time context manipulation from training-time weight updates is crucial. Does AFM require retraining the base LLM (e.g., GPT-4) to remember the allergy constraint?

- **Cosine Similarity & Embeddings**: The mechanism relies on semantic search logic (sim(m_i, q_t)). Without understanding vector embeddings, the "relevance scoring" component is a black box. Why might a user query about "restaurants" fail to semantically match a past message about "anaphylactic shock" if standard embedding distances are used?

## Architecture Onboarding

- **Component map**: FocusManager -> TokenCounter -> Scorer -> Compressor
- **Critical path**: User calls `add_message()` for new input. User calls `build_context(query, budget)`. System computes scores s_i for all history. System assigns "intended" fidelity based on thresholds (τ_high, τ_mid). System loops chronologically, downgrading fidelity if budget b_left is insufficient. Returns packed list of messages to send to main LLM.
- **Design tradeoffs**: Latency vs. Accuracy: AFM adds 3-20s of system overhead (Table 1) for classification and embedding, trading raw speed for constraint reliability. Heuristic vs. LLM: Using `HeuristicCompressor` is free and fast but may lose nuance; `LLMCompressor` is accurate but adds cost/latency. Saliency vs. Completeness: Strict budget enforcement means low-score messages may be dropped entirely, losing potential "weak signal" context.
- **Failure signatures**: The "Soft Refusal" Drift: The model acknowledges the allergy but recommends peanut-heavy dishes with a disclaimer (Generation failure, not memory failure). The "Illusion of Memory": Baseline models retain the text in the prompt window but fail to act on it (Context presence without influence). Classifier False Negative: If the importance classifier misses the constraint, AFM performs strictly worse than naive replay because it actively compresses critical info.
- **First 3 experiments**: Classifier Accuracy Audit: Run the Importance Classifier on a dataset of constraints vs. chaff. Measure Precision/Recall for the CRITICAL label. Crucial because Section 4.9 shows this is the linchpin. Budget Stress Test: Vary the token budget B (e.g., 500, 1000, 4000 tokens) on the allergy benchmark to find the "breaking point" where critical messages are forced into Compression and performance degrades. Ablation on Latency: Measure the end-to-end latency contribution of the `LLMCompressor` vs `HeuristicCompressor` to quantify the cost of the "adaptive" layer.

## Open Questions the Paper Calls Out

- **Can learnable fidelity selection policies outperform the current heuristic scoring function for assigning Full, Compressed, and Placeholder tiers?** Current thresholds (τ_high = 0.45, τ_mid = 0.25) and scoring weights were hand-tuned on small development conversations without large-scale optimization. Comparison of learned vs. heuristic policies across diverse dialogue domains with statistical significance testing would resolve this.

- **How accurate is the LLM-based importance classifier, and how do classification errors propagate to constraint preservation failures?** The ablation shows importance classification is critical (0% pass rate without it), yet classifier reliability remains unevaluated. The 16.7% failure rate on the allergy benchmark may stem from misclassification. Precision/recall analysis of the classifier on labeled importance data; correlation between misclassification and downstream failures would resolve this.

- **Does AFM generalize across different model families, languages, and constraint types beyond the two benchmarks tested?** All experiments use only gpt-4o-mini; only two custom benchmarks (peanut allergy, tax compliance) are evaluated. The approach relies on OpenAI embeddings and compression APIs; performance with local models or non-English dialogue is unknown. Cross-model experiments (e.g., LLaMA, Claude), multilingual benchmarks, and additional constraint categories (e.g., temporal, numerical) would resolve this.

## Limitations

- **Classifier Dependence**: Success hinges entirely on the importance classifier correctly identifying CRITICAL constraints, with no evaluation of classifier accuracy or false negative rates.
- **Budget Sensitivity**: The token budget B is unspecified, making it impossible to assess whether AFM's success depends on artificially generous memory allocation.
- **Qualitative Grading Criteria**: The strict pass/fail grading for "appropriate seriousness" in responses is not operationalized.

## Confidence

- **High Confidence**: The core architectural design (fidelity-based selective memory with budget-aware packing) is technically sound and the token usage reduction claim is well-supported by the methodology.
- **Medium Confidence**: The 83.3% pass rate on the allergy benchmark is credible given the clear performance gap versus baselines, though the exact grading criteria remain unclear.
- **Low Confidence**: The safety implications of classifier failures and the generalization of results to real-world multi-turn conversations beyond the two synthetic benchmarks.

## Next Checks

1. **Classifier Performance Audit**: Run the Importance Classifier on a diverse dataset of constraints vs. chaff to measure Precision/Recall for the CRITICAL label.
2. **Budget Stress Testing**: Systematically vary the token budget B (e.g., 500, 1000, 4000 tokens) on the allergy benchmark to identify the breaking point where critical messages are forced into Compressed fidelity and performance degrades.
3. **Ablation on LLMCompressor Latency**: Measure end-to-end latency contribution of LLMCompressor vs HeuristicCompressor to quantify the real-world cost of the "adaptive" layer.