---
ver: rpa2
title: Evaluating Steering Techniques using Human Similarity Judgments
arxiv_id: '2505.19333'
source_url: https://arxiv.org/abs/2505.19333
tags:
- size
- kind
- prompt
- human
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Large Language Model (LLM) steering techniques
  using a human-inspired triadic similarity judgment task to assess both steering
  accuracy and alignment with human cognition. Researchers tested prompt-based, task
  vector, DiffMean, and sparse autoencoder (SAE) steering methods on Gemma2-27b and
  Gemma2-9b models, measuring accuracy against ground truth and alignment via Procrustes
  correlation with human embeddings.
---

# Evaluating Steering Techniques using Human Similarity Judgments

## Quick Facts
- arXiv ID: 2505.19333
- Source URL: https://arxiv.org/abs/2505.19333
- Reference count: 32
- Primary result: Prompt-based steering methods outperformed activation-based interventions in both accuracy and human alignment, revealing a human-like 'kind' bias in LLMs prior to steering

## Executive Summary
This study evaluates Large Language Model steering techniques using human-inspired triadic similarity judgments to assess both steering accuracy and alignment with human cognition. Researchers tested prompt-based, task vector, DiffMean, and sparse autoencoder (SAE) steering methods on Gemma2-27b and Gemma2-9b models, measuring accuracy against ground truth and alignment via Procrustes correlation with human embeddings. Prompt-based methods outperformed other techniques in both accuracy and alignment, while neutral prompts showed a bias toward 'kind' similarity over 'size' similarity. SAE results were limited to the smaller Gemma2-9b model due to computational constraints. Crucially, no steering method achieved strong alignment with human representations, especially for size judgments, suggesting fundamental differences in how humans and LLMs process semantic similarity.

## Method Summary
The study employed a triadic similarity judgment task where models choose which of two items is more similar to a reference along specified dimensions (size or kind). Five steering methods were evaluated: zero-shot prompts, in-context learning prompts with 15 examples, task vectors, DiffMean, and SAEs. The Round Things Dataset with 46 spherical concepts varying in kind (artifacts/plants) and continuous size was used to construct triplets. Models were tested on Gemma2-9b and Gemma2-27b. The researchers collected ≥2,500 triplet judgments per steering method, generated 2D embeddings using crowd-kernel triplet loss, and computed Procrustes correlation against human embeddings. Accuracy was measured against ground-truth labels, and human alignment was quantified as squared Procrustes correlation (r²) between model-derived and human-derived embeddings.

## Key Results
- Prompt-based steering methods (zero-shot and ICL) outperformed activation-based interventions in both accuracy and human alignment
- Neutral prompts without additional context showed bias toward 'kind' similarity over 'size' similarity
- Task Vector and DiffMean methods performed worse than prompting (βTV = −0.29, βDM = −0.30, p < 0.001)
- No steering method achieved strong alignment with human representations, especially for size judgments (all R² < 0.5)

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Steering Leverages Pre-Trained Context Sensitivity
- Claim: Prompt-based methods outperform activation-based interventions because they exploit the model's native attention mechanisms rather than overriding them.
- Mechanism: Natural language instructions in prompts are processed through the full transformer stack, allowing attention heads to route information across all layers. In contrast, SAE, DiffMean, and Task Vector methods inject vectors at single layers, creating localized perturbations that may conflict with downstream computation.
- Core assumption: The model's pre-training on instruction-following data creates distributed representations that are more robust than single-layer interventions.

### Mechanism 2: Privileged Representational Axes from Pre-Training
- Claim: LLMs develop asymmetric representational structure during pre-training, with "kind" categories being more robustly encoded than continuous "size" dimensions.
- Mechanism: Natural language corpora contain more linguistic scaffolding for categorical (kind) relationships through explicit labeling, co-occurrence patterns, and taxonomic structures. Continuous dimensions like size receive less consistent encoding across contexts.
- Core assumption: The statistical structure of pre-training text creates path dependency in learned representations.

### Mechanism 3: Accuracy-Alignment Dissociation Through Different Computational Paths
- Claim: High task accuracy and human-like representations are orthogonal evaluation axes; models can achieve correct outputs via non-human representational strategies.
- Mechanism: LLMs can isolate task-specific dimensions with high precision when prompted, filtering out irrelevant information. Humans exhibit cross-dimensional "leakage" where kind information influences size judgments (and vice versa). This creates a paradox: models that are more accurate on ground-truth judgments may be *less* aligned with human representations.
- Core assumption: Human semantic cognition integrates rather than isolates representational dimensions.

## Foundational Learning

- Concept: **Triadic Similarity Judgment Task**
  - Why needed here: This is the core evaluation methodology. Understanding how triplet comparisons (x_ref, x_1, x_2) map to embedding spaces is essential for interpreting all results.
  - Quick check question: Given triplet (orange, baseball, pumpkin), which item should be selected for "size" similarity vs. "kind" similarity?

- Concept: **Procrustes Correlation**
  - Why needed here: This is the quantitative metric for human-model alignment. You must understand it to interpret the r² values reported.
  - Quick check question: Why does Procrustes correlation allow for affine transforms when comparing embedding spaces?

- Concept: **Residual Stream Steering**
  - Why needed here: DiffMean, Task Vectors, and SAE methods all operate by modifying the residual stream. Understanding where and how vectors are injected is critical for implementation.
  - Quick check question: At which token position do all steering methods in this paper apply their interventions?

## Architecture Onboarding

- Component map:
  - Round Things Dataset (46 concepts) -> Triplet construction (ground-truth mutually exclusive) -> Steering methods (5 variants) -> Residual stream extraction (final '+' token) -> Embedding generation (crowd-kernel loss) -> Procrustes correlation computation

- Critical path:
  1. Collect 2,500+ triplet judgments per steering method
  2. Generate 2D embeddings from judgments using crowd-kernel triplet loss (Tamuz et al., 2011)
  3. Compute Procrustes correlation with human embeddings
  4. Compare accuracy (ground-truth matching) vs. alignment (representational similarity)

- Design tradeoffs:
  - **Single-layer vs. multi-layer steering**: Paper uses optimal single layer per method; multi-layer injection might improve activation-based methods but increases complexity
  - **Accuracy vs. alignment optimization**: Methods tuned for accuracy (zero-shot prompts) may sacrifice alignment; the paper reveals this tradeoff explicitly
  - **Model scale**: gemma2-27b shows higher accuracy but sometimes lower alignment than gemma2-9b (e.g., Task Vector condition R²_TV-9B = 0.295 vs. R²_TV-27B = 0.095)

- Failure signatures:
  - **SAE non-availability**: gemma2-27b SAE results unavailable due to lack of trained SAEs—check SAE availability before model selection
  - **Size alignment collapse**: All methods show R² < 0.5 for size alignment; do not expect steering to fix this without architectural changes
  - **Accuracy-alignment inversion**: Higher parameter models may show decreased alignment despite improved accuracy

- First 3 experiments:
  1. **Reproduce baseline on gemma2-9b**: Run zero-shot and ICL prompting on 100 triplets to verify accuracy and alignment metrics match reported values before testing new methods
  2. **Ablate steering layer selection**: Test whether Task Vector and DiffMean performance improves with multi-layer injection (layers 10, 15, 20) vs. single optimal layer
  3. **Cross-domain validation**: Apply triadic judgment framework to a new domain (e.g., animals varying in size and habitat) to test whether kind-size asymmetry generalizes or is dataset-specific

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic properties make prompt-based steering more effective than activation-based methods for achieving human-aligned representations?
- Basis in paper: [explicit] Authors state "Future work should seek to further integrate insights from controlled semantic cognition...to uncover the basis of prompting's success in guiding LLM's learned representations in a context-sensitive manner."
- Why unresolved: The study demonstrates prompting's superiority empirically but does not investigate the underlying computational or representational mechanisms that explain this advantage.
- What evidence would resolve it: Ablation studies probing how prompts reshape internal activations, combined with analyses comparing the representational trajectories of prompt-based vs. activation-based steering through model layers.

### Open Question 2
- Question: Does the privileged representation of "kind" over "size" generalize across other semantic dimensions and conceptual domains?
- Basis in paper: [explicit] Authors call for future work "to test a wider variety of contexts beyond size and kind judgments."
- Why unresolved: The study only tested spherical objects varying in size and kind; it remains unknown whether other dimensions (e.g., color, weight, function) show similar asymmetries in LLM representations.
- What evidence would resolve it: Replicating the triadic judgment paradigm across diverse stimulus sets varying along multiple semantic dimensions, measuring which dimensions show default alignment without steering.

### Open Question 3
- Question: Why do smaller models sometimes exhibit better human alignment than larger models despite lower task accuracy?
- Basis in paper: [inferred] The authors observe "a higher alignment with human representations in both task vector conditions for the smaller parameter gemma-9b-it, despite gemma-27b-it having comparatively higher accuracy."
- Why unresolved: This dissociation between accuracy and alignment is noted but not explained; the relationship between model scale and representational alignment remains unclear.
- What evidence would resolve it: Systematic comparison of accuracy-alignment tradeoffs across model scales, potentially examining whether larger models develop more task-specific but less human-like representations.

### Open Question 4
- Question: Do these findings generalize to larger frontier models and to more naturalistic behaviors requiring semantic control?
- Basis in paper: [explicit] Authors note limitations: "we only evaluate a set of models that could be run on consumer grade GPUs" and "there are more complex and naturalistic behaviors that humans perform that require 'steering' semantic representations."
- Why unresolved: Compute constraints limited evaluation to gemma models; the triadic judgment task, while well-validated, is simpler than real-world semantic control demands.
- What evidence would resolve it: Extending the evaluation framework to larger models (e.g., GPT-4, Claude) and to more complex tasks like the controlled semantic cognition tasks from Giallanza et al. (2024).

## Limitations
- Dataset domain constraint: Single domain (round objects) with controlled attributes limits generalizability to broader semantic spaces
- Steering method coverage: Only subset of possible intervention strategies tested; absent multi-layer approaches that might overcome single-layer limitations
- Model scale incompleteness: SAE steering evaluation unavailable on larger Gemma2-27b model due to computational constraints

## Confidence
- **Prompt-based steering superiority**: High confidence (statistically significant differences across multiple comparisons)
- **Kind-size representational asymmetry**: Medium confidence (robust finding but based on single dataset domain)
- **Accuracy-alignment dissociation**: Medium confidence (theoretically compelling but requires additional experiments to rule out alternative explanations)

## Next Checks
1. **Cross-domain replication**: Apply the triadic judgment framework to a new domain (e.g., animals varying in size and habitat) to test whether kind-size asymmetry generalizes beyond round objects or is dataset-specific

2. **Multi-layer steering ablation**: Test whether Task Vector and DiffMean performance improves with multi-layer injection (layers 10, 15, 20) versus single optimal layer, addressing the single-layer limitation identified in the discussion

3. **Human-like representation induction**: Design a training or fine-tuning protocol that explicitly encourages models to exhibit representational mixing (leakage) between kind and size dimensions, testing whether this reduces the accuracy-alignment dissociation observed in the current study