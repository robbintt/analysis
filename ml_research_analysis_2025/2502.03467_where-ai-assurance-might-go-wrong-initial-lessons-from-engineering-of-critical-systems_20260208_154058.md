---
ver: rpa2
title: 'Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical
  systems'
arxiv_id: '2502.03467'
source_url: https://arxiv.org/abs/2502.03467
tags:
- system
- safety
- systems
- assurance
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies three main ways AI assurance might fail:
  addressing wrong risks, using inadequate techniques, and poor communication. Drawing
  from critical systems engineering experience, it advocates for broader system boundaries,
  rigorous hazard analysis, and more robust assurance methods.'
---

# Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems

## Quick Facts
- **arXiv ID**: 2502.03467
- **Source URL**: https://arxiv.org/abs/2502.03467
- **Reference count**: 40
- **Primary result**: AI assurance failures stem from wrong risks, inadequate techniques, and poor communication; critical systems engineering offers structured frameworks like Assurance 2.0 to build indefeasible confidence

## Executive Summary
This paper identifies critical vulnerabilities in AI assurance processes by drawing lessons from decades of experience in engineering safety-critical systems like aviation and nuclear power. The authors argue that AI assurance often fails by addressing wrong risks, using inadequate techniques, and suffering from poor communication between stakeholders. They advocate for broader system boundaries, rigorous hazard analysis, and structured assurance methods like Assurance 2.0 that use claims, arguments, evidence, and defeaters to build indefeasible confidence. The paper emphasizes the orders-of-magnitude difference in confidence required for critical systems versus everyday ones, calling for explicit evaluation of decision criticality alongside system criticality.

## Method Summary
The paper synthesizes lessons from critical systems engineering, particularly aviation and nuclear safety, to identify failure modes in AI assurance. It proposes Assurance 2.0 as a structured methodology using Claims-Arguments-Evidence (CAE) framework with defeaters for adversarial analysis. The approach draws from traditional engineering's eight-step safety process but recognizes that foundation models break this by being context-agnostic. The method emphasizes systematic hazard analysis, requirements validation, and building arguments for indefeasibility rather than probabilistic confidence. No empirical data is presented; instead, the paper relies on theoretical arguments and analogies from established safety-critical domains.

## Key Results
- AI assurance failures occur in three main ways: addressing wrong risks, using inadequate techniques, and poor communication
- Orders-of-magnitude higher confidence needed for critical systems cannot be achieved with everyday assurance techniques
- Assurance 2.0 framework with defeaters can reduce confirmation bias and expose gaps in safety reasoning
- Architectural guards with simpler, verifiable properties can enforce safety boundaries when internal component behavior cannot be verified
- Decision criticality must be evaluated separately from system criticality to properly scale assurance requirements

## Why This Works (Mechanism)

### Mechanism 1: Assurance 2.0 Deductive Argument Structure
- Claim: Structured assurance cases with explicit defeaters reduce confirmation bias and expose gaps in safety reasoning.
- Mechanism: Claims-Arguments-Evidence (CAE) framework enforced through five argument block types (concretion, substitution, decomposition, calculation, evidence incorporation). Defeaters explicitly challenge any node and must be refuted or accepted as residual risk. Arguments aim for "indefeasibility"—no conceivable new information would change the judgment.
- Core assumption: Human reviewers are prone to complacency; explicit adversarial structure counteracts this bias.
- Evidence anchors:
  - [abstract] "structured safety cases using Assurance 2.0, which employs a systematic approach with claims, arguments, evidence, and defeaters"
  - [section 3.4.1] "The primary hazards in assurance are complacency and confirmation bias... defeaters, which are claims that express a doubt and can target any node in the argument"
  - [corpus] "A Taxonomy of Real-World Defeaters in Safety Assurance Cases" (arXiv:2502.00238) supports defeater-based approaches but notes incompleteness challenges
- Break condition: If defeaters are treated superficially rather than genuinely pursued, the framework becomes performative rather than adversarial.

### Mechanism 2: Architectural Guards for Unassured Components
- Claim: When internal component behavior cannot be verified, external guards with simpler, verifiable properties can enforce safety boundaries.
- Mechanism: Define "viability domains"—regions of operation where safety is maintained—monitored by runtime guards performing continuous verification. This shifts assurance burden from understanding internal behavior to verifying external properties. Defense-in-depth applies this recursively.
- Core assumption: Checking a result is simpler than constructing it; guard logic can be made orders of magnitude simpler than primary system.
- Evidence anchors:
  - [section 3.1.2] "viability domains and safety properties can then be enforced using relatively simple external protection systems known as guards or monitors"
  - [section 3.3] "This is appropriate when it is difficult to construct a result, but checking it is simpler"
  - [corpus] "Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems" (arXiv:2510.21254) addresses runtime monitoring but lacks validation for real-world deployment
- Break condition: If guard complexity approaches primary system complexity, the assurance benefit collapses.

### Mechanism 3: Criticality-Scaled Development Methods
- Claim: Critical systems require fundamentally different development methods, not just "trying harder" with everyday techniques.
- Mechanism: Failure rate requirements (e.g., 10⁻⁹ per hour) are outside human experiential intuition. Achieving them requires disciplined, often conservative engineering with massive assurance investment. The eight-step process (environment → requirements → hazard analysis → safety requirements → validation → specification → implementation → verification) must be followed rigorously.
- Core assumption: Historical aviation safety record demonstrates this process works when followed; failures trace to steps 1-5, not step 7.
- Evidence anchors:
  - [abstract] "orders of magnitude difference in confidence needed in critical rather than everyday systems and how everyday techniques do not scale in rigour"
  - [section 3.3] "It is not sufficient to just try harder... critical systems require different methods of development and justification"
  - [corpus] Weak direct corpus support for scaling laws; mostly pedagogical/implementation papers without quantitative failure rate validation
- Break condition: If process steps are shortened or hazard analysis is treated as pro forma, the statistical foundation erodes.

## Foundational Learning

- Concept: Safety vs. Security distinction
  - Why needed here: The paper explicitly contrasts safety (system's harmful impact on environment) with security (environment's harmful impact on system). Conflating these leads to incomplete risk models.
  - Quick check question: For a chatbot, is prompt injection a safety or security issue? What about toxic output generation?

- Concept: Requirements Validation vs. Verification
  - Why needed here: Modern aircraft accidents trace to validation failures (steps 1-5), not verification failures (step 7). AI assurance currently focuses on verification-like testing; the harder problem is validating that requirements address all hazards.
  - Quick check question: If your AI passes all safety tests, have you validated your requirements or verified your implementation?

- Concept: Decision Criticality vs. System Criticality
  - Why needed here: The paper argues for evaluating the criticality of deployment decisions separately from system properties. A moderately capable system deployed at massive scale may pose greater aggregate risk than a highly capable system with limited deployment.
  - Quick check question: Should a consumer AI assistant and a military AI assistant have the same assurance requirements if they share the same base model?

## Architecture Onboarding

- Component map: Eight-step pipeline: (1) Environment Definition → (2) System Requirements → (3) Hazard Analysis → (4) Safety Requirements → (5) Requirements Validation [iterates with 3-4] → (6) System Specification → (7) Implementation + Verification → (8) Assurance Case. Foundation models break this by being context-agnostic; steps 1-5 fall to application developers.

- Critical path: Hazard analysis (Step 3) and requirements validation (Step 5) are where traditional failures concentrate. For AI, the analogous critical path is defining the design basis events/threats that bound the risk space, then validating coverage.

- Design tradeoffs: 
  - Guards vs. diverse replicas: Guards provide higher confidence if verifiable; diversity increases reliability but is hard to quantify.
  - Black-box testing vs. architectural containment: Current AI evaluation (red-teaming, robustness checks) delivers low confidence; architectural guards can compensate but constrain functionality.
  - Narrow vs. broad system boundaries: Narrow boundaries simplify analysis but miss socio-technical risks; broad boundaries are more realistic but harder to model.

- Failure signatures:
  - 737 MCAS pattern: Single sensor fault not adequately analyzed; hazard dismissed rather than mitigated.
  - "Normal accidents" pattern: Tight coupling + interactive complexity → cascading failures.
  - Confirmation bias pattern: Assurance case constructed to support desired conclusion rather than challenge it.

- First 3 experiments:
  1. **Defeater generation exercise**: Take an existing AI safety claim and systematically generate 20+ defeaters. Track how many reveal genuine gaps vs. how many are trivially dismissed.
  2. **Guard complexity measurement**: For a specific AI capability (e.g., code generation), design a guard and measure the ratio of guard code complexity to primary system complexity. Target: >10x simpler.
  3. **Design basis event enumeration**: For a deployed AI application, attempt to enumerate a "necessary and sufficient" set of design basis events. Identify what classes of events cannot be bounded.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What systematic methods can enumerate a necessary and sufficient set of Design Basis Events for AI systems across application domains?
- Basis in paper: [explicit] "For AI applications it may be difficult to enumerate a set with adequate coverage" and the paper calls for moving "from illustrative scenarios to a necessary and sufficient set of events to be addressed."
- Why unresolved: Traditional engineering uses worst-case bounds (largest tsunami, biggest projectile), but AI's input space is high-dimensional and ill-defined.
- What evidence would resolve it: A documented methodology producing validated coverage metrics for hazard enumeration in at least one AI application domain.

### Open Question 2
- Question: What theories can support extrapolation from evaluation coverage on reduced models to deployed AI systems with quantifiable confidence?
- Basis in paper: [explicit] "There is a need for theories and new analysis techniques that allow an extrapolation from coverage and reduced models to deployed ones. In Assurance 2.0 terms, we need theories that allow us to go 'from something measured to something useful.'"
- Why unresolved: Current AI evaluation (red-teaming, robustness checks) delivers very low confidence without theoretical grounding for generalization.
- What evidence would resolve it: Formalized theories linking test coverage measures to probabilistic safety claims, validated empirically across model scales.

### Open Question 3
- Question: How can assurance be provided for Foundation Models when their eventual applications and environments are unknown?
- Basis in paper: [explicit] "For generic AI components, and foundation models in particular, the eventual applications and their environments may be unknown and so it is hard to perform Steps 1 to 5 of the engineering and assurance outline."
- Why unresolved: Traditional assurance requires system context for hazard analysis; foundation models are designed for broad applicability.
- What evidence would resolve it: Identification of general claims about Foundation Models demonstrably valuable across multiple downstream application assurance cases.

### Open Question 4
- Question: How much reliability benefit does architectural diversity actually provide for AI systems, and how can it be quantified?
- Basis in paper: [explicit] "There is little doubt that diversity increases reliability overall, but it is very difficult to quantify by how much, or to provide assurance that it delivers useful benefit in any particular instance."
- Why unresolved: Assuming independent failures is invalid; dependence structure between diverse AI components is unknown.
- What evidence would resolve it: Empirical studies measuring failure correlation between diverse AI components under adversarial and natural distribution shift.

## Limitations
- The paper lacks quantitative validation of its proposed frameworks; most claims rely on analogies from traditional critical systems rather than empirical evidence from AI applications
- No specific parameterized theories are provided for AI assurance, despite claiming these need development
- The relationship between decision criticality and system criticality remains conceptual without practical measurement methods
- The Clarissa toolset for automated case synthesis is mentioned but not described in sufficient detail for implementation

## Confidence
- **High**: The identification of confirmation bias and complacency as primary assurance hazards; the distinction between safety and security; the orders-of-magnitude confidence gap between everyday and critical systems
- **Medium**: The transferability of aviation/nuclear safety processes to AI systems; the feasibility of architectural guards for AI capabilities; the sufficiency of Assurance 2.0 for AI applications
- **Low**: Quantitative claims about failure rates (10⁻⁹/hour) without supporting data; the practical effectiveness of defeater-based adversarial assurance in AI contexts; the scalability of guard-based containment for complex AI behaviors

## Next Checks
1. **Defeater Generation Test**: Apply the defeater framework to a deployed AI system (e.g., a code generation model) by having independent teams generate 20+ defeaters for core safety claims. Measure the proportion that reveal genuine gaps versus trivial objections, and track whether the exercise changes the final assurance judgment.

2. **Guard Complexity Verification**: For a specific AI capability (e.g., autonomous navigation), implement a runtime guard that monitors safety properties. Measure the actual code complexity ratio between the guard and the primary system, and verify whether the guard can be maintained with significantly less effort than the primary system.

3. **Design Basis Event Coverage Analysis**: Take an existing AI application and attempt to enumerate a "necessary and sufficient" set of design basis events. Document which classes of events cannot be bounded (e.g., novel adversarial attacks, emergent behaviors) and assess whether this incompleteness invalidates the assurance approach.