---
ver: rpa2
title: 'Curia: A Multi-Modal Foundation Model for Radiology'
arxiv_id: '2509.06830'
source_url: https://arxiv.org/abs/2509.06830
tags:
- curia
- benchmark
- images
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Curia is a large-scale foundation model for radiology, pre-trained
  on 200 million CT and MRI images using self-supervised learning. It demonstrates
  superior performance across 19 diverse radiological tasks compared to existing models
  like BiomedCLIP and MedImageInsight.
---

# Curia: A Multi-Modal Foundation Model for Radiology

## Quick Facts
- arXiv ID: 2509.06830
- Source URL: https://arxiv.org/abs/2509.06830
- Reference count: 40
- Curia is a large-scale foundation model for radiology, pre-trained on 200 million CT and MRI images using self-supervised learning. It demonstrates superior performance across 19 diverse radiological tasks compared to existing models like BiomedCLIP and MedImageInsight.

## Executive Summary
Curia is a foundation model for radiology trained on 200 million CT and MRI images using self-supervised learning. It achieves state-of-the-art performance across 19 radiological tasks, demonstrating strong cross-modal generalization, few-shot learning capabilities, and clinical reasoning comparable to radiology residents. The model uses a Vision Transformer backbone with frozen features and lightweight adaptation heads for efficient deployment across diverse tasks.

## Method Summary
Curia is pre-trained using the DINOv2 framework on 200 million CT and MRI images (164M CT, 64M MRI) from 150K exams. The model uses ViT-B (86M params) and ViT-L (300M params) architectures with 16×16 patches at 512×512 resolution. Training employs self-supervised distillation with global and local crops, using AdamW optimizer and FP16 precision. Downstream tasks use frozen backbone features with lightweight heads (linear classifiers, attention-based pooling, MLP regressors, or Cox layers) for efficient adaptation.

## Key Results
- Curia outperforms existing models like BiomedCLIP and MedImageInsight across 19 radiological tasks
- Demonstrates strong cross-modality generalization with only 9.17 percentage point accuracy decrease from CT to MRI
- Achieves performance comparable to or exceeding radiology residents in tumor malignancy detection and survival prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pre-training on large-scale clinical CT/MRI data produces transferable anatomical representations.
- Mechanism: The DINOv2 framework combines an image-level objective (aligning class tokens between teacher/student networks), a patch-level objective (masking random patches), and regularization losses. This forces the model to learn both global semantic structure and fine-grained local features without labels.
- Core assumption: The diversity and scale of real-world clinical images (200M slices across modalities) covers enough anatomical variation for emergent generalization.
- Evidence anchors:
  - [abstract]: "pre-trained on 200 million CT and MRI images using self-supervised learning"
  - [section 4.1]: "We adapted the DINOv2 codebase for medical imaging... It is a combination of multiple losses: an image-level objective... a patch-level objective... and multiple regularization losses."
  - [corpus]: Neighbor paper "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)" reports similar findings that multi-disease training improves generalization, supporting the scale-diversity hypothesis.
- Break condition: If downstream tasks require textures or patterns outside the CT/MRI distribution (e.g., ultrasound, nuclear medicine), performance degrades. Paper explicitly acknowledges this limitation.

### Mechanism 2
- Claim: Curia learns modality-agnostic anatomical representations that transfer across CT and MRI without paired training data.
- Mechanism: By training on both CT and MRI simultaneously without explicit pairing, the model learns to map different intensity encodings of the same anatomical structures to similar embedding neighborhoods. This is verified via cross-modality organ recognition and registration.
- Core assumption: Anatomical structure is the primary signal shared across modalities; intensity differences are treated as nuisance variation.
- Evidence anchors:
  - [abstract]: "Curia exhibits clinically significant emergent properties in cross-modality... regimes"
  - [section 2.1]: "On CT to MRI, Curia demonstrated a cross-modal generalization capability, exhibiting a balanced accuracy decrease of 9.17 percentage points... Curia achieved higher accuracy on MRI in this zero-shot setting than other foundation models when trained directly on MRI."
  - [corpus]: Weak direct evidence—no neighbor papers explicitly study CT/MRI cross-modal transfer. The corpus primarily covers multimodal text-image or multi-disease settings, not cross-modality generalization in imaging.
- Break condition: Tasks where CT and MRI show fundamentally different pathology visibility (e.g., acute hemorrhage presents differently) may not transfer cleanly.

### Mechanism 3
- Claim: Frozen features with lightweight aggregation heads achieve competitive performance across diverse task types (classification, regression, survival, segmentation).
- Mechanism: The pre-trained ViT backbone outputs CLS tokens and patch tokens. For downstream tasks, only a small head is trained—linear classifiers, attention-based pooling, or MLP regressors—while the backbone remains frozen. This preserves learned representations while reducing overfitting on small clinical datasets.
- Core assumption: The pre-training corpus captures enough task-relevant features that minimal adaptation suffices.
- Evidence anchors:
  - [abstract]: "Curia demonstrates superior performance across 19 diverse radiological tasks"
  - [section 4.2.1]: "We trained classification, regression, and survival heads on top of our FM, without fine-tuning the ViT weights. This allowed us to have a lightweight and fast adaptation."
  - [corpus]: Neighbor paper on "knee pathology foundation model" similarly uses frozen features with linear probing, achieving strong results. This pattern appears consistent across medical FMs.
- Break condition: Tasks requiring novel reasoning patterns not present in pre-training (e.g., entirely new imaging protocols) may require backbone fine-tuning, which the paper does not extensively evaluate.

## Foundational Learning

- Concept: **Vision Transformer (ViT) tokenization**
  - Why needed here: Curia processes 2D slices as 16×16 patches converted to tokens, plus a CLS token for global representation. Understanding patch-level vs. CLS-level outputs is essential for adapting the model.
  - Quick check question: Can you explain why the CLS token is used for image-level tasks while patch tokens are pooled for object-level predictions?

- Concept: **Self-supervised distillation (DINO-style)**
  - Why needed here: The pre-training uses teacher-student distillation with augmentations. Knowing why this produces semantic features helps diagnose representation quality issues.
  - Quick check question: What role do the global crops vs. local crops play in learning scale-invariant features?

- Concept: **Cox proportional hazards for survival prediction**
  - Why needed here: One of Curia's key results is cancer survival prediction using censored data. The method uses a Cox loss on extracted features.
  - Quick check question: How does censoring in survival data differ from missing labels in classification, and why does this require a different loss function?

## Architecture Onboarding

- Component map:
  Backbone -> Pre-training head (DINO/iBOT) -> Downstream head (linear classifier, attention-based pooling, MLP regressor, or Cox layer) -> Segmentation adapter (SAM decoder)

- Critical path:
  1. Load pre-trained weights from HuggingFace (`raidium/curia`)
  2. Preprocess: resize to 512×512, z-score normalization (no windowing—paper explicitly notes Curia was trained without it)
  3. Extract CLS token for image-level tasks or patch tokens for region-level tasks
  4. Train only the lightweight head; keep backbone frozen unless fine-tuning is explicitly required
  5. For 3D volumes, forward each slice independently, then aggregate (pooling or attention) across the volume

- Design tradeoffs:
  - **2D vs. 3D processing**: Curia processes slices independently, requiring post-hoc aggregation for volumetric tasks. This is computationally efficient but may miss inter-slice context. A native 3D FM would be more expressive but far more expensive.
  - **Frozen vs. fine-tuned backbone**: Frozen features enable rapid deployment and data efficiency; fine-tuning may improve performance on out-of-distribution tasks but risks overfitting and loses the foundation model benefit.
  - **Single-center pre-training**: The 150K exams come from one hospital system, which may embed site-specific biases (scanner protocols, population demographics). Multi-center evaluation (CuriaBench) partially mitigates this but does not eliminate it.

- Failure signatures:
  - **Poor cross-modal transfer**: If training on CT and evaluating on MRI shows >20pp accuracy drop, check whether the MRI protocol or anatomical region is underrepresented in pre-training.
  - **Segmentation collapse with SAM decoder**: If prompted segmentation produces fragmented masks, verify the two-stage training (frozen encoder first, then end-to-end) was followed correctly.
  - **Survival prediction near random**: If c-index ≈ 0.5, confirm censoring is handled correctly and that the Cox loss is applied to the correct feature aggregation level (mask-level for tumor regions).

- First 3 experiments:
  1. **Linear probe on CT Organ Recognition**: Train a single linear layer on frozen Curia-L CLS tokens using the provided Total Segmentator split. Target: match or exceed the 98.40% accuracy reported. This validates correct preprocessing and feature extraction.
  2. **Cross-modal stress test**: Train linear head on CT organ data, evaluate on MRI organ data without any MRI training. Compare the accuracy gap to the reported 9.17pp drop. Large deviations indicate potential data leakage or preprocessing inconsistency.
  3. **Few-shot learning curve**: Train linear heads with 1, 5, 10, 20 samples per class on Kidney Lesion Malignancy. Plot AUC vs. sample count. Verify that Curia approaches its asymptotic performance with ~50 samples, matching Figure 5b. This confirms the data efficiency claim and ensures the frozen-feature pipeline is working as intended.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a native 3D architecture improve performance over the current 2D image-by-image approach for volumetric tasks?
- Basis in paper: [explicit] The authors state that "A native 3D FM could potentially offer improved performance on tasks that require a deeper characterization of volumetric images."
- Why unresolved: Curia currently processes volumes as stacks of 2D images, requiring specialized heads to aggregate features, which may lose inter-slice spatial continuity.
- What evidence would resolve it: A comparative study training a 3D-native Vision Transformer on the same dataset and evaluating it on volumetric segmentation and registration benchmarks.

### Open Question 2
- Question: Can incorporating clinical reports and EHR data enhance the model's contextual understanding and generalization?
- Basis in paper: [explicit] The discussion notes that future evolution will "center on incorporating rich, multimodal data from electronic health records and textual reports."
- Why unresolved: The current model is trained solely on images without accompanying clinical metadata or text, limiting its ability to leverage patient history or clinical context.
- What evidence would resolve it: Experiments integrating text encoders with Curia's visual encoder, evaluated on complex reasoning tasks where visual features alone are ambiguous.

### Open Question 3
- Question: How robust is the model against institutional biases given its pre-training data originates from a single center?
- Basis in paper: [explicit] The limitations section notes "the data source is from a single center, which may introduce institutional biases affecting generalizability."
- Why unresolved: While external benchmarks were used, pre-training on a single hospital's data may encode site-specific protocols or population demographics that limit universal applicability.
- What evidence would resolve it: Pre-training a new model instance on a multi-institutional dataset and comparing the performance delta on out-of-distribution external sites.

## Limitations
- Curia's pre-training corpus is not publicly accessible, creating an irreproducible foundation layer and potential center-specific biases
- Claims about "radiology resident-level performance" are based on comparisons to published studies rather than direct side-by-side benchmarking
- The model's clinical reasoning capabilities are evaluated on curated datasets rather than prospective clinical trials

## Confidence
- **High Confidence**: The technical implementation details of the DINOv2 framework adaptation are well-specified and reproducible. The downstream task evaluation methodology (frozen features with lightweight heads) is clearly described and follows established patterns in medical foundation models.
- **Medium Confidence**: The cross-modality generalization results are internally consistent but rely on specific dataset splits that cannot be independently verified. The few-shot learning curves show expected patterns but may not generalize to all radiological tasks.
- **Low Confidence**: Claims about clinical reasoning capabilities (tumor malignancy detection, survival prediction) are evaluated on curated datasets rather than prospective clinical trials. The comparison to radiology residents, while suggestive, lacks direct experimental validation.

## Next Checks
1. **Center-Independence Test**: Evaluate Curia on multi-center public datasets (e.g., TCIA collections from multiple institutions) to quantify performance variance across scanner vendors and acquisition protocols.
2. **Temporal Generalization**: Test Curia's performance on retrospective cases with known time gaps from training distribution to assess model staleness and adaptation requirements.
3. **Clinical Workflow Integration**: Implement a pilot study where Curia outputs are embedded in actual radiology reporting workflows to measure real-world utility beyond benchmark metrics.