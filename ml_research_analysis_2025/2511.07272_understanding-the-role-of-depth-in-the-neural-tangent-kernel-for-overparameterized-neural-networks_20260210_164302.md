---
ver: rpa2
title: Understanding the role of depth in the neural tangent kernel for overparameterized
  neural networks
arxiv_id: '2511.07272'
source_url: https://arxiv.org/abs/2511.07272
tags:
- kernel
- neural
- networks
- proposition
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the behavior of infinitely wide fully-connected
  ReLU neural networks as their depth increases. The authors show that, under mild
  conditions on initialization and learning rate, the neural tangent kernel converges
  to a limiting kernel as depth grows.
---

# Understanding the role of depth in the neural tangent kernel for overparameterized neural networks

## Quick Facts
- arXiv ID: 2511.07272
- Source URL: https://arxiv.org/abs/2511.07272
- Reference count: 33
- Primary result: NTK converges to a limiting kernel as depth grows, approaching a matrix of ones; closed-form solution converges to a fixed limit on the sphere.

## Executive Summary
This paper investigates how the Neural Tangent Kernel (NTK) and corresponding closed-form solutions behave as depth increases in infinitely wide fully-connected ReLU neural networks. The authors prove that under mild conditions on initialization and learning rate, the NTK converges to a deterministic limit when width dominates depth. They demonstrate that the normalized NTK approaches a matrix of ones as depth grows, and that despite kernel determinant approaching zero, the solution remains bounded. The results apply to arbitrary data on the sphere without requiring assumptions on the spectrum of the Hermite expansion or Mercer decomposition.

## Method Summary
The paper studies infinitely wide fully-connected ReLU networks with data constrained to the sphere S^{n_0-1}. Using recursive kernel formulas and rough path theory, the authors analyze the limiting behavior of the NTK as depth L → ∞. The method involves computing Σ^(L) and Θ^(L)_∞ recursively, normalizing to obtain Θ̄^(L)_∞, and analyzing the convergence properties. No actual training is performed - the work focuses on kernel computation and theoretical analysis of convergence rates. The main theoretical results are supported by empirical evaluations showing convergence behavior as depth increases.

## Key Results
- NTK converges to a deterministic limit when width grows much faster than depth (L ∈ o(min n_l))
- Normalized NTK approaches a matrix of ones as depth → ∞, with correlation coefficient ρ^(L) → 1
- Despite kernel determinant approaching zero, the closed-form solution remains bounded on the sphere
- Results apply to arbitrary data on the sphere without assumptions on kernel spectrum or Hermite expansion

## Why This Works (Mechanism)

### Mechanism 1: NTK Convergence via Width Dominance
- Claim: NTK converges to deterministic limit when width >> depth
- Mechanism: Infinite width and small learning rate linearize gradient descent dynamics, making recursive kernel formula deterministic
- Core assumption: L ∈ o(min_{l=1,...,L-1} n_l)
- Evidence: Theorem 1 reproduces Jacot's result showing deterministic limit; related work confirms deep neural kernels framework

### Mechanism 2: Depth-Induced Kernel Homogenization
- Claim: Normalized NTK converges to matrix of ones as depth → ∞
- Mechanism: Correlation coefficient ρ^(L+1) = h(ρ^(L)) strictly increases toward 1, where h(z) = z·arcsin(z)/π + √(1-z²)/π + z/2
- Core assumption: Initial ρ^(1) ∈ (-1, 1)
- Evidence: Lemma 1 proves ρ^(L)(x,x') → 1; Theorem 2 shows Θ̄^(L)_∞ strictly increases to 1

### Mechanism 3: Solution Boundedness via Rough Path Theory
- Claim: κ_x · κ^{-1} converges to bounded limit despite det(κ) → 0
- Mechanism: Rough path theory constructs paths driven by smooth interpolation; Lyons' Universal Limit Theorem ensures convergence
- Core assumption: Data on compact sphere S^{n_0-1}
- Evidence: Theorem 3 proves ū_∞(t) is bounded with finite bound C'

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: Core object of study; represents gradient update effects in infinite-width limit
  - Quick check: Can you explain why NTK becoming deterministic enables closed-form predictions?

- Concept: **Correlation Coefficient ρ^(L)**
  - Why needed here: Drives entire depth analysis; its convergence to 1 causes kernel homogenization
  - Quick check: Why does ρ → 1 imply all kernel entries approach same value?

- Concept: **Kernel Invertibility on Sphere**
  - Why needed here: Closed-form solution requires invertible κ matrix
  - Quick check: What three data regimes guarantee κ is invertible for L ≥ 2?

## Architecture Onboarding

- Component map: Input (S^{n_0-1}) → [Recursive Kernel Computation] → Θ^(L)_∞ → [Normalization] → Θ̄^(L)_∞ → [Matrix Inversion + Bound Analysis] → κ_x · κ^{-1}

- Critical path:
  1. Verify data on sphere or project via canonical/stereographic projection
  2. Compute Σ^(L) and Θ^(L)_∞ recursively using Propositions 1 and 2
  3. Normalize by n_0 · 2^{L-1}/L to obtain Θ̄^(L)_∞
  4. Check invertibility condition (L ≥ 2 on sphere)
  5. Apply Proposition 3 for closed-form prediction

- Design tradeoffs:
  - Deeper networks → slower convergence but unchanged representation power
  - Width must dominate depth for deterministic kernel; otherwise enter stochastic regime
  - ReLU activation enables closed-form; other activations require Monte Carlo

- Failure signatures:
  - Determinant approaching 0 signals numerical instability
  - Depth/width ratio ≫ 0 leads to high variance (outside paper's scope)
  - Colinear inputs break invertibility

- First 3 experiments:
  1. **Kernel convergence visualization**: Generate random sphere data, compute Θ̄^(L)_∞ for L=1 to 10, plot off-diagonal values toward 1
  2. **Correlation coefficient tracking**: Track ρ^(L) through L=50 for fixed input pair with ρ^(1) ∈ (-1,1)
  3. **Solution stability test**: Compute κ_x · κ^{-1} for increasing depths, verify boundedness despite det(κ) → 0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do limiting depth properties extend to CNNs or architectures with skip connections?
- Basis: Paper explicitly states future research should explore other architectures
- Why unresolved: Theorem 3 proof technique tailored to fully-connected networks; convergence to matrix of ones unknown for convolutional/residual architectures
- What evidence: Extension of Theorem 3 to CNN kernels or empirical evaluations of convergence behavior

### Open Question 2
- Question: Does "pointwise" limit exist when depth >> width (L ≫ max n_l)?
- Basis: Paper hypothesizes pointwise limit might exist despite stochasticity
- Why unresolved: Work focuses on L ∈ o(n_l) regime; Hanin-Nica stochastic regime unclear
- What evidence: Theoretical derivation of NTK distribution limit for individual sample paths in "infinite depth, finite width" setting

### Open Question 3
- Question: Can identified key properties predict limiting behavior of other kernels?
- Basis: Authors provide criteria list but state no experimental validation provided
- Why unresolved: Paper defines candidate kernels satisfying criteria but doesn't verify convergence
- What evidence: Empirical simulations of candidate kernels (like η^(L)) to verify predicted convergence rates

## Limitations
- Convergence of ρ^(L) to 1 is extremely slow, requiring K ≫ 1/δ for practical thresholds
- Analysis assumes perfect ReLU networks without finite precision or activation noise
- Results apply to specific regime where width >> depth; stochastic regime remains unexplored

## Confidence
- **High confidence**: Mechanism 1 (NTK convergence via width dominance) - well-established framework building on Jacot et al.
- **Medium confidence**: Mechanism 2 (Depth-induced kernel homogenization) - rigorous proof but extremely slow convergence limits practical relevance
- **Medium confidence**: Mechanism 3 (Solution boundedness via rough path theory) - novel technique with limited empirical validation

## Next Checks
1. Implement empirical convergence tests for ρ^(L) on sphere data with L up to 50 to quantify "extremely slow" convergence rate
2. Test kernel invertibility conditions on small datasets (n < 10) to verify three regimes guaranteeing κ is invertible for L ≥ 2
3. Reproduce closed-form solution stability experiments with synthetic labels to verify κ_x · κ^{-1} remains bounded as depth increases despite det(κ) → 0