---
ver: rpa2
title: 'Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies
  for Retrieval Augmented Generation'
arxiv_id: '2509.04820'
source_url: https://arxiv.org/abs/2509.04820
tags:
- retrieval
- chunk
- chunks
- query
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores two retrieval strategies to improve evidence
  coverage and answer quality in retrieval-augmented generation (RAG) systems for
  complex QA tasks on government documents. The first strategy is a One-SHOT retrieval
  method that adaptively selects as many relevant chunks as possible within a token
  budget, enhanced by rule-based chunk filtering and LLM-based chunk cropping to refine
  the evidence.
---

# Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for Retrieval Augmented Generation

## Quick Facts
- **arXiv ID**: 2509.04820
- **Source URL**: https://arxiv.org/abs/2509.04820
- **Reference count**: 7
- **Primary result**: Both One-SHOT and iterative retrieval strategies significantly improve RAG accuracy on complex government document QA tasks, with One-SHOT achieving 91% average accuracy

## Executive Summary
This paper addresses the challenge of improving evidence coverage and answer quality in retrieval-augmented generation (RAG) systems for complex questions on government documents. The authors propose two complementary strategies: a One-SHOT approach that uses token-budgeted chunk selection with metadata filtering and LLM-based refinement, and an iterative agentic framework with fallback search and chunk deletion. Experiments on 1,000 questions across four difficulty levels show both approaches significantly outperform traditional RAG baselines, achieving over 10% improvement in accuracy. The One-SHOT strategy reaches 91% average accuracy while the iterative strategy achieves 90%.

## Method Summary
The paper explores two retrieval strategies to improve RAG performance. The One-SHOT approach uses Token-Constrained Top-Kmax optimization (treating chunk selection as a knapsack problem within a token budget), combined with rule-based Chunk Filter using metadata and LLM-based Chunk Cropping. The iterative strategy builds on an agentic RAG framework where an LLM dynamically issues search queries, evaluates results, and refines context over multiple turns using fallback search (parallel original and reformulated queries on first turn) and chunk deletion tools. Both are evaluated on 1,000 government document questions across four difficulty levels (L1-L4) using LLM-as-a-judge evaluation.

## Key Results
- One-SHOT strategy with Chunk Filter achieves 91.0% average accuracy (+10.0% over baseline)
- Iterative strategy achieves 90.0% average accuracy (+8.5% over baseline)
- Token-Constrained Top-Kmax alone improves L3/L4 by +9.5/+13.5 points over baseline
- Fallback search benefits L1/L2 most (+3.5% each), chunk delete benefits L3/L4 most (+4.5%/+6.5%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Relaxing fixed top-k constraints and selecting chunks within a token budget improves evidence coverage in single-pass retrieval.
- **Mechanism**: Token-Constrained Top-Kmax optimization treats chunk selection as a knapsack problem, maximizing total relevance subject to token budget constraints.
- **Core assumption**: Golden evidence chunks exist in the retrieval pool but are excluded by arbitrary top-k cutoffs (48% of failures per Figure 1a).
- **Evidence anchors**: Token-Constrained Top-Kmax alone improves L3/L4 by +9.5/+13.5 points over baseline; abstract mentions adaptive chunk selection within context window.
- **Break condition**: When noise from greedy retrieval overwhelms the LLM's ability to filter, or when token budget forces exclusion of critical long chunks.

### Mechanism 2
- **Claim**: Rule-based chunk filtering using metadata (temporal, location, entity signals) improves precision by removing misaligned chunks before generation.
- **Mechanism**: Drop operation removes chunks lacking entity alignment with query; add operation supplements with chunks containing matching/complementary metadata elements.
- **Core assumption**: Chunk metadata is available and correlates with relevance; entities in queries map to document metadata.
- **Evidence anchors**: Chunk Filter achieves highest overall performance (91.0% avg, +10.0% over baseline); drop operation filters out chunks that lack alignment with query entities.
- **Break condition**: When metadata is sparse, inconsistent, or uncorrelated with semantic relevance (e.g., fictional narratives without strong entity grounding).

### Mechanism 3
- **Claim**: Iterative retrieval with fallback search and chunk deletion mitigates query drift and retrieval laziness in agentic systems.
- **Mechanism**: Fallback search retrieves with both reformulated and original query on first turn; chunk_delete tool removes irrelevant chunks, reducing context length to maintain retrieval motivation.
- **Core assumption**: LLM query reformulation can drift from user intent; longer contexts reduce probability of follow-up retrieval (retrieval laziness).
- **Evidence anchors**: Fallback search benefits L1/L2 most (+3.5% each), chunk delete benefits L3/L4 most (+4.5%/+6.5%); follow-up retrieval probability drops from 95% at 3k tokens to 25% at 12k tokens.
- **Break condition**: When combined with Token-Constrained Top-Kmax, chunk delete fails under extended contexts, removing useful chunks due to cognitive overload.

## Foundational Learning

- **Concept: Knapsack optimization for context windows**
  - Why needed here: Token-Constrained Top-Kmax is fundamentally a 0/1 knapsack problem—understanding greedy vs. dynamic programming approaches informs why the paper uses relevance-per-token heuristics.
  - Quick check question: Given chunks with [(r=0.9, t=500), (r=0.7, t=100), (r=0.6, t=200)] and budget T=600, which subset maximizes relevance?

- **Concept: Query drift in retrieval systems**
  - Why needed here: The fallback search mechanism exists specifically because LLM reformulation can shift intent (Appendix A example: "recover contributions" → "withdrawal regulations").
  - Quick check question: If a user asks "Can I get a refund?" and an LLM reformulates to "return policy timeframe," what semantic shift occurred?

- **Concept: Context window saturation effects**
  - Why needed here: Retrieval laziness (Appendix B) shows LLMs become reluctant to continue searching as context grows—critical for understanding why chunk_delete matters.
  - Quick check question: At what context length does follow-up retrieval probability drop below 50% in the paper's experiments?

## Architecture Onboarding

- **Component map**:
```
One-SHOT Path:              Iterative Path:
Query → Vector DB           Query → LRM (reasoning)
  ↓                            ↓
Token-Constrained           Blake format reasoning
Top-Kmax (Tmax budget)        ↓
  ↓                         chunk_search (top-k=5)
Chunk Filter (rule-based)      ↓
  ↓                         + Fallback Search (Turn 1 only)
Chunk Cropping (LLM)        chunk_delete (as needed)
  ↓                            ↓
LLM → Answer               LLM → Answer (after ≤5 turns)
```

- **Critical path**:
  1. Implement basic Token-Constrained Top-Kmax first (Equation 1)—this alone gives +6.5% average improvement
  2. Add rule-based Chunk Filter using available metadata (highest ROI: +10.0% total)
  3. For iterative path, implement fallback search before chunk_delete (fallback prevents drift; delete prevents laziness)

- **Design tradeoffs**:
  - One-SHOT: Higher initial latency (more chunks to process) but fewer retrieval rounds; better when token budget is sufficient and documents have rich metadata
  - Iterative: Lower per-round cost but more total LLM calls; better when queries require multi-hop reasoning or evidence is scattered
  - Assumption: Do NOT combine Token-Constrained Top-Kmax with chunk_delete (Section 4.4 shows this fails—chunk delete removes 9/10 chunks including correct answer in case study)

- **Failure signatures**:
  - Query drift: L1/L2 accuracy drops in basic agentic setup (Table 4: 89.5→86.5 for L1) → fix with fallback search
  - Retrieval laziness: Complex questions (L3/L4) terminate early with incomplete answers → check context length; fix with chunk_delete
  - Chunk delete failure: Extended contexts (>10k tokens) cause deletion of golden chunks → keep initial retrieval small (k=5-10)

- **First 3 experiments**:
  1. **Token budget sweep**: Test One-SHOT with Tmax ∈ {12k, 16k, 20k, 24k, 28k} on held-out questions. Plot accuracy vs. budget to find saturation point before noise dominates.
  2. **Fallback ablation**: Run iterative retrieval with/without fallback search on L1-L4 categories. Expect L1/L2 to show larger gains (±3.5%) than L3/L4 (±2.0%) per Table 4.
  3. **Context length vs. retrieval probability**: Replicate Appendix B experiment—inject controlled noise to reach 3k/6k/9k/12k tokens, measure follow-up retrieval rate. Validate retrieval laziness threshold for your specific LRM.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific mechanisms can successfully integrate high-recall One-Shot retrieval with iterative agentic reasoning without causing failure in context management tools?
- **Basis in paper**: Section 4.4 states that combining approaches requires "more sophisticated mechanisms to balance context size, relevance filtering, and retrieval motivation" because current combinations failed.
- **Why unresolved**: The authors found that combining a large initial context (One-Shot) with iterative tools caused the "chunk delete" tool to fail and increased "retrieval laziness," making the strategies currently incompatible.
- **What evidence would resolve it**: A proposed architecture where an iterative agent can manage a large initial context (e.g., 30+ chunks) without experiencing decision paralysis or erroneously deleting golden chunks.

### Open Question 2
- **Question**: How can LLM-based context refinement tools maintain accuracy when processing large volumes of retrieved chunks (e.g., >30 chunks)?
- **Basis in paper**: Appendix C and Section 4.4 note that the chunk delete tool performs well with 5 chunks but fails when combined with One-Shot retrieval, which typically retrieves 30+ chunks.
- **Why unresolved**: The paper attributes this to "cognitive overload" or decision paralysis, observing that the tool frequently removed genuinely useful chunks when the option count was high, but offers no solution.
- **What evidence would resolve it**: Evaluation of context pruning tools showing stable precision and recall rates as the number of input chunks scales from single digits to thirty or more.

### Open Question 3
- **Question**: Can "retrieval laziness" (premature termination of search) be mitigated through intrinsic model training rather than external context pruning?
- **Basis in paper**: Section 4.3.1 and Appendix B identify "retrieval laziness" as a critical challenge where heavier contexts reduce the likelihood of subsequent retrieval actions.
- **Why unresolved**: The authors mitigate this using a "Chunk Delete" module to reduce context load, but this is a workaround that introduces new failure modes (deleting golden chunks) rather than solving the model's behavioral tendency.
- **What evidence would resolve it**: A study demonstrating that specific fine-tuning or reinforcement learning (e.g., Search-R1) can maintain a high probability of follow-up retrieval (>90%) even as context length increases.

## Limitations
- Dataset access remains a critical limitation - the 1,000 questions and 40,000+ government documents are not yet publicly available
- Exact metadata fields and filtering rules for Chunk Filter are unspecified, limiting reproducibility
- Combining Token-Constrained Top-Kmax with chunk_delete leads to catastrophic performance degradation, but conditions are not fully characterized
- The paper lacks ablation studies on the knapsack algorithm implementation details

## Confidence
- **High Confidence**: The mechanism of token-constrained chunk selection improving evidence coverage - supported by clear performance gains (+9.5/+13.5 points) and the intuitive knapsack framing
- **Medium Confidence**: The effectiveness of rule-based metadata filtering - shows highest overall performance (+10.0%) but depends heavily on metadata quality and availability that may not generalize
- **Medium Confidence**: The iterative fallback search and chunk deletion approach - demonstrates targeted improvements but fails under extended contexts and has a known failure mode when combined with other strategies
- **Low Confidence**: The generalization of these strategies to domains without rich metadata or when queries require complex reasoning beyond the 4-level categorization system

## Next Checks
1. **Token budget saturation analysis**: Run controlled experiments with varying Tmax values (12k-28k tokens) on a representative subset of questions to identify the point where additional tokens provide diminishing returns or introduce noise, validating the knapsack optimization approach.

2. **Query drift measurement**: Implement logging to track semantic similarity between original and reformulated queries across iterative turns, quantifying the extent of drift and measuring the effectiveness of fallback search in maintaining intent alignment.

3. **Context length threshold validation**: Replicate the retrieval laziness experiment from Appendix B with controlled context injection, measuring the exact token thresholds where follow-up retrieval probability drops below critical levels (50%, 25%) to optimize chunk_delete implementation.