---
ver: rpa2
title: 'CHyLL: Learning Continuous Neural Representations of Hybrid Systems'
arxiv_id: '2512.10117'
source_url: https://arxiv.org/abs/2512.10117
tags:
- latent
- hybrid
- systems
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHyLL learns continuous representations of hybrid systems by reformulating
  the state space as a glued quotient manifold, eliminating discontinuities without
  explicit event functions or mode switching. It jointly learns a singularity-free
  neural embedding into higher-dimensional space and a continuous flow there, then
  decodes back to the original state.
---

# CHyLL: Learning Continuous Neural Representations of Hybrid Systems

## Quick Facts
- arXiv ID: 2512.10117
- Source URL: https://arxiv.org/abs/2512.10117
- Authors: Sangli Teng; Hang Liu; Jingyu Song; Koushil Sreenath
- Reference count: 40
- Primary result: CHyLL achieves lower prediction MSE than Neural ODE, Koopman, and latent ODE variants on hybrid systems while maintaining accuracy beyond training horizons

## Executive Summary
CHyLL introduces a novel approach to learning continuous representations of hybrid systems by reformulating the state space as a glued quotient manifold. This eliminates discontinuities without requiring explicit event functions or mode switching logic. The method jointly learns a singularity-free neural embedding into higher-dimensional space and a continuous flow there, then decodes back to the original state. On benchmark hybrid systems including bouncing ball, torus, Klein bottle, and three-link walker, CHyLL demonstrates superior prediction accuracy and maintains performance beyond training horizons where other methods fail. Topological data analysis confirms the learned representations preserve correct Betti numbers.

## Method Summary
The core innovation of CHyLL lies in treating hybrid systems as continuous flows on quotient manifolds where different modes are glued together. Instead of handling discrete mode transitions explicitly, the method learns a neural embedding that maps the original state space into a higher-dimensional continuous space where the dynamics are smooth. A neural ODE is then trained to model the flow in this embedded space. The decoder maps back to the original state space, enabling prediction and control in the physical domain. This approach sidesteps the need for event detection and mode switching logic while preserving the essential topological structure of the hybrid system.

## Key Results
- CHyLL achieves lower prediction MSE than Neural ODE, Koopman, and latent ODE variants on benchmark hybrid systems
- Maintains prediction accuracy beyond training horizons where competing methods fail
- Topological data analysis confirms learned representations preserve correct Betti numbers (1 for circle, 2 for torus, 1 for Klein bottle)
- Enables effective stochastic optimal control in hybrid tasks like ball juggling

## Why This Works (Mechanism)
CHyLL works by eliminating the fundamental discontinuity problem in hybrid systems through topological reformulation. By embedding the original state space into a higher-dimensional manifold where the glued quotient structure is smooth, the method transforms a discontinuous hybrid problem into a continuous one. The neural ODE can then learn smooth flows in this embedded space without needing to handle mode transitions. The decoder ensures predictions can be mapped back to the physical state space. This approach preserves the essential topology while removing the computational complexity of event detection and mode switching.

## Foundational Learning
- Quotient manifolds: Understanding how to represent hybrid systems as continuous flows on spaces where different modes are glued together is essential for the theoretical foundation
- Neural ODE fundamentals: Required to comprehend how continuous dynamics are learned in the embedded space
- Topological data analysis: Needed to verify that learned representations preserve essential topological features through Betti number computation
- Event detection limitations: Understanding why traditional hybrid system approaches fail at long horizons motivates the need for continuous representations

## Architecture Onboarding

**Component map:** Input State -> Encoder (Neural Net) -> Embedded Space -> Neural ODE Flow -> Decoder (Neural Net) -> Output State

**Critical path:** The encoder-decoder pair with the neural ODE flow in between forms the core learning pipeline. The embedding dimension and neural ODE architecture are critical hyperparameters.

**Design tradeoffs:** Higher embedding dimensions provide more expressive power for representing complex topologies but increase computational cost and risk overfitting. The balance between encoder/decoder complexity and neural ODE depth affects both accuracy and training stability.

**Failure signatures:** Poor prediction accuracy typically indicates insufficient embedding dimension or inadequate neural ODE capacity. Loss of topological structure manifests as incorrect Betti numbers in the learned representation.

**First experiments:** 1) Validate embedding preserves topology on simple quotient spaces like circles and tori before scaling up. 2) Test sensitivity to embedding dimension by varying from 2D to 4D on benchmark systems. 3) Compare prediction accuracy on short vs. long horizons to verify claims about maintaining accuracy beyond training.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance on higher-dimensional or more chaotic hybrid systems remains untested
- Computational overhead of learning higher-dimensional embeddings may limit scalability to very large systems
- Topological preservation claims, while supported by Betti number analysis, don't address whether all relevant dynamical features are captured
- The approach assumes gluing quotient manifolds can adequately represent complex hybrid dynamics without structural loss

## Confidence
**High confidence in:** The core mathematical formulation and the approach's ability to eliminate discontinuities through continuous embedding.

**Medium confidence in:** The empirical performance claims on benchmark systems, as they are based on specific test cases.

**Low confidence in:** The generalizability to arbitrary hybrid systems with complex topology or high-dimensional state spaces.

## Next Checks
1. Test CHyLL on hybrid systems with higher-dimensional state spaces (5+ dimensions) to evaluate scalability and computational efficiency.
2. Evaluate performance on chaotic hybrid systems where small errors in representation could lead to significant divergence in predictions.
3. Conduct ablation studies to quantify the impact of different embedding dimensions on prediction accuracy and topological preservation.