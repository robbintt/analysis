---
ver: rpa2
title: Multi-Agent Debate for LLM Judges with Adaptive Stability Detection
arxiv_id: '2510.12697'
source_url: https://arxiv.org/abs/2510.12697
tags:
- debate
- answer
- should
- accuracy
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-agent debate framework for LLM judges,
  addressing the limitations of static aggregation methods like majority voting. It
  introduces a formal mathematical model of the debate process and proves its advantages
  over static ensembles.
---

# Multi-Agent Debate for LLM Judges with Adaptive Stability Detection

## Quick Facts
- arXiv ID: 2510.12697
- Source URL: https://arxiv.org/abs/2510.12697
- Reference count: 40
- Primary result: Debate framework with adaptive stability detection improves judgment accuracy over majority voting while reducing computational costs.

## Executive Summary
This paper proposes a multi-agent debate framework for LLM judges that addresses the limitations of static aggregation methods like majority voting. The framework introduces a formal mathematical model of the debate process, proving that iterative debate amplifies correctness compared to static ensembles. To ensure computational efficiency, an adaptive stability detection mechanism using time-varying Beta-Binomial mixture modeling and Kolmogorov-Smirnov testing is developed. Experiments across diverse benchmarks and models demonstrate significant improvements in judgment accuracy compared to majority voting, with the adaptive stopping mechanism reducing computational costs while maintaining high accuracy.

## Method Summary
The framework employs a 7-agent ensemble debating over multiple rounds with temperature=1.0 and max 10 rounds. Agents generate responses independently each round, observing the debate history. Correctness counts are extracted and modeled using a 2-component Beta-Binomial mixture via EM algorithm. The Kolmogorov-Smirnov statistic between consecutive rounds' distributions determines stability (threshold 0.05 for 2 consecutive rounds). If stable, debate stops and returns majority vote; otherwise continues. Unanimous consensus triggers early termination.

## Key Results
- Debate framework outperforms majority voting across 6 diverse benchmarks (JudgeBench, LLMBar, TruthfulQA, MLLM-Judge, JudgeAnything, BIG-Bench subsets)
- Adaptive stopping achieves comparable accuracy to full 10-round debate while using 4-8 rounds on average
- Significant accuracy improvements over static ensembles with provable theoretical guarantees under idealized assumptions

## Why This Works (Mechanism)

### Mechanism 1: Latent Concept Belief Amplification
Iterative debate with Bayesian updating increases expected correctness compared to static majority voting. Agents maintain distributions over latent concepts and update beliefs based on others' responses. Theorem 4.1 proves correctness increases when at least one response strongly matches the true concept. Over multiple rounds, this creates compounding effects where correct reasoning paths gain weight. Core assumption: conditional independence of agent responses given the latent concept.

### Mechanism 2: Time-Varying Beta-Binomial Mixture for Correctness Dynamics
The correctness distribution across judges is modeled as a mixture of two Beta-Binomial distributions, capturing heterogeneous behavioral regimes (attentive vs. inattentive judges). This allows bimodal patterns that single Binomial cannot capture. The EM algorithm estimates time-varying parameters at each round, providing a phenomenological model of how correctness rates vary across judges and rounds.

### Mechanism 3: Kolmogorov–Smirnov Adaptive Stopping
The KS statistic between successive CDFs of judge correctness distributions detects when debate has stabilized, enabling early stopping with minimal accuracy loss. After fitting the mixture model at each round, the KS statistic quantifies distributional drift. Debate stops when D_t < 0.05 for 2 consecutive rounds, ensuring the correctness distribution has stabilized while avoiding unnecessary computation.

## Foundational Learning

- **Concept: Bayesian Posterior Updating**
  - Why needed here: Core theory relies on agents updating P(θ*|x, Z^t) using Bayes' rule. Essential for understanding Theorem 4.1 proof.
  - Quick check question: Given prior P(θ)=0.5, likelihood P(z|θ*)=0.8, and P(z|θ')=0.2, compute the posterior P(θ*|z).

- **Concept: Beta and Beta-Binomial Distributions**
  - Why needed here: Stability detection mechanism models judge correctness rates with Beta distributions and observed correct counts with Beta-Binomial mixtures. Necessary for implementing EM algorithm.
  - Quick check question: If a Beta(α=2, β=3) prior is combined with 3 successes in 5 trials, what are the posterior parameters?

- **Concept: Kolmogorov–Smirnov Test**
  - Why needed here: Adaptive stopping criterion uses KS statistic to compare empirical CDFs between rounds. Critical for tuning ε and interpreting convergence.
  - Quick check question: What is the KS statistic between two identical distributions? Between a uniform and a bimodal distribution?

## Architecture Onboarding

- **Component map:** Debate Controller -> Agent Pool -> Latent Concept Module -> Mixture Model Estimator -> Stability Monitor
- **Critical path:** Initialization (agents generate Z⁰) → Loop: generate Z^t → extract correctness → update mixture model → check KS stability → if stable, stop and return majority(Z^t); else continue
- **Design tradeoffs:**
  - Ensemble size (n): Larger n increases accuracy but also token cost and latency. Paper finds n=7 optimal.
  - KS threshold (ε): Lower ε yields stricter stability (more rounds, higher compute) but potentially higher accuracy. Paper uses ε=0.05.
  - Max rounds (T): Upper bound prevents runaway computation. Paper uses T=10.
  - Temperature: Affects diversity. Paper uses 1.0 for most tasks, 0.8 for some.
- **Failure signatures:**
  1. Homogeneous agents: If all agents share same bias, posterior updates may not shift toward θ*; debate fails to improve over majority vote.
  2. No initial seed: If no response in round 0 strongly consistent with θ*, debate may never amplify correct concept.
  3. Premature stopping: If KS threshold ε is too high or persistence m is too low, debate may stop before convergence, sacrificing accuracy.
- **First 3 experiments:**
  1. Reproduce debate vs. majority vote on JudgeBench: Implement basic debate loop with n=7 agents, max T=10 rounds. Compare accuracy to single-round majority vote.
  2. Ablate the stability monitor: Run debate with adaptive stopping (ε=0.05, m=2) vs. fixed T=10. Measure rounds used and accuracy difference.
  3. Test sensitivity to KS threshold: Vary ε ∈ {0.01, 0.05, 0.10, 0.20} on JudgeBench. Plot rounds vs. accuracy to find sweet spot.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the framework perform and scale with very large agent ensembles (e.g., >50 agents)? The authors state scalability to very large numbers of agents warrants further investigation.
- **Open Question 2:** To what extent does the violation of the "Independent Agent Responses" assumption degrade the theoretical correctness guarantees? The analysis relies on assumptions that might not fully capture nuances, specifically mentioning independence assumptions.
- **Open Question 3:** Can a single debate protocol be universally optimal, or is a hybrid approach necessary for tasks with varying complexity? Table 1 shows the Debate method underperforming compared to simple Majority Vote on simpler datasets.

## Limitations

- Theoretical guarantees rely on conditional independence assumption (Assumption 4.4) which may break down in practice due to correlated model behaviors
- Beta-Binomial mixture model is phenomenological fit rather than derived from first principles, raising questions about generalizability
- KS-based stopping assumes stability in correctness distribution correlates with convergence in utility, which may not hold if debate plateaus prematurely

## Confidence

- **High Confidence:** Experimental results showing debate outperforms majority voting (Table 1), and adaptive stopping mechanism reduces computational costs while maintaining accuracy (Table 2)
- **Medium Confidence:** Theoretical proofs of Theorems 4.1 and 4.2, given idealized assumptions that may not hold in practice
- **Low Confidence:** Generalizability of Beta-Binomial mixture model and KS stopping criterion to domains outside tested benchmarks

## Next Checks

1. **Test the conditional independence assumption:** Conduct experiments with highly correlated agents to measure how much debate performance degrades, quantifying the impact of Assumption 4.4 violations
2. **Validate the mixture model across domains:** Apply Beta-Binomial mixture + KS stopping to a new domain (e.g., medical diagnosis) and compare performance to single Binomial baseline
3. **Probe premature stopping risks:** Design adversarial tasks where correctness distribution stabilizes early but majority answer is wrong; measure frequency and impact of premature stopping under different ε and m settings