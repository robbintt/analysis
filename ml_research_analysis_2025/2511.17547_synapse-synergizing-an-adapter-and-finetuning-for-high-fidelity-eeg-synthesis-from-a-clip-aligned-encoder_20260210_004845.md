---
ver: rpa2
title: 'SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis
  from a CLIP-Aligned Encoder'
arxiv_id: '2511.17547'
source_url: https://arxiv.org/abs/2511.17547
tags:
- latent
- image
- diffusion
- synapse
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SYNAPSE addresses EEG-to-image synthesis by introducing a two-stage
  framework that learns a semantically aligned latent representation and efficiently
  conditions a Stable Diffusion model. In Stage 1, a CLIP-aligned EEG autoencoder
  jointly optimizes signal reconstruction and cross-modal alignment, producing a compact
  latent space with reduced parameters.
---

# SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder

## Quick Facts
- **arXiv ID**: 2511.17547
- **Source URL**: https://arxiv.org/abs/2511.17547
- **Authors**: Jeyoung Lee; Hochul Kang
- **Reference count**: 40
- **Primary result**: State-of-the-art EEG-to-image synthesis with FID 46.9 on CVPR40

## Executive Summary
SYNAPSE introduces a two-stage framework for EEG-to-image synthesis that leverages a CLIP-aligned encoder and efficient conditioning of Stable Diffusion. The approach first learns a semantically aligned latent representation through joint signal reconstruction and cross-modal alignment, then conditions the diffusion model using a lightweight adaptation module and selective finetuning. This design enables high-fidelity image generation while maintaining strong generalization across subjects, outperforming previous methods on the CVPR40 benchmark.

## Method Summary
SYNAPSE operates in two stages: first, a CLIP-aligned EEG autoencoder jointly optimizes signal reconstruction and cross-modal alignment to produce a compact latent space with reduced parameters; second, the frozen encoder is integrated with a lightweight adaptation module and selective finetuning of cross-attention layers to enable high-fidelity image generation. This synergistic approach captures both low- and mid-level visual features from EEG signals, even when categorical alignment is imperfect, suggesting a perceptual rather than label-based reconstruction mechanism.

## Key Results
- Achieves state-of-the-art FID score of 46.9 on CVPR40 dataset
- Demonstrates improved perceptual quality over baseline methods
- Shows strong generalization across different subjects in EEG-to-image synthesis

## Why This Works (Mechanism)
The two-stage framework works by first establishing a semantically aligned latent representation through joint optimization of signal reconstruction and cross-modal alignment. This creates a compact, efficient encoding that captures essential visual features from EEG. The second stage then leverages this frozen encoder through a lightweight adaptation module and selective finetuning of cross-attention layers, allowing the Stable Diffusion model to efficiently condition on EEG inputs while preserving the learned semantic alignment. The perceptual reconstruction mechanism enables the model to capture visual features even when categorical labels don't perfectly align, suggesting the system learns to reconstruct based on perceptual similarity rather than strict categorical correspondence.

## Foundational Learning
- **CLIP alignment**: Why needed - to establish semantic correspondence between EEG signals and visual features; Quick check - verify cross-modal similarity metrics between encoded EEG and image features
- **Stable Diffusion conditioning**: Why needed - to leverage powerful image generation capabilities while conditioning on EEG inputs; Quick check - confirm that cross-attention layers properly attend to EEG-derived features
- **Adapter modules**: Why needed - to efficiently adapt pre-trained models to new modalities with minimal parameter overhead; Quick check - measure parameter count reduction compared to full finetuning
- **Cross-modal feature extraction**: Why needed - to bridge the gap between neural signals and visual representations; Quick check - visualize feature similarity distributions between EEG and corresponding images

## Architecture Onboarding
**Component Map**: EEG signal -> CLIP-aligned Autoencoder -> Frozen Encoder -> Adapter Module -> Selective Cross-Attention Finetuning -> Stable Diffusion -> Generated Image

**Critical Path**: The essential processing flow is EEG signal → CLIP-aligned autoencoder (Stage 1) → frozen encoder integration with adapter and cross-attention finetuning (Stage 2) → Stable Diffusion generation. The CLIP alignment in Stage 1 is critical as it establishes the semantic foundation that enables effective conditioning in Stage 2.

**Design Tradeoffs**: The framework trades computational efficiency (through parameter reduction and selective finetuning) against potential model capacity. Using a frozen encoder with an adapter module significantly reduces parameters compared to full finetuning, but may limit the model's ability to learn complex EEG-image mappings. The perceptual reconstruction approach sacrifices strict categorical alignment for improved visual quality and generalization.

**Failure Signatures**: Model may fail when EEG signals are noisy or degraded, as the CLIP-aligned autoencoder's reconstruction quality directly impacts downstream generation. Poor cross-modal alignment in Stage 1 will propagate to Stage 2, resulting in semantically mismatched outputs. The selective finetuning strategy may underperform if critical cross-attention layers are not properly identified.

**First Experiments**:
1. Validate CLIP alignment quality by measuring similarity between encoded EEG and corresponding image features
2. Test adapter module parameter efficiency by comparing to full finetuning baselines
3. Evaluate generalization by measuring performance across different subjects in the CVPR40 dataset

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Relies on a single EEG dataset (CVPR40), limiting generalizability to other experimental conditions or datasets
- Claims of perceptual reconstruction are primarily supported by qualitative examples rather than rigorous quantitative feature analysis
- Model robustness to noisy or degraded EEG signals, common in real-world applications, is not evaluated

## Confidence
- **High confidence**: Framework architecture is technically sound and well-documented; FID score of 46.9 is reproducible given described methodology
- **Medium confidence**: State-of-the-art performance requires validation on additional datasets; generalization across subjects demonstrated but not extensively tested
- **Low confidence**: Perceptual reconstruction mechanism lacks rigorous validation; model robustness to real-world EEG variations not evaluated

## Next Checks
1. Evaluate SYNAPSE on at least two additional EEG-to-image datasets with different experimental paradigms to assess generalizability beyond CVPR40
2. Conduct controlled study comparing feature-level representations between generated images and ground truth across low-, mid-, and high-level visual features using similarity metrics or visualization
3. Test model performance and robustness using degraded or noisy EEG signals to simulate real-world conditions and quantify impact on synthesis quality