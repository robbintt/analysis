---
ver: rpa2
title: Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting
arxiv_id: '2507.18678'
source_url: https://arxiv.org/abs/2507.18678
tags:
- data
- arxiv
- scenes
- depth
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable pipeline to convert 2D images into
  comprehensive, scale- and appearance-realistic 3D representations via depth estimation,
  camera calibration, and scale calibration. The method automatically generates metric-scale
  3D point clouds, camera parameters, and annotations from 2D images.
---

# Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting

## Quick Facts
- arXiv ID: 2507.18678
- Source URL: https://arxiv.org/abs/2507.18678
- Authors: Xingyu Miao; Haoran Duan; Quanhao Qian; Jiuniu Wang; Yang Long; Ling Shao; Deli Zhao; Ran Xu; Gongjie Zhang
- Reference count: 40
- Primary result: Released COCO-3D and Objects365-v2-3D datasets with over 2M scenes; demonstrated 4% improvement on ScanNet instance segmentation and over 30% improvement on ScanNet semantic segmentation

## Executive Summary
This paper introduces a scalable pipeline that automatically converts 2D images into comprehensive 3D representations by combining depth estimation, camera calibration, and scale calibration. The method generates metric-scale 3D point clouds, camera parameters, and annotations from 2D imagery without manual intervention. The approach addresses the critical shortage of large-scale 3D datasets by leveraging the abundance of 2D image data. The authors release two substantial 3D datasets (COCO-3D and Objects365-v2-3D) containing over 2 million scenes and demonstrate significant performance improvements across multiple 3D tasks including instance segmentation, semantic segmentation, referring segmentation, 3D dense captioning, and question answering.

## Method Summary
The proposed pipeline transforms 2D images into 3D representations through a three-stage process. First, depth estimation algorithms generate per-pixel depth maps from individual images using monocular depth estimation networks. Second, camera calibration computes camera parameters including intrinsic and extrinsic matrices by analyzing geometric relationships between multiple views. Third, scale calibration ensures the generated 3D models are in metric scale by identifying reference objects with known dimensions or leveraging scene context. The system automatically processes large collections of 2D images to produce comprehensive 3D datasets with associated annotations, enabling downstream 3D understanding tasks without requiring manual 3D annotation efforts.

## Key Results
- Generated COCO-3D and Objects365-v2-3D datasets with over 2 million scenes
- Achieved 4% improvement on ScanNet instance segmentation task
- Demonstrated over 30% improvement on ScanNet semantic segmentation task
- Validated effectiveness across multiple 3D tasks including referring segmentation, 3D dense captioning, and question answering

## Why This Works (Mechanism)
The pipeline's effectiveness stems from its ability to leverage the vast availability of 2D image data while addressing the critical bottleneck of 3D data scarcity. By automating the 2D-to-3D conversion process, the method creates large-scale 3D datasets that capture both spatial relationships and semantic information. The combination of depth estimation provides geometric structure, camera calibration ensures spatial consistency across views, and scale calibration maintains metric accuracy. This comprehensive approach generates 3D representations that are both appearance-realistic and geometrically accurate, enabling downstream models to learn robust spatial understanding from the augmented dataset.

## Foundational Learning

**Depth Estimation**: Predicting per-pixel distance from camera to scene points using monocular images. Needed because 2D images lack explicit 3D geometry information. Quick check: Visualize depth maps alongside original images to verify coherent distance predictions.

**Camera Calibration**: Computing intrinsic (focal length, principal point) and extrinsic (position, orientation) camera parameters from image sequences. Needed to establish spatial relationships between multiple views and reconstruct 3D geometry. Quick check: Verify reprojection error between 3D points and their 2D projections across different camera views.

**Scale Calibration**: Converting relative 3D reconstructions to metric scale using reference objects or scene context. Needed because monocular depth estimation provides relative depths without absolute measurements. Quick check: Measure known object dimensions in the reconstructed 3D space to verify metric accuracy.

**3D Point Cloud Generation**: Converting depth maps and camera parameters into 3D coordinate representations. Needed as the fundamental data structure for 3D understanding tasks. Quick check: Visualize point clouds from multiple viewing angles to ensure complete and accurate 3D reconstruction.

**Multi-view Geometry**: Understanding geometric relationships between multiple camera views using epipolar constraints and triangulation. Needed for accurate 3D reconstruction from 2D image sequences. Quick check: Verify that corresponding points across views satisfy epipolar constraints within acceptable tolerance.

## Architecture Onboarding

**Component Map**: 2D Images -> Depth Estimation -> Camera Calibration -> Scale Calibration -> 3D Point Clouds + Annotations

**Critical Path**: The pipeline follows a sequential workflow where each stage depends on the previous: depth estimation must complete before camera calibration can establish spatial relationships, which in turn must finish before scale calibration can convert to metric units.

**Design Tradeoffs**: The method trades computational complexity for automation, requiring significant processing power to handle large-scale 2D datasets but eliminating manual annotation costs. The approach balances accuracy with scalability by using established computer vision techniques rather than developing novel 3D reconstruction algorithms.

**Failure Signatures**: The system may fail on scenes with transparent or reflective surfaces where depth estimation struggles, textureless environments that hinder camera calibration, or scenes lacking reference objects for scale calibration. Poor overlap between camera views can also cause reconstruction failures.

**First Experiments**:
1. Process a small dataset of well-structured indoor scenes with known camera parameters to validate the complete pipeline end-to-end
2. Test depth estimation accuracy on scenes with transparent objects to establish failure modes
3. Evaluate scale calibration performance on scenes with and without reference objects of known dimensions

## Open Questions the Paper Calls Out
None

## Limitations

- Depth estimation struggles with transparent, reflective surfaces, and complex occlusions
- Camera calibration accuracy depends on sufficient overlap between frames and distinct visual features
- Scale calibration requires objects with known dimensions or sufficient scene context for metric inference

## Confidence

**High Confidence**: Quantitative improvements on downstream 3D tasks (instance segmentation, semantic segmentation) are well-supported by experimental results; contribution of releasing two large-scale 3D datasets is concrete and verifiable.

**Medium Confidence**: Claim that this approach "bridges the 3D data gap" is reasonable but depends on quality for specialized domains not covered in experiments; generalization to novel scene types requires further validation.

**Low Confidence**: Assertion that this is the "first work to automatically generate comprehensive, scale-and appearance-realistic 3D representations" requires careful comparison with prior 2D-to-3D conversion methods.

## Next Checks

1. Conduct ablation studies isolating the impact of depth estimation accuracy versus camera calibration errors on downstream task performance.

2. Evaluate the method's performance on scenes with minimal texture, transparent objects, and highly reflective surfaces to establish failure modes.

3. Test the generalization capability on domain-specific datasets (e.g., indoor robotics scenarios, outdoor street scenes) not represented in the training data to verify the scalability claims.