---
ver: rpa2
title: 'Mitigating hallucinations and omissions in LLMs for invertible problems: An
  application to hardware logic design automation'
arxiv_id: '2512.03053'
source_url: https://arxiv.org/abs/2512.03053
tags:
- pass
- transform
- logic
- lcts
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using Large Language Models (LLMs) as lossless
  encoders and decoders for invertible problems, drawing an analogy to lossless compression
  in information theory. The core method involves using LLMs to transform data from
  a source domain to a destination domain and then back, allowing for the detection
  of hallucinations and omissions by comparing the original and reconstructed data.
---

# Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation

## Quick Facts
- arXiv ID: 2512.03053
- Source URL: https://arxiv.org/abs/2512.03053
- Reference count: 40
- One-line primary result: LLMs can be used as lossless encoders and decoders for invertible problems, detecting hallucinations and omissions by comparing original and reconstructed data in hardware logic design automation.

## Executive Summary
This paper proposes using Large Language Models (LLMs) as lossless encoders and decoders for invertible problems, drawing an analogy to lossless compression in information theory. The core method involves using LLMs to transform data from a source domain to a destination domain and then back, allowing for the detection of hallucinations and omissions by comparing the original and reconstructed data. The approach is demonstrated using Logic Condition Tables (LCTs) as input to generate Hardware Description Language (HDL) code for a 2D network-on-chip router, and then reconstructing the LCTs from the generated HDL. This method yielded significant productivity improvements, correctly identifying LLM errors and assisting in finding design specification errors.

## Method Summary
The method uses LLMs to encode Logic Condition Tables (LCTs) into Verilog HDL code and then decode the generated HDL back into LCTs, comparing the original and reconstructed tables to detect errors. LCTs are structured CSV tables that specify input conditions and output results for hardware logic blocks. The approach uses different LLMs for forward (encoding) and inverse (decoding) transforms to reduce correlated errors. The system detects three types of errors: X_FW (forward errors in generated HDL), X_INV (inverse errors in reconstruction), and M_SP (specification errors where the original LCT is logically wrong but passes the round-trip check).

## Key Results
- The closed-loop LLM flow generated HDL for 13 hierarchical units (11 logic + 2 connectivity) of a 2D NoC router with significant productivity improvements.
- The round-trip comparison successfully identified LLM hallucinations and omissions, improving design verification.
- Using different LLMs for forward and inverse transforms reduced the likelihood of correlated errors compared to using the same model.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Round-trip transformation verifies semantic consistency by checking for identity reconstruction.
- **Mechanism:** The system treats the LLM as an encoder $T$ (Source $\to$ Destination) and a decoder $T^{-1}$ (Destination $\to$ Source). If the reconstructed source differs from the original, it indicates information loss (omission) or invention (hallucination) in one or both passes.
- **Core assumption:** The source format (Logic Condition Tables) contains sufficient information to describe the logic fully, and the LLM is capable of parsing and serializing this format.
- **Evidence anchors:** [abstract] "comparing the original and reconstructed data"; [section 1] "verifying that this results in an identity map"
- **Break condition:** If the inverse transform ($T^{-1}$) hallucinates an error that exactly cancels the forward transform ($T$) error, the identity check yields a false positive (undetected error).

### Mechanism 2
- **Claim:** Structured tabular inputs (LCTs) reduce ambiguity and force completeness better than natural language.
- **Mechanism:** Logic Condition Tables (LCTs) enumerate input conditions and output results in rows and columns. This structure constrains the LLM's output space, reducing the probability of "imagining" logic that isn't specified.
- **Core assumption:** LLMs handle structured data (CSV/Tables) with higher fidelity than unstructured natural language descriptions for formal logic tasks.
- **Evidence anchors:** [section 3.1] "LCTs are particularly well suited for specifying the parallelism... leaving no room for ambiguous interpretation"; [section 6] "Completeness: An LCT with full coverage... is a completely specified design"
- **Break condition:** If the designer provides a "complete but wrong" specification (logical error in the LCT itself), the system will faithfully implement the error, and the round-trip check will pass.

### Mechanism 3
- **Claim:** Using different models for forward and inverse passes reduces the likelihood of correlated errors.
- **Mechanism:** If Model A generates code with a specific syntax or logic bug, Model B is statistically less likely to share the exact same "blind spot" when reconstructing the spec, making the discrepancy easier to detect.
- **Core assumption:** LLM errors are somewhat stochastic or architecture-specific rather than universal truths across all models.
- **Evidence anchors:** [section 5] "Using different LLMs in the forward and inverse transforms separates the design and the check... reduces this possibility even further"; [table 7] Gemini 2.5 Pro was used as the universal inverse model to check various forward models.
- **Break condition:** If a logical misconception is universally present in the training data of all frontier models (e.g., a common misconception of a specific Verilog syntax), both models may still produce correlated errors.

## Foundational Learning

- **Concept:** **Invertible Functions (Bijectivity)**
  - **Why needed here:** The core analogy relies on information theory—specifically, that no information is lost during the "compression" into code or "decompression" back to spec.
  - **Quick check question:** If an LLM generates HDL that optimizes away a "don't care" state, can the original specification still be reconstructed? (See Section 6 on "valid LCT transform").

- **Concept:** **Logic Condition Tables (LCTs) vs. Truth Tables**
  - **Why needed here:** You must distinguish between binary truth tables (1s/0s) and LCTs, which support multi-bit signals, enumerated types, and "don't care" conditions (X).
  - **Quick check question:** How does an LCT represent a sequential state machine transition versus a combinational logic block?

- **Concept:** **Hallucination vs. Omission**
  - **Why needed here:** To diagnose errors in the loop, you must distinguish between the LLM inventing non-existent logic (Hallucination/False Positive) and failing to implement specified logic (Omission/False Negative).
  - **Quick check question:** In a round-trip failure, would an extra row in the reconstructed table likely indicate a forward hallucination or an inverse omission?

## Architecture Onboarding

- **Component map:** Spec Interface (CSV/Markdown LCT tables) -> Forward Prompt (LCT + Port Map) -> LLM -> Verilog -> Inverse Prompt (Verilog + LCT Definition) -> LLM -> Reconstructed LCT -> Comparator (Original vs Reconstructed LCT)
- **Critical path:** The **Inverse Transform Prompt** is the bottleneck. The paper notes LLMs are not natively trained to output LCTs, requiring "few-shot" examples (Section 3.4) to teach the format.
- **Design tradeoffs:**
  - **Robustness vs. Cost:** Running two LLM calls (Forward + Inverse) per unit doubles inference cost.
  - **Strictness vs. Noise:** The comparator must be semantic, not syntactic—handling row reordering and "don't care" expansion to avoid false alarms (Section 6).
- **Failure signatures:**
  - **X_FW (Forward Error):** LCTs mismatch; the HDL missed logic or added hallucinations.
  - **X_INV (Inverse Error):** LCTs mismatch; the Verilog was correct, but the inverse LLM failed to parse it.
  - **M_SP (Spec Error):** LCTs match, but simulation fails (the original spec was valid but wrong).
- **First 3 experiments:**
  1. **Unit Test:** Implement a 4-input MUX (Table 1) using the round-trip flow to verify your prompt templates handle "don't care" (X) symbols correctly.
  2. **Stress Test:** Implement a 4-state FSM (Table 3) to check if the inverse transform correctly captures sequential logic (next_state) rather than just combinational output.
  3. **Model Swap:** Run the same LCT through two different forward models (e.g., Claude and Llama) but reconstruct with a single inverse model to observe variance in error rates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the logical equivalence comparison between original and reconstructed Logic Condition Tables (LCTs) be fully automated to handle valid transformations like row/column permutation and "don't care" expansion?
- **Basis in paper:** [explicit] The authors note on Page 6 that valid table transformations currently prevent direct one-to-one comparison, stating, "we do not see any significant technical hurdles blocking automation of the LCT comparison step."
- **Why unresolved:** While the authors deem it feasible, they have not yet implemented an automated solver to verify logical equivalence, relying instead on manual inspection or basic diffing for the experiments.
- **What evidence would resolve it:** The development of a software tool or algorithm that accepts two LCTs and formally verifies their logical equivalence regardless of row ordering or syntactic formatting differences.

### Open Question 2
- **Question:** Does the inclusion of natural language comments in LCT columns improve the accuracy or consistency of the LLM's encoding and decoding transformations?
- **Basis in paper:** [explicit] On Page 3, the paper states regarding the "Comments" column: "...it is useful for human table designers (and potentially for the LLM as well—but that hypothesis is untested in this paper)."
- **Why unresolved:** The experiments utilized LCTs with comments, but no ablation study was conducted to isolate the effect of these natural language descriptors on the model's ability to correctly interpret the logic.
- **What evidence would resolve it:** A comparative study running the closed-loop flow on identical designs with and without the comments column enabled, measuring the difference in hallucination and omission rates.

### Open Question 3
- **Question:** Can this closed-loop verification approach mathematically guarantee the detection of errors, or are there specific conditions under which self-cancelling hallucinations evade detection?
- **Basis in paper:** [explicit] The authors state on Page 1 that the method "does not mathematically eliminate them [errors] because either the encoder phase, the decoder phase, or both might introduce self-cancelling hallucinations producing a match."
- **Why unresolved:** The paper establishes the *possibility* of self-cancelling errors but does not characterize the probability or conditions under which they occur, leaving formal verification still necessary.
- **What evidence would resolve it:** A theoretical analysis or large-scale empirical study quantifying the failure rate of the invertible method specifically caused by correlated errors in the forward and inverse transforms.

## Limitations
- **Prompt engineering opacity:** The paper does not provide the exact inverse-transform prompt template or the few-shot MUX2 example, which is critical for reproducing the LCT reconstruction capability.
- **LLM error cancellation risk:** There remains a non-zero probability of "self-cancelling hallucinations" where one model's error is exactly undone by the other's, producing a false identity pass.
- **Specification quality dependence:** The approach assumes the original LCT is correct; "valid but wrong" specifications will pass the round-trip check while still producing incorrect hardware.

## Confidence
- **High confidence:** The round-trip transformation mechanism for detecting LLM errors (Mechanism 1) is theoretically sound and empirically demonstrated in the router case study.
- **Medium confidence:** The claim that structured tabular inputs (LCTs) reduce ambiguity compared to natural language is supported by the case study but lacks ablation studies.
- **Medium confidence:** The assertion that different models reduce correlated errors is logically sound but only partially validated—the paper uses different models but doesn't systematically measure error correlation reduction.

## Next Checks
1. **Error cancellation stress test:** Deliberately introduce a known forward hallucination (e.g., extra logic row) and measure whether the inverse transform consistently detects it across different model pairs, or if self-cancelling errors occur in any combinations.
2. **Specification error differentiation:** Create a test suite with both "correct specs" and "valid but wrong specs" (logical errors in LCTs), then verify the system can distinguish between LLM-induced errors (detected by LCT mismatch) and designer errors (LCTs match but simulation fails).
3. **Generalization probe:** Apply the round-trip method to a different hardware domain (e.g., simple processor pipeline or memory controller) to test whether the LCT+inverse transform approach generalizes beyond 2D NoC routers, particularly for designs with more complex state machines or timing constraints.