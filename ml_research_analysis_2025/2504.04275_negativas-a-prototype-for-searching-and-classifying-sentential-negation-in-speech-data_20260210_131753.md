---
ver: rpa2
title: 'negativas: a prototype for searching and classifying sentential negation in
  speech data'
arxiv_id: '2504.04275'
source_url: https://arxiv.org/abs/2504.04275
tags:
- negation
- data
- structures
- neg2
- neg1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces negativas, a prototype tool designed to automatically\
  \ identify and classify three types of sentential negation structures in Brazilian\
  \ Portuguese speech data: pre-verbal (NEG1), double negation (NEG2), and post-verbal\
  \ (NEG3). Developed using Python and spaCy\u2019s NLP libraries, the tool was trained\
  \ on a dataset of 22 transcribed sociolinguistic interviews from the Falares Sergipanos\
  \ database."
---

# negativas: a prototype for searching and classifying sentential negation in speech data

## Quick Facts
- arXiv ID: 2504.04275
- Source URL: https://arxiv.org/abs/2504.04275
- Reference count: 3
- Tool achieves 93% accuracy in classifying three types of sentential negation in Brazilian Portuguese speech data

## Executive Summary
This work introduces negativas, a prototype tool designed to automatically identify and classify three types of sentential negation structures in Brazilian Portuguese speech data: pre-verbal (NEG1), double negation (NEG2), and post-verbal (NEG3). Developed using Python and spaCy's NLP libraries, the tool was trained on a dataset of 22 transcribed sociolinguistic interviews from the Falares Sergipanos database. The classification process involved rule-based matching with POS-tagging to detect negation patterns. Inter-annotator agreement was moderate (Fleiss' Kappa = 0.57), and the tool achieved a 93% accuracy rate overall. NEG1 was dominant (91.5%), while NEG2 and NEG3 were rare (7.2% and 1.2%, respectively), reflecting their contextual restriction. Challenges arose in distinguishing overlapping negation forms and handling unpunctuated spoken data. The findings underscore the need for NLP improvements to better process spoken language features, supporting more robust linguistic and AI-driven analyses of negation in Portuguese.

## Method Summary
The negativas tool employs a rule-based approach using spaCy's Matcher with the pt_core_news_lg model to identify three sentential negation structures in Brazilian Portuguese: NEG1 (pre-verbal "não" + verb), NEG2 (double negation "não" + verb + "não"), and NEG3 (post-verbal verb + "não"). The system was trained on 22 transcribed sociolinguistic interviews from the Falares Sergipanos database, processed to remove headers and disfluency markings. Patterns were defined using POS-tagging: NEG1 matches ["não", VERB/AUX], NEG2 matches ["não", VERB/AUX, "não"], and NEG3 matches [VERB/AUX, "não"]. Performance was evaluated against human annotations using accuracy, Cohen's Kappa (0.58), and Fleiss' Kappa (0.57) for inter-annotator agreement.

## Key Results
- negativas achieved 93% overall accuracy in classifying negation structures
- NEG1 (pre-verbal) dominated at 91.5% of instances, while NEG2 (7.2%) and NEG3 (1.2%) were rare
- Inter-annotator agreement was moderate with Fleiss' Kappa of 0.57
- Tool struggled with NEG2 classification, sometimes misclassifying as overlapping structures
- Class imbalance highlights need for weighted evaluation metrics

## Why This Works (Mechanism)
The tool works by leveraging spaCy's rule-based Matcher to detect specific lexical and syntactic patterns of negation in Portuguese. The approach capitalizes on the predictable word order of negation structures: pre-verbal negation always places "não" before the verb, post-verbal places it after, and double negation repeats "não" in both positions. By using POS-tagging with the pt_core_news_lg model, the system can identify verbs and auxiliaries reliably in context, allowing pattern matching even in unpunctuated speech data. The rule-based nature ensures deterministic classification and interpretability, while the spaCy framework provides efficient processing of large transcript datasets.

## Foundational Learning
- **POS-tagging**: Part-of-speech tagging identifies grammatical categories (verbs, auxiliaries) needed to distinguish negation structures - quick check: verify pt_core_news_lg correctly tags Portuguese verbs in test sentences
- **Rule-based matching**: Pattern matching using predefined rules rather than machine learning - quick check: test Matcher patterns on isolated negation examples
- **Inter-annotator agreement**: Statistical measure (Fleiss' Kappa) quantifying consistency between human annotators - quick check: calculate Kappa on small annotated sample
- **Class imbalance**: Distribution skew where one class (NEG1) vastly outnumbers others - quick check: examine class frequencies in test data
- **Disfluency handling**: Processing speech transcripts containing pauses, hesitations, and repairs - quick check: verify preprocessing removes GELINS markers
- **Unpunctuated text processing**: Working with speech transcripts lacking standard punctuation - quick check: test boundary detection without sentence delimiters

## Architecture Onboarding
- **Component map**: Input transcripts -> Preprocessing (header/disfluency removal) -> spaCy NLP pipeline (tokenization, POS-tagging) -> Matcher pattern application -> Classification output -> Evaluation metrics
- **Critical path**: The core processing flow is: read text -> clean headers/disfluencies -> apply spaCy pipeline -> match negation patterns -> output classifications -> compute accuracy/Kappa
- **Design tradeoffs**: Rule-based approach offers interpretability and deterministic results but lacks flexibility for unseen patterns; high accuracy on dominant class but poor recall for rare classes; handles unpunctuated data but struggles with sentence boundaries
- **Failure signatures**: NEG2 misclassified as overlapping NEG1/NEG2/NEG3; low recall for NEG3 due to class imbalance; false positives from disfluency artifacts; boundary ambiguity without punctuation
- **First experiments**:
  1. Test Matcher patterns individually on isolated negation examples to verify pattern correctness
  2. Run preprocessing pipeline on sample transcript to confirm header and disfluency removal
  3. Evaluate classification on balanced synthetic dataset to assess true per-class performance

## Open Questions the Paper Calls Out
- How can prosodic information be integrated into NLP models to accurately identify sentence boundaries in unpunctuated speech data?
- How can the tool be refined to disambiguate double negation (NEG2) from overlapping or adjacent pre- and post-verbal structures?
- What modifications are required to adapt the lexical patterns to handle informal orthographic variations such as "n" or "ñ" in social media data?

## Limitations
- Class imbalance (91.5% NEG1 vs. 1.2% NEG3) inflates overall accuracy and masks poor performance on rare negation types
- Moderate inter-annotator agreement (Fleiss' Kappa = 0.57) suggests inherent difficulty in negation annotation and potential ground truth quality issues
- Rule-based approach struggles with sentence boundaries in unpunctuated speech and overlapping negation structures
- Limited to Brazilian Portuguese from one sociolinguistic database, restricting generalizability to other dialects or contexts

## Confidence
- **High**: Methodology is detailed and reproducible with access to code and dataset; standard NLP tools used with clear pattern definitions
- **Medium**: Accuracy metric potentially misleading due to class imbalance; confusion matrix suggests good performance on dominant class but unknown rare class performance
- **Low**: Exact preprocessing steps and regex patterns not fully specified; insufficient details on human annotation process to evaluate ground truth reliability

## Next Checks
1. Recompute per-class F1 scores and weighted metrics to assess whether the 93% accuracy is representative of actual performance across all negation types
2. Test the tool on synthetic Brazilian Portuguese data with controlled negation patterns to isolate performance from data quality issues
3. Evaluate the tool's robustness by introducing disfluencies and unpunctuated structures similar to those found in the original speech data