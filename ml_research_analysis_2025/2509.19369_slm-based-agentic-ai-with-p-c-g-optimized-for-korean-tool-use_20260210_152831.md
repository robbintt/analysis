---
ver: rpa2
title: 'SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use'
arxiv_id: '2509.19369'
source_url: https://arxiv.org/abs/2509.19369
tags:
- tool
- korean
- planner
- caller
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a small language model (SLM)-based agent architecture,\
  \ Planner\u2013Caller\u2013Generator (P-C-G), optimized for Korean tool use. The\
  \ architecture separates planning, tool calling, and response generation into three\
  \ specialized modules to reduce computational overhead and improve efficiency."
---

# SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use

## Quick Facts
- **arXiv ID:** 2509.19369
- **Source URL:** https://arxiv.org/abs/2509.19369
- **Reference count:** 19
- **Primary result:** P-C-G achieves 75.0% call accuracy and 79.7% task success rate with 12-22% token reduction vs. larger models

## Executive Summary
This paper presents a small language model (SLM)-based agent architecture, Planner–Caller–Generator (P-C-G), optimized for Korean tool use. The architecture separates planning, tool calling, and response generation into three specialized modules to reduce computational overhead and improve efficiency. A Korean-first value policy is introduced to minimize execution failures caused by Korean-to-English code switching. Evaluation covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios using an LLM-as-a-Judge protocol averaged over five runs. Results show P-C-G achieves competitive tool-use accuracy (75.0% call accuracy) and end-to-end quality (79.7% task success rate) while reducing token usage by 12–22% compared to larger models, demonstrating that role-specialized SLMs are a cost-effective alternative for Korean tool-use agents.

## Method Summary
The P-C-G architecture implements three specialized SLMs: a Planner that generates initial batch plans with limited on-demand replanning, a Caller that validates schema-value pairs and executes tools with Korean-first value preservation, and a Generator that synthesizes final responses. The system processes Korean tool-use queries by first generating a complete execution plan, then sequentially validating and executing each tool call while preserving Korean values unless schema constraints require otherwise. Evaluation uses an LLM-as-a-Judge protocol (Claude-4) averaged over five runs across 400 Korean tool-use instances spanning single-chain, multi-chain, missing-parameters, and missing-functions scenarios.

## Key Results
- Achieves 75.0% call accuracy and 79.7% task success rate (TSR) on Korean tool-use evaluation
- Reduces token usage by 12-22% compared to larger models while maintaining competitive accuracy
- Demonstrates 92.3% as-planned rate with only 1.6% over-planning, validating batch planning efficiency
- Shows SLM limitations in multi-chain scenarios (33.8% call accuracy) and missing-parameters inference (66.6% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Role-Separated Architecture Reduces Cognitive Load on SLMs
Separating planning, tool calling, and generation into three specialized SLMs compensates for individual model limitations by narrowing each module's task scope. The Planner produces a static tool_chain upfront; the Caller validates schema-value pairs and executes; the Generator synthesizes outputs. This prevents any single SLM from needing to master all capabilities simultaneously.

### Mechanism 2: Initial Batch Planning with Limited On-Demand Replanning Reduces Token Overhead
Generating a full execution plan upfront and replanning only on failure/insufficiency reduces redundant planner invocations and token usage. Planner is called once to emit tool_chain; Caller executes sequentially; Planner is re-invoked only when results are insufficient or contradictory. This contrasts with ReAct-style P→C→P→C loops.

### Mechanism 3: Korean-First Value Policy with Schema-Value Co-Validation Reduces Code-Switching Failures
Explicitly preserving Korean in value fields unless schema-constrained reduces execution failures caused by unintended Korean-to-English switching. Caller applies a Korean-first policy—values stay in Korean by default; conversion to English/codes only occurs when schema explicitly requires (enums, format constraints, whitelisted fields). Joint schema-value validation catches type/range errors before execution.

## Foundational Learning

- **ReAct Loop (Reasoning + Acting)**: Understanding this baseline clarifies what "initial batch planning" replaces. Quick check: Can you explain why ReAct's P→C→P→C pattern increases token usage compared to P-C-G's single upfront plan?

- **Schema Validation (Type/Range/Required Fields)**: The Caller module performs joint schema-value validation; understanding JSON Schema constraints is prerequisite to implementing the validation layer. Quick check: Given a tool parameter with "enum": ["seoul", "busan"] and a user input "Seoul", what validation failure would occur and how should normalization handle it?

- **LLM-as-a-Judge Evaluation Protocol**: All reported metrics are computed via Claude-4 judgments averaged over 5 runs; understanding evaluator bias and variance is critical for interpreting results. Quick check: Why does the paper average over 5 runs, and what sources of variance might exist in LLM-as-a-Judge scoring?

## Architecture Onboarding

- **Component map**: Planner (instruction, tool_info → thought, tool_chain, next) → Caller (messages, tools → normalized call object) → Generator (chat_template → final response)

- **Critical path**: User query → Planner generates tool_chain → Caller iterates through tool_chain, validating and executing each tool → If insufficient results → replan (Planner re-invoked) → If sufficient → Generator produces final answer

- **Design tradeoffs**: 
  - Latency vs. accuracy: Planner adds ~2.8s overhead, but reduces Caller input tokens
  - SLM capacity vs. task complexity: Multi-chain performance drops significantly (33.8% call accuracy vs. 95.6% single-chain)
  - Static plan vs. adaptive replanning: High As-planned rate (92.3%) validates batch approach for current dataset

- **Failure signatures**:
  - Missing Parameters scenarios: 66.6% accuracy (below GPT-4o-mini's 76.8%) suggests SLM struggles with inferring absent required fields
  - Multi-chain dependencies: Low call accuracy (33.8%) indicates state management across steps is a weak point
  - Planner latency: 2.8s overhead may be unacceptable for latency-sensitive applications

- **First 3 experiments**:
  1. Ablate role separation: Replace three specialized SLMs with a single SLM handling all roles; measure TSR and token usage delta
  2. Stress-test replanning threshold: Inject controlled failure rates into tool responses; measure at what failure frequency the "limited replanning" strategy degrades
  3. Cross-lingual transfer: Apply Korean-first policy to English-centric tool schemas; measure whether code-switching reductions generalize

## Open Questions the Paper Calls Out

### Open Question 1
How can Planner lightweighting or caching mechanisms be implemented to reduce the observed latency overhead without degrading the high planning accuracy (92.3% As-planned rate) achieved by the current SLM?
- Basis in paper: [explicit] The Conclusion explicitly lists "reducing latency via Planner lightweighting/caching" as the first item for future work
- Why unresolved: The authors identify the Planner's inference time as a source of latency overhead but have not yet tested specific optimization techniques
- What evidence would resolve it: A comparative study measuring token usage and latency against task success rates after applying quantization or caching specifically to the Planner module

### Open Question 2
What architectural or training modifications are required to close the performance gap in "Missing Parameters" scenarios (currently 66.6%) between SLM-based agents and larger models like GPT-4o-mini (76.8%)?
- Basis in paper: [inferred] The Discussion notes that "fine-grained value completion—such as inferring missing arguments—remains a limitation of SLMs"
- Why unresolved: The paper demonstrates that while the SLM excels at Missing Functions, it struggles with the reasoning required for parameter inference
- What evidence would resolve it: Benchmark results showing improved Missing Parameters accuracy following targeted fine-tuning or architectural changes to the Caller module

### Open Question 3
How can an uncertainty-aware replanning mechanism be integrated to effectively manage partial-failure recovery in multi-chain execution?
- Basis in paper: [explicit] The Conclusion proposes "uncertainty-aware replanning for partial-failure recovery" as a specific direction for future research
- Why unresolved: The current design relies on a "limited on-demand replanning" strategy, but the authors suggest a more sophisticated mechanism is needed
- What evidence would resolve it: Evaluation of a modified P-C-G system in simulated failure modes, measuring recovery success rates and efficiency

## Limitations

- Korean-first value policy's effectiveness is demonstrated only on Korean tool-use datasets; generalization to multilingual environments is untested
- Critical architectural details remain unspecified, including fine-tuning procedures for the three specialized SLMs and exact prompt templates
- The dataset used for evaluation (400 instances) is not publicly available, limiting reproducibility and independent validation
- Paper reports only LLM-as-a-Judge evaluation without human verification, introducing potential evaluator bias

## Confidence

- **High confidence**: Role-separated architecture reduces token usage (12-22% reduction verified against comparison models)
- **Medium confidence**: Korean-first value policy improves execution success by reducing code-switching failures (mechanism is logical but lacks external validation)
- **Medium confidence**: P-C-G achieves competitive tool-use accuracy with smaller models (75.0% call accuracy, but only LLM-judge verified)

## Next Checks

1. Reproduce the 12-22% token reduction by implementing P-C-G architecture and comparing against equivalent single-model baselines on identical Korean tool-use tasks
2. Test Korean-first value policy on mixed-language tool schemas to verify whether code-switching reductions generalize beyond Korean-specific environments
3. Conduct human evaluation on a subset of 50-100 instances to validate LLM-as-a-Judge scoring consistency and identify potential evaluator bias in the 75.0% call accuracy metric