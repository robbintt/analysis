---
ver: rpa2
title: Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models
arxiv_id: '2506.06006'
source_url: https://arxiv.org/abs/2506.06006
tags:
- editing
- world
- playing
- action
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether vision-language foundation models
  (VLMs) like Chameleon-7B can serve as effective world models for action-centric
  image editing tasks. While these models inherently store rich multimodal knowledge,
  the authors find they lack a consistent preference for real-world trajectories over
  manipulated ones.
---

# Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models

## Quick Facts
- **arXiv ID**: 2506.06006
- **Source URL**: https://arxiv.org/abs/2506.06006
- **Reference count**: 40
- **Primary result**: Bootstrapping world models from dynamics models in VLMs improves action-centric image editing by 15% over diffusion models.

## Executive Summary
This paper investigates whether vision-language foundation models (VLMs) like Chameleon-7B can serve as effective world models for action-centric image editing tasks. While these models inherently store rich multimodal knowledge, the authors find they lack a consistent preference for real-world trajectories over manipulated ones. To address this, they propose bootstrapping world models from dynamics models using two strategies: (1) weak supervision via synthetic trajectory generation from unlabelled videos annotated by a dynamics model, and (2) inference-time verification using the dynamics model to score candidate observations. Experiments on the Aurora-Bench dataset show that their world model (CWM) significantly outperforms state-of-the-art diffusion models, achieving 15% improvement on real-world subsets according to GPT4o-as-judge and the best average human evaluation across all subsets.

## Method Summary
The method involves training a dynamics model (CDM) to predict actions from pairs of observations, then using this model to annotate unlabeled video frames and create synthetic training trajectories. These trajectories, combined with ground-truth data, are used to train a world model (CWM) with a recognition-weighted loss that focuses on changing image regions. An optional inference-time verification step uses the dynamics model to score multiple candidates from the world model and select the most plausible output. The approach exploits an asymmetry where learning dynamics models is easier than learning world models for VLMs.

## Key Results
- Base VLMs show no preference for real trajectories over manipulated ones in trajectory preference probe
- CDM fine-tuning significantly enhances action-prediction capabilities through supervised learning
- CWM with synthetic data and recognition-weighted loss achieves 15% improvement on real-world subsets
- Inference-time verification provides additional gains by selecting best candidates from multiple generations

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Learning Difficulty in VLMs
- Claim: For vision-language models, learning a dynamics model (predicting an action from two observations) is significantly easier via supervised fine-tuning than learning a world model (predicting an observation from an action and an observation).
- Mechanism: The inverse task of inferring the action connecting two visual states is more constrained and better aligned with VLM pre-training objectives (e.g., video captioning) than the generative task of synthesizing a new visual state. This creates a learning asymmetry that the method exploits.
- Core assumption: The pre-training data of VLMs contains more signal for recognizing actions between frames than for generating physically plausible image edits from text.
- Evidence anchors:
  - [abstract] "we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model."
  - [section 3.1] Fine-tuning Chameleon as a Dynamics Model (CDM) "significantly enhances action-prediction capabilities... by a wide margin."
  - [corpus] Weak direct corpus support. Related work "Bridging the Gap Between Multimodal Foundation Models and World Models" explores MFMs for world modeling but does not confirm this specific learning difficulty asymmetry.
- Break condition: If a VLM's pre-training data is heavily biased towards generative image editing, the asymmetry could diminish or reverse.

### Mechanism 2: Weak Supervision via Automated Annotation
- Claim: A dynamics model can automatically label unlabeled video data to create synthetic trajectories, providing weak supervision that improves the world model.
- Mechanism: The CDM labels pairs of key-frames (`os`, `ot`) from raw video with an action `a`. These synthetic triplets (`os`, `a`, `ot`) are combined with ground-truth data to train the CWM. A novel recognition-weighted loss further improves learning by focusing the model on image tokens that change between frames.
- Core assumption: The CDM's action predictions are sufficiently accurate and the recognition model correctly identifies semantically important token changes.
- Evidence anchors:
  - [abstract] "the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data."
  - [section 3.2] Describes using CDM to annotate motion key-frames and the recognition-weighted objective (Eq. 2) that weights image tokens by importance.
  - [corpus] The concept of using an inverse dynamics model for data labeling is supported by "Video PreTraining (VPT): Learning to act by watching unlabeled online videos."
- Break condition: If the CDM's annotations are systematically noisy, the world model will be trained on flawed trajectories, potentially degrading performance.

### Mechanism 3: Inference-Time Verification as Guided Search
- Claim: A dynamics model can act as a verifier to score multiple candidates generated by a world model, enabling a guided search that selects the most plausible output without additional training.
- Mechanism: The world model generates N candidate next observations. The dynamics model evaluates each candidate by computing `pCDM(a | os, o_t^i)`, acting as a reward function. The candidate with the highest score is selected, leveraging the stronger dynamics model to correct the weaker world model at inference time.
- Core assumption: The dynamics model's reward is a reliable proxy for output correctness and the world model produces a sufficiently diverse set of candidates.
- Evidence anchors:
  - [abstract] "the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time."
  - [section 3.3] Introduces the test-time verification strategy where "CDM as a verifier to enhance CWM performance."
  - [corpus] No strong corpus evidence found for this specific verification loop in VLMs for world modeling.
- Break condition: If the world model's candidates are all of low quality or the dynamics model is misaligned with the evaluation criteria, verification will fail to improve results.

## Foundational Learning

**Concept: World Model vs. Dynamics Model**
- Why needed here: The entire framework is based on distinguishing between predicting the next state (world model) versus predicting the action between states (dynamics model).
- Quick check question: Given a before-and-after image of a door opening, which model predicts "open door" and which predicts the open door image?

**Concept: Weak Supervision**
- Why needed here: A core contribution is using a model's predictions (CDM) as labels to train another model (CWM), which introduces noise but scales data.
- Quick check question: Why is filtering the CDM-labeled trajectories (e.g., by likelihood) a critical step before using them for training?

**Concept: Token-Level Loss Weighting**
- Why needed here: The method uses a recognition model to weight the loss for each image token, forcing the model to focus on areas that change rather than copying static regions.
- Quick check question: In an image edit showing a ball falling, should the background or the ball's trajectory receive a higher loss weight?

## Architecture Onboarding

**Component map:**
Foundation VLM (Chameleon-7B) -> CDM (Dynamics Model, fine-tuned for action prediction) -> CWM (World Model, fine-tuned with synthetic data) -> Recognition Model (for token importance weights) -> Data Pipeline (extracts and annotates keyframes)

**Critical path:**
1) Train CDM on labeled trajectories
2) Use CDM to annotate and filter unlabeled video frames
3) Train CWM with recognition-weighted loss on combined data

**Design tradeoffs:**
- **Training-time vs. Inference-time Bootstrapping**: Data synthesis provides a permanent model improvement but requires data processing. Inference-time verification offers a training-free boost but increases compute cost per query.
- **Inert vs. Over-Active Editing**: The recognition-weighted loss prevents the "copy" failure mode but can cause "over-editing." The tradeoff is tuned by the loss weighting strength.

**Failure signatures:**
- **Copy Artifact**: Model outputs the source image unchanged. (Mitigated by weighted loss and verification).
- **Object Inconsistency**: Generated objects change identity or texture.
- **Instruction Misinterpretation**: Model struggles with spatial or quantitative instructions (e.g., "add one," "move left").

**First 3 experiments:**
1. Evaluate the base VLM on the **trajectory preference probe** (Section 2) to confirm it lacks a world model prior.
2. Fine-tune and evaluate the **CDM** to validate the "easier-to-train" claim and establish a baseline for annotation quality.
3. Apply **inference-time verification** (K=4, 8) to a baseline CWM to measure the performance gain from guided search before implementing the full data synthesis pipeline.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the proposed bootstrapping strategies (weak supervision and inference-time verification) effectively generalize to other multimodal foundation models beyond Chameleon-7B?
- Basis in paper: [explicit] Appendix A.1 states, "Future work should explore the generalisation to other multimodal foundation models with stronger capabilities."
- Why unresolved: The study exclusively utilizes Chameleon-7B due to its native support for interleaved image-text generation, leaving the transferability of the dynamics-to-world-model pipeline unproven for other architectures.
- What evidence would resolve it: Applying the same CDM/CWM fine-tuning pipeline to alternative models (e.g., GPT-4o, Gemini, or different diffusion-based VLMs) and evaluating on AURORA-BENCH.

**Open Question 2**
- Question: Why does inference-time verification fail to improve performance when scaling the number of candidates ($K$) for specific datasets like MagicBrush?
- Basis in paper: [inferred] Section 4.3 notes that increasing candidates does not always improve performance, suggesting that "bootstrapping with a dynamics model that shares the same foundation model backbone may be limiting."
- Why unresolved: The authors observe that verification helps C-FT but not CWM on certain subsets, hypothesizing that CWM may have already "internalized" CDM's preferences, but the exact failure mode of the verifier on specialized editing tasks remains unclear.
- What evidence would resolve it: Ablation studies using an independent or oracle dynamics model for verification rather than one derived from the same foundation model backbone.

**Open Question 3**
- Question: How can world models be further improved to reliably understand subtle instructions involving spatial or quantitative reasoning?
- Basis in paper: [explicit] Appendix A.1 highlights that "understanding subtle instructions (e.g., spatial or quantitative edits) remains challenging" and represents a limitation of the current approach.
- Why unresolved: Despite loss-weighting, the model struggles with fine-grained control, often defaulting to copying the source observation when faced with ambiguous or highly specific low-level edits.
- What evidence would resolve it: Development of specialized benchmarks or training objectives that isolate spatial/quantitative reasoning, showing significant improvement over the reported CWM metrics for the "WhatsUp" subset.

## Limitations

- **Annotation quality dependency**: The effectiveness of bootstrapping depends heavily on the quality of the dynamics model's annotations, which may be noisy or systematically biased.
- **Increased inference cost**: The verification strategy requires generating multiple candidates and scoring them, significantly increasing computational cost per query.
- **Spatial/quantitative reasoning challenges**: Despite improvements, the model still struggles with subtle instructions involving spatial positioning or quantitative changes.

## Confidence

**High confidence**: The claim that learning a dynamics model is easier than learning a world model for VLMs. This is directly supported by quantitative results showing CDM achieves strong action prediction performance while the base VLM struggles with world modeling.

**Medium confidence**: The effectiveness of weak supervision through synthetic trajectory generation. While the methodology is sound and supported by related work in video pre-training, the exact quality of CDM's annotations and their impact on CWM performance depends on factors not fully disclosed (filtering criteria, annotation noise distribution).

**Medium confidence**: The inference-time verification strategy. The mechanism is clearly explained and shows quantitative improvements (e.g., 15% gains with K=4), but the corpus lacks strong precedents for this specific approach in VLMs, and the method increases computational cost significantly at inference time.

**Low confidence**: The claim of "best average human evaluation across all subsets." Human evaluation results are presented but the sample sizes, rater demographics, and inter-rater reliability metrics are not provided, making it difficult to assess statistical significance.

## Next Checks

1. **Trajectory Preference Probe Replication**: Replicate the trajectory preference experiment with a more diverse set of manipulation types (not just adversarial edits) to determine whether the VLM's weakness is specific to certain types of physical reasoning failures or represents a general world-modeling deficit.

2. **Annotation Quality Analysis**: Conduct an ablation study where CWM is trained with varying quality thresholds for CDM annotations (e.g., only using trajectories where CDM's action prediction confidence exceeds different thresholds). This would quantify how sensitive the bootstrapping approach is to annotation noise.

3. **Generalization Cross-Dataset Test**: Evaluate the bootstrapped CWM on datasets completely disjoint from the training distribution (e.g., robotics manipulation datasets or different action-centric editing benchmarks) to assess whether the world model has learned general physical reasoning or merely memorized training patterns.