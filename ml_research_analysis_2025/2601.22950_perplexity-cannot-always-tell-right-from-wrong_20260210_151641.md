---
ver: rpa2
title: Perplexity Cannot Always Tell Right from Wrong
arxiv_id: '2601.22950'
source_url: https://arxiv.org/abs/2601.22950
tags:
- perplexity
- confidence
- accuracy
- sequence
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of using perplexity as a metric
  for model selection in autoregressive models, particularly decoder-only Transformers.
  The core method idea involves leveraging theoretical results on Transformer continuity
  to rigorously prove that high confidence in correct predictions necessarily implies
  existence of other inputs with low perplexity but incorrect predictions.
---

# Perplexity Cannot Always Tell Right from Wrong

## Quick Facts
- arXiv ID: 2601.22950
- Source URL: https://arxiv.org/abs/2601.22950
- Reference count: 10
- The paper proves that high confidence in correct predictions necessarily implies existence of other inputs with low perplexity but incorrect predictions, and empirically validates this through bitstring copy and parity prediction experiments.

## Executive Summary
This paper demonstrates that perplexity, a standard metric for autoregressive models, can fail to identify the most accurate model when confidence increases disproportionately to accuracy. The authors prove theoretically that any confident model will necessarily have incorrectly predicted inputs with near-identical perplexity to correctly predicted ones, and validate this through controlled experiments on bitstring copy and parity prediction tasks. The key insight is that perplexity rewards confidence but fails to penalize confident wrongness, especially in out-of-distribution settings where accuracy cannot be easily measured.

## Method Summary
The paper combines theoretical analysis with empirical validation. The theoretical component proves that for any sequence a compact CPE Transformer predicts correctly and confidently, there must exist another sequence with very low perplexity but incorrect predictions. This is achieved by leveraging continuity bounds from Pasten et al. (2025) to construct near-identical inputs that the model fails to copy. Empirically, the authors test this through three experiments: (1) bitstring copy tasks on custom CPE Transformers and Gemma 3 4B, (2) analytic study of iso-perplexity curves showing unfavorable regions where accuracy and perplexity decouple, and (3) parity prediction experiments demonstrating that the best-performing model often has the worst perplexity score in out-of-distribution settings.

## Key Results
- Any confident CPE Transformer model must have incorrectly predicted inputs with near-identical perplexity to correctly predicted ones
- Perplexity and accuracy can be poorly correlated in out-of-distribution settings, with the best model often having the worst perplexity score
- Iso-perplexity curves reveal "unfavorable regions" where increasing confidence without commensurate accuracy gains actually reduces accuracy while maintaining perplexity

## Why This Works (Mechanism)

### Mechanism 1
High confidence on correctly-predicted sequences forces existence of incorrectly-predicted sequences with near-identical perplexity. By Pasten et al.'s continuity theorem for CPE Transformers, flipping one bit in a sufficiently long sequence creates a nearby input that the model fails to copy, yet the single error's contribution to average log-perplexity vanishes as sequence length increases. This requires CPE architectures and greedy decoding with consistent tie-breaking.

### Mechanism 2
Perplexity inherently favors confidence over accuracy when confidence increases disproportionately. The log-perplexity decomposition shows that increased confidence (lower γ) penalizes incorrect predictions more severely than it rewards correct ones. Without commensurate accuracy gains, perplexity increases despite potentially better models. This applies to binary classification with uniform confidence across predictions.

### Mechanism 3
Out-of-distribution inputs decouple perplexity from accuracy via miscalibrated confidence. OOD generalization requires high confidence for mechanistic reliability, but when models are confidently wrong OOD, perplexity severely penalizes this, inverting the perplexity-accuracy correlation. The paper shows r = -0.94 IID vs. r = +0.31 OOD on parity prediction, with checkpoints having lowest entropy (highest confidence) showing worst perplexity despite best accuracy.

## Foundational Learning

- **Perplexity as cross-entropy exponentiated**: Why needed here: The entire paper critiques perplexity; understanding its mathematical form (exp(E[-log q])) reveals why averaging dilutes error signals. Quick check question: Given a model with accuracy 0.8 and confidence 0.9 on correct predictions, can you compute the perplexity?

- **Compact Position Embeddings (CPE) and continuity**: Why needed here: The theoretical results depend on Pasten et al.'s continuity theorem, which requires CPE architectures (RoPE, ALiBi) rather than absolute positional encodings. Quick check question: Why does continuity guarantee that nearby inputs (small Hamming distance) produce similar output distributions?

- **Greedy vs. stochastic decoding**: Why needed here: Lemma 3.1 assumes deterministic greedy sampling; stochastic sampling changes the failure mode analysis via temperature-dependent confidence transformation. Quick check question: How does increasing temperature θ transform a (1-γ)-confident model into a (1-γ′)-confident model?

## Architecture Onboarding

- **Component map**: Input tokens → Token embedding → CPE (e.g., RoPE) → Decoder-only Transformer layers → Logits → Softmax (temperature-scaled) → Token probabilities → Autoregressive generation → Perplexity computation (log-averaged)

- **Critical path**: The continuity property flows from CPE → bounded logit differences → perplexity convergence. Any break in CPE assumptions invalidates theoretical guarantees.

- **Design tradeoffs**:
  - Higher confidence: Better IID performance, but severe OOD perplexity penalties if wrong
  - Longer sequences: Better perplexity smoothing for single errors, but increased vulnerability to continuity-based failures
  - Temperature > 0: Reduces overconfidence penalty but increases stochastic error probability (Nγ threshold)

- **Failure signatures**:
  - Copy task: α_N copied perfectly, β_N (1-bit flip) produces α_N as output despite different input
  - Parity OOD: Checkpoints with lowest Shannon entropy (highest confidence) have worst perplexity
  - Iso-perplexity inversion: Model A with lower perplexity than Model B, but lower accuracy

- **First 3 experiments**:
  1. **Copy task validation**: Train small CPE Transformer on bitstring copy (vocab: {0, 1, |}). For N = 16→1024, compare pplx_T(α_N) vs pplx_T(β_N) where β_N = α_N with one flipped bit. Verify convergence as N increases.
  2. **Iso-perplexity curve sweep**: For fixed base accuracy a ∈ {0.5, 0.7, 0.9} and confidence γ ∈ {0.01, 0.1, 0.4}, compute critical accuracy a′ for confidence shifts ∆γ/γ ∈ [0, 1]. Identify "unfavourable regions."
  3. **OOD parity correlation**: Train Transformer on parity task with sequences up to length 16. Save checkpoints every 100 steps. Evaluate on IID (length ≤16) and OOD (length = 128). Compute Pearson correlation between perplexity and F1; verify sign inversion between IID/OOD.

## Open Questions the Paper Calls Out

### Open Question 1
What surrogate metrics can reliably replace perplexity for model selection in domains where ground-truth accuracy is unmeasurable? The authors conclude, "While we do not offer an alternative to perplexity in regimes where accuracy cannot be measured, we hope that our work serves as a useful foundation..." This remains unresolved as the paper strictly critiques the existing standard without proposing or validating a replacement metric immune to the identified confidence-accuracy trade-offs.

### Open Question 2
Do the "unfavourable regions" in iso-perplexity curves appear in standard natural language generation tasks, or are they unique to the mechanistic "easy-but-hard" tasks (e.g., parity, copying) studied? While the theory applies generally, the empirical demonstration is restricted to synthetic tasks with binary vocabularies or specific algorithmic structures. Evidence from standard LLM pre-training runs on diverse NLP benchmarks would resolve this.

### Open Question 3
Can specific uncertainty-aware training objectives prevent models from entering the high-confidence/low-accuracy regime where perplexity fails? The paper identifies that "miscalibrated confidence" drives perplexity failures and notes that optimizing perplexity encourages confident predictions, but does not test interventions like label smoothing or calibration loss to prevent it. A comparison of standard vs. calibration-focused training runs would resolve this.

## Limitations

- Theoretical claims rely specifically on CPE Transformers' continuity properties and may not generalize to absolute positional embeddings or other architectures
- Empirical validation uses relatively small-scale experiments (bitstring copy up to 1024 bits, parity prediction on 128-bit sequences) with uncertain generalizability to larger models
- Analysis assumes greedy decoding with consistent tie-breaking; stochastic sampling introduces additional complexity through temperature scaling

## Confidence

**High Confidence:**
- Theoretical proof that CPE Transformers cannot be both highly confident and accurate without introducing low-perplexity incorrect predictions
- Iso-perplexity curve analysis showing that disproportionate confidence increases can reduce accuracy while maintaining perplexity
- Empirical observation of negative perplexity-accuracy correlation in OOD parity prediction experiments

**Medium Confidence:**
- Generalization of theoretical results to all decoder-only Transformers (assumes CPE architectures are representative)
- Practical implications for model selection in real-world applications (based on limited empirical scope)
- Claim that perplexity should be abandoned as a model selection metric (strong conclusion from specific task evidence)

**Low Confidence:**
- Quantitative predictions about exactly when perplexity fails across different model scales and architectures
- Claims about specific confidence thresholds that guarantee failure across diverse tasks
- Generalization to non-autoregressive or encoder-decoder architectures

## Next Checks

1. **Architecture Dependency Test**: Replicate the copy task experiment using absolute positional embeddings to verify whether the perplexity failure mechanism depends specifically on CPE properties. Compare perplexity-accuracy correlation across CPE vs. non-CPE architectures on the same tasks.

2. **Scale Extension Validation**: Apply the parity prediction experiment to larger models (1B-10B parameters) and more complex sequence tasks (text continuation, code generation) to test whether the negative perplexity-accuracy correlation in OOD settings persists at scale.

3. **Temperature Scaling Analysis**: Systematically vary sampling temperature across the copy task and parity experiments to map how stochasticity affects the perplexity-accuracy relationship. This would clarify whether theoretical results under greedy decoding translate to practical settings where temperature scaling is commonly used.