---
ver: rpa2
title: Causality can systematically address the monsters under the bench(marks)
arxiv_id: '2502.05085'
source_url: https://arxiv.org/abs/2502.05085
tags:
- causal
- language
- reasoning
- more
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that causality provides a systematic
  framework for addressing evaluation challenges in machine learning, particularly
  for large language models. The authors identify common issues like biases, artifacts,
  and failure modes as manifestations of underlying causal structures.
---

# Causality can systematically address the monsters under the bench(marks)

## Quick Facts
- **arXiv ID:** 2502.05085
- **Source URL:** https://arxiv.org/abs/2502.05085
- **Reference count:** 40
- **Key outcome:** Causality provides systematic framework for addressing evaluation challenges in machine learning, particularly for large language models

## Executive Summary
This position paper argues that causality provides a systematic framework for addressing evaluation challenges in machine learning, particularly for large language models. The authors identify common issues like biases, artifacts, and failure modes as manifestations of underlying causal structures. They introduce Common Abstract Topologies (CATs) - simple causal graph templates representing confounding, mediation, and spurious correlations - to make causal modeling more accessible. Through case studies, they demonstrate how explicit causal assumptions clarify experimental design, enable principled analysis, and inspire new approaches. The paper advocates for causality-driven research to produce more reliable, interpretable results and stronger justifications for conclusions drawn from empirical machine learning studies.

## Method Summary
The paper proposes a conceptual framework that maps evaluation problems to Common Abstract Topologies (CATs) - three causal graph templates representing confounding, mediation, and spurious correlations. The approach involves formalizing problems with causal DAGs, identifying which CAT applies, specifying causal estimands, and selecting appropriate analysis methods. The framework is demonstrated through case studies including mechanistic interpretability (mediation CAT), label bias (confounding CAT), and debiasing (spurious correlation CAT). The core insight is that explicit causal assumptions enable principled solutions and reveal hidden assumptions in purely statistical approaches.

## Key Results
- Mapping evaluation problems to three simple CAT templates (confounding, mediation, spurious correlation) enables systematic diagnosis and principled mitigation
- Explicit causal assumptions expose hidden assumptions in statistical approaches and enable use of established causal inference machinery
- Evaluating reasoning requires verifying models use causal features, not merely achieving correct outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping evaluation problems to Common Abstract Topologies (CATs) enables systematic diagnosis and principled mitigation.
- **Mechanism:** The paper proposes three templates—confounding, mediation, and spurious correlations—that capture recurring structural patterns in ML evaluation. By identifying which CAT applies, researchers can select appropriate analytical tools (e.g., controlling for confounders vs. mediation analysis).
- **Core assumption:** Most evaluation "monsters" share underlying causal structures that map to a small set of graph topologies.
- **Evidence anchors:** [abstract] "identify several useful Common Abstract Topologies (CATs) in causal graphs which help gain insight into the reasoning abilities"; [section 3, Table 1] Lists three CATs with example phenomena; Related work on causality frameworks for LLM security supports systematic causal analysis approaches.

### Mechanism 2
- **Claim:** Explicit causal assumptions yield more principled solutions than purely statistical treatments.
- **Mechanism:** The paper contrasts Gardner et al. (2021) with Bansal & Sharma (2023)—both address label bias, but the statistical approach requires a "strong independence assumption" that is unrealistic, while the causal framing derives an equivalent regularization term without this assumption.
- **Core assumption:** Formulating the problem causally exposes hidden assumptions and enables use of established causal inference machinery.
- **Evidence anchors:** [section 4.3] "the causal model not only provided a more intuitive motivation for the approach, but also offered a more powerful, principled method"; [section 4.3] Equation 1 shows the causal condition for debiasing; "Unveiling and Causalizing CoT" applies causal perspectives to Chain-of-Thought.

### Mechanism 3
- **Claim:** Evaluating reasoning requires verifying models use causal features, not merely achieving correct outputs.
- **Mechanism:** Example 1 shows an LLM answering a math problem correctly while making arithmetic errors that cancel out and ignoring unethical context. Correctness alone is insufficient; evaluation must verify the reasoning path relies on robust (causal) relationships.
- **Core assumption:** Good reasoning generalizes because it exploits causal structure; spurious correlations may yield correct answers on test sets but fail on distribution shift.
- **Evidence anchors:** [section 2, Example 1] LLM gives correct answer despite arithmetic errors and missing ethical context; [section 2, Claim 1] "To generalize well, the model's reasoning must rely on robustly predictive (i.e. causal) features and relationships rather than spurious ones."

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs) for causal modeling
  - **Why needed here:** All CATs are expressed as DAGs; understanding nodes, edges, and path types is prerequisite for applying the framework.
  - **Quick check question:** Given variables A → B ← C, does conditioning on B create a spurious association between A and C?

- **Concept:** do-calculus and interventional vs. observational distributions
  - **Why needed here:** The paper distinguishes P(Y|X) from P(Y|do(X)); interventions are central to distinguishing causation from correlation.
  - **Quick check question:** Why does P(Y|X) ≠ P(Y|do(X)) in the presence of confounders?

- **Concept:** Mediation analysis (direct/indirect effects)
  - **Why needed here:** Mechanistic interpretability and understanding component contributions require decomposing total effects into direct and mediated paths.
  - **Quick check question:** In A → M → B (plus A → B), what is the natural indirect effect vs. natural direct effect?

## Architecture Onboarding

- **Component map:**
  1. **Problem formalization** → Identify variables (stimulus, response, potential confounders/mediators)
  2. **CAT selection** → Match to confounding, mediation, or spurious correlation template
  3. **Graph specification** → Draw DAG with explicit assumptions
  4. **Query identification** → Define causal estimand (e.g., ATE, NDE, NIE)
  5. **Analysis selection** → Choose appropriate method based on graph structure

- **Critical path:** Problem formalization → CAT selection → Graph specification determines all downstream analysis choices. Wrong CAT selection propagates errors.

- **Design tradeoffs:**
  - **Precision vs. accessibility:** Simple CATs are more usable but may omit relevant structure
  - **Assumption transparency vs. reviewer scrutiny:** Explicit causal graphs invite critique
  - **Abstraction level:** Finer-grained graphs capture more detail but increase complexity and underdetermination risk

- **Failure signatures:**
  - Applying confounding CAT when relationship is actually mediated (incorrect control)
  - Treating spurious correlation as confounding (confounder not identifiable)
  - Claiming causal effects from purely observational experiments without interventional validation

- **First 3 experiments:**
  1. Take an existing benchmark analysis (e.g., GSM8K robustness study) and reframe using the appropriate CAT; compare conclusions
  2. Draw a causal graph for your current evaluation pipeline; identify at least one uncontrolled confounder or unmeasured mediator
  3. Design an intervention that would distinguish between two competing causal graphs for the same phenomenon (falsification test)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Natural Indirect Effect (NIE) be rigorously quantified for "induction heads" in transformers to confirm they are the causal mediators of in-context learning?
- **Basis in paper:** [explicit] The authors analyze mechanistic interpretability studies and explicitly "identify areas for further investigation, such as quantifying the natural indirect effect to understand the full impact of the induction heads on in-context learning."
- **Why unresolved:** Current analysis often relies on ablation or co-perturbation, which may not successfully isolate the specific causal contribution of the mediator (induction heads) from other parallel pathways or confounders.
- **What evidence would resolve it:** A mediation analysis that successfully isolates the NIE of induction heads while controlling for confounders, demonstrating that the effect explains the majority of the variance in in-context learning behavior.

### Open Question 2
- **Question:** Do the three Common Abstract Topologies (CATs) of confounding, mediation, and spurious correlation sufficiently represent all major failure modes in LLM evaluation?
- **Basis in paper:** [inferred] The paper introduces CATs to "make causal model design more accessible" and handle a "bestiary of monsters," but acknowledges the list of issues (bias, artifacts, leakage) is vast and the graphs are often "severely underdetermined."
- **Why unresolved:** While the paper demonstrates the utility of CATs in case studies, it does not provide a comprehensive audit verifying that these three topologies exhaustively cover the structural causal shapes of all known evaluation pitfalls.
- **What evidence would resolve it:** A systematic review mapping documented evaluation failure modes from the literature onto the CAT framework to identify any prevalent "monsters" that require topologies outside the proposed three.

### Open Question 3
- **Question:** Does a causal regularization approach consistently outperform statistical debiasing methods that rely on "strong independence assumptions" in complex linguistic tasks?
- **Basis in paper:** [inferred] The paper contrasts a statistical approach (Gardner et al.) requiring unrealistic assumptions with a causal approach (Bansal & Sharma), claiming the latter offers a "more powerful, principled method" without the need for strong independence.
- **Why unresolved:** The paper provides a theoretical justification and a specific derivation for the causal method but lacks extensive empirical benchmarking across diverse datasets to prove the practical superiority of the causal formulation over the statistical one.
- **What evidence would resolve it:** Comparative benchmarks on datasets where semantic meaning is mutable (violating independence assumptions), showing that causal regularization yields significantly higher accuracy or robustness than statistical local edits.

## Limitations
- Framework assumes complex evaluation problems can be decomposed into three proposed CATs, which may not hold for cyclic dependencies or multiple interacting causal structures
- Effectiveness of CATs in real-world evaluation design remains to be demonstrated through systematic application
- Accessibility claim depends on practitioners having sufficient causal inference literacy to correctly specify DAGs and interpret results

## Confidence
- **High Confidence:** The mechanism by which explicit causal assumptions expose hidden assumptions in statistical approaches is well-supported by the contrast between Gardner et al. and Bansal & Sharma
- **Medium Confidence:** The claim that most evaluation "monsters" map to three simple CATs is logically sound but lacks comprehensive empirical validation
- **Low Confidence:** The generalizability claim that causal reasoning inherently produces better generalization than spurious correlations is asserted but not empirically tested

## Next Checks
1. Apply the CAT framework to an existing benchmark analysis (e.g., robustness studies on GSM8K) and compare whether the causal reframing yields different conclusions or suggests new mitigation strategies
2. Systematically survey 10-15 recent evaluation papers to classify their failure modes using the three CATs, measuring inter-rater reliability and identifying any phenomena that don't fit the templates
3. Design a controlled experiment where two teams evaluate the same model using traditional statistical methods versus the proposed causal approach, comparing the quality and actionability of their recommendations