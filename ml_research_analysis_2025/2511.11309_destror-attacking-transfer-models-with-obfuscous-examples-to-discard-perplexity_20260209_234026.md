---
ver: rpa2
title: 'destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity'
arxiv_id: '2511.11309'
source_url: https://arxiv.org/abs/2511.11309
tags:
- attack
- dataset
- attacks
- data
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces destroR, a framework for attacking Bangla\
  \ sentiment classification models with adversarial examples to expose and exploit\
  \ their weaknesses. It develops three attack strategies\u2014Bangla Paraphrase Attack,\
  \ Bangla Back Translation Attack, and One-Hot Word Swap Attack\u2014to generate\
  \ semantically similar but structurally different inputs that reduce model performance."
---

# destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity

## Quick Facts
- arXiv ID: 2511.11309
- Source URL: https://arxiv.org/abs/2511.11309
- Reference count: 15
- Primary result: Three adversarial attack strategies reduced Bangla sentiment classifier F1 macro scores by up to 40% on internal datasets

## Executive Summary
The paper introduces destroR, a framework for attacking Bangla sentiment classification models with adversarial examples to expose and exploit their weaknesses. It develops three attack strategies—Bangla Paraphrase Attack, Bangla Back Translation Attack, and One-Hot Word Swap Attack—to generate semantically similar but structurally different inputs that reduce model performance. Tested on models like BanglaBERT and XLM-RoBERTa, the attacks lowered F1 macro scores by up to 40% on internal datasets and 37% on external ones, with the One-Hot Word Swap proving most effective. These attacks expose how models can be misled by subtle textual changes, highlighting the need for robust evaluation and adversarial training, especially for low-resource languages like Bangla.

## Method Summary
The destroR framework implements three parallel attack modules to generate adversarial examples for Bangla sentiment classification. The Bangla Paraphrase Attack uses csebuetnlp/banglat5_banglaparaphrase to create context-aware paraphrases with high lexical diversity. The Bangla Back Translation Attack employs a Bangla→English→Bangla translation pipeline using csebuetnlp/banglat5_nmt_bn_en and csebuetnlp/banglat5_nmt_en_bn models. The One-Hot Word Swap Attack iteratively masks each token, measures prediction confidence drops, ranks tokens by importance, and uses xlm-roberta-base fill-mask pipeline to generate replacements. All attacks target models like ka05ar/banglabert-sentiment and Arunavaonly/Bangla_multiclass_sentiment_analysis_mode, evaluating success via F1 macro reduction and confidence score delta.

## Key Results
- One-Hot BERT Perturb achieved 37–40% F1 reduction, highest among all methods
- Attack success rate varied significantly between internal (22–40% F1 drop) and external datasets (18–37% F1 reduction)
- Back-translation successfully reduced model confidence in 12 of 15 test examples
- Paraphrase attacks maintained semantic similarity with BERTScore ranges of 0.91–0.93

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level structural perturbation via paraphrasing reduces model confidence by exploiting lexical-syntactic variance while preserving semantics.
- Mechanism: The attack uses `csebuetnlp/banglat5_banglaparaphrase` to generate context-aware paraphrases with high PINC scores (0.65–0.80) indicating lexical diversity. The altered sentence structure confuses models that rely on specific syntactic patterns learned during fine-tuning, even when semantic meaning is preserved.
- Core assumption: Models over-rely on surface-level syntactic patterns rather than deep semantic understanding for classification.
- Evidence anchors: [abstract] "Bangla Paraphrase Attack... to generate ambiguous, semantically similar but structurally different inputs"; [section 4.1] "When paraphrased, the structure of a sentence usually changes along with the lexical state"
- Break condition: If models are trained on diverse syntactic paraphrases during fine-tuning, structural perturbation alone may not reduce confidence significantly.

### Mechanism 2
- Claim: Back-translation introduces structural-lexical drift through intermediate language pivot, exploiting translation model alignment inconsistencies.
- Mechanism: Bangla→English→Bangla translation pipeline (`banglat5_nmt_en_bn` and `banglat5_nmt_bn_en`) creates semantically similar but structurally different outputs due to cross-lingual alignment differences. The intermediate English representation loses Bangla-specific syntactic features, producing perturbed reconstructions.
- Core assumption: Machine translation models do not preserve exact syntactic structure across round-trip translation, creating exploitable variance.
- Evidence anchors: [section 4.2] "Translation creates a semantic bridge between two languages; however, it often fails to maintain the same sentence structure due to the difference in alignment property"; [table 5] Example showing original "জয় জনগন ঠেলা সামলা" (Negative, 0.78) becoming "জয ় জনতা, ধাক কা সামলাও" (Positive, 0.51)
- Break condition: If translation quality is too high (minimal drift) or too low (semantic corruption), attack effectiveness degrades.

### Mechanism 3
- Claim: Token-level importance ranking combined with masked language model substitution identifies and replaces high-influence tokens to flip predictions.
- Mechanism: The One-Hot Word Swap Attack iteratively masks each token, measures prediction confidence drop, ranks tokens by importance, then uses `xlm-roberta-base` fill-mask pipeline to generate replacements. Replacing high-importance tokens with contextually plausible alternatives disrupts the model's decision boundary.
- Core assumption: Sentiment classifiers assign disproportionate weight to specific tokens, and masked LM replacements produce semantically similar but sentiment-altering substitutions.
- Evidence anchors: [section 4.3] "The masked language modeling capability of BERT provides us with the weapon to attack these LLMs. We identify these important aka. Biased tokens and replace them"; [table 2] One-Hot BERT Perturb achieved 37–40% F1 reduction, highest among all methods
- Break condition: If token importance is distributed evenly across sequence, or if masked LM replacements preserve sentiment, attack success rate drops.

## Foundational Learning

- Concept: **Adversarial robustness evaluation**
  - Why needed here: The paper fundamentally addresses model vulnerability assessment through systematic attack recipes; understanding robustness metrics (F1 macro, confidence delta) is prerequisite to interpreting results.
  - Quick check question: Can you explain why a 40% F1 reduction indicates model vulnerability rather than dataset noise?

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: The One-Hot Word Swap Attack relies entirely on understanding how `[MASK]` token prediction works in transformers like XLM-RoBERTa.
  - Quick check question: Given a masked sentence "The movie was [MASK]," what determines the probability distribution over replacement tokens?

- Concept: **Transfer learning in low-resource NLP**
  - Why needed here: The target models (`banglabert-sentiment`, `Bangla_multiclass_sentiment_analysis_mode`) are fine-tuned from pretrained multilingual models; understanding this architecture explains why attacks transfer.
  - Quick check question: Why might a model pretrained on 100 languages (XLM-RoBERTa) be more or less vulnerable to Bangla-specific adversarial attacks than a Bangla-only pretrained model?

## Architecture Onboarding

- Component map:
  - Attack pipeline: Three parallel attack modules (Paraphrase, Back-Translation, One-Hot Swap) → Unified evaluation framework → CSV report generation
  - Target models: `banglabert-sentiment` (fine-tuned from csebuetnlp/banglabert) and `Bangla_multiclass_sentiment_analysis_mode` (fine-tuned from xlm-roberta-base)
  - Supporting models: `banglat5_banglaparaphrase` (paraphrasing), `banglat5_nmt_en_bn` / `banglat5_nmt_bn_en` (translation), `xlm-roberta-base` (mask filling)

- Critical path:
  1. Load target sentiment classifier via HuggingFace pipeline
  2. Filter correctly classified samples (skip mispredictions)
  3. Apply augmentation (paraphrase/back-translate/token-swap)
  4. Re-evaluate on augmented text
  5. Record prediction flip (Pass), confidence drop (Fail with delta), or unchanged (Fail)

- Design tradeoffs:
  - One-Hot vs. sentence-level: One-Hot generates up to 10 adversaries per sample (higher coverage) but risks semantic drift; sentence-level attacks preserve semantics better but 1:1 output ratio
  - Internal vs. external datasets: Internal dataset attacks show higher perplexity induction (22–40% F1 drop) vs. external (18–37%), suggesting models are more vulnerable to in-domain perturbations
  - Assumption: The paper does not validate semantic preservation quantitatively; relies on manual inspection of examples

- Failure signatures:
  - Attack returns "Misprediction": Original model already incorrect; no attack applied
  - Attack returns "Fail" with negative delta: Augmentation increased model confidence (attack backfired)
  - Back-translation returns identical text: Translation model reconstructed original exactly (see Table 5, Example 6)
  - One-Hot generates semantically incoherent replacements: Masked LM produces contextually plausible but meaning-altering tokens

- First 3 experiments:
  1. Replicate One-Hot BERT Perturb on `banglabert-sentiment` with a 50-sample subset from `blp23` dataset; verify F1 drop magnitude and identify which token types (subjects, adjectives, negation markers) are highest-priority targets
  2. Test cross-model transfer: Generate adversaries using `xlm-roberta-base` mask-filling against `banglabert-sentiment`, then apply same adversaries to `Bangla_multiclass_sentiment_analysis_mode` to measure transfer success rate
  3. Implement adversarial training defense: Fine-tune `banglabert-sentiment` on generated adversaries (10% augmented data mixed with original); re-evaluate attack success rate to quantify robustness improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generated adversarial examples be successfully utilized for adversarial training to enhance the robustness of Bangla sentiment models?
- Basis in paper: [explicit] The introduction states the goal is to "train the models to make them more robust," and the Future Work section lists "Building resilience... in Bengali models."
- Why unresolved: The current study focuses exclusively on the attack phase and performance degradation; it does not implement or evaluate a defense mechanism or retraining loop.
- What evidence would resolve it: Empirical results from retraining the target models (e.g., banglabert-sentiment) using the generated adversarial examples and measuring their resistance to subsequent attacks.

### Open Question 2
- Question: How can semantic correctness be rigorously evaluated in Bangla adversarial attacks given the absence of usable dependency parse trees?
- Basis in paper: [explicit] The limitations section highlights the "Problematic assessment of semantic correctness after augmentation" and the "Absence of usable dependency parse tree for Bangla language."
- Why unresolved: The authors note a difficulty in assigning numerical weights to findings and ensuring the augmented text preserves meaning without structural analysis tools.
- What evidence would resolve it: The development of a Bangla-specific semantic evaluation metric or dependency parser that correlates highly with human judgment of semantic preservation in adversarial examples.

### Open Question 3
- Question: How can the One-Hot Word Swap attack be refined to prevent "delusional" or semantic-altering substitutions while maintaining high attack success rates?
- Basis in paper: [explicit] The Result Analysis notes that the One-Hot BERT Perturb "may sometimes be delusional and not always create a sound and non-inverting adversary."
- Why unresolved: The current methodology relies on xlm-roberta-base for unmasking, which may suggest tokens that change the fundamental meaning of the sentence rather than just confusing the model.
- What evidence would resolve it: A modified attack algorithm that filters unmasked candidates using a semantic similarity threshold or sentiment-consistency check before substitution.

## Limitations

- The attack algorithms rely on model-specific implementations (BasicTokenizer, sorting functions) not provided in the paper, creating significant reproduction barriers
- Semantic preservation assumption remains unverified quantitatively—no systematic validation of whether adversarial examples maintain intended meaning
- The paper's assertion that attacks reveal fundamental model vulnerabilities in "deep semantic understanding" overstates the evidence; attacks exploit syntactic and lexical variations rather than probing true semantic comprehension

## Confidence

- **High confidence**: The core observation that adversarial attacks can reduce Bangla sentiment classifier performance by 37-40% F1 is well-supported by empirical results across multiple datasets and models
- **Medium confidence**: The claim that "One-Hot Word Swap Attack works better than all other attacks" is supported within tested conditions, but lacks statistical significance testing and proper comparative analysis
- **Low confidence**: The paper's assertion that these attacks reveal fundamental model vulnerabilities in "deep semantic understanding" overstates the evidence—attacks exploit syntactic and lexical variations rather than probing true semantic comprehension

## Next Checks

1. **Semantic preservation validation**: Implement quantitative BERTScore evaluation for all generated adversarial examples, establishing correlation between semantic drift and attack success rate. Test whether attacks succeed primarily on semantically preserved examples (indicating surface-level model vulnerabilities) or whether semantic corruption contributes to failure.

2. **Transferability analysis**: Generate adversaries using the BanglaParaphrase model against one target (e.g., banglabert-sentiment), then test the same adversaries against the other target model (Bangla_multiclass_sentiment_analysis_mode). Measure transfer success rates to determine whether attacks exploit model-specific weaknesses or general linguistic vulnerabilities.

3. **Defense mechanism evaluation**: Implement adversarial training by fine-tuning the target models on a mixture of original and generated adversarial examples (10-20% augmentation). Re-run all three attacks to measure robustness improvement and determine whether simple data augmentation can mitigate these vulnerabilities.