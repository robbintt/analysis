---
ver: rpa2
title: 'AiReview: An Open Platform for Accelerating Systematic Reviews with LLMs'
arxiv_id: '2504.04193'
source_url: https://arxiv.org/abs/2504.04193
tags:
- screening
- systematic
- llms
- aireview
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AiReview is a web-based platform for LLM-assisted systematic review
  creation, addressing the challenge of time-consuming title and abstract screening
  in medical systematic reviews. It provides an extensible framework and user interface
  that allows medical researchers and librarians to directly leverage LLMs for screening
  tasks while maintaining transparency and control.
---

# AiReview: An Open Platform for Accelerating Systematic Reviews with LLMs

## Quick Facts
- arXiv ID: 2504.04193
- Source URL: https://arxiv.org/abs/2504.04193
- Authors: Xinyu Mao; Teerapong Leelanupab; Martin Potthast; Harrisen Scells; Guido Zuccon
- Reference count: 20
- Primary result: Open-source platform enabling LLM-assisted systematic review screening with configurable roles and transparent prompt engineering

## Executive Summary
AiReview is a web-based platform designed to accelerate the time-consuming title and abstract screening phase of medical systematic reviews through LLM assistance. The platform addresses a critical bottleneck in systematic review workflows by providing an extensible framework that allows medical researchers and librarians to leverage LLMs while maintaining transparency and control. It offers configurable interaction levels and multiple LLM roles (pre-reviewer, co-reviewer, post-reviewer) that balance automation efficiency against human judgment preservation.

## Method Summary
AiReview implements a multi-role LLM integration framework for systematic review screening. The platform provides configurable interaction levels (low vs. high) and three distinct LLM roles: pre-reviewer (automated scoring before human view), co-reviewer (live assistance), and post-reviewer (quality control after human decision). The system features open prompt engineering allowing users to modify system prompts and select different LLMs, with a backend architecture using Django, PostgreSQL, RabbitMQ, and Celery for asynchronous processing. The platform supports integration with various commercial and open-source LLMs while maintaining transparency in the screening process.

## Key Results
- First open-source tool bridging LLM-assisted screening methods with practical systematic review workflows
- Supports multiple LLM roles enabling both automated decision-making and live collaboration
- Provides customizable prompts and integration with various commercial and open-source LLMs
- Maintains transparency and control for medical researchers and librarians

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring LLM assistance into distinct temporal roles allows operators to balance screening efficiency against automation bias
- **Mechanism:** The platform decouples LLM inference from human decision-making by offering pre-reviewer, co-reviewer, and post-reviewer roles, allowing AI injection at points maximizing efficiency without overriding human judgment
- **Core assumption:** Anchoring bias is significantly stronger when LLM suggestions are presented before a human forms an independent opinion
- **Evidence anchors:** Explicitly categorizes roles by workflow position and interaction patterns; references "Pre-review to Peer review: Pitfalls of Automating Reviews"
- **Break condition:** If post-reviewer mode consistently fails to catch errors introduced by human fatigue

### Mechanism 2
- **Claim:** Configurable interaction levels serve as a "nudge" mechanism to mitigate the "LLM Effect" (unquestioning acceptance of AI output)
- **Mechanism:** Forcing users to click to reveal AI results in "Low" mode introduces micro-frictions intended to preserve human agency and critical evaluation
- **Core assumption:** Requiring a manual action to view AI suggestions reduces likelihood of "automation blindness"
- **Evidence anchors:** Differentiates low and high interaction by whether LLM response is automatically displayed or triggered by screeners; references "The LLM Effect: Are Humans Truly Using LLMs"
- **Break condition:** If high interaction mode significantly accelerates screening without statistically significant drop in accuracy

### Mechanism 3
- **Claim:** An open, transparent prompt engineering layer allows domain experts to align LLM behavior with specific inclusion criteria better than "black box" commercial tools
- **Mechanism:** The system exposes system prompts, task templates, and model selection directly in the UI, allowing domain experts to iteratively debug the LLM's understanding
- **Core assumption:** End users possess sufficient literacy to modify prompts and select models to improve performance on niche medical topics
- **Evidence anchors:** Shows 'Prompts' interface allowing users to edit objective, persona, and response format; references leveraging LLMs for title and abstract screening with few-shot learning
- **Break condition:** If users modify prompts to be too restrictive or ambiguous, causing LLM to hallucinate justifications or reject relevant studies

## Foundational Learning

- **Concept: PICO Framework (Population, Intervention, Comparison, Outcome)**
  - **Why needed here:** Standard heuristic used to structure inclusion criteria; essential for debugging prompt templates and interpreting LLM's reasoning
  - **Quick check question:** Can you map a clinical query like "Does aspirin reduce fever in adults?" to the four PICO elements?

- **Concept: Asynchronous Task Queuing (Celery/RabbitMQ)**
  - **Why needed here:** LLM API calls for thousands of abstracts are long-running processes; understanding backend Celery usage is vital for diagnosing bottlenecks
  - **Quick check question:** If "Pre-reviewer" screen freezes after uploading 1,000 abstracts, is it a database issue or worker/concurrency issue?

- **Concept: Anchoring Bias in Human-AI Interaction**
  - **Why needed here:** Core architectural philosophy built around mitigating this bias; essential for evaluating utility of interaction modes
  - **Quick check question:** Why might a "Pre-reviewer" role lead to different final decisions than a "Post-reviewer" role, even if LLM suggests same studies?

## Architecture Onboarding

- **Component map:** Frontend (Vue.js + Tailwind CSS) served by Nginx -> Backend (Django) exposing REST APIs + WebSockets -> Data Layer (PostgreSQL + RabbitMQ + Celery) -> External LLM APIs

- **Critical path:**
  1. **Ingest:** User uploads nbib files -> Parsed by Backend -> Stored in PostgreSQL
  2. **Config:** User defines Inclusion Criteria (PICO) -> Selects AI Role -> Edits System Prompts
  3. **Execution:** Pre-reviewer mode: Celery worker sends abstracts to LLM API -> Stores scores/rankings -> UI displays sorted list; Co-reviewer mode: User opens abstract -> UI sends context to LLM via WebSocket -> LLM streams response to side panel
  4. **Export:** User downloads screened nbib files

- **Design tradeoffs:** Openness vs. Safety (user control over prompts vs. curated model safety); Efficiency vs. Bias (pre-reviewer offers highest savings but highest anchoring bias risk)

- **Failure signatures:** HTTP Timeout on Bulk Ingest (Celery worker scaling issues); Token Limit Errors (context window exceeded); Hallucination Drift (LLM hallucinates exclusion criteria in co-reviewer chat)

- **First 3 experiments:**
  1. **"Co-pilot" Latency Test:** Run co-reviewer mode on 10 abstracts; measure WebSocket streaming latency
  2. **Prompt Sensitivity Analysis:** Run same 50 abstracts through pre-reviewer pipeline with two different prompts; compare recall@10
  3. **Bias Stress Test:** Have novice reviewer screen subset using high interaction, expert screen same subset using post-reviewer; compare disagreement rates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is primarily conceptual rather than empirical, relying on established literature rather than original user studies
- Claims about role-based LLM integration effectiveness not validated through controlled user studies
- Open prompt engineering approach assumes sufficient user literacy without usability studies confirming effectiveness

## Confidence

- **High confidence:** Platform architecture is technically sound and integration of LLM roles represents novel contribution
- **Medium confidence:** Theoretical framework for mitigating automation bias through interaction design is well-supported by literature
- **Low confidence:** Claims about effectiveness of open prompt engineering for domain-specific alignment lack supporting user studies

## Next Checks
1. Conduct randomized controlled trial comparing screening accuracy and efficiency across three LLM roles using same document set
2. Measure actual impact of "Low" vs "High" interaction modes on reviewer decision-making through eye-tracking or think-aloud protocols
3. Evaluate prompt modification usability by having medical librarians with varying technical backgrounds attempt to optimize prompts for different review protocols