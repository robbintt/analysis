---
ver: rpa2
title: 'InfoQ: Mixed-Precision Quantization via Global Information Flow'
arxiv_id: '2508.04753'
source_url: https://arxiv.org/abs/2508.04753
tags:
- layer
- sensitivity
- quantization
- layers
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoQ addresses the problem of mixed-precision quantization by
  proposing a global sensitivity metric based on information flow rather than local
  heuristics. The core method measures how quantization of each layer affects mutual
  information in downstream layers through a single forward pass, then uses these
  scores in an integer linear programming formulation to allocate bit-widths efficiently.
---

# InfoQ: Mixed-Precision Quantization via Global Information Flow

## Quick Facts
- arXiv ID: 2508.04753
- Source URL: https://arxiv.org/abs/2508.04753
- Reference count: 40
- InfoQ achieves 69.99% accuracy on ResNet18 under 23.04 G-BitOps constraint (smallest degradation of -0.61%)

## Executive Summary
InfoQ addresses the challenge of mixed-precision quantization (MPQ) by proposing a novel global sensitivity metric based on information flow rather than local heuristics. The method measures how quantization of each layer affects mutual information in downstream layers through a single forward pass, then uses these scores in an integer linear programming formulation to allocate bit-widths efficiently. Experiments show InfoQ achieves state-of-the-art accuracy on ImageNet: 69.99% on ResNet18 under 23.04 G-BitOps constraint (smallest degradation of -0.61%), 77.03% on ResNet50 with 12.2× weight compression (only -0.34% drop from baseline), and 69.83% on MobileNetV2 at 14.00× compression.

## Method Summary
InfoQ computes quantization sensitivity scores by measuring how single-layer quantization affects Sliced Mutual Information (SMI) at downstream observer layers. The method first selects observer layers based on correlation with accuracy degradation, then computes 8-bit baseline SMI values. For each (layer, bit-width) pair, it quantizes only that layer, runs a single forward pass, and measures ΔSMI at observers. Sensitivity scores are calculated as a normalized sum of |ΔSMI| across observers, weighted by bit-width. An integer linear programming solver then allocates optimal bit-widths under resource constraints. The final configuration is fine-tuned using Learned Step-Size Quantization (LSQ). The entire search phase requires only a small calibration dataset (4,800-6,000 samples) and no backpropagation.

## Key Results
- Achieves 69.99% accuracy on ResNet18 under 23.04 G-BitOps constraint with only -0.61% degradation from baseline
- Reaches 77.03% accuracy on ResNet50 with 12.2× weight compression and just -0.34% drop from baseline
- Attains 69.83% accuracy on MobileNetV2 at 14.00× compression
- Uses two orders of magnitude less data than competitors like LIMPQ while providing superior accuracy-efficiency trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Global Information Flow Sensitivity
Quantization sensitivity is a global property of error propagation, not a local property of weight distributions. InfoQ quantizes a single layer and measures the resulting degradation in Sliced Mutual Information (SMI) at downstream "observer" layers. By measuring ΔSMI at these layers, the method captures how quantization noise cascades through the network and destroys task-relevant information in later layers. This global approach outperforms local Hessian-based metrics by considering the entire information flow path.

### Mechanism 2: Dimensionality Reduction via Sliced Mutual Information (SMI)
High-dimensional mutual information is intractable for DNNs, so InfoQ uses SMI which projects activations onto random unit vectors, calculates MI between these 1D projections, and averages the result. This allows the model to gauge statistical dependency without computing massive joint probability density functions. The method mitigates the curse of dimensionality while preserving the statistical relationships critical for quantization sensitivity.

### Mechanism 3: Integer Linear Programming (ILP) for Bit Allocation
Optimal bit-width allocation is solved instantaneously as a constrained optimization problem using pre-computed sensitivity scores. Instead of iterative search, InfoQ converts the problem into minimizing total sensitivity subject to budget constraints. This standard ILP problem is solved efficiently using solvers like PuLP, decoupling sensitivity analysis from hardware constraints and making the final allocation step extremely fast.

## Foundational Learning

- **Concept: Mutual Information (MI) & The Information Bottleneck**
  - Why needed: InfoQ replaces Hessian-based loss-landscape metrics with information-theoretic metrics. You must understand that I(X; L) measures "how much of the input is preserved" and I(L; Y) measures "how much task-relevant info is preserved" to grasp why ΔSMI indicates sensitivity.
  - Quick check: If quantizing a layer reduces I(Lj; Y) significantly but leaves I(X; Lj) unchanged, what does this imply about the layer's role? (Hint: It retains input data but destroys the features needed for classification)

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed: InfoQ is "training-free" in the search phase (PTQ logic) but evaluates the final result after QAT (fine-tuning). Distinguishing these is vital to understand why the paper claims efficiency (PTQ search) but reports high accuracy (QAT results).
  - Quick check: Does InfoQ require backpropagation during the bit-width search phase?

- **Concept: Calibration Data**
  - Why needed: The method relies on a "small labeled calibration dataset" (e.g., 6,000 images) to estimate probability densities for SMI. The effectiveness of the sensitivity metric is conditional on this dataset representing the true data distribution.
  - Quick check: Why might the sensitivity scores fail if the calibration dataset consists only of images of cats, but the test set includes cars?

## Architecture Onboarding

- **Component map:** DINOv2 Encoder -> Observer Layers -> SMI Estimator -> ILP Solver
- **Critical path:**
  1. Observer Selection: Run correlation analysis (Algorithm 2) to identify layers where ΔSMI correlates >0.7 with accuracy drops. Do not skip this; arbitrary observer selection degrades performance.
  2. Baseline Profiling: Run forward pass with all layers at 8-bit to establish baseline SMI at observer layers.
  3. Sensitivity Sweep: For each layer ℓ and bit-width b, quantize ℓ, run forward pass, record ΔSMI at observers.

- **Design tradeoffs:**
  - Data Efficiency vs. Stability: Using fewer calibration samples speeds up the search but may increase variance in SMI estimation, potentially selecting suboptimal bits for outlier layers.
  - Observer Strictness: Selecting only the very last layer as an observer is fast but may miss damage done in early feature extractors; selecting too many observers introduces noise.

- **Failure signatures:**
  - Zero PTQ Accuracy: Likely caused by using "uncorrelated" layers as observers (correlation <0.5, see Appendix A, Figure 7).
  - High Variance in Bit-Selection: Indicates the calibration dataset is too small or the KSG estimator parameters are unstable for the activation distribution.
  - Violation of Budget: The ILP solver failed to converge or the cost function was defined incorrectly for the target hardware.

- **First 3 experiments:**
  1. Observer Validation: On a small subset of data, plot the Pearson correlation between ΔSMI at proposed observer layers and actual accuracy degradation for a few random bit-configs. Ensure ρ > 0.7.
  2. Ablation on Penalty Term: Run InfoQ with and without the 1/b penalty term (Figure 9) on ResNet18 to confirm if the information-theoretic score is doing the heavy lifting or if the bit-width prior is dominating.
  3. Budget Scaling: Generate bit-configs for 3 different model sizes using the same pre-computed sensitivity scores to verify the reusability of the ILP formulation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the global information-flow sensitivity metric generalize effectively to non-convolutional architectures such as Vision Transformers, Large Language Models, or attention-based networks? The method relies on selecting observer layers based on correlation with accuracy degradation—a procedure validated only on residual and depthwise-separable CNNs. Transformers exhibit different information propagation patterns that may require modified observer selection or SMI estimation strategies.

- **Open Question 2:** What is the theoretical relationship between the measured Sliced Mutual Information degradation and the final task performance, and can this relationship be formalized beyond empirical correlation? The paper establishes strong empirical correlations (r=0.90–0.95) but does not provide theoretical guarantees that ΔSMI bounds accuracy loss. A principled theoretical framework connecting information-theoretic perturbations to task loss would enable provable performance bounds.

- **Open Question 3:** Can InfoQ's information-flow framework be extended to jointly optimize mixed-precision quantization with structured pruning, leveraging the Information Bottleneck principle for both? Pruning removes channels entirely while quantization reduces precision. The joint optimization space is substantially larger, and the cascading information effects may interact non-linearly, requiring modified ILP formulations or multi-objective optimization.

## Limitations
- The method requires careful observer layer selection based on correlation with accuracy degradation; failure to meet the 0.7 correlation threshold can cause the method to fail
- The additivity assumption underlying the ILP formulation (that layer sensitivities are independent) is not rigorously tested and may break down in edge cases
- Critical implementation details like KSG estimator parameters and random projection counts for SMI estimation are not fully specified

## Confidence
- **High Confidence:** The ILP formulation and overall search efficiency claims are well-supported by the method description and experimental results
- **Medium Confidence:** The Sliced Mutual Information mechanism is theoretically sound, but practical stability of KSG estimation is not fully validated
- **Low Confidence:** The assumption that layer sensitivities are additive and independent for the ILP formulation is not rigorously tested

## Next Checks
1. Observer Correlation Validation: For a given architecture, systematically test observer selection by measuring Pearson correlation between ΔSMI and actual accuracy degradation across 10-20 random bit configurations. Verify that correlation exceeds 0.7 before proceeding with full search.

2. SMI Estimation Stability: Run the SMI estimation with different random projection seeds and varying numbers of projections (e.g., 100, 500, 1000). Measure variance in sensitivity scores and ensure stable bit-width selection across seeds.

3. ILP Additivity Assumption Test: Design an experiment where two layers with individually low sensitivity are quantized together, and measure whether the combined effect exceeds the sum of individual effects. This tests the fundamental assumption underlying the linear ILP formulation.