---
ver: rpa2
title: 'Unlocking the Black Box: Analysing the EU Artificial Intelligence Act''s Framework
  for Explainability in AI'
arxiv_id: '2502.14868'
source_url: https://arxiv.org/abs/2502.14868
tags:
- intelligence
- artificial
- european
- explainability
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The lack of explainability in AI systems presents a major obstacle
  to ensuring accountability, fairness, and effective regulation. The EU's proposed
  Artificial Intelligence Act aims to address this through requirements for "explainable
  AI" (XAI) techniques, but implementation faces challenges around complexity, scalability,
  and standardization.
---

# Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI

## Quick Facts
- arXiv ID: 2502.14868
- Source URL: https://arxiv.org/abs/2502.14868
- Reference count: 0
- Primary result: The EU AI Act relies on co-regulation via technical standards to operationalize explainability requirements for high-risk AI systems.

## Executive Summary
The EU's proposed Artificial Intelligence Act addresses the critical challenge of AI explainability through a risk-based framework that mandates transparency for high-risk systems. The Act delegates technical specifics to standardization bodies, creating a co-regulatory approach where legal requirements are operationalized through external standards. Implementation faces significant challenges around establishing objective evaluation criteria for explainability methods, balancing transparency with performance and IP protection, and ensuring consistent enforcement across member states through national Market Surveillance Authorities.

## Method Summary
This paper provides legal and policy analysis of the EU AI Act's framework for explainability requirements rather than technical experimentation. The analysis draws from the EU AI Act text, GDPR, and general XAI literature to examine how explainability requirements function within the regulatory architecture. The paper conceptually surveys XAI techniques (feature importance, counterfactual explanations, sensitivity analysis) without implementing or evaluating them quantitatively. Key challenges identified include the lack of objective evaluation criteria for XAI methods and the delegation of technical specifications to standardization bodies like CENELEC.

## Key Results
- The EU AI Act employs a risk-based approach, triggering explainability requirements only for high-risk AI systems
- Technical compliance is achieved through co-regulation, delegating specific metrics to external standardization bodies
- Explainability serves as a proxy for accountability, converting algorithmic processes into auditable legal evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The EU AI Act functions by attaching mandatory obligations to specific risk tiers rather than regulating technology uniformly.
- **Mechanism:** The framework classifies AI systems into categories (unacceptable, high, limited, minimal). "High-risk" systems trigger conditional explainability requirements, while lower-risk systems face fewer obligations. This proportionality balances safety with innovation.
- **Core assumption:** Regulators can objectively define and categorize "risk" in a rapidly evolving technological landscape.
- **Evidence anchors:** [Section 2] The text notes the Act "relies on a 'risk-based approach' that assigns different requirements and obligations to each risk category," where "high-risk" systems must meet specific obligations to enter the market. [Abstract] Mentions the Act aims to address risks through requirements for XAI techniques. [Corpus] Related papers support the general necessity of XAI in high-stakes fields like healthcare and legal systems, implicitly validating the focus on risk-based application.
- **Break condition:** If the definition of "high-risk" fails to capture novel AI harms, or if boundaries between risk categories become blurred by general-purpose AI, conditional obligations may fail to apply where most needed.

### Mechanism 2
- **Claim:** Technical compliance is achieved through "co-regulation," where legislative goals are operationalized by external technical standards.
- **Mechanism:** The EU AI Act sets the legal requirement for explainability but delegates specific technical metrics and methods to standardization bodies (e.g., CENELEC) via the New Legislative Framework. This allows technical definition of "explainability" to adapt without amending the law.
- **Core assumption:** Private or hybrid standardization bodies can keep pace with AI development and produce objective, consensus-based standards for subjective concepts like "explainability."
- **Evidence anchors:** [Section 5] States the "EU has opted for the method of co-regulation through standardisation based on the New Legislative Framework (NLF)," recognizing the role of bodies like CENELEC. [Section 5] Notes that "entrusting the creation of regulations to organizations operating under private legal frameworks... may be problematic at the level of judicial scrutiny."
- **Break condition:** If standardization bodies fail to agree on concrete XAI metrics, or if standards are captured by specific industry interests, the enforcement mechanism will lack a solid evidentiary basis.

### Mechanism 3
- **Claim:** Explainability acts as a proxy for accountability, converting algorithmic processes into auditable legal evidence.
- **Mechanism:** XAI techniques (e.g., feature importance, counterfactuals) translate opaque model behavior into human-comprehensible narratives. This enables Market Surveillance Authorities to audit decisions and individuals to exercise their right to an effective remedy.
- **Core assumption:** Current XAI methods (like SHAP or LIME) are sufficiently robust and accurate to serve as legal proof of fair/non-discriminatory behavior.
- **Evidence anchors:** [Section 3] Describes how XAI techniques range from "quantitative feature importance analysis to counterfactual explanations." [Section 1] Argues that without clear explanations, "authorities have no way to ensure compliance." [Corpus] Notes the "black box problem" undermines legitimacy in legal contexts; argumentation-based XAI is proposed as a bridge.
- **Break condition:** If XAI explanations are misleading, contradictory, or too complex for legal professionals to interpret, the mechanism fails to deliver actual accountability.

## Foundational Learning

- **Concept: The "Black Box" vs. Transparency vs. Explainability**
  - **Why needed here:** The paper distinguishes between mere transparency (access to data/code) and explainability (understanding the *logic* of the decision). Confusing these leads to compliance strategies that satisfy the letter of the law but fail its intent.
  - **Quick check question:** Can you explain why releasing the source code of a deep neural network does not necessarily satisfy the requirement for "explainability"?

- **Concept: Risk-Based Regulation**
  - **Why needed here:** The architecture of the EU AI Act is entirely built around this concept. One cannot implement or audit compliance without first correctly classifying the AI system's risk level.
  - **Quick check question:** Does a "risk-based approach" regulate the AI technology itself or the specific *use cases* of the technology?

- **Concept: The Trade-off between Performance and Interpretability**
  - **Why needed here:** Engineers often prioritize model accuracy. This paper explicitly highlights a legal/ethical tension where highly accurate models may be legally restricted if they cannot be made explainable.
  - **Quick check question:** In a credit scoring scenario, are you legally safer with a 99% accurate "black box" or a 90% accurate interpretable model under the proposed framework?

## Architecture Onboarding

- **Component map:** AI System (High-Risk) -> XAI Layer (Feature Importance, Counterfactuals) -> EU AI Act (Risk Classification) -> CENELEC/ISO (Technical Specs) -> Market Surveillance Authorities (MSAs) & potential EU AI Office

- **Critical path:**
  1. **Classification:** Determine if the system falls under "High-Risk" (Annex III).
  2. **Assessment:** Conduct ex ante conformity assessment (checking explainability and risk management).
  3. **Registration:** Register the system in the EU database.
  4. **Deployment:** Deploy with user-centric explanation interfaces.
  5. **Monitoring:** Subject to ex post surveillance by MSAs.

- **Design tradeoffs:**
  - **Accuracy vs. Compliance:** Simplifying models to increase interpretability may reduce predictive performance.
  - **IP vs. Transparency:** Detailed explanations may risk revealing trade secrets; compliance requires balancing disclosure with IP protection (Directive 2016/943).
  - **Complexity vs. Comprehensibility:** Highly accurate XAI visuals may overwhelm users (cognitive load), while simple explanations may hide crucial details.

- **Failure signatures:**
  - **"Post-hoc Justification":** Explanations that rationalize a decision without reflecting the true causal logic of the model (detected during auditing).
  - **Standardization Deadlock:** Inability to select a technical standard for XAI, leading to legal uncertainty.
  - **Cross-border Friction:** Different MSAs in different member states interpreting "explainability" differently for the same product.

- **First 3 experiments:**
  1. **Risk Tier Audit:** Map your organization's AI inventory against the EU AI Act's "High-Risk" criteria to identify which models require immediate XAI integration.
  2. **Explanation User Test:** Deploy a prototype XAI interface (e.g., counterfactual or feature importance plot) to non-expert users to measure cognitive load and trust.
  3. **Robustness Check:** Apply two different XAI methods (e.g., LIME and SHAP) to the same model output to see if they provide consistent explanations; significant divergence signals a compliance risk.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can objective evaluation criteria be established for diverse XAI methods?
- **Basis in paper:** [explicit] The paper states that a "major obstacle is the lack of agreement on how to perform an objective evaluation of XAI methods."
- **Why unresolved:** XAI methods range from quantitative feature analysis to counterfactual explanations, but no standardized metrics currently exist to assess their validity or utility across different contexts.
- **What evidence would resolve it:** The development of accepted technical standards or benchmarks that allow regulators to compare the effectiveness of different explanation algorithms.

### Open Question 2
- **Question:** Which specific technical explainability techniques will satisfy the EU AI Act's legal requirements in practice?
- **Basis in paper:** [explicit] The abstract notes that "exact XAI techniques and requirements are still to be determined and tested in practice."
- **Why unresolved:** The AI Act outlines principles for high-risk systems, but a precise mapping between legal obligations (e.g., transparency) and technical implementations (e.g., decision trees vs. SHAP) remains undefined.
- **What evidence would resolve it:** Case studies of conformity assessments or judicial rulings that validate specific technical methods as legally sufficient for compliance.

### Open Question 3
- **Question:** Is a supranational EU-level oversight body necessary to effectively enforce XAI standards?
- **Basis in paper:** [explicit] The author argues that "the need for a supranational oversight body at the EU level will inevitably emerge" due to the cross-border nature of AI systems.
- **Why unresolved:** The current framework relies on national Market Surveillance Authorities (MSAs), which may face difficulties coordinating enforcement for AI systems operating across multiple member states.
- **What evidence would resolve it:** The official establishment of a centralized "AI Office" with specific powers to coordinate cross-border investigations and standardize enforcement.

## Limitations
- The analysis is primarily theoretical/legal, not empirical; claims about XAI method effectiveness lack quantitative validation.
- Technical standardization for explainability is delegated to bodies like CENELEC but specific standards are not yet defined.
- The paper acknowledges the lack of objective evaluation criteria for XAI methods, making legal compliance verification difficult.

## Confidence
- **High confidence**: The risk-based regulatory framework described (unacceptable, high, limited, minimal risk tiers) is directly derived from the EU AI Act text and legislative framework.
- **Medium confidence**: The mechanism of co-regulation through technical standardization is well-established in EU law but its practical application to rapidly evolving AI technology remains unproven.
- **Low confidence**: Claims about specific XAI method performance in legal contexts are speculative without empirical testing or established evaluation criteria.

## Next Checks
1. **Standardization Tracking**: Monitor CENELEC/ISO for published XAI technical standards and assess alignment with the AI Act's explainability requirements.
2. **Interoperability Testing**: Compare outputs of multiple XAI methods (SHAP, LIME, counterfactuals) on identical models to identify consistency issues that could undermine legal admissibility.
3. **MSA Training Gap Analysis**: Survey national Market Surveillance Authorities to identify knowledge gaps in evaluating XAI compliance across member states.