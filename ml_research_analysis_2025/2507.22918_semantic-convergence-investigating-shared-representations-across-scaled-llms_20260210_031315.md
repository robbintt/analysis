---
ver: rpa2
title: 'Semantic Convergence: Investigating Shared Representations Across Scaled LLMs'
arxiv_id: '2507.22918'
source_url: https://arxiv.org/abs/2507.22918
tags:
- layers
- svcca
- features
- feature
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines whether two language models of different sizes
  (Gemma-2-2B and Gemma-2-9B) develop shared internal representations despite their
  four-fold scale difference. Using sparse autoencoders to decompose activations into
  interpretable features, the authors aligned features across models via correlation
  and compared their spaces using SVCCA and RSA metrics.
---

# Semantic Convergence: Investigating Shared Representations Across Scaled LLMs

## Quick Facts
- **arXiv ID**: 2507.22918
- **Source URL**: https://arxiv.org/abs/2507.22918
- **Reference count**: 14
- **Primary result**: Gemma-2-2B and Gemma-2-9B show strongest representational alignment in middle layers (SVCCA ~0.73, RSA ~0.20-0.22), with coherent semantic subspaces aligning more strongly than incoherent ones.

## Executive Summary
This study investigates whether language models of different scales develop shared internal representations by comparing feature spaces derived from Sparse Autoencoders (SAEs) applied to Gemma-2-2B and Gemma-2-9B. Using correlation-based feature alignment and rotation-invariant similarity metrics (SVCCA, RSA), the authors find statistically significant alignment across non-input layers, with peak similarity occurring in middle layers. The results support the hypothesis of feature universality across model scales, particularly for high-level semantic abstractions.

## Method Summary
The authors applied pretrained Sparse Autoencoders to residual stream activations of both Gemma-2-2B and Gemma-2-9B models, then aligned monosemantic features using activation correlation (both 1-to-1 and many-to-1 matching). They computed SVCCA and RSA scores to quantify feature space similarity across layers, and validated results against random pairing baselines. The analysis included single-token subspaces, overlapping multi-token subspaces, and manually curated semantic concept groups to test both universal feature alignment and semantic subspace consistency.

## Key Results
- Middle layers show strongest alignment (SVCCA ~0.73 peak, ~0.64-0.71 across contiguous pairs)
- Early and late layers exhibit significantly lower similarity (SVCCA ~0.35 ± 0.05)
- Coherent semantic subspaces align much more strongly than incoherent ones (SVCCA ~0.62 vs ~0.03)
- Statistical significance confirmed against random baselines (p < 0.001)
- Multi-token subspaces suggest early layers encode phrases more strongly than individual tokens

## Why This Works (Mechanism)

### Mechanism 1: Residual Stream Feature Universality via SAE Decomposition
Sparse Autoencoders decompose polysemantic residual stream activations into sparse, interpretable features. When paired via activation correlation, rotation-invariant metrics reveal statistically significant alignment across non-input layers, suggesting both models converge on similar internal representations despite scale differences. This assumes SAEs successfully isolate comparable monosemantic features in both models.

### Mechanism 2: Mid-Layer Semantic Convergence
Middle layers process high-level semantic abstractions that are more invariant to model size, leading to peak SVCCA scores (~0.73) and RSA scores (~0.22). Early layers process lower-level features (syntax, tokens) which may differ due to scale-dependent processing, while late layers are more task/output specialized.

### Mechanism 3: Semantic Subspace Alignment
Models align more strongly on semantically coherent concept groups than incoherent ones, with SVCCA scores drastically higher for related concepts (e.g., 0.62 for "Emotion and Time" vs 0.03 for "Country and People"). This suggests models use similar internal geometries for related concepts, and early layers may encode multi-token phrases more strongly than individual tokens.

## Foundational Learning

**Concept: Sparse Autoencoders (SAEs)**
- Why needed here: SAEs are the core tool for decomposing polysemantic activations into interpretable, monosemantic features. Understanding this decomposition is essential for interpreting alignment results.
- Quick check question: What is the primary output of an SAE that makes it useful for comparing representations across different models?

**Concept: Residual Stream**
- Why needed here: The paper specifically analyzes residual stream activations. Knowing this is the central information pathway where layer outputs accumulate is crucial for understanding the experimental setup.
- Quick check question: On which component of the Transformer architecture did the authors apply SAEs to extract features for comparison?

**Concept: SVCCA (Singular Value Canonical Correlation Analysis)**
- Why needed here: SVCCA is the primary metric for quantifying feature space similarity. Grasping its role as a rotation-invariant method for comparing subspaces is key to interpreting alignment scores.
- Quick check question: Why is a rotation-invariant metric like SVCCA necessary when comparing the feature spaces of two independently trained models?

## Architecture Onboarding

**Component map**: Input tokens → Gemma-2-2B/9B Layers (Residual Stream) → Pretrained SAEs (per layer) → Monosemantic Features → Correlation-based Pairing → SVCCA/RSA Scores

**Critical path**: Residual stream activations → SAE decomposition → Activation correlation pairing (1-to-1 or many-to-1) → Rotation-invariant similarity scoring (SVCCA/RSA) → Statistical validation against random baselines

**Design tradeoffs**:
- **1-to-1 vs. many-to-1 matching**: 1-to-1 yields higher peak scores (~0.73) but may miss feature splitting in larger models. Many-to-1 (~0.69 peak) tests if the smaller model's dictionary embeds in the larger one.
- **Layer selection**: Middle layers provide the strongest signal (~0.64–0.71); early/late layers are noisier (~0.35).
- **Semantic subspace curation**: Manual curation (GPT-4o + WordNet) allows targeted concept testing but may not reflect all internal categories.

**Failure signatures**:
- Random baseline SVCCA scores (~0.005–0.034) are near zero; any non-input layer score above ~0.30 is statistically significant (p < 0.05).
- Low SVCCA for coherent concept pairs (e.g., < 0.10) would indicate failed subspace alignment.
- Incoherent concept pairs (e.g., "Country and People" at 0.03) serve as a negative control.

**First 3 experiments**:
1. **Reproduce layer-wise SVCCA**: Run 1-to-1 feature pairing between Gemma-2-2B L14 and Gemma-2-9B L19; expect SVCCA ≈ 0.73 (p < 0.05).
2. **Validate random baseline**: Perform 5-run random feature pairing; expect mean SVCCA < 0.034 across all layer pairs.
3. **Test semantic subspace alignment**: Select a coherent concept pair (e.g., "Nature and People") and an incoherent pair (e.g., "Country and People"); expect SVCCA ~0.60+ vs. ~0.03, respectively.

## Open Questions the Paper Calls Out

**Open Question 1**: Do internal representations in MLP layers exhibit the same degree of universality across scaled models as the residual stream layers? The current study restricted analysis to residual stream; applying the same methodology to MLP activations could reveal whether associative memory processing also converges.

**Open Question 2**: Is the strong encoding of multi-token phrases in early layers a general phenomenon, or is it specific to the "emotions-time" subspace tested? With only one subspace tested, further validation across diverse semantic categories is needed to determine if this is a universal structural property.

**Open Question 3**: Can training Sparse Autoencoders on multiple layers simultaneously reveal universal features that are missed by single-layer analysis? Features may not be strictly localized; cross-layer SAEs might yield higher alignment scores or uncover distinct semantic features.

**Open Question 4**: Does feature universality persist across models with different tokenizers and architectures? The focus on Gemma-2 models with the same tokenizer leaves open whether observed convergence (SVCCA ~0.73) is family-specific or more fundamental to language modeling.

## Limitations

- SAEs trained independently on each model raise questions about dictionary comparability across scales
- Semantic subspace curation via GPT-4o and WordNet introduces potential subjectivity in defining coherent vs. incoherent concept pairs
- Activation correlation-based alignment assumes linear correspondences between features, which may not capture complex feature-splitting behaviors in larger models

## Confidence

**High Confidence**: Statistical significance of SVCCA/RSA scores against random baselines (p < 0.05) and consistent mid-layer convergence pattern (~0.64-0.73 SVCCA) across multiple experiments.

**Medium Confidence**: Semantic subspace alignment results, given manual curation process and limited direct corpus support for cross-model semantic consistency.

**Low Confidence**: Multi-token phrase encoding claims in early layers, as evidence is preliminary and requires further validation.

## Next Checks

1. **Cross-architectural validation**: Apply the same methodology to compare Gemma-2 with a non-Gemma architecture (e.g., Llama) to test if semantic convergence is family-specific or more universal.

2. **Feature granularity analysis**: Investigate whether larger model features split into more granular components and whether many-to-1 matching captures these relationships better than 1-to-1.

3. **Temporal stability test**: Track semantic alignment evolution during training by analyzing checkpoints at different training stages to determine when convergence patterns emerge.