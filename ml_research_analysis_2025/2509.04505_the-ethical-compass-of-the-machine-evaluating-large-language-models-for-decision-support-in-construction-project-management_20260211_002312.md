---
ver: rpa2
title: 'The Ethical Compass of the Machine: Evaluating Large Language Models for Decision
  Support in Construction Project Management'
arxiv_id: '2509.04505'
source_url: https://arxiv.org/abs/2509.04505
tags:
- ethical
- construction
- project
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study assessed how well large language models can support ethical
  decision-making in construction project management. Three leading LLMs (ChatGPT,
  Gemini, LLaMA) were evaluated using twelve realistic ethical scenarios scored on
  seven dimensions via a novel Ethical Decision Support Assessment Checklist (EDSAC).
---

# The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management

## Quick Facts
- **arXiv ID**: 2509.04505
- **Source URL**: https://arxiv.org/abs/2509.04505
- **Reference count**: 26
- **Primary result**: ChatGPT and Gemini outperformed LLaMA on ethical reasoning in construction scenarios, but all models struggled with contextual relevance and explainability, necessitating human oversight.

## Executive Summary
This study evaluates the capability of large language models to support ethical decision-making in construction project management using a novel Ethical Decision Support Assessment Checklist (EDSAC). Testing ChatGPT, Gemini, and LLaMA against twelve realistic ethical scenarios revealed that while ChatGPT (mean 4.35) and Gemini (4.21) outperformed LLaMA (3.92), all models demonstrated significant weaknesses in contextual relevance, explainability, and bias awareness. Expert interviews with twelve industry professionals confirmed reservations about AI autonomy and emphasized the critical need for human oversight. The research concludes that LLMs are best deployed as decision-support aids rather than autonomous ethical agents, recommending mandatory human-in-the-loop systems and professional training for responsible integration.

## Method Summary
The study employed the EDSAC framework to evaluate three LLMs (ChatGPT, Gemini, LLaMA) across twelve domain-specific ethical scenarios in construction project management. Each scenario was presented to the models three times, with responses scored independently on seven dimensions using a 5-point Likert scale. Twelve industry experts participated in semi-structured interviews to validate findings and provide contextual insights. Scores were calculated as means across dimensions, with thematic analysis applied to interview data following Braun & Clarke (2006) methodology.

## Key Results
- ChatGPT achieved the highest mean EDSAC score (4.35), followed by Gemini (4.21) and LLaMA (3.92)
- All models struggled with contextual relevance, explainability, and bias awareness despite strong performance in ethical soundness
- Expert interviews revealed strong preference for human oversight, with professionals unwilling to delegate final decisions to AI systems

## Why This Works (Mechanism)

### Mechanism 1: RLHF-Driven Ethical Alignment Variability
LLM performance varies based on reinforcement learning from human feedback (RLHF) alignment quality. Models with explicit human feedback alignment develop better structured reasoning and justifiability compared to those with less intensive ethical fine-tuning. This produces measurable performance differences in transparency and accountability dimensions.

### Mechanism 2: Contextual Grounding Deficit in General-Purpose Training
LLMs trained on broad internet data lack domain-specific contextual judgment required for high-stakes professional ethical decisions. Generic training produces pattern-matched responses that fail to account for industry regulations, competing stakeholder pressures, and professional norms.

### Mechanism 3: Socio-Technical Trust Calibration via Explainability
Professional trust in AI decision support is conditional on explainability and clear accountability structures, not merely accuracy. Human experts calibrate trust based on transparency of reasoning chains and ability to maintain professional accountability, producing "co-pilot" adoption patterns.

## Foundational Learning

**Concept: EDSAC Framework (Ethical Decision Support Assessment Checklist)**
- Why needed here: Provides structured, reproducible methodology for evaluating LLM ethical performance across 7 dimensions, enabling systematic comparison rather than anecdotal assessment.
- Quick check question: Can you name 4 of 7 EDSAC dimensions and explain what distinguishes a score of 3 from a score of 5?

**Concept: Human-in-the-Loop (HITL) Architecture**
- Why needed here: Essential design pattern for high-stakes AI deployment where human professionals retain final decision authority; distinguishes "co-pilot" from autonomous agent paradigms.
- Quick check question: In the "co-pilot" model, who holds accountability when an AI recommendation contributes to a negative outcomeâ€”and why does this matter legally?

**Concept: Value Pluralism in Professional Ethics**
- Why needed here: Construction decisions involve competing values (safety vs. cost vs. timeline vs. reputation) that cannot be optimized simultaneously; LLMs struggle with this tradeoff reasoning.
- Quick check question: Why might an LLM give legally compliant advice that still fails contextual relevance in a construction scenario?

## Architecture Onboarding

**Component map:**
LLM Evaluation Layer -> Scenario Library -> Human Oversight Layer -> Decision Flow

**Critical path:**
1. Define/validate ethical scenarios with domain experts
2. Generate LLM responses with consistent prompting
3. Independent multi-rater EDSAC scoring with consensus resolution
4. Human expert review for contextual validity and practical actionability
5. Final decision by qualified professional with documented justification

**Design tradeoffs:**
- Accuracy vs. Explainability: Higher-scoring models may still lack transparency
- Breadth vs. Depth: Generic LLMs offer accessibility but lack regulatory specificity
- Automation vs. Liability: Increased automation reduces time but shifts accountability ambiguity

**Failure signatures:**
- Transparency/Explainability scores <3.5 indicate accountability vacuum risk
- Responses lacking specific regulatory references or regional codes
- No acknowledgment of uncertainty, bias risk, or conflicting stakeholder interests
- Missing escalation guidance when LLM contradicts senior human expertise

**First 3 experiments:**
1. **Baseline EDSAC benchmark**: Run 3+ LLMs through 12 domain scenarios; score all 7 dimensions; identify systematic weak dimensions across models
2. **Explainability-trust correlation**: Measure whether higher EDSAC transparency scores predict human expert willingness-to-adopt ratings in follow-up surveys
3. **Context injection test**: Compare generic LLM responses vs. RAG-augmented responses with construction regulations/case law; measure Contextual Relevance improvement (target: >0.5 point gain)

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning LLMs on domain-specific construction datasets significantly improve their scores in Contextual Relevance and Legal Compliance compared to base models?
- Basis in paper: The discussion states, "Future work should also explore the efficacy of fine-tuning LLMs on domain-specific datasets, such as contractual documents and case law, to see if their contextual and legal reasoning can be improved."
- Why unresolved: The current study evaluated general-purpose models against static scenarios, finding they often lacked "contextual sagacity" and failed to cite specific regulations.
- What evidence would resolve it: A comparative experiment measuring EDSAC scores between base models and models fine-tuned on construction law and ethical case studies.

### Open Question 2
How does human-AI decision-making dynamics and trust evolve over the full lifecycle of a live construction project?
- Basis in paper: The authors explicitly note that "Longitudinal studies are needed to observe how human-AI decision-making evolves over the lifecycle of a real project."
- Why unresolved: The present research utilized a snapshot approach via static scenarios and one-off interviews, which cannot capture changing attitudes or reliance over time.
- What evidence would resolve it: A longitudinal field study tracking professional trust levels and intervention rates during the actual execution of a construction project.

### Open Question 3
Can Retrieval-Augmented Generation (RAG) mitigate the "generic advice" limitation by anchoring LLM responses in specific local regulations?
- Basis in paper: The results highlighted that models "rarely referenced specific UK regulations," yet the methodology did not test mechanisms for grounding responses in external, up-to-date legal documents.
- Why unresolved: It remains unclear if the failure to provide specific legal context is a fundamental reasoning flaw or simply a lack of access to relevant regulatory data.
- What evidence would resolve it: Testing LLMs equipped with RAG pipelines against the EDSAC framework to see if citation accuracy and specificity scores improve.

## Limitations
- Study relies on simulated scenarios rather than real-world decision-making data, potentially missing full complexity of on-site ethical dilemmas
- EDSAC framework, while novel, has not been externally validated against industry standards or real incident outcomes
- Expert interview sample (n=12) provides depth but limited generalizability across the global construction industry

## Confidence
- **High Confidence**: Relative performance rankings between models (ChatGPT > Gemini > LLaMA) and general pattern of LLM struggles with contextual relevance and explainability are robust
- **Medium Confidence**: EDSAC dimension scores are internally consistent but depend on the assumption that Likert-scale scoring captures nuanced ethical reasoning quality
- **Low Confidence**: Direct claims about RLHF mechanism effects on ethical alignment are inferential, as the study did not control for specific training methodologies

## Next Checks
1. **Real-World Correlation Study**: Track actual construction project decisions where LLMs were consulted (with human oversight) and compare outcomes to EDSAC predictions of model performance
2. **EDSAC External Validation**: Test the framework with domain experts from other high-stakes fields (healthcare, aviation) to assess cross-domain applicability and refine scoring thresholds
3. **Mechanism Isolation Experiment**: Compare performance of base models, RLHF-aligned versions, and domain-fine-tuned versions on identical scenarios to quantify the relative contributions of each approach to ethical reasoning quality