---
ver: rpa2
title: The Role of Exploration Modules in Small Language Models for Knowledge Graph
  Question Answering
arxiv_id: '2509.07399'
source_url: https://arxiv.org/abs/2509.07399
tags:
- language
- slms
- knowledge
- exploration
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how small language models (SLMs) perform
  on knowledge graph question answering (KGQA) compared to large language models (LLMs).
  While LLMs significantly benefit from the Think-on-Graph framework for KGQA, SLMs
  do not see similar improvements and can even perform worse than the baseline Chain-of-Thought
  method.
---

# The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2509.07399
- Source URL: https://arxiv.org/abs/2509.07399
- Reference count: 13
- SLMs benefit from retrieval-based exploration modules for KGQA, while LLMs do not see similar improvements.

## Executive Summary
This paper investigates how small language models (SLMs) perform on knowledge graph question answering (KGQA) compared to large language models (LLMs). While LLMs significantly benefit from the Think-on-Graph framework for KGQA, SLMs do not see similar improvements and can even perform worse than the baseline Chain-of-Thought method. The authors identify that SLMs struggle with exploration—effectively traversing and reasoning over knowledge graphs—which is critical for accurate answers. To address this, they propose using lightweight passage retrieval models (like SentenceBERT and GTR) to handle knowledge graph traversal instead of relying on the SLM itself. Experiments show that this approach significantly improves SLM performance on KGQA tasks, validating the effectiveness of decoupling exploration from the language model. The study highlights a practical solution for making KGQA more accessible and efficient for resource-constrained settings.

## Method Summary
The paper adapts the Think-on-Graph (ToG) framework for KGQA to use retrieval-based exploration modules instead of SLM-guided exploration. In the ToG framework's exploration stage, candidate relations and entities are ranked using retrieval models (BM25, SentenceBERT, or GTR) based on semantic similarity to the question. The top-k candidates are selected for path continuation via beam search. The SLM then performs reasoning only over these pre-filtered candidate paths to generate the final answer. This approach decouples exploration from reasoning, allowing SLMs to focus on their strengths while leveraging specialized retrieval models for path selection.

## Key Results
- SLMs struggle with the exploration stage of ToG, showing no improvement over Chain-of-Thought baseline (e.g., Mean SLM with ToG achieves 0.217 EM vs. 0.219 with CoT on CWQ)
- GPT-4.1-assisted exploration improves Qwen2-0.5b from 0.129 to 0.301 EM on CWQ, a 133% relative improvement
- GTR improves Phi-3-mini-3.8b from 0.270 (ToG baseline) to 0.291 EM on CWQ
- SentenceBERT improves Gemma2-2b from 0.382 to 0.520 on WebQSP

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Reasoning Decoupling
Separating exploration from reasoning allows each component to operate within its competency boundary. By replacing the SLM in exploration with a retrieval model, the SLM only performs reasoning over pre-filtered paths rather than attempting both path selection and inference. Evidence shows reasoning is not the bottleneck: with GPT-4.1-assisted exploration, Qwen2-0.5b improved from 0.129 to 0.301 EM on CWQ.

### Mechanism 2: Semantic Similarity as a Proxy for Relevance Ranking
Dense retrieval models (SentenceBERT, GTR) effectively rank KG relation-entity candidates by semantic similarity to the question, outperforming SLM-based candidate pruning. At each exploration step, retrieval models encode both the question and candidate passages, computing relevance via dot product. GTR improved Phi-3-mini-3.8b from 0.270 to 0.291 EM on CWQ.

### Mechanism 3: Model-Capacity Threshold for Zero-Shot KG Navigation
Effective zero-shot KG exploration requires minimum model capacity; below this threshold, SLMs cannot reliably identify relevant paths without external assistance. Larger models develop better implicit representations of relational reasoning through pretraining. Alignment between SLM exploration outputs and GPT-4.1 outputs increases consistently with model size.

## Foundational Learning

- **Knowledge Graphs (triplet structure: head entity, relation, tail entity)**
  - Why needed here: The ToG framework traverses KGs by following relation paths; understanding that KGs store facts as structured triplets is essential for interpreting exploration outputs.
  - Quick check question: Given the triplet (Northern District, country, Israel), what entity would you reach by following the "country" relation from "Northern District"?

- **Beam Search in Graph Traversal**
  - Why needed here: ToG uses beam search to maintain top-k reasoning paths during exploration; retrieval models score and prune candidates at each step.
  - Quick check question: If beam width is 3 and there are 10 candidate next-step relations, how many paths are retained after pruning?

- **Dense Retrieval and Embedding Similarity**
  - Why needed here: GTR and SentenceBERT encode text into dense vectors; relevance is computed as dot product similarity. Understanding this is critical for debugging exploration failures.
  - Quick check question: If a question embedding has dot product 0.85 with relation A and 0.72 with relation B, which relation is ranked higher?

## Architecture Onboarding

- **Component map:**
  ```
  Question
      │
      ▼
  [Initialization] → Topic Entity Extraction (SLM or rule-based)
      │
      ▼
  [Exploration] → Retrieval Module (BM25 / SentenceBERT / GTR)
      │             - Scores candidate relations/entities vs. question
      │             - Retains top-k paths via beam search
      ▼
  [Reasoning] → SLM generates final answer from retrieved paths
      │
      ▼
  Answer
  ```

- **Critical path:**
  1. Extract topic entities from the question (initialization)
  2. At each exploration step, retrieve candidate relations/entities from the KG
  3. Retrieval module ranks candidates by semantic similarity; top-k retained
  4. Repeat exploration for required hop depth (1-4 hops depending on dataset)
  5. SLM reasons over accumulated paths to generate answer

- **Design tradeoffs:**
  - BM25 vs. Dense Retrievers: BM25 is faster but lexical; GTR/SentenceBERT capture semantics but require encoding overhead
  - Beam width (k): Higher k retains more paths but increases compute; paper uses k=3 (implied by prompt design in Table 6)
  - SLM size for reasoning: Larger SLMs (7B-8B) benefit less from retrieval modules than smaller SLMs (0.5B-2B)

- **Failure signatures:**
  - Low EM despite correct topic entity: Likely exploration failure—retrieval model not ranking relevant candidates high enough
  - High cross-entropy with GPT-4.1 exploration outputs: Indicates SLM exploration deviates significantly from expected path selection
  - Retrieval module returns no relevant candidates: Check embedding quality or KG coverage for the domain
  - SLM produces malformed JSON outputs: Apply constrained decoding (Appendix B) to enforce parseable format

- **First 3 experiments:**
  1. Reproduce the SLM ToG vs. CoT baseline comparison (Table 1) to confirm exploration is the bottleneck for your target SLM
  2. Run the GPT-4.1-assisted exploration ablation (Table 3) to establish an upper bound on SLM reasoning performance with ideal exploration
  3. Compare retrieval modules (BM25, SentenceBERT, GTR) on your target dataset, starting with GTR as it showed the strongest results in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does random seed variance affect the statistical significance of the reported improvements for small language models using retrieval-based exploration?
- Basis in paper: The authors explicitly state in the Limitations section that they report results based on a single run without multiple random seeds due to computational constraints.
- Why unresolved: Without variance metrics (e.g., standard deviation), it is difficult to confirm if the performance gains of GTR over the CoT baseline are robust or subject to random initialization noise.
- What evidence would resolve it: Reporting mean and standard deviation scores across multiple runs (e.g., 3-5 seeds) for the main benchmarks (CWQ and WebQSP).

### Open Question 2
- Question: Can the performance of the exploration module be further improved by fine-tuning the retrieval models on the specific knowledge graph structure?
- Basis in paper: The paper utilizes off-the-shelf passage retrieval models (BM25, SentenceBERT, GTR) in a "zero-shot, plug-and-play manner" (Section 2.2) without task-specific training.
- Why unresolved: While zero-shot methods are accessible, the authors do not explore whether adapting the retrieval models to the specific linguistic patterns of the Knowledge Graph (Freebase) could narrow the remaining performance gap with Large Language Models.
- What evidence would resolve it: Experiments comparing the zero-shot GTR performance against a GTR model fine-tuned on KG relation-path datasets.

### Open Question 3
- Question: Does the intrinsic reasoning capability of the Small Language Model remain a bottleneck when using high-quality exploration modules?
- Basis in paper: Table 3 shows that even with GPT-4.1 providing context (optimal exploration), the Llama-3-8b model (0.451 EM) still underperforms GPT-4.1 (0.540 EM).
- Why unresolved: While the paper identifies exploration as a key bottleneck, it leaves open the question of whether the SLM's reasoning ability becomes the new limiting factor once exploration is optimized.
- What evidence would resolve it: Analyzing error rates specifically on "Reasoning" steps versus "Exploration" steps in the optimized pipeline, or comparing SLM reasoning performance on gold-standard paths.

## Limitations
- The key finding that retrieval-based exploration improves SLM performance but not LLM performance needs independent replication
- The assumption that GPT-4.1's exploration outputs represent optimal path selection is not validated against ground truth KG traversal paths
- The paper does not report beam width, top-k values, or max hop depth parameters used in experiments, making exact reproduction difficult

## Confidence
- Medium - The proposed retrieval-based exploration approach shows consistent improvements across multiple SLMs and datasets, but the underlying mechanism explaining why SLMs specifically benefit while LLMs do not requires further investigation

## Next Checks
1. Replicate the core finding by running the same retrieval-based exploration (BM25, SentenceBERT, GTR) on Gemma2-2B with CWQ and WebQSP, comparing against CoT baseline and ToG baseline
2. Conduct an ablation study varying only model size (keeping retrieval module fixed) to confirm the model-capacity threshold hypothesis for effective zero-shot KG navigation
3. Validate that GPT-4.1's exploration outputs align with ground truth paths by manually inspecting a sample of 20 exploration traces and computing accuracy against known optimal paths