---
ver: rpa2
title: A Multi-Stage Augmented Multimodal Interaction Network for Quantifying Fish
  Feeding Intensity Using Feeding Image, Audio and Water Wave
arxiv_id: '2506.14170'
source_url: https://arxiv.org/abs/2506.14170
tags:
- feeding
- fish
- fusion
- intensity
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a Multi-stage Augmented Multimodal Interaction
  Network (MAINet) for quantifying fish feeding intensity in recirculating aquaculture
  systems using synchronized image, audio, and water wave data. The approach combines
  a unified UniRepLKNet feature extractor with an Auxiliary-modality Reinforcement
  Primary-modality Mechanism (ARPM) for cross-modal interaction, and an Evidence Reasoning
  (ER)-based decision fusion strategy to improve quantification accuracy.
---

# A Multi-Stage Augmented Multimodal Interaction Network for Quantifying Fish Feeding Intensity Using Feeding Image, Audio and Water Wave

## Quick Facts
- **arXiv ID:** 2506.14170
- **Source URL:** https://arxiv.org/abs/2506.14170
- **Reference count:** 10
- **Primary result:** MAINet achieves 96.76% accuracy on multimodal fish feeding intensity classification

## Executive Summary
This paper introduces MAINet, a novel architecture for quantifying fish feeding intensity in recirculating aquaculture systems using synchronized image, audio, and water wave data. The approach combines a unified UniRepLKNet feature extractor with an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) for cross-modal interaction, and an Evidence Reasoning (ER)-based decision fusion strategy to improve quantification accuracy. The model demonstrates significant performance improvements over both single-modality and existing multimodal fusion approaches on a newly constructed dataset of 7,089 samples.

## Method Summary
MAINet processes synchronized image, audio (converted to Mel-spectrograms), and water wave data through a unified UniRepLKNet backbone with large 13x13 kernels. The ARPM module performs two-stage cross-modal interaction, first reinforcing auxiliary modalities using the primary modality via cross-attention and channel attention, then fusing results symmetrically. The ER layer handles decision fusion by modeling reliability and conflict between modalities. The model is trained using Adam optimizer (LR=0.001) with batch size 32 for 100 epochs, with learning rate halved if validation performance stalls for 5 epochs.

## Key Results
- Achieves 96.76% accuracy, 96.78% precision, 96.79% recall, and 96.79% F1-score on the multimodal dataset
- Outperforms single-modality models (Image: 83.21%, Audio: 82.35%, Wave: 80.47%) by 13-16 percentage points
- Surpasses existing multimodal fusion models including MFN (93.21%), MMIM (92.86%), and LMF (92.34%)
- Ablation studies confirm ARPM and ER modules contribute 7-9% and 2.5% accuracy improvements respectively

## Why This Works (Mechanism)

### Mechanism 1: Unified Large-Kernel Feature Space
The model converts audio and water wave data into image-like formats and applies UniRepLKNet with 13x13 kernels to all modalities, creating consistent feature spaces for easier fusion compared to heterogeneous backbones.

### Mechanism 2: Hierarchical Attention Reinforcement (ARPM)
ARPM operates asymmetrically first (primary reinforces auxiliary via cross-attention) then symmetrically (mutual fusion), allowing the model to learn what to attend to in auxiliary data based on primary context while reducing noise.

### Mechanism 3: Conflict-Aware Evidence Fusion
The ER layer treats modality outputs as evidence with learnable reliability weights, computing joint confidence scores that penalize conflicting predictions and prevent outlier modalities from dominating decisions.

## Foundational Learning

- **Large-Kernel Convolutions (UniRepLKNet):** Why needed - Larger kernels capture global context in spectrograms and wave data better than standard 3x3 convolutions. Quick check: How does kernel size affect global vs. local context capture in Mel spectrograms?

- **Attention Mechanisms (Self vs. Cross):** Why needed - ARPM requires understanding the difference between finding patterns within a modality (self) versus finding relationships between modalities (cross). Quick check: In DAFN-1, which tensor serves as Query and which as Key/Value when reinforcing auxiliary features?

- **Evidential Reasoning (ER) vs. Softmax Fusion:** Why needed - ER handles uncertainty differently than simple probability averaging by modeling reliability and conflict. Quick check: Why might Probability Averaging fail if one modality produces a confident but incorrect prediction?

## Architecture Onboarding

- **Component map:** Input Preprocessing (Image/Audio/Wave → C×224×224 tensors) → UniRepLKNet (512-dim vectors) → ARPM (CAFN + DAFN-1 + DAFN-2) → Individual Classifiers → ER Fusion Layer
- **Critical path:** The transformation of Water Wave data from time-series to "image" format is the most fragile step requiring careful preprocessing
- **Design tradeoffs:** ER fusion offers ~0.7% accuracy gain over Probability Averaging but adds complexity; selecting the wrong primary modality reduces accuracy by ~5-7%
- **Failure signatures:** Heterogeneous feature extractors drop accuracy to ~76%; validation loss oscillation indicates potential learning rate scheduler issues
- **First 3 experiments:** 1) Run single-modality baselines to confirm Table 3 results; 2) Replace ARPM with Concat fusion to verify performance gap; 3) Compare ER fusion against Majority Voting

## Open Questions the Paper Calls Out
- How to reduce the 41.48M parameter count for efficient deployment on resource-constrained edge devices while maintaining performance
- Whether performance transfers to outdoor aquaculture environments with uncontrolled lighting and higher turbidity
- How the model performs if the designated "primary" modality (Image) experiences temporary occlusion or failure

## Limitations
- Water wave preprocessing method is underspecified, creating reproducibility barriers
- Dataset is domain-specific to aquaculture, limiting external validation
- No comparison with state-of-the-art multimodal transformers or vision-language models

## Confidence
- **High** confidence in multimodal fusion accuracy gains over single-modality baselines, supported by comprehensive ablation studies
- **Medium** confidence in ARPM and ER architectural contributions due to limited external validation
- **Low** confidence in UniRepLKNet generalizability to other multimodal tasks due to domain-specific preprocessing

## Next Checks
1. **Cross-domain validation:** Test MAINet on a different multimodal classification task to assess transferability of the UniRepLKNet + ARPM + ER approach
2. **Alternative fusion comparison:** Implement transformer-based multimodal fusion (e.g., CLIP-style) to determine if performance gains are architecture-specific
3. **Sensor reliability analysis:** Systematically corrupt the primary modality during inference to measure performance degradation and validate conflict-aware fusion robustness