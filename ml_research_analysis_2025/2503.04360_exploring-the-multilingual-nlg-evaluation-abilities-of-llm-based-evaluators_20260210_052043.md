---
ver: rpa2
title: Exploring the Multilingual NLG Evaluation Abilities of LLM-Based Evaluators
arxiv_id: '2503.04360'
source_url: https://arxiv.org/abs/2503.04360
tags:
- sentences
- languages
- evaluation
- original
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates the multilingual evaluation
  capabilities of 10 recent LLMs across high-resource and low-resource languages.
  Results show that excluding reference answers from prompts and using large-parameter
  LLMs improve evaluation performance.
---

# Exploring the Multilingual NLG Evaluation Abilities of LLM-Based Evaluators

## Quick Facts
- **arXiv ID**: 2503.04360
- **Source URL**: https://arxiv.org/abs/2503.04360
- **Reference count**: 23
- **Primary result**: Comprehensive evaluation of 10 recent LLMs across high-resource and low-resource languages shows excluding reference answers and using large-parameter models improves multilingual evaluation performance, with stronger correlations to human judgments in high-resource languages

## Executive Summary
This study investigates the multilingual evaluation capabilities of large language models (LLMs) across 6 languages (English, French, German, Bengali, Hindi, Telugu, Urdu) for NLG tasks including title and summary generation. The research systematically examines how different prompting strategies, model parameters, and fine-tuning approaches affect evaluation performance. Results reveal that excluding reference answers from prompts and using larger model parameters generally improve evaluation accuracy, while performance varies significantly between high-resource and low-resource languages. The study also introduces perturbation analysis to understand model sensitivity and its relationship to evaluation quality.

## Method Summary
The authors evaluate 10 recent LLMs using 6 human evaluation datasets covering different NLG tasks. They employ correlation metrics (Pearson, Spearman, Kendall) to measure agreement between model evaluations and human judgments. The study systematically tests different prompting strategies (with/without reference answers), analyzes the impact of model parameters, and conducts perturbation experiments by modifying generated outputs. Fine-tuning experiments are performed on Llama-3.1-8B using data from specific languages to assess cross-lingual transfer capabilities.

## Key Results
- Excluding reference answers from prompts improves evaluation performance for most LLMs across multiple languages
- High-resource languages (English, French, German) show stronger correlations with human judgments than low-resource languages (Bengali, Hindi, Telugu, Urdu)
- Perturbation sensitivity correlates with evaluation performance, with high-resource languages showing higher sensitivity
- Fine-tuning on specific language data can improve cross-lingual evaluation abilities, though results are not consistent across languages

## Why This Works (Mechanism)
The effectiveness of LLM-based evaluators stems from their ability to capture semantic similarity and quality dimensions through learned representations. Models without reference answers appear to focus more on intrinsic qualities of the generated text rather than comparison-based evaluation. Larger models demonstrate better performance due to their enhanced capacity to handle complex linguistic patterns and cross-lingual variations. Perturbation sensitivity indicates the model's attention to subtle quality differences, which correlates with better evaluation accuracy.

## Foundational Learning
- **Correlation metrics (Pearson, Spearman, Kendall)**: Essential for quantifying agreement between model predictions and human judgments; quick check: verify statistical significance of reported correlations
- **Perturbation analysis**: Method for assessing model sensitivity to input changes; quick check: confirm perturbation types preserve task semantics while varying quality dimensions
- **Cross-lingual transfer learning**: Understanding how fine-tuning on one language affects evaluation in other languages; quick check: analyze representation alignment across languages before/after fine-tuning
- **Reference-free evaluation**: Techniques for evaluating outputs without ground truth references; quick check: compare performance gaps between reference-based and reference-free approaches
- **Prompt engineering strategies**: Design of effective prompts for eliciting quality judgments from LLMs; quick check: test different prompt templates systematically
- **Multilingual evaluation datasets**: Construction and validation of datasets covering multiple languages and task types; quick check: verify language coverage and task diversity

## Architecture Onboarding
Component map: Human evaluation datasets -> LLM-based evaluators -> Correlation metrics -> Performance analysis
Critical path: Input generation → Model evaluation → Quality assessment → Correlation computation
Design tradeoffs: Reference inclusion vs. reference exclusion balances comparison accuracy against model distraction; model parameter size trades computational cost for evaluation quality
Failure signatures: Low correlation with human judgments indicates evaluation misalignment; inconsistent cross-lingual performance suggests representation gaps; high sensitivity to irrelevant perturbations indicates over-sensitivity
First experiments: 1) Compare evaluation performance with and without reference answers across all languages, 2) Test perturbation sensitivity thresholds for different model sizes, 3) Analyze cross-lingual transfer patterns after fine-tuning on each language

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: What are the specific linguistic or cognitive mechanisms that cause reference answers to degrade the multilingual evaluation performance of most LLMs?
- **Basis in paper**: [inferred] The authors found in RQ1 that including reference answers often harms performance, contradicting the intuition that references should aid evaluation.
- **Why unresolved**: The paper notes this casts doubt on existing practices but does not offer a theoretical or mechanistic explanation for why models fail to utilize references effectively in a multilingual context.
- **What evidence would resolve it**: An interpretability study analyzing attention weights or probing classifier performance to determine if the model is distracted or confused by the reference text in different languages.

### Open Question 2
- **Question**: Why does fine-tuning on specific low-resource languages (e.g., Bengali) sometimes yield superior cross-lingual evaluation capabilities compared to fine-tuning on high-resource languages?
- **Basis in paper**: [inferred] In RQ4 (Section 6), the authors observed that fine-tuning Llama-3.1-8B on Bengali data resulted in the best performance across almost all other languages, a result described as "surprising."
- **Why unresolved**: The paper hypothesizes that a lack of training data causes poor evaluation, but the result where low-resource fine-tuning outperforms high-resource fine-tuning for cross-lingual transfer contradicts standard scaling laws without further explanation.
- **What evidence would resolve it**: A comparative analysis of representation alignment before and after fine-tuning to see if low-resource data induces a more robust, language-agnostic feature space than high-resource data.

### Open Question 3
- **Question**: Do the observed correlations between perturbation sensitivity and evaluation performance hold for NLG tasks beyond title and summary generation?
- **Basis in paper**: [explicit] The authors explicitly state in the Limitations section (Section 9) that due to limited resources, experiments were restricted to a "narrow range of task types," limiting generalizability.
- **Why unresolved**: It is unclear if the high sensitivity to perturbations in high-resource languages is a universal indicator of evaluation proficiency or if it is specific to the structural constraints of summarization and title generation tasks.
- **What evidence would resolve it**: Extending the MPE dataset and experimental framework to other tasks, such as dialogue response generation or machine translation, and replicating the perturbation analysis.

## Limitations
- Evaluation framework relies on automated correlation metrics with human judgments, introducing inherent measurement uncertainty
- Experiments restricted to a narrow range of task types (title and summary generation), limiting generalizability to other NLG tasks
- Perturbation analysis uses synthetic perturbations that may not capture all relevant aspects of evaluation sensitivity

## Confidence
- **High confidence**: Excluding reference answers from prompts improves evaluation performance across multiple LLMs and languages
- **Medium confidence**: High-resource languages show stronger correlations with human judgments than low-resource languages
- **Medium confidence**: Perturbation sensitivity correlates with evaluation performance
- **Low confidence**: Fine-tuning does not necessarily improve performance in the fine-tuning language

## Next Checks
1. Replicate the evaluation across additional low-resource languages (e.g., African and Southeast Asian languages) to verify the generalizability of the high-resource vs low-resource performance gap
2. Conduct ablation studies specifically isolating the impact of reference exclusion from prompts versus other prompt engineering techniques to quantify the relative contribution of this factor
3. Test alternative perturbation methodologies (e.g., adversarial examples, semantic-preserving transformations) to validate whether perturbation sensitivity consistently correlates with evaluation quality across different perturbation types