---
ver: rpa2
title: On fine-tuning Boltz-2 for protein-protein affinity prediction
arxiv_id: '2512.06592'
source_url: https://arxiv.org/abs/2512.06592
tags:
- affinity
- protein
- prediction
- boltz-2-ppi
- boltz-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning Boltz-2 for protein-protein affinity prediction shows
  that structure-based models underperform sequence-based ones. Despite high structural
  accuracy, the Boltz-2-PPI model achieved only Pearson r=0.153 (TCR3d) and r=0.338
  (PPB-affinity), compared to sequence models reaching r=0.239 and r=0.48 respectively.
---

# On fine-tuning Boltz-2 for protein-protein affinity prediction

## Quick Facts
- arXiv ID: 2512.06592
- Source URL: https://arxiv.org/abs/2512.06592
- Reference count: 26
- Fine-tuning Boltz-2 for protein-protein affinity prediction shows that structure-based models underperform sequence-based ones, with sequence models achieving r=0.239-0.48 vs structure-based r=0.153-0.338.

## Executive Summary
This paper evaluates whether fine-tuning Boltz-2, a structure-based protein model, for protein-protein affinity prediction can match or exceed sequence-based protein language models. Despite Boltz-2's high structural accuracy (DockQ≈0.91 on seen complexes), the structure-based affinity model achieved only moderate performance (r=0.153 on TCR3d, r=0.338 on PPB-affinity). In contrast, sequence-based models like ESM2 and ProtT5 achieved substantially better results (r=0.239 and r=0.48 respectively). Combining structure and sequence embeddings yielded modest gains, particularly for weaker sequence models, suggesting complementary information. The results indicate current structure-based representations lack the expressiveness needed for affinity prediction, while sequence-based protein language models capture binding signals more effectively.

## Method Summary
The study fine-tunes Boltz-2 for protein-protein affinity prediction by modifying its affinity module to handle protein-protein interactions, training with a combined Huber and ranking loss, and using hard data splits to avoid sequence leakage. The method freezes the Boltz-2 trunk and trains only the affinity module with He initialization. For comparison, several sequence-based protein language models (ESM2, ProtT5, ESM1, ESMFold) are fine-tuned using standard MSE loss. The study also explores simple concatenation of structure and sequence embeddings followed by linear projection. Datasets include TCR3d (248 complexes) and PPB-affinity (filtered to 8,207 entries), with performance evaluated using Pearson r, Spearman ρ, and RMSE.

## Key Results
- Structure-based models (Boltz-2-PPI) underperform sequence-based models (ESM2, ProtT5) on both TCR3d (r=0.153 vs r=0.239) and PPB-affinity (r=0.338 vs r=0.48)
- Combining Boltz-2-PPI embeddings with sequence representations yields modest gains, particularly for weaker sequence models (ESM2 improves from r=0.47 to r=0.487)
- Using experimentally determined structures instead of predicted coordinates did not improve performance, suggesting structural quality is not the primary bottleneck
- High structural accuracy (DockQ≈0.91) does not translate to affinity prediction performance, indicating structural accuracy and affinity prediction are decoupled tasks

## Why This Works (Mechanism)

### Mechanism 1
Sequence-based protein language models capture binding affinity signals more effectively than structure-based representations derived from Boltz-2. Pre-trained protein language models (ESM2, ProtT5) learn evolutionary and functional patterns from massive sequence corpora that implicitly encode binding determinants. Structure-based models like Boltz-2-PPI derive representations optimized for structural accuracy (DockQ), but these embeddings lack expressiveness for regression on affinity values—possibly because structure prediction and affinity prediction require different feature emphasis. Affinity-relevant signals are more accessible from sequence statistics than from current 3D coordinate representations.

### Mechanism 2
Combining sequence and structure embeddings yields complementary improvements, particularly for weaker sequence models. Sequence embeddings capture global patterns; structure embeddings encode interface geometry and pairwise interactions. A linear projection on concatenated features can access both signal types, but the benefit is asymmetric—stronger sequence models (ProtT5) already saturate predictable variance, leaving less room for structural complementarity. Sequence and structure representations encode partially non-overlapping information relevant to affinity.

### Mechanism 3
High structural accuracy does not guarantee affinity prediction performance. Boltz-2 achieves DockQ≈0.91 on seen complexes and ≈0.70 on unseen, yet affinity prediction remains weak (r=0.153–0.338). Using experimentally determined structures instead of predicted coordinates did not improve performance. This suggests the bottleneck is not coordinate precision but the representational capacity of the affinity module to encode energetic determinants. Structural accuracy and affinity prediction are decoupled tasks requiring different learned features.

## Foundational Learning

- **Binding affinity metrics (Kd, pKd, Pearson r, Spearman ρ)**: The paper predicts continuous affinity values (Kd) transformed to pKd. Understanding correlation metrics is essential for interpreting performance comparisons.
  - Quick check question: Can you explain why Pearson r captures linear relationships while Spearman ρ captures monotonic relationships, and why both are reported?

- **Protein language models (PLMs) as feature extractors**: ESM2 and ProtT5 are used as sequence baselines. Understanding their pretraining objectives helps explain why they capture binding-relevant signals.
  - Quick check question: How does masked language modeling on protein sequences encode evolutionary constraints relevant to binding?

- **Pairformer architecture and distance-conditioned representations**: Boltz-2's affinity module builds on Pairformer stacks with distance conditioning. Understanding pairwise attention over 3D coordinates is prerequisite to modifying the architecture.
  - Quick check question: Why would distance-conditioned attention be important for capturing interface interactions versus single-chain representations?

## Architecture Onboarding

- **Component map**: Input sequences -> Boltz-2 trunk (frozen) -> single and pairwise Pairformer representations -> Affinity module (trainable) -> Distance-conditioned Pairformer stack -> Cross-pair pooling -> MLP readout -> Predicted pKd

- **Critical path**: Prepare dataset with hard splits (avoid sequence leakage across train/val/test) -> Extract Boltz-2 trunk representations (freeze weights) -> Initialize affinity module with He initialization -> Train with combined Huber + ranking loss, respecting PMID batch constraints -> Evaluate on held-out test set using Pearson r and Spearman ρ

- **Design tradeoffs**: Freezing trunk limits capacity but prevents overfitting on small datasets; simple MLP vs deep affinity module - paper tested 2-layer MLP and found no improvement; predicted vs true structures - true structures did not help, indicating bottleneck is not coordinate quality

- **Failure signatures**: Low correlation (r < 0.2) despite low training loss → overfitting to dataset artifacts; large gap between training and validation performance → insufficient data or split contamination; no improvement from structure fusion with strong PLM → saturation of predictable variance

- **First 3 experiments**: 
  1. Reproduce sequence baseline (ESM2-650M) on TCR3d: Fine-tune with MSE loss using identical splits. Verify r ≈ 0.24 to confirm environment and data pipeline.
  2. Train Boltz-2-PPI with frozen trunk on PPB-affinity: Use Huber + ranking loss with PMID batching. Target r ≈ 0.34 to validate architecture adaptation.
  3. Concatenate Boltz-2-PPI and ESM2 embeddings with linear probe: Train only the final projection layer. Check if r improves to ≈ 0.49 to confirm complementary signal hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating lower-fidelity binding data (e.g., docking decoys, affinity proxies) improve the generalization of structure-based affinity models? The authors note that while Boltz-2 leverages multi-fidelity supervision, they restricted training to curated datasets; they suggest incorporating broader, lower-fidelity data "could provide richer supervision" and improve generalization. The current study deliberately isolated curated affinity datasets for controlled benchmarking, leaving the potential benefit of auxiliary data unexplored. Training Boltz-2-PPI on a mixed dataset of experimental and computational binding data and measuring performance on the PPB-affinity and TCR3d test sets would resolve this.

### Open Question 2
Do sophisticated sequence-structure fusion strategies significantly outperform simple concatenation for affinity prediction? The Discussion states that "Progress will likely depend on more sophisticated fusion strategies," as the authors observed only modest gains using simple linear projection of concatenated embeddings. The current experiments relied on a basic concatenation of embeddings, which failed to fully leverage complementary signals, particularly for stronger sequence models. Implementing cross-attention mechanisms or intermediate fusion layers between Boltz-2 and PLM representations to capture non-linear interactions would resolve this.

### Open Question 3
Would unfreezing the Boltz-2 trunk allow the model to learn representations expressive enough to close the performance gap with sequence-based models? The authors froze the Boltz-2 trunk and trained the affinity module from scratch, concluding that the resulting structure-based representations "lack the expressiveness needed for affinity regression." It remains unclear if the performance gap is due to a fundamental limitation of structural representations or the restriction of updates to the small affinity module, preventing the trunk from adapting to PPI-specific signals. Ablation studies performing end-to-end fine-tuning of Boltz-2 (possibly with a low learning rate) to see if structural representations can be refined to match sequence-based performance would resolve this.

## Limitations
- The study uses frozen Boltz-2 trunk and simple linear fusion strategies, which may limit the observed complementarity between structure and sequence representations
- Without hyperparameter details (λ, δ, optimizer settings) and without end-to-end fine-tuning experiments, it's unclear whether architectural constraints or representational limitations drive the results
- The small TCR3d dataset (248 samples) raises concerns about statistical power and overfitting risk

## Confidence

- **High confidence**: Structure-based models underperform sequence models (r=0.153-0.338 vs r=0.239-0.48); sequence-structure fusion shows modest gains particularly for weaker sequence models
- **Medium confidence**: Current structure-based representations lack expressiveness for affinity prediction; complementary signals exist between sequence and structure embeddings
- **Low confidence**: The structural accuracy-accuracy gap is due to representational capacity rather than training objectives or fusion strategies; simple concatenation is optimal for multimodal integration

## Next Checks

1. Perform ablation studies with varying λ values (0.5-0.9) to determine optimal loss weighting between Huber and ranking terms
2. Test end-to-end fine-tuning of the Boltz-2 trunk with affinity supervision to assess whether representational gaps persist with full training
3. Implement attention-based fusion mechanisms beyond concatenation to evaluate whether complementarity gains can exceed the modest improvements reported