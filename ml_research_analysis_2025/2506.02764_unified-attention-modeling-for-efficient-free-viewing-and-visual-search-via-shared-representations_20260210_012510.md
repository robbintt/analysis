---
ver: rpa2
title: Unified Attention Modeling for Efficient Free-Viewing and Visual Search via
  Shared Representations
arxiv_id: '2506.02764'
source_url: https://arxiv.org/abs/2506.02764
tags:
- visual
- attention
- search
- free-viewing
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether a shared representation exists between
  free-viewing and task-specific visual search in human attention modeling. Building
  upon the Human Attention Transformer (HAT) model, the authors propose a neural network
  architecture with shared and task-specific layers in the pixel decoder, enabling
  the reuse of features learned from free-viewing for visual search.
---

# Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations

## Quick Facts
- arXiv ID: 2506.02764
- Source URL: https://arxiv.org/abs/2506.02764
- Authors: Fatma Youssef Mohammed; Kostas Alexis
- Reference count: 32
- Key outcome: A shared representation between free-viewing and visual search reduces computational costs by 92.29% in GFLOPs while maintaining performance with only 3.86% drop in semantic sequence score

## Executive Summary
This work investigates whether free-viewing and task-specific visual search can share a common representation for human attention modeling. Building upon the Human Attention Transformer (HAT) model, the authors propose a neural network architecture with shared and task-specific layers in the pixel decoder, enabling the reuse of features learned from free-viewing for visual search. Their results demonstrate that free-viewing and visual search can efficiently share a common representation, with only a 3.86% performance drop in semantic sequence score when transferring knowledge from free-viewing to visual search. This approach reduces computational costs by 92.29% in GFLOPs and 31.23% in trainable parameters, suggesting that bottom-up attention features can be repurposed for task-driven attention in a more efficient and generalizable approach to multi-task attention modeling.

## Method Summary
The authors propose a sequential transfer learning approach that first trains a free-viewing attention model, then freezes shared layers while training a visual search branch. The architecture uses a ResNet-50 encoder (frozen with COCO panoptic pretrained weights), an MSDeformAtten pixel decoder with configurable shared/task-specific layer splits, a foveation module with transformer encoder, an aggregation module with transformer decoder, and fixation prediction components. Training follows a two-stage protocol: Stage 1 trains the free-viewing branch, Stage 2 freezes shared pixel decoder layers and trains the visual search branch. Six different layer-splitting configurations are tested, ranging from all 6 layers shared (LS) to only 1 shared layer (ES1,5), with training conducted for 15 epochs versus 30 in the original HAT model.

## Key Results
- Shared representation approach achieves only 3.86% performance drop in semantic sequence score when transferring from free-viewing to visual search
- Computational efficiency: 92.29% reduction in GFLOPs and 31.23% reduction in trainable parameters
- Later layer splits (LS configuration) maximize computational savings while earlier splits (ES variants) balance performance and efficiency
- Frozen representations show better generalization on unseen data compared to end-to-end training

## Why This Works (Mechanism)

### Mechanism 1: Bottom-Up Feature Reusability for Task-Specific Attention
The features learned during free-viewing capture generic visual saliency patterns that remain useful when frozen and reused for task-driven attention. The visual search branch only needs to learn how to modulate these pre-extracted features toward target-specific regions. This works because bottom-up attention is task- and object-agnostic, providing generic features in early representation stages that can be repurposed for task-driven attention.

### Mechanism 2: Hierarchical Layer Splitting in Feature Extraction
The degree of representation sharing can be controlled by splitting at different depths within the pixel decoder, creating a tradeoff between computational savings and task-specific adaptation. The 6-layer MSDeformAttn pixel decoder is split into shared layers (frozen from free-viewing) and task-specific layers (trained on visual search). Earlier splits allow more task-specific adaptation while later splits maximize parameter reuse.

### Mechanism 3: Sequential Transfer Training with Frozen Representations
A two-stage training protocol—first training on free-viewing, then freezing shared layers while training the visual search branch—preserves transferred features while enabling task adaptation. Stage 1 trains the full free-viewing network, and Stage 2 freezes designated shared layers, preventing gradient updates from overwriting generic features.

## Foundational Learning

- **Concept: Bottom-Up vs. Top-Down Attention**
  - Why needed here: The entire architecture hinges on distinguishing stimulus-driven (free-viewing) from goal-driven (visual search) attention and understanding their potential representational overlap.
  - Quick check question: Can you explain why a bright red object might capture attention in both free-viewing and visual search for "red apple," but only in visual search when looking for "blue cup"?

- **Concept: Transformer-Based Feature Extraction with Multi-Scale Representations**
  - Why needed here: The pixel decoder (MSDeformAtten) produces multi-scale feature maps that feed into foveation modeling. Understanding how transformer layers process and transform visual features is essential for interpreting what gets shared.
  - Quick check question: Why would multi-scale feature maps be important for modeling both foveal (high-resolution) and peripheral (low-resolution) vision?

- **Concept: Scanpath Evaluation Metrics (SemSS, cNSS, cAUC)**
  - Why needed here: Interpreting results requires understanding what each metric measures—SemSS for semantic sequence similarity, cNSS for saliency correspondence at fixation points, cAUC for classification performance.
  - Quick check question: If a model achieves high cAUC but low SemSS, what aspect of human attention is it failing to capture?

## Architecture Onboarding

- **Component map**: Input image -> ResNet-50 encoder (frozen) -> Pixel Decoder (shared + task-specific layers) -> Foveation (tokens from H/4 and H/32 maps) -> Transformer encoder (memory) -> Aggregation decoder (task queries) -> Fixation heatmap + termination probability

- **Critical path**: Input image → ResNet-50 encoder (frozen, COCO panoptic pretrained) → Pixel Decoder (shared + task-specific layers) → Foveation (tokens from H/4 and H/32 maps) → Transformer encoder (memory) → Aggregation decoder (task queries) → Fixation heatmap + termination probability

- **Design tradeoffs**: LS vs. ES configurations balance computational savings (LS) against task-specific performance (ES). Earlier pixel decoder layers capture more transferable features. Training time reduced from 30 to 15 epochs with frozen layers, and ES5,1 slightly outperforms HAT on some metrics.

- **Failure signatures**: Large SemSS drop (>10%) when using LS indicates bottom-up features insufficient for target-directed search strategies. ES variants underperforming LS suggests task-specific layers overfitting or shared layers providing better regularization. Poor generalization on unseen data indicates frozen layers encode dataset-specific biases.

- **First 3 experiments**: 
  1. Reproduce LS configuration: Train HAT on COCO-FreeView, freeze all 6 pixel decoder layers, train visual search branch on COCO-Search18. Verify ~3.86% SemSS drop and 92% GFLOPs reduction.
  2. Ablate split depth: Run all ES configurations (ES5,1 through ES1,5) and plot SemSS/cNSS against GFLOPs reduction to identify optimal tradeoff point.
  3. Cross-dataset generalization test: Evaluate LS and best-performing ES on the paper's collected dataset to verify frozen representations improve generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- The performance degradation of 3.86% in SemSS, while modest, represents a meaningful difference in practical applications and may widen for more complex visual search tasks or different object categories.
- The computational savings figures are specific to the HAT architecture and may not generalize to other attention modeling frameworks.
- The claim about bottom-up features being "task-agnostic" lacks direct validation of what features are being transferred and whether they truly generalize beyond the tested dataset.

## Confidence
- **High Confidence**: The shared representation mechanism and computational savings are well-supported by controlled experiments across six different layer-splitting configurations, with clear performance metrics and ablation studies.
- **Medium Confidence**: The claim about bottom-up features being "task-agnostic" is supported by observed performance but lacks direct validation of what features are being transferred and whether they truly generalize beyond the tested dataset.
- **Low Confidence**: The assertion that this approach offers a "more efficient and generalizable" solution for multi-task attention modeling extends beyond the empirical evidence, which only demonstrates effectiveness for free-viewing to visual search transfer on specific COCO-based datasets.

## Next Checks
1. **Cross-task generalization test**: Evaluate the shared representation approach on a completely different attention task (e.g., reading comprehension or medical image analysis) to verify whether frozen free-viewing features transfer beyond visual search scenarios.
2. **Feature analysis study**: Use feature visualization or attribution methods to identify which specific representations in the frozen layers contribute to visual search performance, testing whether early layers truly capture more transferable features than later layers.
3. **Performance boundary exploration**: Systematically vary object saliency, scene complexity, and target-distractor similarity in the visual search task to determine the conditions under which the 3.86% performance gap widens significantly, establishing practical limits of the transfer approach.