---
ver: rpa2
title: 'Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter
  Study'
arxiv_id: '2508.18515'
source_url: https://arxiv.org/abs/2508.18515
tags:
- planning
- learning
- algorithm
- graph
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effects of various hyperparameters
  on Weisfeiler-Leman Features (WLFs) for learning heuristic functions in planning.
  The authors introduce new WLF hyperparameters and conduct extensive experiments
  with 1,000,000 planning runs to analyze their impact on training and planning performance.
---

# Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study

## Quick Facts
- **arXiv ID:** 2508.18515
- **Source URL:** https://arxiv.org/abs/2508.18515
- **Reference count:** 40
- **Primary result:** Extensive hyperparameter study finds optimal Weisfeiler-Leman Features configuration minimizes model size and execution time rather than maximizing expressivity.

## Executive Summary
This paper conducts a comprehensive hyperparameter study of Weisfeiler-Leman Features (WLFs) for learning heuristic functions in classical planning. The authors test 1,000,000 planning runs across various configurations to determine which hyperparameters yield the best planning performance. They find that contrary to expectations, the optimal configuration uses the simpler WL algorithm (not 2-WL or 2-LWL), only 1 iteration, iterative MaxSAT plus frequency pruning, set hash function, partial state representation, and rank SVM optimizer. This configuration minimizes model size and evaluation time rather than maximizing expressivity. Notably, the study reveals no statistically significant correlation between training metrics and planning performance, suggesting traditional machine learning evaluation methods may not predict planning success for classical approaches.

## Method Summary
The study investigates Weisfeiler-Leman Features for learning heuristic functions in classical symbolic planning. The method involves converting planning states to Instance Learning Graphs (ILGs), applying WL algorithm variants with different hyperparameters to generate features, training linear models using various optimizers, and evaluating the learned heuristics in search planners. The experiments use 10 IPC 2023 Learning Track domains with 1,000,000 planning runs across different hyperparameter combinations. The best configuration found uses WL algorithm, 1 iteration, iterative MaxSAT plus frequency pruning, set hash function, partial state representation, and rank SVM optimizer.

## Key Results
- The optimal WLF configuration minimizes execution time rather than maximizing model expressivity
- No statistically significant correlation exists between training metrics and planning performance
- WL with 1 iteration outperforms higher iterations despite reduced expressivity
- Rank SVM optimizer outperforms regression-based optimizers for planning coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting planning states to graphs and applying the WL algorithm produces informative features for heuristic learning without requiring neural networks.
- **Mechanism:** The Instance Learning Graph (ILG) encodes planning states as graphs with nodes for objects, propositions, and goals; edges encode predicate-argument relationships. The WL algorithm iteratively refines node "colors" by aggregating neighborhood information, then counts color occurrences to produce fixed-size feature vectors.
- **Core assumption:** Planning state similarity correlates with structural graph similarity that the WL algorithm can capture.
- **Evidence anchors:**
  - [abstract] "They have been shown to be both theoretically and empirically superior to existing deep learning approaches for learning value functions for search in symbolic planning."
  - [Section 4.2] Describes how WL algorithm produces canonical graph forms invariant to node orderings, with feature vectors based on color counts.
  - [corpus] Related paper "Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features" confirms WLFs capture structural patterns for planning, though corpus lacks direct comparative mechanism studies.

### Mechanism 2
- **Claim:** Simpler WL configurations (fewer iterations, pruning, set hashing) outperform more expressive variants because they reduce model size and evaluation time during search.
- **Mechanism:** Each WL iteration increases the receptive field (like GNN layers) but also expands the feature space. Pruning removes redundant features; set hashing collapses duplicates. Smaller feature sets mean faster heuristic evaluation during search, which dominates planning time.
- **Core assumption:** Heuristic evaluation speed matters more than feature expressivity for planning coverage under time constraints.
- **Evidence anchors:**
  - [abstract] "the best WLF hyperparameters for learning heuristic functions minimise execution time rather than maximise model expressivity"
  - [Section 6, Table 2] WL with 1 iteration achieves best coverage (497 ΣM), outperforming higher iterations; i-mf pruning and set hashing both show top performance.
  - [corpus] Limited direct evidence; corpus papers focus on hyperparameter optimization in neural networks, not classical ML for planning.

### Mechanism 3
- **Claim:** Ranking-based optimizers (rkSVM) produce better planning heuristics than regression-based optimizers because they learn relative state preferences rather than absolute h* values.
- **Mechanism:** Ranking methods train on pairwise state comparisons (which state is closer to goal), avoiding the need to predict exact optimal cost values. This may better capture the ordinal structure needed for heuristic search guidance.
- **Core assumption:** The relative ordering of states by goal proximity is more learnable and generalizable than absolute cost-to-go values.
- **Evidence anchors:**
  - [Section 5.2] Describes ranking formulations and notes ranking methods use "both states and their siblings" for training.
  - [Section 6, Table 2] rkSVM achieves top coverage (506 ΣM) among optimizers; Lasso (regression) performs worst (388 ΣM).

## Foundational Learning

- **Concept: Classical Planning and State-Space Search**
  - Why needed here: WLFs generate features for planning states; understanding states, actions, goals, and heuristics is essential to grasp what the features represent and how they're used.
  - Quick check question: Given a planning state s and goal G, what does a heuristic function h(s) estimate?

- **Concept: Weisfeiler-Leman Algorithm and Graph Isomorphism**
  - Why needed here: The core feature generation mechanism; WL is a color refinement algorithm for graph comparison. Understanding iterations, color aggregation, and expressivity helps interpret hyperparameter effects.
  - Quick check question: What does increasing WL iterations do to the receptive field and computational cost?

- **Concept: Feature Pruning and Model Complexity Tradeoffs**
  - Why needed here: The paper shows pruning and simpler models improve planning performance; understanding bias-variance tradeoff and feature redundancy explains why.
  - Quick check question: Why might removing features improve generalization even if it reduces training accuracy?

## Architecture Onboarding

- **Component map:**
Planning Task (PDDL) -> Fast Downward grounder -> Lifted planning task -> ILG encoder -> Graph with node features and edge labels -> WL algorithm (specified iterations, hash function) -> Color multiset -> Feature embedding -> Sparse feature vector -> Optimizer (rkSVM/Lasso/etc.) -> Weight vector w -> Heuristic h(s) = Σ w_i × feature_i(s) -> Search engine (Fast Downward/Powerlifted) -> Plan

- **Critical path:**
  1. ILG encoding correctness (node/edge semantics must match domain structure)
  2. WL feature generation efficiency (dominates preprocessing time)
  3. Heuristic evaluation speed during search (dominates planning time)
  4. Model generalization from training to test instances (different object counts)

- **Design tradeoffs:**
  - **Iterations:** Higher = more expressivity but larger models and slower evaluation; paper shows 1 is optimal
  - **Hash function:** Multiset preserves counts (more expressive) vs. set ignores duplicates (better generalization to unseen sizes)
  - **State representation:** Complete (all propositions) vs. partial (pruned static/unreachable); partial is faster but may lose information
  - **Optimizer:** Regression (predict h* directly) vs. ranking (learn relative preferences); ranking works better but requires more training data

- **Failure signatures:**
  - **Memory exhaustion during training:** 2-WL algorithm creates O(n²) features; use WL or 2-LWL instead
  - **Poor generalization to larger instances:** Training metrics look good but planning coverage low (paper shows no correlation); use simpler models, set hashing, pruning
  - **Slow heuristic evaluation:** Too many features; reduce iterations, use i-mf pruning, set hashing
  - **Unseen colors during inference:** Model produces features not in training set; use set hashing to reduce this

- **First 3 experiments:**
  1. **Baseline comparison:** Run WL algorithm with 1 iteration, i-mf pruning, set hash, partial state, rkSVM on 2-3 IPC domains; measure coverage vs. original WLF configuration (4 iterations, no pruning, multiset hash, GPR). This validates the paper's optimal configuration claim.
  2. **Iteration sensitivity:** Fix all hyperparameters except iterations (1-8); plot model size, training time, and coverage. Expect U-shaped or declining coverage as iterations increase beyond 1-2.
  3. **Optimizer ablation:** Compare rkSVM vs. SVR vs. Lasso on same features; measure both training loss and planning coverage. Expect rkSVM best for coverage despite potentially worse training loss, illustrating the training-planning disconnect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What alternative evaluation metrics or validation strategies can reliably predict planning performance for WLF models, given that standard training metrics do not?
- **Basis in paper:** [explicit] The authors state in Section 6 and the Abstract that they "observe no significant correlation between various training metrics and planning performance," leaving model selection difficult.
- **Why unresolved:** The study exhausted standard metrics (loss, training time, model size) across 1,000,000 samples and found no statistically significant links to downstream success.
- **What evidence would resolve it:** Identification of a validation metric or proxy task that exhibits a statistically significant strong correlation (e.g., $|rho| \geq 0.5$) with planning coverage.

### Open Question 2
- **Question:** Do the optimal hyperparameters identified for classical planning (WL, 1 iteration, i-mf pruning) generalize effectively to numeric and probabilistic planning tasks?
- **Basis in paper:** [explicit] Section 3 explicitly notes that WLFs "can also handle numeric planning tasks" and "can be extended to handle probabilistic tasks," but the experiments in Section 6 are restricted to classical IPC domains.
- **Why unresolved:** The efficiency/expressivity trade-offs may differ in numeric or probabilistic contexts where state features have continuous values or uncertainty.
- **What evidence would resolve it:** A comparative hyperparameter study on numeric/probabilistic benchmarks showing similar performance trends (e.g., preference for 1 iteration) or distinct optima.

### Open Question 3
- **Question:** Can automated per-domain hyperparameter configuration systematically outperform the identified robust global configuration?
- **Basis in paper:** [inferred] Section 6 notes that while a robust set of hyperparameters exists, "there are exceptions on a per domain basis" (e.g., Blocksworld requires different pruning/iteration settings).
- **Why unresolved:** The study focuses on a "robust" set that works well *on average*, but does not explore if automated configuration could capture the specific optima for outlier domains like Satellite or Blocksworld.
- **What evidence would resolve it:** An automated configuration algorithm identifying distinct hyperparameter sets per domain that yield statistically significant coverage improvements over the global robust configuration.

## Limitations

- The study focuses exclusively on symbolic planning domains, limiting generalizability to continuous or hybrid domains
- No theoretical justification is provided for why simpler WL configurations outperform more expressive ones
- The disconnect between training metrics and planning performance suggests fundamental limitations in using classical ML evaluation for planning heuristics

## Confidence

- **High confidence:** The empirical finding that WL with 1 iteration, i-mf pruning, set hash, partial state, and rkSVM achieves best coverage. The experimental methodology and statistical analysis are sound.
- **Medium confidence:** The claim about training metrics not correlating with planning performance. While statistically demonstrated, the underlying reasons require further investigation.
- **Low confidence:** The specific mechanisms explaining why ranking optimizers outperform regression ones, as this lacks direct empirical support in the corpus.

## Next Checks

1. Conduct ablation studies on WL iterations (1-8) across diverse planning domains to confirm the non-monotonic relationship between expressivity and coverage.
2. Test the trained heuristics on larger instances than those seen during training to measure true generalization capacity beyond the 10-100 object range.
3. Compare WLF-based heuristics against neural approaches (GNNs, transformers) on the same benchmark to quantify the practical advantage of classical methods.