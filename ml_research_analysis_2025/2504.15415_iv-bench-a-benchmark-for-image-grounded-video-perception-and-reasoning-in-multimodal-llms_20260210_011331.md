---
ver: rpa2
title: 'IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in
  Multimodal LLMs'
arxiv_id: '2504.15415'
source_url: https://arxiv.org/abs/2504.15415
tags:
- video
- image
- reasoning
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IV-Bench is the first benchmark for evaluating image-grounded video
  perception and reasoning in multimodal LLMs, containing 967 videos and 2,585 image-text
  queries across 13 tasks. State-of-the-art models achieved at most 28.9% accuracy,
  substantially underperforming on tasks requiring image context for video comprehension.
---

# IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs

## Quick Facts
- arXiv ID: 2504.15415
- Source URL: https://arxiv.org/abs/2504.15415
- Reference count: 21
- Primary result: First benchmark for image-grounded video perception and reasoning in multimodal LLMs

## Executive Summary
IV-Bench is the first benchmark specifically designed to evaluate image-grounded video perception and reasoning in multimodal large language models (MLLMs). The benchmark contains 967 videos averaging 537 seconds each, paired with 2,585 image-text queries across 13 tasks spanning 5 categories. State-of-the-art models achieve at most 28.9% accuracy, with particularly poor performance on reasoning tasks requiring cross-modal temporal understanding. The benchmark reveals that current MLLMs struggle to integrate external image contexts with video content, and that simple synthetic data alignment yields only marginal improvements.

## Method Summary
The benchmark evaluates MLLMs using multiple-choice question answering (10 options per question) on videos paired with external query images. Videos are uniformly sampled to extract 32 frames by default, with variations tested at 8-64 frames and 72p-240p resolution. Models process video frames, the query image, and the text question in different orderings to assess context integration. Evaluation computes accuracy as exact match of model output to correct answer option. The dataset covers 7 perception tasks and 6 reasoning tasks across 5 categories, with tasks requiring varying degrees of cross-modal understanding between the query image and video content.

## Key Results
- State-of-the-art models achieve at most 28.9% accuracy on IV-Bench
- Top models underperform on tasks requiring image context for video comprehension
- Larger models (>10B parameters) benefit significantly from image contexts placed after video frames
- Increasing frame number consistently improves performance more than increasing resolution
- Simple synthetic data alignment yields only marginal improvements (1-2%)

## Why This Works (Mechanism)

### Mechanism 1: Recency-Enhanced Multimodal Grounding
Placing the query image after the video frames improves performance for large-capacity models (>10B parameters) by mitigating visual context forgetting. Large models appear to leverage the image more effectively when it is positioned temporally closer to the query generation step. When the image precedes a long video sequence, the visual token representations may be diluted or "forgotten" by the time the model processes the video and generates an answer. Positioning the image after the video ensures the grounding signal is fresh in the context window.

### Mechanism 2: Temporal Token Scaling over Spatial Fidelity
For image-grounded video tasks, increasing the number of sampled frames yields more consistent performance gains than increasing spatial resolution, particularly for smaller models. The "existence" and "temporal reasoning" tasks require locating specific visual signals within a continuous stream. Uniformly sampling more frames increases the probability of capturing the specific moment where the query image content appears, even if individual frames are lower resolution.

### Mechanism 3: Capability Gap vs. Data Format Alignment
The failure of current MLLMs on IV-Bench stems from a fundamental lack of cross-modal reasoning capability, not merely a mismatch between training data formats and test inputs. While synthetic pipelines can align training data to the "Video+Image" format, they fail to teach the model how to correlate an external image with specific video dynamics. The benchmark requires reasoning where the image acts as a strict constraint, not just a visual decoration.

## Foundational Learning

- **Concept: Context-Dependent Visual Grounding**
  - Why needed: Unlike standard Video QA where the question is text-only, IV-Bench requires the model to treat an external image as a "visual variable" that defines the query's scope.
  - Quick check: Can your model distinguish between "Find a person" (text query) and "Find this specific person shown in the image" (image-text query)?

- **Concept: Visual Token Budgeting**
  - Why needed: The paper highlights a strict trade-off between temporal coverage (frames) and spatial detail (resolution) given a fixed context window.
  - Quick check: Given a fixed token limit of 4096, does your model perform better on IV-Bench with 64 frames at 144p or 8 frames at 720p?

- **Concept: Spatio-Temporal Cross-Attention**
  - Why needed: To solve "Temporal Reasoning" or "Keyframe Extraction," the model must align spatial features from the static query image with specific frames in the video stream.
  - Quick check: Does your architecture explicitly attend to the query image tokens while processing video frames, or does it process them as disjoint sequences?

## Architecture Onboarding

- **Component map:** Video Encoder -> Image Encoder -> Projector/Adapter -> LLM Backbone
- **Critical path:** The Inference Pattern (Prompt Ordering). You must implement a pipeline that allows flexible ordering of tokens: specifically `Video -> Image -> Text` vs `Image -> Video -> Text` to test the recency bias mechanism.
- **Design tradeoffs:**
  - Frame Count vs. Resolution: The paper shows 32 frames is a robust baseline, but scaling to 64+ frames helps smaller models. Increasing resolution beyond 240p yields diminishing returns unless frame count drops.
  - Synthetic vs. Real Data: Do not rely on synthetic alignment of existing QA datasets. The paper proves this yields <1% gain.
- **Failure signatures:**
  - Text-Only Bias: If the model answers correctly without the image, the image features are not being integrated (Control Check).
  - Recency Collapse: If the model fails when the image is placed before the video, it indicates a context window or attention limitation (common in <10B models).
  - Hallucination: High scores on "Existence" but low scores on "Reverse Existence" suggests the model is guessing based on semantic similarity rather than verifying visual evidence.
- **First 3 experiments:**
  1. Baseline Ordering: Evaluate `Video -> Image -> Text` vs `Image -> Video -> Text` on InternVL2.5-8B vs 78B to replicate the capacity-dependent ordering effect.
  2. Token Sweep: Fix visual tokens, vary (Frame, Resolution) pairs (e.g., 64/144p vs 8/720p) on the "Existence" task to map the temporal vs. spatial utility.
  3. Ablation on Image Necessity: Evaluate models with "Image Masked" (text-only) to quantify the performance drop, ensuring the image is actually being used as a grounding signal.

## Open Questions the Paper Calls Out

### Open Question 1
What specific training paradigms or architectural mechanisms are required to master image-grounded video reasoning, given that simple data format alignment is insufficient? The authors demonstrate that the bottleneck is not merely the absence of "video-image formatted" data, but rather a deeper lack of capability that current scaling and data synthesis techniques fail to address.

### Open Question 2
Why do smaller models fail to leverage image context while larger models succeed only when the image is placed after the video? The paper identifies the "forgetting" phenomenon and the disparity between model scales but does not propose a mechanism to fix the positional bias or enable smaller models to utilize the visual grounding effectively.

### Open Question 3
How can temporal reasoning capabilities be improved for image-grounded tasks where current state-of-the-art models achieve less than 17% accuracy? The paper establishes the difficulty but does not offer a solution for the specific cross-modal temporal grounding required to identify when an image's content appears in a video timeline.

## Limitations
- Context Window Saturation Effects: Analysis limited to token budgets of 4096-16384; relationship at larger scales (32K+ tokens) remains unknown
- Dataset Representation Bias: Generalizability to shorter videos, different content domains, or variable qualities remains uncertain
- Model Architecture Dependencies: Performance patterns could be influenced by factors beyond parameter count, such as attention mechanisms

## Confidence

**High Confidence Claims:**
- IV-Bench is the first benchmark specifically targeting image-grounded video perception and reasoning
- Current state-of-the-art models achieve only 28.9% accuracy on the benchmark
- Increasing frame count improves performance more consistently than increasing resolution
- The synthetic data approach yields only marginal improvements (1-2%)

**Medium Confidence Claims:**
- Larger models (>10B parameters) benefit more from image positioning after video frames
- Smaller models (<10B) show minimal improvement from image context addition
- The 32-frame baseline is optimal for most tasks within the tested resolution range

**Low Confidence Claims:**
- The exact mechanism by which larger models leverage post-video image positioning
- The optimal frame-resolution trade-off for specific model families beyond tested configurations
- The generalizability of the 1-2% synthetic data improvement as an upper bound for all alignment strategies

## Next Checks

1. **Cross-Domain Generalization Study**: Evaluate IV-Bench performance on videos from different domains (news, entertainment, surveillance) and lengths (30s-5min) to assess whether observed patterns hold across varied real-world scenarios.

2. **Architectural Ablation Analysis**: Systematically vary architectural components (attention patterns, visual token processing order, image-video fusion strategies) within a single model family while keeping other factors constant to isolate whether capacity-dependent ordering effects stem from parameter count or specific architectural choices.

3. **Extended Context Window Evaluation**: Test the same models and tasks with 32K+ token context windows, varying frame counts from 8 to 256 while maintaining consistent resolution to determine whether observed frame-resolution trade-offs persist at scales where temporal coverage becomes less constrained by token budgets.