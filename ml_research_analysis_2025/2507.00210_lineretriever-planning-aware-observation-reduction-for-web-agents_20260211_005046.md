---
ver: rpa2
title: 'LineRetriever: Planning-Aware Observation Reduction for Web Agents'
arxiv_id: '2507.00210'
source_url: https://arxiv.org/abs/2507.00210
tags:
- observation
- lineretriever
- agents
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LineRetriever, a planning-aware observation
  reduction method for web agents. The approach uses a small language model to filter
  web page observations, retaining only the lines most relevant for future navigation
  steps.
---

# LineRetriever: Planning-Aware Observation Reduction for Web Agents

## Quick Facts
- **arXiv ID:** 2507.00210
- **Source URL:** https://arxiv.org/abs/2507.00210
- **Reference count:** 7
- **Key result:** LLM-based retrieval achieves 49.1% success rate with 30% observation reduction on WorkArena L1, outperforming embedding baselines (19.4%) while preserving planning-relevant information.

## Executive Summary
LineRetriever addresses context overflow in web agents by using a small language model to filter web page observations, retaining only lines most relevant for future navigation steps. The method achieves up to 73% reduction in observation size while maintaining competitive performance compared to full observation processing. By incorporating planning context and structural preservation, LineRetriever outperforms embedding-based retrieval approaches and demonstrates that LLM-based retrieval with structural awareness enables efficient web agents operating within context constraints.

## Method Summary
LineRetriever uses a small LLM (GPT-4.1-mini or GPT-4.1) to process web page observations represented as numbered AxTree lines. The retriever receives the task goal, action history, and observation, then outputs JSON line ranges indicating which elements are relevant for completing the next navigation step. Two post-processing modes are implemented: direct line removal and structure-preserving filtering that retains parent element IDs and roles. The pruned observation is then passed to the action model within token limits. The approach is evaluated across three benchmarks (WorkArena L1, Weblinx, WebArena) with success rate and observation reduction as primary metrics.

## Key Results
- LLM-based retrieval achieves 49.1% success rate on WorkArena L1 with 30% reduction, versus 19.4% for embedding-based retrieval with 52-72% reduction
- Structure-preserving mode maintains 49.1% success rate while direct removal drops to 44.5% on WorkArena L1
- Small retriever with structure (GPT-4.1-mini + Structure) matches performance of larger retriever without structure (48.2% vs 49.1% on WorkArena L1)
- Embedding baseline fails to capture plan-relevant information, achieving <20% success across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
LLM-based retrieval outperforms embedding-based retrieval for planning-aware observation reduction because LLMs can reason about which elements support future action prediction, not just semantic similarity. The retriever LLM receives the task goal, numbered observation lines, and action history, then explicitly identifies line ranges relevant for completing the next navigation step. This incorporates planning context that embedding models cannot capture. Core assumption: LLMs possess contextual reasoning capabilities that embedding models lack for interactive tasks requiring sequential decision-making.

### Mechanism 2
Preserving hierarchical structure in pruned AxTrees improves downstream agent performance because LLMs rely on learned patterns of tree structure to ground interface elements. The "LineRetriever + Structure" variant retains parent element IDs and roles rather than removing lines entirely. This maintains representational integrity so the pruned tree remains within the distribution of AxTrees the action model has learned to interpret. Core assumption: Aggressive pruning produces "degenerate" AxTrees that fall outside the implicit distribution learned by the action model, causing grounding failures.

### Mechanism 3
A small retriever model with structural context can match or exceed a larger retriever model without structural context, enabling cost-efficient observation reduction. Providing appropriately organized contextual information (hierarchical structure) compensates for reduced model capacity. GPT-4.1-mini with structure achieved 49.1% success vs GPT-4.1 without structure at 48.2% on WorkArena L1. Core assumption: Structural information encodes task-relevant relationships that would otherwise require larger model capacity to infer from raw text.

## Foundational Learning

- **Concept: Accessibility Tree (AxTree) Representation**
  - **Why needed here:** LineRetriever operates on AxTree text representations of web pages. Understanding that AxTrees encode hierarchical UI elements with roles, IDs, and nested relationships is essential for interpreting the pruning mechanism and why structure preservation matters.
  - **Quick check question:** Given a sample AxTree snippet, can you identify which lines represent parent containers vs. leaf interactive elements?

- **Concept: Embedding-Based Retrieval vs. LLM-Based Reranking**
  - **Why needed here:** The paper positions LineRetriever against embedding retrieval baselines. You must understand that embedding models encode semantic similarity in fixed vectors while LLM-based approaches can perform contextual reasoning over the full prompt context.
  - **Quick check question:** Why would cosine similarity on chunk embeddings fail to identify elements relevant for future action prediction?

- **Concept: Context Window Constraints in Web Agents**
  - **Why needed here:** The entire motivation for LineRetriever is that web page observations exceed model context limits. Understanding token budgets and the cost implications of processing long contexts is critical for evaluating the tradeoffs.
  - **Quick check question:** If the action model costs 5x more per token than the retriever model, what minimum reduction ratio is needed for cost neutrality?

## Architecture Onboarding

- **Component map:** Environment -> AxTree extraction -> LineRetriever LLM -> Post-processor -> Action Model LLM -> BrowserGym/AgentLab
- **Critical path:** 1) AxTree extraction from web page at each episode step, 2) Line numbering and prompt construction (goal, history, observation), 3) Retriever LLM inference -> JSON line ranges, 4) Post-processing with structure-preserving or direct-removal mode, 5) Token count validation before passing to action model, 6) Action execution and loop continuation
- **Design tradeoffs:** Compression vs. Performance: Direct removal achieves 58-75% reduction but drops success rate; structure preservation achieves only 18-30% reduction but recovers performance. Retriever Model Size: Larger retriever (GPT-4.1) improves selection quality without structure; smaller retriever (GPT-4.1-mini) requires structural augmentation to match performance. History Inclusion: Including full action history improves context but increases retriever prompt size.
- **Failure signatures:** Embedding retrieval baseline achieves <20% success rate with high compression (52-72%) -> indicates semantic similarity alone is insufficient for planning. Aggressive pruning without structure causes success rate drops on structured input tasks -> indicates hierarchical grounding loss. Higher token counts do not correlate linearly with reduction ratios -> indicates reduction effectiveness depends on observation content, not just length.
- **First 3 experiments:** 1) Baseline reproduction: Run GenericAgent with bottom-truncation on WorkArena L1 to establish reference success rate (~52.7%) and verify your BrowserGym setup. 2) Ablation on structure preservation: Compare LineRetrieverAgent with direct removal vs. structure-preserving modes on WebArena, measuring both success rate and reduction percentage. 3) Retriever model size sweep: Test GPT-4.1-mini vs. GPT-4.1 as retriever across all three benchmarks with structure preservation enabled.

## Open Questions the Paper Calls Out

### Open Question 1
Can observation reduction techniques preserve the "representational integrity" of web pages without sacrificing compression rates? The authors note that aggressive pruning can create "degenerate" AxTrees outside the distribution the agent implicitly understands, and state that "observation reduction must not only aim to compress but also preserve the representational integrity." While "LineRetriever + Structure" improves performance, it drastically lowers compression (e.g., 30% vs 61% reduction on WorkArena), presenting a trade-off between retaining structural hierarchy and achieving efficiency.

### Open Question 2
Does the failure of embedding-based retrieval stem from a fundamental lack of planning capacity or insufficient training data? The authors hypothesize that embedding models "lack sufficient capacity to capture plan-relevant information," leading to poor performance compared to LLM-based retrieval. The paper compares a prompted LLM against a standard embedding model (text-embedding-3-small) but does not test if a specifically fine-tuned embedding model could learn planning-relevant features.

### Open Question 3
Is the performance drop in reduced observations caused by the loss of specific semantic information or the disruption of spatial/layout cues? The authors observe that removing lines disrupts structural coherence, but they also note that "reduction effectiveness depends more on the content of the observation rather than just the token count." The paper identifies the issue as a mix of content loss and structural disruption but does not isolate the specific contribution of layout/spatial cues versus semantic content in agent failure cases.

## Limitations
- The paper does not fully specify the structure-preserving algorithm details, particularly how parent element IDs and roles are identified and retained during pruning.
- The exact prompt template shown in Figure 4 contains formatting issues that prevent direct replication without reconstruction.
- The embedding baseline methodology (chunking strategy and similarity threshold) may significantly impact results, but implementation specifics are sparse.

## Confidence

**High Confidence:** The core finding that LLM-based retrieval significantly outperforms embedding-based retrieval (19.4% vs 49.1% on WorkArena L1) is well-supported by experimental results across three benchmarks.

**Medium Confidence:** The claim that structural preservation improves performance is supported but could be more rigorously quantifiedâ€”the tradeoff between compression ratio and success rate requires deeper analysis.

**Medium Confidence:** The assertion that small retrievers with structural context match larger retrievers without structure is demonstrated but needs broader validation across different model sizes and task types.

## Next Checks

1. Implement and compare the two post-processing variants (direct removal vs. structure-preserving) on a subset of tasks to measure the exact performance-cost tradeoff curve.

2. Conduct ablation studies removing action history from the retriever prompt to quantify its impact on selection quality across different task complexities.

3. Test LineRetriever on out-of-distribution web pages (different structures, non-standard UI patterns) to evaluate robustness beyond benchmark-curated tasks.