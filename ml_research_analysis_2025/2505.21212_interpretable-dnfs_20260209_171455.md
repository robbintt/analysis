---
ver: rpa2
title: Interpretable DNFs
arxiv_id: '2505.21212'
source_url: https://arxiv.org/abs/2505.21212
tags:
- terms
- decision
- term
- nested
- literals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the interpretability of boolean classifiers
  through the lens of abductive explanations, defining a classifier as interpretable
  if every decision can be explained using at most k features. The authors establish
  that such classifiers must have both themselves and their complements expressible
  as k-DNF formulas.
---

# Interpretable DNFs

## Quick Facts
- **arXiv ID:** 2505.21212
- **Source URL:** https://arxiv.org/abs/2505.21212
- **Reference count:** 26
- **Primary result:** k-AXp-interpretable classifiers are exactly those expressible as both a k-DNF and its complement as a k-DNF.

## Executive Summary
This paper establishes a formal connection between the interpretability of boolean classifiers and their ability to be expressed as small DNF formulas. Specifically, a classifier is interpretable if every decision has an abductive explanation of size at most k, which occurs if and only if both the classifier and its complement can be expressed as k-DNFs. The authors introduce nested k-DNFs, a novel family of interpretable models that extends beyond depth-k decision trees in expressiveness while maintaining the interpretability guarantee. Experiments show these models achieve accuracy comparable to CART decision trees while using fewer terms.

## Method Summary
The approach learns k-AXp-interpretable classifiers by constructing nested k-DNF formulas. The algorithm builds a k×k matrix of literals using a greedy heuristic that maximizes the difference between positive and negative example coverage. Terms are generated from combinations of literals in matrix rows, with a selection criterion that ensures compactness. The resulting model guarantees that both positive and negative decisions have explanations of size at most k. The method is compared against CART decision trees with depth k on 11 UCI/Kaggle datasets.

## Key Results
- A boolean classifier is k-AXp-interpretable if and only if both it and its complement can be expressed as k-DNF formulas
- Every k-AXp-interpretable classifier can be expressed using at most k^k terms
- Nested k-DNFs are more expressive than depth-k decision trees for certain dataset sizes
- The heuristic algorithm achieves accuracy comparable to CART while using fewer terms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A boolean classifier is interpretable (having short explanations for every decision) if and only if both the classifier and its complement can be expressed as k-DNF formulas.
- **Mechanism:** A positive decision by a k-DNF is explained by the single term that evaluates to true (size ≤ k). Conversely, for a negative decision, the complement κ̅ (also a k-DNF) provides the term explaining why the input was rejected. This dual-expressibility ensures both abductive and contrastive explanations remain small.
- **Core assumption:** Explanation size is the sole determinant of interpretability (specifically, k-AXp-interpretability), and users can understand conjunctions of up to k literals.
- **Evidence anchors:**
  - [abstract] "interpretable DNFs are those κ for which both κ and κ̅ can be expressed as DNFs composed of terms of bounded size."
  - [section 2, Proposition 1] "A binary boolean classifier... is k-AXp-interpretable if and only if both κ and its complement are expressible as k-DNFs."
  - [corpus] Weak direct link; "Proofs as Explanations" shares the theme of certificates but uses distinct logic.
- **Break condition:** If a classifier requires terms of size > k to express either the function or its complement, the mechanism fails, and the model is deemed not k-AXp-interpretable.

### Mechanism 2
- **Claim:** Restricting explanations to size k mathematically bounds the total number of unique explanations (terms) to k^k.
- **Mechanism:** The paper demonstrates that for a k-AXp-interpretable classifier, the number of prime implicants (distinct minimal explanations) cannot exceed k^k. This prevents the model from relying on an unmanageable number of different rules, ensuring a compact representation.
- **Core assumption:** The bound k^k remains tractable for human comprehension or memory storage for small k.
- **Evidence anchors:**
  - [abstract] "every k-AXp-interpretable classifier can be expressed using at most k^k terms."
  - [section 3, Theorem 1] Proof that |D_κ| < k^k or (1/k)^k · |D_κ| ≤ 1.
  - [corpus] Not explicitly supported by neighbor papers.
- **Break condition:** If k is not small (e.g., k=10 resulting in 10^10 terms), the theoretical guarantee of compactness fails to result in a practical model.

### Mechanism 3
- **Claim:** Nested k-DNFs satisfy the interpretability condition (mim ≤ k) via a specific matrix structure, providing expressiveness superior to depth-k decision trees for certain datasets.
- **Mechanism:** By organizing literals into a k×k matrix and forming terms via selections from rows, the structure prevents large "induced matchings" in the literal-term bipartite graph. This graph-theoretic property guarantees that minimal transversals (explanations for the complement) remain small.
- **Core assumption:** The dataset structure aligns with the nested dependencies allowed by the matrix structure, and the heuristic can find the optimal matrix configuration.
- **Evidence anchors:**
  - [section 4, Proposition 2] "Every boolean function expressible as a nested k-DNF formula is k-AXp-interpretable."
  - [section 5, Proposition 4] Shows N_nested(k, n) > N_DT(k, n) for certain constraints.
  - [corpus] Weak; related papers focus on explaining black boxes (CAMs), not structurally interpretable white boxes.
- **Break condition:** If the function depends on more than k² distinct literals (the matrix capacity), or if the function requires "non-nested" dependencies, this specific architecture fails to represent it.

## Foundational Learning

- **Concept: DNF (Disjunctive Normal Form)**
  - **Why needed here:** This is the fundamental representation language of the paper. Understanding that a DNF is an OR of ANDs (disjunction of terms) is required to grasp how explanations are derived directly from terms.
  - **Quick check question:** Can you identify the "term" in the DNF (A ∧ B) ∨ (C ∧ D) that explains an input where A=1, B=1?

- **Concept: Abductive Explanation (AXp)**
  - **Why needed here:** The paper equates "interpretability" with the existence of short AXps. One must understand that an AXp is a minimal set of features sufficient to trigger a specific prediction.
  - **Quick check question:** If a model predicts "Yes" based on features {X, Y, Z}, and changing only Z doesn't change the prediction, is {X, Y, Z} an abductive explanation?

- **Concept: Certificate Complexity**
  - **Why needed here:** The paper redefines interpretable classifiers as those with certificate complexity ≤ k. This connects the machine learning concept to computational learning theory.
  - **Quick check question:** Does high certificate complexity imply that a classifier is easy or hard to interpret using the paper's definition?

## Architecture Onboarding

- **Component map:** Input (boolean features, target, k) -> Matrix L (k×k literal grid) -> Heuristic Constructor (greedy literal selection) -> Term Generator (create/prune terms) -> Output (nested k-DNF classifier)

- **Critical path:** The construction of Matrix L (Algorithm 1). The heuristic selects literals based on a "greedy" objective function G (Class 1 coverage - Class 0 coverage). If this initialization is poor, the subsequent term generation cannot recover, leading to low accuracy.

- **Design tradeoffs:**
  - Vs Decision Trees: Nested k-DNFs can express functions requiring decision trees of depth >k, but they are not closed under complementation (learning κ vs κ̅ yields different performances).
  - Capacity: The model is strictly limited to k² distinct literals, potentially limiting expressiveness compared to deeper trees on wide datasets.

- **Failure signatures:**
  - High Variability: The paper notes high variance in test accuracy across runs due to the heuristic nature of the matrix construction.
  - Literal Saturation: If features required for classification exceed the k² matrix slots, accuracy degrades.

- **First 3 experiments:**
  1. **Verify Ground Truth:** Test the heuristic on a synthetic dataset κ(a, b, c, d) = (a ∧ b) ∨ (c ∧ d) to confirm it finds the 2-term DNF (paper notes CART requires depth 4 here, while nested DNF needs k=2).
  2. **Ablation on k:** Run the algorithm on UCI datasets (e.g., Car-evaluation, Tic-tac-toe) for k ∈ {2, 3, 4, 5, 6} to plot the accuracy vs. complexity trade-off compared to CART.
  3. **Complement Asymmetry Check:** Train on the positive class, then train on the negative class (swapping labels), and compare the resulting model sizes and accuracies to verify the asymmetry discussed in the paper.

## Open Questions the Paper Calls Out

- **Question:** Do there exist more general families of interpretable DNFs that achieve better accuracy than decision trees while maintaining k-AXp-interpretability?
  - **Basis in paper:** [explicit] The authors state in the conclusion: "An intriguing open question is whether there exist more general families of interpretable DNFs that could achieve better accuracy than decision trees."
  - **Why unresolved:** Nested k-DNFs and depth-k decision trees are just two families within the broader class of k-AXp-interpretable classifiers; the full landscape (Figure 1) includes unexplored regions.
  - **What evidence would resolve it:** Identification of a new family of k-DNF formulas (beyond nested k-DNFs and decision trees) with formal interpretability guarantees and superior empirical accuracy on benchmark datasets.

- **Question:** How do optimal nested k-DNFs compare to optimal depth-k decision trees in terms of accuracy and model size?
  - **Basis in paper:** [explicit] The conclusion notes: "It would be interesting to compare optimal nested k-DNFs and optimal depth-k decision trees."
  - **Why unresolved:** The current experiments use a heuristic algorithm for nested k-DNFs and CART for decision trees—neither guarantees globally optimal models.
  - **What evidence would resolve it:** Experiments using exact or optimal learning algorithms for both model classes, comparing test accuracy and number of terms/leaves across datasets.

- **Question:** Can the limitation that nested k-DNFs cannot contain more than k² distinct literals be overcome while preserving interpretability guarantees?
  - **Basis in paper:** [explicit] The authors note: "nested k-DNFs cannot contain more than k² distinct literals. These limitations come from our definitions and do not arise from fundamental technical reasons, so we believe there is ample room for further improvement."
  - **Why unresolved:** The k² bound emerges from the matrix structure of nested k-DNFs, but k-AXp-interpretability itself does not impose this restriction.
  - **What evidence would resolve it:** Construction of an extended nested k-DNF variant (or alternative k-AXp-interpretable family) that handles more than k² literals while guaranteeing bounded explanation size.

- **Question:** Can efficient learning algorithms be developed for the broader family of k-DNFs with bounded induced matching size (mim(G_D) ≤ k)?
  - **Basis in paper:** [inferred] The paper notes this family is defined by a graph-theoretic condition (Lemma 2) but "its definition is not constructive: without a clear structure, it is difficult to design efficient heuristics for learning formulas of this kind directly from data."
  - **Why unresolved:** The induced matching condition is a sufficient criterion for interpretability, but lacks the explicit structure that enabled the nested k-DNF learning heuristic.
  - **What evidence would resolve it:** A polynomial-time or practically efficient algorithm that learns k-DNF classifiers guaranteed to satisfy mim(G_D) ≤ k, with empirical evaluation showing competitive accuracy.

## Limitations

- The $k^k$ bound on terms is theoretical and not empirically validated for practical k values
- The claim of nested k-DNFs being "more expressive" than depth-k decision trees is combinatorially proven but not exhaustively tested
- The heuristic nature of matrix construction introduces high variance in model performance
- The model is limited to k² distinct literals, which may constrain expressiveness on wide datasets

## Confidence

- **High:** The theoretical equivalence between k-AXp-interpretability and dual k-DNF expressibility (Mechanism 1)
- **Medium:** The k^k bound on terms (Mechanism 2) is mathematically proven but not empirically validated for practical k
- **Medium:** The expressiveness advantage of nested k-DNFs over decision trees is combinatorially established but lacks extensive empirical demonstration across diverse function families

## Next Checks

1. **Ground Truth Verification:** Implement the heuristic on the synthetic DNF (a ∧ b) ∨ (c ∧ d) and verify it recovers the correct 2-term model, contrasting with CART's depth-4 solution.

2. **Expressiveness Testing:** Systematically test the heuristic on a suite of synthetic boolean functions (e.g., parity, threshold functions) to measure when nested k-DNFs succeed where depth-k trees fail.

3. **Complement Asymmetry Analysis:** Train the algorithm on the positive and negative classes of a dataset separately, comparing the resulting model sizes and accuracies to confirm the asymmetry in learning κ vs κ̅.