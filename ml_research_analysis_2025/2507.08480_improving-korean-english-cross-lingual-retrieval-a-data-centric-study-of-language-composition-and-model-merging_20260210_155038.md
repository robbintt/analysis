---
ver: rpa2
title: 'Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study of
  Language Composition and Model Merging'
arxiv_id: '2507.08480'
source_url: https://arxiv.org/abs/2507.08480
tags:
- base
- koenen
- query
- enkoko
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of training data language composition
  on cross-lingual and mono-lingual retrieval performance. The authors construct linguistically
  parallel Korean-English datasets and systematically vary the language combinations
  in training data for embedding models.
---

# Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study of Language Composition and Model Merging

## Quick Facts
- arXiv ID: 2507.08480
- Source URL: https://arxiv.org/abs/2507.08480
- Authors: Youngjoon Jang; Junyoung Son; Taemin Lee; Seongtae Hong; Heuiseok Lim
- Reference count: 25
- Key outcome: This study investigates the impact of training data language composition on cross-lingual and mono-lingual retrieval performance. The authors construct linguistically parallel Korean-English datasets and systematically vary the language combinations in training data for embedding models. Experiments reveal a trade-off: certain language pairs enhance cross-lingual retrieval but degrade mono-lingual retrieval performance. To address this, the authors apply weight-averaged model merging, which effectively mitigates the trade-off, maintaining strong cross-lingual performance while preserving or even improving mono-lingual retrieval capabilities.

## Executive Summary
This study explores how the language composition of training data impacts cross-lingual and mono-lingual retrieval performance for Korean-English. The authors construct synthetic parallel datasets with varied query-positive-negative language combinations and fine-tune multilingual embedding models. Key findings reveal that specific training data configurations enhance cross-lingual retrieval but degrade mono-lingual performance, creating a trade-off. To mitigate this, the authors propose weight-averaged model merging, which successfully balances both retrieval tasks. Experiments demonstrate that merged models outperform or match the best mono-lingual models while preserving strong cross-lingual retrieval capabilities.

## Method Summary
The study fine-tunes multilingual embedding models (bge-m3, multilingual-e5-large/base, gte-multilingual-base) using synthetic {query, positive, hard_negative} triples generated via GPT-4o. Eight language configurations (D(l1l2l3)) are tested, where l1, l2, and l3 represent the languages of the query, positive, and negative respectively. Hard negatives are mined using NV-Embed-v2, selecting 5 per query from ranks 50-300 with specific similarity constraints. Models are evaluated on Belebele and StrategyQA (Korean-English parallel) using NDCG@10. Model merging is performed via simple weight averaging between a CLIR-optimized model and the best mono-lingual performer.

## Key Results
- Training with query-positive pairs matching the target cross-lingual direction improves CLIR performance (e.g., D(koenen) vs D(enkoen)).
- "Query-Negatives Match" configurations (QL ≠ PL & QL = NL) degrade mono-lingual IR performance.
- Weight-averaged model merging effectively mitigates the CLIR vs. mono-lingual trade-off, maintaining strong cross-lingual performance while preserving or improving mono-lingual retrieval.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training with query-positive pairs that match the target cross-lingual direction improves CLIR performance.
- Mechanism: Cross-lingual query-positive pairs force the model to learn direct semantic mappings across the language barrier, reducing reliance on language-specific features.
- Core assumption: The model can form a shared semantic space when exposed to aligned cross-lingual examples during contrastive learning.
- Evidence anchors:
  - [abstract] "...experiments reveal a trade-off: certain language pairs enhance cross-lingual retrieval but degrade mono-lingual retrieval performance."
  - [section] Section 4.1: "Training with query-positive pairs that mirror the target language direction... consistently improves performance... D(enkoen) achieves a higher AVG score (86.64) than... D(enenko) (84.93)."
  - [corpus] Corpus signals confirm this is a known strategy, e.g., "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models" explores retrieval with multilingual models.
- Break condition: If the base model has extremely weak cross-lingual alignment, this method alone may be insufficient without significant pre-training.

### Mechanism 2
- Claim: "Query-Negatives Match" (QL != PL & QL = NL) in training data degrades Mono-Lingual IR performance.
- Mechanism: The model learns to associate the query language primarily with the negative signal (pushing away), while the positive is in a different language. This confuses mono-lingual relevance discrimination where query and positive should be close.
- Core assumption: The contrastive loss creates strong associations between the query's language and the semantic operation of "rejection" when negatives share the query's language.
- Evidence anchors:
  - [abstract] "...while Mono-Lingual IR performance declines."
  - [section] Section 4.2: "Degradation by Query-Negatives Match... training on D(koenko) (80.56) and D(enkoen) (84.59) exhibits substantial performance degradation compared to the base model (87.06)."
  - [corpus] Corpus evidence on this specific negative language interaction mechanism is weak or missing in provided neighbor summaries.
- Break condition: This degradation may be less severe if the model's mono-lingual capability was already very strong and resilient, or if the training volume is low.

### Mechanism 3
- Claim: Weight-Averaged Model Merging mitigates the CLIR vs. Mono-Lingual IR trade-off.
- Mechanism: Averaging weights from a CLIR-optimized model and a Mono-Lingual-optimized model combines their specialized representations into a single model that retains capabilities from both, finding a more favorable point in the loss landscape.
- Core assumption: The features learned for CLIR and Mono-Lingual IR reside in similar but not perfectly overlapping regions of the parameter space, allowing interpolation.
- Evidence anchors:
  - [abstract] "...apply weight-averaged model merging, which effectively mitigates the trade-off, maintaining strong cross-lingual performance while preserving or even improving mono-lingual retrieval capabilities."
  - [section] Section 4.3, Table 2: "Merging Effects on Mono-Lingual IR... increases Mono-Lingual IR score by approximately 1 point... in some cases even surpassing the original base model’s performance."
  - [corpus] Corpus signals mention "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs" but do not explicitly mention model merging as a key strategy in provided summaries.
- Break condition: Merging two highly divergent models (e.g., with different architectures or extremely different training regimes) may fail to produce coherent outputs.

## Foundational Learning

- Concept: **Contrastive Learning with Hard Negatives**
  - Why needed here: The entire study relies on fine-tuning embedding models with a contrastive loss using triplets (query, positive, hard_negative). Understanding how the loss pushes the positive closer and the negative apart is fundamental.
  - Quick check question: In a triplet (Q, P, N), how does the loss function change if N is very similar to Q but not the correct answer vs. if N is randomly sampled?

- Concept: **Cross-Lingual Semantic Alignment**
  - Why needed here: The core problem is making a query in Korean "close" to a relevant document in English in the embedding space. This concept explains why cross-lingual training data helps bridge that gap.
  - Quick check question: Why does a multilingual model pre-trained on parallel corpora generally perform better at CLIR than a model trained on mono-lingual data alone?

- Concept: **Model Merging / Weight Averaging**
  - Why needed here: This is the solution proposed to fix the trade-off. Understanding that model weights can be interpolated to combine capabilities is essential for applying this fix.
  - Quick check question: If you have Model A (great at English) and Model B (great at Korean), what is a simple way to create a new model that might be good at both?

## Architecture Onboarding

- Component map: Data Generator (LLM + KDC) -> Fine-tuning Loop (Contrastive Loss on 8 dataset variants) -> Evaluation (NDCG@10 on Belebele/StrategyQA) -> Model Merger (Weight Averaging).
- Critical path: The most critical path for a new engineer is understanding how the `D(koenko)` (Korean Query, English Positive, Korean Negative) data configuration differs from `D(koenen)` (Korean Query, English Positive, English Negative) and how that specific change drives the trade-off.
- Design tradeoffs: The main tradeoff is between peak CLIR performance and preserved Mono-Lingual performance. Specialized models (fine-tuned on a single dataset variant) achieve higher peak scores on their target task but fail on the other. Merged models offer a balanced, robust performance profile at the cost of not being the absolute best at any single task.
- Failure signatures:
  - **CLIR performance is low:** Check if training data's query-positive language matches the target CLIR direction (e.g., use `D(koenen)` for `T(ko-en)`).
  - **Mono-Lingual IR performance drops:** Check for the "Query-Negatives Match" condition. If your data has `QL != PL` and `QL == NL`, expect degradation.
  - **Merged model is unstable or performs poorly:** The models being merged may be too divergent. Ensure they share the same base architecture and were fine-tuned with compatible hyperparameters.
- First 3 experiments:
  1. **Reproduce the trade-off:** Train a base model (e.g., `bge-m3`) on `D(koenko)` and another on `D(koenen)`. Evaluate both on `T(ko-en)` and `T(ko-ko)` to confirm the trade-off.
  2. **Test model merging:** Take the two models from experiment #1, merge their weights, and re-evaluate. Compare the merged model's scores to the individual models and the base model.
  3. **Ablate hard negative language:** Create a simplified dataset where only the negative language varies (e.g., `D(koenko)` vs. `D(koenen)`) with all other factors constant. Train and evaluate to isolate the specific impact of the negative's language on mono-lingual performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified trade-offs between CLIR and Mono-Lingual IR performance generalize to language pairs with significant typological or script differences outside of Korean-English?
- Basis in paper: [explicit] The authors state in the Limitations section that "generalizability... to other language pairs, particularly those with significant typological or cultural differences, require further validation."
- Why unresolved: The study restricted its scope to Korean and English; it is unknown if the "Query-Negatives Match" degradation applies to languages with different morphological structures or non-Latin scripts.
- What evidence would resolve it: Replicating the experimental pipeline on diverse language pairs (e.g., English-Arabic, Chinese-Russian) to observe if similar data composition trade-offs emerge.

### Open Question 2
- Question: Can sophisticated merging techniques, such as Fisher-weighted averaging or differential layer weighting, outperform the simple weight-averaged merging used in this study?
- Basis in paper: [explicit] The authors acknowledge they employed a "relatively straightforward Weight-Averaged Model Merging approach" and suggest that exploring "more sophisticated... merging methodologies... might yield different outcomes."
- Why unresolved: The paper only validates a simple averaging technique; complex merging methods might better preserve mono-lingual capabilities while enhancing cross-lingual alignment.
- What evidence would resolve it: Comparative experiments applying advanced merging algorithms (e.g., Task Arithmetic, Fisher merging) to the fine-tuned models and measuring the resulting performance trade-offs.

### Open Question 3
- Question: Do the negative impacts of "Query-Negatives Match" training configurations persist when using real-world, human-generated data instead of synthetic LLM-generated data?
- Basis in paper: [explicit] The Limitations section notes that synthetic data "may not fully capture the nuances and breadth of real-world user-generated query-document pairs."
- Why unresolved: LLM-generated datasets may lack the noise, ambiguity, or domain diversity of organic data, potentially masking how robust the observed trade-offs are in production environments.
- What evidence would resolve it: conducting the same language composition experiments using human-curated parallel corpora (e.g., domain-specific web scrapes or translated benchmarks) to verify the degradation patterns.

## Limitations
- The study relies on synthetic data generated by GPT-4o, which may not fully represent real-world retrieval tasks.
- The hard-negative mining process depends on a specific embedding model (NV-Embed-v2) and ranking range, which may not be optimal.
- The model merging approach uses simple weight averaging, which may not capture more complex interactions between CLIR and mono-lingual representations.

## Confidence
- **High**: The existence of the trade-off between CLIR and mono-lingual performance, and the effectiveness of weight-averaged model merging to mitigate it.
- **Medium**: The specific mechanisms (Query-Negatives Match degradation, Cross-Lingual Query-Positive alignment) driving the trade-off, as some corpus signals are weak.
- **Low**: The generalisability of the specific dataset (Belebele/StrategyQA) and the optimal configuration of training data for all cross-lingual retrieval scenarios.

## Next Checks
1. **Replicate the trade-off with a different dataset**: Apply the same 8 training data configurations to a different Korean-English retrieval dataset (e.g., a standard IR benchmark) to confirm the robustness of the observed trade-off.
2. **Ablate the negative language effect**: Create a minimal experimental setup where only the negative language varies (keeping query and positive constant) to isolate and confirm the "Query-Negatives Match" degradation mechanism.
3. **Test alternative merging strategies**: Compare the simple weight-averaged merging with other model combination techniques (e.g., adapter fusion, knowledge distillation) to assess if the mitigation effect is specific to averaging or a more general principle.