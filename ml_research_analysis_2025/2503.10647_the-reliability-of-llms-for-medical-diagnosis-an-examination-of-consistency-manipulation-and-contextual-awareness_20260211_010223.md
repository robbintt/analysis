---
ver: rpa2
title: 'The Reliability of LLMs for Medical Diagnosis: An Examination of Consistency,
  Manipulation, and Contextual Awareness'
arxiv_id: '2503.10647'
source_url: https://arxiv.org/abs/2503.10647
tags:
- diagnostic
- clinical
- llms
- contextual
- clinically
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the diagnostic reliability of Large Language
  Models (LLMs) in medical contexts by evaluating consistency, susceptibility to manipulation,
  and contextual awareness using 52 clinical scenarios. Both Gemini and ChatGPT demonstrated
  perfect diagnostic consistency (100%) for identical clinical inputs.
---

# The Reliability of LLMs for Medical Diagnosis: An Examination of Consistency, Manipulation, and Contextual Awareness

## Quick Facts
- arXiv ID: 2503.10647
- Source URL: https://arxiv.org/abs/2503.10647
- Authors: Krishna Subedi
- Reference count: 40
- Key outcome: LLMs show perfect consistency but high susceptibility to manipulation and limited contextual awareness in medical diagnosis

## Executive Summary
This study evaluated the diagnostic reliability of Large Language Models (LLMs) in medical contexts across three dimensions: consistency, susceptibility to manipulation, and contextual awareness. Using 52 clinical scenarios spanning 8 medical specialties, both Gemini 2.0 Flash and ChatGPT-4o demonstrated perfect diagnostic consistency (100%) when presented with identical clinical inputs. However, both models showed significant vulnerability to manipulation, with Gemini at 40% and ChatGPT at 30% diagnosis change rates when irrelevant details were introduced. ChatGPT exhibited higher context influence (77.8% vs Gemini's 55.6%), though qualitative physician review found Gemini made more clinically appropriate context-driven changes.

The findings reveal that while LLMs exhibit algorithmic consistency, their vulnerability to manipulation and limited contextual awareness pose significant challenges for real-world clinical deployment. Both models displayed anchoring bias and struggled with nuanced contextual integration, often overemphasizing salient data while underweighting contradictory evidence. These limitations necessitate human oversight and robust safeguards before clinical implementation of LLMs in medical diagnosis.

## Method Summary
The study employed 52 de novo clinical scenarios sourced from UpToDate and DynaMed, structured with comprehensive patient data including demographics, medical history, symptoms, vital signs, physical exam findings, test results, differential diagnoses, and final diagnoses. Three experimental protocols were conducted: consistency testing with 624 queries per model (52 scenarios × 4 variations × 3 repetitions), manipulation testing using 10 scenarios with 6 categories of irrelevant details, and contextual awareness testing with 2 baseline cases expanded to 9 contextually varied versions. Standardized prompts instructed models to provide only final diagnoses. Physician reviewers independently categorized context-driven changes as appropriate, inappropriate, or ambiguous (Cohen's κ=0.85).

## Key Results
- Both Gemini and ChatGPT achieved perfect diagnostic consistency (100%) across all identical clinical inputs and repetitions
- Gemini showed 40% susceptibility to manipulation versus ChatGPT's 30% when irrelevant details were introduced
- ChatGPT demonstrated higher context influence (77.8%) compared to Gemini (55.6%), though Gemini made more clinically appropriate context-driven changes (66.7% vs 55.6%)

## Why This Works (Mechanism)
The study reveals that LLMs maintain algorithmic consistency through deterministic processing of identical inputs, but their decision-making processes are vulnerable to contextual manipulation and biased integration of information. The models exhibit anchoring bias by over-relying on salient clinical data while underweighting contradictory evidence, and they struggle with nuanced contextual integration that requires weighing multiple factors simultaneously. This mechanistic vulnerability stems from the models' training on statistical patterns rather than true clinical reasoning, making them susceptible to irrelevant information that exploits their pattern-matching capabilities.

## Foundational Learning
- **Clinical scenario structure**: Standardized patient data format with demographics, symptoms, vitals, and diagnoses - needed for consistent model input; quick check: verify all 52 scenarios contain complete fields
- **Diagnosis equivalence determination**: Physician assessment of exact matches vs clinically synonymous diagnoses - needed for consistency metrics; quick check: establish explicit mapping criteria between diagnostic terms
- **Contextual variation design**: Systematic modification of scenarios to test context influence - needed for measuring contextual awareness; quick check: verify 9 variants per baseline case follow consistent variation patterns
- **Manipulation scenario selection**: Introduction of irrelevant details across 6 categories - needed for susceptibility testing; quick check: confirm each category represents distinct types of misleading information
- **Inter-rater reliability**: Cohen's κ coefficient for physician agreement - needed for validating qualitative assessments; quick check: target κ≥0.80 for clinical appropriateness ratings
- **API parameter control**: Temperature, top_p, and system prompt settings - needed for reproducible generation; quick check: document and standardize all LLM API parameters

## Architecture Onboarding

**Component Map:** Clinical Scenarios -> LLM API Calls -> Raw Outputs -> Consistency Analysis -> Manipulation Analysis -> Contextual Analysis -> Physician Review

**Critical Path:** Scenario generation → API query execution → Output collection → Physician categorization → Statistical analysis → Interpretation

**Design Tradeoffs:** Perfect consistency achieved at cost of vulnerability to manipulation; higher context influence in ChatGPT comes with less clinically appropriate changes versus Gemini's more conservative approach

**Failure Signatures:** Diagnosis changes without clinical justification, overreliance on single data points, failure to integrate contradictory information, binary thinking in complex cases

**3 First Experiments:**
1. Run consistency test with temperature=0 to verify perfect consistency is robust to generation parameters
2. Execute manipulation protocol with expanded scenario set to validate 30-40% susceptibility rates
3. Implement standardized clinical equivalence mapping to objectively verify physician assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Exact LLM API parameters (temperature, top_p) were not specified, potentially affecting reproducibility
- Selection methodology for manipulation and contextual scenarios was only described as "random" without reporting seeds or stratification
- Clinical equivalence determinations relied on physician judgment without standardized diagnostic mapping

## Confidence

**High Confidence:**
- Perfect algorithmic consistency across all models and repetitions

**Medium Confidence:**
- Susceptibility to manipulation rates (30-40%) dependent on scenario representativeness
- Context influence rates and clinical appropriateness ratings subject to physician subjectivity

## Next Checks
1. Replicate consistency test with explicitly defined API parameters (temperature=0, top_p=1) to confirm robustness
2. Conduct expanded manipulation study using stratified sampling across all 52 scenarios to verify susceptibility rates
3. Implement standardized clinical equivalence mapping (SNOMED-CT/ICD-10 crosswalks) to objectively verify physician assessments