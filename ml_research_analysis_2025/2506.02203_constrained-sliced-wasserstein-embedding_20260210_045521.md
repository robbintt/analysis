---
ver: rpa2
title: Constrained Sliced Wasserstein Embedding
arxiv_id: '2506.02203'
source_url: https://arxiv.org/abs/2506.02203
tags:
- learning
- constrained
- sliced
- wasserstein
- slices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a constrained learning framework to optimize
  slicing directions in Sliced Wasserstein Embeddings (SWE) by enforcing that one-dimensional
  transport plans approximate optimal plans in the original high-dimensional space.
  The authors use a primal-dual optimization approach with relaxed constraints to
  achieve this, making the method applicable to various pooling tasks.
---

# Constrained Sliced Wasserstein Embedding

## Quick Facts
- arXiv ID: 2506.02203
- Source URL: https://arxiv.org/abs/2506.02203
- Reference count: 40
- Primary result: Constrained SWE achieves better performance with fewer slices compared to unconstrained SWE, improving efficiency and downstream accuracy across images, point clouds, and protein sequences.

## Executive Summary
This paper proposes a constrained learning framework to optimize slicing directions in Sliced Wasserstein Embeddings (SWE) by enforcing that one-dimensional transport plans approximate optimal plans in the original high-dimensional space. The authors use a primal-dual optimization approach with relaxed constraints to achieve this, making the method applicable to various pooling tasks. Experiments on images, point clouds, and protein sequences demonstrate that constrained SWE achieves better performance with fewer slices compared to unconstrained SWE, improving both efficiency and downstream accuracy.

## Method Summary
The method introduces Constrained Sliced Wasserstein Embeddings (C-SWE) by optimizing slicing directions through a primal-dual framework that enforces geometric constraints. The key innovation is constraining the 1D transport plans to approximate the optimal plan in the original high-dimensional space via Sliced Wasserstein Generalized Geodesics (SWGG) dissimilarity bounds. This is achieved through differentiable softsort approximations (replacing non-differentiable argsort), primal-dual optimization with slack variables, and dynamic dual variable updates that adaptively balance constraint satisfaction with task performance.

## Key Results
- Constrained SWE matches unconstrained SWE performance using approximately 2× fewer slices
- On Tiny ImageNet, constrained SWE with L=128 slices achieves comparable accuracy to unconstrained SWE with L=256 slices
- Point cloud classification on ModelNet40 shows consistent improvements across varying slice counts
- Protein sequence experiments demonstrate constrained SWE maintains advantage even with limited slice budgets

## Why This Works (Mechanism)

### Mechanism 1
Constraining slicing directions via Sliced Wasserstein Generalized Geodesics (SWGG) improves geometric fidelity of embeddings compared to random or task-optimized slicing. Standard SWE projects high-dimensional data onto 1D lines, and if these lines are uninformative, the lifted transport plan deviates significantly from the true optimal transport plan. This method constrains the optimization such that the SWGG dissimilarity is bounded, forcing learned slices to preserve the topology of the original high-dimensional space better than unconstrained slices.

### Mechanism 2
A primal-dual training algorithm with slack variables allows for adaptive balance between task performance and geometric constraints. Rather than treating the geometric constraint as fixed regularization, this method formulates the problem as constrained optimization where dual variables dynamically scale the penalty for violating the SWGG constraint. If a slice violates the constraint, the dual variable increases, forcing the optimizer to prioritize geometric fidelity. Slack variables ensure feasibility even if initial constraint bounds are impossible to satisfy perfectly.

### Mechanism 3
Replacing hard permutation sorting with differentiable "softsort" approximation enables gradient-based learning of slicing directions. The SWGG dissimilarity depends on permutation matrices that align sorted inputs, but standard argsort is non-differentiable. The paper substitutes this with a continuous softmax-based approximation, allowing gradients from the SWGG constraint to flow back to slicing directions, enabling them to be steered toward geometrically meaningful orientations.

## Foundational Learning

- **Sliced Wasserstein Distance**: Base metric that projects d-dimensional distributions to 1D to solve OT cheaply ($O(M \log M)$) rather than expensively ($O(M^3)$). Quick check: Why is calculating Wasserstein distance in 1D significantly faster than in 2D or higher dimensions?

- **Lagrangian Duality / Primal-Dual Optimization**: Core training framework using dual variables as dynamic price tags on constraint violations rather than fixed hyperparameters. Quick check: In this context, if the SWGG constraint is consistently satisfied (slacks are zero), what happens to the dual variable λ?

- **Permutation Invariance in Set Pooling**: Mechanism for pooling (e.g., point clouds, image tokens) where order of input points shouldn't change the embedding. Quick check: How does sorting the projected embeddings (via Monge coupling) ensure the final representation is invariant to the initial order of input tokens?

## Architecture Onboarding

- **Component map**: Backbone (Frozen) -> Slicer (Θ) -> Softsort Operator -> SWGG Calculator -> Primal-Dual Optimizer
- **Critical path**: The gradient flow from the SWGG Calculator through the Softsort back to the Slicer. If softsort is misconfigured, the slicer cannot learn geometric constraints.
- **Design tradeoffs**:
  - Hard vs. Soft Sort: Softsort is O(M²) vs O(M log M) for hard sort, increasing training memory/time but required for learning
  - Constraint Bound (ε): Lower bounds improve geometric fidelity but risk infeasibility (requiring slack)
  - Number of Slices (L): Fewer slices are efficient but lose information; this method aims to maximize utility of small L
- **Failure signatures**:
  - Slack Explosion: Slack variables (s) grow indefinitely → constraints too tight (ε too low) or learning rate ηs too high
  - Dual Collapse: λ drops to zero → constraint too loose, model ignores geometry (behaves like unconstrained SWE)
  - Memory OOM: Softsort computes M×M matrix; large reference size M crashes GPU memory
- **First 3 experiments**:
  1. Overfit Check: Train on Tiny ImageNet subset with L=4 slices. Verify constrained SWE overfits less than unconstrained SWE.
  2. Sweep Constraint ε: Run binary search on ε values. Identify elbow where SWGG violations stop decreasing and slacks start increasing.
  3. Slice Efficiency: Compare test accuracy vs. L (e.g., 4, 8, 16, 32). Confirm constrained SWE matches unconstrained performance using ≈2× fewer slices.

## Open Questions the Paper Calls Out

- Can utilizing dual or slack variables as slice-wise importance weights during aggregation improve the expressiveness of constrained SWE? The authors note that current embeddings are flattened across slices, but more expressive aggregation strategies may improve performance.

- Does a hybrid optimization approach that balances dissimilarity maximization (post-slicing) with SWGG alignment (pre-slicing) lead to stronger generalization capabilities? The authors propose that hybrid approaches may lead to stronger generalization.

- How can the constrained formulation be stabilized to prevent infeasibility as the number of slices (L) increases in high-dimensional tasks? The authors note that gains over unconstrained SWE fade as L increases in protein sequence experiments, potentially due to the constrained optimization problem becoming infeasible.

- Do orthogonality or Max-SW-style constraints provide additional benefits regarding slice heterogeneity or informativeness compared to the proposed SWGG constraints? The authors mention that the framework supports additional constraint types but leave this evaluation as future work.

## Limitations

- The method relies on hyperparameter ε for constraint bounds, requiring grid search that introduces tuning burden comparable to standard regularization techniques.
- The softsort approximation creates significant computational overhead (O(M²) vs O(M log M)), with runtime impact not quantified relative to unconstrained SWE.
- The paper lacks theoretical convergence guarantees specific to this constrained slicing problem, with the relationship between SWGG satisfaction and task performance remaining empirical.

## Confidence

- **High Confidence**: The claim that constrained SWE achieves better efficiency (fewer slices for comparable accuracy) is supported by direct experimental comparisons in Tables 2 and 3.
- **Medium Confidence**: The claim that constrained SWE "approximates optimal plans in the original space" is supported by the SWGG constraint mechanism, but quality depends heavily on ε selection.
- **Low Confidence**: The claim about generalization to arbitrary pooling tasks beyond the three tested domains is not substantiated, with behavior on different dataset characteristics remaining unknown.

## Next Checks

1. **Constraint Sensitivity Analysis**: Systematically vary ε across orders of magnitude (e.g., 0.1, 1, 10, 100) on Tiny ImageNet and plot test accuracy, average slack values, and SWGG constraint violations to quantify the tradeoff between geometric fidelity and task performance.

2. **Computational Overhead Benchmark**: Measure wall-clock training time per epoch for both constrained and unconstrained SWE across different reference set sizes (M=64, 128, 196, 256) on a fixed GPU, including memory usage statistics to quantify the practical cost of the softsort approximation.

3. **Ablation of Dual Variables**: Run experiments with fixed λ (treating constraint as standard regularization), no constraint (standard SWE), and full primal-dual optimization to validate whether the adaptive dual variable mechanism provides benefits over simpler regularization approaches.