---
ver: rpa2
title: 'ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest
  X-ray Understanding'
arxiv_id: '2506.04353'
source_url: https://arxiv.org/abs/2506.04353
tags:
- medical
- question
- rexvqa
- assessment
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ReXVQA, a large-scale visual question answering\
  \ (VQA) benchmark for chest X-ray interpretation comprising ~696K questions paired\
  \ with 160K studies. It evaluates five radiological reasoning tasks\u2014presence,\
  \ location, negation, differential diagnosis, and geometric analysis\u2014using\
  \ eight multimodal LLMs."
---

# ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding

## Quick Facts
- **arXiv ID:** 2506.04353
- **Source URL:** https://arxiv.org/abs/2506.04353
- **Reference count:** 16
- **Key outcome:** AI surpasses expert human performance in chest X-ray interpretation for the first time

## Executive Summary
ReXVQA introduces a large-scale VQA benchmark for chest X-ray interpretation comprising ~696K questions paired with 160K studies. The benchmark evaluates five radiological reasoning tasks—presence, location, negation, differential diagnosis, and geometric analysis—using eight multimodal LLMs. The best-performing model, MedGemma, achieves 83.24% overall accuracy, surpassing expert human readers (77.27%) in a 200-case reader study. This marks the first instance where AI exceeds expert-level human performance in chest X-ray interpretation. ReXVQA provides fine-grained evaluation splits, structured explanations, and category-level breakdowns, establishing a new standard for generalist radiological AI evaluation beyond narrow pathology classification.

## Method Summary
ReXVQA uses a three-layer pipeline to generate MCQs from radiology reports: Report-to-Bullets transformation via GPT-4o, MCQ generation, and validation through ClinicalBERT and expert review. The benchmark includes 572,952 training pairs, 40,826 public test pairs, and 41,007 private test pairs. Models are evaluated on five radiological reasoning skills across 15+ anatomical categories. The top-performing MedGemma-4B-it model uses a SigLIP encoder pre-trained on medical data. Images are provided as 1/4 resolution PNGs, and evaluation focuses on multiple-choice accuracy across different reasoning categories.

## Key Results
- MedGemma achieves 83.24% overall accuracy, surpassing expert human readers (77.27%)
- AI-human agreement measured at κ=0.3-0.5, while human-human agreement is higher
- MedGemma excels in presence detection (85.21%) but struggles with geometric reasoning (76.71%)
- Generalist models like LLaVA fail on technical image quality questions (10.30% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Report-Grounded Synthesis Pipeline
High-quality medical VQA datasets can be generated at scale by using LLMs to translate unstructured clinical reports into structured MCQs, validated through three-layer pipeline (structural, content, expert review). Core assumption: source reports are accurate and comprehensive. Break condition: LLM hallucinations or insufficient expert review sample size.

### Mechanism 2: Hierarchical Cognitive Decomposition
Evaluating generalist medical AI requires decomposing "reasoning" into distinct cognitive skills (presence, location, negation, differential diagnosis, geometric) rather than relying on single accuracy metric. Core assumption: these five categories cover necessary spectrum of clinical reasoning. Break condition: models develop superficial reasoning that short-circuits visual input.

### Mechanism 3: Domain-Specific Visual Pre-training
Medical-specialized vision-language models outperform generalist VLMs on clinical reasoning tasks when pre-trained on domain-specific data (e.g., chest X-rays, histopathology). Core assumption: superior performance stems from domain-specific pre-training data and architecture. Break condition: test set contains data leaks from pre-training corpus.

## Foundational Learning

- **Concept: Report-Text vs. Image-Visual Grounding**
  - Why needed: Benchmark generates questions from text reports, not directly from image pixels, limiting ground truth to radiologist findings
  - Quick check: If a question asks "Is there a fracture?" and the report says "No acute fracture," but a subtle fracture is actually visible on the X-ray, what is the "correct" answer according to ReXVQA?

- **Concept: Negation in Medical NLP**
  - Why needed: ~36% of benchmark is "Negation Assessment" detecting absence of findings, a frequent AI failure mode
  - Quick check: Why might a model trained to maximize positive detections struggle with a negation query like "Is cardiomegaly absent?"

- **Concept: Cohen's Kappa (Inter-reader Agreement)**
  - Why needed: Paper uses Kappa to show AI-human agreement (0.3-0.5) is lower than human-human agreement, implying different reasoning patterns
  - Quick check: If two readers have high accuracy but low Kappa, what does that suggest about their error patterns?

## Architecture Onboarding

- **Component map:** ReXGradient-160K (160K Chest X-rays) → GPT-4o (Report → Bullets → MCQ) → Structural Validators → ClinicalBERT → Expert Review → ReXVQA (~696K MCQs)

- **Critical path:** The prompt engineering for "Report to Bullets" transformation is the linchpin; if it fails to capture hierarchical medical context, downstream MCQs will be medically invalid

- **Design tradeoffs:** MCQ format chosen for scalability and consistent scoring over nuance of long-form clinical explanations; report-based approach is scalable but limits testing to findings already noted by humans

- **Failure signatures:** "Multiple Valid Answers" in early pipeline runs; generalist models failing on technical image quality questions (10.30% accuracy)

- **First 3 experiments:**
  1. Run "Report to Bullets" prompt on 50 random reports and verify "hallucinated information" is < 2%
  2. Evaluate if "Easy/Medium/Hard" metadata tags correlate with model accuracy drops
  3. Replicate reader study on smaller subset (20 cases) focusing on "Geometric" questions to verify low human-model agreement

## Open Questions the Paper Calls Out
The paper acknowledges that because the dataset integrates data from four U.S. hospital systems, it "may not fully represent global radiological practices or diverse international patient populations."

## Limitations
- Limited sample size (200 cases) in reader study with moderate AI-human agreement (κ=0.3-0.5) raises questions about clinical meaningfulness of performance claims
- MCQ format constrains complexity of questions that can be asked, potentially missing nuanced clinical judgment tasks
- Report-based generation limits benchmark to findings already noted by humans; cannot test for "missed" findings or novel visual patterns

## Confidence
- **High Confidence:** Dataset generation methodology is well-documented and reproducible with clear validation layers
- **Medium Confidence:** Comparative performance between models is reliable, though absolute accuracy numbers may be inflated by MCQ format
- **Low Confidence:** Claim that AI "exceeds expert human performance" is questionable given moderate agreement scores and potential format artifacts

## Next Checks
1. **Clinical Validity Test:** Conduct follow-up reader study evaluating whether MCQ format introduces bias or whether open-ended versions would yield different performance rankings
2. **Generalization Stress Test:** Evaluate models on subset where ground truth is derived from fresh expert annotations rather than existing reports
3. **Error Pattern Analysis:** Perform detailed failure mode analysis comparing specific question types where humans excel versus where models excel