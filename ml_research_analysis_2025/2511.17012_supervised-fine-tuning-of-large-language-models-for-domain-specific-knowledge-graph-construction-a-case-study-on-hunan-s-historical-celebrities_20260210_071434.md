---
ver: rpa2
title: Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge
  Graph Construction:A Case Study on Hunan's Historical Celebrities
arxiv_id: '2511.17012'
source_url: https://arxiv.org/abs/2511.17012
tags:
- knowledge
- extraction
- fine-tuning
- historical
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a supervised fine-tuning approach to adapt
  large language models for domain-specific knowledge graph construction, focusing
  on Hunan's historical celebrities. By designing a fine-grained schema and constructing
  a domain-specific instruction-tuning dataset, the authors fine-tune four open-source
  LLMs (Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct)
  using parameter-efficient LoRA.
---

# Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities

## Quick Facts
- arXiv ID: 2511.17012
- Source URL: https://arxiv.org/abs/2511.17012
- Reference count: 30
- Key outcome: Supervised fine-tuning approach achieves 89.3866 score on Hunan historical celebrities knowledge graph extraction using Qwen3-8B

## Executive Summary
This study proposes a supervised fine-tuning approach to adapt large language models for domain-specific knowledge graph construction, focusing on Hunan's historical celebrities. The authors design a fine-grained schema with 14 entity/property types and construct a domain-specific instruction-tuning dataset through manual annotation. Four open-source LLMs are fine-tuned using parameter-efficient LoRA, achieving significant performance improvements across all models. The approach provides a cost-effective method for constructing historical character knowledge graphs in low-resource scenarios and offers insights for applying LLMs in regional historical and cultural domains.

## Method Summary
The method involves designing a fine-grained schema for Hunan historical celebrities, constructing a domain-specific instruction-tuning dataset through multi-source collection and manual annotation, and fine-tuning four open-source LLMs using parameter-efficient LoRA. The fine-tuned models are then used to extract structured information from biographical texts, which is transformed into Cypher queries and loaded into Neo4j to construct the knowledge graph. The approach uses a multi-granularity evaluation system combining exact match for structured attributes and vector similarity for narrative fields, with weighted aggregation to discriminate model performance.

## Key Results
- Qwen3-8B achieved the best performance with score of 89.3866 after fine-tuning on 100 samples with 50 iterations
- Performance improvements observed across all four models (Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct)
- Disabling Chain-of-Thought improved scores by ~4 points compared to enabled mode
- 100 samples with 50 epochs showed comparable performance to 150 samples with 40 epochs, indicating diminishing returns beyond certain sample sizes

## Why This Works (Mechanism)

### Mechanism 1
Schema-guided instruction fine-tuning improves domain-specific extraction by constraining output structure and semantics. The fine-grained schema serves dual functions: as a prompt template specifying what to extract, and as a validation contract forcing JSON-structured output. This reduces the model's search space and aligns its generation with domain-ontology semantics rather than open-ended prose.

### Mechanism 2
LoRA enables domain knowledge injection while preserving base model capabilities through low-rank parameter updates. By freezing the pretrained weight matrix W and training only low-rank matrices A and B, LoRA creates a bypass that approximates full fine-tuning's weight updates while preventing catastrophic forgetting through constrained update space.

### Mechanism 3
Weighting narrative schema components higher than factual ones in evaluation better discriminates model extraction capabilities. Factual properties like birth dates have near-deterministic extraction creating ceiling effects that mask capability differences, while narrative properties requiring semantic compression and inference produce score variance revealing true model adaptation.

## Foundational Learning

### Low-Rank Adaptation (LoRA)
- **Why needed:** Understanding how parameter-efficient fine-tuning works is essential for reproducing this pipeline
- **Quick check question:** If you set LoRA rank r=1, what type of domain knowledge can the model learn?

### Instruction Tuning Data Format
- **Why needed:** The paper uses Alpaca-style (instruction, input, output) triples; understanding this format is prerequisite to constructing your own datasets
- **Quick check question:** In the paper's template, why is the "input" field left blank?

### Schema-Guided Extraction vs. Open Information Extraction
- **Why needed:** The paper's approach constrains extraction to predefined schema types; understanding this tradeoff determines whether the method suits your use case
- **Quick check question:** If your domain has 50 entity types and 200 relation types, what schema design challenges emerge?

## Architecture Onboarding

### Component map:
Schema Designer → Data Pipeline → Fine-tuning Engine → Extraction Inference → Evaluation Module → KG Builder

### Critical path:
1. Schema definition (blocks dataset construction)
2. Manual annotation of 50–150 training samples (blocks fine-tuning)
3. LoRA fine-tuning with 30–50 epochs (blocks evaluation)
4. Evaluation-weight selection to discriminate model versions

### Design tradeoffs:
- **Sample size vs. epochs:** 100 samples × 50 epochs ≈ 150 samples × 40 epochs; small sample increases are valuable early, diminishing returns after 30 epochs
- **CoT vs. direct extraction:** Disabling chain-of-thought improved scores by ~4 points; CoT introduces hallucination risk and efficiency loss for extraction tasks
- **Schema granularity vs. model capacity:** 14-component schema works for 7–8B models; finer schemas may require larger models or hierarchical extraction

### Failure signatures:
- Model outputs prose instead of JSON → prompt template not constraining output format
- Scores plateau before convergence → LoRA rank too low
- High variance across runs → evaluation weights emphasize noisy components
- CoT-enabled model generates repetitive loops → disable thinking mode for extraction tasks

### First 3 experiments:
1. **Baseline replication:** Fine-tune Qwen3-8B with 100 samples, 50 epochs, LoRA r=8; verify score reaches 88–90 range
2. **Schema ablation:** Reduce schema from 14 components to 7; measure performance drop to quantify schema contribution
3. **Sample efficiency curve:** Train with 25, 50, 75, 100, 125, 150 samples at fixed 30 epochs; identify inflection point where sample increases stop improving scores

## Open Questions the Paper Calls Out

### Open Question 1
Can a closed-loop system be established where the constructed knowledge graph feeds back into the LLM to enhance its semantic reasoning and generation capabilities? The current study focuses exclusively on unidirectional extraction and does not evaluate how structured knowledge could refine the base model.

### Open Question 2
How can the schema definition be evolved to capture dynamic temporal complexities, such as causal event chains and evolving relationship trajectories? The current static schema captures entity attributes and binary relations but fails to model chronological or causal progression of historical events.

### Open Question 3
Does the integration of multi-modal information (e.g., historical images, artifact scans) improve the accuracy of knowledge extraction compared to text-only methods? The current methodology relies solely on unstructured text, ignoring visual sources that may contain distinct historical evidence.

### Open Question 4
Why does the Chain-of-Thought mechanism degrade performance in structured extraction tasks, and are there specific constraints that could make it viable? CoT is typically associated with improved reasoning, so its negative impact suggests a mismatch between generative reasoning steps and strict schema adherence requiring further diagnosis.

## Limitations

- Evaluation weights were selected for maximum discriminative power rather than domain representativeness, raising questions about generalizability to different applications
- Manual annotation process lacks explicit inter-annotator agreement metrics or detailed guidelines for reliable reproduction
- LoRA hyperparameter configuration remains unspecified, making it difficult to determine whether performance gains stem from the approach itself or specific parameter choices

## Confidence

- **High Confidence:** Schema-guided instruction fine-tuning improving domain-specific extraction through output structure constraints
- **Medium Confidence:** LoRA enabling effective domain knowledge injection while preserving base capabilities
- **Low Confidence:** Evaluation-weighting strategy's claim of optimal discriminative power

## Next Checks

1. **Schema Transferability Test:** Apply the 14-component schema to a different cultural heritage domain and measure performance degradation to reveal whether success is domain-specific

2. **Evaluation Weight Sensitivity Analysis:** Systematically vary evaluation weights across the spectrum from fact-heavy to narrative-heavy weighting schemes to identify inflection points where model rankings invert

3. **LoRA Hyperparameter Sensitivity:** Conduct a grid search over LoRA ranks (8, 16, 32, 64) and learning rates (1e-4, 2e-4, 5e-4) while keeping all other factors constant to determine whether reported performance is robust to hyperparameter variation