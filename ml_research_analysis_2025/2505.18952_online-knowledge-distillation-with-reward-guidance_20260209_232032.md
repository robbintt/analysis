---
ver: rpa2
title: Online Knowledge Distillation with Reward Guidance
arxiv_id: '2505.18952'
source_url: https://arxiv.org/abs/2505.18952
tags:
- pbkd
- preference
- online
- student
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a preference-based knowledge distillation (PbKD)
  framework that formulates knowledge distillation as a reward-guided imitation learning
  problem. The method optimizes the performance gap between student and teacher policies
  through a min-max game between the student and a reward model, with theoretical
  guarantees on suboptimality and regret bounds.
---

# Online Knowledge Distillation with Reward Guidance

## Quick Facts
- arXiv ID: 2505.18952
- Source URL: https://arxiv.org/abs/2505.18952
- Reference count: 40
- One-line primary result: PbKD consistently outperforms existing methods, including Vanilla Black-Box KD (49.7%→53.6% average accuracy), Proxy-KD (53.1%→53.6%), and white-box baselines like KL divergence and reverse KL divergence.

## Executive Summary
This paper introduces a preference-based knowledge distillation (PbKD) framework that formulates knowledge distillation as a reward-guided imitation learning problem. The method optimizes the performance gap between student and teacher policies through a min-max game between the student and a reward model, with theoretical guarantees on suboptimality and regret bounds. Extensive experiments across five black-box and five white-box KD benchmarks show that PbKD consistently outperforms existing methods, including Vanilla Black-Box KD (49.7%→53.6% average accuracy), Proxy-KD (53.1%→53.6%), and white-box baselines like KL divergence and reverse KL divergence. The online variant with iterative preference data collection further improves performance, demonstrating the effectiveness of reward-guided iterative distillation.

## Method Summary
The method formulates knowledge distillation as a min-max optimization problem between the student policy and a reward model, minimizing the performance gap with teacher. It uses Bradley-Terry-Luce preference modeling to construct a confidence set of reward functions consistent with observed preferences, then optimizes the student policy against the worst-case reward in this set. The framework supports both black-box (output-only) and white-box (access to teacher probabilities) settings, with the white-box variant reformulating the objective using the Performance Difference Lemma to enable moment-matching via Q-functions. An online variant iteratively collects new preference pairs as the student policy evolves, expanding the confidence set to maintain robust alignment.

## Key Results
- PbKD achieves 53.6% average accuracy on black-box benchmarks, outperforming Vanilla Black-Box KD (49.7%) and Proxy-KD (53.1%)
- White-box MM PbKD outperforms KL/RKL/MiniLLM baselines across all five benchmarks
- Online PbKD iterations show consistent gains over offline-only training, with performance steadily improving as iterations increase
- Theoretical guarantees on suboptimality and regret bounds are validated through empirical results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Min-max optimization between student policy and reward model improves distillation robustness over standard behavior cloning or fixed reward pipelines.
- Mechanism: The inner maximization finds the worst-case reward model within a confidence set that maximizes the teacher-student performance gap; the outer minimization then updates the student to minimize this gap. This adversarial formulation forces the student to perform well across all plausible reward functions consistent with observed preferences, rather than overfitting to a single reward estimate.
- Core assumption: The ground-truth reward function lies within the confidence set (Assumption 1: realizability); preference data sufficiently constrains the reward function space.
- Evidence anchors:
  - [abstract] "formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance gap"
  - [section 4.1] "The inner maximization identifies the least favorable reward model r ∈ R(Dpref) that maximizes the performance gap, thereby encouraging robustness to reward uncertainty."
  - [corpus] Neighbor work "Preference Distillation via Value based Reinforcement Learning" addresses similar preference-based alignment but via DPO, not min-max; suggests this is a distinct formulation.
- Break condition: If preference data is sparse or systematically biased such that the confidence set excludes r*, the student optimizes against misspecified rewards.

### Mechanism 2
- Claim: Iterative online preference collection from the evolving student policy reduces distribution mismatch between training and evaluation.
- Mechanism: At each iteration t, new preference pairs (teacher response, student response) are collected and added to Dpref. This expands the confidence set to cover reward differences that emerge as the student policy shifts, preventing reward model obsolescence and enabling tighter teacher-student alignment.
- Core assumption: Teacher responses are always preferred over student responses (explicit simplification in Section 5.1); access to teacher inference at each iteration.
- Evidence anchors:
  - [section 5.1] "we make a simplifying assumption: responses from the teacher policy are always preferred over those from the student policy."
  - [section 7.1] "Ablation on online PbKD iterations shows consistent gains over offline-only training, with performance steadily improving as iterations increase"
  - [corpus] "KEPO: Knowledge-Enhanced Preference Optimization" similarly uses online preference signals but focuses on reasoning chains rather than KD-specific teacher-student gaps.
- Break condition: If teacher responses are not reliably preferred (e.g., teacher hallucinates on domain-specific prompts), preference labels become noisy, degrading reward model quality.

### Mechanism 3
- Claim: Reformulating the reward model as a Q-function enables white-box KD by directly leveraging teacher output probabilities.
- Mechanism: Using the Performance Difference Lemma (Proposition 2), the objective J(πE, r) − J(π, r) is expressed as an expectation over Q-function differences at each step. The confidence set constraint shifts from reward space to Q-function space, allowing the student to match moment distributions rather than just output sequences.
- Core assumption: Teacher's predicted probabilities (logits/soft labels) are accessible; deterministic state transitions in sequence generation.
- Evidence anchors:
  - [section 6] "This formulation can be interpreted as a moment-matching objective: minimizing the expected discrepancy between (1) the teacher's expected Q-value and (2) the student policy's realized Q-value"
  - [table 2] White-box KD results show MM PbKD outperforms KL/RKL/MiniLLM baselines across all five benchmarks.
  - [corpus] "AlignDistil: Token-Level Language Model Alignment" also reformulates alignment as token-level policy distillation but via adaptive objectives rather than Q-function confidence sets.
- Break condition: If the teacher's probabilities are poorly calibrated or the Q-function class is misspecified, the moment-matching objective may pursue uninformative or misleading targets.

## Foundational Learning

### Concept: Min-max (adversarial) optimization in RL
- Why needed here: The core objective (Eq. 3) is a saddle-point problem; understanding that the inner max is not a training adversary but a robustness device against reward uncertainty is critical.
- Quick check question: Can you explain why using a point estimate of the reward (standard MLE) might fail compared to optimizing against the worst-case reward in a confidence set?

### Concept: Bradley-Terry-Luce preference modeling
- Why needed here: The paper models preferences via sigmoid of reward differences (Eq. 1); this links preference probabilities to reward magnitudes and determines how MLE loss shapes the confidence set.
- Quick check question: Given two trajectories τ0 and τ1 with rewards 2.0 and 0.5, what preference probability does the BTL model predict?

### Concept: Performance Difference Lemma and Q-functions in imitation learning
- Why needed here: The white-box extension hinges on rewriting the performance gap as a sum of Q-function differences (Proposition 2); this connects reward optimization to moment-matching.
- Quick check question: How does the PDL avoid summing over all possible trajectories when computing J(πE, r) − J(π, r)?

## Architecture Onboarding

### Component map:
- Student policy (π) -> Reward model (rθ) -> Confidence set (R(Dpref)) -> Min-max optimization -> Performance gap minimization
- (White-box only) Q-function (f) -> Moment-matching objective -> Teacher Q-values

### Critical path:
1. Initialize student with teacher-output SFT (warm start)
2. For each online iteration: sample prompts, generate teacher and student responses, label preferences (teacher preferred)
3. Update RM via gradient ascent on Eq. (9)'s reward term + MLE regularizer
4. Update student via policy gradient on Eq. (9)'s performance gap term (with clipping for stability)
5. (White-box only) Update Q-function similarly; use Eq. (12) for moment-matching

### Design tradeoffs:
- **RM size vs. iteration count**: Larger RMs (70B) achieve higher asymptotic performance but converge slower (Figure 3); smaller RMs (3B) plateau early. Match RM capacity to compute budget and target performance.
- **Offline vs. online preference data**: Offline is cheaper but limited to pre-collected distribution; online adapts to student distribution shift but requires continuous teacher inference.
- **ζ (confidence radius)**: Larger ζ increases robustness but slows convergence; paper sets ζ via theoretical bounds (Theorem 1) but empirical tuning may be needed.

### Failure signatures:
- **Reward hacking**: Student exploits gaps between RM and true r*; manifests as high RM scores but low ground-truth evaluation metrics.
- **Preference label noise**: If teacher responses aren't reliably preferred (violating Section 5.1 assumption), RM learns incorrect preferences; check teacher response quality on held-out prompts.
- **Covariate shift**: Student policy drifts into regions where Dpref has no coverage; RM becomes unreliable. Monitor feature covariance Σt eigenvalues; small eigenvalues indicate coverage gaps.

### First 3 experiments:
1. **Offline PbKD baseline on single dataset**: Implement offline PbKD (Algorithm 1) on databricks-dolly-15k with TinyLLaMA-1.1B student and LLaMA2-13B teacher. Verify suboptimality bound scaling with N by plotting J(π*, r*) − J(π̂, r*) vs. preference sample count.
2. **Ablation on confidence radius ζ**: Sweep ζ ∈ {0.1, 0.5, 1.0, 2.0} on a held-out validation set. Identify the point where larger ζ no longer improves robustness (dimimizing returns) vs. convergence slowdown.
3. **Online iteration sanity check**: Run 5 online iterations with a small RM (3B). Plot per-iteration performance on GSM8K. Verify monotonic improvement per Figure 3 pattern; if performance degrades, investigate preference label quality or student policy instability.

## Open Questions the Paper Calls Out
- Can automated or self-supervised preference acquisition methods match or exceed the performance of human/AI-annotated preference data in the PbKD framework?
- How can the online PbKD framework be made more computationally efficient without sacrificing the iterative improvement gains?
- How does PbKD perform when teacher outputs are genuinely suboptimal for the target downstream task?
- How sensitive is online PbKD to violations of the assumption that teacher responses are always preferred over student responses?

## Limitations
- Theoretical guarantees hinge on Assumption 1 (realizability of r* within the confidence set), which may fail if preference data is sparse or biased
- The paper simplifies teacher-student preferences (teacher always preferred), but this may not hold for complex or domain-specific tasks
- Hyperparameter details (β, ζ, learning rates, feature extractor ϕ) are underspecified, requiring careful tuning for reproduction

## Confidence
- **High**: PbKD outperforms standard KD baselines (KL/RKL) on white-box tasks; iterative online distillation improves performance over offline-only; min-max formulation improves robustness over fixed reward pipelines.
- **Medium**: Theoretical suboptimality and regret bounds hold under realizability; teacher-student preference simplification is sufficient for distillation; confidence set coverage via ζ bounds is tight.
- **Low**: Exact hyperparameter choices (β, ζ, feature extractor ϕ) are optimal; scaling laws for RM size vs. iteration count are universal; teacher always preferred assumption holds across all domains.

## Next Checks
1. **Sensitivity analysis on ζ and β**: Sweep ζ ∈ {0.1, 0.5, 1.0, 2.0} and β ∈ {0.1, 0.5, 1.0} on a held-out validation set. Identify the point where larger ζ or β no longer improves robustness (diminishing returns) vs. convergence slowdown or training instability.
2. **Preference label noise robustness**: Intentionally flip a fraction of preference labels (10%, 25%, 50%) in the offline dataset. Measure degradation in RM accuracy and student performance. If performance collapses early, the method is sensitive to label noise.
3. **Teacher preference violation test**: Manually inspect 100 teacher-student response pairs on a domain-specific task (e.g., coding). Count violations of the "teacher always preferred" assumption. If violations exceed 10-15%, the theoretical simplification may not hold, and preference modeling may need to account for teacher errors.