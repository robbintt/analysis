---
ver: rpa2
title: 'Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language
  Models'
arxiv_id: '2506.22950'
source_url: https://arxiv.org/abs/2506.22950
tags:
- sampling
- group
- memory
- micro
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Infinite Sampling tackles memory inefficiency in large-group GRPO
  training by introducing micro sampling groups, continuous sampling, and length-aware
  scheduling. The method decomposes large sample groups into smaller micro groups
  to reuse KV cache and reduce peak memory usage by over 50% (e.g., from 21.55 GB
  to 10.64 GB on Qwen3-1.7B).
---

# Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models

## Quick Facts
- arXiv ID: 2506.22950
- Source URL: https://arxiv.org/abs/2506.22950
- Reference count: 40
- Memory usage reduced by over 50% in grouped RL training for large language models

## Executive Summary
Infinite Sampling addresses the memory inefficiency of large-group Grouped Reference Policy Optimization (GRPO) training by decomposing sample groups into smaller micro sampling groups, enabling continuous sampling and length-aware scheduling. The method achieves up to 50% reduction in peak memory usage while maintaining stable training and sequence quality. Through continuous sampling across micro groups, the framework improves throughput by minimizing idle slots, and its length-aware scheduler uses prefix-based length prediction to optimize micro group allocation. Experiments show reduced decoding steps by up to 45% and preserved policy gradient stability on benchmarks like GSM8K.

## Method Summary
The Infinite Sampling framework tackles the memory inefficiency of grouped RL training by introducing three key innovations: micro sampling groups, continuous sampling, and length-aware scheduling. Large sample groups are decomposed into smaller micro groups to reuse KV cache and reduce peak memory usage by over 50%. Continuous sampling interleaves decoding across micro groups to improve throughput by minimizing idle slots. The length-aware scheduler predicts sequence lengths from short prefixes using a BERT-based model and employs a hybrid two-stage plan (FPTAS for global grouping, SJF for runtime refill) to balance memory usage and maximize utilization. This approach enables stable GRPO training under tight GPU memory constraints while preserving sequence quality.

## Key Results
- Memory usage reduced from 21.55 GB to 10.64 GB on Qwen3-1.7B (over 50% reduction)
- Throughput improved by over 25% through continuous sampling across micro groups
- Decoding steps reduced by up to 45% on GSM8K benchmark while maintaining policy gradient stability

## Why This Works (Mechanism)
The method works by addressing the fundamental memory inefficiency in grouped RL training where large sample groups consume excessive memory for KV caches. By decomposing groups into micro groups, the framework enables KV cache reuse across sequences, dramatically reducing peak memory requirements. Continuous sampling ensures that while one sequence is undergoing backpropagation, other micro groups continue decoding, minimizing idle computational resources. The length-aware scheduling uses prefix-based prediction to optimally allocate sequences to micro groups, ensuring memory constraints are respected while maximizing throughput. This combination of micro-group decomposition, interleaved execution, and intelligent scheduling creates a synergistic effect that enables efficient large-group training under memory constraints.

## Foundational Learning
- Grouped RL Training: Why needed - to enable efficient batch processing of multiple sequences with shared context; Quick check - verify that sequences in a group share the same prompt prefix
- KV Cache Management: Why needed - transformer attention requires storing intermediate states for efficient decoding; Quick check - confirm that KV cache size scales linearly with sequence length and batch size
- Prefix-Based Length Prediction: Why needed - to anticipate sequence length for optimal micro group allocation; Quick check - measure prediction accuracy on held-out data
- Continuous Batching: Why needed - to keep GPU utilization high by overlapping decoding and backpropagation; Quick check - verify that decoding slots remain filled throughout training
- Fully Polynomial Time Approximation Scheme (FPTAS): Why needed - to solve the NP-hard micro group allocation problem efficiently; Quick check - confirm solution quality approaches optimal within polynomial time
- Shortest Job First (SJF) Scheduling: Why needed - to minimize average waiting time in runtime micro group refill; Quick check - verify that shorter sequences are prioritized in refill queue

## Architecture Onboarding

Component Map:
Infinite Sampling framework: GRPO training loop -> Micro sampling groups decomposition -> Continuous sampling scheduler -> Length-aware scheduler -> BERT length predictor -> KV cache management

Critical Path:
The critical path flows from the GRPO training loop through micro sampling group decomposition, where sequences are allocated to micro groups based on length predictions. The continuous sampling scheduler then manages the interleaved execution of these micro groups, ensuring that while some sequences undergo backpropagation, others continue decoding. The length-aware scheduler uses the BERT predictor to estimate sequence lengths from prefixes and employs FPTAS for initial allocation and SJF for runtime refill, optimizing both memory usage and throughput throughout the training process.

Design Tradeoffs:
The framework trades increased scheduling complexity for significant memory and throughput improvements. While the length prediction adds computational overhead, the benefits of reduced memory usage and improved utilization outweigh this cost. The micro group approach introduces scheduling overhead but enables KV cache reuse that would be impossible with traditional large groups. The continuous sampling model requires careful coordination but eliminates the idle time that plagues conventional grouped training.

Failure Signatures:
Training instability or degraded reward quality may indicate issues with the length prediction accuracy, leading to poor micro group allocation. Excessive memory usage despite the framework suggests that micro group sizes are too large or that the continuous sampling scheduler is not properly managing the interleave. Low throughput improvements may indicate that the continuous sampling is not properly overlapping decoding and backpropagation, or that the micro group sizes are mismatched to the available computational resources.

First Experiments:
1. Measure peak memory usage with varying micro group sizes on a simple benchmark
2. Compare throughput between traditional grouped training and Infinite Sampling on a fixed dataset
3. Evaluate length prediction accuracy on sequences with different length distributions

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can Infinite Sampling be effectively extended to handle multi-prompt batches without negating the memory benefits of shared prompt KV caches?
- Basis in paper: [explicit] The Conclusion states, "Extending our framework to support multi-prompt batches... is an interesting direction for future work," noting the current assumption that "all completions originate from a single prompt."
- Why unresolved: The current architecture relies on a single shared prefill cache to enable continuous sampling; distinct prompts within the same micro-batch would require separate KV caches, potentially fragmenting memory and reducing utilization.
- What evidence would resolve it: A demonstration of Infinite Sampling processing a batch of diverse prompts simultaneously, showing memory usage remains bounded and throughput improvements persist compared to standard continuous batching.

### Open Question 2
- Question: How does the "length bias" inherent in Dynamic-Slot Continuous Sampling affect the convergence rate and final accuracy of GRPO training?
- Basis in paper: [inferred] Section 3.2 and Table 1 note that Dynamic-Slot mode favors shorter sequences (e.g., reducing avg length to 21 tokens vs 186), which "may negatively impact training stability and reward quality," but the paper focuses on the Fixed-Slot variant to avoid this.
- Why unresolved: While the paper identifies the bias toward short sequences in Dynamic-Slot mode, it does not quantify the impact this bias has on the final reward optimization or policy convergence over a full training run.
- What evidence would resolve it: A comparative training study measuring final task performance (e.g., GSM8K accuracy) and policy convergence speed when using the biased Dynamic-Slot scheduler versus the unbiased Infinite Sampling scheduler.

### Open Question 3
- Question: Does the BERT-based length predictor generalize effectively to out-of-distribution domains, such as code generation or low-resource languages?
- Basis in paper: [inferred] Section 4.2 notes the predictor is "fine-tuned... using the LMSYS-Chat dataset," but relies on token-conditioned estimation which may vary significantly in structure-heavy domains like coding.
- Why unresolved: The scheduling efficiency (FPTAS/SJF) depends entirely on prediction accuracy; if the predictor fails on code (where prefix length may not correlate linearly with total length), the scheduling could degrade to naive or inefficient planning.
- What evidence would resolve it: Evaluation of prediction error rates (MAE) and scheduling efficiency (running steps) when the framework is applied to code synthesis benchmarks like HumanEval or MBPP without retraining the predictor.

### Open Question 4
- Question: Can Infinite Sampling be integrated with aggressive KV cache compression techniques (e.g., quantization or eviction) to further reduce the memory footprint?
- Basis in paper: [explicit] The Conclusion lists "integrating with more advanced KV cache compression techniques" as an interesting future direction.
- Why unresolved: The current method relies on precise KV cache management and reuse; compression introduces approximations or eviction policies that might conflict with the requirement to preserve "full-length completions" for stable GRPO training.
- What evidence would resolve it: Experiments combining Infinite Sampling with 4-bit or 8-bit KV cache quantization, reporting the trade-off between memory savings, throughput, and policy gradient stability.

## Limitations
- Scalability to larger models remains uncertain as experiments were primarily conducted on 1.7B and 7B parameter models
- Length prediction accuracy may degrade for out-of-distribution data such as code or low-resource languages
- The framework assumes uniform KV cache reuse patterns that may not hold for all transformer architectures or attention mechanisms

## Confidence
- High confidence in memory efficiency improvements (measured reduction in peak memory usage, consistent across multiple benchmarks)
- Medium confidence in throughput gains (substantial improvements reported, but dependent on specific hardware and workload patterns)
- Medium confidence in length-aware scheduling benefits (significant step reduction shown, but reliant on prediction accuracy which may vary)

## Next Checks
1. Evaluate Infinite Sampling on larger models (e.g., Llama 3.1 8B, Qwen2.5 32B) and under varied GPU memory constraints to confirm scalability and robustness
2. Test the method with different transformer architectures and attention variants to ensure KV cache reuse assumptions remain valid
3. Validate the length prediction model's accuracy and scheduling effectiveness on datasets with highly skewed or unpredictable sequence length distributions