---
ver: rpa2
title: Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning
arxiv_id: '2511.07971'
source_url: https://arxiv.org/abs/2511.07971
tags:
- loren
- gradient
- learning
- rate
- mezo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LOREN is a zeroth-order optimizer for LLM fine-tuning that addresses
  slow convergence and high variance by combining curvature-aware updates with variance
  reduction. It adaptively learns an anisotropic perturbation distribution via a low-rank
  block-diagonal preconditioner and uses the REINFORCE leave-one-out estimator to
  reduce gradient variance.
---

# Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2511.07971
- Source URL: https://arxiv.org/abs/2511.07971
- Authors: Hyunseok Seung; Jaewoo Lee; Hyunsuk Ko
- Reference count: 40
- Primary result: Achieves 10 percentage points higher accuracy than MeZO and 6 points higher than HiZOO on GLUE/SuperGLUE benchmarks

## Executive Summary
LOREN addresses slow convergence and high variance in zeroth-order optimization for LLM fine-tuning by combining curvature-aware updates with variance reduction. The method uses a low-rank block-diagonal preconditioner to learn an anisotropic perturbation distribution and employs the REINFORCE leave-one-out estimator for gradient variance reduction. Compared to state-of-the-art ZO methods, LOREN demonstrates higher accuracy and faster convergence on GLUE and SuperGLUE benchmarks while reducing peak memory usage by up to 27.3%.

## Method Summary
LOREN is a zeroth-order optimizer designed specifically for LLM fine-tuning that addresses two fundamental challenges: slow convergence and high gradient variance. The method constructs an adaptive perturbation distribution through a low-rank block-diagonal preconditioner that captures curvature information. This preconditioner enables anisotropic updates that are more effective than isotropic perturbations used in traditional ZO methods. The REINFORCE leave-one-out estimator is incorporated to reduce gradient variance, improving the quality of parameter updates. The combination of curvature awareness and variance reduction allows LOREN to achieve faster convergence and higher accuracy compared to existing approaches while maintaining reasonable memory efficiency.

## Key Results
- Achieves 10 percentage points higher test accuracy than MeZO and 6 points higher than HiZOO on average
- Demonstrates faster convergence on GLUE and SuperGLUE benchmarks
- Reduces peak memory usage by up to 27.3% compared to existing methods

## Why This Works (Mechanism)
The method works by learning an adaptive perturbation distribution that captures local curvature information through the low-rank block-diagonal preconditioner. This allows for more informed parameter updates that follow the geometry of the loss landscape rather than using isotropic perturbations. The REINFORCE leave-one-out estimator reduces gradient variance by exploiting the structure of the objective function, leading to more stable and reliable updates. The combination of these two components enables more efficient exploration of the parameter space and faster convergence to better optima.

## Foundational Learning
- Zeroth-order optimization: Optimization methods that only require function evaluations, not gradients; needed for scenarios where gradients are unavailable or expensive to compute; quick check: verify that only function values are used in the update rule
- Curvature-aware updates: Optimization techniques that adapt to the local geometry of the loss landscape; needed to accelerate convergence by following more effective update directions; quick check: confirm that the preconditioner captures second-order information
- Variance reduction in ZO: Techniques to reduce the noise inherent in gradient estimates from function evaluations; needed to improve update quality and convergence stability; quick check: verify that the leave-one-out estimator reduces variance compared to standard REINFORCE
- Block-diagonal preconditioning: A computational strategy that approximates the full preconditioner using block-diagonal structure; needed to maintain scalability for high-dimensional LLM parameters; quick check: confirm that memory usage scales linearly with parameter count

## Architecture Onboarding

**Component map:**
LOREN -> Preconditioner Update -> Perturbation Distribution -> REINFORCE Estimator -> Parameter Update

**Critical path:**
Preconditioner Update → Perturbation Distribution → Parameter Update

**Design tradeoffs:**
The low-rank approximation balances capturing curvature information against computational efficiency, while the leave-one-out estimator trades off between variance reduction and additional function evaluations. The block-diagonal structure enables scalability but may miss cross-parameter interactions.

**Failure signatures:**
Slow convergence indicates poor preconditioner learning or inadequate variance reduction. High memory usage suggests the low-rank approximation is insufficient for the problem scale. Instability in training may indicate suboptimal hyperparameter choices for the perturbation scale or preconditioner learning rate.

**3 first experiments:**
1. Compare convergence curves of LOREN against MeZO and HiZOO on a small GLUE task
2. Measure memory usage during training to verify the 27.3% reduction claim
3. Perform ablation study removing either the preconditioner or variance reduction to isolate their contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees are not fully developed, making the method's theoretical properties unclear
- The low-rank preconditioner's effectiveness in capturing true curvature for high-dimensional LLMs is primarily empirical
- Computational overhead of maintaining and updating the preconditioner is not thoroughly analyzed in terms of wall-clock time impact

## Confidence
- **High confidence**: Empirical evaluation methodology and benchmark results are well-documented and reproducible
- **Medium confidence**: Memory usage claims and variance reduction benefits are supported by experiments but lack detailed ablation studies
- **Medium confidence**: Comparative advantages over state-of-the-art methods are demonstrated but could benefit from more extensive hyperparameter sensitivity analysis

## Next Checks
1. Conduct ablation studies to isolate the contribution of the low-rank preconditioner versus the variance reduction component
2. Perform wall-clock time measurements comparing LOREN against baselines across different hardware configurations
3. Test the method on additional LLM architectures and downstream tasks beyond GLUE/SuperGLUE to assess generalizability