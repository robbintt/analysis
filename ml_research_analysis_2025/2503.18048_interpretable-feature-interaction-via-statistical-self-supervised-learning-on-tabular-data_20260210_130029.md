---
ver: rpa2
title: Interpretable Feature Interaction via Statistical Self-supervised Learning
  on Tabular Data
arxiv_id: '2503.18048'
source_url: https://arxiv.org/abs/2503.18048
tags:
- features
- kernel
- feature
- polynomial
- spofe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Interpretable Feature Interaction via Statistical Self-supervised Learning on Tabular Data

## Quick Facts
- **arXiv ID:** 2503.18048
- **Source URL:** https://arxiv.org/abs/2503.18048
- **Authors:** Xiaochen Zhang; Haoyi Xiong
- **Reference count:** 40
- **Primary result:** Achieves superior predictive performance and interpretable feature interactions on six tabular datasets through self-supervised kernel PCA signals and sparse polynomial regression.

## Executive Summary
Spofe introduces a novel framework for interpretable feature interaction discovery in tabular data. The method generates self-supervised signals via kernel principal component analysis (KPCA) and then distills these complex patterns into sparse polynomial functions using a weighted knockoff filter. By combining unsupervised signal generation with rigorous false discovery rate control, Spofe identifies meaningful polynomial feature interactions while maintaining statistical guarantees on feature selection validity.

## Method Summary
Spofe operates through a three-stage pipeline: first, it generates self-supervised signals by projecting data into a high-dimensional Reproducing Kernel Hilbert Space using Kernel PCA; second, it expands the original features into a polynomial basis (including linear, quadratic, and interaction terms); third, it applies a weighted knockoff filter to select significant polynomial features while controlling false discovery rate. The knockoff statistics are weighted by KPCA eigenvalues to emphasize features explaining the most variance in the self-supervised signals.

## Key Results
- Outperforms KPCA and Sparse KPCA baselines on six real-world tabular datasets
- Demonstrates robustness to the number of kernel principal components (m) used
- Maintains interpretability through sparse polynomial feature selection
- Achieves superior predictive performance on downstream regression and classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unsupervised kernel projections may serve as effective proxy labels for identifying non-linear feature interactions.
- **Mechanism:** The algorithm first maps data into a high-dimensional Reproducing Kernel Hilbert Space (RKHS) via Kernel PCA (KPCA) to capture non-linear dependencies. These projected components (KPCs) are treated as "self-supervised signals" that act as target variables, allowing the system to learn which input features explain the non-linear structure without human-provided labels.
- **Core assumption:** The variance captured by the top kernel principal components corresponds to meaningful structural relationships in the data rather than high-dimensional noise.
- **Evidence anchors:**
  - [abstract] "generating self-supervised signals using kernel principal components to model complex patterns"
  - [section 4.1.1] "These KPCA-based signals provide a self-supervised representation... crucial for assessing the importance of polynomial features"
  - [corpus] *PCA-Guided Quantile Sampling* supports the principle of using PCA-derived structures to preserve data geometry.
- **Break condition:** If the kernel function (e.g., RBF, Cosine) is mismatched to the data geometry, the generated signals may encode arbitrary distortions, failing to guide feature selection.

### Mechanism 2
- **Claim:** Sparse polynomial expansions appear to bridge the gap between implicit kernel mappings and explicit interpretability.
- **Mechanism:** While KPCA features are implicit and hard to interpret, the method approximates these complex signals using a linear combination of explicit polynomial basis functions (e.g., $x_1 \cdot x_2$). By enforcing a sparsity constraint ($\|\beta\|_0 \le r$), the model forces the complex kernel behavior to be reconstructed using only a small, human-readable set of interaction terms.
- **Core assumption:** The complexity of the kernel transformation can be losslessly or effectively approximated by a finite, low-order polynomial expansion.
- **Evidence anchors:**
  - [abstract] "distilling these signals into sparse polynomial functions for improved interpretability"
  - [section 3.2] "expressing the eigenvectors through these sparse polynomial functions, we gain deeper insights into how individual features contribute"
  - [corpus] *BaGGLS* similarly utilizes shrinkage frameworks for interpretable interaction modeling, validating the sparse interaction approach.
- **Break condition:** If the true data structure relies on discontinuous or highly complex non-polynomial relationships, the sparse polynomial approximation may yield high reconstruction error (Theorem 2).

### Mechanism 3
- **Claim:** A weighted knockoff filter theoretically controls the False Discovery Rate (FDR) during feature selection.
- **Mechanism:** The method generates "knockoff" copies of the polynomial features that mimic the original correlation structure but are independent of the target (KPC signals). It calculates feature importance by comparing the model coefficient of a real feature against its knockoff. By weighting these statistics by the KPCA eigenvalues (variance explained), it prioritizes features that explain the most significant structural directions while theoretically bounding the selection of false positives.
- **Core assumption:** The exchangeability assumption holds for the knockoff construction, and the weighted aggregation of statistics preserves the FDR guarantees of the underlying knockoff procedure.
- **Evidence anchors:**
  - [abstract] "rigorous false discovery rate (FDR) control via a multi-objective knockoff selection procedure"
  - [section 4.3.3] "weighting the knockoff statistics by these eigenvalues ensures that directions with greater variance are given more emphasis"
  - [corpus] Weak direct evidence for the specific *weighted* knockoff method in the provided corpus; standard knockoff methods are established, but the multi-objective weighting is specific to this architecture.
- **Break condition:** If the feature correlation structure is too complex for the knockoff generator to replicate faithfully (resulting in poor knockoff quality), the FDR control may fail.

## Foundational Learning

- **Concept: Kernel Principal Component Analysis (KPCA)**
  - **Why needed here:** Standard PCA only captures linear relationships. KPCA uses kernel functions to map data into higher dimensions where linear separation is possible, serving as the source of "truth" for Spofe.
  - **Quick check question:** Can you explain why centering the kernel matrix (Equation 1) is necessary before eigen-decomposition?

- **Concept: The Knockoff Filter**
  - **Why needed here:** This is the statistical engine that distinguishes signal from noise. Unlike standard regularization (Lasso) which introduces bias, knockoffs provide rigorous false discovery rate control.
  - **Quick check question:** In the knockoff framework, why must the knockoff features $\tilde{X}$ have the same correlation structure as the original features $X$?

- **Concept: Sparse Polynomial Regression**
  - **Why needed here:** This provides the "vocabulary" for interpretability. It restricts the model to expressing relationships as simple products of variables (interactions) rather than abstract weights.
  - **Quick check question:** How does the $\ell_0$ constraint (limiting the number of non-zero coefficients) differ from the $\ell_1$ constraint used in Lasso regarding feature selection?

## Architecture Onboarding

- **Component map:** Input -> S4Gen (Signal Generator) -> Feature Expander -> WEKO (Selector) -> Output
- **Critical path:** The transition from the *S4Gen* module to the *WEKO* module. If the KPCA signals ($M$) do not capture meaningful variance, the subsequent regression against polynomial features will yield insignificant p-values regardless of the knockoff quality.
- **Design tradeoffs:**
  - **Kernel Choice vs. Interpretability:** The paper tests Cosine, RBF, and Sigmoid kernels. RBF captures complex geometry but may result in dense, less interpretable polynomial approximations compared to the linear kernel.
  - **Sparsity ($r$) vs. Fidelity:** Limiting polynomial terms ($r$) improves interpretability but increases the approximation error ($E^*$ in Theorem 2).
  - **Number of KPCs ($m$):** The paper suggests robustness to $m$ (Figure 1), but setting $m$ too low might miss orthogonal interaction structures.
- **Failure signatures:**
  - **Constant/Trivial Selection:** If the algorithm selects only the constant term or linear terms on a known non-linear dataset, the kernel bandwidth in S4Gen may be misspecified.
  - **Power Loss:** If p-values are uniformly distributed (flat histogram), the knockoffs are too similar to real features (high correlation), or the signal-to-noise ratio in $M$ is too low.
- **First 3 experiments:**
  1. **Reconstruction Validity:** Generate synthetic data from a known polynomial (e.g., $y = x_1^2 + x_2 \cdot x_3$). Run Spofe to verify it recovers these exact terms with high p-values.
  2. **Sensitivity Analysis ($m$):** Replicate Figure 1 on a local dataset. Verify that prediction accuracy plateaus as $m$ increases to confirm the paper's claim of robustness to this hyperparameter.
  3. **Ablation (Lasso vs. Knockoffs):** Compare the features selected by Spofe against a standard Lasso regression on the same polynomial matrix. Check if Spofe maintains a lower False Discovery Rate (FDR) on a dataset with many dummy (noise) variables.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive polynomial bases be formulated and integrated into the Spofe framework to better tailor feature extraction to specific data domains?
- **Basis in paper:** [explicit] The conclusion in Section 6 states that future research could explore "adaptive polynomial bases tailored to specific data domains."
- **Why unresolved:** The current implementation relies on fixed polynomial expansions, which may not optimally capture the unique nonlinear structures inherent in different domains (e.g., physics vs. finance).
- **What evidence would resolve it:** An extension of the algorithm that dynamically selects or constructs basis functions based on data characteristics, demonstrated through improved reconstruction accuracy or interpretability in domain-specific benchmarks.

### Open Question 2
- **Question:** Can theoretical guarantees for False Discovery Rate (FDR) control be formally extended from individual kernel principal components to the aggregated weighted selection process?
- **Basis in paper:** [inferred] Section 4.7 explicitly notes that guarantees apply to "individual component level" selection, whereas the final feature selection relies on a weighted sum of knockoff statistics evaluated only experimentally.
- **Why unresolved:** The statistical independence properties required for FDR control in the standard knockoff filter may not directly translate to the weighted aggregation of statistics across multiple dependent principal components.
- **What evidence would resolve it:** A theoretical proof establishing that the multi-objective weighted knockoff procedure maintains FDR control under the specific dependence structure of kernel principal components.

### Open Question 3
- **Question:** How can advanced visualization tools be specifically integrated with Spofe to enhance the utility and transparency of the identified feature interactions?
- **Basis in paper:** [explicit] Section 6 proposes that "integrating the approach with advanced visualization tools might further enhance its utility and interpretability for data scientists."
- **Why unresolved:** While the paper provides static visualizations of selected features, it does not define interactive or scalable visual methods to help users navigate complex polynomial interactions in high-dimensional settings.
- **What evidence would resolve it:** The design and evaluation of a visual analytics interface that allows users to interactively explore the significance scores and polynomial terms generated by Spofe.

## Limitations

- The exact knockoff statistic model and preliminary feature selection procedure are unspecified
- Theoretical FDR guarantees for the multi-KPC weighted aggregation are not formally proven
- The kernel function choice significantly impacts both signal quality and interpretability

## Confidence

- **Kernel-based self-supervision mechanism:** Medium
- **Weighted knockoff FDR control:** Medium
- **Interpretability through sparse polynomials:** Medium

## Next Checks

1. Synthetic data test: Verify exact recovery of known polynomial interactions
2. Ablation study: Compare Lasso vs knockoff selection on same polynomial features
3. Kernel sensitivity: Test multiple kernel functions and document reconstruction quality trade-offs