---
ver: rpa2
title: 'Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset,
  Taxonomy, and Benchmark'
arxiv_id: '2509.23735'
source_url: https://arxiv.org/abs/2509.23735
tags:
- failure
- root
- cause
- systems
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a dataset AgentFail of 307 annotated failure
  logs from platform-orchestrated agentic systems, identifies failure root causes
  using grounded theory annotation, and builds a taxonomy across agent, workflow,
  and platform levels. The taxonomy improves LLM-based root cause identification accuracy
  from ~10% to ~28-34%, though maximum accuracy remains at 33.6%.
---

# Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark

## Quick Facts
- arXiv ID: 2509.23735
- Source URL: https://arxiv.org/abs/2509.23735
- Reference count: 40
- Dataset of 307 annotated failure logs from platform-orchestrated agentic systems

## Executive Summary
This paper addresses the critical challenge of diagnosing root causes in multi-agent systems orchestrated by low-code platforms like Dify and Coze. The authors construct AgentFail, a dataset of 307 annotated failure logs from 10 different agentic systems, and develop a taxonomy of 15 failure root causes across agent, workflow, and platform levels using grounded theory annotation. The taxonomy significantly improves LLM-based root cause identification accuracy from ~10% to 28-34%, with agent-level failures (especially knowledge limitations and prompt design defects) dominating the dataset. Counterfactual repair experiments validate annotation reliability, showing diagonal dominance in repair success rates. The study provides actionable insights for improving system robustness and establishes benchmarks for automated failure diagnosis in multi-agent platforms.

## Method Summary
The authors constructed AgentFail by collecting failure logs from 10 agentic systems (5 from Dify, 5 from Coze) and annotating them using grounded theory with three experts achieving high inter-rater agreement (Cohen's κ 0.85→1.0). They developed a 15-category taxonomy spanning agent-level (F1.1-F1.7), workflow-level (F2.1-F2.7), and platform-level (F3.1-F3.2) failures. The taxonomy was validated through counterfactual repair experiments where targeted fixes converted failures to successes with high success rates on diagonal categories (90-96%). Six LLMs were benchmarked on root cause identification using three search strategies (All-at-once, Step-by-step, Binary search) with and without taxonomy guidance, showing 2.5-3x accuracy improvement when taxonomy was provided.

## Key Results
- Taxonomy improves LLM-based root cause identification accuracy from ~10% to 28-34% across six models
- Maximum accuracy achieved: 33.6% (DeepSeek-R1 with step-by-step strategy)
- Agent-level failures dominate (knowledge/reasoning limitations and prompt design defects most frequent)
- Counterfactual repair validation shows diagonal dominance (90-96% success rates) confirming annotation reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A structured failure taxonomy improves LLM-based root cause identification accuracy by approximately 2.5-3x compared to unguided classification.
- **Mechanism:** The taxonomy constrains the output space and provides explicit category definitions, reducing ambiguity in LLM predictions. Without guidance, LLMs must infer failure categories from scratch; with taxonomy, they perform structured classification against known patterns.
- **Core assumption:** The taxonomy categories sufficiently cover the failure modes present in the dataset, and the LLM can correctly map observed symptoms to taxonomy entries.
- **Evidence anchors:**
  - [abstract]: "Results show that the taxonomy can largely improve the performance... the accuracy of root cause identification reaches at most 33.6%"
  - [Section 5, Table 3]: Without taxonomy: 8.3-13.0% accuracy; With taxonomy: 24.1-33.6% across six LLMs
  - [corpus]: Neighbor papers on root cause analysis in microservices suggest structured approaches improve diagnosis, though in different domains.
- **Break condition:** If novel failure modes emerge outside the taxonomy, or if the taxonomy categories are too coarse-grained for the specific system being diagnosed, guidance benefits would diminish.

### Mechanism 2
- **Claim:** Counterfactual repair experiments validate annotation correctness by demonstrating that targeted fixes for annotated root causes successfully convert failures to successes.
- **Mechanism:** If annotation claims root cause D, and applying repair strategy R(D) converts the trajectory from failure to success, this provides causal evidence that D was correctly identified. The diagonal dominance in the repair confusion matrix (e.g., 90.1% for D1, 95.6% for D2) indicates annotations capture genuine causal factors.
- **Core assumption:** The repair strategies are sufficiently specific to the root cause categories and do not inadvertently fix unrelated issues.
- **Evidence anchors:**
  - [Section 3.4]: "As shown in the figure, the diagonal values are consistently the highest (e.g., 90.1% for D1... 95.6% for D2... 96.3% for D5)"
  - [Section 3.4]: "This result provides strong evidence for the accuracy and reliability of our annotations."
  - [corpus]: Limited direct corpus evidence on counterfactual repair for agentic systems; this appears to be a novel validation approach in this context.
- **Break condition:** If off-diagonal repairs (e.g., fixing D3 resolves D1-labeled failures) become dominant, this would indicate either annotation errors or shared causal pathways between categories.

### Mechanism 3
- **Claim:** Agent-level failures dominate platform-orchestrated agentic systems, with knowledge/reasoning limitations (F1.4) and prompt design defects (F1.5) being the most frequent root causes.
- **Mechanism:** Low-code platforms abstract away infrastructure complexity, shifting failure modes toward the LLM and prompt layer rather than orchestration or platform layers. Users configure agents visually but still must craft effective prompts and provide adequate knowledge—skills that remain challenging.
- **Core assumption:** The sampled systems (5 from Dify, 5 from Coze) are representative of typical platform-orchestrated agentic systems.
- **Evidence anchors:**
  - [Section 4.2, Figure 3a]: "knowledge and reasoning limitations (F1.4) and poor prompt design (F1.5) are the most frequent categories, with more than fifty instances each"
  - [Section 4.2]: "Overall, we observe that agent-level failures dominate the dataset."
  - [corpus]: Corpus papers on multi-agent failures also identify specification and alignment issues as common, but focus less on prompt design specifically.
- **Break condition:** If systems were built by more experienced practitioners with rigorous prompt engineering practices, workflow-level failures might become relatively more prominent.

## Foundational Learning

- **Concept: Grounded Theory Annotation**
  - **Why needed here:** Understanding how the failure taxonomy was constructed helps users trust and apply it. Grounded theory builds categories from empirical data rather than imposing pre-conceived frameworks.
  - **Quick check question:** Can you explain why grounded theory annotation might produce more practically useful failure categories than a purely theoretical taxonomy?

- **Concept: Decisive Error Localization**
  - **Why needed here:** The paper defines "decisive error" using counterfactual intervention (Eq. 1-2). Understanding this helps practitioners think systematically about which error, among many in a long trace, actually caused the failure.
  - **Quick check question:** Given a failed trajectory with three errors at different steps, how would you determine which is the decisive error using the earliest-in-time principle?

- **Concept: Execution Termination vs. Suboptimal Quality**
  - **Why needed here:** The paper distinguishes failures that crash the workflow from those that complete but produce poor results. Different root causes have different impact profiles (Table 2), which should inform debugging priorities.
  - **Quick check question:** If your system completes execution but produces incorrect answers, which failure categories (agent/workflow/platform level) should you investigate first based on Table 2?

## Architecture Onboarding

- **Component map:**
  - Agent-level nodes: LLM nodes, Tool nodes, Knowledge retrieval nodes — where reasoning, formatting, and retrieval failures occur (F1.1-F1.7)
  - Workflow-level orchestration: Serial/parallel/branching/looping structures, input validation, conditional logic — where coordination failures occur (F2.1-F2.7)
  - Platform layer: Runtime environment, network, model service availability — where infrastructure failures occur (F3.1-F3.2)
  - Taxonomy integration point: LLM prompts for automated diagnosis (Appendix A.9 shows prompt templates with/without taxonomy)

- **Critical path:**
  1. Collect failure log → 2. Determine if execution terminated or completed with suboptimal quality → 3. Apply taxonomy-guided LLM diagnosis → 4. Validate via targeted repair → 5. Iterate on system design

- **Design tradeoffs:**
  - Step-by-step vs. Binary Search vs. All-at-once diagnosis: Step-by-step achieves highest accuracy (33.6% for DeepSeek-R1) but requires more LLM calls; All-at-once is fastest but slightly less accurate (30.0%)
  - Annotation effort vs. coverage: 307 logs from 10 systems provides initial taxonomy, but may not cover all failure modes in production systems
  - Repair specificity vs. generality: Highly specific repairs validate annotations but may not generalize; some off-diagonal repair success suggests shared causal pathways

- **Failure signatures:**
  - F1.2 (Response Formatting Error): High termination rate (78.3%) — system crashes trying to parse malformed output
  - F1.4 (Knowledge/Reasoning Limitation): Low termination (2.1%), high suboptimal quality (72.5%) — system completes but answers are wrong
  - F2.3 (Loops/Deadlocks): Causes infinite execution, requires timeout detection
  - F3.x (Platform failures): Very high termination rates (84-88%) — immediate system failure

- **First 3 experiments:**
  1. **Baseline diagnosis without taxonomy:** Run your own failure logs through an LLM (GPT-4o or similar) with the "Prompt without taxonomy" template from Appendix A.9. Establish baseline accuracy.
  2. **Taxonomy-guided diagnosis:** Provide the same logs with the full taxonomy (Figure 2) in the prompt. Compare accuracy improvement—expect 15-20 percentage point gain based on paper results.
  3. **Targeted repair validation:** For a subset of diagnosed failures, implement the repair strategies from Appendix A.10 (e.g., enforce strict JSON output for F1.2, add input validation for F2.1) and verify whether failures convert to successes. This validates whether the taxonomy diagnoses are actionable.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the accuracy of automated root cause identification be improved beyond the observed 33.6% ceiling?
- **Basis in paper:** [explicit] Section 5 states that despite using the taxonomy as guidance, the maximum accuracy of root cause identification remains at 33.6%, indicating the task is "still remains challenging."
- **Why unresolved:** Failure logs are long and complex, causing errors to propagate through nodes; LLMs struggle to trace symptoms back to the triggering node.
- **What evidence would resolve it:** Novel diagnostic algorithms or specialized reasoning models that demonstrate significantly higher precision (>50%) on the AgentFail benchmark.

### Open Question 2
- **Question:** Can the taxonomy enable fully automated "self-healing" or repair mechanisms rather than just manual counterfactual fixes?
- **Basis in paper:** [inferred] Section 3.4 validates annotation reliability using counterfactual reasoning and manual repair, and Section 1 identifies repair as a key follow-up step, yet the paper provides no automated repair solution.
- **Why unresolved:** The study focuses on diagnosing and validating the root causes, not on implementing autonomous agents capable of executing the identified repairs.
- **What evidence would resolve it:** An automated system that utilizes the taxonomy to successfully patch prompt defects or workflow dependencies without human intervention.

### Open Question 3
- **Question:** Does the failure root cause taxonomy generalize to code-centric multi-agent frameworks (e.g., AutoGen) versus the template-based platforms studied?
- **Basis in paper:** [inferred] Section 7.1 distinguishes "platform-orchestrated" systems from "hand-crafted" or code-based ones, and the dataset is limited to Dify and Coze.
- **Why unresolved:** The specific constraints of low-code platforms (e.g., limited node types) may skew the failure distribution compared to fully programmable frameworks.
- **What evidence would resolve it:** Successful application of the AgentFail taxonomy to failure logs from non-platform, code-first agentic systems with high inter-annotator agreement.

## Limitations

- The dataset of 307 failure logs from only 10 systems may not capture the full diversity of platform-orchestrated agentic systems
- Despite taxonomy guidance, maximum accuracy remains at 33.6%, suggesting fundamental limitations in automated diagnosis
- Counterfactual repair validation uses predefined repair strategies that may not generalize to all system configurations

## Confidence

- **High Confidence**: The taxonomy construction methodology using grounded theory annotation is sound and the validation through counterfactual repair provides strong causal evidence for annotation correctness.
- **Medium Confidence**: The improvement from ~10% to ~28-34% accuracy with taxonomy guidance is well-demonstrated, but the absolute accuracy ceiling of 33.6% suggests the approach has fundamental limitations.
- **Low Confidence**: The generalizability of findings to other low-code platforms beyond Dify and Coze remains untested, and the repair strategies' effectiveness on novel system architectures is uncertain.

## Next Checks

1. **Cross-platform validation**: Apply the taxonomy and diagnosis methodology to 5-10 additional low-code agentic platforms (e.g., AutoGen, CrewAI, LangChain) to test taxonomy generalizability and identify any missing failure categories.

2. **Real-time deployment test**: Implement the taxonomy-guided diagnosis in a live production environment over 30 days, measuring both accuracy and operational impact (mean time to resolution, false positive rates) compared to current debugging practices.

3. **Expert practitioner evaluation**: Have 10 experienced agentic system developers use the taxonomy to diagnose 20 failure logs each, measuring inter-rater agreement and comparing their accuracy to the LLM-based approaches to establish whether the taxonomy provides practical value beyond automated diagnosis.