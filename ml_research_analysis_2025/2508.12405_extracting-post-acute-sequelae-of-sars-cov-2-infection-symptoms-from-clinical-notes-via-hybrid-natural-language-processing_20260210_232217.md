---
ver: rpa2
title: Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical
  Notes via Hybrid Natural Language Processing
arxiv_id: '2508.12405'
source_url: https://arxiv.org/abs/2508.12405
tags:
- notes
- symptom
- pasc
- clinical
- symptoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a hybrid natural language processing pipeline
  to extract Post-Acute Sequelae of SARS-CoV-2 Infection (PASC) symptoms from clinical
  notes. The pipeline integrates rule-based named entity recognition with BERT-based
  assertion detection, using a comprehensive PASC lexicon of 25 symptom categories
  and 798 UMLS concepts.
---

# Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing

## Quick Facts
- arXiv ID: 2508.12405
- Source URL: https://arxiv.org/abs/2508.12405
- Reference count: 40
- A hybrid NLP pipeline extracts PASC symptoms from clinical notes with average F1 score of 0.82 (internal) and 0.76 (external)

## Executive Summary
This study developed MedText, a hybrid NLP pipeline to extract Post-Acute Sequelae of SARS-CoV-2 Infection (PASC) symptoms from clinical notes. The pipeline integrates rule-based named entity recognition with BERT-based assertion detection, using a comprehensive PASC lexicon of 25 symptom categories and 798 UMLS concepts. Evaluated on 160 curated notes across 11 health systems, the model achieved strong performance with average F1 scores of 0.82 in internal validation and 0.76 in external validation for assertion detection. Processing time averaged 2.45 seconds per note. The pipeline effectively and efficiently extracts PASC symptoms from unstructured clinical narratives, demonstrating potential for improving diagnosis and large-scale research.

## Method Summary
The pipeline uses a two-stage approach: first, a rule-based NER module identifies symptom mentions using a curated PASC lexicon mapped to UMLS concepts; second, a fine-tuned BiomedBERT model classifies each mention as "Present" or "Non-positive" (absent, hypothetical, or conditional). The lexicon contains 25 symptom categories and 798 UMLS concepts with all synonyms and narrower children included. The assertion model was fine-tuned on 30 WCM intake notes plus the public i2b2 dataset. Evaluation used 160 notes from 11 health systems, with manual annotation for ground truth. Spearman correlation tests confirmed strong agreement across sites for both positive and negative symptom mentions.

## Key Results
- Assertion detection achieved F1 scores of 0.82 (internal) and 0.76 (external) across 11 health systems
- Processing time averaged 2.45 seconds per note
- Strong correlation between sites: ρ > 0.83 for positive mentions and ρ > 0.72 for negative mentions
- Rule-based NER extracted 640 correct symptom mentions compared to GPT-4's 235

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A curated, ontology-mapped lexicon provides higher recall for PASC symptom extraction than general-purpose language models.
- **Mechanism:** The pipeline uses a clinician-curated lexicon mapped to UMLS/SNOMED CT hierarchies. By matching specific synonyms and "children" concepts (narrower terms) via rule-based Named Entity Recognition (NER), the system captures explicit clinical terminology that statistical models might overlook if the terms are rare or low-frequency in pre-training data.
- **Core assumption:** PASC symptoms are documented using terminology that can be exhaustively listed and mapped to standard ontologies.
- **Evidence anchors:** [abstract] "comprehensive PASC lexicon of 25 symptom categories and 798 UMLS concepts"; [section] "We included all synonyms from the UMLS concepts... [and] all children of a concept."
- **Break condition:** If clinicians document symptoms using entirely novel metaphors, slang, or shorthand absent from UMLS/SNOMED, the rule-based NER will likely fail to detect the entity.

### Mechanism 2
- **Claim:** Deep learning assertion detection is required to distinguish the *presence* of a symptom from its *mention* in complex clinical narratives.
- **Mechanism:** While rules identify the keyword (e.g., "chest pain"), a fine-tuned BiomedBERT model analyzes the context window to classify the assertion status (Present vs. Absent/Hypothetical). This handles negations ("denies chest pain") and history ("history of chest pain") which simple keyword matching would falsely flag as positive.
- **Core assumption:** The contextual patterns of negation and uncertainty in PASC notes are sufficiently similar to the general clinical notes used for pre-training (e.g., i2b2 dataset).
- **Evidence anchors:** [abstract] "integrates rule-based named entity recognition with BERT-based assertion detection"; [section] "BiomedBERT achieved an average F1 score of 0.82... for assertion detection."
- **Break condition:** If a sentence contains multiple symptoms with different assertion statuses separated by long distances (e.g., "Patient denies A, B, but reports C"), the model may misclassify "C" if it falls outside the effective attention window or is confused by complex syntax.

### Mechanism 3
- **Claim:** Hybrid architectures offer a performance-efficiency trade-off suitable for large-scale population studies.
- **Mechanism:** By delegating the "search" (NER) to fast, deterministic rules and the "classification" (Assertion) to the slower GPU-dependent BERT model, the pipeline optimizes throughput. This avoids the computational cost of running a generative LLM (like GPT-4) on the entire text for extraction, while maintaining high accuracy on the classification step.
- **Core assumption:** Processing speed (approx. 2.5 seconds/note) is a critical constraint for batch processing tens of thousands of records.
- **Evidence anchors:** [abstract] "processed each note at 2.448 ± 0.812 seconds on average"; [section] "GPT-4... missed a notable amount of symptom/synonym-related tokens compared to the 640 correct symptom mentions by our rule-based NER module."
- **Break condition:** If the goal shifts from extraction to complex reasoning (e.g., inferring a diagnosis from implied symptoms not explicitly stated), the rule-based NER component will fail to pass relevant inputs to the assertion model.

## Foundational Learning

- **Concept: Assertion Status (Negation & Speculation)**
  - **Why needed here:** In clinical notes, mentioning a symptom often means it is *absent* (e.g., "Patient denies fever"). Treating all mentions as positive would drastically overestimate prevalence.
  - **Quick check question:** In the sentence "Patient is concerned about potential heart palpitations," is the assertion status "Present," "Absent," or "Hypothetical"?

- **Concept: UMLS (Unified Medical Language System) Metathesaurus**
  - **Why needed here:** The pipeline relies on mapping clinical text to a standard ontology (UMLS) to group synonyms (e.g., "heart attack" and "myocardial infarction") into unified concepts.
  - **Quick check question:** If a new slang term for fatigue emerges that is not in UMLS, would the rule-based NER module detect it?

- **Concept: Transfer Learning (Fine-tuning BERT)**
  - **Why needed here:** The assertion model is not trained from scratch; it is a pre-trained BiomedBERT model fine-tuned on a smaller, labeled dataset (i2b2 + WCM notes). This leverages general language understanding for a specific classification task.
  - **Quick check question:** Why is fine-tuning generally preferred over training a BERT model from scratch for a specific clinical task?

## Architecture Onboarding

- **Component map:** Raw clinical note text -> Preprocessor (section splitter & sentence tokenizer) -> NER Module (SpaCy PhraseMatcher with PASC Lexicon) -> Assertion Module (fine-tuned BiomedBERT) -> Structured output (Symptom Concept + Assertion Status)

- **Critical path:** The **NER Module** defines the ceiling for recall (if it's not extracted, it can't be classified). The **Assertion Module** defines the precision (if it's misclassified, the prevalence data is noisy).

- **Design tradeoffs:**
  - **Rules vs. LLM for NER:** The authors found GPT-4 had high precision on assertion but lower recall for extraction compared to the rule-based system. The tradeoff prioritized high recall (finding all symptoms) via rules, accepting that rules are brittle to new synonyms.
  - **Speed vs. Complexity:** The hybrid model runs at ~2.5s/note on a Tesla T4. A full generative LLM pipeline would likely be significantly slower and more expensive per note.

- **Failure signatures:**
  - **"Negation Scope" Errors:** Symptoms listed far from the word "negative for" (e.g., due to excessive spacing) are falsely labeled "Present."
  - **History Confusion:** Symptoms prefaced by "history of" are sometimes classified as "Present" rather than "Past."

- **First 3 experiments:**
  1. **Baseline Validation:** Run the pre-trained pipeline on the provided 30-note internal validation set and verify the F1 score replicates the reported ~0.82.
  2. **Negation Stress Test:** Feed the pipeline synthetic sentences with increasing distance between "denies" and the symptom keyword to find the breaking point of the assertion model.
  3. **Lexicon Gap Analysis:** Run the pipeline on a small set of notes and manually review "unrelated" or missed terms to identify potential candidates for lexicon expansion.

## Open Questions the Paper Calls Out

- **Question:** Does integrating structured EHR data with extracted unstructured symptoms improve diagnostic accuracy for PASC compared to using unstructured data alone?
  - **Basis in paper:** [explicit] The authors state they will address limitations by "integrating structured EHR data and COVID-related symptoms extracted from unstructured clinical notes."
  - **Why unresolved:** The current study evaluated the pipeline solely on unstructured clinical notes, leaving the synergistic effects of combining these with structured codes untested.
  - **What evidence would resolve it:** A comparative study measuring the sensitivity and specificity of PASC diagnosis using the hybrid pipeline versus a multimodal model that includes billing codes and lab results.

- **Question:** To what extent do different clinical note types contribute to identifying high-risk PASC patients?
  - **Basis in paper:** [explicit] The authors identify the need for "evaluating how different note types contribute to identifying high-risk patients" as a key future direction.
  - **Why unresolved:** The current study primarily utilized intake progress notes, which may overrepresent acute or prominent symptoms while missing longitudinal data found in other note types.
  - **What evidence would resolve it:** A comparative analysis of symptom extraction yields and risk prediction accuracy across intake, follow-up, and specialist notes within the same patient cohort.

- **Question:** Can expanding the annotation protocol to review entire notes improve the pipeline's ability to capture all possible PASC symptoms?
  - **Basis in paper:** [explicit] The authors acknowledge it is unclear "whether all possible symptoms are captured" and propose "expanding the annotated datasets... to annotate all possible symptoms given a note."
  - **Why unresolved:** The current annotation process only reviewed symptoms pre-identified by the lexicon, potentially missing false negatives (symptoms present but not in the lexicon).
  - **What evidence would resolve it:** A "negative" annotation study where human annotators label all symptoms in a note without lexicon constraints to calculate the true recall of the current lexicon.

## Limitations
- Reliance on private training data (30 WCM notes) prevents independent replication of reported F1 scores
- Rule-based NER recall is fundamentally bounded by completeness of the PASC lexicon
- Validation sets, while multi-site, remain relatively small (160 notes total)

## Confidence
- **High Confidence:** The hybrid architecture design (rule-based NER + BERT assertion detection) is sound and well-explained. The performance metrics (F1 scores, processing time) are specific and verifiable with the provided data.
- **Medium Confidence:** The claim that the pipeline "effectively and efficiently extracts PASC symptoms" is supported by internal validation but depends on the representativeness of the test sets and the generalizability beyond the 11 health systems.
- **Low Confidence:** The assertion that this pipeline is "potentially suitable for large-scale research" lacks evidence of scalability testing beyond the reported 160 notes. The claim that it "improves diagnosis" is aspirational rather than demonstrated.

## Next Checks
1. **External Validation Replication:** Obtain a new, independent set of 100+ clinical notes from a different health system and run the pre-trained pipeline to verify the external F1 score of 0.76 holds.
2. **Lexicon Coverage Audit:** Manually review a sample of notes with high symptom density to identify any PASC-related terms missed by the NER module, quantifying the recall ceiling imposed by the current lexicon.
3. **Negation Scope Stress Test:** Systematically generate test sentences with varying distances between negation keywords ("denies," "negative for") and symptom mentions to empirically determine the assertion model's breaking point for negation scope.