---
ver: rpa2
title: 'Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs'
arxiv_id: '2509.13480'
source_url: https://arxiv.org/abs/2509.13480
tags:
- italian
- language
- https
- bertscore
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic evaluation of state-of-the-art
  large language models for Italian gender-neutral rewriting, using a two-dimensional
  framework to measure both neutrality and semantic fidelity. The authors compare
  few-shot prompting across multiple models, fine-tune selected models, and apply
  targeted data cleaning to boost task relevance.
---

# Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs

## Quick Facts
- arXiv ID: 2509.13480
- Source URL: https://arxiv.org/abs/2509.13480
- Reference count: 40
- Open-weight LLMs outperform the only existing dedicated model for Italian GNR, with fine-tuned models matching larger models at a fraction of the size.

## Executive Summary
This paper introduces the first systematic evaluation of state-of-the-art large language models for Italian gender-neutral rewriting (GNR), using a two-dimensional framework to measure both neutrality and semantic fidelity. The authors compare few-shot prompting across multiple models, fine-tune selected models, and apply targeted data cleaning to boost task relevance. Results show that open-weight LLMs outperform the only existing dedicated model for Italian GNR, while fine-tuned models match or exceed the best open-weight LLM's performance at a fraction of its size. The study also reveals a trade-off between optimizing training data for neutrality versus meaning preservation.

## Method Summary
The study uses a two-dimensional evaluation framework: (1) Neutrality measured via LLM-as-a-Judge (GPT-4o) for binary classification of whether outputs are gender-neutral, and (2) Meaning preservation measured via BERTScore between output and input. The authors compare few-shot prompting with 8 exemplars across models including Llama 3.1, Qwen3, and GPT-4.1, then fine-tune selected models using LoRA (rank 32) on a repurposed dataset of 162,778 synthetic gendered-neutral pairs. They create two training splits: "full" (all data) and "clean" (top 50% by BERTScore). All experiments use the mGeNTE benchmark test set (750 sentences) and run on 4Ã— NVIDIA A100 GPUs.

## Key Results
- Open-weight LLMs (GPT-4.1, Llama 3.1 70B) outperform the existing Inclusively model on Italian GNR
- Fine-tuned models (Llama 3.1 8B, Qwen3 14B) match or exceed best open-weight LLM performance at a fraction of the size
- A trade-off exists between optimizing training data for neutrality (full split) versus meaning preservation (clean split)

## Why This Works (Mechanism)

### Mechanism 1: In-Context Strategy Activation via Prompting
Large Language Models can perform gender-neutral rewriting in grammatical-gender languages by activating specific linguistic strategies (e.g., using collective nouns or periphrasis) through detailed instructions and examples, rather than requiring explicit rule-based programming. The prompt provides a "system" role message defining valid neutralization strategies and exemplars. The model uses its pre-trained linguistic knowledge to map gendered input structures to these acceptable neutral output structures via in-context learning.

### Mechanism 2: Task Specialization via LoRA Fine-Tuning
Parameter-efficient fine-tuning (LoRA) adapts general-purpose LLMs to the specific constraints of GNR, allowing smaller models (e.g., 8B parameters) to rival the performance of significantly larger untuned models (e.g., 70B parameters). Low-Rank Adaptation updates a small subset of weights to maximize the probability of generating gender-neutral outputs for given gendered inputs.

### Mechanism 3: The Neutrality-Fidelity Trade-off (Metric Gaming)
Rigorous filtering of training data for semantic similarity (high BERTScore) conditions models to preserve input text excessively, impairing their ability to perform the structural changes necessary for neutralization. Filtering the training set to the top 50% of BERTScore pairs creates a dataset where inputs and outputs are lexically and semantically very close. The model effectively learns to "game" the similarity metric by minimizing edits, which directly conflicts with the structural rewriting often required for Italian neutrality.

## Foundational Learning

- **Concept: Grammatical Gender vs. Natural Gender**
  - Why needed here: Unlike English, where gender is largely semantic, Italian requires gender marking on articles, nouns, and adjectives. You cannot simply swap a pronoun; you must restructure the sentence (e.g., "Tutti i senatori" -> "Ogni membro del Senato").
  - Quick check question: Does the rewriting solution rely on finding a gender-neutral synonym, or does it require changing the sentence structure to avoid gendered morphology entirely?

- **Concept: BERTScore vs. BLEU**
  - Why needed here: Standard metrics like BLEU penalize legitimate rewriting because they look for exact token overlap. BERTScore is used here to capture semantic similarity despite lexical divergence.
  - Quick check question: If a model rewrites "He is a doctor" to "The doctor is present," would BLEU capture the semantic equivalence effectively?

- **Concept: Prompt Engineering Strategies for GNR**
  - Why needed here: The paper demonstrates that specific prompting strategies (e.g., "Rewrite" vs. "GFG" prompts) and language (Italian vs. English prompts) significantly impact performance.
  - Quick check question: Does the prompt explicitly forbid non-standard characters (like asterisks) if the goal is "conservative" rather than "innovative" neutrality?

## Architecture Onboarding

- **Component map:** Input (Gendered Reference) -> Generator (LLM or Fine-tuned Model) -> Evaluator (LLM-as-a-Judge for Neutrality, BERTScore for Fidelity)
- **Critical path:**
  1. Data Prep: Filter raw data using BERTScore (be aware of the trade-off). Format as chat (User: Gendered, Assistant: Neutral).
  2. Training: Apply LoRA (rank 32) to a base model (e.g., Qwen3 14B). Use early stopping.
  3. Inference: Generate outputs using the "Rewrite" prompt format for best adherence to guidelines.
  4. Eval: Compare outputs against human reference using the dual-axis framework.

- **Design tradeoffs:**
  - Size vs. Specialization: Larger models (70B) perform well zero-shot, but smaller fine-tuned models (8B-14B) are more efficient and competitive.
  - Clean vs. Full Data: "Clean" data preserves meaning (High BERTScore) but lowers neutrality. "Full" data maximizes neutrality but risks hallucination (Lower BERTScore).
  - Prompt Complexity: Detailed prompts ("Rewrite") guide the model better than concise ones ("GFG"), but may constrain creativity.

- **Failure signatures:**
  - Copy-cat: High BERTScore, Low Neutrality (Model ignored the task, common in "Clean" training).
  - Hallucinator: Low BERTScore, High Neutrality (Model neutralized effectively but invented or lost details, common in "Full" training).
  - Over-refusal: Model claims it cannot rewrite or produces safety refusals.

- **First 3 experiments:**
  1. Establish Baseline: Run the `Inclusively` model and `GPT-4.1` on the `Set-N` of mGeNTE to confirm the performance bounds observed in the paper.
  2. Data Ablation: Fine-tune a mid-sized model (e.g., Llama 3.1 8B) on both `full` and `clean` splits to reproduce the Neutrality vs. Fidelity trade-off.
  3. Metric Validation: Calculate the correlation between BERTScore and BARTScore on your fine-tuned outputs to verify if the model is "gaming" the similarity metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the creation of dedicated, high-quality parallel data enable open-weight models to match the performance of commercial systems like GPT-4.1 in Italian gender-neutral rewriting?
- Basis in paper: The conclusion states that "Future work should... create dedicated, high-quality parallel data to aim at reaching the performance of the commercial system with open-weight models."
- Why unresolved: Current experiments relied on repurposed synthetic data, and a performance gap remains between the best fine-tuned open models and GPT-4.1.
- What evidence would resolve it: Performance parity or superiority of open-weight models over GPT-4.1 on neutrality and semantic fidelity metrics when fine-tuned on a newly created, human-verified dataset.

### Open Question 2
- Question: What data curation strategies can effectively balance the trade-off between optimizing for gender neutrality and preserving semantic fidelity?
- Basis in paper: The authors explicitly call for "data curation strategies that strike a balance between neutrality and similarity" in the conclusion.
- Why unresolved: The study found that filtering data for high similarity (BERTScore) hampered neutrality gains, while using all data improved neutrality but risked meaning divergence.
- What evidence would resolve it: A filtering methodology that improves neutrality scores significantly while maintaining BERTScore within the human-level range, without causing the model to over-fit on the similarity metric.

### Open Question 3
- Question: Does optimizing training data selection using BERTScore inadvertently cause models to "game" the metric by avoiding necessary structural changes?
- Basis in paper: The authors hypothesize that the `clean` dataset was "excessively conditioned" by the BERTScore filtering, leading to a drop in Spearman correlation with BARTScore.
- Why unresolved: The paper suggests high linear correlation but ranking inconsistencies between metrics indicate the model may have learned to reproduce features rewarded by BERTScore rather than performing the rewriting task.
- What evidence would resolve it: A fine-grained linguistic analysis of outputs from models trained on BERTScore-optimized data, showing whether they fail to apply required neutralization strategies (e.g., passive voice, collective nouns) that lower similarity scores.

## Limitations
- The evaluation framework relies heavily on LLM-as-a-judge for neutrality assessment, which introduces potential subjectivity and model-specific biases.
- The study uses a single human reference dataset for mGeNTE, limiting generalizability to other domains or stylistic contexts.
- The focus on Italian, while novel, means results may not transfer to other grammatical-gender languages with different morphological structures.

## Confidence

**High confidence:** The finding that open-weight LLMs outperform the existing `Inclusively` model on Italian GNR is well-supported by the two-dimensional evaluation framework and multiple model comparisons.

**Medium confidence:** The neutrality-fidelity trade-off observation is credible but could benefit from additional experiments isolating whether this stems from BERTScore's limitations as a metric, the data filtering approach, or both.

**Low confidence:** The assertion that the trade-off is primarily due to models "gaming" BERTScore is speculative - alternative explanations (such as fundamental constraints in how structural rewriting affects semantic similarity) are not adequately ruled out.

## Next Checks

1. **Metric validation experiment:** Calculate the correlation between BERTScore and BARTScore across model outputs to determine if models are indeed optimizing for BERTScore specifically, or if the trade-off exists across semantic similarity metrics.

2. **Zero-shot generalization test:** Evaluate the best-performing fine-tuned models on an out-of-domain Italian GNR dataset to assess whether the trade-off observed in mGeNTE holds across different text types and domains.

3. **Ablation study on data filtering:** Create a middle-ground dataset with BERTScore scores between the "full" and "clean" splits, then fine-tune models on this intermediate dataset to map the exact shape of the neutrality-fidelity curve and identify where optimal performance occurs.