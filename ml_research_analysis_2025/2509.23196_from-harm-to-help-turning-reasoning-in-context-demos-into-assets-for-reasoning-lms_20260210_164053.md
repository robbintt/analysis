---
ver: rpa2
title: 'From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning
  LMs'
arxiv_id: '2509.23196'
source_url: https://arxiv.org/abs/2509.23196
tags:
- reasoning
- question
- answer
- arxiv
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem that reasoning large language models
  (RLMs) degrade in performance when given few-shot Chain-of-Thought demonstrations,
  even high-quality ones. The core method, Insight-to-Solve (I2S), converts demonstrations
  into reusable insights and applies them to the target question, optionally followed
  by self-refinement (I2S+).
---

# From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs

## Quick Facts
- arXiv ID: 2509.23196
- Source URL: https://arxiv.org/abs/2509.23196
- Authors: Haonan Wang; Weida Liang; Zihang Fu; Nie Zheng; Yifan Zhang; Yao Tong; Tongyao Zhu; Hao Jiang; Chuang Li; Jiaying Wu; Kenji Kawaguchi
- Reference count: 40
- Key outcome: I2S+ improves GPT-4.1's AIME'25 score by 14.0%, and o1-mini by 2.7% on AIME and 1.7% on GPQA

## Executive Summary
This paper addresses a critical problem in reasoning large language models (RLMs): few-shot Chain-of-Thought demonstrations can degrade performance rather than improve it, even when those demonstrations are high-quality. The authors propose Insight-to-Solve (I2S), a method that converts demonstrations into reusable insights and applies them to target questions through a structured pipeline. I2S+ adds iterative self-refinement. Extensive experiments on AIME'25, GPQA, and General Reasoning tasks show consistent improvements over direct answering and test-time scaling baselines.

## Method Summary
I2S converts reasoning demonstrations into reusable insights through a three-stage pipeline: (1) Comparison stage generates structured comparison between demo question and target question, (2) Analysis stage extracts transferable insights from the demonstration conditioned on this comparison, and (3) Reasoning generation produces the target reasoning trace using only the question and extracted insights, deliberately withholding the demonstration's reasoning trace. I2S+ adds an iterative self-refinement loop where the model suggests issues, reviews candidate corrections, and refines its reasoning trace. The method uses retrieval to find nearest demonstrations from a bank of 114k+ examples, and operates through question-conditioned forward passes rather than in-context conditioning.

## Key Results
- I2S+ improves GPT-4.1's AIME'25 score by 14.0% compared to direct answering
- o1-mini shows 2.7% improvement on AIME and 1.7% on GPQA with I2S+
- On AIME'25, refinement shows clear early returns: most gains appear in the first 1-2 iterations (7B: +3.33; 14B: +6.00)
- I2S outperforms majority@3 at same question-conditioned calls and matches majority@32 on some tasks

## Why This Works (Mechanism)

### Mechanism 1: Semantic Misleading Mitigation via Decoupled Generation
- Claim: Direct conditioning on demonstration content causes verbatim copying when surface similarity is high.
- Mechanism: Withholding the demonstration's reasoning trace during target reasoning generation and forcing construction guided only by abstracted insights breaks the shortcut between demonstrations and answers.
- Core assumption: The model's tendency to conflate superficially similar questions is reduced when intermediate copying paths are removed.
- Evidence: Model transplants "split into two groups, sum to around 18" heuristic from demo to target, ignoring structural requirements for divisibility by 22.

### Mechanism 2: Strategy Extraction Failure Mitigation via Explicit Comparison-Analysis
- Claim: RLMs fail to transfer reasoning strategies when demonstrations are consumed passively; explicit extraction scaffolds transfer.
- Mechanism: The comparison stage forces structural analysis, and the analysis stage filters irrelevant details while retaining transferable strategies. This two-step abstraction converts implicit patterns into explicit guidance.
- Core assumption: Explicit verbalization of strategies improves their application compared to implicit pattern recognition.
- Evidence: Model mis-extracts adjacency enumeration method and mis-applies pairwise constraint instead of triplet-wise constraint.

### Mechanism 3: Iterative Self-Refinement Improves Trace Quality (I2S+)
- Claim: Additional compute spent on structured self-refinement yields accuracy gains, primarily in early iterations.
- Mechanism: The Suggest-Review-Refine loop identifies issues (computational, logical, assumption-based, interpretation), evaluates candidate corrections, and integrates the best correction into the reasoning trace.
- Core assumption: The model can reliably critique its own reasoning when given structured prompts.
- Evidence: On AIME'25, most gains appear in the first 1-2 iterations; larger models exploit additional reasoning passes more effectively.

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: The entire paper addresses how few-shot demonstrations affect RLM performance; understanding ICL baselines is prerequisite.
  - Quick check question: What happens to accuracy when you add 1-shot vs. 3-shot CoT demonstrations to an RLM on AIME'25?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The failure modes (semantic misleading, strategy transfer failure) are specific to reasoning traces in CoT format.
  - Quick check question: Why might a detailed CoT demonstration hurt rather than help a model trained with RLVR?

- **Test-Time Scaling (Parallel vs. Sequential)**
  - Why needed here: I2S is framed as sequential scaling; understanding majority@N baselines contextualizes efficiency claims.
  - Quick check question: How does I2S compare to majority@3 in question-conditioned forward passes, and which achieves higher accuracy?

## Architecture Onboarding

- **Component map:**
  - Retrieval -> Comparison Stage -> Analysis Stage -> Reasoning Generation -> (Optional) Self-Refinement Loop -> Answer Generation

- **Critical path:**
  1. Retrieve top-1 demonstration
  2. Run Comparison → Analysis → Reasoning Generation (3 question-conditioned calls)
  3. If I2S+: Run up to 3 refinement iterations (Suggest/Review/Refine per iteration)
  4. Generate final answer from refined trace

- **Design tradeoffs:**
  - Single vs. multiple demonstrations: Paper restricts to 1-shot due to context length (demo traces can exceed 13k tokens) and semantic misleading risk
  - I2S vs. I2S+: I2S+ adds ~2-3x compute for +1-6% accuracy on math; smaller models may not benefit on open-ended tasks
  - Sequential vs. parallel scaling: I2S outperforms majority@3 at same question-conditioned calls; majority@32 slightly edges I2S+ on AIME but not GPQA

- **Failure signatures:**
  - Semantic misleading: Model copies intermediate steps verbatim from demo
  - Strategy transfer failure: Model applies wrong abstraction (e.g., pairwise instead of triplet constraint)
  - Refinement degradation: On open-ended tasks with LLM-as-judge, smaller models regress after iteration 1

- **First 3 experiments:**
  1. Baseline replication: Run direct inference vs. 1-shot CoT on AIME'25 with R1-Distill-Qwen-7B; confirm ~6-16% degradation
  2. Ablation on decoupling: Implement two-step inference (generate trace with demos, then answer without demos); measure gap vs. direct inference
  3. I2S vs. majority@3: On GPQA, compare I2S (3 question-conditioned calls) to majority@3; report accuracy delta per model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit judge-guided feedback during iterative refinement improve performance on open-ended tasks compared to the current implicit feedback approach?
- Basis: Appendix B.1 states: "Future work includes (i) using multiple references or more tolerant scoring criteria... (ii) incorporating explicit feedback (e.g., judge-guided hints) to provide a stronger corrective signal."
- Why unresolved: Current I2S+ refinement relies on implicit, noisy agreement-based grading; smaller models (7B/1.7B) show flat or negative returns while 14B models benefit.
- What evidence would resolve it: Controlled experiment comparing I2S+ with judge-provided hints versus self-generated suggestions on open-ended benchmarks, measuring accuracy improvements across model scales.

### Open Question 2
- Question: Can the I2S framework be extended to multi-shot settings (2+ demonstrations) while maintaining its benefits and computational efficiency?
- Basis: Table 5 shows reasoning demonstrations reach 13k-14k tokens, and authors restrict to single exemplars to "stabilize generation and control context growth."
- Why unresolved: Long contexts risk introducing confusion and degrading reasoning quality; the tradeoff between multiple diverse insights and context bloat remains unexplored.
- What evidence would resolve it: Experiments measuring I2S performance with 2-5 retrieved demonstrations, controlling for total context length, across AIME and GPQA benchmarks.

### Open Question 3
- Question: Does combining I2S with parallel test-time scaling methods (e.g., Best-of-N sampling over I2S outputs) yield additive gains?
- Basis: Section 5.3 compares I2S against majority@N as separate baselines but does not explore hybrid sequential-parallel approaches.
- Why unresolved: The paper establishes I2S outperforms majority@3 and matches majority@32 on some tasks, but synergy between insight-guided reasoning and parallel aggregation is unknown.
- What evidence would resolve it: Ablation studies running majority@N over I2S-generated candidates versus standard sampling, measuring accuracy-per-FLOP efficiency.

### Open Question 4
- Question: Why do smaller models (7B/1.7B) exhibit unstable intent adherence across refinement passes while 14B models show monotonic improvements?
- Basis: Appendix B.1: "7B/1.7B show unstable intent adherence across passes—edits drift from the objective... so additional passes compound small deviations rather than correct them."
- Why unresolved: The paper hypothesizes scale-dependent utilization of weak signals but does not isolate whether this stems from capacity limits, attention mechanisms, or training data distribution.
- What evidence would resolve it: Layer-wise analysis of attention patterns during refinement, or intervention studies with stronger external feedback signals at smaller scales.

## Limitations

- Scope of retrieval: The paper restricts to single nearest demonstration without exploring top-k retrieval or ensemble strategies, lacking empirical validation across different model families.
- Self-refinement stability: While I2S+ shows early gains, the paper acknowledges degradation on open-ended tasks with smaller models, and LLM-as-judge evaluation introduces subjectivity that compounds across refinement iterations.
- Generalizability: All results derive from three benchmark suites (AIME, GPQA, General Reasoning); efficacy on non-mathematical, non-logical domains remains unknown.

## Confidence

- **High confidence**: The core observation that few-shot demonstrations can degrade RLM performance (measured via AIME'25 degradation rates). The comparative advantage of I2S over direct answering on math benchmarks (AIME accuracy gains 14.0% for GPT-4.1, 2.7% for o1-mini).
- **Medium confidence**: The mechanism explanation (semantic misleading vs. strategy transfer failure). While illustrated with examples, the distinction is based on post-hoc analysis rather than controlled ablation studies.
- **Low confidence**: The self-refinement iteration analysis. The early-return claim appears empirically supported, but the open-ended task results show contradictory behavior across model sizes without clear explanation.

## Next Checks

1. **Controlled retrieval ablation**: Systematically vary k in top-k retrieval (k=1,3,5,10) and measure semantic misleading incidence and accuracy trade-offs to test whether single-retrieval assumption holds.

2. **Judge consistency evaluation**: Run LLM-as-judge evaluations across 3 independent judge models on I2S+ refinement outputs to quantify inter-judge agreement and determine if refinement decisions are reliable.

3. **Domain transfer experiment**: Apply I2S to a non-mathematical benchmark (e.g., HellaSwag or Natural Instructions) where problem structure differs from training demonstrations to measure whether insight extraction still produces actionable guidance.