---
ver: rpa2
title: 'SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks'
arxiv_id: '2602.02179'
source_url: https://arxiv.org/abs/2602.02179
tags:
- survival
- survkan
- time
- function
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SurvKAN, a fully parametric survival model
  based on Kolmogorov-Arnold Networks that addresses the limitations of existing survival
  analysis methods. Unlike traditional approaches such as Cox proportional hazards
  models that rely on restrictive assumptions, SurvKAN treats time as an explicit
  input and directly predicts the log-hazard function through a KAN architecture,
  enabling it to learn arbitrary time-varying risk patterns without proportional hazards
  constraints.
---

# SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2602.02179
- Source URL: https://arxiv.org/abs/2602.02179
- Authors: Marina Mastroleo; Alberto Archetti; Federico Mastroleo; Matteo Matteucci
- Reference count: 31
- Key outcome: SurvKAN achieves relative improvements of 2.27% over linear models, 1.77% over neural-based methods, and 2.05% over KAN-based approaches in concordance index, with interpretable feature-to-risk mappings through learnable univariate functions.

## Executive Summary
This paper introduces SurvKAN, a fully parametric survival model that treats time as an explicit input to a Kolmogorov-Arnold Network (KAN) predicting log-hazard directly. Unlike traditional survival models with proportional hazards assumptions, SurvKAN learns arbitrary time-varying risk patterns without these constraints. The model demonstrates competitive or superior performance across multiple datasets while providing interpretable results through learnable univariate edge functions that can be visualized or converted to symbolic expressions.

## Method Summary
SurvKAN models log-hazard as h(t|x) = KAN([x, t]) where patient features x and time t are concatenated as input. The KAN uses learnable univariate edge functions (linear combinations of identity/SiLU bases and quadratic B-splines) to compose multivariate representations. Training minimizes negative log-likelihood with four regularization terms (L1, entropy, spline L2, smoothness). Cumulative hazard is computed via trapezoidal integration over K=50 points, then used to calculate survival probabilities. The architecture uses [d+1, m, 1] layers with m ∈ {0,1,2,3}, and hyperparameters are optimized using Optuna with 5-fold CV.

## Key Results
- SurvKAN achieves 2.27% relative improvement over linear models and 1.77% over neural-based methods in concordance index
- For integrated Brier score calibration, improvements reach 2.65% over linear models and 3.01% over neural methods
- The model excels particularly on AIDS, Framingham, GBSG2, METABRIC, PBC, and Veterans datasets
- Interpretable results reveal clinically meaningful patterns: tumor biology factors for recurrence-free survival (GBSG2) and systemic factors like age for overall survival (METABRIC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating time as an explicit input feature enables learning arbitrary time-varying hazard patterns without proportional hazards constraints
- Mechanism: SurvKAN concatenates patient features x with time t as input [x, t] to a KAN that directly outputs log-hazard, allowing capture of non-proportional risk dynamics
- Core assumption: Log-hazard is a learnable continuous function of both covariates and time with sufficient events across the time horizon
- Evidence anchors: Abstract states SurvKAN treats time as explicit input enabling arbitrary time-varying patterns; Section III-A shows log-hazard = KAN([x, t]); KAN-AFT paper notes CoxPH limitations with constant hazard ratios

### Mechanism 2
- Claim: Learnable univariate edge functions enable expressive non-linear relationships and interpretable feature-to-risk mappings
- Mechanism: Each edge learns φ(x) = w_b·b(x) + w_s·spline(x) where spline is a quadratic B-spline with learnable coefficients, enabling visualization and symbolic extraction
- Core assumption: True feature-risk relationships decompose into univariate transformations with interactions captured through layer composition
- Evidence anchors: Abstract mentions interpretable results through learnable univariate functions; GBSG2 and METABRIC analyses show clinically meaningful patterns; CoxKAN demonstrates similar interpretability

### Mechanism 3
- Claim: Structured regularization during full-likelihood training yields prunable, clinically meaningful models without overfitting
- Mechanism: Training minimizes L_NLL plus regularization on edge outputs (L1 for sparsity, entropy for balanced contributions) and B-spline coefficients (L2 magnitude, smoothness)
- Core assumption: Regularization strength is appropriately tuned and survival likelihood gradient provides sufficient signal for meaningful hazard learning
- Evidence anchors: Section III-B describes complete training objective with four regularization terms; Section IV-B shows performance improvements over CoxKAN; FPBoost paper notes similar regularization benefits

## Foundational Learning

- Concept: **Survival Functions and Hazard Functions**
  - Why needed here: SurvKAN outputs log-hazard; understanding S(t|x) = P(T > t|x) and h(t|x) is essential to interpret outputs and reconstruction via S(t|x) = exp(−H(t|x))
  - Quick check question: Given h(t) = 0.5t, what is S(2) if H(0) = 0?

- Concept: **Right-Censoring and Likelihood Construction**
  - Why needed here: Training objective uses full survival likelihood with censoring indicator δ_i; understanding why censored observations contribute −H(t_i|x_i) while events contribute log h(t_i|x_i) − H(t_i|x_i) is critical
  - Quick check question: In the loss, why does a censored patient not contribute a log h term?

- Concept: **Kolmogorov-Arnold Representation**
  - Why needed here: Explains why KANs place learnable functions on edges (not nodes) and how univariate function composition yields multivariate approximation
  - Quick check question: In a KAN layer, what operation does a node perform on its incoming edge outputs?

## Architecture Onboarding

- Component map: Input layer [d+1] → Hidden layer(s) width m → Output layer [1] → Cumulative hazard integration → Survival probability

- Critical path:
  1. Data preparation → normalize time to [0,1], handle missing covariates
  2. Model initialization → configure [d+1, m, 1] architecture, initialize B-spline coefficients
  3. Forward pass → evaluate KAN at each time grid point
  4. Integration → trapezoidal rule for H(t|x)
  5. Loss computation → full likelihood + regularization
  6. Backprop → update edge function parameters
  7. Pruning/symbolic extraction → post-training interpretability

- Design tradeoffs:
  - **K (integration points)**: Higher K improves H approximation but increases compute; paper uses K=50
  - **Hidden width m**: Larger m captures more interactions but reduces interpretability; paper uses m ∈ {0,1,2,3}
  - **Regularization strength λ**: Controls sparsity vs. expressivity; requires hyperparameter search
  - **Time normalization range**: [0,1] for numerical stability; extrapolation beyond training range is unreliable

- Failure signatures:
  - **All predictions identical**: Likely regularization too strong or learning rate too low
  - **Log-hazard hitting clamps [−20, 20]**: Numerical instability; check input scaling, reduce learning rate
  - **Survival curves not monotonically decreasing**: Integration error or insufficient K; verify trapezoidal implementation
  - **Post-pruning removes all features**: Entropy or L1 penalty too aggressive; reduce regularization

- First 3 experiments:
  1. **Baseline reproduction**: Run SurvKAN on GBSG2 with architecture [9+1, 2, 1], K=50; verify C-index ~0.68 and recover approximate symbolic form for lymph nodes (√ transform)
  2. **Ablation on time-as-input**: Compare SurvKAN (time as input) vs. SurvKAN with fixed-time inference (no time input, single risk score); quantify C-index and IBS difference on a dataset known to violate PH (e.g., Veterans)
  3. **Regularization sensitivity**: Train on METABRIC with λ ∈ {0.001, 0.01, 0.1}; report number of retained edges post-pruning and corresponding C-index/IBS to identify optimal sparsity-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SurvKAN's performance scale with higher-dimensional covariate spaces, such as genomics or imaging data?
- Basis in paper: All experiments use low-dimensional clinical datasets (maximum ~20 features). KANs claim parameter efficiency, but this is untested in survival settings with hundreds or thousands of covariates
- Why unresolved: The paper does not evaluate SurvKAN on high-dimensional benchmarks where feature selection and regularization become critical
- What evidence would resolve it: Experiments on genomics datasets (e.g., TCGA with full gene expression panels) comparing SurvKAN against regularized neural survival models

### Open Question 2
- Question: Does the numerical integration approximation (trapezoidal rule with K=50 points) introduce systematic bias in hazard estimation for complex temporal patterns?
- Basis in paper: The paper empirically sets K=50 with "diminishing returns for larger values," but provides no sensitivity analysis or comparison to adaptive quadrature methods
- Why unresolved: The approximation error bounds are not characterized, particularly for hazard functions with rapid local changes
- What evidence would resolve it: Ablation studies varying K and comparing against analytical integration where the true hazard is known (simulated data)

### Open Question 3
- Question: Can SurvKAN be extended to handle competing risks while preserving interpretability?
- Basis in paper: The paper acknowledges competing risks in the METABRIC analysis (elderly patients succumbing to comorbidities) but models only a single event type
- Why unresolved: Competing risks require cause-specific hazard functions that may interact, potentially complicating the KAN's univariate interpretability guarantees
- What evidence would resolve it: An extension of SurvKAN to multi-event settings with evaluation on competing risks benchmarks

## Limitations
- Performance scalability to high-dimensional covariate spaces (genomics, imaging) remains untested
- Numerical integration approximation error bounds are not characterized, particularly for complex temporal patterns
- Extension to competing risks scenarios while preserving interpretability has not been demonstrated

## Confidence
- **High Confidence**: The core mechanism of treating time as an explicit input feature and using KAN architecture for direct log-hazard prediction is well-supported by the mathematical formulation and aligns with established KAN principles
- **Medium Confidence**: The interpretability claims through learnable univariate functions are supported by specific examples (GBSG2 and METABRIC), but generalizability across all datasets requires further validation
- **Medium Confidence**: The performance improvements over baselines are demonstrated across multiple datasets, though exact hyperparameter configurations remain unspecified

## Next Checks
1. **PH Assumption Verification**: Systematically test proportional hazards assumption violations across all 10 datasets using Schoenfeld residuals or similar diagnostics, then correlate these violations with SurvKAN's relative performance gains

2. **Hyperparameter Sensitivity Analysis**: Conduct a structured ablation study varying regularization weights, B-spline configurations, and learning rates to establish the stability of reported performance improvements and identify optimal configurations

3. **Temporal Pattern Recovery**: On datasets with known time-varying risk patterns (e.g., simulated data or well-characterized clinical cohorts), verify that SurvKAN accurately recovers the ground truth temporal hazard dynamics, not just overall discrimination metrics