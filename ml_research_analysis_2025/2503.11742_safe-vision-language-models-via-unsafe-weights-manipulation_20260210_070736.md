---
ver: rpa2
title: Safe Vision-Language Models via Unsafe Weights Manipulation
arxiv_id: '2503.11742'
source_url: https://arxiv.org/abs/2503.11742
tags:
- unsafe
- safe
- safety
- performance
- safe-clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of unsafe behaviors in vision-language
  models (VLMs) by addressing the shortcomings of existing training-based safety alignment
  methods. The authors identify that fine-tuning VLMs for safety can inadvertently
  degrade the model's performance on safe inputs.
---

# Safe Vision-Language Models via Unsafe Weights Manipulation

## Quick Facts
- arXiv ID: 2503.11742
- Source URL: https://arxiv.org/abs/2503.11742
- Reference count: 40
- Primary result: A training-free method that manipulates unsafe weights in VLMs improves safety while better preserving knowledge than training-based methods

## Executive Summary
This paper introduces Unsafe Weights Manipulation (UWM), a training-free approach to improve the safety of vision-language models (VLMs) without the knowledge degradation common in training-based safety alignment methods. UWM identifies weights disproportionately associated with unsafe content by comparing activation patterns between safe and unsafe inputs, then manipulates these weights through negation. Experiments show UWM consistently improves safety metrics while outperforming training-based methods on safe queries and preserving zero-shot classification accuracy better. The method generalizes across different VLM architectures and achieves strong performance on the ViSU safety benchmark.

## Method Summary
UWM uses a calibration dataset of safe and unsafe image-text pairs to identify weights associated with unsafe content processing. For each weight, it computes activation-based saliency scores for both safe and unsafe inputs, then aggregates these as a ratio to estimate parameter importance for unsafe behavior. The method then identifies the most influential weights per layer (those contributing to the top τ fraction of cumulative importance) and manipulates them via negation (α=-1), effectively reversing their directional influence. Unlike pruning, this preserves representational structure while reducing unsafe outputs. The approach targets specific layers (output projection for text encoder, Fc2 for vision encoder) where unsafe concepts are most localized, avoiding highly sensitive layers that would cause catastrophic knowledge loss.

## Key Results
- UWM improves unsafe query safety metrics significantly: +6.5% on P^u_t and +12.9% on P^u_v compared to baseline
- Outperforms training-based Safe-CLIP on safe queries (61.3% zero-shot accuracy vs 54.2%) while achieving better safety
- Preserves zero-shot classification accuracy better than training methods, with only ~11% degradation vs original CLIP
- Generalizes across architectures (CLIP, BLIP, SigLIP) with consistent safety improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weights that disproportionately contribute to unsafe content processing can be identified by comparing activation patterns between safe and unsafe inputs.
- Mechanism: The scoring function computes the ratio Φ^uns/Φ^sf for each weight, combining input node activation magnitude (||z_i||) with weight magnitude (|θ_ij|) to estimate information flow. Higher ratios indicate weights more associated with unsafe processing.
- Core assumption: Unsafe concepts are processed through a partially identifiable subset of weights that can be distinguished from general-purpose weights through comparative activation analysis.
- Evidence anchors:
  - [abstract]: "UWM uses a calibration set of safe and unsafe instances to compare activations between safe and unsafe content, identifying the most important parameters for processing the latter."
  - [section 4]: "Eq. (8) estimates how the information flows in a weight looking at the nodes it connects. Specifically, the left term accounts for the information that the output node j receives, while the right term for the information that the input node i emits to the next layer."
  - [corpus]: Weak/absent - corpus neighbors focus on safety benchmarks (MSTS, HoliSafe) and reasoning-based approaches, not weight-level activation mechanisms.

### Mechanism 2
- Claim: A small calibration dataset can reliably estimate which parameters encode unsafe behaviors without requiring gradient computation or training.
- Mechanism: Uses 400 tuples per concept from ViSU training set. For each weight, aggregates activation statistics across safe and unsafe samples separately, normalizing by standard deviation σ(z_i) to capture consistency. The ratio Φ^uns/Φ^sf reveals parameter influence on unsafe behavior.
- Core assumption: A subset of model parameters is consistently activated by unsafe content across different instances, and this activation signature generalizes beyond the calibration set.
- Evidence anchors:
  - [abstract]: "UWM uses a calibration set of safe and unsafe instances to compare activations between safe and unsafe content"
  - [section 5.1]: "We create the calibration set D by randomly sampling 400 tuples per concept from ViSU's training set. We observe consistent results using different calibration sets (e.g., ±0.1 in GS)."
  - [corpus]: Weak - corpus papers address multimodal safety testing and intent awareness but not calibration-based parameter identification.

### Mechanism 3
- Claim: Negating (flipping the sign of) identified unsafe weights reduces unsafe outputs while better preserving general knowledge than pruning (zeroing) or fine-tuning.
- Mechanism: Sets α=-1 for selected weights (those contributing to τ fraction of cumulative importance score per layer), reversing their directional influence. Unlike pruning (α=0) which destroys information entirely, negation preserves representational structure.
- Core assumption: The selected weights encode a directional bias toward unsafe content that can be reversed without destroying their general representational value.
- Evidence anchors:
  - [abstract]: "Their values are then manipulated via negation."
  - [section 4]: "Notably, we experiment with negative values, such as α=-1, effectively reversing the influence of selected weights and intuitively 'flipping' their effect."
  - [table 1]: UWM achieves 61.3% mean zero-shot accuracy vs. 54.2% for Safe-CLIP (training-based), demonstrating superior knowledge preservation.
  - [corpus]: Weak - corpus doesn't discuss weight manipulation or negation approaches.

## Foundational Learning

- Concept: **Information Flow Saliency in Neural Networks**
  - Why needed here: UWM's scoring function measures how information flows through individual weights by combining activation magnitudes with weight magnitudes, requiring understanding of layer-wise signal propagation.
  - Quick check question: Given a layer with weights θ ∈ R^(n×m) and input activations z ∈ R^n, how would you compute which weights carry the most information to the next layer?

- Concept: **Catastrophic Forgetting in Fine-tuning**
  - Why needed here: The paper's central motivation is that training-based safety alignment (Safe-CLIP) causes knowledge forgetting, degrading safety on safe inputs by 23% (P^s_t metric), which motivates training-free approaches.
  - Quick check question: Why does fine-tuning a pre-trained model on a specialized safety objective often degrade performance on the original training distribution?

- Concept: **Contrastive VLM Architecture (CLIP-style)**
  - Why needed here: The method operates on dual-encoder vision-language models where separate encoders project images and text to a shared embedding space; UWM is applied independently to each encoder.
  - Quick check question: In a contrastive VLM, if you modify the text encoder weights but not the image encoder, what types of behaviors might change and what might remain unaffected?

## Architecture Onboarding

- Component map:
  Vision Encoder (f_Img) -> Scoring Module -> Adaptive Selection -> Manipulation Module
  Text Encoder (f_Txt) -> Scoring Module -> Adaptive Selection -> Manipulation Module

- Critical path:
  1. Prepare calibration dataset: 400 tuples per concept (safe/unsafe image-text pairs) from ViSU training set
  2. Forward pass through each encoder to collect activations per layer for both safe and unsafe partitions
  3. Compute safe scores Φ^sf and unsafe scores Φ^uns for each weight using Eq. 8-9
  4. Aggregate via ratio Φ = Φ^uns/Φ^sf; apply magnitude prior |W| for text encoder only
  5. Adaptive selection: identify weights contributing to τ=0.02 of cumulative score per layer
  6. Apply negation (α=-1) to selected weights in Fc2 (image encoder) and output projection (text encoder)
  7. Evaluate using SafeGround metrics (P^s_t, P^u_t, P^s_v, P^u_v, Txt_s, Img_s, PS, PU, GS) and zero-shot classification

- Design tradeoffs:
  - τ (sparsity threshold): Higher τ = more weights manipulated = better safety but worse zero-shot. Paper uses τ=0.02 as optimal tradeoff (Figure 5).
  - α (scaling factor): α=0 (pruning) destroys information; α=-1 (negation) preserves structure while reversing direction; α→1 reverts to original behavior (Figure 6).
  - Layer selection: MLP Fc1 layers are highly sensitive (causes V_s-Ts → 0.0% per Table 8). Output projection and Fc2 are safer targets.
  - Magnitude prior: Beneficial for text encoder (+12.7% Txt_s) but causes 0.0% V_s-Ts for vision encoder (Table 7) - vision encoder weights don't correlate with importance.

- Failure signatures:
  - Zero-shot performance collapse: V_s-Ts dropping to ~0% indicates targeting wrong layers (e.g., Fc1 of text encoder per Table 8).
  - Safety degradation on safe inputs: Drop in P^s_t or P^s_v indicates weights not properly disentangled from general capabilities (Safe-CLIP shows -23% in P^s_t per Figure 2).
  - Text vs. image modality imbalance: Text queries consistently harder to make safe; Img_s > Txt_s gap widens with interventions (Table 1 discussion).
  - Calibration instability: ±0.1 variation in GS with different calibration sets is acceptable; larger variations indicate unreliable importance scores.

- First 3 experiments:
  1. **Baseline comparison on CLIP ViT-L14**: Compare UWM against original CLIP, Safe-CLIP (training-based), G-Unsafe, and G-Safe-CLIP (gradient pruning baselines) on ViSU dataset. Measure all SafeGround metrics and zero-shot classification on 17 datasets. Expect UWM to improve P^u_t (+6.5%) and P^u_v (+12.9%) while achieving 61.3% zero-shot accuracy vs. 54.2% for Safe-CLIP.
  2. **Ablation over scoring function**: Test (a) Φ^uns only without aggregation, (b) Φ^uns/Φ^sf ratio, (c) ratio with adaptive selection. Verify ratio+adaptive achieves best safety-knowledge tradeoff per Table 3 (GS: 4.5% with 32.0% V_s-Ts vs. 13.0% GS with 16.2% V_s-Ts for ratio-only).
  3. **Layer sensitivity analysis**: Apply UWM to different layer combinations (Fc1, Fc2, Value, Output projections) for both encoders per Table 8. Confirm Fc1 is highly sensitive (0.0% V_s-Ts) while Fc2 (image) + Output (text) is optimal configuration (32.0% V_s-Ts, 4.5% GS).

## Open Questions the Paper Calls Out

- Can the UWM scoring function be effectively adapted for generative VLM architectures that lack the explicit contrastive pairs used to derive the current safety scores?
- Is there a universal criterion to predict which layers require modification, eliminating the need to empirically tune layer selection for different pre-training strategies?
- How can the method isolate and manipulate unsafe weights without degrading general capabilities when parameters are entangled with safe, general knowledge?

## Limitations

- The method relies on contrastive VLM architectures and doesn't directly apply to generative models without modification
- Requires careful layer selection that varies depending on VLM architecture and pretraining strategy
- Still experiences ~11% zero-shot performance degradation compared to original CLIP, indicating some entanglement between unsafe and general knowledge

## Confidence

- **High confidence**: Empirical improvements on SafeGround metrics (P^u_t +6.5%, P^u_v +12.9%) and zero-shot classification (61.3% vs 54.2% for Safe-CLIP) are directly measured and reproducible
- **Medium confidence**: The mechanism of identifying unsafe weights through activation ratios is supported by ablation studies but relies on assumptions about neural pathway separability
- **Medium confidence**: Generalization across VLM architectures (CLIP, BLIP, SigLIP) shows consistent improvements but with architecture-specific variations

## Next Checks

1. Test UWM's robustness to adversarial calibration sets - create calibration sets with mixed safe/unsafe concepts and measure performance degradation
2. Apply UWM to other VLM tasks (e.g., visual question answering, image captioning) to assess safety-preservation tradeoffs beyond retrieval
3. Analyze which specific concepts benefit most from UWM by breaking down SafeGround metrics per concept category (violence, nudity, harassment, etc.)