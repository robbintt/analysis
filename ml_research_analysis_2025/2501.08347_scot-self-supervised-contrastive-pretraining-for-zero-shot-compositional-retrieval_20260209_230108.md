---
ver: rpa2
title: 'SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional
  Retrieval'
arxiv_id: '2501.08347'
source_url: https://arxiv.org/abs/2501.08347
tags:
- image
- text
- scot
- retrieval
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCOT addresses zero-shot compositional image retrieval by pretraining
  a composition model using large-scale image-text pairs and a frozen vision-language
  encoder. It generates modification texts and modified captions via an LLM, using
  the resulting text embeddings as supervision targets instead of requiring image
  triplets.
---

# SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval

## Quick Facts
- arXiv ID: 2501.08347
- Source URL: https://arxiv.org/abs/2501.08347
- Reference count: 40
- SCOT achieves R@10 of 38.45% on FashionIQ, outperforming existing zero-shot methods

## Executive Summary
SCOT introduces a self-supervised contrastive pretraining framework for zero-shot compositional image retrieval. The method leverages large-scale image-text pairs and a frozen vision-language encoder to train a composition model that can combine images with textual modifications to retrieve modified images without requiring image triplets during training. By generating modification texts and modified captions via an LLM, SCOT uses text embeddings as supervision targets, making it more scalable than previous approaches that require explicit image triplets.

The framework demonstrates strong performance on two compositional retrieval benchmarks, achieving R@10 of 38.45% and R@50 of 60.03% on FashionIQ, and R@1 of 36.82% and R@10 of 74.48% on CIRR. These results show SCOT approaching fully-supervised performance while maintaining zero-shot capabilities. The approach is particularly effective because it bypasses the need for expensive triplet mining while still learning robust compositional representations.

## Method Summary
SCOT addresses zero-shot compositional retrieval by pretraining a composition model using large-scale image-text pairs and a frozen vision-language encoder. It generates modification texts and modified captions via an LLM, using the resulting text embeddings as supervision targets instead of requiring image triplets. The composition function is trained with a contrastive loss to align the composed image-modification embedding with the target text embedding. On FashionIQ, SCOT achieves R@10 of 38.45% and R@50 of 60.03% using a BLIP-2 backbone, outperforming existing zero-shot methods and approaching fully-supervised performance. On CIRR, SCOT reaches R@1 of 36.82% and R@10 of 74.48%, surpassing other zero-shot approaches. Experiments show that SCOT's text-based supervision is more effective than retrieved image supervision, and that performance improves with better vision-language backbones and larger training datasets.

## Key Results
- SCOT achieves R@10 of 38.45% and R@50 of 60.03% on FashionIQ benchmark
- On CIRR dataset, SCOT reaches R@1 of 36.82% and R@10 of 74.48%
- Text-based supervision outperforms retrieved image supervision in ablation studies

## Why This Works (Mechanism)
The core mechanism works by leveraging large-scale image-text pairs with a frozen vision-language encoder, then using LLM-generated modification texts and modified captions as supervision targets. This approach avoids the need for expensive triplet mining while still learning robust compositional representations. The contrastive loss aligns the composed image-modification embedding with the target text embedding, enabling effective zero-shot compositional retrieval. The framework's effectiveness stems from its ability to learn compositional representations that generalize well to unseen combinations of images and modifications.

## Foundational Learning
- **Vision-Language Encoders**: Models like BLIP-2 and CLIP that map images and text to shared embedding spaces - needed for computing similarity between composed image-modification pairs and target text embeddings; quick check: verify frozen encoder weights remain unchanged during SCOT training
- **Contrastive Learning**: Training objective that pulls together positive pairs while pushing apart negative pairs - needed to align composed embeddings with target text embeddings; quick check: examine loss values for convergence during training
- **LLM-based Text Generation**: Using models like GPT to generate modification texts and modified captions - needed to create supervision targets without requiring image triplets; quick check: validate quality and diversity of generated modification texts
- **Zero-shot Compositional Retrieval**: Task of retrieving modified images using only base images and textual modifications - needed to evaluate framework's ability to generalize to unseen compositions; quick check: test on held-out compositional splits
- **Self-supervised Pretraining**: Training framework that generates its own supervision signals - needed to scale training without manual annotation; quick check: verify training pipeline generates valid supervision targets

## Architecture Onboarding

**Component Map**: Image + Frozen V-L Encoder -> Composition Model -> Composed Embedding; LLM-generated Modification Texts -> Text Encoder -> Target Text Embedding; Composed Embedding <-> Target Text Embedding (Contrastive Loss)

**Critical Path**: Image → Frozen Vision-Language Encoder → Composition Model → Composed Embedding → Contrastive Loss; LLM-generated Modification Texts → Text Encoder → Target Text Embedding → Contrastive Loss

**Design Tradeoffs**: Uses frozen vision-language encoders instead of training from scratch (faster, leverages existing knowledge vs. less adaptability); generates text supervision via LLM rather than requiring image triplets (more scalable vs. potential generation noise); contrastive loss over direct regression (more robust to embedding space structure vs. potentially slower convergence)

**Failure Signatures**: Poor retrieval performance suggests issues with composition model capacity or contrastive loss optimization; high variance in results across runs may indicate sensitivity to LLM-generated modification quality; performance degradation on out-of-distribution modifications suggests limited compositional generalization

**First Experiments**:
1. Verify frozen vision-language encoder weights remain unchanged during SCOT training
2. Test composition model with synthetic image-modification pairs before full training
3. Evaluate contrastive loss convergence with known positive and negative pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated modification texts introduces potential variability and quality concerns not extensively analyzed
- Ablation study comparing text supervision to image supervision is limited to one specific comparison
- Evaluation focuses on specific backbone choices without exploring the full range of available vision-language models
- Generalization to domains outside fashion and natural images is not demonstrated
- Compositional generalization capabilities beyond the training distribution are not thoroughly examined

## Confidence

**High confidence**:
- Technical implementation and benchmark results on the tested datasets

**Medium confidence**:
- Claim that text supervision is universally more effective than image supervision, given limited ablation comparisons
- Scalability claims, as experiments are limited to specific dataset sizes

**Low confidence**:
- Cross-domain generalization capabilities based on the current experimental scope

## Next Checks

1. Conduct ablation studies comparing different LLM models and prompt strategies for generating modification texts to assess robustness to generation quality
2. Test performance on additional compositional retrieval datasets from different domains (e.g., medical imaging, satellite imagery) to evaluate cross-domain generalization
3. Implement controlled experiments with synthetic compositional splits to systematically evaluate compositional generalization beyond the training distribution