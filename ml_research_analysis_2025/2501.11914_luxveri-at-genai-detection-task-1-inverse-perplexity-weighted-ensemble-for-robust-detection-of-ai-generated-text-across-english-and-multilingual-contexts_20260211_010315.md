---
ver: rpa2
title: 'LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for
  Robust Detection of AI-Generated Text across English and Multilingual Contexts'
arxiv_id: '2501.11914'
source_url: https://arxiv.org/abs/2501.11914
tags:
- multilingual
- ensemble
- task
- english
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ensemble-based system for detecting AI-generated
  text across English and multilingual contexts, using inverse perplexity weighting
  to assign greater influence to models with higher confidence. The approach combines
  multiple Transformer models, including RoBERTa-base, BERT-base-cased, RemBERT, and
  XLM-RoBERTa-base, achieving a Macro F1-score of 0.7458 (rank 12/35) for English
  and 0.7513 (rank 4/25) for multilingual tasks.
---

# LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts

## Quick Facts
- **arXiv ID**: 2501.11914
- **Source URL**: https://arxiv.org/abs/2501.11914
- **Reference count**: 10
- **Primary result**: Achieved Macro F1-score of 0.7458 (rank 12/35) for English and 0.7513 (rank 4/25) for multilingual AI-generated text detection tasks.

## Executive Summary
This paper presents an ensemble-based system for detecting AI-generated text across English and multilingual contexts, using inverse perplexity weighting to assign greater influence to models with higher confidence. The approach combines multiple Transformer models, including RoBERTa-base, BERT-base-cased, RemBERT, and XLM-RoBERTa-base, achieving a Macro F1-score of 0.7458 (rank 12/35) for English and 0.7513 (rank 4/25) for multilingual tasks. The system addresses data imbalance by scaling down overrepresented language samples and optimizes model contributions through weighted voting based on perplexity. Results demonstrate the effectiveness of ensemble methods and inverse perplexity weighting in improving robustness for AI-generated text detection across diverse linguistic settings.

## Method Summary
The system uses an ensemble of Transformer models with weighted soft voting based on inverse perplexity. For the multilingual task, the authors downsampled overrepresented languages (English and Chinese) to balance the dataset, reducing English from 610k to 40k samples and Chinese to 20k samples. Models were trained with binary cross-entropy loss using AdamW optimizer with learning rates between 1e-5 and 2e-5. The ensemble voting weights were calculated as the inverse of each model's perplexity minus one, normalized across the ensemble. This approach prioritizes models with higher predictive confidence on specific inputs.

## Key Results
- Achieved Macro F1-score of 0.7458 (rank 12/35) for English subtask and 0.7513 (rank 4/25) for multilingual subtask
- Inverse perplexity weighting outperformed both accuracy-based weighting (0.7251) and majority voting (0.6850) in ensemble performance
- Data balancing strategy through downsampling improved multilingual generalization compared to using full imbalanced dataset

## Why This Works (Mechanism)

### Mechanism 1: Inverse Perplexity Weighted Voting
Assigning ensemble votes based on a model's inverse perplexity appears to improve classification robustness by prioritizing models with higher predictive confidence (lower surprise) on specific inputs. The system calculates perplexity for each model on test data and computes weights as w_i = 1/(P_i - 1), normalized across the ensemble. The final prediction is a weighted sum of probabilities. This shifts decision influence toward models that are less "surprised" by the text distribution. Core assumption: Lower perplexity correlates directly with higher classification accuracy for that sample. Evidence: Inverse Perplexity Weighting (0.7458) outperformed Accuracy Based Weighting (0.7251) and Majority Voting (0.6850).

### Mechanism 2: Majority Class Downsampling for Cross-Lingual Balance
Scaling down overrepresented language samples (English, Chinese) reduces training bias, allowing the model to generalize better to underrepresented languages (Urdu, Arabic, Russian). The authors reduced English training set to 40,000 and Chinese to 20,000 samples, bringing volume closer to other languages rather than synthetically augmenting minority classes. Core assumption: Patterns learned from smaller, balanced dataset are more representative of multilingual task than patterns from large, imbalanced dataset dominated by English. Evidence: Data reduction from 610k to 40k English samples while maintaining strong performance demonstrates effectiveness.

### Mechanism 3: Heterogeneous Architecture Ensembling
Combining models with distinct pre-training objectives and architectures (e.g., RemBERT vs. XLM-RoBERTa) creates more robust decision boundary than single models or homogeneous ensembles. The system ensembles models with complementary strengths: RemBERT for multilingual embedding coupling, XLM-RoBERTa for cross-lingual transfer, and BERT-base-multilingual-cased. Core assumption: Errors made by RemBERT are not perfectly correlated with errors made by XLM-RoBERTa, allowing ensemble to average out model-specific failures. Evidence: Full ensemble (0.7513) outperformed subsets like RemBERT + XLM-R (0.7435).

## Foundational Learning

- **Concept: Perplexity**
  - Why needed: Core signal used to determine voting weights. Without understanding perplexity measures how "surprised" a model is by sequence (lower is better), weighting logic is opaque.
  - Quick check: If Model A has perplexity of 10 and Model B has perplexity of 100, which model gets higher weight in this system?

- **Concept: Soft Voting vs. Hard Voting**
  - Why needed: System uses weighted probabilities (soft voting), not majority class counts (hard voting). Understanding difference required to implement p_ensemble(c) calculation correctly.
  - Quick check: Why might soft voting be preferable when combining "very confident" model with "slightly confident" one?

- **Concept: Class Imbalance & Macro F1**
  - Why needed: Paper explicitly addresses imbalance via downsampling and evaluates using Macro F1. Must understand why Accuracy is poor metric when English samples outnumber Russian 400:1.
  - Quick check: If model predicts "English" for every input, it might have high Accuracy but low Macro F1. Why?

## Architecture Onboarding

- **Component map**: Raw text -> Data Balancer -> Tokenizers -> Encoders (3 parallel Transformers) -> Heads -> Perplexity Calculator -> Weighting Engine -> Aggregator

- **Critical path**: Perplexity Calculation is most fragile part of inference pipeline. Requires access to true label during validation to track perplexity, or consistent proxy during inference.

- **Design tradeoffs**:
  - Robustness vs. Data Volume: Trades raw English data volume (610k â†’ 40k) for improved multilingual balance. Risks overfitting on English if 40k samples aren't representative.
  - Complexity vs. Gain: Inverse perplexity weighting outperformed accuracy weighting by ~2% but requires computing NLL for every sample, adding computational overhead.

- **Failure signatures**:
  - Division by Zero: Formula w = 1/(P-1) fails if model achieves perfect perplexity (P=1)
  - Memory Fragmentation: Sorting data by length improves speed but requires shuffling dataset entirely before training, complicating debugging of specific data indices

- **First 3 experiments**:
  1. Baseline Single Model: Train only XLM-RoBERTa on full (imbalanced) dataset to establish baseline Macro F1
  2. Data Ablation: Train same XLM-RoBERTa on balanced (downsampled) dataset to isolate impact of balancing strategy
  3. Weighting Strategy Comparison: Implement full ensemble (3 models) and compare "Mean Ensemble" vs. "Inverse Perplexity Weighted Ensemble" on validation set

## Open Questions the Paper Calls Out

- How do data augmentation or active learning strategies compare to downsampling approach for mitigating data imbalance in multilingual AI detection? (Future work could explore advanced methods for mitigating data imbalance)
- Can more sophisticated ensemble strategies, such as stacking or meta-learning, outperform inverse perplexity weighted voting method? (More sophisticated ensemble strategies could improve detection accuracy)
- Does inverse perplexity weighting strategy disproportionately favor high-resource languages, potentially masking poor performance on low-resource languages? (Unclear if ensemble's confidence weights correlate with language resource availability)

## Limitations

- Test-time perplexity calculation is unclear - requires true labels traditionally, making test application ambiguous
- Exact data sampling strategy for downsampling English/Chinese not specified (random, stratified, etc.)
- OpenAI detector specification ambiguous - not clearly linked to specific Hugging Face checkpoint

## Confidence

- **High confidence**: General ensemble architecture and inverse perplexity weighting formula clearly specified and supported by ablation results
- **Medium confidence**: Effectiveness of majority class downsampling strategy plausible but hard to isolate causal impact without full dataset access
- **Low confidence**: Test-time application of inverse perplexity weighting weakest link - paper doesn't clarify how perplexity computed without true labels during inference

## Next Checks

1. Compute inverse perplexity weights on validation set, apply to test set, and measure variance in ensemble performance across multiple random seeds to test robustness
2. Train multilingual model on full (unbalanced) dataset and compare per-language F1 scores to downsampled version to quantify trade-off
3. Implement unsupervised perplexity proxy (average token-level NLL) and measure correlation with labeled-perplexity weights on validation set to determine if test-time weighting is meaningful