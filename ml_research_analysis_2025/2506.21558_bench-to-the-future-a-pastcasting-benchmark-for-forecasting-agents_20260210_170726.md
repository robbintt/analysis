---
ver: rpa2
title: 'Bench to the Future: A Pastcasting Benchmark for Forecasting Agents'
arxiv_id: '2506.21558'
source_url: https://arxiv.org/abs/2506.21558
tags:
- questions
- forecasting
- question
- claude
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bench to the Future introduces a "pastcasting" benchmark for evaluating
  LLM forecasting agents using historical web snapshots instead of live data. The
  approach provides a hermetic, repeatable environment with 299 questions and tens
  of thousands of pre-scraped web pages per question, enabling rapid evaluation without
  waiting for real-world resolution.
---

# Bench to the Future: A Pastcasting Benchmark for Forecasting Agents

## Quick Facts
- **arXiv ID**: 2506.21558
- **Source URL**: https://arxiv.org/abs/2506.21558
- **Reference count**: 17
- **Primary result**: Pastcasting benchmark enables rapid, reproducible LLM forecasting evaluation using historical web snapshots, showing steady performance improvements across model generations

## Executive Summary
Bench to the Future introduces a novel "pastcasting" benchmark for evaluating LLM forecasting agents by having them make predictions on historical events with known outcomes using a hermetic web snapshot environment. The approach addresses the fundamental challenge of evaluating forecasting systems by providing 299 questions with tens of thousands of pre-scraped web pages per question, eliminating the need to wait for real-world resolution. The benchmark demonstrates that agent-based forecasters significantly outperform non-agentic approaches, with Gemini 2.5 Pro achieving the best performance using a ReAct agent architecture. A validation study comparing live and retro forecasts found good correspondence overall, validating the pastcasting approach as a scalable framework for measuring AI forecasting capabilities.

## Method Summary
The benchmark uses a "pastcasting" approach where forecasting agents make predictions on historical events with known outcomes using a RetroSearch environment containing ~20,000 pre-scraped web pages per question. Questions are sourced from Metaculus (87 original binary, 135 from multiple-choice, 77 from numeric) and converted to binary format. Agents use a ReAct architecture with a Thought-Observation-Action loop (up to 30 steps, typically ~10) and an evidence-based forecasting prompt. Non-agentic baselines include Fixed Evidence Forecaster (~30 pre-sourced facts), Variable Evidence Forecaster (chain-of-thought pipeline), and No Evidence Forecaster. Forecasts are averaged across 5 runs per question to reduce inter-run variation, and performance is measured using weighted Brier scores with weights based on question correlation.

## Key Results
- Agent-based forecasters significantly outperform non-agentic approaches on the benchmark
- Inter-run variation decreases substantially when averaging 5 forecasts (SD reduced by ~2/3)
- Gemini 2.5 Pro with ReAct agent architecture achieves the best performance
- Validation study shows good correspondence between live and retro forecasts, with rare divergences when live pages contain decisive information not in snapshots
- Steady performance improvements observed across model generations

## Why This Works (Mechanism)
The benchmark's hermetic web snapshot environment enables rapid, repeatable evaluation without waiting for real-world resolution. By constraining the agent to only the information available at the "time" of the historical event, the approach creates a controlled environment where performance can be systematically measured and compared. The use of weighted Brier scores accounts for correlated questions, providing a more accurate assessment of forecasting ability. The averaging of multiple runs per question addresses the inherent stochasticity in LLM-based forecasting agents.

## Foundational Learning
- **Pastcasting methodology**: Making predictions on historical events with known outcomes using controlled information access; needed to enable rapid evaluation without waiting for real-world resolution; quick check: verify all questions have definitive resolution dates and outcomes
- **ReAct agent architecture**: Combining reasoning (Thought), environment interaction (Observation), and action (Action) in a loop for task completion; needed to enable systematic evidence gathering and reasoning for forecasting; quick check: confirm agent can perform search and page query operations
- **Weighted Brier score calculation**: Squaring forecast-error differences and weighting by question correlation (weight_i = log2(N_i + 1) / (N_i + 1)); needed to account for correlated questions in performance measurement; quick check: verify weights sum to total number of questions
- **Inter-run variation reduction**: Averaging 5 forecasts per question to reduce stochastic variation; needed to provide stable performance estimates; quick check: compare SD of single forecasts vs. averaged forecasts
- **RetroSearch environment**: Web search restricted to pre-scraped historical snapshots; needed to create a hermetic forecasting environment; quick check: confirm search returns only pages from the specified snapshot date
- **Implicit-thought mode**: Separating reasoning from action decisions in reasoning models; needed to optimize agent performance for different model types; quick check: verify reasoning and action tokens are properly separated

## Architecture Onboarding

**Component Map**: User -> Benchmark Interface -> RetroSearch Database -> ReAct Agent -> Web Search Tool + Page Query Tool -> Forecast Output

**Critical Path**: Question selection → Agent reasoning loop (up to 30 steps) → Web search → Evidence gathering → Forecast generation → Brier score calculation → Performance aggregation

**Design Tradeoffs**: Hermetic snapshot environment vs. live web access (speed vs. information completeness), averaging 5 runs vs. single forecasts (stability vs. evaluation speed), binary question format vs. original formats (simplicity vs. expressiveness), agent-based vs. non-agentic approaches (performance vs. simplicity)

**Failure Signatures**: Yes-bias in forecasts, high inter-run variation (SD ~0.15-0.25 on single forecasts), Brier scores >1 from percentage outputs (0-100) instead of probabilities (0-1), systematic deviations in calibration plots

**First Experiments**:
1. Implement RetroSearch environment with filtered web search returning only pre-scraped pages; set "today's date" to snapshot date for each question
2. Build ReAct forecaster agent with provided prompt template, search tool, and page querying tool; implement implicit-thought mode for reasoning models
3. Run 5 forecasts per question per model, average results, compute weighted Brier scores; validate with calibration plots and pairwise t-tests

## Open Questions the Paper Calls Out

**Open Question 1**: Is the observed "Yes" bias in forecasts an artifact of the dataset's resolution imbalance or a persistent feature of LLM forecasters? The authors state, "Further analysis is needed to discern whether the observed bias is more an artifact of our data or a persistent feature of LLM forecasters." The current dataset has significantly more "No" resolutions than "Yes" resolutions, making it difficult to distinguish between a systematic model bias and a statistical artifact of the data distribution. Running experiments that intelligently flip the direction of a set of questions (e.g., "Will X happen?" vs. "Will X not happen?") to see if the bias follows the phrasing or the resolution outcome would resolve this.

**Open Question 2**: How do LLM forecasting agents perform relative to human baselines on the BTF benchmark? "One crucial piece that is missing at present is positioning their performance against a human baseline." The authors note that absolute Brier scores are currently difficult to interpret because there is no way to disentangle forecasting ability from question difficulty without human comparison data. Recruiting human forecasters to answer the benchmark questions using the RetroSearch environment to establish a comparative baseline score would resolve this.

**Open Question 3**: Do the relative rankings of forecasting models remain consistent between the RetroSearch environment and live web forecasting? "More research is necessary to validate this, in particular comparing scores and rankings obtained in live and retro mode." While a preliminary study (n=20) showed good correspondence, rare cases of divergence occurred when live web pages contained decisive information missing from the snapshots. A large-scale, prospective evaluation tracking model rankings on live questions versus their performance on the same questions after they are resolved using RetroSearch would resolve this.

## Limitations
- Binary question format and Metaculus sourcing limit coverage to events with clear resolution criteria
- Optimal ReAct agent configuration (number of steps, tool selection strategy) remains unclear
- Questions with resolution dates ≤90 days show slightly higher accuracy, suggesting potential recency bias
- Study does not systematically compare different web search tools or evidence extraction methods

## Confidence

**High Confidence**: Core benchmark methodology (pastcasting approach, weighted Brier score calculation, inter-run variation reduction through averaging) is well-specified and reproducible. Observed performance improvements across model generations and validation study showing correspondence between live and retro forecasts are robust findings.

**Medium Confidence**: Claims about agent-based approaches outperforming non-agentic baselines are supported, but magnitude of improvement depends on specific model choices and prompt engineering. Observed yes-bias in forecasts is well-documented but underlying causes remain partially understood.

**Low Confidence**: Specific performance thresholds (e.g., 0.15-0.25 Brier score inter-run variation for single forecasts) may not generalize across different model architectures or question types. Exact impact of prompt variations on forecasting accuracy remains uncertain.

## Next Checks
1. **Corpus Expansion Validation**: Test the benchmark with a broader set of forecasting questions from diverse sources to assess whether performance patterns hold beyond the Metaculus corpus.

2. **Model Architecture Ablation**: Systematically vary the ReAct agent configuration (number of steps, tool selection strategy) to identify optimal parameters and assess sensitivity to architectural choices.

3. **Live vs Retro Forecast Divergence Analysis**: Conduct a more detailed study of cases where live forecasts diverge from retro forecasts to identify specific types of information or question formats where the snapshot approach breaks down.