---
ver: rpa2
title: 'Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation
  Model Adaptation'
arxiv_id: '2508.16568'
source_url: https://arxiv.org/abs/2508.16568
tags:
- learning
- server
- data
- training
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedMox, a novel framework for adapting foundation
  models under realistic federated learning constraints. It addresses the challenges
  of limited labeled data and computational resources on edge devices by proposing
  a spatial sparse Mixture-of-Experts architecture with soft-mixture strategy.
---

# Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation

## Quick Facts
- **arXiv ID:** 2508.16568
- **Source URL:** https://arxiv.org/abs/2508.16568
- **Reference count:** 40
- **Primary result:** FedMox achieves up to 0.478 mAP@50 on BDD100K while requiring only 1.25GB memory per client versus 8.89GB for full-model training.

## Executive Summary
This paper introduces FedMox, a novel framework for adapting foundation models under realistic federated learning constraints. It addresses the challenges of limited labeled data and computational resources on edge devices by proposing a spatial sparse Mixture-of-Experts architecture with soft-mixture strategy. The method enables effective foundation model adaptation in a practical semi-supervised federated learning setting where clients have only unlabeled, low-resolution data while the server holds limited labeled, high-resolution data. Experiments on autonomous driving datasets demonstrate that FedMox significantly outperforms existing federated learning methods, achieving up to 0.478 mAP@50 on BDD100K and 0.475 mAP@50 on Cityscapes, while requiring minimal memory on edge devices (1.25GB with frozen backbone vs 8.89GB for full model).

## Method Summary
FedMox addresses practical federated learning for foundation model adaptation by combining three key innovations: a spatial sparse Mixture-of-Experts (MoE) architecture for resolution-agnostic expert routing, a soft-mixture strategy for balancing supervised and unsupervised learning, and a frozen backbone approach to minimize client memory usage. The framework operates in a semi-supervised setting where the server has limited labeled high-resolution data while clients have unlabeled low-resolution data. The method uses ViT-Adapter-Small as a frozen backbone (DINOv2 pretrained), with only the task head (FPN + Faster R-CNN with MoE) being trainable on clients. During federated learning, clients perform unsupervised training using pseudo-labels, the server aggregates models via FedAvg and applies soft-mixture interpolation, then performs supervised training on labeled data before the next round.

## Key Results
- FedMox achieves up to 0.478 mAP@50 on BDD100K and 0.475 mAP@50 on Cityscapes, outperforming baselines by 0.04-0.08 mAP@50
- Memory consumption reduced from 8.89GB to 1.25GB per client by freezing the foundation model backbone
- Soft-mixture strategy with α≈0.1-0.3 optimally balances supervised precision and unsupervised generalization
- Spatial sparse MoE with K=3-4 experts provides optimal capacity without overfitting on limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The spatial sparse Mixture-of-Experts (MoE) architecture compensates for limited learning capacity caused by a frozen backbone while maintaining resolution-agnostic routing across server and client inputs.
- **Mechanism:** A spatial router implemented as a 1×1 convolution routes each pixel in the feature map independently to one of K experts via top-1 (hard-max) activation. This design requires only a fixed channel dimension, allowing the same router to process feature maps from different input resolutions (1280×720 on server vs. 640×360 on clients).
- **Core assumption:** Spatial expert assignment patterns should be consistent across resolutions—i.e., pixels at corresponding spatial locations should activate the same expert regardless of input scale.
- **Evidence anchors:** [section 3.3]: "the router acts as a 1×1 convolution along the channel dimension, which only requires a fixed channel dimension of each pixel, producing a routing map m ∈ R^(K×H×W)"; [Figure 6]: Visualization confirms expert routing consistency between server (high-res) and client (low-res) inputs across weather domains.

### Mechanism 2
- **Claim:** The Soft-Mixture strategy stabilizes sequential semi-supervised federated learning by balancing supervised (server) precision and unsupervised (client) generalization through parameter interpolation.
- **Mechanism:** After client aggregation produces w̄^(t+1), the server computes a soft-mixed model: w̄̃^(t+1) = α·w^(t) + (1−α)·w̄^(t+1), where w^(t) is the previous server model. This interpolates between the supervised-biased server model (better perception, zero pseudo-label error) and the unsupervised-biased aggregated model (better generalization, lower H-divergence to test distribution).
- **Core assumption:** An optimal α* exists in [0,1] that minimizes combined risk; empirically, α ≈ 0.1–0.3 favors the aggregated model and yields better trade-offs.
- **Evidence anchors:** [section 3.4]: Theoretical bound derived showing combined risk L(w_α; D_t) ≤ α[ℒ̂(w) + d_H(D_s, D_t)] + (1−α)[ℒ̂(w̄) + d_H(D_u, D_t)] + O(1/√n_s + 1/√n); [Figure 7 right]: Performance peaks at intermediate α (≈0.1–0.3), degrading at α=0 (aggregated only) and α=1 (server only).

### Mechanism 3
- **Claim:** Freezing the foundation model backbone and training only the task head reduces client-side memory from 8.89GB to 1.25GB, enabling deployment on resource-constrained edge devices (e.g., 2GB Raspberry Pi).
- **Mechanism:** The FM backbone (ViT-Adapter with DINOv2) is transmitted once to each client before FL begins. Only the task head parameters (FPN neck + Faster R-CNN head) participate in local training and communication, eliminating intermediate feature storage for backbone backpropagation.
- **Core assumption:** The frozen backbone provides sufficiently general representations that a trainable task head can adapt to downstream distributions without backbone fine-tuning.
- **Evidence anchors:** [section 1, Figure 1]: Explicit memory comparison showing 1.25GB with frozen backbone + low resolution vs. 8.89GB for full-model training; [Table 1]: SCL with frozen backbone achieves competitive baseline (0.416–0.449 mAP@50), confirming backbone sufficiency.

## Foundational Learning

- **Mixture of Experts (MoE) Routing**
  - **Why needed here:** FedMox relies on spatial sparse routing to enable resolution-agnostic expert selection. Understanding how routers assign inputs to experts and why sparsity reduces computation is essential.
  - **Quick check question:** Explain why top-1 (sparse) routing has similar computational cost to a single expert, whereas dense routing (softmax over all experts) requires K× more computation.

- **Semi-Supervised Object Detection (Soft Teacher)**
  - **Why needed here:** Client-side training uses unlabeled data via pseudo-labeling. The Soft Teacher framework generates and filters pseudo-boxes with confidence thresholds.
  - **Quick check question:** What is the role of the pseudo-labeling threshold (e.g., 0.9 for classification) in preventing confirmation bias during self-training?

- **Federated Averaging (FedAvg)**
  - **Why needed here:** Aggregation of client models follows FedAvg: w̄ = Σ(n_i/n)·w_i. Understanding weighted averaging under non-IID data is foundational.
  - **Quick check question:** If one client has 10× more data than others, how does FedAvg weight its contribution, and what risks does this introduce?

## Architecture Onboarding

- **Component map:** ViT-Adapter-Small backbone (frozen) -> FPN (5-level, 256-dim) -> Faster R-CNN MoE head (spatial router on RPN, traditional router on ROI head) -> Soft-Mixture aggregation

- **Critical path:**
  1. Server warm-up: Train task head on labeled high-res data (T_w=50 epochs) → initial global model w^(0)
  2. Broadcast w^(t) to sampled clients (ratio r=0.33)
  3. Client local training: Unsupervised Soft Teacher on low-res data (1 epoch) → w_i^(t+1)
  4. Server aggregates: w̄^(t+1) = FedAvg({w_i^(t+1)})
  5. Apply Soft-Mixture: w̄̃^(t+1) = α·w^(t) + (1−α)·w̄^(t+1)
  6. Server supervised training on w̄̃^(t+1) with high-res data → w^(t+1)
  7. Repeat for T=50 rounds; evaluate final w^(T) on high-res test set

- **Design tradeoffs:**
  - **K (number of experts):** More experts increase capacity but saturate quickly (Figure 7 left). K=3–4 optimal for evaluated datasets.
  - **α (soft-mixture weight):** Lower α (0.1–0.3) favors aggregated model → better generalization; higher α favors server model → better precision on labeled domain.
  - **Client resolution:** 640×360 enables 2GB devices but risks small-object detection degradation; 1280×720 requires ~3× memory.

- **Failure signatures:**
  - **Overfitting to server domain:** High α (>0.7) or MoE without FL (Table 5, row 4) → high Cloudy performance, poor Overcast/Rainy/Snowy.
  - **Training instability with excessive experts:** K>5 leads to saturation or degradation due to limited labeled data for expert specialization.
  - **Resolution misalignment symptoms:** If spatial routing fails, expert assignments become inconsistent across resolutions (verify via visualization like Figure 6).

- **First 3 experiments:**
  1. **Reproduce ablation (Table 5):** Incrementally enable HR, FL, MoE, SM to isolate each component's contribution on BDD100K with N=9.
  2. **Sweep α (Figure 7 right):** Test α ∈ {0.0, 0.1, 0.3, 0.5, 0.7, 1.0} to find optimal trade-off for your target domain balance.
  3. **Vary expert count (Figure 7 left):** Test K ∈ {1, 2, 3, 4, 5} to identify saturation point; expect diminishing returns beyond K=4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FedMox's spatial sparse MoE architecture generalize effectively to dense prediction tasks beyond object detection (e.g., semantic segmentation, depth estimation)?
- Basis in paper: [explicit] "Beyond object detection, our proposed method is also adaptable to a broad range of computer vision tasks, which we plan to explore in future research."
- Why unresolved: Experiments were limited to object detection on autonomous driving datasets; other dense prediction tasks may have different spatial routing requirements or resolution sensitivity.
- What evidence would resolve it: Evaluation on segmentation benchmarks (e.g., ADE20K, Cityscapes segmentation) showing comparable performance gains over baselines.

### Open Question 2
- Question: How can communication overhead from the MoE task head be further reduced while maintaining performance gains?
- Basis in paper: [explicit] "Communication cost is another significant challenge in FL... various methods exist to further compress communication costs [13], this aspect is outside the scope of this paper and will be explored in future work."
- Why unresolved: Adding K experts increases parameters from 14.52M to 46.35M; while backbone isn't transmitted, the head overhead could become problematic at scale.
- What evidence would resolve it: Integration with compression techniques (e.g., quantization, sparsification) demonstrating reduced communication without significant mAP degradation.

### Open Question 3
- Question: What mechanisms can automatically determine the optimal number of experts K for a given task and data distribution?
- Basis in paper: [inferred] Figure 7 shows performance saturates/degrades beyond optimal K (3-4 experts), with authors noting "Over-parameterizing the model with too many experts can lead to diminishing returns" without proposing automatic selection.
- Why unresolved: Current approach requires manual tuning per dataset (K=4 for BDD100K, K=3 for others); no principled method provided.
- What evidence would resolve it: An adaptive expert selection algorithm that converges to optimal K without manual specification, validated across multiple datasets.

### Open Question 4
- Question: What are the privacy-utility trade-offs in PSSFL for complex tasks like object detection under realistic adversarial threats?
- Basis in paper: [inferred] Authors note "existing attacks for FL are under strong assumptions where classification objective and small batch size, making it relatively extremely challenging for complex tasks such as object detection" but provide no formal privacy analysis.
- Why unresolved: Differential privacy guarantees or formal attack resistance not evaluated; PSSFL's sequential updates and MoE structure may introduce novel vulnerabilities.
- What evidence would resolve it: Formal privacy analysis or empirical evaluation against reconstruction/membership inference attacks in the PSSFL setting.

## Limitations

- **Data Configuration Uncertainty**: The exact client-domain assignment strategy for different client counts (N=3, 6, 9) is not specified, which could significantly impact learning dynamics and results.
- **Training Hyperparameter Gaps**: Critical training details are missing, including batch sizes for both server and client training, and the initialization strategy for MoE routers.
- **Domain Generalization Scope**: While the method shows strong cross-weather and cross-city generalization, it does not explore temporal distribution shifts or sensor modality changes (e.g., night vs. day, LiDAR vs. camera).

## Confidence

**High Confidence**:
- Memory reduction claims (1.25GB vs 8.89GB) are well-supported by explicit calculations and Table 1.
- MoE routing mechanism and spatial consistency visualization (Figure 6) provide strong evidence for resolution-agnostic expert selection.
- Soft-Mixture interpolation between supervised and unsupervised models is theoretically grounded with derived bounds and empirically validated via α-sweep (Figure 7 right).

**Medium Confidence**:
- Generalization across weather domains (Overcast, Rainy, Snowy) is demonstrated but limited to specific datasets (BDD100K, SODA10M, Cityscapes).
- Optimal K=3-4 experts is based on Figure 7 left but may not generalize to other tasks or data distributions.
- Client resolution choice (640×360) balances memory and performance but may degrade small-object detection, which is not extensively validated.

**Low Confidence**:
- Exact client sampling and domain assignment strategy for N=3,6,9 clients.
- Router initialization method and its impact on expert specialization.
- Long-term stability of the federated learning process beyond 50 rounds.

## Next Checks

1. **Reproduce the ablation study (Table 5)**: Incrementally enable HR, FL, MoE, and SM components on BDD100K with N=9 to isolate each contribution. This will validate the additive benefits claimed and test the sensitivity to component interactions.

2. **Validate spatial routing consistency**: Create visualizations similar to Figure 6 comparing expert routing maps between high-res server and low-res client inputs for different weather domains. This will confirm whether spatial correspondence holds across diverse conditions.

3. **Test memory constraints on target devices**: Deploy the trained model on a 2GB Raspberry Pi or similar edge device to verify the 1.25GB memory claim under real-world constraints, including inference latency and thermal performance.