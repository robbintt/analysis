---
ver: rpa2
title: 'AgentGuard: Runtime Verification of AI Agents'
arxiv_id: '2509.23864'
source_url: https://arxiv.org/abs/2509.23864
tags:
- verification
- agent
- agentic
- systems
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentGuard introduces Dynamic Probabilistic Assurance for runtime
  verification of AI agents, shifting from static verification to continuous, quantitative
  guarantees. The framework observes raw agent I/O, abstracts it into formal events,
  and learns a Markov Decision Process (MDP) online to model emergent behavior.
---

# AgentGuard: Runtime Verification of AI Agents

## Quick Facts
- arXiv ID: 2509.23864
- Source URL: https://arxiv.org/abs/2509.23864
- Reference count: 27
- Primary result: Runtime verification framework providing quantitative probabilistic guarantees for AI agents through online MDP learning

## Executive Summary
AgentGuard introduces Dynamic Probabilistic Assurance for runtime verification of AI agents, shifting from static verification to continuous, quantitative guarantees. The framework observes raw agent I/O, abstracts it into formal events, and learns a Markov Decision Process (MDP) online to model emergent behavior. Probabilistic model checking then verifies properties like success probability and expected completion time. Applied to RepairAgent, AgentGuard learned transition patterns (e.g., 75% probability of searching code base after hypothesizing) and enabled real-time verification of success likelihood and resource usage. The system demonstrates that runtime verification can provide actionable probabilistic assurances for autonomous agents, though future work includes automated state abstraction and incremental verification for scalability.

## Method Summary
AgentGuard operates as a runtime verification framework that instruments AI agents to capture state transitions, learns an MDP from execution traces, and performs probabilistic model checking to provide quantitative guarantees. The system requires defining states, actions, and properties in a YAML configuration, then instruments agent code with logging calls. An AnalyzerThread processes transitions to update the MDP using frequency-based learning, converts the model to PRISM language, and invokes the Storm model checker via stormpy bindings. The framework outputs quantitative results like success probability (Pmax=?) and expected cycles (Emin=?) while supporting threshold-based alerts. Applied to the RepairAgent with 14 tools, AgentGuard demonstrates real-time verification of agent behavior through learned transition probabilities.

## Key Results
- Learned MDP transition probabilities from raw agent traces (e.g., 75% probability of searching code base after hypothesizing)
- Provided real-time verification of quantitative properties like success probability and expected completion time
- Demonstrated framework applicability to RepairAgent with 14 tools performing bug fixing tasks
- Enabled continuous probabilistic guarantees through online learning and model checking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstracting raw agent I/O into formal events enables verification of otherwise opaque stochastic behavior.
- Mechanism: The Trace Monitor captures LLM calls, tool invocations, and observations, then maps them to discrete state transitions (s → a → s'). This abstraction creates a traceable symbolic representation from unstructured agent activity.
- Core assumption: Agent behavior can be meaningfully discretized into finite states and actions without losing critical information about decision paths.
- Evidence anchors:
  - [abstract] "AgentGuard operates as an inspection layer that observes an agent's raw I/O and abstracts it into formal events corresponding to transitions in a state model."
  - [section IV-B] "Its responsibilities are to capture raw I/O... and to abstract it into a stream of formal events corresponding to transition in a state model."
  - [corpus] TriCEGAR (arxiv 2601.22997) addresses related trace-driven abstraction for agentic AI, suggesting this is an active research direction but not yet settled science.
- Break condition: If state abstraction is too coarse, critical behavioral distinctions collapse; if too fine, state space explodes and model checking becomes intractable.

### Mechanism 2
- Claim: Online learning of an MDP from execution traces enables modeling emergent behavior that cannot be specified a priori.
- Mechanism: The Online Model Learner maintains observed transition frequencies and updates MDP probabilities incrementally. As the agent executes, the model converges toward the agent's actual behavioral distribution.
- Core assumption: Agent behavior is sufficiently stationary that learned probabilities remain valid over relevant time horizons; drift is detectable and recoverable.
- Evidence anchors:
  - [abstract] "It then uses online learning to dynamically build and update a Markov Decision Process (MDP) that formally models the agent's emergent behavior."
  - [section IV-A.2] "Previous work in predictive RV and online learning shows that a probabilistic model (like an MDP, or in our case an AMDP) can be learned dynamically from execution traces."
  - [corpus] "Are Agents Probabilistic Automata?" (arxiv 2510.23487) provides theoretical grounding but limited empirical validation; corpus evidence for learning convergence in production settings is currently weak.
- Break condition: Non-stationary environments or concept drift cause learned probabilities to diverge from actual behavior faster than the model can adapt.

### Mechanism 3
- Claim: Probabilistic model checking on the learned MDP provides quantitative guarantees (e.g., success probability, expected completion time) for real-time decision support.
- Mechanism: Once the MDP is constructed, probabilistic temporal logic (PCTL) queries are evaluated using Storm model checker. Properties like P_max=?[F success] yield numerical bounds rather than binary verdicts.
- Core assumption: The learned MDP is structurally and parametrically accurate enough that model checking results transfer to actual agent behavior.
- Evidence anchors:
  - [abstract] "Using probabilistic model checking, the framework then verifies quantitative properties in real-time."
  - [section IV-D] "This model enables real-time verification... Success probability (P_max=?) predicts fix likelihood and guides resource allocation."
  - [corpus] Continuous assurance work (arxiv 2511.14805) aligns conceptually but doesn't validate specific PMC accuracy for learned MDPs.
- Break condition: Model checking results are only as reliable as the MDP; if transition probabilities are misestimated or states are misspecified, guarantees are misleading.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: Core formalism AgentGuard uses to represent agent behavior. Without understanding states, actions, transition probabilities, and rewards, the verification logic is opaque.
  - Quick check question: Can you explain why an MDP separates nondeterministic agent choices from probabilistic environment outcomes?

- Concept: **Runtime Verification (RV)**
  - Why needed here: AgentGuard is fundamentally an RV framework. Understanding the tradeoff—RV cannot prove properties about unobserved paths but avoids state-space explosion—is essential.
  - Quick check question: What can runtime verification guarantee that offline model checking cannot, and vice versa?

- Concept: **Probabilistic Temporal Logic (PCTL)**
  - Why needed here: The language for expressing verification queries (e.g., "probability of reaching success within k steps"). Required to write meaningful properties.
  - Quick check question: How would you express "the probability of failure within 10 cycles is below 5%" in PCTL?

## Architecture Onboarding

- Component map:
  - **Trace Monitor & Event Abstrator**: Instruments agent framework, logs s→a→s' transitions
  - **Online Model Learner**: Maintains MDP, updates transition probabilities from frequencies
  - **Probabilistic Model Checker (Storm)**: Evaluates PCTL properties on MDP
  - **Dashboard & Actuator**: Displays guarantees, triggers alerts/callbacks on threshold breach

- Critical path: Raw agent I/O → Event abstraction → MDP update → PMC query → Dashboard output. The AnalyzerThread orchestrates this loop concurrently with agent execution.

- Design tradeoffs:
  - Manual vs. automated state abstraction: Paper explicitly flags this as future work; currently requires developer to define states in YAML config
  - Periodic full re-verification vs. incremental: Full verification is simpler but introduces latency as model grows; incremental is planned but not implemented
  - Framework-agnostic API vs. deep integration: Logging-style API eases adoption but may miss internal state nuance

- Failure signatures:
  - **Low confidence signals**: High variance in learned transition probabilities, frequent model structure changes
  - **Divergence indicators**: Predicted success probability consistently mismatches observed outcomes; expected completion time diverges from actual
  - **State explosion**: Model checking latency spikes as state space grows beyond hundreds of states

- First 3 experiments:
  1. **Baseline calibration**: Run target agent on a controlled task set (e.g., 50 bugs for RepairAgent) with AgentGuard logging only. Compare learned transition probabilities against manually audited ground truth to validate abstraction quality.
  2. **Property sensitivity test**: Define 3-5 PCTL properties (success probability, expected cycles, resource bounds). Vary thresholds and observe false positive/negative rates for intervention triggers.
  3. **Drift detection pilot**: Intentionally modify agent behavior mid-run (e.g., change prompting strategy) and measure how quickly AgentGuard's learned MDP reflects the new distribution.

## Open Questions the Paper Calls Out

- Can techniques such as Partially Observable Markov Decision Processes (POMDPs) effectively automate the state abstraction process currently requiring manual definition? The paper suggests exploring automated state abstraction as future work, noting that manual state definition creates a scalability bottleneck limiting the framework's adaptability to new agentic systems without expert intervention.

- Can incremental verification algorithms sufficiently reduce the runtime overhead associated with periodic re-verification? The authors identify that periodic re-verification can introduce substantial overhead and note the need to incorporate incremental verification algorithms in future work to maintain real-time constraints.

- Can the framework be extended to model Multi-Agent Systems (MAS) using stochastic games? The paper explicitly lists integrating stochastic games via PRISM-games to support analysis of MAS as a necessary improvement, as current MDP modeling cannot capture competitive or cooperative dynamics in multi-agent environments.

## Limitations

- Manual state abstraction requirement creates significant scalability bottleneck and requires developer expertise
- Learning mechanism lacks detail on smoothing, convergence guarantees, or handling non-stationary behavior
- No systematic evaluation of whether learned models actually predict future behavior accurately
- Model checking results presented without validation against ground truth or measurement of false positive/negative rates

## Confidence

- **High Confidence**: The architectural components are clearly specified and technically sound. The three-stage pipeline (trace → MDP → PMC) is implementable as described, and the Storm model checker integration is well-established technology.
- **Medium Confidence**: The qualitative benefits are demonstrated (real-time verification, quantitative guarantees), but quantitative performance metrics are sparse. The example probabilities and properties are illustrative rather than systematically validated.
- **Low Confidence**: The scalability and robustness claims are largely future work. State explosion handling, automated abstraction, and incremental verification are mentioned but not implemented or evaluated.

## Next Checks

1. **Ground Truth Calibration**: Run AgentGuard on a controlled set of 20-50 RepairAgent executions with known outcomes. Compare learned transition probabilities and predicted success rates against actual outcomes to measure prediction accuracy and identify systematic biases in the abstraction or learning process.

2. **Property Sensitivity Analysis**: Define a suite of 5-10 PCTL properties covering success probability, expected resource usage, and safety constraints. Systematically vary thresholds and measure false positive/negative rates for triggering alerts, establishing the reliability of the verification results for decision-making.

3. **Non-Stationarity Stress Test**: Design an experiment where agent behavior changes mid-execution (e.g., prompt modification, tool availability changes). Measure how quickly the learned MDP adapts, whether the system detects the drift, and if verification results remain meaningful during the transition period.