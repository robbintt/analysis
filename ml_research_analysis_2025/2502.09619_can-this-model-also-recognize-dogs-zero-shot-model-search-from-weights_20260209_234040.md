---
ver: rpa2
title: Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights
arxiv_id: '2502.09619'
source_url: https://arxiv.org/abs/2502.09619
tags:
- probes
- search
- arxiv
- probelog
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of searching for classification
  models that can recognize a specific target concept (e.g., "dog") within large model
  repositories, without access to model metadata or training data. The authors propose
  ProbeLog, a method that generates logit-level descriptors for each output dimension
  of models by observing their responses to a fixed set of input probes.
---

# Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights

## Quick Facts
- arXiv ID: 2502.09619
- Source URL: https://arxiv.org/abs/2502.09619
- Authors: Jonathan Kahana; Or Nathan; Eliahu Horwitz; Yedid Hoshen
- Reference count: 22
- Primary result: 40% top-1 accuracy in retrieving models that can recognize ImageNet concepts from text queries using only model weights

## Executive Summary
This paper introduces ProbeLog, a method for searching large repositories of classification models to find those capable of recognizing specific target concepts, without requiring metadata or training data. The approach works by probing each model with a fixed set of inputs and using the resulting logit response patterns as functional descriptors for each output dimension. These descriptors are then used for semantic matching, enabling both concept-based and text-based retrieval. The method is extended with cross-modal text alignment using CLIP and accelerated through Collaborative Probing, which uses matrix factorization to reduce the number of required model queries by 3x while maintaining accuracy.

## Method Summary
ProbeLog represents each output dimension (logit) of a model by its response vector to a fixed set of probe inputs. These logit descriptors capture the functional behavior of the model and enable semantic matching across different architectures and class orderings. For text-based search, the method synthesizes zero-shot descriptors using a multimodal model like CLIP to project text queries into the probe response space. To reduce computational cost, Collaborative Probing employs matrix completion to impute missing probe responses from a sparse encoding matrix. Retrieval is performed using an asymmetric top-k discrepancy metric that focuses on the most confident probe responses.

## Key Results
- ProbeLog achieves over 40% top-1 accuracy in retrieving models capable of recognizing ImageNet target concepts from text queries
- Collaborative Probing reduces the number of required probes by 3x while maintaining retrieval accuracy
- Asymmetric top-k discrepancy significantly outperforms standard distance metrics, improving accuracy from 15.3% to 40.6%
- The method works across diverse model architectures and can identify models capable of recognizing concepts they were never explicitly trained on

## Why This Works (Mechanism)

### Mechanism 1: Functional Representation via Logit Probing
Representing individual output dimensions by their response patterns to fixed inputs allows semantic matching invariant to model architecture and class ordering. The vector of responses for a specific logit across all probes forms a "functional fingerprint" that correlates with logits detecting the same concept, even across different architectures.

### Mechanism 2: Asymmetric Top-K Discrepancy
Comparing logit descriptors using only the highest-activating probes for the query improves retrieval accuracy by filtering out low-confidence noise. This metric focuses on the "semantic core" of the logit's behavior by computing distance only on the top-k values of the query descriptor, re-indexed on the gallery descriptors.

### Mechanism 3: Cross-Modal Text Alignment
A zero-shot text query can be matched to model logits by synthesizing a pseudo-descriptor using CLIP. The method calculates cosine similarities between the text embedding and each probe's image embedding, creating a synthetic descriptor that approximates how a perfect classifier for that concept would respond to the probes.

## Foundational Learning

- **Collaborative Filtering / Matrix Completion**: Used to reduce computational cost by imputing missing probe responses. Low-rank assumption allows guessing a model's response to unseen images based on responses to other images and similar models.
- **Zero-Shot Learning (CLIP)**: Essential for mapping natural language queries to numerical probe-response space without training dedicated classifiers. CLIP enables comparison of text strings to image features through shared embedding space.
- **Logits and Softmax**: The method operates on pre-softmax logits rather than probabilities. Raw logit values are preferred for distinguishing fine-grained features across different models due to their dynamic range and lack of normalization constraints.

## Architecture Onboarding

- **Component map**: Probe Bank -> Encoder (Models -> Logits) -> Collaborative Filter (Optional) -> Normalizer -> Query Interface (Text -> CLIP -> Similarity Vector) -> Search Engine (Nearest Neighbor with Top-K asymmetric distance)
- **Critical path**: Probe Bank selection and Normalization logic. If probes are not diverse or normalization fails to align domains, retrieval fails.
- **Design tradeoffs**: More probes increase accuracy but computational cost; Collaborative Probing reduces compute by 3x but introduces approximation error; in-domain probes work best but are overfitted while out-of-domain probes are more general but slightly less accurate.
- **Failure signatures**: Semantic drift (retrieving similar but wrong concepts), domain mismatch (failure on specialized tasks), and scale mismatch (text and model descriptors in different vector scales without proper normalization).
- **First 3 experiments**: 1) Verify ProbeLog descriptors for same-class logits from different architectures have high correlation, 2) Compare asymmetric top-k metric vs standard distance on small retrieval task, 3) Test SVD completion by masking 50% of probe responses and measuring reconstruction error and retrieval drop.

## Open Questions the Paper Calls Out

- Can non-random probe selection substantially reduce the number of probes required compared to random sampling? The authors hypothesize a curated probe set could improve efficiency.
- Can ProbeLog be modified to search generative models where output dimensions don't encode semantic concepts? The current method relies on observing responses at specific output dimensions.
- Can adaptive sampling strategies improve Collaborative Probing efficiency beyond the current random selection and SVD-based imputation? The authors suggest selecting probes adaptively based on initial responses could yield better results.

## Limitations
- Effectiveness depends on probe dataset diversity and relevance, with specialized tasks potentially requiring domain-specific probes
- Collaborative Probing introduces approximation errors that aren't fully characterized, particularly when repository structure assumptions break down
- CLIP-based text alignment may fail for highly specialized or abstract concepts outside its training distribution

## Confidence
- **High Confidence**: Core mechanism of using logit response patterns as functional descriptors (validated through direct comparisons showing high correlation between models trained on same data)
- **Medium Confidence**: Collaborative Probing approach (theoretically sound but implementation details underspecified)
- **Low Confidence**: Scalability claims to millions of models (not empirically validated in paper)

## Next Checks
1. **Probe Bank Sensitivity Analysis**: Systematically vary probe dataset composition and measure impact on retrieval accuracy for different task categories
2. **Collaborative Probing Error Bounds**: Conduct controlled experiments masking different percentages of probe responses to characterize approximation error and retrieval degradation
3. **Cross-Domain Retrieval Failure Modes**: Test on intentionally mismatched domains to identify and document specific failure patterns and limitations