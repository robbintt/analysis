---
ver: rpa2
title: Towards Scalable Lottery Ticket Networks using Genetic Algorithms
arxiv_id: '2508.08877'
source_url: https://arxiv.org/abs/2508.08877
tags:
- accuracy
- network
- networks
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a genetic algorithm approach to identify
  strong lottery ticket subnetworks, which are highly sparse neural network configurations
  that match the performance of fully trained models without requiring any training.
  The method leverages evolutionary optimization to directly search for optimal pruning
  masks, bypassing gradient-based training entirely.
---

# Towards Scalable Lottery Ticket Networks using Genetic Algorithms

## Quick Facts
- arXiv ID: 2508.08877
- Source URL: https://arxiv.org/abs/2508.08877
- Reference count: 0
- Primary result: Genetic algorithm identifies strong lottery ticket subnetworks without training, outperforming edge-popup on accuracy and sparsity

## Executive Summary
This paper introduces a genetic algorithm approach to identify strong lottery ticket subnetworks, which are highly sparse neural network configurations that match the performance of fully trained models without requiring any training. The method leverages evolutionary optimization to directly search for optimal pruning masks, bypassing gradient-based training entirely. Experiments on binary and multi-class classification tasks show that the proposed approach achieves better accuracy and sparsity than state-of-the-art methods like edge-popup, while also offering scalability to larger networks.

## Method Summary
The method uses a genetic algorithm to search for optimal binary pruning masks within randomly initialized, overparameterized neural networks. It employs a population-based evolutionary search with selection, crossover, and mutation operators to optimize mask configurations. The approach uses accuracy as the fitness function for binary classification tasks and cross-entropy loss for multi-class problems. Post-evolutionary pruning is applied to further enhance sparsity without degrading performance.

## Key Results
- GA approach achieves better accuracy and sparsity than edge-popup on binary and multi-class classification tasks
- Cross-entropy loss optimization outperforms accuracy maximization for multi-class problems, yielding more confident predictions
- Post-evolutionary pruning significantly increases sparsity while maintaining performance
- Method scales to larger networks and offers competitive performance without gradient information

## Why This Works (Mechanism)

### Mechanism 1
Overparameterized, randomly initialized networks contain high-performing subnetworks that match trained model accuracy without weight updates. The genetic algorithm searches the discrete space of binary pruning masks applied to fixed random weights, iteratively improving accuracy and sparsity through combinatorial optimization over possible subnetworks.

### Mechanism 2
Loss minimization outperforms accuracy maximization as a fitness signal for multi-class problems, yielding more confident and generalizable subnetworks. Directly optimizing accuracy in multi-class settings can yield high-accuracy, high-loss solutions (uncertain predictions). Minimizing cross-entropy loss provides a richer fitness signal that encourages confident predictions and better generalization.

### Mechanism 3
Gradient-free genetic algorithms explore complex, non-convex binary mask spaces more effectively than gradient-based methods for this discrete problem. The GA uses population-based search with crossover and mutation over binary masks, escaping local optima without requiring differentiability.

## Foundational Learning

- **Strong Lottery Ticket Hypothesis (SLTH)**: Why needed: This is the core theoretical premise asserting that randomly weighted, overparameterized networks contain subnetworks achieving trained-model performance without training. Quick check: Can you explain why SLTH implies training might be unnecessary for finding high-performing subnetworks?

- **Genetic Algorithms (GAs) for Combinatorial Optimization**: Why needed: The paper's core method is a GA applied to the discrete problem of finding optimal binary pruning masks. Understanding selection, crossover, mutation, and fitness-based survival is essential. Quick check: How does a GA's population-based search differ from gradient descent when exploring a discrete search space?

- **Model Pruning & Sparsity**: Why needed: The goal is to find subnetworks that are both high-performing and sparse (few non-zero weights). Pruning removes weights via a binary mask; sparsity quantifies this reduction. Quick check: How does a binary mask represent a subnetwork, and what does sparsity measure?

## Architecture Onboarding

- **Component map**: Fixed Random Network (Weights) -> Binary Mask (Genotype) -> Masked Subnetwork (Phenotype) -> Fitness Evaluation (Accuracy/Loss) -> GA Loop (Selection, Crossover, Mutation) -> Post-Evolutionary Pruning

- **Critical path**:
  1. Initialize a fixed, random weight network
  2. Generate initial population of random binary masks (with optional adaptive accuracy bound)
  3. Evaluate fitness (accuracy for binary, loss for multi-class) and sparsity of each masked subnetwork
  4. Select parents based on performance
  5. Apply crossover and mutation to generate offspring
  6. Select survivors based on performance and sparsity
  7. Repeat for set generations or until convergence
  8. Apply post-evolutionary pruning to the best found mask to further increase sparsity without hurting performance

- **Design tradeoffs**: Accuracy vs. Sparsity (managed through two-stage selection), Accuracy vs. Loss objective (use accuracy for binary, loss for multi-class), Population Size vs. Generations (larger populations/generations improve exploration but increase computational cost)

- **Failure signatures**: Low final accuracy (network may be underparameterized or task too complex), High variance between runs (search space may be highly non-convex or initialization poorly suited), Poor multi-class performance (using accuracy instead of loss as fitness function often leads to high-accuracy, low-confidence solutions)

- **First 3 experiments**:
  1. Reproduce binary classification result: Apply GA to moons dataset with small network ([2, 75, 2]), use accuracy as fitness function, verify >97% test accuracy and >50% sparsity
  2. Test loss vs. accuracy objective: On multi-class blobs dataset (7 clusters), run two GA experiments (one optimizing accuracy, one optimizing cross-entropy loss), compare final test accuracies and training loss curves
  3. Assess post-evolutionary pruning impact: Take best mask from experiment 1, apply post-evolutionary pruning, quantify additional sparsity gain without accuracy drop

## Open Questions the Paper Calls Out

### Open Question 1
Why do Kaiming normal and signed Kaiming constant initializations cause the edge-popup algorithm to fail on the circles dataset? The authors observed edge-popup performed no better than random guessing with Kaiming methods on this dataset, hypothesizing "Gaussian distortions" but providing no definitive proof.

### Open Question 2
Can promising search areas be identified at the start of the evolution to restrict the combinatorial search space and accelerate convergence? The current GA searches a vast discrete space; restricting this search a priori could improve efficiency, but methods for early identification are unknown.

### Open Question 3
Are the current genetic operators prone to suboptimal local minima in high-dimensional parameter spaces? Regarding the performance drop on the digits dataset, the authors state the exact reasons and potential of more sophisticated operators remain content of future work.

## Limitations
- Empirical evaluation limited to small-scale binary and multi-class classification problems
- Limited comparison to state-of-the-art methods beyond edge-popup
- Lacks comprehensive ablation studies on GA hyperparameters
- Scalability claims not rigorously validated on larger networks or real-world datasets

## Confidence

- **High**: The SLTH premise and general effectiveness of GA for mask optimization on small datasets
- **Medium**: The superiority over edge-popup and scalability claims
- **Low**: The robustness of the method to initialization schemes and dataset complexity beyond tested cases

## Next Checks

1. **Ablation Study**: Systematically vary GA hyperparameters (mutation rate, crossover type, population size) to identify their impact on accuracy and sparsity

2. **Scalability Test**: Evaluate the method on larger networks (e.g., ResNet architectures) and real-world datasets (e.g., CIFAR-10, ImageNet) to validate scalability claims

3. **Runtime Analysis**: Compare computational cost (training/inference time) of GA approach with gradient-based methods like edge-popup across different network sizes and datasets