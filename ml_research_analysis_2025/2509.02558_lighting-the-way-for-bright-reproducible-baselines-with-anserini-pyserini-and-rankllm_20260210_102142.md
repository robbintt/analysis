---
ver: rpa2
title: 'Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini,
  and RankLLM'
arxiv_id: '2509.02558'
source_url: https://arxiv.org/abs/2509.02558
tags:
- retrieval
- bm25
- bright
- query
- anserini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents reproducible baselines for the BRIGHT benchmark,
  which evaluates retrieval systems on reasoning-intensive queries. The authors establish
  solid retrieval baselines using BM25, BGE-large-en-v1.5, and SPLADE-v3 integrated
  into Anserini, Pyserini, and RankLLM toolkits.
---

# Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini, and RankLLM

## Quick Facts
- arXiv ID: 2509.02558
- Source URL: https://arxiv.org/abs/2509.02558
- Reference count: 40
- Primary result: Established reproducible baselines for BRIGHT benchmark, showing query-side BM25 and LLM reranking significantly improve nDCG@10 from 0.162-0.172 to 0.241-0.274.

## Executive Summary
This paper establishes reproducible baselines for the BRIGHT benchmark, which evaluates retrieval systems on reasoning-intensive queries. The authors implement and test sparse retrievers (BM25, SPLADE-v3) and dense retrievers (BGE-large-en-v1.5) within the Anserini, Pyserini, and RankLLM toolkits. They discover that applying BM25 weighting to query tokens (query-side BM25) outperforms traditional bag-of-words approaches for long reasoning queries, and implement this method in both toolkits. The experiments show that BM25-weighted query vectors achieve higher nDCG@10 scores than bag-of-words for medium-length queries (16-256 tokens). Pairwise fusion of BM25 with dense or sparse retrievers improves effectiveness, and second-stage reranking with LLMs (Qwen3-8b and gpt-oss-20b) provides additional gains. The authors also identify data quality issues in the BRIGHT corpus and provide adjusted relevance judgments.

## Method Summary
The paper establishes baselines using Anserini, Pyserini, and RankLLM toolkits on the BRIGHT dataset (12 reasoning tasks across StackExchange, Coding, Theorem categories). First-stage retrieval employs BM25 (k1=0.9, b=0.4), BGE-large-en-v1.5, and SPLADE-v3, with a novel query-side BM25 implementation that applies BM25 weighting to query tokens. Pairwise Reciprocal Rank Fusion (RRF) combines BM25 with other retrievers. Second-stage listwise reranking uses LLMs (Qwen3-8b, gpt-oss-20b) with extended 16k context windows. The study identifies corpus data quality issues including duplicate documents and missing gold IDs, providing adjusted relevance judgments for more accurate evaluation.

## Key Results
- Query-side BM25 outperforms traditional bag-of-words BM25 for medium-length reasoning queries (16-256 tokens), achieving higher nDCG@10 scores.
- Pairwise RRF fusion of BM25 with SPLADE or BGE improves effectiveness by mitigating individual model errors.
- LLM-based second-stage reranking (Qwen3-8b, gpt-oss-20b) increases nDCG@10 from 0.162-0.172 to 0.241-0.274.
- Data quality issues in BRIGHT corpus (duplicates, missing gold IDs) affect evaluation scores, with adjusted qrels recommended for accurate assessment.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Applying BM25 weighting to query tokens (query-side BM25) likely improves retrieval effectiveness for medium-length reasoning queries compared to standard bag-of-words (BoW) representations.
- **Mechanism:** For longer queries (16–256 tokens), simply counting term frequency (BoW) allows repetitive tokens to dominate. Applying BM25 weighting introduces saturation and length normalization to the query vector, down-weighting frequent terms and better calibrating the importance of unique tokens in long prompts.
- **Core assumption:** Reasoning-intensive queries contain sufficient term repetition and length variation for the BM25 saturation function to alter the ranking meaningfully compared to raw frequency counts.
- **Evidence anchors:** [abstract] "query-side BM25 outperforms traditional bag-of-words... for long reasoning queries." [section 4.1] Figure 1 shows nDCG@10 gains for query lengths between 16–256 tokens.

### Mechanism 2
- **Claim:** Pairwise Reciprocal Rank Fusion (RRF) of lexical and semantic retrievers appears to robustly improve effectiveness by mitigating individual model errors.
- **Mechanism:** BM25 (lexical), SPLADE (learned sparse), and BGE (dense) capture different relevance signals. RRF aggregates ranked lists based on rank positions rather than raw scores, allowing systems to retain documents that appear high in only one list but are critical for complex reasoning.
- **Core assumption:** The relevance judgments required for reasoning tasks are distributed such that no single retriever dominates across all query types.
- **Evidence anchors:** [abstract] "Pairwise fusion... improves effectiveness." [section 4.2] "BM25 and learned models tend to complement one another by mitigating each other's errors."

### Mechanism 3
- **Claim:** Second-stage listwise reranking with LLMs provides significant effectiveness gains by applying parametric reasoning capabilities to the retrieval candidates.
- **Mechanism:** First-stage retrievers filter the corpus, but LLMs analyze the top-100 candidates to re-score based on the reasoning requirements in the prompt, effectively handling "reasoning-intensive" constraints that embedding similarity misses.
- **Core assumption:** The LLM's context window is sufficient to handle the concatenated query and candidate documents without truncating critical reasoning clues.
- **Evidence anchors:** [abstract] "second-stage reranking... provides additional gains, with nDCG@10 increasing from 0.162-0.172 to 0.241-0.274." [section 3.2] Mentions extending RankLLM context length to 16k tokens.

## Foundational Learning

- **Concept: BM25 Saturation & Length Normalization**
  - **Why needed here:** To understand why "query-side BM25" differs from "bag-of-words." You must grasp how the $k_1$ parameter saturates term frequency and how $b$ normalizes document (or query) length.
  - **Quick check question:** If a query repeats the word "optimization" five times, how does BoW vs. BM25 weighting differ in the resulting query vector?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** This is the glue combining disparate retrievers in the paper's best baselines.
  - **Quick check question:** If Document A is rank 1 in List 1 but rank 100 in List 2, and Document B is rank 10 in both, which document does RRF generally prefer?

- **Concept: Listwise Reranking**
  - **Why needed here:** The paper highlights this as the key step for "reasoning-intensive" gains.
  - **Quick check question:** Why does listwise reranking (scoring multiple docs at once) often outperform pointwise reranking (scoring one doc at a time) for relevance tasks?

## Architecture Onboarding

- **Component map:** Anserini/Pyserini (first-stage indexing and retrieval) -> Fusion Layer (RRF) -> RankLLM (LLM reranking)

- **Critical path:**
  1. Load BRIGHT corpus into Anserini/Pyserini
  2. Generate retrieval outputs using the specific "query-side BM25" generator (not default BoW)
  3. Fuse BM25 + SPLADE/BGE outputs via RRF
  4. Feed top-100 fused results into RankLLM (16k context) for final ranking

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** The paper notes "accurate" BM25 (exact norms) vs. "quantized" (Lucene default). Quantized is faster but theoretically less precise, though the paper finds the difference negligible.
  - **Context Window:** Standard RankLLM (4k) will fail on BRIGHT; must use 16k window, increasing GPU memory load.

- **Failure signatures:**
  - **Low nDCG with BM25:** Check if you are using the standard BoW generator instead of the new `BM25QueryGenerator`
  - **Reranker Error:** "Context length exceeded" implies the 16k window extension is not active or query/docs are too long
  - **Missing Gold IDs:** The paper notes duplicates/missing IDs in BRIGHT corpora; evaluation scores may be artificially lower than reality

- **First 3 experiments:**
  1. **Baseline Validation:** Run Anserini BM25 on BRIGHT using both BoW and Query-side BM25 to replicate the score gap (Table 1)
  2. **Fusion Ablation:** Combine BM25 + SPLADE using RRF and compare against BM25 + BGE to identify the strongest first-stage pair (Table 3)
  3. **Reranking Integration:** Take the strongest fusion output and apply `gpt-oss-20b` via RankLLM to verify the jump to ~0.27 nDCG (Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does query-side BM25 provide similar effectiveness gains on traditional information retrieval benchmarks (e.g., BEIR) as observed on the reasoning-intensive BRIGHT dataset?
- **Basis in paper:** [explicit] The authors state in Section 5: "Future work should examine its behavior on traditional benchmarks like BEIR, both to validate its robustness and to further explore its potential correlation with query length or structure."
- **Why unresolved:** The study limited the evaluation of query-side BM25 to BRIGHT, leaving its generalizability to datasets with shorter, keyword-based queries unknown.

### Open Question 2
- **Question:** How do retrievers optimized specifically for reasoning via reinforcement learning compare against the standard baselines (BM25, BGE, SPLADE) established in this work?
- **Basis in paper:** [explicit] Section 5 notes that "new retrievers—particularly those optimized for reasoning-intensive workloads via reinforcement learning... are being developed at a rapid pace" and were excluded from the current evaluation.
- **Why unresolved:** The authors restricted experiments to widely-used, standard retrievers to ensure reproducibility, leaving the performance of specialized reasoning retrievers on BRIGHT untested within this framework.

### Open Question 3
- **Question:** How significantly do the duplicate documents and missing gold IDs in the BRIGHT corpus affect the evaluation scores of high-performing, state-of-the-art retrieval systems?
- **Basis in paper:** [inferred] The authors identified data quality issues and adjusted relevance judgments, noting: "We expect even larger discrepancies for stronger retrievers and rerankers, which are more likely to be incorrectly penalized for retrieving missing gold IDs."
- **Why unresolved:** The analysis of data quality impact was performed on the authors' baselines; the effect on the highest-scoring leaderboard entries, which may retrieve different sets of documents, remains estimated.

### Open Question 4
- **Question:** What are the computational and latency trade-offs of applying LLM-based query expansion or agentic multi-turn retrieval compared to the single-shot retrieval methods evaluated?
- **Basis in paper:** [explicit] Section 5 highlights that while LLM-based expansion helps, "these improvements come at the cost of increased computation and latency, raising concerns about efficiency and the practicality of deployment in real-world settings."
- **Why unresolved:** The paper focused on establishing "one-shot retrieval" baselines without expansion to maintain simplicity and reproducibility, deferring the efficiency analysis of complex methods.

## Limitations
- Effectiveness claims rely on specific hyperparameter choices (RRF K=60, BM25 k1=0.9, b=0.4) that are not explicitly validated through ablation studies.
- The significant performance gains from query-side BM25 versus bag-of-words are demonstrated, but the paper does not systematically ablate the contribution of each component (BM25 weighting, saturation function, length normalization) to isolate which mechanism drives the improvement.
- The "gpt-oss-20b" model used for reranking lacks clear identification and access documentation, which could hinder exact reproduction.
- The paper notes data quality issues in BRIGHT (duplicate documents, missing gold IDs) but does not quantify how these affect the reported scores versus a "clean" dataset.

## Confidence
- **High Confidence:** The core observation that query-side BM25 outperforms bag-of-words for medium-length reasoning queries (16-256 tokens) is well-supported by Figure 1 and consistent with the known behavior of BM25 saturation and length normalization. The effectiveness gains from LLM reranking (nDCG@10 increasing from ~0.16 to ~0.27) are also directly measurable and reproducible given the infrastructure.
- **Medium Confidence:** The claim that pairwise RRF fusion of lexical and semantic retrievers robustly improves effectiveness is supported by the ablation showing BM25 + SPLADE outperforms individual models, but the paper does not test whether other fusion methods (e.g., weighted sum) would yield similar results. The assumption that the LLM's 16k context window is sufficient for all BRIGHT queries is reasonable given the paper's extension, but edge cases with extremely long queries could still truncate critical reasoning clues.
- **Low Confidence:** The exact identity and performance characteristics of "gpt-oss-20b" are unclear, as is the precise RankZephyr prompt template. Without these specifics, exact reproduction of the 0.27 nDCG@10 score is uncertain.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary the RRF K parameter (e.g., K=30, 60, 100) and BM25 parameters (k1=0.5, 0.9, 1.2; b=0.3, 0.4, 0.6) to quantify their impact on nDCG@10 and determine if the reported scores are robust to these choices.
2. **Query-side BM25 Mechanism Ablation:** Create controlled ablations of the query-side BM25 implementation to isolate the contribution of term frequency saturation versus length normalization. For example, test a variant that applies only length normalization without saturation, and another that applies only saturation without length normalization, to pinpoint the active mechanism for the performance gain.
3. **Data Quality Impact Assessment:** Using the adjusted qrels provided by the authors, re-run the full pipeline (BM25 + SPLADE + RRF + LLM reranking) and compare the nDCG@10 scores to those obtained with the original qrels. Quantify the performance difference to measure the extent to which data quality issues in BRIGHT affect the benchmark results.