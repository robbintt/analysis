---
ver: rpa2
title: 'Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation'
arxiv_id: '2506.09376'
source_url: https://arxiv.org/abs/2506.09376
tags:
- diffusion
- generative
- training
- distillation
- one-step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models excel at image generation but require many iterative
  steps, limiting efficiency. This work identifies that knowledge transfer in distillation
  suffers from mismatched local minima between teacher and student models.
---

# Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation

## Quick Facts
- arXiv ID: 2506.09376
- Source URL: https://arxiv.org/abs/2506.09376
- Reference count: 31
- One-step generator achieved FID=1.16 on ImageNet 64×64 with minimal data

## Executive Summary
Diffusion models excel at image generation but require many iterative steps, limiting efficiency. This work identifies that knowledge transfer in distillation suffers from mismatched local minima between teacher and student models. To address this, the authors propose D2O, which uses a standalone GAN objective instead of distillation losses, allowing the student to align with the data distribution directly. Further, they introduce D2O-F, a one-step generator obtained by fine-tuning a pre-trained diffusion model with 85% of parameters frozen, achieving strong performance with minimal data. D2O-F reaches FID=1.54 on CIFAR-10 with only 5M images and FID=1.16 on ImageNet 64×64, outperforming most distillation methods while using far fewer training steps. Frequency-domain analysis reveals that diffusion models learn time- and block-wise frequency-specific processing, supporting the idea that diffusion training acts as generative pre-training. Overall, the work reframes diffusion training as a foundation for efficient one-step generation.

## Method Summary
The authors propose D2O and D2O-F to convert pre-trained diffusion models into one-step generators. D2O uses a standalone GAN objective (no distillation loss) with a Projected GAN discriminator. D2O-F freezes 85.8% of convolution weights from the pre-trained EDM U-Net, training only normalization, QKV projections, and skip connections. The method uses non-saturating GAN loss with R1 regularization (γ_r1 = 1e-4 to 4e-4), no data augmentation, and Adam optimizer (β1=0, β2=0.99). Key innovations include identifying mismatched local minima in distillation, using GAN objectives instead, and demonstrating that freezing convolutional layers preserves generative capability while preventing overfitting on small datasets.

## Key Results
- D2O-F achieves FID=1.54 on CIFAR-10 using only 5M images
- D2O-F reaches FID=1.16 on ImageNet 64×64 with 5M images
- D2O outperforms all distillation-based methods while requiring fewer training steps
- Frequency analysis shows diffusion models learn time- and block-wise frequency-specific processing

## Why This Works (Mechanism)

### Mechanism 1
Teacher-student distillation fails because multi-step teachers and one-step students occupy different local minima, making instance-level imitation suboptimal. Teachers process through multiple latent-space transformations; students perform one. This creates divergent optimization landscapes. Forcing x_student ≈ x_teacher constrains the student to a minimum that may be unattainable with its architecture. FID divergence between teacher and student outputs reflects genuinely different local optima, not just optimization noise. Evidence: FID between 2-step teacher and 1-step student = 1.78 despite similar dataset FIDs; FID increases with teacher steps.

### Mechanism 2
A standalone GAN objective—without any distillation loss—suffices to convert pre-trained diffusion models into one-step generators by aligning directly with data distribution. GAN discriminator provides distributional feedback rather than instance-wise targets. Student explores its own parameter space freely, finding its optimal solution rather than chasing teacher-specific outputs. The pre-trained diffusion U-Net already contains sufficient generative capability; GAN fine-tuning merely redirects it toward one-step output. Evidence: Adding CD loss to D2O slows early convergence; D2O-F matches performance without distillation term.

### Mechanism 3
Diffusion training functions as generative pre-training, encoding frequency-specific processing patterns that persist and can be repurposed. Diffusion models learn time-dependent frequency processing—low frequencies early in denoising, high frequencies later. Block-wise specialization emerges naturally from U-Net resolution hierarchy. These patterns remain accessible post-fine-tuning. Evidence: D_FFT(O_t, O_{t-1}) shows frequency shift from low→high across time steps; block depth correlates with frequency range.

## Foundational Learning

- Concept: **Diffusion ODE Solvers**
  - Why needed here: The paper references Euler/Heun solvers and their role in iterative sampling; understanding why multi-step solvers create different optimization landscapes than single-step passes is central.
  - Quick check question: Can you explain why S_ϕ applied twice (teacher) vs. once (student) might reach different outputs even from identical noise?

- Concept: **GAN Training Dynamics (Non-saturating loss, R1 regularization)**
  - Why needed here: D2O replaces distillation with GAN objectives; discriminator choice and regularization critically affect convergence.
  - Quick check question: Why would R1 regularization prevent discriminator overfitting when training data is limited (0.2M images)?

- Concept: **FID as Distribution Distance**
  - Why needed here: The paper's core claim relies on FID divergence between teacher/student as evidence of different local minima.
  - Quick check question: FID compares feature statistics, not pixel-level alignment—why is this appropriate for detecting local minimum mismatch?

## Architecture Onboarding

- Component map:
  - Generator: Pre-trained EDM U-Net (NCSN++ or ADM architecture); in D2O-F, 85.8% of conv weights frozen, only Norm/QKV/Skip tunable
  - Discriminator: Projected GAN with VGG16-BN + EfficientNet-lite0 feature networks (CIFAR-10); DeiT + EfficientNet-lite0 for larger datasets
  - Training loop: Non-saturating GAN loss with R1 regularization (γ_r1 ∈ [1e-4, 4e-4]); no augmentation; Adam (β1=0, β2=0.99)

- Critical path:
  1. Load pre-trained EDM checkpoint
  2. Initialize generator from score model g_ϕ
  3. For D2O-F: freeze all Conv layers, leave Norm/QKV/Skip unfrozen
  4. Train discriminator and generator with separate Adam optimizers
  5. Apply EMA to generator weights (half-life varies by dataset)

- Design tradeoffs:
  - Freezing convs (D2O-F): More stable, less overfitting, slightly better FID—requires pre-training quality
  - Multi-scale discriminator (PG): Better than vanilla VGG but adds complexity; must tune γ_r1 carefully to avoid numerical explosion
  - No augmentation: Contradicts standard GAN practice but justified since EDM pre-training already used augmentation

- Failure signatures:
  - Mode collapse with StyleGAN2 discriminator: Switch to PG or LPIPS
  - Numerical instability with large γ_r1: Reduce by 10×
  - Poor convergence with augmentation enabled: Disable entirely (pre-training covered this)
  - CD loss slowing training: Remove it; D2O-F achieves same quality faster

- First 3 experiments:
  1. Reproduce CIFAR-10 baseline: EDM → D2O with 5M images, PG discriminator, γ_r1=1e-4; target FID ≈ 1.66
  2. Ablate freezing: Train D2O-F (conv frozen) vs. full fine-tuning on 0.2M images; expect D2O-F FID ≈ 4.1 vs. full collapse
  3. Frequency analysis: Visualize D_FFT across time steps on a held-out batch; confirm low→high frequency progression matches Figure 4B

## Open Questions the Paper Calls Out

- Question: Can the generative pre-training and freezing strategy be effectively applied to Diffusion Transformer (DiT) architectures?
  - Basis in paper: Section 6.2 states, "Our model is based on the diffusion U-Net architecture. The effectiveness of our approach on different architectures, such as DiT (Peebles & Xie, 2023), remains to be explored."
  - Why unresolved: The current study validates the D2O and D2O-F methods exclusively on U-Net architectures (EDM), leaving the transferability of the "generative pre-training" hypothesis to attention-based backbones unverified.
  - What evidence would resolve it: Successful application of the D2O-F freezing protocol (freezing 85% of weights) to a DiT backbone, achieving comparable one-step generation performance with minimal data.

- Question: Does the method maintain its data efficiency and performance when scaled to high-resolution, complex datasets like COCO?
  - Basis in paper: Section 6.2 notes the method "has yet to be tested on datasets with higher resolution and more complex generation tasks, such as the COCO (Lin et al., 2015) dataset."
  - Why unresolved: The experiments are restricted to 64×64 resolutions (CIFAR-10, ImageNet 64×64), which possess different semantic complexities and frequency distributions compared to the high-resolution scenes found in COCO.
  - What evidence would resolve it: Demonstrating that D2O-F can achieve competitive FID scores on 256×256 or higher COCO images using the same low-data regime (e.g., millions rather than billions of images) used for CIFAR-10.

- Question: What is the precise theoretical mechanism linking time-step-dependent frequency processing to the one-step generative capability?
  - Basis in paper: Section 6.2 concedes that "efforts to explore the generative capabilities through the diffusion models' frequency response are only preliminary and need to be expanded."
  - Why unresolved: While the paper provides empirical visualizations of block-wise frequency specialization, it lacks a formal theoretical explanation for why freezing convolutional layers preserves the specific capabilities required for one-step generation.
  - What evidence would resolve it: A theoretical framework or ablation study that manipulates specific frequency components or freezes specific blocks based on frequency response, quantitatively predicting the success of the fine-tuning process.

## Limitations
- Method has yet to be tested on datasets with higher resolution and more complex generation tasks, such as the COCO dataset
- The theoretical understanding of how frequency processing relates to one-step generative capability remains preliminary
- Effectiveness on non-U-Net architectures like Diffusion Transformers remains unexplored

## Confidence
- Mechanism 1 (Distillation local minima mismatch): High - Supported by direct FID comparisons and ablation studies
- Mechanism 2 (GAN objective sufficiency): High - Demonstrated through controlled experiments removing distillation losses
- Mechanism 3 (Frequency pre-training): Medium - Empirical evidence shown but theoretical mechanism not fully explained

## Next Checks
1. Verify that freezing convolutional layers while training only normalization/QKV/skip connections preserves FID performance on CIFAR-10
2. Confirm that adding consistency distillation loss to D2O slows convergence compared to standalone GAN objective
3. Replicate frequency analysis showing low→high frequency progression across time steps in diffusion models