---
ver: rpa2
title: 'Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal
  Output Framework for Financial Advisory Videos'
arxiv_id: '2509.20961'
source_url: https://arxiv.org/abs/2509.20961
tags:
- financial
- multimodal
- summary
- arxiv
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FASTER is a multimodal summarization framework for financial advisory\
  \ videos that addresses the challenge of generating concise, factually accurate\
  \ summaries from long-form multimodal content (30\u201340 minutes). The method integrates\
  \ BLIP-2 for visual feature extraction, OCR for textual pattern recognition, and\
  \ Whisper-based transcription with speaker diarization as BOS features."
---

# Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos

## Quick Facts
- arXiv ID: 2509.20961
- Source URL: https://arxiv.org/abs/2509.20961
- Reference count: 40
- The FASTER framework integrates BLIP-2, OCR, and speaker diarization with curriculum-based DPO to generate factually accurate summaries from financial advisory videos.

## Executive Summary
FASTER is a multimodal summarization framework designed for long-form financial advisory videos (30–40 minutes). It addresses the challenge of generating concise, factually accurate summaries by integrating visual features (BLIP-2), textual patterns (OCR), and speaker attribution (Whisper + Pyannote) as BOS features. A modified Direct Preference Optimization (DPO) framework with curriculum learning and fact-checking ensures precision and relevance. A ranker-based mechanism aligns keyframes with summarized content for multimodal output. Evaluated on the Fin-APT dataset of 470 financial advisory videos, FASTER demonstrates strong performance in cross-domain experiments against LLMs and VLMs.

## Method Summary
FASTER extracts BOS features including BLIP-2 visual captions, OCR-extracted on-screen text, and Whisper transcripts with Pyannote speaker diarization. These features are fused into a unified prompt for Gemma2-9b fine-tuning. The model generates four candidate summaries ranked by GPT-4o, with curriculum DPO progressively training on preference pairs from easy (best vs. worst) to hard (best vs. second-best). A trainable ranker (ResNet-50 + BERT) aligns top-k keyframes with summary content using attention-based scoring and diversity regularization.

## Key Results
- BOS+DPO injection raises Gemma2-9b ROUGE-1 from 40.72 (base) to 47.49 on Fin-APT dataset
- Trainable ranker outperforms cosine similarity on RMSE (0.41 vs 2.76), SSIM (0.44 vs 0.54), and F1 (0.32 vs 0.08)
- Expert evaluation shows 61.35% inter-agreement with higher summary quality ratings compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BOS feature injection provides richer context than transcripts alone, improving factual grounding in generated summaries.
- Mechanism: Optical flow identifies keyframes → OCR extracts on-screen text → BLIP-2 generates visual captions → Whisper transcribes with timestamps → Pyannote diarization attributes segments to speakers. These are concatenated into a unified prompt for the LLM, enabling the model to reference who said what, when, and what was shown.
- Core assumption: Visual and speaker cues are relevant to the summary; static podcast visuals provide sufficient signal despite limited dynamic content.
- Evidence anchors:
  - [abstract] "BLIP-2 for semantic visual descriptions, OCR for textual patterns, and Whisper-based transcription with Speaker diarization as BOS features."
  - [section 5.1, Table 4] T+V+S improves BERTScore to 88.40 vs. 87.80 (text-only); Table 5 shows speaker-specific summaries receive higher ratings.
  - [corpus] Neighboring work on multimodal video summarization (arXiv:2506.23714) supports that integrating text, audio, and visual cues improves summarization; however, no direct causal claim about BOS is externally validated.
- Break condition: If OCR consistently fails on low-quality frames or diarization confuses speakers, the prompt degrades and the LLM may hallucinate speaker attributions.

### Mechanism 2
- Claim: Curriculum-based Direct Preference Optimization (DPO) improves summary quality by progressively training on harder preference distinctions.
- Mechanism: The model generates four candidate summaries → GPT-4o ranks them → training starts with best vs. worst (easy) → iterates toward best vs. second-best (hard) → modified DPO loss aligns model with human preferences at each curriculum stage.
- Core assumption: GPT-4o rankings approximate human preferences, and curriculum ordering yields better optimization than random pair selection.
- Evidence anchors:
  - [abstract] "A modified Direct Preference Optimization (DPO)-based loss function, equipped with BOS-specific fact-checking, ensures precision, relevance, and factual consistency."
  - [section 4.1, Table 2] BOS+DPO injection raises Gemma2-9b ROUGE-1 from 40.72 (base) to 47.49; similar gains across baselines.
  - [corpus] No direct external validation of curriculum DPO in financial summarization; related RAG work (FinRAGBench-V, arXiv:2505.17471) focuses on retrieval, not preference optimization.
- Break condition: If ranking quality is noisy or inconsistent, the curriculum signal inverts and DPO optimizes toward flawed preferences.

### Mechanism 3
- Claim: A trainable ranker-based alignment mechanism selects keyframes that better match summary content than naive similarity methods.
- Mechanism: ResNet-50 extracts frame features → BERT encodes text summary → both projected to 512-dim space → attention scores relevance → top-k frames selected with diversity regularization (L_diversity) to avoid redundancy.
- Core assumption: Perceptual similarity and attention-based alignment correlate with human judgments of relevance; SSIM/RMSE/F1 capture meaningful quality differences.
- Evidence anchors:
  - [abstract] "A ranker-based retrieval mechanism further aligns keyframes with summarized content, enhancing interpretability and cross-modal coherence."
  - [section 5.1, Table 3] Trainable scorer outperforms cosine similarity on RMSE (0.41 vs 2.76), SSIM (0.44 vs 0.54), and F1 (0.32 vs 0.08).
  - [corpus] Multimodal summarization literature (arXiv:2506.23714, arXiv:2504.06275) supports attention-based frame-text alignment, but financial-domain validation remains limited.
- Break condition: If frames are near-identical (static backgrounds) or text lacks visual referents, the ranker overfits to spurious correlations.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Standard RLHF requires training a separate reward model; DPO directly optimizes policy from preference pairs, reducing complexity while aligning outputs with human judgments.
  - Quick check question: Given responses A (preferred) and B (rejected), does the model increase log-π(A|x) while decreasing log-π(B|x) under the DPO loss?

- Concept: Speaker Diarization
  - Why needed here: Financial podcasts often have multiple speakers; attributing advice to the correct source is critical for factual accuracy and accountability in summaries.
  - Quick check question: Can you identify which speaker advocated a specific investment strategy in a multi-turn transcript?

- Concept: Curriculum Learning
  - Why needed here: Directly training on subtle preference distinctions is unstable; starting with obvious differences (best vs. worst) stabilizes optimization before refining toward harder comparisons.
  - Quick check question: Does the model's accuracy on held-out preference pairs improve monotonically across curriculum stages?

## Architecture Onboarding

- Component map:
  - Video → GM-Flow (keyframe selection) → OCR + BLIP-2 (visual context) || Audio → Whisper + Pyannote (transcript + speaker diarization)
  - BOS prompt construction (concatenating captions, OCR text, speaker-attributed transcript)
  - Gemma2-9b fine-tuned with curriculum DPO
  - ResNet-50 (frames) + BERT (summary) → projection → attention scoring → top-k selection
  - Text summary + aligned keyframes

- Critical path:
  1. Keyframe extraction quality determines visual signal richness.
  2. Diarization accuracy governs speaker attribution correctness.
  3. Curriculum DPO training determines preference alignment.
  4. Ranker alignment affects multimodal coherence of final output.

- Design tradeoffs:
  - Latency vs. fidelity: BOS extraction (BLIP-2, OCR, diarization) adds preprocessing overhead but improves grounding.
  - Model size vs. scalability: Gemma2-9b is smaller than GPT-4o; BOS+DPO compensates but may not close all gaps.
  - Visual reliance vs. domain fit: Financial podcasts often have static visuals; the model leans more on text/audio than video.

- Failure signatures:
  - OCR failures on blurry frames → missing financial data (stock prices, charts) in prompts.
  - Diarization swaps → misattributed advice in summaries.
  - DPO overfitting to GPT-4o rankings → summaries that please the ranker but not human experts.
  - Ranker selecting near-duplicate frames → redundant multimodal output.

- First 3 experiments:
  1. Ablate speaker diarization (use Whisper only) and measure summary rating drop on multi-speaker videos to quantify diarization contribution.
  2. Replace curriculum DPO with standard DPO on the same preference pairs; compare ROUGE/BLEU/BERTScore to isolate curriculum effects.
  3. Evaluate ranker on held-out videos with high visual variability (e.g., charts, on-screen text) vs. static backgrounds; measure SSIM/F1 gap to assess where visual alignment matters most.

## Open Questions the Paper Calls Out
- None explicitly called out in the provided content.

## Limitations
- Static visual domain constraint: Financial advisory videos often have limited visual dynamism, raising questions about BOS+DPO feature transfer to more visually rich domains.
- GPT-4o ranking dependency: Human expert agreement was only 61.35%, suggesting GPT-4o may not perfectly align with human judgment for DPO optimization.
- Curriculum DPO novelty: No external validation of this specific implementation in financial summarization, making it difficult to assess whether gains are from curriculum ordering or other factors.

## Confidence
- High confidence: BOS feature injection improves grounding (supported by BERTScore improvements and speaker-specific summary ratings)
- Medium confidence: Curriculum DPO improves optimization (supported by ROUGE/BERTScore gains, but no ablation against standard DPO)
- Low confidence: Ranker-based alignment is superior (RMSE/SSIM/F1 improvements shown, but financial-domain visual variability is limited)

## Next Checks
1. Ablation study: Remove speaker diarization and measure summary quality degradation on multi-speaker videos to quantify diarization's contribution to factual accuracy.
2. Curriculum validation: Compare curriculum DPO against standard DPO on the same preference pairs to isolate the effect of iterative ordering on optimization outcomes.
3. Cross-domain testing: Evaluate BOS+DPO on visually dynamic video domains (e.g., news, tutorials) to assess generalizability beyond static financial content.