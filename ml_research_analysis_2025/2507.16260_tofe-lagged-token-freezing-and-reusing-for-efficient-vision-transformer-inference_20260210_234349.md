---
ver: rpa2
title: 'ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference'
arxiv_id: '2507.16260'
source_url: https://arxiv.org/abs/2507.16260
tags:
- token
- tokens
- uni00000010
- tofe
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ToFe, a novel token freezing and reusing framework
  for efficient vision transformer inference. ToFe addresses the limitation of existing
  token reduction methods that irreversibly discard unimportant tokens, potentially
  losing useful information for later blocks.
---

# ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference

## Quick Facts
- **arXiv ID:** 2507.16260
- **Source URL:** https://arxiv.org/abs/2507.16260
- **Reference count:** 40
- **Primary result:** Reduces LV-ViT model computational cost by 50% with <2% drop in Top-1 accuracy

## Executive Summary
This paper addresses the inefficiency of vision transformer inference by proposing a novel token freezing and reusing framework called ToFe. Unlike existing methods that irreversibly discard unimportant tokens, ToFe temporarily freezes less important tokens at each stage and selectively reuses them later when needed. The framework uses a prediction module to identify important tokens and a lightweight approximation module to recover frozen tokens, trained end-to-end with a computation budget-aware approach. Experimental results demonstrate that ToFe achieves a better trade-off between performance and complexity compared to state-of-the-art token reduction methods.

## Method Summary
ToFe introduces a three-stage token reduction framework for vision transformers that preserves token information through freezing rather than discarding. At each stage, a Token Selector (3-layer MLP + Gumbel-Softmax) identifies important tokens to keep and freezes the rest. Frozen tokens bypass standard transformer blocks but are later approximated using a lightweight Token Approximator (bottleneck MLP) before reintegration. The framework is trained with a computation budget-aware loss that includes classification loss, approximation loss, and FLOPs penalty. This approach allows tokens deemed unimportant in early layers to be recovered and reused when they become relevant in deeper layers, preventing irreversible information loss while maintaining efficiency.

## Key Results
- Reduces LV-ViT computational cost by 50% with less than 2% drop in Top-1 accuracy
- Outperforms state-of-the-art token reduction methods in the trade-off between performance and complexity
- Achieves learned keep ratios of 0.49, 0.25, and 0.14 across three stages, superior to fixed uniform ratios

## Why This Works (Mechanism)

### Mechanism 1: Lagged Token Reuse via Freeze-and-Thaw
The framework preserves "unimportant" tokens rather than discarding them, preventing irreversible information loss as token importance fluctuates across network depth. Tokens are frozen at stage s and re-evaluated at stage s+1, allowing previously frozen tokens to re-enter the active computation path when they become relevant. This addresses the limitation of existing methods where tokens reduced in early blocks might be useful later.

### Mechanism 2: Error Compensation via Lightweight Approximation
Frozen tokens accumulate drift because they skip layer-specific weight updates (MHSA/MLP). A lightweight Token Approximator (bottleneck MLP) predicts the residual updates that frozen tokens would have learned, compensating for distribution shift before merging with active tokens. The framework assumes frozen token transformations across consecutive blocks are simple enough to be modeled by a shallow MLP.

### Mechanism 3: Budget-Aware Gumbel-Softmax Selection
Hard thresholding (Top-K) is non-differentiable and requires manual tuning. ToFe uses Gumbel-Softmax to generate binary masks in a differentiable manner, adding a FLOPs-based penalty term to the loss function. This allows the model to learn optimal token counts dynamically for specific hardware budgets, optimizing the trade-off between accuracy and computational cost.

## Foundational Learning

- **Vision Transformer (ViT) Block Structure**: Understanding that freezing a token skips both MHSA and MLP calculations is crucial. Does freezing a token skip just the self-attention calculation or the MLP and LayerNorm as well?

- **[CLS] Token Attention as a Proxy**: Standard methods use [CLS] attention to judge token importance, but ToFe critiques this approach. In a standard ViT, if the [CLS] token attends 0% to a background patch, does that guarantee the patch is useless for final classification?

- **Differentiable Sampling (Gumbel-Softmax)**: The Token Selector must make discrete decisions (Keep=1, Freeze=0) while maintaining gradient flow. Why can't we use standard Softmax for selecting tokens without keeping a weighted sum of all tokens (which defeats the purpose of reduction)?

## Architecture Onboarding

- **Component map**: Input -> Token Selector -> Parallel Processing (Keep path + Freeze path) -> Token Approximator -> Rearrange -> Next Stage
- **Critical path**: 
  1. Input enters Stage s
  2. Selector generates Mask, splitting input into Keep and Freeze tokens
  3. Keep tokens process through standard ViT blocks; Freeze tokens process through Token Approximator
  4. Keep and approximated Freeze tokens are concatenated and rearranged to original spatial order
  5. Proceed to Stage s+1

- **Design tradeoffs**: 
  - Simple 3-layer MLP selector preferred over complex selectors to avoid overhead forcing lower keep-ratios
  - Bottleneck MLP approximator chosen over full Transformer block (which consumes 31% of GFLOPs)
  - Budget-aware loss enables learned token counts rather than manual tuning

- **Failure signatures**:
  - Accuracy collapse (>5% drop): Check if Approximator is disabled or too weak
  - Throughput stall: Check if batch size is too small or overhead dominates
  - Budget drift: Check loss weight Î»_FLOPs in Eq. 15

- **First 3 experiments**:
  1. Ablation of Approximation: Run ToFe with Approximator set to "Identity" vs. Bottleneck MLP on DeiT-S
  2. Budget Sensitivity: Train models with target FLOPs at 100%, 70%, and 50% of backbone
  3. Selector Architecture Test: Swap 3-layer MLP selector for linear or attention-based selector

## Open Questions the Paper Calls Out

- Adapting ToFe to various downstream tasks beyond image classification
- Combining the method with other efficient ViT architectures for further acceleration
- Investigating the limits of the lightweight MLP approximator for datasets where token features evolve non-linearly

## Limitations

- The approximation function complexity assumption (linear/approximable frozen token drift) is unproven
- Gumbel-Softmax temperature management details are not specified, which is critical for training stability
- Hardware-agnostic FLOPs budget may not translate directly to real-world throughput gains across different platforms
- Long-range dependency preservation is not analyzed, raising concerns about global context disruption

## Confidence

- **High Confidence**: Core mechanism of freezing-then-reusing tokens to prevent irreversible information loss is conceptually sound with strong empirical support
- **Medium Confidence**: Effectiveness of bottleneck MLP approximator is demonstrated but relies on unproven assumptions about approximation complexity
- **Low Confidence**: Claim of "better trade-off between performance and complexity" is relative to specific baselines tested and may not generalize

## Next Checks

1. **Deep Skip Validation**: Train ToFe with varying freeze depths (skip 2, 4, 6 consecutive blocks) while monitoring accuracy and approximation loss to test the core assumption about approximation complexity.

2. **Hardware-Agnostic Evaluation**: Implement ToFe on CPU and mobile platforms in addition to GPU, measuring actual latency and energy consumption rather than just GFLOPs to validate real-world efficiency gains.

3. **Attention Pattern Analysis**: Visualize and quantify attention maps of reused tokens before and after approximation to verify global context preservation and understand why approximation succeeds or fails for different token types.