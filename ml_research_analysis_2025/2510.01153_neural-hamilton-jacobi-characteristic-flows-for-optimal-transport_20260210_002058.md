---
ver: rpa2
title: Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport
arxiv_id: '2510.01153'
source_url: https://arxiv.org/abs/2510.01153
tags:
- transport
- loss
- optimal
- neural
- mnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network framework for solving optimal
  transport (OT) problems based on the Hamilton-Jacobi (HJ) equation. The method leverages
  the method of characteristics to derive closed-form, bidirectional transport maps
  without numerical integration, avoiding adversarial training and dual-network architectures.
---

# Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport

## Quick Facts
- arXiv ID: 2510.01153
- Source URL: https://arxiv.org/abs/2510.01153
- Authors: Yesom Park; Shu Liu; Mo Zhou; Stanley Osher
- Reference count: 40
- Key outcome: A neural network framework for optimal transport based on Hamilton-Jacobi equations that derives closed-form bidirectional transport maps without numerical integration or adversarial training.

## Executive Summary
This paper introduces a novel neural network framework for solving optimal transport problems by leveraging the Hamilton-Jacobi equation and the method of characteristics. The approach derives closed-form, bidirectional transport maps that eliminate the need for numerical integration, enabling efficient and accurate transport between distributions. A single neural network is trained using a combined loss function that enforces the HJ equation structure and aligns the transported source distribution with the target. The method supports general cost functions and class-conditional transport, with theoretical analyses establishing consistency and stability.

## Method Summary
The method learns an implicit neural representation $u_\theta(x,t)$ of the Hamilton-Jacobi solution, from which transport maps are derived using the method of characteristics. The forward map is $T_\mu^\nu(x) = x + t_f \nabla h(\nabla_x u_\theta(x,0))$ and the backward map is $T_\nu^\mu(y) = y - t_f \nabla h(\nabla_x u_\theta(y,t_f))$. Training uses a combined loss: an implicit HJ loss that enforces the PDE structure and an MMD loss that aligns the transported source distribution with the target. The approach avoids dual-network architectures and adversarial training, using a single MLP or ResNet trained with Adam optimizer. Experiments cover 2D synthetic data, high-dimensional Gaussians, color transfer, and MNIST class-conditional transport.

## Key Results
- The method derives closed-form bidirectional transport maps without numerical integration or adversarial training
- Superior accuracy and efficiency compared to baselines on synthetic, color transfer, and MNIST datasets
- High accuracy (low UVP) and low FID scores achieved across experiments
- Theoretical analyses establish consistency and stability for quadratic cost functions

## Why This Works (Mechanism)

### Mechanism 1: Closed-Form Transport via Straight-Line Characteristics
The method avoids iterative ODE solvers by identifying that optimal transport characteristics for suitable costs form straight lines $x(t) = t\nabla h(p) + x(0)$. This allows the transport map $T(x)$ to be computed analytically using the network's spatial gradient at $t=0$, rather than solving a differential equation over time. The core assumption is that the cost function $\ell$ must be strictly convex and sub-differentiable (e.g., quadratic cost), ensuring the characteristics do not intersect.

### Mechanism 2: Implicit HJ Regularization
A single network is sufficient to learn the transport dynamics by enforcing the HJ equation structure as a soft constraint. Instead of solving the HJ equation directly (which is ill-posed), the network $u_\theta(x,t)$ is trained to satisfy an implicit solution formula derived from characteristics. The loss $\mathcal{L}_{HJ}$ penalizes deviations from this PDE structure, steering the network toward the viscosity solution without requiring a convex architecture or adversarial game.

### Mechanism 3: Distributional Alignment via MMD
The HJ loss ensures optimality structure, while a separate distributional loss ensures the map connects the specific source $\mu$ and target $\nu$. The HJ equation describes valid transport dynamics but does not uniquely specify the destination without a correct initial condition. The method uses Maximum Mean Discrepancy (MMD) to explicitly minimize the distance between the transported samples $T_\#\mu$ and the target $\nu$, resolving the ambiguity of the initial function $g$.

## Foundational Learning

- **Concept: The Method of Characteristics**
  - Why needed: This is the core mathematical engine. You must understand how a PDE can be reduced to a system of ODEs along "characteristic" curves to understand why the transport map is closed-form.
  - Quick check: If the Hamiltonian is convex, why does the gradient $\nabla u$ remain constant along a characteristic line?

- **Concept: Viscosity Solutions**
  - Why needed: The HJ equation typically lacks classical smooth solutions. The paper relies on "viscosity solutions" to guarantee a unique, well-defined transport map even where gradients are discontinuous.
  - Quick check: Why is a "viscosity solution" necessary for HJ equations, and what happens to the transport map at points of non-differentiability?

- **Concept: Push-forward Operator ($T_\#\mu$)**
  - Why needed: This defines the transport problem itself ($T_\#\mu = \nu$). Understanding how a map $T$ transforms a distribution is essential for debugging the MMD loss and sampling.
  - Quick check: If $x \sim \mu$ and $y = T(x)$, what is the relationship between the density of $y$ and the Jacobian of $T$?

## Architecture Onboarding

- **Component map:** Network $u_\theta(x,t)$ -> Gradient computation $\nabla u_\theta$ -> Implicit HJ loss evaluation -> MMD loss evaluation -> Transport map computation

- **Critical path:**
  1. Sample $(x, t)$ from the spatio-temporal domain
  2. Compute $\nabla u_\theta$ via autodiff
  3. Evaluate the implicit HJ residual to regularize the potential shape
  4. Sample $x \sim \mu, y \sim \nu$, transport $x$ using the map, and compute MMD between transported $x$ and $y$

- **Design tradeoffs:**
  - Minimization vs. Min-Max: The paper uses a pure minimization loop (HJ+MMD). This is stable but relies on the HJ loss successfully constraining the solution space to the optimal map.
  - INR vs. Convex Architectures: Unlike dual formulations that require Input Convex Neural Networks (ICNNs), this method uses standard MLPs/ResNets. This is more flexible but relies on the loss function to enforce convexity implicitly.

- **Failure signatures:**
  - Trajectory Crossing: If the HJ loss is too weak, characteristic lines may intersect, implying a non-optimal or invalid transport.
  - Mode Collapse: If MMD is weak, the map may satisfy the HJ equation but fail to reach the target distribution.

- **First 3 experiments:**
  1. **2D Gaussians:** Verify that the learned map matches the analytical affine transformation $T^*(x) = Ax + b$. Plot characteristic lines to confirm they are straight and parallel.
  2. **Swiss Roll â†’ Moons:** Stress test the bidirectional capability. Ensure a single network can twist the roll into moons and invert the process without numerical drift.
  3. **Ablation on HJ Weight ($\lambda$):** Vary the weighting of the implicit loss. Observe if the transport map becomes "entangled" (non-optimal) as the HJ constraint is relaxed.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the stability analysis be extended from the restricted quadratic parameterization to general neural network architectures with provable convergence guarantees? The conclusion states extending stability analysis to general neural architectures would provide deeper theoretical understanding. Theorem 5.4 proves stability only for a simplified quadratic parameterization, and the proof relies on explicit algebraic structure that general architectures lack.

- **Open Question 2:** How can the method be adapted to perform optimal transport directly in pixel space rather than requiring latent space embeddings for high-dimensional image data? The conclusion states future directions include improving high-dimensional performance beyond latent-space implementations. The method currently requires VAE preprocessing for image data, and gradient evaluations become prohibitively expensive in high dimensions.

- **Open Question 3:** Does explicitly enforcing the monotonicity condition $D_x T^\mu_\nu[u] \succ O_d$ improve convergence speed or final transport accuracy in practice? Remark A.5 states it remains an interesting direction for future research to investigate efficient strategies for enforcing monotonicity and to assess the potential benefits. The monotonicity condition is theoretically linked to c-concavity but is not enforced during training.

## Limitations

- The method's theoretical guarantees rely on strictly convex and sub-differentiable cost functions, limiting applicability to non-convex costs or complex real-world scenarios.
- MMD kernel effectiveness in high-dimensional spaces remains a potential limitation, particularly for complex image datasets where simple kernels may fail to capture fine-grained distributional differences.
- Scalability claims for high-dimensional data and complex image datasets lack detailed analysis of computational complexity and robustness to hyperparameters.

## Confidence

- **High Confidence:** The mechanism for deriving closed-form transport maps via straight-line characteristics is mathematically sound and well-supported by experimental evidence.
- **Medium Confidence:** The implicit HJ regularization and MMD alignment are plausible based on the mathematical framework and ablation studies, but their effectiveness in more complex, high-dimensional scenarios requires further validation.
- **Low Confidence:** The scalability claims for high-dimensional data and complex image datasets are based on reported results but lack detailed analysis of computational complexity or robustness to hyperparameters.

## Next Checks

1. **Dimensionality Scaling Analysis:** Systematically evaluate the method's performance on high-dimensional Gaussian distributions (d=2-64) with varying levels of separation between means and covariance matrices. Measure both accuracy (UVP) and computational time to assess true scalability.

2. **Kernel Sensitivity Study:** Compare MMD performance using different kernels (Gaussian, Laplacian, negative distance) across the full range of experiments, particularly for MNIST and CycleGAN. Analyze how kernel bandwidth affects convergence and accuracy.

3. **Non-Convex Cost Extension:** Test the method with non-convex cost functions (e.g., exponential, power-law) on 2D toy problems where analytical solutions exist. Measure the impact on characteristic line behavior and overall transport accuracy.