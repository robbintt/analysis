---
ver: rpa2
title: 'Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO
  and Multi-Aspect Question Reformulation'
arxiv_id: '2601.20614'
source_url: https://arxiv.org/abs/2601.20614
tags:
- question
- dgpo
- arxiv
- questions
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathForge, a two-pronged framework for improving
  mathematical reasoning in large language models through reinforcement learning with
  verifiable rewards (RLVR). It identifies that existing methods neglect harder questions,
  which are critical for overcoming the model's solvable weaknesses.
---

# Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation

## Quick Facts
- arXiv ID: 2601.20614
- Source URL: https://arxiv.org/abs/2601.20614
- Reference count: 21
- Key outcome: MathForge achieves 42.17% average score on math benchmarks, outperforming GRPO's 37.61%

## Executive Summary
This paper introduces MathForge, a two-pronged framework for improving mathematical reasoning in large language models through reinforcement learning with verifiable rewards (RLVR). It identifies that existing methods neglect harder questions, which are critical for overcoming the model's solvable weaknesses. To address this, MathForge combines a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. DGPO rectifies an implicit imbalance in Group Relative Policy Optimization (GRPO) by normalizing update magnitudes using difficulty-balanced group advantage estimation and prioritizes harder questions via difficulty-aware weighting. MQR reformulates questions across multiple aspects—adding story background, introducing abstract terminology, and nesting sub-problems—while preserving the original answer to maintain mathematical logic. Together, they form a synergistic loop where MQR expands the data frontier and DGPO efficiently learns from it. Experiments on models like Qwen2.5-Math-7B and benchmarks like MATH500 show MathForge significantly outperforms existing methods, validating its effectiveness and generalizability.

## Method Summary
MathForge combines two key innovations: (1) Difficulty-Aware Group Policy Optimization (DGPO), which replaces GRPO's standard deviation normalization with mean absolute deviation (MAD) to ensure consistent update magnitudes across difficulty levels, and adds difficulty-aware question-level weighting to prioritize harder questions; and (2) Multi-Aspect Question Reformulation (MQR), which generates harder variants of existing questions through three strategies—adding story background, introducing abstract terminology, and nesting sub-problems—while preserving the original answer for automatic verification. The framework is implemented on top of Open-R1 and trained on Qwen2.5-Math-7B with 30k questions (7.5k original + 22.5k MQR variants), using G=8 responses per question, batch size 32, and learning rate 5e-7 (1e-6 with MQR).

## Key Results
- MathForge achieves 42.17% average accuracy across AIME24, AIME25, AMC23, MATH500, Minerva, and Olympiad benchmarks, compared to GRPO's 37.61%
- DGPO alone (without MQR) improves performance by 0.94% over GRPO by fixing the implicit imbalance in update magnitudes
- MQR increases dataset difficulty while maintaining mathematical equivalence (97-99% equivalence rate), with reformulated questions showing 72-77% accuracy vs 79.77% on original questions
- MathForge generalizes to smaller models (1.5B, 3B) and maintains performance gains, demonstrating scalability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard deviation with mean absolute deviation (MAD) in advantage normalization creates consistent update magnitudes across all question difficulty levels.
- **Mechanism:** In GRPO, the advantage estimate $\hat{A}_{GR,i} = \frac{r_i - \text{mean}(\{r_i\})}{\text{std}(\{r_i\})}$ leads to update magnitudes proportional to $2G\sqrt{p(1-p)}$ where $p$ is the accuracy rate—peaking at $p=0.5$ and suppressed for easy ($p \to 1$) or hard ($p \to 0$) questions. DGPO's $\hat{A}_{DG,i} = \frac{r_i - \text{mean}(\{r_i\})}{\text{MAD}(\{r_i\})}$ ensures total update magnitude equals exactly $G$ (constant) regardless of $p$.
- **Core assumption:** Consistent gradient magnitude across difficulties is beneficial; harder questions warrant equal or greater learning signal, not less.
- **Evidence anchors:**
  - [abstract] "GRPO suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions."
  - [section 3.1.1, Theorem 1 & 2] Mathematical proof that GRAE yields variable update magnitude dependent on $p$, while DGAE yields constant $G$.
  - [corpus] DARO (arXiv:2510.09001) also identifies difficulty imbalance in GRPO, proposing difficulty-aware reweighting—provides external validation that this is a recognized problem class.

### Mechanism 2
- **Claim:** Difficulty-aware question-level weighting (DQW) explicitly prioritizes harder questions within each batch, amplifying their contribution to policy updates.
- **Mechanism:** After DGAE normalizes advantages, DQW assigns weight $\lambda_s = \frac{B_v \cdot \exp(D_s/T)}{\sum \exp(D_s/T)}$ where $D_s = -\text{mean}(\{r_{si}\})$ (negative accuracy = difficulty). Temperature $T$ controls sharpness. Harder questions get higher $\lambda_s$, directly scaling their token-level losses.
- **Core assumption:** Difficulty, measured as inverse accuracy on sampled responses, correlates with learning value; hard but solvable questions expose model weaknesses.
- **Evidence anchors:**
  - [abstract] "prioritizes harder questions via difficulty-aware question-level weighting."
  - [section 3.1.2, Eq. 6] Explicit formula for $\lambda_s$ using softmax over difficulty scores.
  - [corpus] GRPO-LEAD (arXiv:2504.09696) proposes similar difficulty-aware reweighting, suggesting this is an emerging design pattern in GRPO variants.

### Mechanism 3
- **Claim:** Multi-Aspect Question Reformulation (MQR) increases training data difficulty while preserving answer integrity, creating a harder training frontier that transfers to test performance.
- **Mechanism:** Three reformulation strategies—(1) adding irrelevant story background, (2) introducing abstract terminology, (3) nesting sub-problems—generate variants that require: filtering noise, grasping abstractions, and multi-step cross-domain reasoning. Crucially, the gold answer is preserved, enabling automatic verification without re-deriving solutions.
- **Core assumption:** Harder reformulations maintain mathematical equivalence; the model learns transferable reasoning by struggling with complexity.
- **Evidence anchors:**
  - [abstract] "MQR reformulates questions across multiple aspects... while preserving the original gold answer."
  - [section 3.2] Describes three strategies and their rationale; Appendix H shows 97-99% equivalence rates on sampled questions.

## Foundational Learning

- **Concept: Group Relative Advantage Estimation (GRAE)**
  - **Why needed here:** Understanding GRPO's baseline mechanism is essential to see why DGPO's normalization change matters. GRAE compares each response's reward to the group mean/std, creating relative advantages without a value function.
  - **Quick check question:** For a question with 2 correct and 6 incorrect responses (G=8), what happens to total update magnitude under GRAE vs. DGAE?

- **Concept: Policy Gradient Variance Reduction**
  - **Why needed here:** DGPO's design is fundamentally about controlling gradient variance across difficulties. Understanding why standard deviation normalization introduces variance-dependent scaling helps explain the MAD fix.
  - **Quick check question:** Why might MAD be more stable than std when rewards are sparse binary (0/1) and group sizes are small?

- **Concept: Curriculum Learning / Difficulty-Weighted Loss**
  - **Why needed here:** DQW is a form of curriculum weighting—harder examples get higher loss weights. This connects to broader RL and supervised learning practices where example difficulty modulates learning signal.
  - **Quick check question:** What happens if temperature $T \to 0$ in DQW? What if $T \to \infty$?

## Architecture Onboarding

- **Component map:**
  - Input: Question batch $\{q_s\}$ from dataset $D$ (original or MQR-augmented)
  - Response Generation: Sample $G$ responses per question using old policy $\pi_{\theta_{old}}$
  - Reward Computation: Binary accuracy from rule-based verifier
  - DGAE: Normalize advantages using MAD: $\hat{A}_{DG,si} = \frac{r_{si} - \text{mean}}{\text{MAD}}$
  - DQW: Compute difficulty $D_s = -\text{mean}(\{r_{si}\})$, apply softmax weighting $\lambda_s$
  - Loss Aggregation: Token-level PPO-style clipped loss, weighted by $\lambda_s$, averaged over valid tokens
  - Policy Update: Backprop through $\theta$ with valid-token averaging

- **Critical path:**
  1. Ensure response sampling produces non-uniform rewards (mix of correct/incorrect) for valid queries
  2. Guard against MAD = 0 (all rewards identical) → skip or flag as invalid
  3. Verify MQR-generated questions preserve answers (spot-check with verifier)

- **Design tradeoffs:**
  - **Temperature $T$:** Lower $T$ focuses sharply on hardest questions (risk: overfitting to outliers); higher $T$ flattens weights (diminished difficulty prioritization). Paper finds $T=2.0$ optimal.
  - **Group size $G$:** Larger $G$ improves advantage estimation but increases compute. Paper uses $G=8$.
  - **MQR vs. original data:** MQR increases difficulty and diversity but requires upfront cost (~$184 for 22.5k questions per paper) and depends on reformulator quality.

- **Failure signatures:**
  - **All responses incorrect:** Query becomes invalid, no gradient contributed (expected behavior, not a bug)
  - **MAD near-zero:** Numerical instability in advantage normalization; may indicate degenerate sampling (all responses same reward)
  - **Answer corruption in MQR:** Reformulated question has different answer → all responses incorrect → invalid query → no learning (self-mitigating, but wastes compute)

- **First 3 experiments:**
  1. **Ablate DGAE alone:** Train with MAD normalization but no DQW ($\lambda_s = 1$) to isolate normalization effect. Expect improvement over GRPO baseline (paper shows +0.94% from DGAE alone).
  2. **Ablate DQW alone:** Keep GRAE but add question-level weighting. Expect marginal gain (paper shows DQW adds +1.14% on top of DGAE).
  3. **Test MQR difficulty:** Evaluate policy on MQR-augmented vs. original questions (no training) to confirm reformulations are harder (paper shows accuracy drops from 79.77% to 72-77% on reformulated subsets).

## Open Questions the Paper Calls Out

- **Can MathForge be applied to non-mathematical domains with verifiable rewards, such as code generation, or does the "Harder is Better" principle rely on the specific structure of mathematical language?**
  - Basis in paper: The paper exclusively evaluates on mathematical benchmarks (MATH, AIME, GeoQA) and cites RLVR's applicability to code, yet the MQR strategies (abstract terminology, sub-problem nesting) are tailored to math.
  - Why unresolved: It is unclear if MQR's "Background" or "Term" strategies translate effectively to coding syntax or logical constraints without breaking the verifiability of the code.
  - What evidence would resolve it: Application of MathForge to standard coding benchmarks (e.g., HumanEval, MBPP) using code-specific reformulation strategies.

- **Can the Multi-Aspect Question Reformulation (MQR) be implemented as a self-play mechanism where the policy model generates its own harder data, removing the reliance on stronger external teacher models like OpenAI o3?**
  - Basis in paper: The methodology relies on a capable external reformulator (OpenAI o3 or Qwen3-30B), whereas Related Work (Liang et al., 2025) is cited for self-play approaches.
  - Why unresolved: The paper demonstrates that smaller reformulators (Qwen2.5-7B) are less effective; achieving high-quality reformulation via the policy model itself without external supervision remains an open challenge.
  - What evidence would resolve it: Experiments showing convergence and performance stability when the policy model acts as the sole reformulator in a closed loop.

- **Does the performance of DGPO collapse if the MQR strategy increases difficulty beyond the model's capacity, resulting in uniformly incorrect responses (zero rewards) that are filtered out as invalid queries?**
  - Basis in paper: The paper defines "valid queries" as those with non-uniform rewards and explicitly targets "solvable weaknesses," implying a risk if questions become unsolvable.
  - Why unresolved: The DQW component upweights "harder" questions (lower accuracy), but if accuracy drops to 0%, the query is ignored; the precise tipping point between "optimally hard" and "too hard" is not mapped.
  - What evidence would resolve it: Analysis of training dynamics and gradient flow when applying MQR to a base model with significantly lower capacity relative to the dataset difficulty.

## Limitations
- MQR's difficulty escalation relies on external reformulators (OpenAI o3), creating dependency on stronger models and increasing computational cost
- The framework's generalizability to non-mathematical domains remains unproven despite RLVR's broader applicability
- Performance gains depend on maintaining 97-99% answer equivalence in reformulations; undetected answer changes could silently corrupt training

## Confidence
- **High confidence:** DGAE's variance reduction mechanism and its derivation from GRPO's implicit imbalance; DQW's formulation and its role in amplifying hard-question signals; core experimental results showing MathForge outperforming GRPO on MATH500 and Olympiad datasets
- **Medium confidence:** MQR's systematic difficulty escalation and its synergistic effect with DGPO; generalization to other models (1.5B, 3B) based on single runs; temperature sensitivity claims (T=2.0 optimal)
- **Low confidence:** Long-term robustness of DGAE under varying reward sparsity; scalability of MQR to non-mathematical domains; potential overfitting to MQR-augmented data given its expanded difficulty frontier

## Next Checks
1. **Ablation on MQR validity:** Sample 100 MQR-generated questions per strategy, run through verifier, and manually inspect 20 for answer preservation; quantify false equivalence rate and correlate with downstream performance degradation
2. **Temperature sweep robustness:** Retrain MathForge with T ∈ {0.5, 1.0, 2.0, 3.0, 5.0} on a subset of MATH; plot accuracy vs. T to identify stability bounds and confirm T=2.0 is near-optimal
3. **Cross-dataset generalization:** Apply MathForge to GSM8K or TabMWP; evaluate whether difficulty-aware weighting transfers to non-mathematical or tabular reasoning, testing the framework's domain generality