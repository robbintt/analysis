---
ver: rpa2
title: Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic
  Model Problems
arxiv_id: '2508.21022'
source_url: https://arxiv.org/abs/2508.21022
tags:
- gradient
- natural
- sketch-and-project
- which
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes subsampled natural gradient descent (SNG) using
  the lens of randomized linear algebra. The key insight is to view SNG as a sketch-and-project
  method rather than a stochastic preconditioning scheme.
---

# Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems

## Quick Facts
- **arXiv ID**: 2508.21022
- **Source URL**: https://arxiv.org/abs/2508.21022
- **Reference count**: 40
- **Primary result**: Shows subsampled natural gradient descent (SNG) can be analyzed as a sketch-and-project method, achieving linear convergence rates that exploit spectral decay in model Jacobians

## Executive Summary
This paper analyzes subsampled natural gradient descent (SNG) through the lens of randomized linear algebra, reframing SNG as a sketch-and-project method rather than a stochastic preconditioning scheme. The key theoretical contribution is showing that under squared volume sampling, SNG converges linearly with a rate determined by two quantities: the standard sketch-and-project convergence rate (α) and a new quantity (γ) related to the second moment of the sketch-and-project step. This framework provides a theoretical explanation for SNG's empirical success in exploiting spectral decay in model Jacobians, achieving superlinear convergence rate scaling with sample size that outperforms standard stochastic gradient descent.

## Method Summary
The method analyzes subsampled natural gradient descent (SNG) for linear least-quadratics problems where the model is parameterized by a Jacobian matrix J. The SNG iteration updates parameters using a preconditioned gradient computed from a subsampled subset of rows. The theoretical analysis employs squared volume sampling to establish exact expectation calculations, enabling rigorous convergence analysis. The paper also introduces SPRING, a momentum-enhanced variant of SNG, as an accelerated sketch-and-project method. The convergence analysis shows that SNG's rate scales with α/γ where α captures spectral structure exploitation and γ modulates the transition to the natural gradient setting.

## Key Results
- Reframes SNG as sketch-and-project method rather than stochastic preconditioning scheme
- Proves SNG converges linearly with rate scaling as α/γ under squared volume sampling
- Shows SNG can achieve superlinear convergence rate scaling with sample size k when Jacobian exhibits spectral decay
- Explains SPRING momentum scheme as accelerated sketch-and-project method
- Demonstrates theoretical advantage over SGD's linear scaling through spectral decay exploitation

## Why This Works (Mechanism)

### Mechanism 1: Sketch-and-Project Reframing
SNG can be rigorously analyzed as a sketch-and-project method rather than a stochastic preconditioning scheme. The SNG iteration is derived by applying a regularized block Kaczmarz step to the natural gradient subproblem. This reframing allows single-mini-batch analysis without decoupling gradient and preconditioner. Squared volume sampling enables exact expectation calculations showing the expected SNG direction equals a preconditioned gradient descent step.

### Mechanism 2: Convergence via α/γ Decomposition
Under squared volume sampling, SNG converges linearly with rate scaling as α/γ where α is the sketch-and-project rate and γ is a second-moment quantity. The α captures spectral structure exploitation while γ modulates transition to natural gradient setting. The analysis reveals that SNG can exploit spectral decay in the model Jacobian, providing theoretical explanation for empirical success.

### Mechanism 3: Superlinear Scaling via Spectral Decay
SNG exploits spectral decay in J to achieve superlinear convergence rate scaling with sample size k, unlike SGD's linear scaling. The sketch-and-project rate α can scale superlinearly in k when J has fast spectral decay. This provides distinct advantage relative to SGD, which enjoys at best linear scaling.

## Foundational Learning

- **Natural Gradient Descent and the Fisher Information Matrix**: Why needed here - SNG approximates deterministic NGD; understanding target clarifies what subsampling sacrifices. Quick check question: Why does NGD use J^T J (Fisher) as a preconditioner rather than the Hessian of the loss?

- **Sketch-and-Project / Randomized Block Kaczmarz Methods**: Why needed here - core theoretical contribution reframes SNG as sketch-and-project. Quick check question: In the convergence bound, what does α = λ_min^+(P) represent, and why does it control convergence?

- **Determinantal Point Processes and Squared Volume Sampling**: Why needed here - SVS is theoretical tool enabling exact expectation calculations. Quick check question: What property of SVS allows E_S[(J_S J_S^T + λI)^{-1}] to be computed in closed form?

## Architecture Onboarding

- **Component map**: θ ∈ R^n -> J ∈ R^{m×n} -> Sample S (|S|=k) -> Compute J_S, r_S -> Form K_S = J_S J_S^T + λI -> Solve K_S v = r_S -> d = J_S^T v -> θ_{t+1} = θ_t - η d

- **Critical path**: 1) Sample S and extract J_S, r_S; 2) Form kernel K_S = J_S J_S^T + λI; 3) Solve K_S v = r_S for v; 4) Compute direction d = J_S^T v; 5) Update parameters θ_{t+1} = θ_t - η d

- **Design tradeoffs**: Sample size k vs convergence (larger k gives faster convergence but higher per-iteration cost O(k²)); Regularization λ (higher λ stabilizes inversion but may slow asymptotic rate); Momentum μ (SPRING acceleration most beneficial when α is small)

- **Failure signatures**: Two-mini-batch proxy diverges from single-batch SNG behavior (indicates coupling between gradient and preconditioner is essential); γ >> 1 would suppress superlinear scaling; Inconsistent problems require tail averaging

- **First 3 experiments**: 1) Replicate Figure 2: Compare SVS-SNG proxy vs two-mini-batch proxy vs realistic uniform SNG on consistent LLQ; 2) Replicate Figure 3: Measure empirical scaling of α and γ with sample size k for J with quadratic spectral decay; 3) Replicate Figure 5: Compare SNG vs SPRING convergence across k values

## Open Questions the Paper Calls Out

### Open Question 1
Can Conjecture 6.2 regarding the convergence of SPRING for the linear least-quadratics (LLQ) problem be rigorously proven? The authors state, "We defer the resolution of this conjecture to future works," referring to proving the bound involving $\sqrt{\alpha/\beta}/\gamma$.

### Open Question 2
What are the precise theoretical bounds for the quantity $\gamma$ and its impact on the convergence rate? The authors provide a preliminary characterization but "defer a thorough analysis to future works."

### Open Question 3
Can the convergence guarantees be extended to relax the squared volume sampling (SVS) assumption? The conclusion lists "exploring how to relax our SVS assumption" as a promising theoretical direction.

### Open Question 4
Can acceleration schemes be designed to achieve convergence rates scaling with $\kappa^{-1/2}(H)$? The authors note that current methods do not improve dependence on the condition number and identify achieving $\kappa^{-1/2}(H)$ rates as "a promising direction for future works."

## Limitations
- Theoretical analysis relies critically on computationally prohibitive squared volume sampling rather than practical uniform sampling
- Superlinear scaling requires strong spectral decay and commutativity conditions that may not hold generally
- γ parameter bounds are loose and may not preserve superlinear scaling in practice

## Confidence
- **High confidence**: Sketch-and-project reframing of SNG and α/γ decomposition framework (mathematically rigorous with complete proofs)
- **Medium confidence**: Superlinear scaling claims (supported by Figure 4 but limited by absence of comprehensive theoretical bounds on γ)
- **Low confidence**: Practical implications of SVS-SNG theory for real-world uniform sampling implementations (empirical validation exists but theoretical guarantees don't directly transfer)

## Next Checks
1. **SVS vs Uniform Sampling Gap**: Systematically compare SVS-SNG and uniform-SNG convergence across diverse spectral decay patterns to quantify when SVS is a faithful proxy
2. **γ Parameter Sensitivity**: Measure empirical γ values across problem instances with varying condition numbers and spectral decay rates to validate whether Proposition 5.3 bounds are tight enough to preserve superlinear scaling
3. **SPRING Momentum Calibration**: Characterize the optimal μ-k relationship empirically across different spectral decay patterns to determine if Conjecture 6.2 holds generally or is restricted to specific problem classes