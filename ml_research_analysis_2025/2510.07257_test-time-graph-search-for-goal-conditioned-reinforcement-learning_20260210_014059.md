---
ver: rpa2
title: Test-Time Graph Search for Goal-Conditioned Reinforcement Learning
arxiv_id: '2510.07257'
source_url: https://arxiv.org/abs/2510.07257
tags:
- ttgs
- distance
- hiql
- planning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Test-Time Graph Search (TTGS) addresses the long-horizon planning
  challenge in offline goal-conditioned reinforcement learning by leveraging the geometric
  structure encoded in learned value functions. The method constructs a graph over
  dataset states using value-derived distances and performs Dijkstra search to generate
  subgoal sequences for a frozen policy at test time.
---

# Test-Time Graph Search for Goal-Conditioned Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.07257
- Source URL: https://arxiv.org/abs/2510.07257
- Authors: Evgenii Opryshko; Junwei Quan; Claas Voelcker; Yilun Du; Igor Gilitschenski
- Reference count: 17
- One-line primary result: TTGS improves success rates on OGBench long-horizon navigation tasks by decomposing distant goals into sequences of subgoals using graph search over value-derived distances.

## Executive Summary
Test-Time Graph Search (TTGS) addresses the long-horizon planning challenge in offline goal-conditioned reinforcement learning by leveraging the geometric structure encoded in learned value functions. The method constructs a graph over dataset states using value-derived distances and performs Dijkstra search to generate subgoal sequences for a frozen policy at test time. TTGS requires no retraining, additional supervision, or online interaction, making it a lightweight planning wrapper. On OGBench long-horizon locomotion tasks, TTGS consistently improves success rates across multiple base learners (HIQL, GCIQL, QRL), with notable gains on giant maze navigation and stitching tasks where one-shot execution fails.

## Method Summary
TTGS is a test-time planning wrapper that accepts any pretrained goal-conditioned policy and offline dataset. It samples M states from the dataset as graph vertices, computes pairwise distances using value-derived or domain-specific metrics, applies edge-length penalties to suppress unreliable shortcuts, and runs Dijkstra's algorithm to find shortest paths between start and goal states. During execution, the frozen policy follows adaptively selected subgoals within a step budget threshold, executing toward each subgoal sequentially rather than attempting one-shot long-range execution.

## Key Results
- On pointmaze-giant-stitch, TTGS improves HIQL from 0% to 84% success.
- Across OGBench tasks, TTGS consistently improves long-horizon navigation success rates.
- Edge-length penalty significantly improves performance by discouraging unreliable shortcuts.
- TTGS supports domain-specific distances beyond value-based planning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long-horizon tasks into sequences of short hops improves success rates when base policies are reliable only at short ranges.
- Mechanism: TTGS constructs a graph over sampled dataset states, computes shortest paths via Dijkstra's algorithm, and adaptively selects intermediate subgoals within a step-budget threshold T from the current state. The frozen policy executes toward each subgoal sequentially rather than attempting one-shot long-range execution.
- Core assumption: Goal-conditioned policies are reliable at short horizons but suffer from error accumulation and navigation failures when targeting distant goals directly.
- Evidence anchors:
  - [abstract]: "TTGS accepts any state-space distance or cost signal, builds a weighted graph over dataset states, and performs fast search to assemble a sequence of subgoals that a frozen policy executes."
  - [section 3]: "In long, complex tasks like maze navigation, agents that attempt to reach far-away goals can get stuck or run off course. However, for shorter horizons, they tend to be reliable."
  - [corpus]: Related work on temporal abstraction confirms hierarchical decomposition aids long-horizon GCRL tasks.
- Break condition: If the base policy cannot reliably reach subgoals within T steps, or if dataset coverage is insufficient to form connected paths between start and goal states.

### Mechanism 2
- Claim: Goal-conditioned value functions implicitly encode geometric reachability structure that can be repurposed as distance metrics for planning.
- Mechanism: Transform learned value V(s,g) into predicted step distance using closed-form mappings. For per-step penalty rewards (−1 until goal), use d̂(s,g) = log_γ(1 + (1−γ)V*(s,g)). This converts value estimates into approximate environment steps without handcrafted metrics.
- Core assumption: Value signals are monotone with respect to reachability and can be calibrated to approximate step counts; triangle inequality may not hold but predictions are reliable for nearby states.
- Evidence anchors:
  - [abstract]: "When the base learner is value-based, the distance is derived directly from the learned goal-conditioned value function, so no handcrafted metric is needed."
  - [section 3.1]: "Any value signal that is monotone with respect to reachability suffices."
  - [corpus]: Quasimetric learning approaches confirm value functions can encode temporal distances between state pairs.
- Break condition: If value estimates are excessively noisy, optimistic, or inconsistent, distance predictions may guide search toward infeasible or suboptimal paths.

### Mechanism 3
- Claim: Penalizing long edges in the graph suppresses unreliable shortcuts while preserving local metric structure.
- Mechanism: Apply a soft horizon threshold τ. Edges with predicted distance D_ij < τ retain original costs; edges with D_ij ≥ τ receive superlinear penalty p(x) = x · 1000^(x/τ). This steers shortest paths toward sequences of short, reliably traversable hops.
- Core assumption: Distance predictions are trustworthy locally but become unreliable at longer ranges; value-derived distances may be optimistic and connect states that are difficult or impossible to traverse.
- Evidence anchors:
  - [section 3.2]: "Using D directly for planning can produce paths that jump across large gaps... value-derived distances can be optimistic and may connect states that are hard or even impossible to traverse."
  - [section 4.2 ablation]: "Penalizing long edges consistently improves success by discouraging unreliable shortcuts while keeping local geometry intact."
  - [corpus]: Limited direct corpus evidence on this specific penalty formulation.
- Break condition: If τ is too small, paths become inefficiently dense with excessive subgoals; if too large, the policy cannot reliably traverse long hops, causing execution failures.

## Foundational Learning

- Concept: **Goal-Conditioned Reinforcement Learning (GCRL)**
  - Why needed here: The entire framework operates on pretrained goal-conditioned policies π(a|s,g) that must reach arbitrary goals from arbitrary starts. Understanding how these policies are trained (via value functions, hindsight relabeling) is prerequisite.
  - Quick check question: Can you explain how a goal-conditioned policy differs from a standard single-task policy, and why the goal specification decouples behavior from task rewards?

- Concept: **Value Functions and Temporal Distance**
  - Why needed here: TTGS extracts distance signals directly from V(s,g). You must understand what V(s,g) represents (expected discounted reward when navigating from s to g) and how reward conventions (sparse vs. per-step penalty) affect the value-to-distance transformation.
  - Quick check question: Given a goal-conditioned value function with per-step penalty −1, what does V(s,g) = −50 imply about the distance from s to g?

- Concept: **Graph Search and Shortest-Path Algorithms**
  - Why needed here: The planning core uses Dijkstra's algorithm over a weighted graph. Understanding time complexity (O(M²) distance computation, efficient GPU-based Dijkstra) informs scalability expectations.
  - Quick check question: Why might Dijkstra's algorithm produce paths with long edges if edge weights are unmodified predicted distances, and how does the edge-length penalty address this?

## Architecture Onboarding

- Component map: Offline Dataset D -> Vertex Sampling (uniform random, M states) -> Graph G = (V, E, w̃) -> Pretrained Value Function V(s,g) -> Distance Transformation f(V) -> Edge Weight Matrix D -> Penalty Application -> w̃ -> Test-Time Query (s₀, g) -> Nearest Vertex Lookup -> Dijkstra -> Guide Path (p₀,...,p_L) -> Execution Loop: Current state -> Subgoal Selection (within threshold T) -> Frozen Policy π(a|s, subgoal) -> Action

- Critical path:
  1. Sample M vertices from dataset (default M=4000)
  2. Compute pairwise distances using value-derived or domain-specific metric
  3. Apply edge-length penalty with threshold τ
  4. At episode start: locate nearest vertices to s₀ and g, run Dijkstra once
  5. During execution: select farthest reachable subgoal within T steps, invoke frozen policy

- Design tradeoffs:
  - Larger M: better graph coverage but O(M²) distance computation overhead (~45-100s GPU time)
  - Smaller τ: denser, more reliable subgoals but potentially longer, less direct routes
  - Smaller T: more conservative subgoal selection, reduced risk of failed hops but may cause micromanagement
  - Value-derived vs. domain-specific distance:前者 requires no handcrafting but depends on value quality;后者 may be more reliable when available

- Failure signatures:
  - Near-zero success with TTGS: base policy unreliable even at short range (<T steps), or value function provides severely noisy distance estimates
  - Suboptimal or circuitous paths: limited dataset coverage creates gaps, or distance predictions systematically misrank alternatives
  - No improvement over base learner: value function already captures planning structure (rare), or dataset lacks trajectory diversity
  - Performance degradation: τ or T misconfigured for environment scale

- First 3 experiments:
  1. Replicate `pointmaze-giant-stitch-v0` results with HIQL baseline: expect base ~0% -> TTGS ~80%+
  2. Ablate edge-length penalty: compare full TTGS vs. No-Penalty variant on `antmaze-giant-stitch-v0`
  3. Hyperparameter sensitivity sweep: vary τ ∈ {12, 24, 48} and T ∈ {24, 48, 96} on `humanoidmaze-giant-stitch-v0` to characterize robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining multiple noisy distance estimates produce more robust edge weights than single value-derived or domain-specific metrics?
- Basis in paper: [explicit] Section 6 states "Future work could explore combining multiple noisy distance estimates to obtain more robust edge weights."
- Why unresolved: The paper evaluates only single distance sources (value-derived or L2 position), and value functions can produce inconsistent estimates that degrade planning.
- What evidence would resolve it: Experiments showing that ensemble or learned combinations of distance signals improve success rates over single metrics, particularly on tasks where value-derived distances are noisy.

### Open Question 2
- Question: What principled methods for selecting representative states as graph vertices outperform uniform random sampling?
- Basis in paper: [explicit] Section 6 states "Future work could explore... developing more sophisticated strategies for selecting representative states as graph vertices."
- Why unresolved: The authors tested trajectory efficiency filtering and clustering but found no significant gain over random sampling; the question remains whether better vertex selection exists.
- What evidence would resolve it: A vertex selection method demonstrating statistically significant improvements over random sampling across multiple OGBench tasks, particularly for sparse or uneven datasets.

### Open Question 3
- Question: Why does TTGS substantially improve HIQL but often fail to improve GCIQL and QRL on complex tasks like humanoidmaze?
- Basis in paper: [inferred] Table 3 shows HIQL+TTGS achieving 78.1% on humanoidmaze-giant-stitch while GCIQL+TTGS remains near 0%, suggesting the method's effectiveness depends on base learner properties not fully analyzed.
- Why unresolved: The paper does not investigate why TTGS transfers well to some base learners and poorly to others despite using their respective value functions.
- What evidence would resolve it: Analysis correlating value function calibration quality or policy local-reachability with TTGS improvement magnitude across base learners.

## Limitations

- Dataset coverage is critical but not thoroughly analyzed—graph connectivity may fail if sampled states don't adequately represent traversable space.
- The soft horizon penalty parameter τ requires task-specific tuning without systematic guidance.
- Claims about computational efficiency (45-100s graph construction) lack comparison to alternative planning methods.

## Confidence

- **High Confidence**: TTGS improves success rates when base policies fail on long-horizon tasks (OGBench giant maze results). The mechanism of decomposing long paths into subgoals is sound when local execution is reliable.
- **Medium Confidence**: Value-derived distance transformations work as described, but their quality depends on value function accuracy and may vary across environments. The edge penalty effectively suppresses unreliable shortcuts, though optimal τ selection remains heuristic.
- **Low Confidence**: The benefit of TTGS for less challenging tasks (small mazes) is unclear.

## Next Checks

1. **Base Policy Reliability Test**: Systematically measure success rates of frozen policies executing directly toward distant goals across multiple step budgets to verify the short-horizon reliability assumption.
2. **Dataset Coverage Analysis**: Evaluate how graph connectivity and path quality vary with different sampling strategies (uniform, trajectory-based, density-weighted) and vertex counts M.
3. **Cross-Environment Generalization**: Test TTGS on non-maze environments (e.g., robotic manipulation, sparse-reward tasks) to assess whether value-derived distances generalize beyond navigation.