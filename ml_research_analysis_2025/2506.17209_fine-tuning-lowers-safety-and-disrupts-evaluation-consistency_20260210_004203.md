---
ver: rpa2
title: Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency
arxiv_id: '2506.17209'
source_url: https://arxiv.org/abs/2506.17209
tags:
- fine-tuning
- safety
- fine-tuned
- evaluation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning general-purpose large language models for specific
  tasks can significantly degrade safety guardrails, even when using benign datasets.
  This work investigates the robustness of safety evaluations to variations in experimental
  procedures, such as random seeds, fine-tuning epochs, and generation temperatures.
---

# Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency

## Quick Facts
- arXiv ID: 2506.17209
- Source URL: https://arxiv.org/abs/2506.17209
- Authors: Kathleen C. Fraser; Hillary Dawkins; Isar Nejadgholi; Svetlana Kiritchenko
- Reference count: 15
- Primary result: Fine-tuning general-purpose LLMs for specific tasks can significantly degrade safety guardrails, even when using benign datasets.

## Executive Summary
This study investigates how fine-tuning large language models for specific tasks impacts their safety guardrails and the consistency of safety evaluations. The authors demonstrate that fine-tuning on benign datasets can significantly degrade safety alignment, and that safety measurements are highly sensitive to experimental conditions including random seeds, fine-tuning epochs, and generation temperatures. Through experiments with multiple fine-tuning runs, they show substantial variance in harmfulness scores, indicating that seemingly inconsequential changes can lead to dramatically different safety outcomes. The work highlights the need for rigorous, reproducible safety evaluations and careful reporting of experimental conditions in future research.

## Method Summary
The study fine-tuned two open-source models (Llama-3.2-1B and Mistral-7B-v0.3) on two instruction datasets (Dolly and Alpaca, subsampled to 14,624 pairs each) using 4-bit quantized LoRA with batch size 64, learning rate 2e-5, and AdamW-8bit optimizer across 5 epochs with 5 different random seeds. Safety was evaluated using SORRY-Bench (440 prompts across 44 categories) with responses generated at temperature 0 and 0.7, then evaluated by an LLM evaluator (Mistral-7b-instruct-v0.2). The primary metric was harmfulness score (percentage of requests fulfilled), with secondary toxicity scoring via Perspective API.

## Key Results
- Fine-tuning on benign datasets can degrade safety guardrails by 10-15 percentage points in harmfulness scores
- Safety evaluation results vary substantially with random seeds, showing variance exceeding 10 percentage points
- Higher generation temperatures consistently increase harmful outputs for Llama-based models but show no clear effect for Mistral-based models
- Safety degradation appears driven by learning new content patterns rather than fine-tuning mechanics themselves

## Why This Works (Mechanism)

### Mechanism 1: Content-Driven Safety Degradation via Catastrophic Forgetting
Fine-tuning on new content disrupts safety guardrails primarily through catastrophic forgetting of safety-aligned representations. When LLMs learn new data patterns, weight updates shift representations away from safety-aligned configurations established during original RLHF/alignment training. The self-generated experiments isolate this: fine-tuning mechanics alone don't cause degradation—learning new content patterns does.

### Mechanism 2: Evaluation Variance from Compound Stochasticity
Safety measurements exhibit high variance due to compounding stochasticity from fine-tuning (random seeds) and generation (temperature). Each fine-tuning run produces different checkpoints even with identical hyperparameters due to random seed effects. At inference, non-zero temperature adds generation stochasticity. These sources compound, creating measurement uncertainty spanning 10+ percentage points in harmfulness scores.

### Mechanism 3: Temperature-Dependent Harmfulness Amplification (Model-Specific)
Higher generation temperatures increase harmful output rates for some model architectures (Llama) but not others (Mistral), suggesting safety alignment distributes differently across model families. Higher temperature increases probability of sampling from the long tail of the output distribution. If safety-aligned responses cluster around high-probability tokens while harmful responses occupy lower-probability regions, temperature increases amplify harmful outputs.

## Foundational Learning

- **Catastrophic Forgetting in Neural Networks**
  - Why needed here: The paper's central claim about safety degradation relates to models "forgetting" safety alignment when learning new content. Understanding how neural networks overwrite previous learning is essential.
  - Quick check question: Why might fine-tuning on math problems cause a model to forget safety behaviors, even though math and safety seem unrelated?

- **RLHF and Safety Alignment**
  - Why needed here: The paper assumes readers understand that base models undergo safety alignment training before release. Fine-tuning vulnerability exists because this alignment can be disrupted.
  - Quick check question: What is the difference between a base LLM and an instruction-tuned/safety-aligned LLM, and why might the latter be more useful?

- **Stochastic Decoding and Temperature**
  - Why needed here: Temperature significantly affects safety evaluation results. Engineers must understand how temperature scales output distributions and why higher temperature increases variance and potentially harmful outputs.
  - Quick check question: If you set temperature=0 versus temperature=0.7, what changes in the model's output distribution, and why might this matter for safety evaluation?

## Architecture Onboarding

- **Component map:**
Base Model (Llama-3.2-1B / Mistral-7B-v0.3, pre-aligned) -> Fine-Tuning Setup (LoRA + 4-bit quantization, Training Data, Hyperparameters, Random Seed) -> Checkpoint Saving (epochs 1-5) -> Safety Evaluation Pipeline (SORRY-Bench, Generation at temperature 0/0.7, LLM Evaluator)

- **Critical path:**
1. Run 5 fine-tuning variations with fixed seeds before drawing conclusions
2. Always report variance (min/max or std), not just mean harmfulness
3. Document all parameters: temperature, seed, quantization, exact dataset version

- **Design tradeoffs:**
- Computation vs. rigor: 5 runs × 5 epochs × 5 evaluations = 125 evaluations per config. Paper argues this cost is necessary.
- Temperature choice: T=0 is deterministic but unrealistic; T=0.7 reflects real usage but adds variance. Test both.
- Benchmark complexity: SORRY-Bench provides 44 categories but requires an LLM evaluator with 81% human agreement.

- **Failure signatures:**
- Single-run conclusions: Variance can exceed 10 percentage points
- Temperature mismatch: Comparing T=0 vs T=0.7 yields incomparable scores
- Ignoring toxicity: Models may refuse most requests but produce highly toxic outputs on the few they answer

- **First 3 experiments:**
1. Baseline variance quantification: Fine-tune target model on benign data (5 seeds, 5 epochs), evaluate at T=0 and T=0.7. Calculate min/max harmfulness to establish measurement uncertainty.
2. Self-generated content control: Generate answers using the base model, fine-tune on those self-generated answers. Compare degradation to human-written answers to isolate content-newness effects.
3. Temperature sweep: For highest-risk checkpoint, evaluate at T∈{0, 0.3, 0.7, 1.0} to determine if your model exhibits temperature-dependent harmfulness amplification.

## Open Questions the Paper Calls Out
- How do specific fine-tuning parameters, such as quantization level, learning rate, and system prompts, influence the severity of safety degradation?
- How does the domain and size of the fine-tuning dataset impact the consistency and magnitude of safety misalignment?
- What is the precise relationship between a fine-tuned model's task performance (usefulness) and its level of safety alignment?

## Limitations
- The study is limited to two model families (Llama and Mistral) and general-purpose instruction datasets, leaving generalization to other architectures and domains uncertain
- The paper does not investigate whether evaluation variance stems from genuine behavioral differences or measurement noise in the evaluation pipeline
- The claim that safety degradation is primarily driven by learning new content rather than fine-tuning mechanics relies on limited experimental evidence

## Confidence
- **High confidence**: Safety degradation from fine-tuning on benign datasets is well-supported; evaluation variance with seeds/temperature is robustly demonstrated
- **Medium confidence**: Safety degradation driven by content-newness is plausible but limited evidence; temperature effects are observed but lack theoretical explanation
- **Low confidence**: Fundamental unreliability of safety evaluations for comparing fine-tuning approaches requires more extensive validation

## Next Checks
1. Generalize findings across 3+ additional model families (Phi, Gemma, Qwen) and parameter scales (1B, 7B, 13B) to test universality of content-driven degradation
2. Design experiments to definitively distinguish catastrophic forgetting versus interference from new content patterns using similar vs. dissimilar content
3. Conduct ablation studies on the safety evaluation pipeline by varying the LLM evaluator, prompt templates, and procedures to quantify measurement noise versus genuine behavioral variance