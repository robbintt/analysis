---
ver: rpa2
title: Policy Testing in Markov Decision Processes
arxiv_id: '2505.15342'
source_url: https://arxiv.org/abs/2505.15342
tags:
- policy
- lemma
- proof
- problem
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the policy testing problem in discounted Markov
  decision processes under a fixed-confidence setting. The goal is to determine whether
  the value of a given policy exceeds a specified threshold while minimizing the number
  of observations.
---

# Policy Testing in Markov Decision Processes

## Quick Facts
- arXiv ID: 2505.15342
- Source URL: https://arxiv.org/abs/2505.15342
- Reference count: 40
- Primary result: Algorithm PTST achieves instance-specific sample complexity lower bound for policy testing in discounted MDPs

## Executive Summary
This paper addresses the policy testing problem in discounted Markov decision processes under a fixed-confidence setting. The goal is to determine whether a given policy's value exceeds a specified threshold while minimizing the number of observations. The authors develop PTST, an algorithm that matches the instance-specific lower bound on sample complexity by reformulating the problem as policy optimization in a reversed MDP. This approach allows leveraging recent advances in policy gradient methods while maintaining statistical optimality.

## Method Summary
The method involves a static sampling rule that tracks allocation by selecting state-action pairs minimizing N_sa(t-1)/ω_sa. The stopping rule uses projected policy gradient on a reversed MDP where states are (s,a) pairs, actions are next-states, and the original transition kernel becomes the optimization variable. The algorithm computes u_ζ_t by running M iterations of projected gradient descent with step size 1/L, projecting onto the KL-constrained set Π^p_σ. The decision rule returns sign(V^π_̂p_τ(ρ)) when u_ζ_t - ζ_t ≥ 0.

## Key Results
- PTST achieves instance-specific sample complexity lower bound, making it statistically optimal
- The algorithm remains computationally tractable through the reversed MDP reformulation
- Numerical experiments show PTST outperforms existing methods in sample complexity across different MDP settings
- The approach works for both small and larger MDPs (up to 5-state, 5-action examples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-specific sample complexity lower bound characterized by non-convex optimization
- Mechanism: The set of confusing transition kernels Alt(p) forms a non-convex set, creating the fundamental barrier to instance-optimal algorithms. Change-of-measure arguments yield T*_ω(p)^-1 := inf_{q∈Alt(p)} ∑_{s,a} ω_{sa} KL_{sa}(p, q).
- Core assumption: Assumptions 1 and 2 ensure the confusing sets are nonempty and all π-supported state-action pairs are sampled.
- Evidence anchors: Abstract states lower bound involves non-convex constraints; Section 4 proves non-convexity with explicit example.

### Mechanism 2
- Claim: Interchanging objective and constraints yields tractable dual formulation
- Mechanism: The paper shows σ_NC(u, ω, p) and u_NO(σ, ω, p) are inverse mappings, transforming primal (non-convex constraint, convex objective) into dual (convex constraint, non-convex objective). This admits interpretation as policy optimization in a reversed MDP.
- Core assumption: Assumption 3 conditions hold for the policy testing objective, verified via simulation lemma and reversed-MDP structure.
- Evidence anchors: Abstract mentions interchanging roles of objective and constraints; Section 6.1-6.2 establishes bijection via Proposition 2.

### Mechanism 3
- Claim: Projected policy gradient on reversed MDP converges to global optimum
- Mechanism: The convex set Π^p_σ allows projected gradient descent with O(1/k) convergence. The value function satisfies gradient-mapping domination with closed-form gradient via policy gradient theorem adapted to reversed MDPs.
- Core assumption: L-smoothness of value function and gradient-mapping dominance with specific ω.
- Evidence anchors: Section 6.3 provides finite-time bound and pseudo-code; Section D gives full convergence proof with gradient-mapping domination.

## Foundational Learning

- **Change-of-measure arguments in bandits/MDPs**: Why needed - Theorem 1's lower bound derives from KL divergence between distributions under true vs. confusing kernels. Quick check - Can you derive KL divergence between T-length trajectories under p vs. q for fixed policy π?
- **Policy gradient theorem and smoothness**: Why needed - Computing ∇_p V^π_p(ρ) where p is both optimization variable and in value function. Quick check - How does ∇_π V^π(s) relate to state-action visitation distribution d^π?
- **Convex conjugates and duality in optimization**: Why needed - Proposition 2's inverse mapping requires understanding when min-max duality holds for non-convex problems. Quick check - For min_x f(x) s.t. g(x) ≤ 0, what conditions ensure primal-dual equality?

## Architecture Onboarding

- Component map: Static Sampler -> Empirical Estimator -> Stopping Rule Evaluator -> Projected Policy Gradient -> Decision Rule
- Critical path: Stopping condition u_ζ_t - ζ_t ≥ 0 checked every round, requiring optimization to accuracy ζ_t. Tighter accuracy demands more policy gradient iterations per round.
- Design tradeoffs: Smaller ζ_t → better statistical efficiency but more compute per round; static vs. adaptive sampling; generative model assumption enables direct sampling.
- Failure signatures: Assumption 2 violation causes infinite lower bound; inaccurate projection causes premature stopping; numerical instability when V^π_p(ρ) ≈ 0.
- First 3 experiments: 1) Validate lower bound tightness on 2-state example; 2) Ablate ζ_t sequence on sample complexity; 3) Scalability test on |S|=|A|=5 MDP measuring wall-clock time vs. M.

## Open Questions the Paper Calls Out

- Can the PTST framework extend to adaptive sampling rules while maintaining instance-specific optimality? The current analysis relies on fixed allocation ω, and adaptive sampling introduces time-varying allocations that complicate lower bound optimization.
- Can the reversed MDP reformulation achieve instance-optimal best policy identification? The inverse relationship requires specific conditions that may not hold for best policy identification's more complex constraint structure.
- What are precise conditions for the reversed MDP reformulation preserving bijection for general pure exploration tasks? The paper proves conditions for policy testing and sketches policy evaluation, but general characterization is missing.

## Limitations

- The approach requires a generative model, limiting applicability when direct state-action sampling isn't available
- Computational complexity scales with the number of iterations M needed for gradient approximation accuracy
- The method's effectiveness depends on the distance from the decision threshold, with performance degrading near zero

## Confidence

- **Statistical optimality**: Medium confidence - While asymptotic optimality is proven, practical constants and their dependence on problem parameters aren't characterized
- **Computational tractability**: Low confidence - KL-constrained projections may become expensive for large state-action spaces; per-iteration complexity isn't characterized
- **Assumption validity**: Medium confidence - Reversed MDP structure ensures conditions hold, but smoothness constant L may be conservative affecting iteration complexity

## Next Checks

1. Implement the 2-state MDP from Section 4 and empirically verify that E_p[τ]/log(1/δ) converges to T*_ω(p)^-1 as δ → 0, confirming asymptotic optimality

2. Measure wall-clock time per PTST iteration for increasing |S| and |A| (5×5, 10×10, 20×20 MDPs) to characterize practical computational burden of KL-constrained projections

3. Run experiments with varying |V^π_p(ρ)| values (0.1, 0.01, 0.001) to quantify how sample complexity scales with distance from decision threshold, verifying (1-γ)^5|V^π_p(ρ)| dependence