---
ver: rpa2
title: 'Gender Bias in Large Language Models for Healthcare: Assignment Consistency
  and Clinical Implications'
arxiv_id: '2510.08614'
source_url: https://arxiv.org/abs/2510.08614
tags:
- gender
- patient
- clinical
- across
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated gender bias in large language models (LLMs)
  used in healthcare by assigning gendered personas (female, male, or unspecified)
  to multiple open-source and proprietary models. Using clinical case studies from
  the NEJM Image Challenge, the models were evaluated for consistency in diagnostic
  accuracy and their assessments of the clinical relevance or necessity of patient
  gender.
---

# Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications

## Quick Facts
- arXiv ID: 2510.08614
- Source URL: https://arxiv.org/abs/2510.08614
- Reference count: 0
- Key outcome: Models show high diagnostic consistency across gender assignments but substantial inconsistency in judging patient gender's clinical relevance, with open-source models exhibiting greater disparity.

## Executive Summary
This study reveals a critical gap in LLM evaluation for healthcare: while diagnostic accuracy remains stable when models are assigned different gendered personas, their judgments about whether patient gender is clinically relevant or necessary show substantial inconsistency. Using NEJM Image Challenge cases, the research demonstrates that minor prompt manipulations—changing only the assigned gender identity—can significantly alter models' assessments of gender relevance without affecting core diagnostic reasoning. This "token bias" effect is more pronounced in smaller open-source models than in larger proprietary ones, raising concerns about reliability and fairness in clinical applications.

## Method Summary
The study assigned gendered personas (female, male, unspecified) to six LLMs (Gemma-2-2B, Phi-4-mini, LLaMA-3.1-8B, OpenAI o3-mini, GPT-4.1, Gemini 2.5 Pro) and evaluated their performance on 117 NEJM Image Challenge cases. Each case was processed three times per model with different gender assignments, generating diagnostic outputs plus assessments of patient gender's relevance and necessity. Consistency rates (identical outputs across gender conditions) were calculated alongside accuracy metrics, with statistical significance assessed using Cochran's Q test and 95% confidence intervals via bootstrapping.

## Key Results
- Diagnostic consistency across gender assignments exceeded 90% for all models, with most achieving 95-100% stability
- Relevance judgment consistency ranged from 58.97% (LLaMA-3.1-8B) to 90.23% (GPT-4.1), showing substantial variability
- All models exhibited logical violations where necessity judgments exceeded relevance judgments
- Open-source models demonstrated greater gender-assignment effects than proprietary models

## Why This Works (Mechanism)

### Mechanism 1: Token Bias in Persona-Based Reasoning
- Claim: LLMs may rely on superficial identity cues (assigned gender) rather than clinical evidence when making subjective judgments about patient factors.
- Mechanism: When a gendered persona is assigned via prompt, the model's attention mechanisms may weight gender-related associations from training data more heavily, affecting downstream judgments without altering core diagnostic reasoning.
- Core assumption: The gender token activates gender-correlated patterns in the model's representations that influence relevance/necessity assessments independently of clinical content.
- Evidence anchors: All models demonstrated substantial inconsistency in judging relevance of patient gender; related work on token bias in LLM reasoning supports this pattern.

### Mechanism 2: Systemic Vulnerability to Identity-Assignment Manipulation
- Claim: Minor prompt modifications (changing only the gender descriptor in the persona) can induce measurable behavioral changes in clinical reasoning outputs.
- Mechanism: Instruction-tuned models learn associations between identity markers and behavioral patterns; when deployed with persona prompts, these associations become active and influence judgment tasks that have no objective ground truth.
- Core assumption: The model's training corpus contains gender-correlated reasoning patterns that are activated by persona framing but remain dormant in neutral prompts.
- Evidence anchors: LLaMA-3.1-8B systematically showed higher relevance and necessity rates when assigned male persona vs female persona (p < 0.001); similar vulnerability found with voice profiles in audio LLMs.

### Mechanism 3: Model Scale and Alignment Moderate Identity Effects
- Claim: Larger proprietary models with stronger safety alignment exhibit smaller persona-induced inconsistencies than smaller open-source models.
- Mechanism: Alignment training (RLHF or equivalent) may suppress identity-correlated output variations by enforcing consistency constraints; larger parameter counts provide more capacity to separate persona styling from substantive reasoning.
- Core assumption: Alignment procedures inadvertently or intentionally reduce sensitivity to identity framing in judgment tasks.
- Evidence anchors: GPT-4.1 achieved >90% consistency on relevance judgments; LLaMA-3.1-8B achieved only 58.97%; open-source models exhibited greater effects of LLM gender than proprietary models.

## Foundational Learning

- **Concept: Persona Assignment Consistency**
  - Why needed here: The study's central metric—measuring whether outputs remain stable when only the assigned identity changes—is not standard in LLM evaluation. Understanding this requires distinguishing consistency (stability across conditions) from accuracy (correctness against ground truth).
  - Quick check question: Can a model be 95% consistent but only 50% accurate? (Yes—Gemma-2B in this study demonstrates this.)

- **Concept: Relevance vs. Necessity Judgments**
  - Why needed here: The study separates these two assessment types; necessity (is gender required?) should logically imply relevance (is gender related?), but models violate this constraint.
  - Quick check question: If a model judges patient gender "necessary" but "not relevant," what does this indicate about its reasoning coherence?

- **Concept: Identity-Conditioned Evaluation**
  - Why needed here: Standard bias evaluations test how models respond to different patient demographics; this study flips the frame to test how models behave when assigned different identities themselves.
  - Quick check question: What is the difference between testing patient gender bias vs. testing assigned-model gender effects?

## Architecture Onboarding

- **Component map:** Persona Injection Layer -> Clinical Task Router -> Consistency Auditor -> Violation Detector
- **Critical path:** 1. Define persona variants (minimum: female, male, unspecified) 2. Run identical clinical cases through all persona conditions 3. Compare diagnostic outputs (accuracy) and judgment outputs (consistency) 4. Flag consistency failures and logical violations for review
- **Design tradeoffs:** More persona variants → better coverage but higher compute cost; Open-source models → full transparency but higher observed inconsistency; Proprietary APIs → better consistency but less control and auditability
- **Failure signatures:** Diagnostic consistency >90% but relevance consistency <70% → persona-driven judgment instability; Systematic directional bias (e.g., male persona always rates relevance higher) → training data correlation leakage; Necessity rate > relevance rate → reasoning incoherence
- **First 3 experiments:** 1. Establish baseline: Run 20 cases through your target model with all three persona conditions; compute consistency rates for diagnosis, relevance, and necessity 2. Stress test by specialty: Identify high-disparity specialties from Figure 4 (e.g., cardiology, infectious disease) and run focused consistency checks 3. Violation audit: Count cases where necessity > relevance; if >5%, the model's reasoning coherence is compromised

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can debiasing strategies such as counterfactual prompting effectively mitigate identity-assignment inconsistencies in clinical LLMs without degrading diagnostic accuracy?
- Basis in paper: Authors state: "In future work, we will... explore debiasing strategies such as counterfactual prompting to mitigate identity-assignment inconsistencies in LLM outputs."
- Why unresolved: The study identifies the inconsistency problem but does not test any mitigation approaches.
- What evidence would resolve it: A controlled intervention study comparing consistency and accuracy metrics before and after applying counterfactual prompting or similar debiasing techniques.

### Open Question 2
- Question: What is the clinically appropriate benchmark for when patient gender should be considered relevant or necessary for specific diagnoses, and how do LLM judgments compare against expert clinician standards?
- Basis in paper: Authors acknowledge: "We did not assess the clinical appropriateness of including or excluding patients' gender for each specific diagnosis" and plan to "integrate expert annotations to benchmark clinical justification."
- Why unresolved: The study measured consistency across gender assignments, not whether the relevance/necessity judgments themselves were medically correct.
- What evidence would resolve it: Expert physician annotations of gender relevance for each case, enabling comparison of LLM judgment accuracy against clinical ground truth.

### Open Question 3
- Question: Does the LLM gender-assignment effect persist or diminish in multimodal clinical scenarios that incorporate imaging, laboratory data, and longitudinal patient information?
- Basis in paper: From the limitation that "we used only static, text-only data and did not incorporate longitudinal patient data or multimodal inputs."
- Why unresolved: Real clinical workflows involve multiple data modalities; it is unknown whether richer clinical context reduces sensitivity to gender framing.
- What evidence would resolve it: Replication of the experimental design using multimodal cases (e.g., full NEJM cases with original images, lab panels, and follow-up data) with the same consistency metrics.

## Limitations

- The study cannot distinguish whether observed inconsistencies reflect true bias versus legitimate variation in reasoning approaches across different persona framings
- NEJM Image Challenge corpus represents a narrow slice of medical reasoning and may not capture broader healthcare scenarios
- The persona assignment mechanism relies on simple gendered prefixes without testing alternative framing strategies

## Confidence

**High Confidence:** The finding that diagnostic accuracy remains stable across gender assignments while relevance/necessity judgments show substantial inconsistency. This pattern is robustly demonstrated across all tested models with clear statistical significance.

**Medium Confidence:** The claim that larger proprietary models show systematically better consistency than smaller open-source models. While the trend is clear, the mechanistic explanation (alignment training effects) remains inferential without direct evidence.

**Low Confidence:** The interpretation that observed effects constitute "bias" rather than legitimate variation in reasoning. The study frames inconsistency as problematic, but alternative explanations are not adequately explored.

## Next Checks

1. **Cross-task consistency audit:** Test whether models showing high inconsistency in gender relevance judgments also exhibit similar instability in other subjective clinical judgments (e.g., treatment urgency, prognosis estimates) when assigned different personas.

2. **Persona framing sensitivity:** Systematically vary the gender assignment prompt structure (implicit cues vs explicit prefixes) to determine whether the observed effects persist across different persona injection methods.

3. **Clinical specialty stress test:** Conduct deeper analysis on the 5 specialties showing highest inconsistency (dermatology, infectious disease, oncology, cardiology, hematology) using larger case samples to confirm whether specialty-specific patterns hold.