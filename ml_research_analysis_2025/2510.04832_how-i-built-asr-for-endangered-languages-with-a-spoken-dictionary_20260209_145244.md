---
ver: rpa2
title: How I Built ASR for Endangered Languages with a Spoken Dictionary
arxiv_id: '2510.04832'
source_url: https://arxiv.org/abs/2510.04832
tags:
- speech
- languages
- data
- manx
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that spoken dictionaries and non-utterance-level
  data can enable ASR for endangered languages without requiring curated corpora.
  It demonstrates that 40 minutes of short-form multi-speaker speech can achieve <50%
  WER for Manx, and 8 minutes can bootstrap segmentation of long-form recordings for
  further refinement.
---

# How I Built ASR for Endangered Languages with a Spoken Dictionary

## Quick Facts
- **arXiv ID:** 2510.04832
- **Source URL:** https://arxiv.org/abs/2510.04832
- **Reference count:** 0
- **Primary result:** Demonstrated ASR for Manx with <50% WER using 40 minutes of multi-speaker short-form data and forced alignment

## Executive Summary
This work demonstrates that spoken dictionaries and non-utterance-level data can enable ASR for endangered languages without requiring curated corpora. The approach shows that 40 minutes of short-form multi-speaker speech can achieve <50% WER for Manx, and 8 minutes can bootstrap segmentation of long-form recordings for further refinement. The study reveals that HMM models with external language models benefit most from text data, while E2E models like Whisper generalize better to conversational speech. Results indicate the barrier to building ASR for low-resource languages is far lower than previously assumed, offering a practical path for communities with limited resources.

## Method Summary
The methodology involves collecting short-form spoken dictionary data (isolated words/phrases ≤2s) from multiple speakers, then using forced alignment to segment long-form audio recordings. A monophone GMM-HMM trained on short-form data bootstraps Viterbi forced alignment of unsegmented long-form audio, producing utterance-level segments. These segments are then used to train domain-specific ASR models. The approach supports both hybrid HMM systems (with external language models) and end-to-end models (Whisper, Wav2Vec), with the choice depending on available text resources and target domain characteristics.

## Key Results
- 40 minutes of multi-speaker short-form data produces <50% WER for Manx ASR
- 8 minutes of short-form data can bootstrap segmentation of long-form recordings
- HMM models with external LMs show ~20 absolute WER point gains, while E2E models generalize better to conversational speech
- Cornish experiment achieved 7.72% WER on in-domain read speech with minimal data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Short-form spoken dictionary data (≤2s clips) can substitute for utterance-level corpora when multi-speaker diversity is present.
- **Mechanism:** Isolated word/phrase recordings provide acoustic-phonetic coverage without requiring pre-segmented utterances. Multi-speaker data captures pronunciation variation that enables generalization beyond training speakers.
- **Core assumption:** Short-form transcripts are verbatim; non-verbatim clips degrade alignment quality.
- **Evidence anchors:**
  - [abstract] "40 minutes of such data produces usable ASR for Manx (<50% WER)"
  - [section 4.1] "Source diversity in speakers, styles and domains is the most determining factor; the single-speaker utterance-level systems were outperformed... by the LearnManx spoken dictionary systems"
  - [corpus] Related work on low-resource ASR (FormosanBench, Multimodal ICL) focuses on LLM adaptation rather than data format substitution—suggesting this mechanism addresses an underexplored bottleneck.
- **Break condition:** If speaker diversity is too low (e.g., Forvo with 1 dominant speaker), performance degrades severely (97%+ WER in 16-min condition).

### Mechanism 2
- **Claim:** A weak ASR trained on minimal short-form data can bootstrap segmentation of long-form recordings through forced alignment.
- **Mechanism:** Monophone GMM-HMM trained on short-form audio learns enough acoustic modeling to perform Viterbi forced alignment on unsegmented long audio, producing utterance-level segments for subsequent training.
- **Core assumption:** Long-form recordings have verbatim transcripts (or partial matches) that can be aligned; alignment success rate of ~70-80% indicates transcript quality matters.
- **Evidence anchors:**
  - [abstract] "8 minutes can bootstrap segmentation of long-form recordings for further refinement"
  - [section 3.4] "we train a monophone GMM-HMM on our short-form speech (Manx 102 mins, Cornish 8 mins) which is then used to bootstrap Viterbi forced alignment of the long-form audio. We had an alignment success rate between 70-80%"
  - [corpus] Weak or missing—neighbor papers do not address bootstrap alignment from spoken dictionaries.
- **Break condition:** Alignment success below ~70% suggests transcript-audio mismatches requiring manual review.

### Mechanism 3
- **Claim:** HMM systems benefit more from external text-based language models than E2E models, but E2E models generalize better to out-of-domain conversational speech.
- **Mechanism:** HMM architecture separates acoustic and language modeling, allowing external LMs trained on web text to constrain decoding. E2E models internalize LM knowledge during pretraining, making external LM integration (via n-best rescoring) less impactful but retaining robustness to domain shift.
- **Core assumption:** Substantial text data exists for the language (4.8M Manx words used); for text-scarce languages, HMM advantage diminishes.
- **Evidence anchors:**
  - [abstract] "HMM models with external language models benefit most from text data, while E2E models like Whisper generalize better to conversational speech"
  - [section 4.2] "LM contributes most to the HMMs, typically ~20 absolute WER points... whereas it resulted in 2 point average gain for wav2vec 2.0 and none for Whisper"
  - [section 4.2] "Whisper finishes best on careful (14.66% WER) and conversational (31.30% WER)"
  - [corpus] Consistent with prior work on hybrid vs. E2E trade-offs, though low-resource specific evidence remains sparse.
- **Break condition:** For text-scarce languages (Cornish experiment used no external LM), rely on E2E fine-tuning instead.

## Foundational Learning

- **Concept: Forced Alignment**
  - **Why needed here:** Core technique for converting long-form audio + transcripts into utterance-level segments using a (possibly weak) acoustic model.
  - **Quick check question:** Given audio of a sentence and its transcript, can you explain how Viterbi decoding finds word boundaries?

- **Concept: Language Model Integration (Decoding vs. Rescoring)**
  - **Why needed here:** HMM systems integrate LMs during decoding (directly shaping hypotheses); E2E systems typically use LMs for post-hoc n-best rescoring with smaller gains.
  - **Quick check question:** Why would shallow LM integration (rescoring) help less than deep integration (during beam search)?

- **Concept: Domain Mismatch in Low-Resource ASR**
  - **Why needed here:** Cornish achieved 7.72% WER in-domain but 72.55% out-of-domain—speaker/style diversity in training data determines generalization.
  - **Quick check question:** If your training data is all read speech from one speaker, what failure mode do you expect on conversational test audio?

## Architecture Onboarding

- **Component map:** Short-form data collection -> Text normalization -> GMM-HMM training -> Forced alignment -> Segmentation -> Model training (HMM/E2E) -> External LM integration
- **Critical path:**
  1. Collect ≥40 min multi-speaker short-form data with verbatim transcripts
  2. Train monophone GMM-HMM on short-form data → perform forced alignment on long-form audio
  3. Filter successful alignments (target 70%+ success rate)
  4. Train domain-specific models using segmented utterances
  5. Add external LM if text resources available (expect ~20 WER point gain for HMM, ~2 for E2E)
- **Design tradeoffs:**
  - **HMM vs. E2E:** HMM + external LM excels on read speech with abundant text; Whisper generalizes better to conversational/specialized domains
  - **Speaker diversity vs. data quantity:** Multi-speaker short-form (102 min) outperformed single-speaker utterance-level data—diversity trumps volume
  - **Text availability:** If no web text corpus exists, skip external LM and rely on E2E fine-tuning (as done for Cornish)
- **Failure signatures:**
  - WER >95% on all test sets → likely single-speaker/low-diversity training (Forvo 16-min condition)
  - Alignment success <70% → transcript-audio mismatch; manually audit long-form transcripts
  - Large in-domain vs. out-of-domain gap (>50% WER difference) → speaker/domain overfitting; add diverse sources
- **First 3 experiments:**
  1. **Baseline short-form ASR:** Train GMM-HMM on 40 min multi-speaker short-form data; evaluate on held-out careful speech (expect <50% WER per paper)
  2. **Alignment quality check:** Run forced alignment on 30 min long-form audio; manually inspect 20 random segments for boundary accuracy
  3. **Model comparison with/without LM:** Train both HMM and Whisper on aligned utterances; test with and without external LM to quantify architecture-specific gains (expect ~20 pt HMM gain, ~0-2 pt Whisper gain)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does speaker diversity in the long-form refinement stage mitigate the domain mismatch observed in single-speaker corpora?
- **Basis in paper:** [inferred] The authors note the severe performance drop in Cornish (72.55% WER on out-of-domain data) and explicitly suspect this is because the 39-hour training corpus derived from long-form data contained only a single speaker.
- **Why unresolved:** The study lacked multi-speaker long-form Cornish data to isolate whether the drop was due to domain shift or speaker overfitting.
- **What evidence would resolve it:** A controlled ablation study varying speaker count in the long-form refinement data while holding data quantity constant.

### Open Question 2
- **Question:** Can this pipeline be effectively adapted for unwritten languages where external text data for Language Models is unavailable?
- **Basis in paper:** [explicit] The paper states that for unwritten languages, leveraging text data is "impractical," and while they simulated this for Cornish by disabling the external LM, the primary results relied heavily on text resources.
- **Why unresolved:** The study confirms HMMs benefit significantly from text (approx. 20 WER points improvement), but it is unclear if the remaining pipeline is viable for languages with zero orthographic resources.
- **What evidence would resolve it:** Replicating the methodology on an endangered language with no written standard or text corpus.

### Open Question 3
- **Question:** Does the "short-form" spoken dictionary approach generalize to languages with complex tonal or prosodic systems different from Manx and Cornish?
- **Basis in paper:** [inferred] The paper demonstrates success on Manx and Cornish (Indo-European languages) but uses graphemic lexicons, potentially limiting applicability to languages where phonology diverges significantly from orthography.
- **Why unresolved:** The acoustic modeling relies on bootstrapping from short clips; it is unproven whether 40 minutes of data is sufficient to model complex tone or pitch-accent systems not captured by standard graphemic approaches.
- **What evidence would resolve it:** Application of the pipeline to a low-resource tonal language (e.g., from the Niger-Congo or Sino-Tibetan families) using the same data constraints.

## Limitations
- **Text dependency:** The reported 20-point WER improvement from external language models assumes substantial web-scraped text corpora (4.8M words for Manx), which may not exist for genuinely text-scarce languages.
- **Single-experiment architecture decisions:** The 102-minute Manx success may not scale linearly to languages with fewer available recordings, and the study relies heavily on specific data sources that may not generalize.
- **Single-speaker overfitting:** The Cornish experiment showed severe domain mismatch (72.55% WER out-of-domain) due to single-speaker training data, suggesting the need for multi-speaker diversity validation.

## Confidence

**High Confidence (3 claims):**
- Short-form spoken dictionary data can enable ASR without utterance-level corpora
- Forced alignment with weak acoustic models can bootstrap segmentation
- Multi-speaker diversity is more critical than data quantity

**Medium Confidence (2 claims):**
- HMM models benefit more from external LMs than E2E models
- Whisper generalizes better to conversational speech than domain-specific models

**Low Confidence (1 claim):**
- 8 minutes of short-form data is sufficient for alignment bootstrapping (Cornish experiment has limited validation)

## Next Checks
1. **Cross-language replication test:** Apply the exact pipeline to a third endangered language with 40-100 minutes of short-form data. Measure whether WER <50% is achievable and whether forced alignment success rate consistently exceeds 70%.

2. **Text-scarcity ablation:** Replicate the Cornish experiment with and without external language models to quantify the exact WER penalty when web text is unavailable. This validates the claimed architecture preferences under realistic resource constraints.

3. **Speaker diversity experiment:** Train models on Forvo data with varying speaker diversity (1-speaker vs. 3-speaker vs. 10-speaker conditions) using identical total minutes. Measure the WER degradation to confirm the mechanism requiring multi-speaker variation.