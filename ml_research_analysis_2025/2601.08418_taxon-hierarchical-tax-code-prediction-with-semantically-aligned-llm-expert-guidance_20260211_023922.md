---
ver: rpa2
title: 'Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert
  Guidance'
arxiv_id: '2601.08418'
source_url: https://arxiv.org/abs/2601.08418
tags:
- hierarchical
- code
- semantic
- classification
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Taxon addresses hierarchical tax code prediction for large-scale
  e-commerce invoicing, mapping products to nodes in a ten-level national taxonomy.
  It integrates a feature-gating mixture-of-experts (MoE) architecture that adaptively
  routes multi-modal product metadata across taxonomy levels, and a semantic consistency
  model distilled from large language models that ensures alignment between product
  titles and official tax definitions.
---

# Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance

## Quick Facts
- arXiv ID: 2601.08418
- Source URL: https://arxiv.org/abs/2601.08418
- Reference count: 40
- Primary result: State-of-the-art hierarchical tax code prediction with >89% path-level macro F1 on 10-level taxonomy, deployed at 500k+ daily queries

## Executive Summary
Taxon addresses hierarchical tax code prediction for large-scale e-commerce invoicing, mapping products to nodes in a ten-level national taxonomy. It integrates a feature-gating mixture-of-experts (MoE) architecture that adaptively routes multi-modal product metadata across taxonomy levels, and a semantic consistency model distilled from large language models that ensures alignment between product titles and official tax definitions. The system also employs a multi-source training pipeline combining business records, validation logs, and expert-curated data to mitigate noisy supervision. Evaluated on proprietary and public benchmarks, Taxon achieves state-of-the-art performance, with improvements in both macro and micro F1 scores. A post-processing step, RePath, further enhances path-level structural consistency. The framework is deployed in production, handling over 500,000 daily tax code queries with high accuracy, interpretability, and scalability.

## Method Summary
Taxon is a hierarchical text classification system for mapping product metadata to a 10-level national tax taxonomy. It uses a feature-gating mixture-of-experts (MoE) architecture with ten parallel MoE modules, each responsible for predicting labels at a specific hierarchy level. The system fuses text embeddings (BERT) with one-hot business metadata (BU/OU codes) for routing decisions. An auxiliary semantic consistency loss, distilled from LLM judgments, aligns predictions with official tax definitions. During inference, a RePath post-processing step reconstructs valid paths from leaf predictions to ensure structural consistency. The model is trained on a proprietary dataset of over 850k samples and validated on public benchmarks.

## Key Results
- Achieves 89.37% path-level macro F1 and 92.21% leaf-level macro F1 on the 10-level TaxCode dataset
- Outperforms baselines by +1.25% absolute gain in MoE vs linear classifier on Validation Record set
- Improves path-level macro F1 from 85.06% to 89.37% via RePath post-processing
- Handles over 500,000 daily tax code queries in production deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Feature-gating MoE adaptively routes multi-modal inputs to level-specific experts, reducing interference across taxonomy depths.
- **Mechanism:** Ten parallel MoE modules, one per hierarchy level, receive concatenated text+structured features. A gating network computes routing weights from one-hot business metadata (BU/OU codes), directing each input to relevant experts. Each expert specializes in subsets of product types, enabling level-specific decision boundaries without explicit hierarchy encoders.
- **Core assumption:** Business metadata carries implicit distributional cues that correlate with tax code patterns (e.g., certain BUs predominantly handle specific product categories).
- **Evidence anchors:**
  - [abstract] "feature-gating mixture-of-experts (MoE) architecture that adaptively routes multi-modal product metadata across taxonomy levels"
  - [Section II-A] "we deploy ten MoE modules in parallel, each responsible for predicting labels within its respective level"
  - [Section III-D, Tab. VIII] MoE classifier achieves +1.25% absolute gain over linear on Validation Record set
- **Break condition:** If business metadata is uniformly distributed or uncorrelated with tax categories, gating signal degrades to noise; routing becomes near-uniform, collapsing to standard classifier behavior.

### Mechanism 2
- **Claim:** LLM-distilled semantic consistency supervision aligns product representations with official tax definitions, correcting noisy labels.
- **Mechanism:** Qwen-Max labels a 64K development set with Y/N/- consistency judgments (product title ↔ tax code definition). These labels train a lightweight "semantic model" that generates consistency signals for the full 8M corpus. The auxiliary loss (Eq. 3) forces predicted embeddings toward reference tax definition embeddings.
- **Core assumption:** The LLM's semantic judgments generalize beyond labeled examples and reflect ground-truth regulatory semantics better than historical noisy annotations.
- **Evidence anchors:**
  - [abstract] "semantic consistency model distilled from large language models acting as domain experts"
  - [Section II-B.3] "Qwen-Max... prompted with structured JSON-based instructions to determine whether each product title aligns"
  - [Section III-D, Tab. X] Semantic loss yields moderate gains; paper notes qualitative improvements in correcting knowledge-level misclassifications
  - [corpus] Weak direct evidence; neighbor papers on tax code prediction do not replicate this distillation mechanism
- **Break condition:** If LLM judgments systematically misalign with actual regulatory semantics (e.g., over-literal interpretation misses domain conventions), distilled model introduces systematic bias.

### Mechanism 3
- **Claim:** RePath post-processing eliminates intermediate-node inconsistencies by reconstructing paths from reliable leaf predictions.
- **Mechanism:** After prediction, leaf-level output (highest-confidence leaf) is taken as anchor. The algorithm traces upward through the taxonomy's parent-child edges to recover the full root-to-leaf path, bypassing independent intermediate-level classifiers that may output structurally invalid nodes.
- **Core assumption:** Leaf predictions are substantially more reliable than intermediate-level predictions; the taxonomy graph is complete and correct.
- **Evidence anchors:**
  - [Section III-C] "residual errors mainly arise from structural inconsistency along the predicted path rather than semantic misunderstanding at the leaf level"
  - [Section III-C, Tab. IV] RePath improves path-level macro F1 from 85.06% to 89.37% while leaf scores unchanged
  - [corpus] No external replication of RePath; mechanism is domain-specific
- **Break condition:** If leaf predictions are unreliable (deep hierarchy, sparse data), RePath propagates leaf errors through entire path.

## Foundational Learning

- **Hierarchical Text Classification (HTC):**
  - Why needed here: Taxon formulates tax code prediction as single-path HTC with 10 levels, 4,482 labels—standard flat classifiers cascade errors.
  - Quick check question: Can you explain why enforcing parent-child constraints during training reduces error accumulation compared to independent per-level classifiers?

- **Mixture-of-Experts Routing:**
  - Why needed here: The gating mechanism requires understanding how sparse expert selection works; routing weights determine which experts process each input.
  - Quick check question: Given a gating network output `[0.1, 0.7, 0.2]` for three experts, what happens to gradients if Expert 2's predictions are consistently wrong?

- **Knowledge Distillation (LLM → Small Model):**
  - Why needed here: Taxon uses Qwen-Max as teacher, distilling into a lightweight semantic classifier; without understanding distillation, the training pipeline is opaque.
  - Quick check question: Why distill rather than query the LLM at inference time? (Hint: latency, cost, throughput constraints in production.)

## Architecture Onboarding

- **Component map:**
  Input layer (product title + category name + BU/OU codes) -> Text encoder (BERT) + embedding lookup -> Concatenated features -> MoE gating (one-hot driven routing) -> 10 parallel level-specific experts -> Per-level classifiers -> Semantic branch (training only) -> Output (leaf-first confidence ranking) -> RePath post-processing

- **Critical path:**
  1. Raw product metadata → text encoder (BERT) + embedding lookup
  2. Concatenated features → MoE gating (one-hot driven routing)
  3. Expert outputs → level classifiers (10 cross-entropy heads)
  4. Semantic branch computes auxiliary loss during training only
  5. Inference: RePath reconstructs full path from top leaf prediction

- **Design tradeoffs:**
  - MoE vs. GCN/TreeLSTM hierarchy encoders: MoE is implicit, data-driven; GCN is explicit but rigid (paper avoids explicit structure encoders)
  - Semantic loss weight (ω_s=0.2): Higher weights risk overfitting to potentially imperfect LLM labels
  - Confidence threshold (0.9) for development set sampling: Low threshold includes noisy hard examples; high threshold yields cleaner but less diverse data

- **Failure signatures:**
  - Gating collapse: All routing weights converge to uniform distribution—indicates metadata uninformative or learning rate too high on gates
  - Leaf-level accuracy high but path-level low: RePath not applied, or intermediate classifiers output invalid nodes
  - Semantic model over-correction: Disagreement with ground truth increases but actual tax accuracy decreases—suggests LLM labels misaligned

- **First 3 experiments:**
  1. **Baseline sanity check:** Train BERT + linear head (no MoE, no semantic loss) on cleansed subset; verify macro F1 ≈ 80% (Tab. IV HGCLR baseline) before adding complexity.
  2. **Ablation by depth:** Evaluate Taxon vs. baselines across path depths 2–6 (Fig. 10, 11); confirm performance gains concentrate at depth 4–5 where most samples lie.
  3. **Semantic labeling quality audit:** Sample 100 LLM annotations; manually verify Y/N/- labels against tax definitions to estimate distillation noise floor before training semantic model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the semantic consistency distillation pipeline be refined to handle the "labeling inaccuracies" inherent in the teacher LLM?
- Basis: [explicit] Section III-D notes that "residual errors arise partly from imperfect semantic labels–the LLM's own labeling inaccuracies occasionally misguide training, leaving a small portion of hard cases unresolved."
- Why unresolved: The current framework treats LLM judgments as ground truth for distillation without explicitly correcting for the teacher's own hallucinations or errors in the "uncertain" (-) or "incorrect" (N) categories.
- What evidence would resolve it: A comparative study using different teacher LLMs or a noise-robust distillation loss that outperforms the current Qwen-Max baseline on the hard cases identified in Table II.

### Open Question 2
- Question: Can the structural corrections provided by RePath be integrated into the model's training loss rather than applied as post-processing?
- Basis: [inferred] Section III-C introduces RePath as a "post-processing step" required because the base model occasionally produces structurally inconsistent paths (e.g., correct leaf but incorrect intermediate nodes).
- Why unresolved: The current hierarchical classification loss ($L_c$) does not explicitly enforce valid parent-child paths during training, relying instead on inference-time reconstruction to fix structural breaks.
- What evidence would resolve it: The implementation of a differentiable path-consistency loss that achieves the RePath performance gains (e.g., +4% macro F1) without requiring external reconstruction.

### Open Question 3
- Question: What is the minimum hierarchy depth required for the RePath mechanism to provide statistically significant improvements?
- Basis: [inferred] Results in Section III-C show RePath significantly improves the 10-level TaxCode dataset but provides "negligible improvement" on the 2-level WOS dataset.
- Why unresolved: It is unclear if RePath is universally applicable to hierarchical classification or if its utility is strictly limited to deep taxonomies where error accumulation is more likely.
- What evidence would resolve it: An evaluation of Taxon on benchmark datasets with varying depths (e.g., 3 to 7 levels) to identify the threshold where RePath begins to impact performance metrics.

## Limitations
- MoE configuration details (expert count per level, routing sparsity, capacity factors) are underspecified, making exact replication difficult
- Semantic distillation pipeline assumes Qwen-Max labels are trustworthy but does not validate against ground-truth regulatory interpretations
- No latency or throughput benchmarks provided for production scalability claims (500k daily queries)

## Confidence
- **High:** MoE routing mechanism (well-defined in architecture, clear empirical gains in Tab. VIII)
- **Medium:** Semantic consistency supervision (evidence exists but LLM label quality unverified)
- **Low:** Production scalability claims (500k daily queries cited but no latency or throughput benchmarks provided)

## Next Checks
1. **Semantic label audit:** Manually verify 100 Qwen-Max consistency annotations against tax authority definitions to estimate distillation noise floor
2. **RePath sensitivity test:** Vary leaf prediction confidence thresholds and measure path-level F1 degradation to bound RePath's reliance on leaf accuracy
3. **MoE ablation by level:** Freeze intermediate-level experts and evaluate whether gains come from leaf experts alone, isolating MoE's contribution per hierarchy depth