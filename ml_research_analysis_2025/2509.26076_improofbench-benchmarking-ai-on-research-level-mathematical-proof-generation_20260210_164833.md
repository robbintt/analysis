---
ver: rpa2
title: 'IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation'
arxiv_id: '2509.26076'
source_url: https://arxiv.org/abs/2509.26076
tags:
- mathematical
- problems
- question
- tool
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IMProofBench introduces a new benchmark for evaluating large language
  models on research-level mathematical proof generation. Unlike prior benchmarks
  focused on final-answer problems or high-school competition questions, IMProofBench
  consists of 39 peer-reviewed problems authored by expert mathematicians, each requiring
  a detailed proof and paired with auto-gradable subproblems.
---

# IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation

## Quick Facts
- **arXiv ID:** 2509.26076
- **Source URL:** https://arxiv.org/abs/2509.26076
- **Reference count:** 40
- **Key outcome:** GPT-5 achieves full solutions for 22% of problems; GROK-4 leads final-answer accuracy at 52%

## Executive Summary
IMProofBench introduces a new benchmark for evaluating large language models on research-level mathematical proof generation. Unlike prior benchmarks focused on final-answer problems or high-school competition questions, IMProofBench consists of 39 peer-reviewed problems authored by expert mathematicians, each requiring a detailed proof and paired with auto-gradable subproblems. The evaluation setup simulates a realistic research environment with access to tools like SageMath, web search, and Python. Models are graded by human experts on proof quality and automatically on subquestion accuracy. Results show GPT-5 achieves full solutions for 22% of problems, with GROK-4 leading final-answer accuracy at 52%. Most models demonstrate strong familiarity with mathematical literature but are prone to logical errors, hallucinations, and rarely abstain from answering. IMProofBench is designed as an evolving platform to remain relevant as AI capabilities advance.

## Method Summary
IMProofBench evaluates LLMs on research-level mathematical proof generation using a hybrid approach combining human expert grading with automated subquestion evaluation. The benchmark consists of 39 private, peer-reviewed problems authored by mathematicians, each with a main proof question and 2-3 subquestions with unique answers. Models are evaluated in an agentic framework using the UK AI Security Institute's Inspect tool, with access to Python, SageMath, Bash, and web search. The evaluation uses a submit tool for final answers, with 300k tokens for main questions and 100k per subquestion, plus 15-minute tool timeouts. Human experts grade main proofs on a 0-3 scale with 8 binary error/achievement indicators, while subquestions are automatically graded via exact-match comparison to ground truth.

## Key Results
- GPT-5 achieves full solutions (score 3) for 22% of problems
- GROK-4 leads final-answer accuracy at 52% despite only 19% full proof solutions
- Correlation between subquestion scores and human progress scores is 0.45
- Logical errors (~80% in Claude-Opus-4.1) and hallucinations (50% in Gemini-2.5-Pro) are the most common error types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining human expert grading with automated subquestion evaluation provides both qualitative depth and quantitative scalability for proof assessment.
- Mechanism: Main proof solutions receive 0–3 scores from domain experts using structured rubrics (error indicators: logic, hallucination, calculation, conceptual; achievement indicators: understanding, insight, usefulness). Subquestions with unique answers enable automated comparison to ground truth, allowing broader model coverage without proportional human cost.
- Core assumption: Subquestion accuracy correlates meaningfully with proof quality; experts can consistently apply rubrics across diverse mathematical domains.
- Evidence anchors:
  - [abstract] "Each problem requires a detailed proof and is paired with subproblems that have final answers, supporting both an evaluation of mathematical reasoning capabilities by human experts and a large-scale quantitative analysis through automated grading."
  - [section 4.1] "the correlation coefficient between author-weighted subquestion scores and the 0–3 progress scores assigned by human graders is 0.45"
  - [corpus] Related work (Open Proof Corpus, RealMath) similarly combines human evaluation with automated components, suggesting convergent validation of this hybrid approach.
- Break condition: If subquestion–proof correlation drops substantially (e.g., <0.3) or inter-rater reliability among experts is low, the dual-evaluation premise weakens.

### Mechanism 2
- Claim: Agentic evaluation with computational tools reveals capabilities and failure modes invisible in text-only benchmarks.
- Mechanism: Models operate in a multi-turn environment with Python, SageMath, Bash, and web search, each with 15-minute timeouts and 8GB RAM limits. This surfaces tool-use strategies (e.g., GROK-4 downloading arXiv papers via bash; models querying OEIS) and tool-misuse failures (syntactically invalid code for specialized packages).
- Core assumption: Tool access approximates real research conditions and does not introduce confounding variance unrelated to mathematical reasoning.
- Evidence anchors:
  - [section 3.3] "models are evaluated within an agentic framework designed to approximate real research conditions... [with] Python, Bash, SageMath, Web search"
  - [section 4.3] "GROK-4 is the only model to make heavy use of the bash tool... frequent use of the command line to download research papers from arXiv"
  - [corpus] RLMEval and RealMath similarly argue for tool-augmented evaluation, though neither implements proof-specific human grading at this scale.
- Break condition: If tool availability primarily rewards engineering cleverness over mathematical reasoning, or if models exploit tool quirks rather than demonstrating understanding.

### Mechanism 3
- Claim: Private, peer-reviewed problems authored by domain experts reduce contamination and ensure genuine research-level difficulty.
- Mechanism: Problems are created by mathematicians within their expertise, undergo dual review (administrator + domain expert), and remain private. Authors can retire problems if new publications make them significantly easier. This targets the contamination and difficulty ceiling issues affecting public benchmarks.
- Core assumption: Privacy effectively prevents pre-training contamination; peer review catches ambiguities and ensures appropriate difficulty.
- Evidence anchors:
  - [section 3.2] "A problem is only accepted once the reviewers have no further concerns."
  - [section 3.4] "problems are added on a rolling basis... authors are encouraged to revisit and possibly retire their problems once new publications or techniques make them significantly easier"
  - [corpus] FrontierMath criticism (privileged access concerns) and HLE contamination risks are cited as motivations for this design.
- Break condition: If problems leak, or if expert authors inadvertently create problems solvable via pattern-matching against existing literature without genuine reasoning.

## Foundational Learning

- **Mathematical proof vs. final-answer evaluation**:
  - Why needed here: The paper explicitly distinguishes proof-writing (requiring logical argumentation) from final-answer tasks (which permit shortcuts). Understanding this gap is essential for interpreting the 22% vs. 52% performance disparity.
  - Quick check question: Can you articulate why a model might achieve 52% on final-answer subquestions but only 22% on full proof solutions?

- **Agentic evaluation frameworks (Inspect-style)**:
  - Why needed here: The benchmark uses the UK AI Security Institute's Inspect framework with multi-turn tool access. Familiarity with how tool calls, timeouts, and submission workflows operate is prerequisite to reproducing or extending the evaluation.
  - Quick check question: How does a submit tool differ from a standard generation endpoint in constraining model behavior?

- **Benchmark contamination dynamics**:
  - Why needed here: The paper's private-by-design choice responds to contamination in public benchmarks. Understanding how problems can enter training data (direct inclusion, paraphrase, concept exposure) clarifies why privacy + rotation matters.
  - Quick check question: Why might a benchmark remain useful even if individual problems become contaminated, provided it is continuously refreshed?

## Architecture Onboarding

- **Component map**:
  Problem authoring interface -> Peer-review pipeline -> Evaluation environment -> Grading interface -> Results dashboard

- **Critical path**:
  1. Author drafts problem with subquestions → 2. Reviewers verify correctness and difficulty → 3. Problem accepted to benchmark → 4. Model evaluation in agentic framework → 5. Subquestions auto-graded; main solutions human-graded → 6. Results aggregated with error/achievement annotations

- **Design tradeoffs**:
  - **Private vs. public**: Privacy reduces contamination but limits community scrutiny; paper addresses this by planning sample problem releases.
  - **Human vs. automated grading**: Human grading provides nuance but doesn't scale; subquestions enable broader coverage but imperfectly proxy proof quality (r=0.45).
  - **Tool access vs. isolation**: Tools simulate research but introduce variance; paper standardizes via fixed environment (Arch Linux Docker, specific package versions).
  - **Scale vs. depth**: 39 problems is small; paper argues pilot-phase insights are valuable (citing USAMO/IMO precedents) but acknowledges this as primary limitation.

- **Failure signatures**:
  - **Logical errors**: Unfounded assumptions, invalid implications (most common error type; ~80% in Claude-Opus-4.1)
  - **Hallucinated results**: Non-existent theorems/papers cited confidently (50% in Gemini-2.5-Pro)
  - **Hidden mistakes**: Single incorrect claim simplifies problem, presented rhetorically as "well-known"
  - **Non-abstention**: Models almost never decline to answer, even on open problems
  - **GROK-4 brevity**: Extremely short outputs obscure reasoning, leading to frequent "Not Sure" grades

- **First 3 experiments**:
  1. **Reproduce subquestion scoring**: Run 2–3 models on the 31 problems with subquestions; verify automated parser matches reported scores; check correlation with human progress scores.
  2. **Tool ablation**: Evaluate one model (e.g., GPT-5 equivalent) with tools disabled vs. enabled on a held-out subset; quantify contribution of web search vs. computational tools to final-answer vs. proof performance.
  3. **Inter-rater reliability pilot**: Have two experts independently grade the same 10 model solutions; compute Cohen's κ on the 0–3 scale and binary indicators to validate grading consistency claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can effective abstention mechanisms be developed that cause models to refuse answering when they lack sufficient confidence, rather than producing convincing but incorrect proofs?
- Basis in paper: [explicit] "Models rarely abstain from claiming a solution... models almost never abstain from providing a solution attempt, preferring to present convincing but incorrect proofs rather than admit they are stuck."
- Why unresolved: Current RLHF training may incentivize answering over abstention; no benchmark systematically evaluates calibration of when-to-abstain.
- What evidence would resolve it: A modified IMProofBench evaluation rewarding abstention on unsolvable problems, showing models can reliably distinguish tractable from intractable questions.

### Open Question 2
- Question: What explains the persistent gap between final-answer accuracy and proof-writing capability, and can it be narrowed?
- Basis in paper: [explicit] GROK-4 achieves 52% final-answer accuracy but only 19% full solutions; the authors note "a persistent gap between final-answer accuracy and genuine proof-writing ability."
- Why unresolved: Final-answer benchmarks may reward shortcut heuristics that bypass rigorous reasoning; the cognitive processes for producing correct answers vs. proofs may be fundamentally different.
- What evidence would resolve it: Fine-grained analysis of reasoning traces showing whether models use different strategies for answer-only vs. proof tasks, or targeted training showing convergence between the two.

### Open Question 3
- Question: Why do independently developed models converge on identical incorrect shortcuts or "well-known results" that are actually false?
- Basis in paper: [explicit] "Sometimes, different models even independently converge on the same shortcut, leading to parallel arguments that can create a false sense of consensus."
- Why unresolved: Training data overlap, shared mathematical misconceptions in web corpora, or fundamental limitations in how LLMs represent mathematical truth could all contribute.
- What evidence would resolve it: Systematic comparison of error patterns across models with different training data compositions, or analysis of whether shared errors correlate with specific training corpus sources.

### Open Question 4
- Question: How reliably can subquestion accuracy serve as a proxy for full proof evaluation, and what types of reasoning does it fail to capture?
- Basis in paper: [explicit] The correlation between subquestion scores and human proof grades is 0.45; authors state this "suggests that final-answer evaluation is a useful proxy... but human grading provides essential nuance."
- Why unresolved: The 0.45 correlation is moderate but leaves substantial unexplained variance; the specific failure modes of proxy evaluation are not characterized.
- What evidence would resolve it: Regression analysis identifying which proof qualities (e.g., logical coherence, creativity, correctness of intermediate steps) are least captured by subquestion performance.

## Limitations
- Private problems create reproducibility bottleneck requiring access to 39 benchmark problems and solutions
- Small scale (39 problems) limits statistical power despite acknowledged pilot-phase value
- 0.45 correlation between subquestion scores and human proof grades indicates partial proxy at best
- Inter-rater reliability for human grading not quantified in paper

## Confidence

- **High confidence**: The hybrid human-automated evaluation mechanism (combining expert grading with subquestion scoring) is technically sound and addresses known scalability issues in proof assessment.
- **Medium confidence**: Claims about contamination mitigation via private problems and continuous retirement are plausible but depend on effective problem secrecy and timely retirement protocols.
- **Medium confidence**: The agentic evaluation framework with tools provides realistic research simulation, but the extent to which tool use reflects mathematical reasoning versus engineering skill remains unclear.

## Next Checks
1. **Inter-rater reliability validation**: Have two independent domain experts grade the same 10 model solutions and compute Cohen's κ for both the 0-3 progress scale and binary error/achievement indicators.
2. **Subquestion proxy validation**: Expand the subquestion-proof correlation analysis to include all 39 problems and test whether adding problem-specific covariates (e.g., domain, difficulty) improves predictive power beyond the reported r=0.45.
3. **Tool contribution ablation**: Evaluate 2-3 models on a subset of problems with all tools disabled, then incrementally re-enable specific tools (web search, Python, SageMath) to isolate their individual contributions to final-answer versus proof performance.