---
ver: rpa2
title: 'ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking
  Theory of Mind'
arxiv_id: '2501.08838'
source_url: https://arxiv.org/abs/2501.08838
tags:
- tomato
- mental
- llms
- states
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToMATO, a comprehensive Theory of Mind benchmark
  that evaluates large language models across five mental state categories (beliefs,
  intentions, desires, emotions, knowledge) and their false beliefs. The benchmark
  is constructed using LLM-LLM conversations with "Inner Speech prompting" that makes
  mental states explicit through thoughts, and incorporates information asymmetry
  and diverse personality traits.
---

# ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind

## Quick Facts
- arXiv ID: 2501.08838
- Source URL: https://arxiv.org/abs/2501.08838
- Reference count: 27
- Large language models significantly underperform human baselines on Theory of Mind tasks, especially on false belief scenarios and with diverse personality traits

## Executive Summary
This paper introduces ToMATO, a comprehensive Theory of Mind benchmark that evaluates large language models across five mental state categories (beliefs, intentions, desires, emotions, knowledge) and their false beliefs. The benchmark is constructed using LLM-LLM conversations with "Inner Speech prompting" that makes mental states explicit through thoughts, and incorporates information asymmetry and diverse personality traits. ToMATO contains 5.4k questions from 753 conversations with 15 personality trait patterns. Evaluation of nine LLMs shows that even the best models (GPT-4o mini, Llama-3.1-70B) significantly underperform human baselines, especially on false belief tasks and with diverse personality traits, indicating current LLMs still lack robust ToM capabilities.

## Method Summary
ToMATO evaluates Theory of Mind through multiple-choice questions derived from LLM-LLM conversations. Two role-playing LLMs generate thoughts in `({thought}) "{utterance}"` format using Inner Speech prompts for five mental state types (beliefs, intentions, desires, emotions, knowledge) and their second-order variants. Information asymmetry is created by hiding each agent's thoughts, goals, and personality from the other agent. Personality traits from the Big Five model (15 patterns) are assigned to agents to diversify mental state patterns. Questions are generated from first- and second-order mental states, with false beliefs detected by comparing an agent's stated mental state against another agent's belief about it. The benchmark contains 5.4k questions from 753 conversations, with quality validated by both human annotators and GPT-4o mini.

## Key Results
- All evaluated LLMs significantly underperform human baselines on ToMATO, with accuracy gaps ranging from 7.8% to 36.7%
- False belief tasks are particularly challenging, with even the best model (GPT-4o mini) scoring only 39.0% accuracy
- Personality diversity affects performance, with certain combinations (e.g., high neuroticism, low agreeableness) causing greater accuracy drops
- Larger models generally perform better, but the gap between human and model performance remains substantial even for the largest models

## Why This Works (Mechanism)

### Mechanism 1: Inner Speech Prompting for Explicit Mental State Grounding
- Claim: Requiring role-playing LLMs to verbalize thoughts before utterances creates observable ground-truth mental states for benchmark questions.
- Mechanism: Inner Speech prompts (e.g., "I think that he/she thinks...") enforce structured generation where first- and second-order mental states are explicitly stated in parentheses before each utterance. These verbalized thoughts serve as the correct answers for multiple-choice questions, eliminating ambiguity about what the agent "actually" believes.
- Core assumption: LLMs can generate coherent, contextually appropriate mental state verbalizations that meaningfully correspond to their role-playing behavior.
- Evidence anchors:
  - [abstract]: "By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories."
  - [Section 4, Table 2]: Inner Speech prompts defined for each mental state type (e.g., "I think" for beliefs, "I feel" for emotions, "I think that he/she thinks" for second-order).
  - [corpus]: Limited direct corpus support for this specific prompting technique; most ToM benchmarks use template-based narratives rather than conversation-based generation.
- Break condition: If LLMs generate inconsistent or incoherent thoughts that don't align with subsequent utterances, ground-truth labels become unreliable.

### Mechanism 2: Information Asymmetry Induces False Belief Generation
- Claim: Hiding one agent's thoughts, goals, and personality from the other agent naturally produces false belief scenarios without manual construction.
- Mechanism: Each agent receives only the other's utterances (not thoughts or system prompts). This asymmetry causes agents to form incorrect second-order beliefs about the other's mental states. The paper detects false beliefs by comparing Agent A's first-order mental state with Agent B's second-order belief about A's state.
- Core assumption: Information asymmetry in LLM-LLM interactions functions similarly to human false-belief scenarios (e.g., Sally-Anne tasks).
- Evidence anchors:
  - [abstract]: "The information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states."
  - [Section 6, Table 6]: Ablation study shows false belief probability drops from 46.6% (full asymmetry) to 39.0% (no asymmetry) per GPT-4o mini judge.
  - [corpus]: Related work (FANToM, Kim et al. 2023b) uses character joining/leaving for information asymmetry; ToMATO extends this to asymmetry about mental states themselves.
- Break condition: If agents infer hidden information too accurately through conversational context, false belief generation rate would drop below useful threshold.

### Mechanism 3: Personality Traits Diversify Mental State Patterns
- Claim: Assigning Big Five personality trait combinations to role-playing agents increases benchmark diversity and reveals model robustness gaps.
- Mechanism: System prompts encode 15 patterns of high/low values across Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Personality traits affect both utterances and verbalized thoughts, creating varied mental state distributions across conversations.
- Core assumption: LLMs can consistently embody assigned personality traits in their outputs.
- Evidence anchors:
  - [abstract]: "Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts."
  - [Section 6, Figure 2]: Z-statistics analysis shows personality-correlated vocabulary (e.g., "worried" for neurotic agents, "happy" for stable agents).
  - [Section 6, Table 14]: Pairwise comparison shows 67-80% success rate in personality reflection (lowest for Conscientiousness).
  - [corpus]: PsyPlay (arxiv 2502.03821) similarly explores personality-infused role-playing but focuses on conversational style rather than ToM benchmarking.
- Break condition: If personality traits don't meaningfully affect generated thoughts, benchmark diversity claims are overstated.

## Foundational Learning

- **Theory of Mind (ToM) Taxonomy**:
  - Why needed here: Understanding first-order (A thinks X), second-order (B thinks that A thinks Y), and false belief distinctions is essential for interpreting benchmark categories.
  - Quick check question: If Alice wants coffee but Bob thinks Alice wants tea, is this a first-order false belief or second-order false belief?

- **Information Asymmetry in Social Reasoning**:
  - Why needed here: The core mechanism for generating false beliefs relies on partial observability between agents.
  - Quick check question: What three types of information are hidden between agents in ToMATO's conversation generation?

- **Big Five Personality Model (OCEAN)**:
  - Why needed here: Interpreting robustness results requires knowing which personality factors cause performance drops.
  - Quick check question: Which personality factor showed the lowest success rate in being reflected in generated outputs?

## Architecture Onboarding

- **Component map**: Scenario sampler -> Two agent instances with personality prompts -> Inner Speech prompting -> Conversation generation -> Thought extraction -> False belief detection -> Question/option generation -> Quality validation -> Final benchmark

- **Critical path**: Scenario → System prompts with personality → Inner Speech prompting → Conversation generation → Thought extraction → False belief detection → Question construction → Quality validation → Final benchmark

- **Design tradeoffs**:
  - Llama-3-70B for generation ensures transparency but creates potential contamination when evaluating Llama models
  - Random sampling for incorrect options avoids manual bias but may create easier distinctions
  - 7-turn limit prevents redundancy but may truncate complex social dynamics

- **Failure signatures**:
  - Low false belief detection rate (<30%) suggests information asymmetry is insufficient
  - High lexical overlap baseline performance indicates shortcut solutions
  - Inconsistent personality reflection (especially Conscientiousness) reduces diversity claims

- **First 3 experiments**:
  1. Replicate conversation generation with 5 scenarios, verify thought-utterance coherence manually
  2. Ablate information asymmetry conditions (Table 6), measure false belief frequency with GPT-4o mini
  3. Evaluate a small model (e.g., Llama-3-8B) on first-order beliefs subset only, compare to Table 3 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ToM evaluation be extended to multi-modal contexts while maintaining the comprehensive mental state coverage (belief, intention, desire, emotion, knowledge) that ToMATO provides for text-only conversations?
- Basis in paper: [explicit] The conclusion states: "Future work includes extending our work to evaluating ToM with multi-modal contexts (Mao et al. 2024), decision-making (Guo et al. 2024), and multi-agent settings (Cross et al. 2024)."
- Why unresolved: The paper notes that percepts and non-literal communications "require a multimodal context and/or pose challenges in verbalization," so these were excluded from the current scope.
- What evidence would resolve it: A benchmark integrating visual/audio inputs with text that can assess ToM across all five mental state categories for both first- and second-order reasoning.

### Open Question 2
- Question: What prompting strategies or training approaches can reliably induce conscientiousness in role-playing LLM outputs, given that it was the least successfully reflected personality trait in ToMATO?
- Basis in paper: [explicit] Section 4 states: "Among the five, C is less reflected as intended, which is consistent with Jiang et al. (2023). Inducing conscientiousness in outputs is future work."
- Why unresolved: Despite using extended prompts combining all five Big Five factors, conscientiousness was less reflected in outputs compared to openness, extraversion, agreeableness, and neuroticism.
- What evidence would resolve it: A modified prompting or fine-tuning approach that achieves >80% success rate in pairwise comparison for conscientiousness reflection, matching the success rates of other personality factors.

### Open Question 3
- Question: Can ToM performance be improved through training without degrading general social intelligence capabilities on other benchmarks?
- Basis in paper: [explicit] Appendix D reports: "Fine-tuning significantly improved the scores of Llama-3-8B-Instruct for both the ID and OOD split of ToMATO. However, when evaluated on SocialIQa, the fine-tuning degraded the scores... Improving ToM performance while keeping performance on other datasets is future work."
- Why unresolved: Supervised fine-tuning on ToM-specific data caused overfitting to the ToM task at the expense of general social reasoning.
- What evidence would resolve it: A training methodology that improves ToMATO scores while maintaining or improving SocialIQa performance compared to the pre-fine-tuning baseline.

## Limitations
- Benchmark construction relies on LLM-generated thoughts as ground truth, creating potential circularity and reliability concerns
- Use of Llama-3-70B for both generation and evaluation introduces contamination risk when evaluating Llama models
- Limited scenario diversity (753 conversations from 8 categories) may not adequately test generalization to novel social contexts

## Confidence
**High confidence** in: (1) the benchmark construction methodology using Inner Speech prompting and information asymmetry, (2) the finding that current LLMs underperform human baselines on ToM tasks, (3) the relative performance ordering across model sizes and architectures.

**Medium confidence** in: (1) the false belief detection mechanism's accuracy, (2) the personality trait effects on model performance, (3) the robustness claims based on 15 personality patterns.

**Low confidence** in: (1) the absolute numerical performance differences across mental state categories, (2) the specific z-score thresholds used for personality vocabulary analysis, (3) the claim that the benchmark represents "comprehensive" ToM evaluation.

## Next Checks
1. **Ground-truth verification study**: Conduct human annotation of a random sample (n=100) of generated conversations to verify that verbalized thoughts actually correspond to the agent's likely mental states given the utterance context. Measure inter-annotator agreement and compare against LLM-generated thoughts.

2. **Cross-generative validation**: Recreate the benchmark using a different LLM (e.g., GPT-4o mini) as the generator to test whether performance patterns remain consistent across generation engines. This would address contamination concerns and test the robustness of the prompting methodology.

3. **Stress test on personality diversity**: Create additional conversations with extreme personality trait combinations (e.g., very high/low in each Big Five dimension) and evaluate whether performance drops significantly beyond what's observed in the current 15 patterns. This would validate the claimed importance of personality diversity for robustness testing.