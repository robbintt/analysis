---
ver: rpa2
title: 'UCD: Unlearning in LLMs via Contrastive Decoding'
arxiv_id: '2506.12097'
source_url: https://arxiv.org/abs/2506.12097
tags:
- unlearning
- forget
- arxiv
- utility
- tofu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UCD, an inference-time unlearning algorithm\
  \ for large language models that uses contrastive decoding to remove specific information\
  \ while preserving overall performance. The method leverages two smaller auxiliary\
  \ models\u2014one trained on the forget set and one on the retain set\u2014to guide\
  \ the outputs of the original model by adjusting token logits based on their difference."
---

# UCD: Unlearning in LLMs via Contrastive Decoding

## Quick Facts
- arXiv ID: 2506.12097
- Source URL: https://arxiv.org/abs/2506.12097
- Reference count: 40
- This paper introduces UCD, an inference-time unlearning algorithm for large language models that uses contrastive decoding to remove specific information while preserving overall performance.

## Executive Summary
This paper introduces UCD, an inference-time unlearning algorithm for large language models that uses contrastive decoding to remove specific information while preserving overall performance. The method leverages two smaller auxiliary models—one trained on the forget set and one on the retain set—to guide the outputs of the original model by adjusting token logits based on their difference. Evaluated on TOFU and MUSE benchmarks, UCD significantly improves the tradeoff between forget quality and model utility, achieving results indistinguishable from retraining on forget tasks while maintaining or improving utility. Notably, UCD scales efficiently to large models like Llama2-70B, requiring minimal compute compared to existing methods, and can also enhance existing unlearning baselines when exact clean models are unavailable.

## Method Summary
UCD operates by training two auxiliary models from a base architecture: one fine-tuned on the forget set (A_corr) and one fine-tuned on the retain set (A_clean). At inference, it applies contrastive decoding to the reference model by subtracting a scaled difference of the auxiliary models' logits: log P_aligned(y|x) ← log P_corr(y|x) − α · (log A_corr(y|x) − log A_clean(y|x)). This modifies the token distribution without changing model weights. The method requires only forward passes through three models at inference, avoiding optimization challenges like local minima and symmetry-breaking associated with weight-based unlearning.

## Key Results
- UCD achieves forget quality indistinguishable from retraining on forget tasks while maintaining or improving model utility on retain sets
- The method scales efficiently to large models (Llama2-70B) with minimal compute requirements compared to existing unlearning approaches
- UCD can enhance existing unlearning baselines by using their outputs as approximate clean models when exact clean models are unavailable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The difference in token-level preferences between a model fine-tuned on forget data versus retain data encodes a signal that can suppress forgotten content when applied as a correction to a reference model.
- Mechanism: Two auxiliary models are trained separately—one fine-tuned on D_forget (A_corr) and one on D_retain (A_clean). Their logit difference Δ_A(y|x) = log A_corr(y|x) - log A_clean(y|x) captures how much more the forget-tuned model prefers a token. This difference is subtracted (scaled by α) from the reference model's logits: log P_aligned(y|x) ← log P_corr(y|x) - α·Δ_A(y|x). When Δ_A is large and positive (forget-tuned model strongly prefers y), the correction suppresses that token.
- Core assumption: The logit difference between A_corr and A_clean proportionally approximates the logit difference between the corrupted model P_corr and an ideal clean model P_clean (Proposition 1, Eq. 3).
- Evidence anchors:
  - [abstract]: "leveraging two auxiliary smaller models, one trained without the forget set and one trained with it, to guide the outputs of the original model using their difference during inference"
  - [section 3]: Eq. 1 defines the contrastive update; Section 3.1 Proposition 1 formalizes the proportionality assumption
  - [corpus]: Limited direct corpus validation. Related work "From Logits to Latents" similarly uses contrastive shaping in prediction space, but focuses on representation persistence rather than logit-level corrections.
- Break condition: If the proportionality in Eq. 3 fails substantially—i.e., auxiliary models do not generalize token preference trends from large models—UCD may under- or over-correct.

### Mechanism 2
- Claim: Operating at inference-time on logits rather than modifying weights avoids optimization challenges (local minima, symmetry-breaking) inherent to fine-tuning-based unlearning.
- Mechanism: UCD is a gray-box method requiring only forward passes through three models at inference. No gradients or weight updates are computed. The correction happens in token-space, directly modifying the sampling distribution.
- Core assumption: Inference-time distribution modification is sufficient for unlearning without needing persistent weight changes.
- Evidence anchors:
  - [section 7]: "it directly modifies token-level logits rather than model weights... naturally avoids common issues associated with weight-based optimization, such as multiple local minima and symmetry-breaking"
  - [section 3]: "no new model is computed; instead, only the logits—used to define the next-token distribution—are modified"
  - [corpus]: Not directly validated in corpus. Weight-based unlearning tradeoffs are noted in Shi et al. (MUSE benchmark) as cited in the paper.
- Break condition: If unlearning requires persistent model state changes (e.g., for deployment without auxiliary models), inference-time methods impose ongoing compute overhead.

### Mechanism 3
- Claim: Smaller auxiliary models can effectively proxy the contrastive signal that would be produced by full-sized clean/corrupted models, enabling scalability.
- Mechanism: Rather than requiring P_clean (the retrained-from-scratch 70B model), UCD uses much smaller auxiliaries (e.g., 7B models for a 70B reference). This is feasible when the auxiliaries preserve directional preference relationships even if magnitude differs.
- Core assumption: Smaller models trained on the same forget/retain partition exhibit proportional logit differences to larger models (Eq. 5 relaxes strict proportionality to bounded ratios).
- Evidence anchors:
  - [section 5.3]: "UCD scales effectively to even larger models, specifically Llama2-70B... only requiring 2 L40s for unlearning on Llama2-13B and 4 NVIDIA H200s for unlearning on Llama2-70B"
  - [section 3.1]: Eq. 5 defines approximate alignment condition with constants c1, c2 bounding the ratio
  - [corpus]: Limited corpus evidence. Neighbor papers do not address cross-scale auxiliary model transfer.
- Break condition: If small auxiliary models fail to capture the relevant forget/retain distinctions (e.g., insufficient capacity), the contrastive signal will be noisy or misaligned.

## Foundational Learning

- Concept: **Logits and Token Distributions**
  - Why needed here: UCD operates entirely on logit differences. Understanding how logits define next-token probability distributions is essential to grasp why subtracting a scaled difference suppresses or amplifies specific outputs.
  - Quick check question: Given logits [2.0, 1.0, 0.1] for three tokens, what are their softmax probabilities?

- Concept: **Contrastive Decoding**
  - Why needed here: The method builds on contrastive decoding (Li et al., 2023), where an "amateur" model's outputs are subtracted from an "expert" to improve generation. UCD repurposes this for unlearning.
  - Quick check question: In standard contrastive decoding, what is the role of the amateur model?

- Concept: **Forget Set vs. Retain Set**
  - Why needed here: The entire unlearning formulation depends on partitioning data into what should be forgotten versus preserved. The auxiliary models are trained on these distinct partitions.
  - Quick check question: If you accidentally include forget-set examples in your retain set, what unlearning behavior would you expect?

## Architecture Onboarding

- Component map:
  - Reference model (P_corr) -> Auxiliary model A_corr -> Auxiliary model A_clean -> Contrastive combiner

- Critical path:
  1. Identify D_forget and D_retain partitions from training data
  2. Train A_corr by fine-tuning base model A on D_forget
  3. Train A_clean by fine-tuning base model A on D_retain
  4. At inference, for each token step: compute logits from all three models, apply contrastive update, sample from adjusted distribution

- Design tradeoffs:
  - **α hyperparameter**: Controls correction strength. Paper finds optimal values 0.5-1.0 for TOFU, 1.0 for MUSE. Lower values under-correct; higher values risk over-suppression.
  - **UCD vs. UCS**: UCD allows both suppression and amplification. UCS (contrastive suppression) only applies positive Δ values, being more conservative when using approximate clean models.
  - **Clean model availability**: Exact clean models (trained without forget data) yield best results. Approximate clean models (from other unlearning methods like NPO+RT) can bootstrap but with degraded performance.
  - **Auxiliary model size**: Smaller auxiliaries reduce compute but may weaken signal quality. Paper uses 7B auxiliaries for 13B/70B reference models.

- Failure signatures:
  - **Over-unlearning**: Model utility drops excessively on retain set. Often caused by α too high or poor auxiliary model quality.
  - **Under-unlearning**: Forget quality remains detectable. α too low, or auxiliary models fail to capture forget/retain distinction.
  - **PrivLeak near ±100**: Indicates model is trivially distinguishable from retrained baseline (Table 13 shows α=0.01 gives PrivLeak=-100).
  - **OOM during auxiliary training**: Ensure D_forget and D_retain are sized appropriately for available GPU memory.

- First 3 experiments:
  1. **Alpha sweep**: On a held-out validation split, test α ∈ {0.01, 0.1, 0.5, 1.0}. Plot forget quality vs. model utility. Identify Pareto frontier.
  2. **Auxiliary model ablation**: Compare using same-size auxiliaries (if feasible) vs. smaller auxiliaries. Measure gap from retrained baseline.
  3. **Bootstrap test**: If no clean model is available, substitute best available unlearning method output (e.g., NPO+RT) as A_clean. Compare UCD vs. UCS performance to determine which correction mode is more robust.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What formal definition of successful unlearning is appropriate for inference-time methods that modify token distributions rather than model weights?
- Basis in paper: [explicit] "Our method operates solely at inference-time, leaving open questions around what precisely constitutes meaningful unlearning in generative models that go beyond equivalence in weights."
- Why unresolved: Traditional unlearning definitions require updated weights to match those from retraining from scratch, which does not translate to UCD's inference-only approach.
- What evidence would resolve it: A formal definition of unlearning for distributional modifications, with corresponding verification protocols or certification mechanisms.

### Open Question 2
- Question: Under what conditions does the proportionality assumption in Equation 3 hold approximately in practice, and how does violation of this assumption affect unlearning quality?
- Basis in paper: [inferred] Proposition 1 assumes strict proportionality between auxiliary and main model logit differences, but the paper notes "the strict proportionality in (3) may be too strong to hold exactly in practice."
- Why unresolved: The paper provides intuition but lacks characterization of when the approximation breaks down or its impact on performance.
- What evidence would resolve it: Theoretical analysis bounding approximation error, or empirical studies systematically varying model capacities and data distributions to identify failure modes.

### Open Question 3
- Question: How sensitive is UCD to tokenization mismatches between the reference and auxiliary models, and can this be mitigated?
- Basis in paper: [explicit] "Our approach requires careful matching of tokenization schemes between reference and auxiliary models; discrepancies here could degrade the quality of the unlearning results."
- Why unresolved: The paper acknowledges this limitation but does not quantify the sensitivity or propose solutions.
- What evidence would resolve it: Experiments with varying tokenization schemes, or development of alignment methods that handle tokenization discrepancies robustly.

## Limitations
- The core mechanism relies on an unproven proportionality assumption between auxiliary and main model logit differences
- Evaluation is limited to TOFU and MUSE benchmarks, which may not capture all real-world unlearning scenarios
- The method requires running three models at inference, imposing ongoing computational overhead

## Confidence
- **High confidence**: The basic mechanism of contrastive decoding (Mechanism 2) and the scalability demonstration (Mechanism 3) are well-supported by empirical results.
- **Medium confidence**: The effectiveness of using smaller auxiliary models to proxy large model behavior (Mechanism 3) is demonstrated but could benefit from more rigorous validation across model families.
- **Medium confidence**: The specific hyperparameters (α values, model sizes) that work best for TOFU and MUSE may not generalize to other unlearning scenarios.

## Next Checks
1. **Cross-model validation**: Test UCD with auxiliary models from different families (e.g., using Mistral-7B auxiliaries for Llama2-13B reference) to assess robustness of the proportionality assumption across architectures.

2. **Distribution shift analysis**: Evaluate UCD performance when reference model inference data distribution differs from auxiliary model training data, measuring degradation in forget quality and model utility.

3. **Long-context evaluation**: Assess UCD's effectiveness on tasks requiring extended context (beyond 4K tokens in TOFU) to determine if contrastive signals remain effective over longer sequences.