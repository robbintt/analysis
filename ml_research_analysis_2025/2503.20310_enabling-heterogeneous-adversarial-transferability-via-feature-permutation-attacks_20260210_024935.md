---
ver: rpa2
title: Enabling Heterogeneous Adversarial Transferability via Feature Permutation
  Attacks
arxiv_id: '2503.20310'
source_url: https://arxiv.org/abs/2503.20310
tags:
- adversarial
- feature
- transferability
- attacks
- permutation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring adversarial
  attacks across heterogeneous neural architectures (CNNs, Vision Transformers, and
  MLPs), where transferability significantly degrades due to fundamental architectural
  differences. The authors propose Feature Permutation Attack (FPA), a zero-FLOP,
  parameter-free method that enhances transferability by strategically rearranging
  pixel values in selected feature maps within CNNs.
---

# Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks

## Quick Facts
- arXiv ID: 2503.20310
- Source URL: https://arxiv.org/abs/2503.20310
- Authors: Tao Wu; Tie Luo
- Reference count: 40
- One-line primary result: Feature Permutation Attack (FPA) achieves up to 14.57% absolute gains in attack success rates for transferring adversarial examples across heterogeneous neural architectures.

## Executive Summary
This paper addresses the fundamental challenge of transferring adversarial attacks across heterogeneous neural architectures (CNNs, Vision Transformers, and MLPs), where traditional methods suffer significant performance degradation due to architectural differences. The authors propose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method that strategically rearranges pixel values in selected feature maps within CNNs. This operation simulates long-range dependencies, making CNNs behave more like ViTs and MLPs, thereby improving feature diversity and transferability. Extensive evaluations on 14 state-of-the-art architectures demonstrate that FPA achieves maximum absolute gains in attack success rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming existing black-box attacks while seamlessly integrating with other transfer-based attacks.

## Method Summary
FPA inserts a virtual feature permutation (FP) layer into CNN surrogates before convolution operations, rearranging pixel values in selected feature channels with probability p and ratio γ. The method implements two permutation strategies: FPA-N (neighborhood permutation) and FPA-R (random permutation). FPA-N swaps each pixel with one of its four neighbors, while FPA-R applies arbitrary shuffling. The permutation operation creates non-local connections in early layers where CNNs normally have limited receptive fields, simulating the long-range dependencies that ViTs and MLPs naturally model. The method is highly generalizable and can seamlessly integrate with other transfer-based attacks to further boost their performance, requiring no additional parameters or FLOPs.

## Key Results
- Maximum absolute ASR gains: 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs
- FPA outperforms existing black-box attacks across 14 state-of-the-art architectures
- FPA achieves zero additional computational overhead (no FLOPs or parameters)
- FPA-N (neighborhood permutation) shows robust performance across all hyperparameter settings
- FPA can be combined with other attacks (DIM, TIM) for additive performance gains

## Why This Works (Mechanism)

### Mechanism 1: Long-range dependency simulation
Rearranging feature map pixels simulates the long-range dependencies that ViTs and MLPs naturally model, improving cross-architecture transfer. The FP operation shuffles pixel values within selected feature channels before convolution, creating non-local connections in early layers where CNNs normally have limited receptive fields. When gradients backpropagate through these permuted features, the resulting adversarial perturbations incorporate global context that aligns with how ViTs (self-attention) and MLPs (fully-connected layers) process information.

### Mechanism 2: Feature-space diversity augmentation
Feature-space perturbations create diversity that reduces surrogate overfitting, analogous to input-space augmentation methods but with lower computational cost. Unlike DIM/TIM which transform input images, FPA permutes downsampled feature maps. Each iteration applies a different permutation pattern (stochastic via probability p), exposing the attack optimization to varied internal representations. This implicit ensemble effect prevents perturbations from over-specializing to the surrogate's specific feature patterns.

### Mechanism 3: Spatial coherence preservation
Neighborhood permutation (FPA-N) outperforms random permutation (FPA-R) by preserving local spatial coherence while still enabling long-range mixing. FPA-N swaps each pixel with one of its 4 neighbors; FPA-R shuffles arbitrarily. Neighborhood swaps maintain local texture patterns while gradually propagating information across the feature map through successive layers. Random shuffling disrupts spatial structure, requiring higher-layer insertion (smaller feature maps) to avoid destroying useful representations.

## Foundational Learning

- **Transfer-based black-box attacks**
  - Why needed here: FPA is designed as a plug-in enhancement for transfer attacks, not a standalone attack. Understanding the threat model (surrogate vs. target, white-box gradient access vs. black-box evaluation) is essential.
  - Quick check question: Can you explain why transfer attacks require gradient access to the surrogate but not the target?

- **Receptive fields and long-range dependencies**
  - Why needed here: The paper's central hypothesis is that CNNs' limited early-layer receptive fields cause transfer failure to global-attention architectures. You need to understand what receptive fields are and how they grow through depth.
  - Quick check question: In a standard ResNet, why does a neuron in conv2 see a smaller region of the input than a neuron in conv5?

- **Iterative FGSM and momentum (MI-FGSM)**
  - Why needed here: All experiments use I-FGSM as the base attack with 50 iterations. Understanding iterative gradient sign methods is prerequisite to seeing where FPA inserts its permutation operation.
  - Quick check question: What does the `sign()` function do in Equation 2, and why is momentum (MI-FGSM) commonly added?

## Architecture Onboarding

- **Component map**: Feature maps → FP Layer → Permutation operation (π) → Selected channels (γ ratio) → Neighborhood or random swap → Conv layer → Gradients

- **Critical path**:
  1. Select surrogate CNN (paper uses ResNet-50)
  2. Choose insertion layer (paper: conv_2 for FPA-N, conv_5 for FPA-R)
  3. Set γ and p (paper: FPA-N uses γ=0.6, p=0.5; FPA-R uses γ=0.3, p=0.2)
  4. Wrap base attack (I-FGSM, MI-FGSM, DIM, etc.) with FP operation in forward pass
  5. Run iterative attack; permutation is re-sampled each iteration
  6. Evaluate generated AEs on target models

- **Design tradeoffs**:
  - FPA-N vs. FPA-R: N is more robust to hyperparameter choices; R may work better at higher layers with smaller feature maps
  - γ (channel ratio): Higher γ increases diversity but risks destroying critical features; FPA-R needs lower γ
  - Insertion layer: Early layers have larger feature maps (more disruption potential); later layers have smaller maps (more global structure preserved)
  - FPA is architecture-agnostic but all experiments use CNN surrogates; ViT/MLP surrogate behavior is unexplored

- **Failure signatures**:
  - ASR drops below baseline: Check if γ is too high (over-disruption) or insertion layer is too early for FPA-R
  - No improvement over baseline: Verify permutation is being re-sampled each iteration (not fixed); check that FP layer is in the computational graph (gradients flow through it)
  - High variance across runs: Expected due to stochasticity; increase evaluation samples or average multiple runs

- **First 3 experiments**:
  1. **Baseline replication**: Reproduce Table 1 for a single target (e.g., ResNet-152) using FPA-N with MI-FGSM on ResNet-50 surrogate; verify ASR improvement matches paper (~+20%)
  2. **Layer ablation**: Test FPA-N inserted after conv_1 through conv_5 on a ViT target (e.g., ViT-B); plot ASR vs. layer position to confirm conv_2 is optimal or find task-specific optimum
  3. **Integration test**: Combine FPA-N with DIM (input transformation) on an MLP target (e.g., Mixer-B); verify additive gains per Table 2 (~+15-20% over DIM alone)

## Open Questions the Paper Calls Out
None

## Limitations
- Surrogate architecture dependence: All experiments use CNN surrogates (ResNet-50), leaving untested whether FPA works when using ViT or MLP surrogates
- Architectural generality: Effectiveness on smaller datasets or non-standard architectures remains unexplored
- Transfer gap quantification: Does not systematically quantify how much of the heterogeneous transfer gap stems from CNNs' local inductive bias versus other factors

## Confidence
- **High confidence**: The zero-FLOP, parameter-free nature of FPA and its integration methodology are clearly specified and reproducible
- **Medium confidence**: The core mechanism of using feature permutation to simulate long-range dependencies is theoretically sound and supported by experimental results
- **Low confidence**: Claims about FPA's effectiveness on non-ImageNet scales or with non-CNN surrogates are not substantiated by the presented evidence

## Next Checks
1. **Cross-surrogate validation**: Test FPA using ViT-Base as the surrogate against CNN targets (ResNet-152) to verify if the method provides similar gains in the reverse transfer direction
2. **Mechanism isolation**: Compare FPA against a baseline that applies random noise to the same feature channels (without permutation) to isolate whether the permutation operation specifically drives the improvements
3. **Architecture ablation**: Evaluate FPA on a diverse set of architectures including smaller models (MobileNet, EfficientNet) and non-standard architectures (RegNet, ConvNeXt) to test robustness across the CNN family before claiming heterogeneous transfer benefits