---
ver: rpa2
title: 'Low-Precision Training of Large Language Models: Methods, Challenges, and
  Opportunities'
arxiv_id: '2505.01043'
source_url: https://arxiv.org/abs/2505.01043
tags:
- training
- quantization
- precision
- low-precision
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of low-precision
  training techniques for large language models (LLMs), addressing the significant
  computational and memory costs associated with training these models. The authors
  systematically categorize existing methods based on numerical formats: fixed-point
  and integer-based, floating-point-based, and customized formats.'
---

# Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities

## Quick Facts
- arXiv ID: 2505.01043
- Source URL: https://arxiv.org/abs/2505.01043
- Authors: Zhiwei Hao; Jianyuan Guo; Li Shen; Yong Luo; Han Hu; Guoxia Wang; Dianhai Yu; Yonggang Wen; Dacheng Tao
- Reference count: 40
- Primary result: Systematic survey of low-precision training techniques for LLMs, categorizing methods by numerical format and identifying key challenges and future research directions.

## Executive Summary
This survey provides a comprehensive overview of low-precision training techniques for large language models (LLMs), addressing the significant computational and memory costs associated with training these models. The authors systematically categorize existing methods based on numerical formats: fixed-point and integer-based, floating-point-based, and customized formats. They discuss key components of training that can be optimized, including weights, activations, gradients, and optimizer states, while highlighting challenges such as quantization noise, numerical stability, and memory overhead. The survey also covers quantization-aware training (QAT) methods and system-level support frameworks.

## Method Summary
The survey systematically reviews low-precision training methods by organizing them according to numerical formats (fixed-point/integer, floating-point, customized) and their application to specific training components (weights, activations, gradients, optimizer states). For modern hardware like NVIDIA H100, the primary method highlighted is FP8 training using Transformer Engine with E4M3/E5M2 formats and delayed scaling strategies. Key implementation involves using framework-specific modules (e.g., transformer_engine.pytorch.Linear) and context managers for automatic quantization, with high-precision master weights and accumulation to maintain stability.

## Key Results
- Integer quantization (INT8) effectively reduces memory for weights and gradients with bounded accuracy loss when combined with proper scaling
- 8-bit and 4-bit floating-point formats (FP8, FP4) enable extreme efficiency gains on modern hardware while maintaining convergence
- Optimizer state compression, particularly for adaptive methods like Adam, is critical for memory reduction but remains challenging due to sensitivity to reduced precision

## Why This Works (Mechanism)

### Mechanism 1
Reducing numerical precision directly lowers memory footprint and enables hardware-accelerated computation, with bounded accuracy loss under specific conditions. Lower bit-width representations (e.g., FP32→FP16→FP8) halve memory per component. Hardware Tensor Cores provide higher throughput for low-precision matrix operations. The paper documents FP16/BF16 as widely adopted, with FP8 gaining traction on Hopper+ architectures. Core assumption: Quantization noise introduced by reduced precision remains within bounds the optimization process can absorb.

### Mechanism 2
Fine-grained scaling strategies (per-tensor, per-channel, per-block) recover accuracy by adapting to heterogeneous tensor distributions. Global scaling factors fail when tensor values span wide dynamic ranges. Block-level scaling (e.g., 128×128 for weights, 1×128 for activations in DeepSeek-V3) computes local maximums, reducing quantization error for outliers while maintaining low-precision arithmetic. Core assumption: The overhead of storing additional scaling factors is acceptable relative to memory savings from low-precision storage.

### Mechanism 3
Error compensation (stochastic rounding, error feedback) enables convergence at lower bit-widths by preserving gradient information across iterations. Deterministic rounding loses small gradient values to zero. Stochastic rounding maintains unbiased gradient expectations. Error feedback accumulates quantization residuals into subsequent updates, correcting systematic bias. Core assumption: Gradient statistics remain relatively stable across mini-batches, allowing error accumulation to average out noise.

## Foundational Learning

- **Concept**: Fixed-point vs. floating-point vs. integer representations
  - Why needed here: The paper's taxonomy is organized by numerical format; understanding bit allocation (exponent vs. mantissa) determines dynamic range and precision tradeoffs.
  - Quick check question: Given E4M3 (4-bit exponent, 3-bit mantissa), can you compute the representable range and explain why it suits forward passes better than E5M2?

- **Concept**: Gradient flow through non-differentiable operations
  - Why needed here: Quantization is discontinuous; the Straight-Through Estimator (STE) is the standard workaround for backpropagation through quantization functions.
  - Quick check question: Why does STE approximate ∂(quantize(x))/∂x as 1, and when does this approximation fail?

- **Concept**: Optimizer state memory structure (Adam: m, v)
  - Why needed here: Optimizer states consume 2-3× model parameter memory for Adam; the survey targets these specifically for compression.
  - Quick check question: For a 7B parameter model in BF16, how much memory do Adam's first and second moment states require?

## Architecture Onboarding

- **Component map**: Forward pass: Quantize weights → compute activations → store compressed activations for backward pass; Backward pass: Dequantize/quantize activations → compute gradients (weight gradients, activation gradients) → quantize gradients; Optimizer step: Quantize/dequantize optimizer states (momentum, variance) → update master weights (FP32 or BF16); Communication: Compress gradients/activations before all-reduce (distributed training)

- **Critical path**: Gradient quantization during backpropagation is the most sensitive; layer-wise gradients often exhibit multiple distributions along channel dimensions.

- **Design tradeoffs**: Precision vs. stability: Lower precision (INT4, FP4) requires finer-grained scaling but yields larger memory savings; Scaling granularity vs. overhead: Per-block scaling improves accuracy but stores more scaling factors; Master weight precision: FP32 master weights improve convergence but add memory; BF16 master weights are increasingly viable; Hardware compatibility: FP8 requires Hopper+; INT8 is widely supported; custom formats need specialized kernels

- **Failure signatures**: Gradient underflow: Loss plateaus, gradients become all-zeros → increase loss scaling or use higher precision for gradient accumulation; Divergence after warm-up: Error feedback accumulates bias → check error correction implementation or reduce compression ratio; Accuracy gap vs. baseline: Outliers dominate quantization error → apply Hadamard transform or increase scaling granularity; Numerical instability in LayerNorm/BatchNorm: Quantization noise amplifies → retain higher precision for normalization layers

- **First 3 experiments**: Establish BF16 baseline with native PyTorch AMP on target model; measure memory, throughput, and final accuracy; Enable FP8 training (Transformer Engine on H100, or FP8-LM framework) with automatic tensor-wise scaling; compare convergence curve and memory reduction; Apply INT8 gradient quantization with per-tensor scaling; introduce stochastic rounding if accuracy degrades >1%; measure communication bandwidth reduction in distributed setting

## Open Questions the Paper Calls Out

- **Open Question 1**: How can second-order statistics in adaptive optimizers be effectively compressed or quantized without sacrificing model convergence? Current methods struggle to maintain the stability provided by high-precision statistics when moving to low-bit representations, often leading to divergence.

- **Open Question 2**: What are the theoretical convergence guarantees and generalization bounds for ultra-low precision (4-bit or 2-bit) training? While empirical success exists, rigorous theory explaining why and when extreme quantization works is currently lacking.

- **Open Question 3**: Can low-precision training be synergistically combined with other efficiency techniques like pruning or low-rank approximation? The interaction between reduced numerical precision and the sparsity or structural changes induced by other efficiency methods is complex and potentially destabilizing.

## Limitations
- Performance gains vary significantly across model sizes and datasets, with extreme quantization showing much wider accuracy gaps
- Hardware coverage is uneven, with sparse coverage of alternative platforms and emerging custom formats
- Long-term stability and convergence proofs for many ultra-low-precision methods are still theoretical or based on limited experiments

## Confidence
- **High Confidence**: Claims about FP16/BF16 training effectiveness, supported by extensive production adoption and consistent empirical validation
- **Medium Confidence**: Claims regarding INT8 quantization for weights and gradients, which show good results but exhibit sensitivity to model architecture
- **Low Confidence**: Claims about FP4 and ultra-low-precision training, where only preliminary results exist from a few specialized implementations

## Next Checks
1. Systematically compare per-tensor vs. per-block scaling for a 7B parameter model across multiple architectures to quantify accuracy-cost tradeoffs
2. Implement identical low-precision training pipelines across different hardware platforms (H100, MI300X, Trainium) to measure performance portability
3. Conduct extended training experiments comparing standard precision, FP16/BF16, and INT8 methods on identical models to validate long-term convergence