---
ver: rpa2
title: Large Language Model Based Multi-Agent System Augmented Complex Event Processing
  Pipeline for Internet of Multimedia Things
arxiv_id: '2501.00906'
source_url: https://arxiv.org/abs/2501.00906
tags:
- agents
- system
- agent
- video
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a proof-of-concept for a Large Language Model
  (LLM) based multi-agent system augmented with a publish/subscribe (pub/sub) paradigm
  to address limitations in current complex event processing (CEP) systems. The system
  integrates the Autogen LLM orchestration framework with Kafka message brokers to
  create an autonomous CEP pipeline capable of handling complex workflows, particularly
  for video query processing.
---

# Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things

## Quick Facts
- arXiv ID: 2501.00906
- Source URL: https://arxiv.org/abs/2501.00906
- Reference count: 40
- LLM-based multi-agent system with Kafka pub/sub for video query processing

## Executive Summary
This paper presents a proof-of-concept for a Large Language Model (LLM) based multi-agent system augmented with a publish/subscribe (pub/sub) paradigm to address limitations in current complex event processing (CEP) systems. The system integrates the Autogen LLM orchestration framework with Kafka message brokers to create an autonomous CEP pipeline capable of handling complex workflows, particularly for video query processing. Extensive experiments evaluated system performance across varying agent counts and video complexities. Results showed that while higher agent counts and video complexities increase latency, the system maintains high consistency in narrative coherence. The optimal configuration for this use case was four agents, balancing functionality and performance. The system demonstrates strong integration with existing tools and technologies, showcasing potential for easy adoption of LLM multi-agent systems in distributed AI applications.

## Method Summary
The method implements an LLM-based multi-agent system (LLM-MAS) augmented Complex Event Processing (CEP) pipeline for video query processing using the Autogen framework with Kafka message brokers. The system configures Autogen agents (User Proxy Agent, Conversable Agent, Engineer Agents) with external tool calls for Kafka consumption, frame extraction, video creation, and GPT-4o model inference. Video streams from multiple cameras are published to Kafka topics, with agents subscribing to relevant topics. The User Proxy Agent serves as the entry point, relaying user queries and receiving final responses. The Reflection Agent manages Q/A, detects keywords, and routes to engineers. Engineer Agents execute tool calls including Kafka consume, frame extraction, video reconstruction, and GPT-4o vision-language model inference. The workflow processes videos of varying complexity levels (1-5) at different resolutions (1080p, 720p, 360p, 144p) to evaluate system performance.

## Key Results
- 4-agent configuration provides optimal balance between functionality and latency (22-25s) for video query processing
- Video complexity significantly impacts accuracy, with Level 5 videos scoring 0.6 correctness versus 0.8-0.9 for Levels 1-3
- Resolution reduction from 1080p to 144p approximately halves GPT model call latency
- Pub/sub message brokers enable real-time data source integration for LLM agents via topic subscription

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-agent role specialization enables complex video query workflows that single agents cannot sustain.
- **Mechanism:** The Autogen framework orchestrates distinct agent types—User Proxy (human interface), Engineer (tool execution), and Reflection (Q/A coordination)—each handling a specialized subset of the workflow. Agents maintain shared context and delegate tasks based on role boundaries.
- **Core assumption:** Task decomposition into specialized roles yields better workflow continuity than monolithic agent behavior.
- **Evidence anchors:**
  - [abstract] "Utilizing the Autogen framework...the system demonstrates an autonomous CEP pipeline capable of handling complex workflows."
  - [section 2.4.1] Defines three foundation agent types with distinct capabilities.
  - [corpus] Weak direct corpus support for this specific Autogen-Kafka integration; related work shows LLM-MAS for planning workflows (Section 3.3.1) but not pub/sub CEP.
- **Break condition:** With fewer than 3 agents, the system cannot maintain conversational continuity beyond initial prompts (Section 5.2).

### Mechanism 2
- **Claim:** Pub/sub message brokers (Kafka) can serve as real-time data sources for LLM agents via topic subscription.
- **Mechanism:** Tool-backed agents execute pre-defined functions to consume frames from Kafka topics (e.g., "camera-1"), decoupling data producers (cameras) from LLM consumers. This leverages existing pub/sub infrastructure without modification.
- **Core assumption:** Frame-level granularity and near-real-time consumption are sufficient for video query use cases.
- **Evidence anchors:**
  - [abstract] "integrates...Autogen LLM orchestration framework with Kafka message brokers"
  - [section 4.3] Describes edge instances as message brokers forwarding frames to agents via topic subscription.
  - [corpus] Related paper "Real-Time Health Analytics Using Ontology-Driven CEP and LLM Reasoning" demonstrates CEP+LLM integration but in healthcare, not video.
- **Break condition:** If frame extraction or video creation tools fail, the workflow terminates (Appendix shows tool call chain dependencies).

### Mechanism 3
- **Claim:** Keyword-driven speaker selection enables controlled state transitions in multi-agent workflows.
- **Mechanism:** Custom speaker selection replaces Autogen's default Auto/Round-Robin modes. Keywords like "RECHECK" or "QUERY" in agent outputs trigger deterministic transitions to specific agents (Figure 9), reducing conversational drift.
- **Core assumption:** Keywords can be reliably generated and detected in agent outputs to drive state transitions.
- **Evidence anchors:**
  - [section 5.2.1] "The process begins with the User Proxy Agent...If the 'RECHECK' keyword is detected, the Reflection Agent selects Engineer-1 Agent as next speaker."
  - [section 5.6] Notes that keyword-based state machines become unmanageable beyond ~4 agents.
  - [corpus] No direct corpus evidence; this appears novel to this POC.
- **Break condition:** As agent count grows, coding all possible keyword-triggered states becomes impractical (Section 5.6 limitation).

## Foundational Learning

- **Concept: Publish/Subscribe Decoupling**
  - **Why needed here:** Understanding how Kafka topics decouple video producers from LLM consumers is prerequisite to grasping the system's scalability model.
  - **Quick check question:** If camera-1 publishes to topic "camera-1" and an agent subscribes, what happens if the agent crashes—does the camera detect this?

- **Concept: Conversable Agent State Machines**
  - **Why needed here:** Autogen agents maintain conversation history and context; understanding state persistence explains why context loss occurs with more agents (Section 5.6).
  - **Quick check question:** What context does the Reflection Agent retain between user queries, and how does it differ from the Engineer Agent's context?

- **Concept: Latency-Functionality Trade-offs in LLM-MAS**
  - **Why needed here:** The paper's core finding is that agent count and video complexity increase latency non-linearly.
  - **Quick check question:** If you double the number of agents from 2 to 4, does latency double? What does Table 1 suggest?

## Architecture Onboarding

- **Component map:**
  - User Proxy Agent -> Reflection Agent -> Engineer Agent -> Kafka consume -> Frame extraction -> Video reconstruction -> GPT-4o inference -> User Proxy Agent

- **Critical path:** User query → User Proxy → Reflection Agent → Engineer Agent → Kafka consume (topic subscription) → Frame extraction → Video reconstruction → GPT-4o inference → Response → User Proxy → User.

- **Design tradeoffs:**
  - **Agent count vs. latency:** 2 agents = 5-8s latency (incomplete workflow); 4 agents = 22-25s latency (full functionality). Optimal for this use case: 4 agents.
  - **Video resolution vs. processing time:** 1080p ~2× latency of 144p (Figure 16-20).
  - **Complexity vs. accuracy:** Level 5 videos score 0.6 correctness vs. 0.8-0.9 for Levels 1-3 (Table 3).
  - **Pre-defined tools vs. dynamic code generation:** Paper uses pre-defined tools to avoid security risks and breaking changes.

- **Failure signatures:**
  - **Tool not found:** Engineer Agent calls `reanalyze()` which doesn't exist → error propagated to Reflection Agent (Figure 21).
  - **Context loss:** Information from initial prompts lost when third agent generates dynamic prompts (Section 5.6).
  - **Speaker selection drift:** With >4 agents using Auto/Round-Robin, workflow becomes unpredictable (Section 5.2.1).
  - **Inconsistent accuracy drops:** Complex videos (Level 4-5) show 0.5-0.6 detailed orientation scores.

- **First 3 experiments:**
  1. **Baseline latency test:** Run 2-agent, 3-agent, and 4-agent configurations with the same video/prompt. Measure total latency and agent overhead. Expect: 2-agent fails to complete; 4-agent ~22-25s.
  2. **Video complexity sweep:** Process 5 complexity levels (Table 2) at 1080p with 2-agent system. Score correctness, detail, context, temporal, consistency per Section 5.1.1. Expect: Scores decline from Level 1 → Level 5.
  3. **Resolution impact test:** For each complexity level, process at 1080p, 720p, 360p, 144p. Plot GPT model call latency. Expect: ~2× reduction from 1080p to 144p across all levels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can inter-agent communication protocols be optimized to mitigate the latency overhead associated with increasing the number of agents beyond four?
- **Basis in paper:** [Explicit] The conclusion states, "Enhancing inter-agent communication efficiency is a primary focus, aiming to reduce latency and improve overall system performance without sacrificing functionality."
- **Why unresolved:** The experiments demonstrated that increasing agent count from 2 to 4 caused "Agent Overhead" to rise from 1–2 seconds to 12–16 seconds, a trade-off currently managed by capping the system at four agents.
- **What evidence would resolve it:** A comparative analysis of communication protocols showing stable or reduced latency in a 5+ agent configuration compared to the current Autogen baseline.

### Open Question 2
- **Question:** To what extent can federated learning and edge computing be integrated into this CEP architecture to improve scalability and reduce reliance on centralized cloud instances?
- **Basis in paper:** [Explicit] Section 6 states, "Future research should explore the integration of these technologies to create a more distributed and efficient CEP framework."
- **Why unresolved:** The authors note in Section 5.6 that the current implementation "does not fully leverage the potential of federated learning and edge computing," limiting the system to centralized processing.
- **What evidence would resolve it:** A modified architectural deployment where edge instances process video frames locally, demonstrating reduced latency or bandwidth usage compared to the centralized cloud setup.

### Open Question 3
- **Question:** How can the system manage speaker selection and state transitions in high-complexity workflows without relying on manually coded keyword-based finite state machines?
- **Basis in paper:** [Inferred] Section 5.6 notes that as workflows expand, "coding all possible states depending on keywords becomes a challenging task," identifying a "practical limit" to workflow complexity.
- **Why unresolved:** The current optimization relies on a reflection agent detecting specific hardcoded keywords (e.g., "RECHECK") to determine the next speaker, which is brittle for dynamic or unforeseen scenarios.
- **What evidence would resolve it:** A dynamic routing mechanism or learning-based agent that successfully orchestrates complex workflows without explicit manual state definitions.

## Limitations
- Agent prompt configuration and role specifications are not provided, creating uncertainty in reproducing workflow state transitions
- Kafka integration details (frame publishing frequency, serialization format, topic partitioning) are unspecified
- Tool implementation boundaries and execution overhead are not detailed
- Video complexity calibration mapping is not fully specified

## Confidence
- **High confidence:** Keyword-driven speaker selection for controlled state transitions in multi-agent workflows (Section 5.2.1)
- **Medium confidence:** 4-agent configuration provides optimal balance between functionality and latency for this video query use case
- **Low confidence:** Scalability of this approach beyond 4 agents due to keyword-based state machine limitations

## Next Checks
1. **Agent count threshold validation:** Systematically test 2, 3, 4, and 5-agent configurations with identical video prompts to verify the claimed workflow completion failure at 2 agents and identify the precise agent count threshold where speaker selection drift becomes problematic.
2. **Real-time performance stress test:** Implement the Kafka pub/sub integration and measure end-to-end latency with varying frame extraction sampling rates (0.5fps, 1fps, 2fps) to validate the claimed scalability benefits of the pub/sub architecture.
3. **Video complexity calibration replication:** Reconstruct the video complexity levels using the provided criteria (Table 2) and validate that Level 5 videos consistently score below 0.6 in correctness while Levels 1-3 maintain 0.8+ scores.