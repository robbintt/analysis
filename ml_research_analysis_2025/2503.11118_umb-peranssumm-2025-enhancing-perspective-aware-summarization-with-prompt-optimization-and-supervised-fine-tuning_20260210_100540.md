---
ver: rpa2
title: 'UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt
  Optimization and Supervised Fine-Tuning'
arxiv_id: '2503.11118'
source_url: https://arxiv.org/abs/2503.11118
tags:
- prompt
- perspective
- dspy
- optimization
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach for perspective-aware summarization
  in community question-answering (CQA) threads. The authors use ensemble learning
  combining BERT, RoBERTa, and DeBERTa models for perspective span identification,
  achieving 82.9% F1-score on the test set.
---

# UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt Optimization and Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2503.11118
- Source URL: https://arxiv.org/abs/2503.11118
- Reference count: 9
- Achieved 82.9% F1-score on perspective span identification test set

## Executive Summary
This paper presents an approach for perspective-aware summarization in community question-answering (CQA) threads. The authors use ensemble learning combining BERT, RoBERTa, and DeBERTa models for perspective span identification, achieving 82.9% F1-score on the test set. For summarization, they design Chain-of-Thought (CoT) prompting strategies that incorporate keyphrases and guide information, then apply DSPy framework for prompt optimization and supervised fine-tuning (SFT) on Llama-3. Their experimental results show that CoT prompting with keyphrases and guidance improves summary alignment with references, while combining DSPy prompt optimization with SFT yields significant improvements in both relevance (ROUGE-L: +21.4%, Meteor: +7.0%, BLEU: +8.8%) and factuality metrics.

## Method Summary
The approach consists of two main components: perspective span identification and perspective-aware summarization. For span identification, three transformer models (BERT, RoBERTa, DeBERTa) are fine-tuned and combined through ensemble averaging of probability distributions. For summarization, a Llama-3 model is guided through a 4-step Chain-of-Thought prompting process that extracts keyphrases, integrates them with guide information, and generates summaries. The prompt is then optimized using DSPy's 0-shot MIPRO optimizer with a composite metric, and the model is further adapted through supervised fine-tuning with LoRA for domain-specific performance.

## Key Results
- Ensemble model achieves 82.9% F1-score on perspective span identification test set
- CoT prompting with keyphrases and guidance improves ROUGE-1 by +8.1% and BERTScore by +1.5%
- DSPy optimization combined with SFT yields +21.4% ROUGE-L, +7.0% Meteor, and +8.8% BLEU improvements
- Factuality metrics show mixed results, with AlignScore slightly decreasing after SFT

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Averaging for Span Identification
The ensemble computes final predictions by averaging probability distributions from BERT, RoBERTa, and DeBERTa models, exploiting their different error patterns and attention biases. This reduces variance and improves generalization on unseen test data. The core assumption is that individual models make uncorrelated errors, so their weaknesses don't compound when averaged. The ensemble achieved 82.9% F1-score, with RoBERTa excelling at strict matching and DeBERTa performing better in proportional matching.

### Mechanism 2: Structured Chain-of-Thought (CoT) Prompting
Decomposing perspective-aware summarization into explicit reasoning steps improves alignment with reference summaries. The sequential process forces the model to ground its summary generation in extracted keyphrases and predefined perspective-specific guidance, reducing hallucination and ensuring output conforms to specific perspective types. This grounding acts as an in-context control signal. The approach improved ROUGE-1 by +8.1% and BERTScore by +1.5%.

### Mechanism 3: Combined DSPy Optimization and Supervised Fine-Tuning (SFT)
Combining automated prompt optimization with supervised fine-tuning yields additive improvements in relevance and factuality metrics. DSPy's 0-shot MIPRO optimizer iteratively refines the prompt to maximize a custom objective function composed of multiple relevance metrics. Separately, SFT (via LoRA) updates model weights to adapt to the medical CQA domain. The combination applies both better input instruction and better-adapted model, achieving +21.4% ROUGE-L, +7.0% Meteor, and +8.8% BLEU improvements.

## Foundational Learning

**Concept: Transformer Ensemble Averaging**
- Why needed: Critical for understanding the span identification component where multiple models are combined
- Quick check: If Model A predicts 0.9, Model B predicts 0.1, and Model C predicts 0.5 for a class, what is the ensemble's prediction? (Answer: (0.9+0.1+0.5)/3 = 0.5)

**Concept: Chain-of-Thought (CoT) Prompting**
- Why needed: The core summarization strategy relies on a 4-step CoT process
- Quick check: In a CoT prompt, why is it beneficial to ask the model to "extract keyphrases" before asking it to "generate a summary"? (Answer: It forces the model to attend to important details first, grounding the final output)

**Concept: Low-Rank Adaptation (LoRA) for SFT**
- Why needed: The paper uses LoRA for efficient fine-tuning of Llama-3
- Quick check: How does LoRA reduce computational cost during fine-tuning compared to full parameter updates? (Answer: By freezing pre-trained weights and training only small, low-rank adapter matrices)

## Architecture Onboarding

**Component map**: The system has two main pipelines. 1) **Span Identification Pipeline**: Input CQA Thread -> Three Transformers (BERT, RoBERTa, DeBERTa) fine-tuned on the task -> Average Probabilities -> Predicted Spans. 2) **Summarization Pipeline**: Input CQA Thread + Predicted Spans -> Llama-3 + CoT Prompt Template (with Keyphrases and Guide) -> (Optional) DSPy Optimized Prompt -> (Optional) SFT-Adapted Llama-3 Model -> Final Perspective-Aware Summary.

**Critical path**: The most critical path for performance is the Summarization Pipeline, specifically the integration of DSPy optimization and SFT. The span identification provides the input, but the summarizer's quality dictates the final scores. The prompt template (Figure 3) is the central artifact.

**Design tradeoffs**:
- Ensemble vs. Single Model: The ensemble offers more stable performance but is 3x more computationally expensive at inference. It did not outperform the best single model (RoBERTa) on all metrics.
- DSPy vs. Manual Prompting: DSPy automates optimization but requires defining a good objective metric. The authors chose equal weights, which is an untested assumption.
- SFT Impact: SFT substantially boosts relevance (ROUGE-L +21.4%) but has a mixed impact on factuality, suggesting a tradeoff between stylistic alignment and factual consistency.

**Failure signatures**:
- Span ID: Low proportional match F1 but high strict match F1 indicates models are precise but fail to capture full spans
- Summarization: High ROUGE scores but low AlignScore/SummaC indicates fluent, hallucinated summaries that are not grounded in the source
- DSPy: A performance drop on test set vs. validation suggests the optimizer found a prompt that overfits the validation data

**First 3 experiments**:
1. **Reproduce Baselines**: Train individual transformers (BERT, RoBERTa, DeBERTa) and run vanilla prompting with Llama-3 to establish baseline F1 and ROUGE scores
2. **Ablate CoT Components**: Test the full CoT prompt, then remove keyphrase extraction, then remove guide information, to measure the contribution of each component
3. **Analyze SFT vs. DSPy**: Compare SFT alone, DSPy optimization alone, and their combination on the test set. Track both relevance (ROUGE-L) and factuality (AlignScore) to confirm reported tradeoffs

## Open Questions the Paper Calls Out

**Open Question 1**: Would alternative DSPy optimizers (MIPRO with bootstrapped demonstrations or OPRO) yield significant improvements over the 0-shot MIPRO optimizer for perspective-aware summarization? The study only implemented the 0-shot MIPRO optimizer within computational budget constraints without comparing to other available optimizers.

**Open Question 2**: Can learned or weighted metric combinations outperform the equal-weight (0.25 each) formulation for DSPy prompt optimization? The balanced metric assigns equal weights to ROUGE-L, BLEU, Meteor, and BERTScore based on empirical assumptions without validation.

**Open Question 3**: How does the prompt optimization strategy generalize to non-medical CQA domains or other summarization tasks? The CoT prompts and guide information were specifically tailored to medical CQA forums.

**Open Question 4**: Why does supervised fine-tuning improve relevance metrics (ROUGE-L: +21.4%) while having limited impact on factuality metrics (AlignScore, SummaC)? The mechanism by which SFT affects these metric categories differently is not investigated.

## Limitations

- Limited domain-specific validation: Effectiveness has not been validated on other medical CQA datasets or different domains
- Factuality vs. relevance tradeoff: SFT improves relevance metrics substantially but has less positive impact on factuality, suggesting fundamental tension
- Missing technical specifications: Critical implementation details including LoRA configuration parameters and exact DSPy optimization iterations are absent

## Confidence

**High confidence**: The ensemble approach for perspective span identification achieving 82.9% F1-score is well-supported with clear methodology and reasonable baseline comparisons.

**Medium confidence**: The claim that CoT prompting with keyphrases and guidance improves summary alignment is supported by experimental results showing +8.1% ROUGE-1 and +1.5% BERTScore improvements, but the exact contribution of each CoT component is not isolated.

**Medium confidence**: The combined DSPy optimization and SFT showing significant improvements (+21.4% ROUGE-L, +7.0% Meteor, +8.8% BLEU) is supported by validation set results, but the effect on the hidden test set and the correlation between the composite optimization metric and actual summary quality remain unverified.

## Next Checks

1. **Ablation study of CoT components**: Systematically remove each component of the Chain-of-Thought prompt (keyphrase extraction, guide information, integration step) to quantify their individual contributions to the reported performance improvements.

2. **Test set evaluation of DSPy optimization**: Evaluate whether the prompt optimized by DSPy on the validation set maintains its performance gains on the hidden test set, checking for overfitting to validation data during the iterative optimization process.

3. **Factuality-relevance tradeoff analysis**: Conduct a controlled experiment comparing SFT-adapted models with and without factuality constraints during training to determine if the observed drop in AlignScore is an inherent limitation of the approach or can be mitigated through modified training objectives.