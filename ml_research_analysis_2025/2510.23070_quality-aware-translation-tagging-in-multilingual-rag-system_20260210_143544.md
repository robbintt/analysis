---
ver: rpa2
title: Quality-Aware Translation Tagging in Multilingual RAG system
arxiv_id: '2510.23070'
source_url: https://arxiv.org/abs/2510.23070
tags:
- translation
- quality
- language
- query
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quality-Aware Translation Tagging in Multilingual RAG system (QTT-RAG)
  introduces an LLM-driven quality assessment module that evaluates translation reliability
  across three dimensions-semantic equivalence, grammatical accuracy, and naturalness
  & fluency-and attaches these scores as metadata without modifying original content.
  Evaluated across six instruction-tuned LLMs (2.4B-14B parameters) on two multilingual
  QA benchmarks (XOR-TyDi, MKQA) in Korean, Finnish, and Chinese, QTT-RAG consistently
  improves character 3-gram recall compared to CrossRAG and DKM-RAG baselines, with
  gains ranging from 0.4% to 6.8% in low-resource settings.
---

# Quality-Aware Translation Tagging in Multilingual RAG system

## Quick Facts
- arXiv ID: 2510.23070
- Source URL: https://arxiv.org/abs/2510.23070
- Reference count: 16
- QTT-RAG consistently improves character 3-gram recall compared to CrossRAG and DKM-RAG baselines, with gains ranging from 0.4% to 6.8% in low-resource settings.

## Executive Summary
Quality-Aware Translation Tagging in Multilingual RAG system (QTT-RAG) introduces an LLM-driven quality assessment module that evaluates translation reliability across three dimensions—semantic equivalence, grammatical accuracy, and naturalness & fluency—and attaches these scores as metadata without modifying original content. Evaluated across six instruction-tuned LLMs (2.4B-14B parameters) on two multilingual QA benchmarks (XOR-TyDi, MKQA) in Korean, Finnish, and Chinese, QTT-RAG consistently improves character 3-gram recall compared to CrossRAG and DKM-RAG baselines, with gains ranging from 0.4% to 6.8% in low-resource settings. The approach preserves factual integrity by enabling generator models to prioritize higher-quality translations, demonstrating particular effectiveness in low-resource languages where translation quality is heterogeneous.

## Method Summary
QTT-RAG is a multilingual RAG framework that enhances retrieval-augmented generation by attaching LLM-evaluated quality scores to translated documents. The system uses a multilingual retriever (BGE-M3) to fetch passages, translates non-English documents using NLLB-200-600M, and evaluates translation quality across three dimensions using Llama-3.1-8B-Instruct. These quality scores are embedded in the generation prompt as structured metadata, allowing the generator to prioritize high-quality translations without modifying the original content. This approach addresses hallucinations introduced by rewriting-based refinement methods while preserving all translated documents.

## Key Results
- QTT-RAG achieves consistent improvements in character 3-gram recall across all tested languages and model sizes compared to CrossRAG and DKM-RAG baselines
- Performance gains are particularly pronounced in low-resource languages (Korean, Finnish) with translation quality heterogeneity
- The approach maintains factual integrity by avoiding rewriting-induced entity hallucination, which affected 11.5% of cases in baseline rewriting methods
- Higher gains observed when cross-lingual share (r_lang) is larger, indicating greater utility in heterogeneous translation quality scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attaching translation quality scores as metadata enables generator models to prioritize reliable sources without modifying content.
- Mechanism: An LLM-based quality assessor scores translations along three dimensions (semantic equivalence, grammatical accuracy, naturalness & fluency). These scores are injected into the generation prompt as structured metadata, allowing the generator to up-weight high-quality translations and down-weight low-quality ones.
- Core assumption: The generator can reliably interpret and follow quality-based prioritization instructions.
- Evidence anchors:
  - [abstract] "attaches these scores as metadata without modifying original content... enabling generator models to prioritize higher-quality translations."
  - [section 3.3] "This tagging approach preserves and fully utilizes all translated documents while providing the quality information to guide the generation model."
  - [corpus] Limited direct evidence in neighboring corpus for this specific tagging mechanism.
- Break condition: If the generator has weak instruction-following capabilities or ignores metadata, quality tags will not influence generation.

### Mechanism 2
- Claim: Non-destructive quality tagging avoids the factual distortion introduced by rewriting-based refinement.
- Mechanism: Rewriting-based methods (e.g., DKM-RAG) modify translated content, which can insert query-relevant entities not present in the source. QTT-RAG preserves original translations and exposes quality signals instead of altering content.
- Core assumption: The quality assessor itself does not introduce hallucinations when evaluating translations.
- Evidence anchors:
  - [abstract] "Existing approaches... utilize the rewriting method, which introduces factual distortion and hallucinations. To mitigate these problems, we propose... QTT-RAG."
  - [section 4.3] "In 214 cases (11.5%), entities in the query... that were absent from the original documents are added during rewriting."
  - [corpus] No direct corpus comparison between quality tagging and rewriting approaches.
- Break condition: If the quality assessor is unreliable or miscalibrated, tagging may not reduce hallucinations.

### Mechanism 3
- Claim: Multi-dimensional quality scoring captures heterogeneous translation failure modes in low-resource settings.
- Mechanism: Semantic equivalence, grammatical accuracy, and fluency scores provide orthogonal signals (meaning preservation, syntactic correctness, naturalness). This granularity helps the generator selectively rely on translations that are strong along relevant dimensions.
- Core assumption: The three dimensions are sufficiently independent and comprehensive for detecting key failure modes.
- Evidence anchors:
  - [abstract] "evaluates translation reliability across three dimensions-semantic equivalence, grammatical accuracy, and naturalness & fluency."
  - [section 3.3] "Each criterion is scored based on the ELO rating system from 0.0 to 5.0."
  - [corpus] No corpus validation for the specific three-dimensional scheme.
- Break condition: If dimensions are highly correlated or miss critical failure types, the scoring may not guide generation effectively.

## Foundational Learning

- Concept: Language preference bias in multilingual RAG
  - Why needed here: Explains why matching retrieved documents to the query language improves generation quality, motivating translation.
  - Quick check question: Why does generation degrade when retrieved passages are in a different language than the query?

- Concept: Cross-lingual document translation vs query translation
  - Why needed here: Justifies the architectural choice to translate documents rather than queries.
  - Quick check question: Which approach preserves cultural knowledge and word sense boundaries better in mRAG?

- Concept: Knowledge drift in LLM rewriting
  - Why needed here: Helps recognize why rewriting translated passages can introduce factual distortions and hallucinations.
  - Quick check question: How can rewriting inadvertently add query entities absent from the original source?

## Architecture Onboarding

- Component map:
  Retrieval (BGE-M3) → Reranking (BGE-M3) → Language Detection → Translation (NLLB-200-600M) → Quality Tagging (Llama-3.1-8B-Instruct) → Generation (instruction-tuned LLM)

- Critical path:
  Quality tagging must complete before generation; tagged documents are passed via structured prompt to the generator. Latency is added by translation + quality assessment steps.

- Design tradeoffs:
  Quality tagging vs hard filtering: Tagging preserves coverage (useful when high-quality translations are sparse); hard filtering simplifies input but risks discarding rare critical information and is threshold-sensitive.
  Latency vs reliability: Adding LLM-based quality assessment increases inference cost but avoids rewriting-induced hallucinations.

- Failure signatures:
  Low cross-lingual share (r_lang) in high-resource query languages (e.g., Chinese) yields smaller QTT-RAG gains.
  Generators with weak instruction-following or limited context windows may not fully exploit quality tags.
  Miscalibrated quality assessor leads to misleading tags.

- First 3 experiments:
  1. Replicate baseline comparison (Base, CrossRAG, DKM-RAG) using character 3-gram recall on XOR-TyDi–ko and MKQA–ko to validate reported gains.
  2. Run ablation comparing Hard filtering (threshold 3.5 across all dimensions) vs QTT-RAG tagging to measure coverage/accuracy tradeoff.
  3. Measure cross-lingual share (r_lang = N_translated / N_input) for Korean, Finnish, and Chinese splits to correlate with QTT-RAG effectiveness and identify deployment boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the QTT-RAG framework maintain its effectiveness and robustness when evaluated across a more diverse set of languages?
- **Basis in paper:** [explicit] The "Limitations" section states that experiments were restricted to Korean, Finnish, and Chinese, and "Further experiments on a more diverse set of languages are required."
- **Why unresolved:** The current evaluation covers only three languages, which may not capture the full variability of translation quality or structural differences in extremely low-resource or morphologically distinct languages.
- **What evidence would resolve it:** Empirical results from benchmarks including a wider array of languages, particularly those with non-Latin scripts or drastically different syntactic structures from English.

### Open Question 2
- **Question:** How does a hybrid retrieval strategy (e.g., English-only retrieval for non-English queries) impact the performance of QTT-RAG?
- **Basis in paper:** [explicit] The "Discussion" section explicitly lists plans to "explore hybrid retrieval strategies, such as deliberately inducing cross-lingual usage via English-only retrieval for non-English queries."
- **Why unresolved:** Current experiments use a multilingual retriever (BGE-M3). It is unclear if forcing cross-lingual retrieval to maximize the utility of the translation tagging mechanism would improve overall recall or degrade retrieval relevance.
- **What evidence would resolve it:** A comparative study measuring character 3-gram recall when QTT-RAG is coupled with an English-only retrieval index versus a multilingual index.

### Open Question 3
- **Question:** To what extent does the generator model's instruction-following capability moderate the effectiveness of quality tagging?
- **Basis in paper:** [inferred] The "Limitations" section notes that "models with weaker instruction-following capabilities... may fail to fully exploit these quality cues."
- **Why unresolved:** While the study tests models of different sizes (2.4B to 14B), it does not isolate "instruction-following ability" as a variable to determine the minimum threshold required for the generator to correctly interpret and prioritize the quality metadata.
- **What evidence would resolve it:** An analysis correlating specific instruction-following benchmark scores of various generators with their respective performance gains using QTT-RAG.

## Limitations

- Evaluation restricted to three low-resource languages (Korean, Finnish, Chinese), limiting generalizability to high-resource or morphologically diverse languages
- Quality assessment relies on a single LLM model without validation of assessor robustness or ensemble approaches
- Character 3-gram recall metric may not fully capture improvements in factual accuracy or end-user utility
- No evidence provided about computational overhead or latency implications of adding quality assessment to the pipeline

## Confidence

**High Confidence**: The core claim that attaching translation quality scores as metadata can improve generation quality in low-resource settings is well-supported by the experimental results showing consistent improvements across all tested languages and model sizes.

**Medium Confidence**: The claim that QTT-RAG reduces hallucinations compared to rewriting-based approaches is supported by qualitative analysis of entity preservation, but the quantitative evidence is limited to a small sample of cases (214 cases out of 1,856 queries).

**Low Confidence**: The generalizability of the approach to high-resource languages and different LLM architectures remains uncertain, with no evidence of performance on diverse document types or domains beyond the evaluated QA benchmarks.

## Next Checks

1. **Cross-lingual share correlation study**: Systematically measure the cross-lingual share (r_lang) across all tested languages and model configurations, then analyze the relationship between r_lang values and QTT-RAG performance gains to identify deployment boundaries and characterize when the approach provides diminishing returns.

2. **Hard filtering vs quality tagging ablation**: Implement and evaluate a threshold-based filtering approach (e.g., removing documents scoring below 3.5 on any dimension) and compare its performance, coverage, and hallucination rates against QTT-RAG's tagging approach to quantify the tradeoff between simplicity and preservation of critical information.

3. **Quality assessor robustness evaluation**: Test QTT-RAG with multiple quality assessment models (different sizes, architectures, and training data) to evaluate the sensitivity of performance gains to the choice of assessor and identify whether the approach is robust to assessor variation or requires careful model selection.