---
ver: rpa2
title: 'KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein
  Interaction Prediction'
arxiv_id: '2512.09365'
source_url: https://arxiv.org/abs/2512.09365
tags:
- protein
- knowledge
- prediction
- graph
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KGOT, a framework that combines a large-scale
  multimodal biological knowledge graph with optimal transport-based pseudo-labeling
  to improve molecule-protein interaction prediction. The method addresses data scarcity
  by generating high-quality pseudo-labels for unlabeled molecule-protein pairs and
  enriches representations using diverse biological entities (genes, pathways, protein
  families).
---

# KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction

## Quick Facts
- arXiv ID: 2512.09365
- Source URL: https://arxiv.org/abs/2512.09365
- Reference count: 21
- Combines large-scale multimodal biological knowledge graph with optimal transport-based pseudo-labeling to improve molecule-protein interaction prediction

## Executive Summary
This paper introduces KGOT, a framework that addresses data scarcity in molecule-protein interaction (MPI) prediction by generating high-quality pseudo-labels for unlabeled pairs using optimal transport. The method integrates diverse biological entities (genes, pathways, protein families) into a unified knowledge graph to enrich molecular representations beyond direct structural similarity. KGOT achieves state-of-the-art performance on virtual screening benchmarks (DUD-E AUROC 83.45%, LIT-PCBA AUROC 62.45%) and demonstrates strong knowledge graph link prediction capabilities (Hits@5 up to 74.9%).

## Method Summary
KGOT combines pretrained Uni-Mol encoders with a two-stage training pipeline. First, an MLP-based score function is trained on labeled molecule-protein pairs using an Inverse Optimal Transport (IOT) loss that aligns predicted transport matrices with ground truth distributions. Second, pseudo-labels are generated via Sinkhorn-Knopp algorithm with similarity constraints, then injected into a multimodal biological knowledge graph containing 8 node types and 29 edge relations. The augmented KG is trained using embedding methods like TorusE and RotatE, enabling zero-shot generalization to unseen pairs through contextual biological relationships.

## Key Results
- Virtual screening: DUD-E AUROC 83.45%, LIT-PCBA AUROC 62.45%
- KG link prediction: Hits@5 up to 74.9% on MPI datasets
- Ablation shows OT loss outperforms InfoNCE (MRR 0.256 vs 0.243)
- Multimodal KG integration improves Hits@1 from 36.3% to 53.4%

## Why This Works (Mechanism)

### Mechanism 1: Optimal Transport for Constrained Pseudo-Labeling
The method generates pseudo-labels by treating interaction assignment as an optimal transport problem. A cost matrix is constructed from initial interaction scores, then solved to minimize cost while satisfying uniform marginal constraints and molecular similarity priors. This approach enforces global distributional constraints and structural consistency beyond simple score thresholding.

**Core assumption:** The underlying interaction distribution is dense enough for uniform marginal constraints, and molecular embedding similarity correlates with interaction profile similarity.

**Evidence anchors:** Ablation shows "OT + similarity" (74.9% Hits@5) outperforms "Top-k" (72.0%) and "Random" (66.0%).

**Break condition:** If the initial score matrix is extremely noisy or uniform distribution assumption is violated, forced marginal constraints could generate false positive labels.

### Mechanism 2: Inverse Optimal Transport for Encoder Training
Instead of pairwise contrastive loss, IOT loss forces predicted transport matrices to match theoretical optimal transport matrices using KL divergence. This captures relational structures better by modeling interactions as coupled probabilities within batches rather than independent binary events.

**Core assumption:** Batch-level transport matrices better represent interaction systems than independent pairwise comparisons.

**Evidence anchors:** OT Loss achieves MRR 0.256 versus InfoNCE Loss at 0.243.

**Break condition:** Performance may degrade with small batch sizes unable to capture required statistical properties.

### Mechanism 3: Knowledge Graph Propagation for Zero-Shot Generalization
Heterogeneous biological context (genes, pathways) enables generalization to unseen pairs by inferring relationships through intermediate entities. Link prediction algorithms propagate information across shared biological function connections, allowing inference based on functional context rather than structural similarity alone.

**Core assumption:** Entities sharing connections to same context nodes (e.g., pathways) are likely to interact even without direct evidence.

**Evidence anchors:** Hits@1 increases from 36.3% to 53.4% when adding Pathway and Enzyme relations to KG.

**Break condition:** If external knowledge bases are incomplete or contain noisy annotations, the model risks semantic hallucination based on false biological priors.

## Foundational Learning

**Concept: Optimal Transport (Sinkhorn-Knopp)**
- **Why needed here:** Core novelty uses OT to generate pseudo-labels via Sinkhorn algorithm's iterative scaling to satisfy row/column marginal constraints
- **Quick check question:** Can you explain why adding entropy regularization term makes the transport problem solvable in closed-form iterations?

**Concept: Knowledge Graph Embeddings (KGE)**
- **Why needed here:** Final stage uses KGE methods (TorusE, RotatE) for link prediction with triplet scoring functions
- **Quick check question:** How does the "pseudo interaction" edge type differ from standard relations in KGE loss functions?

**Concept: Molecular Representations (Uni-Mol)**
- **Why needed here:** System relies on pretrained encoders to generate initial embeddings for OT and KG processing
- **Quick check question:** What is the input modality for Uni-Mol and how does it affect similarity calculation in OT constraints?

## Architecture Onboarding

**Component map:** Uni-Mol Encoders (f,g) -> Embeddings -> MLP Score Function -> Score Matrix S -> OT Solver -> Pseudo-Label Matrix T -> KG Engine (KGE Model)

**Critical path:** Quality of OT Solver. If cost matrix C from Score Function is uncalibrated, Sinkhorn algorithm diffuses probability mass to incorrect pairs, poisoning KG with noisy pseudo-labels.

**Design tradeoffs:**
- **OT Entropy (ε) vs. Sparsity:** High ε (0.1) creates dense/smooth but less confident assignments; low ε (0.01) creates sparse, sharp assignments but may fail convergence
- **Similarity Weight (λ):** High λ forces structural consistency but may override actual interaction signals if embeddings are biased

**Failure signatures:**
- **Uniform Collapse:** OT matrix becomes essentially uniform (1/MN) - check cost matrix scaling or increase ε
- **Memory OOM:** Building full MxN cost matrix for large datasets - check chunking or approximation strategies

**First 3 experiments:**
1. **Sanity Check:** Overfit Score Function on small labeled dataset to verify pipeline connectivity
2. **Hyperparameter Sweep:** Vary λ and ε in OT step to find sweet spot between coverage and precision
3. **Leakage Control:** Run "Murcko-scaffold-out" test to ensure model isn't memorizing chemical scaffolds

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but several remain unresolved:
- Impact of protein functional similarity constraints in OT objective
- Scalability to full unlabeled pair sets versus sampled subsets
- Generalization to inductive settings with new entities at inference time

## Limitations
- Performance depends heavily on quality and completeness of external knowledge bases (KEGG, GO, PFAM)
- Uniform marginal constraints in OT assume dense interaction distributions that may not reflect biological reality
- System performance is sensitive to quality of pretrained Uni-Mol embeddings and their similarity metrics

## Confidence
- **High confidence**: Performance improvements on established benchmarks (DUD-E AUROC 83.45%, LIT-PCBA AUROC 62.45%)
- **Medium confidence**: Claims about OT superiority over thresholding for pseudo-label quality
- **Medium confidence**: Generalization through KG context

## Next Checks
1. Test model performance on organisms/pathways not well-represented in KEGG/GO to assess knowledge base completeness sensitivity
2. Implement systematic scaffold-out and family-out cross-validation to verify true zero-shot generalization
3. Conduct ablation studies with varying OT entropy regularization (ε) to identify optimal precision-coverage tradeoff