---
ver: rpa2
title: Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged
  by Triplet Mining
arxiv_id: '2502.14573'
source_url: https://arxiv.org/abs/2502.14573
tags:
- depth
- ours
- reflective
- surfaces
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of monocular depth estimation on
  reflective surfaces, where self-supervised methods fail due to photometric constancy
  violations. The proposed reflection-aware triplet mining loss identifies reflective
  regions by contrasting photometric errors across viewpoints and selectively applies
  triplet mining to penalize incorrect gradient updates on such regions.
---

# Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining

## Quick Facts
- **arXiv ID:** 2502.14573
- **Source URL:** https://arxiv.org/abs/2502.14573
- **Reference count:** 32
- **Primary result:** Achieves significant improvements over state-of-the-art self-supervised monocular depth estimation methods on reflective surfaces while maintaining strong generalization on non-reflective scenes

## Executive Summary
This work addresses the challenge of monocular depth estimation on reflective surfaces, where self-supervised methods fail due to photometric constancy violations. The proposed reflection-aware triplet mining loss identifies reflective regions by contrasting photometric errors across viewpoints and selectively applies triplet mining to penalize incorrect gradient updates on such regions. This is combined with a reflection-aware knowledge distillation scheme that fuses depth predictions from a triplet-trained model and a conventional model to retain high-frequency details. The method achieves significant improvements over state-of-the-art self-supervised monocular depth estimation methods, especially on reflective surfaces, while maintaining strong generalization on non-reflective scenes, without requiring auxiliary annotations or complex 3D reconstruction.

## Method Summary
The method introduces a reflection-aware triplet mining loss that identifies reflective regions by comparing photometric errors between same-viewpoint and cross-viewpoint image pairs. When a surface is reflective, the cross-view error becomes abnormally low due to the disparity of reflected objects rather than the actual surface. For identified reflective pixels, the loss applies a hinge penalty that encourages the cross-view error to exceed the same-view error by a margin, preventing the model from incorrectly minimizing photometric error on surfaces where the assumption doesn't hold. Additionally, a reflection-aware knowledge distillation scheme fuses predictions from a triplet-trained model and a conventional model, selecting the appropriate prediction per region to retain high-frequency details while gaining reflection robustness.

## Key Results
- Significant performance improvements on ScanNet-Reflection test set compared to state-of-the-art self-supervised methods
- Maintains strong generalization on non-reflective scenes (7-Scenes dataset) without degradation
- Outperforms conventional methods on highly reflective scenes (Booster dataset)
- Ablation studies confirm the importance of selective triplet loss application and knowledge distillation

## Why This Works (Mechanism)

### Mechanism 1
Reflective regions can be localized by comparing photometric errors between same-viewpoint and cross-viewpoint image pairs. The method computes two photometric errors: E+ (between synthesized image Is2r and reference Iref) and E- (between two synthesized images from different viewpoints). On reflective surfaces, E- becomes abnormally low because reflection lobes appear closer in image coordinates, violating the typical expectation that cross-view errors should be large.

### Mechanism 2
Triplet mining loss selectively neutralizes contaminated gradients on reflective regions without harming non-reflective areas. For identified reflective pixels (Mr=1), the loss applies a hinge penalty: (E+ - E- + δ)hinge, which encourages E- to exceed E+ by margin δ. For non-reflective pixels, standard photometric loss E+ is used. This prevents the model from incorrectly minimizing photometric error on surfaces where the assumption doesn't hold.

### Mechanism 3
Fusing predictions from triplet-trained and conventionally-trained models preserves high-frequency details while gaining reflection robustness. Two teacher models are trained separately—one with Ltri, one with conventional photometric loss. A student model learns from pseudo-depth Dpse = Mr ⊙ Dtri + (1-Mr) ⊙ Dori, selecting the appropriate teacher's prediction per region.

## Foundational Learning

- **Concept: Photometric Constancy / Lambertian Reflectance**
  - Why needed here: The entire SSMDE paradigm relies on this assumption; understanding its violation on specular/reflective surfaces is prerequisite to grasping why the proposed method works.
  - Quick check question: Can you explain why a mirror surface violates photometric constancy when viewed from two different camera positions?

- **Concept: View Synthesis via Depth Warping**
  - Why needed here: The method generates cross-view synthesized images (Is2r, Ir2s) to compute the positive/negative photometric error pairs required for reflective region localization.
  - Quick check question: Given depth Dref and relative pose [R|t], how would you warp source image Isrc to reference viewpoint?

- **Concept: Triplet Mining in Metric Learning**
  - Why needed here: The loss adapts triplet mining from classification (same-class vs different-class) to depth estimation (same-view vs different-view photometric error).
  - Quick check question: In standard triplet loss for face recognition, what are the anchor, positive, and negative? How does this map to the proposed formulation?

## Architecture Onboarding

- **Component map:** Depth network Fθ -> Depth map Dref; Pose network Gϕ -> Relative pose [R|t]; Warping module -> Synthesized images Is2r, Ir2s; Photometric error module -> E+, E-; Reflective mask generator -> Mr; Loss combiner -> Ltri or standard loss

- **Critical path:** 1. Forward pass: Iref, Isrc → Dref, Dsrc, [R|t]r2s; 2. Synthesize: Is2r, Ir2s via warping; 3. Compute: E+ = P(Is2r, Iref), E- = P(Is2r, Ir2s); 4. Localize: Mr = (E- - E+ ≤ δ); 5. Loss: Ltri per Equation 9

- **Design tradeoffs:** Margin δ selection: Paper uses adaptive selection based on Q1 of E+ and Q3 of E-; too low → false positives in Mr; too high → missed reflective regions; End-to-end vs distillation: End-to-end (Ours) is simpler but may lose high-frequency details; distillation (Ours†) adds 2x training cost but preserves details; Pose source: Paper uses GT pose during training; Assumption: performance may degrade with predicted pose

- **Failure signatures:** Black-hole effect persists → Mr not detecting reflective regions (δ misconfigured); Non-reflective edges blur → Mr has high false-positive rate; Training instability → Check hinge loss gradient flow on Mr regions

- **First 3 experiments:** 1. Baseline validation: Train Monodepth2 with standard photometric loss on ScanNet-Reflection train split; evaluate on test split to quantify the black-hole effect baseline; 2. Ablation on Mr: Compare Mr=0 (baseline), Mr=1 (triplet everywhere), and adaptive Mr (Equation 8) to validate selective loss application hypothesis; 3. Cross-dataset generalization: Train on ScanNet-Reflection, evaluate zero-shot on 7-Scenes (non-reflective) and Booster (highly reflective) to confirm the method doesn't overfit to training distribution

## Open Questions the Paper Calls Out

### Open Question 1
Can the reflection-aware triplet mining strategy be adapted to accurately estimate depth for transparent or mirror (ToM) objects, which the current method explicitly fails to handle? The proposed method relies on identifying reflective regions based on photometric error discrepancies characteristic of specular highlights on surfaces. Transparent or perfect mirror surfaces involve different physical interactions (transmission or perfect reflection of distant scenes) that likely violate the specific photometric error assumptions used to generate the reflection mask Mr.

### Open Question 2
How robust is the reflection-aware triplet mining loss when trained with estimated camera poses rather than the ground truth poses used in the experiments? The method relies on synthesizing cross-view images (Is2r and Ir2s) to calculate the photometric error difference (E- vs E+). If the pose network provides inaccurate poses, the alignment required to distinguish reflective regions from non-reflective ones could be corrupted, potentially degrading the localization of the reflective mask Mr.

### Open Question 3
How does the reflective region localization perform in complex lighting scenarios, such as surfaces with multiple reflection lobes, where the standard photometric error assumptions are violated? The localization logic (Equation 8) hinges on a specific relationship between E+ and E- (using a margin δ). Multiple reflection lobes might create complex photometric signatures that do not satisfy E- - E+ ≤ δ, leading to failures in identifying the reflective regions correctly.

## Limitations
- The method cannot handle transparent or mirror (ToM) objects due to different physical interactions that violate the photometric error assumptions
- Reliance on ground truth camera poses during training limits real-world applicability where pose estimation quality is variable
- The adaptive margin selection (δ = Q1(E+) - Q3(E-)) is heuristic and may not generalize well across diverse reflective scenarios with varying photometric error distributions

## Confidence

- **High Confidence:** The observation that reflective surfaces violate photometric constancy and cause black-hole artifacts in self-supervised depth estimation
- **Medium Confidence:** The effectiveness of triplet mining loss for selectively correcting gradients on reflective regions
- **Low Confidence:** The knowledge distillation approach consistently preserving high-frequency details

## Next Checks
1. **Reflective Mask Robustness:** Evaluate how sensitive the method is to reflective mask quality by intentionally perturbing Mr with noise and measuring performance degradation
2. **Pose Estimation Sensitivity:** Replace ground truth poses with predicted poses from a state-of-the-art pose network and measure performance drop
3. **Cross-Dataset Generalization:** Train on a mixed dataset containing both reflective and non-reflective scenes, then evaluate on ScanNet-Reflection to test whether the method maintains performance without overfitting to reflective surfaces during training