---
ver: rpa2
title: Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal
  Belief
arxiv_id: '2509.01564'
source_url: https://arxiv.org/abs/2509.01564
tags:
- confidence
- score
- eagle
- calibration
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EAGLE (Expectation of Aggregated Internal Belief) improves LLM
  uncertainty estimation by extracting internal representations from multiple intermediate
  layers during self-evaluation. Instead of relying on final output or verbalized
  confidence scores, EAGLE aggregates layer-wise hidden states, projects them to logits,
  and computes the final confidence score as the expectation over the resulting probability
  distribution.
---

# Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief

## Quick Facts
- **arXiv ID:** 2509.01564
- **Source URL:** https://arxiv.org/abs/2509.01564
- **Reference count:** 7
- **Primary result:** EAGLE improves LLM uncertainty estimation, reducing ECE by up to 21.7 points and achieving AUROC improvements of up to 14.1 points

## Executive Summary
This paper introduces EAGLE (Expectation of Aggregated Internal Belief), a novel approach for uncertainty estimation in large language models that extracts internal representations from multiple intermediate layers during self-evaluation. Unlike existing methods that rely on final output probabilities or verbalized confidence scores, EAGLE aggregates layer-wise hidden states, projects them to logits, and computes the final confidence score as the expectation over the resulting probability distribution. Experiments across multiple models (Llama3, Qwen2.5) and datasets (TriviaQA, GSM8k, MMLU) demonstrate that EAGLE significantly outperforms baselines, with systematic ablation studies confirming the effectiveness of each component.

## Method Summary
EAGLE works by first generating an answer to a given question, then appending a self-evaluation prompt asking the model to assign a confidence score from 0-9. During the self-evaluation phase, the model's hidden states are extracted from the last k layers at the position of the self-evaluation token. Each hidden state is projected to vocabulary logits via the unembedding function, then averaged across layers to produce aggregated logits. The method applies softmax only over the score tokens (0-9), then computes the final confidence score as the expectation over this distribution rather than selecting the maximum probability score. This approach captures richer uncertainty signals distributed across multiple layers while preserving information that would be lost by normalizing probabilities before aggregation.

## Key Results
- EAGLE reduces Expected Calibration Error (ECE) by up to 21.7 points compared to baseline methods across all tested datasets and models
- Achieves AUROC improvements of up to 14.1 points for failure prediction tasks
- Improves answer selection accuracy by 1.5-3.0 points when choosing between high-confidence and low-confidence predictions
- Ablation studies confirm that logits aggregation and expectation-based scoring are critical for performance gains
- Layer-wise analysis shows optimal performance when aggregating from later layers of the transformer

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Hidden State Aggregation
Aggregating hidden states from multiple later layers captures richer and more robust confidence signals than relying solely on the final layer. During self-evaluation, EAGLE extracts hidden states from the last k layers at the position of the self-evaluation token, projects each to logits via unembedding, then averages them. This integrates information processed at different depths of the transformer, where intermediate layers have been shown to encode uncertainty-relevant signals that separate high- from low-confidence predictions.

### Mechanism 2: Pre-Softmax Logits Aggregation Preserves Signal
Aggregating logits (pre-softmax) across layers yields substantially better calibration than aggregating probabilities (post-softmax). Each layer's hidden state is projected to vocabulary logits, which are then averaged before applying softmax. Probability aggregation, by contrast, normalizes each layer independently via softmax first, which discards the relative magnitude information across layers that carries meaningful information about confidence intensity.

### Mechanism 3: Expectation over Score Distribution Captures Nuanced Uncertainty
Computing the final confidence score as the expectation over the probability distribution yields better calibration than taking the maximum-probability score. After aggregating logits and applying softmax over candidate score tokens {0,1,...,9}, EAGLE computes the confidence as the weighted sum of scores, which weights each possible score by its probability rather than discarding all but the mode. This captures uncertainty that the mode alone cannot capture.

## Foundational Learning

- **Model Calibration and Expected Calibration Error (ECE)**
  - Why needed here: The entire paper frames success in terms of calibration improvement. Without understanding ECE, the magnitude of improvements (e.g., "21.7 points ECE reduction") has no context.
  - Quick check question: Given a model with 80% accuracy on high-confidence predictions (confidence > 0.9) and 40% accuracy on low-confidence predictions (confidence < 0.5), is it well-calibrated?

- **Transformer Hidden States and Unembedding**
  - Why needed here: EAGLE's core operation extracts hidden states from intermediate layers and projects to logits via unembedding. Understanding this pipeline is essential for implementing the method.
  - Quick check question: If a 32-layer model has hidden dimension 4096 and vocabulary size 50,000, what is the shape of the logits tensor at a single token position?

- **Self-Evaluation Prompting Paradigm**
  - Why needed here: EAGLE is built on self-evaluation, requiring the model to assess its own output. The prompt design directly affects the hidden states extracted.
  - Quick check question: Why might a self-evaluation prompt that asks for chain-of-thought reasoning degrade EAGLE's performance compared to a direct scoring prompt?

## Architecture Onboarding

- **Component map:** Input question + generated answer → Self-evaluation prompt → Forward pass through LLM → Extract hidden states H^(l)_n from last k layers at self-evaluation token position → Project to logits via unembedding → Average logits across layers → Apply softmax over score tokens {0,1,...,9} → Compute expectation for final confidence score

- **Critical path:** Layer selection: Experiments indicate the last few layers are most informative. Start with k=4-8 final layers as a default. Token alignment: The hidden state extraction must occur at the position of the self-evaluation token. Score token mapping: Ensure tokenization of "0" through "9" maps to the expected indices before masking the softmax.

- **Design tradeoffs:** More layers → better calibration but higher memory/compute for hidden state storage; empirically, last-n is near-optimal. Granular score range (0-9) vs. binary (0-1): Table 5 shows 0-9 yields ECE of 7.6 vs. 11.1 for binary on GSM8K. Prompt simplicity vs. chain-of-thought: CoT prompts degraded TriviaQA performance (ECE 13.6 vs. 1.7), suggesting reasoning adds noise to confidence signals.

- **Failure signatures:** ECE similar to baseline: Check if aggregation is happening over logits (not probabilities) and if the correct token position is being extracted. AUROC near random (50%): The hidden states may not encode uncertainty signals for the specific model architecture. Large variance across prompt variants: EAGLE may be overfitting to specific prompt phrasings.

- **First 3 experiments:**
  1. Baseline replication: Implement EAGLE with k=8 final layers on Llama3 8B with TriviaQA. Verify ECE drops from ~15 (SE baseline) to ~2 (EAGLE target from Table 1).
  2. Ablation sweep: Run the three ablations from Table 2 (last layer vs. last-n; probability vs. logits aggregation; max vs. expectation) on a single model/dataset pair to confirm each component's contribution.
  3. Layer-range heatmap: Replicate Figure 2 by sweeping start and end layers (m to n) on GSM8K with Qwen2.5 7B. Confirm that the top-right quadrant (later layers) shows lowest ECE and highest AUROC.

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does Chain-of-Thought (CoT) prompting degrade EAGLE's calibration performance on certain tasks like TriviaQA? The paper identifies the degradation but does not investigate the underlying mechanism or propose solutions to reconcile CoT reasoning with internal belief extraction.

- **Open Question 2:** Can EAGLE generalize effectively to encoder-decoder architectures or other non-decoder-only transformer models? The paper explicitly states "Current mainstream LLMs are decoder-only transformers" but does not address whether the approach transfers to encoder-decoder models like T5 or BART.

- **Open Question 3:** What is the computational overhead of EAGLE compared to baseline methods, and how does it scale with model size and number of aggregated layers? The method requires extracting and processing hidden states from k layers, but the paper provides no latency measurements, memory analysis, or efficiency comparisons against baselines.

## Limitations

- **Implementation sensitivity:** Different prompt templates yield varying performance, with Chain-of-Thought prompting degrading calibration on TriviaQA, but exact prompt formats for baseline methods are not fully specified.
- **Computational overhead:** The method requires extracting and processing hidden states from multiple layers during inference, but the paper does not report memory or latency overhead which could limit practical deployment.
- **Dataset and task specificity:** Evaluation focuses primarily on single-answer generation tasks and multiple-choice benchmarks, without addressing whether benefits transfer to open-ended generation or continuous output domains.

## Confidence

**High confidence:** EAGLE improves calibration (lower ECE) compared to baselines - directly supported by Tables 1-3 showing consistent ECE reductions across multiple models and datasets.

**Medium confidence:** EAGLE's gains are primarily due to aggregating logits from later layers rather than probabilities - while ablation results are compelling, the paper doesn't explore why later layers specifically contain better uncertainty signals.

**Medium confidence:** EAGLE improves answer selection accuracy by 1.5-3.0 points - Table 5 shows improvements, but the practical significance depends on baseline selection accuracy and whether this translates to real-world downstream performance.

## Next Checks

1. **Statistical validation:** Replicate the experiments with 5-10 random seeds per configuration and report mean ± standard error for ECE and AUROC to determine whether reported improvements are statistically significant.

2. **Cross-task generalization:** Test EAGLE on open-ended generation tasks like summarization or story continuation to determine if calibration improvements transfer beyond QA-style tasks.

3. **Resource overhead measurement:** Profile memory usage and inference latency for EAGLE vs. baseline methods on Llama3-8B and Llama3-70B to measure the trade-off between calibration improvement and computational cost.