---
ver: rpa2
title: 'Position: Stop Acting Like Language Model Agents Are Normal Agents'
arxiv_id: '2502.10420'
source_url: https://arxiv.org/abs/2502.10420
tags:
- arxiv
- agents
- lmas
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language Model Agents (LMAs) are increasingly treated as capable
  of autonomously navigating interactions with humans and tools. Their design and
  deployment tends to presume they are normal agents capable of sustaining coherent
  goals, adapting across contexts and acting with a measure of intentionality.
---

# Position: Stop Acting Like Language Model Agents Are Normal Agents

## Quick Facts
- **arXiv ID**: 2502.10420
- **Source URL**: https://arxiv.org/abs/2502.10420
- **Reference count**: 40
- **Primary result**: Language Model Agents (LMAs) inherit fundamental structural problems from LLMs that undermine their agency properties

## Executive Summary
Language Model Agents are increasingly deployed under the assumption that they possess normal agent capabilities like coherent goal pursuit, contextual adaptation, and intentional action. This paper argues these assumptions are fundamentally flawed because LMAs inherit critical pathologies from their LLM foundations, including hallucinations, unpredictability, and ontological instability. The authors contend that treating LMAs as normal agents leads to serious problems for their utility and trustworthiness in industrial, social, and governmental applications.

The paper identifies specific pathologies that destabilize core ontological properties of LMAs, including identifiability, continuity, persistence, and consistency. Despite architectural scaffolding like external memory and tools, LMAs remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. The authors propose measuring these ontological properties throughout the deployment lifecycle to mitigate negative effects, though implementation details remain limited.

## Method Summary
This position paper synthesizes existing literature on LLM limitations and agent theory to construct an argument about LMA pathologies. The authors review documented LLM behaviors (hallucinations, jailbreaking, misalignment) and map these to agent-related ontological properties. They employ philosophical analysis of agency concepts to identify which properties are compromised in LMAs, drawing on interdisciplinary perspectives from computer science, philosophy of mind, and AI safety research. The methodology is primarily conceptual rather than empirical, using logical argumentation and literature synthesis rather than experimental validation.

## Key Results
- LMAs exhibit inherent pathologies (hallucinations, jailbreaking, misalignment) that destabilize their agency properties
- Core ontological properties including identifiability, continuity, persistence, and consistency are compromised in LMAs
- Despite scaffolding mechanisms, LMAs remain fundamentally stateless, stochastic, and semantically unstable
- Treating LMAs as normal agents undermines their utility and trustworthiness in real-world deployments

## Why This Works (Mechanism)
The argument works by establishing a logical chain: LLMs have fundamental limitations that directly impact the agency properties LMAs need to function as normal agents. When these pathologies interact with agentic behaviors, they create cascading failures in the very properties that would make LMAs reliable and trustworthy. The mechanism relies on demonstrating that certain agency requirements (like persistent identity and coherent goal pursuit) are incompatible with LLM architecture at a fundamental level, not just a surface implementation issue.

## Foundational Learning
- **Agency Ontology**: Understanding what constitutes normal agency (persistent identity, coherent goals, intentional action) - needed to establish baseline against which LMAs can be evaluated; quick check: compare LMA behaviors against established philosophical definitions of agency
- **LLM Pathologies**: Hallucinations, jailbreaking, misalignment, and stochastic outputs - needed to identify the root causes of LMA instability; quick check: catalog documented LLM failures and their frequency across different model families
- **Ontological Properties**: Identifiability, continuity, persistence, and consistency as measurable dimensions - needed to create a framework for assessing LMA capability; quick check: develop operational definitions for each property that can be measured empirically
- **Linguistic Mediation**: How language-based interfaces create semantic instability - needed to explain why LMAs are particularly vulnerable to context shifts; quick check: analyze how small prompt variations affect LMA outputs across multiple domains
- **Statelessness**: The absence of internal state persistence in transformer architectures - needed to explain fundamental continuity limitations; quick check: measure memory decay in LMAs across extended task sequences
- **Stochastic Generation**: The probabilistic nature of LLM outputs - needed to quantify unpredictability in agentic behaviors; quick check: measure output variance for identical prompts across multiple runs

## Architecture Onboarding
- **Component Map**: LLM Core -> Scaffolding (Memory, Tools, Planning) -> Action Interface -> Environment
- **Critical Path**: Input processing through LLM generation to action selection and execution
- **Design Tradeoffs**: Flexibility and language understanding vs. predictability and consistency
- **Failure Signatures**: Hallucinated tool usage, context collapse, goal drift, inconsistent identity across sessions
- **First Experiments**:
  1. Measure LMA output consistency when given identical prompts across time intervals
  2. Test LMA persistence of goals across multi-step task sequences
  3. Evaluate semantic sensitivity by varying semantically equivalent prompts

## Open Questions the Paper Calls Out
Several major uncertainties remain unresolved in the paper. The claim that LMAs lack normal agent properties relies heavily on contested philosophical assumptions about agency and intentionality that are not universally accepted. Empirical evidence for many listed pathologies comes primarily from anecdotal observations or theoretical arguments rather than systematic experimental validation. The paper does not clearly distinguish LMAs from other AI agent types, making it difficult to isolate LLM-specific issues. While the ontology-based mitigation strategy is conceptually appealing, it lacks concrete implementation details or validation studies. The authors acknowledge their position paper format but do not adequately address potential counterarguments about hybrid approaches or scenarios where partial agency might be sufficient.

## Limitations
- Relies heavily on contested philosophical assumptions about agency and intentionality
- Empirical evidence for many pathologies is anecdotal rather than systematically validated
- Distinction between LMAs and other AI agent types remains unclear
- Proposed mitigation strategy lacks concrete implementation details or validation
- Does not address potential counterarguments about hybrid approaches or partial agency

## Confidence
- Core argument about LMA pathologies: Medium
- Specific pathology characterizations: Low-Medium
- Proposed mitigation strategy: Low
- Distinction from other agent types: Medium

## Next Checks
1. Conduct controlled experiments measuring LMA continuity and consistency across task sequences using standardized benchmarks
2. Develop and validate quantitative metrics for the proposed ontological properties (identifiability, persistence, etc.)
3. Compare LMA behavior with non-LLM agents on identical tasks to isolate LLM-specific pathologies