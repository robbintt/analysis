---
ver: rpa2
title: An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints
arxiv_id: '2505.21841'
source_url: https://arxiv.org/abs/2505.21841
tags:
- adversarial
- lemma
- sahk
- term
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of safe reinforcement learning
  in dynamic environments with adversarial constraints, modeled as constrained Markov
  decision processes (CMDPs). The authors propose the Optimistic Mirror Descent Primal-Dual
  (OMDPD) algorithm, which achieves optimal regret and strong constraint violation
  bounds without relying on Slater's condition or requiring a known safe policy.
---

# An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints

## Quick Facts
- arXiv ID: 2505.21841
- Source URL: https://arxiv.org/abs/2505.21841
- Reference count: 40
- This paper proposes the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, achieving optimal regret and strong constraint violation bounds for online constrained MDPs with adversarial constraints.

## Executive Summary
This paper addresses safe reinforcement learning in dynamic environments with adversarial constraints, modeled as constrained Markov decision processes (CMDPs). The authors propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, which achieves optimal regret and strong constraint violation bounds without relying on Slater's condition or requiring a known safe policy. OMDPD uses optimistic estimates and a surrogate objective function with exponential potential to adaptively regulate constraint violations during learning. The algorithm achieves $\tilde{O}(\sqrt{K})$ regret and strong constraint violation bounds, where $K$ is the number of episodes.

## Method Summary
OMDPD addresses online CMDPs with adversarial constraints by combining optimistic estimation, exponential potential functions, and optimistic mirror descent optimization. The algorithm constructs optimistic estimates for rewards and transitions (UCB-type bonuses) and pessimistic estimates for costs, then solves a convex optimization problem within a confidence set to find the optimal occupancy measure. The exponential potential function ensures strict constraint violation bounds by penalizing positive violations more aggressively. The optimistic mirror descent updates adapt to environmental predictability, achieving optimal regret bounds for both stochastic and adversarial settings.

## Key Results
- Achieves $\tilde{O}(\sqrt{K})$ regret and strong constraint violation bounds for online CMDPs with adversarial constraints
- Removes need for Slater's condition or known safe policy
- Demonstrates sublinear violation growth in synthetic experiments with both stochastic and adversarial cost settings
- If accurate estimates of rewards and transitions are available, regret can be improved to $O(1)$

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm incentivizes exploration of unknown transitions and constraints by systematically overestimating potential rewards and underestimating potential costs.
- **Mechanism:** OMDPD constructs "optimistic estimates" (UCB-type bonuses) for rewards ($\tilde{r}$) and transitions, and "pessimistic estimates" for costs ($\tilde{d}$). By solving the optimization problem within a confidence set $B_k$, the agent prefers state-action pairs with high uncertainty, naturally balancing exploration and exploitation without requiring prior knowledge of the environment.
- **Core assumption:** The true transition kernel and reward/cost functions lie within the constructed confidence sets with high probability (the "Good Event" $G$).
- **Break condition:** If the confidence radius $\beta$ is set too low for the environment's noise level, the "Good Event" fails, and the optimal policy may fall outside the candidate set $Q_k$, causing linear regret.

### Mechanism 2
- **Claim:** An exponential potential function allows the controller to strictly bound cumulative safety violations (Strong Constraint Violation) without allowing positive errors to cancel out negative ones.
- **Mechanism:** Instead of a standard linear Lagrangian, the algorithm uses a surrogate objective $f_k(q)$ featuring an exponential potential $\Phi(\lambda) = e^{\beta\lambda} - 1$. This function grows non-linearly as the dual variable $\lambda$ increases. This "violates-fast" property forces the optimizer to prioritize reducing *any* positive violation immediately, ensuring the strict "strong violation" bound of $\tilde{O}(\sqrt{K})$.
- **Core assumption:** The cost functions are bounded in $[-1, 1]$.
- **Break condition:** If the learning rate $\eta_k$ or scaling parameter $\alpha$ is mismatched with the potential steepness $\beta$, the dual updates may become unstable.

### Mechanism 3
- **Claim:** Optimistic Mirror Descent (OMD) enables optimal regret bounds by adapting to the predictability of the environment (stochastic vs. adversarial).
- **Mechanism:** The algorithm predicts the next gradient using historical data (the "optimistic" step $\hat{q}_{k+1}$) before receiving the actual gradient. If the environment is stochastic (predictable), the gradient variation is low, allowing the algorithm to effectively use larger steps or tighter bounds.
- **Core assumption:** The cumulative variation of consecutive gradients is bounded.
- **Break condition:** In highly non-stationary environments where gradient variations scale linearly with $K$, the regret bound degrades from $\sqrt{K}$ to $K$.

## Foundational Learning

- **Concept: Occupancy Measures**
  - **Why needed here:** The paper reformulates the policy search problem into linear programming over "occupancy measures" $q$ (expected state-action visit counts). You cannot interpret the algorithm's update steps without understanding that the optimizer is searching over $q \in Q_k$, not raw policy parameters.
  - **Quick check question:** Can you explain how Eq. (11) reconstructs a policy $\pi$ from an occupancy measure $q$?

- **Concept: Slater's Condition (and why it's missing)**
  - **Why needed here:** Most safe RL papers assume a "strictly feasible" solution exists (Slater's condition) to bound dual variables. This paper explicitly *removes* that assumption. Understanding this highlights why the "exponential potential" is necessaryâ€”to act as a barrier function keeping dual variables bounded even when feasibility is marginal.
  - **Quick check question:** Why does removing Slater's condition make the control problem "harder" in terms of constraint violation?

- **Concept: Strong vs. Weak Constraint Violation**
  - **Why needed here:** The paper distinguishes between allowing unsafe actions to be "cancelled out" by safe ones (Weak) vs. summing only positive errors (Strong). The algorithm is designed specifically for the "Strong" metric.
  - **Quick check question:** If an agent incurs a cost of $+1$ (violation) followed by $-1$ (safe), what is the *Strong* Violation total?

## Architecture Onboarding

- **Component map:** Estimator Module -> Dual Controller -> OMD Optimizer -> Decoder
- **Critical path:**
  1. Execute $\pi_k$ and observe trajectory.
  2. Update Empirical Estimates ($\hat{p}, \hat{r}, \hat{d}$).
  3. Update Dual Variable $\lambda_{k+1}$ using the exponential scaling factor $\alpha$.
  4. Compute Gradient of Surrogate $f_k$ and run the Mirror Descent step.
  5. Reconstruct $\pi_{k+1}$.

- **Design tradeoffs:**
  - **Complexity vs. Safety:** The OMD + Exponential Potential approach offers tighter theoretical bounds than simpler Lagrangian methods but requires solving a convex optimization problem at every step (higher compute cost).
  - **Knowledge Requirement:** While it removes the need for a "safe policy," it assumes knowledge of state-space dimensions $S,A$ to calculate confidence bounds.

- **Failure signatures:**
  - **Constraint Oscillation:** If the learning rate $\eta$ is too high, the dual variable $\lambda$ may overshoot, causing the policy to alternate between "too conservative" and "too risky."
  - **Linear Regret:** If the transition kernel estimation error is high (insufficient exploration), the "Good Event" fails, and regret scales linearly with episodes $K$.

- **First 3 experiments:**
  1. **Validation of Bounds:** Run OMDPD on the synthetic CMDP and plot Cumulative Violation against $\sqrt{K}$ to verify the sublinear trend predicted by Theorem 5.1.
  2. **Ablation on Potential:** Compare the "Exponential Potential" $\Phi(x)=e^{\beta x}-1$ against a standard Linear Potential to demonstrate the reduction in *Strong* Constraint Violation.
  3. **Adversarial Stress Test:** Introduce a "worst-case" adversary that sets costs $d_k$ to maximize violation at every step, verifying that the bounds hold under the "Anytime Adversarial" condition.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the OMDPD framework be extended to multi-agent CMDPs with adversarial constraints, and what are the additional challenges in achieving similar regret and violation guarantees?
  - **Basis in paper:** Future research directions include extending our framework to multi-agent settings.
  - **Why unresolved:** Multi-agent settings introduce strategic interactions and non-stationarity from other agents' policies, potentially breaking the single-agent analysis.
  - **What evidence would resolve it:** A theoretical extension with bounded regret/violation analysis for multi-agent CMDPs, plus empirical validation showing sublinear violation growth in multi-agent environments.

- **Open Question 2:** Can strong constraint violation bounds be achieved under pure bandit feedback for adversarial costs, rather than requiring full cost vector revelation?
  - **Basis in paper:** The paper assumes the full cost vector $d_k$ is revealed after episode $k$ for adversarial constraints.
  - **Why unresolved:** Bandit feedback provides only scalar cost observations, making it harder to identify which constraint components are violated and to track adversarial changes without cancellations.
  - **What evidence would resolve it:** An algorithm achieving $\tilde{O}(\sqrt{K})$ regret and violation under bandit feedback for adversarial costs, or a formal lower bound showing separation from full-information settings.

- **Open Question 3:** Can the $O(1)$ regret bound under known transitions be achieved without requiring a perfect simulator or generative model?
  - **Basis in paper:** Remark 5.2 shows $O(1)$ regret "if a perfect simulator (generative model) is given," identifying estimation error as the bottleneck.
  - **Why unresolved:** Perfect simulators are rarely available in practice. Learning accurate transition models online while maintaining safety guarantees remains challenging.
  - **What evidence would resolve it:** An algorithm achieving $O(1)$ or improved regret using only online experience, or a lower bound proving that simulator access is necessary for constant regret.

## Limitations
- The algorithm's performance heavily depends on the accuracy of the confidence set construction, with specific failure probability not reported in the main text
- Computational complexity of solving the convex optimization at each episode is not explicitly quantified
- Experimental evaluation is limited to a single synthetic CMDP with relatively small dimensions (S=5, A=3)

## Confidence
- **High confidence** in the theoretical regret and constraint violation bounds, as they are rigorously proven with explicit dependence on K
- **Medium confidence** in the practical efficacy of the exponential potential function, supported by the synthetic experiment but lacking ablation studies
- **Low confidence** in the algorithm's ability to handle truly adversarial constraints in continuous or high-dimensional state spaces, given the current evaluation scope

## Next Checks
1. **Confidence Set Failure Rate:** Run OMDPD for multiple random seeds on the synthetic CMDP and report the empirical frequency of the "Good Event" failure. Compare this to the theoretical failure probability bound.
2. **Scalability Test:** Implement OMDPD on a larger CMDP (e.g., S=20, A=5) and measure both the computational time per episode and the regret/violation growth rate. This will reveal practical limitations not captured by small-scale theory.
3. **Continuous State Extension:** Modify the algorithm to handle a simple continuous-state CMDP (e.g., linear quadratic regulator with constraints) using function approximation for the occupancy measure. Evaluate whether the regret bounds still hold approximately, or if the discretization assumption is critical.