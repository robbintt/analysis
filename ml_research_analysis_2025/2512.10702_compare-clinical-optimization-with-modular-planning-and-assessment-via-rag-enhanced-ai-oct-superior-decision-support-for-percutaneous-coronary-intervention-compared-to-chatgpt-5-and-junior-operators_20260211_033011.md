---
ver: rpa2
title: 'COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced
  AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared
  to ChatGPT-5 and Junior Operators'
arxiv_id: '2512.10702'
source_url: https://arxiv.org/abs/2512.10702
tags:
- stent
- ca-gpt
- junior
- clinical
- coronary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A CA-GPT-based AI-OCT system integrating optical coherence tomography
  (OCT) image analysis with a retrieval-augmented generation (RAG) framework was developed
  to support OCT-guided percutaneous coronary intervention (PCI) decision-making.
  The system was compared with general-purpose ChatGPT-5 and junior physicians in
  a retrospective analysis of 96 patients.
---

# COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators

## Quick Facts
- **arXiv ID:** 2512.10702
- **Source URL:** https://arxiv.org/abs/2512.10702
- **Reference count:** 0
- **Primary result:** CA-GPT-based AI-OCT system achieved significantly higher agreement with expert PCI decisions than ChatGPT-5 or junior physicians across 10 evaluation metrics

## Executive Summary
A CA-GPT-based AI-OCT system integrating optical coherence tomography (OCT) image analysis with retrieval-augmented generation (RAG) was developed to support OCT-guided percutaneous coronary intervention (PCI) decision-making. The system was compared with ChatGPT-5 and junior physicians in a retrospective analysis of 96 patients. For pre-PCI planning, CA-GPT achieved significantly higher median agreement scores (5[IQR 3.75-5]) versus ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001), with superior performance in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analyses confirmed robust advantages in complex scenarios. The CA-GPT-based system provides standardized, reliable OCT-guided PCI decision support, demonstrating potential to augment operator expertise and optimize outcomes.

## Method Summary
The CA-GPT system employs a dual-layer architecture: a small-model layer performs structured OCT analysis (13 functions including lumen segmentation, plaque characterization, stent apposition, OCT-FFR), and a large-model layer (DeepSeek-R1, 14B parameters) uses RAG to generate procedural recommendations. The system was retrospectively validated against 96 PCI cases, comparing agreement scores (0-5) with expert-derived procedural records. Key evaluation metrics included stent diameter/length selection, revascularization decisions, and post-PCI assessments of minimal stent area, expansion, apposition, dissection, and tissue prolapse.

## Key Results
- CA-GPT achieved median agreement score of 5 for pre-PCI planning versus 3 for ChatGPT-5 and 4 for junior physicians (P<0.001)
- Superior stent diameter agreement: 90.3% for CA-GPT vs. 72.2% for ChatGPT-5 (P<0.05)
- Superior stent length agreement: 80.6% for CA-GPT vs. 52.8% for ChatGPT-5 (P<0.01)
- Post-PCI assessment: CA-GPT maintained excellent agreement (5[4.75-5]) versus ChatGPT-5 (4[4-5], P<0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG grounding reduces clinical hallucination and improves domain-specific agreement.
- Mechanism: The system retrieves from a curated knowledge base (current guidelines + >100,000 annotated PCI cases) during generation, constraining outputs to evidence-traceable recommendations rather than free-form inference from pretrained weights.
- Core assumption: The retrieval corpus aligns with the reference standard (Chinese expert consensus) and is up-to-date; retrieval quality determines grounding effectiveness.
- Evidence anchors:
  - [abstract] "integrating optical coherence tomography (OCT) image analysis with a retrieval-augmented generation (RAG) framework"
  - [Methods] "RAG integrates analytical outputs with a knowledge base of current guidelines and over 100,000 annotated PCI cases. This integration facilitates a 'retrieve-reason-generate' cycle"
  - [corpus] Neighbor work on RAG for ACS guidelines (Alexandrou et al., JACC Cardiovasc Interv 2025) supports RAG improving guideline-adherent outputs, but no direct corpus evidence for this specific system.
- Break condition: Retrieval corpus is stale, incomplete, or mismatched to local practice guidelines; retrieval fails to surface relevant cases for rare presentations.

### Mechanism 2
- Claim: A dual-layer "small-model + large-model" architecture separates perception from reasoning.
- Mechanism: The small-model layer performs structured OCT analysis (13 functions including lumen segmentation, plaque characterization, stent apposition, OCT-FFR). The large-model layer (DeepSeek-R1, 14B) consumes these structured outputs and, via RAG, generates procedural recommendations—reducing the burden on the LLM to perform direct image understanding.
- Core assumption: Small-model outputs are accurate and reliable; errors in segmentation or quantification will propagate to downstream recommendations.
- Evidence anchors:
  - [Methods] "small model layer conducts structured OCT analysis, performing 13 core functions... six of which are proprietary algorithms. The large model layer is built upon the open-source DeepSeek-R1"
  - [Results] High agreement on MSA (100%), stent apposition (93.2%), and severe dissection detection (97.7%) suggests small-model perception is reasonably accurate.
  - [corpus] No directly comparable dual-layer OCT+LLM corpus evidence; general LLM-as-reasoning-engine patterns appear in MEDDxAgent, but not validated for PCI.
- Break condition: Small-model segmentation fails on poor-quality images, heavy calcification, or unusual anatomy; structured inputs become unreliable.

### Mechanism 3
- Claim: Domain-specific training and local guideline alignment outperform general-purpose LLMs.
- Mechanism: CA-GPT is tuned for PCI decision-making with Chinese expert consensus as the grounding reference. ChatGPT-5, trained predominantly on Western data and guidelines, produces recommendations that diverge from local standards.
- Core assumption: Reference standard (expert procedural records adjudicated per Chinese consensus) represents correct decisions; divergence from it indicates error rather than legitimate practice variation.
- Evidence anchors:
  - [Discussion] "ChatGPT-5's training data predominantly comes from Western populations and international guidelines... which may not fully align with the Chinese expert consensus"
  - [Results] ChatGPT-5 achieved 63.9% stent diameter agreement vs. CA-GPT's 90.3% (P<0.001).
  - [corpus] NOHARM benchmark (arXiv:2512.01241) documents safety concerns with general LLMs in clinical contexts, supporting the need for domain-specific systems.
- Break condition: Expert consensus changes, or local practice legitimately diverges from guidelines; general-purpose models improve via medical fine-tuning.

## Foundational Learning

- Concept: OCT image interpretation (lumen area, plaque type, calcium scoring, stent apposition, dissection, tissue prolapse)
  - Why needed here: All system outputs depend on accurate extraction of these 13 parameters from pullback images; users must understand what the small model is measuring.
  - Quick check question: Given an OCT cross-section, can you identify the lumen boundary, differentiate calcium from fibrous plaque, and assess malapposition threshold (>400μm or ≥1mm length)?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The system's reliability hinges on RAG reducing hallucinations; engineers must understand retrieval-index design, chunking strategies, and relevance scoring.
  - Quick check question: If the retriever fails to surface the relevant guideline chunk for a heavily calcified lesion, what downstream error would you expect in pretreatment recommendations?

- Concept: PCI decision workflow (pre-PCI: revascularization, pretreatment, stent sizing; post-PCI: MSA, expansion, apposition, dissection, prolapse)
  - Why needed here: The 10 evaluation metrics map to this workflow; understanding clinical logic is essential for debugging failure cases.
  - Quick check question: A stent has MSA 4.2mm² and expansion 68%—which two post-PCI metrics would likely trigger recommendations for further optimization?

## Architecture Onboarding

- Component map: OCT hardware (P80, Vivolight) → raw pullback images → small-model OCT analysis (13 functions) → structured quantitative parameters → RAG retrieval from guidelines + >100k cases → large-model reasoning (DeepSeek-R1) → structured recommendations

- Critical path: Image quality validation → small-model segmentation → quantitative parameter extraction → RAG retrieval → large-model reasoning → structured output. Failures at segmentation or retrieval propagate directly to recommendations.

- Design tradeoffs:
  - 14B model (smaller, deployable on-prem) vs. larger proprietary models (better reasoning, but data leaves site)
  - RAG grounding (reduces hallucination, adds latency and retrieval maintenance burden) vs. pure parametric inference
  - Tolerance thresholds (stent diameter ±0.5mm, length ±5mm) balance clinical acceptability vs. strictness of evaluation

- Failure signatures:
  - Low agreement on pretreatment device selection (73.6%) suggests ambiguity in calcium/lesion complexity assessment or guideline ambiguity
  - Performance drops in severely calcified lesions (calcium score=4) vs. mild (calcium score<4)—small-model perception limits
  - ChatGPT-5 underperformance on stent expansion (33.0%) indicates lack of structured image input

- First 3 experiments:
  1. Segmentation stress test: Run small-model OCT analysis on held-out images with motion artifacts, heavy calcification, and thrombus—measure segmentation failure rate and parameter drift.
  2. Retrieval ablation: Disable RAG and compare CA-GPT outputs vs. expert records—isolate the contribution of grounding vs. parametric knowledge.
  3. Cross-center validation: Apply system to external center data using different OCT hardware and local guidelines—measure agreement degradation and identify calibration gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the superior decision-making agreement observed with the CA-GPT system translate into improved long-term clinical endpoints, such as reduced Major Adverse Cardiac Events (MACE) or mortality?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "Future prospective studies will be required to determine whether this consistency translates into improved clinical outcomes."
- Why unresolved: This study was retrospective and focused solely on decision consistency metrics rather than tracking patient prognosis after discharge.
- What evidence would resolve it: A prospective randomized controlled trial (RCT) correlating CA-GPT-guided PCI decisions with long-term clinical follow-up data.

### Open Question 2
- Question: Is the performance of the CA-GPT system generalizable to other healthcare settings, different OCT hardware platforms, and diverse patient populations?
- Basis in paper: [explicit] The authors acknowledge that "this was a single-center, retrospective study, limiting the generalizability... External validation across multiple centers... is required."
- Why unresolved: The model was trained and validated exclusively on data from Tangdu Hospital using specific Vivolight hardware (P80).
- What evidence would resolve it: External validation studies conducted at multiple centers using different OCT imaging systems and patient demographics.

### Open Question 3
- Question: How does the hybridization of human-AI workflows, where AI recommendations are dynamically weighted against operator judgment, optimize procedural outcomes compared to human-only decision-making?
- Basis in paper: [explicit] The authors state that "future work should continue to explore the hybridization of human-AI workflows... to optimize clinical outcomes."
- Why unresolved: The current study design compared AI outputs and junior physician decisions against a static reference standard, rather than evaluating the synergy of a human operator assisted by the AI in real-time.
- What evidence would resolve it: Studies analyzing final procedural decisions and outcomes in scenarios where operators actively modify AI suggestions versus those where they follow them strictly.

## Limitations
- Proprietary algorithms: 6 of 13 small-model OCT analysis functions are not publicly specified, limiting full reproducibility
- Corpus validation: The 100,000 annotated cases knowledge base is not independently verifiable
- Generalizability: Performance in non-Chinese populations and with non-Vivolight OCT hardware remains untested

## Confidence
- **High confidence**: Superior agreement scores of CA-GPT vs. junior physicians and ChatGPT-5 (P<0.001 for most metrics)
- **Medium confidence**: Mechanism of dual-layer architecture improving decision accuracy (indirect evidence only)
- **Medium confidence**: RAG grounding reducing hallucinations (supported by relative performance but not directly measured)

## Next Checks
1. Segmentation stress test: Evaluate small-model OCT analysis accuracy on images with motion artifacts, heavy calcification, and thrombus
2. Retrieval ablation study: Compare CA-GPT outputs with RAG disabled to quantify RAG's contribution to accuracy
3. Cross-center validation: Deploy system at external centers using different OCT hardware and local guidelines to assess generalizability