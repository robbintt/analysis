---
ver: rpa2
title: 'D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion
  MLLMs'
arxiv_id: '2511.12280'
source_url: https://arxiv.org/abs/2511.12280
tags:
- tokens
- visual
- step
- token
- merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce D3ToM, a method that accelerates inference
  in diffusion-based multimodal large language models by dynamically merging redundant
  visual tokens during the denoising process. The approach uses tokens generated in
  the previous denoising step (decider tokens) to build an importance map over visual
  tokens, retaining the most salient ones and merging the rest through similarity-based
  aggregation.
---

# D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs

## Quick Facts
- arXiv ID: 2511.12280
- Source URL: https://arxiv.org/abs/2511.12280
- Authors: Shuochen Chang; Xiaofeng Zhang; Qingyang Liu; Li Niu
- Reference count: 20
- Primary result: Achieves over 96% of baseline performance with only 10% of visual tokens

## Executive Summary
D$^{3}$ToM introduces a dynamic token merging technique for accelerating diffusion-based multimodal large language models during inference. The method uses tokens generated in the previous denoising step to build an importance map over visual tokens, retaining the most salient ones while merging redundant tokens through similarity-based aggregation. This approach integrates into a single transformer layer without modifying model parameters, achieving significant computational savings while maintaining high performance across various multimodal tasks.

## Method Summary
The D$^{3}$ToM method employs a decider-guided dynamic token merging strategy that operates during the denoising process of diffusion-based MLLMs. The core innovation lies in using tokens generated in the previous denoising step (decider tokens) to create an importance map that identifies redundant visual tokens. These redundant tokens are then merged through similarity-based aggregation, effectively shortening the visual sequence length. The method introduces a timestep-dependent merge ratio that adapts to the evolving visual saliency during diffusion, and the entire process integrates as a plug-and-play module within a single transformer layer.

## Key Results
- Achieves over 96% of baseline performance with only 10% of visual tokens
- Reduces computational cost to 30% of FLOPs compared to baseline models
- Achieves 43% of wall-clock time while maintaining high accuracy across multiple datasets
- Compatible with existing KV-cache optimizations for additional efficiency gains

## Why This Works (Mechanism)
The mechanism exploits temporal redundancy in the diffusion denoising process by identifying and merging similar visual tokens across timesteps. By using previously generated tokens as reference points for importance scoring, the method captures evolving visual saliency patterns that guide selective token retention. The similarity-based aggregation ensures that merged tokens preserve essential visual information while eliminating redundancy, maintaining task performance despite significant token reduction.

## Foundational Learning
- **Diffusion denoising in MLLMs**: Why needed - Understanding the iterative refinement process where visual tokens are progressively refined through denoising steps; Quick check - Verify how visual tokens evolve across denoising timesteps
- **Token importance scoring**: Why needed - To identify which tokens carry essential visual information versus redundant content; Quick check - Examine the distribution of importance scores across different visual regions
- **Similarity-based token aggregation**: Why needed - To merge redundant tokens while preserving visual semantics; Quick check - Validate that merged tokens maintain semantic coherence with original tokens
- **KV-cache optimization**: Why needed - To understand how token reduction interacts with existing caching mechanisms for inference acceleration; Quick check - Confirm compatibility with different caching strategies

## Architecture Onboarding

**Component Map**: Input visual tokens -> Importance scoring (using decider tokens) -> Similarity-based merging -> Shortened visual sequence -> Transformer processing

**Critical Path**: The most time-sensitive operations are the similarity computation for token merging and the importance map generation, as these directly impact the overall inference latency.

**Design Tradeoffs**: The method trades minor potential information loss from token merging against significant computational savings. The timestep-dependent merge ratio provides flexibility but requires careful tuning to balance performance and efficiency.

**Failure Signatures**: Potential failure modes include over-merging critical visual tokens, leading to performance degradation, or under-merging redundant tokens, resulting in suboptimal computational savings. Performance drops may occur on tasks requiring fine-grained visual detail.

**First Experiments**:
1. Ablation study on merge ratio schedules across different diffusion timesteps
2. Cross-dataset performance evaluation to test generalization beyond the four main datasets
3. Hardware-specific benchmarking on different GPU/CPU configurations to validate computational savings claims

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Performance claims are based on specific datasets (WikiHow, RecipeQA, VQAv2, ScienceQA) which may not generalize to all multimodal domains
- Computational savings metrics are presented relative to baseline models without accounting for potential overhead from the merge-decision mechanism
- The timestep-dependent merge ratio lacks extensive ablation studies showing sensitivity to different merge ratio schedules
- The method's effectiveness may depend on specific architectural choices in the underlying diffusion MLLM

## Confidence

**High confidence**: The core methodology of dynamic token merging using previous-step decider tokens is technically sound and well-implemented

**Medium confidence**: The reported computational savings and performance retention metrics, as they depend on specific implementation details and hardware configurations

**Medium confidence**: The claimed compatibility with KV-cache optimizations, as this requires empirical validation across different systems

## Next Checks
1. Conduct extensive ablation studies on the timestep-dependent merge ratio to quantify sensitivity and determine optimal scheduling across different diffusion timesteps
2. Test cross-architecture generalization by applying D3ToM to different diffusion MLLM variants beyond the specific models used in experiments
3. Perform hardware-specific benchmarking across different GPU/CPU configurations to validate the claimed computational savings in real-world deployment scenarios