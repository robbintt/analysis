---
ver: rpa2
title: 'LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source
  Attribution and Expert Review'
arxiv_id: '2507.05319'
source_url: https://arxiv.org/abs/2507.05319
tags:
- discharge
- medical
- summary
- generation
- lcds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LCDS is a logic-controlled discharge summary generation system
  designed to address the hallucination and attribution challenges in automated medical
  summarization. It constructs a source mapping table to localize content accurately,
  uses logic-guided prompts to improve factual correctness, and supports sentence-level
  attribution for expert review.
---

# LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review

## Quick Facts
- arXiv ID: 2507.05319
- Source URL: https://arxiv.org/abs/2507.05319
- Reference count: 26
- Evaluated LCDS on 500 EMRs across 15 departments, achieving 77.60 ROUGE-L, 75.26 LLM-as-a-Judge, and 79.45 human evaluation scores.

## Executive Summary
LCDS addresses hallucination and attribution challenges in automated discharge summary generation by combining source mapping, logic-guided prompting, and sentence-level attribution. The system constructs a source mapping table using BM25 similarity to filter relevant EMR content, applies structured logic types to constrain generation, and supports expert review with traceable source links. Evaluated on real-world clinical data, LCDS outperforms baselines in both automated and human evaluations while enabling iterative model improvement through expert feedback.

## Method Summary
LCDS fine-tunes ChatGLM3-6B with LoRA to create EMRLLM, then generates discharge summaries using a three-stage pipeline: source mapping via BM25 similarity (threshold >0.8) to constrain input scope, logic-guided prompts with five types (extraction, summarization, judgment, inference, knowledge) processed by GPT-4o, and sentence-level attribution linking generated content to source EMR sentences. Experts review and correct silver summaries to create golden summaries for incremental fine-tuning. The system handles 8 EMR document types converted to unified JSON format and generates 6 discharge summary fields.

## Key Results
- Achieved 77.60 ROUGE-L score on discharge summary generation task
- Scored 75.26 on LLM-as-a-Judge evaluation across accuracy, completeness, standardization, and practicality
- Attained 79.45 in human evaluation, demonstrating superior performance over baseline systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining input scope via source mapping reduces hallucinations from irrelevant content.
- Mechanism: LCDS constructs a source mapping table by calculating textual similarity (BM25) between EMR fields and discharge summary fields. For long-text fields, it performs semantic segmentation (e.g., separating surgery/chemotherapy/pathology), then retrieves only high-similarity source segments (>0.8 threshold) as valid inputs for generation. This narrows the model's attention to clinically relevant records.
- Core assumption: Irrelevant or redundant EMR content contributes disproportionately to hallucinations, and similarity-based filtering preserves necessary information.
- Evidence anchors:
  - [abstract] "LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content."
  - [section] "Fields with similarity scores exceeding 0.8 are considered valid sources... ensuring that the model generates high-quality discharge summaries that better meet clinical needs within the constraints of limited scope."
  - [corpus] Related work on discharge summarization (e.g., arXiv:2512.06812) similarly emphasizes extractive pre-filtering, but corpus papers do not directly validate similarity-threshold mechanisms for hallucination reduction.
- Break condition: If similarity scores do not correlate with clinical relevance (e.g., key temporal events have low textual similarity), filtering may exclude critical information.

### Mechanism 2
- Claim: Logic-guided prompts with explicit rule types constrain generation space and improve factual correctness.
- Mechanism: LCDS categorizes generation into five logic types—Extraction, Summarization, Judgment, Inference, Knowledge—each with tailored prompt structures. GPT-4o performs three-stage processing (Task Parsing, Rule Matching, Logic Orchestration) to convert natural-language rules into structured prompts. This reduces open-ended generation and enforces department-specific constraints.
- Core assumption: Structured logical constraints suppress hallucinations more effectively than generic chain-of-thought prompting, and experts can encode reliable rules.
- Evidence anchors:
  - [abstract] "LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields."
  - [section] "To suppress hallucinations caused by free-text generation while accommodating the specific needs of different medical departments, we establish explicit generation rules and constraints."
  - [corpus] Corpus papers on LLM summarization (e.g., arXiv:2502.20647) highlight hallucination risks in abstractive summarization but do not evaluate logic-guided prompting specifically.
- Break condition: If logical rules are incomplete, ambiguous, or conflict across departments, the system may over-constrain or produce brittle outputs.

### Mechanism 3
- Claim: Sentence-level attribution enables expert feedback loops that incrementally improve model quality.
- Mechanism: LCDS segments generated summaries at sentence level, uses GPT-4o to identify supporting source sentences (via unique identifiers), and presents aligned content in the review UI. Experts correct, annotate, and export "golden" summaries, which accumulate into a fine-tuning corpus for iterative model improvement.
- Core assumption: Accurate attribution is feasible, and expert-reviewed corrections generalize to future cases.
- Evidence anchors:
  - [abstract] "LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs."
  - [section] "On the user interface, when a user clicks on a sentence in the generated DS, the system highlights the corresponding original medical record sentences with the same identifier, facilitating easy comparison and verification."
  - [corpus] Related work (e.g., arXiv:2506.14101) emphasizes hallucination risks in discharge summarization but does not validate iterative human-in-the-loop correction pipelines.
- Break condition: If attribution fails (mismatched sentences, ambiguous references), experts cannot efficiently verify, and the feedback loop degrades.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) with similarity scoring**
  - Why needed here: LCDS relies on BM25 and semantic similarity to filter source content before generation. Understanding how retrieval quality affects downstream accuracy is essential.
  - Quick check question: Can you explain why high similarity does not always imply clinical relevance?

- Concept: **Prompt engineering with structured logic types**
  - Why needed here: The system uses five logic types (Extraction, Summarization, Judgment, Inference, Knowledge) with multi-stage prompt construction. You must understand how to design and validate these constraints.
  - Quick check question: How would you test whether a "Judgment" rule is too strict or too lenient?

- Concept: **Human-in-the-loop fine-tuning pipelines**
  - Why needed here: LCDS accumulates silver/golden summaries for incremental fine-tuning. You need to understand data curation, annotation consistency, and feedback loop design.
  - Quick check question: What safeguards prevent expert-reviewed "golden" summaries from encoding systematic biases?

## Architecture Onboarding

- Component map:
  Input Module -> JSON Conversion -> Source Mapping Engine -> Logic-Guided Prompt Constructor -> Summary Generator -> Attribution Engine -> Review Interface -> Fine-Tuning Pipeline

- Critical path:
  1. EMR upload → JSON conversion (consistency check).
  2. Source mapping table construction (verify similarity thresholds).
  3. Logic-guided prompt generation (validate rule coverage per department).
  4. Silver summary generation.
  5. Attribution alignment (spot-check sentence-to-source links).
  6. Expert review → golden summary export.
  7. Periodic fine-tuning with accumulated data.

- Design tradeoffs:
  - Similarity threshold (0.8) balances precision vs. recall; lower thresholds include more context but risk noise.
  - Rule-based logic improves consistency but requires domain expert maintenance.
  - Sentence-level attribution increases granularity but may miss multi-sentence dependencies.

- Failure signatures:
  - Low similarity scores across key fields → empty or sparse source mapping.
  - Rule mismatches → generic or incomplete summaries.
  - Attribution gaps → unlinked sentences, expert confusion.
  - Fine-tuning degradation → if golden summaries contain systematic errors.

- First 3 experiments:
  1. **Source mapping ablation**: Vary similarity threshold (0.6, 0.7, 0.8, 0.9) and measure ROUGE-L, hallucination rate, and expert review time.
  2. **Logic type coverage test**: For a single department, systematically disable each logic type and measure impact on accuracy/completeness scores.
  3. **Attribution accuracy audit**: Randomly sample 50 generated sentences, manually verify source links, and quantify false positive/negative attribution rates.

## Open Questions the Paper Calls Out

- **Question**: How robust is the LCDS framework when applied to healthcare settings with EMR structures significantly different from the 15 departments used in the current study?
- **Basis in paper**: [explicit] The authors state in the Limitations section that reliance on a specific dataset "may limit the model’s generalization ability and result in degraded performance when applied to different healthcare settings."
- **Why unresolved**: The current evaluation uses a specific set of 500 EMRs for training and 150 for testing, which may not represent the structural variance found in different hospital systems or international records.
- **What evidence would resolve it**: Performance metrics (ROUGE-L, Human Eval) from cross-institutional validation tests using out-of-distribution EMR formats.

- **Question**: What is the specific accuracy of the GPT-4o attribution mechanism in correctly identifying source sentences compared to a human gold standard?
- **Basis in paper**: [inferred] While the system supports attribution to aid expert review, the paper evaluates the quality of the final summary but does not report quantitative metrics (e.g., precision/recall) for the sentence-level source mapping step itself.
- **Why unresolved**: The system relies on LLM-based attribution (GPT-4o) to verify content, but if the attribution itself is hallucinated or incorrect, the "traceability" benefit is undermined.
- **What evidence would resolve it**: A separate evaluation of the attribution module showing the percentage of generated sentences correctly linked to their ground-truth source text.

- **Question**: Does the use of LCDS actually reduce the time physicians spend on creating discharge summaries in a live clinical workflow?
- **Basis in paper**: [explicit] The authors note that despite automated evaluation, "a more comprehensive assessment... remains necessary" and suggest "future work could incorporate... real-world clinical testing."
- **Why unresolved**: The current results rely on retrospective generation and scoring (automatic and human), rather than prospective time-motion studies or user experience feedback in an active clinical environment.
- **What evidence would resolve it**: Results from a prospective user study measuring the time taken for "expert review and rectification" versus manual writing from scratch.

## Limitations
- System effectiveness depends heavily on expert-curated rule sets and similarity thresholds that may not generalize across institutions
- Sentence-level attribution assumes clean segmentation which may not hold for multi-sentence dependencies or implicit reasoning
- Study evaluates on a fixed dataset from a single institution, limiting external validity

## Confidence

- **High confidence**: LCDS architecture and implementation details are clearly specified; the system demonstrably generates structured discharge summaries with source attribution capabilities.
- **Medium confidence**: The reported performance improvements over baselines are internally valid but require independent replication on external datasets to confirm generalizability.
- **Low confidence**: Claims about hallucination reduction mechanisms lack direct ablation studies isolating individual components (e.g., source mapping vs. logic constraints).

## Next Checks

1. **External validation**: Evaluate LCDS on discharge summaries from a different hospital system to assess cross-institutional performance and rule generalizability.

2. **Ablation study**: Systematically disable source mapping filtering and logic-guided prompting separately to quantify each mechanism's contribution to hallucination reduction.

3. **Attribution robustness audit**: Conduct comprehensive manual review of 100 randomly sampled generated sentences to measure false positive/negative rates in source attribution and identify edge cases where the system fails.