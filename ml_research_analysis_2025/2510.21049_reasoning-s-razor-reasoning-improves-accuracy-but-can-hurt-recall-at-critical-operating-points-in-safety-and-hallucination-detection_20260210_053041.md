---
ver: rpa2
title: 'Reasoning''s Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical
  Operating Points in Safety and Hallucination Detection'
arxiv_id: '2510.21049'
source_url: https://arxiv.org/abs/2510.21049
tags:
- think
- reasoning
- classification
- safety
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates reasoning's impact on classification\
  \ tasks under strict low false positive rate (FPR) regimes. The authors compare\
  \ two inference paradigms\u2014Think On (with reasoning) and Think Off (without\
  \ reasoning)\u2014across safety detection and hallucination detection tasks."
---

# Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection

## Quick Facts
- arXiv ID: 2510.21049
- Source URL: https://arxiv.org/abs/2510.21049
- Reference count: 40
- Primary result: Reasoning degrades recall at low false positive rates (≤1%) despite improving overall accuracy in safety and hallucination detection tasks.

## Executive Summary
This paper reveals a critical trade-off in using reasoning for classification tasks: while reasoning improves overall accuracy, it degrades recall at strict low false positive rate (FPR) operating points essential for safety-critical applications. Through systematic evaluation across multiple safety and hallucination detection datasets, the authors demonstrate that reasoning causes confidence polarization—incorrect predictions become increasingly overconfident during reasoning chains, making them harder to filter out at strict thresholds. Token-based scoring substantially outperforms self-verbalized confidence at low FPR, and simple ensembling of both inference modes recovers the strengths of each approach.

## Method Summary
The authors evaluate two inference paradigms—Think On (with reasoning) and Think Off (without reasoning)—across safety detection and hallucination detection tasks using fine-tuned Llama-3-8B and various large reasoning models. They implement token-based scoring by extracting classification probabilities from logits of specific tokens ("Safe"/"Unsafe" or "PASS"/"FAIL") after reasoning, and self-verbalized confidence by parsing integer confidence statements (0-100) from model outputs. The primary evaluation metric is TPR@FPR=α, measuring recall achievable while maintaining false positive rates at or below specific thresholds (α=0.01, 0.03, 0.05). The study includes 10 safety datasets (ToxicChat, OpenAI Moderation, AegisSafetyTest, WildGuardTest, HarmBench, SafeRLHF, BeaverTails, XSTestResponse) and 7 hallucination datasets (HaluBench comprising HaluEval, DROP, PubMedQA, CovidQA, FinanceBench, RAGTruth).

## Key Results
- Fine-tuned safety classifiers achieve 40.0% recall without reasoning but only 13.8% with reasoning at 1% FPR, while maintaining higher overall accuracy with reasoning (82.3% vs 77.9%)
- Token-based scoring outperforms self-verbalized confidence at low FPR thresholds, with verbalized confidence achieving zero recall on multiple datasets at 1% FPR
- Reasoning causes confidence polarization, where incorrect predictions become increasingly overconfident during reasoning chains
- Ensembling Think On and Think Off scores recovers complementary strengths, achieving both high accuracy and practical low-FPR recall

## Why This Works (Mechanism)

### Mechanism 1: Confidence Polarization Through Extended Reasoning
Reasoning chains systematically inflate model confidence toward extreme values, causing incorrect predictions to appear with near-certainty. As tokens are generated through reasoning, the model progressively commits to its initial classification direction. The authors demonstrate this by measuring class probabilities at each token position—showing that "positive-class probabilities steadily increase during reasoning—approaching near-certain confidence even when the ground truth is negative" (Section 3.2, Figure 3). This creates heavier distribution tails, making false positives harder to separate at strict thresholds.

### Mechanism 2: Token-Based vs. Self-Verbalized Confidence Divergence at Low FPR
Token-based scoring provides continuous probability values enabling fine-grained threshold tuning. Self-verbalized confidence uses discrete integer categories (0-100), creating quantization effects that prevent precise FPR control. The paper finds reasoning affects these methods oppositely: reasoning degrades token-based performance at low FPR but modestly improves verbalized confidence (Table 2, Section 3.4).

### Mechanism 3: Mode Ensembling Recovers Complementary Strengths
Think On and Think Off exhibit dataset-dependent trade-offs. On CovidQA, Think On achieves 70.6% TPR@FPR=1% vs. Think Off's 26.6%; on HaluEval, Think Off achieves 66.2% vs. Think On's 45.9% (Figure 7). The ensemble captures whichever mode is stronger per-dataset, producing robust weighted-average performance across both metrics.

## Foundational Learning

- **Concept: Operating Point Analysis (TPR@FPR=α)**
  - Why needed here: Standard accuracy and AUROC metrics mask performance in deployment-critical low-FPR regimes. The paper's core finding only emerges when evaluating at FPR ≤ 1%.
  - Quick check question: Given a classifier with 90% accuracy but 15% greedy FPR, what recall can you achieve if you must keep FPR below 1%?

- **Concept: Confidence Calibration and Score Distributions**
  - Why needed here: Understanding how reasoning shifts score distributions (polarization vs. moderation) is essential for interpreting why Think Off outperforms Think On at low FPR despite lower accuracy.
  - Quick check question: If a model's confidence scores for incorrect predictions are concentrated near 0.95, what happens to recall when you set a threshold to achieve 1% FPR?

- **Concept: Token-Logit Probability Extraction**
  - Why needed here: The paper's evaluation methodology relies on extracting classification probabilities from specific tokens rather than from generated text.
  - Quick check question: How would you extract the probability that a model assigns to "Unsafe" vs. "Safe" given the logits for these two tokens?

## Architecture Onboarding

- **Component map:** Think On (reasoning → classification) -> Think Off (direct classification) -> Token-based scoring (logits) -> Self-verbalized scoring (confidence parsing) -> Evaluation (TPR@FPR)

- **Critical path:**
  1. Select task (safety or hallucination detection)
  2. Choose model (fine-tuned, zero-shot LLM, or LRM)
  3. Run both Think On and Think Off inference, extracting token logits
  4. Plot score distributions (logit-transformed) to assess polarization
  5. Compute TPR@FPR=0.01, 0.03, 0.05 across both modes
  6. If needed, ensemble scores with equal weighting

- **Design tradeoffs:**
  - Think On: Higher accuracy, interpretability via reasoning traces, but degraded low-FPR recall
  - Think Off: Better low-FPR recall, lower accuracy, no interpretability
  - Ensemble: Best of both but doubles inference cost
  - Token-based scoring: Requires logit access; self-verbalized works with API-only models but fails at strict FPR

- **Failure signatures:**
  - TPR@FPR=0.01 near zero despite reasonable accuracy → confidence polarization
  - Large gap between greedy FPR and target FPR → miscalibrated default thresholds
  - Self-verbalized confidence achieving zero recall at 1% FPR → quantization limits

- **First 3 experiments:**
  1. Reproduce polarization visualization: Take a false positive case, plot token-by-token probability evolution (as in Figure 3a). Verify confidence inflates through reasoning.
  2. Compare modes at multiple FPR thresholds: Compute TPR@FPR∈{0.01, 0.03, 0.05, 0.15} for both modes on your target dataset. Identify where Think Off overtakes Think On.
  3. Test ensemble on your domain: Combine Think On/Off scores with 0.5 weights. Report accuracy and TPR@FPR=0.01. Compare to individual modes.

## Open Questions the Paper Calls Out

- Does the accuracy-recall trade-off at low FPR generalize to other reasoning paradigms (multi-step, reflective, or tool-augmented reasoning), or is it specific to standard chain-of-thought approaches?
- Why does reasoning polarize model confidence toward extreme values, and can this polarization be prevented through architectural or training modifications?
- Why does reasoning improve self-verbalized confidence calibration at low FPR while degrading token-based scoring, and can this divergence be unified?

## Limitations

- Conclusions are drawn from safety and hallucination detection tasks and may not generalize to other classification domains or multi-class problems
- All experiments use Llama-3-8B or similar models, leaving architecture-specific effects unexplored
- Results depend heavily on prompt engineering, and robustness across different prompt formulations is not established
- The paper does not quantify practical deployment trade-offs between accuracy gains and computational overhead from ensembling

## Confidence

- **High confidence (5/5):** The empirical observation that reasoning degrades recall at low FPR thresholds is robustly supported across multiple datasets and models
- **Medium confidence (3/5):** The attribution of low-FPR degradation to confidence polarization is mechanistically plausible but alternative explanations cannot be ruled out
- **Low confidence (2/5):** The claim that self-verbalized confidence fails catastrophically at 1% FPR due to quantization effects lacks mechanistic depth

## Next Checks

- Replicate the core experiments (TPR@FPR=0.01 comparison) using different model architectures to determine whether confidence polarization is architecture-specific
- Systematically vary the reasoning prompt structure while keeping the classification target constant to establish robustness to prompt engineering
- Implement a stopping criterion that halts reasoning when confidence converges and compare this to full-length reasoning to test whether polarization can be mitigated through early stopping