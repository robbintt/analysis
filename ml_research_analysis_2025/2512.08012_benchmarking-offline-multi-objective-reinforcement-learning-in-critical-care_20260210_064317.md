---
ver: rpa2
title: Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care
arxiv_id: '2512.08012'
source_url: https://arxiv.org/abs/2512.08012
tags:
- learning
- cpql
- peda
- offline
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks offline multi-objective reinforcement learning\
  \ (MORL) for critical care decision-making using the MIMIC-IV dataset. The study\
  \ evaluates three MORL algorithms\u2014Conditioned CPQL, Adaptive CPQL, and PEDA\
  \ Decision Transformer\u2014against three scalarized single-objective baselines\
  \ (BC, CQL, DDQN) using WIS and FQE off-policy evaluation metrics."
---

# Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care

## Quick Facts
- arXiv ID: 2512.08012
- Source URL: https://arxiv.org/abs/2512.08012
- Reference count: 22
- Primary result: PEDA Decision Transformer achieves superior flexibility across varying preference vectors compared to scalarized baselines in offline multi-objective critical care RL

## Executive Summary
This paper benchmarks offline multi-objective reinforcement learning (MORL) for critical care decision-making using the MIMIC-IV dataset. The study evaluates three MORL algorithms—Conditioned CPQL, Adaptive CPQL, and PEDA Decision Transformer—against three scalarized single-objective baselines (BC, CQL, DDQN) using WIS and FQE off-policy evaluation metrics. The key finding is that the PEDA DT algorithm demonstrates superior flexibility across varying preference vectors compared to static scalarized baselines, successfully learning a diverse set of policies along the Pareto frontier. This confirms that Decision Transformer architectures remain effective when extended to multi-objective settings and validates MORL as a promising framework for enabling personalized, adjustable decision-making in critical care without retraining. The work also highlights the limitations of single-objective approaches that optimize fixed trade-offs and cannot adapt to changing clinical priorities.

## Method Summary
The study addresses the challenge of learning ICU policies that balance mortality minimization and length-of-stay (LOS) reduction. Three MORL algorithms (Conditioned CPQL, Adaptive CPQL with Preference Attention, PEDA Decision Transformer) are compared against scalarized baselines (BC, CQL, DDQN) using the MIMIC-IV dataset. All algorithms use WIS and FQE off-policy evaluation metrics across preference vectors ω ∈ {[0.0,1.0], ..., [1.0,0.0]}. The baselines optimize a fixed scalarized reward R_scalar = 0.5·r_mortality + 0.5·r_LOS, while MORL methods can adapt to different preferences at inference time. The primary performance metric is estimated policy value across the preference spectrum.

## Key Results
- PEDA DT demonstrates superior flexibility across varying preference vectors compared to scalarized baselines
- WIS and FQE metrics show PEDA DT learns diverse policies along the Pareto frontier
- Scalarized single-objective baselines (BC, CQL, DDQN) produce rigid policies with flat performance across preferences
- MORL framework enables personalized, adjustable decision-making without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-conditioned sequence modeling enables policy adaptation without retraining.
- Mechanism: PEDA DT accepts a preference vector ω at inference time, conditioning action generation on the desired trade-off between mortality and LOS. The transformer architecture treats (state, action, reward-to-go, preference) as a sequence, learning to map preferences to corresponding optimal behaviors via attention over historical trajectories.
- Core assumption: The offline dataset contains sufficient coverage of trajectories that span the Pareto frontier, allowing the model to generalize to unseen preference combinations through compositional reasoning.
- Evidence anchors:
  - [abstract] "allowing for dynamic preference selection at test time"
  - [section 4.3] "uniquely well-suited for the 'stitching' of suboptimal trajectories required in multi-objective conditioned generation"
  - [corpus] Neighbor papers confirm Pareto Set Learning as an active MORL approach, but none specifically validate transformer-based stitching in healthcare.
- Break condition: If the offline dataset has poor coverage of high-mortality or prolonged-LOS trajectories, preference conditioning may extrapolate unsafely.

### Mechanism 2
- Claim: Scalarized single-objective baselines produce rigid policies that cannot adapt to shifting clinical priorities.
- Mechanism: BC, CQL, and DDQN optimize for a fixed reward R_scalar = 0.5·r_mortality + 0.5·r_LOS. This commits the policy to a single trade-off during training. When clinical needs change (e.g., prioritize survival during a pandemic), the policy cannot adjust without full retraining.
- Core assumption: Clinical preferences are known and stable at training time—an assumption the paper argues is often false in practice.
- Evidence anchors:
  - [abstract] "Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies"
  - [section 4.2, Table 2] BC and CQL show flat FQE scores (~0.55–0.60) across all preference vectors
  - [corpus] "Limitations of Scalarisation in MORL" (arXiv:2511.16476) corroborates scalarization struggles in complex environments
- Break condition: If preferences truly are fixed and known a priori, scalarized methods may be more sample-efficient and simpler to deploy.

### Mechanism 3
- Claim: Off-Policy Evaluation (OPE) metrics WIS and FQE provide proxy estimates of policy value without patient deployment, but have confidence-interval trade-offs.
- Mechanism: WIS re-weights observed returns by the likelihood ratio between learned and behavior policies; FQE learns a Q-function via Bellman bootstrapping. Both estimate policy value offline, enabling safe comparison.
- Core assumption: The behavior policy π_b is known or well-estimated, and the dataset supports reliable importance sampling without extreme variance.
- Evidence anchors:
  - [section 3.2.1] WIS formula with cumulative importance ratio ρ_{1:t}
  - [section 4.1, Table 1] WIS confidence intervals widen substantially for CPQL variants at extreme preferences (e.g., [0.9, 0.1] shows 0.471±0.843)
  - [corpus] No direct validation of OPE accuracy in healthcare MORL; this remains an open concern
- Break condition: If importance ratios explode due to distributional shift, WIS estimates become unreliable; FQE may overestimate values if Q-function extrapolates.

## Foundational Learning

- Concept: **Pareto Frontier in Multi-Objective Optimization**
  - Why needed here: MORL aims to learn a set of policies where no single policy dominates another across all objectives. Understanding trade-off curves is essential for interpreting why multiple policies matter.
  - Quick check question: Given two policies where Policy A achieves lower mortality but longer LOS than Policy B, can both be Pareto-optimal?

- Concept: **Offline Reinforcement Learning Constraints**
  - Why needed here: Healthcare prohibits online exploration. You must learn from fixed historical data, which introduces distributional shift and overestimation risks.
  - Quick check question: Why does standard Q-learning overestimate values for actions not well-represented in the offline dataset?

- Concept: **Sequence Modeling for Decision-Making (Decision Transformers)**
  - Why needed here: PEDA DT frames RL as autoregressive sequence prediction over (state, action, return-to-go, preference). This contrasts with value-based methods and enables trajectory stitching.
  - Quick check question: How does conditioning on return-to-go differ from maximizing expected return in traditional RL?

## Architecture Onboarding

- Component map:
  - State encoder (vitals, labs) + preference vector ω → embedded representation
  - Transformer backbone (attention layers) over sequence of (s_t, a_t, R_t, ω) tuples
  - Action head (predicts discrete intervention like vasopressor dosage, fluid administration)
  - OPE evaluator (separate WIS and FQE modules for offline policy scoring)

- Critical path:
  1. Extract and preprocess MIMIC-IV trajectories (physiological states, interventions, mortality, LOS)
  2. Train PEDA DT with preference-conditioned sequence modeling
  3. Run OPE (WIS + FQE) across preference grid ω ∈ [0.0, 1.0]
  4. Compare against scalarized baselines (BC, CQL, DDQN) trained with fixed ω = [0.5, 0.5]

- Design tradeoffs:
  - **PEDA DT vs. CPQL**: Transformers offer flexible conditioning but are less interpretable; CPQL variants are value-based with clearer Q-function semantics but showed wider confidence intervals in this study
  - **WIS vs. FQE**: WIS is model-free but high-variance; FQE is lower-variance but assumes Q-function accuracy
  - **Preference granularity**: Finer preference grids increase flexibility but may require more data coverage

- Failure signatures:
  - Exploding WIS confidence intervals at extreme preferences (e.g., ω near 0 or 1) indicate insufficient trajectory coverage
  - Flat FQE scores across preferences suggest preference conditioning failed to propagate
  - Large train-test gap in OPE metrics signals overfitting to behavior policy

- First 3 experiments:
  1. Reproduce scalarized baselines (BC, CQL, DDQN) with ω = [0.5, 0.5] and verify static performance across preference sweeps
  2. Train PEDA DT and plot WIS/FQE curves vs. ω; check that estimated values vary with preference and confidence intervals remain bounded
  3. Ablate the preference conditioning mechanism (set ω = constant) and confirm performance degrades to scalarized baseline levels

## Open Questions the Paper Calls Out

- **Explainability for OMORL**: The paper explicitly calls for developing explainability methods for offline multi-objective RL (OMORL), noting that the "black-box nature of Transformer models remains a barrier to clinical adoption." While PEDA DT demonstrates high performance, it currently provides no transparent justification for why it recommends a specific trade-off to a clinician.

- **Model-Based RL Extensions**: Section 5.2 identifies "Model-Based RL" as a critical avenue, suggesting that World Models could provide a "more reliable training environment" for simulating counterfactuals in OMORL settings. This study focused exclusively on model-free algorithms, leaving the potential benefits of learning a dynamics model unexplored.

- **OPE Reliability**: The paper's conclusion that PEDA DT is superior based on mean OPE metrics is potentially undermined by the extremely wide confidence intervals observed (e.g., 0.899 ± 0.768 for PEDA DT at [0.0, 1.0]). The overlapping confidence intervals with baselines and the massive variance make statistical significance unclear.

## Limitations

- The study relies on OPE metrics rather than ground-truth online evaluation, leaving open the possibility that WIS/FQE estimates misalign with actual clinical performance
- The offline dataset's coverage of extreme preference regions remains unverified, which could invalidate PEDA DT's ability to safely interpolate across the Pareto frontier
- The paper does not provide sufficient detail on cohort definitions or reward scaling to enable exact reproduction

## Confidence

- **High confidence**: PEDA DT architecture's ability to condition on preference vectors at inference time; MORL framework's relevance to healthcare personalization
- **Medium confidence**: OPE metric rankings (WIS/FQE); relative performance of scalarized baselines
- **Low confidence**: Extrapolation safety to extreme preferences; absolute OPE value accuracy; exact model hyperparameter effects

## Next Checks

1. Verify OPE estimates with online rollouts in a simulated critical care environment using the same reward functions and preference vectors
2. Perform sensitivity analysis on preference vector granularity and check WIS confidence intervals across the full [0,1] range
3. Compare behavior policy estimation methods (empirical vs. learned) and measure their impact on WIS variance for extreme preferences