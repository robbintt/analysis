---
ver: rpa2
title: 'Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic
  User Intents'
arxiv_id: '2506.01689'
source_url: https://arxiv.org/abs/2506.01689
tags:
- video
- query
- queries
- user
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of current query-answering
  datasets that focus on textual responses, neglecting complex queries that require
  visual demonstrations. The authors construct RealVideoQuest, a benchmark for evaluating
  text-to-video (T2V) models on real-world, visually grounded user queries.
---

# Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents

## Quick Facts
- arXiv ID: 2506.01689
- Source URL: https://arxiv.org/abs/2506.01689
- Reference count: 26
- Current T2V models struggle with real-world user queries despite good visual quality scores

## Executive Summary
This paper addresses the limitation of current query-answering datasets that focus on textual responses, neglecting complex queries that require visual demonstrations. The authors construct RealVideoQuest, a benchmark for evaluating text-to-video (T2V) models on real-world, visually grounded user queries. The dataset comprises 7.5K real user queries with video response intents sourced from Chatbot-Arena and 4.5K high-quality query-video pairs built through a multistage retrieval and refinement process. A multi-angle evaluation system with four metrics (relevance, correctness, coherence, completeness) is developed to assess generated video answers. Experiments show that current T2V models struggle to effectively address real user queries, with low scores on QA-quality metrics despite generating visually appealing videos.

## Method Summary
The authors construct RealVideoQuest by first collecting user queries from Chatbot-Arena and filtering them through a Video Intent Recognition (VIR) system to identify queries that benefit from video responses. They then retrieve YouTube videos for these queries, split them into semantically coherent segments using PySceneDetect and ImageBind embeddings, and refine both the video segments and original queries through a multistage process involving similarity scoring with BGE embeddings and query rewriting with GPT-4o. The resulting benchmark contains 4.5K high-quality query-video pairs across four intent categories. Evaluation uses GPT-4o-mini as an LLM-as-a-Judge to assess video answers across four QA-quality metrics (relevance, correctness, coherence, completeness) on a 0-4 scale, supplemented by traditional visual quality metrics (VBench and VideoScore).

## Key Results
- All tested T2V models (T2V-Turbo-v2, CogVideoX-5B, Hunyuan, SkyReels, Wan2.1) score poorly on QA-quality metrics (0.12-0.38) despite good visual quality scores (0.75-0.97) on VBench
- Completeness proves especially challenging for current T2V models when handling multi-step queries
- The benchmark successfully identifies a gap between visual quality and task completion capability in current T2V systems

## Why This Works (Mechanism)

### Mechanism 1: Video Intent Recognition Pipeline
- Claim: Filtering queries that benefit from video responses enables focused benchmark construction from heterogeneous conversation data.
- Mechanism: GPT-3.5-Turbo-based classifier assigns 3-scale labels (0=text better, 1=equal, 2=video better) to queries from Chatbot-Arena, retaining labels ≥1 as video-intent queries.
- Core assumption: LLM-based classification reliably distinguishes query types that benefit from visual vs textual responses.
- Evidence anchors:
  - [abstract]: "identifies 7.5K real user queries with video response intents from Chatbot-Arena"
  - [section]: Table 4 shows VIR achieves 0.87 accuracy and 1.0 recall, though precision is only 0.2778; authors prioritize recall since "downstream retrieval can inherently filter out unsuitable queries"
  - [corpus]: No direct corpus evidence for this specific intent recognition approach
- Break condition: If downstream retrieval cannot effectively filter false positives, low precision may introduce noise that degrades benchmark quality.

### Mechanism 2: Query-Video Alignment Through Multi-stage Retrieval
- Claim: Refining both video segments and queries improves semantic alignment between user intent and ground-truth answers.
- Mechanism: YouTube retrieval → PySceneDetect splitting → ImageBind embedding with 0.3 cosine threshold merging → Qwen2-VL captioning → BGE similarity scoring → GPT-4o query rewriting conditioned on video content.
- Core assumption: Retrieved YouTube videos adequately represent ideal responses users expect from generative T2V systems.
- Evidence anchors:
  - [abstract]: "multistage video retrieval and refinement process" producing "4.5K high-quality query-video pairs"
  - [section]: Figure 1(c) shows 80.44% of rewritten queries rated as "Relevant" (score 3/4) to original queries
  - [corpus]: VC4VG paper supports optimizing video captions for T2V training alignment
- Break condition: If YouTube videos systematically differ from what generative models should ideally produce, benchmark may conflate retrieval quality with generation capability.

### Mechanism 3: LLM-as-a-Judge for QA-Quality Evaluation
- Claim: MLLMs can evaluate video answer quality across dimensions that traditional visual-quality metrics miss.
- Mechanism: GPT-4o-mini evaluates relevance, correctness, coherence, completeness on 0-4 scale using both reference-free (query+video only) and reference-based (includes ground-truth video) modes.
- Core assumption: MLLM judgments correlate with human assessment of whether videos answer queries informatively.
- Evidence anchors:
  - [abstract]: "multi-angle evaluation system to assess the quality of generated video answers"
  - [section]: Table 1 shows all models score 0.12-0.38 on QA metrics despite Table 2 showing VBench scores of 0.75-0.97 on visual quality; "completeness proves especially challenging"
  - [corpus]: VidCapBench similarly connects caption evaluation to T2V generation assessment
- Break condition: If MLLM evaluations don't correlate with human judgments for this specific task, metric validity collapses.

## Foundational Learning

- Concept: Text-to-Video Diffusion Models
  - Why needed here: Core architecture being evaluated; generates videos by iteratively denoising latent representations conditioned on text embeddings.
  - Quick check question: Can you explain how diffusion models balance temporal consistency vs frame-level quality?

- Concept: Multimodal Embedding Alignment
  - Why needed here: ImageBind encodes video clips; BGE computes query-caption similarity; cosine thresholds determine semantic matching.
  - Quick check question: How do you select similarity thresholds for cross-modal retrieval vs same-modal retrieval?

- Concept: Instruction-Following Evaluation
  - Why needed here: Traditional T2V metrics measure visual quality; this benchmark measures whether videos actually fulfill user intents.
  - Quick check question: What's the difference between text-to-video alignment (traditional) and query-answering completeness (this benchmark)?

## Architecture Onboarding

- Component map:
  - Data pipeline: Chatbot-Arena → VIR filter → YouTube retrieval → PySceneDetect → ImageBind/Qwen2-VL → BGE scoring → Query rewriter
  - Evaluation pipeline: Generated video → GPT-4o-mini judge (4 metrics) + VBench (6 dimensions) + VideoScore (5 dimensions)
  - Test set: 998 queries across 4 intent categories (skill demonstration, knowledge explanation, art creation, human-machine interaction)

- Critical path:
  1. Query collection and intent filtering (VIR recall critical)
  2. Video retrieval and segmentation (semantic coherence)
  3. Query-video alignment (BGE similarity + rewriting)
  4. Multi-metric evaluation (LLM-as-Judge prompt design)

- Design tradeoffs:
  - Recall vs precision in VIR: Authors chose high recall (1.0) at cost of low precision (0.28), relying on downstream retrieval to filter
  - Reference-based vs reference-free evaluation: Reference-based provides grounding but may over-constrain acceptable answers
  - Single video per query: Computational cost limits evaluation; may miss model variance

- Failure signatures:
  - High VBench scores + low QA metrics = model generates pretty videos that don't answer questions (observed in all tested models)
  - Low completeness scores = generated videos too short to cover multi-step queries
  - Factual inconsistencies in case studies (e.g., apple slices appearing "out of nowhere") indicate missing world knowledge

- First 3 experiments:
  1. Reproduce baseline evaluation on 50-100 query subset to validate evaluation pipeline produces scores in expected ranges
  2. Ablate reference-based vs reference-free evaluation modes to measure how ground-truth videos influence judge scores
  3. Test prompt sensitivity: vary LLM-as-Judge prompt wording to assess evaluation stability

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on LLM-based filtering with low precision (0.28), though authors mitigate this by relying on downstream retrieval to filter unsuitable candidates
- Assumption that YouTube videos represent ideal generative outputs may not hold, potentially conflating retrieval quality with generation capability
- Multi-stage pipeline introduces multiple failure points where errors compound

## Confidence

**High confidence:** The empirical finding that all tested T2V models score poorly on QA-quality metrics (0.12-0.38) despite good visual quality scores (0.75-0.97) on VBench - this pattern is clearly observed across multiple models and metric sets.

**Medium confidence:** The claim that completeness is especially challenging for current T2V models - while supported by low scores, this may partly reflect the difficulty of generating long-form content rather than a fundamental limitation.

**Medium confidence:** The benchmark construction methodology - the multi-stage pipeline is well-documented but depends on several assumptions (LLM reliability, YouTube video quality, semantic alignment thresholds) that haven't been fully validated.

## Next Checks
1. Manually sample 50-100 queries from the final benchmark to verify that VIR filtering didn't introduce significant noise despite low precision.
2. Conduct ablation studies comparing reference-based vs reference-free evaluation modes to quantify how much ground-truth videos influence judge scores.
3. Test prompt sensitivity by varying the LLM-as-Judge evaluation prompts to assess the stability of QA-quality metric scores.