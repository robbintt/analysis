---
ver: rpa2
title: Long-tailed Medical Diagnosis with Relation-aware Representation Learning and
  Iterative Classifier Calibration
arxiv_id: '2502.03238'
source_url: https://arxiv.org/abs/2502.03238
tags:
- classifier
- classes
- learning
- long-tailed
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the long-tailed problem in medical image classification,
  where a few common diseases dominate the dataset, leading to poor performance on
  rare diseases. The proposed Long-tailed Medical Diagnosis (LMD) framework addresses
  this by enhancing representation learning and classifier calibration.
---

# Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration

## Quick Facts
- arXiv ID: 2502.03238
- Source URL: https://arxiv.org/abs/2502.03238
- Reference count: 10
- Primary result: Proposes a two-stage framework (RRL + ICC) that outperforms SOTA on three long-tailed medical datasets (Hyper-Kvasir, ISIC-2019-LT, ISIC-Archive-LT) with significantly higher AUC, BACC, F1, and Kappa scores.

## Executive Summary
This paper addresses the challenge of long-tailed medical image classification where rare diseases are underrepresented in training data. The proposed Long-tailed Medical Diagnosis (LMD) framework consists of two stages: Relation-aware Representation Learning (RRL) to improve feature robustness for tail classes using Multi-view Relation-aware Consistency, and Iterative Classifier Calibration (ICC) to fine-tune both encoder and classifier using balanced virtual features. Experiments demonstrate significant performance improvements over state-of-the-art methods across three public datasets.

## Method Summary
LMD employs a two-stage approach to tackle long-tailed medical classification. Stage 1 uses Relation-aware Representation Learning (RRL) with Multi-view Relation-aware Consistency (MRC) to enhance encoder representation through teacher-student learning with strong and weak augmentations. Stage 2 employs Iterative Classifier Calibration (ICC) with Virtual Features Compensation (VFC) and Feature Distribution Consistency (FDC) loss to generate balanced virtual features and iteratively fine-tune the model. The framework is built on ResNet-18 backbone and trained with specific augmentation strategies and loss functions.

## Key Results
- On Hyper-Kvasir: Achieved 98.96% AUC, 62.99% F1, and 92.43% Kappa, surpassing existing methods by notable margins
- On ISIC-2019-LT and ISIC-Archive-LT: Consistently outperformed state-of-the-art methods across all metrics
- MRC ablation showed significant performance drop, confirming its effectiveness in improving tail-class feature robustness
- VFC demonstrated superior performance compared to standard class-balanced resampling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forcing encoder consistency across different augmentations improves feature robustness for tail classes with scarce data.
- Mechanism: The Multi-view Relation-aware Consistency (MRC) module processes strongly-augmented and weakly-augmented versions of the same input through a student and an EMA teacher encoder, respectively. It minimizes a consistency loss based on the squared difference between the Gram matrices of their feature maps. This explicitly trains the encoder to capture stable semantic features that are invariant to perturbations, compensating for the lack of diverse tail class samples.
- Core assumption: The semantic identity of an image is preserved through strong augmentation, and the teacher model's weights provide a stable, noise-free target.
- Evidence anchors:
  - [section 3.3]: "The MRC module constrains student and teacher models to ensure consistency across different perturbations of the same input."
  - [section 4.6]: "disabling the MRC results in a... decrease in BACC, ... which indicates the effectiveness of the MRC module in improving the representation ability of the encoder."
  - [corpus]: 'A Tale of Two Classes: Adapting Supervised Contrastive Learning...' suggests standard contrastive learning struggles with long-tailed data, reinforcing the need for adapted consistency mechanisms like MRC.
- Break condition: Using augmentations that destroy semantic content will cause model collapse. If the EMA decay rate is too low, the teacher model will not provide a stable target, leading to unstable training.

### Mechanism 2
- Claim: Generating balanced virtual features from estimated distributions calibrates the classifier without discarding head-class information.
- Mechanism: The Virtual Features Compensation (VFC) module estimates a multivariate Gaussian distribution for each class in the encoder's latent space. It then samples an equal, large number of "virtual" feature vectors from these distributions. This creates a perfectly balanced training set for the classifier, preserving the feature statistics of all classes, unlike resampling which discards head data.
- Core assumption: The true feature distribution of each class can be reasonably approximated by a single multivariate Gaussian.
- Evidence anchors:
  - [abstract]: "generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner."
  - [section 3.4.1]: "generate balanced virtual features for each category under multivariate Gaussian distributions."
  - [corpus]: 'Class Incremental Fault Diagnosis...' tackles limited data with contrastive learning. VFC offers a generative alternative by synthesizing new feature samples.
- Break condition: If a class's true feature distribution is multimodal or non-Gaussian, the sampled virtual features will be unrepresentative, leading to a poorly calibrated classifier for that class.

### Mechanism 3
- Claim: An iterative encoder-classifier feedback loop is needed to align the feature space with a balanced decision boundary.
- Mechanism: The Iterative Classifier Calibration (ICC) uses an Expectation-Maximization (EM) loop. In the M-step, the classifier is updated on balanced virtual features. In the E-step, the encoder is updated using a Feature Distribution Consistency (FDC) loss. This loss pulls a sample's feature toward its own class mean and pushes it away from other class means, iteratively reshaping the latent space to better suit the calibrated classifier.
- Core assumption: The feature space learned in Stage 1 is suboptimal for the balanced classifier trained in Stage 2, and joint fine-tuning can reach a better global optimum.
- Evidence anchors:
  - [section 3.4.3]: "By regularizing the cross-entropy loss with the proposed constraint in the Expectation step, the encoder can be optimized regarding the updates of the classifier in the previous iteration without involving the imbalance bias."
  - [corpus]: 'BAPE: Learning an Explicit Bayes Classifier...' focuses on probabilistic calibration. ICC focuses on geometric calibration of the feature space.
- Break condition: The iterative process can diverge if the encoder's learning rate is too high, or if the FDC loss weight is too strong, potentially distorting the learned features.

## Foundational Learning

- **Concept: Teacher-Student Learning (Mean Teacher)**
  - Why needed here: This is the backbone of the Stage 1 (RRL) training. You must understand that the teacher model is not trained via gradients but is an exponential moving average (EMA) of the student, providing stable targets.
  - Quick check question: How are the teacher model's weights updated at each training step? (Answer: $\theta'_{teacher} = \alpha \theta'_{teacher} + (1-\alpha) \theta_{student}$)

- **Concept: Multivariate Gaussian Distribution**
  - Why needed here: This is the statistical foundation for the VFC module in Stage 2. You need to know how to estimate the mean vector and covariance matrix from a set of feature vectors and then sample from the resulting distribution.
  - Quick check question: When sampling from a class's feature distribution, what two statistics must you first estimate from the encoder's outputs? (Answer: Mean vector $\mu$ and Covariance matrix $\Sigma$)

- **Concept: Expectation-Maximization (EM) Algorithm**
  - Why needed here: This provides the theoretical framing for the ICC stage's training loop. The model alternates between an E-step (refining the feature distribution) and an M-step (calibrating the classifier).
  - Quick check question: In the M-step of this framework, is the encoder trained? (Answer: No, it is frozen. The classifier is trained on virtual features.)

## Architecture Onboarding

- **Component map:**
  - **Stage 1 (RRL)**: Input -> [Strong Augmenter] -> Student Encoder -> Classifier
  - Input -> [Weak Augmenter] -> Teacher Encoder (EMA of Student)
  - Loss: Cross-Entropy (on student classifier) + MRC (consistency between student & teacher features).
  - **Stage 2 (ICC)**:
    - **M-Step**: Encoder (Frozen) -> Feature Bank -> VFC (Samples from Gaussian) -> Classifier (Trained).
    - **E-Step**: Encoder (Unfrozen) -> Real Input -> FDC Loss (Pulls feature to class mean, pushes from others) -> Classifier (Frozen).

- **Critical path**: The VFC module's distribution estimation. If the Gaussian estimate for a tail class is poor (e.g., due to <5 samples), the virtual features will be inaccurate, and the entire calibration will fail for that class.

- **Design tradeoffs:**
  - **Augmentation strength**: Must be strong enough for consistency to be meaningful, but not so strong that it destroys semantics. The paper uses "grid dropout" and "color jitter".
  - **Number of virtual samples ($R$)**: A higher $R$ (e.g., 50,000) gives a more stable classifier gradient but increases memory usage.
  - **Iterative balance**: The encoder's LR ($10^{-6}$) is two orders of magnitude smaller than the classifier's ($10^{-5}$) to prevent the encoder from catastrophically forgetting its Stage 1 knowledge.

- **Failure signatures:**
  - **Class Collapse in Stage 1**: If the MRC loss dominates, all features may map to a single point. Monitor feature variance.
  - **Numerical Instability in VFC**: Tail class covariance matrices may be singular with very few samples. Add a small epsilon to the diagonal for regularization.
  - **Performance Plateau**: If ICC iterations do not improve BACC, the encoder's LR may be too low to respond to the FDC loss.

- **First 3 experiments:**
  1. **Stage 1 Isolation**: Run only the RRL stage with a frozen encoder. Compare its tail-class feature separation (t-SNE) against a baseline ResNet trained with CE.
  2. **VFC Ablation**: Run the full pipeline but replace VFC with standard class-balanced resampling. This will quantify the specific contribution of virtual feature generation.
  3. **Hyperparameter Sweep**: On a validation split, tune the FDC loss weight $\lambda_e$ (try $10^{-3}, 10^{-4}, 10^{-5}$). Plot BACC vs. $\lambda_e$ to find the stability peak.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the LMD framework perform when applied to more modern backbone architectures, such as Vision Transformers (ViT) or hierarchical transformers (e.g., Swin Transformer)?
- **Basis in paper:** [Inferred] The implementation details explicitly state the use of ResNet-18 as the sole backbone for all experiments, without exploring the interaction between the proposed RRL/ICC modules and transformer-based architectures which handle global context differently.
- **Why unresolved:** The paper validates the method on CNNs, but it remains unclear if the relation-aware consistency and virtual feature calibration translate effectively to the attention mechanisms and patch-based processing of transformers.
- **What evidence would resolve it:** Comparative results on the ISIC-LT and Hyper-Kvasir datasets using ViT or Swin Transformer backbones compared against the ResNet-18 baseline.

### Open Question 2
- **Question:** Does the assumption that class features follow a multivariate Gaussian distribution hold for diseases with high intra-class heterogeneity or multi-modal visual appearances?
- **Basis in paper:** [Inferred] The Virtual Features Compensation (VFC) module (Eq. 6) generates virtual features by estimating a class-specific multivariate Gaussian distribution $\mathcal{N}(\hat{\mu}_k, \hat{\Sigma}_k)$.
- **Why unresolved:** Real-world medical image features are often complex and non-Gaussian; a single Gaussian estimate might fail to capture distinct sub-clusters within a single disease category, potentially generating unrealistic virtual features.
- **What evidence would resolve it:** A statistical analysis of the actual feature distribution shapes per class (e.g., using normality tests) or a comparison of VFC against a non-parametric or mixture-model generative approach.

### Open Question 3
- **Question:** Can the key hyperparameters, specifically the virtual feature resampling size $R$ and loss weight $\lambda_1$, be dynamically adapted or set autonomously rather than empirically tuned per dataset?
- **Basis in paper:** [Explicit] The paper states regarding the resampling size $R$: "the optimal selection of R is often empirical... highlighting the need for a case-specific selection strategy."
- **Why unresolved:** The current methodology requires manual tuning to balance performance across different long-tailed distributions (e.g., Hyper-Kvasir vs. ISIC-Archive-LT), which limits the framework's "plug-and-play" usability in clinical settings.
- **What evidence would resolve it:** The development and validation of an adaptive heuristic or learning-based module that automatically determines $R$ and $\lambda$ based on the input data distribution, achieving comparable performance without manual search.

## Limitations

- The VFC module's Gaussian assumption for tail classes is critical but unverified, with no reported covariance matrix regularization or handling of classes with <10 samples
- Specific parameters for "strong" augmentation (e.g., grid dropout patterns, jitter magnitudes) are not detailed, making exact replication difficult
- The method requires empirical tuning of hyperparameters like virtual feature resampling size $R$ and loss weights, limiting plug-and-play usability

## Confidence

- **High Confidence**: Stage 1 MRC mechanism improves tail-class feature robustness (supported by ablation showing BACC drop without MRC)
- **Medium Confidence**: VFC + ICC framework achieves SOTA results (based on reported metrics, but implementation details like covariance regularization are unclear)
- **Low Confidence**: Gaussian distribution assumption for all classes is universally valid (this is an unverified assumption for tail classes)

## Next Checks

1. **Tail Class Covariance Stability**: Test VFC with diagonal covariance regularization (add εI to Σₖ). Measure performance drop when regularization is removed.
2. **MRC Ablation Depth**: Run experiments disabling MRC and compare feature space visualization (t-SNE) of tail classes to quantify separation improvement.
3. **VFC vs. Resampling**: Implement a direct ablation replacing VFC with standard class-balanced resampling (without virtual features) to isolate the contribution of generative sampling.