---
ver: rpa2
title: 'Integrating Linguistics and AI: Morphological Analysis and Corpus development
  of Endangered Toto Language of West Bengal'
arxiv_id: '2510.22629'
source_url: https://arxiv.org/abs/2510.22629
tags:
- language
- toto
- languages
- linguistic
- endangered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study documents the critically endangered Toto language of
  West Bengal by developing a trilingual (Toto-Bangla-English) language learning application.
  The research includes fieldwork-based linguistic documentation and creation of a
  morpheme-tagged parallel corpus, which trained a Small Language Model (SLM) and
  Transformer-based translation engine.
---

# Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal

## Quick Facts
- arXiv ID: 2510.22629
- Source URL: https://arxiv.org/abs/2510.22629
- Reference count: 0
- This study develops a trilingual (Toto-Bangla-English) learning app and AI translation engine for the critically endangered Toto language using morpheme-tagged corpus and Small Language Model.

## Executive Summary
This interdisciplinary study documents the critically endangered Toto language of West Bengal by developing a trilingual language learning application. The research combines fieldwork-based linguistic documentation with AI technologies, creating a morpheme-tagged parallel corpus to train both a Small Language Model (SLM) and a Transformer-based translation engine. The approach addresses extreme data scarcity while preserving linguistic complexity through detailed morphological analysis including inflectional and derivational patterns. Script standardization and Unicode integration enable digital literacy tools that support both language preservation and multilingual education.

## Method Summary
The study collected fieldwork data to create a trilingual (Toto-Bangla-English) parallel corpus with morpheme-level annotations for person-number-gender, tense-aspect-mood, and case marking. A Small Language Model (5M parameters) was trained using Masked Language Modeling on ~20,000 Toto sentences with SentencePiece tokenization. A Transformer-based translation engine was developed using MarianMT/OpenNMT frameworks, fine-tuned on 5,000-10,000 aligned sentences with language control tags. Unicode script integration enabled digital literacy tools, while data augmentation techniques (synonym substitution, conjugation expansion, reordering) addressed data scarcity. The system was deployed as a lightweight mobile/web application with offline inference capability.

## Key Results
- Developed trilingual corpus with morpheme-level annotations for person-number-gender, tense-aspect-mood, and case marking
- Created functional translation engine and language learning application with offline capability
- Successfully integrated Toto script with Unicode for digital literacy and keyboard input
- Demonstrated viability of AI-driven tools for endangered language documentation and revitalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morpheme-level annotation enables downstream SLM training for low-resource languages.
- Mechanism: Granular morphological tagging (PNG, TAM, case) creates structured supervision signals that help models learn agglutinative patterns with limited raw text.
- Core assumption: Morpheme boundaries and grammatical tags in Toto are consistent enough to support generalization.
- Evidence anchors:
  - [abstract] "creation of a morpheme-tagged, trilingual corpus used to train a Small Language Model (SLM) and a Transformer-based translation engine"
  - [section 4.1] "Each entry was annotated with morpheme-level tags, part-of-speech, and syntactic boundaries for linguistic richness."
  - [corpus] Neighbor paper LakotaBERT similarly uses transformer-based approaches for critically endangered languages, suggesting transferability but not proof.
- Break condition: If morpheme segmentation is inconsistent or inaccurate across annotators, model learning degrades.

### Mechanism 2
- Claim: Unicode script standardization enables both human digital literacy and machine readability.
- Mechanism: Mapping recently developed Toto script (2015) to Unicode codepoints permits keyboard input, rendering, and text processing pipelines.
- Core assumption: The script has sufficient grapheme stability for consistent Unicode mapping.
- Evidence anchors:
  - [abstract] "Unicode script integration and a structured language corpus"
  - [section 4.2] "Custom mapping tables were constructed to normalize multi-character graphemes and to support Unicode rendering."
  - [corpus] No direct corpus evidence on Unicode effectiveness for newly scripted endangered languages; related work on Kashmiri OCR addresses script digitization but differs in context.
- Break condition: If community uses multiple script variants or Roman fallback predominantly, Unicode adoption remains limited.

### Mechanism 3
- Claim: Data augmentation compensates for extreme data scarcity in endangered language settings.
- Mechanism: Synonym substitution, conjugation expansion, and reordering artificially expand a small corpus (~20,000 sentences) to improve model robustness.
- Core assumption: Augmented sentences preserve grammatical and semantic validity.
- Evidence anchors:
  - [section 4.3] "Data augmentation techniques such as synonym substitution, conjugation expansion, and reordering were employed to artificially expand the dataset."
  - [section 4.4.1] Architecture was trained on "~20,000 Toto sentences" with 5M parameters.
  - [corpus] Related work (Ge'ez morphological synthesizer, Old English LLM framework) similarly addresses morphological complexity with limited data, but effectiveness metrics are not directly comparable.
- Break condition: If augmentation introduces noise or incorrect forms, model quality degrades rather than improves.

## Foundational Learning

- Concept: Morpheme-tagged parallel corpora
  - Why needed here: The entire SLM and translation pipeline depends on aligned Toto-Bangla-English sentence triples with grammatical annotations.
  - Quick check question: Can you explain the difference between inflectional (e.g., tense markers) and derivational (e.g., noun-to-verb conversion) morphemes in a sentence?

- Concept: Small Language Models for low-resource settings
  - Why needed here: The paper uses a distilled transformer (2-4 layers, 5M parameters) rather than large models, due to data and compute constraints.
  - Quick check question: Why would a 5M-parameter model be preferable to a 7B-parameter model for a language with only 20,000 sentences?

- Concept: SentencePiece subword tokenization
  - Why needed here: Toto is morphologically rich; subword tokenization handles rare words and morpheme boundaries better than word-level tokenization.
  - Quick check question: How does subword tokenization help with out-of-vocabulary words compared to word-level tokenization?

## Architecture Onboarding

- Component map:
  Fieldwork recording (audio/video) -> Transcription & translation -> Morpheme tagging -> Corpus (JSON/TSV format) -> Script normalization (Unicode mapping) -> Tokenizer training (SentencePiece) -> SLM training (MLM objective, 5M params) -> Trilingual translator (MarianMT/OpenNMT encoder-decoder) -> App deployment (mobile/web, offline ONNX/TFLite inference)

- Critical path:
  1. High-quality trilingual sentence alignment is the bottleneck—without it, both SLM and translator fail.
  2. Morpheme annotation consistency directly affects downstream model quality.
  3. Script rendering must work before app deployment is meaningful.

- Design tradeoffs:
  - Model size (5M params) vs. expressiveness—chosen for low-resource feasibility but may limit translation quality.
  - Roman transliteration fallback vs. native script primacy—improves accessibility but may slow script adoption.
  - Data augmentation volume vs. noise risk—helps with scarcity but requires validation.

- Failure signatures:
  - BLEU/chrf scores remain low despite training -> likely insufficient or misaligned parallel data.
  - Script renders incorrectly on target devices -> Unicode mapping or font support issue.
  - Model produces grammatically invalid outputs for common TAM patterns -> morpheme annotation gaps or tokenizer issues.

- First 3 experiments:
  1. Validate trilingual alignment: Manually inspect 100 random sentence triples for semantic equivalence and annotation accuracy before training.
  2. Tokenizer sanity check: Verify SentencePiece vocabulary covers core morphemes (e.g., plural -bɪ, tense markers -mi/-na/-ro) with appropriate frequency.
  3. Baseline translation evaluation: Train a minimal translator on 5,000 aligned sentences, evaluate BLEU/chrf, and compare against rule-based or zero-shot baselines to quantify corpus quality contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise grammatical function and distribution of the morpheme "-he" in Toto?
- Basis in paper: [explicit] The authors identify "-he" as a potential emphatic marker but state, "More detailed study about -he is left for future study."
- Why unresolved: The current analysis relies on limited examples where the morpheme places phonetic stress on the verb, but its full syntactic and semantic range is undetermined.
- What evidence would resolve it: A targeted corpus analysis of "-he" in diverse syntactic environments and pragmatic contexts.

### Open Question 2
- Question: What linguistic rules govern the interchangeable use of the tense markers "-na" and "-mi"?
- Basis in paper: [explicit] The analysis notes that distinct morphemes exist for present (-mi) and past (-na), yet "morphemes -na and -mi are sometimes used interchangeably for present and past tense."
- Why unresolved: While the paper documents the existence of this variation, it does not define the semantic, aspectual, or sociolinguistic conditions that trigger the swap.
- What evidence would resolve it: Elicitation of minimal pairs from multiple informants to determine if the variation correlates with aspect, evidentiality, or speaker demographics.

### Open Question 3
- Question: How effective is the Unicode-based standardized script in enabling digital literacy for the Toto community?
- Basis in paper: [explicit] This is listed as a fundamental research question in Section 1.2 ("How effective is the Unicode-based standardized Toto script...?").
- Why unresolved: While the paper describes the technical implementation of the script and keyboard, it acknowledges challenges due to the script's recent origin (2015) and the community's oral tradition.
- What evidence would resolve it: Usability studies and error-rate analysis of native speakers interacting with the digital application interface.

### Open Question 4
- Question: To what extent does Toto morpho-syntax vary across genders and specific age cohorts?
- Basis in paper: [inferred] The methodology section notes that sampling was restricted to "regional male speakers" and explicitly states that sociolinguistic variation is "not the focus of this study."
- Why unresolved: The exclusion of female speakers and the broad grouping of age cohorts limit the generalizability of the morphological rules presented.
- What evidence would resolve it: Comparative fieldwork data collected specifically from female speakers and stratified across narrower age bands.

## Limitations

- Data scarcity remains a fundamental challenge despite augmentation techniques, with only ~20,000 Toto sentences available for SLM training
- Community adoption of the newly developed Unicode script is uncertain given the community's oral tradition and recent script origin (2015)
- Key hyperparameters, evaluation baselines, and detailed validation results are not specified, limiting reproducibility and assessment of model effectiveness

## Confidence

- High Confidence: The interdisciplinary approach combining linguistics and AI for endangered language preservation is valid and necessary; framework for building morpheme-tagged trilingual corpus is sound
- Medium Confidence: Mechanisms of morpheme annotation enabling SLM training and Unicode standardization supporting digital literacy are plausible but depend on data quality and community adoption
- Low Confidence: Specific effectiveness of SLM and translation engine for Toto given extreme data scarcity and complex morphology cannot be fully assessed without detailed evaluation results

## Next Checks

1. Validate trilingual alignment and annotation: Manually inspect 100 random sentence triples from the corpus for semantic equivalence and morpheme annotation accuracy, checking consistency in PNG, TAM, and case marking across annotators.

2. Assess script rendering and adoption: Test Unicode rendering of Toto script on target devices used by the community; survey community members on preferred writing system (native script vs. Roman transliteration) and actual usage patterns.

3. Evaluate model performance with baselines: Train SLM and translation model on specified dataset sizes; compare translation metrics (BLEU, chrF) against rule-based and zero-shot baselines; conduct human acceptability scoring on translation samples.