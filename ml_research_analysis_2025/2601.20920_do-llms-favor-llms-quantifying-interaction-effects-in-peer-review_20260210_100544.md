---
ver: rpa2
title: Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review
arxiv_id: '2601.20920'
source_url: https://arxiv.org/abs/2601.20920
tags:
- papers
- reviews
- llm-aided
- human
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-assisted reviews treat LLM-assisted
  papers differently than human-written papers. The authors analyze over 125,000 paper-review
  pairs from ICLR, NeurIPS, and ICML using observational data and synthetic LLM-generated
  reviews.
---

# Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review

## Quick Facts
- arXiv ID: 2601.20920
- Source URL: https://arxiv.org/abs/2601.20920
- Reference count: 40
- Primary result: LLM-assisted reviews show rating compression and do not preferentially favor LLM-assisted papers when controlling for paper quality

## Executive Summary
This study investigates whether LLM-assisted reviewers treat LLM-assisted papers differently than human-written papers in ML conference peer review. Analyzing over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML, the authors find that while LLM-assisted reviews appear more lenient toward lower quality papers, this effect disappears when controlling for paper quality. The observed interaction is driven by the over-representation of LLM-assisted papers among weaker submissions rather than preferential treatment. Fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. The analysis also reveals that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores.

## Method Summary
The authors analyze observational data from three major ML conferences (ICLR, NeurIPS, ICML) covering over 125,000 paper-review pairs. They use statistical methods to detect and quantify interaction effects between reviewer LLM usage and paper LLM usage. To validate observational findings, they generate synthetic reviews using LLMs to control for confounding factors and directly test treatment effects. The analysis examines rating distributions, reviewer leniency patterns, and metareview decisions across different LLM usage scenarios.

## Key Results
- LLM-assisted reviews show rating compression and are more lenient toward lower quality papers, but this effect disappears when controlling for paper quality
- Fully LLM-generated reviews fail to discriminate paper quality effectively, while human reviewers using LLMs substantially reduce leniency
- LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher

## Why This Works (Mechanism)
The study demonstrates that apparent preferential treatment of LLM-assisted papers by LLM-assisted reviewers is actually a spurious interaction effect driven by the differential distribution of paper quality across LLM usage categories. Lower quality papers are more likely to be LLM-assisted, and LLM-assisted reviewers are more lenient toward these papers, creating the appearance of bias. When paper quality is controlled for, the interaction effect vanishes. The mechanism involves LLM-assisted reviewers providing more compressed ratings that fail to capture quality differences, particularly when reviewing lower quality papers.

## Foundational Learning

**Observational causal inference** - Why needed: To establish whether LLM usage by reviewers affects their treatment of LLM-assisted papers while accounting for confounding factors. Quick check: Can identify association patterns but cannot definitively establish causation without randomized experiments.

**Synthetic data generation** - Why needed: To create controlled experiments that isolate the treatment effect of LLM assistance while holding paper quality constant. Quick check: Allows direct testing of causal hypotheses that cannot be established from observational data alone.

**Rating distribution analysis** - Why needed: To detect compression effects and changes in discrimination ability across different LLM usage scenarios. Quick check: Reveals whether LLM-assisted reviews maintain the same granularity and quality differentiation as human reviews.

**Interaction effect modeling** - Why needed: To quantify whether reviewer LLM usage and paper LLM usage have combined effects beyond their individual contributions. Quick check: Identifies whether there are genuine preferential treatment effects or spurious correlations.

## Architecture Onboarding

Component map: Observational data -> Quality proxy extraction -> Interaction effect detection -> Synthetic review generation -> Causal validation -> Policy implications

Critical path: Paper quality assessment -> Reviewer LLM usage identification -> Rating collection -> Statistical interaction analysis -> Synthetic review validation -> Meta-analysis of decisions

Design tradeoffs: The study balances the ecological validity of observational data with the causal rigor of synthetic experiments, accepting that observational data may contain unmeasured confounders while synthetic data may not fully capture real-world usage patterns.

Failure signatures: Rating compression across all quality levels indicates LLM over-reliance, while persistent interaction effects after quality control suggests genuine preferential treatment bias.

First experiments: 1) Test rating distributions across quality tiers for different LLM usage combinations, 2) Compare reviewer leniency patterns between human-only and LLM-assisted reviews, 3) Validate synthetic review quality by comparing to observational rating distributions.

## Open Questions the Paper Calls Out
None

## Limitations
- The observational nature cannot definitively establish causation, only associations
- Quality proxies may not capture all relevant dimensions of paper quality and reviewer behavior
- LLM-generated synthetic reviews may not fully represent real-world usage patterns
- Analysis focused on three ML conferences may limit generalizability to other venues

## Confidence

High confidence: LLM-assisted reviews show rating compression and reduced quality discrimination is well-supported by both observational and synthetic data analyses.

Medium confidence: Interaction effects are driven by differential paper quality rather than preferential treatment requires assumptions about quality proxies that warrant further validation.

Medium confidence: Differential behavior between human-assisted and fully LLM-generated reviews is robust but may depend on specific LLM implementations and prompts used.

## Next Checks

1. Conduct randomized controlled trials where the same papers are reviewed with and without LLM assistance to directly measure treatment effects while holding paper quality constant.

2. Expand analysis to include diverse conference types and research fields to assess generalizability of findings beyond ML venues.

3. Implement blinded studies where reviewer LLM usage status is hidden to test whether human biases about LLM usage influence rating behavior independently of review content quality.