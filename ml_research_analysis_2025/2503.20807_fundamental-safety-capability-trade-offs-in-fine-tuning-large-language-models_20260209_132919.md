---
ver: rpa2
title: Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models
arxiv_id: '2503.20807'
source_url: https://arxiv.org/abs/2503.20807
tags:
- safety
- alignment
- fine-tuning
- capability
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a theoretical framework for understanding
  the fundamental safety-capability trade-offs in fine-tuning large language models
  (LLMs). The authors analyze two practical safety-aware fine-tuning strategies: alignment
  loss constraint and alignment parameter constraint.'
---

# Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models

## Quick Facts
- arXiv ID: 2503.20807
- Source URL: https://arxiv.org/abs/2503.20807
- Authors: Pin-Yu Chen; Han Shen; Payel Das; Tianyi Chen
- Reference count: 21
- Primary result: Presents theoretical framework for safety-capability trade-offs in LLM fine-tuning

## Executive Summary
This paper establishes a theoretical framework for understanding the fundamental trade-offs between safety preservation and capability enhancement during LLM fine-tuning. The authors analyze two practical fine-tuning strategies—alignment loss constraint and alignment parameter constraint—and derive bounds on how data similarity and context overlap affect these trade-offs. The theoretical findings are validated through experiments on Llama-2-7B, demonstrating that higher similarity between proxy and original safety datasets reduces safety alignment gaps, while context overlap between safety and capability datasets controls the severity of the trade-off.

## Method Summary
The paper analyzes two fine-tuning strategies for safety-aware capability enhancement. Case I uses an alignment loss constraint with penalty term λ balancing safety preservation against capability improvement. Case II employs an alignment parameter constraint with radius ε₂ limiting parameter updates from the aligned model. Theoretical bounds are derived for both cases, relating safety and capability loss gaps to distribution mismatch metrics (TV distance, KL divergence) and local Lipschitz constants. Experiments validate these bounds using Llama-2-7B fine-tuned with LoRA adapters on various capability datasets while preserving safety alignment from Orca data.

## Key Results
- Higher similarity between original and proxy safety datasets reduces safety alignment gaps under alignment loss constraint
- Context overlap between safety alignment and capability datasets directly bounds capability loss gaps
- Local loss landscape sensitivity governs parameter-constrained safety preservation
- Numerical experiments validate theoretical findings across multiple capability datasets

## Why This Works (Mechanism)

### Mechanism 1: Data Similarity Mitigates Safety Degradation
- Claim: Higher similarity between original and proxy safety datasets reduces the safety alignment gap under alignment loss constraint fine-tuning.
- Mechanism: The safety alignment gap is bounded by distribution mismatch terms—specifically, the total variation distance between input distributions and both the TV distance and KL divergence between output distributions. When proxy data closely approximates the original safety data, these divergence terms shrink, tightening the bound on safety loss.
- Core assumption: Assumption 2 requires that the LLM parameterization can realize the target output distributions; without this, the bounds do not hold.
- Evidence anchors:
  - [abstract] "higher similarity between original and proxy safety datasets reduces safety alignment gaps"
  - [Section 2.4] "using Alpaca-GPT-4 as the proxy alignment dataset achieved a lower alignment loss gap than using Alpaca... because the target outputs of Alpaca-GPT-4 and Orca are both generated by GPT-4"
  - [corpus] Related paper "Objective Matters: Fine-Tuning Objectives Shape Safety" confirms fine-tuning objectives systematically affect alignment outcomes, but does not directly test data similarity mechanisms.
- Break condition: If the original safety distribution is inaccessible and proxy data has fundamentally different output characteristics (e.g., different generator model, different refusal patterns), similarity-based mitigation fails regardless of λ tuning.

### Mechanism 2: Context Overload Between Safety and Capability Data Controls Trade-off Severity
- Claim: The intersection of input distribution supports between alignment and capability datasets directly bounds the capability loss gap and thus controls the safety-capability trade-off.
- Mechanism: When safety alignment inputs and capability fine-tuning inputs overlap in context (supp(D̂) ∩ supp(D_f) is large), the optimization faces conflicting objectives—especially if the output distributions differ (safety refusal vs. task completion). The capability loss gap is upper-bounded by λ multiplied by KL divergence terms summed only over this overlapping region.
- Core assumption: The bound assumes the penalty formulation (Eq. 2) is solved optimally; suboptimal training may not exhibit the predicted trade-off behavior.
- Evidence anchors:
  - [Section 2.2] Theorem 2 shows G_f(P_θ) ≤ λ Σ_{x∈supp(D̂)∩supp(D_f)} D̂(x) D_KL(μ̂(x)|μ_f(x))
  - [Section 2.4] "fine-tuning on Commitpackft achieves the smallest capability loss gap, because Commitpackft's inputs focus on code generation prompts, which have little context overlap with Orca's input domain"
  - [corpus] Weak direct evidence—corpus papers discuss safety trade-offs but do not isolate context overlap as a mechanism.
- Break condition: If fine-tuning data is adversarially constructed to maximize context overlap while having divergent output targets, the trade-off becomes severe regardless of mitigation strategy.

### Mechanism 3: Local Loss Landscape Sensitivity Governs Parameter-Constrained Safety Preservation
- Claim: Under alignment parameter constraint, safety degradation is bounded by the product of local Lipschitz constant and the constraint radius around the aligned model.
- Mechanism: Safety loss changes smoothly in a local neighborhood of the aligned parameters (Assumption 3). Constraining updates to this neighborhood (||θ - θ_s|| ≤ ε_2) ensures the safety gap remains bounded by L_s · ε_2. However, capability improvement is simultaneously limited by the gradient magnitude and local smoothness (L'_f), creating a fundamental trade-off.
- Core assumption: Assumptions 3 and 4 require local Lipschitz continuity and smoothness; if the loss landscape is highly non-smooth near θ_s, the bounds loosen significantly.
- Evidence anchors:
  - [Section 2.3] Theorem 3: G_s(P_θ) ≤ L_s(θ_s, ε_2)ε_2 + G_s(P_{θ_s})
  - [Section 2.4] "Case II is more restrictive than Case I for capability improvement... the local neighborhood constraint limits the capability improvement"
  - [corpus] "The Hidden Dimensions of LLM Alignment" explores linear directions in activation space for safety, suggesting local geometry matters, but does not directly test parameter constraint bounds.
- Break condition: If the aligned model sits at a sharp local minimum with high curvature, even small parameter perturbations cause disproportionate safety degradation, violating the Lipschitz assumptions.

## Foundational Learning

- **Total Variation (TV) Distance and KL Divergence**:
  - Why needed here: Theorems 1 and 2 express safety and capability gaps in terms of distribution mismatch metrics; understanding these bounds requires fluency in TV and KL properties.
  - Quick check question: Given two distributions p and q, which metric (TV or KL) is asymmetric and which is bounded in [0, 1]?

- **Constrained Optimization via Penalty Methods**:
  - Why needed here: Both Case I (loss constraint) and Case II (parameter constraint) are formulated as constrained problems and solved via penalty terms; understanding how λ trades off constraint satisfaction is essential.
  - Quick check question: As λ → ∞ in a penalty formulation, what happens to constraint violation? What happens to the objective value?

- **Lipschitz Continuity and Local Smoothness**:
  - Why needed here: Case II bounds rely on local Lipschitz constants (L_s, L'_f) to characterize how safety and capability losses change near the aligned model; interpreting these bounds requires understanding gradient bounds.
  - Quick check question: If a function is L-Lipschitz, what is the maximum change in function value for a parameter change of distance δ?

## Architecture Onboarding

- **Component map**:
  - Aligned base model (θ_s) -> Case I pipeline (capability data + proxy safety data + penalty λ) OR Case II pipeline (capability data + parameter constraint ||θ - θ_s|| ≤ ε_2) -> Evaluation metrics (G_s, G_f)

- **Critical path**:
  1. Start from aligned model θ_s (full SFT on safety data like Orca)
  2. Choose strategy: Case I (prepare proxy safety data) or Case II (set constraint radius ε_2)
  3. Fine-tune on capability dataset with chosen constraint
  4. Evaluate both G_s and G_f on held-out safety and capability samples

- **Design tradeoffs**:
  - **Case I vs Case II**: Case I allows larger capability gains but requires proxy safety data; Case II needs no extra data but restricts capability improvement
  - **λ tuning**: Higher λ improves safety preservation but degrades capability (Fig. 1 right); optimal λ depends on context overlap
  - **Proxy data selection**: Must maximize similarity to original safety data (same input distribution, same output generator) to minimize G_s

- **Failure signatures**:
  - Safety collapse: G_s spikes suddenly as λ decreases or ε_2 increases—indicates constraint too weak
  - Capability stagnation: G_f remains high despite training—indicates constraint too strong or proxy data has high context overlap with capability data
  - Proxy mismatch: G_s remains elevated even with high λ—suggests proxy data poorly approximates original safety distribution

- **First 3 experiments**:
  1. **Baseline trade-off curve**: Vary λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} with fixed proxy data and capability data (replicate Fig. 1 right) to establish model-specific trade-off baseline
  2. **Context overlap ablation**: Compare fine-tuning on datasets with high overlap (e.g., Alpaca vs Orca) vs low overlap (e.g., Commitpackft vs Orca) while holding λ constant to validate Theorem 2 mechanism
  3. **Proxy similarity test**: Use multiple proxy datasets with known generator differences (text-davinci-003 vs GPT-4 outputs) and measure G_s to validate that output distribution similarity reduces alignment gap per Theorem 1

## Open Questions the Paper Calls Out
- **Open Question 1**: To what extent do the derived theoretical bounds degrade when the "Realizable output distribution" assumption (Assumption 2) is violated by finite-capacity models?
- **Open Question 2**: Do the identified trade-offs and sensitivity constraints hold for reinforcement learning-based fine-tuning (RLHF) or DPO, or are they specific to supervised next-token prediction?
- **Open Question 3**: How does the scale of the model (e.g., 70B vs. 7B parameters) impact the tightness of the safety-capability trade-off bounds?

## Limitations
- Theoretical bounds rely on specific distributional assumptions (Assumption 2) and local smoothness conditions (Assumptions 3-4) that may not hold for all model architectures
- Experimental validation uses a single model architecture (Llama-2-7B) and relatively small LoRA adapters, limiting claims about scalability
- Assumes access to aligned base models and proxy safety data, which may not always be available in practice

## Confidence
- **High**: The theoretical framework and proofs are mathematically rigorous; the mechanisms linking data similarity and context overlap to safety-capability trade-offs are well-established within the paper's assumptions
- **Medium**: The experimental validation on Llama-2-7B supports the theoretical claims, but the single-model focus limits broader applicability claims
- **Low**: The practical implications for real-world deployment depend heavily on the availability of proxy safety data and the validity of local smoothness assumptions, which the paper does not fully address

## Next Checks
1. Test the framework across multiple model architectures (e.g., GPT, Mistral) and scales (7B to 70B parameters) to assess generalizability of the theoretical bounds
2. Conduct adversarial testing where capability datasets are deliberately designed to maximize context overlap with safety data to stress-test the theoretical worst-case bounds
3. Validate whether the proxy data similarity mechanism holds when proxy data comes from fundamentally different generative models (e.g., human-written vs. AI-generated safety refusals)