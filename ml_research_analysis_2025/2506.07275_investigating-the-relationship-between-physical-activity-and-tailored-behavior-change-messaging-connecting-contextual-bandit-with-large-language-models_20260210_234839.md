---
ver: rpa2
title: 'Investigating the Relationship Between Physical Activity and Tailored Behavior
  Change Messaging: Connecting Contextual Bandit with Large Language Models'
arxiv_id: '2506.07275'
source_url: https://arxiv.org/abs/2506.07275
tags:
- intervention
- activity
- message
- physical
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid intervention approach that combines
  contextual multi-armed bandit (cMAB) algorithms with large language models (LLMs)
  to personalize motivational messages aimed at increasing physical activity. The
  cMAB framework dynamically selects intervention types (behavioral self-monitoring,
  gain-framed, loss-framed, or social comparison) based on real-time contextual factors
  such as self-efficacy, social influence, and regulatory focus, while the LLM generates
  tailored message content using participants' written narratives and psychological
  context.
---

# Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models

## Quick Facts
- arXiv ID: 2506.07275
- Source URL: https://arxiv.org/abs/2506.07275
- Reference count: 36
- Key outcome: Hybrid cMAB+LLM approach dynamically personalizes physical activity interventions using real-time context

## Executive Summary
This pilot study introduces a hybrid intervention approach that combines contextual multi-armed bandit (cMAB) algorithms with large language models (LLMs) to personalize motivational messages aimed at increasing physical activity. The cMAB framework dynamically selects intervention types based on real-time contextual factors, while the LLM generates tailored message content using participants' written narratives and psychological context. Over a 7-day trial with 5 participants, daily messages were assigned via four experimental models: cMAB-only, LLM-only, combined cMABxLLM, or equal randomization. The hybrid cMABxLLM approach aims to leverage the interpretability and adaptive selection of cMABs with the nuanced, personalized language generation capabilities of LLMs, producing interventions that are both psychologically attuned and behaviorally effective.

## Method Summary
The study combines contextual multi-armed bandit algorithms with large language models to deliver personalized physical activity interventions. Participants complete daily ecological momentary assessments collecting self-efficacy, social influence, and regulatory focus scores along with written narratives. The cMAB uses Thompson Sampling with linear payoff to select from four intervention types (behavioral self-monitoring, gain-framed, loss-framed, or social comparison) based on contextual factors. The LLM generates personalized messages using participants' narratives and psychological context. Four experimental conditions were tested: cMAB-only (bandit selects arm), LLM-only (LLM selects arm), cMABxLLM (bandit selects, LLM personalizes), and RCT (equal randomization). Primary outcomes were daily step count and message acceptance measured via 5-point Likert scale, with analysis conducted using linear mixed-effects models and Bayesian linear regression.

## Key Results
- Hybrid cMABxLLM approach successfully integrates bandit-based intervention selection with LLM-powered message personalization
- Study demonstrates proof-of-concept for real-time contextual adaptation of behavior change messaging
- Ecological momentary assessment framework enables dynamic collection of psychological context for personalization

## Why This Works (Mechanism)
The approach works by combining the adaptive decision-making capabilities of contextual multi-armed bandits with the nuanced language generation of LLMs. The cMAB framework provides interpretable, data-driven selection of intervention types based on real-time psychological context, while the LLM translates these selections into highly personalized, psychologically-attuned messages that incorporate participants' own narratives. This dual mechanism ensures both optimal intervention type selection and message content that resonates with individual users.

## Foundational Learning
- **Contextual Multi-Armed Bandits (cMAB)**: Reinforcement learning framework for adaptive intervention selection based on real-time context; needed to dynamically choose intervention types based on psychological state; quick check: verify arm selection entropy decreases as cMAB learns participant preferences.
- **Thompson Sampling**: Bayesian approach to balancing exploration and exploitation in cMAB; needed to maintain uncertainty-aware decision making; quick check: confirm posterior updates after each reward signal.
- **Ecological Momentary Assessment (EMA)**: Real-time data collection method for capturing momentary psychological states; needed to provide fresh context for cMAB and LLM; quick check: validate EMA completion rates and data quality.
- **Prompt Engineering for LLMs**: Structured prompt design to guide LLM output toward specific intervention types; needed to ensure message consistency with selected intervention; quick check: verify all LLM outputs match assigned intervention type specifications.
- **Linear Mixed-Effects Models**: Statistical framework for analyzing hierarchical data with repeated measures; needed to account for participant-level clustering in outcome analysis; quick check: confirm model convergence and appropriate random effects structure.
- **Regulatory Focus Theory**: Psychological framework categorizing motivation as promotion (gain) or prevention (loss) oriented; needed to inform intervention type selection and message framing; quick check: validate regulatory focus score distribution across participants.

## Architecture Onboarding
- **Component Map**: EMA -> Context Collection -> cMAB Selection -> LLM Generation -> Message Delivery -> Outcome Collection -> Reward Update (cMAB conditions only)
- **Critical Path**: Daily EMA completion → Context encoding → cMAB arm selection (if applicable) → LLM message generation → Message delivery → Outcome EMA → Reward signal (if cMAB condition)
- **Design Tradeoffs**: cMABxLLM combines adaptive selection with personalization but requires coordination between two AI systems; LLM-only allows full personalization but loses bandit-driven optimization; cMAB-only maintains bandit benefits but sacrifices LLM personalization depth.
- **Failure Signatures**: Low message acceptance rates suggest poor intervention-context matching; inconsistent LLM output indicates prompt engineering issues; cMAB convergence problems point to insufficient data or poor feature scaling.
- **3 First Experiments**: 1) Test cMAB performance with synthetic context data to verify arm selection logic; 2) Validate LLM output quality and intervention type adherence with sample contexts; 3) Run pilot EMA deployment to verify data collection pipeline functionality.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=5) and short duration (5 days) limit statistical power and generalizability
- Critical cMAB hyperparameters (prior means, covariance structure) and LLM parameters (temperature, max_tokens) are unspecified
- Causal relationship between message acceptance and actual behavior change remains unestablished
- LLM output variability and lack of control over message generation consistency

## Confidence
- cMAB algorithm implementation and integration with LLM: **Medium** - Core framework specified but key hyperparameters missing
- LLM message generation fidelity to intervention types: **Low** - Output quality depends heavily on undocumented prompt engineering
- Causal inference validity for message acceptance effects: **Medium** - Methodological framework sound but limited by data constraints

## Next Checks
1. **cMAB hyperparameter sweep**: Systematically test different prior means and covariance structures for m_0 and S_0 to assess sensitivity of arm selection and reward estimation.
2. **LLM output validation**: Implement automated checks to verify that generated messages conform to the assigned intervention type's linguistic and thematic requirements before participant delivery.
3. **Feature scaling impact analysis**: Compare model performance using different normalization strategies for the context features (self-efficacy, social influence, regulatory focus) to determine optimal scaling for both cMAB and LLM components.