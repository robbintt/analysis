---
ver: rpa2
title: Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural
  Understanding by LLMs
arxiv_id: '2601.12921'
source_url: https://arxiv.org/abs/2601.12921
tags:
- jurnal
- journal
- indonesian
- text
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndoSoSci, a novel dataset created from 151
  open-source Indonesian social science journals, aimed at improving large language
  models' (LLMs) understanding of Indonesian culture. The dataset contains approximately
  212 million tokens from journal articles covering various social science topics.
---

# Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs

## Quick Facts
- arXiv ID: 2601.12921
- Source URL: https://arxiv.org/abs/2601.12921
- Reference count: 40
- This paper achieves state-of-the-art accuracy of 81.4% on the IndoCulture benchmark for Indonesian cultural understanding.

## Executive Summary
This paper introduces IndoSoSci, a dataset of approximately 212 million tokens extracted from 151 open-source Indonesian social science journals, to enhance large language models' (LLMs) understanding of Indonesian culture. The authors propose a retrieval-augmented generation (RAG) approach that extracts cultural facts from academic journals and uses LLM-generated hypothetical documents as queries for retrieval. When evaluated on the IndoCulture benchmark, which tests understanding of Indonesian culture across eleven provinces, the method achieves a new state-of-the-art accuracy of 81.4%, outperforming the previous best of 76.4%. The results demonstrate that combining IndoSoSci with Indonesian Wikipedia further enhances performance, highlighting the complementary value of social science journals to more common knowledge sources.

## Method Summary
The authors create a retrieval corpus from Indonesian social science journals by first parsing PDFs with a finetuned LayoutLMv3 model to identify text regions, then extracting cultural facts using an LLM (Sailor2-20B-Chat) with a specific prompt. The extracted facts are chunked, embedded with BGE-M3, and indexed using FAISS. For retrieval, they generate hypothetical answer documents using an LLM as queries, which are then used to retrieve relevant passages from the corpus. The retrieved passages are combined with the original question and passed to the LLM for final answer generation. The method is evaluated on the IndoCulture benchmark using various SEA-focused LLMs, with the best performance achieved using a mixed corpus of journal facts and Wikipedia.

## Key Results
- Achieves state-of-the-art accuracy of 81.4% on IndoCulture benchmark (up from 76.4%)
- Fact extraction from academic journals improves performance by creating cleaner retrieval signals
- Combining journal corpus with Indonesian Wikipedia yields better results than either alone
- Using hypothetical documents as queries (HyDE) improves retrieval effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Academic-Text-to-Facts Distillation
Converting academic prose into concise factual statements improves RAG performance by creating a denser signal-to-noise ratio in the retrieval corpus. Academic articles contain methodological discussion, author opinions, and statistical procedures that introduce noise during similarity matching. Extracting only cultural facts presents knowledge in a format that LLMs recognize from their Wikipedia-heavy training data.

### Mechanism 2: Hypothetical-Document-as-Query (HyDE)
Using an LLM-generated hypothetical answer document as the retrieval query outperforms using the benchmark question directly due to a distributional gap between short questions and the longer, declarative factual entries in the corpus. Generating a synthetic document makes the query representationally similar to the target corpus, improving embedding-based similarity matching.

### Mechanism 3: Journal–Wikipedia Complementarity
Social science journals contain cultural knowledge distinct from and complementary to Wikipedia. Wikipedia tends to cover widely-known, consensus cultural knowledge, while academic journals may document minority practices, historical contexts, or localized ethnographic observations absent from Wikipedia.

## Foundational Learning

- **Concept: RAG (Retrieval-Augmented Generation)**
  - Why needed: This paper's core contribution is a RAG pipeline specialized for cultural knowledge. Understanding indexing, retrieval, and generation stages is prerequisite to following the method.
  - Quick check: If you retrieve 20 passages but only 2 are relevant, what happens to LLM generation quality? (Answer: Irrelevant context can confuse the model, increasing hallucination risk.)

- **Concept: HyDE (Hypothetical Document Embeddings)**
  - Why needed: The paper uses HyDE to bridge the distributional gap between questions and corpus documents. Without this concept, the hypothetical document generation step seems redundant.
  - Quick check: Why use a possibly-incorrect hypothetical document for retrieval instead of the original question? (Answer: To match the embedding space of the corpus; correctness is irrelevant for retrieval alignment.)

- **Concept: Academic Text Structure and Noise**
  - Why needed: The paper's fact-extraction step is motivated by the observation that academic articles contain non-factual content (methods, opinions) that interferes with RAG. Recognizing this helps justify the preprocessing pipeline.
  - Quick check: What percentage of tokens in IndoSoSci survive fact extraction, and why? (Answer: ~7.1%, because most academic text is methodological or discursive, not declarative cultural facts.)

## Architecture Onboarding

- **Component map**:
  PDF Crawler -> Layout Detection -> Text Extraction -> Fact Extraction -> Chunking -> Embedding -> Indexing -> Query Generation -> Retrieval -> Generation

- **Critical path**:
  1. Layout detection must correctly identify text regions; errors here cascade into corrupted chunks.
  2. Fact extraction prompt must consistently extract only cultural facts; inconsistent extraction degrades retrieval relevance.
  3. Hypothetical document quality depends on the LLM's ability to generate plausible structure; weak models may produce poor queries.
  4. Retrieved passages must fit within the LLM's context window (Sailor2 long-context variants used).

- **Design tradeoffs**:
  - Fact extraction vs. raw text: Cleaner signal but possible information loss if context is stripped.
  - Hypothetical queries vs. direct questions: Better retrieval for strong models, but adds inference overhead and dependency on LLM quality.
  - Mixed corpus vs. single source: Better coverage, but increased index size and potential for contradictory evidence.
  - D=20 retrieved passages: Higher recall but higher context noise; optimal D is model- and task-dependent.

- **Failure signatures**:
  - Layout detection failures: Mixed columns, headers/footnotes polluting text.
  - Fact extraction drift: Extraction prompt captures opinions or methodological text instead of cultural facts.
  - Hypothetical query hallucination: Retrieval returns irrelevant passages because the query document is lexically similar to wrong content.
  - Contradictory retrieval: Journal and Wikipedia passages disagree; LLM may select the more confidently-written but incorrect passage.

- **First 3 experiments**:
  1. **Baseline RAG with raw journal text**: Validate that fact extraction helps by comparing baseline results vs. fact extraction results.
  2. **Ablation on hypothetical queries**: Compare results with and without hypothetical documents to confirm HyDE contribution per model.
  3. **Corpus mixing ratio**: Start from Wikipedia-only baseline, incrementally add journal facts to find the optimal ratio and confirm complementarity.

## Open Questions the Paper Calls Out
- In what specific semantic or topical domains does the IndoSoSci corpus provide knowledge that is absent or contradictory in Indonesian Wikipedia?
- Can the IndoSoSci dataset effectively support culturally sensitive conversational agents, or is its utility limited to factual question-answering about cultural practices?
- To what extent does the reliance on Indonesian and English journals result in a loss of cultural nuance present in Indonesia's 700+ local languages?

## Limitations
- The IndoSoSci dataset and extraction pipeline are not yet publicly available for independent verification
- The method's effectiveness is highly model-dependent and may not generalize to non-SEA LLMs
- The approach relies on journals written in Indonesian/English, potentially missing cultural nuance from local languages
- No temporal filtering is applied, so journal facts may contain outdated cultural practices

## Confidence
- **High confidence**: The SOTA improvement on IndoCulture (76.4% → 81.4%) is well-supported by ablation studies and consistent across multiple model families.
- **Medium confidence**: The fact-extraction pipeline is validated by token yield ratios, but the quality of extracted facts is not independently audited.
- **Low confidence**: The claim that journal and Wikipedia facts are non-redundant and non-contradictory is asserted but not empirically validated.

## Next Checks
1. Reproduce core results with Wikipedia-only baseline: Implement the HyDE RAG pipeline using only the publicly available Indonesian Wikipedia dump. Compare no-RAG vs. RAG accuracy on IndoCulture to confirm the 2-4 point gains reported for strong models.
2. Analyze fact extraction quality and overlap: Sample 100 extracted journal facts and 100 Wikipedia facts. Annotate for redundancy, contradictions, and temporal validity. Compute Jaccard overlap to quantify complementarity.
3. Test on out-of-domain cultural questions: Create a small test set of Indonesian cultural questions not covered by IndoCulture (e.g., minority ethnic practices, historical events). Evaluate whether the journal+Wikipedia mixture outperforms either alone, testing the claim of broader cultural coverage.