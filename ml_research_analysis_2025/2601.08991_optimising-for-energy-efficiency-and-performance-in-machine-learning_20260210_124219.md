---
ver: rpa2
title: Optimising for Energy Efficiency and Performance in Machine Learning
arxiv_id: '2601.08991'
source_url: https://arxiv.org/abs/2601.08991
tags:
- energy
- efficiency
- ecopt
- consumption
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ECOpt, a hyperparameter tuner that optimizes
  machine learning models for both energy efficiency and performance. The key innovation
  is framing this trade-off as a multi-objective optimization problem using multi-objective
  Bayesian optimization, enabling discovery of Pareto frontiers that quantify the
  relationship between accuracy and energy efficiency.
---

# Optimising for Energy Efficiency and Performance in Machine Learning

## Quick Facts
- arXiv ID: 2601.08991
- Source URL: https://arxiv.org/abs/2601.08991
- Reference count: 40
- This paper presents ECOpt, a hyperparameter tuner that optimizes machine learning models for both energy efficiency and performance.

## Executive Summary
This paper introduces ECOpt, a hyperparameter tuner that optimizes machine learning models for both energy efficiency and performance by framing the trade-off as a multi-objective optimization problem. Using multi-objective Bayesian optimization, ECOpt discovers Pareto frontiers that quantify the relationship between accuracy and energy efficiency, enabling the identification of models that balance both objectives. Experiments demonstrate that parameter counts and FLOPs are unreliable proxies for energy consumption, that Transformer energy efficiency scales predictably with model size, and that ECOpt can identify models that improve state-of-the-art when considering both accuracy and energy efficiency together.

## Method Summary
ECOpt employs multi-objective Bayesian optimization (MOBO) using the Ax/BoTorch framework with qNEHVI acquisition to optimize hyperparameter configurations across two objectives: performance accuracy and energy efficiency (samples/Joule). The method fits separate Gaussian process surrogate models to each objective, then iteratively samples hyperparameter configurations that maximally expand the dominated hypervolume. The search space includes CNN architecture parameters (layers, filters, kernel sizes, pooling) and Transformer parameters (hidden size, layers, attention heads, sequence length, batch size). Energy consumption is measured using CodeCarbon, which leverages RAPL for CPU and nvidia-smi for GPU power monitoring. The optimization runs 40 Sobol quasi-random initial samples followed by 160 MOBO iterations, with models trained using Adam optimizer (LR=0.001, batch=64) and early stopping.

## Key Results
- Parameter counts and FLOPs are unreliable proxies for energy consumption, with Pearson correlation coefficients of -0.12 and 0.26 respectively.
- ECOpt finds optimal batch size of 831 for Transformers, representing a 38-fold increase in energy efficiency compared to batch size of one.
- The tool identifies seven CIFAR-10 models that balance performance with energy efficiency better than existing approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective Bayesian optimization can discover Pareto frontiers that quantify the trade-off between model accuracy and energy efficiency.
- Mechanism: ECOpt fits separate Gaussian process surrogate models to performance ($f_p$) and energy efficiency ($f_e$) objectives, then uses the qNEHVI acquisition function to iteratively sample hyperparameter configurations that maximally expand the dominated hypervolume. This balances exploration (high surrogate variance) with exploitation (low surrogate mean) while handling measurement noise.
- Core assumption: The objective functions are amenable to Gaussian process modeling with homoscedastic additive zero-mean Gaussian noise, and the search space can be explored within practical iteration limits.
- Evidence anchors:
  - [abstract] "framing this trade-off as a multi-objective optimization problem using multi-objective Bayesian optimization, enabling discovery of Pareto frontiers"
  - [section 4.3] "We fit a separate Gaussian process model to each objective function... we employ the parallel noisy EHVI (qNEHVI) acquisition function"
  - [corpus] Weak direct validation; neighbor papers focus on energy measurement rather than MOBO specifically.
- Break condition: If objective functions are highly discontinuous or the search space has high dimensionality (>10-15 hyperparameters), Gaussian process surrogate quality degrades and MOBO may fail to converge.

### Mechanism 2
- Claim: Parameter count and FLOPs are unreliable proxies for energy consumption in CNN inference.
- Mechanism: Architectural choices like stride length and pooling affect computational workload without changing parameter count. Stride skips pixels during convolution, reducing operations without modifying filters. Pooling reduces spatial dimensions without adding parameters. This decouples parameter count from actual compute.
- Core assumption: Energy consumption correlates with actual operations performed, not just with stored weights.
- Evidence anchors:
  - [abstract] "parameter counts and FLOPs can be unreliable proxies for energy consumption"
  - [section 5.3.1] "parameter count is not proportional to the other cost metrics... a larger stride skips over pixels, thus reducing the number of operations... but does not change the number of parameters"
  - [section 5.4.2] "parameter count is not correlated with energy consumption during inference. We find that it has a Pearson's correlation coefficient of -0.12... FLOPs is also an unreliable cost metric... coefficient of 0.26"
  - [corpus] Neighbor papers assume parameter/FLOP correlation; none directly contradict or validate this specific finding.
- Break condition: If model architecture is fully dense without pooling/striding variations, parameter count may correlate better with energy, though this remains untested.

### Mechanism 3
- Claim: Optimizing batch size for energy efficiency yields a non-obvious optimum that does not saturate VRAM.
- Mechanism: GPU compute utilization and memory bandwidth interact nonlinearly with batch size. Small batches underutilize compute units; very large batches may cause memory bottlenecks. The optimal batch size balances throughput against per-sample overhead without necessarily maximizing memory usage.
- Core assumption: GPU power draw scales with utilization, and there exists a batch size where throughput-per-watt is maximized before memory constraints dominate.
- Evidence anchors:
  - [abstract] "parameter counts and FLOPs can be unreliable proxies for energy consumption" (implies direct measurement needed)
  - [section 5.4.1] "ECOpt finds the optimal to be 831, with an efficiency of 2.67 tokens/J, a maximum GPU memory utilisation of 81%... This configuration represents a 38-fold increase in energy efficiency, compared to the default batch size of one"
  - [corpus] Weak; neighbor papers don't specifically address batch size optimization for energy.
- Break condition: If the workload is memory-bandwidth-bound rather than compute-bound, the optimal batch size may differ significantly; results may not generalize across hardware architectures.

## Foundational Learning

- Concept: **Pareto dominance and frontiers**
  - Why needed here: ECOpt's core output is a Pareto frontier showing configurations where accuracy cannot improve without sacrificing energy efficiency (or vice versa). Understanding dominance is essential to interpret results.
  - Quick check question: Given two configurations where A achieves 80% accuracy at 10 samples/J and B achieves 85% accuracy at 8 samples/J, does either dominate?

- Concept: **Gaussian process regression and uncertainty quantification**
  - Why needed here: MOBO relies on GP surrogate models to predict objective values and quantify uncertainty across unexplored regions of the hyperparameter space.
  - Quick check question: Why does Bayesian optimization prefer sampling points where the GP surrogate has high variance?

- Concept: **Energy measurement scope and PUE**
  - Why needed here: ECOpt measures compute energy via software meters (RAPL, nvidia-smi) but excludes cooling overhead. PUE (~1.56 global average) multiplies compute energy to estimate total datacenter consumption.
  - Quick check question: If a model consumes 1 kWh of compute energy on hardware with PUE 1.5, what is the estimated total energy consumption?

## Architecture Onboarding

- Component map:
  - Model wrapper class -> ECOpt meter -> ECOpt optimizer -> Pareto frontier manager

- Critical path:
  1. Define hyperparameter search space (domains: range, choice, fixed)
  2. Implement model wrapper with `construct()`, `train()`, `evaluate()`, and performance metric computation
  3. Configure anti-ideal reference point (worst-case objective values for hypervolume)
  4. Run optimization loop (Sobol samples → MOBO iterations → frontier extraction)

- Design tradeoffs:
  - **Direct measurement vs. proxies**: Accurate energy readings require root access (RAPL) and add measurement overhead; proxies (FLOPs, parameters) are fast but shown unreliable.
  - **Sample efficiency vs. search space dimensionality**: MOBO scales poorly beyond ~15 dimensions; high-dimensional spaces may require sparse subspace techniques.
  - **Training inclusion vs. inference-only**: `skip_train` argument avoids training overhead but limits applicability to architecture-only hyperparameters.

- Failure signatures:
  - **OOM errors during evaluation**: Return zero energy efficiency to penalize configuration; handler required in model wrapper.
  - **RAPL permission denied**: Fallback to estimated CPU power; reduces measurement accuracy.
  - **Flat frontier (single dominant point)**: Suggests objectives are not in conflict or search space is too constrained.
  - **High variance in repeated measurements**: Indicates unstable workload; consider increasing warmup or batch size.

- First 3 experiments:
  1. **Validate energy meter consistency**: Run the same model inference 10 times on identical inputs; confirm relative standard deviation <2% before proceeding with optimization.
  2. **Batch size sweep (1D)**: On a single model (e.g., small Transformer), sweep batch size from 1 to VRAM limit; plot tokens/J vs. batch size to verify non-monotonic behavior and identify rough optimum before MOBO.
  3. **2-objective NAS on simple CNN**: Define a 3-4 hyperparameter search space (layers, filters, kernel size); run 20 Sobol + 50 MOBO iterations on CIFAR-10 subset; visualize frontier to confirm trade-off emerges before scaling to larger problems.

## Open Questions the Paper Calls Out
None

## Limitations
- Energy measurement accuracy is limited by software meters (RAPL, nvidia-smi) with documented up to 40% error, and excludes cooling overhead (PUE ~1.56).
- MOBO approach's sample efficiency depends on smooth objective functions amenable to Gaussian process modeling, which may fail for highly discontinuous objectives or high-dimensional search spaces.
- Finding that parameter count and FLOPs are unreliable energy proxies is based on specific CNN architectures with stride/pooling variations and may not generalize to fully dense networks.

## Confidence
- **High confidence**: The observation that ECOpt can identify models balancing accuracy and energy efficiency better than existing approaches, supported by direct empirical comparison on CIFAR-10 with multiple runs.
- **Medium confidence**: The claim that MOBO can discover Pareto frontiers quantifying the trade-off between accuracy and energy efficiency, based on established theory but limited to the specific experimental setup.
- **Medium confidence**: The finding that Transformer energy efficiency scales predictably with model size, derived from controlled experiments but not extensively validated across different model families.

## Next Checks
1. **Test energy-proxy correlation across diverse architectures**: Evaluate parameter count and FLOPs correlation with actual energy consumption on a benchmark suite including dense networks, recurrent architectures, and models without stride/pooling variations to determine the scope of the finding.

2. **Validate MOBO robustness to noise and discontinuities**: Systematically vary the Gaussian process noise level and search space dimensionality to identify breaking points where the MOBO approach fails to converge or produces unreliable frontiers.

3. **Measure full-stack energy including PUE overhead**: Run identical experiments with and without facility cooling overhead (using PUE scaling) to quantify the difference between compute-only and total energy consumption, and assess how this affects Pareto frontier rankings.