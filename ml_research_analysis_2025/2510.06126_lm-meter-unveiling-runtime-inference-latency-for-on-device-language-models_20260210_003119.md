---
ver: rpa2
title: 'lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models'
arxiv_id: '2510.06126'
source_url: https://arxiv.org/abs/2510.06126
tags:
- latency
- on-device
- inference
- runtime
- profiling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: lm-Meter is a lightweight online profiler for on-device LLM inference
  that provides accurate phase- and kernel-level latency measurements with minimal
  system overhead. It addresses the challenge of profiling LLMs on resource-constrained
  mobile and edge devices where existing profilers are either too heavy or lack fine-grained
  visibility.
---

# lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models

## Quick Facts
- arXiv ID: 2510.06126
- Source URL: https://arxiv.org/abs/2510.06126
- Authors: Haoxin Wang; Xiaolong Tu; Hongyu Ke; Huirong Chai; Dawei Chen; Kyungtae Han
- Reference count: 40
- Primary result: 99.99% phase-level and 96.82% kernel-level profiling accuracy with minimal overhead on mobile devices

## Executive Summary
lm-Meter introduces a lightweight, accurate profiler for on-device LLM inference that addresses the critical need for fine-grained latency measurement on resource-constrained mobile and edge devices. Unlike existing heavy profilers that cause significant throughput degradation, lm-Meter achieves near-zero overhead for phase-level profiling and maintains sub-3% throughput reduction even under power-constrained settings. The system enables both semantic phase profiling (embedding, prefill, decode, softmax, sampling) and low-level kernel execution tracking through GPU event timestamps, revealing that prefill—not decode—is the dominant bottleneck on mobile hardware.

## Method Summary
lm-Meter instruments the native inference engine (MLC runtime) by inserting monotonic clock timers at phase boundaries and attaching GPU event callbacks for kernel-level profiling. Phase timing uses std::chrono::steady_clock for minimal overhead, while kernel timing leverages OpenCL's clGetEventProfilingInfo to capture QUEUED, SUBMIT, START, and END timestamps. Ground truth validation employs kernel duplication (50 copies for >1ms kernels, 1000 copies otherwise) combined with AGI SDK and Perfetto Tracing. The system runs on Android devices with quantized models (4-bit weights, 16-bit activations) using fixed sampling parameters (temperature=0.0, top_p=1.0) across multiple benchmark tasks.

## Key Results
- Achieves 99.99% accuracy for phase-level profiling and 96.82% accuracy for kernel-level profiling on commercial mobile devices
- Maintains only 2.58% throughput reduction during prefill and 0.99% during decode under power-constrained settings
- Reveals prefill as the dominant bottleneck on mobile devices, with latency scaling 158× versus decode's 10× across model sizes
- Introduces Harmonic Quantization score (H_Q) that reveals architecture- and task-specific quantization trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase-level latency profiling can achieve ≥99.99% accuracy relative to ground-truth measurements while maintaining low overhead.
- Mechanism: lm-Meter instruments the native inference engine (e.g., MLC runtime's C++ backend) by inserting `std::chrono::steady_clock` timers immediately before and after each semantic phase (embedding, prefill, decode, softmax, sampling). The monotonic clock provides sub-microsecond precision while incurring minimal call overhead.
- Core assumption: The inference engine exposes phase boundaries in its native code path, and steady_clock overhead is negligible relative to phase durations (milliseconds to seconds).
- Evidence anchors:
  - [abstract] "lm-Meter achieves 99.99% accuracy for phase-level profiling... with only 2.58% throughput reduction during prefill"
  - [Section 3.1] "steady_clock strikes an effective balance between portability, accuracy, and low instrumentation overhead"
  - [corpus] Limited direct comparison; neighbor papers focus on on-device LLM performance characterization rather than profiling methodology validation.
- Break condition: Phases with extremely short durations (<100 μs, e.g., sampling at 50–80 μs) show reduced accuracy (75–92%) due to timing noise becoming a substantial fraction of total duration.

### Mechanism 2
- Claim: Kernel-level latency profiling can achieve ≥96.82% mean accuracy without modifying GPU kernel source code.
- Mechanism: lm-Meter leverages GPU library event callback mechanisms (e.g., OpenCL's `clGetEventProfilingInfo`) to capture four standardized timestamps per command: QUEUED, SUBMIT, START, and END. This reconstructs the full execution timeline including host-to-device queuing, scheduling delays, and device-side execution.
- Core assumption: The target framework uses OpenCL/Vulkan/Metal and exposes event objects through its runtime; GPU drivers implement profiling timestamps accurately.
- Evidence anchors:
  - [Section 3.2] "lm-Meter instruments this process by attaching profiling callbacks to each OpenCL command via the native cl_event interface"
  - [Section 4.3] "on the Pixel 8 Pro, the mean and median accuracy across all 16 evaluated kernels are 96.82% and 97.35%, respectively"
  - [corpus] No corpus papers evaluate kernel-level profiling accuracy; focus is on end-to-end latency or task accuracy.
- Break condition: Proprietary GPU drivers that do not expose event profiling interfaces would require alternative instrumentation paths.

### Mechanism 3
- Claim: The Harmonic Quantization score (H_Q) reveals architecture- and task-specific quantization trade-offs that single-metric evaluations miss.
- Mechanism: H_Q computes a harmonic mean over three normalized ratios: quantized-to-original accuracy (M_a), and latency speedups for prefill (M_prefill) and decode (M_decode). The harmonic formulation penalizes severe degradation in any single component.
- Core assumption: Accuracy and latency improvements should be balanced; a quantized model that severely degrades one metric is undesirable regardless of gains elsewhere.
- Evidence anchors:
  - [Section 5.1, Eq. 3] Full H_Q formula definition
  - [Section 5.1, Fig. 9] "Pythia benefits from scaling up to 1.4B, while SmolLM2 peaks at 135M-360M"
  - [corpus] DP-LLM (arXiv:2508.06041) addresses dynamic quantization but does not propose a unified accuracy-latency scoring metric.
- Break condition: Tasks where accuracy collapses to near-zero (e.g., SmolLM2-1.7B on GSM8K with 4-bit quantization) produce uninformative H_Q scores; such cases require separate failure-mode analysis.

## Foundational Learning

- Concept: **LLM Inference Phases**
  - Why needed here: lm-Meter profiles five distinct phases—embedding, prefill, decode, softmax, and sampling—each with different computational characteristics. Understanding that prefill processes the entire input context in parallel while decode generates tokens autoregressively is essential for interpreting profiling results.
  - Quick check question: If you observe prefill latency growing 158× when scaling from 70M to 1.4B parameters but decode latency only growing 10×, what does this suggest about mobile GPU parallelism constraints?

- Concept: **GPU Kernel Execution Model**
  - Why needed here: Kernel-level profiling relies on understanding command queues, host-to-device submission, and kernel lifecycle events. The paper shows 21% GPU idle time during decode due to inter-kernel gaps from host-side preparation.
  - Quick check question: What is the difference between CL_PROFILING_COMMAND_SUBMIT and CL_PROFILING_COMMAND_START timestamps, and what does a large gap between them indicate?

- Concept: **KV Cache and Sequence-Length Scaling**
  - Why needed here: The paper demonstrates that paged-attention kernel latency grows nearly linearly with decode steps as the KV cache expands. This is critical for predicting per-step latency and understanding long-sequence bottlenecks.
  - Quick check question: Why does GPU idle time decrease from 21% to 12% when generating 256 tokens versus 16 tokens, even though per-step latency increases?

## Architecture Onboarding

- Component map:
  - **Phase Profiler** (C++): Instruments native runtime with steady_clock timers at phase boundaries; tags records with phase name and sequence index.
  - **Kernel Profiler** (C++): Hooks into OpenCL/Vulkan/Metal event callbacks; maintains per-kernel data structures with five timestamps.
  - **Build Tooling** (Python): Model compilation pipeline with optional kernel duplication for ground-truth validation.
  - **Android App** (Java/Kotlin): Frontend for on-device profiling execution and result collection.
  - **Post-Processing** (Python): Aligns phase/kernel traces with input prompts; computes accuracy metrics via lm-evaluation-harness integration.

- Critical path:
  1. Compile model with MLC/TVM, ensuring OpenCL backend is enabled.
  2. Insert phase-level timers in runtime source (embedding, prefill, decode, softmax, sampling boundaries).
  3. Enable kernel-level profiling via `clGetEventProfilingInfo` callbacks.
  4. Run inference with fixed sampling parameters (temperature=0.0, deterministic generation).
  5. Collect timestamps and compute per-phase/per-kernel latencies.

- Design tradeoffs:
  - **Accuracy vs. overhead**: Phase-level profiling is near-zero overhead; kernel-level profiling adds modest overhead but is still <3% throughput reduction. MELTing Point by contrast causes 22–93% degradation.
  - **Portability vs. depth**: steady_clock is cross-platform but requires native code access; OpenCL profiling is widely supported but may not cover all GPU backends (e.g., proprietary vendor extensions).
  - **Ground-truth validation cost**: Kernel duplication technique (n=50 or n=1000 copies) is accurate but requires model recompilation per kernel evaluated.

- Failure signatures:
  - **Sampling phase accuracy drops to 75–85%**: Expected for sub-100 μs phases; timing noise dominates. Not a profiler bug.
  - **H_Q collapses for certain model-task combinations**: Indicates quantization-induced accuracy failure (e.g., arithmetic-sensitive layers). Investigate layer-wise quantization sensitivity.
  - **Kernel timestamps show CL_PROFILING_COMMAND_START before SUBMIT**: Driver-level anomaly or timestamp rollover; verify GPU driver version.
  - **High GPU idle time (>30%)**: Likely host-side bottleneck (CPU data preparation, I/O stalls); not a profiler issue.

- First 3 experiments:
  1. **Validate phase-level accuracy**: Run a 4-bit quantized Gemma-2-2B-it on Pixel 8 Pro; compare lm-Meter phase latencies against AGI ground-truth using Perfetto Tracing SDK. Target: α ≥ 99.9% for prefill and decode phases.
  2. **Measure kernel-level overhead**: Profile the same model with kernel tracing enabled vs. disabled under Performance, Conservative, and Powersave CPU governors. Target: throughput reduction <3% under Powersave.
  3. **Characterize prefill bottleneck**: Run Pythia suite (70M to 1.4B) on ARC-Easy, ARC-Challenge, HellaSwag, GSM8K; plot prefill latency per input token vs. decode latency per output token. Confirm prefill scales ~158× vs. decode ~10× across model sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Harmonic Quantization score ($H_Q$) perform when evaluating diverse quantization techniques beyond the standard 4-bit post-training methods tested?
- Basis in paper: [explicit] The authors state in Section 5.1, "We consider evaluating diverse quantization techniques using $H_Q$ as our future work."
- Why unresolved: The current study only applies $H_Q$ to a specific set of 4-bit quantized models; it does not validate the metric's sensitivity or utility across the broader landscape of compression strategies like hybrid-precision or activation sparsity.
- What evidence would resolve it: A comparative benchmark showing $H_Q$ scores for models compressed via techniques like GPTQ, AWQ, or structured pruning on mobile hardware.

### Open Question 2
- Question: Can lm-Meter serve as an effective telemetry backend for runtime adaptation engines to dynamically optimize on-device LLM inference?
- Basis in paper: [explicit] Section 3.3 notes that while adaptation is not implemented, lm-Meter is designed to "deliver structured latency feedback" to an "Adaptation Engine" for dynamic scheduling.
- Why unresolved: The paper validates the profiler's measurement accuracy but does not demonstrate a closed-loop system where real-time lm-Meter data triggers successful dynamic actions like model switching or frequency scaling.
- What evidence would resolve it: A system implementation showing improved energy efficiency or throughput by consuming lm-Meter's real-time data to adjust execution parameters on the fly.

### Open Question 3
- Question: Do the observed phase- and kernel-level bottlenecks, specifically the dominance of prefill latency, generalize to non-mobile edge platforms?
- Basis in paper: [inferred] Section 5.3 (Limitations) explicitly notes the study is "limited to a subset of mobile SoCs" and that "Other edge platforms (Jetsons and Intel NPUs) may exhibit different bottlenecks."
- Why unresolved: The empirical conclusions are drawn exclusively from Google Pixel devices (Tensor chips); resource constraints and architectural trade-offs may differ significantly on other edge hardware.
- What evidence would resolve it: Replicating the empirical study using lm-Meter on distinct edge hardware classes (e.g., NVIDIA Jetson, Intel NPU) to verify if prefill remains the primary bottleneck.

## Limitations
- Single hardware/software stack: Study limited to Google Pixel 8 Pro with MLC runtime and OpenCL backend, raising questions about generalizability
- Kernel duplication cost: Ground-truth validation requires model recompilation per kernel, which may not scale to very large models
- Harmonic Quantization score: Becomes uninformative when quantization causes catastrophic accuracy collapse in specific tasks

## Confidence
- **High Confidence** (α ≥ 99.99%): Phase-level latency profiling accuracy - straightforward mechanism with near-perfect validation
- **Medium Confidence** (α ~ 96.82%): Kernel-level latency profiling accuracy - depends on GPU driver implementation, strong accuracy on Pixel 8 Pro but not extensively validated elsewhere
- **Medium Confidence**: Architecture- and task-specific insights - derived from single hardware stack and specific model-task combinations

## Next Checks
1. **Cross-Platform Generalization**: Validate lm-Meter's kernel-level profiling accuracy on an Apple device (e.g., iPhone 15 Pro with A17 Pro chip) using Metal backend. Target: Maintain α ≥ 90% for major compute kernels while measuring any platform-specific overhead differences.

2. **Extreme-Compression Regime**: Evaluate the Harmonic Quantization score on 3-bit and binary-weight models (e.g., 1-bit LLMs) across the benchmark suite. Target: Identify failure modes where H_Q becomes misleading and develop supplementary metrics for extreme quantization cases.

3. **Dynamic Frequency Scaling Impact**: Repeat kernel duplication experiments under controlled DVFS conditions (fixed CPU/GPU frequencies) to isolate timing accuracy from thermal and power management effects. Target: Quantify the variance introduced by frequency scaling and determine if kernel-level profiling requires frequency locking for sub-millisecond accuracy.