---
ver: rpa2
title: 'Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive
  Strategies'
arxiv_id: '2507.00606'
source_url: https://arxiv.org/abs/2507.00606
tags:
- reasoning
- large
- dataset
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of task-specific prompt engineering
  for large language models (LLMs), which limits adaptability and efficiency. The
  authors propose Mixture of Reasoning (MoR), a training framework that embeds diverse
  reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external
  prompt engineering.
---

# Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies

## Quick Facts
- **arXiv ID:** 2507.00606
- **Source URL:** https://arxiv.org/abs/2507.00606
- **Reference count:** 4
- **Primary result:** MoR150 achieved 0.730 accuracy with CoT prompting and 0.734 with IO prompting, representing 2.2% and 13.5% improvements over baselines respectively

## Executive Summary
The paper addresses the challenge of task-specific prompt engineering for large language models by proposing Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies directly into LLMs. MoR operates in two phases: generating reasoning chain templates and constructing a supervised fine-tuning dataset by pairing these templates with benchmark problems. The results demonstrate that MoR significantly enhances reasoning performance without requiring explicit prompt engineering, with MoR150 achieving optimal results across multiple benchmarks.

## Method Summary
MoR uses a teacher-student framework where a stronger model (GPT-4o) generates diverse reasoning templates and selects the optimal one for each training sample. The target model (Qwen2.5-7B-Instruct) then generates answers using the selected template, and only correct responses are kept for training. This creates an SFT dataset that teaches the model to reason using appropriate strategies without explicit prompting. The framework is evaluated across five benchmarks with template counts ranging from 50 to 500, showing that performance improves with template diversity up to an optimal point (150 templates) before declining.

## Key Results
- MoR150 achieved 0.730 accuracy with CoT prompting (2.2% improvement over baseline)
- MoR150 achieved 0.734 accuracy with IO prompting (13.5% improvement over baseline)
- Increasing templates beyond 150 (MoR300, MoR500) showed performance degradation, particularly in CoT settings
- The framework demonstrated autonomous reasoning capability without requiring explicit task-specific prompts

## Why This Works (Mechanism)

### Mechanism 1: Strategy Internalization via Diverse Templating
Embedding a wide variety of reasoning templates during training allows the model to internalize problem-solving heuristics, reducing reliance on external prompt engineering. By exposing the model to 50-500 distinct reasoning chain templates generated by a stronger teacher model, the student learns to recognize structural patterns in problems and activate corresponding reasoning pathways natively. This shifts the reasoning burden from the context window to the model weights. The core assumption is that the model has sufficient capacity to store and generalize these reasoning patterns to unseen tasks.

### Mechanism 2: Teacher-Guided Template Selection (Filtering)
Reasoning performance improves when a high-capacity model (GPT-4o) dynamically selects the optimal reasoning template for a specific training sample. Instead of random assignment, the framework uses a "Thought Generation" phase where a selector model evaluates templates against a problem instance and identifies the best template to structure the reasoning path. This creates high-quality alignment between problem types and reasoning strategies in the training data. The core assumption is that the teacher model can accurately assess which reasoning strategy maximizes the likelihood of a correct answer for the student model.

### Mechanism 3: Performance Saturation and Data Efficiency
Increasing the number of reasoning templates improves performance only up to an optimal point (MoR150), after which data scarcity relative to template diversity causes performance drops. A larger mixture of strategies requires more training data to cover the variance. With fixed training data, too many templates may dilute the learning signal for any single effective strategy or cause the model to struggle with "choice paralysis" without explicit prompting. There exists a balance between reasoning diversity and the volume of training instances available to learn that diversity.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** MoR is designed to internalize the behavior that CoT prompting usually elicits externally. Understanding CoT is necessary to evaluate if the model has successfully learned to "reason step-by-step" autonomously.
  - **Quick check question:** Can you distinguish between a model output that *simulates* reasoning vs. one that actually follows a logical deduction chain?

- **Concept: Supervised Fine-Tuning (SFT)**
  - **Why needed here:** This is the core engine of the MoR framework. One must understand how gradient updates on specific instruction-answer pairs modify model behavior to embed new "skills."
  - **Quick check question:** How does the composition of the SFT dataset (input-output pairs) directly influence the model's tendency to hallucinate vs. reason?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** MoR relies on a stronger "teacher" (GPT-4o) to generate the reasoning templates and select the best ones. The system's success depends on transferring this capability to a smaller "student" (Qwen2.5-7B).
  - **Quick check question:** What are the failure modes when a student model tries to mimic the reasoning style of a much larger teacher model?

## Architecture Onboarding

- **Component map:** Template Bank -> Selector Module -> SFT Engine -> Target Model

- **Critical path:**
  1. **Generation:** Create M templates using GPT-4o
  2. **Selection:** For each training sample, select top-k templates, identify the one yielding a correct answer
  3. **Training:** Fine-tune the student model on the resulting dataset
  4. **Inference:** Run the student model with standard or no prompting to verify internalized reasoning

- **Design tradeoffs:**
  - **Template Count vs. Stability:** Higher template counts (500) offer diversity but risk instability in CoT performance, whereas moderate counts (150) balance diversity and robust learning
  - **Prompt Dependency:** The paper suggests MoR allows IO (direct Input-Output) prompting to outperform CoT in some high-template settings, trading explicit control for autonomous capability

- **Failure signatures:**
  - **Reasoning Collapse:** The model generates reasoning steps that are syntactically correct but logically irrelevant to the specific question (over-fitting to template style)
  - **Performance Degradation:** Accuracy drops below baseline, indicating the model is confused by insufficient or poor-quality templates

- **First 3 experiments:**
  1. **Baseline vs. MoR Inference:** Run Qwen2.5-7B baseline with CoT vs. MoR150-finetuned model with simple "Let's think step by step" on a held-out set to measure the "internalization gap"
  2. **Template Ablation:** Train four separate models with 50, 150, 300, and 500 templates to replicate the saturation curve and identify the optimal complexity for your specific dataset size
  3. **Prompt Sensitivity Test:** Evaluate the MoR150 model using *no* reasoning prompts (Direct IO) vs. CoT prompts to verify if the model has truly internalized the reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does integrating MoR with advanced training paradigms (e.g., RLHF or DPO) affect reasoning performance and convergence speed?
- **Basis in paper:** The Conclusion states, "Future work will explore... integrating MoR with other advanced training paradigms to further enhance its effectiveness."
- **Why unresolved:** The current study focuses solely on Supervised Fine-Tuning (SFT), leaving the interaction with preference-based or reinforcement learning methods unexplored.
- **What evidence would resolve it:** Experimental results comparing standard MoR-SFT against MoR-DPO/RLHF pipelines on the same benchmarks.

### Open Question 2
- **Question:** Does the optimal number of reasoning templates (e.g., 150 vs. 500) scale linearly with model parameter count or dataset complexity?
- **Basis in paper:** The paper notes MoR150 often outperforms MoR500, but experiments are restricted to the 7B parameter Qwen model.
- **Why unresolved:** It is unclear if the "saturation" or performance drop at 500 templates is a limitation of the 7B model's capacity or a general property of the method.
- **What evidence would resolve it:** A scaling law analysis plotting performance against template count for models of varying sizes (e.g., 3B, 14B, 70B).

### Open Question 3
- **Question:** To what extent is the framework dependent on GPT-4o for the specific task of "best template" selection during dataset construction?
- **Basis in paper:** Section 3.2 describes using GPT to select `Tbest` from a subset, and Section 3.1 relies on GPT for generation.
- **Why unresolved:** If the selector model is weaker than the student model, the utility of the filtered dataset might diminish; the sensitivity of the pipeline to the selector's intelligence is not analyzed.
- **What evidence would resolve it:** Ablation studies replacing GPT-4o with weaker selector models (e.g., Llama-3-8B) to measure the impact on final SFT quality.

## Limitations

- **Template selection opacity:** The exact prompt format and selection criteria used by GPT-4o to choose optimal templates are not specified, creating a black box in the methodology
- **Base model dependency:** The filtering mechanism requires the base model to generate correct answers before training, which may introduce selection bias and limit applicability to weaker models
- **Dataset-specific optimization:** The finding that 150 templates is optimal may be specific to the tested benchmarks and may not generalize to other domains

## Confidence

- **High Confidence (8/10):** The empirical observation that increasing reasoning templates improves performance up to a point (MoR150) then degrades (MoR300/MoR500) is well-supported by the presented data in Table 1
- **Medium Confidence (6/10):** The claim that MoR enables autonomous reasoning without external prompt engineering is partially supported but requires additional validation
- **Low Confidence (4/10):** The assertion that the teacher-guided template selection process reliably identifies the optimal reasoning strategy for each sample lacks sufficient evidence

## Next Checks

1. **Template Selection Transparency Test:** Implement the template selection process using the exact prompt format described in the paper (once available) and compare its choices against a simpler random selection baseline on a held-out validation set to quantify whether the selection mechanism adds meaningful value

2. **Base Model Dependency Analysis:** Reproduce the SFT dataset construction pipeline using a stronger base model (like GPT-3.5) to generate answers instead of the 7B model, then compare the resulting dataset size and model performance to determine whether the current approach is limited by base model capability

3. **Cross-Dataset Generalization:** Evaluate MoR150 on datasets not used in training (e.g., ARC Challenge, OpenBookQA) to test whether the internalized reasoning strategies transfer beyond the specific benchmark domains, which would validate the claim of truly adaptive reasoning versus domain-specific memorization