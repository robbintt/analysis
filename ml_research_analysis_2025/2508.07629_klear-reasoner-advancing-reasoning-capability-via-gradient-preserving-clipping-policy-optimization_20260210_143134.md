---
ver: rpa2
title: 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping
  Policy Optimization'
arxiv_id: '2508.07629'
source_url: https://arxiv.org/abs/2508.07629
tags:
- training
- data
- policy
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Klear-Reasoner advances mathematical and programming reasoning
  through Gradient-Preserving Clipping Policy Optimization (GPPO), which retains gradient
  information from all tokens rather than discarding clipped ones. This approach improves
  exploration and accelerates learning from negative samples while maintaining training
  stability.
---

# Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization

## Quick Facts
- **arXiv ID:** 2508.07629
- **Source URL:** https://arxiv.org/abs/2508.07629
- **Reference count:** 21
- **Key outcome:** Klear-Reasoner-8B achieves state-of-the-art performance on AIME 2024 (90.5%), AIME 2025 (83.2%), LiveCodeBench V5 (66.0%), and LiveCodeBench V6 (58.1%) through Gradient-Preserving Clipping Policy Optimization.

## Executive Summary
Klear-Reasoner advances mathematical and programming reasoning through a novel reinforcement learning approach called Gradient-Preserving Clipping Policy Optimization (GPPO). This method modifies standard Proximal Policy Optimization by retaining gradient information from all tokens rather than discarding clipped ones, enhancing exploration and accelerating learning from negative samples while maintaining training stability. Combined with quality-focused supervised fine-tuning and soft reward mechanisms, the 8B parameter model achieves state-of-the-art performance on challenging reasoning benchmarks.

## Method Summary
Klear-Reasoner employs a two-stage training pipeline starting from Qwen3-8B-Base. First, a Long Chain-of-Thought supervised fine-tuning phase uses 1.5M samples distilled from DeepSeek-R1-0528, with data carefully curated from high-quality sources. Second, a two-stage reinforcement learning process applies GPPO: Math reasoning is trained first using binary rewards plus format penalties, followed by code generation training with soft rewards based on test case pass rates. The GPPO mechanism modifies standard PPO clipping to preserve gradients from tokens outside the clipping range, enhancing exploration while maintaining stability.

## Key Results
- Achieves 90.5% on AIME 2024 and 83.2% on AIME 2025, setting new state-of-the-art for 8B models
- Scores 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6, demonstrating strong coding capabilities
- GPPO ablation shows 7.4% improvement on AIME 2024 compared to standard PPO clipping
- Soft reward mechanism increases average rewards by 19.5% while reducing variance by 48.1% in code RL

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Preserving Clipping
Standard PPO zeros gradients for tokens outside the clipping range, suppressing exploration signals. GPPO modifies the backward pass to preserve controlled gradients from these tokens, allowing the model to learn from high-entropy exploratory tokens and negative samples without waiting for repeated sampling. This gentle backpropagation maintains stability while accelerating convergence.

### Mechanism 2: Contrastive Learning from Incorrect Samples
For difficult reasoning tasks, incorrect reasoning chains provide valuable contrastive signals that help the model discriminate between valid and invalid paths. This is particularly effective when the dataset is skewed toward hard problems (88% in this study). For easy tasks, however, incorrect paths serve as noise that interferes with learning correct patterns.

### Mechanism 3: Soft Reward Optimization
Binary rewards in code generation create sparse feedback and high variance. Soft rewards based on test case pass rates provide granular feedback that preserves learning signals from partially correct solutions. This smooths the optimization landscape and prevents reward hacking while maintaining stable training.

## Foundational Learning

**Concept: Proximal Policy Optimization (PPO) & Importance Sampling**
- **Why needed:** GPPO is a modification of PPO's clipping mechanism. Understanding that standard PPO kills gradients outside the clipping range is essential to grasp GPPO's contribution.
- **Quick check:** In standard PPO, if the importance sampling ratio is 1.5 and the clip range is [0.8, 1.2], what is the gradient of the loss with respect to the ratio? (Answer: 0)

**Concept: Token-level Policy Gradients**
- **Why needed:** The paper optimizes at the token level rather than the sample level. Understanding token-level loss summation is required to interpret the gradient equations.
- **Quick check:** How does normalizing by the total number of tokens (âˆ‘T_j) affect the gradient scale compared to normalizing by the number of samples (M)?

**Concept: Knowledge Distillation (SFT)**
- **Why needed:** The Long CoT SFT phase uses a stronger teacher model (DeepSeek-R1) to generate reasoning chains. Understanding the quality-diversity tradeoff is central to their data preparation.
- **Quick check:** Why might a small set of high-quality data outperform a large, diverse set with lower average quality in reasoning tasks?

## Architecture Onboarding

**Component map:** Qwen3-8B-Base -> Long CoT SFT (1.5M samples, 32K ctx) -> Math RL (GPPO, binary rewards) -> Code RL (GPPO, soft rewards)

**Critical path:** Implementing GPPO (Eq. 11) requires a custom clip function where gradients flow through the input ratio even when it exceeds bounds, effectively clamping the gradient magnitude to boundary values rather than zeroing it.

**Design tradeoffs:**
- GPPO vs. Standard Clipping: GPPO explores more and converges faster but requires careful implementation to maintain stability
- Filtered vs. Unfiltered Data: Filters code problems with weak test cases but retains incorrect reasoning traces for hard problems

**Failure signatures:**
- Zero-gradient lockup: Incorrect GPPO implementation reverts to standard clipping, losing exploration signals
- Reward hacking: Binary rewards may lead to syntactically valid but logically empty code solutions

**First 3 experiments:**
1. **GPPO Validation:** Reproduce Figure 2 ablation comparing standard clip vs. GPPO on Math RL subset, measuring AIME24 scores and gradient norms
2. **Soft Reward Baseline:** Implement Code RL with binary vs. soft rewards, verifying higher average rewards and lower variance
3. **SFT Data Quality:** Ablate SFT data mix training on "Top-1" vs. "Top-6" sources to validate quality-over-quantity hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost is not explicitly quantified, making practical feasibility assessment difficult
- Generalizability to reasoning tasks beyond the specific benchmarks tested remains unclear
- Extensive hyperparameter tuning and data curation may be difficult to replicate without exact filtering criteria

## Confidence

**High Confidence Claims:**
- GPPO improves exploration and learning from negative samples compared to standard PPO clipping
- Soft rewards provide more stable training signals than binary rewards in code generation
- Quality-focused SFT approach using high-quality distilled data contributes significantly to performance

**Medium Confidence Claims:**
- Relative contributions of each component to overall performance are not fully isolated
- Long-term stability and generalization beyond tested benchmarks requires further validation

**Low Confidence Claims:**
- Data-efficiency assertion lacks comparison to baseline methods' resource requirements
- Limited analysis of failure cases or edge scenario robustness

## Next Checks

1. **Ablation Study Replication:** Reproduce the GPPO vs. standard PPO and binary vs. soft reward ablations to independently verify reported improvements in exploration and stability

2. **Generalization Testing:** Evaluate performance on additional benchmarks (GSM8K, MATH, HumanEval) to assess whether improvements transfer beyond AIME and LiveCodeBench

3. **Computational Cost Analysis:** Measure and report total computational requirements (GPU hours, training steps) for each training stage to provide complete resource assessment