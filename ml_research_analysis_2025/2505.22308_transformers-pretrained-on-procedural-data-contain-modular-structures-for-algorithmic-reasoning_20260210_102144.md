---
ver: rpa2
title: Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic
  Reasoning
arxiv_id: '2505.22308'
source_url: https://arxiv.org/abs/2505.22308
tags:
- data
- pretraining
- procedural
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how pretraining transformers on procedural\
  \ data\u2014synthetic sequences generated from simple algorithms\u2014can improve\
  \ their performance on algorithmic reasoning tasks. The authors pretrain small transformers\
  \ on various forms of procedural data, including nested brackets (k-DYCK), stack\
  \ operations, identity functions, set operations, and cellular automata."
---

# Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning

## Quick Facts
- arXiv ID: 2505.22308
- Source URL: https://arxiv.org/abs/2505.22308
- Reference count: 33
- Key outcome: Pretraining on procedural data creates modular structures in transformers that can be combined to improve multiple algorithmic reasoning tasks simultaneously

## Executive Summary
This paper demonstrates that pretraining small transformers on synthetic procedural data—sequences generated from simple algorithms like nested brackets, stack operations, and cellular automata—creates transferable computational structures that improve performance on algorithmic reasoning tasks. The authors show that different procedural tasks impart distinct but complementary inductive biases to the model, with beneficial structures localizing to specific architectural components (typically attention layers, sometimes MLPs). Crucially, these learned structures are modular and can be composed across different pretrained models to simultaneously support multiple reasoning capabilities. Perturbation experiments confirm that improvements depend on precise weight structures rather than simple magnitude adjustments, suggesting the exciting possibility of disentangling knowledge acquisition from reasoning capabilities in language models.

## Method Summary
The authors pretrain small GPT-2 transformers on six types of procedural data (k-DYCK, STACK, IDENTITY, SET, ECA Rule 110, and shuffled variants) and then fine-tune on diagnostic tasks (HAYSTACK, ADDITION, REVERSED ADDITION, SORTING, MULTIPLICATION, LANGUAGE MODELING). They systematically test weight transfer strategies, comparing full model transfer against selective transfer of attention-only or MLP-only weights. The key innovation is modular composition—combining attention layers from one pretrained model with MLP layers from another to achieve good performance across multiple tasks simultaneously. The architecture follows T=(E,A,F) notation where E=embeddings, A=attention layers, F=MLP layers, with selective transfer configurations like T_attn=(E_rand, A_pre, F_rand).

## Key Results
- Different procedural pretraining tasks create distinct but complementary inductive biases that accelerate learning on downstream algorithmic tasks
- Attention layers typically carry the most transferable information, though some tasks benefit from MLP structures instead
- Modular composition of structures from different pretraining sources enables simultaneous competence across multiple tasks
- Perturbation experiments (Gaussian noise, weight shuffling) confirm that precise weight structures—not just magnitude—are crucial for performance gains
- Selective transfer (attention-only or MLP-only) often outperforms full model transfer by avoiding interference from non-beneficial task-specific weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different procedural pretraining tasks impart distinct, complementary soft inductive biases to transformer weights.
- Mechanism: Training on algorithmic sequences creates weight configurations that encode specific computational patterns, which subsequently accelerate fine-tuning on downstream tasks requiring similar algorithmic operations.
- Core assumption: Benefits arise from learned computational structure rather than trivial effects like adjusted weight magnitudes.
- Evidence anchors: k-DYCK improves HAYSTACK from ~11% to ~98%, while ECA improves REVERSED ADDITION from ~76% to ~91%; each procedural type shows distinct improvement patterns across tasks.

### Mechanism 2
- Claim: Beneficial inductive biases localize to specific architectural components—typically attention layers, but sometimes MLPs.
- Mechanism: Procedural tasks train attention heads to implement specific operations (e.g., token retrieval, position tracking), while MLPs may encode task-specific transformations. Transferring only the beneficial component avoids interference from non-transferable task-specific weights.
- Core assumption: Architectural components can encode relatively independent computational functions.
- Evidence anchors: For HAYSTACK with IDENTITY pretraining, attention-only transfer improves by 80 percentage points over full transfer (18.8% → 99.0%); Tables 4-7 show attention-only often matches or exceeds full transfer.

### Mechanism 3
- Claim: Learned structures are modular and can be composed across pretrained models to support multiple capabilities simultaneously.
- Mechanism: Since different procedural tasks create useful structures in different components, combining attention layers from one pretrained model with MLPs from another yields an initialization that transfers across multiple downstream tasks.
- Core assumption: Structures in attention and MLP layers are sufficiently decoupled to combine without destructive interference.
- Evidence anchors: SET (attention) + ECA (MLPs) achieves 94.4% HAYSTACK, 80.3% ADDITION, 82.9% REVERSED ADDITION, 99.4% SORTING—performing well on all four tasks where individual models failed on at least one.

## Foundational Learning

- Concept: **Inductive bias in neural networks**
  - Why needed here: The paper frames pretraining as creating "soft inductive biases"—learned constraints that guide subsequent learning. Understanding this concept is essential to interpret why procedural data affects downstream task performance.
  - Quick check question: Can you explain how a soft inductive bias (learned from data) differs from a hard inductive bias (architectural constraint)?

- Concept: **Partial/transfer learning and weight initialization**
  - Why needed here: The core methodology involves transferring subsets of pretrained weights (attention vs MLP) to new models. Understanding what gets transferred and why it matters is critical.
  - Quick check question: If you transfer only attention weights from a pretrained model, what must be randomly initialized in the target model?

- Concept: **GPT-2 style transformer architecture**
  - Why needed here: The paper uses a specific architecture (layers, attention heads, embeddings) and discusses where useful structures reside. Familiarity with the component breakdown (E, A, F) is necessary.
  - Quick check question: In the notation T = (E, A, F), what do E, A, and F represent, and which component contains the attention heads?

## Architecture Onboarding

- Component map: T = (E, A, F) where E=Embedding layers, A={A(1),...,A(L)}=Attention layers, F={F(1),...,F(L)}=MLP layers
- Critical path: 1) Pretrain T_pre on procedural data, 2) Identify which component transfers best via T_full, T_attn, T_mlp ablations, 3) Transfer selected weights to T_target, reset remaining weights randomly, 4) Fine-tune entire T_target on downstream diagnostic task, 5) Compare to random initialization baseline
- Design tradeoffs: Full transfer may include task-specific interference; attention-only maximizes transferability for most tasks but loses MLP benefits; combined transfer requires compatible architectures and may need vocabulary alignment
- Failure signatures: Performance drops to random baseline (transferred weights contained no useful structure), selective transfer dramatically outperforms full transfer (non-transferred component contains interfering patterns), high variance across seeds (>20%)
- First 3 experiments: 1) Replicate selective transfer for HAYSTACK: pretrain on IDENTITY, transfer attention-only vs full model, verify ~80 percentage point improvement, 2) Run perturbation analysis: apply Gaussian noise (σ∈{0.01,0.05,0.1}) and weight shuffling, confirm performance degrades and HAYSTACK/ADDITION are shuffle-sensitive, 3) Test modular composition: combine SET attention layers with ECA MLP layers, evaluate on all four diagnostic tasks, verify combined model maintains reasonable performance across all tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do specific forms of procedural data (e.g., k-DYCK) improve certain downstream capabilities (e.g., HAYSTACK) while structurally similar variants (e.g., k-DYCK SHUFFLE) do not?
- Basis in paper: The authors state, "We miss a first-principle explanation why specific forms of data help specific capabilities. E.g. why does HAYSTACK benefit from k-DYCK but not the SHUFFLE variant?"
- Why unresolved: The study empirically demonstrates transfer of inductive biases but doesn't provide theoretical explanation for why specific algorithmic rules map to specific architectural benefits.
- What evidence would resolve it: Mechanistic interpretability analysis comparing internal representations in models trained on nested vs. shuffled brackets to isolate specific algorithmic properties being transferred.

### Open Question 2
- Question: Can the modular structures induced by procedural pretraining be expressed in a closed-form initialization, bypassing the need to process millions of synthetic examples?
- Basis in paper: The authors ask, "Can we characterise the resulting structure in pretrained models to directly instantiate it in initial weights?" noting that generating training examples is computationally wasteful.
- Why unresolved: While the paper shows precise weight structures are necessary, it doesn't determine if these structures can be derived mathematically rather than through optimization.
- What evidence would resolve it: Deriving analytical initialization scheme based on functional requirements of procedural tasks that replicates performance of pretrained models without training.

### Open Question 3
- Question: How should distinct procedural data types be mixed or scheduled as a "pre-pretraining" curriculum to optimize reasoning capabilities in generalist Large Language Models (LLMs)?
- Basis in paper: The authors note, "It is not clear which specific forms of procedural data could help training a generalist LLM," and suggest "Data mixture optimization... could help balance multiple procedural rules."
- Why unresolved: The paper validates modular composition in small transformers on diagnostic tasks but doesn't test if benefits scale or how they interact with semantic knowledge acquisition required for generalist models.
- What evidence would resolve it: Scaling experiments where LLMs are pretrained on various mixtures of procedural and natural language data, evaluated on both reasoning benchmarks (e.g., GSM8K) and knowledge benchmarks (e.g., MMLU).

## Limitations

- The modular composition claim is least experimentally validated—only one specific combination (SET attention + ECA MLPs) is tested, without exploring robustness to different pretraining task pairings or architectural mismatches
- Perturbation experiments demonstrate that weight magnitude alone doesn't explain performance gains but don't completely exclude alternative mechanisms like implicit regularization effects from the pretraining process itself
- The study uses small transformers and diagnostic tasks; benefits may not scale to larger models or real-world applications requiring both reasoning and semantic understanding

## Confidence

**High Confidence** - The core mechanism that different procedural pretraining tasks create distinct, transferable inductive biases in specific architectural components (attention vs MLPs) is well-supported by ablation experiments. Selective transfer results showing attention-only often outperforms full transfer are robust and reproducible across multiple task pairs.

**Medium Confidence** - The claim that structures are "modular" and can be composed across different pretrained models is supported but limited. Single successful composition experiment demonstrates the principle but doesn't establish generality; failure modes for incompatible combinations are not explored.

**Medium Confidence** - Perturbation experiments convincingly show weight magnitude alone doesn't explain performance gains, supporting structural interpretation. However, experiments don't completely exclude other mechanisms like implicit regularization or optimization landscape shaping.

## Next Checks

1. **Cross-task composition robustness**: Systematically test all pairwise combinations of the five procedural pretraining tasks (k-DYCK, STACK, IDENTITY, SET, ECA) to identify which combinations transfer successfully and which fail, revealing boundaries of modularity.

2. **Architecture compatibility testing**: Transfer weights between models with different hidden dimensions and layer counts to establish whether beneficial structures are truly architectural component-specific or depend on precise dimensional alignment.

3. **Mechanism isolation experiment**: Pretrain on procedural task, then fine-tune on downstream task while freezing either attention or MLP layers to determine whether transferred weights primarily provide better initialization or ongoing computational support during fine-tuning.