---
ver: rpa2
title: Reinforce LLM Reasoning through Multi-Agent Reflection
arxiv_id: '2506.08379'
source_url: https://arxiv.org/abs/2506.08379
tags:
- feedback
- arxiv
- dpsdp
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DPSDP trains actor-critic LLM systems to iteratively refine reasoning
  answers via direct preference learning on self-generated data, enabling coordinated
  multi-agent exploration and feedback. It achieves strong in- and out-of-distribution
  improvements: on MATH 500, five refinement steps raise accuracy from 58.2% to 63.2%
  with Ministral models; similar gains appear across Llama and Qwen base models.'
---

# Reinforce LLM Reasoning through Multi-Agent Reflection

## Quick Facts
- **arXiv ID**: 2506.08379
- **Source URL**: https://arxiv.org/abs/2506.08379
- **Reference count**: 40
- **Primary result**: DPSDP trains actor-critic LLM systems to iteratively refine reasoning answers via direct preference learning on self-generated data, achieving strong in- and out-of-distribution improvements across multiple base models.

## Executive Summary
DPSDP introduces a reinforcement learning framework that trains actor-critic LLM systems to iteratively refine reasoning answers through direct preference optimization on self-generated data. The method uses a Markovian state design that considers only the most recent answer and feedback, enabling generalization to longer refinement sequences at test time than seen during training. By employing restart sampling for coordinated exploration and avoiding complex value models through oracle-based Q-value estimation, DPSDP achieves significant accuracy improvements on mathematical reasoning benchmarks across multiple base models.

## Method Summary
DPSDP operates through a three-phase pipeline: (1) preliminary supervised fine-tuning (SFT) on oracle-generated feedback and refinements to establish proper instruction following, (2) data collection using restart sampling where multiple diverse responses are generated from each state and Q-values are estimated via correctness, and (3) DPO training of actor and critic models using preference pairs constructed from the collected data. The method employs a Markovian state design that considers only the most recent answer and feedback, avoiding distribution shift issues when test-time horizons exceed training horizons.

## Key Results
- On MATH 500, five refinement steps raise accuracy from 58.2% to 63.2% with Ministral models
- Similar gains appear across Llama and Qwen base models, demonstrating broad applicability
- Ablations confirm benefits of specialized roles and show the Markovian state design ensures robustness to test-time horizon shifts

## Why This Works (Mechanism)

### Mechanism 1: Markovian State Design for Horizon Generalization
Limiting the context window to only the most recent answer and feedback allows the policy to generalize to more refinement steps at test time than seen during training. By stripping conversation history, the state distribution remains consistent regardless of the total number of turns, mitigating the distribution shift that typically occurs when test-time horizons exceed training horizons.

### Mechanism 2: Coordinated Exploration via Restart Sampling
Using a "restart" data collection strategy—sampling multiple diverse responses from the same state—creates higher-quality preference pairs than single-trajectory rollouts. This approach explores the action space more thoroughly before committing to gradient updates, improving the quality of the training signal.

### Mechanism 3: Direct Preference Optimization on Self-Generated Q-Values
The system avoids training a complex value model by using ground-truth correctness of sampled rollouts to estimate Q-values, which are then converted into binary preference pairs for DPO. This transforms the RL problem into a classification loss on preference data, simplifying the training pipeline while retaining a signal for multi-step value.

## Foundational Learning

- **Markov Decision Processes (MDP) in NLP**: Understanding the formulation of conversation as an MDP where $s_h$ is the state and $a_h$ is the action is required to grasp why "Markovian" states differ from "Non-Markovian" (full history). Quick check: If the state is defined as only the last answer, does the model have access to the original question?
- **Direct Preference Optimization (DPO)**: The core training loop uses DPO loss instead of standard PPO/Actor-Critic gradients. You must understand how DPO optimizes a policy using a reference model and preference pairs $(y_w, y_l)$ without an explicit reward model. Quick check: In DPO, what role does the $\beta$ parameter play in relation to the KL divergence?
- **Distribution Shift in RL**: A major claim of the paper is that non-Markovian (full-history) settings induce distribution shift. You need to understand that as the test sequence gets longer, the inputs (states) the model sees diverge from the training distribution. Quick check: Why would a model trained on 1-step refinement fail at 5-step refinement if it sees the full history of the 5 steps?

## Architecture Onboarding

- **Component map**: Actor ($\pi_a$) -> Critic ($\pi_c$) -> Reference Policy ($\pi_{ref}$) -> Evaluator (Oracle)
- **Critical path**: 1. Preliminary SFT: Warm-start Actor and Critic using high-quality demonstrations to ensure they can follow the instruction format. 2. Data Collection (Restart): Sample trajectories, estimate Q-values via correctness, extract DPO pairs. 3. DPO Training: Update Actor and Critic using the constructed pairs.
- **Design tradeoffs**: Generative critics provide richer feedback for complex tasks (MATH) but may suffer from "over-thinking" on simple tasks (GSM8K), where a simple binary critic performs better.
- **Failure signatures**: Over-thinking occurs when critic generates excessive or contradictory feedback on simple problems, causing the Actor to change a correct answer to an incorrect one. Format drift happens without preliminary SFT, where models fail to adhere to the "Answer/Feedback" structure.
- **First 3 experiments**: 1. Verify SFT Necessity: Run DPSDP on base models without preliminary SFT stage to confirm performance collapse. 2. Ablate Context (Markovian Check): Run inference with full conversation history enabled vs. last-turn-only to observe distribution shift penalty. 3. Critic Type Comparison: On mixed simple and complex problems, compare "Generative Critic" vs. "Non-Generative Critic" to identify "over-thinking" threshold.

## Open Questions the Paper Calls Out

### Open Question 1
Can DPSDP be effectively adapted into an online or iterative algorithm to actively adapt to state distribution shifts during training? The current practical algorithm relies on offline data from a fixed reference policy, which may limit adaptation as the agent's policy evolves.

### Open Question 2
Can a "mixed generation objective" for a single agent leverage positive transfer between answer generation and feedback provision to surpass specialized multi-agent systems? While specialized roles generally perform better, the single-agent approach showed advantages on simpler benchmarks, suggesting untapped potential in unified training.

### Open Question 3
Is the performance degradation in non-Markovian settings solely due to distribution shift, or does the Markovian state design intrinsically cap reasoning capabilities? It is unclear if the restriction to the most recent answer acts as an informational bottleneck for complex error correction, even if distribution shift could be mitigated.

## Limitations
- The theoretical grounding in PSDP is approximate; the paper replaces the true Q-value with a binary correctness label, which may not fully satisfy theoretical requirements for convergence
- The approach requires ground truth answers during training, limiting its applicability to domains without clean evaluation metrics
- The Markovian state design's assumption that only the most recent answer and feedback are needed may not hold for all reasoning tasks where historical context could be crucial for correction

## Confidence
- **High confidence**: Empirical results showing DPSDP's performance improvements across multiple models and datasets are well-supported by experimental evidence
- **Medium confidence**: Mechanism claims about Markovian state design enabling horizon generalization are supported by ablation studies but rely on theoretical assumptions that aren't fully validated beyond empirical observations
- **Medium confidence**: The restart sampling mechanism's benefits are demonstrated through direct comparisons, but the analysis could be more thorough in explaining why this specific exploration strategy outperforms alternatives

## Next Checks
1. Test Markovian assumption rigorously: Design experiments comparing performance when using only the last answer vs. full history on tasks where historical context might be genuinely important
2. Validate Q-value approximation: Implement a small-scale experiment with a learned value function (Critic) instead of oracle-based Q-values to assess the impact of this approximation on final performance
3. Test scaling to longer horizons: Evaluate DPSDP with training horizons beyond H=3 to verify whether the diminishing returns claim holds, and whether the Markovian design truly enables robust generalization to much longer refinement sequences