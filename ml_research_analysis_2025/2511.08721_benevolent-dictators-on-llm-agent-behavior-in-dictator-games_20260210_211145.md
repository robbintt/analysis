---
ver: rpa2
title: Benevolent Dictators? On LLM Agent Behavior in Dictator Games
arxiv_id: '2511.08721'
source_url: https://arxiv.org/abs/2511.08721
tags:
- behavior
- system
- prompt
- agent
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces the LLM-ABS framework to evaluate the robustness\
  \ of LLM agent behavior in dictator games, a behavioral economics paradigm. By systematically\
  \ varying system prompts, endowment amounts, and units, the authors found that LLM\
  \ agents exhibit a strong tendency toward fairness, often splitting resources equally\u2014\
  more altruistically than human subjects."
---

# Benevolent Dictators? On LLM Agent Behavior in Dictator Games

## Quick Facts
- **arXiv ID**: 2511.08721
- **Source URL**: https://arxiv.org/abs/2511.08721
- **Reference count**: 2
- **Primary result**: LLM agents exhibit strong fairness bias in dictator games, often splitting resources equally—more altruistically than human subjects, with behavior highly sensitive to system prompt complexity.

## Executive Summary
This study introduces the LLM-ABS framework to evaluate how LLM agents behave in dictator games, a behavioral economics paradigm where one party unilaterally decides how to split an endowment. By systematically varying system prompts, endowment amounts, and units, the authors found that LLM agents consistently favor fair 50/50 splits, exceeding human generosity baselines. System prompt complexity significantly influences generosity levels, with detailed prompts inducing more altruistic behavior. The work demonstrates that while LLM behavior is prompt-sensitive, it often aligns with fairness norms, underscoring the need for careful experimental design when assessing AI social behavior.

## Method Summary
The LLM-ABS framework evaluates LLM behavior in dictator games by varying system prompts, endowment amounts, and units while holding user prompts neutral. Using OpenRouter API, the study queries 8 models (DeepSeek, Claude, Gemini, Grok, Mixtral, and others) with temperature=1.0 across 10 repetitions and 10 neutral user prompt variants, yielding up to 100 observations per configuration. Open responses are processed through an LLM-as-a-Judge approach that converts them to closed JSON format (kept/distributed/refusal). The framework systematically tests system prompt variations, amount sensitivity, and unit sensitivity to assess behavioral robustness and fairness tendencies.

## Key Results
- LLM agents exhibit strong fairness bias, often splitting resources equally (50/50), more altruistically than human subjects who retain >66% on average.
- System prompt complexity significantly influences behavior, with detailed prompts inducing more generous splits.
- Models varied in consistency: DeepSeek and Claude showed stable behavior, while Gemini, Grok, and Mixtral displayed high sensitivity to prompt changes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System prompt framing shapes agent decision-making values.
- Mechanism: Elaborate system prompts encode implicit normative guidance that shifts output distributions toward generosity; shorter prompts provide weaker framing.
- Core assumption: The causal path is prompt content → activation of fairness-related representations → behavioral output.
- Evidence anchors:
  - [abstract] "System prompt complexity significantly influences behavior, with detailed prompts inducing more generous splits."
  - [section 4.1] "More elaborate system prompts, such as the one used by DeepSeek, appear to induce a marked shift toward generosity in several agents."
  - [corpus] "Computational Basis of LLM's Decision Making" notes characters/contexts shape behavior, supporting context-sensitivity.
- Break condition: If models were retrained without fairness-aligned RLHF, the framing effect may attenuate.

### Mechanism 2
- Claim: Training and alignment biases produce fairness preferences exceeding human baselines.
- Mechanism: Post-training alignment (RLHF/RLAIF) encodes prosocial norms as safe/desirable outputs; the 50/50 split emerges as a canonical "fair" template.
- Core assumption: The 50/50 central tendency is not emergent reasoning but reflects alignment priors.
- Evidence anchors:
  - [abstract] "LLM agents exhibit a strong tendency toward fairness, often splitting resources equally—more altruistically than human subjects."
  - [section 4.1] "This is a far more altruistic division than the economically rational choice... humans retain on average more than a two-third share."
  - [corpus] Weak direct evidence; neighboring papers focus on game frameworks, not alignment-to-fairness links.
- Break condition: If models were explicitly trained to maximize self-interest in game-theoretic contexts, fairness tendency would diminish.

### Mechanism 3
- Claim: Model architecture and training induce differential prompt sensitivity.
- Mechanism: Some models (Gemini, Grok, Mixtral) exhibit high output variance across semantically equivalent prompts; others (DeepSeek, Claude) show stability—likely due to differences in tokenization, instruction tuning, or attention patterns.
- Core assumption: Stability correlates with robustness to small semantic perturbations.
- Evidence anchors:
  - [abstract] "Models varied in consistency: DeepSeek and Claude showed stable behavior, while Gemini, Grok, and Mixtral displayed high sensitivity to prompt changes."
  - [section 4.1] "Gemini is far more altruistic when the unit is presented as 'BTC' compared to 'Bitcoin'."
  - [corpus] Corpus neighbors do not directly address cross-model sensitivity variance.
- Break condition: If temperature is set to 0, variance decreases but does not eliminate semantic sensitivity.

## Foundational Learning

- Concept: Dictator game paradigm
  - Why needed here: It isolates non-strategic altruism since the recipient cannot retaliate.
  - Quick check question: Why is the dictator game better than the ultimatum game for measuring pure generosity?

- Concept: System prompt vs. user prompt
  - Why needed here: The paper's central finding is that system-level framing causally influences behavior independent of task description.
  - Quick check question: What happens to DeepSeek's generosity if you swap its system prompt with Claude's?

- Concept: Prompt sensitivity/robustness
  - Why needed here: High sensitivity undermines reliability of behavioral conclusions; the LLM-ABS framework mitigates this via neutral variations.
  - Quick check question: If "BTC" and "Bitcoin" produce different splits, what does that imply for reproducibility?

## Architecture Onboarding

- Component map: Experiment setup (define system/user prompts with template variables) → Dispatcher (route to OpenRouter API) → Model inference (produce open responses) → LLM-as-a-Judge (reduce open responses to closed JSON)
- Critical path: Define neutral user prompt variants → inject template variables (amount, unit) → query models → validate closed-form JSON → aggregate across repetitions
- Design tradeoffs: Using each model as its own judge simplifies pipeline but introduces self-consistency bias; ensemble judging was discarded for simplicity
- Failure signatures: Refusals, invalid JSON formats, or non-numeric values in closed responses—these are filtered out before evaluation
- First 3 experiments:
  1. Baseline: Fix amount=10, unit=$, vary system prompts across all 8 models with 10 neutral user prompts × 10 repetitions
  2. Amount sensitivity: Fix system prompt to "You are a helpful assistant," unit=$, vary amounts (10, 1000, 42, π, etc.)
  3. Unit sensitivity: Fix system prompt and amount, vary units ($, BTC, Bitcoin, liters of water) to test semantic framing

## Open Questions the Paper Calls Out
None

## Limitations
- The behavioral stability observed may not generalize to more complex social dilemmas beyond dictator games.
- The LLM-as-a-Judge approach introduces potential self-consistency bias since each model judges its own outputs.
- The automated linguistic analysis of justifications may not capture genuine reasoning depth, lacking human baseline comparisons.

## Confidence
- **High confidence**: The basic empirical finding that LLM agents tend toward equal splits in dictator games, exceeding human generosity baselines.
- **Medium confidence**: The claim that system prompt complexity causally influences generosity levels.
- **Low confidence**: The interpretation of epistemic diversity in justifications as evidence of richer reasoning.

## Next Checks
1. **Cross-validation with ensemble judging**: Replace self-judging with a diverse ensemble of judge models to assess whether current findings hold without self-consistency bias.
2. **Human baseline comparison**: Conduct parallel dictator game experiments with human subjects using identical prompts to establish whether LLM fairness exceeds human behavior in the same experimental conditions.
3. **Generalization test**: Extend the LLM-ABS framework to a different social dilemma (e.g., public goods game or trust game) to verify whether prompt sensitivity and fairness tendencies persist across game types.