---
ver: rpa2
title: 'GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling
  of OpenMP Codes'
arxiv_id: '2512.12091'
source_url: https://arxiv.org/abs/2512.12091
tags:
- uncertainty
- thermal
- graphperf-rt
- dvfs
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphPerf-RT introduces a graph neural network surrogate for performance
  prediction on heterogeneous embedded systems, achieving real-time accuracy with
  uncertainty quantification. The method constructs a heterogeneous graph integrating
  task DAGs, CFG-derived code semantics, and runtime context (DVFS, thermal state)
  through typed nodes and edges.
---

# GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes

## Quick Facts
- arXiv ID: 2512.12091
- Source URL: https://arxiv.org/abs/2512.12091
- Authors: Mohammad Pivezhandi; Mahdi Banisharif; Saeed Bakhshan; Abusayeed Saifullah; Ali Jannesari
- Reference count: 40
- One-line primary result: Graph neural network surrogate achieves real-time performance prediction with uncertainty quantification for OpenMP scheduling on heterogeneous embedded systems.

## Executive Summary
GraphPerf-RT introduces a graph neural network surrogate for performance prediction on heterogeneous embedded systems, achieving real-time accuracy with uncertainty quantification. The method constructs a heterogeneous graph integrating task DAGs, CFG-derived code semantics, and runtime context (DVFS, thermal state) through typed nodes and edges. Evidential regression with Normal-Inverse-Gamma distributions provides calibrated uncertainty decomposition into aleatoric and epistemic components. Evaluation across three ARM platforms and 42 benchmarks demonstrates R²=0.81 on log-transformed makespan prediction with Spearman ρ=0.95 and conservative uncertainty calibration (PICP=99.9% at 95% confidence). Integration with multi-agent model-based RL achieves 66% makespan reduction and 82% energy reduction versus model-free baselines while maintaining zero thermal violations.

## Method Summary
GraphPerf-RT constructs a heterogeneous graph representing OpenMP task-parallel programs by integrating ALF/CFG-derived task semantics, DAG topology, and runtime context (DVFS, thermal state, performance counters). The graph features three node types (Task, Resource, Memory) and four edge types (precedence, placement, contention, bandwidth) with type-specific attention mechanisms. A 3-6 layer heterogeneous GAT processes this graph through type-specific encoders and hierarchical pooling. The model outputs evidential Normal-Inverse-Gamma distributions providing mean predictions and uncertainty decomposition. Training follows a 3-stage process: MSE pretraining, evidential fine-tuning with non-saturating regularization, and post-hoc calibration to ensure conservative uncertainty bounds. The model serves as a world model for model-based RL, enabling safe synthetic rollouts for thermal-aware scheduling.

## Key Results
- Makespan prediction achieves R²=0.81 on log-transformed targets with Spearman ρ=0.95 correlation
- Uncertainty calibration shows conservative coverage with PICP=99.9% at 95% confidence
- Multi-agent model-based RL integration achieves 66% makespan reduction and 82% energy reduction versus model-free baselines
- Zero thermal violations maintained during RL exploration with epistemic uncertainty gating

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Graph Representation for Cross-Layer Interaction Modeling
- **Claim:** Explicitly typing nodes (task, resource, memory) and edges (precedence, placement, contention, bandwidth) captures performance-critical cross-layer interactions that tabular or homogeneous graph approaches miss.
- **Mechanism:** Task nodes encode CFG-derived semantics; resource nodes encode DVFS/thermal state; memory nodes encode cache hierarchy. Type-specific message passing learns distinct attention patterns per edge type (e.g., task-task for parallelism, task-resource for affinity, resource-resource for cache contention).
- **Core assumption:** Performance variation (3–5× observed) primarily emerges from coupling between application structure, hardware state, and scheduling—not from any single factor in isolation.
- **Evidence anchors:**
  - [abstract]: "first to unify task DAG topology, CFG-derived code semantics, and runtime context... in a heterogeneous graph with typed edges encoding precedence, placement, and contention"
  - [Section 4.1 / Table 1]: Ablation shows heterogeneous edges contribute largest ranking improvement (Spearman +8%, ΔR² = -0.04 when removed)
  - [corpus]: Related work (HiDVFS, ZeroDVFS) addresses DVFS scheduling but does not unify graph-based code semantics with runtime context—no direct validation of this specific mechanism in corpus
- **Break condition:** If target workloads exhibit near-uniform task structures with negligible dependency/affinity effects, homogeneous GCN or tabular models may suffice.

### Mechanism 2: Evidential Regression with Normal-Inverse-Gamma Priors for Calibrated Uncertainty
- **Claim:** Single-forward-pass NIG distribution outputs provide conservative, well-calibrated prediction intervals essential for thermal-safe scheduling.
- **Mechanism:** The model outputs (γ, ν, α, β) parameters per target metric. Predictive mean = γ; aleatoric uncertainty = β/(α−1); epistemic = β/[ν(α−1)]. Non-saturating regularization prevents evidence contraction on high-error samples.
- **Core assumption:** Out-of-distribution or high-risk configurations should receive wider prediction intervals, not overconfident point estimates.
- **Evidence anchors:**
  - [abstract]: "Evidential regression with Normal-Inverse-Gamma priors provides calibrated uncertainty... PICP = 99.9% at 95% confidence"
  - [Section 4.3 / Table 3]: Uncertainty decomposition shows 94% aleatoric / 6% epistemic; PICP exceeds nominal at all confidence levels (conservative calibration)
  - [corpus]: TAPAS and thermal prediction papers use DL for thermal management but do not provide calibrated uncertainty bounds—no direct corpus validation
- **Break condition:** If downstream scheduler cannot consume uncertainty (e.g., greedy point-estimate optimizer), evidential overhead is wasted. If inference latency > 10ms, evidential benefits may be outweighed by computational cost on ultra-low-latency loops.

### Mechanism 3: World Model for Safe Synthetic Rollouts in Model-Based RL
- **Claim:** Using GraphPerf-RT as a deterministic transition model with epistemic uncertainty gating enables sample-efficient RL while avoiding hazardous on-device exploration.
- **Mechanism:** Dyna-style planning interleaves real environment steps with synthetic rollouts from GraphPerf-RT. Uncertainty gate filters proposals: gate(a) = (Epi(a) ≤ η) ∧ (PI(a) ≤ T_max). Safe actions execute; risky proposals are rejected before thermal violation.
- **Core assumption:** Model accuracy (R²=0.81, ρ=0.95) is sufficient for planning; epistemic uncertainty reliably flags out-of-distribution states.
- **Evidence anchors:**
  - [abstract]: "multi-agent model-based RL with GraphPerf-RT as the world model achieves 66% makespan reduction and 82% energy reduction versus model-free baselines, with zero thermal violations"
  - [Section 5.3 / Table 4]: MAMBRL-D3QN converges in ~60 episodes vs. ~150 for model-free; achieves 2°C lower average temperature
  - [corpus]: Cloud-Fog-Edge DDPG scheduling and LLM-upgraded graph RL address scheduling but lack uncertainty-gated world models for thermal safety—no direct corpus validation
- **Break condition:** If model error distribution has heavy tails or systematic bias under specific (benchmark, platform) combinations, synthetic rollouts may mislead policy. Assumption: offline validation covers operational distribution.

## Foundational Learning

- **Heterogeneous Graph Neural Networks (HGT, GAT)**
  - **Why needed here:** The model uses type-specific encoders and attention over four edge types. Understanding how message passing differs by edge type is essential for debugging attention patterns and modifying the schema.
  - **Quick check question:** Given a task node connected to two resource nodes via E_TR edges with different affinity values, how does the attention mechanism weight their contributions?

- **Evidential Deep Learning (NIG Regression)**
  - **Why needed here:** The uncertainty quantification is not Bayesian sampling but evidential—single-pass outputs with learned distribution parameters. Misunderstanding this leads to incorrect interpretation of (γ, ν, α, β) outputs.
  - **Quick check question:** If ν drops sharply for a given input while α remains stable, what does this indicate about the model's confidence, and which uncertainty component (aleatoric/epistemic) is affected?

- **Dyna-Style Model-Based RL**
  - **Why needed here:** The MAMBRL integration uses synthetic rollouts to augment real experience. Understanding the replay buffer structure and planning ratio (ζ) is critical for reproducing RL results.
  - **Quick check question:** If synthetic rollouts are added without uncertainty gating, what failure mode might occur during policy updates?

## Architecture Onboarding

- **Component map:**
  OpenMP Source -> [OMPi + ALF-llvm + SWEET] -> ALF/CFG graphs -> [Post-processing] -> Heterogeneous graph G
  Runtime (DVFS, thermal, counters) -> Resource/Memory node features
  G + Device Sheet D + State S + Action a -> [Type Encoders] -> [Hetero GAT × L layers] -> [Hierarchical Pool] -> h_G
  h_G -> [Evidential Heads per metric] -> (γ, ν, α, β) -> Mean prediction + Uncertainty decomposition

- **Critical path:**
  1. Graph construction correctness (ALF→DAG pipeline, node/edge counts)
  2. Type-specific encoder alignment (feature schemas per node type)
  3. Evidential loss convergence (NLL + non-saturating regularization balance)
  4. Calibration post-hoc (τ scaling for PICP > 95%)

- **Design tradeoffs:**
  - L=3 GAT layers vs. L=6: Deeper models oversmooth on small graphs; L=3 optimal in ablation
  - Conservative calibration (PICP=99.9%) vs. sharp intervals: Wider intervals reduce false confidence but may reject viable configurations
  - Deterministic world model vs. stochastic: Single-pass inference (2–7ms) vs. sampling-based uncertainty (10–100× slower)

- **Failure signatures:**
  - PICP dropping below 90%: Non-saturating regularization λ_NS may be too low or disabled
  - Spearman < 0.85: Check heterogeneous edge construction; may have collapsed to homogeneous
  - Cross-platform transfer R² < 0: Device sheet mismatch or feature normalization error
  - RL thermal violations despite gating: Epistemic threshold η too permissive or model drifted from training distribution

- **First 3 experiments:**
  1. **Reproduce ablation:** Train full model, then remove heterogeneous edges (use homogeneous GCN). Confirm Spearman drops from ~0.95 to ~0.87 per Table 1.
  2. **Calibration sanity check:** On held-out test set, compute PICP at 90/95/99% confidence. Verify conservative over-coverage (PICP > nominal). If under-coverage, increase λ_NS.
  3. **Single-step RL integration:** Implement uncertainty-gated action selection only (no full MAMBRL training). Enumerate 10 candidate (mask, DVFS) configurations, score with GraphPerf-RT, filter by gate, execute top-ranked. Compare makespan vs. random selection over 50 episodes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms can enable zero-shot or few-shot cross-platform transfer for GraphPerf-RT across heterogeneous ARM platforms with different core counts and thermal characteristics?
- Basis in paper: [explicit] Cross-platform transfer experiments show R² dropping to 0.28–0.56 (negative for TX2), with authors noting "cross-platform transfer as a known limitation; practical deployment requires platform-specific fine-tuning or adaptation."
- Why unresolved: Device sheet broadcasting and per-device normalization only partially mitigate domain shift. The fundamental challenge of transferring learned representations across architectures with different core counts (6 vs. 8), core types (Denver+A57 vs. Cortex-A72 vs. A78AE), and thermal behaviors remains unaddressed.
- What evidence would resolve it: Experiments demonstrating ≥0.70 R² on held-out platforms using meta-learning, domain adaptation, or hardware embedding techniques without platform-specific fine-tuning.

### Open Question 2
- Question: How can continual learning be integrated with evidential uncertainty quantification to handle workload drift while maintaining calibrated confidence?
- Basis in paper: [explicit] Future directions state: "online adaptation via continual learning to handle workload drift without full retraining."
- Why unresolved: The current architecture requires Stage 2 evidential fine-tuning and Stage 3 post-hoc calibration. Online updates could disrupt the carefully calibrated NIG parameters, potentially causing uncertainty estimates to become miscalibrated during adaptation.
- What evidence would resolve it: A continual learning variant maintaining PICP≥95% at 95% confidence across distribution shift episodes, with analysis of how NIG parameters evolve during online updates.

### Open Question 3
- Question: What architectural modifications are required for GraphPerf-RT to handle GPU workload scheduling granularity (thousands of concurrent threads vs. tens of OpenMP tasks)?
- Basis in paper: [explicit] Appendix I.12 states: "GPU workloads exhibit different scheduling granularity... requiring architectural changes to handle large graphs efficiently (e.g., mini-batching over SMs, hierarchical pooling). We leave GPU integration as future work."
- Why unresolved: The current GAT architecture with 3–6 layers and hierarchical pooling over task/resource/memory nodes assumes graph sizes typical of OpenMP task DAGs. CUDA kernels mapped to task nodes with SMs as resource nodes would create substantially larger graphs with different attention pattern requirements.
- What evidence would resolve it: Scalability experiments on synthetic GPU-sized graphs demonstrating sub-10ms inference with maintained Spearman correlation, or a modified architecture explicitly designed for GPU scheduling domains.

## Limitations
- Heterogeneous graph construction pipeline (ALF/CFG parsing, node/edge typing) is underspecified and depends on unpublished code
- Evidential regression calibration process is described but not fully reproducible without exact τ scaling procedure
- Conservative uncertainty calibration (PICP=99.9%) may be overly pessimistic, potentially rejecting near-optimal configurations
- Generalization to non-DAG or irregular workloads is untested

## Confidence
- **Graph-driven performance prediction accuracy (R²=0.81, ρ=0.95):** High—supported by ablation showing heterogeneous edges are essential and conservative calibration validated across three platforms
- **Uncertainty decomposition into aleatoric/epistemic components:** Medium—correct implementation of evidential NIG regression is plausible but depends on exact training procedure and regularization tuning
- **Zero thermal violations with model-based RL:** Medium—achieved in evaluation but assumes model fidelity holds under all OoD conditions; no stress-testing of epistemic gating under distributional shift reported

## Next Checks
1. **Ablation reproduction:** Train the full model and a homogeneous GCN variant on identical data; confirm Spearman drops from ~0.95 to ~0.87 and R² decreases by ~0.04 when heterogeneous edges are removed.
2. **Calibration audit:** Compute PICP at 90/95/99% nominal confidence on held-out test set; verify conservative over-coverage (PICP > nominal). If under-coverage, adjust λ_NS regularization and retrain.
3. **Single-step RL integration:** Implement uncertainty-gated action selection only (no full MAMBRL training). Enumerate 10 candidate (mask, DVFS) configurations, score with GraphPerf-RT, filter by gate, execute top-ranked. Compare makespan vs. random selection over 50 episodes to confirm 2× improvement.