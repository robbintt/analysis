---
ver: rpa2
title: 'Delta-Audit: Explaining What Changes When Models Change'
arxiv_id: '2508.19589'
source_url: https://arxiv.org/abs/2508.19589
tags:
- attribution
- changes
- occlusion
- shift
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Delta-Attribution (\u2206-Attribution) is a model-agnostic framework\
  \ that explains what changed between two model versions A and B by differencing\
  \ their per-feature attributions: \u2206\u03D5(x) = \u03D5B(x) - \u03D5A(x). It\
  \ evaluates these deltas with a comprehensive quality suite measuring magnitude/sparsity,\
  \ agreement/shift, behavioural alignment, and robustness."
---

# Delta-Audit: Explaining What Changes When Models Change

## Quick Facts
- **arXiv ID**: 2508.19589
- **Source URL**: https://arxiv.org/abs/2508.19589
- **Reference count**: 27
- **Primary result**: A model-agnostic framework that explains model version changes via differenced attributions, validated across 45 tabular settings showing it distinguishes benign tweaks from behaviorally meaningful reliance shifts.

## Executive Summary
Delta-Attribution (∆-Attribution) is a model-agnostic framework that explains what changed between two model versions A and B by differencing their per-feature attributions: ∆ϕ(x) = ϕB(x) - ϕA(x). It evaluates these deltas with a comprehensive quality suite measuring magnitude/sparsity, agreement/shift, behavioural alignment, and robustness. The framework is instantiated via fast occlusion/clamping in standardized space, with a class-anchored margin and baseline averaging.

Evaluated across 45 settings spanning five ML families (logistic regression, SVM, random forests, gradient boosting, kNN) and three datasets, the results show that inductive-bias changes (e.g., kernel, depth) produce large, behaviour-aligned deltas (e.g., SVC poly→rbf on Breast Cancer: BAC≈0.998, DCE≈6.6), while cosmetic tweaks (e.g., SVC gamma=scale vs. auto) yield rank-overlap@10=1.0 and DCE≈0. The largest redistribution appears for deeper gradient boosting on Breast Cancer (JSD≈0.357). The framework provides a lightweight update audit that distinguishes benign changes from behaviourally meaningful or risky reliance shifts.

## Method Summary
Delta-Attribution computes feature attributions for both model versions using occlusion/clamping in standardized space, with each feature clamped to a baseline (mean or median of training data). The framework anchors both models to the same predicted class c(x) and computes deltas: ∆ϕ(x) = ϕB(x) - ϕA(x) and ∆f(x) = fB(x) - fA(x). A quality suite evaluates these deltas across magnitude, concentration, rank overlap, Jensen-Shannon divergence, Delta Conservation Error, Behavioural Alignment Correlation, and robustness to noise. The method was tested on 45 model pairs across five ML families and three tabular datasets (Breast Cancer, Wine, Digits).

## Key Results
- Inductive-bias changes (kernel, depth) produce large, behaviour-aligned deltas with high BAC (e.g., SVC poly→rbf: BAC≈0.998, DCE≈6.6)
- Cosmetic tweaks yield near-zero deltas with rank-overlap@10=1.0 and DCE≈0
- The largest redistribution appears for deeper gradient boosting on Breast Cancer (JSD≈0.357)
- Framework successfully distinguishes benign changes from behaviourally meaningful or risky reliance shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differencing per-feature attributions between model versions isolates where reliance shifted, independent of the absolute importance values.
- Mechanism: By computing ∆ϕ(x) = ϕB(x) - ϕA(x) using a shared baseline and shared reference class c(x), spurious attribution variation cancels while genuine reliance changes accumulate.
- Core assumption: The explainer produces stable, comparable attributions when baseline and anchor class are held constant.
- Break condition: When attribution methods are unstable or baseline-dependent, ∆ϕ may amplify noise rather than signal. High DCE values flag this.

### Mechanism 2
- Claim: Behaviour-attribution coupling (BAC) indicates whether attribution deltas explain output changes.
- Mechanism: BAC computes Pearson correlation between ∥∆ϕ∥₁ and |∆f(x)| across samples. When structural changes reallocate reliance in ways that directly affect the class margin, the correlation approaches 1.0.
- Core assumption: Output changes are primarily driven by additive feature contributions that occlusion captures.
- Break condition: When model behavior is driven by feature interactions not captured by single-feature occlusion, BAC degrades. Use grouped occlusion to probe.

### Mechanism 3
- Claim: Standardized occlusion with baseline averaging reduces baseline-sensitivity artifacts in delta computations.
- Mechanism: Clamping features to training-set baselines (mean, optionally averaged with median) in standardized space ensures that baseline choice affects both models similarly, canceling in the difference.
- Core assumption: Training-set statistics provide a meaningful counterfactual for feature removal.
- Break condition: For extreme distributions or domain-specific baselines, mean/median may be inappropriate; DCE and stability metrics will flag fragility.

## Foundational Learning

- Concept: Feature attribution via occlusion/clamping
  - Why needed here: The framework instantiates delta attribution through occlusion—setting each feature to a baseline and measuring the score drop. Understanding this primitive is essential.
  - Quick check question: If clamping feature j reduces the class margin by 0.3 for model A and 0.5 for model B, what is ∆ϕ_j?

- Concept: Class-anchored margins and log-odds
  - Why needed here: The score function f(x) must be comparable across models. Margins (decision_function) or log-odds (predict_proba) anchored to B's predicted class enable this.
  - Quick check question: Why anchor to the class predicted by model B rather than the true label?

- Concept: Jensen-Shannon divergence and rank overlap
  - Why needed here: These metrics distinguish redistribution (features swapping importance) from reweighting (same features, different magnitudes), a core diagnostic in the quality suite.
  - Quick check question: JSD = 0.357 between |ϕA| and |ϕB|—does this indicate cosmetic or structural change?

## Architecture Onboarding

- Component map: Preprocessing pipeline -> Score function selector -> Occlusion explainer -> Delta calculator -> Quality suite
- Critical path:
  1. Fit shared preprocessing on training data
  2. Train models A and B with identical preprocessing
  3. For each test sample: anchor to B's predicted class, compute ϕA and ϕB via occlusion
  4. Compute ∆ϕ, ∆f, and all suite metrics
  5. Aggregate across samples; flag based on BAC/DCE/JSD thresholds
- Design tradeoffs:
  - Occlusion vs. path methods (IG, SHAP): Occlusion is O(nd) and non-additive; path methods are additivity-guaranteed but more expensive
  - Sample cap (256) vs. full test set: Cap reduces runtime; may miss tail behavior
  - Class anchor (B's prediction) vs. true label: B's prediction stabilizes DCE/BAC; true label better for error analysis
- Failure signatures:
  - High DCE with low BAC: Occlusion not capturing additive structure; consider grouped occlusion or switch to IG
  - High ∆ magnitude with RankOverlap@10 ≈ 1.0: Likely reweighting, not redistribution; check JSD
  - Low stability under noise (σ=0.05): Attribution fragile; inspect baseline choice
- First 3 experiments:
  1. Sanity check: Run identical A/B (same hyperparameters)—expect RankOverlap@10=1.0, JSD≈0, DCE≈0
  2. Cosmetic tweak: SVC gamma=scale vs. auto—expect near-zero ∆, high rank overlap, DCE=0
  3. Structural change: SVC poly→rbf kernel—expect high BAC (>0.9), moderate DCE, elevated JSD; inspect top-5 features by |∆ϕ|

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the $\Delta$-Attribution framework be effectively adapted for high-dimensional unstructured data (e.g., LLMs, high-res vision) where standard occlusion is computationally prohibitive?
- Basis in paper: The authors identify "extending to text/vision and LLMs" as a key area for future work, noting in "Threats to validity" that current methods may require "sparse/grouped occlusions or alternative path methods" for such settings.
- Why unresolved: The current instantiation relies on feature-wise occlusion ($O(nd)$ complexity), which is feasible for the tested tabular datasets ($d \leq 64$) but likely scales poorly and lacks semantic meaning for high-dimensional inputs like raw pixels or token embeddings.
- What evidence would resolve it: Empirical results applying the framework to deep neural networks using adapted explainers (e.g., sparse occlusion or attention-based methods), demonstrating comparable behavioral alignment (BAC) and robustness without excessive runtime.

### Open Question 2
- Question: Does replacing the non-additive occlusion explainer with an additive method (e.g., SHAP or Integrated Gradients) significantly reduce Delta Conservation Error (DCE) and alter the audit conclusions?
- Basis in paper: The paper acknowledges in Section III and the Conclusion that occlusion is non-additive, meaning DCE serves only as a "diagnostic" rather than a guarantee, and explicitly lists "integrating additional explainers (IG/SHAP/TreeSHAP)" as future work.
- Why unresolved: High DCE values observed in Random Forest updates could stem from the explainer's properties rather than the model's actual reliance shifts; it is unclear if an additive explainer would provide a more faithful delta signal.
- What evidence would resolve it: A comparative ablation study running the $\Delta$-Attribution suite on the same model pairs using both occlusion and additive explainers, measuring the discrepancy in DCE and flagging decisions.

### Open Question 3
- Question: Do the proposed heuristic thresholds for "benign," "aligned," and "risky" updates generalize to diverse domains when calibrated against human expert judgment?
- Basis in paper: The authors define specific thresholds (e.g., BAC < 0.2 for benign) in Section VI but list "calibrating thresholds with human studies" as a necessary next step.
- Why unresolved: The current thresholds are "engineering defaults" derived from a limited set of 45 tabular settings; their utility in real-world governance pipelines depends on whether they align with actual practitioner intuition of risk.
- What evidence would resolve it: A user study where ML practitioners review flagged model updates, quantifying the agreement rate between the automated flags and human assessments of update safety.

## Limitations
- Occlusion-based delta attribution may be unstable when features are correlated or when models rely on feature interactions
- Baseline choice sensitivity, though reduced by averaging mean/median, may not eliminate artifacts for all distributions
- Framework validation limited to tabular datasets; generalization to high-dimensional domains like images or text remains unproven

## Confidence

- **High**: BAC as a diagnostic for behavioral alignment; RankOverlap@10 for detecting redistribution vs. reweighting
- **Medium**: DCE as a robustness filter—reliable when attribution is stable but noisy when it is not
- **Low**: Baseline averaging as a general fix—validated only on these three tabular datasets

## Next Checks

1. Run identical A/B models—expect DCE≈0, RankOverlap@10≈1.0; failure indicates baseline or implementation bug
2. Test occlusion vs. path methods (IG, SHAP) on one structural change—compare BAC and DCE to assess occlusion stability
3. Apply the framework to a small image dataset (e.g., MNIST) with a kernel change—check if BAC remains meaningful when standardized-space baselines are less interpretable