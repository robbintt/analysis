---
ver: rpa2
title: Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference
  Mismatch
arxiv_id: '2511.17826'
source_url: https://arxiv.org/abs/2511.17826
tags:
- inference
- across
- uni00000013
- different
- sizes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-deterministic inference
  in large language models caused by varying tensor parallel (TP) sizes. The core
  issue stems from the non-associativity of floating-point arithmetic and inconsistent
  reduction orders across GPUs, which leads to different outputs for identical inputs
  under different TP configurations.
---

# Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch

## Quick Facts
- arXiv ID: 2511.17826
- Source URL: https://arxiv.org/abs/2511.17826
- Reference count: 22
- This paper introduces Tree-Based Invariant Kernels (TBIK) that achieve bit-wise identical LLM inference across varying tensor parallelism sizes, eliminating training-inference mismatch in RL pipelines.

## Executive Summary
This paper addresses the critical problem of non-deterministic inference in large language models caused by varying tensor parallel (TP) sizes. The core issue stems from floating-point non-associativity combined with inconsistent reduction orders across GPUs, which leads to different outputs for identical inputs under different TP configurations. The authors propose TBIK, a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Experiments demonstrate zero probability divergence and bit-wise reproducibility across different TP sizes, enabling fully deterministic LLM inference for RL training pipelines.

## Method Summary
TBIK achieves determinism through a unified hierarchical binary tree structure that aligns both intra- and inter-GPU reduction orders. For row-parallel layers, TBIK replaces cuBLAS GEMM with a tree-based MatMul kernel using L=log₂(T/K_first) accumulator levels and carry-over operations. Inter-GPU reduction uses a custom Tree All-Reduce (All-Gather + pairwise binary reduction) instead of NCCL. Column-parallel layers use batch-invariant operations (BIO) with batch-dimension parallelization. The method requires disabling chunked prefill, CUDA Graphs, and ensuring shared kernel implementations across vLLM and FSDP frameworks.

## Key Results
- Zero probability divergence achieved across all 12 runtime configurations (TP=1/2/4/8 × BS=8/16/32)
- 56-135% latency overhead for bit-wise determinism, with All-Reduce contributing 28-50%
- Bit-wise identical probabilities between vLLM and FSDP in RL training pipelines
- 2-3x performance improvement over non-deterministic TP=1 baseline in some configurations

## Why This Works (Mechanism)

### Mechanism 1
Floating-point non-associativity causes output divergence when reduction order varies across TP sizes. IEEE 754 operations satisfy (a+b)+c ≠ a+(b+c) due to accumulated rounding errors. When row-parallel MatMul shards across GPUs, each GPU computes partial products XiWi, then All-Reduce sums them. Changing TP size changes both the local K-dimension reduction depth and the inter-GPU reduction topology, producing different arithmetic sequences and thus different outputs even under greedy decoding.

### Mechanism 2
Unified hierarchical binary tree reduction guarantees identical reduction order regardless of TP size. TBIK imposes a fixed binary-tree topology on both intra-GPU tile accumulation and inter-GPU All-Reduce. Within each GPU, tiles along the K-dimension are accumulated into a log₂(Tk/C)-level accumulator buffer using carry-over operations that trigger when level counters reach capacity. Across GPUs, All-Gather collects partial results, then pairwise reduction follows the same tree depth. Theorem 1 proves this yields T(k₁,...,kₙ)=T(T(L₁),...,T(Lc)) for any power-of-two C.

### Mechanism 3
Combining TBIK with batch-invariant operations achieves cross-framework bit-wise determinism. Column-parallel layers (QKV, up/gate proj, lm_head) use BIO with batch-dimension parallelization for batch-size invariance. Row-parallel layers (o proj, down proj) use TBIK for TP-size invariance. Additional alignment includes: fixed TritonAttention tile sizes, disabled chunked prefill, shared RMSNorm/RoPE/SiLU implementations across vLLM and FSDP. This closes the training-inference probability gap in RL pipelines.

## Foundational Learning

- **Floating-point associativity and IEEE 754 rounding**: Understanding why (a+b)+c≠a+(b+c) in BF16/FP16 is the root cause of non-determinism; without this, the motivation for tree-structured reduction is unclear. Quick check: Given BF16 values a=1.0, b=2⁻¹⁴, c=2⁻¹⁴, does (a+b)+c equal a+(b+c)?

- **Tensor parallelism (column-parallel vs row-parallel)**: Column-parallel layers concatenate outputs (no reduction); row-parallel layers require All-Reduce (reduction order matters). Knowing which layers are which determines where TBIK applies. Quick check: In a transformer, which requires All-Reduce: QKV projection or output projection?

- **Binary tree reduction and carry-over accumulation**: TBIK's core data structure is a fixed-depth accumulator buffer with carry-over logic; understanding this is essential for implementing or debugging the kernel. Quick check: If you have 8 tiles and a binary tree, what is the maximum number of accumulator levels needed?

## Architecture Onboarding

- **Component map**: Tree-Reduce MatMul Kernel (Triton) -> Custom Tree All-Reduce -> Batch-Invariant Ops (BIO) -> Framework Patches (vLLM/FSDP)

- **Critical path**: 1) Identify row-parallel layers in model (o_proj, down_proj) 2) Replace cuBLAS GEMM with Tree-Reduce MatMul kernel 3) Replace NCCL All-Reduce with custom Tree All-Reduce 4) Ensure BIO for column-parallel layers and shared auxiliary kernels 5) Disable non-deterministic scheduling (chunked prefill, CUDA Graph)

- **Design tradeoffs**: Correctness vs performance: 56-135% latency overhead for bit-wise determinism; tree-based All-Reduce contributes 28-50% due to naive implementation and lack of NVLink. Flexibility vs constraint: Requires TP sizes be powers of two; block sizes fixed per dtype (BF16: 64×256×128). Framework coupling vs portability: Must synchronize kernel implementations across vLLM and FSDP.

- **Failure signatures**: Non-zero Maximum Probability Divergence across TP sizes → Tree structure misaligned or BIO not applied to column-parallel layers. Run-to-run variation at same TP size → Chunked prefill enabled or CUDA Graph active. vLLM-FSDP probability gap → Attention kernel mismatch or RMSNorm/RoPE implementation differs.

- **First 3 experiments**: 1) Reproduce baseline non-determinism: Run same prompt across TP=1/2/4/8 with BF16, count unique outputs (expect ~12 unique sequences across 12 configs) 2) Validate TBIK alone: Apply TBIK to row-parallel layers only, measure Maximum Probability Divergence (should drop but not reach zero if batch sizes vary) 3) Full BIO+TBIK with cross-framework check: Enable both, run vLLM (TP=4) and FSDP (TP=1) on same prompts, verify bit-wise identical probabilities and measure latency overhead

## Open Questions the Paper Calls Out

- **Can TBIK be adapted for low-bit quantized inference (e.g., INT4/INT8)?** The Conclusion explicitly identifies extending guarantees to "low-bit matmul kernels" as a key direction for future work. Quantized kernels introduce non-deterministic rounding, scaling, and fused dequantization paths not currently aligned with the tree-structured accumulation logic.

- **Can the TBIK performance overhead be eliminated to match standard cuBLAS throughput?** The authors acknowledge the kernels are "unoptimized" and state that "performance can be further improved" via techniques like warp specialization. The tree structure necessitates extra temporary accumulator buffers and I/O, currently limiting kernel throughput to 63% of cuBLAS.

- **Does TBIK eliminate router divergence in Mixture-of-Experts (MoE) models?** The paper notes MoE models are "much more severe" regarding probability perturbations, yet the evaluation is restricted to dense model architectures. It is uncertain if bit-wise consistency in linear layers is sufficient to force identical expert routing decisions across different TP sizes.

## Limitations

- **Performance overhead**: 56-135% latency overhead (28-50% from All-Reduce) may be prohibitive for production-scale deployments
- **Implementation dependency**: BIO kernel implementations are cited from prior work but not fully detailed in this paper
- **Scalability constraints**: The paper doesn't address whether these overheads are acceptable for production RLHF pipelines with 70B+ parameter models

## Confidence

- **High confidence** in the core mathematical mechanism: The floating-point non-associativity argument and binary tree reduction proof are well-established principles
- **Medium confidence** in practical implementation: Integration requires careful kernel patching and configuration management
- **Medium confidence** in performance claims: 56-135% overhead is significant but alternative deterministic approaches aren't benchmarked

## Next Checks

1. **Cross-architecture determinism test**: Run identical TBIK-enabled inference across different GPU architectures (NVIDIA vs AMD) to verify whether the binary tree reduction produces truly architecture-independent results

2. **Production-scale performance evaluation**: Benchmark TBIK in an end-to-end RLHF pipeline with real-world model sizes (70B+ parameters) to measure the practical impact of 50-135% overhead on training throughput

3. **Alternative deterministic kernels comparison**: Implement and compare TBIK against other deterministic approaches like compensated summation (Kahan/Babushka) or FP32 accumulators to quantify the performance-accuracy tradeoff