---
ver: rpa2
title: Random forest-based out-of-distribution detection for robust lung cancer segmentation
arxiv_id: '2508.19112'
source_url: https://arxiv.org/abs/2508.19112
tags:
- segmentation
- cancer
- detection
- scans
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of reliable lung cancer segmentation
  in computed tomography (CT) scans, particularly when models encounter out-of-distribution
  (OOD) data such as scans with pulmonary embolism, COVID-19, or abdominal conditions.
  To tackle this, the authors introduce RF-Deep, a random forest classifier that leverages
  deep features from a pretrained Swin Transformer encoder to detect OOD cases.
---

# Random forest-based out-of-distribution detection for robust lung cancer segmentation

## Quick Facts
- **arXiv ID:** 2508.19112
- **Source URL:** https://arxiv.org/abs/2508.19112
- **Reference count:** 39
- **Primary result:** RF-Deep achieves FPR95 of 18.26% on pulmonary embolism, 27.66% on COVID-19, and less than 1% on abdominal CTs, significantly outperforming confidence-based OOD methods.

## Executive Summary
This paper introduces RF-Deep, a random forest-based out-of-distribution (OOD) detection framework for lung cancer segmentation in CT scans. The method combines a Swin Transformer encoder pretrained with masked image modeling (SimMIM) for segmentation with a random forest classifier that uses deep multi-scale features for OOD detection. Evaluated on 603 CT scans across five datasets, RF-Deep demonstrates superior performance compared to existing OOD detection methods, particularly for near-OOD cases like pulmonary embolism and COVID-19 that share anatomical proximity with lung cancer.

## Method Summary
RF-Deep leverages a Swin Transformer encoder pretrained via SimMIM on 10,432 unlabeled CT scans, then fine-tuned for lung cancer segmentation. The segmentation model outputs tumor predictions, and multi-scale features are extracted from the frozen encoder around predicted tumor regions. These features from early and mid-stage Swin blocks are aggregated via global average pooling and used to train a random forest classifier with outlier exposure. The system achieves scan-level OOD detection by averaging predictions across eight tumor-centered crops per scan, with class-balanced random forest trained on 40/60 train/test split across 100 seeds.

## Key Results
- RF-Deep achieves FPR95 of 18.26% on pulmonary embolism, 27.66% on COVID-19, and less than 1% on abdominal CTs
- Significant improvement over MaxSoftmax (37.01% FPR95 on PE) and other confidence-based methods
- Mid- and early-stage Swin Transformer features prove most effective for OOD detection
- Tumor-anchored feature aggregation outperforms whole-image approaches for near-OOD cases

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining with masked image modeling (SimMIM) produces encoder features that are more separable between ID and OOD scans than confidence-based metrics. The encoder learns to reconstruct randomly masked 3D CT patches from 10,432 diverse scans, forcing it to capture generalizable anatomical and textural patterns rather than overfitting to the lung cancer training distribution. When later fine-tuned for segmentation, the encoder retains discriminative features that differ systematically between lung cancer and other pathologies/sites.

### Mechanism 2
Focusing OOD detection on tumor-predicted regions rather than whole images improves detection accuracy for near-OOD cases sharing the same anatomical site. The segmentation model outputs a binary tumor mask; OOD scoring aggregates only over predicted tumor regions. This excludes irrelevant anatomy that may have similar statistics across ID/OOD, focusing instead on regions where the model is most likely to produce confidently incorrect predictions.

### Mechanism 3
A random forest trained on multi-scale encoder features (from early and mid-stage Swin blocks) outperforms confidence-based OOD methods by capturing non-linear feature interactions. Features are extracted from the patch embedding layer and Swin blocks across all four stages, then aggregated via global average pooling. The random forest learns decision boundaries on these ~1300 features without gradient-based optimization, making it robust to overfitting on limited OOD examples.

## Foundational Learning

- **Concept: Out-of-distribution (OOD) detection vs. anomaly detection**
  - Why needed here: OOD detection in this paper is supervised (outlier exposure) rather than unsupervised; understanding this distinction prevents misapplying one-class methods.
  - Quick check question: Does the method require labeled OOD examples during training? (Yes—RF-Deep uses outlier exposure.)

- **Concept: Hierarchical transformer features (multi-scale)**
  - Why needed here: The paper extracts features from all four Swin stages; early stages capture local texture while later stages capture global context. Ablation shows mid-stage features are most important.
  - Quick check question: Which stage's features would you expect to best distinguish a lung nodule texture from COVID-19 ground-glass opacity? (Likely early/mid stages.)

- **Concept: FPR95 as a threshold-sensitive metric**
  - Why needed here: AUROC measures separability but doesn't reflect operational performance at high recall; FPR95 (false positive rate at 95% true positive rate) directly measures safety-critical failure modes.
  - Quick check question: Why is FPR95 more clinically relevant than AUROC for OOD detection? (Because high recall is required to catch all OOD cases; false positives at that threshold matter.)

## Architecture Onboarding

- **Component map:**
  1. Swin Transformer encoder (4 stages, 2-2-12-2 depth, 4×4×4 patches) — pretrained via SimMIM, fine-tuned for segmentation
  2. Convolutional decoder (U-Net style) — trained jointly with encoder on lung cancer masks
  3. Feature extractor — frozen encoder, multi-scale features from PE + Swin blocks, GAP-aggregated
  4. RF-Deep classifier — 1000 trees, max depth 20, balanced class weights, trained on ID + OOD features

- **Critical path:**
  1. Pretrain encoder with SimMIM on diverse CT corpus (10K scans)
  2. Fine-tune encoder + decoder on lung cancer segmentation (317 scans)
  3. Freeze encoder, extract features from ID + OOD scans (tumor-centered crops)
  4. Train RF on features with outlier exposure
  5. At inference: extract features → RF predicts ID/OOD → if ID, trust segmentation; if OOD, flag for review

- **Design tradeoffs:**
  - Outlier exposure vs. OOD generalization: RF-Deep requires representative OOD examples during training; may fail on novel OOD types not seen during training
  - Tumor-anchored aggregation: Improves near-OOD detection but risks failure if segmentation model outputs no predictions
  - Random forest vs. neural OOD detector: RF is interpretable (SHAP analysis shown) and trains fast on small data, but may not scale to larger feature spaces or more complex boundaries

- **Failure signatures:**
  - Empty segmentation mask on OOD scan → no tumor region for aggregation (undefined behavior)
  - OOD scan with visually similar tumor-like structures → segmentation model produces confident but incorrect predictions; RF must still detect via encoder features
  - OOD type not represented in training (e.g., novel pathology) → RF may misclassify as ID (generalization limit not tested)

- **First 3 experiments:**
  1. Reproduce RF-Deep on public lung cancer dataset: Fine-tune pretrained Swin encoder on NSCLC-Radiomics data (N=317), extract features, train RF with 40/60 split, verify AUROC/FPR95 on held-out ID + PE/COVID-19 OOD sets.
  2. Ablate feature stages: Train separate RF models using features from each Swin stage alone; confirm mid-stage features yield best OOD detection as reported.
  3. Test empty-mask handling: Synthetically zero out segmentation predictions on subset of OOD scans; determine if fallback to whole-image features is needed (not addressed in paper).

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the RF-Deep framework perform when applied to multi-class segmentation tasks rather than the binary tumor segmentation evaluated in this study?
  - Basis in paper: The conclusion explicitly states that future work involves "understanding multi-class behavior."
  - Why unresolved: The current study evaluates performance solely on binary lung cancer segmentation; the interaction between multi-class feature representations and the random forest OOD classifier remains unknown.
  - What evidence would resolve it: Evaluation of the RF-Deep pipeline on a multi-class dataset (e.g., simultaneous segmentation of organs and different tumor types) comparing OOD detection metrics against the binary baseline.

- **Open Question 2:** Does the effectiveness of the deep features utilized by RF-Deep generalize to different backbone architectures and pretraining strategies?
  - Basis in paper: The authors identify "evaluating generalization across other backbone pretraining strategies" as a direction for future work.
  - Why unresolved: The results are specific to a Swin Transformer encoder pretrained with SimMIM; it is uncertain if the feature robustness holds for CNN-based backbones or other self-supervised methods.
  - What evidence would resolve it: Comparative experiments implementing RF-Deep on standard CNN architectures (e.g., ResNet) or transformers pretrained with contrastive learning (e.g., MoCo) to assess relative performance drops.

- **Open Question 3:** How robust is RF-Deep when scaled to anatomical disease sites beyond lung cancer?
  - Basis in paper: The conclusion lists "scaling to more disease sites" as a primary area for future investigation.
  - Why unresolved: While abdominal scans were used as OOD data, the segmentation model itself was trained only for lung cancer; the utility of this specific feature extraction method for other cancer targets is unverified.
  - What evidence would resolve it: Application of the RF-Deep pipeline to other segmentation tasks (e.g., brain tumor or prostate segmentation) to determine if similar FPR95 improvements over MaxSoftmax are achieved.

## Limitations
- RF-Deep requires representative OOD examples during training and may fail on novel OOD types not seen during training
- The tumor-anchored aggregation approach has undefined behavior when the segmentation model produces empty masks on OOD scans
- Key architectural hyperparameters (Swin window size, attention heads, decoder details) are unspecified, potentially affecting reproducibility

## Confidence
- **High confidence:** The RF-Deep architecture (Swin-UNet + RF classifier) is technically sound and the reported metrics (FPR95, AUROC) are valid for OOD detection
- **Medium confidence:** The SimMIM pretraining provides generalizable features, but without direct ablation against other pretraining strategies, this remains partially supported
- **Medium confidence:** Tumor-anchored aggregation improves near-OOD detection, though the empty-mask failure mode is not addressed

## Next Checks
1. Test RF-Deep on a fourth OOD type (e.g., brain CT or liver cancer) not represented in the training set to evaluate generalization limits
2. Implement fallback mechanism for empty segmentation masks (e.g., whole-image feature aggregation) and measure performance degradation
3. Conduct ablation study comparing SimMIM pretraining against random initialization or supervised pretraining on lung cancer data