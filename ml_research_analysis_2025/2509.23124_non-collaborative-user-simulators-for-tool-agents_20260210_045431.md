---
ver: rpa2
title: Non-Collaborative User Simulators for Tool Agents
arxiv_id: '2509.23124'
source_url: https://arxiv.org/abs/2509.23124
tags:
- user
- agent
- your
- dialogue
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the gap in current tool agent evaluation\
  \ by proposing a user simulation framework that models non-collaborative behaviors\
  \ observed in real-world interactions. The authors define four categories of non-collaborative\
  \ behaviors\u2014unavailable service requests, tangential conversations, impatience,\
  \ and incomplete utterances\u2014and develop a simulation architecture that incorporates\
  \ these behaviors while maintaining goal alignment."
---

# Non-Collaborative User Simulators for Tool Agents

## Quick Facts
- **arXiv ID**: 2509.23124
- **Source URL**: https://arxiv.org/abs/2509.23124
- **Reference count**: 40
- **Primary result**: State-of-the-art tool agents show 14-31% performance degradation when encountering non-collaborative users

## Executive Summary
This paper addresses the gap in current tool agent evaluation by proposing a user simulation framework that models non-collaborative behaviors observed in real-world interactions. The authors define four categories of non-collaborative behaviors—unavailable service requests, tangential conversations, impatience, and incomplete utterances—and develop a simulation architecture that incorporates these behaviors while maintaining goal alignment. Experiments on MultiWOZ and τ-bench demonstrate that state-of-the-art tool agents show significant performance degradation when encountering non-collaborative users, with performance drops ranging from 14% to 31% across different behaviors. The framework successfully simulates challenging yet realistic dialogue scenarios while ensuring all task-relevant information is communicated, providing a valuable tool for developing more robust tool agents capable of handling the complexities of real-world user interactions.

## Method Summary
The framework extends a collaborative user simulator (Yao et al. 2024) by adding four non-collaborative behavior modules that operate on user goals rather than relying on prompt engineering. The simulator uses a Dialogue State Tracker to shard user goals into information pieces and track what has been conveyed, ensuring goal alignment even when behaviors truncate or divert conversations. Non-collaborative behaviors are injected through specialized modules: Unavailable Service augments goals with unfulfillable requests, Tangential generates persona-based off-topic utterances merged with task content, Impatience triggers anger expressions on failures/delays, and Incomplete Utterance applies style transfer or truncation to mimic real user patterns. Agents are evaluated using the ReAct framework with a 30-step reasoning limit across MultiWOZ (89 scenarios) and τ-bench (157 scenarios).

## Key Results
- Tool agents experience 14-31% performance drops when facing non-collaborative users across different behavior types
- Goal alignment remains high (97.5-99.5%) even with combined non-collaborative behaviors, validating the framework's design
- Agent reasoning resources are exhausted through repetitive API calls, excessive apologizing, and parameter hallucination errors
- Even advanced models like GPT-4.1-mini show significant degradation, indicating these challenges affect current state-of-the-art systems

## Why This Works (Mechanism)

### Mechanism 1: Behavior Injection via Goal Augmentation
The framework achieves more realistic non-collaborative behaviors by structurally modifying user goals rather than using prompt engineering. This goal-augmentation approach creates behaviors grounded in the intent structure itself, producing more challenging scenarios than surface-level linguistic patterns. The method adds unavailable service requests to goals, merges tangential utterances with task-focused ones, and probabilistically triggers impatience based on agent failures or delays. Evidence shows this produces significantly more challenging scenarios than prompt-only approaches, with goal alignment maintained at 97.5-99.5% even under combined behaviors.

### Mechanism 2: Performance Degradation Through Reasoning Resource Exhaustion
Non-collaborative behaviors degrade agent performance primarily by consuming limited reasoning steps without progressing toward task completion. Agents exhibit repetitive helper API calls in unavailable service mode, excessive apology generation in impatience mode, and cascading parameter hallucination errors in incomplete utterance mode. These behaviors consume the 30-step reasoning limit without advancing the task, leading to failure. Evidence shows GPT-4.1-nano's failure rate increases significantly under non-collaborative conditions, with 44% of simulations exceeding the 30-step limit in tangential mode versus 15% in collaborative mode.

### Mechanism 3: Goal Alignment Preservation Through State Tracking
Maintaining goal alignment while exhibiting non-collaborative behaviors requires explicit state tracking rather than relying on LLM self-regulation. The framework uses a Dialogue State Tracker to monitor which information pieces from the user goal have been conveyed, and an Ending Verifier to prevent premature dialogue termination. When non-collaborative behaviors truncate utterances or divert conversation, the tracker ensures remaining information is eventually delivered. The architecture achieves 97.5-99.5% goal alignment scores even with combined behaviors, compared to what would likely be lower alignment from prompt-only approaches without explicit tracking.

## Foundational Learning

- **Concept: Task-Oriented Dialogue Systems**
  - **Why needed here:** The entire framework operates within task-oriented dialogue settings where agents must complete specific goals through multi-turn interaction. Understanding MultiWOZ vs. τ-bench differences in API accessibility and task structure is prerequisite.
  - **Quick check question:** Can you explain how MultiWOZ differs from τ-bench in terms of API accessibility and task structure?

- **Concept: ReAct Framework for Tool-Using Agents**
  - **Why needed here:** The tool agents in experiments use ReAct-style reasoning (interleaving thought and action). Understanding this architecture is essential for interpreting why agents get stuck in repetitive API calls or excessive apologizing.
  - **Quick check question:** What are the two action types available to ReAct agents, and how does the 30-step reasoning limit constrain behavior?

- **Concept: User Simulation for Dialogue System Evaluation**
  - **Why needed here:** The paper's contribution is fundamentally about improving user simulation methodology. Understanding why simulation is necessary and what makes a simulator "good" is foundational.
  - **Quick check question:** What are the trade-offs between prompt-based user simulators and architectures with explicit state tracking modules?

## Architecture Onboarding

- **Component map:**
User Goal → Collaborative User Simulator → Base Utterance → [Non-Collaborative Modules] → Dialogue State Tracker → Ending Verifier → Final User Utterance → Tool Agent

- **Critical path:**
Start with the collaborative simulator backbone, add one non-collaborative module at a time (recommend Incomplete Utterance first), verify goal alignment using the state tracker, then combine multiple behaviors while monitoring IGA scores.

- **Design tradeoffs:**
Complexity vs. Controllability: More LLM modules increase behavioral diversity but make debugging harder. Realism vs. Solvability: Aggressive behaviors may create unsolvable scenarios balanced by 30-step limit and goal alignment checks. Prompt-only vs. Architecture-based: Simpler but shows less performance impact on agents.

- **Failure signatures:**
Goal misalignment: Simulator terminates without conveying all information → unsolvable dialogues. Agent excessive apologies: In impatience mode, agents apologize repeatedly without progressing → reasoning consumption. Helper API loops: In unavailable service mode, agents repeatedly fetch documentation → reasoning exhaustion. Parameter hallucination: In incomplete utterance mode, agents fabricate API parameters → booking failures.

- **First 3 experiments:**
1. Reproduce collaborative baseline on MultiWOZ (89 scenarios) with GPT-4.1-mini. Target: ~93% success rate.
2. Add only Incomplete Utterance mode. Compare success rates and error types against collaborative baseline.
3. Run combined non-collaborative modes (e.g., Tangential + Incomplete) and measure IGA scores. If <90%, debug state tracker logic.

## Open Questions the Paper Calls Out

### Open Question 1
**What training methodologies optimally improve agent robustness to non-collaborative user behaviors?**
The authors note that "fine-tuning on collaborative data alone produces agents vulnerable to non-collaborative user behaviors" and present preliminary results with mixed collaborative/non-collaborative training data, but do not systematically explore curriculum strategies, data proportions, or reinforcement learning approaches. Only two weighting schemes were tested on one small model; no comparison of training paradigms or optimization techniques was conducted. Systematic experiments across multiple model scales comparing curriculum learning, reinforcement learning from simulator feedback, and varying data mixture ratios would resolve this.

### Open Question 2
**How can agents be designed to handle multiple simultaneous non-collaborative behaviors without compounding performance degradation?**
The authors observe that "even GPT-4.1-mini, which demonstrated minimal performance degradation with individual non-collaborative behaviors, experiences significant performance drops when facing users exhibiting two behaviors concurrently." The analysis identifies the problem but provides no investigation into architectural modifications, prompting strategies, or specialized modules for multi-behavior handling. Development and evaluation of agent architectures or prompting approaches specifically designed for multi-behavior scenarios would resolve this.

### Open Question 3
**Which domains and task structures exhibit similar agent vulnerability patterns to non-collaborative behaviors, and why do patterns diverge?**
Appendix F shows different performance patterns across ColBench, MINT, MultiWOZ, and τ-bench (e.g., Qwen3-30b-a3b showed robustness to unavailable service in booking tasks but degradation in MINT), but no analysis explains these domain-specific differences. The framework is demonstrated as extensible, but no systematic comparison of failure mechanisms across different task types is provided. Controlled experiments across diverse task domains with analysis of how task structure mediates non-collaborative behavior impact would resolve this.

## Limitations
- Evaluation focuses primarily on LLM-based tool agents, leaving open questions about non-LLM architectures
- The 30-step reasoning limit may not fully capture real-world deployment scenarios with flexible computational budgets
- Goal alignment metric may not fully capture information delivery quality—technically complete but confusing information may still pass

## Confidence
- **High confidence**: Non-collaborative behaviors significantly degrade tool agent performance (14-31% drops) is well-supported by systematic experiments
- **Medium confidence**: Performance degradation mechanism through reasoning resource exhaustion is plausible but could reflect implementation limitations
- **Medium confidence**: Explicit state tracking over prompt-only approaches is justified empirically, though prompt-only achieving similar alignment would challenge this

## Next Checks
1. **Cross-architecture validation**: Test non-collaborative behavior impact on rule-based or hybrid tool agents to determine if challenges are architecture-specific
2. **Extended reasoning limits**: Evaluate agent performance with increased reasoning limits (e.g., 50 steps) to distinguish fundamental robustness failures from implementation constraints
3. **Human evaluation of goal alignment**: Assess whether high goal alignment scores (97.5-99.5%) translate to information delivery that would be considered helpful and clear by real users