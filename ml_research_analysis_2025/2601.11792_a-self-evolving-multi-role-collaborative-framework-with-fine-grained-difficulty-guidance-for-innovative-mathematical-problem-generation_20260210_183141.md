---
ver: rpa2
title: A self-evolving multi-role collaborative framework with fine-grained difficulty
  guidance for innovative mathematical problem generation
arxiv_id: '2601.11792'
source_url: https://arxiv.org/abs/2601.11792
tags:
- uni00000013
- uni00000011
- problem
- uni00000010
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-evolving, multi-role collaborative
  framework with fine-grained difficulty guidance for innovative mathematical problem
  generation (IMPG). The method constructs a multi-role collaborative mechanism including
  a sampler, generator, evaluator, state machine, and memory, enabling iterative optimization
  informed by self-assessment and external feedback.
---

# A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation

## Quick Facts
- arXiv ID: 2601.11792
- Source URL: https://arxiv.org/abs/2601.11792
- Reference count: 40
- Generates innovative Chinese high school math problems with fine-grained difficulty control

## Executive Summary
This paper introduces a self-evolving, multi-role collaborative framework for innovative mathematical problem generation (IMPG) that addresses the "innovation curse" in LLM-generated educational content. The framework constructs a multi-role collaborative mechanism including sampler, generator, evaluator, state machine, and memory to iteratively optimize problem generation through self-assessment and external feedback. By decoupling generation from evaluation and introducing fine-grained difficulty control via a 16-bit encoding scheme and data-driven association-guided path sampling (DAPS), the system significantly improves problem innovation and discrimination while maintaining high correctness rates, outperforming baseline models across core quality dimensions.

## Method Summary
The framework employs a multi-stage training pipeline on Qwen3-32B: CPT for domain knowledge (LoRA r=16, lr=2e-5), SFT for generator and evaluator roles (LoRA r=64, lr=3e-5), and GRPO for fine-tuning (lr=2e-6). The DAPS algorithm samples valid 8-dimensional difficulty encodings from historical problem correlations, ensuring semantic validity through constrained random walks on a transition probability matrix. Self-evolution occurs via knowledge distillation from an expert model (Gemini-2.5-Pro) to the apprentice model, teaching evaluation capabilities. The system uses HSM3K-CN dataset (3,160 problems) split into generation and evaluation sets, with inference via vLLM (temp=0.2, top_p=0.7, top_k=20).

## Key Results
- The multi-role framework significantly alleviates the innovation-correctness tradeoff, improving innovation scores by 1.2-1.6 points while maintaining correctness above 9.3/10
- DAPS algorithm achieves 83-95% semantic rationality across difficulty levels, outperforming random sampling baselines
- Self-evolution through knowledge distillation successfully transfers expert evaluation capabilities to the apprentice model
- Overall quality (G-eval) reaches 9.1/10, with superior performance in core dimensions (Correctness-P, Correctness-S, Innovation) compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework mitigates the "Innovation Curse" by decoupling generation from evaluation in a closed-loop system
- **Mechanism:** A single LLM plays dual roles (Generator and Evaluator) or uses an external Expert model. The Generator proposes a solution, and the Evaluator provides negative feedback (revision suggestions) if quality thresholds aren't met. This iterative refinement allows the Generator to adopt aggressive innovation strategies (learned via SFT), while the Evaluator acts as a safety filter to recover correctness.
- **Core assumption:** The Evaluator model (especially the "Expert" mode) possesses superior reasoning or error-detection capabilities compared to the Generator model, allowing it to identify logical flaws the Generator introduces.
- **Evidence anchors:** [abstract] "...ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback." [section 5.2.2] "This phenomenon reveals an inherent conflict between correctness and innovation... confirming the innovation curse... The multi-role collaborative framework alleviates the innovation curse through role specialization."

### Mechanism 2
- **Claim:** Fine-grained control over problem difficulty is achieved by mapping abstract difficulty levels to concrete 16-bit feature vectors via DAPS
- **Mechanism:** Instead of prompting "Hard difficulty," the system uses DAPS to sample a valid 8-dimensional encoding (e.g., "Scientific background" + "Complex reasoning"). DAPS constructs a transition probability matrix from historical data to ensure that sampled dimensions are semantically compatible (e.g., avoiding contradictory constraints).
- **Core assumption:** Historical problem datasets contain valid correlation patterns between difficulty dimensions (e.g., Jaccard coefficients) that can be modeled as a Markov chain for random walks.
- **Evidence anchors:** [abstract] "It introduces an improved difficulty model with 16-bit encoding and employs a data-driven association-guided path sampling (DAPS) algorithm for semantic validity." [section 3.3.2] "A complete difficulty encoding is an eight-tuple vector... DAPS algorithm... mines encoding co-occurrence patterns."

### Mechanism 3
- **Claim:** System self-evolution occurs via knowledge distillation from a stronger "Expert" model
- **Mechanism:** The system generates problems using the Apprentice. The Expert evaluates these problems and generates detailed "reasoning" for its scores. This Expert reasoning is then used as ground truth to fine-tune the Apprentice (distillation), teaching the Apprentice the Expert's evaluation criteria.
- **Core assumption:** The Expert model's evaluation logic can be compressed into the Apprentice model without significant loss of reasoning fidelity.
- **Evidence anchors:** [abstract] "System self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation." [section 5.5] "Knowledge distillation successfully transferred the evaluation capabilities of the expert model... bringing its evaluation performance closer to the expert model."

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF) vs. GRPO**
  - **Why needed here:** The system uses GRPO to tune the generator. Unlike standard RLHF, GRPO estimates baselines from group outputs rather than a separate value model, which is critical for understanding the training efficiency claims in Section 3.4.3.
  - **Quick check question:** How does GRPO estimate the advantage $\hat{A}_{i,t}$ without training a separate value model? (Answer: By normalizing rewards within a group of outputs).

- **Concept: Markov Chain Monte Carlo (MCMC) / Random Walks**
  - **Why needed here:** The DAPS algorithm relies on a random walk on a graph of difficulty nodes constrained by transition probabilities. Understanding this is necessary to debug why the sampler might generate specific difficulty combinations.
  - **Quick check question:** In the DAPS transition matrix, why is the matrix non-symmetric even if the underlying Jaccard association is symmetric? (Answer: Because normalization factors differ per column/row, reflecting conditional probability).

- **Concept: Prompt Engineering / Role-Playing in LLMs**
  - **Why needed here:** The "Homogeneous Multi-role" mechanism relies on a single model acting as Sampler, Generator, and Evaluator. This requires distinct "System Prompts" (see Appendix A) to switch behaviors.
  - **Quick check question:** How does the framework ensure the Evaluator remains "stateless"? (Answer: It is designed to evaluate only the current content without memory of previous iterations, preventing bias from prior failed attempts).

## Architecture Onboarding

- **Component map:** User Request -> Sampler (DAPS) -> Generator -> Evaluator -> State Machine -> Output
- **Critical path:** 1. User Request → Sampler (DAPS) creates fine-grained prompt. 2. Prompt → Generator creates candidate problem. 3. Candidate → Evaluator (Expert or Apprentice) checks logic. 4. State Machine reviews scores: If Pass → Output; If Fail → Feed Feedback to Generator → Repeat.
- **Design tradeoffs:** Apprentice vs. Expert Mode: Apprentice mode is low-cost/low-latency but may suffer from "blind spot consistency" (it cannot fix errors it created). Expert mode is high-cost (API calls) but maximizes correctness. The distillation step attempts to move the Apprentice toward Expert performance to close this gap. 16-bit Encoding Granularity: 8 dimensions allow high control but increase the search space for the sampler, potentially reducing the probability of finding valid encodings for "Expert" difficulty levels.
- **Failure signatures:** Infinite Loops: The State Machine enters an endless cycle if the Generator consistently produces output below the threshold τ but the Evaluator's feedback is unhelpful. Semantic Drift: The Generator ignores the specific 16-bit difficulty constraints during the "Innovation" phase, reverting to generic "Easy" problems to ensure safety (observed in baseline comparisons). Overfitting to History: DAPS generates only "Standard" problems because the transition matrix heavily weights common historical combinations (low entropy in dimensions A and G).
- **First 3 experiments:** 1. Ablation on Difficulty Model: Run the system with Random Sampling (RS) vs. DAPS. Measure the "Semantic Rationality" score (does the problem make sense?) to verify the contribution of the transition matrix. 2. Mode Comparison: Generate 100 problems using Apprentice-only vs. Expert-mode. Compare the average number of iterations (efficiency) and Correctness-S score (quality) to quantify the "Self-Evolution" gap. 3. Stress Test DAPS: Request problems at "Expert" difficulty. Monitor the constraint satisfaction rate. If low, check if the historical dataset (HSM3K-CN) contains enough high-difficulty samples to build a valid transition matrix.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the difficulty encoding scheme and IMPG framework effectively transfer to English or non-Chinese educational contexts without extensive re-annotation?
- **Basis in paper:** [explicit] The study relies exclusively on the Chinese HSM3K-CN dataset for training and evaluation.
- **Why unresolved:** The semantic rationality learned by DAPS is data-driven, and difficulty dimensions may be culturally or linguistically specific to the Chinese high school curriculum.
- **What evidence would resolve it:** Experiments applying the current encoding to English math datasets (e.g., MATH or GSM8K) and comparing generation quality.

### Open Question 2
- **Question:** How can the DAPS algorithm maintain high constraint satisfaction rates for "Expert" level difficulties where historical training data is sparse?
- **Basis in paper:** [explicit] Results show DAPS efficiency drops for Hard/Expert levels because the algorithm "reproduces the distribution of historical data where Easy and Medium items predominate."
- **Why unresolved:** The data-driven transition matrix fails to generate valid paths when target encodings fall in the long tail of the historical distribution.
- **What evidence would resolve it:** Performance metrics of a modified DAPS (e.g., using synthetic data or rule-based augmentation) on the Expert difficulty tier.

### Open Question 3
- **Question:** Can the self-evolving multi-role framework be extended to generate and evaluate multimodal problems (e.g., geometry with diagrams) while maintaining difficulty consistency?
- **Basis in paper:** [inferred] The current system is text-only and the difficulty model is text-centric, whereas real-world geometry problems often require visual aids.
- **Why unresolved:** Evaluating visual correctness and innovation requires distinct evaluator capabilities and potentially a new multimodal difficulty encoding schema.
- **What evidence would resolve it:** Integration of a visual generation module into the framework and ablation studies measuring difficulty alignment in visual problem generation.

## Limitations

- The framework's reliance on historical correlation patterns (DAPS) introduces a fundamental limitation: if the training corpus underrepresents certain difficulty combinations, the system cannot generate novel problem types outside these patterns
- The "innovation curse" mitigation through multi-role collaboration assumes the Evaluator model is consistently more capable than the Generator, but this capability gap may narrow after multiple self-evolution cycles
- The 16-bit difficulty encoding, while granular, may oversimplify complex mathematical concepts that don't decompose cleanly into the predefined dimensions

## Confidence

- **High confidence:** The multi-role collaborative framework's architecture and basic implementation details are well-specified. The effectiveness of role specialization in alleviating the innovation-correctness tradeoff is supported by experimental evidence.
- **Medium confidence:** The self-evolution mechanism through knowledge distillation is conceptually sound, but the long-term stability and capability of the distilled Apprentice model relative to the Expert requires further validation across extended use.
- **Medium confidence:** The DAPS algorithm for fine-grained difficulty control is technically described, but its practical effectiveness depends on the richness and representativeness of the historical data, which isn't fully characterized.

## Next Checks

1. **Correlation matrix validation:** Analyze the sparsity and entropy of the DAPS transition matrix to quantify how much of the difficulty space is actually reachable versus constrained by historical patterns.
2. **Long-term self-evolution stability:** Run extended distillation cycles (5+ generations) and measure whether the Apprentice model's evaluation performance plateaus, degrades, or continues improving relative to the Expert baseline.
3. **Cross-domain generalizability test:** Apply the framework to a different mathematical domain (e.g., geometry) with a new difficulty encoding to assess whether the multi-role collaboration and DAPS approach transfers beyond the derivative/sequence/probability scope of HSM3K-CN.