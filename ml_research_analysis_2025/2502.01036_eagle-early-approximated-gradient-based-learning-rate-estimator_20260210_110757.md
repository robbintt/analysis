---
ver: rpa2
title: 'eagle: early approximated gradient based learning rate estimator'
arxiv_id: '2502.01036'
source_url: https://arxiv.org/abs/2502.01036
tags:
- loss
- update
- eagle
- rule
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# eagle: early approximated gradient based learning rate estimator

## Quick Facts
- arXiv ID: 2502.01036
- Source URL: https://arxiv.org/abs/2502.01036
- Reference count: 0
- Key outcome: EAGLE achieves 40-75% lower loss than Adam in early training stages (epochs 2-10) on UCI Iris/Wine datasets, but shows decreased final convergence performance.

## Executive Summary
EAGLE is a novel optimizer that accelerates early-stage neural network training by estimating local loss curvature from consecutive parameter and gradient changes. The core mechanism computes an adaptive learning rate as Δθ/Δg, enabling rapid convergence on convex loss regions. When this estimation becomes unreliable (small gradient differences or locally convex regions), EAGLE switches to Adam. While EAGLE significantly outperforms Adam in early training stages, it exhibits decreased final convergence performance on some datasets, highlighting the need for late-stage optimization strategies.

## Method Summary
EAGLE computes parameter updates using the formula θ_{n+1} = θ_n - (Δθ/Δg) · g_n, where Δθ and Δg are changes in parameters and gradients between consecutive steps. The ratio Δθ/Δg serves as an adaptive learning rate that estimates local curvature. When |Δg| falls below a threshold (0.0005) or when gradient sign patterns indicate locally convex regions, EAGLE switches to Adam to maintain stability. The optimizer was tested on UCI Iris (4→25→3 FC), UCI Wine (13→15→3 FC), MNIST (2×[Conv3×3, BN, ReLU, MaxPool2×2]→FC(3136→128, BN, ReLU)→FC(128→10)), and CIFAR-10 with ViT-H/14.

## Key Results
- 40-75% lower loss than Adam at epoch 2 on Iris/Wine datasets
- EAGLE usage rate decreases from ~50% (epoch 10) to ~2% (epoch 100) on Wine
- Decreased final convergence performance on some datasets compared to Adam
- MNIST results show EAGLE performs similarly to Adam with less early-stage advantage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EAGLE accelerates early-stage convergence by using the ratio of parameter changes to gradient changes as a dynamic learning rate that implicitly estimates local loss curvature.
- **Mechanism:** The update rule θ_{n+1} = θ_n - (Δθ/Δg) · g_n treats Δθ/Δg as an adaptive learning rate β. When the loss landscape is locally convex, this ratio approximates the inverse curvature, enabling larger steps in flat regions and smaller steps in steep regions. For quadratic loss functions, this can theoretically reach the minimum in a single step.
- **Core assumption:** The local loss landscape between consecutive steps can be approximated as convex/quadratic with monotonic gradient changes.
- **Evidence anchors:**
  - [abstract] "The update algorithm estimates optimal parameters by computing the changes in parameters and gradients between consecutive training steps and leveraging the local curvature of the loss landscape derived from these changes."
  - [Section 2, Pages 3-4] Demonstrates single-step convergence on L=(θ-2)²+2, where θ_{opt}=2 is reached exactly.
  - [corpus] No direct corpus evidence for this specific curvature estimation approach; related work on gradient estimation (PLUMAGE, Stein-Rule) addresses different optimization aspects.
- **Break condition:** When gradient differences Δg become extremely small (near optima or flat regions), Δθ/Δg → ±∞, causing update divergence.

### Mechanism 2
- **Claim:** Switching to Adam when gradient differences fall below a threshold prevents numerical instability while maintaining convergence benefits.
- **Mechanism:** Condition 1 triggers Adam when |Δg| < threshold. This handles two scenarios: (1) approaching minima where gradients naturally decrease, and (2) locally flat regions where gradients remain nearly constant. The threshold (default 0.0005) balances EAGLE's acceleration against stability.
- **Core assumption:** Small gradient differences signal regions where EAGLE's curvature estimation becomes unreliable.
- **Evidence anchors:**
  - [Section 3.1, Page 5-6] "This challenge can be expressed by the following relationship: Δg→0 ∴ Δθ/Δg→±∞"
  - [Section 6.4, Tables 9-11] Shows usage rates decrease from ~50% (epoch 10) to ~2% (epoch 100) on Wine, demonstrating adaptive switching behavior.
  - [corpus] GRADSTOP addresses early stopping but not adaptive optimizer switching; no direct corpus parallel.
- **Break condition:** If threshold is set too low, divergence occurs; if too high, EAGLE's early acceleration is suppressed.

### Mechanism 3
- **Claim:** Switching based on gradient sign patterns prevents inappropriate updates in locally convex (non-concave) loss regions.
- **Mechanism:** Condition 2 triggers Adam when both (g_{n-1}·g_n ≥ 0) AND (g_n·Δg ≥ 0)—indicating gradients are increasing in magnitude, which suggests a locally convex region where zero-gradient points are not minima. This prevents EAGLE from "overshooting" toward local maxima or saddle points.
- **Core assumption:** EAGLE converges correctly only when zero-gradient points correspond to minima (concave regions); convex regions produce incorrect estimates.
- **Evidence anchors:**
  - [Section 3.2, Table 1] Classifies transition patterns; Transitions 2-1 and 2-2 (increasing |g|) are marked "Ineffective" for EAGLE.
  - [Section 3.2, Page 10] "EAGLE update rule is confirmed to be ineffective when the following two conditions are simultaneously satisfied."
  - [corpus] No corpus papers address this specific gradient-sign-based switching logic.
- **Break condition:** False positives (switching to Adam when EAGLE would work) reduce acceleration; false negatives cause inappropriate updates.

## Foundational Learning

- **Concept: Adaptive Learning Rates**
  - Why needed here: EAGLE's core innovation is a curvature-derived adaptive rate (Δθ/Δg); understanding how Adam's β₁, β₂ create per-parameter adaptation provides context for when/why switching occurs.
  - Quick check question: Can you explain why Adam uses both first and second moment estimates, and what EAGLE replaces them with?

- **Concept: Loss Landscape Geometry (Convexity vs. Concavity)**
  - Why needed here: EAGLE's switching mechanism depends entirely on detecting locally convex vs. concave regions through gradient sign patterns.
  - Quick check question: Given gradient values g_{n-1}=+5 and g_n=+8, is the local region convex or concave, and which optimizer should EAGLE select?

- **Concept: Numerical Stability in Optimization**
  - Why needed here: The Δθ/Δg ratio can diverge; understanding division-by-small-numbers issues explains why Condition 1 exists.
  - Quick check question: What happens to Δθ/Δg when training approaches a minimum with g→0, and how does the threshold prevent this?

## Architecture Onboarding

- **Component map:** param_prev, grad_prev -> Δparam, Δgrad -> eagle_update; adam_update; condition_grad_diff: |Δgrad| < threshold -> Adam; condition_landscape: (grad_prev·grad ≥ 0) ∧ (grad·Δgrad ≥ 0) -> Adam; switching_logic: OR of both conditions determines final update rule

- **Critical path:** First step always uses Adam (g_prev=0 satisfies condition 2 trivially—see Section 3.2 proof). From step 2 onward, evaluate both conditions per-parameter before computing updates.

- **Design tradeoffs:**
  - Threshold (0.0005 default): Lower → more EAGLE usage, faster early convergence, higher instability risk. Higher → more Adam-like behavior, stable but slower.
  - Memory overhead: Requires storing previous parameters and gradients (~2× parameter memory vs. Adam alone).
  - Per-parameter vs. per-tensor: Paper applies switching element-wise; global switching would reduce overhead but lose granularity.

- **Failure signatures:**
  - Exploding updates/NaN loss: Threshold too low, Condition 1 not catching small Δgrad early enough.
  - No acceleration vs. Adam: EAGLE usage rate <1% (check via logging); threshold may be too high or loss landscape lacks convex regions (e.g., Transformers—see CIFAR-10 ViT appendix results).
  - Oscillating loss curves: Threshold too low (0.0001 in Iris showed instability—Figure 26).

- **First 3 experiments:**
  1. **Baseline comparison on simple MLP (Iris/Wine equivalent):** Compare EAGLE vs. Adam vs. SGD on a 3-layer network. Log training loss at epochs 2, 4, 6, 8, 10. Expected: 40-75% lower loss at epoch 2 vs. Adam. Verify switching by logging EAGLE usage rate.
  2. **Threshold sensitivity sweep:** Test thresholds [1e-3, 5e-4, 1e-4] on same task. Monitor: early loss (epoch 10), final loss (epoch 100), and EAGLE usage rate. Confirm 0.0005± range provides stability.
  3. **Loss landscape visualization:** After training, sample 20-50 parameters and plot loss vs. parameter value (±5 range around optimum). Classify shapes as convex/oscillatory/flat. Correlate with EAGLE effectiveness per dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the introduction of momentum terms or a forced transition to Adam in later training stages resolve the issue of decreased final convergence performance observed in EAGLE?
- Basis in paper: [explicit] The authors note that EAGLE exhibits "decreased final convergence performance" due to oscillatory loss functions and suggest "introducing momentum terms to suppress oscillations or complete switching to Adam in later learning stages" as a future challenge.
- Why unresolved: The current implementation focuses on early-stage acceleration but lacks mechanisms to handle the cumulative impact of small oscillations near the optimal solution, resulting in higher final loss values compared to Adam or RAdam on some datasets.
- What evidence would resolve it: Experiments demonstrating that a modified EAGLE variant with late-stage switching or momentum achieves comparable or lower final loss values than standard Adam while retaining early acceleration.

### Open Question 2
- Question: How can the gradient difference threshold be dynamically adjusted or automatically set based on dataset and model scale to ensure robustness?
- Basis in paper: [explicit] The paper identifies "mechanisms for dynamically adjusting thresholds as learning progresses or methods for automatically setting thresholds based on model scale" as necessary future work to replace the current fixed default of 0.0005.
- Why unresolved: The current reliance on a static hyperparameter creates a trade-off where excessively low thresholds cause instability (Iris) and high thresholds reduce the optimizer's utility to being nearly identical to Adam (MNIST).
- What evidence would resolve it: A proposed heuristic or adaptive algorithm for the threshold that maintains stable convergence across diverse architectures without requiring manual tuning for each dataset.

### Open Question 3
- Question: Can the memory efficiency of EAGLE be improved to handle large-scale models without sacrificing the individual parameter gradient tracking required for its update rule?
- Basis in paper: [explicit] The authors state that the "current implementation requires calculating gradient variations individually for each parameter, increasing memory usage," and identify examining "implementation methods to improve memory efficiency" as a future challenge.
- Why unresolved: Storing previous gradients and parameters for every element specifically for the EAGLE update rule creates a memory overhead that limits its applicability compared to optimizers like Adafactor or Lion which focus on memory reduction.
- What evidence would resolve it: A modified implementation that reduces the memory footprint (e.g., via approximation or quantization) while maintaining the rapid early convergence properties on large models like Transformers.

### Open Question 4
- Question: Would a switching mechanism based on update magnitude thresholds prevent divergence more effectively than the current method of branching on gradient difference magnitude?
- Basis in paper: [explicit] The paper suggests that "introducing a switching mechanism that prevents update magnitude divergence by setting thresholds for the update magnitude itself needs to be considered" as an alternative to the current gradient-based Condition 1.
- Why unresolved: The current condition triggers based on gradient differences, but the actual risk to training stability is the divergence of the update step size itself (θ → ∓∞); linking the switch directly to update magnitude might be more robust.
- What evidence would resolve it: A comparative analysis showing that update-magnitude thresholds reduce instances of divergence and oscillation more reliably than gradient-difference thresholds on complex, non-convex loss landscapes.

## Limitations
- Decreased final convergence performance compared to Adam on some datasets
- Fixed threshold parameter (0.0005) requires manual tuning and creates stability vs. acceleration trade-offs
- High memory overhead due to storing previous parameters and gradients for every element

## Confidence
- EAGLE early acceleration claims: High
- Mechanism validity for curvature estimation: Medium
- Switching condition effectiveness: Medium
- Final convergence performance claims: Medium

## Next Checks
1. Verify EAGLE achieves 40-75% lower loss than Adam at epoch 2 on simple FC models
2. Test threshold sensitivity to confirm 0.0005 provides optimal stability/acceleration balance
3. Analyze loss landscape shapes to correlate with EAGLE effectiveness per dataset<|end_of_text|><|begin_of_text|>4. Check switching behavior by logging EAGLE usage rate across training epochs
5. Verify first-step behavior uses Adam (g_prev=0 should trigger condition 2)