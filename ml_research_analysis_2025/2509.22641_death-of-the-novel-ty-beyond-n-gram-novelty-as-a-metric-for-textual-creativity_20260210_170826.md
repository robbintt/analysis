---
ver: rpa2
title: 'Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity'
arxiv_id: '2509.22641'
source_url: https://arxiv.org/abs/2509.22641
tags:
- novelty
- creativity
- expressions
- passage
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the limitations of n-gram novelty as a
  metric for textual creativity, revealing that while it correlates with creativity,
  91% of highly novel expressions are not judged creative by expert writers. The research
  operationalizes creativity into novelty, sensicality, and pragmaticality, using
  7,542 expert annotations of human and AI-generated text.
---

# Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity

## Quick Facts
- **arXiv ID:** 2509.22641
- **Source URL:** https://arxiv.org/abs/2509.22641
- **Reference count:** 35
- **Primary result:** 91% of highly novel expressions are not judged creative by expert writers, revealing n-gram novelty's limitations as a creativity metric

## Executive Summary
This study investigates the limitations of n-gram novelty as a metric for textual creativity, revealing that while it correlates with creativity, 91% of highly novel expressions are not judged creative by expert writers. The research operationalizes creativity into novelty, sensicality, and pragmaticality, using 7,542 expert annotations of human and AI-generated text. Results show that higher n-gram novelty in open-source LLMs correlates with lower pragmaticality, suggesting novelty alone is insufficient for creativity. Frontier models produce less creative expressions than humans. LLM-as-a-Judge models can identify creative expressions but struggle with non-pragmatic ones. The study highlights the need for more comprehensive creativity metrics beyond n-gram novelty.

## Method Summary
The study operationalizes creativity into three components: novelty, sensicality, and pragmaticality. Researchers collected 7,542 expert annotations from 26 MFA-level writers evaluating 2,514 unique expressions across 50 New Yorker stories and LLM-generated passages. N-gram novelty was computed using ∞-probability against Dolma (OLMo) and v4 (OLMo-2) corpora via the infini-gram package. Mixed-effects logistic regression models with random intercepts for annotators, passages, and generation sources analyzed the relationship between novelty and creativity. LLM-as-a-Judge models were evaluated using zero-shot, few-shot, and LoRA-finetuned approaches on close reading extraction tasks, with F1 scores measuring performance on identifying novel versus non-pragmatic expressions.

## Key Results
- 91% of top-quartile expressions by n-gram novelty are not judged creative by expert writers
- Higher n-gram novelty in open-source LLMs correlates with lower pragmaticality (β=−0.48, p<0.001 for OLMo-2)
- LLM-as-a-Judge models achieve F1>40 for novel expression detection but F1<20 for non-pragmatic expression detection
- Frontier models (GPT-5, Claude-4.1) produce less creative expressions than humans when evaluated by experts

## Why This Works (Mechanism)

### Mechanism 1: N-gram Novelty Fails as Creativity Proxy
- Claim: N-gram novelty correlates with creativity but 91% of highly novel expressions are not creative
- Mechanism: N-gram novelty measures only statistical rarity in pretraining corpora, missing the "appropriateness" dimension (sensicality and pragmaticality) required by the standard definition of creativity
- Core assumption: Creativity = novelty × appropriateness, not novelty alone (per Runco & Jaeger, 2012)
- Evidence anchors:
  - [abstract]: "~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone"
  - [section 4]: "approximately 24% of creative expressions fall below the mean perplexity, and 7% are in the lowest quartile"
  - [corpus]: "Measuring LLM Novelty As The Frontier Of Original And High-Quality Output" notes "original outputs may be of low quality"
- Break condition: When text is nonsensical or contextually inappropriate despite being n-gram novel (e.g., "feet carved solitary sonnets")

### Mechanism 2: Asymmetric Novelty-Pragmaticality Tradeoff in LLMs vs Humans
- Claim: Higher n-gram novelty in open-source LLMs correlates with lower pragmaticality, but this relationship does not exist for human text
- Mechanism: LLMs optimize for perplexity without implicit pragmatic constraints; pushing toward novelty produces contextually incoherent text while human writers naturally satisfy both
- Core assumption: Statistical optimization objectives do not encode contextual appropriateness as a hard constraint
- Evidence anchors:
  - [abstract]: "higher n-gram novelty in open-source LLMs correlates with lower pragmaticality"
  - [section 4, Figure 3]: "Linear hypothesis tests show no evidence that n-gram novelty affects pragmaticality for human-written text (β=0.01, p=0.94). In contrast... OLMo (β=−0.18, p=0.026) and OLMo-2 (β=−0.48, p<0.001)"
  - [corpus]: Related work limited; corpus papers focus on enhancing creativity, not the novelty-quality tradeoff mechanism
- Break condition: When generation temperature/sampling is tuned for coherence over diversity

### Mechanism 3: LLM-as-Judge Partial Alignment via Statistical Pattern Recognition
- Claim: LLMs identify novel expressions (F1>40) better than non-pragmatic ones (F1<20)
- Mechanism: Novelty detection aligns with statistical pattern recognition from pretraining; pragmaticality detection requires contextual reasoning not captured by next-token prediction objectives
- Core assumption: The training objective favors surface-level novelty recognition over holistic coherence evaluation
- Evidence anchors:
  - [abstract]: "LLM-as-a-Judge models can identify creative expressions but struggle with non-pragmatic ones"
  - [section 5]: "The non-pragmatic expression identification task appears to be significantly harder, with F1 scores below 20, whereas model performance on the novel expression task was above 40"
  - [corpus]: "Beyond Divergent Creativity" notes DAT "focuses on novelty, ignoring appropriateness"—similar limitation in existing frameworks
- Break condition: When evaluating text types or domains underrepresented in training data

## Foundational Learning

- Concept: Mixed-effects (hierarchical) regression modeling
  - Why needed here: The paper uses multilevel models with varying intercepts for annotators, passages, and generation sources to isolate the novelty-creativity relationship from confounders
  - Quick check question: Why would fitting a simple logistic regression without random effects give misleading estimates of the novelty-creativity relationship?

- Concept: Close reading annotation methodology
  - Why needed here: Expert evaluation of creativity requires context-aware analysis, not just surface features; close reading operationalizes this as an annotation protocol
  - Quick check question: How does the pre-highlighting sampling strategy (50% based on novel n-gram percentage) affect what expressions get evaluated?

- Concept: ∞-probability and perplexity as novelty proxies
  - Why needed here: The infini-gram backoff mechanism assigns probability to any expression against trillion-token corpora; understanding this clarifies what "n-gram novelty" actually measures
  - Quick check question: What does high perplexity indicate about an expression's relationship to the pretraining corpus?

## Architecture Onboarding

- Component map:
  Data layer -> Novelty computation (infini-gram package) -> Statistical modeling (mixed-effects logistic regression) -> LLM evaluation (zero-shot, few-shot, LoRA-finetuned) -> Validation (external preference datasets)

- Critical path:
  1. Operationalize creativity → novelty + sensicality + pragmaticality
  2. Collect annotations via close reading interface with inter-rater reliability checks
  3. Fit mixed-effects models controlling for annotator/passage/source variation
  4. Evaluate LLM-as-Judge on extraction tasks (F1 with approximate matching)
  5. Validate on held-out preference datasets

- Design tradeoffs:
  - **Open-source vs. frontier models**: Full n-gram novelty estimation only possible with OLMo/OLMo-2 (known corpora); GPT-5/Claude-4.1 studied exploratorily without corpus access
  - **Expression-level vs. passage-level**: Fine-grained analysis enables causal claims but misses holistic narrative effects
  - **Prevalence imbalance**: Only 7% novel, 9% non-pragmatic expressions—limits statistical power and agreement metric interpretability

- Failure signatures:
  - **Low inter-rater agreement on rare labels**: Free-Marginal Kappa 0.72–0.78; traditional Cohen's κ would fail due to prevalence bias
  - **Non-pragmatic identification collapse**: All models F1<20 on this task—fundamental limitation for LLM-as-Judge
  - **Corpus access requirement**: Cannot compute n-gram novelty for closed models without pretraining data

- First 3 experiments:
  1. **Replicate core finding**: Download dataset from github.com/asaakyan/ngram-creativity (upon release), fit the mixed-effects model in Table 4, verify the 91% non-creative rate for top-quartile novel expressions
  2. **Test LLM-as-Judge generalization**: Apply the few-shot prompt from Figure 6 to a model not in the paper (e.g., Llama-3.1-70B), compute F1 on the test split, compare to reported 40+/20- baseline
  3. **Validate novelty-pragmaticality tradeoff on new domain**: Generate passages with controlled temperature variation, annotate pragmaticality, test whether higher temperature (→ higher novelty) correlates with lower pragmaticality for your target model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does higher n-gram novelty correlate with lower pragmaticality in open-source LLMs but not in human-written text?
- Basis in paper: [explicit] The authors state: "unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality" and report negative slopes for OLMo (β=−0.18, p=0.026) and OLMo-2 (β=−0.48, p<0.001) but no effect for humans (β=0.01, p=0.94).
- Why unresolved: The paper identifies this asymmetry but does not investigate the mechanistic causes—whether it stems from training objectives, architecture, or data distribution.
- What evidence would resolve it: Ablation studies manipulating training objectives or decoding strategies, coupled with analysis of attention patterns when generating novel vs. pragmatic expressions.

### Open Question 2
- Question: Can LLM-as-a-Judge models be improved to reliably identify non-pragmatic expressions?
- Basis in paper: [explicit] The authors report that models "leave room for improvement, especially struggling to identify non-pragmatic expressions" with F1 scores below 20% compared to ~40% for novel expression identification.
- Why unresolved: The paper documents the failure but does not propose interventions; it speculates that the difficulty may stem from LLMs' own tendency to produce non-pragmatic expressions at high novelty.
- What evidence would resolve it: Targeted fine-tuning with contrastive examples of pragmatic vs. non-pragmatic text, or probing whether models lack representations of discourse coherence.

### Open Question 3
- Question: How do expert and crowd preferences diverge regarding non-pragmatic writing, and why?
- Basis in paper: [inferred] Appendix D shows pragmaticality scores significantly predict crowd preferences (p=0.020) but not expert preferences (p=0.832), which the authors suggest "may indicate a misalignment between expert and crowd preferences on non-pragmatic expressions."
- Why unresolved: The study was not designed to systematically compare expert-crowd alignment; the finding emerges from auxiliary analysis with different datasets.
- What evidence would resolve it: A controlled study where the same passages are evaluated by both expert writers and crowd workers on pragmaticality, with qualitative analysis of disagreement patterns.

## Limitations

- Sampling bias and annotation scope: The 50/50 novel/non-novel expression selection strategy may not reflect true text distribution, and reliance on MFA-level experts limits generalizability
- Generalizability constraints: Results based primarily on New Yorker creative nonfiction may not extend to other genres, languages, or text types
- Statistical power limitations: Severe class imbalance (9% non-pragmatic, 7% novel) affects statistical power and interpretability of inter-rater agreement metrics

## Confidence

- **High confidence**: The 91% non-creative rate for highly novel expressions is well-supported by annotation data and mixed-effects modeling
- **Medium confidence**: LLM-as-Judge evaluation shows consistent patterns but F1<20 for non-pragmatic detection suggests fundamental limitations
- **Low confidence**: Comparative creativity assessment of frontier models is exploratory and limited by inability to compute n-gram novelty against unknown pretraining corpora

## Next Checks

1. **Replicate the novelty-pragmaticality tradeoff** on a different text domain (e.g., poetry or technical writing) by generating controlled-temperature outputs and testing whether the asymmetric relationship holds across genres.

2. **Test LLM-as-Judge generalization** using a held-out model (e.g., Llama-3.1-70B) with the same few-shot prompts and compute F1 scores to verify the 40+/20- baseline pattern.

3. **Validate the 91% non-creative rate** by replicating the mixed-effects regression on a newly annotated corpus of creative expressions from different sources, checking whether the novelty-creativity disconnect persists across different text types.