---
ver: rpa2
title: Evaluating the Evaluation of Diversity in Commonsense Generation
arxiv_id: '2506.00514'
source_url: https://arxiv.org/abs/2506.00514
tags:
- diversity
- metrics
- sets
- sentences
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates diversity metrics for commonsense
  generation, revealing that content-based metrics (e.g., VS-Embed, Chamfer) consistently
  outperform form-based metrics (e.g., self-BLEU, Distinct) in capturing meaningful
  diversity. Using an LLM-based annotation approach, the authors found that form-based
  metrics often overestimate diversity, especially for low-quality generations, while
  content-based metrics maintain high accuracy across varying quality levels.
---

# Evaluating the Evaluation of Diversity in Commonsense Generation

## Quick Facts
- arXiv ID: 2506.00514
- Source URL: https://arxiv.org/abs/2506.00514
- Reference count: 24
- Key outcome: Content-based diversity metrics consistently outperform form-based metrics in capturing meaningful diversity for commonsense generation tasks.

## Executive Summary
This study systematically evaluates diversity metrics for commonsense generation, revealing that content-based metrics (e.g., VS-Embed, Chamfer) consistently outperform form-based metrics (e.g., self-BLEU, Distinct) in capturing meaningful diversity. Using an LLM-based annotation approach, the authors found that form-based metrics often overestimate diversity, especially for low-quality generations, while content-based metrics maintain high accuracy across varying quality levels. Experiments on CommonGen, ComVE, and DimonGen datasets demonstrate that content-based metrics achieve up to 80.7% accuracy in pairwise diversity judgments. The study recommends using content-based metrics for reliable diversity evaluation in future commonsense generation research.

## Method Summary
The paper conducts a meta-evaluation of 12 diversity metrics (6 form-based, 6 content-based) for Generative Commonsense Reasoning tasks. Sentence sets are generated using GPT-4-turbo, Llama3.1-8B, and Qwen2.5-14B from CommonGen, ComVE, and DimonGen datasets. GPT-4o annotates diversity using score-based prompting with few-shot examples, achieving 80.6% agreement with human experts. Metrics are evaluated by pairwise accuracy against LLM annotations, with content-based metrics (particularly VS-Embed-0.5) showing superior performance, especially on high-quality generations.

## Key Results
- VS-Embed-0.5 achieves 80.7% accuracy on GPT-4-turbo generations vs. self-BLEU-3 at 48.4%
- Form-based metrics overestimate diversity on low-quality generations (self-BLEU-3 drops to 27.6% accuracy)
- Content-based metrics maintain high accuracy across all quality levels and datasets
- Inter-metric agreement is highest among content-based metrics (VS-Embed variants)

## Why This Works (Mechanism)

### Mechanism 1
Content-based diversity metrics align more closely with human judgments than form-based metrics because they capture semantic rather than lexical variation. Content-based metrics (VS-Embed, Chamfer, self-CosSim) compute pairwise similarity using sentence embeddings (SimCSE), then aggregate into a set-level diversity score. This captures meaning differences that n-gram overlap misses—e.g., paraphrases with different words but identical meaning receive low diversity scores under content-based metrics but high scores under form-based metrics. Core assumption: Sentence embeddings from SimCSE adequately represent semantic content relevant to commonsense diversity judgments. Evidence: VS-Embed-0.5 achieves 80.7% accuracy on GPT-4-turbo generations vs. self-BLEU-3 at 48.4%.

### Mechanism 2
Form-based metrics overestimate diversity for low-quality generations because random lexical variation inflates n-gram differences without semantic significance. Metrics like self-BLEU compute average n-gram overlap across sentence pairs. Randomly shuffled or nonsensical sentences have low n-gram overlap by construction, yielding spuriously high diversity scores—even when humans judge them as non-diverse due to lacking coherent meaning. Core assumption: Human diversity judgments require coherent, commonsense-bearing content; nonsense does not constitute meaningful diversity. Evidence: self-BLEU-3 accuracy drops to 27.6% on low-quality GPT-4-turbo generations (vs. 73.5% on high-quality).

### Mechanism 3
LLM-based annotation provides a scalable proxy for human diversity judgments when properly constrained with few-shot examples and score-based prompting. GPT-4o rates sentence sets on a 1-5 diversity scale using explicit criteria (low redundancy, comprehensive coverage, thematic coherence). Score-based prompting (rather than direct preference selection) avoids ordering sensitivity. Few-shot examples from human-annotated high-agreement pairs calibrate the LLM to human notions of diversity. Core assumption: GPT-4o's internal representation of "diversity" aligns sufficiently with expert human annotators for the task. Evidence: average pairwise accuracy across all annotators was 80.6% between GPT-4o and five linguistically trained humans.

## Foundational Learning

- Concept: **Sentence Embeddings (SimCSE)**
  - Why needed here: All content-based metrics in the paper depend on embedding sentences into a shared vector space where semantic similarity correlates with cosine distance.
  - Quick check question: Can you explain why two sentences with different words but similar meaning would have high cosine similarity in an embedding space?

- Concept: **N-gram Overlap Metrics (BLEU, Distinct)**
  - Why needed here: Form-based diversity metrics like self-BLEU and Distinct-k rely on counting shared n-grams; understanding their limitations is central to the paper's critique.
  - Quick check question: Why would "The dog runs" and "The cat runs" have higher n-gram overlap than "The dog runs" and "A canine sprints"?

- Concept: **Vendi Score (VS)**
  - Why needed here: VS generalizes diversity measurement using eigenvalue entropy of a similarity kernel; it bridges form-based (n-gram kernels) and content-based (embedding kernels) approaches.
  - Quick check question: What does the order parameter q control in the generalized Vendi Score, and why might q=0.5 be preferred for imbalanced datasets?

## Architecture Onboarding

- Component map: Generator LLMs -> Annotator LLM -> Diversity Metrics -> Embedding Model -> Human Validation Layer
- Critical path: 1) Generate candidate sets using controlled perturbations 2) Compute diversity scores using target metrics 3) Obtain LLM annotations with score-based prompting 4) Measure pairwise agreement accuracy between metrics and LLM judgments 5) Validate LLM annotations against human subset
- Design tradeoffs: VS-Embed q-parameter: Lower q (0.5) increases sensitivity to rare elements; higher q (∞) focuses on dominant features. Paper finds q=0.5 performs best for GCR. Temperature for annotator LLM: Temperature=1.0 provides stable ratings; lower temperatures may reduce variance but risk calibration drift. Filtering threshold: Pairs with LLM score difference <0.5 are excluded to ensure meaningful comparisons.
- Failure signatures: Form-based metrics on low-quality data: Accuracy drops below 40%—if you see this pattern, content-based metrics are required. LLM ordering sensitivity: If direct preference selection shows >80% bias toward one position, switch to score-based prompting. High overlap in score distributions: If Default vs. Paraphrase distributions heavily overlap, the metric cannot distinguish meaningful diversity differences.
- First 3 experiments: 1) Reproduce Table 1 on a held-out split: Generate sentences from CommonGen validation set using one generator LLM; compute all 12 metrics; measure accuracy against GPT-4o annotations to verify content-based metric advantage generalizes. 2) Ablate embedding model: Replace SimCSE with multilingual embeddings (e.g., LaBSE) on translated CommonGen examples to test robustness across languages. 3) Quality-diversity joint metric prototype: Combine a quality metric (e.g., SPICE) with VS-Embed-0.5 using weighted product; evaluate whether this reduces spurious high scores on low-quality sets while maintaining discrimination on high-quality sets.

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified evaluation metric be developed that simultaneously incorporates both quality and diversity aspects for GCR, rather than evaluating them separately? Basis: "This observation highlights an important limitation of existing GCR diversity evaluation metrics: diversity should not be evaluated without considering quality. A promising future research direction would be to develop an evaluation metric for GCR that simultaneously incorporates both quality and diversity aspects." Unresolved because current metrics treat quality and diversity as orthogonal; Table 4 shows all metrics assign higher scores to low-quality generations than high-quality ones. Evidence needed: A new metric that penalizes nonsensical outputs while rewarding semantically diverse content, validated against human judgments.

### Open Question 2
Do content-based diversity metrics maintain their advantage over form-based metrics in morphologically rich languages (e.g., Chinese, Japanese)? Basis: "The experiments conducted in this paper were limited to English, a morphologically limited language... It would be important to conduct similar meta-evaluation for the diversity metrics in commonsense generation for other languages." Unresolved because no non-English commonsense reasoning datasets comparable to CommonGen/ComVE/DimonGen currently exist. Evidence needed: Meta-evaluation on translated or newly created GCR datasets in morphologically diverse languages.

### Open Question 3
How do social biases (gender, racial) manifest in diverse LLM generations for commonsense tasks, and do diversity metrics correlate with bias propagation? Basis: "It is important to also evaluate the social biases in the diverse LLM generations before a diversification method for GCR is deployed in an NLG application." Unresolved because the study evaluated quality and diversity but explicitly did not evaluate social biases in generations. Evidence needed: Bias evaluation on generated outputs using established bias benchmarks, correlated with diversity metric scores.

## Limitations
- LLM-based annotation approach may introduce distribution shifts if GPT-4o's diversity judgments differ from broader human populations
- Focus on English-language datasets and SimCSE embeddings limits generalizability to multilingual contexts
- Quality-filtering approach (excluding pairs with <0.5 rating difference) may remove challenging boundary cases

## Confidence

- **High confidence**: Content-based metrics consistently outperform form-based metrics across all tested conditions and datasets, with VS-Embed-0.5 achieving up to 80.7% pairwise accuracy.
- **Medium confidence**: The mechanism explaining form-based metrics' overestimation on low-quality generations is well-supported by experimental evidence but relies on assumptions about human judgment criteria.
- **Medium confidence**: LLM annotation reliability is demonstrated through human validation but may not generalize to all commonsense domains or cultural contexts.

## Next Checks

1. Test VS-Embed-0.5 on a multilingual CommonGen translation to verify robustness across languages, addressing the paper's own limitation about English-specific embeddings.
2. Compare GPT-4o annotations against a second LLM (e.g., Claude-3) on the same calibration subset to assess whether the annotation approach is model-specific or generalizes across LLMs.
3. Evaluate whether combining VS-Embed-0.5 with a quality metric (e.g., SPICE) reduces spurious high diversity scores on low-quality sets while maintaining discrimination on high-quality sets, addressing the paper's suggestion about joint metrics.