---
ver: rpa2
title: 'Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol
  Compliance in Simulation-Based Training'
arxiv_id: '2508.08652'
source_url: https://arxiv.org/abs/2508.08652
tags:
- checklist
- item
- compliance
- context
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Prompt-and-Check, a method using large language
  models (LLMs) to evaluate communication protocol compliance from simulation-based
  training transcripts. The approach combines temporal and semantic context selection
  with structured prompting to assess whether checklist items are fulfilled in maritime
  simulation transcripts.
---

# Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training

## Quick Facts
- arXiv ID: 2508.08652
- Source URL: https://arxiv.org/abs/2508.08652
- Reference count: 12
- Primary result: LLM-based Prompt-and-Check achieves 93.6% weighted accuracy for protocol compliance evaluation in maritime simulation transcripts without fine-tuning

## Executive Summary
This paper presents Prompt-and-Check, a method using large language models (LLMs) to evaluate communication protocol compliance from simulation-based training transcripts. The approach combines temporal and semantic context selection with structured prompting to assess whether checklist items are fulfilled in maritime simulation transcripts. Evaluated on three open-weight models (LLaMA 2 7B, LLaMA 3 8B, Mistral 7B) on a maritime dataset, the method achieved up to 93.6% weighted compliance accuracy. LLaMA 3 8B delivered the best performance with 93.6% accuracy and 1.8/2.0 justification alignment score. The study demonstrates that prompt-based LLMs can effectively support automated, interpretable assessment of protocol adherence in high-stakes training environments without task-specific fine-tuning.

## Method Summary
The Prompt-and-Check framework uses zero-shot LLM inference to classify protocol compliance from maritime simulation transcripts. It employs a two-stage context selection: temporal extraction of utterances within event windows, followed by semantic filtering using MiniLM embeddings (τ=0.7). Structured prompts with JSON schema validation extract {iscompleted, index, evidence} for each checklist item. The method evaluates three open-weight models (LLaMA 2 7B, LLaMA 3 8B, Mistral 7B) on RTX 4070 GPU, measuring weighted checklist compliance accuracy and justification alignment scores.

## Key Results
- LLaMA 3 8B achieved highest weighted compliance accuracy of 93.6%
- Justification alignment score reached 1.8/2.0 for LLaMA 3 8B
- Context selection averaged ~6 utterances per checklist item
- No task-specific fine-tuning required for protocol compliance evaluation

## Why This Works (Mechanism)
The approach leverages LLMs' natural language understanding capabilities for semantic pattern matching between checklist items and transcript content. The two-stage context selection isolates relevant utterances while minimizing noise, and structured prompting with JSON validation ensures consistent, interpretable outputs. The method exploits the inherent reasoning abilities of modern LLMs without requiring domain-specific adaptation.

## Foundational Learning
- **Semantic similarity filtering**: Why needed - isolates relevant transcript portions from protocol checklists; Quick check - verify cosine similarity threshold (0.7) consistently captures relevant utterances
- **Temporal context windowing**: Why needed - focuses analysis on relevant time periods around protocol events; Quick check - confirm ~6 utterance average matches Table IV
- **JSON schema validation**: Why needed - ensures structured, machine-readable LLM outputs; Quick check - test schema compliance rate across all model responses
- **Zero-shot prompting**: Why needed - avoids expensive fine-tuning while maintaining accuracy; Quick check - compare performance against few-shot baselines if available
- **Weighted accuracy metrics**: Why needed - accounts for varying importance of different checklist items; Quick check - verify weight assignments align with expert priorities
- **Justification alignment scoring**: Why needed - provides human-interpretable explanations for model decisions; Quick check - assess inter-rater reliability of Likert scoring

## Architecture Onboarding
- **Component map**: ASR transcript → Temporal extraction → Semantic filtering → Structured prompt → LLM inference → JSON parsing → Classification output
- **Critical path**: Context selection (temporal + semantic) → Structured prompting → JSON validation → Classification decision
- **Design tradeoffs**: Zero-shot vs. fine-tuning (simplicity vs. potential accuracy gains), semantic threshold selection (precision vs. recall), JSON rigidity (consistency vs. flexibility)
- **Failure signatures**: JSON schema violations, context selection too broad/noisy (>15 utterances), justification hallucination citing non-existent utterances
- **3 first experiments**: 1) Test context selection window size optimization, 2) Evaluate different similarity thresholds on context quality, 3) Compare JSON schema compliance rates across models

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the Prompt-and-Check framework perform when applied to safety protocols in domains with significantly different communication patterns, such as healthcare or aviation, without re-calibrating the semantic similarity thresholds?
- Basis in paper: [explicit] The conclusion states, "Future work may explore generalising across different types of protocols," specifically mentioning aviation, emergency response, and healthcare.
- Why unresolved: The current study is a case study limited to the maritime domain using a specific checklist structure; it is unclear if the empirically derived threshold (τ=0.7) and prompt templates transfer effectively to other domains.
- What evidence would resolve it: Evaluation results of the identical model pipeline (LLaMA 3 8B) and thresholds on a benchmark dataset of aviation or healthcare simulation transcripts with expert-annotated compliance labels.

### Open Question 2
- Question: Can the current context selection methodology be extended to incorporate multimodal inputs, such as video feeds or sensor data, to detect non-verbal compliance indicators?
- Basis in paper: [explicit] The conclusion explicitly identifies the need for "incorporating multimodal inputs (e.g., video, sensor data)" as a direction for future work.
- Why unresolved: The current method (Equation 5) relies solely on the function fθ(T, c) where T is text-only transcripts; the architecture does not currently process or fuse visual or sensor time-series data.
- What evidence would resolve it: A modified framework capable of synchronizing transcript data with visual/sensor timestamps and a comparative study showing performance gains (or losses) when adding these modalities.

### Open Question 3
- Question: What are the latency and computational constraints of this approach when adapted for real-time compliance monitoring during live simulation training?
- Basis in paper: [explicit] The paper suggests "extending this method for real-time decision support or debriefing tools" as a future application.
- Why unresolved: The reported results are based on post-hoc analysis of transcripts; the paper does not provide measurements for inference latency or the computational overhead of running the context selection windowing in a streaming environment.
- What evidence would resolve it: Latency benchmarks (ms/transaction) for the LLaMA 3 8B model processing streaming utterances on the target RTX 4070 hardware, specifically measuring the delay between an utterance and the compliance judgment.

### Open Question 4
- Question: How robust is the classification accuracy when the input transcripts contain high Word Error Rates (WER) characteristic of noisy operational environments?
- Basis in paper: [inferred] The paper notes the use of an ASR model [6] for transcription but evaluates compliance on the resulting text; the methodology assumes the input text is reliable despite acknowledging that inputs can be "noisy."
- Why unresolved: The study does not report the WER of the maritime ASR model or test the LLM's performance on synthetic transcripts with injected noise to simulate poor audio conditions.
- What evidence would resolve it: A sensitivity analysis measuring the degradation of the 93.6% compliance accuracy when evaluated against transcripts with artificially inflated WER (e.g., phonetic substitutions, missing tokens).

## Limitations
- Closed-access maritime dataset prevents independent validation and replication
- Temporal buffer values (∆p, ∆f) and semantic threshold (0.7) lack experimental justification
- Manual Likert-scale justification scoring introduces subjective variability
- Cross-domain generalization claims remain speculative without transfer experiments

## Confidence
- **High Confidence**: Structured prompting with JSON schema validation methodology is sound and reproducible
- **Medium Confidence**: Comparative performance ranking across models is likely robust
- **Low Confidence**: Generalization to other domains and elimination of fine-tuning needs are speculative

## Next Checks
1. Submit formal request to authors for maritime transcript dataset or equivalent simulation data with expert annotations to enable independent replication of the 93.6% accuracy benchmark
2. Systematically vary the MiniLM similarity threshold (0.5-0.9) and temporal buffer sizes to quantify their impact on context selection quality and final accuracy
3. Apply the exact methodology to a different protocol-based domain (e.g., aviation crew resource management transcripts) to assess cross-domain transferability