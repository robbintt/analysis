---
ver: rpa2
title: Dual-Domain Fusion for Semi-Supervised Learning
arxiv_id: '2503.11824'
source_url: https://arxiv.org/abs/2503.11824
tags:
- fusion
- training
- data
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Dual-Domain Fusion (DDF), a model-agnostic semi-supervised
  learning framework for time-series signals. DDF addresses the challenge of training
  accurate machine learning models with limited labeled data by combining 1D time-domain
  signals with their 2D time-frequency representations.
---

# Dual-Domain Fusion for Semi-Supervised Learning

## Quick Facts
- **arXiv ID**: 2503.11824
- **Source URL**: https://arxiv.org/abs/2503.11824
- **Reference count**: 40
- **Primary result**: 8-46% accuracy improvement over baseline SSL methods on fault diagnosis datasets

## Executive Summary
This paper introduces Dual-Domain Fusion (DDF), a model-agnostic semi-supervised learning framework for time-series signals. DDF addresses the challenge of training accurate machine learning models with limited labeled data by combining 1D time-domain signals with their 2D time-frequency representations. The method employs a tri-model architecture consisting of time-domain, time-frequency, and fusion components, which enables joint training across domains without increasing inference complexity. The time-frequency model is discarded at test time, maintaining the same inference cost as standard time-domain models. Experimental results on two public fault diagnosis datasets demonstrate substantial accuracy improvements over widely used SSL methods including FixMatch, MixMatch, Mean Teacher, Adversarial Training, and Self-training.

## Method Summary
DDF proposes a dual-domain approach to semi-supervised learning for time-series data by leveraging both time-domain and time-frequency representations. The method employs a tri-model architecture where time-domain and time-frequency models are trained jointly through a fusion module. The time-frequency representation is generated using continuous wavelet transform (CWT), providing complementary information to the raw time-domain signal. During training, all three components (time-domain model, time-frequency model, and fusion module) are optimized together, but at inference time only the time-domain model is used, ensuring no additional computational overhead. The framework is designed to be model-agnostic, allowing integration with various backbone architectures while maintaining the benefits of multi-domain information fusion.

## Key Results
- Achieved 8-46% accuracy improvements over baseline SSL methods including FixMatch, MixMatch, Mean Teacher, Adversarial Training, and Self-training
- Demonstrated effectiveness on two public fault diagnosis datasets
- Maintained inference efficiency by discarding time-frequency model at test time
- Showed model-agnostic compatibility with different backbone architectures

## Why This Works (Mechanism)
The dual-domain approach works by exploiting complementary information from both time-domain and time-frequency representations of time-series data. Time-domain signals capture temporal patterns and dynamics, while time-frequency representations reveal frequency characteristics and transient events that may be invisible in the time domain alone. By training models on both representations simultaneously and fusing their predictions, the method can capture a more complete picture of the underlying signal characteristics. The joint optimization ensures that both models learn complementary features rather than redundant information, while the fusion module learns optimal weightings for combining predictions from different domains.

## Foundational Learning

**Continuous Wavelet Transform (CWT)**
*Why needed*: Converts 1D time-domain signals into 2D time-frequency representations that reveal frequency content across time
*Quick check*: Verify CWT parameters (scales, mother wavelet) are appropriate for the signal characteristics

**Semi-Supervised Learning (SSL)**
*Why needed*: Enables model training with limited labeled data by leveraging large amounts of unlabeled data
*Quick check*: Ensure consistency between pseudo-label generation and model predictions

**Multi-Domain Fusion**
*Why needed*: Combines complementary information from different signal representations to improve learning
*Quick check*: Validate that fusion weights are learned appropriately and not dominated by a single domain

## Architecture Onboarding

**Component map**
Time-domain model -> Fusion module <- Time-frequency model -> Final prediction

**Critical path**
Raw signal -> Time-domain model -> Fusion module -> Final prediction
Raw signal -> CWT transformation -> Time-frequency model -> Fusion module

**Design tradeoffs**
- Training complexity vs. inference efficiency (three models during training, one at inference)
- Choice of time-frequency transformation (CWT vs alternatives) affecting feature quality
- Fusion strategy (weighted averaging vs learned attention) impacting final performance

**Failure signatures**
- Poor performance when time-frequency transformation introduces artifacts
- Overfitting to labeled data when unlabeled data is not properly utilized
- Suboptimal fusion when time-frequency model fails to capture relevant patterns

**First experiments**
1. Compare DDF performance with different time-frequency transformations (CWT, STFT, wavelet packet)
2. Evaluate fusion strategies (weighted averaging, attention-based, gating mechanisms)
3. Test scalability with varying ratios of labeled to unlabeled data

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on quality of time-frequency transformation, which may introduce artifacts or lose information for certain signal types
- Training-time computational overhead is significant as three models are maintained simultaneously, though inference efficiency is preserved
- Generalization to non-fault diagnosis domains or signals with different characteristics (non-periodic, non-stationary) remains untested
- Assumes time-frequency representations consistently enhance learning, which may not hold for all time-series applications

## Confidence
- **High confidence**: The 8-46% accuracy improvements over baseline SSL methods are well-supported by experimental results on two public datasets
- **Medium confidence**: The claim of maintaining inference efficiency by discarding the time-frequency model at test time is valid, but real-world latency impacts are not quantified
- **Medium confidence**: The model-agnostic nature of DDF is demonstrated, but the extent of compatibility with diverse architectures (e.g., transformers, RNNs) is not fully explored

## Next Checks
1. Test DDF on additional time-series domains (e.g., healthcare, finance) to assess cross-domain generalization
2. Quantify the training-time computational overhead (GPU memory, training time) compared to single-domain SSL methods
3. Evaluate the impact of different time-frequency transformations (e.g., STFT, wavelet packet decomposition) on DDF's performance