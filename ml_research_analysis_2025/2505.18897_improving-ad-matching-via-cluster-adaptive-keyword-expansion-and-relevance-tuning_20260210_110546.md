---
ver: rpa2
title: Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance
  tuning
arxiv_id: '2505.18897'
source_url: https://arxiv.org/abs/2505.18897
tags:
- keyword
- relevance
- semantic
- expansion
- keywords
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning

## Quick Facts
- arXiv ID: 2505.18897
- Source URL: https://arxiv.org/abs/2505.18897
- Reference count: 10
- Primary result: Achieved 96% True Positive Rate and 9% CTR lift using cluster-adaptive thresholds and relevance tuning

## Executive Summary
This paper presents a semantic keyword expansion system for search advertising that identifies related keyword variants through dense vector embeddings and nearest neighbor search. The core innovation is a cluster-adaptive thresholding mechanism that adjusts similarity cutoffs based on local semantic density, outperforming global thresholds. The system also incorporates a lightweight incremental learning approach to adapt the relevance model for expanded-keyword items, achieving improved prediction accuracy without destabilizing the base model.

## Method Summary
The method uses a pre-trained siamese neural network to generate dense vector representations of ad keywords, then applies k-means clustering to identify semantically coherent groups. Cluster-adaptive thresholds based on intra-cluster distance distributions filter retrieved candidates, with post-processing filters for gender and numeric consistency. A relevance model combining a frozen GBDT with incrementally trained shallow decision trees adjusts predictions for expanded-keyword items. The system operates on a daily refresh cycle via Airflow, with FAISS flat index supporting scalable nearest neighbor search.

## Key Results
- Achieved 96% True Positive Rate on keyword expansion with 99.9999th percentile cluster thresholds
- Delivered 9% Click-Through Rate lift compared to baseline semantic expansion
- Reduced RMSE on relevance prediction by >4% for lower-quality items through incremental model adjustment

## Why This Works (Mechanism)

### Mechanism 1
Dense vector representations enable identification of semantically related keyword variants even when surface forms differ significantly. A pre-trained siamese neural network encodes keywords into high-dimensional vectors where similar concepts cluster together. Nearest neighbor search via FAISS retrieves semantically proximate variants using cosine similarity. Core assumption: Keywords are typically subsets of buyer queries, so a model trained for query semantic equivalence transfers effectively to keyword expansion tasks.

### Mechanism 2
Cluster-adaptive similarity thresholds outperform global thresholds by accounting for heterogeneous semantic density across the embedding space. K-means partitions embeddings into M clusters. For each cluster, the p-th quantile of intra-cluster distance distribution defines a local threshold τ_m. Denser clusters receive stricter thresholds; larger, ambiguous clusters receive more permissive thresholds. Core assumption: Semantic density varies meaningfully across clusters, and local threshold adaptation preserves precision without sacrificing recall.

### Mechanism 3
Stacking incrementally trained shallow decision trees onto a frozen GBDT relevance model improves prediction accuracy for expanded-keyword items without destabilizing the base model. Base model f_GBDT outputs relevance scores. An additional 1-2 trees (depth ≤5) trained on human-judged expanded-keyword data produce adjustments g_i(x). Final score: f_adj(x) = f_GBDT(x) + Σg_i(x). Core assumption: Residual errors from the base model are structured enough for shallow trees to correct; the expanded-keyword distribution differs sufficiently to warrant specialized adjustment.

## Foundational Learning

- **Siamese neural networks with contrastive learning**: Understanding how the embedding model encodes semantic equivalence between queries/keywords. *Quick check: Can you explain why contrastive loss encourages similar items to cluster and dissimilar items to separate in embedding space?*

- **Approximate nearest neighbor (ANN) search with FAISS**: The system relies on scalable retrieval of semantically similar keywords at production latency. *Quick check: What is the tradeoff between a flat index (exact search) and an IVF/HNSW index (approximate) in terms of latency versus recall?*

- **Incremental learning with Gradient Boosted Decision Trees**: The relevance model adds shallow trees to adjust predictions without retraining the full ensemble. *Quick check: Why does adding trees to a GBDT risk overfitting, and how does limiting tree depth mitigate this?*

## Architecture Onboarding

- **Component map**: Embedding Service → Clustering Module → FAISS Index → Threshold Filter → Item Retrieval → Relevance Scorer → Market Threshold Gate → Match Output
- **Critical path**: New keyword → embedding → cluster assignment → FAISS retrieval → threshold filter → item retrieval → relevance scoring (base + adjustment) → market threshold → match output
- **Design tradeoffs**: 
  - Quantile p selection: Higher p → more permissive expansion (recall up, precision down)
  - Index type: Flat index ensures accuracy; IVF/HNSW reduces latency at cost of recall
  - Incremental vs full retrain: Incremental is faster and stable; full retrain may capture more signal but risks instability
- **Failure signatures**: 
  - Gender/numeric mismatches (e.g., "men's shoes" → "women's sandals") — mitigated by post-processing filters
  - Relevance score drift for expanded-keyword items — detect via RMSE monitoring on held-out human judgments
  - Cluster assignment instability after corpus growth — trigger re-clustering when WCSS exceeds threshold
- **First 3 experiments**:
  1. Threshold quantile sweep: Evaluate TPR and impression lift at p ∈ {95%, 99%, 99.9%, 99.999%} with and without post-processing filters
  2. Relevance model ablation: Compare base GBDT vs. stacked model on held-out expanded-keyword items using RMSE per relevance tier
  3. Online A/B test: Deploy embedding-only vs. embedding+cluster threshold vs. full pipeline (including relevance adjustment); measure CTR, impression lift, BI/click

## Open Questions the Paper Calls Out

### Open Question 1
Can similarity thresholds be adapted in real-time using user interaction data to improve upon the static cluster-based quantiles? The conclusion identifies "real-time adaptive thresholds based on user interactions" as a necessary future step. The current implementation relies on offline, static p-th quantile cutoffs calculated during periodic retraining. A/B tests showing that dynamic thresholds react faster to semantic drift and improve CTR compared to the static baseline would resolve this.

### Open Question 2
How does incorporating query-side signals like click feedback impact the precision of the semantic expansion model? The authors list "incorporating query-side signals (e.g., click feedback)" as a primary avenue for future work. The present system performs document-side expansion only, ignoring user behavior signals during the embedding generation phase. Offline evaluation of expansion quality using human judgment on sets generated with versus without query-side features would resolve this.

### Open Question 3
To what extent does personalized keyword expansion increase relevance and conversion rates compared to the global expansion approach? The conclusion suggests "personalized keyword expansion" to further boost scalability and relevance. The current pipeline generates a uniform set of expanded keywords for all users, treating the inventory globally. User-level metrics showing statistically significant lifts in conversion rates when expansion candidates are filtered by user profile would resolve this.

## Limitations

- Cluster-adaptive thresholding lacks external validation beyond internal offline metrics and may not generalize across different e-commerce contexts
- Human-labeled relevance dataset size and distribution remain unspecified, creating uncertainty about model generalization
- Gender/numeric consistency filters are described as "lightweight" but lack implementation details that would affect reproducibility

## Confidence

- **High confidence**: The fundamental mechanism of using dense embeddings for semantic keyword expansion is well-established and technically sound. The incremental GBDT stacking approach is a reasonable adaptation technique with clear implementation path.
- **Medium confidence**: The cluster-adaptive thresholding mechanism shows theoretical promise and internal validation, but lacks external validation or comparison to simpler alternatives (global thresholds with post-filtering).
- **Low confidence**: The gender/numeric filter effectiveness and the specific threshold tuning methodology (99.9999th percentile) appear to be based on eBay's internal corpus characteristics without validation that these parameters generalize to other e-commerce contexts.

## Next Checks

1. **External replication test**: Apply the complete pipeline to a public e-commerce dataset (e.g., Amazon product titles/queries) and compare TPR and CTR metrics against a baseline using global thresholding. This would validate whether cluster-adaptive thresholds provide consistent benefits across different marketplaces.

2. **Ablation study**: Conduct controlled experiments comparing four variants: (a) semantic expansion only, (b) expansion + global threshold, (c) expansion + cluster-adaptive threshold, and (d) full pipeline with relevance adjustment. This would isolate the contribution of each component to the claimed performance gains.

3. **Threshold sensitivity analysis**: Systematically vary the quantile parameter p across a broader range (95% to 99.999%) and measure the precision-recall tradeoff curve with and without post-processing filters. This would determine whether the 99.9999th percentile is optimal or merely conservative for eBay's specific corpus characteristics.