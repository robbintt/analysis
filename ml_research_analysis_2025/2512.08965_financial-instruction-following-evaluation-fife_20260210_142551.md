---
ver: rpa2
title: Financial Instruction Following Evaluation (FIFE)
arxiv_id: '2512.08965'
source_url: https://arxiv.org/abs/2512.08965
tags:
- evaluation
- arxiv
- available
- instructions
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIFE introduces a high-difficulty instruction-following benchmark
  for finance with 88 human-authored prompts that test complex, interdependent constraints
  like table schemas, conditional formatting, and domain-specific data normalization.
  The evaluation uses a strict/loose scoring system with regex-based verification
  to ensure compliance.
---

# Financial Instruction Following Evaluation (FIFE)

## Quick Facts
- arXiv ID: 2512.08965
- Source URL: https://arxiv.org/abs/2512.08965
- Reference count: 40
- Key outcome: FIFE introduces 88 human-authored financial prompts testing complex interdependent constraints, with Llama-4 Maverick 18B achieving 76.1% strict compliance (surpassing proprietary systems at 65.9%)

## Executive Summary
FIFE is a benchmark for evaluating instruction-following capabilities in financial applications, featuring 88 human-authored prompts that test complex, interdependent constraints like table schemas, conditional formatting, and domain-specific data normalization. The evaluation uses a strict/loose scoring system with regex-based verification to ensure compliance. Across 53 models, the top open-weight model achieved 76.1% strict compliance, surpassing the leading proprietary system at 65.9%, while the best open-source model scored 45.5%. Even top models struggled with procedural formatting errors rather than factual inaccuracies, highlighting the benchmark's difficulty.

## Method Summary
FIFE evaluates LLMs using 88 financial prompts with chainable, regex-based constraints that verify specific compliance requirements like table schemas, word limits, and formatting styles. Models generate responses via zero-shot inference, and a verification system applies strict (exact match) and loose (normalized) modes to assess compliance. The benchmark categorizes prompts by composition type (single, and, chained, nested) to stress-test interdependent constraint handling. Evaluation is performed on 53 models across proprietary, open-weight, and open-source categories, with results aggregated by composition type and model family.

## Key Results
- Llama-4 Maverick 18B (76.1% strict / 79.5% loose) outperformed the top proprietary system (65.9% strict / 70.5% loose)
- Open-source models achieved 45.5% strict compliance, with significant room for improvement
- Failures predominantly stemmed from procedural formatting issues rather than semantic errors
- The strict-loose score gap (2.4-5.6 percentage points across top models) revealed widespread "near misses" where content was correct but formatting violated constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regex-based constraint verification enables automated, reproducible evaluation of instruction-following compliance.
- Mechanism: A registry of 88 chainable constraints maps instruction identifiers to checker classes. Each checker validates specific constraint types (cardinality, table schema, ordering, style, word limits). The evaluation engine applies strict/loose modes—loose mode normalizes minor formatting artifacts (whitespace, capitalization) before verification.
- Core assumption: Regex patterns can adequately capture compliance without semantic understanding; minor formatting deviations are recoverable via normalization.
- Evidence anchors: [abstract] "verification system with chainable, verifiable constraints for fine-grained reward signals"; [section 3.2] "evaluation relies on regex-based validators, while transparent and reproducible"

### Mechanism 2
- Claim: Composition type diversity (single/and/chained/nested) stress-tests interdependent constraint handling.
- Mechanism: Prompts are categorized by composition: single (one instruction), and (concurrent instructions), chained (sequential dependencies), nested (conditional logic). This forces models to track multiple constraint types simultaneously, with failures often occurring when earlier constraint violations cascade into later non-compliance.
- Core assumption: Financial workflows genuinely require this composition complexity; the 88 SME-curated tasks accurately represent real-world scenarios.
- Evidence anchors: [section 3.1] "four composition types: single (a single instruction), and (multiple concurrent instructions), chained (sequential instructions), and nested (conditional instructions)"; [section 4] "failures often stem from an inability to adhere to precise formatting or structural requirements"

### Mechanism 3
- Claim: The strict-loose score gap diagnoses procedural vs. semantic failure modes.
- Mechanism: Strict mode requires exact pattern compliance; loose mode normalizes outputs. The gap between scores (e.g., 76.1 strict vs. 79.5 loose for top model) reveals "near misses"—semantically correct but procedurally flawed outputs. This diagnostic guides targeted interventions (formatting training vs. reasoning training).
- Core assumption: Normalization rules correctly identify recoverable vs. unrecoverable errors.
- Evidence anchors: [abstract] "top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose)"; [section 4] "These 'near misses,' where semantic content is correct but a specific constraint is violated, are reflected in the significant gap between 'strict' and 'loose' scores"

## Foundational Learning

- Concept: **Regex pattern construction and escaping**
  - Why needed here: Building checkers requires precise regex for table schemas, delimiters, and formatting cues. The paper notes brittleness when minor deviations (extra spaces, wrong HTML tags) cause verification failures.
  - Quick check question: Can you write a regex that matches `<font color="red">` but rejects `<span style="color:red">` while allowing optional whitespace?

- Concept: **Instruction composition semantics (and/chained/nested)**
  - Why needed here: Understanding how constraints combine determines whether failure is localized or cascading. Nested conditional instructions (e.g., "if X appears, do Y") require tracking state across the output.
  - Quick check question: Given three instructions (bold header, 3 bullets, word limit <100), which composition type applies and what's the failure propagation?

- Concept: **Strict vs. loose evaluation trade-offs**
  - Why needed here: The scoring mode determines what counts as compliance. Strict mode enforces exactness (critical for regulatory outputs); loose mode forgives cosmetic deviations (useful for exploratory tasks).
  - Quick check question: If a model outputs `**Confidence: Low.**` with an extra space before the period, which scoring mode passes and why?

## Architecture Onboarding

- Component map:
  - `instructions_registry` -> Maps 88 instruction IDs to checker classes (registry pattern)
  - `evaluation_lib` -> Orchestrates evaluation, requests checkers from registry (factory pattern)
  - `finance_instructions` -> Concrete checker implementations inheriting from base class
  - `instructions_util` -> Shared text-processing utilities (word count, table detection, sentence splitting)
  - `generate_responses.py` -> Multi-provider gateway for model inference
  - `evaluation_bin.py` -> CLI entry point

- Critical path:
  1. `build_input_jsonl` → assembles prompts from registry
  2. `generate_responses` → calls external models via API
  3. `evaluation_bin` → loads prompts + responses
  4. `evaluation_lib` → for each prompt, fetches checker from registry
  5. Checker validates → outputs pass/fail JSONL

- Design tradeoffs:
  - Regex vs. LM-as-judge: Regex is reproducible and fast but brittle; the paper notes future work could explore adaptive evaluators
  - Single-sample evaluation: Cost constraints limited robustness against stochasticity; multi-sample inference would improve reliability
  - Text-only scope: Deferring multimodal evaluation limits applicability to chart/table rendering tasks

- Failure signatures:
  - "Near miss" pattern: Semantic content correct but constraint violated (e.g., `<font>` instead of `<span>`)
  - Cascade failure: Early constraint violation (wrong table headers) prevents downstream checkers from running
  - False negative: Regex rejects semantically valid output due to minor formatting (extra space, capitalization)

- First 3 experiments:
  1. Run evaluation on 3 models (one proprietary, one open-weight, one open-source) comparing strict vs. loose scores; analyze the gap distribution across composition types
  2. Introduce controlled perturbations (extra whitespace, tag substitutions) to 10 outputs; measure which normalization rules recover compliance
  3. Build a new checker for a domain-specific constraint (e.g., ISIN format validation); integrate into registry and validate against synthetic outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal models achieve comparable instruction-following compliance on financial tasks requiring interpretation of charts, tables, and document images?
- Basis in paper: [explicit] Section 3.2 states: "we evaluate a wide range of text-only LMs, deferring multimodal evaluation to future work."
- Why unresolved: The current benchmark only tests text-only models, leaving untested whether vision-language models can handle the complex, interdependent constraints when visual elements are involved.
- What evidence would resolve it: Extending FIFE with multimodal prompts and evaluating leading vision-language models using the same strict/loose scoring protocol.

### Open Question 2
- Question: Does multi-sample inference significantly reduce variance in compliance scores compared to single-response evaluation?
- Basis in paper: [explicit] Section 5 acknowledges: "relying on a single response per prompt due to API cost limits robustness against model stochasticity." Appendix D adds: "future research can investigate multi-sample inference techniques to mitigate stochastic model variation."
- Why unresolved: Current results may conflate model capability with single-sample randomness, particularly for borderline "near-miss" failures.
- What evidence would resolve it: Re-evaluating a subset of models with n>1 samples per prompt and comparing score distributions against single-sample baselines.

### Open Question 3
- Question: Why do reasoning-oriented models underperform on precise instruction following despite their multi-step capabilities?
- Basis in paper: [inferred] Section 4 notes that "several models designed for complex reasoning performed weakly" and "current reasoning-oriented tuning does not necessarily translate to more precise instruction following."
- Why unresolved: The paper identifies the phenomenon but does not investigate whether the cause is training objective mismatch, attention distribution issues, or architectural factors.
- What evidence would resolve it: Ablation studies comparing reasoning-specialized variants against base models, with attention analysis on instruction-critical tokens.

### Open Question 4
- Question: Can adaptive evaluators (e.g., LM-as-a-judge) improve evaluation fidelity over regex-based verification without sacrificing reproducibility?
- Basis in paper: [explicit] Section 3.2 notes regex validators "can be brittle" and "future work could explore more adaptive evaluators." Appendix D mentions "LM-as-a-judge or hybrid semantic checks" as promising.
- Why unresolved: Regex brittleness causes false negatives on semantically correct outputs, but adaptive evaluators introduce their own variability and potential bias.
- What evidence would resolve it: Comparative evaluation of regex vs. LM-as-a-judge on a human-annotated gold standard subset of FIFE responses.

## Limitations
- Regex brittleness causes false negatives on semantically correct outputs due to minor formatting variations (e.g., `<font>` vs `<span>` tags)
- Single-sample evaluation introduces uncertainty about score stability across model generations
- Text-only scope limits applicability to multimodal financial tasks requiring chart/table rendering

## Confidence
- High confidence: The relative performance ranking of models (Llama-4 Maverick 18B > proprietary systems > open-source) is well-supported by the strict/loose score comparisons and consistent across composition types
- Medium confidence: The claim that composition diversity accurately reflects real-world financial workflows depends on the representativeness of SME-curated tasks, which is asserted but not independently validated
- Low confidence: The assertion that regex-based verification "enables automated, reproducible evaluation" may be overstated given the documented brittleness and potential for false negatives due to minor formatting variations

## Next Checks
1. **Robustness validation**: Generate 50 perturbed outputs per model (varying whitespace, HTML tags, capitalization) and measure how many "near misses" become strict-mode passes. This quantifies the practical impact of loose-mode normalization and identifies whether regex patterns need refinement.

2. **Cross-task generalizability**: Apply FIFE's constraint registry to 20 non-financial instruction-following tasks (e.g., legal document formatting, medical report generation). Measure whether the same checker classes generalize or require domain-specific modifications.

3. **Multi-sample stability**: For the top 5 models, generate 5 responses per prompt and compute score variance. If strict-mode scores vary by >5%, the single-sample evaluation may not reliably rank models, suggesting a need for ensemble scoring.