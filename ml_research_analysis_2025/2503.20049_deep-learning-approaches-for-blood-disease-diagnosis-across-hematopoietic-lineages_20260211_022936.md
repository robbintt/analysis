---
ver: rpa2
title: Deep Learning Approaches for Blood Disease Diagnosis Across Hematopoietic Lineages
arxiv_id: '2503.20049'
source_url: https://arxiv.org/abs/2503.20049
tags:
- classification
- disease
- hematopoietic
- cell
- cells
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a foundation modeling framework using deep learning
  to uncover latent genetic signatures across hematopoietic lineages. The approach
  trains a fully connected autoencoder on multipotent progenitor cells, compressing
  over 20,000 gene features into a 256-dimensional latent space.
---

# Deep Learning Approaches for Blood Disease Diagnosis Across Hematopoietic Lineages

## Quick Facts
- arXiv ID: 2503.20049
- Source URL: https://arxiv.org/abs/2503.20049
- Reference count: 4
- Primary result: >95% accuracy for multi-class blood disease classification using progenitor cell embeddings

## Executive Summary
This work presents a foundation modeling framework using deep learning to uncover latent genetic signatures across hematopoietic lineages. The approach trains a fully connected autoencoder on multipotent progenitor cells, compressing over 20,000 gene features into a 256-dimensional latent space. This embedding captures predictive information applicable to both progenitor and downstream differentiated cells like monocytes and lymphocytes. The study validates these embeddings by training feed-forward, transformer, and graph convolutional networks for blood disease diagnosis tasks.

## Method Summary
The method trains a fully-connected autoencoder on multipotent progenitor cells to compress ~29,150 gene features into a 256-dimensional latent space via reconstruction loss. These embeddings are then used to train feed-forward, transformer, and graph convolutional networks for blood disease classification. The framework enables zero-shot prediction, where a classifier trained on progenitor embeddings can classify disease states in downstream differentiated cells (monocytes, lymphocytes) without retraining.

## Key Results
- Multi-class classification achieves >95% accuracy on progenitor cells
- Zero-shot prediction on monocytes achieves >0.7 F1-score on binary classification
- Lymphocyte classification remains challenging at ~64% accuracy, requiring further refinement

## Why This Works (Mechanism)

### Mechanism 1
Autoencoder compression preserves cross-lineage predictive information. A fully-connected autoencoder trained on multipotent progenitor cells learns to compress ~29,150 gene features into a 256-dimensional latent space by minimizing reconstruction error. This bottleneck forces the model to retain maximally informative features that generalize to downstream differentiated cells.

### Mechanism 2
Zero-shot prediction leverages shared latent structure across hematopoietic hierarchy. A classifier trained exclusively on progenitor cell embeddings can classify disease states in monocytes and lymphocytes without retraining, because the autoencoder embeddings encode lineage-invariant features.

### Mechanism 3
Architecture choice modulates which embedding features are exploited for classification. Feed-forward networks capture non-linear decision boundaries; self-attention models learn global feature-to-feature relationships; GCNs aggregate neighbor cell features via cosine-similarity graphs. Each architecture extracts different signal types from the same embeddings.

## Foundational Learning

- **Autoencoders and Latent Space Learning**: Why needed here - the entire framework depends on understanding how reconstruction loss enforces useful compression, and why bottleneck dimensions force the model to learn transferable representations. Quick check: Can you explain why minimizing reconstruction error might produce features useful for a task the autoencoder was never explicitly trained for?

- **Multi-head Self-Attention Mechanisms**: Why needed here - the transformer classifier treats the 256-dimensional embedding as a sequence, using attention to capture feature-to-feature relationships; understanding Q/K/V computation is essential for debugging or modifying this component. Quick check: In Equation 3, why does the softmax operate on QK^T / √D, and what would happen if you removed the √D scaling?

- **Graph Convolutional Networks and Neighborhood Aggregation**: Why needed here - the GCN approach constructs a cell similarity graph and propagates information; understanding adjacency normalization and message passing is required to interpret why GCNs underperformed on progenitor data due to memory constraints. Quick check: In Equation 5, what is the role of D̂^(-1/2) terms, and how would removing them change the aggregation behavior?

## Architecture Onboarding

- **Component map**: Input Layer (29,150 gene expression values) -> Autoencoder Encoder (BatchNorm + ReLU layers) -> 256-dim latent vector z -> Classification Heads (FFN/Transformer/GCN) -> Output (Softmax for 7-class or sigmoid for binary)

- **Critical path**: 1) Load single-cell gene expression data; filter to target cell types 2) Train autoencoder on progenitor cells only 3) Encode all cell types using the trained encoder 4) Train classifier on progenitor embeddings; evaluate zero-shot on monocyte/lymphocyte embeddings 5) Optionally fine-tune classifiers on downstream cell embeddings

- **Design tradeoffs**: Latent dimension (256) balances information preservation vs overfitting; dropout (0.1-0.2) prevents overfitting; attention heads (4) balance performance vs training time; GCN edge thresholds (0.4) manage memory vs relationship capture

- **Failure signatures**: Lymphocyte underperformance (63-65% accuracy) indicates progenitor embeddings don't capture lymphocyte-specific patterns; GCN memory failure on large progenitor dataset (>199K cells) requires applying GCN only to smaller downstream datasets; zero dropout causing 100% train / 94% test gap indicates classic overfitting

- **First 3 experiments**: 1) Train FFN classifier on progenitor embeddings with dropout=0.1; verify >95% test accuracy 2) Freeze progenitor-trained classifier and evaluate F1-score directly on monocyte/lymphocyte embeddings 3) Train transformer and GCN classifiers on progenitor embeddings; compare test accuracy to FFN baseline

## Open Questions the Paper Calls Out

### Open Question 1
How can the autoencoder's latent space be refined to achieve clinical-grade robustness in lymphocyte disease classification? The abstract and conclusion explicitly state that future work should improve embeddings further to increase robustness on lymphocyte classification specifically.

### Open Question 2
Why does the zero-shot transfer model outperform models trained directly on lymphocyte data? The text notes the surprising result that zero-shot classification matched, and even surpassed, models trained directly on lymphocyte embeddings.

### Open Question 3
Can Graph Convolutional Networks (GCNs) be adapted to scale to the full progenitor cell dataset? The paper states in a footnote that GCN could not train on the large progenitor cell data, limiting its use to downstream cell types only.

## Limitations
- Exact autoencoder architecture (number and width of intermediate layers) is unspecified
- Claim that progenitor genetic signatures persist through differentiation lacks direct lineage tracing evidence
- Zero-shot performance differences between monocytes (0.74 F1) and lymphocytes (0.64 F1) suggest framework limitations

## Confidence

- **High confidence**: Multi-class classification accuracy (>95% on progenitor cells) - well-supported by ablation studies and standard validation
- **Medium confidence**: Zero-shot transfer to monocytes (>0.7 F1) - demonstrated but mechanism not fully explained
- **Low confidence**: Zero-shot transfer to lymphocytes (~0.64 F1) - significantly worse performance suggests limitations in the approach

## Next Checks

1. **Architecture sensitivity**: Train the autoencoder with varying latent dimensions (128, 256, 512) and measure downstream classification accuracy to confirm 256 is optimal

2. **Transfer mechanism**: Compare zero-shot performance when classifier is trained on monocytes vs. progenitors for monocyte classification to test whether lineage proximity matters

3. **Lymphocyte refinement**: Train a hybrid model that fine-tunes the progenitor autoencoder on lymphocyte data and measure classification improvement vs. pure zero-shot approach