---
ver: rpa2
title: 'GRASP: Municipal Budget AI Chatbots for Enhancing Civic Engagement'
arxiv_id: '2503.23299'
source_url: https://arxiv.org/abs/2503.23299
tags:
- budget
- information
- grasp
- chatbot
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRASP is an AI chatbot framework for answering municipal budget
  queries. It combines Retrieval-Augmented Generation (RAG) and ReAct agent workflows
  with prompt engineering and domain knowledge to produce accurate, grounded responses.
---

# GRASP: Municipal Budget AI Chatbots for Enhancing Civic Engagement

## Quick Facts
- arXiv ID: 2503.23299
- Source URL: https://arxiv.org/abs/2503.23299
- Reference count: 14
- Primary result: AI chatbot framework for municipal budget queries using RAG and ReAct workflows

## Executive Summary
GRASP is an AI chatbot framework designed to answer municipal budget queries through a combination of Retrieval-Augmented Generation (RAG) and ReAct agent workflows. The system retrieves information from official budget documents and uses iterative refinement to produce accurate, grounded responses. In testing, GRASP achieved 78% accuracy in answering budget-related questions, significantly outperforming baseline models like GPT-4o (60%) and Gemini (35%). This approach aims to improve public understanding of municipal budgets and enhance transparency and civic engagement.

## Method Summary
GRASP combines RAG and ReAct agent workflows with prompt engineering and domain knowledge to answer municipal budget queries. The framework retrieves information from official budget documents and iteratively refines answers using an agentic workflow. This approach leverages both information retrieval from structured documents and reasoning capabilities to provide accurate responses to complex budget-related questions.

## Key Results
- GRASP achieved 78% accuracy in answering municipal budget queries
- Outperformed GPT-4o (60% accuracy) and Gemini (35% accuracy)
- Demonstrated effectiveness in enhancing public understanding of municipal budgets

## Why This Works (Mechanism)
Assumption: The combination of RAG and ReAct workflows enables GRASP to ground responses in official documents while iteratively refining answers through reasoning steps. This dual approach addresses both factual accuracy (through retrieval) and logical coherence (through reasoning), resulting in more reliable budget information for citizens.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG)**: Combines information retrieval with text generation to ground responses in source documents. Why needed: Ensures factual accuracy by pulling directly from official budget documents rather than relying solely on model training data. Quick check: Verify retrieved passages directly support the generated response.
- **ReAct Agent Workflows**: Integrates reasoning and action-taking in a single framework. Why needed: Enables iterative refinement of answers through multiple reasoning steps. Quick check: Confirm each iteration improves response quality or moves toward the correct answer.
- **Prompt Engineering**: Crafting specific instructions to guide model behavior. Why needed: Directs the model to focus on budget-specific terminology and formatting requirements. Quick check: Test prompts with varied phrasing to ensure consistent performance.

## Architecture Onboarding

Component Map:
Document Retriever -> Context Selector -> ReAct Agent -> Response Generator

Critical Path:
The critical path flows from document retrieval through iterative refinement: the system retrieves relevant budget passages, selects appropriate context, processes through the ReAct agent for reasoning and action, and generates the final response. Each step must complete successfully for accurate output.

Design Tradeoffs:
- Accuracy vs. Response Time: The iterative ReAct workflow improves accuracy but increases latency
- Document Coverage vs. Precision: Broader retrieval may capture more relevant information but introduces noise
- Model Complexity vs. Interpretability: More sophisticated reasoning improves performance but reduces transparency

Failure Signatures:
- Retrieval failures: Missing or irrelevant documents lead to inaccurate responses
- Context selection errors: Choosing wrong passages despite relevant ones being available
- ReAct agent getting stuck in loops: Iterative refinement fails to converge
- Generation hallucinations: Fabricated information not supported by retrieved documents

3 First Experiments:
1. Test retrieval accuracy with budget documents of varying formats and structures
2. Evaluate iterative refinement effectiveness by comparing single-step vs. multi-step responses
3. Assess response quality when critical budget information is missing from documents

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, which is a limitation in understanding the research's broader implications and future directions.

## Limitations
- 22% error rate leaves room for improvement in public-facing applications
- Performance depends on quality and consistency of municipal budget documentation
- Computational costs and latency implications of multi-step workflows not addressed
- No discussion of handling missing information or contradictory data across sources
- Unknown: No mention of how the system handles non-standard budget formats or documents with poor OCR quality

## Confidence

High confidence:
- The technical architecture combining RAG with ReAct workflows is sound and follows established patterns in the field

Medium confidence:
- The comparative performance claims against GPT-4o and Gemini are likely accurate given the methodology described
- The potential for enhancing civic engagement through improved budget transparency is plausible but not empirically validated in this study

## Next Checks
1. Conduct field testing with actual municipal residents to evaluate real-world query accuracy and user satisfaction across diverse demographic groups
2. Test GRASP's performance across multiple municipalities with varying budget document structures and update frequencies to assess generalizability
3. Perform error analysis on the 22% of responses that were not accurate to identify systematic failure modes and their root causes