---
ver: rpa2
title: Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain
  Contexts
arxiv_id: '2601.15479'
source_url: https://arxiv.org/abs/2601.15479
tags:
- ficl
- causal
- cause
- effect
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study benchmarks 13 open-source LLMs on pairwise causal discovery
  (PCD) across 12 diverse biomedical and multi-domain datasets. Models were evaluated
  on two core tasks: detecting if text contains causal links and extracting exact
  cause-effect spans, tested across explicit/implicit markers and single/multiple
  sentence scenarios.'
---

# Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts

## Quick Facts
- **arXiv ID:** 2601.15479
- **Source URL:** https://arxiv.org/abs/2601.15479
- **Reference count:** 40
- **Primary result:** Best model for detection (DeepSeek-R1-Distill-Llama-70B) achieved 49.57% accuracy; best for extraction (Qwen2.5-Coder-32B-Instruct) reached 47.12%.

## Executive Summary
This study benchmarks 13 open-source LLMs on pairwise causal discovery (PCD) across 12 diverse biomedical and multi-domain datasets. Models were evaluated on two core tasks: detecting if text contains causal links and extracting exact cause-effect spans, tested across explicit/implicit markers and single/multiple sentence scenarios. Using varied prompting strategies (zero-shot to advanced Chain-of-Thought and Few-shot In-Context Learning), the best model for detection (DeepSeek-R1-Distill-Llama-70B) achieved 49.57% accuracy, while the best for extraction (Qwen2.5-Coder-32B-Instruct) reached 47.12%. Performance dropped sharply for implicit, inter-sentential, and multi-pair cases. The results show current models struggle with nuanced causal reasoning, especially in realistic biomedical text, highlighting the need for targeted fine-tuning on domain-specific causal corpora.

## Method Summary
The study evaluated 13 open-source LLMs (3B–70B) on 12 datasets spanning biomedical, financial, and news domains. The evaluation used two tasks: detection (binary classification of causal text) and extraction (identifying exact cause/effect spans). Models were prompted using six strategies: zero-shot, Few-shot In-Context Learning (FICL), Chain-of-Thought (CoT), Hybrid CoT+FICL, Least-to-Most (LtM), and ReAct. Detection used F1-score, while extraction used a probabilistic scoring system based on SentenceLM cosine similarity bands and Hungarian matching for multiple pairs. Hyperparameters included seed=4000, temperature=0.7, top_k=0.7, and batch_size=64.

## Key Results
- DeepSeek-R1-Distill-Llama-70B achieved the highest detection accuracy at 49.57%, while Qwen2.5-Coder-32B-Instruct achieved the highest extraction F1 at 47.12%.
- Performance plummeted for implicit relationships, inter-sentential cases, and multi-pair extractions, with detection accuracy falling below 34% in these scenarios.
- Models frequently hallucinated data or missed relations entirely in complex biomedical texts, indicating significant limitations in nuanced causal reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs primarily leverage surface-level pattern matching of explicit causal markers, rather than deep semantic reasoning, to perform pairwise causal discovery (PCD).
- **Mechanism:** The models’ pre-training on vast corpora creates strong associations between lexical cues (e.g., "causes," "due to") and the presence of causality. When such markers are present (M=1), models activate these associations to make predictions, bypassing more resource-intensive discourse integration or world-knowledge inference.
- **Core assumption:** This behavior stems from a statistical shortcut learned during pre-training, where marker presence is a reliable but imperfect proxy for causality.
- **Evidence anchors:**
  - [abstract] "Performance plummeted for more difficult (and realistic) cases, such as implicit relationships."
  - [section, Analysis and Discussion] "Models’ struggles on tasks 2c and 2g... highlight a significant bottleneck in their ability to synthesize information from dense research articles."
  - [corpus] Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse (arXiv:2510.13417) finds LLMs often fail on implicit causal chains, supporting the reliance on explicit cues.
- **Break condition:** This mechanism fails when texts contain causal markers without true causality (e.g., "The cause is unknown") or when causality is implicit. The paper’s Task 1a false class (sentences with markers but no causality) was designed to exploit this weakness.

### Mechanism 2
- **Claim:** Decompositional prompting strategies (e.g., Least-to-Most) mitigate extraction failures by reducing the complexity of the output generation task.
- **Mechanism:** These strategies break the extraction problem into a sequence of simpler sub-tasks (e.g., first extract cause, then extract effect). This reduces the cognitive load on the model’s generation process, decreases the chance of format errors, and improves the likelihood of producing valid, parseable outputs for each sub-task.
- **Core assumption:** The performance gain from decomposition is primarily a function of reducing task complexity for the language model, not from inducing deeper reasoning.
- **Evidence anchors:**
  - [section, Prompting Strategies] Describes Least-to-Most (LtM) as "decomposes the task into a sequence of simpler sub-problems."
  - [Table VI, Experiment 2A] Shows L2M prompting often yields higher F1 scores for extraction compared to other strategies (e.g., for Qwen2.5-Coder-32B-I, L2M F1=0.5514 vs. GIP F1=0.4555).
  - [corpus] Evidence is limited in the provided corpus; related work on prompting (e.g., Chain-of-Thought) is common, but specific studies on decomposition for PCD are not prominent in the neighbors.
- **Break condition:** This mechanism's benefits may diminish when the sub-tasks themselves are highly ambiguous or when the model fails at the first decomposition step (e.g., cannot identify any cause span), cascading into total failure.

### Mechanism 3
- **Claim:** A trade-off exists between a model’s competence in causal detection (binary decision) and causal extraction (span generation), influenced by architectural traits and prompting.
- **Mechanism:** Models optimized for reasoning (e.g., DeepSeek-R1-Distill-Llama-70B) excel at the binary detection task which aligns with their training to produce a final answer. However, their verbose reasoning traces can consume output token budgets, leaving insufficient space for the full JSON extraction output, leading to truncated or missing results. Conversely, models with strong instruction-following for structured output (e.g., Qwen2.5-Coder-32B-I) balance detection and extraction better.
- **Core assumption:** The trade-off is partly an artifact of fixed output token limits interacting with model-specific generation patterns (e.g., reasoning vs. direct answering).
- **Evidence anchors:**
  - [Figure 2 & section, Results] "The DS-R1D-L70B model demonstrates the highest average detection performance (49.57%)... In contrast, the Qwen models are the top performers in terms of average extraction performance (47.12%)."
  - [section, Results] "For deepseek models, missing values were the highest in our error analysis... typically, due to deepseek reasoning taking extra tokens... it shuts off before giving a correct answer."
  - [corpus] Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine (arXiv:2505.16982) envisions integrating causal reasoning, implicitly acknowledging current LLMs' fragmented capabilities.
- **Break condition:** This trade-off is not absolute. Models with larger output windows, better instruction tuning for brevity, or fine-tuning on the specific output format could mitigate the issue, potentially allowing a single model to excel at both tasks.

## Foundational Learning

- **Concept:** **Causality Taxonomy (Explicit/Implicit, Intra-/Inter-sentential)**
  - **Why needed here:** The paper’s entire experimental design is built on this taxonomy. Understanding it is prerequisite to interpreting the results, which show models perform radically differently across these categories.
  - **Quick check question:** Given the sentence "The patient was given a sedative. He fell asleep quickly," is this an explicit or implicit, intra- or inter-sentential causal relationship? (Answer: Implicit and Inter-sentential).

- **Concept:** **Token Budget Awareness in LLM Deployment**
  - **Why needed here:** The paper identifies output token limits as a key factor in the detection/extraction trade-off, especially for reasoning models. Engineers must consider this when selecting models and designing prompts.
  - **Quick check question:** A reasoning model is tasked with extracting 5 causal pairs from a long text into a structured JSON format. What is a primary risk, and how might you mitigate it? (Answer: Risk of output truncation. Mitigation: use a model with a larger output window, prompt for concise reasoning, or break the text into chunks).

- **Concept:** **Error Taxonomy in PCD (Hallucination, Swapped Pairs, Missing Relations)**
  - **Why needed here:** The paper’s error analysis (Table 7) categorizes failures. Knowing these categories helps in diagnosing model weaknesses and guiding targeted improvements (e.g., data augmentation for recall, prompt engineering for directionality).
  - **Quick check question:** Your model frequently outputs "Effect: [cause text], Cause: [effect text]". Which error category is this, and what might be a solution? (Answer: Switched Cause-Effect (CE). Solution: Few-shot examples that explicitly label cause and effect, or a prompt step asking the model to verify directionality).

## Architecture Onboarding

- **Component map:** Data Layer (12 datasets normalized into 12 experimental tasks) -> Prompting Layer (5 strategies: GIP, FICL, CoT, L2M, ReAct) -> Model Layer (13 open-source LLMs) -> Evaluation Layer (Binary scoring for detection; SentenceLM + Hungarian matching for extraction).

- **Critical path:** 1) Data Prep: Curate sentences, apply annotation protocol (κ≥0.758). 2) Experiment Run: For each model-prompt-task combination, generate predictions. 3) Scoring: Calculate F1, accuracy, and competence metrics (D, X). 4) Analysis: Perform statistical tests (Friedman, t-tests) and error bucketization.

- **Design tradeoffs:**
  - **Model choice vs. task fit:** Reasoning models (DeepSeek-R1) for detection; instruction-following/code models (Qwen-Coder) for extraction. No single model wins across all tasks.
  - **Prompt complexity vs. reliability:** Simple prompts (GIP) are reliable but low-performance. Complex prompts (CoT+FICL) boost performance but may introduce formatting errors and variance.
  - **Approximate vs. exact evaluation:** Exact string matching is too strict. The adopted SentenceLM-based approximate matching is more robust but requires calibration.

- **Failure signatures:**
  - **High missing value rate:** Indicates model is running out of token budget (common in DeepSeek models) or prompt is failing to elicit structured output.
  - **High "Switched CE" errors:** Model struggles with causal direction. Prompt needs directionality emphasis.
  - **High "Hallucinated data" errors:** Model is over-generating. Prompt needs negative constraints or Few-shot examples of non-causal text.

- **First 3 experiments:**
  1.  **Baseline Detection (Task 1a):** Run all models with zero-shot (GIP) on explicit, intra-sentential data. This establishes a lower-bound and reveals which models can follow basic causal detection instructions.
  2.  **Extraction Trade-off Test:** Select the top detection model (e.g., DS-R1D-L70B) and top extraction model (e.g., Qwen2.5-C-32B-I). Run both on Task 2a (simple extraction) and Task 2h (complex, implicit, multi-pair extraction). Quantify the performance gap to understand the trade-off magnitude.
  3.  **Prompting Ablation:** For a mid-tier model (e.g., Mistral-7B-I), compare FICL and L2M prompting on Task 2f (implicit, intra-sentential, multi-pair). This isolates the impact of decomposition vs. in-context examples on a challenging extraction task, directly testing Mechanism 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does targeted fine-tuning on domain-specific causal corpora (e.g., Electronic Health Records) significantly improve LLM performance on implicit and inter-sentential causal discovery tasks compared to prompt-based approaches?
- **Basis in paper:** [explicit] The authors state in the Conclusion that "future work must prioritize fine-tuning on domain-specific causal corpora" to bridge the performance gap on complex clinical narratives.
- **Why unresolved:** Current models rely heavily on explicit markers; performance plummets on implicit, realistic biomedical text (e.g., $C_{detect}$ dropping below 34% for implicit inter-sentential tasks).
- **What evidence would resolve it:** A comparative study benchmarking fine-tuned models against the prompt-based results reported (e.g., improving upon DeepSeek-R1-Distill-Llama-70B's 49.57% detection accuracy).

### Open Question 2
- **Question:** How does the integration of medical knowledge graphs affect the extraction accuracy of multi-pair causal relations in dense biomedical literature?
- **Basis in paper:** [explicit] The Conclusion suggests "integrating medical knowledge graphs" as a necessary step to build models capable of supporting clinical decisions.
- **Why unresolved:** Models currently struggle with multi-pair extraction, often hallucinating data or missing relations entirely; it is unclear if structured knowledge retrieval can mitigate these reasoning failures.
- **What evidence would resolve it:** Experiments showing that KG-enhanced models outperform the top extraction model (Qwen2.5-Coder-32B-Instruct) specifically on the multi-pair tasks (2e-2h).

### Open Question 3
- **Question:** Do state-of-the-art proprietary models (e.g., GPT-4, Gemini) outperform the best open-source models (like DeepSeek-R1-Distill-Llama-70B) on the specific failure cases of implicit causal discovery?
- **Basis in paper:** [inferred] The Limitations section notes the scope was restricted to open-source models, implying the performance of proprietary systems on this specific benchmark remains unknown.
- **Why unresolved:** It is uncertain if the ~50% accuracy ceiling observed is a limitation of the current open-source model landscape or a fundamental limitation of LLM causal reasoning capabilities.
- **What evidence would resolve it:** Applying the paper's unified evaluation framework to proprietary models and comparing their $C_{detect}$ and $C_{extract}$ metrics against Table 5.

## Limitations
- **Model Coverage Gap:** Only 13 open-source LLMs were evaluated, primarily dense architectures with a few MoE variants. The absence of proprietary models (e.g., GPT-4, Claude) leaves a performance ceiling unknown, and the results may not generalize to frontier models with different training objectives.
- **Annotation and Evaluation Subjectivity:** While inter-annotator agreement was high (κ≥0.758), the re-annotation process introduces a new layer of subjectivity. Furthermore, the SentenceLM-based approximate matching for extraction, while more robust than exact matching, still relies on a probabilistic calibration map that may not be universally applicable across domains or embedding models.
- **Prompt Sensitivity:** The study tested six prompting strategies, but the impact of prompt phrasing, example selection (for Few-shot), and the interaction between prompt and model-specific quirks (e.g., DeepSeek's token budget issues) were not exhaustively explored. Performance could vary significantly with minor prompt adjustments.

## Confidence
- **High Confidence:** The observation that model performance degrades significantly on implicit, inter-sentential, and multi-pair causal relationships is well-supported by the experimental results across all 12 tasks.
- **Medium Confidence:** The identification of DeepSeek-R1-Distill-Llama-70B as the best detection model and Qwen2.5-Coder-32B-Instruct as the best extraction model is robust for the tested models and prompts, but the specific ranking might shift with a different model pool or prompting strategy.
- **Low Confidence:** The claim that the trade-off between detection and extraction competence is primarily an artifact of output token limits interacting with model generation patterns is plausible but not definitively proven. Alternative explanations, such as fundamental architectural differences in how models approach classification vs. generation tasks, are not ruled out.

## Next Checks
1. **Proprietary Model Benchmark:** Evaluate a small set of top proprietary LLMs (e.g., GPT-4, Claude-3) on the same 12 tasks to establish a performance upper bound and validate if the observed trade-offs and failure modes persist at the frontier of LLM capabilities.
2. **Fine-tuning on Causal Corpora:** Select the top-performing open-source models and fine-tune them on a curated corpus of domain-specific causal text (e.g., biomedical literature with explicit causality annotations). Re-run the benchmark to measure the impact of targeted domain adaptation on both detection and extraction tasks, particularly for implicit and inter-sentential relationships.
3. **Output Format Robustness Test:** For the best detection and extraction models, systematically vary the output format (e.g., JSON, plain text, structured lists) and token limits in the prompts. Measure the impact on missing values and hallucination rates to isolate whether the observed issues are due to model architecture or prompt-output mismatch.