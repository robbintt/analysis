---
ver: rpa2
title: 'From superposition to sparse codes: interpretable representations in neural
  networks'
arxiv_id: '2503.01824'
source_url: https://arxiv.org/abs/2503.01824
tags:
- neural
- sparse
- arxiv
- representations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a theoretical framework for understanding
  how neural networks represent information through superposition, where multiple
  concepts are linearly combined in a shared representational space. The authors propose
  a three-step approach: (1) identifiability theory shows that neural networks trained
  for classification recover latent features up to a linear transformation, (2) sparse
  coding methods can extract disentangled features from these representations by leveraging
  compressed sensing principles, and (3) quantitative interpretability metrics can
  assess the success of these methods.'
---

# From superposition to sparse codes: interpretable representations in neural networks

## Quick Facts
- **arXiv ID:** 2503.01824
- **Source URL:** https://arxiv.org/abs/2503.01824
- **Reference count:** 40
- **Primary result:** Theoretical framework showing neural networks learn linear superpositions of latent features, enabling sparse code recovery via compressed sensing

## Executive Summary
This paper presents a theoretical framework for understanding how neural networks represent information through superposition, where multiple concepts are linearly combined in a shared representational space. The authors propose a three-step approach: (1) identifiability theory shows that neural networks trained for classification recover latent features up to a linear transformation, (2) sparse coding methods can extract disentangled features from these representations by leveraging compressed sensing principles, and (3) quantitative interpretability metrics can assess the success of these methods. The framework bridges insights from theoretical neuroscience, representation learning, and interpretability research, providing implications for both neural coding theories and AI transparency.

## Method Summary
The framework proposes verifying additivity in neural representations by testing whether the representation of combined concepts (f(A ∧ B)) approximates the sum of individual representations (f(A) + f(B)). This involves generating synthetic images via text-to-image models, extracting embeddings from a pre-trained vision transformer (ViT-B/16), and computing cosine similarity between combined and summed representations. The theoretical backbone combines identifiability theory (showing neural networks recover latent features up to linear transformation) with compressed sensing principles (proving sparse code recovery is possible under specific dimensionality and sparsity conditions). Sparse coding algorithms, including standard and gated Sparse Autoencoders, are then applied to extract interpretable features from the superposed representations.

## Key Results
- Neural representations exhibit additivity: combined concepts' representations approximate the sum of individual concept representations (c.s. = 0.92 in validation examples)
- Identifiability theory proves neural networks recover latent features up to a linear transformation when trained for classification tasks
- Compressed sensing principles guarantee sparse code recovery when dimensionality and sparsity conditions are met (M > O(K log(N/K)))

## Why This Works (Mechanism)

### Mechanism 1: Linear Representation Emergence via Identifiability
- Claim: Neural networks trained on classification tasks learn to invert nonlinear generative processes up to a linear transformation.
- Mechanism: When a continuous encoder f minimizes cross-entropy with a linear classifier W under the assumed data-generating process (latent variables z → nonlinear generative function → data), the composition h = f∘g becomes linear. This transforms an intractable nonlinear ICA problem into a tractable linear one.
- Core assumption: The data arises from latent variables z passed through an invertible generative function g, and all z dimensions are task-relevant.
- Evidence anchors: [abstract] "Identifiability theory shows that neural networks trained for classification recover latent features up to a linear transformation." [section: Step one] Theorem 1 (Reizinger et al., 2024) proves h = f∘g is a linear map from S^d-1 to R^d under specified assumptions.
- Break condition: If latent variables have non-Euclidean topology (e.g., object pose on curved manifolds), or if dimensions of z and y are mismatched (M ≠ N), the linearity guarantee may hold only approximately.

### Mechanism 2: Sparse Code Recovery via Compressed Sensing
- Claim: Sparse coding can uniquely recover latent variables from superposed neural representations when sparsity and dimensionality conditions are met.
- Mechanism: If y = Φz where z ∈ R^N is K-sparse and y ∈ R^M with M < N, compressed sensing theory guarantees unique recovery (up to permutation) when M > O(K log(N/K)). Sparse coding solves for both the dictionary Θ and codes ẑ by minimizing reconstruction error plus L1 sparsity penalty.
- Core assumption: True latent variables z are sparse (at most K active components per sample) and the measurement matrix Φ satisfies restricted isometry properties.
- Evidence anchors: [abstract] "Sparse coding methods can extract disentangled features from these representations by leveraging principles from compressed sensing." [section: Step two] Equation 7 provides the recovery bound; Figure 4 visualizes invertible regions for different embedding sizes.
- Break condition: Standard SAEs with linear-nonlinear encoders provably fail to achieve optimal recovery (O'Neill et al., 2025). Non-sparse inputs or dictionary learning convergence to spurious local minima also break recovery.

### Mechanism 3: Additivity Enables Concept Decomposition
- Claim: Neural representations of combined concepts approximate the sum of individual concept representations.
- Mechanism: The composition f∘g exhibits additivity: f∘g(z₁ + z₂) ≈ f∘g(z₁) + f∘g(z₂). This implies linear directions in representation space correspond to interpretable concepts (e.g., a "pink" direction independent of context), enabling analogy-making and concept arithmetic.
- Core assumption: The generative function g and representation function f compose to approximate a linear map.
- Evidence anchors: [abstract] "A key finding is that neural representations exhibit additivity—the representation of combined concepts approximates the sum of individual representations." [section: Mathematical world model] Figure 1 shows empirical validation: ViT-B/16 representations of "elephant" + "pink ball" - "ball" ≈ "pink elephant" (c.s. = 0.92).
- Break condition: Additivity is demonstrated locally on specific examples; the paper notes it may not hold universally. The binding problem (how relational information is preserved under summation) remains unsolved.

## Foundational Learning

- Concept: **Compressed Sensing Theory**
  - Why needed here: Provides the theoretical guarantee that sparse signals can be recovered from fewer measurements than dimensions. Understanding the M > O(K log(N/K)) bound is essential for determining whether a given representation dimension can support recovery of the hypothesized number of sparse features.
  - Quick check question: Given a 768-dimensional neural representation (CLIP) and assuming 100 active concepts per input, what is the approximate maximum number of total concepts that could be uniquely recovered?

- Concept: **Sparse Coding / Dictionary Learning**
  - Why needed here: This is the practical algorithm for recovering sparse codes from superposed representations. Understanding the two-step process (sparse inference + dictionary update) and the L1 penalty's role is critical for implementing or debugging SAEs.
  - Quick check question: Why does the L1 norm promote sparsity more effectively than L2 for the sparse inference step? What is the "amortization gap" in SAE encoders?

- Concept: **Independent Component Analysis (ICA)**
  - Why needed here: The paper situates its theory within the nonlinear ICA framework. Understanding how ICA recovers sources up to permutation/scaling helps contextualize why identifiability theory matters and what "up to linear transformation" means.
  - Quick check question: In ICA, why can we not recover the original sources exactly, and what indeterminacies remain acceptable for interpretability purposes?

## Architecture Onboarding

- Component map:
  Input layer (neural activations y ∈ R^M) -> Sparse coding module (SAE or iterative sparse inference) -> Dictionary (Θ ∈ R^(M×N)) -> Reconstruction decoder -> Evaluation layer (interpretability metrics)

- Critical path:
  1. Collect neural activations from target model layer across diverse inputs
  2. Train sparse coding dictionary (or SAE) to minimize reconstruction loss + sparsity penalty
  3. Extract sparse codes for analysis; map each dictionary element to interpretable concepts via activation maximization or top-activating samples
  4. Validate interpretability using permutation-invariant metrics

- Design tradeoffs:
  - SAE encoder complexity vs. recovery quality: Simple linear-ReLU encoders are fast but provably suboptimal for recovery; MLP encoders improve recovery but increase compute (O'Neill et al., 2025)
  - Expansion factor (N/M): Higher expansion captures more features but requires more data and compute; Figure 4 provides theoretical bounds
  - Sparsity penalty λ: Higher λ increases sparsity but risks feature suppression (Tibshirani, 1996 shrinkage); gated SAEs (Rajamanoharan et al., 2024) address this

- Failure signatures:
  - Dead features: Dictionary elements that never activate—typically indicates learning rate too high or L1 penalty too strong
  - Feature splitting: Multiple dictionary elements encoding near-identical concepts—suggests expansion factor set too high
  - Poor reconstruction: SAE fails to reconstruct activations—dictionary may be underparameterized or training insufficient
  - Inconsistent features across runs: Different SAE training runs produce different feature sets—O'Neill et al. show this may reflect suboptimal recovery rather than true feature variability

- First 3 experiments:
  1. Additivity verification: Before applying sparse coding, test whether target model exhibits additivity using the Figure 1 protocol (cosine similarity between combined and summed representations vs. calibrated baseline).
  2. Compressed sensing bound check: Estimate typical sparsity K (active features per sample) using an initial SAE; verify M > O(K log(N/K)) holds for your chosen expansion factor.
  3. Recovery quality baseline: On synthetic data with known sparse structure, compare SAE recovery accuracy against iterative sparse inference (Equation 8) to quantify the amortization gap for your architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can neural representations solve the "binding problem" to encode relational structures (e.g., "blue triangle and red square") without losing information to simple additivity?
- Basis in paper: [explicit] The Conclusion identifies the binding problem as a key challenge, noting that summing feature vectors results in identical representations for distinct relational pairings.
- Why unresolved: The superposition principle relies on linear additivity, which creates commutativity (A+B = B+A), thereby failing to bind specific attributes to specific objects.
- What evidence would resolve it: A theoretical framework or mechanism demonstrating how neural networks can encode object-attribute bindings non-commutatively within superposition.

### Open Question 2
- Question: What sparse coding architectures can close the "amortization gap" to achieve the optimal recovery rates predicted by compressed sensing theory?
- Basis in paper: [explicit] Section 5.2 asks what algorithms can scale to millions of features, noting that standard Sparse Autoencoders (SAEs) provably fail to achieve optimal recovery.
- Why unresolved: SAEs utilize linear-nonlinear encoders that lack the expressivity required to fully invert the generative process, leaving a performance gap compared to theoretical bounds.
- What evidence would resolve it: Large-scale sparse coding models (e.g., SAEs with MLP encoders) that successfully recover latent variables with the statistical efficiency guaranteed by compressed sensing.

### Open Question 3
- Question: How can quantitative interpretability metrics be modified to account for the context-dependency of human similarity judgments?
- Basis in paper: [explicit] Section 5.3 states that the dependence of human judgments on context motivates "future research into contextualized similarity metrics."
- Why unresolved: Current metrics rely on static, context-free pairwise distances, whereas human interpretation of a concept shifts based on the surrounding reference set.
- What evidence would resolve it: A new metric that correlates strongly with human behavioral tasks by dynamically adjusting for the semantic context provided by a set of inputs.

## Limitations
- The identifiability theory relies on specific assumptions about data generation (continuous latent variables, invertible generative functions) that may not hold in real-world settings
- Compressed sensing recovery guarantees assume sparsity and restricted isometry properties that may be violated in complex neural representations
- Additivity experiments demonstrate the hypothesis on specific examples but lack systematic testing across diverse model architectures and tasks
- The framework does not address the binding problem of how relational information is preserved under linear summation

## Confidence
- **High confidence:** The compressed sensing theory foundations and sparse coding methodology are well-established with rigorous mathematical guarantees.
- **Medium confidence:** The identifiability theory for neural networks is novel and theoretically sound but depends on assumptions that require empirical validation in real-world settings.
- **Low confidence:** The additivity hypothesis, while supported by compelling examples, lacks systematic testing across diverse model architectures and tasks, and the claim that neural networks encode features linearly "despite their nonlinear architectures" may be overstated.

## Next Checks
1. Systematic additivity testing: Conduct controlled experiments across multiple model architectures (transformers, MLPs, CNNs) and diverse concept pairs to quantify when and where additivity breaks down, particularly for relational concepts and complex scene compositions.
2. Sparsity verification: Measure actual sparsity distributions in learned representations across different model layers and tasks to validate the compressed sensing assumptions, and test recovery performance when sparsity assumptions are violated.
3. Identifiability robustness: Test the identifiability theory under realistic conditions where not all latent dimensions are task-relevant, where latent variables have non-Euclidean structure, or where the generative function is not perfectly invertible.