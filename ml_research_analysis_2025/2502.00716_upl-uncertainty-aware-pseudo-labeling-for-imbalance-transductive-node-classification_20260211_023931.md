---
ver: rpa2
title: 'UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node Classification'
arxiv_id: '2502.00716'
source_url: https://arxiv.org/abs/2502.00716
tags:
- node
- imbalance
- classification
- graph
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of imbalanced node classification
  in graph-structured data by proposing a novel uncertainty-aware pseudo-labeling
  (UPL) algorithm. The authors first establish a theoretical upper bound on population
  risk for imbalanced transductive node classification, which depends on the maximum
  degree of nodes per class and the minimum degree of all nodes.
---

# UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node Classification

## Quick Facts
- arXiv ID: 2502.00716
- Source URL: https://arxiv.org/abs/2502.00716
- Reference count: 0
- Primary result: UPL achieves superior balanced accuracy and F1-scores on imbalanced node classification benchmarks compared to state-of-the-art methods

## Executive Summary
This paper addresses the challenge of imbalanced node classification in graph-structured data by proposing a novel uncertainty-aware pseudo-labeling (UPL) algorithm. The authors first establish a theoretical upper bound on population risk for imbalanced transductive node classification, which depends on the maximum degree of nodes per class and the minimum degree of all nodes. They then introduce UPL, an iterative algorithm that mitigates imbalance by selectively assigning pseudo-labels to unlabeled minority class nodes using an uncertainty-aware approach. UPL employs a selective edge removal method to estimate uncertainty, balancing node selection across a broader confidence range to improve model performance. Comprehensive experiments on various benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that UPL consistently outperforms state-of-the-art methods, achieving superior balanced accuracy and F1-scores. The approach is also shown to be computationally efficient compared to similar algorithms.

## Method Summary
UPL is an iterative algorithm that addresses imbalanced transductive node classification through uncertainty-aware pseudo-labeling. The method begins by training a base GNN (e.g., GCN) with BalancedSoftmax loss to handle class imbalance. It then estimates uncertainty for each unlabeled node using Selective Edge Removal (SER), which involves perturbing the graph structure by randomly removing edges (with probability proportional to node degree) and measuring prediction variance across 100 perturbations. Nodes with consistent predictions (low entropy variance) despite structural changes are considered reliable. UPL applies a "band-pass" confidence filter, selecting pseudo-labels only for minority class nodes whose predicted probabilities fall between thresholds ηl and ηu, and whose uncertainty is below a threshold Qα. This process iterates for 10 rounds, with pseudo-labels added to the training set and the model retrained each iteration.

## Key Results
- UPL achieves superior balanced accuracy and F1-scores compared to state-of-the-art methods on benchmark datasets
- The approach demonstrates effectiveness on both homophilic (Cora, CiteSeer, PubMed) and heterophilic graphs (Chameleon, Squirrel, Wisconsin)
- UPL shows computational efficiency compared to ensemble-based uncertainty estimation methods

## Why This Works (Mechanism)

### Mechanism 1: Minority Class Augmentation via Risk Bound Optimization
- **Claim:** The algorithm posits that increasing the effective sample size of the minority class reduces the generalization error, as defined by a derived upper bound on population risk.
- **Mechanism:** The authors derive an upper bound for imbalanced transductive node classification (Theorem 4.6) which depends on the maximum degree per class and the minimum degree of the graph. The bound suggests that the population risk is dominated by the scarcity of minority class samples (m_i). UPL selectively assigns pseudo-labels only to unlabeled minority class nodes to strictly increase m_i, thereby tightening the error bound.
- **Core assumption:** The derived theoretical bound accurately reflects the empirical generalization gap, and pseudo-labels serve as effective proxies for ground truth labels in the risk calculation.
- **Evidence anchors:**
  - [abstract]: "The authors first establish a theoretical upper bound on population risk... which depends on the maximum degree of nodes per class."
  - [section 4]: "Theorem 4.6 reveals that the population risk upper bound is fundamentally dominated by the number of samples in the minority class."
  - [corpus]: Weak direct support; standard imbalanced learning literature focuses on re-sampling, but this specific degree-dependent bound derivation is novel to this paper.
- **Break condition:** If pseudo-labels for the minority class have high error rates (noise), the augmentation will increase empirical risk rather than reducing the population risk bound.

### Mechanism 2: Topology-Aware Uncertainty Estimation via Selective Edge Removal (SER)
- **Claim:** Perturbing the graph structure allows for a more robust estimation of prediction uncertainty than standard softmax probabilities alone.
- **Mechanism:** To estimate uncertainty without an ensemble of models, UPL uses Selective Edge Removal (SER). It randomly removes edges (with probability proportional to node degree) to create perturbed graph views. It then measures the variance of entropy across predictions on these perturbations. Nodes with consistent predictions (low entropy variance) despite structural changes are considered "certain" and reliable for labeling.
- **Core assumption:** Aleatoric and epistemic uncertainty can be approximated by measuring prediction sensitivity to local neighborhood perturbations.
- **Evidence anchors:**
  - [abstract]: "UPL employs a selective edge removal method to estimate uncertainty."
  - [section 5.1]: "Nodes showing consistent predictions (low entropy variance) across perturbations are considered more reliable... offering a computationally efficient way to improve pseudo-label quality."
  - [corpus]: "Graph-Based Uncertainty-Aware Self-Training" (Neighbor) addresses over-confidence, supporting the need for better uncertainty metrics, though UPL's edge-removal approach is distinct.
- **Break condition:** On graphs where the semantic meaning is strictly tied to the full topology (e.g., specific molecular bonds), removing edges might destroy the feature, leading to high entropy variance even for "certain" nodes.

### Mechanism 3: Confidence Band-Pass Filtering
- **Claim:** Selecting pseudo-labels based on an intermediate confidence range (a "band-pass") rather than a simple high-confidence threshold mitigates confirmation bias and distribution shift.
- **Mechanism:** Standard methods select nodes with probability > η. UPL introduces a lower ηl and an upper ηu threshold. It ignores very high-confidence nodes (which are often redundant or biased) and very low-confidence nodes (noise), focusing on a "sweet spot" of informative minority candidates.
- **Core assumption:** Highly confident predictions in an imbalanced setting often represent biased, redundant information that reinforces the majority class distribution.
- **Evidence anchors:**
  - [section 5.1]: "To avoid overfitting to these high-confidence samples, we introduce an upper threshold, ηu, which selects samples with intermediate confidence levels."
  - [section 2]: Mentions prior work (Liu et al., 2022) observing that "over-reliance on highly certain nodes risks a distribution shift."
  - [corpus]: "Semi-Supervised Regression with Heteroscedastic Pseudo-Labels" (Neighbor) discusses filtering strategies, validating the difficulty of threshold selection.
- **Break condition:** If the imbalance is extreme (e.g., 1:100), the "intermediate" confidence band may contain almost zero minority class samples, causing the selection mechanism to fail.

## Foundational Learning

- **Concept: Transductive Learning**
  - **Why needed here:** Unlike inductive learning, transductive learning assumes the test set (unlabeled nodes) is known during training and part of the graph structure. UPL relies on this because its theoretical bounds (Section 4) and edge-removal strategy depend on the fixed adjacency matrix of the entire graph.
  - **Quick check question:** Can UPL classify a node that was added to the graph *after* the training phase without retraining? (Answer: No, it is transductive).

- **Concept: Class Imbalance in Graphs**
  - **Why needed here:** Standard imbalance techniques (like re-weighting) fail here because they do not alter the message-passing pathways; UPL addresses the structural aspect of imbalance by manipulating the node degrees and pseudo-label sets.
  - **Quick check question:** Why does re-weighting the loss function often fail to fix node classification bias? (Answer: It doesn't change the structural connectivity that biases message passing).

- **Concept: Entropy and Calibration**
  - **Why needed here:** UPL uses entropy not just to find the "best" class, but to measure the *stability* of that class prediction across structural perturbations.
  - **Quick check question:** Does UPL prefer a node with 51% probability that is stable across perturbations, or 99% probability that is unstable? (Answer: It prefers the stable one, using the band-pass filter to balance these factors).

## Architecture Onboarding

- **Component map:** Backbone GNN -> Uncertainty Block (SER) -> Selector Block -> Loss Function (Balanced Softmax)
- **Critical path:** Train Base Model → Inference with Edge Removal (Computational bottleneck) → Calculate Uncertainty → Select Minority Nodes (Crucial step) → Add to Training Set → Retrain with Balanced Softmax
- **Design tradeoffs:**
    - Edge Removal Iterations (S_k): Higher counts improve uncertainty estimation accuracy but increase inference time linearly. Paper suggests S_k=100 is sufficient (Appendix G).
    - Thresholds (η_l, η_u): A wide band captures more nodes but introduces noise; a narrow band is safe but may starve the minority class of new labels.
- **Failure signatures:**
    - Heterophily Collapse: On heterophilic graphs (where neighbors have different labels), removing edges might disconnect nodes from their informative (but dissimilar) neighbors, causing performance drops.
    - Stagnation: If the "Band-Pass" is too strict, the pseudo-label set Y_p remains empty, and the model fails to iterate.
- **First 3 experiments:**
    1. Threshold Sweep: Validate the "Band-Pass" theory by sweeping η_l and η_u on a validation set. Verify that the optimum is not at η_u=1.0 (which would mimic standard pseudo-labeling).
    2. Ablation on SER: Compare UPL uncertainty estimation vs. simple Entropy-based filtering. Does the expensive edge-removal process actually yield better pseudo-labels?
    3. Heterophily Stress Test: Run UPL on a heterophilic dataset (like Chameleon) and analyze if the performance gain over Balanced Softmax alone justifies the complexity.

## Open Questions the Paper Calls Out

- **Question:** Can the theoretical upper bound on population risk be generalized from binary to multi-class classification scenarios?
  - **Basis in paper:** [explicit] The authors state their current theoretical analysis "focuses on γ-margin loss functions in binary classification" and identify extending results to "multi-class scenarios" as a future work aim.
  - **Why unresolved:** The mathematical derivation of the bound relies on binary classification constraints which do not directly translate to the multi-class setting.
  - **What evidence would resolve it:** A derivation of the population risk upper bound that holds for k > 2 classes.

- **Question:** How does the generalization error bound behave in inductive node classification settings?
  - **Basis in paper:** [explicit] The authors propose to "expand the analysis to Inductive node classification settings where unlabeled nodes are not observed during training."
  - **Why unresolved:** The current proof utilizes transductive Rademacher complexity, which assumes the model has access to the entire graph structure (including unlabeled nodes) during training.
  - **What evidence would resolve it:** A theoretical bound on generalization error derived specifically for inductive learning where test data is unseen during training.

- **Question:** Can the UPL framework be optimized to handle heterophilic graphs more effectively?
  - **Basis in paper:** [explicit] The authors mention a "marked decline in performance on heterophilic datasets" and propose to "integrate our approach with H2GCN" or "investigate theoretical bounds for heterophilic graphs."
  - **Why unresolved:** The current uncertainty estimation relies on edge removal which may disrupt the specific structural dependencies required for learning in heterophilic networks.
  - **What evidence would resolve it:** A modified theoretical bound for heterophilic data and empirical results showing improved performance on datasets like Chameleon or Squirrel when combined with heterophily-specific architectures.

## Limitations

- The theoretical risk bound derivation relies on idealized assumptions about pseudo-label quality that may not hold in practice
- Edge removal uncertainty estimation may not generalize well to heterophilic graphs where edge structure carries essential semantic information
- The band-pass filtering mechanism requires careful tuning and may fail in extreme imbalance scenarios

## Confidence

- **High Confidence**: The experimental results showing UPL outperforming baselines on benchmark datasets are reproducible and well-documented. The computational efficiency claims relative to ensemble methods are supported by the single-model architecture.
- **Medium Confidence**: The theoretical risk bound derivation and its connection to the algorithm's effectiveness requires further empirical validation. The selective edge removal uncertainty estimation method shows promise but lacks comparison to alternative uncertainty quantification approaches.
- **Low Confidence**: The claim that band-pass filtering consistently improves over standard high-confidence thresholding across all imbalance levels needs more systematic validation, particularly for extreme imbalance ratios.

## Next Checks

1. **Risk Bound Validation**: Conduct controlled experiments varying the minimum node degree and maximum degree per class to empirically verify the theoretical relationship between these parameters and classification performance.
2. **SER Ablation Study**: Compare UPL's edge removal uncertainty estimation against alternative methods (Monte Carlo dropout, deep ensembles) on the same computational budget to quantify the trade-off between efficiency and accuracy.
3. **Extreme Imbalance Testing**: Systematically test UPL on datasets with imbalance ratios ranging from 1:2 to 1:100 to identify the failure threshold where the band-pass filtering mechanism breaks down.