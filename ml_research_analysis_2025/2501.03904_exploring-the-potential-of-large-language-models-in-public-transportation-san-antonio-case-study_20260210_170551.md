---
ver: rpa2
title: 'Exploring the Potential of Large Language Models in Public Transportation:
  San Antonio Case Study'
arxiv_id: '2501.03904'
source_url: https://arxiv.org/abs/2501.03904
tags:
- llms
- information
- transportation
- transit
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) demonstrate potential for enhancing
  public transit systems, but their effectiveness depends heavily on careful engineering
  and fine-tuning. In a San Antonio case study, GPT-3.5-turbo and GPT-4 achieved accuracy
  ranging from 47.97% to 98.44% on transportation understanding tasks and 60.53% to
  90.48% on information retrieval.
---

# Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study

## Quick Facts
- arXiv ID: 2501.03904
- Source URL: https://arxiv.org/abs/2501.03904
- Reference count: 40
- Primary result: LLMs achieved 47.97-98.44% accuracy on transit understanding tasks and 60.53-90.48% on information retrieval, with significant performance drops on complex queries

## Executive Summary
This study evaluates GPT-3.5-turbo and GPT-4 on public transit understanding and information retrieval tasks using San Antonio's GTFS data. The models demonstrated strong performance on schema comprehension and simple information retrieval but struggled with complex multi-file queries and categorical mapping due to semantic similarity issues. Performance varied significantly between OpenAI interfaces and showed sensitivity to question ambiguity. The findings indicate LLMs hold promise for transit applications but require targeted improvements in fine-tuning, context management, and disambiguation techniques before real-world deployment.

## Method Summary
The researchers evaluated LLMs using zero-shot inference on two tasks: (1) Transportation understanding—answering 195 multiple-choice questions about GTFS schemas without context, and (2) Information retrieval—extracting answers from provided GTFS files. They tested on San Antonio VIA data (98 routes, subset to 3 routes for context limitations) using both GPT-3.5-turbo and GPT-4/GPT-4o via OpenAI API. Accuracy was measured through exact match for MCQs and semantic equivalence for short-answer questions, with augmented questions adding "none of these" options to test ambiguity handling.

## Key Results
- Schema understanding accuracy: 85-98% across term definitions, file structure, and attribute mapping tasks
- Simple information retrieval: 90.48% accuracy for single-file lookups
- Complex queries: Performance dropped to 60.53% for multi-file joins requiring relational integration
- Categorical mapping: Struggled with 51.35% average accuracy due to semantic similarity between transit categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained LLMs retain sufficient domain knowledge for structured transit schema understanding but exhibit uneven coverage across subdomains.
- Mechanism: General pre-training on web-scale corpora includes exposure to technical documentation, transit agency websites, and open data specifications, creating latent representations of domain structures.
- Core assumption: The domain knowledge was present in pre-training data and is retrievable without additional context.
- Evidence anchors: [abstract]: "GPT-3.5-turbo and GPT-4 achieved accuracy ranging from 47.97% to 98.44% on transportation understanding tasks"; [section 6.1]: Term Definition (85-96%), File Structure (68-95%), Attribute Mapping (90-98%) indicate strong schema comprehension.

### Mechanism 2
- Claim: LLMs perform competent single-hop information retrieval from provided context but struggle with multi-hop relational joins across structured data files.
- Mechanism: When GTFS data is provided in-context, the attention mechanism can locate relevant tokens for direct attribute lookups, but complex queries exceeding three file joins exceed the model's ability to maintain consistent intermediate state representations.
- Core assumption: The model can parse structured text formats and perform implicit relational operations.
- Evidence anchors: [abstract]: "performance declined significantly for complex queries requiring data integration across multiple files"; [section 6.2]: Simple queries achieved 90.48% (GPT-4o) vs. 64% average for complex queries.

### Mechanism 3
- Claim: Semantic similarity between categorical labels in transit taxonomies induces classification errors, particularly for nuanced distinctions.
- Mechanism: Transit categories often share overlapping semantic features (e.g., "Rail" vs. "Light Rail"). LLMs encode these as high-similarity embeddings, making disambiguation difficult without explicit boundary definitions or examples.
- Core assumption: Standard tokenization and embedding approaches cannot distinguish fine-grained categorical boundaries without fine-tuning or prompt engineering.
- Evidence anchors: [section 7.1]: "Categorical Mapping task, where both models achieved relatively low accuracy (an average of 51.35% and 51.01%)...likely attributed to the semantic similarity between certain categories."

## Foundational Learning

- Concept: General Transit Feed Specification (GTFS) structure
  - Why needed here: All experiments assume familiarity with GTFS file types, their relationships, and attribute schemas.
  - Quick check question: Given a stop_name, which sequence of GTFS files must you traverse to find the route_long_name for trips serving that stop?

- Concept: Zero-shot evaluation methodology
  - Why needed here: All experiments use zero-shot prompting without task-specific training.
  - Quick check question: Why does zero-shot evaluation potentially understate the practical deployability of LLMs for transit applications?

- Concept: Context window constraints and token budgeting
  - Why needed here: The study reduced from 98 to 3 bus routes due to context length limitations.
  - Quick check question: For a transit agency with 500 routes and 10,000 stops, what strategies could address context limitations beyond truncation?

## Architecture Onboarding

- Component map: Pre-trained LLM (GPT-3.5-turbo or GPT-4o) -> GTFS data files -> Prompt template -> Model inference -> Response parsing -> Accuracy evaluation
- Critical path: 1. Dataset preparation -> 2. Context window management -> 3. Prompt construction -> 4. Model inference -> 5. Response parsing -> 6. Accuracy evaluation
- Design tradeoffs: Model selection (GPT-4o provides ~15% accuracy improvement but higher cost), context vs. coverage (larger context enables more route data but increases latency), MCQ vs. open-ended (MCQ enables automated evaluation but may not reflect real query patterns)
- Failure signatures: Categorical mapping queries (expect 50-55% accuracy baseline), multi-file join queries with >3 hops (expect ~60% accuracy), augmented questions with "none of these" option (expect ~10% accuracy drop)
- First 3 experiments: 1. Baseline understanding test: Run 195 MCQs from the GTFS Understanding Benchmarking dataset; 2. Augmented difficulty test: Run augmented MCQs with "none of these" option; 3. Simple vs. complex retrieval test: Provide 3-route subset of GTFS data, test 10 simple and 10 complex queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What techniques can effectively improve LLM accuracy on categorical mapping tasks where semantic similarity between categories causes confusion?
- Basis in paper: [explicit] Authors state the Categorical Mapping task showed low accuracy (~51%) likely "attributed to the semantic similarity between certain categories."
- Why unresolved: Standard pre-trained LLMs lack transportation-specific embeddings, and the paper did not test alternative approaches.
- What evidence would resolve it: Benchmarking transportation-specific embeddings or specialized classification methods showing improved categorical mapping accuracy.

### Open Question 2
- Question: What architectural or prompting strategies can enable LLMs to reliably integrate information across multiple GTFS files for complex transit queries?
- Basis in paper: [explicit] "Performance declined to an average of approximately 64% when handling complex questions requiring data integration across multiple files."
- Why unresolved: The study tested only zero-shot approaches; no multi-hop reasoning or retrieval-augmented methods were evaluated.
- What evidence would resolve it: Demonstrating techniques (e.g., RAG, chain-of-thought prompting, fine-tuning) that significantly improve accuracy on multi-file complex queries.

### Open Question 3
- Question: What are the root causes of output inconsistencies between OpenAI's Playground and API interfaces when using identical models?
- Basis in paper: [explicit] "OpenAI's Playground and programmatic API may produce inconsistent results, even if both of them use the same pre-trained model."
- Why unresolved: Multiple confounding factors are hypothesized but not isolated experimentally.
- What evidence would resolve it: Controlled ablation experiments isolating each factor, followed by documentation of standardized configurations ensuring reproducible outputs.

## Limitations

- Zero-shot evaluation without fine-tuning likely understates practical LLM potential for transit applications
- Semantic equivalence evaluation method for short-answer questions remains unspecified
- Interface inconsistency between API and Playground suggests uncontrolled variability affecting reproducibility
- Context window limitations (reducing 98 routes to 3) raise questions about scalability to real-world transit systems

## Confidence

- **High Confidence**: Schema understanding capabilities (Term Definition, File Structure, Attribute Mapping categories showing 85-98% accuracy)
- **Medium Confidence**: Single-hop information retrieval performance (90.48% for simple queries) within tested context window constraints
- **Low Confidence**: Claims about multi-hop query performance and semantic similarity challenges due to limited testing scope and unclear evaluation methodology

## Next Checks

1. **Semantic Equivalence Validation**: Implement a standardized method (e.g., embedding-based similarity thresholds or human review protocol) for evaluating short-answer responses and verify consistency across model versions.

2. **Context Window Scalability Test**: Replicate the information retrieval experiments with progressively larger route subsets (10, 25, 50 routes) to quantify accuracy degradation as context approaches window limits.

3. **Fine-tuning Impact Assessment**: Fine-tune GPT-3.5-turbo on the GTFS Understanding Benchmarking dataset and compare performance against zero-shot results to establish the practical improvement potential.