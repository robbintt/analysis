---
ver: rpa2
title: 'Debating Truth: Debate-driven Claim Verification with Multiple Large Language
  Model Agents'
arxiv_id: '2507.19090'
source_url: https://arxiv.org/abs/2507.19090
tags:
- evidence
- claim
- debate
- debatecv
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DebateCV, the first debate-driven claim verification
  framework that uses multiple LLM agents to verify claims. The system employs two
  Debaters with opposing stances and a Moderator to adjudicate their arguments over
  multiple rounds.
---

# Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents

## Quick Facts
- **arXiv ID:** 2507.19090
- **Source URL:** https://arxiv.org/abs/2507.19090
- **Reference count:** 40
- **Primary result:** DebateCV achieves 2.6-5.8% higher accuracy than state-of-the-art baselines on the AVeriTeC benchmark across different evidence quality conditions

## Executive Summary
This paper introduces DebateCV, the first debate-driven claim verification framework that uses multiple LLM agents to verify claims. The system employs two Debaters with opposing stances and a Moderator to adjudicate their arguments over multiple rounds. A key challenge addressed is that zero-shot Moderators often default to neutral verdicts even when evidence supports a definitive judgment. To solve this, the authors propose Debate-SFT, a novel post-training framework that synthesizes debate data from existing claim verification datasets to train better Moderators. Experiments on the AVeriTeC benchmark show DebateCV outperforms state-of-the-art single-agent and multi-agent baselines by 2.6-5.8% in accuracy across different evidence quality conditions, while also producing higher-quality justifications as judged by human experts.

## Method Summary
DebateCV uses a multi-agent debate framework where two LLM debaters (D+ and D-) argue opposing stances on a claim using provided evidence, with a Moderator adjudicating the debate. The system addresses the common problem of zero-shot moderators defaulting to neutral verdicts by introducing Debate-SFT - a post-training framework that synthesizes debate data from existing claim verification datasets to train more decisive moderators. The framework uses LoRA fine-tuning with rank 128 and alpha 256, and includes convergence detection to terminate debates early when arguments stop evolving. The approach is evaluated on the AVeriTeC benchmark across three evidence conditions: golden (human-collected), Retrieved-H (HerO), and Retrieved-I (InFact).

## Key Results
- DebateCV outperforms state-of-the-art single-agent and multi-agent baselines by 2.6-5.8% in accuracy across different evidence quality conditions
- Debate-SFT reduces false positive rate for "Not Enough Evidence" from 25.0% to 0.4% by addressing moderator neutrality bias
- The framework produces higher-quality justifications as judged by human experts compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Error Surfacing
- **Claim:** Opposing-stance debates expose evidence-analysis errors that single agents miss.
- **Mechanism:** Two Debaters (affirmative D+ and negative D−) receive identical evidence E and argue from assigned stances across rounds. The adversarial pressure forces each side to surface overlooked evidence, misinterpretations, or overreliance on speculation that a single agent's linear reasoning chain would not catch.
- **Core assumption:** Errors in claim verification are often asymmetrically discoverable—weaknesses in one position are more visible to an opponent than to the original reasoner.
- **Evidence anchors:** [abstract] "two Debaters argue opposing stances to surface subtle errors in single-agent assessments"; [Section 1] cites real-world "star chamber" sessions at PolitiFact where panels critique each other's assessments; [Section 5.2 Case Study] documents three error patterns addressed: claim/evidence misinterpretation, overlooking evidence, and overreliance on speculation.
- **Break condition:** If evidence is extremely sparse or both debaters converge on speculation without concrete anchors, adversarial pressure yields diminishing returns and may amplify hallucinated arguments.

### Mechanism 2: Moderator Neutrality Bias Mitigation via Debate-SFT
- **Claim:** Supervised fine-tuning on synthetic debates with corrected verdicts shifts moderators from unwarranted neutral judgments to decisive adjudication.
- **Mechanism:** Zero-shot moderators exhibit conformity bias, defaulting to "Not Enough Evidence" or "Conflicting Evidence" even when evidence supports a definitive verdict. Debate-SFT constructs training pairs where incorrect predictions are paired with LLM-generated justifications aligned to ground-truth labels, teaching the moderator to weigh argument strength more confidently.
- **Core assumption:** The debate transcript contains sufficient signal for the ground-truth verdict to be derivable, and the Corrector can articulate that derivation without introducing new hallucinations.
- **Evidence anchors:** [abstract] "zero-shot Moderators are biased toward neutral judgments, and no datasets exist for training them"; [Section 5.3/Table 4] false positive rate for "Not Enough Evidence" drops from 25.0% to 0.4% with Debate-SFT; [corpus] CRAVE paper addresses conflicting reasoning but does not specifically target neutrality bias in multi-round adjudication.
- **Break condition:** If the original debate transcript genuinely lacks evidence for any verdict (true ambiguity), forcing decisiveness may increase error rates on edge cases.

### Mechanism 3: Early-Termination via Convergence Detection
- **Claim:** Round-by-round summaries enable moderators to detect argument repetition and terminate debates efficiently.
- **Mechanism:** After each round t, the Moderator synthesizes a summary St and compares it to St−1. High similarity signals that no new insights are emerging, triggering early termination before Tmax. This reduces computational cost while preserving verification quality for simpler claims.
- **Core assumption:** Semantic similarity between summaries reliably indicates exhausted argument space, not just linguistic repetition of unresolved issues.
- **Evidence anchors:** [Section 3.2] "The Moderator then assesses whether the debate has converged, e.g., it deems that St and St−1 are highly similar"; [Section 5.3/Figure 2] 261 of 500 claims resolved in 1 round with Debate-SFT moderator; [corpus] MERMAID uses iterative grounding but does not explicitly model convergence detection for debate termination.
- **Break condition:** Claims requiring nuanced multi-step reasoning may be prematurely terminated if summaries appear similar superficially while deeper contradictions remain unexplored.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG) for fact-checking
  - **Why needed here:** DebateCV operates on the claim verification stage after evidence retrieval; understanding RAG pipelines clarifies where DebateCV plugs in and how evidence quality (golden vs. retrieved) affects performance.
  - **Quick check question:** Given a claim and a retrieved evidence set containing both relevant and misleading documents, how would you distinguish which evidence a single agent might overlook versus what adversarial debate would surface?

- **Concept:** Supervised Fine-Tuning (SFT) with LoRA
  - **Why needed here:** Debate-SFT uses LoRA (rank 128, alpha 256) to post-train moderators; understanding parameter-efficient fine-tuning is necessary to reproduce training and reason about overfitting risks on synthetic data.
  - **Quick check question:** If your SynDeC dataset contains 30% corrected error samples and 70% originally correct samples, what curriculum strategy might you use to prevent the moderator from overfitting to error patterns?

- **Concept:** Multi-agent turn-taking and role prompts
  - **Why needed here:** DebateCV relies on strict turn order and role-specific system prompts; understanding prompt engineering for role adherence helps diagnose debater failures (e.g., abandoning assigned stance).
  - **Quick check question:** A Negative Debater starts agreeing with the affirmative argument mid-debate. What prompt or structural intervention would you implement to enforce stance persistence?

## Architecture Onboarding

- **Component map:** Affirmative Debater (D+) -> Negative Debater (D−) -> Moderator (M) -> [repeat for subsequent rounds] -> Final verdict
- **Critical path:**
  1. Pre-debate: Initialize D+, D−, M with role prompts; provide claim c and evidence E
  2. Round 1: D+ opens → D− rebuts → M summarizes and checks convergence
  3. Subsequent rounds: D+ counters → D− rebuts → M summarizes/checks (repeat until convergence or Tmax)
  4. Termination: M outputs verdict ŷ and justification ĵ
- **Design tradeoffs:**
  - Cost vs. accuracy: Multi-round debates with proprietary debaters (GPT-4o-Mini) cost ~$0.0022/claim vs. single-agent baselines; justified for high-stakes fact-checking but may be prohibitive for bulk processing
  - Evidence quality dependency: Performance drops 10-17 points from golden to retrieved evidence; retrieval quality remains a bottleneck
  - Corrector reliability: Synthetic justifications for error samples assume the debate contains sufficient signal; human spot-checking recommended
- **Failure signatures:**
  - Zero-shot moderator outputs high rate of "Not Enough Evidence" (>20% FPR) → needs Debate-SFT
  - Debaters abandon assigned stance → strengthen role prompts or add explicit stance-enforcement prompts
  - Debate extends to Tmax with circular arguments → convergence detection threshold may be too strict
- **First 3 experiments:**
  1. Run zero-shot DebateCV on 100 claims from AVeriTeC dev split; measure verdict distribution and identify neutral-bias rate
  2. Train Debate-SFT moderator on SynDeC subset (e.g., 1,000 samples); compare accuracy vs. zero-shot on same 100 claims
  3. Ablate training data: train moderator with Ccorrect only vs. Cerror only to isolate contribution of error-correction samples

## Open Questions the Paper Calls Out
- How can the inference cost efficiency of the DebateCV framework be enhanced to make it more viable for large-scale deployment?
- What strategies can effectively improve claim verification performance under extremely noisy or heavily polluted evidence conditions?
- To what extent does the synthetic SynDeC training data limit the Moderator's ability to generalize compared to human-curated debate data?

## Limitations
- The approach relies entirely on synthetic data for training, with no validation of the quality gap between synthetic and human-curated debate data
- Performance significantly degrades (10-17% accuracy drop) when moving from golden to retrieved evidence, indicating retrieval quality remains a bottleneck
- The computational cost (~$0.0022/claim) may be prohibitive for large-scale deployment despite accuracy improvements

## Confidence
- **High confidence:** DebateCV's accuracy improvements over baselines (2.6-5.8%) and reduction in neutral-bias false positives (25% to 0.4%) with Debate-SFT
- **Medium confidence:** The generalizability across different LLM backbones (GPT-4o-Mini, Llama-3.1-8B, Qwen-2.5-7B) given the specific implementation details
- **Low confidence:** The scalability of the approach for production use given the computational cost and dependency on high-quality retrieval

## Next Checks
1. Implement ablation study comparing DebateCV with fixed-round debates (no early termination) to quantify convergence detection's contribution to efficiency
2. Test Debate-SFT training with synthetic debate data generated from different LLM backbones to assess robustness to synthesis model variation
3. Conduct human evaluation of corrected justifications from the Corrector to verify that synthetic error corrections maintain factual accuracy and logical consistency