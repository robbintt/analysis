---
ver: rpa2
title: Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition
arxiv_id: '2505.20006'
source_url: https://arxiv.org/abs/2505.20006
tags:
- lora
- accent
- speech
- fine-tuning
- mas-lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses improving ASR robustness for non-native multi-accent
  speech using low-resourced training data. It introduces Mixture of Accent-Specific
  LoRAs (MAS-LoRA), a parameter-efficient fine-tuning method that uses multiple LoRA
  experts, each specialized for a specific accent, combined at inference time.
---

# Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2505.20006
- Source URL: https://arxiv.org/abs/2505.20006
- Reference count: 0
- Primary result: MAS-LoRA achieves 11.77% WER on L2-ARCTIC, outperforming regular LoRA (12.32%) and full fine-tuning (12.21%) when accent is unknown

## Executive Summary
This work addresses improving ASR robustness for non-native multi-accent speech using low-resourced training data. It introduces Mixture of Accent-Specific LoRAs (MAS-LoRA), a parameter-efficient fine-tuning method that uses multiple LoRA experts, each specialized for a specific accent, combined at inference time. Experiments on the L2-ARCTIC corpus show MAS-LoRA achieves a WER of 11.77%, significantly outperforming regular LoRA (12.32%) and full fine-tuning (12.21%) when the accent is unknown. MAS-LoRA also shows better performance (11.90% vs 12.32%) and less catastrophic forgetting than regular LoRA when the accent is known. It maintains performance on native speech and generalizes well to unseen accents.

## Method Summary
MAS-LoRA trains separate LoRA experts for each accent using only that accent's data, preventing gradient interference during optimization. Each sample routes only through its corresponding accent expert during training. At inference, experts are combined with either equal weights (accent-agnostic) or a learned weighting parameter β (accent-aware). The method applies accent-specific LoRA to the encoder only, using standard LoRA for the decoder. Experiments use Whisper small with LoRA rank 16, trained for 3 epochs on L2-ARCTIC's 6 accents with 8-fold cross-validation.

## Key Results
- MAS-LoRA achieves 11.77% WER on L2-ARCTIC, outperforming regular LoRA (12.32%) and full fine-tuning (12.21%) when accent is unknown
- For accent-aware inference, MAS-LoRA achieves 11.90% WER vs regular LoRA's 12.32%, with optimal β=2 weighting
- MAS-LoRA shows less catastrophic forgetting than full fine-tuning on LibriSpeech (5.95% vs 7.90% WER degradation)

## Why This Works (Mechanism)

### Mechanism 1: Accent-Specific Expert Isolation During Training
Training separate LoRA experts on single-accent data enables cleaner specialization than joint training on mixed-accent data. Each sample routes only through its corresponding accent expert, preventing gradient interference between accents with conflicting phonetic patterns. This works when accents have sufficiently distinct phonetic features that separating them during optimization reduces task interference.

### Mechanism 2: Linear Weight Averaging as Implicit Regularization
Averaging expert outputs at inference improves generalization even when the correct accent is known. Equal-weight averaging acts as ensemble-style regularization, smoothing accent-specific overfitting. The paper shows using only the matched expert degrades results vs. combining all experts, interpreted as a form of regularization. This works when each expert captures complementary information and no single expert is universally optimal.

### Mechanism 3: Encoder-Decoder Asymmetric Adaptation
Accent-related adaptation is more effective in the encoder than decoder; decoder benefits from accent-independent fine-tuning. The encoder processes raw acoustic features where accent manifests (pronunciation, prosody), while the decoder operates on higher-level linguistic representations. MAS-LoRA in encoder + standard LoRA in decoder achieves best WER of 11.77%. This works when acoustic variation across accents is the primary challenge and linguistic decoding is relatively accent-invariant.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: Core technique; MAS-LoRA is built on LoRA's decomposition of weight updates into low-rank matrices
  - Quick check question: Can you explain why LoRA's linearity allows weight merging without inference overhead?

- **Mixture of Experts (MoE) Routing**
  - Why needed here: MAS-LoRA is an MoE variant; understanding why this paper avoids learned routers contextualizes the design choice
  - Quick check question: What is the difference between learned routing and fixed-weight averaging, and why might the latter help in low-resource settings?

- **Catastrophic Forgetting in Fine-Tuning**
  - Why needed here: A key claim is MAS-LoRA reduces forgetting; understanding this phenomenon clarifies the evaluation motivation
  - Quick check question: Why does full fine-tuning degrade LibriSpeech WER from 5.78% to 7.90% while MAS-LoRA preserves 5.95%?

## Architecture Onboarding

- **Component map:** Audio -> Whisper encoder -> Accent-specific LoRA experts -> Weight combiner -> Whisper decoder -> Text output
- **Critical path:**
  1. Instantiate n LoRA experts, each associated with one accent
  2. During training, route each sample through only its accent's expert
  3. After training, merge expert weights for inference (agnostic) or apply β-weighting (aware)
  4. Decoder always uses standard LoRA (not accent-specific)

- **Design tradeoffs:**
  - Q,V vs Q,K,V,O: Q,V only matches Q,K,V,O performance with fewer parameters (1.91% vs 3.76%)
  - Encoder-only vs encoder+decoder MAS-LoRA: Decoder MAS-LoRA adds complexity without clear gain
  - β selection (accent-aware): β=2 is optimal; lower over-emphasizes single expert, higher dilutes accent-specific signal

- **Failure signatures:**
  - WER increases when only the matched expert is used (β=1) → indicates overfitting to individual experts
  - LibriSpeech WER > 6.5% after fine-tuning → indicates excessive forgetting; reduce LoRA rank or use encoder-only adaptation
  - Unseen accent WER > No-FT baseline → indicates poor generalization

- **First 3 experiments:**
  1. Train MAS-LoRA-qv on L2-ARCTIC (all 6 accents), evaluate with equal-weight averaging. Target: WER ≈ 11.77%
  2. Compare encoder-only MAS-LoRA vs encoder+decoder on held-out accent. Confirm decoder MAS-LoRA is unnecessary
  3. For accent-aware inference, sweep β ∈ {1,2,3,4,5,6} and plot WER. Verify β=2 is optimal and β=1 degrades performance

## Open Questions the Paper Calls Out

- Can a trainable routing mechanism be developed that outperforms the static weighting strategies used in MAS-LoRA?
  - Basis: The authors state they "have tried learning routers, but it did not show any improvement"
  - Why unresolved: The failure of initial routing attempts suggests optimization difficulty, but more sophisticated routing architectures could capture accent features better
  - Evidence needed: A learned routing function achieving lower WER than static averaging on L2-ARCTIC

- How does MAS-LoRA performance scale when applied to significantly larger ASR backbones or wider variety of accents?
  - Basis: Experiments are restricted to Whisper small and only 6 specific accents
  - Why unresolved: Benefits in low-resource, small-model settings may diminish or require different hyperparameters in large-scale settings
  - Evidence needed: Benchmarks on Whisper Large-v3 or models trained on datasets with 50+ accents

- Can the optimal weighting parameter β for accent-aware inference be predicted dynamically without ground-truth accent labels?
  - Basis: Section 4.3 analyzes β but relies on knowing ground truth accent
  - Why unresolved: In practical scenarios, the system must infer accent or optimal weight from audio itself
  - Evidence needed: A mechanism estimating β from input audio features achieving comparable WER to oracle settings

## Limitations

- Data sparsity with only 4 speakers per accent (~1h data each) may reflect speaker-specific patterns rather than true accent generalization
- Limited analysis of which accents transfer well versus which fail catastrophically for unseen accent evaluation
- β parameter sensitivity lacks theoretical justification and may require different values for different accent distributions

## Confidence

- **High confidence:** MAS-LoRA outperforms regular LoRA and full fine-tuning when accent is unknown (11.77% vs 12.32% vs 12.21%)
- **Medium confidence:** MAS-LoRA reduces catastrophic forgetting on native speech (5.95% vs 7.90% WER degradation)
- **Low confidence:** Equal-weight averaging of experts acts as implicit regularization due to degradation when using single experts

## Next Checks

1. **Speaker-level ablation study:** Train MAS-LoRA on 3 speakers per accent, test on held-out speakers from same accents, and measure WER variance to determine if improvements stem from accent generalization or speaker memorization

2. **Expert contribution analysis:** For accent-aware inference with β=2, measure individual expert WER contributions and correlation with training data quality to validate whether equal averaging truly acts as regularization

3. **Generalization stress test:** Train on 5 accents, test on the 6th (unseen), then systematically remove one training accent and retest on the held-out accent to quantify how accent similarity affects transfer