---
ver: rpa2
title: 'MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding'
arxiv_id: '2510.02790'
source_url: https://arxiv.org/abs/2510.02790
tags:
- image
- maskcd
- attention
- heads
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses hallucinations in large vision-language models\
  \ (LVLMs), where models generate content contradictory to their input images or\
  \ text. The authors propose MaskCD, a contrastive decoding method that mitigates\
  \ hallucinations by masking \"image heads\"\u2014attention heads in the LLM backbone\
  \ that disproportionately attend to image tokens."
---

# MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding

## Quick Facts
- arXiv ID: 2510.02790
- Source URL: https://arxiv.org/abs/2510.02790
- Authors: Jingyuan Deng; Yujiu Yang
- Reference count: 23
- One-line primary result: MaskCD reduces LVLM hallucinations by up to 75.59% (CHAIR_s) while preserving or improving general capabilities

## Executive Summary
This paper addresses hallucinations in large vision-language models (LVLMs), where models generate content contradictory to their input images or text. The authors propose MaskCD, a contrastive decoding method that mitigates hallucinations by masking "image heads"—attention heads in the LLM backbone that disproportionately attend to image tokens. These image heads are identified by analyzing attention scores across multiple images and selecting those exceeding a threshold. The masked image heads are used to construct contrastive samples, which are then subtracted from original logits to reduce hallucinated content. Evaluated on LLaVA-1.5-7b and Qwen-VL-7b across benchmarks like CHAIR, POPE, AMBER, and MME, MaskCD significantly reduces hallucination metrics while preserving or improving general capabilities.

## Method Summary
MaskCD identifies "image heads" in LVLM LLM backbones by aggregating attention scores to image tokens across multiple images, then selects heads exceeding a threshold τ. During inference, it performs two forward passes—one with the original model and one with masked image heads—and applies contrastive decoding by subtracting logits. This removes ungrounded probability mass (hallucinations) while preserving visually-grounded content. The method is training-free and requires pre-computing a static mask for each model architecture.

## Key Results
- Reduces CHAIR_s by up to 75.59% and CHAIR_i by up to 50.57% compared to baseline
- Outperforms VCD, M3ID, and OPERA in stability and hallucination suppression
- Maintains or improves performance on general capability benchmarks (POPE, AMBER, MME)
- Achieves best results with τ=0.9 for LLaVA-1.5-7b and τ=0.975 for Qwen-VL-7b

## Why This Works (Mechanism)

### Mechanism 1: Image Head Identification via Attention Statistics
The method identifies attention heads that consistently pay disproportionate attention to image tokens across diverse inputs. By aggregating attention scores from 500+ images and selecting heads exceeding threshold τ, it creates a stable mask of heads primarily responsible for visual information flow.

### Mechanism 2: Contrastive Decoding via Logit Subtraction
Subtracting logits from an image-masked model removes hallucination-prone probability mass while preserving grounded content. The masked model retains language priors but lacks visual grounding, so its output represents "ungrounded" probability mass that, when subtracted, leaves visually-grounded content amplified.

### Mechanism 3: Threshold-Controlled Head Selection
An intermediate threshold τ≈0.9 optimally balances hallucination suppression against information loss. Higher τ masks fewer heads (less suppression, more information preserved), while lower τ masks more heads (more suppression, risk of removing useful signal).

## Foundational Learning

- **Multi-Head Self-Attention in Transformers**: Understanding how attention scores aggregate across heads and layers is essential for interpreting "image heads" and their role in visual information flow. Quick check: In a 32-layer decoder with 32 heads per layer, what is the shape of the attention matrix for one head at one layer, and how do you compute the total attention a head pays to image tokens?

- **Contrastive Decoding**: The core technique; you must understand why subtracting logits from a degraded model amplifies desired properties. Quick check: If `logits_masked[y_hallucinated]` is high but `logits_original[y_hallucinated]` is moderate, what happens to `p(y_hallucinated)` after applying Equation 3 with α=1.0?

- **LVLM Architecture Components**: The paper targets LLM backbone attention heads; knowing where visual tokens enter clarifies why masking specific heads disrupts visual grounding. Quick check: In LLaVA-1.5, image tokens are projected via MLP and concatenated with text tokens—how does this architecture affect which layers might develop strong "image heads"?

## Architecture Onboarding

- **Component map**: Image Head Identification Pipeline -> MaskCD Inference Engine -> Hyperparameter Registry
- **Critical path**: 1) Pre-computation: Run 500 captioning tasks, aggregate attention statistics, compute and save ImageHeadMask. 2) Load mask at inference; for each token step, execute two forward passes, apply Equation 3. 3) Mask is LLM-backbone-specific—changing models requires re-computation.
- **Design tradeoffs**: Compute vs. Quality (2x inference cost vs. better hallucination suppression), Generality vs. Precision (pre-computed mask generalizes but not across LLM backbones), α robustness (stable across wide range vs. τ sensitivity).
- **Failure signatures**: CHAIR metrics worse than baseline (τ too low or α poorly tuned), MME non-hallucination scores drop (visual information over-suppressed), high variance across image types (mask may not generalize).
- **First 3 experiments**: 1) Reproduce Figure 2 visualization on target model with 100-200 images to confirm image head distribution. 2) Ablate τ (0.5, 0.7, 0.9, 0.95) on CHAIR subset to find optimal threshold. 3) Compare MaskCD vs. VCD vs. M3ID on POPE adversarial split to stress-test robustness.

## Open Questions the Paper Calls Out
- The paper mentions the need to explore dynamic mask construction during model operation to eliminate pre-processing overhead and static storage requirements.
- No specific open questions are called out in the paper text itself.

## Limitations
- Static mask requires pre-computation on 500+ images and is model-specific, limiting adaptability to new models or distribution shifts.
- Doubling inference time due to two forward passes may limit deployment in latency-sensitive settings.
- Single threshold τ may not be optimal across all task types (captioning vs. VQA), as evidenced by variance in Table 16.

## Confidence
- **High**: Contrastive decoding with masked heads reduces hallucination metrics across multiple benchmarks and outperforms existing methods.
- **Medium**: Image head identification process reliably isolates heads that, when masked, yield valid contrastive samples, with cross-domain overlap suggesting generalization within natural image domains.
- **Low**: Single τ value optimality across all LVLMs and tasks is not fully validated; different models use different optimal thresholds.

## Next Checks
1. **Cross-Domain Robustness Test**: Apply MaskCD to LLaVA-1.5-7b on medical images or charts and measure CHAIR_s/i degradation to verify mask generalization beyond natural images.
2. **Task-Type Ablation**: Evaluate MaskCD on VQA vs. captioning tasks with the same mask to determine if task-specific masks or thresholds are needed.
3. **Dynamic Thresholding**: Implement per-image τ selection based on image complexity or entropy and compare against static τ=0.9 to test for residual limitations of static masking.