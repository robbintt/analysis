---
ver: rpa2
title: 'Automating modeling in mechanics: LLMs as designers of physics-constrained
  neural networks for constitutive modeling of materials'
arxiv_id: '2512.01735'
source_url: https://arxiv.org/abs/2512.01735
tags:
- cann
- constitutive
- neural
- https
- gencann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can autonomously design and implement
  physics-constrained neural networks for constitutive modeling of materials, achieving
  accuracy comparable to or exceeding human-engineered models. The approach combines
  LLM-driven architecture selection, physical constraint integration, and code generation
  to create task-specific constitutive artificial neural networks (CANNs) on demand.
---

# Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials

## Quick Facts
- arXiv ID: 2512.01735
- Source URL: https://arxiv.org/abs/2512.01735
- Reference count: 40
- Primary result: LLM-generated CANNs achieve R² scores above 0.90, matching or exceeding human-engineered models

## Executive Summary
Large language models can autonomously design and implement physics-constrained neural networks for constitutive modeling of materials, achieving accuracy comparable to or exceeding human-engineered models. The approach combines LLM-driven architecture selection, physical constraint integration, and code generation to create task-specific constitutive artificial neural networks (CANNs) on demand. Evaluation on brain, rubber, and skin datasets demonstrates that LLM-generated CANNs achieve R² scores above 0.90 across all test cases, with superior generalization to unseen loading scenarios and extrapolation to large deformations. This framework significantly reduces the expertise required for constitutive modeling while maintaining the accuracy and physical consistency of specialized CANNs, representing a step toward practical end-to-end automation in mechanics.

## Method Summary
The framework uses OpenAI's O3 model to generate CANN implementations based on material classification and continuum mechanics theory prompts. The LLM receives a prompt containing the task description, relevant physics theory (strain energy derivatives, invariants), and a code skeleton. It generates complete CANN code including PsiNetwork, StructureTensor, and CANN classes. The generated code is trained on the provided dataset, with R² scores fed back for three refinement rounds. The process is repeated five times per dataset, with the best-performing model selected. The approach focuses on hyperelastic incompressible materials, with isotropic and transversely isotropic classes handled via different prompts.

## Key Results
- LLM-generated CANNs achieved R² scores above 0.90 across brain, rubber, and skin datasets
- GenCANNs demonstrated superior generalization to unseen loading scenarios compared to purely data-driven baselines
- The refinement process yielded small but consistent accuracy improvements across all test cases
- Extrapolation to large deformations showed lower error rates than baseline CANNs trained on the same data

## Why This Works (Mechanism)

### Mechanism 1
Embedding continuum mechanics theory into the prompt enables LLMs to generate physically consistent neural architectures that outperform purely data-driven baselines. The prompt encodes the strain-energy-to-stress relationship via automatic differentiation, implementing preprocessing (invariant computation), a learnable strain energy network, and postprocessing (stress derivation via `tape.gradient(Psi, F)`), guaranteeing objectivity and thermodynamic admissibility by construction.

### Mechanism 2
Iterative refinement with performance feedback improves model quality beyond single-shot generation. After initial CANN generation, the framework trains the model, computes R² scores, and returns both code and metrics to the LLM for three refinement rounds. This creates a bilevel loop: LLM as outer optimizer proposing architectures; gradient descent as inner optimizer fitting parameters.

### Mechanism 3
The gray-box structure (mechanics-informed preprocessing/postprocessing + neural core) enables reliable extrapolation beyond training loading paths. By reducing the learning problem from tensor-to-tensor mapping to scalar invariant-to-energy regression, the network operates in a physically meaningful latent space. Stress is derived analytically, so extrapolation errors in the energy function do not compound into physically impossible stress states.

## Foundational Learning

- **Concept: Deformation invariants (I₁, I₂, I₄, I₅)**
  - Why needed here: These scalar invariants of the right Cauchy–Green tensor C encode deformation independent of coordinate system, enabling frame-invariant constitutive models.
  - Quick check question: Given a deformation gradient F = diag(1.5, 0.8, 0.833), compute I₁ and verify it exceeds 3 (indicating net extension).

- **Concept: Strain energy density and thermodynamic consistency**
  - Why needed here: Hyperelastic materials are defined by a scalar potential Ψ; stresses derived as Ψ gradients automatically satisfy energy conservation and prevent non-physical dissipation.
  - Quick check question: If Ψ(I₁) = c₁(I₁ − 3), derive P_iso and confirm stress vanishes at zero deformation.

- **Concept: Automatic differentiation for physics constraints**
  - Why needed here: Computing ∂Ψ/∂F via autodiff eliminates manual derivative derivation and ensures exact gradients for training and stress evaluation.
  - Quick check question: In the code skeleton, what breaks if `tape.watch(F)` is omitted?

## Architecture Onboarding

- **Component map:**
  Static prompt selector → (material class → prompt template) → LLM (O3) → generates CANN code (PsiNetwork, StructureTensor, CANN classes) → Training loop → TensorFlow/PyTorch optimizer, MSE loss on stress predictions → Refinement loop → returns code + R² for 3 iterations → Output → trained GenCANN model

- **Critical path:**
  1. Classify material (isotropic vs. transversely isotropic)
  2. Select corresponding prompt with mechanics theory + code skeleton
  3. LLM generates complete CANN implementation
  4. Validate syntax, execute training, check R² > 0
  5. If errors, retry; if valid, run 3 refinement rounds
  6. Select best of 5 independent generations

- **Design tradeoffs:**
  - Larger networks (256–128–64 vs. 16–16) fit training data better but risk overfitting on small datasets; constrained GenCANNs remain competitive
  - Static prompt limits flexibility; fully agentic orchestration could adapt to novel material classes but increases complexity
  - R² as sole metric ignores stress distribution physics; multi-objective loss could improve robustness

- **Failure signatures:**
  - Syntax errors (31% of generations): LLM hallucinates invalid API calls or mismatched tensor shapes
  - Training errors (5%): Negative R² indicates divergence, often from poor initialization or loss landscape
  - Overfitting: Near-perfect training R² but poor leave-one-path-out generalization

- **First 3 experiments:**
  1. **Isotropic rubber validation:** Run GenCANN on Treloar's dataset; compare against baseline CANN on uniaxial/equibiaxial/pure shear. Expect R² > 0.99 across all modes.
  2. **Extrapolation stress test:** Train only on uniaxial + pure shear; evaluate on equibiaxial and intermediate biaxial states in the invariant plane. Quantify error growth vs. distance from training paths.
  3. **Anisotropic fiber learning:** On porcine skin data, inspect learned fiber angle α from `get_alpha()`. Compare against known collagen alignment; verify the LLM correctly estimates direction from biaxial stress asymmetry.

## Open Questions the Paper Calls Out

### Open Question 1
Can LLM-driven agents successfully replace the static code currently used for prompt selection and training orchestration to improve system adaptability? The authors state that "Components that are currently static... could be assigned to LLM-driven agents" to expand the design space and improve adaptability. This remains unresolved as the current implementation relies on hard-coded scripts to select prompts and manage training.

### Open Question 2
Can the framework be extended to model path-dependent inelastic behaviors, such as viscoelasticity or plasticity, without manual architectural redesign? The authors restrict the scope to "hyperelastic incompressible materials," despite referencing data-driven methods for viscoelasticity and plasticity in the background. The provided prompt scaffolding focuses on strain energy functions, which may be insufficient for history-dependent models requiring internal variables or specialized network structures.

### Open Question 3
How can the reliability of the code generation step be improved to minimize the 36% combined syntax and training failure rate? Statistical analysis shows a 31% syntax error rate and 5% training error rate, which the system currently resolves through multiple retries. The high failure rate suggests the LLM struggles with implementation consistency, relying on stochastic retries rather than deterministic correctness.

## Limitations

- The approach is currently limited to hyperelastic incompressible materials, with unclear generalizability to viscoelastic, plastic, or damage behaviors
- The 36% failure rate in code generation requires multiple retries, suggesting reliability issues in the LLM's implementation capabilities
- Static prompt selection limits adaptability to novel material classes beyond the isotropic/transversely-isotropic binary

## Confidence

- **High confidence**: The core mechanism of using LLMs to generate CANN architectures is validated by the consistent R² > 0.90 results across all datasets and successful extrapolation to unseen loading scenarios
- **Medium confidence**: The refinement loop demonstrably improves results, but the magnitude of improvement (small but consistent) suggests diminishing returns after initial generations
- **Low confidence**: Claims about the approach "significantly reducing expertise required" are supported by the automation success but lack comparative studies against practitioners with varying experience levels

## Next Checks

1. **Cross-validation stress analysis**: Implement k-fold cross-validation on each dataset and analyze the stress error distribution (not just R²) to identify systematic biases in the generated models.

2. **Material class expansion**: Test the framework on materials requiring additional invariants (e.g., orthotropic composites) or path-dependent behavior to evaluate prompt adaptability limits.

3. **Expert comparison study**: Have experienced constitutive modelers design CANNs for the same datasets and compare performance, development time, and robustness to hyperparameter variations against the LLM-generated models.