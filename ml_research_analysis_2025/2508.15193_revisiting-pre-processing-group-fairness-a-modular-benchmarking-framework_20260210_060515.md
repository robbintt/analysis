---
ver: rpa2
title: 'Revisiting Pre-processing Group Fairness: A Modular Benchmarking Framework'
arxiv_id: '2508.15193'
source_url: https://arxiv.org/abs/2508.15193
tags:
- fairness
- pre-processing
- metrics
- data
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FairPrep, a modular benchmarking framework
  for evaluating fairness-aware pre-processing techniques on tabular datasets. Built
  on AIF360, FairPrep enables standardized evaluation of data-level fairness interventions
  through a two-stage architecture: pre-processing (data transformation and fairness
  metric computation) and benchmarking (model training and performance/fairness evaluation).'
---

# Revisiting Pre-processing Group Fairness: A Modular Benchmarking Framework

## Quick Facts
- **arXiv ID**: 2508.15193
- **Source URL**: https://arxiv.org/abs/2508.15193
- **Reference count**: 37
- **Primary result**: FairPrep achieves perfect group fairness (DI=1.0, SPD=0.0) with Reweighing while maintaining data integrity.

## Executive Summary
This paper introduces FairPrep, a modular benchmarking framework for evaluating fairness-aware pre-processing techniques on tabular datasets. Built on AIF360, FairPrep enables standardized evaluation of data-level fairness interventions through a two-stage architecture separating pre-processing transformations from model benchmarking. The framework supports four pre-processing methods and evaluates them across five benchmark datasets, demonstrating that Reweighing achieves perfect group fairness while preserving data integrity, whereas Learned Fair Representations, despite ideal fairness metrics, often produces unrealistic label distributions.

## Method Summary
FairPrep operates in two independent stages: pre-processing and benchmarking. The pre-processing stage applies fairness-aware transformations (Reweighing, Learned Fair Representations, Disparate Impact Remover, Optimised Pre-processing) to datasets and computes fairness metrics. The benchmarking stage trains models using a 70/15/15 split and evaluates performance and fairness across 99 threshold values (0.01-0.99). The framework uses command-line interfaces for modular execution and YAML-based batch processing for scalable experiments, focusing on binary classification with group fairness metrics.

## Key Results
- Reweighing achieves perfect group fairness (disparate impact = 1.0, statistical parity difference = 0.0) while preserving label distributions
- Learned Fair Representations produces ideal fairness metrics but often removes all positive labels, making data unrealistic
- Disparate Impact Remover shows minimal effectiveness in improving group fairness metrics
- FairPrep provides standardized, reproducible evaluation pipelines specifically for pre-processing fairness methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighing achieves perfect group fairness metrics while preserving data integrity through sample-level weight adjustment.
- Mechanism: Reweighing assigns weights to samples based on observed vs expected probabilities of (group, label) pairs, assuming independence between group membership and outcome. For a given pair (s, y), the weight W(s,y) = Pr(S=s) × Pr(Y=y) / Pr(S=s ∧ Y=y). This adjusts the training distribution to reduce disparity between groups without modifying features or labels.
- Core assumption: Assumes that fairness can be achieved through distribution reweighting without requiring changes to underlying data structure.
- Evidence anchors:
  - [abstract]: "Reweighing achieves perfect group fairness (disparate impact = 1.0, statistical parity difference = 0.0) while maintaining data integrity"
  - [section 3.2]: "RW improves group fairness across all datasets, achieving perfect disparate impact (1.0) and eliminating statistical parity difference (0.0), while leaving label distributions unchanged. This outcome aligns with its design, which adjusts instance weights without modifying features or labels"
  - [corpus]: "Task-tailored Pre-processing: Fair Downstream Supervised Learning" distinguishes data fairness vs task-tailored fairness approaches, supporting the distinction between data-level vs training-level interventions
- Break condition: Reweighing may fail if downstream models don't properly incorporate sample weights into their loss functions, or if the independence assumption between group membership and outcome is fundamentally violated.

### Mechanism 2
- Claim: Learned Fair Representations (LFR) can achieve ideal fairness metrics at the cost of extreme data distortion.
- Mechanism: LFR learns a latent representation Z that satisfies three objectives: (1) mapping from X to Z satisfies statistical parity, (2) Z retains information unrelated to sensitive attribute S, and (3) composed mapping from X to Z to Y approximates the original classifier. The representation learning process optimizes for fairness but may drastically alter label distributions.
- Core assumption: Assumes that learning fair latent representations will preserve sufficient task-relevant information for realistic downstream use.
- Evidence anchors:
  - [abstract]: "Learned Fair Representations, despite ideal fairness metrics, often removes positive labels making data unrealistic"
  - [section 3.2]: "LFR introduces substantial changes to the data, often removing all positive labels (Adult, COMPAS) or setting all to positive (German). While this results in ideal fairness metrics, such extreme shifts make the data unrealistic and potentially unusable for downstream tasks"
  - [corpus]: Weak corpus support - neighboring papers don't specifically discuss LFR's data distortion issues
- Break condition: LFR fails when the learned representation removes critical task information or produces degenerate label distributions that make data unusable for practical applications.

### Mechanism 3
- Claim: FairPrep's two-stage modular architecture enables systematic, reproducible evaluation of pre-processing fairness methods.
- Mechanism: The framework separates pre-processing (data transformation and fairness metric computation) from benchmarking (model training and performance/fairness evaluation). This decoupling allows independent execution via command-line interfaces, with YAML-based batch execution supporting scalable experiments. Thresholds are swept from 0.01 to 0.99 in 0.01 increments for fine-grained analysis.
- Core assumption: Assumes that separating pre-processing from model training provides clearer attribution of fairness improvements to data-level interventions.
- Evidence anchors:
  - [abstract]: "FairPrep enables standardized evaluation of data-level fairness interventions through a two-stage architecture: pre-processing (data transformation and fairness metric computation) and benchmarking (model training and performance/fairness evaluation)"
  - [section 2.1]: "FairPrep operates in two stages... These are executed independently via command-line interfaces, allowing modular and flexible usage"
  - [corpus]: "Fair-IRT" paper mentioned as bridging "both model- and data-level evaluation" supporting the value of integrated fairness evaluation frameworks
- Break condition: The architecture may fail to isolate pre-processing effects if there are complex interactions between data transformations and specific model architectures that aren't captured by the framework's design.

## Foundational Learning

- Concept: Disparate Impact
  - Why needed here: This metric measures group-level fairness by quantifying outcome ratios across sensitive groups. It's one of the primary evaluation metrics in FairPrep, with disparate impact = 1.0 indicating perfect group fairness. Understanding this is essential for interpreting the benchmark results.
  - Quick check question: If disparate impact is 0.788 for a dataset (as shown in COMPAS original data), does this indicate bias toward or against the unprivileged group?

- Concept: Statistical Parity Difference
  - Why needed here: This metric captures the difference in outcome rates between privileged and unprivileged groups. FairPrep uses both statistical parity difference (where 0.0 indicates fairness) and disparate impact (where 1.0 indicates fairness) as complementary measures.
  - Quick check question: What does a statistical parity difference of -0.195 (Adult Census original) tell you about the relationship between the sensitive attribute and positive outcomes?

- Concept: Pre-processing vs In-processing vs Post-processing Fairness Interventions
  - Why needed here: FairPrep specifically focuses on pre-processing methods that operate at the data level. Understanding this distinction is crucial for appreciating why FairPrep fills a gap—most existing tools focus on in-processing or post-processing approaches.
  - Quick check question: Why might pre-processing methods be particularly appealing for model-agnostic deployment and privacy compliance?

## Architecture Onboarding

- Component map: Dataset ingestion -> Pre-processing method application -> Data caching -> Model training -> Threshold sweep -> Metric recording -> Optimal threshold identification

- Critical path:
  1. Dataset ingestion with sensitive attribute specification
  2. Pre-processing method application and metric computation
  3. Data caching for both original and transformed versions
  4. Model training with holdout validation
  5. Threshold sweep (0.01-0.99) with metric recording at each point
  6. Optimal threshold identification and visualization

- Design tradeoffs:
  - **Binary classification focus**: Framework currently supports only binary classification, limiting applicability to multi-class or regression tasks
  - **Group fairness emphasis**: Individual fairness measures not currently implemented, restricting evaluation scope
  - **Modular execution**: Independent CLI stages provide flexibility but require manual pipeline orchestration for end-to-end workflows
  - **Threshold sweep granularity**: 0.01 increments provide fine-grained analysis but may be computationally expensive for large datasets

- Failure signatures:
  - **LFR degenerate outputs**: If positive label count drops dramatically or goes to 0, the LFR representation has failed to preserve task information
  - **DIR ineffectiveness**: If disparate impact and statistical parity difference show minimal change from original data, DIR's rank-preserving transformations are insufficient for the dataset
  - **Model weight incompatibility**: If Reweighing shows no fairness improvement in downstream model, the model likely doesn't support sample weighting in its loss function
  - **Threshold instability**: If fairness metrics fluctuate wildly across thresholds (as seen in original Adult data), the dataset has inherent fairness instability

- First 3 experiments:
  1. **Baseline establishment**: Run FairPrep on Adult Census dataset with all four pre-processing methods using Logistic Regression, comparing threshold-fairness curves to identify which methods stabilize fairness across decision boundaries
  2. **Data integrity validation**: Apply each pre-processing method to German Credit dataset and examine label distribution changes (positive/negative counts, base rate) to verify which methods preserve data realism
  3. **Cross-dataset generalization**: Test Reweighing across all five benchmark datasets (Adult, Bank Marketing, COMPAS, German Credit, MEPS) to confirm consistent perfect fairness achievement while tracking any dataset-specific utility losses

## Open Questions the Paper Calls Out
- **Open Question 1**: How can pre-processing fairness methods be effectively adapted and evaluated for multi-class classification and regression tasks? (Basis: "Future work will extend the framework to accommodate multi-class and regression tasks...")
- **Open Question 2**: Can Learned Fair Representations (LFR) be tuned or constrained to achieve fairness without producing unrealistic label distributions? (Basis: LFR removes all positive labels in some datasets)
- **Open Question 3**: Does combining Disparate Impact Remover with other pre-processing methods yield synergistic fairness improvements? (Basis: DIR "may benefit from being used in conjunction with other techniques")

## Limitations
- Binary classification only: Framework currently supports only binary classification tasks, limiting applicability to multi-class or regression problems
- Group fairness focus: Individual fairness measures are not implemented, restricting evaluation scope to group-level metrics
- Limited dataset scope: Evaluation is based on only five benchmark datasets, which may not represent all real-world scenarios

## Confidence
- **High Confidence**: Claims about Reweighing achieving perfect disparate impact (1.0) and statistical parity difference (0.0) while preserving label distributions are directly supported by the experimental results and align with the method's theoretical design.
- **Medium Confidence**: Claims about LFR's extreme data distortion and unrealistic outputs are supported by the results, but the generalizability to other LFR implementations or datasets remains uncertain.
- **Medium Confidence**: The assertion that FairPrep fills a critical gap in pre-processing evaluation tools is reasonable given the literature, but comprehensive analysis of existing tools' capabilities is not provided.

## Next Checks
1. Test Reweighing across additional tabular datasets (including non-binary classification tasks) to verify consistent fairness achievement and identify any dataset-specific limitations or utility trade-offs.
2. Experiment with modified LFR hyperparameters (e.g., different reconstruction weights, fairness regularization strengths) to determine if data realism can be improved without sacrificing fairness metrics.
3. Implement and test model architectures that properly support sample weighting (e.g., weighted loss functions) to validate whether Reweighing's fairness benefits are fully realized in practice or limited by downstream model implementation.