---
ver: rpa2
title: Regret minimization in Linear Bandits with offline data via extended D-optimal
  exploration
arxiv_id: '2508.08420'
source_url: https://arxiv.org/abs/2508.08420
tags:
- offline
- toff
- data
- regret
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles regret minimization in linear bandits with access
  to offline data. The key innovation is Offline-Online Phased Elimination (OOPE),
  which uses an extended D-optimal design to leverage offline data during exploration
  phases.
---

# Regret minimization in Linear Bandits with offline data via extended D-optimal exploration

## Quick Facts
- **arXiv ID:** 2508.08420
- **Source URL:** https://arxiv.org/abs/2508.08420
- **Reference count:** 40
- **Primary result:** OOPE algorithm achieves Õ(√d_eff T log(|A|T) + d²) regret using offline data via extended D-optimal design.

## Executive Summary
This paper introduces Offline-Online Phased Elimination (OOPE), a novel algorithm for regret minimization in linear bandits that leverages offline data. The key innovation is using an extended D-optimal design that incorporates offline data during exploration phases to reduce the effective problem dimension. By carefully balancing the weighting between offline and online samples across geometrically increasing phases, OOPE achieves significant regret reduction when offline data is abundant and well-explored. The paper also provides matching minimax lower bounds and improves the high-dimensional regret term using Frank-Wolfe approximation.

## Method Summary
The method combines phased elimination for online learning with extended D-optimal design to incorporate offline data. The algorithm operates in phases, computing an effective dimension from the offline data's spectral properties, then solving an optimization problem that maximizes the information gain while considering both online and offline samples. In each phase, it determines how many samples to pull from each arm using both offline and online data, constructs an OLS estimate of the reward parameter, and eliminates suboptimal arms based on confidence intervals. The weighting between offline and online data is carefully scheduled to balance sample consumption and regret cost.

## Key Results
- OOPE achieves Õ(√d_eff T log(|A|T) + d²) regret, where d_eff is the effective dimension measuring poorly explored directions
- The regret bound scales inversely with √d_eff when offline data is abundant and well-explored
- Frank-Wolfe approximation improves the d² term to O(d²/d_eff) in high-dimensional settings
- Matching minimax lower bounds establish optimality in certain regimes
- Algorithm guarantees statistical independence by incorporating previously unseen offline data in each phase

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating offline data into an extended D-optimal design allows the algorithm to focus exploration exclusively on poorly understood directions, reducing sample complexity proportional to the effective dimension.
- **Mechanism:** The algorithm maximizes $\log \det((1-\alpha_l)\mathbf{V}_{\pi} + \alpha_l \mathbf{V}_{\pi_{\text{off}}})$ instead of standard $\log \det(\mathbf{V}_{\pi})$, treating the offline Gram matrix as a "pre-loaded" information background to avoid redundant sampling.
- **Core assumption:** Offline data was generated by a non-adaptive policy, ensuring the eigen-spectrum of the offline Gram matrix is unbiased.
- **Evidence anchors:** [abstract] mentions extended D-optimal design to leverage offline information; [section 4] defines the optimization objective and explains it minimizes confidence ellipsoid volume.
- **Break condition:** If offline data is adaptive, the eigen-spectrum may correlate with noise, violating independence assumptions.

### Mechanism 2
- **Claim:** A static weighting ratio $\alpha_l = T_{\text{off}}/(T_{\text{off}} + T)$ balances finite offline data consumption against online regret cost across phases.
- **Mechanism:** The algorithm decides how many offline samples to "burn" in each phase. Too high wastes offline samples early; too low forces expensive online exploration when cheap offline data would suffice.
- **Core assumption:** Total offline samples $T_{\text{off}}$ and online horizon $T$ are known a priori.
- **Evidence anchors:** [section 4] Proposition 4.1 optimizes $\alpha_l$ schedule; text describes trade-off between offline exhaustion and online sample increases.
- **Break condition:** If offline data distribution significantly differs from online environment, high $\alpha$ over-relies on misleading data.

### Mechanism 3
- **Claim:** Frank-Wolfe iterations reduce support size from $O(d^2)$ to $\tilde{O}(d)$, mitigating additive regret terms in high dimensions.
- **Mechanism:** Exact extended D-optimal design may require pulling up to $d(d+1)/2$ unique arms. FW approximation finds an $\epsilon$-optimal solution with fewer support points, trading slight confidence width increase for massive support reduction.
- **Core assumption:** Kumar & Yildirim initialization provides feasible starting point close to optimal volume.
- **Evidence anchors:** [section 5] Introduces OOPE-FW and bounds iteration count; [abstract] notes improvement from $O(d^2)$ to $O(d^2/d_{\text{eff}})$.
- **Break condition:** If initialization fails to capture live arms geometry, FW may converge slowly or stall.

## Foundational Learning

**Concept: Linear Bandits & Phased Elimination**
- **Why needed:** Base paradigm where algorithm eliminates suboptimal arms in rounds based on confidence intervals.
- **Quick check:** In phased elimination, does an arm get eliminated if its estimated reward is slightly worse than best, or only if statistically significantly worse? (Answer: The latter; if gap exceeds $2\epsilon_l$).

**Concept: Optimal Experimental Design (D-Optimality)**
- **Why needed:** Core logic of maximizing $\log \det(\cdot)$ to get most information per sample.
- **Quick check:** Does D-optimal design aim to minimize volume of confidence ellipsoid or variance of specific arm? (Answer: Minimize volume/determinant).

**Concept: Gram Matrix Eigen-spectrum**
- **Why needed:** Entire theory relies on $d_{\text{eff}}$ defined via eigenvalues; connects "small eigenvalues" to "poorly explored directions".
- **Quick check:** If smallest eigenvalue of offline Gram matrix is 0, what does that imply? (Answer: Zero information in at least one direction).

## Architecture Onboarding

**Component map:** Input Layer -> Pre-processor -> Design Optimizer -> Sampling Controller -> Estimator -> Eliminator

**Critical path:** Design Optimizer is the bottleneck. In OOPE, this is convex optimization over probability simplex. For huge action sets, requires Frank-Wolfe or first-order methods rather than enumeration.

**Design tradeoffs:**
- **OOPE vs. OOPE-FW:** Use OOPE for small-to-medium $d$ where exact $O(d^2)$ support is acceptable. Use OOPE-FW for high dimensions ($d \gtrsim \sqrt{T}$) where $d^2$ dominates regret budget.
- **$\alpha_l$ Sensitivity:** Default $\alpha_l = T_{\text{off}}/(T_{\text{off}}+T)$ assumes valuable offline data. For high distribution shift, may need to clamp $\alpha$ lower manually.

**Failure signatures:**
- **Poorly Explored Offline Data:** If $d_{\text{eff}} \approx d$, algorithm degrades to standard Phased Elimination with no speedup.
- **Exhausted Offline Buffer:** Proposition 4.3 guarantees algorithm won't ask for more offline samples than available before last phase. Reusing same offline samples in multiple phases breaks concentration bounds.

**First 3 experiments:**
1. **Sanity Check (Varying $T_{\text{off}}$):** Compare OOPE with $T_{\text{off}} = 0$ vs. $T_{\text{off}} = 10 \times T$. Verify regret scales down inversely with $\sqrt{T_{\text{off}}}$ as $d_{\text{eff}}$ drops.
2. **Spectral Quality Test:** Generate two offline datasets with equal size: one well-explored (uniform over arms) and one poorly explored (concentrated subset). Plot regret against calculated $d_{\text{eff}}$ for both.
3. **High-Dimension Stress Test (OOPE-FW):** Set $d \approx 50$ or $100$ and compare regret of standard OOPE (should have $d^2$ penalty) against OOPE-FW to verify FW reduces regret by reducing support size.

## Open Questions the Paper Calls Out

**Open Question 1:** How can regret minimization analysis be extended to handle offline data generated by adaptive policies rather than current restriction to non-adaptive designs?
- **Basis:** Conclusion explicitly states need to relax assumption and study adaptive policies.
- **Why unresolved:** Current framework relies on Assumption 3.2 (fixed design) to avoid complexities in analyzing offline Gram matrix without knowledge of offline policy's dependencies.
- **What evidence would resolve it:** Algorithm achieving sub-linear regret using offline data collected by standard adaptive algorithms (e.g., LinUCB or Thompson Sampling).

**Open Question 2:** Can algorithm and bounds be modified to accommodate known, bounded distribution shift in reward parameter $\theta^*$ between offline and online phases?
- **Basis:** Conclusion suggests modeling online data from slightly shifted $\theta^*$ with known shift budget.
- **Why unresolved:** Current work relies on Assumption 3.2 assuming constant reward-generating distribution throughout offline and online stages.
- **What evidence would resolve it:** Modified OOPE incorporating "shift budget" into confidence widths with regret bound degrading gracefully with shift magnitude.

**Open Question 3:** Can refined minimax lower bounds be established for non-uniformly explored offline data to close theoretical gap with upper bounds?
- **Basis:** Conclusion asks for refined lower bounds matched in non-uniformly explored data; current bounds only tight in extreme regimes.
- **Why unresolved:** Remark 4.11 notes multiplicative gap between upper and lower bounds in intermediate regime where offline data explores some directions well but others poorly.
- **What evidence would resolve it:** Lower bound construction depending on effective dimension ($d_{\text{eff}}$) matching $\tilde{O}(\sqrt{d_{\text{eff}}T})$ upper bound for all possible eigenspectra.

## Limitations

- **Assumption Dependency:** Effectiveness critically depends on offline data being generated by non-adaptive policy; adaptive data violates independence assumptions.
- **Distribution Shift Sensitivity:** Paper doesn't extensively discuss robustness to distribution shift between offline and online data, a common challenge in offline-to-online settings.
- **Practical Frank-Wolfe Performance:** While theoretical guarantees exist, practical impact of Frank-Wolfe approximation versus exact optimization remains empirical.

## Confidence

- **High Confidence:** Regret bound Õ(√d_eff T log(|A|T) + d²) is rigorously derived; mechanism of leveraging offline data through extended D-optimal design is well-founded.
- **Medium Confidence:** Matching minimax lower bounds depending on offline data quality are significant theoretical contribution, but practical tightness across all regimes harder to verify.
- **Medium Confidence:** Frank-Wolfe approximation's improvement from O(d²) to O(d²/d_eff) is theoretically sound, but practical performance and initialization impact need further empirical assessment.

## Next Checks

1. **Distribution Shift Robustness:** Design experiment with slightly shifted offline data distribution. Measure OOPE performance degradation with different α_l settings to identify breaking point of non-adaptive assumption.
2. **Practical Frank-Wolfe Convergence:** Implement OOPE-FW and compare regret and support size reduction against exact OOPE across range of problem dimensions (d) to validate theoretical iteration bounds translate to practical benefits.
3. **Spectral Quality Dependency:** Generate offline datasets with controlled eigenvalue spectra (many small eigenvalues vs. more uniform). Run OOPE on both and plot achieved regret against calculated d_eff to empirically confirm it predicts performance.