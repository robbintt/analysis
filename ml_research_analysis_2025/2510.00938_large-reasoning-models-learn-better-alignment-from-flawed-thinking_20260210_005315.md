---
ver: rpa2
title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
arxiv_id: '2510.00938'
source_url: https://arxiv.org/abs/2510.00938
tags:
- reasoning
- safety
- recap
- arxiv
- prefilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECAP is a principled reinforcement learning method that teaches
  large reasoning models to override flawed reasoning trajectories and reroute to
  safe, helpful responses. It constructs counter-aligned reasoning prefills that induce
  models to "think unsafe" for harmful prompts and "think overly conservative" for
  benign ones, training them to recover appropriate reasoning instead of following
  the flawed prefix.
---

# Large Reasoning Models Learn Better Alignment from Flawed Thinking

## Quick Facts
- **arXiv ID**: 2510.00938
- **Source URL**: https://arxiv.org/abs/2510.00938
- **Reference count**: 40
- **Primary result**: RECAP improves safety by +12.3% on direct harmful benchmarks and +21.0% on jailbreaking, while reducing overrefusal by +7.8% and preserving math reasoning capability (+0.9%)

## Executive Summary
RECAP is a principled reinforcement learning method that teaches large reasoning models to override flawed reasoning trajectories and reroute to safe, helpful responses. It constructs counter-aligned reasoning prefills that induce models to "think unsafe" for harmful prompts and "think overly conservative" for benign ones, training them to recover appropriate reasoning instead of following the flawed prefix. Extensive experiments show RECAP substantially improves safety, reduces overrefusal, and preserves math reasoning capability while maintaining inference-time token budget.

## Method Summary
RECAP employs reinforcement learning to teach large reasoning models to recognize and override flawed reasoning trajectories. The method generates counter-aligned reasoning prefills that deliberately lead the model toward either unsafe conclusions for harmful prompts or overly conservative responses for benign prompts. During training, the model learns to identify these flawed reasoning paths and reroute toward appropriate, safe responses. This approach enables models to engage in self-reflection and revise their reasoning mid-trajectory when they detect mistakes or unsafe patterns, rather than blindly following initial reasoning prefixes.

## Key Results
- Safety improvements of +12.3% on direct harmful benchmarks and +21.0% on jailbreaking attacks
- Overrefusal reduction of +7.8% on helpfulness score while maintaining safety
- Math reasoning capability preserved with +0.9% improvement on MATH-500 benchmark
- Models trained with RECAP engage in self-reflection far more often, revising unsafe or mistaken reasoning mid-trajectory

## Why This Works (Mechanism)
RECAP works by teaching models to recognize and override flawed reasoning trajectories through explicit training on counter-aligned reasoning prefills. By exposing models to deliberately flawed reasoning patterns during training, RECAP enables them to develop the capability to identify when their reasoning is going astray and make course corrections. This creates a self-reflective capability where models can detect and recover from reasoning mistakes in real-time, rather than being locked into potentially harmful or overly conservative reasoning paths. The method leverages the inherent reasoning capabilities of large language models while adding a layer of meta-reasoning about the quality and safety of the reasoning process itself.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - to align model behavior with human preferences; Quick check - verify reward model accurately captures safety and helpfulness preferences
- **Counterfactual Reasoning**: Why needed - to generate synthetic training examples that expose models to flawed reasoning patterns; Quick check - ensure counter-aligned prefills effectively trigger the intended flawed reasoning
- **Safety Alignment**: Why needed - to prevent harmful outputs while maintaining helpfulness; Quick check - validate safety improvements across multiple harm categories and attack types
- **Reasoning Chain Optimization**: Why needed - to maintain reasoning quality while improving safety; Quick check - confirm math reasoning capability preservation across diverse mathematical tasks

## Architecture Onboarding
**Component Map**: RECAP Framework -> Counter-aligned Prefill Generator -> RL Training Pipeline -> Self-reflection Capability
**Critical Path**: Counter-aligned prefills are generated → Model processes flawed reasoning → RL reward signals reinforce correct recovery behavior → Model learns to override flawed trajectories
**Design Tradeoffs**: Safety vs. helpfulness (avoiding overrefusal) vs. reasoning capability preservation vs. inference efficiency
**Failure Signatures**: Model follows flawed reasoning without recovery, produces overly conservative responses, or fails to maintain reasoning quality on mathematical tasks
**3 First Experiments**: 1) Test counter-aligned prefill generation on diverse prompt types, 2) Validate RL training converges to desired recovery behavior, 3) Measure safety improvements on curated harmful/non-harmful benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology relies on curated benchmarks rather than adversarial human-in-the-loop testing, leaving real-world robustness uncertain
- Synthetic counter-aligned reasoning prefills may not capture full diversity of adversarial reasoning patterns
- Math reasoning preservation claims based on single MATH-500 benchmark may not generalize to broader mathematical reasoning tasks

## Confidence
- **High confidence**: Core mechanism of counter-aligned reasoning prefix training is technically sound; self-reflection analysis shows clear behavioral differences
- **Medium confidence**: Safety improvements are robust across multiple benchmarks but may not generalize to all harm categories
- **Medium confidence**: Overrefusal reduction is measured but tradeoff between safety and helpfulness remains context-dependent
- **Low confidence**: Claims about maintaining inference-time efficiency and budget constraints need broader validation across diverse use cases

## Next Checks
1. Conduct red-team adversarial testing with human experts to probe for safety bypasses not captured in automated benchmarks
2. Test generalization of math reasoning preservation across multiple mathematical domains beyond MATH-500
3. Measure real-world inference costs and latency under production workloads to verify claimed efficiency benefits