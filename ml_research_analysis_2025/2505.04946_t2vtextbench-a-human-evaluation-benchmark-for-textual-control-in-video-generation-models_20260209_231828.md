---
ver: rpa2
title: 'T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation
  Models'
arxiv_id: '2505.04946'
source_url: https://arxiv.org/abs/2505.04946
tags:
- text
- prompt
- uni00000013
- uni00000011
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T2VTextBench is a human evaluation benchmark for on-screen text
  fidelity in text-to-video generation models. The benchmark evaluates ten state-of-the-art
  models across 73 prompts covering stepwise visualization, app/web UI simulation,
  everyday digital moments, cinematic scenes, math-related, and multilingual (Chinese)
  scenarios.
---

# T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models

## Quick Facts
- arXiv ID: 2505.04946
- Source URL: https://arxiv.org/abs/2505.04946
- Reference count: 30
- Primary result: Human evaluation benchmark revealing all tested text-to-video models struggle with on-screen text fidelity, with highest average score of 0.37 (Sora)

## Executive Summary
T2VTextBench is a human evaluation benchmark designed to assess on-screen text fidelity in text-to-video generation models. The benchmark evaluates ten state-of-the-art models across 73 prompts covering stepwise visualization, app/web UI simulation, everyday digital moments, cinematic scenes, math-related, and multilingual scenarios. Human annotators scored outputs on a 0-1 scale for text accuracy and temporal consistency. The study reveals that all models struggle with text generation, particularly with geometric transformations and random character sequences, suggesting fundamental limitations in current architectures' ability to understand and render precise text.

## Method Summary
The benchmark employs human evaluators who score video outputs on a binary 0-1 scale for two criteria: text accuracy (whether generated text matches the prompt) and temporal consistency (whether text remains stable throughout the video). Ten text-to-video models were evaluated across 73 prompts divided into six categories: stepwise visualization, app/web UI simulation, everyday digital moments, cinematic scenes, math-related, and Chinese. The human evaluation process involved annotators viewing videos and determining whether the on-screen text met the specified requirements, with scores aggregated across multiple annotators for reliability.

## Key Results
- All evaluated models scored poorly on text fidelity, with Sora achieving the highest average score of 0.37
- Models performed worst on geometric transformations and random character sequences, indicating reliance on word-level memorization
- Wan 2.1 emerged as the most cost-effective option, achieving second-best performance at no API cost
- Temporal consistency remains challenging across all models, with text often degrading or changing over video duration

## Why This Works (Mechanism)
The benchmark works by directly measuring human-perceived text fidelity through binary scoring, capturing both the accuracy of generated text and its temporal stability. This human-centered approach identifies specific failure modes that automated metrics might miss, such as subtle text degradation over time or incorrect character rendering that still passes OCR checks. The 0-1 scoring system simplifies evaluation while maintaining granularity through aggregation across multiple annotators and prompt categories.

## Foundational Learning
1. **Text-to-video generation architectures** - Understanding how diffusion transformers and other architectures handle text conditioning is essential for interpreting model limitations.
   - Why needed: Provides context for why models struggle with arbitrary text generation
   - Quick check: Can you explain the difference between text-to-image and text-to-video generation pipelines?

2. **Human evaluation methodologies** - The benchmark relies on subjective human scoring to assess text fidelity.
   - Why needed: Helps understand the reliability and limitations of the evaluation approach
   - Quick check: What are the advantages and disadvantages of binary vs. Likert scale human evaluation?

3. **OCR and automated text evaluation** - The paper notes limitations of automated metrics for this task.
   - Why needed: Contextualizes why human evaluation was necessary and what gaps exist in current automated approaches
   - Quick check: Can you name three automated metrics that might be used for text evaluation in images/videos?

## Architecture Onboarding

**Component map:** Text prompt -> Video generation model -> Output video -> Human evaluation scoring

**Critical path:** The critical path is the human evaluation process, where annotators must view each generated video and score it on text accuracy and temporal consistency. This creates a bottleneck that limits scalability and introduces subjective variability.

**Design tradeoffs:** The benchmark trades scalability for accuracy by using human evaluators instead of automated metrics. While this provides nuanced assessment of text fidelity, it cannot easily scale to millions of test cases and introduces inter-annotator variability.

**Failure signatures:** Models consistently fail on prompts requiring geometric transformations of text, random character sequences, and maintaining temporal consistency. These failures suggest models rely on memorizing common words rather than understanding character composition and spatial relationships.

**3 first experiments:**
1. Test inter-annotator reliability by having multiple evaluators score the same videos to establish agreement levels
2. Conduct ablation studies removing different model components to identify which architectures perform best on text generation
3. Evaluate whether fine-tuning on text-specific datasets improves performance on geometric transformations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can text-to-video architectures be modified to generate arbitrary character sequences accurately rather than relying on word-level memorization from training data?
- Basis in paper: [explicit] Observation 4.5 notes that models perform worst on "random character" prompts, suggesting they "primarily memorize textual training data at the word level" rather than understanding character composition.
- Why unresolved: The paper identifies this reliance on memorization as a fundamental gap but does not propose architectural changes to enforce character-level generation.
- What evidence would resolve it: A modified diffusion transformer or control mechanism that achieves high scores (>0.8) on the benchmark's random character prompts, indicating true compositional understanding.

### Open Question 2
- Question: What specific training constraints or inductive biases are required to preserve text fidelity during complex geometric transformations (e.g., rotation, perspective shifts)?
- Basis in paper: [explicit] Observation 4.3 and 4.4 highlight that models consistently score lowest on geometric transformations compared to visual or structural changes, identifying a distinct failure mode in maintaining text structure under motion.
- Why unresolved: The paper demonstrates the failure through ablation studies but does not investigate whether this is due to a lack of spatial invariance in the latent space or insufficient training data.
- What evidence would resolve it: A study showing that augmenting training data with spatial transformations or introducing geometric consistency losses significantly improves scores on the "Geometric" prompt category.

### Open Question 3
- Question: Can automated evaluation metrics (e.g., using VLMs or OCR) be developed that robustly correlate with human judgments of temporal text consistency?
- Basis in paper: [explicit] Appendix B (Limitation) states that the reliance on human evaluators "may not be scalable for testing millions of different prompts."
- Why unresolved: The benchmark relies entirely on a 0-1 scale human evaluation standard; the paper does not validate any automated metric as a scalable proxy for this specific human judgment.
- What evidence would resolve it: A correlation analysis demonstrating that a proposed automated metric achieves high agreement (e.g., Spearman > 0.85) with the human annotator scores across the 73 benchmark prompts.

## Limitations
- Human evaluation methodology relies on subjective scoring, introducing inter-annotator variability that wasn't fully characterized
- Benchmark covers 73 prompts across six categories, representing a limited sample of real-world text rendering use cases
- Results may not generalize beyond the specific ten models tested, limiting broader applicability

## Confidence
- **High confidence**: All evaluated models struggle with text generation fidelity; Wan 2.1 provides strong performance-to-cost ratio
- **Medium confidence**: Models rely on word-level memorization rather than true text understanding; temporal consistency remains challenging across all models
- **Low confidence**: Benchmark results generalize to broader text-to-video generation landscape; subjective scoring captures true text rendering quality

## Next Checks
1. Conduct inter-annotator reliability analysis with Fleiss' kappa to quantify agreement levels and identify ambiguous scoring cases
2. Expand benchmark to include at least 50 additional prompts spanning diverse text styles, languages, and visual contexts
3. Test whether architectural modifications (e.g., dedicated text rendering modules) improve performance on geometric transformations and random character sequences