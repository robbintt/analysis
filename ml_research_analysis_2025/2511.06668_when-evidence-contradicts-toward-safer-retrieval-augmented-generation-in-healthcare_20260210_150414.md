---
ver: rpa2
title: 'When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in
  Healthcare'
arxiv_id: '2511.06668'
source_url: https://arxiv.org/abs/2511.06668
tags:
- retrieval
- documents
- account
- medical
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates how contradictory evidence
  in retrieval-augmented generation (RAG) systems impacts medical question answering.
  Using a novel dataset linking TGA consumer medicine information with PubMed abstracts,
  the research investigates five LLMs' performance when retrieving temporally diverse
  and potentially contradictory medical literature.
---

# When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare

## Quick Facts
- arXiv ID: 2511.06668
- Source URL: https://arxiv.org/abs/2511.06668
- Reference count: 0
- Primary result: Contradictory evidence degrades medical RAG performance by 18.2% R1 score decline

## Executive Summary
This study systematically evaluates how contradictory evidence in retrieval-augmented generation (RAG) systems impacts medical question answering. Using a novel dataset linking TGA consumer medicine information with PubMed abstracts, the research investigates five LLMs' performance when retrieving temporally diverse and potentially contradictory medical literature. Results show that contradictions consistently degrade model performance, with average R1 scores declining by 18.2% when contradictory documents are present. The analysis reveals that even semantically similar documents often contain conflicting information, with over 5,400 document pairs exhibiting high contradiction scores. These findings demonstrate that retrieval similarity alone is insufficient for reliable medical RAG, highlighting the need for contradiction-aware filtering strategies in high-stakes healthcare applications where factual accuracy is critical.

## Method Summary
The study creates a novel dataset by pairing 1,476 TGA medicines (generating 8,856 queries from section headings) with ~400k PubMed abstracts filtered to 91,662 PMIDs (28,873 unique). Queries are formulated using three-tier strategies and executed via PubMed API. Retrieved documents undergo temporal-citation balanced selection (Algorithm 1) with max 20 abstracts per query. The system uses BAAI/bge-small-en-v1.5 embeddings (384d) indexed via FAISS HNSW, applies MMR with temporal diversity (λ=0.7, α=0.7), and detects contradictions using SPECTER embeddings + PubMedBERT-MNLI-MedNLI classifier (θ_sent=0.75). Five LLMs (Falcon3-7B, Gemma-3-270m, GPT-OSS-20B, Med-LLaMA3-8B, Mixtral-8x7B) generate answers under three retrieval conditions (most-similar, most-contradictory, least-contradictory), evaluated using ROUGE, BERTScore, VSIM, JSD, and KLD against TGA ground-truth.

## Key Results
- Contradictory evidence consistently degrades RAG performance, with average R1 scores declining by 18.2% when contradictory documents are present
- Semantic similarity is insufficient for contradiction filtering, with 5,492 document pairs showing high contradiction scores (0.8-1.0) despite moderate diversity-aware scores (0.2-0.4)
- Temporal distribution of contradictions shows increasing conflict rates in recent literature (2010-2025), with higher contradiction bins accounting for around half or more of all documents per interval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contradictory evidence in retrieved contexts degrades RAG performance proportionally to contradiction intensity.
- Mechanism: When models receive documents with high contradiction scores (CNT ≥ 0.8), they cannot reliably reconcile conflicting claims, leading to inconsistent or incorrect outputs. The 18.2% R1 decline shows this is systematic, not random noise.
- Core assumption: The NLI-based contradiction scoring (PubMedBERT-MNLI-MedNLI) accurately captures semantically meaningful conflicts, not just surface-level differences.
- Evidence anchors:
  - [abstract] "Results show that contradictions consistently degrade model performance, with average R1 scores declining by 18.2% when contradictory documents are present."
  - [section 5.1] "Across models, performance consistently degraded when moving from most-similar to most-contradictory retrieval conditions."
  - [corpus] "Retrieval-Augmented Generation with Conflicting Evidence" (arXiv:2504.13079) corroborates that handling ambiguous and conflicting information remains an open challenge, though lacks quantitative degradation metrics.
- Break condition: If contradiction scores were uncorrelated with performance drops, or if lexical metrics improved while semantic metrics declined, the mechanism would be invalid.

### Mechanism 2
- Claim: Semantic similarity is insufficient to filter contradictions because high-similarity document pairs frequently contain conflicting information.
- Mechanism: Documents addressing the same medical topic (e.g., drug interactions) from different time periods or study designs achieve high embedding similarity but diverge in conclusions. The 5,492 documents at intersection of CNT [0.8–1.0] and similarity [0.2–0.4] demonstrate this decoupling.
- Core assumption: Embedding models (BAAI/bge-small-en-v1.5, SPECTER) capture topical relevance accurately but do not encode truth values or temporal validity.
- Evidence anchors:
  - [abstract] "These findings demonstrate that retrieval similarity alone is insufficient for reliable medical RAG."
  - [section 5.2] "The highest frequency of documents (5,492) falls in the intersection of high contradiction scores (0.8-1.0) and moderate diversity-aware scores (0.2-0.4)."
  - [corpus] "Rethinking All Evidence" (arXiv:2507.01281) similarly identifies knowledge conflicts from noisy retrieved content as a reliability threat, supporting the general problem.
- Break condition: If high-similarity documents showed uniformly low contradiction scores, the mechanism would fail.

### Mechanism 3
- Claim: Temporal distribution of contradictions has shifted toward higher conflict rates in recent literature.
- Mechanism: Rapid expansion of biomedical research and overturning of earlier clinical consensus causes later publication years (2010–2025) to show proportionally more documents in high-contradiction bins. This suggests filtering by recency alone is insufficient.
- Core assumption: The contradiction detection framework is not biased toward modern language patterns; the observed shift reflects genuine changes in research consistency.
- Evidence anchors:
  - [section 5.3] "From the early 2000s onwards, the share of documents in higher contradiction bins ([0.6–0.8) and [0.8–1]) rises steadily. By 2010–2025, these bins account for around half or more of all documents per interval."
  - [abstract] "investigates... retrieval of PubMed abstracts... stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence"
  - [corpus] "HoH: A Dynamic Benchmark" (arXiv:2503.04800) confirms outdated information in knowledge bases is a critical challenge, though focuses on staleness rather than contradiction density.
- Break condition: If older documents showed equally high contradiction rates, the temporal mechanism would not hold.

## Foundational Learning

- Concept: **Maximal Marginal Relevance (MMR)**
  - Why needed here: The paper uses MMR extended with temporal diversity to balance relevance and redundancy in retrieval. Understanding MMR is prerequisite to interpreting the diversity-aware scoring function (Equation 6–8).
  - Quick check question: If λ = 1.0 in MMR, what happens to diversity consideration? (Answer: Diversity term is ignored; ranking becomes pure query similarity.)

- Concept: **Natural Language Inference (NLI) for contradiction detection**
  - Why needed here: The contradiction scoring framework (Section 3.4) uses a biomedical NLI classifier to assign entailment/neutral/contradiction probabilities to sentence pairs. This is the core mechanism for quantifying conflicts.
  - Quick check question: Why does the framework use sentence-level rather than document-level NLI classification? (Answer: Document-level contradictions may be missed; sentence pairs provide finer-grained conflict signals.)

- Concept: **ROUGE and BERTScore evaluation for RAG**
  - Why needed here: The paper evaluates RAG outputs using lexical overlap (ROUGE-1/2/L) and semantic similarity (BERTScore). Understanding these metrics is necessary to interpret the 18.2% degradation finding.
  - Quick check question: Why might BERTScore degrade less than ROUGE under contradictory evidence? (Answer: Semantic embeddings preserve conceptual meaning even when exact lexical matches decrease.)

## Architecture Onboarding

- Component map:
  Query formulation -> PubMed API search -> Temporal-citation selection -> BAAI/bge-small-en-v1.5 embeddings -> FAISS HNSW indexing -> MMR+temporal scoring -> SPECTER+PubMedBERT NLI contradiction detection -> Three retrieval conditions (most-similar, most-contradictory, least-contradictory) -> Five LLMs generation -> ROUGE/BERTScore evaluation

- Critical path:
  1. Query expansion must yield PubMed PMIDs; if API returns empty, downstream components fail.
  2. Temporal-citation selection must produce ≥K documents; otherwise retrieval conditions cannot be instantiated.
  3. Contradiction scoring requires NLI model loaded; failure here defaults to similarity-only ranking.
  4. LLM inference requires context window ≥ concatenated document length; truncation may bias results.

- Design tradeoffs:
  - **Citation count as quality proxy**: Prioritizes influential research but may suppress recent valid findings with low citations.
  - **Sentence-level NLI**: Captures fine-grained conflicts but may miss holistic document-level contradictions.
  - **Fixed K=5**: Standardizes comparison but may not reflect optimal context sizes per query.
  - **Temperature=0**: Ensures reproducibility but eliminates uncertainty signaling in model outputs.

- Failure signatures:
  - **ROUGE R1 near 0 with high BERTScore**: Model generating semantically correct but lexically divergent responses (possible with Gemma-3-270m).
  - **High KLD in most-contradictory condition**: Model producing out-of-distribution outputs when overwhelmed by conflicts.
  - **Contradiction scores all near 0 or 1**: NLI classifier miscalibrated or embedding threshold θ_sent=0.75 inappropriate.

- First 3 experiments:
  1. **Reproduce contradiction scoring on 100 random document pairs**: Verify NLI classifier produces expected entailment/neutral/contradiction distributions; check calibration.
  2. **Ablate temporal component (α=1.0 vs. α=0.7)**: Isolate contribution of temporal diversity to retrieval quality; expect more homogeneous year distribution when α=0.7.
  3. **Compare Med-LLaMA3 vs. base LLaMA3 on most-contradictory condition**: Test whether domain fine-tuning helps contradiction handling; if Med-LLaMA3 shows smaller degradation, fine-tuning provides resilience.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can argumentation mining and evidence synthesis techniques effectively resolve contradictions in retrieved medical documents, and what performance gains would they yield over current similarity-based retrieval?
- Basis in paper: [explicit] "Future work should explore more sophisticated contradiction resolution strategies, including argumentation mining and evidence synthesis techniques."
- Why unresolved: The paper quantifies the problem (18.2% R1 degradation with contradictions) but only detects contradictions via NLI classification without attempting automated resolution.
- What evidence would resolve it: Comparative experiments integrating argumentation frameworks into the RAG pipeline, measuring whether resolution strategies reduce the performance gap between most-contradictory and most-similar conditions.

### Open Question 2
- Question: How can temporal reasoning and uncertainty quantification be integrated into medical RAG architectures to explicitly model evidence strength and the evolution of clinical consensus?
- Basis in paper: [explicit] The conclusion states "Future systems must therefore integrate contradiction detection and resolution, leveraging temporal reasoning and uncertainty quantification."
- Why unresolved: The current framework includes temporal diversity in retrieval scoring but does not model how evidence strength changes over time or communicate uncertainty to users.
- What evidence would resolve it: Architecture designs that weight evidence by recency, citation impact, and consensus status, with evaluation showing improved accuracy on queries involving temporally evolving knowledge.

### Open Question 3
- Question: Do document-level contradictions that bypass sentence-level NLI detection significantly impact RAG performance, and what detection architectures can capture them?
- Basis in paper: [explicit] "First, our contradiction detection relies on sentence-level, which may miss document-level contradictions."
- Why unresolved: The methodology uses sentence-pair NLI classification, so contradictions arising from logical inference across multiple sentences remain undetected.
- What evidence would resolve it: Annotation of document-level contradictions in the dataset, followed by experiments comparing sentence-level vs. document-level contradiction detection accuracy and their correlation with RAG performance degradation.

### Open Question 4
- Question: Can incorporating clinical guidelines and expert knowledge bases as authoritative sources systematically resolve contradictions in retrieved PubMed evidence?
- Basis in paper: [explicit] "incorporating clinical guidelines and expert knowledge bases could help resolve contradictions by establishing authoritative sources."
- Why unresolved: The study uses TGA consumer information and PubMed abstracts but does not integrate structured clinical guidelines as arbitration mechanisms.
- What evidence would resolve it: Hybrid retrieval systems that prioritize guideline-backed evidence over abstracts when contradictions arise, with human evaluation of answer accuracy and clinical appropriateness.

## Limitations
- NLI-based contradiction detection framework relies on sentence-level classification, potentially missing document-level contradictions or nuanced contextual conflicts in medical literature
- Use of citation count as quality proxy in Algorithm 1 may systematically favor older, established research over recent but valid findings, potentially introducing temporal bias
- Study assumes PubMedBERT-MNLI-MedNLI's calibration remains stable across temporal spans (1975-2025), but shifting language patterns could affect contradiction score reliability

## Confidence
- **High confidence** in core finding that contradictory evidence degrades RAG performance (18.2% R1 decline): Directly supported by quantitative results across five models and three retrieval conditions
- **Medium confidence** in mechanism that semantic similarity is insufficient for contradiction filtering: Document pairs demonstrate decoupling, but practical impact requires further validation
- **Low confidence** in temporal shift hypothesis: Observed trend could reflect changes in research practices rather than actual increases in conflicting evidence

## Next Checks
1. **Temporal bias validation**: Re-run contradiction scoring with time-stratified sampling to control for potential language model calibration drift across publication years
2. **Document-level contradiction detection**: Implement and compare document-level NLI classification against current sentence-level approach to assess whether finer-grained analysis improves contradiction detection accuracy
3. **Alternative quality metrics**: Replace citation count with alternative quality indicators (e.g., journal impact factor, study design indicators) in Algorithm 1 to test whether temporal distribution of contradictions persists under different selection criteria