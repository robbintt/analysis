---
ver: rpa2
title: Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning
arxiv_id: '2512.19777'
source_url: https://arxiv.org/abs/2512.19777
tags:
- devices
- learning
- digital
- codebook
- quantisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the communication bottleneck in federated edge
  learning (FEEL) by proposing a learned digital over-the-air computation framework.
  The method integrates an unsourced random access (URA) codebook with vector quantization
  and AMP-DA-Net, an unrolled AMP-style decoder trained end-to-end with the digital
  codebook and local training statistics.
---

# Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning

## Quick Facts
- **arXiv ID**: 2512.19777
- **Source URL**: https://arxiv.org/abs/2512.19777
- **Reference count**: 40
- **Primary result**: Extends reliable digital OTA operation by more than 10 dB into low SNR regimes while maintaining or improving performance across the full SNR range.

## Executive Summary
This work addresses the communication bottleneck in federated edge learning (FEEL) by proposing a learned digital over-the-air computation framework. The method integrates an unsourced random access (URA) codebook with vector quantization and AMP-DA-Net, an unrolled AMP-style decoder trained end-to-end with the digital codebook and local training statistics. Experiments demonstrate that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while maintaining or improving performance across the full SNR range. The framework also generalizes OTA aggregation beyond averaging to symmetric functions like trimmed means and majority voting, showing robustness under message corruption.

## Method Summary
The framework combines vector quantization with URA encoding and AMP-DA-Net decoding. Device updates are split into fragments, quantized using a codebook learned from BS local updates, and transmitted simultaneously using URA encoding. At the server, AMP-DA-Net performs sparse recovery using an unrolled AMP-style decoder with learned parameters, CNN denoising, and EM updates for active device estimation. The system is trained end-to-end on a PA FEEL dataset collected from a baseline FEEL simulation. The learned components compensate for practical violations of AMP assumptions, including finite codebooks, non-uniform codeword usage, and SNR variability.

## Key Results
- Extends reliable digital OTA operation by more than 10 dB into low SNR regimes
- Achieves or improves performance across the full SNR range compared to AMP-DA baseline
- Generalizes OTA aggregation to symmetric functions like trimmed means and majority voting
- Shows robustness to device corruption and heterogeneity

## Why This Works (Mechanism)

### Mechanism 1: Learned Compensation for Model Mismatch in AMP Decoding
End-to-end training of the decoder and codebook jointly adapts residual scaling, damping, and priors to compensate for violations of classical AMP assumptions in practical FEEL settings. Classical AMP assumes large random sensing matrices, well-matched noise statistics, and uniform codeword usage. The unrolled AMP-DA-Net learns per-layer parameters (γ^(ℓ), η^(ℓ), β^(ℓ), τ^(ℓ)) and a CNN denoiser that correct for finite codebooks, non-uniform popularity, and SNR variability—conditions that break analytical AMP guarantees. The training dataset must capture sufficient statistics of real FEEL update distributions and channel conditions to generalize.

### Mechanism 2: Popularity-Ordered Codebook with Structured Prior
Ordering quantization centroids by BS-estimated popularity and learning the URA codebook jointly with decoding improves sparse recovery by providing consistent, exploitable structure. The BS clusters its local updates, orders centroids by frequency of use, and broadcasts this ordered codebook. The decoder learns a spike-and-slab prior with Poisson slab where earlier codewords have higher activity probability. This creates a structured sparsity pattern the decoder can exploit, rather than treating all codewords symmetrically. BS local update distribution must provide a reasonable proxy for device update distributions.

### Mechanism 3: Separable Sparse Recovery Enabling Generalized Aggregation
Recovering the multiset of transmitted messages (rather than only their sum) enables post-hoc application of arbitrary symmetric aggregation functions. The sparse recovery formulation estimates the count vector x̂ for each fragment slot, which records how many devices selected each codeword. This preserves the full multiset structure, allowing the server to apply trimmed mean, majority voting, or other robust rules after decoding—rather than committing to arithmetic mean at the physical layer. Individual message recovery must be sufficiently accurate to support meaningful post-processing.

## Foundational Learning

- **Concept: Approximate Message Passing (AMP)**
  - Why needed: AMP provides the algorithmic skeleton for sparse recovery from underdetermined linear measurements. AMP-DA-Net unrolls this iterative scheme into a learnable network.
  - Quick check: Can you explain why AMP requires an Onsager correction term to maintain Gaussian residual statistics?

- **Concept: Unsourced Random Access (URA)**
  - Why needed: URA provides the theoretical framework for massive access with a shared codebook, which is essential for scalability to large device populations where individual identification is unnecessary.
  - Quick check: In URA, why does the receiver only need to recover the set of transmitted messages, not which device sent which message?

- **Concept: Vector Quantization with Error Feedback**
  - Why needed: Quantization compresses continuous model updates into discrete codewords; error feedback accumulates quantization residuals to prevent systematic drift over multiple rounds.
  - Quick check: What happens to convergence if error feedback is disabled under sustained quantization error?

## Architecture Onboarding

- **Component map:**
  Device Side: Local SGD → Error Accumulator → Fragment Splitter → Vector Quantizer → URA Encoder → Simultaneous Transmission
  Server Side: Received y = Cx + n → AMP-DA-Net → Post-processing → De-quantize → Symmetric Aggregation → Global Update
  Offline Training: PA FEEL Simulation → Collect fragments → Pre-train (C, AMP-DA-Net, optionally Q)

- **Critical path:**
  1. Sparse recovery accuracy (x̂ ≈ x) determines whether aggregation receives correct inputs
  2. Active device estimation (K̂_a) controls global update scaling—underestimation causes instability, overestimation slows convergence
  3. Quantization distortion bounds excess loss under L-smoothness

- **Design tradeoffs:**
  - Unrolled layers L: More layers improve recovery but increase latency and memory (L=10 used)
  - Codebook size n: Larger n reduces quantization error but increases sparse recovery difficulty (n=128 used)
  - Error feedback: Improves convergence under clean conditions but can destabilize under corruption
  - Fragment dimension d: Larger fragments reduce overhead per parameter but increase per-fragment recovery complexity (d=20 used)

- **Failure signatures:**
  - Low SNR divergence: Test accuracy collapses, K̂_a estimation MAE exceeds 0.5
  - Codebook mismatch: Quantization distortion spikes, convergence degrades
  - Error feedback instability: Accumulated residuals cause device messages to diverge

- **First 3 experiments:**
  1. Baseline replication at multiple SNRs: Implement AMP-DA-Net with default hyperparameters, train on ResNet-generated dataset, evaluate at SNR ∈ {0, 3, 5, 10, 15, 20} dB
  2. Ablation on codebook learning: Compare learned URA + popularity ordering vs. learned URA without ordering vs. fixed random codebook
  3. Robust aggregation stress test: Inject 10-30% corrupted devices, compare arithmetic mean vs. trimmed mean vs. majority voting

## Open Questions the Paper Calls Out

- **Cross-fragment coupling for active device estimation:** The current implementation applies EM updates independently per fragment, requiring post-hoc averaging of K_a estimates. Enforcing an explicit equality constraint on the estimated number of active devices across all fragments within a round could improve decoding accuracy and global convergence.

- **Error feedback stability under corruption:** Error feedback caused instability under corruption scenarios. Tuning the stability-performance trade-off between error feedback benefits under clean conditions and its destabilizing effects under corruption is a promising direction.

- **Extension to fading and multi-antenna channels:** The current design assumes AWGN channels with single-antenna devices and BS. Future work can extend the framework to fading and multi-antenna channels to handle inter-symbol interference and channel estimation errors.

## Limitations
- Performance gains critically depend on accurate BS estimation of device activity and consistent codebook popularity ordering
- Introduces significant computational overhead at the server through AMP-DA-Net's unrolled layers and CNN denoiser
- Error feedback can destabilize the system under corruption or severe heterogeneity

## Confidence
- **High:** >10 dB SNR extension over AMP-DA baseline is well-supported by experimental results
- **Medium:** Mechanism of learned codebook ordering improving sparse recovery needs stronger statistical theory
- **Medium:** Analysis of when post-hoc robust aggregation fails could be more rigorous

## Next Checks
1. **Cross-architecture generalization test:** Train AMP-DA-Net on ResNet-generated updates, then evaluate on VGG and CNN-9 models without fine-tuning
2. **Codebook mismatch stress test:** Simulate BS-device distribution divergence and measure recovery accuracy degradation
3. **Error feedback stability analysis:** Systematically vary corruption levels (0-50%) and quantify when error feedback transitions from helpful to harmful