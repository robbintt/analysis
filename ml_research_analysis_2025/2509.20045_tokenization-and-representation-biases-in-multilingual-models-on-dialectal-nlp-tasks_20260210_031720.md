---
ver: rpa2
title: Tokenization and Representation Biases in Multilingual Models on Dialectal
  NLP Tasks
arxiv_id: '2509.20045'
source_url: https://arxiv.org/abs/2509.20045
tags:
- language
- tasks
- performance
- languages
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the dialect gap in multilingual models\
  \ by correlating two representational bias metrics\u2014Tokenization Parity (TP)\
  \ and Information Parity (IP)\u2014with downstream performance on dialectal NLP\
  \ tasks. The study compares encoder-only (e.g., mBERT) and decoder-only (e.g., Phi-3.5,\
  \ Llama 3.2) LLMs across dialect identification, topic classification, and extractive\
  \ question answering, controlling for script (Latin vs."
---

# Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks

## Quick Facts
- arXiv ID: 2509.20045
- Source URL: https://arxiv.org/abs/2509.20045
- Reference count: 22
- Primary result: Encoder-only models consistently outperform decoder-only LLMs on dialectal tasks; tokenization and information parity metrics predict performance differently by task type and language characteristics.

## Executive Summary
This paper investigates representational biases in multilingual models by analyzing Tokenization Parity (TP) and Information Parity (IP) against downstream performance on dialectal NLP tasks. The study compares encoder-only (mBERT) and decoder-only (Phi-3.5, Llama 3.2) models across dialect identification, topic classification, and extractive question answering, controlling for script type and resource availability. Results demonstrate that encoder models consistently outperform decoders, TP better predicts performance on morphology/syntax-heavy tasks while IP better predicts semantic tasks, and decoder models show greater tokenization fragmentation for non-Latin scripts. The findings highlight how representational biases in tokenization and compression significantly impact dialectal task performance and vary by model type, task nature, and language characteristics.

## Method Summary
The study compares encoder-only (mBERT) and decoder-only (Phi-3.5, Llama 3.2) models on dialectal tasks using FLORES-200 parallel corpus for calculating TP (token count ratio relative to English) and IP (NLL ratio). Downstream tasks include Dialect Identification (NADI, GDI, ILI, DSL-ML datasets), Topic Classification (SIB-200), and Extractive QA (SDQA). Encoder models are fine-tuned with full fine-tuning (3 epochs, AdamW, lr=2e-5), while decoder models use PEFT/LoRA with 4-bit quantization and Optuna hyperparameter optimization. Performance is measured via F1-scores and correlated with TP/IP values using Pearson correlation.

## Key Results
- Encoder-based models consistently outperform decoder-only LLMs across all dialectal tasks.
- TP better predicts performance on tasks reliant on morphological and syntactic features (e.g., extractive QA).
- IP better predicts performance in semantic tasks (e.g., topic classification).
- Decoder-only models exhibit greater tokenizer fragmentation for non-Latin scripts, which can degrade performance.

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Fragmentation Impacts Task Performance
Non-Latin scripts (e.g., Arabic, Hindi) are split into smaller subword or byte-level tokens by byte-level tokenizers when vocabulary coverage is poor, increasing sequence length and fragmenting linguistic units. This degrades performance on syntax/morphology-heavy tasks like Extractive QA where precise span identification is required. TP values greater than 1 indicate suboptimal representation and potential performance degradation.

### Mechanism 2: Information Parity Reflects Semantic Representation Quality
Higher IP scores indicate more efficient compression of semantic content across languages relative to English. For tasks requiring semantic understanding like Topic Classification, this efficient representation facilitates better performance. IP is a stronger predictor for semantic tasks than TP because it captures the quality of semantic representation within the model.

### Mechanism 3: Encoder vs. Decoder Architectural Differences
Encoder-only models use WordPiece tokenization optimized for coverage and bidirectional context (MLM training), advantageous for understanding full input sequences and handling variations. Decoder models use byte-level BPE/tiktoken, leading to more fragmentation for non-Latin scripts and trained with causal LM objectives less efficient for these non-generative tasks.

## Foundational Learning

- **Tokenization Parity (TP)**
  - Why needed: Quantifies tokenizer fairness across languages by comparing token counts relative to English
  - Quick check: If a tokenizer produces 20 tokens for English and 40 for Arabic, what is TP for Arabic? (Answer: 2.0)

- **Information Parity (IP)**
  - Why needed: Measures representational bias by quantifying information compression efficiency relative to English
  - Quick check: Does higher IP indicate more or less efficient information compression? (Answer: More efficient)

- **Byte-Level Fallback in Tokenization**
  - Why needed: Modern tokenizers break unknown characters into UTF-8 byte sequences, causing fragmentation for non-Latin scripts
  - Quick check: How does byte-level fallback affect token sequence length for non-Latin scripts? (Answer: Increases length by splitting multi-byte characters into individual byte tokens)

## Architecture Onboarding

- **Component map:** Parallel corpus -> Tokenizer Module -> Bias Metrics Calculator (TP/IP) -> Model Backbone (Encoder/Decoder) -> Task-Specific Heads -> Evaluation -> Correlation Analyzer
- **Critical path:** Select model/tokenizer pair → Compute TP/IP using parallel corpus → Fine-tune model on dialectal task → Evaluate to get F1 scores → Correlate TP/IP with F1 to test hypotheses
- **Design tradeoffs:** Tokenizer granularity (aggressive splitting helps OOV but increases sequence length), model architecture (bidirectional vs causal context), metric choice (TP is surface-level/easy but ignores semantics; IP is deeper but requires model inference)
- **Failure signatures:** High TP (>2.0) + poor EQA performance indicates fragmentation harming syntax-sensitive tasks; low IP + poor TC performance suggests semantic representation failure; unexpected positive TP/DI correlation indicates brittle surface-level exploitation; byte-level garbage (Latin-1 bytes for non-Latin input) indicates tokenizer fallback bottleneck
- **First 3 experiments:** 1) Tokenizer audit: Compare mBERT vs Llama-3.2 tokenization on non-Latin dialect sentences and inspect for fragmentation 2) Single-task correlation: Fine-tune mBERT on TC, correlate IP vs F1 per dialect 3) Ablation on tokenizer: Fine-tune decoder on EQA, retokenize with language-specific tokenizer, measure performance change

## Open Questions the Paper Calls Out

- **Adaptive Tokenizers:** Can language-aware, adaptive tokenizers effectively mitigate byte-level fragmentation and performance degradation in decoder-only models for non-Latin scripts?
- **Generative Task Generalization:** Do TP/IP predictive relationships generalize to generative dialectal tasks like translation or summarization?
- **Semantic vs Surface Trade-off:** Does achieving higher IP (semantic compression efficiency) inherently degrade a model's ability to distinguish surface-level dialectal features required for identification?

## Limitations

- Metric sensitivity to reference language choice (English) may not be optimal for all language families
- Controlled dataset scope may not generalize to all dialectal variations or low-resource languages
- Architectural generalization limited by comparing single encoder model with few decoder models, potentially confounding model size, pre-training data, and tokenizer implementations

## Confidence

- **High Confidence:** Encoder models outperform decoders; TP predicts morphology/syntax task performance; decoder models show greater tokenization fragmentation for non-Latin scripts
- **Medium Confidence:** IP better predicts semantic task performance than TP; performance gap primarily due to tokenization strategy and pre-training objectives
- **Low Confidence:** Full explanation for positive TP/DI correlation; precise numerical thresholds for optimal TP are calibrated for all languages/tasks

## Next Checks

1. Reference Language Ablation Study: Repeat TP/IP calculations using French/Spanish as reference instead of English and recalculate correlations
2. Manual Tokenizer Output Inspection: For Bengali/Tamil, manually inspect mBERT vs Llama-3.2 tokenizer output and correlate fragmentation with TP scores and EQA performance
3. Architectural Ablation with Controlled Tokenizers: Fine-tune decoder on EQA using native tiktoken vs language-specific tokenizer, compare performance to isolate tokenizer impact