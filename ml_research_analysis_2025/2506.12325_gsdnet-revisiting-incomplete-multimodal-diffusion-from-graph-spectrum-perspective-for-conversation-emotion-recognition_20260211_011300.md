---
ver: rpa2
title: 'GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective
  for Conversation Emotion Recognition'
arxiv_id: '2506.12325'
source_url: https://arxiv.org/abs/2506.12325
tags:
- graph
- data
- modality
- missing
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal emotion recognition
  in conversations (MERC) when some modalities (video, audio, text) are missing. The
  authors propose GSDNet, a novel Graph Spectral Diffusion Network that combines graph
  neural networks with diffusion models for modality completion.
---

# GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition

## Quick Facts
- **arXiv ID**: 2506.12325
- **Source URL**: https://arxiv.org/abs/2506.12325
- **Reference count**: 11
- **Primary result**: State-of-the-art multimodal emotion recognition in conversations with missing modalities using graph spectral diffusion

## Executive Summary
This paper addresses the challenge of multimodal emotion recognition in conversations when some modalities (video, audio, text) are missing. The authors propose GSDNet, a novel Graph Spectral Diffusion Network that combines graph neural networks with diffusion models for modality completion. Unlike previous approaches that add Gaussian noise directly to adjacency matrices (which destroys graph structure), GSDNet restricts diffusion to the spectral space by adding noise only to eigenvalues while preserving eigenvectors. Experiments on CMU-MOSI and CMU-MOSEI datasets show GSDNet achieves state-of-the-art performance across various missing modality scenarios, with improvements of 2-3% in accuracy metrics compared to baselines like GCNet and IMDer.

## Method Summary
GSDNet encodes multimodal features (text, audio, video) into a common dimension using 1D-Conv layers with positional embeddings, constructs an interaction graph, and performs graph spectral diffusion to recover missing modalities. The key innovation is restricting diffusion to the spectral space by applying noise only to eigenvalues while preserving eigenvectors. The method uses conditional score-based models to estimate the gradient of the log-likelihood of missing data conditioned on observed data. Two distinct score networks model the distribution of missing modalities and eigenvalues, which are jointly optimized with reconstruction loss and score matching losses. The recovered features are fused via GCN layers for emotion classification.

## Key Results
- Achieves 2-3% improvement in accuracy metrics over state-of-the-art baselines (GCNet, IMDer) on CMU-MOSI and CMU-MOSEI datasets
- Maintains data distribution consistency through spectral space diffusion, outperforming direct adjacency matrix diffusion
- Demonstrates robust performance across various missing modality combinations and rates (0.0-0.7)

## Why This Works (Mechanism)

### Mechanism 1
Restricting diffusion to the spectral space (eigenvalues) preserves graph topology during modality completion, which standard diffusion fails to do. The method performs eigendecomposition on the graph adjacency matrix (A = UΛU^T), applies the forward diffusion process to the diagonal eigenvalues (Λ) while freezing the eigenvectors (U), and during reverse diffusion denoises the eigenvalues to recover the data distribution without altering the graph's structural basis.

### Mechanism 2
Conditioning the recovery of missing modalities on observed modalities allows for semantically consistent feature synthesis. GSDNet uses a conditional score-based model that estimates the gradient of the log-likelihood of the missing data (∇ log p(X_missing)) conditioned on the observed data (X_observed). The observed modality features act as a guiding signal during the reverse SDE process to steer noise into valid missing features.

### Mechanism 3
Jointly optimizing feature reconstruction and eigenvalue reconstruction ensures the generated graph is consistent with the original data distribution. The architecture employs two distinct score networks (s_θ for features X, s_φ for eigenvalues Λ) and minimizes a joint loss combining reconstruction loss (L_rec) and score matching losses (L_s), effectively aligning the generated feature space with the graph's spectral properties before fusing them with a GCN.

## Foundational Learning

- **Concept: Graph Eigendecomposition (Spectral Theory)** - Separates graph structure (eigenvectors U) from intensity/weights (eigenvalues Λ). Required to understand how GSDNet modifies Λ while keeping U fixed. *Quick check*: If you add noise to an adjacency matrix directly, why does the SNR drop faster than adding noise only to its eigenvalues?

- **Concept: Score-Based Generative Models (Diffusion)** - Uses Stochastic Differential Equations (SDEs) to model the transition from noise to data. Understanding "forward noising" vs. "reverse denoising" is required to implement the training loop. *Quick check*: In Eq. 2, what role does the score function ∇ log p_t(X̄_t) play in the reverse process?

- **Concept: Multimodal Emotion Recognition in Conversations (MERC)** - Provides context on why "missing modalities" are a critical failure mode (e.g., sensor failure, occlusion) and why simple zero-filling fails. *Quick check*: Why does the paper argue that fusing complementary semantic information is difficult when one modality is missing?

## Architecture Onboarding

- **Component map**: Input (Missing) -> 1D-Conv -> Eigendecomposition -> Reverse SDE (Conditioned on Observed) -> Feature Reconstruction -> GCN Fusion -> Classifier
- **Critical path**: Input (Missing) → 1D-Conv → Eigendecomposition → Reverse SDE (Conditioned on Observed) → Feature Reconstruction → GCN Fusion → Classifier
- **Design tradeoffs**: GSDNet sacrifices the generative flexibility of creating new graph structures (since U is fixed) to guarantee stability and preserve the semantic topology of the conversation, requiring more computation than simple encoder-recovery methods.
- **Failure signatures**: Oversmoothing if GCN fusion layers are too deep; spectral collapse if noise levels are too high relative to eigenvalue magnitude; modality imbalance if observed modality provides weak conditioning signal.
- **First 3 experiments**: 1) Run GSDNet without spectral constraint to verify performance drop. 2) Test missing rate robustness from 0.1 to 0.7. 3) Generate t-SNE plots of recovered features vs. ground truth to confirm distribution consistency.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations identified.

## Limitations

- **Architectural detail gaps**: Missing specifications for score networks, reconstruction modules, and GCN fusion layers
- **Diffusion parameter sensitivity**: Lack of details on SDE type, diffusion steps, and noise schedules
- **Generalization concerns**: Experiments limited to two datasets from the same domain

## Confidence

**High Confidence**:
- Spectral diffusion preserves graph topology better than direct adjacency matrix diffusion
- Conditioning on observed modalities improves recovery quality
- GSDNet outperforms baseline methods on tested datasets

**Medium Confidence**:
- Joint optimization of feature and eigenvalue reconstruction is necessary for data distribution consistency
- Performance advantage scales with higher missing rates
- GCN fusion effectively combines recovered and observed features

**Low Confidence**:
- Specific contribution of each architectural component to performance
- Whether improvements generalize to datasets with different feature characteristics
- Computational efficiency relative to simpler approaches

## Next Checks

1. **Ablation Study Verification**: Implement a baseline that applies diffusion directly to the full adjacency matrix (without spectral decomposition) and compare performance to GSDNet to validate the core novelty claim.

2. **Missing Rate Robustness Testing**: Systematically test GSDNet at missing rates from 0.1 to 0.7 for different modality combinations (AV, AT, VT) to verify stability claims across various conditions.

3. **Feature Distribution Analysis**: Generate t-SNE visualizations of recovered vs. ground truth features for high missing rates (>0.5) to empirically validate the "distribution consistency" claim.