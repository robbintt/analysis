---
ver: rpa2
title: 'DAWM: Diffusion Action World Models for Offline Reinforcement Learning via
  Action-Inferred Transitions'
arxiv_id: '2509.19538'
source_url: https://arxiv.org/abs/2509.19538
tags:
- offline
- learning
- world
- action
- dawm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating complete state-action-reward
  transitions for offline reinforcement learning (RL) using diffusion-based world
  models. While diffusion models excel at generating realistic trajectories, many
  existing approaches fail to generate actions alongside states and rewards, limiting
  compatibility with standard one-step TD learning methods.
---

# DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions

## Quick Facts
- arXiv ID: 2509.19538
- Source URL: https://arxiv.org/abs/2509.19538
- Authors: Zongyue Li; Xiao Han; Yusong Li; Niklas Strauss; Matthias Schubert
- Reference count: 18
- Key outcome: DAWM generates complete state-action-reward transitions using diffusion models with inverse dynamics inference, achieving 9.3% (TD3BC) and 9.5% (IQL) improvements over DWM on D4RL benchmarks

## Executive Summary
This paper addresses the challenge of generating complete state-action-reward transitions for offline reinforcement learning using diffusion-based world models. While diffusion models excel at generating realistic trajectories, many existing approaches fail to generate actions alongside states and rewards, limiting compatibility with standard one-step TD learning methods. The authors propose DAWM, a modular framework that combines a diffusion-based world model with an inverse dynamics model (IDM) to infer actions and produce complete transitions. DAWM generates future state-reward trajectories conditioned on the current state, action, and return-to-go, then uses an IDM to efficiently infer actions.

## Method Summary
DAWM is a modular framework that integrates diffusion-based trajectory generation with inverse dynamics modeling to create complete state-action-reward transitions for offline RL. The core approach uses a diffusion model to generate future state-reward trajectories conditioned on current state, action, and return-to-go, then employs an inverse dynamics model to infer the actions from these generated transitions. This design enables compatibility with standard one-step TD learning algorithms like TD3BC and IQL. The framework addresses a key limitation of previous diffusion-based world models that could generate states and rewards but not actions, making them incompatible with conventional offline RL algorithms that require complete transition tuples.

## Key Results
- DAWM consistently outperforms prior diffusion-based baselines on D4RL benchmarks
- Achieves average improvements of 9.3% (TD3BC) and 9.5% (IQL) over DWM
- Demonstrates approximately 4.0× faster inference than planning-based alternatives
- Can achieve comparable performance to agents trained on real offline datasets

## Why This Works (Mechanism)
DAWM works by decoupling the trajectory generation process from action inference. The diffusion model focuses on generating realistic future states and rewards, which it can do effectively due to its strong generative capabilities. The inverse dynamics model then infers actions from the state transitions, leveraging the fact that actions can often be reliably predicted from state changes in many environments. This separation allows each component to specialize in what it does best - the diffusion model generates plausible futures while the IDM efficiently extracts the necessary actions for policy learning.

## Foundational Learning

**Diffusion Models**
- Why needed: Generate realistic future state-reward trajectories for world modeling
- Quick check: Can the diffusion model produce trajectories that match the distribution of the training data?

**Inverse Dynamics Models**
- Why needed: Infer actions from state transitions when diffusion models cannot directly generate actions
- Quick check: Does the IDM accurately predict actions from consecutive state pairs in held-out data?

**Return-to-Go Conditioning**
- Why needed: Guide trajectory generation toward specific reward targets for efficient planning
- Quick check: Does conditioning on return-to-go produce trajectories that achieve the desired cumulative rewards?

**Conservative Offline RL**
- Why needed: Prevent overestimation of Q-values when learning from static datasets
- Quick check: Does the learned policy achieve higher returns on held-out validation episodes?

## Architecture Onboarding

**Component Map**
Diffusion Model -> State-Reward Trajectory Generation -> Inverse Dynamics Model -> Action Inference -> Complete Transition Tuples -> TD Learning Algorithm

**Critical Path**
The critical path involves: (1) diffusion model generates future state-reward trajectory, (2) IDM infers actions from state transitions, (3) complete transitions feed into TD learning algorithm. The bottleneck is typically the diffusion model's sampling speed, which DAWM improves by 4.0× over planning-based alternatives.

**Design Tradeoffs**
The framework trades computational complexity (training both diffusion and IDM) for improved compatibility with standard RL algorithms. An alternative design could attempt to train a single model to generate all components, but this would likely sacrifice the specialization benefits of the modular approach.

**Failure Signatures**
- Poor action inference quality from IDM manifests as unrealistic actions that cause state transitions to deviate from training data distribution
- Diffusion model collapse results in repetitive or unrealistic state-reward trajectories
- Mismatch between generated and real data distributions leads to overestimation in TD learning

**3 First Experiments**
1. Evaluate IDM accuracy on held-out state transition pairs before integrating with diffusion model
2. Compare DAWM performance with and without IDM to quantify its contribution
3. Test DAWM on simple gridworld environments before scaling to D4RL benchmarks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- IDM accuracy directly impacts learning performance - poor action inference yields suboptimal transitions
- Requires careful hyperparameter tuning of noise schedule and denoising process
- Significant computational overhead for training diffusion model and IDM compared to simpler model-free approaches

## Confidence

**High confidence**: DAWM's ability to generate complete state-action-reward transitions and its empirical performance improvements over DWM on D4RL benchmarks

**Medium confidence**: The claim of 4.0× faster inference speed, as this depends on specific hardware configurations and implementation details not fully specified in the paper

**Medium confidence**: The comparability to agents trained on real offline datasets, as the evaluation metrics and dataset characteristics require careful interpretation

## Next Checks

1. Conduct ablation studies removing the inverse dynamics model to quantify its contribution to performance gains
2. Test DAWM on continuous control tasks with higher dimensional state spaces to evaluate scalability
3. Compare DAWM's performance across multiple random seeds and different dataset qualities (e.g., medium, medium-replay, expert) to assess robustness