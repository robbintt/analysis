---
ver: rpa2
title: Learning Representations Through Contrastive Neural Model Checking
arxiv_id: '2510.01853'
source_url: https://arxiv.org/abs/2510.01853
tags:
- learning
- checking
- representations
- circuit
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CNML, a contrastive learning framework for
  learning aligned representations of LTL specifications and AIGER circuits. The key
  innovation is using the model checking task as a contrastive learning objective,
  where circuits and specifications that satisfy each other are brought closer in
  embedding space.
---

# Learning Representations Through Contrastive Neural Model Checking

## Quick Facts
- **arXiv ID:** 2510.01853
- **Source URL:** https://arxiv.org/abs/2510.01853
- **Reference count:** 40
- **Primary result:** Contrastive learning framework that learns aligned representations of LTL specifications and AIGER circuits using model checking as the supervision signal

## Executive Summary
This paper introduces CNML, a contrastive learning framework for learning aligned representations of LTL specifications and AIGER circuits. The key innovation is using the model checking task as a contrastive learning objective, where circuits and specifications that satisfy each other are brought closer in embedding space. CNML employs a self-supervised approach with separate encoders for each modality, trained on synthetic data generated through reactive synthesis. The model significantly outperforms both algorithmic and neural baselines on cross-modal and intra-modal retrieval tasks, achieving high Recall@1% and Recall@10% scores. Importantly, CNML demonstrates generalization capabilities, successfully transferring learned representations to downstream tasks and generalizing from simple formulas to complex multi-guarantee specifications. The approach shows that model checking can serve as an effective objective for learning representations of formal languages.

## Method Summary
CNML uses a bi-encoder architecture with two independent CodeBERT models to encode LTL specifications and AIGER circuits separately. The training objective is a symmetric contrastive loss that pushes satisfying circuit-specification pairs together in embedding space while pushing non-satisfying pairs apart. Synthetic training data is generated through reactive synthesis using the Strix tool, ensuring all circuit-specification pairs are semantically valid (satisfying). The model employs a projection layer to map embeddings to a shared 1024-dimensional space, followed by L2 normalization. Training uses a greedy batch construction algorithm to minimize false negative pairs, with symmetric cross-entropy loss plus representation regularization. The approach is evaluated on both cross-modal and intra-modal retrieval tasks, as well as downstream model checking applications.

## Key Results
- **High Retrieval Performance:** CNML achieves Recall@1% of 96.8% and Recall@10% of 99.3% on cross-modal retrieval tasks
- **Superior to Baselines:** Significantly outperforms both algorithmic (MUC and MRU) and neural (Word2Vec and SBERT) baselines across all evaluation metrics
- **Strong Generalization:** Successfully transfers learned representations to downstream model checking tasks, achieving 83% accuracy on simplified specifications with single guarantees
- **Cross-Modal Effectiveness:** Demonstrates strong performance on both cross-modal (circuit→spec and spec→circuit) and intra-modal retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Self-Supervision from Model Checking Oracle
The model checking relationship (satisfies/violates) provides a natural binary supervision signal that can be leveraged through contrastive learning without manual labeling. Each mini-batch of N known-satisfying pairs implicitly constructs N²-N negative pairs (circuit_i with specification_j where i≠j). The symmetric cross-entropy loss pushes satisfying pairs together in embedding space while pushing non-satisfying pairs apart. This creates dense signal from sparse labels—each sample gets N-1 negative comparisons per batch rather than just one binary label. Core assumption: Non-diagonal pairs in a batch are likely non-satisfying. The paper reports ~4% false negative rate through cross-checking, which is acceptably low.

### Mechanism 2: Modality-Separate Encoders Force Semantic Alignment
Using two independent encoders (no parameter sharing) forces each encoder to learn modality-specific features while the contrastive objective aligns only the semantic content. E_φ processes only LTL specifications; E_c processes only AIGER circuits. They never see the other modality during encoding. The only coupling occurs through the loss function, which operates on the projected embeddings. This prevents the model from exploiting superficial syntactic correlations (e.g., keyword matching) that might exist in paired training data. Core assumption: The projection matrices and contrastive loss can bridge two fundamentally different syntactic representations (temporal logic formulas vs. gate-level circuits) into a shared semantic space.

### Mechanism 3: Synthesis-Based Data Generation Ensures Semantic Validity
Reactive synthesis guarantees that generated circuit-specification pairs are semantically correct (satisfying), avoiding the validity problems of stochastic generation. Rather than randomly generating circuits and checking if they satisfy specifications (computationally infeasible), the approach uses the Strix synthesis tool to construct circuits that provably satisfy each specification. This is inverted from the typical model checking flow—synthesis constructs from specification rather than verification checking specification against circuit. Core assumption: Synthesized circuits are representative of real-world circuits the model will encounter at test time. Distribution shift between synthesized and hand-designed circuits could limit transfer.

## Foundational Learning

- **Contrastive Learning (InfoNCE-style objectives):**
  - **Why needed here:** The entire training framework depends on understanding how positive/negative pair construction and symmetric cross-entropy loss creates the embedding space structure.
  - **Quick check question:** Given a batch of 4 circuit-specification pairs, how many implicit negative pairs are created, and why does the loss compute both row-wise and column-wise?

- **Linear Temporal Logic (LTL) Semantics:**
  - **Why needed here:** Understanding the assume-guarantee format and temporal operators is necessary to interpret what the specification encoder must capture and why the generalization experiment (single-guarantee → multi-guarantee) is meaningful.
  - **Quick check question:** What does the formula `□(i₀ → ○(¬i₁ → o₁))` express, and how does the assume-guarantee format restructure this?

- **Model Checking as a Decision Problem:**
  - **Why needed here:** The core insight is that model checking provides the supervision signal—understanding the three outcomes (satisfies, violates, timeout) clarifies why only positive pairs are directly available for training.
  - **Quick check question:** Why can't we efficiently generate negative training examples by running a model checker on random circuit-specification pairs?

## Architecture Onboarding

- **Component map:**
  LTL Specification → E_φ (CodeBERT) → Pool → Projection (768→1024) → L2 Norm → v_φ
  AIGER Circuit → E_c (CodeBERT) → Pool → Projection (768→1024) → L2 Norm → u_c
                                                                            ↓
  [u_c₁...u_c_N] × [v_φ₁...v_φ_N]^T → Cosine Similarity Matrix (N×N)
                                                                            ↓
  L_CE (symmetric cross-entropy) + λ·L_RR (representation regularization)

- **Critical path:**
  1. Data loading: Ensure no duplicate circuits/specifications within each batch (greedy batch construction)
  2. Forward pass: Both encoders process independently—verify they receive only their modality
  3. Similarity computation: Matrix should show highest values on diagonal for satisfying pairs
  4. Loss backward: Monitor both components; L_RR prevents embedding collapse

- **Design tradeoffs:**
  - **CodeBERT init vs. scratch:** CodeBERT provides transfer from code understanding, but risks domain mismatch. Paper shows it works, but domain-specific pretraining could improve further.
  - **Projection dimension (1024):** Paper reports diminishing returns above 1024—likely not worth additional parameters.
  - **No parameter sharing between encoders:** Sacrifices potential cross-modal knowledge transfer for cleaner semantic alignment. Ablation not reported.
  - **Fixed logit scaling:** Different from CLIP; prevents temperature from masking learning issues but may limit peak performance.

- **Failure signatures:**
  - **High training loss, low retrieval:** Check false negative rate in batch construction; may need larger batches or better deduplication
  - **Embedding collapse (all vectors similar):** Increase λ for regularization loss; check learning rate isn't too high (catastrophic forgetting in BERT)
  - **Good cross-modal but poor intra-modal:** Model may be overfitting to pairing patterns; increase data augmentation
  - **Generalization gap (simple→complex):** Training distribution may not cover necessary primitives; consider curriculum learning

- **First 3 experiments:**
  1. **Baseline sanity check:** Train with frozen CodeBERT (no CNML pretraining), fine-tune on downstream model checking task. Should see 83% accuracy per Table 3. If significantly different, check data pipeline.
  2. **Embedding visualization:** t-SNE/UMAP of test set embeddings colored by: (a) satisfying vs. violating pairs, (b) specification complexity, (c) circuit size. Should see clear separation in (a), gradual structure in (b,c).
  3. **Ablation on batch size:** Table 1 shows batch size matters (N=100 vs N=1000). Run retrieval with N=50, 100, 200, 500 to characterize scaling. If Recall@1% doesn't improve with batch size, negative sampling may be ineffective.

## Open Questions the Paper Calls Out
None

## Limitations
- **Synthetic Data Dependency:** The approach relies heavily on synthetic training data generated through reactive synthesis, which may not fully capture the diversity of real-world circuits and specifications.
- **False Negative Rate Impact:** The 4% false negative rate in batch construction, though reported as acceptable, could still introduce noise that limits peak performance.
- **Untested Architectural Alternatives:** The paper doesn't explore parameter sharing between modality encoders or alternative projection architectures that might improve performance.

## Confidence

- **High Confidence:** The core mechanism of using model checking as contrastive supervision is technically sound and well-supported by experimental results. The retrieval performance improvements over baselines are statistically significant and robust across different batch sizes.
- **Medium Confidence:** The generalization from simple to complex specifications is demonstrated but only on the specific path from single-guarantee to multi-guarantee LTL. The paper doesn't explore other generalization directions or failure cases.
- **Low Confidence:** The claim that modality-separate encoders are necessary for semantic alignment is plausible but untested against joint encoding approaches. The specific choice of 1024 projection dimension appears reasonable but isn't thoroughly ablated.

## Next Checks

1. **Generalization Breadth Test:** Evaluate CNML on multi-guarantee formulas with different structural patterns than those seen during training (e.g., nested guarantees, alternating assumptions). This would test whether the model truly learns temporal logic primitives rather than memorizing specific formula templates.

2. **False Negative Sensitivity Analysis:** Systematically vary the false negative rate in training batches (0%, 4%, 10%, 20%) and measure the impact on retrieval performance. This would quantify the robustness of the contrastive objective to noisy negative sampling.

3. **Cross-Domain Transfer:** Apply the pre-trained CNML model to specifications in related formal languages (e.g., CTL, LTL with past operators) without fine-tuning. Success would demonstrate that the learned representations capture fundamental temporal reasoning rather than LTL-specific patterns.