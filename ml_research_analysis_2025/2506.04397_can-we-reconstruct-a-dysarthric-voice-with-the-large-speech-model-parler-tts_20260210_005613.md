---
ver: rpa2
title: Can we reconstruct a dysarthric voice with the large speech model Parler TTS?
arxiv_id: '2506.04397'
source_url: https://arxiv.org/abs/2506.04397
tags:
- speech
- speaker
- intelligibility
- voice
- speakers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether a large speech model, Parler TTS,
  can reconstruct dysarthric speech by improving intelligibility while maintaining
  speaker identity. Using a curated dataset combining dysarthric speech from the Speech
  Accessibility Project and healthy controls from MLS, the authors fine-tuned Parler
  TTS to generate speech with varying intelligibility levels.
---

# Can we reconstruct a dysarthric voice with the large speech model Parler TTS?

## Quick Facts
- arXiv ID: 2506.04397
- Source URL: https://arxiv.org/abs/2506.04397
- Reference count: 0
- Primary result: Parler TTS can sample from intelligibility levels but struggles with consistent control and speaker identity

## Executive Summary
This study investigates whether the large speech model Parler TTS can reconstruct dysarthric speech by improving intelligibility while maintaining speaker identity. The authors fine-tuned the model on a curated dataset combining dysarthric speech from the Speech Accessibility Project with healthy controls from MLS. Results show the model can learn to generate from the distribution of both speech types but struggles to consistently control intelligibility and maintain speaker identity. Expert listening confirmed the model's ability to sample from desired intelligibility levels, while objective metrics showed limited variation across ratings.

## Method Summary
The researchers fine-tuned Parler TTS Mini v0.1 on a dataset combining 22.97 hours of dysarthric speech from the Speech Accessibility Project with 22.94 hours of matched healthy speech from MLS. They generated prompts via DataSpeech that included speaker names, intelligibility ratings (0-7), and voice characteristics. The model was fine-tuned for 2 epochs using Adam optimizer with specific hyperparameters on NVIDIA A100 80GB. Evaluation involved running objective metrics on natural speech first to validate, then on synthetic speech, supplemented by expert listening tests.

## Key Results
- The model can learn to generate from the distribution of healthy and disordered speech traits
- It struggles to consistently control intelligibility levels as specified in prompts
- Speaker identity maintenance remains inconsistent across generated samples
- Objective metrics showed limited variation in WER and speaker similarity across intelligibility ratings

## Why This Works (Mechanism)

### Mechanism 1: Distribution Learning via Fine-tuning on Mixed Healthy/Dysarthric Data
Fine-tuning Parler TTS on combined dysarthric and healthy speech enables the model to learn the distribution of both speech types, allowing it to sample from varying intelligibility levels. The model is exposed to intelligibility ratings (0-7) during fine-tuning, learning to represent acoustic characteristics of both disordered and healthy speech. Core assumption: The pre-trained model's representations are sufficiently flexible to accommodate dysarthric speech features without catastrophic forgetting. Evidence: Expert listening revealed the model samples from various intelligibility levels and speaker identities. Break condition: If dysarthric features are too distant from pre-training distribution or fine-tuning data per speaker is insufficient.

### Mechanism 2: Natural Language Prompt Conditioning for Intelligibility Control
Text descriptions embedded via a pre-trained NLP model provide control signals for speech generation, but this control is currently too weak for consistent results. The entire prompt (speaker name + intelligibility rating + style descriptors) is embedded by an NLP model, then attended to by the speech language model during generation. Core assumption: NLP embedding captures semantic meaning of intelligibility ratings in a way that maps to learnable acoustic features. Evidence: The prompt-based approach struggles to provide consistent results during inference. Break condition: If NLP embeddings don't disentangle intelligibility from other attributes or attention mechanisms fail to weight intelligibility signals appropriately.

### Mechanism 3: Speaker Identity Binding via Fine-tuning with Named Labels
Assigning unique speaker names in prompts during fine-tuning enables partial speaker identity learning, but the base model's architecture limits consistency. The model maps speaker names to voice characteristics through supervised fine-tuning where each speaker's recordings are paired with their name. Core assumption: The pre-trained model can specialize to individual speakers with limited fine-tuning data. Evidence: Speaker similarity remains inconsistent across generated samples. Break condition: If base model lacks fine-grained speaker bindings or per-speaker data is too limited.

## Foundational Learning

- Concept: Dysarthria and Intelligibility Assessment
  - Why needed here: Dysarthria manifests as imprecise consonants, slow rate, and slurred articulation. The study uses speech therapist ratings on a 1-7 Likert scale as ground truth for intelligibility.
  - Quick check question: Why did the authors choose "imprecise consonants" as the intelligibility proxy rather than other SAP ratings like harshness or speech monotony?

- Concept: Generative Model Variability and Sampling
  - Why needed here: Parler TTS is generative, meaning each inference produces a random sample from the learned distribution. This inherent variability complicates evaluation.
  - Quick check question: How does generative variability explain why WER and speaker similarity appeared flat across intelligibility ratings in Figures 5-6?

- Concept: Objective Metric Reliability on Disordered Speech
  - Why needed here: Standard metrics (WER via Whisper, UTMOS, speaker similarity via Resemblyzer) were validated on healthy speech. UTMOS failed on dysarthric speech (scores < 3 even for intelligible speakers).
  - Quick check question: What steps did the authors take to validate objective metrics before applying them to synthetic speech, and which metric failed validation?

## Architecture Onboarding

- Component map: SAP dataset (22 speakers, 22.97h dysarthric) + MLS matched controls (22 speakers, 22.94h healthy) -> DataSpeech prompt generation (speaker name, intelligibility rating 0-7, voice characteristics) -> Fine-tuning Parler TTS Mini v0.1 (2 epochs, batch size 2, Adam optimizer) -> Expert listening + objective metrics evaluation

- Critical path:
  1. Data curation: Match SAP dysarthric speakers to MLS controls by predicted gender and data length
  2. Annotation: Generate prompts via DataSpeech with intelligibility ratings and speaker names
  3. Fine-tuning: Adapt Parler TTS Mini on combined 45.91-hour dataset
  4. Inference: Generate samples at varying intelligibility levels by modifying prompt text
  5. Evaluation: Run objective metrics on natural speech first to validate, then on synthetic speech + expert listening

- Design tradeoffs:
  - Prompt-based vs. Direct Conditioning: Prompts are flexible but weak; authors suggest direct categorical embeddings for speaker ID may improve consistency
  - UTMOS vs. Expert Listening: UTMOS did not generalize to dysarthric speech; expert listening required as fallback
  - Data augmentation potential: Authors propose articulatory-domain augmentation to increase effective training data, but this is unimplemented
  - Control granularity: Only imprecise consonants used; SAP contains additional ratings (harshness, monotony) that could provide finer control

- Failure signatures:
  - Flat WER across intelligibility ratings: Generated speech shows ~20-25% WER regardless of prompt-specified intelligibility
  - Inconsistent speaker similarity: Variance in speaker similarity even with identical speaker prompts
  - UTMOS scores < 3 for dysarthric speakers: Objective naturalness metric fails on disordered speech
  - Generative variability masking prompt effects: Random output variation requires extensive sampling to detect signal

- First 3 experiments:
  1. Validate objective metrics on natural dysarthric speech first: Run WER, speaker similarity, and UTMOS on the fine-tuning data to establish baseline reliability before synthetic evaluation
  2. Ablate intelligibility prompt while holding speaker constant: For one dysarthric speaker, generate samples at ratings 5â†’0; measure if WER actually decreases or remains flat
  3. Quantify speaker identity drift via embedding variance: Generate 50 samples per speaker with identical prompts; compute standard deviation of speaker similarity scores to measure consistency

## Open Questions the Paper Calls Out
None

## Limitations
- The model cannot reliably produce consistent outputs for a given prompt, limiting clinical applicability
- Limited fine-tuning data per speaker (0.23-2.66 hours) constrains stable identity learning
- Standard objective metrics like UTMOS fail to generalize to dysarthric speech
- Generative variability requires extensive sampling to detect signal in evaluation

## Confidence
**High Confidence**: The model can learn to sample from the distribution of healthy and disordered speech traits when fine-tuned on mixed data.

**Medium Confidence**: The model struggles to control intelligibility and maintain speaker identity consistently, though objective metrics show limited variation suggesting metric limitations may play a role.

**Low Confidence**: The specific architectural changes needed to improve controllability and identity consistency, as these remain theoretical recommendations without empirical validation.

## Next Checks
1. Ablation study on intelligibility control: For a single dysarthric speaker, generate 50 samples each at intelligibility ratings 7, 5, and 0. Calculate mean and variance of WER across each rating level to determine if prompt-based control actually affects intelligibility.

2. Speaker identity consistency quantification: Generate 100 samples per speaker with identical prompts. Compute standard deviation of speaker similarity scores across samples to establish baseline variability in speaker identity reproduction.

3. Direct conditioning experiment: Implement a modified version using categorical speaker embeddings and numerical intelligibility embeddings instead of textual descriptions. Fine-tune on the same dataset and compare controllability metrics to the original prompt-based approach.