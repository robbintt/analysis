---
ver: rpa2
title: Hierarchy-Aware Multimodal Unlearning for Medical AI
arxiv_id: '2512.09867'
source_url: https://arxiv.org/abs/2512.09867
tags:
- unlearning
- forget
- section
- retain
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MEDFORGET introduces the first hierarchy-aware multimodal unlearning\
  \ benchmark for medical AI, modeling hospital data as nested Institution \u2283\
  \ Patient \u2283 Study \u2283 Section structures to enable fine-grained evaluation\
  \ of forgetting completeness and utility preservation. Existing unlearning methods\
  \ show a consistent deletion-utility trade-off: training-based approaches preserve\
  \ retain and general medical performance but leave substantial residual memorization,\
  \ while aggressive training-free methods achieve stronger deletion at the cost of\
  \ significant utility degradation."
---

# Hierarchy-Aware Multimodal Unlearning for Medical AI

## Quick Facts
- arXiv ID: 2512.09867
- Source URL: https://arxiv.org/abs/2512.09867
- Reference count: 39
- Primary result: Introduces MEDFORGET benchmark and CHIP method achieving superior hierarchy-aware multimodal unlearning performance

## Executive Summary
MEDFORGET introduces the first hierarchy-aware multimodal unlearning benchmark for medical AI, modeling hospital data as nested Institution ⊃ Patient ⊃ Study ⊃ Section structures to enable fine-grained evaluation of forgetting completeness and utility preservation. Existing unlearning methods show a consistent deletion-utility trade-off: training-based approaches preserve retain and general medical performance but leave substantial residual memorization, while aggressive training-free methods achieve stronger deletion at the cost of significant utility degradation. CHIP, a training-free hierarchy-aware method, exploits sibling-differential directions by isolating target-specific weight subspaces across both language and vision–language fusion layers while preserving sibling-shared information. CHIP achieves the highest forget-retain performance gap across all hierarchy levels, maintaining competitive downstream utility and demonstrating superior robustness to hierarchical reconstruction attacks compared to baselines. The benchmark and method advance trustworthy medical MLLM deployment under privacy regulations like HIPAA and GDPR.

## Method Summary
CHIP is a training-free unlearning method that exploits hierarchical structure in medical data by computing sibling-differential directions to isolate target-specific representations while preserving sibling-shared information. The method collects activations from both language and VL merger layers, computes importance scores for each neuron, selects top-k% neurons, and applies orthogonal subspace projection to remove forget-specific weight components. The sibling-differential direction computation subtracts the mean activation over sibling nodes from the target mean, canceling shared hierarchical structure while isolating target-specific information. CHIP then applies weight projection to both upper language layers (22-27) and vision-language merger layers, ensuring both unimodal and cross-modal representations related to the target are removed.

## Key Results
- CHIP achieves the highest forget-retain performance gap (F/R Diff) across all hierarchy levels compared to training-based and other training-free methods
- At Section level, CHIP achieves F/R Diff of 12.60 while maintaining competitive retain performance (Factuality 7.86 vs baseline 7.85)
- Training-based methods (Grad-Diff, NPO, KL-Min, MANU) preserve retain and general medical performance but leave substantial residual memorization
- CHIP demonstrates superior robustness to hierarchical reconstruction attacks compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sibling-differential direction computation isolates target-specific representations while canceling shared hierarchical patterns
- Mechanism: For target node $n_{target}$ with siblings, CHIP computes $d = \text{normalize}(\mu^{(l)}_{target} - \mu^{(l)}_{siblings})$, subtracting the mean activation over sibling nodes from the target mean. This removes shared hierarchical structure (e.g., institutional formatting, common imaging characteristics) from the computed direction.
- Core assumption: Hierarchically related data encodes shared information across all siblings while target-specific information is localized to each node.
- Evidence anchors:
  - [abstract]: "deletes information by selectively removing target-specific weight subspaces while preserving sibling-shared information"
  - [Section 4]: "By subtracting the sibling mean, the shared hierarchical structure is canceled, isolating target-specific representations"
  - [corpus]: Weak direct evidence; related unlearning work (MLLMEraser, OFFSIDE) focuses on flat structures rather than hierarchical sibling relationships
- Break condition: If siblings share no meaningful representations with the target (e.g., highly heterogeneous nodes), the differential direction may not effectively isolate forget-specific information.

### Mechanism 2
- Claim: Orthogonal subspace projection removes forget-specific weight components while preserving orthogonal information
- Mechanism: Given projection basis $Q^{(l)}$ from SVD, CHIP updates weights as $W^{(l)}_{S,:} \leftarrow (I - Q^{(l)}(Q^{(l)})^\top)W^{(l)}_{S,:}$, projecting onto the orthogonal complement of the forget-specific subspace. Only top-k% highest-importance neurons are modified.
- Core assumption: Forget-specific information is linearly encoded in a low-rank subspace of selected weight rows.
- Evidence anchors:
  - [Section 4]: "This projection removes components aligned with the forget-specific subspace while preserving orthogonal information"
  - [Table 5]: Zero-out pruning produces negative F/R Diff (−4.07 to −1.05) while subspace projection achieves positive values (6.56 to 12.60), suggesting orthogonal preservation matters
  - [corpus]: LEACE (Belrose et al. 2023) provides theoretical grounding for closed-form concept erasure via orthogonal projection
- Break condition: If forget and retain information are not approximately linearly separable in weight space, projection may either leave residual memorization or damage retain performance.

### Mechanism 3
- Claim: Joint modification of language and VL merger layers is necessary for effective multimodal unlearning
- Mechanism: CHIP applies weight projection to both upper language layers (22-27) and vision-language merger layers, using separately aggregated visual and textual activations with weighted combination $a^{(l)} = \alpha \cdot a^{(l)}_{vision} + (1-\alpha) \cdot a^{(l)}_{lang}$.
- Core assumption: Cross-modal associations encoding forgotten information require coordinated removal across modalities.
- Evidence anchors:
  - [Section 4]: "CHIP therefore applies surgery jointly to upper language and vision-language merger layers, ensuring that both unimodal and cross-modal representations related to the target are removed"
  - [Table 5]: Lang-only variant achieves lower F/R Diff than full model across all levels (1.67 vs 6.56 at Section level)
  - [corpus]: MANU (Liu et al. 2025b) similarly uses modality-aware neuron selection, supporting cross-modal importance
- Break condition: If vision and language representations are not coupled in the target knowledge, multimodal intervention may add computational overhead without benefit.

## Foundational Learning

- **Machine Unlearning**:
  - Why needed here: Understand the deletion-utility trade-off that existing methods struggle with, and why flat unlearning fails for hierarchical data
  - Quick check question: Can you explain why gradient ascent alone causes utility degradation in unlearning?

- **Orthogonal Projection / SVD**:
  - Why needed here: CHIP's core operation projects weights onto the null space of forget-specific directions
  - Quick check question: Given matrix $Q$ with orthonormal columns, what does $I - QQ^\top$ compute?

- **Vision-Language Fusion in MLLMs**:
  - Why needed here: CHIP targets both language backbone and VL merger layers, requiring understanding of where cross-modal binding occurs
  - Quick check question: In a typical MLLM architecture, which layers connect visual tokens to language representations?

## Architecture Onboarding

- **Component map**:
  1. **Activation Collection**: Extracts activations from language + VL merger layers, pooling vision/language tokens separately
  2. **Neuron Selection**: Computes importance scores $s^{(l)}_i = I^{(l)}_{forget,i} - I^{(l)}_{retain,i}$, selects top k%
  3. **Direction Computation**: For each target, computes sibling-differential direction; aggregates via SVD to basis $Q^{(l)}$
  4. **Weight Projection**: Applies $(I - Q^{(l)}(Q^{(l)})^\top)$ to selected neuron weights

- **Critical path**: Sibling activation collection → Direction computation (requires sibling data access) → SVD aggregation → Layer-wise projection

- **Design tradeoffs**:
  - k% neuron selection: Lower k = more conservative, may leave residual; higher k = aggressive, risks over-pruning
  - SVD variance threshold τ: Controls rank r of projection basis
  - Layer selection: Empirical choice (layers 22-27 + merger); earlier layers may encode more general features

- **Failure signatures**:
  - Negative F/R Diff: Indicates retain performance dropped below forget (seen in zero-out pruning ablation)
  - High forget-rephrase scores: Suggests paraphrased prompts still recover forgotten content
  - Hierarchical reconstruction leakage: Coarse-level unlearning still leaks via fine-grained sibling context

- **First 3 experiments**:
  1. **Baseline sanity check**: Run vanilla model on forget/retain splits to establish F/R Diff ≈ 0 (Table 2 shows −0.61 to 0.82)
  2. **Ablation order**: Test w/o Sibling first (Table 5 shows largest degradation: F/R Diff drops from 6.56→2.25 at Section level)
  3. **Hyperparameter sweep**: Vary k% (neuron ratio) and τ (SVD threshold) on Section level; monitor F/R Diff as primary metric to detect indiscriminate destruction vs selective forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training-based unlearning methods be enhanced to better exploit hierarchical structure, potentially surpassing training-free approaches like CHIP on the deletion-utility trade-off?
- Basis in paper: [explicit] The paper states that "training-based approaches preserve retain and general medical performance but leave substantial residual memorization, while aggressive training-free methods achieve stronger deletion at the cost of significant utility degradation."
- Why unresolved: CHIP achieves best F/R Diff as a training-free method, but the paper does not explore whether training-based methods could incorporate sibling-differential objectives to improve their balance.
- What evidence would resolve it: Systematic comparison of hierarchy-aware training objectives (e.g., contrastive sibling loss) against CHIP across all hierarchy levels.

### Open Question 2
- Question: How well do hierarchy-aware unlearning methods generalize to other medical imaging modalities beyond chest X-rays (e.g., CT, MRI, pathology slides)?
- Basis in paper: [inferred] MEDFORGET is built exclusively on MIMIC-CXR chest radiographs; no experiments or discussion addresses other imaging modalities that also exhibit hierarchical data structures.
- Why unresolved: Different modalities have distinct visual features, report structures, and clinical workflows that may affect how information is hierarchically encoded.
- What evidence would resolve it: Evaluation of CHIP on multimodal datasets with CT or MRI paired with radiology reports, using analogous institution–patient–study–section hierarchies.

### Open Question 3
- Question: How robust is CHIP under multiple sequential unlearning requests, where overlapping hierarchical nodes are forgotten over time?
- Basis in paper: [inferred] The paper evaluates single unlearning requests per hierarchy level; it does not address cumulative effects when, for example, one patient and then a sibling patient are both forgotten.
- Why unresolved: Sequential projections may compound errors or degrade sibling-shared representations in ways that single-request experiments do not reveal.
- What evidence would resolve it: Multi-round unlearning experiments measuring F/R Diff and retain performance decay as progressively more nodes are removed from the hierarchy.

### Open Question 4
- Question: What is the optimal balance (α) between vision and language activations when computing sibling-differential directions across diverse tasks and hierarchy levels?
- Basis in paper: [inferred] The ablation shows Vision-Lang Separation improves F/R Diff, but the paper uses a fixed α without exploring whether task-specific or level-specific tuning could yield better trade-offs.
- Why unresolved: Classification, cloze, and generation tasks may rely differently on visual versus textual representations.
- What evidence would resolve it: Grid search over α values per task type and hierarchy level, reporting F/R Diff and downstream utility.

## Limitations

- The benchmark uses synthetic institutions constructed from MIMIC-CXR, which may not fully capture real-world institutional data patterns
- The evaluation focuses primarily on factuality and ROUGE-L metrics, but does not extensively validate whether CHIP truly removes fine-grained medical knowledge versus merely degrading performance on related prompts
- The sibling-differential direction mechanism assumes hierarchical structure exists in the activation space, but this may not hold for all medical datasets or modalities

## Confidence

- **High confidence**: The hierarchical structure of MEDFORGET and the basic observation that existing methods show deletion-utility trade-offs are well-established and clearly demonstrated
- **Medium confidence**: The core CHIP mechanism and its superior performance relative to baselines is supported by the evidence, though some claims about sibling-shared information cancellation could benefit from more direct validation
- **Medium confidence**: The robustness claims against hierarchical reconstruction attacks are demonstrated but could be strengthened with additional attack variants and larger-scale validation

## Next Checks

1. **Knowledge verification test**: After CHIP unlearning, test the model with paraphrased medical questions that target the same underlying knowledge as forgotten content. If the model still performs well, this would indicate incomplete knowledge removal rather than degraded general performance.

2. **Cross-institutional validation**: Apply CHIP to unlearning tasks where sibling nodes come from different real hospitals rather than synthetically grouped patients. This would test whether the sibling-differential mechanism generalizes beyond the MIMIC-CXR-based synthetic hierarchy.

3. **Longitudinal performance tracking**: Monitor the model's general medical knowledge retention over multiple unlearning operations. If repeated CHIP applications cause progressive degradation in general medical tasks, this would indicate that the utility preservation claim may not scale well with multiple unlearning operations.