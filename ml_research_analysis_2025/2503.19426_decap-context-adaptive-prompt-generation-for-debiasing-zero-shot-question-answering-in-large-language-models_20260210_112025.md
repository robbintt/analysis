---
ver: rpa2
title: 'DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question
  Answering in Large Language Models'
arxiv_id: '2503.19426'
source_url: https://arxiv.org/abs/2503.19426
tags:
- question
- bias
- answer
- decap
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of bias propagation in Large
  Language Models (LLMs) when answering socially sensitive questions in zero-shot
  Question Answering tasks. The proposed DeCAP method introduces Context-Adaptive
  Prompt Generation, which consists of two key components: Question Ambiguity Detection
  and Neutral Answer Guidance Generation.'
---

# DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models

## Quick Facts
- arXiv ID: 2503.19426
- Source URL: https://arxiv.org/abs/2503.19426
- Authors: Suyoung Bae; YunSeok Choi; Jee-Hyung Lee
- Reference count: 40
- Primary result: 22.87% accuracy improvement on BBQ benchmark while reducing bias by 12.78%

## Executive Summary
This paper introduces DeCAP, a method for mitigating social bias in Large Language Models during zero-shot Question Answering tasks. DeCAP employs Context-Adaptive Prompt Generation with two key components: Question Ambiguity Detection and Neutral Answer Guidance Generation. The method first classifies questions as ambiguous or unambiguous based on context-reasoning overlap using ROUGE scores, then generates neutral guidance from external knowledge to prevent bias propagation. Across eight LLMs tested on BBQ and UNQOVER benchmarks, DeCAP achieves state-of-the-art performance with significant accuracy gains and bias reductions.

## Method Summary
DeCAP addresses bias propagation in LLMs by implementing a two-stage adaptive prompting system. First, a question ambiguity detector generates reasoning for each question-context pair and calculates ROUGE similarity between the reasoning and context. If similarity falls below 0.35, the context is classified as ambiguous; otherwise, it's unambiguous. Second, based on this classification, DeCAP retrieves neutral demonstrations from the SQUARE dataset and generates a context-appropriate guidance sentence. The final prompt combines the selected prefix instruction, original context, neutral guidance, and question before being presented to the target LLM for answering.

## Key Results
- BBQ benchmark: 22.87% accuracy improvement over baseline methods
- UNQOVER benchmark: 52.89% accuracy improvement over baseline methods
- BBQ bias reduction: 12.78% decrease in bias scores
- UNQOVER bias reduction: 2.27% decrease in bias scores
- Outperforms state-of-the-art debiasing methods including SD, Def-1, and Def-2 across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Context-Reasoning Overlap as Ambiguity Proxy
Question ambiguity can be detected by measuring lexical overlap between the LLM's reasoning and the provided context. When context is unambiguous, the LLM's reasoning heavily references the context (high ROUGE score). When context is ambiguous, the LLM generates reasoning that draws from internal knowledge beyond the context (low ROUGE score). The method assumes LLMs generate "convincing explanations" that reflect their reasoning sources. Evidence shows DeCAP's detector achieves 88.1% accuracy on ambiguous and 87.9% on unambiguous classification, with ROUGE scores showing clearer separation between ambiguous (mean=0.251) and unambiguous (mean=0.448) questions than BERT-score.

### Mechanism 2: Type-Conditional Instruction Reduces Trade-offs
Applying different instructions based on detected ambiguity type reduces performance trade-offs between question types. Ambiguous contexts receive instructions to select "unknown" when information is insufficient, preventing stereotyped guesses. Unambiguous contexts receive instructions to answer based on provided information, preserving accuracy. Evidence shows DeCAP reduces the accuracy gap between ambiguous and unambiguous questions compared to existing methods, with accuracy increasing from 18.7% → 92.0% (ambiguous) and 11.0% → 71.2% (unambiguous) when detection is correct.

### Mechanism 3: External Neutral Knowledge Suppresses Internal Bias
Retrieving and injecting neutral guidance sentences from an external dataset reduces the influence of internal biased knowledge on answers. Demonstrations from SQUARE (containing sensitive questions with acceptable, neutral responses) are retrieved based on embedding similarity. A guidance generator LLM then produces a neutral sentence that contextually connects to the question, which is inserted after the context in the final prompt. Even when detection is incorrect, adding neutral guidance reduces bias score from 17.2 → 5.9 (unambiguous, incorrect detection).

## Foundational Learning

- **ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)**
  - Why needed here: Used as the core signal for ambiguity detection—measures n-gram overlap between context and LLM reasoning to determine if context is sufficient.
  - Quick check question: Given two sentences—"The cat sat on the mat" and "The feline rested on the rug"—would ROUGE-1 be higher or lower than for "The cat sat on the mat" and "A dog ran outside"?

- **Few-shot In-Context Learning with Retrieval**
  - Why needed here: The neutral answer guidance generator relies on retrieving relevant demonstrations from SQUARE and presenting them as examples to guide generation.
  - Quick check question: Why might retrieving demonstrations based on embedding similarity perform better than using random examples from the same dataset?

- **Bias Score Calculation in QA Benchmarks**
  - Why needed here: Understanding how BBQ and UNQOVER quantify bias is essential for interpreting the reported 12.78% and 2.27% reductions.
  - Quick check question: In ambiguous questions where the correct answer is always "Unknown," what does a high non-unknown response rate indicate about model bias?

## Architecture Onboarding

- **Component map:**
  1. Question Ambiguity Detector (Llama3-instruct, temp=0.6): Generates reasoning → ROUGE calculation → threshold comparison (0.35) → outputs classification
  2. Prefix Selector: Maps classification to predefined instruction (p_ambig or p_unambig)
  3. Demonstration Retriever (MPNet embeddings): Encodes question+context → retrieves top-5 similar pairs from SQUARE
  4. Neutral Answer Guidance Generator (Llama3-instruct): Few-shot prompt with retrieved demonstrations → generates neutral sentence
  5. Final QA Prompt Assembler: Combines prefix + context + neutral guidance + question + options

- **Critical path:**
  Input question → Detector generates reasoning → ROUGE vs context → classify → select prefix → retrieve demos → generate guidance → assemble prompt → target LLM answers

- **Design tradeoffs:**
  - ROUGE vs BERT-score: ROUGE chosen because it shows clearer separation (Appendix B.1); BERT-score has higher variance and closer means between types
  - Threshold = 0.35: Selected from 0.3-0.4 range based on moderate similarity interpretation; performance stable across ±0.05 (Table 7)
  - Top-5 demonstrations: Not explicitly ablated; assumes sufficient context for generation without excessive token cost
  - SQUARE dataset: External neutral source; assumption is that human-machine collaboration produces acceptable responses

- **Failure signatures:**
  1. High "Out-of-Answer" rate: Target LLM ignores multiple-choice format and generates text
  2. Ambiguity detector has poor accuracy: Classification fails, leading to wrong Prefix Instructions
  3. Degraded performance on Unambiguous questions: Model selects "Unknown" even when facts are present

- **First 3 experiments:**
  1. Reproduce detector accuracy table (Table 2): Implement ROUGE-based detector on BBQ subset; compare against Llama-3 and GPT-3.5 baselines
  2. Ablate neutral guidance (DeCAP w/o g): Run full pipeline without guidance generation step; expect accuracy drop from 69.80 → ~69.05 on BBQ
  3. Test threshold sensitivity: Sweep threshold from 0.30 to 0.40 on BBQ; verify performance stability matches Table 7

## Open Questions the Paper Calls Out

### Open Question 1
Can DeCAP effectively mitigate bias in open-ended generation tasks where a multiple-choice "unknown" option is unavailable? The current architecture and evaluation rely heavily on the presence of an "unknown" option to handle ambiguity, which does not exist in free-form generation.

### Open Question 2
Does the generated Neutral Answer Guidance align with human judgments of neutrality and coherence? While efficient, automated evaluation by LLMs may not fully capture the subtle social nuances or "unbiased" nature of text as perceived by human users.

### Open Question 3
Is the ROUGE-based ambiguity detector robust across diverse linguistic domains and data distributions? Similarity scores like ROUGE are sensitive to text length and domain-specific vocabulary; the fixed threshold may fail on domains with distinct linguistic patterns.

## Limitations
- The method is currently limited to multiple-choice QA formats and cannot handle open-ended generation tasks
- Effectiveness on non-socially sensitive domains remains untested
- Computational overhead of generating reasoning and guidance for each question could limit practical deployment
- Reliance on external dataset (SQUARE) quality without verification of absence of subtle biases

## Confidence

**High Confidence (8/10):** The core mechanism of using context-reasoning overlap for ambiguity detection is well-supported by quantitative evidence showing clear separation between ambiguous and unambiguous questions, with statistically significant accuracy improvements across eight LLMs.

**Medium Confidence (6/10):** The effectiveness of external neutral knowledge suppression depends on the quality and representativeness of the SQUARE dataset, though reported coherence and neutrality rates are promising.

**Low Confidence (4/10):** Generalizability to domains beyond social bias and scalability to larger model families remain uncertain, with computational costs and latency implications not characterized.

## Next Checks

1. **Faithfulness validation:** Conduct a human evaluation study where annotators assess whether the LLM's generated reasoning actually reflects the knowledge sources used, testing the core assumption that ROUGE overlap indicates context sufficiency.

2. **Cross-domain robustness test:** Apply DeCAP to non-socially sensitive domains (e.g., technical QA, medical diagnosis) to evaluate whether the ambiguity detection and neutral guidance mechanisms generalize beyond social bias scenarios.

3. **Computational overhead measurement:** Benchmark the end-to-end latency of DeCAP (including reasoning generation, ROUGE calculation, demonstration retrieval, and guidance generation) compared to baseline methods across different hardware configurations to assess practical deployment viability.