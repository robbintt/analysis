---
ver: rpa2
title: 'DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors'
arxiv_id: '2512.04799'
source_url: https://arxiv.org/abs/2512.04799
tags:
- sentences
- corruption
- linguistic
- danish
- acceptability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DaLA, a new Danish linguistic acceptability
  benchmark that improves upon existing resources by introducing 14 corruption types
  derived from real-world Danish errors. The dataset is constructed by systematically
  corrupting correct Danish sentences using predefined functions, with quality validated
  through both automatic and manual methods.
---

# DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors

## Quick Facts
- **arXiv ID:** 2512.04799
- **Source URL:** https://arxiv.org/abs/2512.04799
- **Reference count:** 14
- **Key outcome:** DaLA is a Danish linguistic acceptability benchmark using 14 real-world error types that shows 6.04% lower LLM performance than ScaLA while better distinguishing high-performing from underperforming models

## Executive Summary
This paper presents DaLA, a new Danish linguistic acceptability benchmark that improves upon existing resources by introducing 14 corruption types derived from real-world Danish errors. The dataset is constructed by systematically corrupting correct Danish sentences using predefined functions, with quality validated through both automatic and manual methods. The resulting benchmark consists of 3,328 sentence pairs (extended versions available up to 7,656 sentences) and demonstrates higher difficulty and discriminatory power than the current state-of-the-art ScaLA dataset. When evaluated on nine open-source and open-weight LLMs, DaLA shows an average performance decrease of 6.04% compared to ScaLA, with some models experiencing drops up to 14.55%. The results indicate DaLA better distinguishes between high-performing and underperforming models, making it a more rigorous assessment of linguistic acceptability for Danish.

## Method Summary
The DaLA benchmark is constructed by corrupting grammatically correct Danish sentences from the Universal Dependencies Danish corpus using 14 error-specific functions derived from real-world Danish errors documented in the Danish Language Council's "Typiske Problemer" resource. The corruption process applies functions iteratively in frequency-sorted order to ensure balanced error type representation, validated through a combination of automatic grammaticality detection (writeassistant) and manual annotation. The final dataset consists of minimal pairs (grammatical vs. ungrammatical sentences) split into train/validation/test sets with minimal distribution drift. Evaluation uses the Matthews Correlation Coefficient as the primary metric, with encoder-only models fine-tuned and decoder/encoder-decoder models evaluated few-shot.

## Key Results
- DaLA achieves an average MCC of 0.676 compared to ScaLA's 0.736, representing a 6.04% performance decrease across nine evaluated LLMs
- Reasoning models show the largest performance decline (14.32% vs ScaLA), suggesting DaLA better captures complex linguistic acceptability judgments
- The benchmark successfully distinguishes between high-performing and underperforming models, with the best model (Gemini Pro 1.0) achieving 0.839 MCC versus the worst (GPT-2) at 0.439

## Why This Works (Mechanism)

### Mechanism 1: Real-World Error-Guided Corruption Design
Corruption types derived from attested Danish errors produce a more discriminatory benchmark than generic syntactic manipulations. Authors analyzed "Typiske Problemer" (a Danish Language Council resource), scored errors by implementation complexity and error salience, then implemented 14 corruption functions targeting phenomena like nogle/nogen confusion, the "R problem" in suffixes, and ligge/lægge verb confusion. The core assumption is that errors common in native Danish writing are more diagnostic of genuine linguistic competence than artificial word-swap or deletion operations. Evidence shows reasoning models experienced 14.32% average decline vs. ScaLA, suggesting the benchmark exposes weaknesses not captured by simpler corruptions.

### Mechanism 2: Frequency-Balanced Iterative Corruption
Prioritizing rare corruption types prevents generic "basic corruptions" from dominating the dataset. The algorithm sorts corruption functions by applicability frequency, applies least-applicable corruptions first, then marks sentences as corrupted to prevent re-processing. The core assumption is that a balanced error-type distribution yields better model discrimination than a natural-frequency distribution. Evidence shows "basic corruptions" have potential coverage of 100% but are constrained to ~10% of actual training corruptions, ensuring diverse error representation.

### Mechanism 3: Hybrid Validation with Precision Estimation
Combining automatic grammaticality detection with targeted manual annotation achieves high corruption precision without exhaustive human review. The system runs Writeassistant (a Danish grammar checker) on all corrupted sentences, manually annotates false positives for high-frequency corruptions, and estimates true precision via equation `prec_new = tp_new / #corruptions`. The core assumption is that a high-precision automatic system plus stratified manual sampling generalizes to the full dataset. Evidence shows average precision of 99.7% for identifying truly corrupted sentences, with final precision scores ranging from 0.80 to 1.0 across corruption types.

## Foundational Learning

- **Concept: Linguistic Minimal Pairs**
  - Why needed here: DaLA follows the BLiMP paradigm—paired sentences differing by a single error—rather than isolated acceptability judgments.
  - Quick check question: Why does a minimal-pair design reduce annotation noise compared to collecting independent acceptability ratings?

- **Concept: Matthews Correlation Coefficient (MCC)**
  - Why needed here: MCC is the primary evaluation metric, chosen for robustness to class imbalance.
  - Quick check question: For a benchmark with 50% acceptable/unacceptable split, would MCC and F1 give identical model rankings? Why or why not?

- **Concept: POS-Tag and Dependency-Based Corruption**
  - Why needed here: Many corruption functions (e.g., indefinite determiner swap, som/der confusion) rely on dependency labels and morphological features rather than surface patterns.
  - Quick check question: If your source sentences lack POS tags, which corruption types would become impossible to apply reliably?

## Architecture Onboarding

- **Component map:** Source Sentences (UD Danish) -> Cleaning Filters -> Corruption Functions (14 error-specific + 2 basic) -> Validation Pipeline (Writeassistant → manual subset annotation) -> Dataset Splits -> LLM Evaluation (fine-tuning for encoders, few-shot for decoders)

- **Critical path:**
  1. Extract sentences from Universal Dependencies Danish corpus
  2. Apply Algorithm 1 with frequency-sorted corruption functions
  3. Validate corruption precision; filter or re-annotate low-precision cases
  4. Generate minimal pairs; split with JS divergence < 0.01 between train/test
  5. Run evaluation via EuroEval framework with MCC metric

- **Design tradeoffs:**
  - Conservative filtering vs. coverage: Strict applicability criteria reduce false positives but may exclude valid corruption opportunities
  - Single-error vs. multi-error corruption: Currently limited to one error per sentence; multi-error could increase realism but complicate error attribution
  - Generic fallbacks vs. purity: Basic corruptions ensure full coverage but dilute real-world error focus

- **Failure signatures:**
  - Corrupted sentences still acceptable → precision drops below 0.90 for any corruption type
  - Training-test distribution drift → JS divergence exceeds 0.05; model performance becomes unpredictable
  - Reasoning models underperform dramatically → may indicate benchmark captures non-grammatical artifacts

- **First 3 experiments:**
  1. Corruption validation replication: Sample 50 corrupted sentences per error type; manually verify unacceptability; compute precision and compare to Table 1
  2. Cross-benchmark comparison: Evaluate an untested LLM on both ScaLA and DaLA; verify MCC drop of 4–8 percentage points
  3. Generalization test: Apply corruption functions to a different Danish corpus (e.g., DaKultur sentences); measure corruption applicability rates and validate a subsample

## Open Questions the Paper Calls Out
None

## Limitations
- **Corpus Coverage and Representativeness**: The benchmark is built exclusively from the Universal Dependencies Danish corpus, which may not capture the full range of real-world Danish usage patterns.
- **Corruption Function Implementation Details**: While the paper provides high-level descriptions of the 14 corruption types, the exact implementation rules and edge-case handling are only available in Appendix A.
- **Automatic Validation Reliability**: The writeassistant tool achieves 99.7% average precision for identifying corrupted sentences, but this depends on the tool's specific error category mappings and internal algorithms.

## Confidence
- **High Confidence**: The benchmark construction methodology (iterative corruption with frequency balancing, hybrid validation) is well-specified and reproducible.
- **Medium Confidence**: The claim that real-world error-guided corruptions are more discriminatory than generic syntactic manipulations is supported by reasoning model results but could benefit from ablation studies.
- **Low Confidence**: The assertion that DaLA better distinguishes between high-performing and underperforming models is based on correlation analysis that isn't fully detailed in the paper.

## Next Checks
1. **Corruption Precision Validation**: Sample 50 corrupted sentences from each of the 14 error types and manually verify unacceptability using independent Danish linguistic expertise. Compute per-corruption type precision and compare to the reported Table 1 values to identify any systematic over- or under-generation issues.

2. **Cross-Corpus Generalization Test**: Apply the same corruption functions to a different Danish corpus (e.g., DaKultur or a subset of Danish Wikipedia) and measure: (a) corruption applicability rates for each function, (b) precision of corrupted sentences when validated with writeassistant, and (c) whether the resulting dataset maintains similar performance gaps between models compared to the original DaLA.

3. **Ablation Study on Corruption Strategy**: Create three versions of the benchmark using: (a) only the 14 real-world error corruptions, (b) only the 2 basic ScaLA corruptions, and (c) a random mix of both. Evaluate the same set of models on all three versions to quantify the contribution of error-specific vs. generic corruptions to the overall benchmark difficulty and discrimination power.