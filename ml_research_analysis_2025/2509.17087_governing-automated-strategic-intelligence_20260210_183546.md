---
ver: rpa2
title: Governing Automated Strategic Intelligence
arxiv_id: '2509.17087'
source_url: https://arxiv.org/abs/2509.17087
tags:
- intelligence
- accessed
- data
- systems
- national
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the potential for AI to democratize high-quality
  intelligence analysis, a capability traditionally limited to skilled analysts. It
  conducts a preliminary uplift study comparing novice intelligence analysts with
  and without LLM assistance to skilled analysts, using a novel similarity metric
  to measure analytical quality.
---

# Governing Automated Strategic Intelligence

## Quick Facts
- **arXiv ID**: 2509.17087
- **Source URL**: https://arxiv.org/abs/2509.17087
- **Reference count**: 28
- **Primary result**: LLM-assisted novices' responses were significantly more similar to those of skilled analysts than novices without assistance (β = 0.148, p = 0.0355).

## Executive Summary
This study investigates whether large language models can democratize high-quality intelligence analysis, a capability traditionally limited to skilled analysts. Through a controlled uplift study, the research team compared novice intelligence analysts with and without LLM assistance to skilled analysts using a novel similarity metric. The experiment involved 20 novices (with ~39 hours average experience) and 2 skilled analysts (with 350+ hours experience) answering 12 intelligence questions across numeric and conceptual categories. The core finding demonstrates that LLM assistance substantially improves novice performance, with assisted novices producing responses significantly more similar to those of skilled analysts than unassisted novices. This suggests publicly available LLMs can effectively bridge the experience gap in intelligence analysis.

## Method Summary
The study employed a controlled uplift design where novice participants were randomly assigned to either a control group (no LLM assistance) or a treatment group (LLM-assisted). All participants answered 12 intelligence questions with a three-hour time limit per question. Responses were evaluated using a novel similarity metric: numeric answers were compared using symmetric log-ratio similarity with adaptive error tolerances, while conceptual answers were scored by Gemini 2.5 Pro's semantic similarity function. The analysis used OLS regression with fixed effects for metric type and question, clustering standard errors at the participant level. Robustness checks included stratified permutation tests and paired residual-difference tests. The similarity scores ranged from 0 (no similarity) to 1 (perfect similarity), providing a quantitative measure of analytical quality.

## Key Results
- LLM-assisted novices produced responses with significantly higher similarity to skilled analysts' answers (β = 0.148, p = 0.0355)
- The effect was consistent across both numeric and conceptual question types
- Robustness checks aligned in direction, with one achieving statistical significance
- LLM assistance effectively reduced the performance gap between novices and experienced analysts

## Why This Works (Mechanism)
The mechanism appears to operate through LLM-assisted structuring and refinement of analytical reasoning. The study found that assisted novices produced responses with significantly higher similarity to skilled analysts' answers (β = 0.148, p = 0.0355), suggesting that LLMs help bridge the gap in analytical quality between experienced and novice analysts. However, the specific pathways through which LLM assistance improves analytical reasoning remain unspecified in the source material.

## Foundational Learning
- **Uplift study design**: Why needed - To isolate the causal effect of LLM assistance on novice performance; Quick check - Verify randomization was successful and groups are balanced
- **Similarity metrics for intelligence analysis**: Why needed - To create a quantitative measure of analytical quality; Quick check - Confirm metric range and interpretation align with research objectives
- **Fixed effects regression with clustering**: Why needed - To control for question difficulty and participant-level variation; Quick check - Review coefficient stability across different clustering levels
- **Symmetric log-ratio similarity**: Why needed - To handle proportional data in numeric answer comparison; Quick check - Validate that τ parameter appropriately handles different question scales
- **LLM-as-judge methodology**: Why needed - To automate evaluation at scale; Quick check - Test for systematic bias in LLM scoring patterns

## Architecture Onboarding

**Component Map**: Novices (Control/LLM-assisted) -> Answer Questions -> LLM Extraction/Scoring -> Similarity Calculation -> OLS Regression -> Results

**Critical Path**: Question assignment → Answer collection → LLM evaluation → Similarity computation → Statistical analysis

**Design Tradeoffs**: Single LLM for both extraction and scoring (efficiency vs. potential bias) vs. multiple evaluators (accuracy vs. cost/time)

**Failure Signatures**: 
- LLM-as-judge bias: Assisted responses score systematically higher than unassisted
- Ground truth instability: Results change dramatically with different analyst references
- Time constraint effects: Treatment effect varies by question difficulty or response length

**First Experiments**:
1. Human-validate a 25% sample of Gemini similarity scores against expert ratings
2. Bootstrap resample analyst answers to assess ground truth stability
3. Systematically vary ε_rel parameters to test sensitivity of numeric similarity scores

## Open Questions the Paper Calls Out
The study does not explicitly identify open questions in the source material. However, several areas warrant further investigation based on the methodology and findings:
- The generalizability of LLM-assisted performance improvements across different types of intelligence analysis
- The long-term impact of LLM assistance on novice skill development
- The potential for systematic bias when using a single LLM for both extraction and scoring
- The optimal error tolerance parameters for different types of intelligence questions

## Limitations
- Reliance on a single LLM (Gemini 2.5 Pro) for both answer extraction and similarity scoring introduces potential bias
- Only two skilled analysts provided ground truth answers, limiting reference standard stability
- Three-hour time constraint per question may differentially affect novice performance
- Unspecified question categorization and error tolerance parameters limit precise replication

## Confidence
- **High Confidence**: The directional finding that LLM assistance improves novice performance (β = 0.148, p = 0.0355) is robust across multiple specification checks
- **Medium Confidence**: The magnitude of the treatment effect and its practical significance require additional validation due to sample size constraints and potential LLM-as-judge bias
- **Low Confidence**: The specific error tolerance parameters (ε_abs, ε_rel) and their application across different question types could materially affect numeric similarity calculations

## Next Checks
1. **Human Validation Sample**: Recruit 2-3 additional skilled analysts to independently score a random 25% sample of LLM-assisted and unassisted novice responses, comparing human-derived similarity scores against Gemini's evaluations to quantify potential model bias.

2. **Sensitivity Analysis on Error Parameters**: Systematically vary ε_rel from 10^-6 to 10^-3 across all numeric questions and document how similarity scores and treatment effects change, establishing bounds on result sensitivity to this critical parameter.

3. **Analyst Ground Truth Stability Test**: Conduct bootstrap resampling of the two analysts' answers (sampling with replacement) 1,000 times to generate confidence intervals around the reference answers and assess how much instability in the ground truth affects the estimated treatment effect.