---
ver: rpa2
title: Heart Disease Prediction using Case Based Reasoning (CBR)
arxiv_id: '2512.13078'
source_url: https://arxiv.org/abs/2512.13078
tags:
- heart
- disease
- data
- will
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a heart disease prediction system using Case-Based
  Reasoning (CBR) to address the limitations of traditional diagnostic methods relying
  on doctor experience. The CBR algorithm processed a heart disease dataset through
  data pre-processing and splitting into training and testing sets.
---

# Heart Disease Prediction using Case Based Reasoning (CBR)

## Quick Facts
- arXiv ID: 2512.13078
- Source URL: https://arxiv.org/abs/2512.13078
- Reference count: 28
- Achieved 97.95% accuracy in heart disease prediction

## Executive Summary
This study develops a heart disease prediction system using Case-Based Reasoning (CBR) to address limitations of traditional diagnostic methods. The CBR algorithm processes a heart disease dataset through data pre-processing and splitting into training and testing sets. The approach achieves 97.95% accuracy in predicting heart disease outcomes, with analysis revealing males have higher probability (57.76%) compared to females (42.24%). The research demonstrates CBR as an effective intelligent system for accurate heart disease prediction, outperforming traditional diagnostic approaches.

## Method Summary
The study implements a CBR approach using the Kaggle heart disease dataset (1,025 entries, 13 features). Data undergoes min-max normalization to [0,1] scale, then splits 60:40 (615 training, 410 testing) using non-random sequential sampling. The CBR engine calculates local similarity for each attribute, computes global similarity as weighted sum (all weights=1), and for each test case finds training case with highest global similarity to assign diagnosis. The method uses 4 CBR stages: retrieve, reuse, revise, retain, with reuse directly copying the solution from the most similar case without modification.

## Key Results
- Achieved 97.95% prediction accuracy on heart disease classification
- Males show higher heart disease probability (57.76%) compared to females (42.24%)
- CBR outperforms traditional diagnostic approaches for heart disease prediction

## Why This Works (Mechanism)

### Mechanism 1
Prediction accuracy depends on aggregation of normalized attribute distances (Global Similarity). The system calculates Local Similarity scores for 13 attributes between test and training cases, aggregating into Global Similarity score using weighted sum. Solution of training case with highest Global Similarity is reused. Assumes similar problems have similar solutions. Accuracy degrades if feature weights are set uniformly for attributes with low predictive power.

### Mechanism 2
Data normalization enables similarity calculation by equalizing feature scales. Raw medical data is transformed into 0-1 range to prevent attributes with large numerical ranges from dominating distance calculation. Assumes linear and uniform relationship between min/max range and diagnosis across all features. Performance degrades if dataset contains significant outliers that compress normal data into small sub-range.

### Mechanism 3
The "Reuse" phase functions as nearest-neighbor classifier without adaptation. Unlike systems that might adapt solutions, this system directly copies output of single most similar case. Assumes dataset is dense enough that near-perfect match exists in training set for any test case. Provides no confidence score or nuanced prediction when test case falls between two classes.

## Foundational Learning

- **Concept: Min-Max Normalization**
  - Why needed: Ensures attributes like Serum Cholesterol (mg/dL) don't numerically overpower binary attributes during similarity calculation
  - Quick check: If max cholesterol in training is 300 but test has 400, what happens to normalized value? (Likely clips to 1.0 or causes error)

- **Concept: Local vs. Global Similarity**
  - Why needed: Core logic of CBR engine; Local compares one attribute, Global combines them to find single closest historical patient
  - Quick check: Does system use weighted average or simple sum for Global Similarity? (Check "weightage" declaration)

- **Concept: Non-Random Sampling (Data Splitting)**
  - Why needed: Uses 60:40 split with non-random sampling, implying ordering matters rather than shuffled distribution
  - Quick check: Why might using most current data as test set simulate real-world deployment better than random shuffling?

## Architecture Onboarding

- **Component map:** Data Ingestion (heart.csv) -> Preprocessor (Min-Max Normalization) -> Splitter (60:40 Non-Random) -> CBR Engine (Retriever, Reuser, Retainer) -> Evaluator (Compare predicted vs actual)

- **Critical path:** Normalization parameters (Min/Max of training data) -> Similarity Calculation (Euclidean/Local) -> Argmax (Selecting highest Global Similarity)

- **Design tradeoffs:** Speed vs. Memory (CBR is slow with large datasets due to O(N*M) complexity); Static vs. Dynamic Weights (uniform weights simplify implementation but ignore medical domain knowledge)

- **Failure signatures:** Accuracy Drop (if test data contains values outside training Min/Max range); Performance Lag (linear increase with database size); Stale Knowledge (if Retain phase not utilized)

- **First 3 experiments:**
  1. Reproduce Baseline: Implement CBR with uniform weights (1.0) and 60:40 non-random split to verify 97.95% accuracy
  2. Weighted Sensitivity: Alter Global Similarity to assign higher weights to high-correlation features and observe accuracy changes
  3. Scaling Stress Test: Introduce synthetic outliers into test set to verify normalization logic handling

## Open Questions the Paper Calls Out

1. How does prediction accuracy and computational latency scale when applied to datasets significantly larger than 1,025 entries?
2. Can the CBR system be adapted for real-time clinical decision support without compromising 97.95% accuracy?
3. How does the CBR model perform relative to contemporary ensemble methods (XGBoost, LightGBM) on same data splits?

## Limitations
- Limited dataset size (1,025 entries) insufficient for optimal accuracy
- Static, non-real-time prediction system unable to process live data streams
- Did not compare against contemporary ensemble methods like XGBoost or LightGBM

## Confidence
- **High confidence**: General CBR methodology and data normalization approach
- **Medium confidence**: Accuracy claim is plausible but requires exact implementation details
- **Low confidence**: Gender disparity analysis lacks statistical testing and may reflect dataset bias

## Next Checks
1. Verify exact distance metric and normalization formula used in similarity calculation
2. Replicate experiment with k-fold cross-validation to establish confidence intervals for accuracy claim
3. Test model's performance on temporally separated test set to evaluate real-world generalization