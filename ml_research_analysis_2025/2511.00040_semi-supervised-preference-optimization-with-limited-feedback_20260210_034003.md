---
ver: rpa2
title: Semi-Supervised Preference Optimization with Limited Feedback
arxiv_id: '2511.00040'
source_url: https://arxiv.org/abs/2511.00040
tags:
- data
- sspo
- preference
- unpaired
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preference optimization for
  language models, which typically requires large amounts of expensive human-labeled
  preference data. The authors propose a semi-supervised preference optimization (SSPO)
  framework that leverages both a small amount of labeled pairwise comparisons and
  a large pool of unpaired data.
---

# Semi-Supervised Preference Optimization with Limited Feedback

## Quick Facts
- arXiv ID: 2511.00040
- Source URL: https://arxiv.org/abs/2511.00040
- Authors: Seonggyun Lee, Sungjun Lim, Seojin Park, Soeun Cheon, Kyungwoo Song
- Reference count: 40
- Primary result: SSPO achieves 26.7% length-controlled win rate on 1% UltraFeedback data, outperforming best baseline (19.1%) trained on 10% data

## Executive Summary
This paper addresses the challenge of preference optimization for language models, which typically requires large amounts of expensive human-labeled preference data. The authors propose a semi-supervised preference optimization (SSPO) framework that leverages both a small amount of labeled pairwise comparisons and a large pool of unpaired data. The core method uses a theoretically grounded approach: it learns a reward function from labeled data, identifies an optimal reward threshold to pseudo-label unpaired responses, and applies adaptive scheduling to balance the influence of paired and unpaired data during training. Experiments show that SSPO significantly outperforms strong baselines, achieving superior performance with just 1% of labeled preference data compared to baselines trained on 10%.

## Method Summary
SSPO combines limited labeled pairwise preference data with large-scale unpaired responses through a theoretically grounded semi-supervised approach. The method trains a reward function on paired data, estimates an optimal threshold via kernel density estimation to separate winning from losing responses, and applies adaptive scheduling to balance paired and unpaired data influence during training. The framework uses EMA-stabilized reward normalization to track non-stationary reward distributions, with pseudo-labels assigned to unpaired data based on the learned threshold. The combined loss function weighs SimPO loss on paired data and pseudo-labeled binary classification loss on unpaired data, with the weight controlled by an adaptive scheduler that starts at 1.0 (pure paired data) and decays toward a minimum determined by dataset sizes.

## Key Results
- SSPO with 1% UltraFeedback data achieves 26.7% length-controlled win rate on Mistral-7B-Instruct, surpassing the best baseline (19.1%) trained on 10% data
- Across three domains (general, medical, business), SSPO consistently outperforms SimPO and DPO baselines when trained on 1% or 10% of preference data
- On MT-Bench, SSPO with 10% UltraFeedback achieves 6.23 score, outperforming DPO (5.95) and SimPO (5.78) baselines

## Why This Works (Mechanism)

### Mechanism 1
A statistically principled reward threshold can reliably separate winning from losing responses in unpaired data. The paper proves that under sub-Gaussian reward distributions with µ_w > µ_l, there exists a threshold δ* in the interval [µ_l + t_1, µ_w - t_2] that separates winning and losing rewards with probability ≥ 1-α. Practically, this threshold is estimated via kernel density estimation on the labeled paired data by minimizing Bayes risk. If winning/losing reward distributions have substantial overlap (µ_w ≈ µ_l), no useful threshold exists; pseudo-labels become noise.

### Mechanism 2
Adaptive scheduling creates curriculum learning that prevents confirmation bias from noisy pseudo-labels. The scheduler starts at 1.0 (pure paired data) and decays toward γ_min = n_L/(n_L + n_U). Early training relies on reliable human labels; as the reward model stabilizes and reward separation emerges, unpaired data contribution increases. If decay rate λ is too aggressive, unpaired data contributes before reward separation; if too slow, benefits of unlabeled data are delayed or lost.

### Mechanism 3
EMA-stabilized reward normalization tracks the non-stationary reward distribution without collapsing. Rewards are normalized as (r_θ - µ_t) / σ_t where µ_t and σ_t are EMA statistics (momentum m=0.95). The threshold δ̂_t is also EMA-smoothed. If reward distribution shifts rapidly (e.g., from model collapse or distributional shift in prompts), EMA lag causes threshold mismatch with true optimal boundary.

## Foundational Learning

- **Bradley-Terry Model**: The preference classifier f_θ is built on Bradley-Terry, mapping reward differences to preference probabilities via sigmoid. Understanding this connects SSPO to DPO/SimPO foundations. Quick check: Can you explain why P(y_w preferred over y_l) = σ(r(x,y_w) - r(x,y_l)) rather than a simple softmax over rewards?

- **Semi-Supervised Pseudo-labeling**: SSPO's core innovation is pseudo-labeling unpaired data. You need to understand how pseudo-labels propagate signal (and noise) and why threshold quality matters. Quick check: What happens to model performance if pseudo-label accuracy is below 50%? (Hint: gradient cancellation in Eq. B.2)

- **Kernel Density Estimation (KDE)**: The Bayes-risk threshold is computed by estimating reward densities p(r|s=1) and p(r|s=0) via KDE. Bandwidth selection affects threshold quality. Quick check: If the KDE bandwidth h is too large, what happens to the estimated threshold and resulting pseudo-label accuracy?

## Architecture Onboarding

- Component map: Paired Data (D_L) -> SimPO Loss (R_DL) -> Combined Loss; Paired Rewards -> KDE -> Bayes-Risk Threshold (δ̂) -> Pseudo-Labels (s̃) -> R_DU -> Combined Loss; Adaptive Scheduler (γ') coordinates between components; EMA Norm stabilizes threshold and reward statistics

- Critical path: (1) Train reward function on paired data until separation emerges -> (2) Estimate threshold via KDE on paired rewards -> (3) Apply EMA-normalized threshold to unpaired rewards -> (4) Compute pseudo-labeled loss weighted by adaptive scheduler

- Design tradeoffs: Prior P_DU(s=1) default 0.5 works well, but domain-specific priors could help if you know response quality distribution; EMA momentum m=0.95 balances stability and adaptation; Unpaired data source assumption that SFT data contains implicit preference signals

- Failure signatures: Threshold collapse if δ̂ converges to extreme value (all pseudo-labels same class); No improvement over baseline if paired data fraction is extremely limited (<0.1%); Training instability from oscillating EMA statistics

- First 3 experiments: (1) Validate threshold quality by training SimPO on paired data, computing winning/losing reward distributions, and running KDE threshold estimation; (2) Ablate adaptive scheduler by comparing SSPO with γ' fixed at {0.1, 0.5, 1.0} vs. adaptive; (3) Sanity check with synthetic data by replicating toy experiment with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the Bayes-risk threshold estimation to violations of the sub-Gaussian assumption in reward distributions for complex, multi-modal tasks? The theoretical guarantee assumes sub-Gaussian reward distributions, while practical implementation relies on KDE to approximate unknown distributions. Real-world reward distributions for LLMs may be multi-modal or heavy-tailed, potentially invalidating theoretical guarantees without empirical verification.

### Open Question 2
Does SSPO maintain its performance advantage when the unpaired dataset contains significantly lower quality or contradictory latent preference signals compared to the labeled set? The paper assumes SFT data contains "valuable implicit preferences," but the impact of noise in the unpaired data on real-world tasks is not explicitly ablated. If unpaired data rewards are chaotic or misleading, the pseudo-labeling threshold may fail to separate winning/losing behaviors.

### Open Question 3
Can SSPO be extended to an iterative framework to further refine the reward threshold and pseudo-labels over multiple generations? The current method estimates the threshold dynamically via EMA, but the underlying data is fixed; iteratively re-generating or re-filtering unpaired responses as the policy improves might yield tighter decision boundaries.

## Limitations
- KDE bandwidth selection methodology is underspecified and could significantly affect pseudo-label quality
- EMA stabilization contribution lacks validation beyond tracking visualization
- Domain generalization claims have limited empirical support beyond three tested domains
- The method assumes unpaired data contains implicit preference signals, which may not hold for all data sources

## Confidence
- **High**: Theoretical existence of optimal threshold (Theorem 1), SimPO baseline implementation, core experimental results (Table 2, 5)
- **Medium**: Adaptive scheduler design and effectiveness, length-controlled evaluation methodology
- **Low**: EMA stabilization contribution, KDE bandwidth selection methodology, domain generalization claims

## Next Checks
1. Ablate KDE bandwidth selection: Systematically vary h in Eq. 9 and measure threshold quality and downstream performance
2. Test scheduler decay sensitivity: Run ablation varying λ ∈ {0.0001, 0.001, 0.01} on 1% UltraFeedback
3. Validate threshold emergence timing: Monitor reward separation onset and correlate with scheduler behavior