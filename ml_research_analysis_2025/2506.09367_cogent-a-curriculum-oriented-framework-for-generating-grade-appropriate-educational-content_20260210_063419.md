---
ver: rpa2
title: 'COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational
  Content'
arxiv_id: '2506.09367'
source_url: https://arxiv.org/abs/2506.09367
tags:
- curriculum
- grade
- science
- cogent
- birds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COGENT, a curriculum-oriented framework for
  generating grade-appropriate educational content. The framework addresses challenges
  in aligning AI-generated educational materials with curriculum standards and maintaining
  consistent reading levels across grade levels, particularly for STEM education.
---

# COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content

## Quick Facts
- arXiv ID: 2506.09367
- Source URL: https://arxiv.org/abs/2506.09367
- Reference count: 23
- Primary result: COGENT significantly improves curriculum alignment and maintains appropriate reading levels for elementary science content

## Executive Summary
This paper introduces COGENT, a curriculum-oriented framework for generating grade-appropriate educational content. The framework addresses challenges in aligning AI-generated educational materials with curriculum standards and maintaining consistent reading levels across grade levels, particularly for STEM education. COGENT incorporates three curriculum components (science concepts, core ideas, and learning objectives), controls readability through length, vocabulary, and sentence complexity, and adopts a "wonder-based" approach to increase student engagement. Experiments with three LLMs (Gemma-2-9B, GPT-4o, Claude-3.5-Sonnet) demonstrate that COGENT significantly improves curriculum alignment (4.62 vs 4.08, p < .05) and maintains high comprehensibility while producing passages closer to intended grade levels compared to baseline approaches. Expert analysis shows COGENT-generated passages achieve comparable or superior quality to human-written materials, establishing a viable approach for scaling adaptive learning resources.

## Method Summary
COGENT implements a three-component framework for generating curriculum-aligned educational content. The system decomposes NGSS standards into hierarchical curriculum components (science concepts, core ideas, learning objectives) and uses these as explicit constraints in prompt engineering. For readability control, it specifies word counts (grade × 100 words) and Flesch-Kincaid grade level targets. The "wonder-based" approach transforms core ideas into curiosity-driven questions that bridge everyday observations with scientific explanations. The framework was evaluated across three LLMs (Gemma-2-9B-IT, GPT-4o, Claude-3.5-Sonnet) using a combination of LLM-as-a-judge evaluation (Claude-3.5-Sonnet) and expert teacher assessment (6 teachers) on curriculum alignment, comprehensibility, and readability metrics.

## Key Results
- COGENT achieves significantly higher curriculum alignment scores (Mean = 4.62) compared to baseline (Mean = 4.08), p < .05
- COGENT passages adhere more closely to target grade reading levels, while baseline exceeds intended levels by ~2.5 grades in lower grades
- Expert teacher ratings show COGENT-generated passages achieve comparable or superior quality to human-written materials

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Curriculum Decomposition
- Claim: Structured curriculum inputs improve alignment between generated content and pedagogical standards.
- Mechanism: NGSS standards are decomposed into three hierarchical levels—science concepts, core ideas, and learning objectives—creating explicit constraints that guide LLM generation toward grade-specific depth and terminology.
- Core assumption: LLMs can follow structured instructional scaffolding when curriculum elements are explicitly specified in prompts.
- Evidence anchors: COGENT achieves significantly higher alignment scores (Mean = 4.62) vs BASE (Mean = 4.08), p < .05

### Mechanism 2: Explicit Readability Constraints
- Claim: Controlling linguistic features (word count, vocabulary, sentence complexity) produces text closer to target grade levels.
- Mechanism: Prompts specify Flesch-Kincaid grade level targets and word counts (grade × 100 words), constraining LLM output complexity to match elementary reading proficiency.
- Core assumption: Statistical readability metrics correlate with actual student comprehension; LLMs can modulate output complexity on command.
- Evidence anchors: COGENT passages adhere more closely to elementary reading levels, while BASE exceeds intended levels by ~2.5 grades in lower grades

### Mechanism 3: Wonder-Based Inquiry Framing
- Claim: Framing content as curiosity-driven questions increases student engagement without sacrificing scientific accuracy.
- Mechanism: Core ideas are transformed into "wonder why" questions (e.g., "Why do birds migrate?") that trigger curiosity, then passages bridge everyday observations with scientific explanations.
- Core assumption: Inquiry-based framing maintains curriculum alignment while improving engagement scores.
- Evidence anchors: COGENT achieves highest comprehensibility ratings from expert teachers, with significant difference vs human-written passages (p < .05)

## Foundational Learning

- **Curriculum Standards Architecture (e.g., NGSS)**
  - Why needed here: Understanding how standards decompose into concepts, core ideas, and learning objectives is prerequisite to constructing valid prompts.
  - Quick check question: Can you map a given science topic to its corresponding NGSS core idea and grade-specific learning outcome?

- **Readability Metrics (Flesch-Kincaid, Gunning Fog, etc.)**
  - Why needed here: Interpreting readability scores is necessary to calibrate prompt constraints and validate output quality.
  - Quick check question: What Flesch-Kincaid grade level corresponds to a typical 3rd-grade reading passage?

- **LLM-as-a-Judge Evaluation**
  - Why needed here: Automated evaluation using LLMs requires understanding scoring consistency and bias mitigation strategies.
  - Quick check question: Why does the paper average three samples per topic when using LLM-as-a-judge?

## Architecture Onboarding

- **Component map:**
  1. Curriculum Formulation Module (NGSS decomposition → {concept, core idea, learning outcome} tuples)
  2. Wonder Topic Generator (transforms core ideas → inquiry questions)
  3. Controllable Content Generator (prompt engineering with readability + curriculum constraints)
  4. Multi-dimensional Evaluator (LLM-as-a-judge + statistical readability metrics)

- **Critical path:** Curriculum decomposition → Wonder topic generation → Constrained passage generation → Readability validation → Alignment scoring

- **Design tradeoffs:**
  - Word count formula (grade × 100) is heuristic; may under/overestimate appropriate length for some topics
  - LLM-as-a-judge (Claude-3.5-Sonnet) introduces model-specific bias; human expert evaluation is costly but necessary for validation
  - Wonder-based framing may increase engagement but requires careful mapping to preserve curriculum alignment

- **Failure signatures:**
  - BASE approach produces text 2-3 grades above target level (Figure 4) → indicates missing readability constraints
  - Alignment scores drop in grades 3-5 when wonder topics extracted from human references are poorly matched (Section 5.2)
  - High comprehensibility but low alignment → prompt missing curriculum components

- **First 3 experiments:**
  1. Replicate curriculum alignment scoring comparison (BASE vs COGENT) on a held-out set of 20 curriculum items to validate improvement magnitude
  2. Ablate wonder-based framing: generate passages with curriculum constraints only (no wonder questions) to isolate engagement contribution
  3. Test readability constraint sensitivity: vary target Flesch-Kincaid levels (±1 grade) to measure LLM responsiveness and output quality degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does COGENT-generated content improve actual learning outcomes when used by elementary students over extended periods?
- Basis in paper: "Since COGENT is a general framework, future work could explore... long-term learning outcomes to further enhance automated educational content generation."
- Why unresolved: All evaluations used LLM-as-a-judge and expert teachers; no students were involved in assessing learning gains or retention.
- What evidence would resolve it: A controlled study comparing pre/post-test scores and retention rates of students using COGENT materials versus human-written passages over a semester.

### Open Question 2
- Question: Can the COGENT framework maintain effectiveness when adapted to middle and high school curricula with more complex scientific concepts?
- Basis in paper: "Our framework focused on elementary education (grades 1-5); future work could extend it to middle and high school curricula and adapt the evaluation metrics for more complex science concepts."
- Why unresolved: Current evaluation only covers grades 1-5; readability control parameters and curriculum decomposition may not scale to abstract concepts requiring deeper analytical thinking.
- What evidence would resolve it: Application of COGENT to grades 6-12 NGSS standards, with evaluation showing comparable alignment and comprehensibility scores to elementary-level results.

### Open Question 3
- Question: How do elementary students directly perceive and engage with COGENT-generated passages compared to human-written materials?
- Basis in paper: "We did not include elementary students in our sample analysis due to several considerations: their limited subject knowledge and lack of understanding of curriculum standards would affect their ability to evaluate quality."
- Why unresolved: Student perspectives on engagement, motivation, and content accessibility remain unmeasured; expert ratings may not reflect actual student experience.
- What evidence would resolve it: Student surveys, think-aloud protocols, or behavioral engagement metrics (reading time, comprehension questions) collected from target-age learners.

### Open Question 4
- Question: Does fine-grained personalization based on individual student characteristics (prior knowledge, reading ability, interests) improve the effectiveness of COGENT-generated content?
- Basis in paper: "Since COGENT is a general framework, future work could explore fine-grained personalization... to further enhance automated educational content generation."
- Why unresolved: Current implementation uses grade-level as the primary personalization factor; individual adaptation mechanisms are unexplored.
- What evidence would resolve it: An adaptive version of COGENT incorporating student-specific parameters, evaluated against the current grade-based approach using learning outcome measures.

## Limitations

- Curriculum alignment measurement relies entirely on LLM-as-a-judge evaluation without sufficient validation of correlation with human ratings
- Readability control shows residual mismatch, with passages remaining 1-2 grade levels above target in higher grades
- Wonder-based framing impact is only validated through expert comprehensibility ratings, not direct student engagement or learning outcome measurements

## Confidence

**High confidence** - The technical implementation of curriculum decomposition and readability constraint integration is well-specified and demonstrably improves alignment and grade-appropriateness compared to unconstrained generation.

**Medium confidence** - The claim that COGENT produces "comparable or superior quality" to human-written materials is supported by expert evaluation but limited by small sample size (6 teachers) and lack of standardized rubric application.

**Low confidence** - The assertion that wonder-based framing significantly improves engagement and learning outcomes lacks direct evidence beyond readability metrics and expert comprehensibility ratings.

## Next Checks

1. **Human validation study replication** - Replicate the curriculum alignment scoring with a larger, diverse panel of curriculum experts using a standardized rubric to validate LLM-as-a-judge results and assess inter-rater reliability.

2. **Student comprehension testing** - Conduct a controlled study where actual elementary students read COGENT-generated, human-written, and BASE-generated passages, then assess comprehension through targeted questions to validate the gap between readability metrics and actual understanding.

3. **Cross-model generalization testing** - Test COGENT's framework across additional LLM architectures (e.g., Llama-3, Mistral) and smaller parameter models to assess whether the curriculum alignment improvements are model-agnostic or specific to the tested configurations.