---
ver: rpa2
title: 'DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video
  Reasoning via Agentic Reinforcement Learning'
arxiv_id: '2511.12908'
source_url: https://arxiv.org/abs/2511.12908
tags:
- sports
- video
- wang
- reasoning
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DeepSport addresses the challenge of comprehensive sports video
  understanding by introducing the first end-to-end trained multimodal large language
  model (MLLM) framework for multi-task, multi-sport video reasoning. The approach
  employs a novel two-stage training strategy: Supervised Fine-Tuning (SFT) followed
  by Agentic Reinforcement Learning (RL) with a specialized gated tool-use reward
  function.'
---

# DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2511.12908
- **Source URL**: https://arxiv.org/abs/2511.12908
- **Reference count**: 40
- **Primary result**: First end-to-end trained MLLM framework achieving state-of-the-art sports video reasoning with 38.29 average score across 12 sports and 4 task dimensions

## Executive Summary
DeepSport introduces the first end-to-end trained multimodal large language model for comprehensive sports video understanding. The framework employs a two-stage training strategy: Supervised Fine-Tuning on distilled Chain-of-Thought data followed by Agentic Reinforcement Learning with a specialized gated reward function. The model actively interrogates video content using a frame-extraction tool, enabling iterative reasoning across multiple sports and task types. DeepSport achieves state-of-the-art performance on a 6.7k-question testing benchmark, significantly outperforming both proprietary and open-source baselines in multi-sport video reasoning tasks.

## Method Summary
The framework uses a two-stage training approach on a Qwen2.5-VL-7B backbone. First, Supervised Fine-Tuning (SFT) on 15k Chain-of-Thought trajectories distilled from Qwen3-VL-235B provides cold-start initialization for tool-use syntax and reasoning format. Second, Agentic Reinforcement Learning using Group Relative Policy Optimization (GRPO) optimizes the model's ability to actively interrogate videos via a frame-extraction tool. The gated reward function incentivizes effective tool use while preventing reward hacking, with separate gates for format validation, tool usage, and accuracy assessment. The model engages in multi-turn reasoning loops, strategically calling the frame extraction tool to retrieve additional frames from specific temporal windows during reasoning.

## Key Results
- Achieves state-of-the-art performance with 38.29 average score on 6.7k-question testing benchmark
- Significantly outperforms both proprietary and open-source baselines in multi-sport video reasoning
- Demonstrates effectiveness of active, tool-augmented video reasoning for complex sports intelligence applications
- Handles 12 different sports across four core task dimensions: Fine-Grained Recognition, Rule & Procedural Logic, Assessment & Coaching, and Live Commentary

## Why This Works (Mechanism)

### Mechanism 1: Active Frame Interrogation via Tool-Augmented Reasoning
The frame extraction tool enables iterative, targeted video reasoning rather than passive single-pass processing. The model receives initial sparse frames (k=8), then can invoke `<tool call>frame extraction tool(idx_start, idx_end)</tool call>` to request additional frames from specific temporal windows. This creates multi-turn reasoning trajectories where each step builds on newly retrieved visual evidence.

### Mechanism 2: Gated Reward Shaping for Tool-Use Behavior
The multi-component reward function with conditional gates incentivizes effective tool use while preventing reward hacking. Reward R(τ) = g_fmt(R_acc + R_tool) + P_format. Three gates: format gate validates syntax, tool usage gate activates if tool called, accuracy gate activates if accuracy ≥0.5. Successful tool use gets 0.5×acc bonus; exploratory use gets 0.03; failed format gets -0.05 penalty.

### Mechanism 3: Cold-Start SFT Stabilizes RL Exploration
SFT phase on distilled CoT data creates stable initialization for RL by teaching reasoning format and tool syntax. 15k high-quality CoT trajectories teach model the interleaved thought-action format before RL optimization, reducing invalid action space exploration during RL.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm using group-wise advantage normalization to reduce variance without a separate value function. *Quick check*: Can you explain why GRPO samples G trajectories per prompt rather than using a critic network?
- **Chain-of-Thought Distillation**: Training data generation method where teacher model generates reasoning traces, then LLM judge filters for quality. *Quick check*: What failure modes occur if the teacher model's CoT contains hallucinations that pass the judge filter?
- **Agentic Tool Use in MLLMs**: Paradigm where model learns when/how to invoke external tools during reasoning, not just at inference time. *Quick check*: How does training-time tool use differ from inference-time tool augmentation (e.g., ReAct prompting)?

## Architecture Onboarding

**Component map**: Video Input → Frame Sampler (initial k=8) → Vision Encoder (Qwen2.5-VL backbone) → LLM Backbone (7B params) → Action Space: {<tool call>, <answer>} → If <tool call> → Frame Extraction Tool → New frames → Loop → If <answer> → Terminate → Output

**Critical path**: SFT data quality → SFT convergence → RL reward shaping → Tool grounding accuracy → Final performance. Errors compound forward.

**Design tradeoffs**: 7B backbone vs. larger models (3% parameters of Qwen3-VL-235B, competitive performance but gaps in generative tasks); 14.39 avg frames vs. fixed 16 (efficiency gain but tool grounding is primary failure mode); video-level data splitting (prevents leakage but limits data reuse).

**Failure signatures**: Tool Grounding Failure (42.9%): Model calls tool but retrieves wrong temporal window; Visual Hallucination (37.1%): Correct frames, wrong interpretation; Redundant tool calls: Model calls same interval twice (explicitly penalized by format gate); Format errors: Malformed <tool call> syntax triggers -0.05 penalty.

**First 3 experiments**: 1) Reproduce SFT→RL ablation: Train with SFT-only, RL-only, and SFT+RL to validate cold-start hypothesis. Metric: valid action rate in early RL steps. 2) Reward component ablation: Remove each gate individually. Expect: format removal → syntax errors spike; tool gate removal → reduced tool usage. 3) Tool grounding diagnosis: On failure cases, visualize which temporal windows model requested vs. ground-truth event timestamps.

## Open Questions the Paper Calls Out

**Open Question 1**: How can the temporal precision of the frame-extraction tool be improved to mitigate Tool Grounding Failures in long video reasoning? The authors state in the Conclusion: "Future work will focus on improving the temporal precision of the retrieval module." The error analysis identifies "Tool Grounding Failure" as the primary bottleneck (42.9% of errors), where the model invokes the tool but retrieves a window that misses the critical event.

**Open Question 2**: Does increasing the visual encoder resolution effectively reduce the rate of Visual Hallucination in fine-grained sports tasks? The error analysis suggests that "future work should focus on enhancing the fine-grained visual encoder resolution" to address the 37.1% error rate caused by Visual Hallucination.

**Open Question 3**: How can the framework effectively generalize to "niche" sports that lack dense linguistic annotations for training? The limitations section notes that "niche sports like fencing or diving often lack such dense, linguistic annotations," making tasks like commentary generation currently infeasible.

## Limitations
- Significant failure modes in tool grounding (42.9%) and visual hallucination (37.1%) suggest fundamental limitations in current approach
- General video benchmark performance drops marginally compared to backbone, indicating domain-specific trade-offs
- Limited generalizability to low-resource sports lacking dense linguistic annotations for training

## Confidence

**High Confidence**: The two-stage SFT→RL training approach is well-established and the paper provides sufficient detail for this methodology. The 7B parameter architecture constraint and its competitive performance relative to much larger models are verifiable claims.

**Medium Confidence**: The novel gated reward function design and its effectiveness in shaping tool-use behavior are plausible but require independent validation. The 38.29 average score depends on the LLM-as-Judge evaluation quality.

**Low Confidence**: The scalability and robustness claims for real-world deployment across diverse sports scenarios. The paper's error analysis suggests fundamental limitations that aren't fully addressed.

## Next Checks

1. **Error Type Analysis Validation**: Reproduce the error classification on the test set to verify the reported distribution: 42.9% Tool Grounding Failure, 37.1% Visual Hallucination, and 20.0% other failures.

2. **Ablation of Reward Gates**: Systematically disable each component of the gated reward function (format gate, tool usage gate, accuracy gate) to empirically validate their individual contributions.

3. **Temporal Grounding Stress Test**: Design a controlled experiment where ground truth event timestamps are known. Measure the model's ability to correctly identify and extract frames from these critical temporal windows across different sports with varying event durations.