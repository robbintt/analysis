---
ver: rpa2
title: 'SplineFormer: An Explainable Transformer-Based Approach for Autonomous Endovascular
  Navigation'
arxiv_id: '2501.04515'
source_url: https://arxiv.org/abs/2501.04515
tags:
- guidewire
- navigation
- endovascular
- segmentation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present SplineFormer, a transformer-based approach
  for autonomous endovascular navigation that predicts the continuous geometry of
  guidewires using B-splines instead of segmentation masks. The method addresses the
  challenge of real-time shape prediction in dynamic vascular environments where traditional
  segmentation often fails.
---

# SplineFormer: An Explainable Transformer-Based Approach for Autonomous Endovascular Navigation

## Quick Facts
- **arXiv ID**: 2501.04515
- **Source URL**: https://arxiv.org/abs/2501.04515
- **Reference count**: 40
- **Primary result**: 50% success rate for autonomous BCA cannulation vs 5.6% baseline

## Executive Summary
SplineFormer introduces a transformer-based approach for autonomous endovascular navigation that predicts guidewire geometry using B-splines rather than traditional segmentation masks. The method addresses the challenge of real-time shape prediction in dynamic vascular environments where conventional segmentation often fails due to discontinuity issues. By predicting continuous B-spline parameters (control points and knots), SplineFormer generates smooth, explainable guidewire paths while maintaining focus on critical navigation regions through attention visualization.

## Method Summary
SplineFormer employs a Vision Transformer encoder to process 1024×1024 fluoroscopic images, with patches embedded and passed through multi-head self-attention layers. A dedicated tip predictor (Conv→ReLU→Linear) identifies the initial guidewire position, which conditions a transformer decoder that autoregressively generates B-spline control points and knots. The model predicts the continuous geometry of guidewires using B-splines instead of segmentation masks. The architecture combines ViT patch embeddings, positional encoding, and a separate tip predictor with transformer decoder layers featuring masked self-attention and cross-attention. Training uses Adam optimizer (lr=1e-5) for 300 epochs on 8,746 fluoroscopic images with polyline annotations converted to spline control points.

## Key Results
- 50% success rate for autonomous brachiocephalic artery (BCA) cannulation on physical robot
- 150±45.6s average completion time
- 5.6% success rate for Behavior Cloning baseline (vs 50% for SplineFormer)
- Attention visualization demonstrates focus on key regions for accurate navigation

## Why This Works (Mechanism)

### Mechanism 1: Parametric Shape Representation via B-Splines
The model predicts B-spline curve parameters (control points Pi and knots ti) directly from input images, providing a continuous, smooth representation that overcomes segmentation discontinuity issues. The curve C(t) = ΣPiBi,p(t) enforces built-in smoothness prior, bypassing pixel-wise classification prone to fragmentation.

### Mechanism 2: Transformer-Based Attention for Feature Localization
Vision Transformer encoder with multi-head self-attention captures global context, while decoder's cross-attention focuses on salient features like the low-contrast guidewire tip. This global attention mechanism helps disambiguate guidewire from background noise and vessel structures.

### Mechanism 3: Independent Tip Predictor for Anchor Initialization
A separate CNN-based predictor (Conv→ReLU→Linear) finds initial tip position, providing stable starting anchor for autoregressive sequence generation. This dedicated module improves accuracy over generic [SOS] tokens by specifically targeting the most critical navigation feature.

## Foundational Learning

**Vision Transformer (ViT) Patch Embeddings**
- *Why needed*: Transformer processes sequences, so images must be converted to learnable tokens
- *Quick check*: For 1024×1024 image with 16×16 patches, sequence length = (1024/16) × (1024/16) = 4096 tokens

**Autoregressive Sequence Generation**
- *Why needed*: Decoder generates spline parameters sequentially, with each prediction conditioned on previous points
- *Quick check*: Training uses "teacher forcing" (ground truth tokens), but inference uses own predictions - this exposure bias can accumulate errors

**B-Spline Geometry**
- *Why needed*: Core output is mathematical curve, not pixel map, requiring understanding of control point influence
- *Quick check*: B-spline control points have local support - moving a control point only affects curve within its knot span

## Architecture Onboarding

**Component map**: Input Image → Patch Division & Embedding → ViT Encoder (Ne layers) → Tip Predictor → Transformer Decoder (ND layers) → Linear Heads (knots & control points)

**Critical path**: Tip Predictor is most critical non-standard component - entire sequence generation hinges on accurate initial point. Cross-Attention in decoder is second most critical, bridging visual features with geometric sequence.

**Design tradeoffs**: Parametric B-splines chosen for smoothness and explainability vs. pixel-accurate segmentation maps. Independent tip predictor adds complexity but improves accuracy over [SOS] token. Transformer chosen for global attention vs. potential higher computational cost compared to pure CNN.

**Failure signatures**: Curve Offset (parallel but shifted curve), Tip Drift (incorrect tip localization leading to wrong trajectory), Oscillations (high-frequency curve instability), LCCA Navigation Failure (struggles with complex turns).

**First 3 experiments**:
1. Tip Ablation: Replace dedicated tip predictor with [SOS] token, quantify drop in success rate and increase in prediction error
2. Curvature Loss Weighting: Grid search λc to find optimal balance between smoothness and capturing tight bends
3. Attention Map Visualization: For LCCA failures, visualize attention maps to identify incorrect feature focus

## Open Questions the Paper Calls Out
- Can SplineFormer achieve success on LCCA target with architectural or training modifications?
- How to reduce sensitivity of parametric B-spline representation to small parameter perturbations?
- How will SplineFormer perform when transferred from phantom to clinical/porcine settings?
- Can SplineFormer close performance gap with semi-autonomous methods while preserving full autonomy?

## Limitations
- Struggles with LCCA navigation due to complex 3D anatomical structures and sharper turns
- Single tip prediction point creates vulnerability - errors propagate through entire sequence
- Unspecified spline degree and control point count may limit capture of complex geometries
- Phantom environment may not fully represent clinical tissue interactions and patient variability

## Confidence
- **High Confidence**: B-spline methodology effectiveness (50% vs 5.6% baseline) and dedicated tip predictor
- **Medium Confidence**: Transformer attention mechanism contribution to localization in low-contrast regions
- **Medium Confidence**: Architectural choice of separate tip predictor over [SOS] token (logical but lacks direct ablation data)

## Next Checks
1. Perform tip predictor ablation study by replacing with [SOS] token and measuring impact on success rate
2. Conduct systematic grid search on curvature consistency loss weight λc to optimize smoothness vs accuracy tradeoff
3. Analyze attention maps for LCCA navigation failures to identify incorrect anatomical feature focus