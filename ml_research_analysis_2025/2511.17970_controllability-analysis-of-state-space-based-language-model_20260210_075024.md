---
ver: rpa2
title: Controllability Analysis of State Space-based Language Model
arxiv_id: '2511.17970'
source_url: https://arxiv.org/abs/2511.17970
tags:
- influence
- state
- score
- mamba
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces and validates the Influence Score, a control-theoretic
  metric derived from Mamba's state-space parameters to quantify token-level controllability
  in SSM-based language models. By computing influence via a backward recurrence analogous
  to observability analysis, the method measures how strongly each token affects subsequent
  model states and outputs.
---

# Controllability Analysis of State Space-based Language Model

## Quick Facts
- **arXiv ID:** 2511.17970
- **Source URL:** https://arxiv.org/abs/2511.17970
- **Reference count:** 23
- **Primary result:** Introduces Influence Score metric derived from Mamba's state-space parameters to quantify token-level controllability in SSM-based language models

## Executive Summary
This study introduces the Influence Score, a control-theoretic metric that quantifies how strongly each input token influences subsequent model states and outputs in State Space Model (SSM)-based language models. The score is computed via a backward recurrence analogous to observability analysis, measuring both direct and propagated token influence through the model's discretized state-space parameters. Experiments across three Mamba variants (130M, 2.8B, and 2.8B-slimpj) demonstrate that the metric captures meaningful architectural differences, scales with model size and training quality, and reveals consistent patterns like recency bias and layer-wise influence concentration.

## Method Summary
The Influence Score computes token-level controllability by capturing per-token SSM parameters (Δₖ, Bₖ, Cₖ) at each Mamba layer, discretizing them to (Āₖ, B̄ₖ, C̄ₖ), then applying a backward recurrence that aggregates influence from future outputs. The recurrence maintains a "future influence propagator" P that accumulates downstream effects, computing each token's score as the sum of direct influence (|C̄ₖB̄ₖ|) and propagated influence through future states. This O(L) algorithm efficiently approximates the aggregated Jacobian of all future outputs with respect to each input token.

## Key Results
- Influence Score scales with model size and training data quality, with mamba-2.8b-slimpj showing unique content-word prioritization and noise resilience
- Scores concentrate in mid-to-late layers (11.4× higher than early layers in 130M model) revealing architectural specialization
- Strong recency bias observed: back-critical prompts achieve 2.82e+02 mean score vs. 2.54e+02 for front-critical (2.8b-slimpj)
- Metric proves robust to sampling temperature (Spearman ρ ≈ 1.0) and stable across prompt complexities

## Why This Works (Mechanism)

### Mechanism 1: Influence Score via Backward Recurrence
- **Claim:** The Influence Score quantifies token-level controllability by computing the aggregated Jacobian of all future outputs with respect to each input token.
- **Mechanism:** The score decomposes into two terms: (1) Direct Influence = ‖C̄ₖB̄ₖ‖ measures immediate input-output coupling at position k; (2) Propagated Influence = Σ‖C̄ⱼ(ΠĀᵢ)B̄ₖ‖ sums contributions to all future outputs via state transitions. A backward recurrence (Algorithm 1, lines 18-23) efficiently computes this in O(L) by accumulating a "future influence propagator" P that rolls up downstream effects.
- **Core assumption:** The feedthrough matrix D is negligible; the Frobenius norm adequately aggregates multi-dimensional influence.
- **Evidence anchors:**
  - [abstract]: "computed through a backward recurrence analogous to system observability"
  - [section 3, Eq. 13]: Formal decomposition into Direct and Propagated Influence terms
  - [corpus]: Weak direct validation—neighbor papers (LaTIM, Mamba Knockout) address related token-interaction questions but do not evaluate this specific Jacobian formulation
- **Break condition:** If D matrices become significant (e.g., architectures with strong skip connections bypassing the SSM), direct influence estimates will be underestimated.

### Mechanism 2: Layer-wise Influence Concentration
- **Claim:** Influence accumulates monotonically across layers, with mid-to-late layers exhibiting 5-11× higher scores than early layers.
- **Mechanism:** Early layers encode inputs with low control authority; the recurrent state hₜ propagates through successive Ā transformations, and the product of state transitions amplifies or attenuates influence. By late layers, the state has integrated sufficient context that perturbations strongly manifest in outputs.
- **Core assumption:** Layer depth correlates with semantic abstraction; the C̄ matrices in late layers read out more task-relevant state dimensions.
- **Evidence anchors:**
  - [abstract]: "concentrated influence in mid-to-late layers"
  - [section 4.5, Table 3]: Late/Early ratios of 11.4× (130M), 6.7× (2.8B), 6.3× (2.8B-slimpj)
  - [corpus]: No direct corroboration—neighbor papers do not analyze layer-wise influence distribution
- **Break condition:** Architectures with residual connections that bypass SSM layers may flatten this curve, reducing layer-wise differentiation.

### Mechanism 3: Recency Bias in Recurrent State Dynamics
- **Claim:** Tokens nearer the end of the input sequence exert higher control authority over generation due to the sequential state update mechanism.
- **Mechanism:** Each state update hₖ = Āₖhₖ₋₁ + B̄ₖuₖ depends on the immediately preceding state. The product ΠĀᵢ decays influence from early positions exponentially (via discretization exp(ΔA)). Back-critical prompts show highest mean scores (2.82e+02) vs. front-critical (2.54e+02 for 2.8B-slimpj).
- **Core assumption:** The learned Ā matrices have eigenvalues with magnitude < 1 after discretization, causing sequential decay.
- **Evidence anchors:**
  - [abstract]: "recency bias (position sensitivity)"
  - [section 4.6, Figure 8]: Late/early token ratio < 1.0 across all models; back-critical > front-critical across all conditions
  - [corpus]: PerfMamba and Understanding Input Selectivity papers discuss related state-propagation dynamics but do not quantify position-dependent controllability
- **Break condition:** If Mamba's selectivity mechanism learns to amplify specific early tokens (counteracting decay), recency bias may weaken for semantically critical content.

## Foundational Learning

- **Concept: Discrete-time State-Space Models**
  - **Why needed here:** The Influence Score is derived directly from the discretized matrices (Ā, B̄, C̄). Understanding how continuous ODEs become recurrent updates is prerequisite to interpreting the metric.
  - **Quick check question:** Given hₖ = Āhₖ₋₁ + B̄uₖ, what does Ā represent and how does its norm affect long-range influence?

- **Concept: Controllability and Observability Gramians**
  - **Why needed here:** The paper frames influence as "observability-weighted controllability." The backward recurrence is analogous to computing an Observability Gramian (Wₒ) that measures how internal states manifest in outputs.
  - **Quick check question:** For a linear system, what does it mean if the Controllability Gramian Wc has a very small eigenvalue in some direction?

- **Concept: Jacobian Sensitivity Analysis**
  - **Why needed here:** The Influence Score is fundamentally a sum of Jacobian norms ‖∂yⱼ/∂uₖ‖. Understanding how perturbations propagate through composite functions (chain rule) clarifies why backward recurrence is efficient.
  - **Quick check question:** If y = f(g(x)), how would you compute ∂y/∂x, and why might a backward pass be more efficient than forward-mode differentiation?

## Architecture Onboarding

- **Component map:** Input → Linear (split into X_in, Z) → Conv1D → Activation → Linear_proj (produces Δ_raw, B_raw, C_raw) → Softplus on Δ → Discretization (Ā = exp(Δ⊗A), B̄ = ΔB) → Selective scan (recurrent state update)

- **Critical path:**
  1. Intercept hidden state tensor X (B×L×D) at each Mamba layer
  2. Extract per-token SSM parameters (Δₖ, Bₖ, Cₖ) from the projection layer
  3. Discretize: Āₖ = exp(Δₖ⊗A), take absolute values
  4. Backward recurrence: Initialize P=0; for k=L-2→0: P ← |C̄ₖ₊₁| + Āₖ₊₁⊙P; Iₖ ← |C̄ₖB̄ₖ| + B̄ₖ⊙P
  5. Aggregate: Sum over state dimension, average over batch

- **Design tradeoffs:**
  - Absolute value vs. signed matrices: Using |Ā|, |B̄|, |C̄| prevents cancellation but may overestimate influence for oscillatory dynamics
  - Per-layer vs. aggregated scores: Layer-wise analysis reveals functional specialization but requires storing intermediate tensors
  - Computational overhead: O(L) per layer vs. O(L²) for attention-based interpretability; negligible during inference with hooks

- **Failure signatures:**
  - **Gradient explosion in small models:** Mamba-130M shows +6.2% influence spike under typos (brittle confusion response) vs. -2 to -7% for 2.8B-slimpj (robust dismissal)
  - **Vanishing influence for early tokens:** If Late/Early ratio ≫ 10, model may not integrate long-range context; check Ā eigenvalue magnitudes
  - **Instability under high temperature:** Spearman ρ ≈ 0 indicates metric stability; deviations suggest sampling noise contaminating the signal

- **First 3 experiments:**
  1. **Temperature sweep (0.3, 0.5, 0.7, 1.0, 1.5):** Verify metric stability—run identical prompts, compute coefficient of variation (CV) across temperatures. Expected: CV < 0.1 for robust models.
  2. **Position sensitivity (front-critical vs. back-critical prompts):** Quantify recency bias—place key instructions at start vs. end of prompt, compare mean Influence Scores. Expected: back-critical > front-critical by 5-15%.
  3. **Layer-wise profiling:** Compute Influence Score per layer for a fixed prompt, plot distribution. Expected: monotonic increase with late/early ratio > 5×.

## Open Questions the Paper Calls Out
None

## Limitations
- The Influence Score's reliance on Frobenius-norm aggregation and absolute-value matrix operations may obscure directional dynamics and conflate opposing influences
- The method assumes D matrices are negligible, which may not hold for architectures with significant feedthrough or residual connections
- The metric's behavior on longer sequences (>2048 tokens) remains unexplored, and its sensitivity to architectural variations beyond the tested Mamba variants is unknown

## Confidence
- **Influence Score Mechanism:** High - Mathematical formulation is sound with computational verification
- **Layer-wise Influence Concentration:** Medium - Empirical pattern is clear but underlying mechanism lacks direct validation
- **Recency Bias Pattern:** High - Position sensitivity is consistently observed with theoretically sound mechanism

## Next Checks
1. **Architectural Stress Test:** Apply the Influence Score to Mamba variants with explicit residual connections and feedthrough matrices. Measure how the Late/Early ratio and overall scores change compared to the tested models.

2. **Directional Influence Decomposition:** Modify the Influence Score computation to preserve matrix signs rather than taking absolute values. Compare the resulting directional scores with the original magnitude-based scores across the same prompt sets.

3. **Long Sequence Generalization:** Evaluate the Influence Score on prompts of increasing length (256, 512, 1024, 2048 tokens) using mamba-2.8b-slimpj. Track how the Late/Early ratio, layer-wise concentration, and noise resilience scale with sequence length.