---
ver: rpa2
title: Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated
  Texts
arxiv_id: '2503.17965'
source_url: https://arxiv.org/abs/2503.17965
tags:
- text
- arxiv
- rlhf
- language
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how Reinforcement Learning from Human
  Feedback (RLHF) affects the quality and detectability of LLM-generated texts. The
  authors trained Llama-7B on question answering and instruction following tasks using
  RLHF, then evaluated the outputs with two detection methods: training-based (GPT-Zero)
  and zero-shot (Fast-DetectGPT).'
---

# Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts

## Quick Facts
- arXiv ID: 2503.17965
- Source URL: https://arxiv.org/abs/2503.17965
- Reference count: 11
- Primary result: RLHF improves text quality but produces longer, more repetitive outputs that are more detectable by both training-based and zero-shot detectors.

## Executive Summary
This study investigates how Reinforcement Learning from Human Feedback (RLHF) affects the quality and detectability of LLM-generated texts. The authors trained Llama-7B on question answering and instruction following tasks using RLHF, then evaluated the outputs with two detection methods: training-based (GPT-Zero) and zero-shot (Fast-DetectGPT). Results showed RLHF improved text quality and win rates in instruction following but also produced longer, more repetitive outputs with reduced syntactic and semantic diversity. These characteristics made RLHF-enhanced texts more detectable. Training-based detectors struggled with short texts and mixed natural language/code snippets, while zero-shot detectors proved more robust. The findings suggest that while RLHF improves alignment with human preferences, it also creates distinctive patterns that enhance detectability, highlighting trade-offs between quality and stealth in LLM-generated content.

## Method Summary
The authors trained Llama-7B using a three-stage pipeline: Supervised Fine-Tuning (SFT) on task-specific datasets (StackExchange for QA, AlpacaFarm for instruction following), Reward Modeling to learn human preferences, and Reinforcement Learning via Proximal Policy Optimization (PPO). They evaluated output quality using ROUGE, BERTScore, and AlpacaEval win rates, and measured diversity through distinct n-grams and SentenceBERT similarity. Detectability was assessed using GPT-Zero (training-based) and Fast-DetectGPT (zero-shot) on 100 samples per condition, with AUROC as the primary metric. The study used LoRA for parameter-efficient fine-tuning on a single 80GB A800 GPU.

## Key Results
- RLHF improved instruction following win rates from 1.14% to 20.62% but increased text length and repetition
- Fast-DetectGPT achieved 0.91 AUROC on RLHF instruction following outputs vs 0.81 for GPT-Zero
- Training-based detectors failed on short texts (<150 characters) and code-mixed content
- Diversity metrics showed clear decline: distinct n-grams dropped from 0.42 (base) to 0.18 (RLHF)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF increases text detectability despite improving output quality.
- Mechanism: RLHF produces longer, more repetitive outputs with reduced syntactic and semantic diversity. The training process exposes models to substantial LLM-generated data, causing them to internalize and reproduce features inherently associated with machine generation—specific syntactic structures, lexical choices, and fluency patterns.
- Core assumption: Reduced diversity and increased length create statistically distinguishable patterns from human writing.
- Evidence anchors:
  - [abstract]: "Although RLHF improves the quality of LLM-generated texts, we find that it also tends to produce more detectable, lengthy, and repetitive outputs."
  - [section 9]: "The outputs of the RLHF model are lengthier and more repetitive, making them more easily detectable by both detectors."
  - [corpus]: Weak direct support; related papers focus on detection methods rather than RLHF's effect on detectability.
- Break condition: If RLHF were combined with explicit diversity-promoting objectives or adversarial training to reduce repetitive patterns.

### Mechanism 2
- Claim: Zero-shot detectors are more robust than training-based detectors for mixed-content and short texts.
- Mechanism: Fast-DetectGPT leverages conditional probability curvature—a statistical property inherent to how language models generate text—rather than relying on training data patterns. This makes it less sensitive to domain shifts (e.g., code + natural language) and output length.
- Core assumption: The conditional probability distribution of LLM-generated text differs systematically from human text regardless of domain.
- Evidence anchors:
  - [abstract]: "Training-based detectors are vulnerable to short texts and to texts that incorporate code, whereas zero-shot detectors exhibit greater robustness."
  - [section 8.1]: "GPT-Zero is a training-based detector, which relies more on its training data... Fast-DetectGPT pays attention to the inherent differences between human-written and LLM-generated texts."
  - [corpus: 2510.20810] Suggests definitional inconsistency in "LLM-generated text" affects detection consistency.
- Break condition: If the conditional probability distribution were deliberately perturbed via paraphrasing or adversarial attacks.

### Mechanism 3
- Claim: Progressive fine-tuning (SFT → RLHF) narrows output diversity and amplifies machine signatures.
- Mechanism: Each training stage constrains the model's output distribution further. SFT aligns with task-specific patterns; RLHF then optimizes for human preferences that correlate with longer, more structured responses. Diversity metrics (distinct n-grams: 0.42 → 0.24 → 0.18) confirm this narrowing.
- Core assumption: Human preference data used in RLHF systematically favors certain linguistic patterns over diverse expression.
- Evidence anchors:
  - [section 8.2, Table 6]: Diversity decreases across Llama → LlamaSFT → LlamaPPO for both syntactic (n-grams) and semantic (SBERT) measures.
  - [section 8.1]: "The iterative training process promotes certain linguistic patterns or stylistic features more commonly seen in LLM-generated texts, decreasing the diversity of the output."
  - [corpus: 2506.21602] Highlights sampling parameters affect detectability—suggesting training-stage effects interact with decoding strategies.
- Break condition: If reward modeling explicitly penalized repetitive or over-structured outputs.

## Foundational Learning

- Concept: **Reinforcement Learning from Human Feedback (RLHF) Pipeline**
  - Why needed here: The paper's core intervention; understanding SFT → Reward Modeling → PPO stages explains how diversity contracts.
  - Quick check question: Can you explain why the reward model's preference scoring might systematically favor longer, more structured responses?

- Concept: **Zero-Shot vs Training-Based Detection**
  - Why needed here: Determines which detector to use based on content type and length; explains performance gaps.
  - Quick check question: Why would a conditional probability curvature approach handle mixed code/natural language better than a classifier trained on specific text domains?

- Concept: **Text Diversity Metrics (Distinct N-grams, SentenceBERT)**
  - Why needed here: Quantifies how training stages affect output variety; provides measurable evidence for detectability mechanisms.
  - Quick check question: If distinct n-grams drop from 0.42 to 0.18, what does this imply about the model's lexical repetition patterns?

## Architecture Onboarding

- Component map: Pre-trained Llama-7B → Supervised Fine-Tuning (SFT) → Reward Model (preference scoring) → PPO optimization → Generated text evaluation (ROUGE, BERTScore, diversity metrics) → Detection (GPT-Zero, Fast-DetectGPT)

- Critical path: The reward model's preference learning directly shapes PPO's policy updates; if reward signals favor verbose/structured outputs, PPO will amplify those traits. Detection evaluation must use both detector types to surface robustness gaps.

- Design tradeoffs:
  - Quality vs. detectability: Higher win rates (1.14% → 20.62%) correlate with higher AUROC (0.68 → 0.91 for Fast-DetectGPT).
  - Length vs. robustness: Longer outputs improve detection but may reduce utility for conciseness-sensitive tasks.
  - Domain specificity: Training-based detectors fail on code-mixed text; zero-shot detectors maintain performance but may have higher false positive rates.

- Failure signatures:
  - GPT-Zero fails on short texts (<150 characters) and code-mixed content (AUROC drops to 0.71-0.82 vs. 0.91 for Fast-DetectGPT).
  - RLHF models show 2-3× longer outputs than base models, potentially unsuitable for token-limited applications.
  - Diversity collapse (n-grams: 0.42 → 0.18) may cause repetitive, unnatural-sounding outputs in open-ended generation.

- First 3 experiments:
  1. Replicate the diversity measurement pipeline: Train Llama-7B with SFT and PPO on AlpacaFarm, compute distinct n-grams and SBERT scores for 100 samples per stage. Verify the 0.42 → 0.18 n-gram trend.
  2. Test detection robustness across lengths: Sample RLHF-generated texts at word counts of 25-50, 50-100, and >100. Run both GPT-Zero and Fast-DetectGPT. Confirm GPT-Zero's sensitivity to short texts (expect AUROC variance >0.1 across buckets vs. <0.05 for Fast-DetectGPT).
  3. Probe code-mixed detection failure: Create a synthetic dataset mixing natural language with Python/LaTeX snippets (proportions 50/50). Compare GPT-Zero vs. Fast-DetectGPT AUROC. Expect >0.15 gap favoring zero-shot.

## Open Questions the Paper Calls Out

- Question: Does the increase in detectability and reduction of diversity observed in RLHF models generalize to other generative tasks such as text summarization or dialogue generation?
  - Basis in paper: [explicit] The authors state in the Limitations section that they "only evaluated the performance of the detectors on two tasks," and suggest exploring other tasks like summarization and dialogue.
  - Why unresolved: The study was constrained to question answering and instruction following; different tasks may induce different structural changes in the model outputs during RLHF.
  - What evidence would resolve it: Repeating the experimental pipeline using RLHF-trained models on text summarization and dialogue datasets, followed by analysis of output diversity and detector AUROC scores.

- Question: Are adversarial-based or ensemble-based detectors more robust against the distinct patterns introduced by RLHF compared to training-based and zero-shot methods?
  - Basis in paper: [explicit] The authors note in the Limitations section that they "only used two types of detectors" and suggest that investigating adversarial or ensemble-based detectors would be beneficial.
  - Why unresolved: The study only tested GPT-Zero (training-based) and Fast-DetectGPT (zero-shot), leaving the efficacy of other detection architectures unknown.
  - What evidence would resolve it: Evaluating the RLHF-generated texts from this study against a suite of adversarial and ensemble detectors to compare performance metrics.

- Question: Does the trade-off between improved quality and increased detectability scale with model size, or do larger models learn to mimic human diversity better during RLHF?
  - Basis in paper: [inferred] The study relies exclusively on the Llama-7B model (and AlpacaFarm derivatives), but it does not test if the findings hold for larger parameter counts or different architectures.
  - Why unresolved: It is unclear if the "lengthy and repetitive" nature of the RLHF outputs is an artifact of the 7B parameter scale or a fundamental property of the alignment process.
  - What evidence would resolve it: Conducting the same RLHF training and detection experiments on larger models (e.g., Llama-70B) to see if the diversity loss and detection rates persist.

## Limitations

- The study uses only one model size (Llama-7B) and relatively small sample sizes (100 samples per condition), limiting generalizability to larger models or different architectures.
- Detection methods tested (GPT-Zero and Fast-DetectGPT) represent only two approaches in a rapidly evolving field, leaving open whether other methods would perform differently.
- The paper doesn't explore whether detectability patterns persist under different decoding strategies or temperature settings during generation.

## Confidence

- **High confidence:** The core finding that RLHF produces longer, more repetitive outputs that are more detectable. This is supported by direct measurements of text length, repetition, and diversity metrics across multiple conditions.
- **Medium confidence:** The superiority of zero-shot detectors over training-based detectors. While the trend is clear, the specific AUROC values may vary with different implementations or model versions.
- **Medium confidence:** The quality improvements measured by win rates and ROUGE/BERTScore. These metrics capture alignment with human preferences but may not fully represent real-world utility.

## Next Checks

1. Test whether the detectability patterns persist when varying sampling temperatures (0.1-1.0) during generation, as this could reveal whether the detected patterns are inherent to RLHF or artifacts of generation parameters.
2. Evaluate the models on additional tasks beyond QA and IF (e.g., creative writing, code generation) to assess whether detectability effects generalize across domains.
3. Measure the impact of RLHF on more granular linguistic features (e.g., part-of-speech distributions, syntactic complexity measures) to better understand which specific linguistic patterns drive detectability.