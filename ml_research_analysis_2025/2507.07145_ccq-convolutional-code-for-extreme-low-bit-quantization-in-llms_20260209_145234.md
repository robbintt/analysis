---
ver: rpa2
title: 'CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs'
arxiv_id: '2507.07145'
source_url: https://arxiv.org/abs/2507.07145
tags:
- quantization
- code
- encoding
- convolutional
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Convolutional Code Quantization (CCQ), an inference-optimized
  quantization approach that compresses large language models to 2.0-2.75 bits with
  minimal accuracy loss. CCQ departs from error-prone scalar quantization or slow
  vector quantization by integrating hardware-aware bit-shift encoding and decoding
  with Convolutional Code, Hybrid Encoding, and Code Cluster algorithms.
---

# CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs

## Quick Facts
- arXiv ID: 2507.07145
- Source URL: https://arxiv.org/abs/2507.07145
- Authors: Zhaojing Zhou; Xunchao Li; Minghao Li; Handi Zhang; Haoshuang Wang; Wenbin Chang; Yiqun Liu; Qingqing Dang; Dianhai Yu; Yanjun Ma; Haifeng Wang
- Reference count: 9
- Primary result: Compresses LLMs to 2.0-2.75 bits per weight with minimal accuracy loss

## Executive Summary
CCQ (Convolutional Code Quantization) introduces an inference-optimized quantization method that compresses large language models to 2.0-2.75 bits per weight with minimal accuracy degradation. The approach departs from traditional scalar or vector quantization by integrating hardware-aware bit-shift encoding with convolutional codes, hybrid encoding, and code clustering. This enables lookup-free decoding during inference, making it particularly suitable for extreme low-bit quantization where traditional methods struggle with accuracy or speed.

## Method Summary
CCQ operates through a multi-stage process: weights are reshaped into groups (typically size 64), convolutional codebooks are generated using (L,N,S) configurations to define state transitions, and encoding searches find optimal codebook indices minimizing MSE. For 2.06 bpw, a code cluster step applies uniform quantization to code values. Scales are optimized and compressed, enabling bit-shift decoding during inference without codebook lookups. The method supports multiple configurations including (4,3,2) for 2.75 bpw, hybrid (3,3,2)/(3,4,2) for 2.28 bpw, and (6,4,3) with clustering for 2.06 bpw.

## Key Results
- Compresses DeepSeek-V3 (671B parameters) to 184GB and ERNIE-4.5-300B-A47B to 89GB
- Achieves up to 71.3% compression from 8-bit models with <3% accuracy degradation
- Enables single-GPU deployment of ERNIE 4.5 without inter-card communication
- 2-bit ERNIE-4.5-300B-A47B model and inference engine open-sourced

## Why This Works (Mechanism)

### Mechanism 1
Convolutional code codebooks enable inference-speed bit-shift decoding without memory lookups. The (L,N,S) encoding configuration creates overlapping bit patterns between adjacent states. Since each state shares L−S bits with its predecessor, storing only the initial state and transition bits suffices. During dequantization, right-shift operations extract each weight value directly from the packed integer—no codebook lookup required.

### Mechanism 2
Hybrid encoding achieves arbitrary bit rates by alternating configurations within a single group. Since storage must align to 8-bit boundaries, single configurations often waste bits. Alternating (L=3,N=3,S=2) and (L=3,N=4,S=2) encodes 7 values into 16 bits, bypassing data-type constraints while maintaining decode coherence.

### Mechanism 3
Code Cluster reduces encoding space to 8-bit while preserving 6-bit effective precision via observed normal distribution of code values. After convolutional encoding, code values per channel cluster near common values. Uniform quantization maps the full [0,2^T−1] space to [0,255], eliminating unused codes. Dequantization first expands UINT8→UINT16, then applies bit-shifts.

## Foundational Learning

- **Concept: Trellis/Convolutional Codes**
  - Why needed here: CCQ reinterprets channel coding as weight quantization; understanding state transitions explains why bit-sharing works
  - Quick check question: Given (L=4,N=3,S=2), how many bits encode 3 states and what is the bpw?

- **Concept: Group-wise Quantization with Scales**
  - Why needed here: CCQ's accuracy depends on per-group scales; scale optimization and compression directly affect final bpw
  - Quick check question: If group size is 64 and you have 256 groups, how many scale values must be stored?

- **Concept: Vector Quantization vs. Scalar Quantization Tradeoffs**
  - Why needed here: CCQ positions itself as a hybrid—vector-like quality with scalar-like speed. Understanding VQ's lookup bottleneck clarifies the design motivation
  - Quick check question: Why does VQ degrade inference throughput despite better compression fidelity?

## Architecture Onboarding

- **Component map**: Weight Reshaping -> Scale Computation -> Codebook Construction -> Encoding Search -> Code Cluster (2.06 bpw only) -> Scale Compression -> Dequantization Kernel
- **Critical path**: The encoding search (Algorithm 1) is quantization-time bottleneck; inference critical path is the dequantization kernel (Algorithm 2/3) inside GEMV/Grouped-GEMM
- **Design tradeoffs**: Lower bpw (2.06) requires Code Cluster → two dequant passes → slightly higher latency vs. 2.75 bpw; smaller group size improves outlier handling but increases scale storage overhead; hybrid encoding (2.5 bpw) balances compression and decode complexity
- **Failure signatures**: Accuracy cliff on C-Eval/MMLU (likely instruction-following failure; check if early layers need WINT4 instead of CCQ); inference slower than WINT2 (verify bit-shift kernel is fused; separate scale loads cause memory stalls); quantization OOM (codebook expansion is memory-intensive; process channel-by-channel)
- **First 3 experiments**:
  1. Baseline validation: Quantize a 7B model at 2.75 bpw; compare perplexity and GSM8K against WINT4 baseline
  2. Ablation on group size: Test g=32,64,128; measure accuracy vs. scale overhead tradeoff
  3. Kernel profiling: Benchmark CCQ dequant kernel against VPTQ lookup and standard WINT2; confirm Table 4 trends on your hardware

## Open Questions the Paper Calls Out

### Open Question 1
Can loss-based dynamic bit allocation strategies effectively identify pivotal layers in LLMs to optimize the trade-off between compression rate and accuracy retention? The authors observe that allocating WINT4 to only the 6th MoE layer maintained high accuracy, suggesting certain layers are critical and prompting research into dynamic allocation. A systematic, automated method for identifying critical layers during quantization is not developed.

### Open Question 2
How does Convolutional Code Quantization perform on dense LLM architectures compared to state-of-the-art 2-bit methods like QTIP and VPTQ? The authors state that while experiments validate efficacy on MoE architectures, "further investigation is warranted for dense models under state-of-the-art 2-bit quantization methods such as QTIP and VPTQ." The experimental results focus heavily on Mixture-of-Experts models.

### Open Question 3
Can the CCQ calibration process be modified to improve instruction-following adherence in 5-shot tasks, where the method currently experiences the most significant accuracy drops? The results section notes that accuracy drops "primarily occur in 5-shot C-Eval and MMLU tasks due to instruction-following failures," explicitly identifying this as a focus for future work. The current method minimizes MSE loss, which preserves weight numerical precision but apparently fails to preserve specific reasoning capabilities required for few-shot tasks.

## Limitations

- Codebook scalability becomes challenging for very large models due to exponential growth in search space
- Distribution assumptions may not hold across all model architectures, particularly in irregular weight distributions
- Hybrid encoding overhead and configuration-switching mechanism lacks detailed quantification
- Scale quantization impact on reconstruction quality is not thoroughly analyzed

## Confidence

**High Confidence Claims**: CCQ achieves 2.0-2.75 bpw compression with acceptable accuracy degradation; bit-shift decoding mechanism works as described for single configurations; hardware acceleration benefits from lookup-free dequantization

**Medium Confidence Claims**: Code Cluster's uniform quantization works across all model layers; hybrid encoding's configuration switching has negligible overhead; 2.06 bpw models maintain instruction-following capabilities

**Low Confidence Claims**: Cross-model generalization of convolutional code effectiveness; performance on non-English benchmarks; scalability to trillion-parameter models

## Next Checks

1. **Distribution Validation**: Quantize models from different families (decoder-only, encoder-decoder, multimodal) and measure code value distributions per layer. Verify the normal-like distribution assumption holds across architectures, or identify which layers violate this assumption and quantify the resulting accuracy impact.

2. **Scale Precision Ablation**: Systematically vary scale quantization precision (8-bit to 16-bit) and measure accuracy degradation at each bpw level. Determine the minimum scale precision required to maintain the claimed <3% accuracy loss, particularly for 2.06 bpw models.

3. **Kernel Overhead Measurement**: Implement hybrid encoding with configuration switching and measure the actual memory and compute overhead versus theoretical savings. Compare against simpler mixed-precision approaches to quantify the practical benefit of hybrid encoding in real inference scenarios.