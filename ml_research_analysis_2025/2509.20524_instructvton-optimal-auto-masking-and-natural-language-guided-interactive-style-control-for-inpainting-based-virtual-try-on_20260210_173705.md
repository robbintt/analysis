---
ver: rpa2
title: 'InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive
  Style Control for Inpainting-Based Virtual Try-On'
arxiv_id: '2509.20524'
source_url: https://arxiv.org/abs/2509.20524
tags:
- try-on
- image
- mask
- garment
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InstructVTON introduces an agentic system that automates virtual
  try-on by replacing manual binary mask creation with Vision Language Models (VLMs)
  and image segmentation models. The core innovation, AutoMasker, generates optimal,
  minimally-invasive masks based on garment type and natural language style instructions,
  improving mask efficiency from 0.73 to 0.83 on average across datasets.
---

# InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On

## Quick Facts
- arXiv ID: 2509.20524
- Source URL: https://arxiv.org/abs/2509.20524
- Authors: Julien Han; Shuwen Qiu; Qi Li; Xingzi Xu; Mehmet Saygin Seyfioglu; Kavosh Asadi; Karim Bouyarmane
- Reference count: 40
- Key outcome: Automates virtual try-on with VLMs and segmentation, improving mask efficiency from 0.73 to 0.83, SSIM 0.91, LPIPS 0.07 on DressCode

## Executive Summary
InstructVTON introduces an agentic system that automates virtual try-on by replacing manual binary mask creation with Vision Language Models (VLMs) and image segmentation models. The core innovation, AutoMasker, generates optimal, minimally-invasive masks based on garment type and natural language style instructions. The system supports multi-garment try-on and complex styling (e.g., "sleeves rolled up", "jacket open") through iterative VTON execution and dummy garment generation. It achieves SSIM of 0.91 and LPIPS of 0.07 on DressCode, matching or exceeding state-of-the-art baselines.

## Method Summary
InstructVTON is a two-agent system for virtual try-on as image-guided inpainting. The Top-level Agent parses multi-garment instructions and orders garments based on layering logic, then summarizes per-garment instructions. The VTO Agent executes the plan sequentially, using AutoMasker to generate minimally-invasive binary masks from BPSM and CSM segmentation outputs. For styling instructions incompatible with the source image (e.g., "sleeves rolled up" on a long-sleeve shirt), the system uses dummy garment inpainting—generating an intermediate image with exposed body parts, then applying the target garment. The system works with existing VTON models without retraining.

## Key Results
- Mask efficiency improves from 0.73 to 0.83 average across datasets
- SSIM of 0.91 and LPIPS of 0.07 achieved on DressCode dataset
- Supports complex styling instructions and multi-garment try-on through iterative execution
- Latency of ~1 minute for 3-garment try-on due to multiple model calls

## Why This Works (Mechanism)

### Mechanism 1
Minimimally-invasive masks that cover only strictly necessary image regions preserve more source content while achieving equivalent try-on quality. AutoMasker computes two traces—the B-trace (body parts that the target garment will occupy) and C-trace (existing clothing to remove)—using Body Part Segmentation Maps (BPSM) and Clothing Segmentation Maps (CSM). Parts inclusion rules, conditioned on garment type and style instruction, determine which segments from B and C to union into the final mask. Core assumption: Segmentation granularity is sufficient to approximate the ideal garment position; VLMs can accurately interpret style instructions to select correct body parts.

### Mechanism 2
Natural language instructions can be decomposed into ordered single-garment try-on tasks with per-garment style paraphrases. Top-level Agent reorders target garments based on layering logic (e.g., shirt before open jacket), then summarizes style instruction per garment. VTO Agent executes sequentially, passing each output as the human model image for the next step. Core assumption: VLMs possess sufficient commonsense reasoning about garment layering; open-loop planning without feedback produces correct orderings.

### Mechanism 3
Styling instructions requiring garment-body configurations incompatible with the source image can be achieved via intermediate "dummy garment" inpainting. When style instruction (e.g., "sleeves rolled up") conflicts with current coverage, VTO Agent fetches or synthesizes a dummy garment (e.g., tank top), generates an intermediate image with exposed body parts, then applies the original target garment with reduced mask coverage. Core assumption: Dummy garments can be retrieved or synthesized that plausibly match body pose; two-round inpainting preserves coherence.

## Foundational Learning

- Concept: Image inpainting with diffusion models
  - Why needed here: VTO is formulated as inpainting; understanding how masks control generation layout is prerequisite for AutoMasker reasoning
  - Quick check question: Given a masked image and diffusion model, what determines which regions are regenerated vs. preserved?

- Concept: Semantic segmentation (body parsing and garment parsing)
  - Why needed here: AutoMasker relies on BPSM and CSM outputs to compute traces; incorrect segment boundaries produce misaligned masks
  - Quick check question: If a body parts segmentation model merges upper and lower arms into one segment, what styling instructions become impossible to achieve?

- Concept: Vision-Language Models for instruction following
  - Why needed here: Both agents use VLMs to parse free-text style instructions and map them to structured actions
  - Quick check question: How would a VLM interpret "wear the jacket open but buttoned at the bottom"—what mask modification would be required?

## Architecture Onboarding

- Component map: Top-level Agent -> VTO Agent -> AutoMasker -> VTO model -> (if style conflict) dummy garment generator/library
- Critical path: Top-level Agent planning → VTO Agent per-garment loop → AutoMasker mask generation → VTO model inpainting → (if style conflict) dummy garment insertion → final I_try-on
- Design tradeoffs:
  - Mask efficiency vs. robustness: Minimally-invasive masks preserve content but risk incomplete garment replacement; conservative baselines mask more but destroy detail
  - Latency vs. capability: Multi-garment + multi-round styling ~1 minute for 3 garments; real-time use cases excluded
  - Segmentation granularity vs. style precision: Current BPSM limits fine control (e.g., partial sleeve coverage); finer segmentation increases compute
- Failure signatures:
  - Error propagation: Incorrect first-step ordering or mask produces degraded final output (open-loop planner)
  - Segmentation boundary artifacts: Visible seams when mask doesn't align with true garment edges
  - Style instruction misinterpretation: VLM fails to map "jacket open" to center-torso mask exclusion
- First 3 experiments:
  1. Ablate AutoMasker against fixed conservative masks (full upper-body/lower-body) on DressCode—measure SSIM, LPIPS, and mask efficiency to validate minimally-invasive claim
  2. Stress test multi-round dummy garment pipeline on edge cases: long-sleeve→rolled-up, pants→shorts with exposed legs—assess identity preservation and pose coherence across rounds
  3. Evaluate Top-level Agent ordering accuracy on 50 multi-garment outfits with ground-truth layering annotations—measure ordering error rate and correlation with final SSIM

## Open Questions the Paper Calls Out

### Open Question 1
Can InstructVTON be distilled into a single end-to-end model that preserves mask efficiency and styling control while reducing inference latency from ~1 minute to real-time speeds? Basis: Authors state they are "actively working on" distilling the agent end-to-end using InstructVTON as teacher to overcome the latency limitation from multiple model calls. Unresolved because multi-model pipeline requires serial execution; distillation must compress reasoning without losing AutoMasker's optimal masking logic.

### Open Question 2
How can body part segmentation granularity be increased to support intermediate styling instructions (e.g., "sleeves rolled to 3/4 length") that fall between current discrete segment boundaries? Basis: Authors note that coarse segmentation limits flexibility for fine-grained style control like specific sleeve lengths. Unresolved because current BPSM produces discrete segments; no mechanism for partial or proportional masking within segments.

### Open Question 3
Would reformulating InstructVTON agents as closed-loop planners with Markov Decision Processes (MDPs) and reinforcement learning improve robustness to error propagation in multi-garment try-on? Basis: Authors identify open-loop planning as a limitation and propose exploring MDP modeling and RL optimization to mitigate error propagation. Unresolved because open-loop planners commit to action sequences without feedback; early errors compound through later steps.

### Open Question 4
What is the performance boundary of rule-based AutoMasking versus learned masking approaches when generalizing to uncommon garment types not well-represented in training data? Basis: Authors mention handling "uncommon garments" as motivation for future MDP exploration, and current parts inclusion rules are based on structured garment attributes that may not cover rare categories. Unresolved because rule-based systems require manual specification; performance on out-of-distribution garment types remains unquantified.

## Limitations
- Latency of ~1 minute for 3-garment try-on due to multiple model calls prevents real-time use
- Coarse segmentation granularity limits fine-grained styling control (e.g., partial sleeve coverage)
- Open-loop agent planning allows error propagation without feedback correction
- No published segmentation model specifications or parts inclusion rules prevent exact reproduction

## Confidence
- High confidence: Mask efficiency improvements (0.83 vs 0.73) and quantitative SSIM/LPIPS