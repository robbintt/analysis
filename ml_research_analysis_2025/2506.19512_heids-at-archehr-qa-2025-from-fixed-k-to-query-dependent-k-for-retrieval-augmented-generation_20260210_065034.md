---
ver: rpa2
title: 'heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval
  Augmented Generation'
arxiv_id: '2506.19512'
source_url: https://arxiv.org/abs/2506.19512
tags:
- clinical
- attribution
- pipeline
- patient
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes heiDS's approach for the ArchEHR-QA 2025 shared
  task on generating answers with attributions from clinical notes. The team designed
  a retrieval-augmented generation pipeline that explores query-dependent-k retrieval
  strategies instead of fixed-k approaches.
---

# heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2506.19512
- Source URL: https://arxiv.org/abs/2506.19512
- Reference count: 39
- Primary result: Best configuration used surprise RLT strategy with post-retrieval attribution, achieving strict F1-score of 0.37 and overall relevance of 0.35

## Executive Summary
This paper describes heiDS's approach for the ArchEHR-QA 2025 shared task on generating attributed answers from clinical notes. The team developed a retrieval-augmented generation pipeline that replaces fixed-k retrieval with query-dependent-k strategies, exploring methods like surprise, autocut, autocut*, and elbow to determine how many sentences to include based on similarity score distributions. Their best-performing configuration combined the surprise RLT strategy with post-retrieval attribution, matching baseline performance while demonstrating practical advantages for real-world applications where complete note inclusion is infeasible due to LLM input constraints.

## Method Summary
The method employs a RAG pipeline with query-dependent-k retrieval strategies, specifically using the surprise RLT method that applies extreme value theory (generalized Pareto distributions) to determine adaptive score thresholds for truncation. The pipeline combines patient and clinician questions as a unified query, retrieves relevant sentences using BAAI/bge-large-en-v1.5 embeddings in FAISS, and generates answers using LLaMA-3.3-70B or Mixtral-8x7B with one-shot prompting. Post-retrieval attribution passes retrieved sentences with identifiers to the LLM for inline citation generation, while post-generation attribution uses similarity-based matching. The approach addresses the practical limitation of using all sentences (k=54) in real-world applications where clinical notes exceed LLM context windows.

## Key Results
- Surprise RLT with post-retrieval attribution achieved strict F1-score of 0.37 and overall relevance of 0.35
- Post-retrieval attribution outperformed post-generation (F1 0.37 vs 0.27), demonstrating higher factuality
- Combined patient+clinician queries showed slight improvement (F1 0.30) over single-question formulations (F1 0.27)
- Best configuration matched baseline performance but highlighted practical benefits for larger real-world clinical notes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-dependent-k retrieval strategies can reduce manual tuning overhead while maintaining performance comparable to fixed-k approaches in clinical RAG systems.
- Mechanism: The surprise RLT method applies extreme value theory (generalized Pareto distributions) to retrieval score distributions, establishing adaptive score thresholds rather than fixed count cut-offs. This allows the number of retrieved sentences to vary per query based on similarity score characteristics.
- Core assumption: The distribution of similarity scores contains meaningful inflection points that correlate with relevance boundaries (assumption, not empirically validated in this work).
- Evidence anchors:
  - [abstract] "...query-dependent-k retrieval strategy, including the existing surprise and autocut methods and two new methods proposed in this work, autocut* and elbow."
  - [section 2.3] "This method determines the number k of sentences to consider by first adjusting retrieval scores using generalized Pareto distributions from extreme value theory"
  - [corpus] Neighbor papers (Dynamic Context Selection, Fishing for Answers) similarly explore adaptive retrieval but through different mechanisms (distractor mitigation, iterative strategies); limited direct corpus evidence for surprise-specific mechanisms.
- Break condition: Uniformly decreasing similarity scores without clear distributional discontinuities may prevent the surprise method from identifying meaningful truncation points, potentially defaulting to poor k selection.

### Mechanism 2
- Claim: Post-retrieval attribution with explicit sentence identifiers yields higher factuality scores than post-generation attribution approaches.
- Mechanism: Retrieved sentences are passed to the LLM with associated identifiers; the model is prompted to include citations (|id| format) within generated sentences, enforcing direct grounding. Post-processing validates citation placement.
- Core assumption: LLMs can reliably follow citation formatting instructions when provided with numbered evidence (supported by experiments showing post-retrieval outperforming post-generation).
- Evidence anchors:
  - [abstract] "The proposed approach employs a retrieval augmented generation framework that uses query-dependent-k retrieval strategies"
  - [section 3.2] "surprise retrieval strategy achieves a strict F1-score of 0.37 and overall relevance of 0.35, making it our top post-retrieval configuration"
  - [section 3.2] Post-generation attribution "yields a strict F1-score of 0.27... it underperforms relative to the best-performing post-retrieval configuration"
  - [corpus] RAGentA (corpus neighbor) similarly targets attributed QA with multi-agent RAG, suggesting domain consensus on attribution importance.
- Break condition: Complex multi-sentence reasoning requiring synthesis across evidence may cause citation omissions or incorrect attributions; prompt sensitivity can degrade citation quality.

### Mechanism 3
- Claim: Combining patient and clinician questions as a unified query improves retrieval and generation performance over single-question formulations.
- Mechanism: Patient questions provide contextual narrative while clinician questions offer precise clinical terminology; concatenating both creates a richer query representation for embedding and retrieval.
- Core assumption: The embedding model can effectively represent combined question semantics without dilution (partially supported by ablation results).
- Evidence anchors:
  - [section 2.2] "a query that is constructed using both patient and clinical questions instead of considering only one of them"
  - [appendix D, table 3] Patient+Clinician query yields F1=0.30 vs. 0.27 for single-question variants
  - [corpus] No direct corpus evidence for this specific query formulation strategy.
- Break condition: Contradictory information between patient and clinician questions, or excessive query length, may degrade embedding quality.

## Foundational Learning

- Concept: **Retrieval Augmented Generation (RAG)**
  - Why needed here: The entire pipeline is built on RAG principles—understanding how retrieval and generation interact is essential for debugging and optimization.
  - Quick check question: Can you explain why a RAG approach is preferred over direct LLM generation for clinical QA with attribution requirements?

- Concept: **Ranked List Truncation (RLT)**
  - Why needed here: The core innovation is replacing fixed-k with adaptive RLT strategies; understanding score-based truncation is critical for implementing and extending these methods.
  - Quick check question: Given a descending similarity score list [0.92, 0.88, 0.84, 0.82, 0.78, 0.41, 0.39], where would autocut* likely truncate and why?

- Concept: **Extreme Value Theory (EVT) in IR**
  - Why needed here: The surprise method specifically applies EVT and generalized Pareto distributions; without this foundation, the scoring adjustment is opaque.
  - Quick check question: Why might EVT be suitable for modeling the tail behavior of retrieval score distributions?

## Architecture Onboarding

- Component map:
  1. **Embedding & Indexing**: BAAI/bge-large-en-v1.5 → FAISS index (clinical note sentences → 1024-dim vectors)
  2. **Query Formulation**: Patient question + Clinician question (concatenated)
  3. **Retrieval**: Cosine similarity → Surprise RLT (or elbow/autocut*/fixed-k alternatives)
  4. **Generation**: LLaMA-3.3-70B or Mixtral-8x7B with one-shot prompting
  5. **Attribution**: Post-retrieval (inline citations) or Post-generation (similarity-based matching)
  6. **Post-processing**: Citation validation and formatting

- Critical path: Query construction → Embedding lookup → RLT truncation → LLM prompt construction → Answer generation → Attribution validation

- Design tradeoffs:
  - Fixed-k vs. Query-dependent-k: Fixed-k offers predictability and higher recall (k=54 = all sentences); query-dependent-k adapts to query complexity but may miss relevant content (lower recall, see Table 1).
  - Post-retrieval vs. Post-generation attribution: Post-retrieval yields higher F1 (0.37 vs. 0.27) but requires prompt engineering; post-generation is model-agnostic but requires similarity threshold tuning.
  - One-shot vs. Zero-shot: One-shot improves formatting compliance but adds prompt complexity and token overhead.

- Failure signatures:
  - Low recall with surprise/elbow: Likely over-aggressive truncation on queries with uniformly declining scores.
  - Missing/invalid citations: Prompt formatting issues or LLM instruction-following failures.
  - Over-attribution (post-generation): Threshold set too low, causing spurious sentence linkages.
  - Under-attribution (post-generation): Threshold set too high, missing valid evidence connections.

- First 3 experiments:
  1. **Baseline replication**: Run fixed-k=54 with all sentences + zero-shot prompting to match organizer baseline; verify strict F1 ~0.43.
  2. **RLT strategy comparison**: Implement surprise, elbow, and autocut* on dev set; measure retrieval metrics (P/R/F1) against essential/supplementary labels to understand truncation behavior.
  3. **Ablation on query formulation**: Test patient-only, clinician-only, and combined queries with a fixed retrieval strategy (e.g., k=15) to isolate query formulation impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does domain-specific text enrichment, such as expanding medical acronyms, significantly improve the retrieval performance of query-dependent-k strategies?
- Basis in paper: [explicit] The authors state in the Limitations section that "Expanding medical acronyms to their complete form... before indexing could improve retrieval performance," as no preprocessing was currently applied.
- Why unresolved: Current embeddings may fail to match semantic similarity effectively if clinical notes use dense abbreviations that differ from the query terminology.
- What evidence would resolve it: A comparative evaluation on the ArchEHR-QA dataset measuring retrieval F1-scores with and without a medical acronym expansion pre-processing step.

### Open Question 2
- Question: How do query-dependent-k strategies perform on full-scale EHRs where inputting all text is infeasible?
- Basis in paper: [explicit] The authors justify their pipeline by noting that real-world applications "can contain far more text" than the shared task dataset, making the baseline approach of using all sentences "infeasible due to LLMs input length constraints."
- Why unresolved: The baseline outperformed the proposed method by utilizing the full context (max 54 sentences), but this comparison does not reflect the constraints of longer, real-world clinical records.
- What evidence would resolve it: Benchmarking the pipeline on a dataset with significantly longer document lengths (e.g., full patient histories) that exceed the context window of the LLM.

### Open Question 3
- Question: Can the "surprise" method be refined to improve recall while maintaining its high precision?
- Basis in paper: [inferred] The results show the submitted "surprise" pipeline achieved high precision (0.62) but very low recall (0.26), leading the authors to observe that the model "frequently overlooks relevant evidence sentences."
- Why unresolved: The current truncation logic appears too aggressive, potentially cutting off relevant information that falls just beyond the score threshold.
- What evidence would resolve it: Experiments testing hybrid approaches, such as applying re-ranking before truncation or adjusting the surprise threshold, to balance strict factuality with evidence coverage.

## Limitations
- Unvalidated distributional assumptions: Surprise RLT method relies on generalized Pareto distributions from extreme value theory without empirical validation of retrieval score distribution patterns
- Prompt sensitivity and truncation: One-shot prompt template is partially truncated, and format violations occur frequently enough to require post-processing validation
- Lack of cross-dataset generalization: All experiments conducted on single ArchEHR-QA dataset without validation on different clinical QA datasets or question distributions

## Confidence

- **High confidence**: The observation that post-retrieval attribution outperforms post-generation (F1 0.37 vs 0.27) is directly supported by experimental results and consistent across ablation configurations.

- **Medium confidence**: The mechanism of surprise RLT applying extreme value theory is well-explained in the literature, but the specific implementation details and parameter choices are not fully specified, limiting confidence in exact replication.

- **Low confidence**: The claim that combining patient and clinician questions improves performance over single-question formulations is based on limited ablation results (F1 0.30 vs 0.27) without broader validation or ablation of the specific components of this combined representation.

## Next Checks

1. **Implementation validation**: Re-implement the surprise RLT truncation algorithm and test it on the development set with multiple threshold configurations to verify the claimed F1 scores and understand truncation behavior patterns.

2. **Prompt robustness testing**: Systematically vary the one-shot prompt template (with full template reconstruction) and measure attribution performance to identify the sensitivity threshold and optimal prompt structure.

3. **Cross-dataset evaluation**: Apply the best-performing configuration (surprise RLT with post-retrieval attribution) to a different clinical QA dataset or a subset of ArchEHR-QA with modified question distributions to assess generalizability of the query-dependent-k approach.