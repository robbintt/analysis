---
ver: rpa2
title: 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward'
arxiv_id: '2511.03108'
source_url: https://arxiv.org/abs/2511.03108
tags:
- formal
- theorem
- informal
- statement
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We analyze the miniF2F benchmark from an end-to-end AI math Olympiad
  perspective, where models must read informal statements, formalize them in Lean,
  and prove them, with credit only if the final proof matches the original problem.
  We find that while SoTA autoformalization and theorem proving models achieve high
  individual accuracies (97% and 69%), the full pipeline drops to 36% due to discrepancies
  between formal and informal statements in over half the problems.
---

# miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward

## Quick Facts
- arXiv ID: 2511.03108
- Source URL: https://arxiv.org/abs/2511.03108
- Reference count: 40
- Primary result: End-to-end pipeline accuracy drops from 97% (autoformalization) and 69% (prover) to 36% due to formal-informal misalignment, improving to 70% on corrected miniF2F-v2.

## Executive Summary
This paper critically examines the miniF2F benchmark for automated theorem proving, revealing that reported high accuracies for individual components mask a significant drop in end-to-end performance due to discrepancies between informal and formal problem statements. By creating a corrected version of the benchmark (miniF2F-v2), the authors demonstrate that aligning formal and informal statements dramatically improves pipeline effectiveness, while also exposing the unreliability of LLM-based evaluation metrics. The work provides a more rigorous framework for evaluating progress in formal reasoning systems.

## Method Summary
The study analyzes the end-to-end pipeline from informal math problems to formal proofs in Lean, identifying that performance bottlenecks stem from semantic misalignment between formal and informal statements rather than raw model capabilities. The authors correct over 300 formal and informal statements to create miniF2F-v2, then evaluate multiple autoformalization and theorem proving models under both simplified and competition-level settings. They also compare LLM-based evaluation against human expert verification to quantify inflation in reported accuracies.

## Key Results
- End-to-end pipeline accuracy drops to 36% on miniF2F-v1 despite 97% autoformalization and 69% prover accuracy due to statement misalignment
- Correcting formal/informal discrepancies in miniF2F-v2 improves end-to-end accuracy to 70%
- Human expert evaluation reveals LLM-based judges inflate autoformalization accuracy by ~30 percentage points (97% vs 66%)
- Previously unprovable statements in miniF2F-v1 become provable in miniF2F-v2 after corrections

## Why This Works (Mechanism)

### Mechanism 1: Formal-Informal Alignment as a Primary Pipeline Bottleneck
When formal statements diverge from informal problems, autoformalizers may produce semantically incorrect translations that pass compilation but lead to failed proofs. Since credit is only given when the proved statement matches the original problem, these misalignments directly reduce end-to-end accuracy.

### Mechanism 2: Inflated Autoformalization Accuracy via LLM-as-a-Judge
LLM-based semantic equivalence checking systematically overestimates accuracy by treating semantically significant differences as negligible. Human expert verification reveals substantially lower accuracy than LLM judges, exposing this evaluation inflation.

### Mechanism 3: Improved Benchmark Quality Enables Higher Effective Pipeline Accuracy
Correcting errors in the benchmark eliminates unprovable states and reduces failures caused by mismatched inputs. Despite restoring original competition difficulty, the alignment allows models to work on valid goals, improving effective accuracy.

## Foundational Learning

**End-to-End Evaluation Pipeline (vs. Component-wise Evaluation)**
Why needed: Measuring individual component accuracy is misleading; only end-to-end performance matters for Olympiad settings.
Quick check: If autoformalizer has 97% accuracy and prover has 69% accuracy, what's the naive upper bound on pipeline accuracy? Why might actual accuracy be lower?

**Formal Semantics & Statement Alignment**
Why needed: Understanding semantic gaps between formal and informal statements is critical for reliable theorem proving.
Quick check: Why doesn't proving an "excessively simplified" problem earn credit? What's an example of simplification?

**The Role of Human-in-the-Loop for Evaluation**
Why needed: LLM-based judges are unreliable for subtle semantic distinctions; human verification is essential for trustworthy evaluation.
Quick check: What's the primary failure mode for LLM-as-a-Judge? How did the paper quantify the discrepancy?

## Architecture Onboarding

**Component map:**
Informal Problem Statement → Autoformalizer → Lean REPL (Syntax Checker) → Semantic Verifier (Judge) → Theorem Prover → Proof Verifier (Lean) → Judge (Credit Assignment)

**Critical path:** 2 → 4 → 7 (Autoformalizer → Semantic Verifier → Final Judge). Failures here cause cascading pipeline failures.

**Design tradeoffs:**
- Simplified vs Competition benchmarks: Easier evaluation vs realistic difficulty
- LLM vs Human Judge: Scalable but unreliable vs accurate but expensive
- @k Sampling vs @1: Improved success rates vs increased incorrect translation risk

**Failure signatures:**
- Autoformalization Failure: Non-compilable Lean code
- Translation Failure: Compiles but semantically wrong (most insidious)
- Prover Failure: Cannot find proof for formalized statement
- Unprovable Statement: Formal statement itself is incorrect

**First 3 experiments:**
1. Run baseline pipeline on miniF2F-v1, measure effective accuracy
2. Ablate judge component, compare reported vs human-verified accuracy
3. Evaluate on miniF2F-v2 (simplified), observe quality impact

## Open Questions the Paper Calls Out

**Open Question 1:** Do current state-of-the-art autoformalization models suffer from data contamination on miniF2F?
Basis: Section 4 notes specialized models performed worse on corrected dataset, suggesting possible contamination.
Resolution: Evaluation on newly curated, previously unseen Olympiad problems.

**Open Question 2:** Can reliable automated evaluation metrics replace LLM-based judges?
Basis: Section 4-6 demonstrate LLM judges inflate accuracy by treating discrepancies as negligible.
Resolution: Development of metrics correlating with human expert verification without generative LLM reasoning.

**Open Question 3:** What's the relative contribution of training data quality versus model architecture?
Basis: Appendix J calls for systematic ablation studies isolating data quality vs architecture impact.
Resolution: Isolated training runs using consistent architectures on original vs corrected datasets.

## Limitations

- Human expert evaluation doesn't scale, creating bottleneck for large-scale adoption
- Results may not generalize beyond Lean + math Olympiad domain
- Manual corrections to create miniF2F-v2 could introduce systematic biases affecting benchmark representativeness

## Confidence

**High Confidence:** Dramatic difference between component-wise and end-to-end accuracy due to formal-informal misalignment
**Medium Confidence:** ~30 percentage point inflation from LLM-based evaluation (specific numbers clear, broader validation needed)
**Medium Confidence:** Quality corrections improve effective accuracy despite increased difficulty (mechanism sound, effect magnitude depends on error distribution)

## Next Checks

1. Scale human evaluation to 100+ randomly selected autoformalization outputs across multiple models on miniF2F-v2
2. Replicate end-to-end pipeline analysis on different formal system (e.g., Isabelle/HOL) to test universality of misalignment bottleneck
3. Conduct ablation study systematically reintroducing known errors into miniF2F-v2 to isolate benchmark quality impact from difficulty increase