---
ver: rpa2
title: Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification
arxiv_id: '2511.11699'
source_url: https://arxiv.org/abs/2511.11699
tags:
- time
- verification
- area
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepPrism, a novel method for verifying the
  robustness of Recurrent Neural Networks (RNNs) by tightly enclosing the three-dimensional
  nonlinear surfaces generated by the Hadamard product. The key idea is to use a truncated
  rectangular prism approximation formed by two linear relaxation planes, minimizing
  both its volume and surface area to reduce overestimation.
---

# Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification

## Quick Facts
- arXiv ID: 2511.11699
- Source URL: https://arxiv.org/abs/2511.11699
- Authors: Xingqi Lin; Liangyu Chen; Min Wu; Min Zhang; Zhenbing Zeng
- Reference count: 40
- Primary result: Introduces DeepPrism method achieving higher certified accuracy than state-of-the-art baselines for RNN robustness verification

## Executive Summary
This paper introduces DeepPrism, a novel method for verifying the robustness of Recurrent Neural Networks (RNNs) by tightly enclosing the three-dimensional nonlinear surfaces generated by the Hadamard product. The key idea is to use a truncated rectangular prism approximation formed by two linear relaxation planes, minimizing both its volume and surface area to reduce overestimation. A refinement-driven method with multi-plane approximations is proposed to further improve verification accuracy. Experiments on four datasets across three tasks (image classification, speech recognition, sentiment analysis) show that DeepPrism achieves higher certified accuracy than state-of-the-art baselines while maintaining reasonable computational efficiency. The approach provides both theoretical insights and practical improvements for RNN robustness verification.

## Method Summary
DeepPrism approximates the nonlinear surface σ(x) ⊙ tanh(y) using truncated rectangular prisms formed by linear upper and lower planes. The core innovation is a hybrid volume-area optimization that jointly minimizes both the prism volume and surface area through a linear programming formulation. A weight α=0.674 balances these competing objectives. The method extends to multi-plane approximation by dividing the input region into sub-regions and combining their bounds using gradient descent refinement. Verification propagates these bounds through LSTM layers using DeepPoly backsubstitution to compute output intervals and determine certified robustness.

## Key Results
- Achieves higher certified accuracy than state-of-the-art baselines across four datasets (MNIST, GSC, FSDD, RT)
- Multi-plane refinement with 16 divisions improves certified accuracy by 15-20% over single-plane methods
- Volume-area hybrid optimization with α=0.674 consistently outperforms distance-based approaches
- Rectangular divisions (4-rec, 9-rec, 16-rec) outperform triangular divisions under high perturbation
- Maintains reasonable computational efficiency despite tighter approximations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the volume of the truncated rectangular prism enclosing the nonlinear surface produces tighter over-approximations than minimizing pointwise vertical distances.
- Mechanism: The volume V = (u_x - l_x)·(u_y - l_y)·height depends on the centroid height (z_c^(u) - z_c^(l)) rather than sampling points. By solving a single LP that jointly optimizes upper and lower planes A_u·x + B_u·y + C_u and A_l·x + B_l·y + C_l, the planes develop a synergistic relationship rather than being optimized independently.
- Core assumption: The centroid height adequately captures the "tightness" of the enclosure; reducing volume necessarily reduces over-approximation error in downstream verification.
- Evidence anchors: [abstract] "minimize both its volume and surface area for tighter over-approximation"; [section "Volume-based Method"] "the volume does not depend on the sampling points, thus reducing the influence of sampling randomness"; [corpus] Limited corpus support for volume-based optimization in neural verification.

### Mechanism 2
- Claim: Adding surface area minimization to the objective function produces more compact prisms that yield tighter output intervals.
- Mechanism: Surface area S is positively correlated with sum of |z_i - z_c| across corner points. The hybrid objective α·height + (1-α)·sum (with α=0.674 found optimal) balances "thickness" (volume proxy) against "flatness" (surface area proxy). Flatter prisms with the same volume have smaller output range projections.
- Core assumption: Surface area serves as a valid proxy for tightness of output intervals; the weighting α=0.674 generalizes across tasks and perturbation levels.
- Evidence anchors: [section "Hybrid Volume-Area-based Method"] "proper weighting found to be 0.674 favoring volume"; Appendix C Figure 7-8 shows red prism (smaller surface area) produces tighter output intervals than yellow prism (same volume, larger surface area); [corpus] No direct corpus validation for surface-area-weighted LP objectives in neural verification.

### Mechanism 3
- Claim: Multi-plane approximation with gradient descent refinement achieves tighter global bounds than single-plane methods.
- Mechanism: Divide the input region [l_x, u_x] × [l_y, u_y] into sub-regions (triangular or rectangular). For each sub-region τ_k, compute local bounds LB_k via LP, then combine: LB = Σλ_k·LB_k where λ_k are learned via gradient descent minimizing loss L = -g(x, ε, p, λ).
- Core assumption: The loss landscape is sufficiently smooth for gradient descent to find useful λ; subdivision captures local surface curvature better than global planes.
- Evidence anchors: [section "Multi-plane Approximation"] "the single-plane approximation is upgraded into a multi-plane approximation"; [Appendix F Figures 10-13] Multi-plane consistently outperforms single-plane across MNIST, GSC, FSDD, RT; [corpus] "Lipschitz-aware Linearity Grafting" uses piecewise linear approximations but does not validate gradient-based refinement specifically.

## Foundational Learning

- Concept: **Abstract Interpretation for Neural Verification**
  - Why needed here: DeepPrism operates within this framework, mapping RNN operations to geometric shapes (truncated prisms) whose properties can be efficiently computed.
  - Quick check question: Given an input interval [x-ε, x+ε], can you trace how a linear constraint propagates through one LSTM gate?

- Concept: **Linear Programming Relaxation of Nonlinear Functions**
  - Why needed here: The core technical contribution is reformulating tight bounding as LP with specific objective functions (volume-area hybrid).
  - Quick check question: For σ(x) ⊙ tanh(y) on domain [-1,1]×[-1,1], what constraints ensure a plane A·x + B·y + C is a valid upper bound?

- Concept: **DeepPoly Backsubstitution**
  - Why needed here: After obtaining linear bounds at each layer, DeepPrism uses backsubstitution to compute output intervals without explicit enumeration.
  - Quick check question: If neuron z has symbolic bound z ≤ a·x + b, and x has numerical bound x ∈ [l, u], what is the numerical bound on z?

## Architecture Onboarding

- Component map: Input perturbation → LP plane computation (bottleneck: LP solve time scales with divisions) → Symbolic bound propagation → Output interval → Robustness check
- Critical path: Input perturbation → LP plane computation (bottleneck: LP solve time scales with divisions) → Symbolic bound propagation → Output interval → Robustness check
- Design tradeoffs:
  - Single-plane vs. Multi-plane: Single-plane is faster (~2-15s on MNIST) but lower accuracy; Multi-plane with 16-rec achieves ~15-20% higher certified accuracy at 2-3× runtime cost
  - Division strategy: Rectangular divisions (4-rec, 9-rec, 16-rec) outperform triangular under high perturbation; finer divisions help more at larger ε
  - α weighting: 0.674 optimizes for volume dominance; deviating toward surface area (lower α) may help on surfaces with high corner variation
- Failure signatures:
  - **Timeout (>120s)**: Indicates LP is infeasible or region division too fine for model complexity
  - **Certified accuracy drops to 0%**: Perturbation ε too large relative to model capacity; bounds become vacuously wide
  - **Multi-plane no better than single-plane**: Surface curvature low in verified region; division overhead provides no benefit
- First 3 experiments:
  1. **Sanity check on synthetic σ(x)⊙tanh(y) surface**: Define known domain, verify that DeepPrism's computed planes satisfy all constraints at grid points; compare volume vs. Prover's distance-based planes
  2. **Single-plane α sweep on MNIST subset**: Run verification with α ∈ {0.0, 0.3, 0.5, 0.674, 0.8, 1.0} on 50 samples; plot certified accuracy vs. runtime to confirm 0.674 is near-optimal
  3. **Division strategy ablation**: On same MNIST subset with fixed ε=0.012, compare 2-tri-up, 4-tri, 4-rec, 16-rec; record accuracy/time tradeoff curve to guide production configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of the hybrid volume-area optimization and gradient descent refinement be reduced to improve scalability for deep learning models?
- Basis in paper: [explicit] The Conclusion explicitly states that future work "will focus on reducing computational overhead and enhancing the time efficiency of the approach."
- Why unresolved: While DeepPrism achieves higher verification accuracy, the multi-plane approximation method involves solving linear programming problems and gradient descent, which increases running time compared to single-plane baselines.
- What evidence would resolve it: A modified algorithm or optimization technique that maintains the high certified accuracy of DeepPrism but reduces the latency to match or beat the speed of existing single-plane verifiers.

### Open Question 2
- Question: Is the empirically determined weight α=0.674 mathematically optimal for all RNN architectures and perturbation ranges, or is it dataset-specific?
- Basis in paper: [inferred] The authors state the "proper weighting found to be 0.674 favoring volume" through exhaustive search, but they do not provide a theoretical proof that this value generalizes beyond the specific experimental conditions tested.
- Why unresolved: The optimal balance between volume and surface area likely depends on the geometry of the approximation region, which varies with model depth and input perturbations.
- What evidence would resolve it: A theoretical derivation of the optimal α or comprehensive ablation studies across varied perturbation magnitudes and network depths demonstrating that 0.674 consistently yields the best results.

### Open Question 3
- Question: Can an adaptive refinement strategy be developed to select the optimal geometric division (e.g., triangular vs. rectangular) based on local surface non-linearity rather than using fixed divisions?
- Basis in paper: [inferred] The paper compares fixed strategies (e.g., 4-rec, 16-rec) in RQ3 and notes that finer divisions capture features better but cost more time, yet it does not explore dynamic partitioning.
- Why unresolved: Fixed divisions may be inefficient, over-partitioning flat regions while under-partitioning steep, non-linear regions of the Hadamard product surface.
- What evidence would resolve it: An implementation of an adaptive partitioning algorithm that outperforms fixed strategies by adjusting division granularity according to local curvature, optimizing the trade-off between accuracy and runtime.

## Limitations
- **Sampling Dependency**: The LP formulation relies on sampled points to enforce constraints but the paper does not specify sampling density, creating potential soundness gaps if undersampled
- **Task-Specific Weighting**: While α=0.674 was found optimal for the tested tasks, its generalization to different RNN architectures or perturbation norms remains unproven
- **Computational Scaling**: Multi-plane refinement with 16 divisions increases verification time by 2-3×, potentially limiting deployment for real-time applications

## Confidence
- **High Confidence**: Core volume-area hybrid LP formulation (Mechanism 1 & 2) - directly specified with mathematical proofs and empirical validation across four datasets
- **Medium Confidence**: Multi-plane refinement effectiveness (Mechanism 3) - demonstrated across tasks but relies on gradient descent without detailed hyperparameter specification
- **Medium Confidence**: Generalization across diverse RNN tasks - results show consistent improvements but may not extend to all sequence modeling domains

## Next Checks
1. **Sanity check on synthetic σ(x)⊙tanh(y) surface**: Define known domain, verify that DeepPrism's computed planes satisfy all constraints at grid points; compare volume vs. Prover's distance-based planes
2. **Single-plane α sweep on MNIST subset**: Run verification with α ∈ {0.0, 0.3, 0.5, 0.674, 0.8, 1.0} on 50 samples; plot certified accuracy vs. runtime to confirm 0.674 is near-optimal
3. **Division strategy ablation**: On same MNIST subset with fixed ε=0.012, compare 2-tri-up, 4-tri, 4-rec, 16-rec; record accuracy/time tradeoff curve to guide production configuration