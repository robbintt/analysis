---
ver: rpa2
title: Graph Representations for Reading Comprehension Analysis using Large Language
  Model and Eye-Tracking Biomarker
arxiv_id: '2507.11972'
source_url: https://arxiv.org/abs/2507.11972
tags:
- graph
- nodes
- task
- reading
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes using LLM-generated graph representations to
  analyze reading comprehension, addressing the limitation of single-word analysis
  by capturing phrase-level semantic relationships. The method converts sentences
  into graph structures with nodes and edges based on semantic meaning and question-oriented
  prompts, then compares eye fixation distributions on important versus non-important
  nodes and edges.
---

# Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker

## Quick Facts
- arXiv ID: 2507.11972
- Source URL: https://arxiv.org/abs/2507.11972
- Authors: Yuhong Zhang; Jialu Li; Shilai Yang; Yuchen Xu; Gert Cauwenberghs; Tzyy-Ping Jung
- Reference count: 21
- Primary result: LLM-generated graph representations show alignment with human eye-tracking biomarkers in reading comprehension tasks

## Executive Summary
This study proposes using LLM-generated graph representations to analyze reading comprehension by capturing phrase-level semantic relationships beyond single-word analysis. The method converts sentences into graph structures with nodes and edges based on semantic meaning and question-oriented prompts, then compares eye fixation distributions on important versus non-important nodes and edges. Results demonstrate that LLM-generated graph topological structures exhibit high consistency with human eye-tracking biomarkers, with PageRank centrality achieving ROC-AUC scores up to 0.651 across different tasks. The analysis confirms that important nodes receive significantly more fixations (mean difference 0.291) compared to non-important nodes, validating the alignment between LLM-derived importance labels and human cognitive attention.

## Method Summary
The proposed method converts sentences into graph structures by first tokenizing text into words, then using LLMs to generate graph nodes and edges based on semantic meaning and question-oriented prompts. Each node and edge receives an importance score from the LLM. For eye-tracking analysis, fixation distributions are compared between important and non-important nodes/edges using centrality measures like PageRank and betweenness. The study employs 10 participants reading 10 reading comprehension articles with associated questions and answers, generating eye-tracking data that is analyzed alongside the LLM-generated graph representations.

## Key Results
- PageRank centrality achieved ROC-AUC scores up to 0.651 across different reading comprehension tasks
- Eye fixation analysis showed important nodes received significantly more fixations (mean difference 0.291) compared to non-important nodes
- Graph topological structures demonstrated high consistency with human eye-tracking biomarkers
- The approach successfully captures phrase-level semantic relationships beyond single-word analysis

## Why This Works (Mechanism)
The mechanism works by leveraging LLMs to generate graph representations that encode semantic relationships and importance scores at the phrase level, which aligns with how humans process text during reading. By comparing eye fixation patterns on important versus non-important graph elements, the study validates that LLM-derived importance labels correspond to actual cognitive attention patterns captured through eye-tracking biomarkers.

## Foundational Learning

**Graph Theory Basics** - Understanding nodes, edges, and centrality measures (PageRank, betweenness) is essential for interpreting how text is represented as graph structures and analyzed. Quick check: Can identify how centrality scores are calculated and what they represent in text analysis.

**Eye-Tracking Metrics** - Knowledge of fixation counts, saccade patterns, and their relationship to cognitive processing is crucial for interpreting the alignment between graph representations and human reading behavior. Quick check: Can explain why fixation duration correlates with processing difficulty.

**LLM Prompt Engineering** - Understanding how question-oriented prompts guide LLM graph generation is important for grasping the methodology's bias toward question-answering contexts. Quick check: Can describe how different prompt formulations might affect graph output.

## Architecture Onboarding

**Component Map**: Text -> Tokenization -> LLM Graph Generation -> Centrality Scoring -> Eye-Tracking Data Collection -> Fixation Distribution Analysis -> Validation

**Critical Path**: The core workflow flows from raw text through LLM processing to generate graph representations, which are then validated against eye-tracking data through statistical analysis of fixation distributions.

**Design Tradeoffs**: The approach trades computational complexity for richer semantic representation by using LLMs instead of simpler rule-based methods, while introducing potential bias toward question-oriented contexts through prompt design.

**Failure Signatures**: Low ROC-AUC scores or non-significant differences in fixation distributions would indicate poor alignment between LLM-generated importance labels and actual cognitive attention patterns.

**First Experiments**:
1. Test the method on single-sentence comprehension tasks to establish baseline performance
2. Compare fixation patterns on manually annotated important phrases versus LLM-generated important nodes
3. Evaluate the impact of different centrality measures on alignment with eye-tracking data

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The approach's effectiveness across diverse text types and reading tasks beyond comprehension remains untested
- Reliance on fixation counts as the sole eye-tracking biomarker may not capture full cognitive processing complexity
- The modest ROC-AUC score of 0.651 indicates substantial room for improvement in predictive accuracy

## Confidence

**High confidence**: The alignment between LLM-generated important nodes and human eye fixation patterns is well-supported by statistical analysis showing significant fixation count differences (mean difference 0.291).

**Medium confidence**: The claim that graph representations overcome single-word analysis limitations is plausible but requires further validation across different text genres and reading purposes.

**Low confidence**: The broader implications for educational applications and generalizability to real-world reading scenarios are not sufficiently supported by current evidence.

## Next Checks

1. Test the graph representation approach across multiple text genres (narrative, expository, technical) to evaluate generalizability beyond reading comprehension tasks.

2. Incorporate additional eye-tracking metrics (saccade amplitude, reading duration, pupil response) to validate whether fixation patterns alone adequately represent cognitive processing.

3. Conduct a cross-linguistic validation study to determine if LLM-generated graph structures maintain consistency with eye-tracking biomarkers across different languages and writing systems.