---
ver: rpa2
title: 'MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis'
arxiv_id: '2511.18676'
source_url: https://arxiv.org/abs/2511.18676
tags:
- size
- image
- medical
- vlms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Current vision-language models (VLMs) are optimized for categorical\
  \ or descriptive medical image analysis, leaving a critical gap in quantitative\
  \ reasoning\u2014essential for clinical tasks like tumor size estimation and joint\
  \ angle measurement. To address this, we introduce MedVision, a large-scale dataset\
  \ spanning 22 public datasets with 30.8 million image-annotation pairs across diverse\
  \ anatomies and modalities."
---

# MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis

## Quick Facts
- arXiv ID: 2511.18676
- Source URL: https://arxiv.org/abs/2511.18676
- Reference count: 40
- Current vision-language models (VLMs) are optimized for categorical or descriptive medical image analysis, leaving a critical gap in quantitative reasoning—essential for clinical tasks like tumor size estimation and joint angle measurement.

## Executive Summary
Current vision-language models excel at categorical medical image analysis but struggle with quantitative reasoning tasks essential for clinical practice, such as measuring tumor sizes or joint angles. To address this gap, we introduce MedVision, a large-scale dataset spanning 22 public datasets with 30.8 million image-annotation pairs across diverse anatomies and modalities. We benchmark three representative quantitative tasks: detection of anatomical structures and abnormalities, tumor/lesion size estimation, and angle/distance measurement. Off-the-shelf VLMs perform poorly on these tasks, but supervised fine-tuning on MedVision significantly improves performance across detection metrics (recall, precision, F1, IoU) and measurement accuracy (reduced absolute and relative errors). Despite gains, small object detection and precise angle measurements remain challenging. MedVision provides a foundation for developing VLMs capable of clinically useful quantitative reasoning.

## Method Summary
MedVision aggregates 22 public medical imaging datasets with 30.8 million image-annotation pairs, standardizing them into 2D slices with quantitative annotations including bounding boxes, major/minor axes for tumor/lesion size, and angles/distances. The benchmark evaluates three tasks: detection (IoU, recall, precision), tumor/lesion size estimation (MAE, MRE), and angle/distance measurement (MAE, MRE, success rate). We employ supervised fine-tuning with LoRA (r=16, α=16, dropout=0.05) on representative VLMs (Qwen2.5-VL 7B/32B), using task-specific prompts that include physical spacing information for measurement tasks. Training uses 70/30 patient-level splits, with 1M samples for detection and 5K for size/measurement tasks.

## Key Results
- Off-the-shelf VLMs show poor performance on quantitative tasks (IoU ~8%, MAE >10mm)
- Supervised fine-tuning on MedVision dramatically improves detection (IoU ~70%) and measurement accuracy (MAE reduced by 60-80%)
- Small object detection (<5% relative size) and precise angle measurements (<10°) remain challenging
- Physical spacing injection in prompts significantly improves measurement accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised fine-tuning with task-specific quantitative annotations dramatically improves VLM performance on medical measurement tasks.
- **Mechanism:** LoRA-based fine-tuning (r=16, α=16, dropout=0.05) adapts model weights while preserving pretrained knowledge. Training on structured measurement annotations (bounding boxes, bidirectional diameters, angle/distance values) teaches the model to output calibrated numeric predictions rather than qualitative descriptions.
- **Core assumption:** The visual encoder already captures sufficient spatial information; the bottleneck is in the language decoder's ability to map visual representations to precise numeric outputs.
- **Evidence anchors:** [abstract] "with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement"; [section 2.2] "Fine-tuning employed LoRA with r=16, α=16, dropout 0.05, applied to all linear projections"; [corpus] Related work on QVLM architecture notes VLMs "fail at quantitative spatial reasoning because their architectures destroy pixel-level information through patch embeddings"
- **Break condition:** Performance gains may not transfer to modalities or anatomies absent from training data (OOD evaluation shows degradation). Tasks requiring sub-pixel precision may remain fundamentally limited by encoder resolution.

### Mechanism 2
- **Claim:** Explicit physical spacing information in prompts enables unit-aware quantitative predictions.
- **Mechanism:** Medical images carry pixel-to-physical-unit metadata (mm spacing). By injecting adjusted pixel dimensions into the prompt, the model can convert relative visual estimates to absolute measurements. The adjustment accounts for each VLM's specific image preprocessing pipeline (resizing, padding).
- **Core assumption:** Models can learn to use numerical context in prompts to scale outputs, rather than treating spacing as irrelevant text.
- **Evidence anchors:** [section 2.1] "We restricted the medical image modalities to those with physical spacing information... essential for generating ground truth quantitative measurements"; [section 2.2] "A key feature of our dataset is the inclusion of quantitative annotations, which requires careful handling of physical spacing in text prompts"
- **Break condition:** If models fail to attend to numerical context tokens, spacing information won't influence predictions. Complex resize/crop pipelines may produce incorrect adjusted spacing values.

### Mechanism 3
- **Claim:** Task decomposition into detection → measurement enables progressive skill acquisition.
- **Mechanism:** The benchmark separates (1) localization (bounding box), (2) size estimation (major/minor axes), and (3) geometric measurement (angles/distances). Each task requires different visual reasoning: coarse localization vs. fine boundary perception vs. landmark identification.
- **Core assumption:** Localization precedes measurement; a model that cannot find the structure cannot measure it accurately.
- **Evidence anchors:** [section 3.1] Detection IoU correlates positively with target size; small objects (<5% relative size) remain challenging; [section 3.2-3.3] Measurement tasks show residual errors even after SFT, with failure modes linked to difficult localization (small angles, small lesions)
- **Break condition:** Detection improvement doesn't guarantee measurement improvement — a model may localize well but output inaccurate diameters if the ellipse-fitting concept isn't learned.

## Foundational Learning

- **Concept: Vision encoder patch embeddings and spatial resolution loss**
  - **Why needed here:** Understanding why off-the-shelf VLMs fail at quantitative tasks requires recognizing that patch-based encoders (ViT-style) compress local pixel information, losing the precise spatial indexing needed for sub-region measurements.
  - **Quick check question:** Can you explain why a 14×14 patch embedding might struggle to localize a 5-pixel lesion?

- **Concept: LoRA (Low-Rank Adaptation) fine-tuning**
  - **Why needed here:** The paper uses LoRA for efficient adaptation. Understanding rank, alpha, and target modules helps interpret what's being learned vs. frozen.
  - **Quick check question:** If LoRA rank is too low, what type of knowledge might fail to transfer?

- **Concept: Physical units in medical imaging (DICOM spacing)**
  - **Why needed here:** Quantitative medical analysis requires converting pixel measurements to mm. Understanding how spacing metadata works is essential for correctly generating prompts and interpreting outputs.
  - **Quick check question:** If an image has 0.5mm pixel spacing but the VLM resizes it to 2× the original size, what's the adjusted spacing?

## Architecture Onboarding

- **Component map:** 22 source datasets → standardized RAS+ orientation → 2D slice extraction → annotation generation (b-box, ellipse-fitting, landmark calculations) → Prompt layer (image description + task instruction + physical spacing + format requirement) → Model layer (Vision encoder → projector → LLM decoder with LoRA adapters) → Evaluation layer (Parse numeric outputs → compute task-specific metrics)

- **Critical path:** Verify source dataset has pixel-spacing metadata → Generate ground-truth annotations using provided ellipse-fitting and landmark code → Compute adjusted pixel spacing based on target VLM's resize strategy → Apply LoRA fine-tuning on 70/30 train/test split at patient level → Parse outputs using specified format constraints; compute weighted metrics

- **Design tradeoffs:** Multi-instance exclusion (slices with multiple bounding boxes excluded, limiting real-world complexity); Plane-specific training (detection/TL tasks use axial slices only; A/D uses all planes due to data scarcity — may bias generalization); LoRA vs. full fine-tuning (LoRA preserves base capabilities but may underfit on novel geometric reasoning; full FT risks catastrophic forgetting)

- **Failure signatures:** Small target detection collapse (IoU drops sharply for targets <5% of image area — encoder resolution bottleneck); Mode collapse in baselines (Pre-trained VLMs output limited value sets instead of diverse measurements — suggests regression head not calibrated); Small-angle confusion (Angles <10° have high error — geometric reasoning breaks down for small angular differences); Plane-OOD degradation (Coronal/sagittal performance drops when trained only on axial — limited cross-plane transfer)

- **First 3 experiments:** 1) Reproduction check: Load MedVision using `load_dataset()`, fine-tune Qwen2.5-VL-7B with specified LoRA config on detection task (1M samples), verify IoU improvement from ~8% to ~70% on test set; 2) Spacing ablation: Train with and without physical spacing information in prompts; quantify MAE difference in tumor size estimation to isolate the mechanism's contribution; 3) Cross-plane generalization: Fine-tune on axial slices only, evaluate on sagittal/coronal; measure precision/recall drop to characterize spatial reasoning transfer limitations.

## Open Questions the Paper Calls Out

- **Can VLMs perform robust quantitative reasoning when multiple anatomical structures or lesions are present simultaneously in a single image?** The authors exclude 2D slices with multiple b-boxes, limiting real-world applicability. Clinical images frequently contain multiple structures; single-instance detection is insufficient for clinical workflows.

- **What architectural modifications or training paradigms would enable VLMs to achieve reliable detection and measurement of structures smaller than 5% of image area?** Performance degrades systematically for small targets (<5% relative size), suggesting fundamental limitations in current visual encoders or tokenization strategies that fine-tuning alone cannot overcome.

- **Do MedVision-trained VLMs produce consistent quantitative measurements when the same structure is imaged across different anatomical planes or imaging modalities?** The paper reports plane-OOD performance but doesn't assess whether measurements of the same physical structure are consistent across axial, coronal, and sagittal views, which is critical for clinical reliability.

## Limitations

- Small object detection remains fundamentally challenging (IoU drops below 0.5 for targets under 5% of image area), suggesting architectural limits beyond what fine-tuning can overcome.
- The evaluation excludes multi-instance slices and mixed-modality images, creating an artificial simplicity that may not reflect clinical workflows.
- Cross-plane generalization is limited, with performance degrading when training on axial slices only, suggesting spatial reasoning doesn't transfer well across anatomical planes.

## Confidence

**High confidence**: The dataset construction methodology, task definitions, and evaluation metrics are well-specified and reproducible. The improvement in detection metrics (IoU from ~8% to ~70%) and measurement accuracy (reduced MAE/MRE) following supervised fine-tuning is robust and clearly demonstrated.

**Medium confidence**: The attribution of performance gains to specific mechanisms (LoRA fine-tuning, physical spacing injection, task decomposition) is plausible but not definitively isolated. The spacing injection mechanism's effectiveness depends on unstated assumptions about VLM attention patterns.

**Low confidence**: Claims about architectural limitations (e.g., patch embeddings destroying pixel-level information) are supported by related work but not directly tested within this study. The exclusion criteria (single-instance slices only) may overstate real-world performance.

## Next Checks

1. **Spacing ablation study**: Train the same SFT model with and without physical spacing information in prompts on tumor size estimation. Measure the absolute difference in MAE to quantify the spacing mechanism's contribution independent of other factors.

2. **Small target detection analysis**: Stratify detection performance by relative target size (e.g., <5%, 5-10%, >10% of image area). Plot IoU/F1 metrics across these bins to precisely characterize the performance cliff and determine if it's a universal limit or dataset-specific.

3. **Cross-plane transfer evaluation**: Fine-tune on axial slices only, then evaluate on coronal and sagittal planes. Measure precision, recall, and measurement accuracy drop to quantify the spatial reasoning generalization gap and inform whether the architecture can learn truly modality-agnostic geometric concepts.