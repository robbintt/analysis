---
ver: rpa2
title: 'GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language
  Models'
arxiv_id: '2505.24340'
source_url: https://arxiv.org/abs/2505.24340
tags:
- classes
- classification
- image
- zero-shot
- vllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoVision Labeler (GVL) addresses the challenge of geospatial image
  classification in scenarios where labeled data is scarce. It introduces a strictly
  zero-shot framework combining a vision Large Language Model (vLLM) to generate detailed
  image descriptions and a Large Language Model (LLM) to classify these descriptions
  into user-defined categories.
---

# GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models

## Quick Facts
- arXiv ID: 2505.24340
- Source URL: https://arxiv.org/abs/2505.24340
- Authors: Gilles Quentin Hacheme; Girmaw Abebe Tadesse; Caleb Robinson; Akram Zaytar; Rahul Dodhia; Juan M. Lavista Ferres
- Reference count: 16
- One-line primary result: Achieves up to 93.2% zero-shot accuracy on binary SpaceNet v7 task and 86.4% on UC Merced meta-classes using vision and language models.

## Executive Summary
GeoVision Labeler (GVL) is a strictly zero-shot geospatial classification framework that leverages vision and language models to classify satellite imagery without requiring labeled data or fine-tuning. It employs a two-stage pipeline: a vision Large Language Model (vLLM) generates rich, human-readable descriptions of input images, which are then classified into user-defined categories by a Large Language Model (LLM). This modular design enables flexible adaptation to various classification tasks and achieves competitive accuracy on both binary and multi-class benchmarks.

## Method Summary
GVL uses a two-stage pipeline: (1) a vLLM (e.g., Kosmos-2 or Llama 3.2 Vision) generates detailed image descriptions from satellite imagery, optionally augmented with geographic metadata; (2) an LLM (e.g., GPT-4o or Phi-3) classifies these descriptions into user-defined classes. For complex multi-class tasks, GVL employs hierarchical LLM-driven clustering, grouping semantically related classes into meta-classes and refining classifications at successive depths. A CLIP fallback mechanism handles cases where the LLM fails to map descriptions to the provided label set. The method is evaluated on binary (SpaceNet v7) and multi-class (UC Merced, RESISC45) tasks.

## Key Results
- Achieves 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task in SpaceNet v7.
- Delivers 86.4% accuracy on UC Merced meta-classes and 84.3% on RESISC45 meta-classes using hierarchical classification.
- Demonstrates up to 20 percentage points improvement in accuracy when injecting geo-context into the vLLM prompt.
- Shows that hierarchical clustering mitigates confusion among similar classes, though accuracy drops by 20-30 percentage points at finer classification depths.

## Why This Works (Mechanism)

### Mechanism 1: Sequential Decoupling of Perception and Reasoning
Decomposing image classification into a visual description task followed by a text-based reasoning task allows the system to bypass the need for a jointly trained multimodal classifier. A vision Large Language Model (vLLM) acts as a semantic extractor, converting pixel data into human-readable text. A text-only LLM then performs the classification logic. This modularity allows the system to adapt to new class definitions without retraining visual weights.

### Mechanism 2: Hierarchical Semantic Partitioning
Grouping fine-grained labels into coarser "meta-classes" improves accuracy by reducing the density of the decision boundary at each inference step. Instead of classifying among 45 classes simultaneously, an LLM first clusters labels into semantic groups. The pipeline classifies the image into a meta-class first, then recursively refines the classification within that subset.

### Mechanism 3: Prompt-Based Context Injection
Injecting geographic metadata and class lists directly into the vLLM prompt functions as a conditioning signal, improving visual grounding and classification accuracy. Providing the vLLM with the image filename (containing coordinates) or the list of target classes helps constrain the generative space of the description, steering it toward relevant features.

## Foundational Learning

- **Concept: Zero-Shot Learning (ZSL)**
  - Why needed here: GVL is strictly zero-shot, meaning no gradient updates or fine-tuning occur. You must understand that the model relies entirely on pre-trained knowledge and prompt engineering.
  - Quick check question: Can you explain why GVL is considered "strictly zero-shot" compared to RS-CLIP which requires "task-specific pretraining"?

- **Concept: Vision-Language Models (vLLMs)**
  - Why needed here: The pipeline uses models like Kosmos-2 or Llama 3.2 Vision. Understanding the difference between a standard CNN (feature extractor) and a vLLM (generative describer) is critical for debugging the first stage.
  - Quick check question: If the vLLM outputs "A gray area," why might the downstream LLM fail to classify it as an "Airport"?

- **Concept: Hierarchical Classification**
  - Why needed here: For complex datasets (e.g., RESISC45), the system moves from flat classification to a tree-based approach. Understanding "Depth" (D=0, D=1) is essential for interpreting the results tables.
  - Quick check question: Why does accuracy typically drop as the hierarchy depth increases (e.g., from Meta-class to Original class)?

## Architecture Onboarding

- **Component map**: Input Image + Optional Geo-Context → vLLM (Description) → LLM (Classification) → Output Label (+ CLIP Fallback if needed).
- **Critical path**: The quality of the Stage 1 description is the primary bottleneck. If the description lacks specificity (e.g., "green fields" vs. "circular agricultural plots"), the Stage 2 LLM cannot recover the correct class.
- **Design tradeoffs**:
  - Kosmos-2 vs. Llama 3.2 (Vision): Kosmos-2 shows stronger visual grounding for RS (up to 20 percentage points higher accuracy in some tests) but is more sensitive to long class lists in prompts.
  - Flat vs. Hierarchical: Hierarchical improves multi-class accuracy (up to 86.4% on meta-classes) but adds latency and complexity.
  - Prompting: Adding class lists helps binary/simple tasks but hurts performance on complex tasks (e.g., RESISC45) due to attention dilution.
- **Failure signatures**:
  - Description Dilution: Output becomes generic when too many classes are listed in the prompt.
  - CLIP Fallback Loop: High reliance on the CLIP fallback indicates the LLM classifier is failing to map descriptions to the provided label set.
  - Hierarchy Error Propagation: If D=0 classification is wrong, D=1 is guaranteed wrong.
- **First 3 experiments**:
  1. **Binary Baseline**: Run GVL on SpaceNet v7 (Buildings vs. No Buildings) with and without geo-context injection to verify the basic pipeline and prompt sensitivity.
  2. **Hierarchical Stress Test**: Run UC Merced classification. Compare "Flat" mode vs. "Hierarchical" mode (D=0 then D=1) to measure the confusion reduction delta.
  3. **Prompt Length Ablation**: On RESISC45, test the vLLM prompt with "Classes=Yes" vs. "Classes=No" to demonstrate the dilution effect on descriptive quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can vLLMs be adapted or pre-trained to effectively process multi-spectral bands (e.g., near-infrared) to overcome GVL's current RGB-only limitation?
- Basis in paper: [explicit] The Conclusion states: "Future work will explore integrating vLLMs pretrained on multi-spectral remote sensing data."
- Why unresolved: Current vLLMs used in the study (Kosmos-2, Llama 3.2) are primarily trained on RGB data, restricting GVL's utility for satellite platforms that leverage non-visible spectral bands for better classification.
- What evidence would resolve it: A study evaluating GVL's performance using a vLLM specifically fine-tuned on multi-spectral data compared against the current RGB baseline on identical tasks.

### Open Question 2
- Question: What adaptation strategies can be integrated into the pipeline to close the performance gap with supervised or fine-tuned models like RS-CLIP without violating the strict zero-shot constraint?
- Basis in paper: [explicit] The Conclusion notes: "Future work will explore... adaptation strategies to further close the gap to supervised baselines."
- Why unresolved: GVL currently lags behind fine-tuned baselines (e.g., ResNet50, RS-CLIP) on complex multi-class tasks, and the authors identify closing this gap while maintaining the zero-shot nature as a necessary advancement.
- What evidence would resolve it: Demonstrating a modified GVL workflow that approaches or exceeds the accuracy of RS-CLIP (95.9% on UC Merced) without requiring task-specific pretraining or labeled data.

### Open Question 3
- Question: How can the hierarchical classification strategy be modified to arrest the significant accuracy degradation observed at finer depths (D=1 and D=2) in complex multi-class datasets?
- Basis in paper: [inferred] The Results section highlights that while meta-class classification (D=0) achieves high accuracy (up to 86.4%), performance "tapers" and drops by 20-30 percentage points when distinguishing original classes at finer depths.
- Why unresolved: The paper establishes that hierarchical clustering helps coarse classification but does not propose a mechanism to maintain that advantage when drilling down to the specific, semantically similar classes where most errors occur.
- What evidence would resolve it: A revised hierarchical mechanism that yields a lower accuracy differential between the coarse meta-class level (D=0) and the final leaf-node classification (D=2).

## Limitations
- The strict zero-shot claim is contingent on the quality of the vLLM's natural language descriptions, and the performance ceiling is unclear without benchmarking fine-tuned vLLMs.
- Reliance on proprietary models (e.g., GPT-4o) introduces potential reproducibility concerns for non-academic users.
- The CLIP fallback mechanism's contribution to overall accuracy is not isolated, making it difficult to assess whether the LLM classifier is genuinely solving the task.

## Confidence
- **High Confidence**: The two-stage pipeline design (vLLM → LLM) and its modular adaptability to new class definitions are well-supported by ablation results.
- **Medium Confidence**: The hierarchical clustering mechanism improves accuracy for multi-class tasks, but the exact implementation details (e.g., meta-class generation prompts) are underspecified.
- **Low Confidence**: The CLIP fallback's contribution to overall accuracy is not isolated, making it unclear how much the LLM classifier is independently solving the task.

## Next Checks
1. **Prompt Sensitivity Test**: Systematically vary the vLLM prompt length and content (e.g., class list inclusion, geo-context injection) on RESISC45 to quantify the dilution effect and identify optimal prompt structures.
2. **CLIP Fallback Analysis**: Run GVL on SpaceNet v7 and log the frequency and accuracy of CLIP fallback classifications to assess the LLM classifier's true performance.
3. **Hierarchy Robustness Check**: Modify the hierarchical clustering on UC Merced to include ambiguous class pairs (e.g., "dense residential" vs. "sparse residential") and measure error propagation through the tree.