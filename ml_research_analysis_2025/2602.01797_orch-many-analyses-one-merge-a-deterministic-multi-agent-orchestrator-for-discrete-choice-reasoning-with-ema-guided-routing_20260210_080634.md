---
ver: rpa2
title: 'ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for
  discrete-choice reasoning with EMA-guided routing'
arxiv_id: '2602.01797'
source_url: https://arxiv.org/abs/2602.01797
tags:
- orch
- accuracy
- routing
- agent
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ORCH, a deterministic multi-agent orchestration
  framework for discrete-choice reasoning that improves accuracy over single-model
  baselines by 10+ points on MMLU-Pro and 50+ points on GSM8K. The system decomposes
  tasks into parallel analyses by multiple heterogeneous LLMs, then merges results
  via a dedicated merge agent.
---

# ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing

## Quick Facts
- arXiv ID: 2602.01797
- Source URL: https://arxiv.org/abs/2602.01797
- Authors: Hanlin Zhou; Huah Yong Chan
- Reference count: 6
- One-line primary result: Improves accuracy over single-model baselines by 10+ points on MMLU-Pro and 50+ points on GSM8K

## Executive Summary
This paper introduces ORCH, a deterministic multi-agent orchestration framework for discrete-choice reasoning. It decomposes tasks into parallel analyses by multiple heterogeneous LLMs, then merges results via a dedicated merge agent. An optional EMA-guided router uses historical accuracy, latency, and cost to adaptively select agents. Experiments show ORCH consistently outperforms vote and single-model baselines, with statistically significant gains on MMLU-Pro and GSM8K. Ablations confirm multi-agent collaboration and routing both contribute to performance. ORCH offers a reproducible, interpretable approach for deploying LLM ensembles in high-value reasoning tasks.

## Method Summary
ORCH implements a "Many Analyses, One Merge" architecture for discrete-choice reasoning. The system dispatches the same query (or decomposed sub-questions) to multiple distinct LLM agents (e.g., GPT-4o-mini, DeepSeek, Grok). Each generates an independent analysis. A dedicated "merge agent" then reviews these distinct outputs, identifies consensus, and synthesizes a final discrete choice. The system enforces deterministic routing rules (fixed agent priority, temperature=0) to stabilize behavior. An optional EMA-guided router maintains running performance scores for each agent and adaptively selects which agents to invoke based on historical accuracy, latency, and cost. The framework is evaluated on MMLU, MMLU-Pro, and GSM8K benchmarks using accuracy, latency, and cost as primary metrics.

## Key Results
- ORCH improves accuracy over single-model baselines by 10+ points on MMLU-Pro and 50+ points on GSM8K
- EMA-guided routing provides measurable but variable boosts in efficiency and accuracy
- Multi-agent collaboration and routing both contribute to performance gains
- ORCH offers a reproducible, interpretable approach for deploying LLM ensembles in high-value reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Cross-Agent Error Correction via Structured Merging
- **Claim:** Aggregating parallel analyses from heterogeneous LLMs through a dedicated merge agent improves accuracy over single-model baselines by filtering errors and synthesizing strengths.
- **Mechanism:** The system dispatches the same query (or decomposed sub-questions) to multiple distinct LLM agents (e.g., GPT-4o-mini, DeepSeek, Grok). Each generates an independent analysis. A dedicated "merge agent" then reviews these distinct outputs, identifies consensus, and synthesizes a final discrete choice, effectively acting as a meta-reasoner that cancels out individual agent hallucinations or blind spots.
- **Core assumption:** The merge agent possesses sufficient reasoning capability to adjudicate conflicting analyses, and the base agents have uncorrelated error profiles.
- **Evidence anchors:**
  - [abstract]: "...multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice."
  - [section 3.2]: "ORCH follows a 'Many Analyses, One Merge' architecture... their outputs are subsequently consolidated by a dedicated merger to produce a single, final decision."
  - [corpus]: The neighbor paper "Multi-Agent Collaboration via Evolving Orchestration" supports the general finding that multi-agent collaboration scales problem-solving but often relies on static structures, which ORCH attempts to systematize.
- **Break condition:** If the base agents share a systematic bias or hallucination, the merge agent fails to detect it, or the cost/latency of invoking multiple agents exceeds the value of the accuracy gain (e.g., real-time low-stakes tasks).

### Mechanism 2: Protocol-Level Determinism for Reproducibility
- **Claim:** Enforcing fixed routing and aggregation rules (rather than stochastic routing) stabilizes system behavior and reproducibility without requiring model retraining.
- **Mechanism:** ORCH replaces heuristic or random agent selection with deterministic rules (e.g., fixed agent priority, temperature=0). "Determinism" here is defined at the orchestration layer: given the same input and fixed agent roles, the workflow path is identical. This reduces variance in evaluation and deployment, making failures easier to debug.
- **Core assumption:** The underlying LLM APIs remain sufficiently stable across calls; "determinism" refers to the orchestration logic, not strict bit-level reproducibility of the remote model weights.
- **Evidence anchors:**
  - [abstract]: "Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol..."
  - [section 3.1]: "ORCH implements a reproducible discrete-choice routing rule (e.g., argmax with a fixed priority/tie-break)... it therefore does not convert ORCH into a stochastic router."
  - [corpus]: Evidence is weak regarding specific deterministic protocols in neighbors; most related works (e.g., "Multi-Agent Collaboration via Evolving Orchestration") focus on capability scaling rather than reproducibility guarantees.
- **Break condition:** If the downstream application requires exploration or adaptation to novel tasks where fixed routing fails to find the optimal agent, or if API version drift changes the behavior of "fixed" agents.

### Mechanism 3: EMA-Guided Adaptive Routing (Optional)
- **Claim:** Using Exponential Moving Average (EMA) of historical performance (accuracy, latency, cost) to select agents provides a measurable but variable boost in efficiency and accuracy.
- **Mechanism:** The router maintains a running EMA score for each agent based on past success rates and latency. It adaptively selects which agents to invoke or assigns roles (e.g., "dispatcher" vs. "merger") based on these scores. This allows the system to phase out underperforming or costly agents for specific task types without manual intervention.
- **Core assumption:** Historical accuracy on benchmark tasks correlates with future performance on similar queries, and valid feedback signals (gold labels or proxies) are available to update the EMA.
- **Evidence anchors:**
  - [abstract]: "An optional EMA-guided router uses historical accuracy, latency, and cost to adaptively select agents."
  - [section 3.5]: "This mechanism is inherently feedback-dependent: EMA updates require knowledge of whether the last prediction was correct."
  - [corpus]: Related work like "Knowledge Base-Aware Orchestration" discusses dynamic orchestration but typically via knowledge bases rather than performance metrics like EMA.
- **Break condition:** In live production where ground truth is unavailable or delayed, the EMA cannot update effectively. Fast "concept drift" in query types may also cause the EMA (a lagging indicator) to select suboptimal agents.

## Foundational Learning

- **Concept: Discrete-Choice Reasoning**
  - **Why needed here:** The entire ORCH framework is optimized for tasks where the output is a single selection from a finite set (e.g., MMLU A-D, GSM8K final integer), distinct from open-ended generation.
  - **Quick check question:** Can you explain why "voting" is a viable aggregation strategy for discrete choices but potentially problematic for open-ended text generation?

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** This is the mathematical core of the optional routing module. You must understand how the smoothing factor ($\alpha$) balances recent performance against historical stability.
  - **Quick check question:** If an agent suddenly fails 5 times in a row, how would a high $\alpha$ (e.g., 0.8) affect its routing score differently than a low $\alpha$ (e.g., 0.1)?

- **Concept: Heterogeneous Model Ensembles**
  - **Why needed here:** ORCH relies on "heterogeneous LLMs" (different providers/families). Understanding that diverse models make different errors is key to understanding why the "merge" step works.
  - **Quick check question:** Why might using three instances of GPT-4 with different prompts yield less "error correction" benefit than using GPT-4, Claude, and DeepSeek together?

## Architecture Onboarding

- **Component map:**
  1. **Intake Layer:** Standardizes input (e.g., "Question: ... Options: A...").
  2. **Dispatcher (Optional):** Decomposes complex questions into sub-questions.
  3. **Agent Pool:** Parallel execution of heterogeneous LLMs (e.g., Agent-O, Agent-D, Agent-X).
  4. **Merger Agent:** A specific LLM prompted to synthesize the Agent Pool's outputs into one final answer.
  5. **Router (Optional):** EMA-based logic to select which agents are active or assigned to specific roles.

- **Critical path:** The prompt engineering for the **Merger Agent** is the highest-leverage component. If the merge prompt fails to extract the correct logic from the conflicting analyses, the system performs poorly (or defaults to the fallback model).

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** ORCH latency (~11-18s) is significantly higher than single models (<2s) due to sequential API calls and the final merge step.
  - **Cost:** API costs are roughly 3x-6x higher than a single model or simple vote because of the "analyze + merge" token consumption.

- **Failure signatures:**
  - **Consensus Hallucination:** All agents agree on the wrong answer confidently; the merger accepts it.
  - **Parsing Failure:** The Merger outputs a justification but fails to include the required final tag (e.g., "[FINAL: A]"), triggering fallback logic.
  - **Router Oscillation:** In EMA mode, if feedback is noisy, the router rapidly switches agents, preventing stability.

- **First 3 experiments:**
  1. **Baseline Replication:** Run the single-agent baselines (OpenAI, DeepSeek, XAI) on a small MMLU subset (e.g., 30 questions) to establish local accuracy benchmarks.
  2. **The Merge Test:** Implement the "VOTE" baseline vs. the "ORCH Merger" (using a simple prompt for the merger) to verify that the merger logic outperforms simple majority voting on conflicting answers.
  3. **Ablation (ORCH-1):** Remove one agent (e.g., XAI) and observe the accuracy drop to quantify the marginal value of that specific agent's diversity to the ensemble.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the EMA-guided routing mechanism be adapted for live deployment scenarios where gold-label feedback is unavailable?
- **Basis in paper:** [explicit] The authors note the router relies on "answer-based feedback" and is "mainly intended for benchmarking," explicitly stating the design is "not yet adapted to real-world 'unlabelled or weak-feedback' environments."
- **Why unresolved:** The current performance-based routing updates depend on knowing the correct answer post-inference, a condition that rarely exists in production environments.
- **What evidence would resolve it:** A study demonstrating effective routing using proxy signals (e.g., verifier scores, self-consistency confidence, or delayed user feedback) that correlates with accuracy gains.

### Open Question 2
- **Question:** Does the ORCH framework generalize to open-ended or generative tasks beyond discrete-choice reasoning?
- **Basis in paper:** [explicit] The paper acknowledges evaluation is limited to discrete-choice benchmarks and lists extending to "code reasoning, retrieval-augmented QA, and multi-turn dialogue" as a specific direction for future work.
- **Why unresolved:** The "Many Analyses, One Merge" paradigm is optimized for selecting from finite options; it is unclear how the merger agent would synthesize coherent long-form text or code from heterogeneous analyses without contradictions.
- **What evidence would resolve it:** Successful application of the ORCH architecture on generative benchmarks (e.g., HumanEval or MT-Bench) showing statistically significant improvements over single-model baselines.

### Open Question 3
- **Question:** Can the collaborative reasoning traces produced by ORCH be distilled into a single student model to preserve accuracy while reducing latency and cost?
- **Basis in paper:** [explicit] Future work proposes distilling traces to achieve an "offline multi-agent, online single-model" deployment pattern.
- **Why unresolved:** While the paper establishes that the ensemble improves reasoning, it is uncertain if a smaller, single model can effectively learn the synthesized "merge" logic without the explicit diversity of the agent pool.
- **What evidence would resolve it:** Experiments showing a distilled single model retaining a significant portion of ORCH's performance edge (e.g., >50% of the accuracy gain) over standard single-model baselines.

## Limitations
- The reproducibility of ORCH's performance gains is limited by the absence of explicit prompt templates for the dispatcher, agent, and merger components
- The 10+ point MMLU-Pro and 50+ point GSM8K improvements are tightly coupled to specific prompt engineering and the choice of heterogeneous models, which are not fully disclosed
- The EMA routing module introduces additional variability through its feedback-dependent updates, which require access to ground truth labelsâ€”a condition not always met in live deployment

## Confidence
- **High Confidence:** The deterministic orchestration framework (fixed routing, merging protocol) and its role in stabilizing system behavior
- **Medium Confidence:** The claim that multi-agent collaboration outperforms single models by 10+ points on MMLU-Pro and 50+ on GSM8K, given the absence of full prompt templates and the reliance on specific agent combinations
- **Low Confidence:** The magnitude and stability of EMA-guided routing benefits, as these depend on feedback quality and task drift, neither of which are controlled in the reported experiments

## Next Checks
1. **Prompt Template Replication:** Implement and test the dispatcher, agent, and merger prompts using the paper's described logic (decomposition, analysis, merging). Measure accuracy drop if prompts are approximated versus exact.
2. **Agent Diversity Impact:** Systematically remove one agent at a time (e.g., XAI, DeepSeek) and measure accuracy loss to quantify the marginal value of heterogeneous model diversity.
3. **EMA Routing in Live Feedback Mode:** Deploy ORCH with EMA routing on a dataset where ground truth is delayed or unavailable, and assess whether routing stability degrades over time.