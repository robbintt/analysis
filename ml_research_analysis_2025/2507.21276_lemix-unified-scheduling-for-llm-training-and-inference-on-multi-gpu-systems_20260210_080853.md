---
ver: rpa2
title: 'LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems'
arxiv_id: '2507.21276'
source_url: https://arxiv.org/abs/2507.21276
tags:
- node
- training
- inference
- task
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L EMIX, a unified system for scheduling LLM
  training and inference workloads on multi-GPU systems. The core idea is to co-locate
  and dynamically manage concurrent training and inference tasks using offline profiling,
  execution prediction, and runtime scheduling to maximize resource utilization while
  meeting response time SLOs.
---

# LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems

## Quick Facts
- arXiv ID: 2507.21276
- Source URL: https://arxiv.org/abs/2507.21276
- Reference count: 40
- Primary result: Co-locates training and inference workloads to improve GPU utilization while maintaining SLOs

## Executive Summary
LeMix introduces a unified scheduling system that co-locates LLM training and inference workloads on shared multi-GPU clusters. The system addresses inefficiencies from dynamic request arrivals and pipeline idleness by using offline profiling, execution prediction, and runtime scheduling to maximize resource utilization while meeting response time SLOs. Evaluations show LeMix improves throughput by up to 3.53×, reduces inference loss by up to 0.61×, and achieves up to 2.12× higher SLO attainment compared to traditional separate setups.

## Method Summary
LeMix builds on vLLM for inference/continuous batching and DeepSpeed for pipeline parallelism. The scheduler uses offline profiling to estimate latency coefficients and memory thresholds, then employs execution planning to forecast idle periods and response times across nodes. Tasks are assigned using a priority scoring mechanism that balances utilization, inference accuracy, and SLO compliance. Runtime memory-aware scheduling prevents memory overruns through wait-or-offload policies. The system handles concurrent workloads by interweaving training tasks into inference idle periods while deprioritizing training when inference SLOs are at risk.

## Key Results
- Throughput improvement of up to 3.53× over separate setups
- SLO attainment improvement of up to 2.12× with priority scoring enabled
- Inference loss reduction of up to 0.61× through intelligent co-location

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Co-locating training and inference workloads increases GPU utilization by filling idle periods inherent to both workload types.
- **Mechanism:** Inference serving exhibits serving idleness during low request rates; pipeline-parallel training exhibits pipeline idleness (bubbles) due to forward-backward dependencies and workload heterogeneity. LEMIX interweaves tasks so training occupies inference gaps and vice versa. The system predicts "usable" vs "unusable" idle periods via forward/backward path forecasting (Algorithm 1).
- **Core assumption:** Request arrivals follow patterns where predictable idle gaps exist and can be filled without violating inference SLOs.
- **Evidence anchors:** [abstract] "co-locating workloads on shared GPUs... maximizes GPU utilization while maintaining serving responsiveness" [§II-C] "inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training" [§III-A] "NAIVE MIX alleviates serving idleness by running training workloads within the arrival gaps"
- **Break condition:** If request patterns become uniformly dense with no idle gaps, or if heterogeneity becomes extreme (σ ≫ μ for query lengths), utilization gains diminish as interference dominates.

### Mechanism 2
- **Claim:** Task-specific priority scoring enables multi-objective optimization of utilization, inference accuracy, and SLO compliance.
- **Mechanism:** LEMIX computes a node priority score f = (IP + λ₂·LC) / (λ₁·R), where IP = idleness profit (penalizes nodes where new task increases idle periods), LC = length consistency (Gaussian similarity to node's historical workload distribution), and R = estimated response time. Tasks are assigned to highest-scoring nodes. Queue-level deprioritization defers training tasks when min(estimated inference response time) exceeds SLO threshold τ_R.
- **Core assumption:** Offline-profiled latency coefficients (η_F, η_B) accurately predict online execution; length consistency correlates with training convergence benefits.
- **Evidence anchors:** [§IV-C] "Tasks are assigned to nodes with the highest priority scores... enabling workload consolidation" [§IV-C, Eq. 3] f = (IP + λ₂·LC) / (λ₁·R) [§VI-F] "removing prioritization causes SLO attainment to collapse... from 95% to below 30% at 100 rps"
- **Break condition:** If profiling coefficients drift significantly from actual runtime (e.g., due to thermal throttling or background processes), priority scores misrank nodes, potentially causing SLO violations.

### Mechanism 3
- **Claim:** Runtime memory-aware scheduling prevents memory overruns while preserving responsiveness via wait-or-offload policy.
- **Mechanism:** Before each forward pass, LEMIX checks if memory utilization < M_threshold. If exceeded, task waits in Δt increments up to T_max. If still constrained, KV cache and intermediate activations are offloaded to CPU. Execution traces calibrate forecasted paths post-completion.
- **Core assumption:** Memory thresholds and T_max from offline profiling bound contention without excessive latency degradation.
- **Evidence anchors:** [§IV-D, Algorithm 2] Lines 6-11: wait loop with offload fallback [§VI-F] "disabling memory awareness causes SLO attainment to drop from 95% to 55% at 100 rps" [§IV-A] M_threshold = κ · M_peak where κ is empirically selected safety factor
- **Break condition:** If offload bandwidth (CPU-GPU) is saturated or T_max set too aggressively, tasks stall or cascade delays across pipeline stages.

## Foundational Learning

- **Concept: Pipeline Parallelism (PP) and Pipeline Bubbles**
  - **Why needed here:** LEMIX exploits pipeline idle periods ("bubbles") from micro-batch scheduling. Without understanding PP's forward-backward stage dependencies, the idleness prediction mechanism is opaque.
  - **Quick check question:** In a 4-stage pipeline with 8 micro-batches, why does the first stage experience idle time after processing its final micro-batch forward pass before backward pass begins?

- **Concept: Continuous Batching in LLM Serving**
  - **Why needed here:** Inference tasks use iteration-level FCFS batching (Algorithm 3). LEMIX's response time prediction requires knowing how prefill requests merge with ongoing decode batches.
  - **Quick check question:** What is the difference between request-level batching and iteration-level continuous batching, and why does the latter reduce tail latency?

- **Concept: SLO (Service Level Objective) Attainment**
  - **Why needed here:** LEMIX's core constraint is maintaining τ_R (e.g., 5× forward latency). Understanding SLO metrics clarifies the deprioritization trigger.
  - **Quick check question:** If TTFT SLO is 500ms and forward latency is 80ms, is an SLO of 5× forward latency achievable under 150 rps with 50% training rate? What system component determines this?

## Architecture Onboarding

- **Component map:**
  [Offline Profiler] → latency coefficients (η_F, η_B), memory thresholds (M_threshold, T_max)
  ↓
  [Global Scheduler] ← receives incoming tasks (arrival time a, length ℓ, batch C)
  ├── Execution Planning (Algorithm 1): computes II, R per node
  ├── Priority Scoring (Eq. 3): f = (IP + λ₂·LC) / (λ₁·R)
  ├── Node Assignment: assigns to argmax(f)
  └── Deprioritization (Eq. 4): checks if training tasks risk SLO breach
  ↓
  [Node-Level Executor] (per node, multi-threaded per stage)
  ├── Trace Queues (Q_train, Q_inference): maintains execution history
  ├── Memory Checker: wait-or-offload before each forward (Algorithm 2)
  └── Stage Threads: execute forward/backward per pipeline dependencies

- **Critical path:**
  1. New task arrives → Execution Planning forecasts II and R for all N nodes (O(N·S) complexity)
  2. Priority scores computed → Task assigned to highest-scoring node
  3. If inference SLO at risk, training tasks deprioritized in global queue
  4. At node, memory check before stage execution → wait or offload KV cache
  5. Post-execution: forecasted paths calibrated against actual traces

- **Design tradeoffs:**
  - λ₁ (response weight) ↑: Lower latency but reduced utilization (fewer tasks consolidated per node)
  - λ₂ (length consistency weight) ↑: Better training convergence but potentially higher fragmentation
  - τ (idleness tolerance) ↑: Higher throughput but risk of SLO violations under burst arrivals
  - κ (memory safety factor) ↑: More stability but underutilized memory capacity

- **Failure signatures:**
  - SLO collapse at high rps: Deprioritization threshold τ_R too aggressive or profiling coefficients stale
  - Memory OOM under burst: M_threshold too high or T_max too short (insufficient wait buffer)
  - Utilization plateau: Length heterogeneity σ ≫ μ causing II overestimation
  - Queue starvation: Training tasks perpetually deprioritized → inference loss plateaus

- **First 3 experiments:**
  1. Baseline calibration: Run SEPARATE vs NAIVE MIX vs LEMIX on single-node 2-GPU setup with synthetic Poisson arrivals (50 rps, 50% training rate) on GPT-400M; measure GPU utilization and TTFT distribution. Confirm NAIVE MIX improves ~8% and LEMIX improves ~22% over NAIVE MIX.
  2. SLO sensitivity sweep: Vary τ_R from 2× to 10× forward latency under 150 rps; plot SLO attainment vs throughput tradeoff curve. Identify "sweet spot" where both metrics >80%.
  3. Memory stress test: Reduce KV cache pool size by 50%, run at 100 rps with 70% training rate; measure offload frequency and TBT degradation. Validate Algorithm 2 prevents OOM without >2× TBT increase.

## Open Questions the Paper Calls Out

- **Question:** Can LeMix maintain its efficiency advantages when scaling to large production clusters with hundreds or thousands of nodes?
- **Basis in paper:** [inferred] The evaluation uses only 4-node clusters, and §V-C notes the cluster-level scheduler handles "< 150 rps," which is far below large-scale deployment requirements.
- **Why unresolved:** The O(N·S) complexity is claimed efficient, but coordination overhead, global queue management, and load balancer integration at massive scale remain untested.
- **What evidence would resolve it:** Experiments on 50+ node clusters with realistic production traffic patterns.

- **Question:** How does the decentralized model synchronization strategy affect long-term model convergence compared to centralized checkpointing?
- **Basis in paper:** [explicit] §V-B mentions nodes "independently update local model instances" similar to federated learning, but no convergence analysis is provided.
- **Why unresolved:** Decentralized updates may cause model drift across nodes, potentially harming serving consistency and final accuracy.
- **What evidence would resolve it:** Comparative study of model accuracy over extended training periods (thousands of iterations) between decentralized and centralized synchronization.

- **Question:** Can the priority weights (λ₁, λ₂) and idleness threshold (τ) be automatically tuned online without manual calibration?
- **Basis in paper:** [inferred] Figure 18 shows trade-offs with different parameter values, but selection requires practitioners to manually balance objectives.
- **Why unresolved:** Optimal parameters likely vary with workload characteristics, making static selection suboptimal under dynamic conditions.
- **What evidence would resolve it:** An adaptive tuning mechanism that adjusts parameters based on real-time feedback while maintaining or improving performance.

## Limitations

- Evaluation focuses on controlled synthetic workloads rather than real-world production traffic patterns
- Offline profiling coefficients may drift under dynamic production conditions without online recalibration
- Memory thresholds and T_max values are fixed and may not adapt well to heterogeneous hardware

## Confidence

- **High Confidence:** The core observation that pipeline-parallel training exhibits predictable idle periods that can be exploited by inference workloads is well-supported by the evaluation results (3.53× throughput improvement). The memory-aware scheduling mechanism preventing OOM is also strongly validated (95% → 55% SLO drop when disabled).
- **Medium Confidence:** The priority scoring mechanism's effectiveness (up to 2.12× SLO improvement) is demonstrated, but the underlying assumption that offline-profiled coefficients generalize to online execution lacks systematic error analysis. The trade-offs between λ₁, λ₂, and τ are discussed qualitatively but not rigorously quantified.
- **Low Confidence:** The claim that length consistency improves training convergence through co-location is asserted but not directly measured. The relationship between deprioritization thresholds and training accuracy is not explored. The evaluation's synthetic Poisson arrivals may not capture bursty or adversarial request patterns common in production.

## Next Checks

1. **Coefficient Drift Validation:** Run LEMIX under gradually shifting workload distributions (e.g., increasing length variance over time) and measure how quickly latency predictions degrade. This would quantify the need for online coefficient recalibration.

2. **Adversarial Pattern Testing:** Design request arrival patterns that deliberately target the scheduler's weaknesses—such as synchronized bursts at pipeline boundaries or length distributions that maximize interference. Measure SLO collapse points compared to baseline.

3. **Training Convergence Study:** Deploy LEMIX with and without co-location on the same training workload, measuring final validation loss. This would directly test whether the claimed quality improvements from length consistency are real or merely artifacts of scheduling decisions.