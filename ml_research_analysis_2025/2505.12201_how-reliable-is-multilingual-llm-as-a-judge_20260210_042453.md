---
ver: rpa2
title: How Reliable is Multilingual LLM-as-a-Judge?
arxiv_id: '2505.12201'
source_url: https://arxiv.org/abs/2505.12201
tags:
- evaluation
- multilingual
- consistency
- llm-as-a-judge
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of multilingual LLM-as-a-Judge
  by assessing its consistency across parallel multilingual data. Five models from
  different model families are evaluated across five diverse tasks involving 25 languages.
---

# How Reliable is Multilingual LLM-as-a-Judge?

## Quick Facts
- arXiv ID: 2505.12201
- Source URL: https://arxiv.org/abs/2505.12201
- Reference count: 15
- Primary result: Multilingual LLM-as-a-Judge achieves only ~0.3 Fleiss' Kappa consistency across 25 languages, with particularly poor performance in low-resource languages.

## Executive Summary
This paper investigates the reliability of using large language models as judges for multilingual content evaluation. The authors assess five different LLM models across five diverse tasks involving 25 languages, finding that these models struggle to achieve consistent judgment results across languages. The study reveals that neither multilingual training data nor model scale directly improves judgment consistency, with low-resource languages showing particularly poor performance. An ensemble strategy is proposed that significantly improves consistency by aggregating judgments from diverse models.

## Method Summary
The study evaluates five LLM models (GPT-3.5-turbo, GPT-4o, Llama-3.3-70b, Qwen-2.5-72b, Aya-Expanse-32b) on five parallel multilingual datasets (XQuAD, MGSM, WMT23, WikiLingua, XDailyDialog) covering 25 languages. The evaluation uses pointwise comparison with both binary (Yes/No) and graded (1-5) outputs. Consistency is measured using Fleiss' Kappa across languages, while accuracy and average grade provide additional quality metrics. An ensemble approach using majority voting from three open-source models is tested as a potential improvement strategy.

## Key Results
- Average Fleiss' Kappa across models and tasks is approximately 0.3, indicating poor consistency
- Low-resource languages show significantly worse consistency than high-resource languages
- Neither multilingual training nor model scale directly improves judgment consistency
- Ensemble strategy significantly improves consistency compared to individual models
- GPT-4o shows highest single-model consistency (~0.54 Kappa on some tasks) but still falls short of reliability

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Proximity and Resource Dominance
LLMs process non-English inputs by mapping them to a representation space dominated by English features. When linguistic distance is high or training data is sparse, the "judgment" function becomes unstable, leading to inconsistent outputs for semantically identical inputs across languages. This resource-based degradation would theoretically diminish if models were trained on perfectly balanced, parallel corpora with enforced representation alignment.

### Mechanism 2: Task Competency Constraints
Consistency is bounded by the model's inherent capability to perform the specific task. An LLM can only judge reliably if it understands the logic of the content. If a model lacks domain capability (e.g., mathematical reasoning), its judgment becomes a surface-level hallucination, varying arbitrarily across languages. This suggests evaluation is a "higher-order" task requiring problem-solving capability first.

### Mechanism 3: Ensemble Variance Reduction
Aggregating judgments from diverse models cancels out idiosyncratic language biases of individual models. Different models have different failure modes, and majority voting increases the probability that the "correct" signal persists while random errors are discarded, provided errors are uncorrelated across models.

## Foundational Learning

- **Concept: Fleiss' Kappa** - The primary metric used to measure consistency across more than two languages. Without understanding it, one cannot interpret the "0.3" reliability score or the improvement deltas in results.
  - Quick check: If three models judge a Spanish sample as "Correct" but the English sample as "Incorrect," would Fleiss' Kappa capture this as a consistency failure?

- **Concept: Pointwise Evaluation** - The paper focuses on "pointwise" (single answer assessment) rather than "pairwise" (comparing two answers) to avoid the complexity of parallel *incorrect* candidates.
  - Quick check: Why does pointwise evaluation make it easier to construct parallel multilingual test sets compared to pairwise evaluation?

- **Concept: High-Resource vs. Low-Resource Languages** - The paper's critical finding is the divergence in reliability based on resource availability. Understanding this hierarchy is essential for prioritizing deployment.
  - Quick check: Would the ensemble strategy be strictly necessary if the target application only supported English, Spanish, and German?

## Architecture Onboarding

- **Component map:** Parallel datasets (XQuAD, MGSM, etc.) -> Template Engine with language placeholder -> Judge Model (LLM) -> Output Aggregation -> Fleiss' Kappa Calculation

- **Critical path:**
  1. Verify parallel data alignment (ground truth must be identical in meaning across languages)
  2. Inject language identifier into the standard English prompt template
  3. Collect `Yes/No` or `Grade` output from the target Judge Model(s)
  4. Compute inter-language agreement (Consistency) rather than just accuracy

- **Design tradeoffs:**
  - GPT-4o vs. Open Source Ensemble: GPT-4o offers highest single-model consistency (~0.54 Kappa) but poses data leakage/cost risks. Ensemble provides "good enough" safety net but requires 3x inference.
  - Yes/No vs. Grade: Binary judgment yields higher consistency than grading because discrete scales reduce decision complexity.

- **Failure signatures:**
  - Strictness Trap: High accuracy but lower consistency indicates the model is applying a stricter rubric than intended
  - Math/Reasoning Drop: Sudden collapse in Kappa scores on MGSM dataset indicates the judge lacks domain capability

- **First 3 experiments:**
  1. Baseline Consistency Check: Run judge on XQuAD for English vs. 1 high-resource (German) and 1 low-resource (Thai) language. Calculate Cohen's Kappa to confirm language resource gap.
  2. Rubric Ablation: Test judge with "General Rubric" vs. "Specific Rubric" on small sample to verify if over-specifying instructions degrades performance.
  3. Ensemble Validation: Implement majority vote for 3 open models on WikiLingua subset. Verify ensemble Kappa > Min(Kappa of individual models).

## Open Questions the Paper Calls Out

- **Open Question 1:** Does utilizing pairwise comparison on parallel multilingual data yield higher consistency than the pointwise evaluation method used in this study? The study focused exclusively on pointwise evaluation because constructing parallel multilingual candidates for pairwise comparison is difficult and data-scarce.

- **Open Question 2:** Do LLMs with parameters significantly exceeding 70 billion demonstrate substantially improved multilingual judgment consistency? Hardware limitations restricted the experimental scope to models at or below 70B, leaving the scaling laws for this specific task unverified for larger architectures.

- **Open Question 3:** Can specific instruction tuning or preference optimization on parallel evaluation data resolve the consistency gap for low-resource languages? The paper identifies the symptom and the failure of general training but does not propose or test a training-based remedy.

## Limitations
- Resource disparity assumptions may oversimplify the relationship between training data volume and cross-linguistic representation alignment
- Task competency causality is based on observed performance drops without direct ablation studies
- Ensemble robustness assumptions untested for models sharing common training biases
- Study limited to models up to 70B parameters due to GPU constraints

## Confidence
- **High Confidence:** Consistency metrics (Fleiss' Kappa values and observed ranges), dataset characteristics and parallel alignment methodology, basic ensemble voting implementation
- **Medium Confidence:** Resource-based consistency degradation mechanism, task competency constraints, ensemble effectiveness in improving consistency
- **Low Confidence:** Specific causal mechanisms linking training data volume to representation misalignment, exact conditions under which ensemble voting fails

## Next Checks
1. Ablation on task-specific fine-tuning: Fine-tune one open-source model on MGSM training data before using it as a judge, then measure consistency improvement on the same task to validate the competency constraint mechanism.

2. Ensemble bias analysis: Analyze error patterns across individual models to quantify correlation of failures across languages, then simulate ensemble voting with artificially correlated errors to determine the threshold at which ensemble benefits disappear.

3. Representation alignment probe: Extract model embeddings for parallel sentences across high-resource and low-resource languages, then measure alignment quality (e.g., via mean average precision or Procrustes distance) to directly test the resource-based representation hypothesis.