---
ver: rpa2
title: Sharpness-Guided Group Relative Policy Optimization via Probability Shaping
arxiv_id: '2511.00066'
source_url: https://arxiv.org/abs/2511.00066
tags:
- arxiv
- grpo
- reasoning
- optimization
- grpo-sg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose Sharpness-Guided GRPO (GRPO-SG), a generalization-oriented
  variant of GRPO that reduces update sharpness in reinforcement learning with verifiable
  rewards (RLVR) by weighting tokens according to model confidence. Specifically,
  GRPO-SG uses a monotone function of token probabilities to downweight low-confidence
  tokens that would otherwise induce sharp, unstable gradients.
---

# Sharpness-Guided Group Relative Policy Optimization via Probability Shaping

## Quick Facts
- arXiv ID: 2511.00066
- Source URL: https://arxiv.org/abs/2511.00066
- Authors: Tue Le; Linh Ngo Van; Trung Le
- Reference count: 40
- One-line result: GRPO-SG improves RLVR reasoning across math, QA, and logic by reducing gradient sharpness through probability-weighted tokens

## Executive Summary
GRPO-SG addresses generalization issues in RLVR by reducing parameter update sharpness through probability-shaped token weighting. The method assigns weights to tokens based on model confidence, downweighting low-confidence tokens that would otherwise induce sharp, unstable gradients. Across math reasoning, agentic QA, and logic puzzles, GRPO-SG consistently outperforms standard GRPO, achieving gains such as 61.5% improvement on K&K logic puzzles and nearly doubling accuracy in agentic QA.

## Method Summary
GRPO-SG modifies GRPO by adding probability-shaped token weights to the surrogate loss. For each token, weight w_{i,t} = clip(α·(σ(sg[π_θ(o_{i,t})]/τ) - μ), L, U) is computed using stop-gradient on token probabilities, then multiplied into the importance ratio before the min/clip operation. This stabilizes gradients by suppressing contributions from low-confidence tokens while preserving critical reasoning signals from high-probability tokens. The method maintains GRPO's group sampling and advantage normalization while adding minimal overhead (~5% training time).

## Key Results
- K&K logic puzzles: Improves average accuracy from 0.39 to 0.63 (61.5% relative gain)
- Agentic QA: Nearly doubles accuracy compared to baseline across NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MuSiQue, Bamboogle
- Gradient norm trajectories show significantly smoother updates, consistent with reduced sharpness and improved generalization

## Why This Works (Mechanism)

### Mechanism 1: Sharpness-Guided Generalization via Gradient Norm Regularization
- Claim: Smaller gradient norms during training correspond to less sharp parameter updates, which improves generalization under the robust generalization bound
- Mechanism: Theorem 3.3 establishes that generalization loss is upper-bounded by empirical loss plus a sharpness surrogate (gradient norm). By explicitly reducing ∥∇θLS(πθ)∥2, GRPO-SG tightens this bound, encouraging convergence to flatter minima that generalize better
- Core assumption: The SAM-style local robustness assumption L_D(θ) ≤ E_{ε∼N(0,ρ)} L_D(θ+ε) holds for RLVR losses, and the first-order approximation in Eq. (15) is valid
- Evidence anchors:
  - [abstract] "where the generalization loss is upper bounded by a combination of the empirical loss and a sharpness surrogate measured by the gradient norm"
  - [Section 3.1] Theorem 3.3 and Eq. (15)-(16) provide the formal bound and first-order expansion
  - [corpus] Limited direct corpus support; related GRPO variants (NGRPO, DARO) address different issues like negative sampling and difficulty-aware reweighting, not sharpness explicitly

### Mechanism 2: Probability-Shaped Token Weighting Stabilizes Per-Token Gradients
- Claim: Weighting tokens by a monotone function of model probability stabilizes the product w_{i,t}(1-π_θ(o_{i,t})), suppressing extreme gradients from low-confidence tokens
- Mechanism: Low-probability tokens have large (1-π_θ) but receive small weights w; high-probability tokens have small (1-π_θ) but larger weights. The product w_{i,t}(1-π_θ(o_{i,t})) is bounded by Theorem 3.5-3.6, preventing any single token from dominating gradient magnitude
- Core assumption: High-probability tokens carry more critical reasoning signal (operators, brackets, logical connectors), while low-probability tokens are more noisy/replaceable
- Evidence anchors:
  - [abstract] "assigning each token a weight based on a monotone function of the model's own token probability, thereby downweighting tokens that would otherwise induce sharp, unstable gradients"
  - [Section 3.2, Eq. (19)] Defines w_{i,t} = clip(α·(σ(sg[π_θ]/τ) - μ), L, U) with stop-gradient
  - [Appendix C.3, Figure 3] Word cloud analysis shows high-probability tokens are structural/mathematical operators; low-probability tokens are generic content words

### Mechanism 3: Stop-Gradient Prevents Reward Hacking of Weights
- Claim: Applying stop-gradient to token weights prevents the policy from artificially inflating weights on easy tokens to reduce loss without improving reasoning
- Mechanism: The sg[·] operator in Eq. (19) treats π_θ(o_{i,t}) as a constant during backpropagation. Weights influence gradients but are not themselves optimized, breaking the potential loop where the model manipulates its own confidence scores
- Core assumption: The model could otherwise learn to game the weighting scheme by increasing π on high-weight tokens without genuine capability improvement
- Evidence anchors:
  - [Section 3.2] "stop-gradient to avoid the model gaming the weights"
  - [corpus] No direct corpus evidence on this specific mechanism; it's a design choice inferred from reward hacking literature

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO-SG is a modification of GRPO; understanding the baseline is essential. GRPO estimates advantages by normalizing rewards within groups of sampled responses, removing the need for a separate value function
  - Quick check question: Can you explain why GRPO uses group-normalized advantages instead of learning a value network?

- Concept: **Sharpness-Aware Minimization (SAM) intuition**
  - Why needed here: The generalization bound in Theorem 3.3 is adapted from SAM. Understanding that flatter minima generalize better provides the theoretical motivation
  - Quick check question: In SAM, why does optimizing max_{∥θ'-θ∥≤ρ} L_S(θ') encourage flat minima?

- Concept: **Policy gradient with importance sampling ratios**
  - Why needed here: The token weighting modifies the standard importance ratio r_{i,t}(θ) = π_θ/π_old in the clipped surrogate objective. Understanding PPO-style clipping is prerequisite
  - Quick check question: What role does the clipping function play in PPO/GRPO, and how does GRPO-SG's w_{i,t} factor interact with it?

## Architecture Onboarding

- Component map:
  - Token probability extraction → Sigmoid transform with temperature scaling → Shift and clip → Stop-gradient application → Weight multiplication with importance ratio

- Critical path:
  1. Forward pass generates token probabilities π_θ(o_{i,t}) for all sampled responses
  2. Stop-gradient on probabilities before weight computation
  3. Compute w_{i,t} = clip(α·(σ(π_θ/τ) - μ), L, U)
  4. Modify surrogate: min(w_{i,t} · r_{i,t} · Â_{i,t}, clip(...)) instead of min(r_{i,t} · Â_{i,t}, clip(...))
  5. Backward pass with weighted gradients

- Design tradeoffs:
  - τ (temperature): Lower values create sharper weight disparities; higher values flatten weights toward uniform. Paper finds τ≈9.0 works best—moderate discrimination
  - Clipping bounds (L, U): Wider ranges allow stronger reweighting but risk instability; (1.0, 1.4) is conservative and works across settings
  - Overhead: ~5% training time increase from weight computation (Table 4), negligible memory overhead

- Failure signatures:
  - Gradient norm trajectory shows increasing spikes despite weighting → check if stop-gradient is correctly applied
  - Performance degrades vs. baseline on high-difficulty tasks → τ may be too low, over-suppressing rare but important tokens
  - Weights converge to uniform → τ too high or α/μ misconfigured

- First 3 experiments:
  1. Reproduce K&K logic puzzle results with Qwen2.5-3B-Instruct using hyperparameters from Table 6 and (α=2.0, μ=0.25, L=1.0, U=1.4, τ=9.0). Verify gradient norm smoothness matches Figure 1
  2. Ablate τ: sweep [0.5, 2.0, 7.0, 9.0, 10.0, 20.0] on a held-out subset. Confirm performance peaks near τ=9 as in Table 7
  3. Ablate stop-gradient: run identical setup without sg[·] on token probabilities. Monitor for weight manipulation artifacts (unusual probability inflation) and generalization gap

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope of validation: Method tested only on binary or exact-match reward tasks; effectiveness on continuous or structured reward settings remains unknown
- Theoretical assumptions not fully validated: Local robustness assumption and first-order approximation stated but not empirically verified in RLVR context
- Potential overfitting to specific model families: Results shown primarily with Qwen models; generalization to other architectures untested

## Confidence
- Theoretical framework validity: Medium confidence - Theorem 3.3-3.6 provide mathematical foundation but assumptions not empirically validated
- Empirical results: High confidence - consistent improvements across three distinct reasoning domains with supporting ablation studies
- Cross-model generalization: Medium confidence - results primarily shown with Qwen models, untested on other architectures
- Reproducibility: Low confidence - critical implementation details and hyperparameter tuning procedures not fully specified

## Next Checks
1. **Cross-architecture validation**: Implement GRPO-SG on a different LLM family (e.g., Llama 3.1 8B) using the same three task domains. Compare gradient-norm trajectories and accuracy gains to verify the method generalizes beyond Qwen models.

2. **Robustness to theoretical assumptions**: For a subset of training runs, compute the actual generalization gap (validation loss - training loss) and correlate it with gradient norm and sharpness metrics. This would empirically test whether Theorem 3.3's bound meaningfully predicts generalization behavior in RLVR.

3. **Ablation of probability shaping**: Run a controlled experiment replacing the probability-based weighting with an alternative weighting scheme (e.g., uniform weights or reward-based weights) while keeping all other GRPO-SG components constant. This would isolate the contribution of the probability shaping mechanism itself versus other factors in GRPO-SG's success.