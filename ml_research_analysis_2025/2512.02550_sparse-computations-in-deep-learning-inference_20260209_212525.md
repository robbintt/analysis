---
ver: rpa2
title: Sparse Computations in Deep Learning Inference
arxiv_id: '2512.02550'
source_url: https://arxiv.org/abs/2512.02550
tags:
- sparse
- sparsity
- performance
- arxiv
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively explores sparsity in deep learning
  inference, focusing on structured and unstructured weight pruning, activation sparsity,
  and sparse attention. It bridges the knowledge gap between domain experts and performance
  engineers by detailing how dense computations transform into sparse kernels (SpMM,
  SDDMM, SpMV, sparse convolution) and evaluating their performance on CPUs and GPUs.
---

# Sparse Computations in Deep Learning Inference

## Quick Facts
- arXiv ID: 2512.02550
- Source URL: https://arxiv.org/abs/2512.02550
- Reference count: 40
- This survey comprehensively explores sparsity in deep learning inference, focusing on structured and unstructured weight pruning, activation sparsity, and sparse attention.

## Executive Summary
This survey bridges the knowledge gap between domain experts and performance engineers by detailing how dense DNN computations transform into sparse kernels (SpMM, SDDMM, SpMV, sparse convolution) and evaluating their performance on CPUs and GPUs. Despite significant research, sparse kernels exhibit low efficiency—typically under 3% of peak GPU performance—due to memory bottlenecks and irregular access patterns. While semi-structured N:M sparsity shows promise with hardware support (e.g., NVIDIA Tensor Cores), unstructured sparsity remains challenging for deployment. The work highlights critical needs: better datasets reflecting modern architectures, standardized benchmarks, and integrated frameworks to make sparse DNN inference practical at scale.

## Method Summary
The survey conducted comprehensive benchmarking of sparse matrix kernels across multiple implementations (cuSPARSE, ASpT, RoDe, dgSPARSE, Sputnik, MKL) on A100 GPU and EPYC 7402 CPU using SuiteSparse graphs and DLMC matrices. Evaluation involved running 128 iterations per kernel-dataset-format configuration, averaging over 3 runs, and excluding preprocessing. The study examined performance effects across sparsity levels (50-99%), embedding dimensions (32, 128, 1024), and matrix formats (CSR, COO, N:M compressed). Additionally, the survey analyzed activation sparsity patterns in transformers and evaluated attention mask sparsity in modern architectures.

## Key Results
- Sparse kernels achieve typically under 3% of peak GPU performance due to memory bottlenecks and irregular access patterns
- Semi-structured N:M sparsity (2:4) shows promise with hardware acceleration on NVIDIA Tensor Cores (~2× speedup)
- Unstructured sparsity remains challenging for deployment despite research advances, with throughput inversely correlated with sparsity (12× drop from 50% to 99%)

## Why This Works (Mechanism)

### Mechanism 1: Weight Pruning → Sparse Matrix Kernels
Pruning zeros individual weights based on magnitude or learned importance, transforming dense GEMM into sparse kernels (SpMM/SpMV) with irregular memory access. The zeroed weights contribute minimally to model output accuracy after fine-tuning, enabling compressed storage formats (CSR, COO) and sparse computation.

### Mechanism 2: Semi-Structured N:M Sparsity → Hardware-Aware Compression
Constraining zeros to N:M patterns (e.g., 2:4) creates predictable access patterns enabling Sparse Tensor Core acceleration with ~2× throughput gains. Exactly N nonzeros per M-element group allows hardware to decode 2-bit metadata per group, skipping zero multiplications without software overhead.

### Mechanism 3: Activation Sparsity → Dynamic Zero-Skipping
ReLU-family activations inherently zero ~50-90% of neuron outputs, creating input sparsity for downstream layers. These zeros propagate to subsequent layers' inputs, enabling sparse convolution. The challenge is that zero patterns are input-dependent and generated at inference time, requiring lightweight on-the-fly encoding.

## Foundational Learning

- **Sparse Matrix Formats (CSR, COO, N:M compressed)**: Understanding storage overhead and access patterns is essential for kernel selection. CSR stores row pointers + column indices + values; N:M formats store 2-bit indices per group. Quick check: Given a 1000×1000 matrix with 95% sparsity, estimate CSR storage vs. dense storage overhead.

- **Core DNN Layers → Dominant Operations**: Mapping architectures to kernels informs where sparsity applies. FC layers → GEMM/SpMM; RNNs/LSTMs → GEMV/SpMV; Transformers → attention (SDDMM + SpMM). Quick check: For a Transformer decoder with batch=1 autoregressive generation, which sparse kernel dominates?

- **Memory-Bound vs. Compute-Bound Kernels**: Sparse kernels are memory-bound due to irregular access; arithmetic intensity is low. Peak performance (18 TFLOPs for A100 GEMM) is rarely achieved (<3% typical). Quick check: Why does increasing embedding dimension K improve SpMM throughput while extreme sparsity degrades it?

## Architecture Onboarding

- **Component map**: Sparsification (pruning, N:M, structured) → Sparse formats (CSR, BSR, N:M compressed) → Kernels (SpMM, SDDMM, SpMV, Sparse Conv) → Hardware (Dense Tensor Cores, Sparse Tensor Cores, CUDA cores) → Libraries (cuSPARSE, MKL, TACO, research kernels)

- **Critical path**: 1. Identify sparsity source (weight pruning, activation, attention) 2. Select format matching hardware (unstructured → CSR; N:M → compressed; block → BSR) 3. Choose kernel implementation (vendor library vs. research kernel) 4. Validate correctness + measure throughput vs. dense baseline

- **Design tradeoffs**: Unstructured sparsity offers highest accuracy retention but lowest hardware efficiency (~2-3% peak). N:M sparsity provides hardware-accelerated performance but limited to 50% compression (2:4) and requires compatible GPU. Structured sparsity enables easiest deployment but highest accuracy loss per pruned parameter.

- **Failure signatures**: Throughput inversely correlated with sparsity (12× drop from 50% → 99%). Kernel compilation succeeds but produces incorrect results (common in research implementations). LLC overflow on CPU causes 6× performance cliff. N:M model deployed on incompatible hardware receives no acceleration.

- **First 3 experiments**: 1. Benchmark cuSPARSE SpMM vs. dense GEMM on a pruned ResNet-50 layer from DLMC; measure GFLOPs and compare to theoretical peak. 2. Convert a 2:4 pruned BERT layer to NVIDIA's ASP format; measure TensorRT-LLM throughput vs. dense baseline. 3. Profile ReLU activation sparsity across layers in a small transformer; estimate potential SpMM savings if sparse input format overhead were zero.

## Open Questions the Paper Calls Out

### Open Question 1
How can the research community generate or standardize sparse datasets that reflect the architecture-specific characteristics (e.g., GQA, MLA) and massive scales of modern LLMs like Llama 3 or Qwen? Current DLMC datasets rely on outdated models (2019-2020) and fail to capture the weight structures and execution flows of modern models with features like Grouped-Query Attention or compressed latent KV caches.

### Open Question 2
What methodologies or frameworks are required to deploy unstructured sparsity with production-grade quality and ease of use? While semi-structured (N:M) sparsity has solid software and hardware support, an end-to-end framework for deploying unstructured sparse DNNs with production quality is currently missing. Existing unstructured kernels are highly complex, often unstable, and difficult to integrate into standard deep learning frameworks.

### Open Question 3
How can researchers isolate and benchmark learnable attention masks and activation sparsity patterns without requiring complex, end-to-end network integration? Datasets to support experimentation with learnable sparse attention are missing, and evaluating activation sparsity currently requires incorporating the approach into an end-to-end network—a workflow familiar to domain experts but less so to performance engineers.

## Limitations
- Memory bandwidth constraints limit SpMM efficiency to under 3% of peak GPU performance regardless of kernel implementation
- Research landscape suffers from fragmentation with 12 different sparse kernels and no unified benchmark suite
- Reliance on CUDA-specific implementations creates vendor lock-in and CPU sparse kernels show dramatic LLC overflow sensitivity

## Confidence
- **High Confidence**: Claims about N:M sparsity hardware acceleration (2:4 pattern, ~2× speedup on Ampere+) are well-documented by NVIDIA specifications and confirmed across multiple benchmarks
- **Medium Confidence**: Activation sparsity potential is theoretically sound but practical gains remain unproven due to runtime overhead in sparse format generation and lack of CPU/GPU hardware support
- **Low Confidence**: Unstructured sparsity performance numbers vary significantly across implementations due to differences in format conversion pipelines, memory allocation strategies, and kernel launch configurations

## Next Checks
1. Measure arithmetic intensity (FLOPs/byte) and memory bandwidth utilization for SpMM kernels across sparsity levels 50-99% on A100 GPU to confirm <3% peak performance is due to memory bandwidth saturation
2. Profile end-to-end inference latency for a small transformer with ReLU activations, comparing sparse input format generation overhead against theoretical compute savings from skipping zero multiplications
3. Test 2:4 pruned BERT layers on both NVIDIA Ampere GPU (with Sparse Tensor Cores) and AMD RDNA 4 GPU (with sparse compute units) to quantify hardware-specific acceleration vs. storage-only benefits