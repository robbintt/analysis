---
ver: rpa2
title: 'Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding
  Denoising Auto Encoder Transformer'
arxiv_id: '2509.17165'
source_url: https://arxiv.org/abs/2509.17165
tags:
- data
- time
- series
- forecasting
- charging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid deep learning model combining Bi-LSTM,
  denoising autoencoder, and transformer architectures for short-term EV charging
  load forecasting. The model addresses the challenge of accurately predicting electric
  vehicle charging demand, which is crucial for grid stability and energy management.
---

# Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer

## Quick Facts
- arXiv ID: 2509.17165
- Source URL: https://arxiv.org/abs/2509.17165
- Reference count: 33
- Primary result: Hybrid BDT model outperforms Transformer, RNN, LSTM, CNN, and GRU benchmarks in 80% of time horizons tested for EV charging load forecasting

## Executive Summary
This study presents a hybrid deep learning model combining Bi-LSTM, denoising autoencoder, and transformer architectures for short-term EV charging load forecasting. The model addresses the challenge of accurately predicting electric vehicle charging demand, which is crucial for grid stability and energy management. The Bi-LSTM component extracts bidirectional temporal features, the denoising autoencoder filters input noise and errors, and the transformer captures complex dependencies in the data. Performance evaluation on three years of residential EV charging data shows the proposed BDT model outperforms benchmark models including Transformer, RNN, LSTM, CNN, and GRU in 80% of time horizons tested.

## Method Summary
The proposed BDT model operates in three stages: (1) Bi-LSTM embedding processes sequences bidirectionally and concatenates hidden states, (2) Denoising Autoencoder (DAE) corrupts embedded input, encodes, and decodes to reconstruct clean data, and (3) Transformer encoder applies multi-head self-attention to denoised representations. The model uses Norwegian residential EV charging data (Dec 2018–Jan 2020) with 6,878 charging sessions from 1,113 apartments, split 80/10/10 for train/validation/test. Training uses MSE loss while evaluation employs RMSE and MAE metrics. Hyperparameter ranges include layers {1,3,6}, epochs {10,50,100}, heads {1,8}, and model dimension {32,64} with manual search optimization.

## Key Results
- BDT model outperforms benchmark models in 80% of tested time horizons
- For 48-120 hour horizons, achieved RMSE values between 0.092-0.120 and MAE values between 0.066-0.089
- Demonstrated MAE reduction of up to 56.1% compared to the second-best model
- Model underperformed at 24-hour horizon compared to simpler Transformer, GRU, and LSTM models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bidirectional temporal embedding creates richer input representations than unidirectional approaches for EV charging patterns.
- **Mechanism:** Two LSTM units process sequences in opposite directions (forward: t₁→tₙ, backward: tₙ→t₁). Hidden states from both directions are concatenated (h_t = [h_t_forward, h_t_backward]), then projected through a fully connected layer. This produces embeddings that encode both historical context and future temporal patterns at each timestep.
- **Core assumption:** EV charging load exhibits bidirectional dependencies where future context (e.g., approaching weekend) informs current predictions.
- **Evidence anchors:** Abstract states "Bi-LSTM component extracts bidirectional temporal features"; section 3.A describes "two LSTM units that work in both directions to incorporate past and future information"; corpus provides weak support—neighbor papers mention LSTM for load forecasting but don't validate bidirectional advantage specifically for EV data.
- **Break condition:** If EV charging patterns are predominantly unidirectional or dataset is too short to capture meaningful bidirectional context, this degrades to standard LSTM performance.

### Mechanism 2
- **Claim:** Denoising autoencoder preprocessing improves prediction by learning to reconstruct clean representations from corrupted inputs.
- **Mechanism:** Clean embeddings X_em are corrupted (X̃_em = f_c(X_em)) using Gaussian noise or element zeroing. The encoder compresses to latent h, then decoder reconstructs X̂_em. Training minimizes reconstruction loss L = MSE(X_em, X̂_em), forcing latent space to retain only robust signal-bearing features.
- **Core assumption:** Input EV charging data contains separable noise/artifacts that can be filtered through reconstruction learning.
- **Evidence anchors:** Abstract states "denoising autoencoder filters input noise and errors"; section 3.C explains "By comparing the reconstructed data to the original data, the denoising autoencoder can identify which parts of the input data are important"; corpus provides no direct validation—corpus papers don't specifically address DAE for EV load forecasting.
- **Break condition:** If noise is systematic rather than random, or data is already clean, DAE may overfit or provide no filtering benefit.

### Mechanism 3
- **Claim:** Transformer self-attention captures long-range temporal dependencies that sequential models miss.
- **Mechanism:** Denoised embeddings X̂_em are projected to Query/Key/Value matrices. Attention computes A = softmax(QK^T/√d_k)V, creating weighted connections across all timesteps simultaneously. Multi-head attention runs parallel attention operations, enabling detection of patterns at different temporal scales.
- **Core assumption:** EV charging contains long-range dependencies (weekly cycles, seasonal patterns) requiring direct temporal access beyond sequential processing.
- **Evidence anchors:** Abstract states "transformer captures complex dependencies in the data"; section 3.B notes "transformers are able to access any point in the past without suffering from vanishing gradients"; corpus validates transformer effectiveness through Patchformer paper for long-term multi-energy load forecasting.
- **Break condition:** For short horizons (≤24h) with primarily local patterns, transformer complexity may not justify use—the paper shows Transformer alone outperformed BDT at 24-hour horizon.

## Foundational Learning

- **Concept: Bidirectional Recurrent Networks**
  - Why needed here: Understanding how forward/backward information flow combines is essential for debugging embedding quality
  - Quick check question: Given sequence [Mon, Tue, Wed, Thu, Fri], what does backward LSTM contribute when processing Wednesday?

- **Concept: Denoising Autoencoder Reconstruction Loss**
  - Why needed here: The noise-filtering mechanism operates through reconstruction; understanding this helps diagnose when filtering fails
  - Quick check question: Why does corrupting input and training to reconstruct clean data produce better representations than training on clean input directly?

- **Concept: Self-Attention Query-Key-Value Mechanism**
  - Why needed here: The transformer's ability to capture long-range dependencies depends on understanding Q/K/V projections
  - Quick check question: How does computing attention scores between all timestep pairs differ from sequential RNN processing for capturing a 96-hour pattern?

## Architecture Onboarding

- **Component map:** Input [X, timestamps] → Bi-LSTM Embedding (forward+backward → concat → FC) → Denoising AE (corrupt → encode → decode) → Transformer (Q,K,V → multi-head attention) → Output [Y predicted]

- **Critical path:** Bi-LSTM creates X_em → DAE encoder produces latent h → DAE decoder outputs X̂_em → Transformer attention computes predictions. X̂_em quality directly determines downstream attention effectiveness.

- **Design tradeoffs:**
  - Embedding dimension: Larger captures complexity but risks overfitting on limited EV data
  - Corruption level: Higher forces robustness but may destroy signal
  - Attention heads: More heads capture diverse patterns but increase parameters/training time

- **Failure signatures:**
  - 24-hour underperformance vs. simpler models: Model over-tuned for long-range patterns
  - High RMSE/MAE variance across runs: Training instability or hyperparameter sensitivity
  - DAE reconstruction loss plateauing high: Noise filtering not learning meaningful representations

- **First 3 experiments:**
  1. **Ablation test:** Run Bi-LSTM→Transformer (skip DAE) and DAE→Transformer (skip Bi-LSTM) to isolate each component's contribution
  2. **Horizon boundary detection:** Test 12h, 36h, 48h, 60h to pinpoint where BDT begins outperforming baselines
  3. **Corruption sensitivity:** Vary DAE noise level (σ=0.1, 0.3, 0.5 Gaussian) to find optimal filtering threshold for your EV dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the BDT architecture be adapted to improve predictive accuracy for short-term (24-hour) forecasting horizons?
- Basis in paper: The results in Table 2 show the proposed model underperforms compared to Transformer, GRU, and LSTM models at the 24-hour horizon, despite excelling at longer time steps.
- Why unresolved: The authors attribute the success at longer horizons (48–120h) to the model's ability to capture long-term dependencies, but they do not identify a specific mechanism to fix the poorer short-term performance.
- Evidence: Modifications to the model that result in statistically significant RMSE/MAE reductions at the 24-hour horizon, bringing it in line with or below the benchmark models.

### Open Question 2
- Question: Does the proposed hybrid method maintain its performance advantages when applied to forecasting different energy sources or contexts?
- Basis in paper: The Conclusion states that "examining the proposed model's effectiveness in different contexts such as energy sources can be explored."
- Why unresolved: The current study validates the model exclusively on a dataset of residential EV charging in Norwegian apartment buildings.
- Evidence: Application of the BDT model to distinct domains (e.g., solar or wind generation) showing consistent superiority over standard benchmarks like CNN and RNN.

### Open Question 3
- Question: What impact do additional input parameters, such as meteorological or calendar data, have on the model's forecasting error rates?
- Basis in paper: The authors suggest "expanding the dataset in terms of quantity and input parameters" as a direction for future research.
- Why unresolved: The current implementation relies on historical load data and timestamps without incorporating exogenous variables that might influence charging behavior.
- Evidence: A comparative analysis demonstrating error reduction (RMSE/MAE) when exogenous features are introduced versus the current univariate approach.

## Limitations

- Model architecture combines three complex components without ablation studies to isolate individual contributions
- Dataset comes from a single Norwegian apartment building with only 1,113 units, limiting generalizability
- Hyperparameter selection relied on manual search rather than systematic optimization
- Computational requirements for the full BDT model are not reported

## Confidence

- **High confidence**: Overall framework architecture is clearly described and comparative performance results against multiple baselines are well-documented with appropriate metrics (RMSE, MAE) and statistical reporting (mean ± std over 5 runs)
- **Medium confidence**: Bidirectional embedding mechanism and transformer self-attention components are well-established techniques, but specific implementation details and hyperparameters for this application are not fully specified, limiting exact reproduction
- **Low confidence**: Denoising autoencoder's contribution is theoretically justified but lacks empirical validation through ablation studies or comparison with alternative noise filtering approaches

## Next Checks

1. **Ablation Study**: Implement and test three reduced variants—Bi-LSTM → Transformer (skip DAE), DAE → Transformer (skip Bi-LSTM), and Transformer alone—to quantify each component's marginal contribution to the performance gains claimed.

2. **Dataset Generalization**: Apply the trained BDT model to a different EV charging dataset (e.g., workplace charging, public charging stations, or data from another geographic region) to assess whether the performance improvements transfer beyond the original Norwegian residential context.

3. **Computational Efficiency Analysis**: Measure inference time, memory usage, and training duration for the full BDT model versus the best-performing baseline (Transformer) across all forecasting horizons to evaluate whether the performance gains justify the increased complexity.