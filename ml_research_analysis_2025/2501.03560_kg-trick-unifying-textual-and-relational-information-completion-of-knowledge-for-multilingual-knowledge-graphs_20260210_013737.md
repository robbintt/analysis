---
ver: rpa2
title: 'KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge
  for Multilingual Knowledge Graphs'
arxiv_id: '2501.03560'
source_url: https://arxiv.org/abs/2501.03560
tags:
- entity
- information
- entities
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KG-TRICK, a unified framework for completing
  textual and relational information in multilingual knowledge graphs (KGs). The key
  idea is to reformulate both knowledge graph completion (KGC) and knowledge graph
  enhancement (KGE) as a single multilingual text-to-text generation task, enabling
  models to leverage complementary information across languages.
---

# KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs

## Quick Facts
- arXiv ID: 2501.03560
- Source URL: https://arxiv.org/abs/2501.03560
- Reference count: 24
- Primary result: Unified framework achieving strong KGC (hit@1: 36.6) and KGE (precision: 52.2) metrics across 10 languages

## Executive Summary
KG-TRICK introduces a unified framework that reformulates both Knowledge Graph Completion (KGC) and Knowledge Graph Enhancement (KGE) as a single multilingual text-to-text generation task. The approach leverages mBART's cross-lingual capabilities to predict missing relations and textual information simultaneously, treating KG completion as a constrained generation problem. The authors also introduce WikiKGE-10++, a manually-curated benchmark with over 25,000 entities across 10 languages for evaluating KGE systems.

## Method Summary
The framework verbalizes KG triplets into text prompts using the format (source_lang, target_lang, head, relation, ?) and trains mBART-large-50 to generate tail entity text. For KGC, the model predicts relation triples; for KGE, it predicts missing names/descriptions. The training uses a 50-50 mix of KGC and KGE data from Wikidata, with entity descriptions incorporated to improve disambiguation. Inference involves generating text in multiple target languages and ensemble voting to map back to KG entity IDs.

## Key Results
- KG-TRICK outperforms similarly-sized state-of-the-art models on both KGC (hit@1: 36.6, hit@3: 40.4, hit@10: 42.6) and KGE (precision: 52.2, coverage: 31.5) metrics
- Unified training of KGC and KGE leads to mutual improvements compared to training tasks in isolation
- Joint multilingual training benefits all languages, outperforming bilingual baselines
- Achieves competitive performance compared to larger language models while using only 0.6B parameters

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Mutual Regularization
If KGC and KGE are trained jointly, shared parameters learn representations that improve both tasks. Relational constraints help generate precise text, while textual context resolves relational ambiguity. Core assumption: informational gaps in relations and text are semantically correlated. Break condition: performance degrades with heavy data imbalance (150M KGC vs 16M KGE).

### Mechanism 2: Verbalized Cross-Lingual Transfer
Verbalizing entities and relations enables mBART to transfer knowledge from high-resource to low-resource languages more effectively than ID-based models. Core assumption: pre-trained model has sufficient cross-lingual alignment. Break condition: fails for languages poorly supported in mBART vocabulary or for rare entity names.

### Mechanism 3: Description-Based Entity Disambiguation
Including entity descriptions in input/output helps resolve ambiguous entities (e.g., "Paris" city vs person). Attention mechanism distinguishes entities with identical labels but different relational neighborhoods. Core assumption: high-quality descriptions are available. Break condition: degrades with missing, noisy, or generic descriptions.

## Foundational Learning

- **Sequence-to-Sequence (Seq2Seq) Generation**: Reformulating graph problems into text generation problems using Encoder-Decoder architectures. *Why needed*: Core to understanding how KG-TRICK generates tail entities from text prompts. *Quick check*: How does the loss function differ between classification-based KGC and generation-based KG-TRICK?

- **Knowledge Graph Triplets (h, r, t)**: Core unit of processing where (h, r, t) are verbalized into prompts. *Why needed*: Understanding the distinction between structural IDs and textual labels is crucial. *Quick check*: For input `(en, es, "Apple", "instance_of", ?)`, what does the model generate and in which language?

- **Cross-Lingual Transfer**: Leveraging English data to improve non-English completion through shared parameters. *Why needed*: Key to understanding the "multilingual by design" aspect. *Quick check*: Why might performance be worse on low-resource language tail entities compared to English head entities?

## Architecture Onboarding

- **Component map**: Verbalizer -> mBART-large-50 -> Ensemble Module
- **Critical path**: Data prep (verbalize Wikidata) → Input formatting (5-element tuple) → Fine-tuning (joint KGC/KGE) → Inference (generate text → entity linking)
- **Design tradeoffs**: Data balancing (50-50 sweet spot identified), closed vs open world assumption (closed world limits novel entity discovery)
- **Failure signatures**: Low Hit@10 (sampling capacity constraint), hallucination (incorrect generation for sparse tail entities)
- **First 3 experiments**: 1) Ablate descriptions to measure disambiguation value, 2) Compare monolingual vs multilingual training for cross-lingual transfer, 3) Vary KGC/KGE mixing ratio to find stability range

## Open Questions the Paper Calls Out

1. **Open-world extension**: Can KG-TRICK be extended to predict entities not in the KG? The current closed-world assumption ignores novel entity predictions.

2. **Graph context integration**: How to integrate explicit subgraph structural context into the seq2seq input? The current 1-hop approach may miss structural reasoning benefits.

3. **Decoding strategy optimization**: How to increase candidate diversity and improve Hit@10 performance? Generative models produce limited candidates versus embedding-based ranking.

## Limitations

- Closed-world assumption limits discovery of truly novel entities
- Performance sensitive to training data balance (150M KGC vs 16M KGE)
- Relies on high-quality descriptions for entity disambiguation
- Scalability to truly low-resource languages remains unproven

## Confidence

**High Confidence**: Core architectural innovation and mutual improvement claims are well-supported by experimental results.

**Medium Confidence**: Description-based disambiguation benefits have some support but lack rigorous ablation studies.

**Low Confidence**: Claims about fundamentally changing multilingual KG completion landscape are overstated given closed-world constraints.

## Next Checks

1. **Description ablation study**: Retrain using only entity names to quantify disambiguation contribution.

2. **Open-world extension**: Modify inference to handle novel entity generation and evaluate on genuinely new entities.

3. **Low-resource language stress test**: Systematically evaluate performance degradation across languages with varying mBART pre-training data.