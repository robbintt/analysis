---
ver: rpa2
title: Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning
arxiv_id: '2505.13628'
source_url: https://arxiv.org/abs/2505.13628
tags:
- languages
- alignment
- language
- multilingual
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of aligning multilingual sentence
  representations without requiring parallel text data, which is expensive and time-consuming
  to obtain for low-resource languages. The authors propose using visual information
  as a shared modality to align text representations across languages by fine-tuning
  multilingual encoders using image-caption pairs in multiple languages.
---

# Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning

## Quick Facts
- arXiv ID: 2505.13628
- Source URL: https://arxiv.org/abs/2505.13628
- Authors: Nathaniel Krasner; Nicholas Lanuzo; Antonios Anastasopoulos
- Reference count: 14
- Primary result: Multilingual image-caption alignment achieves 55.7% bitext retrieval accuracy vs 18.3% for English-only and 62.2% for text-text alignment, enabling cross-lingual transfer without parallel text data.

## Executive Summary
This work addresses the challenge of aligning multilingual sentence representations without requiring parallel text data, which is expensive and time-consuming to obtain for low-resource languages. The authors propose using visual information as a shared modality to align text representations across languages by fine-tuning multilingual encoders using image-caption pairs in multiple languages. The core method involves training a multilingual text encoder jointly with an image encoder using contrastive learning on image-caption pairs from English, Spanish, Japanese, Hindi, and Quechua. The primary results show that multilingual text-image alignment produces representations that perform well on bitext retrieval tasks and enable effective cross-lingual transfer for low-resource languages without requiring parallel text data.

## Method Summary
The method employs a multilingual text encoder (XLM-Roberta-Large) and a vision encoder (ViT-Base-patch16-224-in21k) that are jointly fine-tuned using contrastive learning on image-caption pairs. The text encoder's output is mean-pooled and projected to 512 dimensions, as is the image encoder's output. A contrastive loss (InfoNCE) is applied between matching image-caption pairs across five languages: English, Spanish, Japanese, Hindi, and Quechua. The training follows a two-phase approach: first, only projection layers are trained ("warm-up") until loss stabilizes, then encoders are unfrozen and joint fine-tuning continues. This approach implicitly aligns text representations across languages through their shared connection to visual representations.

## Key Results
- Multilingual image-caption alignment achieves 55.7% bitext retrieval accuracy vs 18.3% for English-only training and 62.2% for text-text alignment
- Adding Quechua captions improves Quechua bitext retrieval from 18% to 29.2% while maintaining performance on other languages
- Downstream NLI task performance is preserved and improved with this alignment approach (XNLI accuracy increases from 43.8% to 51.3-51.6%)
- Languages unseen during pretraining (Quechua) can be incorporated into the aligned representation space using only monolingual image-caption data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual text-to-image contrastive alignment implicitly creates text-to-text cross-lingual alignment.
- Mechanism: When encoders for multiple languages are trained to map captions to the same image representations, their output spaces converge. Images function as a shared semantic anchor—a "pivot modality"—so minimizing text-image distance for each language indirectly minimizes inter-language distance for semantically equivalent sentences.
- Core assumption: The visual encoder provides a sufficiently stable and semantically rich target space that different languages can map to without collapsing distinct meanings.
- Evidence anchors:
  - [abstract]: "We find that multilingual image-caption alignment can implicitly align the text representations between languages."
  - [section 3.2]: "the multilingual text-image aligned encoder (Multilingual in table 1) is still very capable in the bi-text retrieval task" with 55.7% accuracy vs. 62.2% for explicit text-text alignment.
  - [corpus]: Neighboring work M3DR and SwasthLLM similarly leverage multimodal contrastive learning for cross-lingual alignment, suggesting broader validity, though corpus evidence is limited and largely from concurrent work.
- Break condition: If image representations lack semantic granularity (e.g., generic or low-information images), languages may align to noise rather than meaning, reducing transfer quality.

### Mechanism 2
- Claim: Languages entirely unseen during pretraining can be integrated into the aligned representation space using only monolingual image-caption data.
- Mechanism: Adding new-language captions to the contrastive training mix forces the encoder to project those captions toward the same image-anchored region as existing languages. No bitext or translation supervision is required.
- Core assumption: The pretrained multilingual encoder has sufficient transferable subword/character representations to process the unseen language meaningfully after tokenization.
- Evidence anchors:
  - [abstract]: "languages unseen by the encoder in pretraining can be incorporated into this alignment post-hoc"
  - [section 3.3]: Quechua bitext retrieval improved from 18.0% to 29.2% when Quechua captions were added; XLM-R never saw Quechua during pretraining.
  - [corpus]: No direct corpus evidence for this specific mechanism; related works focus on fine-tuning existing multilingual models rather than adding truly unseen languages.
- Break condition: If the new language is typologically distant and shares negligible subword vocabulary with pretraining languages, tokenization may produce uninformative sequences, limiting integration.

### Mechanism 3
- Claim: Text-image alignment preserves or improves downstream NLU capabilities rather than overwriting them.
- Mechanism: Contrastive image-caption tuning adds a semantic objective that complements existing linguistic knowledge. The encoder retains sentence-level features useful for reasoning tasks while gaining cross-lingual alignment as an emergent property.
- Core assumption: The text encoder's pretraining has already encoded sufficient syntactic/semantic structure; fine-tuning does not catastrophically interfere with these representations.
- Evidence anchors:
  - [section 3.4]: XNLI accuracy improved from 43.8% (baseline XLM-R) to 51.3% (Multilingual) and 51.6% (+Quechua), with English performance also increasing.
  - [section 4]: "the alignment of the text encoder with the space of the image encoder does not damage the quality of the text representations for downstream use, but actually improves them."
  - [corpus]: Weak direct evidence; related contrastive alignment papers (AlignX, CoLAP) focus on alignment gains, not NLU preservation explicitly.
- Break condition: Excessive image-dominated training could shift representations toward visual semantics at the cost of linguistic nuance (e.g., losing negation or abstract concept handling).

## Foundational Learning

- Concept: Contrastive Learning (InfoNCE-style)
  - Why needed here: The entire alignment mechanism depends on understanding how contrastive loss pulls matching pairs together and pushes non-matching pairs apart in representation space.
  - Quick check question: Given a batch of 4 image-caption pairs, what would the loss gradient encourage for the similarity matrix?

- Concept: Multilingual Encoders and Representation Disjointness
  - Why needed here: Without understanding why XLM-R produces language-isolated clusters, the motivation for alignment and the significance of improvement remain unclear.
  - Quick check question: Why might a multilingual encoder trained on imbalanced corpora produce different representations for semantically equivalent sentences in different languages?

- Concept: Zero-Shot Cross-Lingual Transfer
  - Why needed here: The paper's downstream evaluation trains NLI classifiers only on English; understanding transfer is critical to interpreting cross-lingual XNLI results.
  - Quick check question: If an encoder's representations are perfectly aligned across languages, what should happen to a classifier trained on language A when applied to language B?

## Architecture Onboarding

- Component map: Text Encoder (XLM-R) -> Mean-pool -> Linear projection (512-dim) -> Cosine similarity; Vision Encoder (ViT) -> Linear projection (512-dim) -> Cosine similarity

- Critical path:
  1. Initialize both encoders from pretrained weights
  2. Train projection layers only ("warm-up") until loss stabilizes (paper: half of first epoch)
  3. Unfreeze encoders and continue joint fine-tuning
  4. Evaluate on bitext retrieval (Flores-200) and downstream NLU (XNLI)

- Design tradeoffs:
  - Projection-only warm-up vs. full fine-tuning from start: warm-up prevents early instability but delays encoder adaptation
  - Language rotation per image vs. multi-caption per image: rotation reduces per-language data seen; the paper trades data volume for experimental comparability
  - Adding low-resource language vs. maintaining high-resource performance: slight overall drop observed (55.7% → 50.4%) but paper notes this is likely due to reduced data per language, not inherent interference

- Failure signatures:
  - Bitext retrieval accuracy near baseline (~0.5% for vanilla XLM-R): projection layers may not have warmed up sufficiently, or temperature is poorly initialized
  - Language-specific clusters visible in t-SNE despite training: insufficient multilingual data mixing or encoder frozen too long
  - NLI accuracy drops after alignment: contrastive objective may be dominating; reduce learning rate or unfreeze more gradually

- First 3 experiments:
  1. Reproduce the Eng-Only vs. Multilingual comparison on a held-out language pair to validate implicit alignment on your infrastructure.
  2. Ablate warm-up duration (no warm-up, 25% epoch, 50% epoch) to test sensitivity of alignment quality to projection initialization.
  3. Add a truly unseen language (not in Flores-200) with synthetically generated captions to test post-hoc integration in a realistic low-resource scenario.

## Open Questions the Paper Calls Out
None

## Limitations
- Data Distribution and Quality: Performance evaluated on Flores-200 (translations) rather than real-world low-resource scenarios; Quechua experiment uses only 2.4k image-caption pairs
- Generalizability Across Languages: Effectiveness for typologically distant languages or scripts remains untested; Quechua still uses Latin script and shares vocabulary with Spanish
- Trade-off Between Alignment and Representation Quality: Adding Quechua captions decreased overall performance (55.7% → 50.4%), attributed to reduced data per language but mechanism requires further investigation

## Confidence
- High Confidence: Core claim that multilingual image-caption alignment implicitly aligns text representations across languages (55.7% vs 18.3% bitext retrieval accuracy)
- Medium Confidence: Claim that approach preserves/improves downstream NLU performance (XNLI results support but need more extensive validation)
- Low Confidence: Claims about effectiveness in truly extreme low-resource scenarios not directly tested; robustness to noisy/ambiguous pairs untested

## Next Checks
1. **Typological Diversity Test**: Evaluate the method on a language pair with completely different scripts and linguistic structures (e.g., Arabic and Korean) that were not seen during pretraining to assess the limits of post-hoc integration capability.

2. **Ablation on Downstream Tasks**: Conduct comprehensive ablation studies across multiple NLU tasks (sentiment analysis, NER, QA) to determine whether NLI improvements generalize and identify any tasks where alignment degrades performance.

3. **Real-World Low-Resource Scenario**: Test the method using a truly low-resource language with minimal web presence (fewer than 10k sentences in pretraining corpora) and evaluate whether the contrastive alignment can meaningfully improve representations compared to vanilla multilingual encoders.