---
ver: rpa2
title: Matrix-Driven Identification and Reconstruction of LLM Weight Homology
arxiv_id: '2508.06309'
source_url: https://arxiv.org/abs/2508.06309
tags:
- zhang
- homology
- mdir
- matrix
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Matrix-Driven Identification and Reconstruction
  (MDIR), a novel white-box method for detecting weight homology between large language
  models. MDIR operates solely on model weights without requiring inference, using
  matrix analysis, polar decomposition, and Large Deviation Theory to compute rigorous
  p-values for statistical significance.
---

# Matrix-Driven Identification and Reconstruction of LLM Weight Homology

## Quick Facts
- arXiv ID: 2508.06309
- Source URL: https://arxiv.org/abs/2508.06309
- Reference count: 40
- Primary result: MDIR achieves perfect AUC and accuracy on LeaFBench for detecting LLM weight homology

## Executive Summary
This paper introduces Matrix-Driven Identification and Reconstruction (MDIR), a novel white-box method for detecting weight homology between large language models. MDIR operates solely on model weights without requiring inference, using matrix analysis, polar decomposition, and Large Deviation Theory to compute rigorous p-values for statistical significance. The method achieves perfect scores on both AUC and accuracy metrics on LeaFBench, outperforming existing approaches. MDIR can detect homology even with different tokenizers or layer counts, reconstruct weight correspondence mappings, and identify specific transformations like permutations or scaling.

## Method Summary
MDIR detects weight homology by computing the orthogonal transformation between model weight matrices using polar decomposition. For embeddings, it aligns matrices E and E' by solving an optimization problem to find the optimal orthogonal transformation Ũ. When tokenizers differ, it restricts analysis to common tokens C. The method computes trace(Ũ) and converts it to a p-value using Large Deviation Theory bounds. If significant, it repeats the analysis on attention and MLP layers, using Hungarian algorithm to resolve permutation mappings between neurons.

## Key Results
- Achieves perfect AUC and accuracy scores on LeaFBench benchmark
- Detects homology across different tokenizers by analyzing common token embeddings
- Provides rigorous p-value estimation using Large Deviation Theory
- Outperforms existing methods on both AUC and accuracy metrics
- Successfully reconstructs weight correspondence mappings including permutations and scaling

## Why This Works (Mechanism)

### Mechanism 1
The method detects homology by identifying preserved coordinates within the invariant transformation space of the model weights. Under idealized training conditions, gradients along the invariant group orbit are zero, so coordinates along this orbit remain fixed at initialization values. If two models are homologous, the transformation aligning them should lie close to the identity in this space, whereas independent models will be distributed randomly. This assumes the optimizer preserves initialization structure along invariant directions even with non-orthogonal optimizers like AdamW.

### Mechanism 2
Rigorous p-values are derived by applying Large Deviation Theory to the trace of the recovered orthogonal transformation matrix. Under the null hypothesis (no homology), the alignment matrix Ũ is uniformly distributed over the orthogonal group O(n). The probability of observing a trace near n (perfect alignment) decreases exponentially. This computes the probability to determine if observed alignment is statistically significant or random chance.

### Mechanism 3
Homology can be effectively detected by analyzing only vocabulary embedding matrices, even across different tokenizers. The method aligns embeddings E and E' using polar decomposition to find the optimal orthogonal transformation Ũ. It handles different tokenizers by restricting comparison to common tokens C. If trace of alignment is high (and p-value low), homology is detected, leveraging the assumption that semantic structure of common tokens remains approximately aligned across models from the same initialization.

## Foundational Learning

- **Concept: Polar Decomposition**
  - *Why needed here:* Extracts the orthogonal transformation (rotation/permutation) relating two weight matrices, filtering out non-orthogonal noise from training
  - *Quick check question:* Given a matrix product A^T B, how does extracting its orthogonal factor help isolate the structural transformation from the noise introduced by fine-tuning?

- **Concept: Hungarian Algorithm (Linear Sum Assignment)**
  - *Why needed here:* Reconstructs the specific permutation mapping (e.g., which neuron i maps to neuron j) by maximizing the trace of the alignment matrix
  - *Quick check question:* Why is finding the permutation with maximum trace equivalent to a linear sum assignment problem?

- **Concept: Haar Measure**
  - *Why needed here:* Defines the statistical baseline (null hypothesis) - what a "random" orthogonal matrix looks like, essential for calculating p-value significance
  - *Quick check question:* Why is the assumption that Ũ follows the Haar measure critical for claiming that a high trace value implies homology rather than coincidence?

## Architecture Onboarding

- **Component map:** Input Handler -> Orthogonal Solver -> Permutation Resolver -> Statistical Engine
- **Critical path:** The "Fast Path" is checking Embeddings only: Load -> Common Vocab -> E^T E' -> Ortho -> Trace -> p-value
- **Design tradeoffs:**
  - *Embedding-only vs. Layer-wise:* Checking only embeddings is extremely fast and robust against pruning/layer-scaling but might fail if only specific layers were stolen or if embeddings were re-initialized. Layer-wise provides forensic detail but is computationally heavier
  - *Heuristic vs. Hungarian:* For high-dimensional models, simple heuristics (max element per row) can approximate the Hungarian algorithm to reduce O(n^3) cost
- **Failure signatures:**
  - High p-value on known derivative: Indicates heavy fine-tuning on embeddings, different random seed initialization, or significant adversarial noise injection
  - Non-convergence of Polar Decomp: Suggests rank deficiency, meaning common token set was insufficient
- **First 3 experiments:**
  1. Seed Ablation: Train two models from different seeds on same data. Verify MDIR returns p ≈ 1 (Not Significant) to prove it detects initialization lineage, not just data similarity
  2. Tokenizer Stress Test: Take base model, replace tokenizer, retrain embeddings on subset of data. Plot trace/p-value over training steps to see when homology "locks in"
  3. Noise Injection: Add Gaussian noise to weights. Compare MDIR's trace degradation vs. baseline similarity metrics (CKA/Cosine) to demonstrate robustness

## Open Questions the Paper Calls Out

### Open Question 1
Can MDIR be extended to resolve model phylogeny, such as identifying the "last common ancestor" among three or more related models? The current framework is designed for pairwise binary classification and transformation reconstruction, lacking the logic to trace lineage across multiple distinct models.

### Open Question 2
Can MDIR be adapted to detect adversarial obfuscation involving complex combinations of transformations that currently evade detection? The algorithm relies on identifying specific structural patterns (like permutations) via the Hungarian algorithm; general orthogonal transformations that preserve functionality but lack these patterns may not yield significant p-values.

### Open Question 3
To what extent does the non-invariance of the AdamW optimizer degrade the preservation of G-coordinates over trillions of training tokens? The theoretical justification relies on invariant optimizers, creating a gap between the statistical model and the behavior of AdamW over extremely long training horizons.

## Limitations

- Theoretical mechanism assumes invariant coordinate preservation with non-orthogonal optimizers like AdamW, lacking direct empirical validation for LLMs
- Statistical assumptions about Haar measure may not hold when models share pretraining data or architectural similarities
- Cannot resolve model phylogeny or identify common ancestors among multiple related models
- May fail against adversarial obfuscation using complex transformations beyond simple permutations and scaling

## Confidence

- **High:** Implementation correctness, benchmark performance metrics, polar decomposition mechanics
- **Medium:** Statistical significance testing validity, cross-tokenizer performance claims
- **Low:** Invariant coordinate preservation theory, universal applicability across all training scenarios

## Next Checks

1. **Invariant Space Theory Validation:** Train multiple models from same initialization with different optimizers (SGD, AdamW, Adagrad) and varying token counts. Verify MDIR consistently identifies homology despite optimizer differences, directly testing invariant coordinate preservation claim.

2. **Haar Measure Baseline Testing:** Generate synthetic orthogonal matrices from different distributions (random orthogonal vs. structured matrices with known correlations) and compare their trace distributions against MDIR's p-value calculations. This validates whether statistical assumptions hold under controlled conditions.

3. **Adversarial Transformation Robustness:** Systematically apply transformations beyond simple permutations and scaling (e.g., non-linear remappings, sparse neuron deletion, or structured weight pruning) to homologous models. Test whether MDIR maintains detection capability and identify breaking point for each transformation type.