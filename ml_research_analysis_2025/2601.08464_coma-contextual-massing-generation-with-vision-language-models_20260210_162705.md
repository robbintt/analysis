---
ver: rpa2
title: 'CoMa: Contextual Massing Generation with Vision-Language Models'
arxiv_id: '2601.08464'
source_url: https://arxiv.org/abs/2601.08464
tags:
- building
- massing
- site
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of automated building massing
  generation in architectural design, which traditionally relies heavily on manual
  designer intuition. The authors introduce CoMa-20K, a novel dataset containing 20,000
  samples with 3D massing geometries, functional requirements, and urban context imagery,
  addressing the lack of suitable training data for data-driven approaches.
---

# CoMa: Contextual Massing Generation with Vision-Language Models

## Quick Facts
- **arXiv ID**: 2601.08464
- **Source URL**: https://arxiv.org/abs/2601.08464
- **Reference count**: 22
- **Primary result**: Introduces CoMa-20K dataset and benchmarks VLMs for automated building massing generation, showing trade-offs between geometric validity and contextual adaptation.

## Executive Summary
This paper addresses the challenge of automated building massing generation in architectural design by introducing CoMa-20K, a novel dataset containing 20,000 samples with 3D massing geometries, functional requirements, and urban context imagery. The authors formulate massing generation as a conditional vision-language task and evaluate both fine-tuned and zero-shot Vision-Language Models. Results demonstrate that while fine-tuned models attempt complex geometries, they produce artifacts; large zero-shot models generate cleaner outputs but overly simplistic forms. The study establishes a foundational benchmark and highlights key challenges in geometric reasoning and contextual adaptation for AI-augmented architectural design.

## Method Summary
The method uses Vision-Language Models (VLMs) to generate building massings conditioned on three inputs: textual functional requirements (floor count, usable area), site contour coordinates, and contextual imagery. Building massings are represented as lists of horizontal extrusions (polygon footprint + bottom/top elevations) in JSON format. The model is trained to autoregressively generate this structured representation. The CoMa-20K dataset provides paired examples for training. Evaluation uses metrics including Site IoU, Floor Error, Area Error, Contextual Relevance, and JSON Validity to assess geometric accuracy, constraint satisfaction, and contextual integration.

## Key Results
- Fine-tuned models attempt complex geometries but produce artifacts (self-intersecting polygons, invalid elevations)
- Large zero-shot models produce geometrically clean and consistent outputs but exhibit strong limitations in complexity and contextual adaptation
- JSON Validity ranges from 0.63-0.99 across models, indicating learnability of the structured output format
- Contextual Relevance remains low (0.15-0.24), suggesting limited integration of visual context

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-modal conditioning (textual requirements + site coordinates + visual context) enables VLMs to learn mappings between programmatic constraints and 3D geometric outputs.
- **Mechanism**: The model receives a concatenated prompt containing functional requirements, site contour coordinates, and contextual imagery. Training on paired examples creates conditional associations between input constraints and valid massing geometries.
- **Core assumption**: The statistical regularities in real-world urban development (captured in the CoMa-20K dataset) encode learnable relationships between site constraints and architectural solutions.
- **Evidence anchors**:
  - [abstract] "We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs)"
  - [Section 4.1] "The model input is a concatenated prompt consisting of the site's requirements formatted as a text string, the site contour coordinates, and a contextual image"
  - [corpus] Related work "Generative AI for Urban Design" uses similar multi-modal diffusion approaches for site-specific design, supporting the multi-modal conditioning paradigm.
- **Break condition**: If geometric complexity exceeds what the tokenized representation can express, or if the model fails to learn spatial reasoning from visual context alone.

### Mechanism 2
- **Claim**: Autoregressive token generation can produce structured 3D geometry when trained on consistent JSON-formatted massing representations.
- **Mechanism**: Building massings are represented as lists of horizontal extrusions (polygon footprint + bottom/top elevations). The VLM learns to generate this structured format autoregressively, with validity measured by JSON parseability.
- **Core assumption**: Tokenization of coordinate-based geometry preserves sufficient spatial information for reconstruction, and the model can learn geometric consistency constraints from data.
- **Evidence anchors**:
  - [Section 3.1.1] "Each building massing is composed of a list of horizontal extrusions. Each extrusion is defined by a polygonal footprint (polygons), a bottom elevation, and a top elevation"
  - [Table 2] JSON Validity metric shows 0.63-0.99 across models, indicating the format is learnable
  - [corpus] Weak corpus evidence for this specific mechanism—no directly comparable autoregressive 3D generation papers found.
- **Break condition**: If generated coordinates drift from valid geometric configurations (self-intersecting polygons, impossible elevations), as observed in fine-tuned model artifacts.

### Mechanism 3
- **Claim**: Zero-shot VLMs with explicit architectural rules in prompts can produce geometrically valid but simplistic massings without training.
- **Mechanism**: The 235B parameter model receives detailed prompts specifying spatial logic (site boundaries, coordinate consistency), placement logic (area estimation, buffers), and geometric articulation rules, enabling rule-guided generation.
- **Core assumption**: Large pre-trained VLMs encode sufficient spatial reasoning and instruction-following capability to interpret and apply explicit geometric constraints.
- **Evidence anchors**:
  - [Section 4.2] "The model is prompted with detailed architectural rules and spatial logic constraints"
  - [Section 5.3] "the large zero-shot model produces geometrically clean and consistent outputs, but exhibits strong limitations in complexity and contextual adaptation"
  - [corpus] "UrbanSense" paper demonstrates VLMs can perform quantitative urban analysis, supporting general spatial reasoning capability claims.
- **Break condition**: If the visual context provides information the model cannot integrate without training (specific site geometry relationships).

## Foundational Learning

- **Concept: Vision-Language Models (VLMs)**
  - **Why needed here**: The entire framework depends on VLMs' ability to process images (context), text (requirements), and coordinates simultaneously. Understanding the Qwen3-VL architecture (image encoder + LLM decoder) is prerequisite.
  - **Quick check question**: Can you explain how a VLM aligns visual and textual embeddings, and why LoRA fine-tuning only modifies attention projections?

- **Concept: 3D Massing as Extrusion Representation**
  - **Why needed here**: The geometry format (horizontal extrusions defined by polygon + elevations) is the output vocabulary. Without understanding this, you cannot parse or evaluate model outputs.
  - **Quick check question**: Given a JSON massing output, can you reconstruct its 3D form and compute its total usable area?

- **Concept: Conditional Generation Metrics**
  - **Why needed here**: The benchmark uses multiple metrics (IoU, floor error, area error, contextual relevance) that measure different failure modes. Understanding what each captures is essential for debugging.
  - **Quick check question**: If a model has low Floor Error but high Site IoU, what type of geometric problem does this indicate?

## Architecture Onboarding

- **Component map**: Requirements dict → text formatter → prompt concatenator; Site coordinates → normalized format; Context images → env_render_high selection → Qwen3-VL (image encoder + vision-language adapter + transformer decoder) → Autoregressive token generation → JSON extraction → validation → geometry reconstruction

- **Critical path**:
  1. Load CoMa-20K sample (requirements + contour + image)
  2. Format prompt string with requirements and coordinates
  3. Forward pass through VLM with image + text
  4. Autoregressively generate token sequence
  5. Extract and validate JSON
  6. Compute metrics against ground truth

- **Design tradeoffs**:
  - **Fine-tuning vs. Zero-shot**: Fine-tuned models learn complex forms but produce artifacts (geometric self-intersections); zero-shot produces clean but simplistic rectangles. Paper shows no current solution achieves both.
  - **Model scale**: 2B→8B improves ID IoU (0.46→0.75) and Contextual Relevance (0.15→0.24) but Area Error degrades then improves nonlinearly—suggesting capacity alone doesn't solve geometric reasoning.
  - **Dataset balance**: Dominance of simple massings in CoMa-20K may limit complex geometry learning.

- **Failure signatures**:
  - **Low JSON Validity (fine-tuned small models)**: Model hasn't learned output format—check prompt formatting and tokenization
  - **High Floor/Area Error with valid JSON**: Model learned format but not constraint satisfaction—check training coverage for those constraint ranges
  - **Valid geometry but low Contextual Relevance**: Model ignores visual context—verify image encoder inputs and attention patterns
  - **Self-intersecting polygons (fine-tuned)**: Tokenized representation loses topological constraints—consider post-processing or constrained decoding

- **First 3 experiments**:
  1. **Baseline reproduction**: Fine-tune Qwen3-VL-2B on CoMa-20K with paper's LoRA config (r=8, lr=5e-4, 3 epochs). Verify JSON Validity ~0.63 and Floor Error ~0.76 as sanity check.
  2. **Ablate context modality**: Train three variants removing (a) context image, (b) site coordinates, (c) requirements text. Measure impact on Site IoU and Contextual Relevance to understand which input drives which output property.
  3. **Prompt engineering sweep on zero-shot**: Test the 235B model with varying rule specificity (minimal vs. detailed architectural constraints) to identify which prompt components reduce simplicity bias without training.

## Open Questions the Paper Calls Out
None

## Limitations
- **Geometric Representation Constraint**: The tokenization of 3D geometry as coordinate sequences is fundamentally lossy for complex topological relationships, limiting achievable fidelity.
- **Dataset Bias**: CoMa-20K contains predominantly simple massing geometries, creating an implicit ceiling on what models can learn.
- **Evaluation Gap**: Current metrics measure quantitative fit but don't capture qualitative architectural quality, aesthetic coherence, or contextual appropriateness.

## Confidence
**High Confidence**: The fundamental challenge of automated massing generation is real and well-established. The multi-modal conditioning approach is technically sound. The observation that zero-shot models produce cleaner but simpler outputs while fine-tuned models attempt complexity but fail geometrically is reproducible and well-supported.

**Medium Confidence**: The claim that current VLMs cannot simultaneously achieve geometric validity and contextual adaptation is supported by results but may be premature. The specific architectural rules in prompts may not be optimal, and the dataset bias could be masking potential.

**Low Confidence**: The assertion that this establishes a "foundational benchmark" assumes no other massing datasets or evaluation protocols exist in the broader architectural ML literature.

## Next Checks
1. **Representation Ablation Study**: Implement a post-processing geometric validation step that rejects self-intersecting outputs and regenerates them. If this significantly improves fine-tuned model scores, it confirms the representation (not the model) is the limiting factor.

2. **Dataset Expansion Analysis**: Manually create a small set of complex massings (multi-extrusion, articulated forms) not present in CoMa-20K. Fine-tune on the original dataset plus these examples and measure whether the model can learn this pattern or if it fails consistently.

3. **Cross-Modal Attention Inspection**: Visualize attention patterns in the fine-tuned 2B model when processing contextual images. Determine if the model attends to relevant site features (boundaries, context buildings) or if visual context is being ignored despite being present in the input.