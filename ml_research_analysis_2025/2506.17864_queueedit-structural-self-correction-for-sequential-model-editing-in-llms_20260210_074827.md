---
ver: rpa2
title: 'QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs'
arxiv_id: '2506.17864'
source_url: https://arxiv.org/abs/2506.17864
tags:
- editing
- parameters
- knowledge
- llms
- queue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QueueEDIT addresses the challenge of maintaining LLM performance
  during sequential model editing (SME), where continuous corrections can degrade
  general capabilities due to parameter bias. The core idea is to use a queue-based
  self-correction framework that maps knowledge triplets to specific transformer neurons,
  stores edited parameters in a queue, and dynamically aligns previously edited parameters
  through a structural mapping editing loss.
---

# QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs

## Quick Facts
- arXiv ID: 2506.17864
- Source URL: https://arxiv.org/abs/2506.17864
- Authors: Taolin Zhang; Haidong Kang; Dongyang Li; Qizhou Chen; Chengyu Wang Xiaofeng He; Richang Hong
- Reference count: 20
- One-line primary result: QueueEDIT achieves significant improvements in sequential model editing (SME) by using a queue-based self-correction framework that maps knowledge triplets to specific transformer neurons and dynamically aligns previously edited parameters.

## Executive Summary
QueueEDIT addresses the critical challenge of maintaining both editing accuracy and general LLM capabilities during sequential model editing (SME). The method introduces a novel structural mapping that associates components of knowledge triplets (subject, relation, object) with specific transformer neurons, combined with a queue-based self-correction mechanism that dynamically aligns semantically related past edits. This approach significantly outperforms strong baselines across three benchmarks (ZSRE, CounterFact, RIPE) under 1000 sequential edits, achieving average accuracies of 72.4% (Rel.), 65.5% (Gen.), and 50.6% (Loc.) on GPT-J, and 69.1% (Rel.), 61.8% (Gen.), and 61.2% (Loc.) on LLaMA3, while maintaining better general capability consistency than competing methods.

## Method Summary
QueueEDIT operates through a two-phase approach: First, it performs structural mapping editing by associating the subject entity with the first MLP matrix ($W_{fc}$), the relation with the second MLP matrix ($W_{proj}$), and the object with gradient backpropagation representation in the Transformer's FFN layer. This is optimized through a structural editing loss that treats the relation as a bridge between subject and object. Second, it implements queue-based self-correction by storing edited parameters in a FIFO queue and dynamically aligning previously edited parameters through semantic distance calculations, selecting Top-K related parameters for updates while preserving the model's general capabilities through controlled dequeuing.

## Key Results
- Outperforms strong baselines (ROME, MEMIT, GRACE) on ZSRE, CounterFact, and RIPE benchmarks under 1000 sequential edits
- Achieves average accuracies of 72.4% (Rel.), 65.5% (Gen.), and 50.6% (Loc.) on GPT-J; 69.1% (Rel.), 61.8% (Gen.), and 61.2% (Loc.) on LLaMA3
- Maintains better general capability consistency than competing methods while achieving higher editing accuracy

## Why This Works (Mechanism)

### Mechanism 1: Structural Mapping Editing
- Claim: Mapping knowledge triplet components to distinct FFN matrices improves editing precision and knowledge association
- Mechanism: Subject→$W_{fc}$, Relation→$W_{proj}$, Object→gradient backpropagation; uses structural loss $L_{st} = ||k_s^* \times W_{proj}^{l_0} + h_r - v^*||^2$
- Core assumption: FFN layer stores knowledge in key-value format with distinct matrices for triplet components
- Evidence anchors: [abstract] structural mapping editing loss; [section 3.2] explicit mapping of subject/relation/object; [corpus] Multiplicative Orthogonal Sequential Editing
- Break condition: FFN layer doesn't store knowledge in structured key-value format or relation can't be decoupled from subject

### Mechanism 2: Queue-Based Self-Correction for Dependency Management
- Claim: FIFO queue with dynamic alignment mitigates catastrophic forgetting and maintains logical consistency
- Mechanism: Store edited parameters in queue, calculate Euclidean distances, select Top-K related past edits, perform semantic alignment updates
- Core assumption: Sequential edits have semantic dependencies identifiable via parameter-space distance
- Evidence anchors: [abstract] store parameters in queue and dynamically align; [section 3.3] Top-K editing parameters for sequential updates; [corpus] LyapLock and Model Merging for Knowledge Editing
- Break condition: Sequential edits are independent or parameter distance is poor proxy for semantic relatedness

### Mechanism 3: Locality Preservation via Selective Parameter Dequeuing
- Claim: Controlled dequeuing preserves general capabilities by preventing accumulation of outdated edits
- Mechanism: FIFO with dequeuing condition based on semantic distance threshold $\eta_{deq}$; removes oldest parameter only if distance < threshold
- Core assumption: "Freshness" of knowledge tied to temporal position; removing old low-similarity parameters preserves general NLP performance
- Evidence anchors: [abstract] update parameters at queue head to ensure they don't harm general abilities; [section 3.3] calculate similarity to determine whether to dequeue; [corpus] Edit Less, Achieve More
- Break condition: Important long-term knowledge consistently dequeued or threshold poorly tuned

## Foundational Learning

- **Locate-and-Edit Paradigm**
  - Why needed here: Foundation technique QueueEDIT builds upon; identifies specific neurons responsible for facts before modifying them
  - Quick check question: How does causal tracing identify which layer in the Transformer to edit?

- **Knowledge Graph Triplets (<s, r, o>)**
  - Why needed here: Core innovation maps triplet structure directly to model components
  - Quick check question: In <USA, President, Biden>, which component maps to second MLP matrix ($W_{proj}$)?

- **Catastrophic Forgetting in Sequential Editing**
  - Why needed here: Central problem QueueEDIT solves; sequential updates overwrite or distort previously learned knowledge
  - Quick check question: Why does adding new parameters for each edit fail to solve logical dependency problem in SME?

## Architecture Onboarding

- Component map: Input -> Structural Mapping Module -> Knowledge Queue -> Self-Correction Engine -> Dequeue Logic -> Output
- Critical path: 1) New triplet arrives 2) Structural Mapping computes initial edit 3) New parameter added to queue 4) Queue scanned, Top-K related parameters updated 5) Head checked for removal
- Design tradeoffs:
  - Queue Length vs. Memory: Longer queue (50%) captures more dependencies but increases memory usage; paper found 30% optimal
  - Top-K Selection vs. Precision: Higher K ensures more dependencies updated but risks updating unrelated parameters
  - FIFO vs. Semantic Dequeue: Strict FIFO discards old knowledge too aggressively; similarity threshold preserves relevant old knowledge
- Failure signatures:
  - Logical Inconsistency: Related questions answered incorrectly (e.g., "President's Wife" wrong after "President" updated) -> self-correction failing or K too small
  - General Capability Degradation: Performance on standard NLP benchmarks drops significantly -> dequeue threshold too high or Top-K too broad
  - Editing Accuracy Collapse: Reliability scores drop over time -> queue length insufficient or structural mapping loss ineffective
- First 3 experiments:
  1. Baseline Comparison on Reliability/Generality/Locality: Run against ROME, MEMIT, GRACE on ZSRE/CounterFact with 1000 edits
  2. Ablation on Queue Components: Run with/without queue, structural loss, random Top-K selection
  3. General Capability Stress Test: Track performance on general NLP benchmark (e.g., MMLU) after every 100 edits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can queue management be refined to mitigate semantic noise and performance degradation when queue lengths exceed specific ratios of total edit count?
- Basis in paper: [explicit] Section 4.3 notes longer queue lengths lead to less consistent editing performance due to semantic noise
- Why unresolved: Identifies counter-intuitive result but doesn't propose filtering mechanism beyond static Top-K selection
- What evidence would resolve it: Comparative analysis using dynamic queue pruning strategies versus current FIFO approach on sequences exceeding 1000 edits

### Open Question 2
- Question: Is Euclidean distance sufficient for capturing complex logical dependencies, or does it fail on semantically related but geometrically distant knowledge?
- Basis in paper: [inferred] Section 3.3 relies exclusively on Euclidean distance for Top-K selection
- Why unresolved: Parameter proximity doesn't always equate to semantic dependency; may fail to detect logical implications in distant parameter regions
- What evidence would resolve it: Ablation study comparing Euclidean distance against semantic similarity metrics for Top-K selection

### Open Question 3
- Question: Does computational overhead of iterative self-correction impede practical deployment on larger models (>70B parameters) or resource-constrained environments?
- Basis in paper: [inferred] Testing on Qwen2.5-14B limited by GPU resources; results on larger backbones restricted to 14B parameters
- Why unresolved: While memory consumption analyzed, latency cost of calculating Top-K distances for every edit in massive models remains unquantified
- What evidence would resolve it: Latency measurements for QueueEDIT compared to retrieval-based baselines on 70B parameter model

## Limitations
- Queue Threshold Sensitivity: Effectiveness of semantic distance thresholds (η_que, η_deq) not extensively explored; poor tuning could degrade performance
- Triplet Representation & Mapping: Exact implementation details for deriving representations (particularly v* via backpropagation) not fully detailed
- Scalability and Computational Overhead: Additional computational overhead for maintaining queue and performing Top-K alignments not discussed for very long sequences or larger models

## Confidence
- High Confidence: Core queue-based self-correction mechanism is sound and well-justified by semantic dependency problem; reported performance improvements on specific metrics likely reproducible
- Medium Confidence: Structural mapping of triplets to FFN matrices is novel and plausible but requires further validation across different knowledge types and model architectures
- Low Confidence: Specific hyperparameter values not thoroughly ablated; optimal settings may be sensitive to dataset and model size, limiting generalizability

## Next Checks
1. Conduct systematic ablation study varying loss coefficients (α1, α2), queue length (10%-50% of edits), Top-K values, and distance thresholds (η_que, η_deq); report effects on Reliability, Generality, Locality, and general capability preservation on MMLU
2. After 1000 sequential edits, evaluate edited models on standard NLP benchmarks (MMLU, SuperGLUE, ANLI) and compare performance drop against pre-trained baseline and competing SME methods
3. For subset of edits where reliability degrades, perform detailed error analysis to identify if errors due to failed self-correction (related parameters not updated) or over-correction (unrelated parameters incorrectly aligned)