---
ver: rpa2
title: 'BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood
  Inverse Reinforcement Learning'
arxiv_id: '2511.22210'
source_url: https://arxiv.org/abs/2511.22210
tags:
- reward
- learning
- expert
- offline
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BiCQL-ML, a policy-free offline inverse reinforcement
  learning (IRL) algorithm that avoids explicit policy learning by jointly optimizing
  a reward function and a conservative Q-function in a bi-level framework. The method
  alternates between (i) learning a conservative Q-function via Conservative Q-Learning
  (CQL) under the current reward, and (ii) updating the reward parameters to maximize
  the expected Q-values of expert actions while suppressing over-generalization to
  out-of-distribution actions.
---

# BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2511.22210
- **Source URL**: https://arxiv.org/abs/2511.22210
- **Reference count**: 34
- **Primary result**: Outperforms offline IRL baselines on MuJoCo tasks from D4RL benchmark

## Executive Summary
This paper introduces BiCQL-ML, a policy-free offline inverse reinforcement learning algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework. By alternating between learning a conservative Q-function via CQL under the current reward and updating reward parameters to maximize expert action likelihood, the method avoids explicit policy optimization while maintaining stability in offline settings. The approach is theoretically grounded with convergence guarantees to a reward function under which the expert policy is soft-optimal, and demonstrates strong empirical performance on continuous control tasks.

## Method Summary
BiCQL-ML operates by alternating between two optimization levels: a lower-level conservative Q-learning phase that learns Q-values under the current reward function with a CQL penalty to suppress overestimation on out-of-distribution actions, and an upper-level reward update phase that maximizes the likelihood of expert actions by regressing the reward network to match the soft advantage computed from the Q-function. The method avoids explicit policy learning by leveraging the induced Boltzmann policy from the Q-values, making it suitable for offline settings where policy optimization would be unstable.

## Key Results
- Achieves higher average return than BC, DAC, and ValueDice on MuJoCo tasks from D4RL
- Demonstrates effectiveness across both low-data (1 trajectory) and high-data (10 trajectories) regimes
- Successfully recovers reward functions that lead to expert-level downstream policy performance
- Maintains robustness in offline settings without requiring environment interaction or explicit policy optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-level decomposition enables stable offline IRL by eliminating explicit policy optimization.
- Mechanism: The framework alternates between (i) lower-level conservative Q-learning under current reward parameters θ, and (ii) upper-level reward updates maximizing expert action likelihood under induced soft Boltzmann policy π(a|s) ∝ exp(Q(s,a)). This avoids adversarial training and rollout-based policy updates that are unstable or infeasible in offline settings.
- Core assumption: Expert demonstrations are generated by a soft-optimal policy, and the Q-function learned via CQL reliably reflects the current reward.
- Break condition: If the Q-function fails to converge or oscillates due to insufficient conservatism (α too low), reward updates inherit noisy gradients and the bi-level loop destabilizes.

### Mechanism 2
- Claim: Conservative Q-learning regularizer suppresses overestimation of out-of-distribution actions, preventing reward misalignment.
- Mechanism: CQL adds a penalty term α · [E_{s∼D, a∼μ}[Q(s,a)] - E_{(s,a)∼D}[Q(s,a)]] that pushes down Q-values for actions sampled from a broader distribution μ while maintaining dataset action values. This bounds extrapolation errors that would otherwise corrupt reward inference.
- Core assumption: Offline dataset D provides sufficient coverage of expert-relevant state-action pairs; conservatism coefficient α is appropriately tuned.
- Break condition: If α is too high, Q-values become overly conservative, potentially underestimating expert action values and biasing reward learning toward trivial solutions.

### Mechanism 3
- Claim: Soft advantage regression provides tractable gradients for reward learning when direct MLE gradients vanish.
- Mechanism: Since ∇_θ L_R(θ) = 0 when Q_ϕ is fixed during reward optimization, the method regresses r_θ(s,a) onto the soft advantage A(s,a) = Q_ϕ(s,a) - γ·log Σ_{a'} exp(Q_ϕ(s',a')). This surrogate objective preserves the MLE structure—higher rewards for expert-preferred actions—while enabling gradient flow.
- Core assumption: The soft advantage computed from the current Q-function is a meaningful target for reward regression, and the squared loss landscape is sufficiently smooth.
- Break condition: If Q_ϕ is inaccurate (e.g., early in training), the soft advantage target is noisy, causing reward parameters to chase misleading signals until Q stabilizes.

## Foundational Learning

- Concept: **Offline Reinforcement Learning Constraints**
  - Why needed here: BiCQL-ML operates purely on static datasets without environment interaction; understanding distributional shift and OOD generalization is essential to grasp why conservatism is required.
  - Quick check question: Can you explain why standard Q-learning overestimates values for actions not in the offline dataset, and how this affects reward inference in IRL?

- Concept: **Soft (Maximum Entropy) Bellman Operator**
  - Why needed here: The method uses soft value functions V(s) = log Σ_a exp(Q(s,a)) and soft optimality; this differs from standard Bellman backups and underpins the Boltzmann policy assumption.
  - Quick check question: What is the soft Bellman backup for Q(s,a), and how does the entropy term change the optimal policy compared to standard RL?

- Concept: **Bi-Level Optimization**
  - Why needed here: The algorithm structures reward learning as an upper-level problem with Q-learning as a lower-level constraint; understanding nested optimization and fixed-point convergence is necessary for implementation.
  - Quick check question: In bi-level optimization, why must the lower-level solution be differentiable or approximated with respect to upper-level parameters for gradient-based methods?

## Architecture Onboarding

- Component map:
  - Offline Dataset D -> Q-Network Q_ϕ(s,a) -> Soft Bellman Targets -> Conservative Q-values
  - Expert Demonstrations D_E -> Soft Advantage Targets -> Reward Network r_θ(s,a)
  - CQL Regularizer Module -> Conservative Penalty α
  - Target Q-Network Q_ϕ⁻ -> Stable Bellman Updates

- Critical path:
  1. Initialize θ, ϕ; sample batch from D.
  2. **Lower-level**: Compute soft Bellman targets using r_θ; add CQL penalty; update Q_ϕ for K_Q steps.
  3. **Upper-level**: Sample expert batch from D_E; compute soft advantage targets from Q_ϕ; regress r_θ for K_R steps.
  4. Periodically sync target network ϕ⁻ ← ϕ.
  5. Repeat until convergence (fixed-point condition or max iterations).

- Design tradeoffs:
  - **Higher α (conservatism)**: More robust to OOD actions but risks underestimating expert values; may slow convergence.
  - **K_Q vs K_R ratio**: More Q-updates per reward update improves Q-accuracy but increases compute; fewer updates risks stale Q-targets for reward regression.
  - **Dataset split (D vs D_E)**: Larger expert subset improves reward signal but reduces data for Q-learning coverage.

- Failure signatures:
  - **Q-values diverging or exploding**: CQL penalty insufficient (α too low) or learning rate η_Q too high.
  - **Reward collapse to constant**: Soft advantage targets near-zero or Q not yet converged; check early stopping on reward updates.
  - **Expert likelihood not improving**: Q-function may be inaccurate; verify lower-level convergence before upper-level updates.
  - **Slow convergence**: Lipschitz condition L_Q · L_ML < 1 may be near boundary; reduce learning rates or increase conservatism.

- First 3 experiments:
  1. **Sanity check on tabular MDP**: Implement BiCQL-ML on a small discrete MDP with known reward; verify that learned r_θ recovers ground truth and expert Q-values are soft-optimal. This validates the bi-level loop without neural network confounds.
  2. **Ablation on conservatism α**: Run BiCQL-ML on HalfCheetah with α ∈ {0.1, 1.0, 10.0}; plot Q-value distributions for dataset vs OOD actions and final policy returns. Identify the α range where OOD suppression is effective without excessive underestimation.
  3. **Comparison with and without soft advantage regression**: Replace the surrogate loss (Eq. 8) with direct MLE gradients (which should vanish) or with a fixed target Q; measure whether reward learning progresses. This confirms the mechanism claim about gradient vanishing and surrogate necessity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive mechanisms be designed to dynamically adjust the conservatism penalty ($\alpha$) based on offline data quality?
- Basis in paper: [explicit] The authors suggest "integrating adaptive mechanisms for dynamically adjusting conservatism could enhance performance and robustness."
- Why unresolved: The current implementation relies on a fixed hyperparameter, which may be suboptimal for datasets with varying distributional shifts.
- Evidence to resolve: An adaptive variant of BiCQL-ML that automatically tunes $\alpha$ to outperform fixed baselines on heterogeneous datasets without manual tuning.

### Open Question 2
- Question: Can the framework be theoretically extended to incorporate auxiliary signals like human preference feedback?
- Basis in paper: [explicit] Future work proposes exploring "extensions that incorporate additional modalities such as human preference feedback."
- Why unresolved: The current maximum likelihood formulation is derived strictly for trajectory demonstrations and lacks a mechanism to ingest preference labels.
- Evidence to resolve: A unified objective function that integrates preference-based constraints into the bi-level optimization while maintaining convergence guarantees.

### Open Question 3
- Question: Does the convergence guarantee hold practically given the strict contraction condition ($L_Q \cdot L_{ML} < 1$) required for neural network approximators?
- Basis in paper: [inferred] Theorem 1 relies on Assumption 1(d), a contraction property that is difficult to verify or enforce for standard deep neural networks.
- Why unresolved: If the product of Lipschitz constants exceeds 1 during training, the theoretical fixed-point guarantee is void for the empirical algorithm.
- Evidence to resolve: Empirical analysis measuring the Lipschitz constants of the reward and Q-updates during training, or proof of convergence when spectral normalization is applied.

## Limitations

- **Underspecified hyperparameters**: Neural network architectures, learning rates, batch sizes, and dataset splits are not provided, making faithful reproduction difficult.
- **Soft optimality assumption**: The theoretical guarantees rely on expert demonstrations being generated by a soft-optimal policy, which may not hold in practice.
- **Limited empirical scope**: Evaluation is confined to D4RL-MuJoCo tasks; scalability to more complex domains remains untested.

## Confidence

- **High Confidence**: The theoretical convergence guarantee that BiCQL-ML converges to a reward under which the expert policy is soft-optimal (Section IV).
- **Medium Confidence**: The mechanism claims for how CQL prevents reward misalignment and how soft advantage regression resolves gradient vanishing; supported by internal logic but lacking external corpus validation.
- **Low Confidence**: Claims about scalability and robustness to varying dataset sizes beyond the tested D4RL regimes; not sufficiently explored.

## Next Checks

1. **Ablation on conservatism α**: Run BiCQL-ML on HalfCheetah with α ∈ {0.1, 1.0, 10.0}; plot Q-value distributions for dataset vs OOD actions and final policy returns. Identify the α range where OOD suppression is effective without excessive underestimation.
2. **Comparison with and without soft advantage regression**: Replace the surrogate loss (Eq. 8) with direct MLE gradients (which should vanish) or with a fixed target Q; measure whether reward learning progresses. This confirms the mechanism claim about gradient vanishing and surrogate necessity.
3. **Scalability test on more complex domains**: Evaluate BiCQL-ML on DeepMind Control Suite or Atari tasks (if feasible) to assess whether the bi-level framework scales beyond MuJoCo, and whether reward recovery and downstream performance remain stable.