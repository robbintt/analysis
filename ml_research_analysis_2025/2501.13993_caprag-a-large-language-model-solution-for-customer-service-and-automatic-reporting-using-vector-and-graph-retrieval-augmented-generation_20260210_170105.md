---
ver: rpa2
title: 'CAPRAG: A Large Language Model Solution for Customer Service and Automatic
  Reporting using Vector and Graph Retrieval-Augmented Generation'
arxiv_id: '2501.13993'
source_url: https://arxiv.org/abs/2501.13993
tags:
- graph
- query
- information
- bank
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAPRAG, a hybrid retrieval-augmented generation
  system that combines vector and graph RAG to enhance customer service in the banking
  sector. The method addresses the limitations of semantic similarity in capturing
  complex relationships and geospatial information by using a dual-knowledge base
  approach with both vector and graph databases.
---

# CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2501.13993
- Source URL: https://arxiv.org/abs/2501.13993
- Reference count: 6
- Primary result: Dual Vector+Graph RAG achieves >0.8 answer relevance vs 0.75 baseline for banking customer service

## Executive Summary
This paper introduces CAPRAG, a hybrid retrieval-augmented generation system that combines vector and graph RAG to enhance customer service in the banking sector. The method addresses the limitations of semantic similarity in capturing complex relationships and geospatial information by using a dual-knowledge base approach with both vector and graph databases. CAPRAG employs a processing pipeline that refines text data, utilizes query expansion, and routes queries through a router LLM to construct final queries for an open-source LLM response generation. The system achieves improved answer relevance (about 0.75 baseline, over 0.8 with enhanced retrieval) and context relevance while maintaining competitive groundedness scores, demonstrating effectiveness in handling both relationship-based and contextual banking queries.

## Method Summary
CAPRAG processes banking documents (SEC filings, annual reports, brochures) through a semantic chunking pipeline, storing chunks in a vector database and entities in a Neo4j graph database with a Document→Section→Chunk hierarchy plus Person, Location (with coordinates), and Product nodes. When a user query arrives, a query expansion module enriches it before a router LLM classifies the query type and routes it to vector, graph, or hybrid retrieval. Vector retrieval uses semantic embeddings with FlashRank reranking, while graph retrieval converts queries to Cypher via text-to-Cypher (using static templates due to LLM syntax issues). Retrieved context is aggregated and passed to a Zephyr-7b-beta LLM for generation. The system is evaluated using LLM-as-a-judge metrics (answer relevance, context relevance, groundedness) on a manually curated banking dataset.

## Key Results
- Answer relevance improves from ~0.75 baseline to >0.8 with enhanced retrieval
- Context relevance remains competitive across configurations
- Groundedness scores remain lower (~0.55) despite retrieval improvements
- Hybrid retrieval outperforms pure vector or graph approaches for relationship and geospatial queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual knowledge base architecture improves retrieval for both contextual and relationship-based queries.
- Mechanism: Vector database handles semantic similarity queries while graph database captures entity relationships and geospatial data. A router LLM classifies queries and routes them to the appropriate retrieval system, then combines results for generation.
- Core assumption: Query types are distinguishable and benefit from different retrieval paradigms.
- Evidence anchors:
  - [abstract] "We proposed a hybrid Customer Analysis Pipeline RAG (CAPRAG) that effectively addresses both relationship-based and contextual queries"
  - [section 5.1] Figure 4 visualizes the expanded graph schema with nodes (Document, Section, Chunk, Person, Location) and edges (UNDER-SECTION, HAS-DOCUMENT, HAS_PARENT, CONSECUTIVE)
  - [corpus] No direct corpus evidence for dual RAG; corpus focuses on customer service chatbots and data analysis, not hybrid retrieval architectures
- Break condition: If queries do not clearly favor one retrieval type, routing adds latency without quality gain.

### Mechanism 2
- Claim: Query expansion and retrieval enhancement increase answer relevance from ~0.75 to >0.8.
- Mechanism: Query expansion enriches user queries before retrieval. A reranker (FlashRank) reorders retrieved chunks post-retrieval. Together, they improve chunk-query alignment.
- Core assumption: Expanded queries and reranking surface more relevant context without introducing noise.
- Evidence anchors:
  - [abstract] "When a user submits a query, it is first expanded by a query expansion module before being routed to construct a final query"
  - [section 4] Table 2 and Figure 3 show enhanced retrieval yields the highest answer relevance (>0.8), though groundedness remains lower (~0.55)
  - [corpus] Weak corpus evidence; related work on context-aware NLU (arXiv:2506.01781) suggests attention mechanisms improve intent classification, but does not evaluate query expansion
- Break condition: Over-expansion or aggressive reranking retrieves irrelevant chunks, degrading groundedness.

### Mechanism 3
- Claim: Graph-based retrieval with Cypher queries enables geospatial and relationship reasoning that vector similarity cannot capture.
- Mechanism: User queries are converted to Cypher queries via an LLM (text-to-Cypher). The graph schema encodes spatial coordinates (Latitude, Longitude) for Location nodes and hierarchical relationships (Section-to-Section, Chunk-to-Section) for structured traversal.
- Core assumption: The LLM can reliably generate valid Cypher queries given the schema, or a static Cypher repository covers common patterns.
- Evidence anchors:
  - [abstract] "The Cypher query component is employed to effectively query the graph database"
  - [section 5.1, 5.2] The schema includes geospatial coordinates for Location; example query: "I am in Ariana and I am wondering what's the nearest bank A ATM branch to me?" Table 3 compares Vector vs. Graph RAG outputs for executive background queries
  - [corpus] No corpus evidence for graph-based geospatial RAG; TDA paper (arXiv:2508.14136) uses topological methods for segmentation, not graph retrieval
- Break condition: If the LLM fails to generate valid Cypher or the schema lacks required entities, queries fall back to less effective vector retrieval.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) fundamentals
  - Why needed here: CAPRAG extends basic RAG with dual retrieval; understanding baseline RAG is prerequisite.
  - Quick check question: Can you explain how a vanilla RAG pipeline retrieves context and generates answers?

- Concept: Graph databases and Cypher query language
  - Why needed here: Graph RAG requires schema design and Cypher query construction for relationship traversal.
  - Quick check question: How would you write a Cypher query to find all Sections under a given Document node?

- Concept: LLM evaluation metrics (answer relevance, context relevance, groundedness)
  - Why needed here: The paper uses these metrics to compare configurations; understanding them is essential for replication.
  - Quick check question: What is the difference between answer relevance and groundedness in RAG evaluation?

## Architecture Onboarding

- Component map: User Query -> Query Expansion -> Router LLM -> (Vector DB or Graph DB) -> Reranker/Template Engine -> Context Aggregation -> Generation LLM -> Response
- Critical path:
  1. User query → Query Expansion
  2. Router LLM determines retrieval path(s)
  3. Parallel retrieval from Vector and/or Graph DB
  4. Context aggregation → LLM generation
- Design tradeoffs:
  - Vector RAG: Better for contextual, semantic queries; weaker for relationships and geospatial.
  - Graph RAG: Strong for structured relationships; requires schema design and Cypher generation.
  - Hybrid: Higher complexity but broader coverage; latency increases with dual retrieval.
- Failure signatures:
  - Low groundedness (~0.55) indicates retrieved context not fully utilized in generation.
  - Text-to-Cypher failures if LLM outputs invalid syntax; paper used static Cypher repository as fallback.
  - Chunking optimization may fragment context, reducing groundedness.
- First 3 experiments:
  1. Baseline Vector RAG: Implement vanilla RAG with semantic chunking, measure answer relevance and groundedness.
  2. Enhanced Retrieval: Add FlashRank reranker and query expansion, compare metrics to baseline.
  3. Graph RAG for Relationships: Build schema for Document-Section-Chunk hierarchy, test Cypher queries for relationship-based questions (e.g., executive backgrounds).

## Open Questions the Paper Calls Out

- Open Question 1: How does CAPRAG perform when evaluated against standard open-source benchmarks compared to proprietary datasets?
  - Basis in paper: [explicit] The authors state they "couldn't evaluate our pipelines on open data benchmarks in this field or conclude to comparisons" due to hardware and API restrictions.
  - Why unresolved: The reported results rely solely on a manually created dataset specific to "Bank A," limiting the ability to generalize the system's effectiveness.
  - What evidence would resolve it: Comparative metrics (Answer Relevance, Groundedness) from CAPRAG run on established financial or general RAG benchmarks.

- Open Question 2: Can dynamic text-to-Cypher generation outperform the static template approach used in the current pipeline?
  - Basis in paper: [explicit] The authors note that LLMs often "fall to respect" valid rigid formats for Cypher queries, leading them to use a static repository of templates instead.
  - Why unresolved: Resource constraints prevented the fine-tuning of models or the use of specialized agents to solve the syntax adherence issue dynamically.
  - What evidence would resolve it: A study showing that a fine-tuned agentic workflow can generate valid Cypher queries with higher success rates than the template matching method.

- Open Question 3: How can retrieval enhancements be optimized to prevent the observed degradation in groundedness scores?
  - Basis in paper: [inferred] While Table 2 shows Answer Relevance exceeding 0.8, the discussion notes that Groundedness scores decrease or remain significantly lower (around 0.55) with chunking and retrieval optimizations.
  - Why unresolved: The paper suggests a trade-off where improving the relevance of retrieved chunks may inadvertently reduce the factual alignment of the generated answer.
  - What evidence would resolve it: A modified pipeline configuration that yields a concurrent increase in both answer relevance and groundedness metrics.

## Limitations

- The evaluation relies entirely on LLM-as-a-judge rather than human evaluation, which may not capture nuanced aspects of customer service quality
- The effectiveness of automated Cypher generation and routing decisions in production settings is not well-validated
- The reported groundedness score (~0.55) suggests retrieved context is not fully utilized in generation, indicating potential issues with chunk granularity or source attribution

## Confidence

- High: The dual knowledge base architecture addresses real limitations in pure vector RAG for relationship and geospatial queries
- Medium: Enhanced retrieval improves answer relevance metrics as claimed
- Low: The effectiveness of automated Cypher generation and routing decisions in production settings

## Next Checks

1. **Cypher Generation Validation**: Test the text-to-Cypher module with a diverse set of relationship-based queries to measure syntax validity and correctness rates
2. **Router Decision Analysis**: Create a labeled test set of queries to evaluate the router LLM's accuracy in selecting appropriate retrieval paths
3. **Groundedness Investigation**: Analyze specific failure cases where high answer relevance coexists with low groundedness to identify whether this stems from chunking strategy or generation behavior