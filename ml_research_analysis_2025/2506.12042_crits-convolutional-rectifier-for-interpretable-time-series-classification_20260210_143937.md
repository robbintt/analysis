---
ver: rpa2
title: 'CRITS: Convolutional Rectifier for Interpretable Time Series Classification'
arxiv_id: '2506.12042'
source_url: https://arxiv.org/abs/2506.12042
tags:
- crits
- input
- explanations
- weights
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRITS, an intrinsically interpretable convolutional
  model for time series classification that provides local explanations without relying
  on gradient computation, random perturbations, or upsampling. The model architecture
  combines a single convolutional layer, a max-pooling layer, and a fully-connected
  rectifier network.
---

# CRITS: Convolutional Rectifier for Interpretable Time Series Classification

## Quick Facts
- arXiv ID: 2506.12042
- Source URL: https://arxiv.org/abs/2506.12042
- Reference count: 40
- This paper introduces CRITS, an intrinsically interpretable convolutional model for time series classification that provides local explanations without relying on gradient computation, random perturbations, or upsampling

## Executive Summary
CRITS is an intrinsically interpretable time series classification model that combines a single convolutional layer, global max-pooling, and a fully-connected rectifier network to provide exact feature weights for each input time step and variable. The model "unwraps" the rectifier network to extract instance-dependent linear weights, then traces these through the convolutional and pooling layers to compute precise saliency maps without gradient computation. Evaluated on six datasets, CRITS achieves classification F1-scores comparable to ROCKET (0.62-1.00) while demonstrating high alignment between explanations and predictions across most perturbation methods.

## Method Summary
CRITS uses a 1D convolutional layer followed by global max-pooling and a deep rectifier network (ReLU-only) with sigmoid output for binary classification. The key innovation is the "unwrapping" mechanism that extracts exact feature weights by identifying the binary activation pattern of the rectifier network for each input, collapsing it into a linear transformation. These weights are then deconvolved through the convolutional kernel to produce sparse, gradient-free saliency maps. The model was trained using Adam optimizer with random search over kernel size [3,16], filters [16,256], rectifier network layers [3,5], and neurons [20,200].

## Key Results
- Classification F1-scores range from 0.62 to 1.00, comparable to ROCKET baseline
- High alignment between explanations and predictions across most datasets and perturbation methods
- Good input-sensitivity that increases with input changes
- Relatively good sparsity compared to post-hoc methods like SmoothGRAD, DeepSHAP, and GradientSHAP

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Linear Unwrapping
The rectifier network's ReLU-only architecture allows it to be decomposed into an instance-dependent linear system. By fixing the binary activation states for a specific input, the non-linear network collapses into a linear transformation that exactly replicates the output. This requires only ReLU activations—any other non-linearity breaks the piecewise linear property.

### Mechanism 2: Sparse Max-Pooling Traceability
Global max-pooling selects a single maximum value per filter, creating a sparse, traceable link between classifier and specific input patch. During backpropagation, relevance weights project exclusively onto input coordinates that produced the maximum, resulting in sparse masks where irrelevant time steps have zero weight.

### Mechanism 3: Gradient-Free Deconvolution
Instead of computing gradients, the method multiplies the weight derived from the rectifier network for a specific filter by the filter's kernel weights. This product is placed at input locations identified during unpooling, constructing exact saliency maps analytically from internal weights rather than through noisy gradient backpropagation.

## Foundational Learning

- **Concept: ReLU as a Linear Switch**
  - Why needed here: The core "unwrapping" logic relies on treating ReLU not as a curve, but as a switch that activates a specific linear slope
  - Quick check question: Given a trained ReLU neuron $y = \max(0, wx + b)$, if the input $x$ causes the neuron to fire (output > 0), what is the local derivative with respect to $x$?

- **Concept: Convolution as Feature Matching**
  - Why needed here: To understand why multiplying the kernel weights by the RN relevance creates a valid "saliency map"
  - Quick check question: In a 1D convolution, if a kernel $[1, -1, 1]$ slides over an input sequence, what does the resulting high-activation value indicate about the match between the kernel pattern and the input pattern?

- **Concept: Global Max-Pooling**
  - Why needed here: To understand how the model selects a single "prototypical" patch from a long time series
  - Quick check question: If a feature map has a dimension of $1 \times 100$ (time steps) after convolution, what is the dimension after Global Max-Pooling, and what information is lost?

## Architecture Onboarding

- **Component map:** Input (T × m) -> 1D Conv (Stride 1, Padding valid) -> Feature Maps -> Global Max-Pooling -> Dense Vector -> Fully Connected Rectifier Network (Deep, ReLU only) -> Sigmoid neuron (Classification) -> Parallel Path: "Unwrapper" module (analytical function) runs backward from Layer 3 to Input

- **Critical path:** The implementation of Equations 1, 2, and 3. Specifically, the logic to record the `argmax` indices during the forward pass of the pooling layer so they can be used to scatter the weights during the backward explanation pass.

- **Design tradeoffs:**
  - Interpretability vs. Performance: The single convolutional layer limits the model's ability to learn hierarchical features compared to deep ResNets or Transformers
  - Sharpness vs. Completeness: The reliance on Max-Pooling ensures sparse, sharp explanations but may ignore secondary patterns that contribute to the classification

- **Failure signatures:**
  - Low Sparsity: If explanations highlight the entire time series, the Rectifier Network may not have learned distinct activation patterns (check for dead neurons or insufficient depth)
  - Explanation Instability: If explanations change drastically with minor input noise, check if the Max-Pooling indices are flipping (common when feature map values are very close)

- **First 3 experiments:**
  1. Toy Dataset Validation: Create a synthetic time series with a known discriminative "bump" at a random location. Train CRITS and verify that the explanation saliency map highlights exactly the bump location.
  2. Linearity Check: For a single instance, perform the "unwrapping" to get $w_i$. Compute $w_i^T x_i$. Manually calculate the model output (pre-sigmoid) and confirm they are identical (up to floating point precision).
  3. Perturbation Ablation: Implement the "Inverse" perturbation on the top 10% of relevant features identified by CRITS vs. random features, and plot the drop in classification probability (Alignment metric).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the unwrapping mechanism be extended to deep convolutional architectures (multiple convolutional layers) without sacrificing the exactness of the feature weights or classification performance?
- Basis in paper: [explicit] The authors state, "We are aware that additional work is needed... including potential modifications to CRITS to make it more accurate in complex scenarios," and the current architecture is limited to a single convolutional layer.
- Why unresolved: The current methodology relies on a direct traceback from a single max-pooling layer; multiple layers introduce hierarchical non-linearities that complicate the "unwrapping" process defined in Eq. 3.
- What evidence would resolve it: A modified CRITS architecture with >1 convolutional layers that maintains the "exact weight" property while showing improved F1-scores on complex datasets.

### Open Question 2
- Question: Why does CRITS demonstrate low alignment specifically with the "mean" perturbation method compared to "zero" or "inverse" methods?
- Basis in paper: [inferred] The results in Section 4.2 show CRITS performs poorly with mean perturbations, positioning last in some datasets, which suggests the relevance weights may not be robust to changes in feature averages or smoothing.
- Why unresolved: The paper observes the phenomenon but does not analyze why the "exact weights" used by the model fail to predict the model's behavior when features are replaced by their mean.
- What evidence would resolve it: An analysis of how the max-pooling selection shifts under mean-value perturbations compared to zero-value perturbations.

### Open Question 3
- Question: How do CRITS explanations compare in human understandability and utility against other intrinsically interpretable models, such as shapelets or attention-based networks?
- Basis in paper: [inferred] The evaluation focuses on alignment/sensitivity against post-hoc methods (SmoothGRAD, SHAP) and classification against ROCKET, but lacks a comparative user study or qualitative benchmark against other *intrinsic* time series explainers.
- Why unresolved: Quantitative metrics like sparsity are used as proxies for understandability, but the actual cognitive load on users remains unmeasured.
- What evidence would resolve it: A user study where domain experts classify instances using explanations from CRITS versus shapelet-based models.

## Limitations
- The single convolutional layer may limit feature learning capacity compared to deeper architectures, potentially affecting classification performance on complex datasets
- The method requires the rectifier network to learn distinct activation patterns; dead neurons or insufficient depth could lead to low sparsity in explanations
- Temporal resolution depends on convolutional stride and kernel size; large strides may lose fine-grained temporal information during backpropagation

## Confidence
- High confidence: Classification performance claims (F1-scores 0.62-1.00 vs ROCKET baseline), as these are directly measurable from the reported results
- Medium confidence: Explanation quality metrics (Alignment, Input-Sensitivity, Sparsity), as these depend on correct implementation of the perturbation framework and comparison methodology
- Medium confidence: The piecewise linear unwrapping mechanism, as the mathematical formulation is sound but implementation details are critical

## Next Checks
1. Linearity verification: For a single test instance, compute the unwrapped weights and verify that $w_i^T x_i + b_i$ exactly matches the model's pre-sigmoid output
2. Perturbation ablation study: Compare classification probability drops when perturbing features identified by CRITS versus random features to validate the Alignment metric
3. Hyperparameter consistency check: Reconcile the kernel size search range discrepancy between Table 3 ([3,16]) and Table 4 (reported values up to 30) by verifying the actual search bounds used in experiments