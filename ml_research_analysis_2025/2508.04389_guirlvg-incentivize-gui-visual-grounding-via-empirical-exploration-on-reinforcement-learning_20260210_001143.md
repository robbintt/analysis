---
ver: rpa2
title: 'GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement
  Learning'
arxiv_id: '2508.04389'
source_url: https://arxiv.org/abs/2508.04389
tags:
- arxiv
- reward
- training
- qwen2
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GuirlVG, a reinforcement learning approach\
  \ to GUI visual grounding (GUI-VG) that challenges the conventional reliance on\
  \ large-scale supervised fine-tuning (SFT). Through systematic empirical exploration,\
  \ the authors decompose the GRPO framework into its core components\u2014format\
  \ reward, accuracy reward, and KL penalty\u2014and propose the Adversarial KL Factor\
  \ to dynamically stabilize training."
---

# GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.04389
- Source URL: https://arxiv.org/abs/2508.04389
- Reference count: 40
- Primary result: RL fine-tuning (5.2K samples) outperforms SFT (up to 13.58M samples) on GUI visual grounding benchmarks

## Executive Summary
GuirlVG challenges the conventional reliance on large-scale supervised fine-tuning for GUI visual grounding by introducing a reinforcement learning approach. Through systematic empirical exploration of the GRPO framework, the authors decompose it into format reward, accuracy reward, and KL penalty components, proposing an Adversarial KL Factor to dynamically stabilize training. Using LoRA-based fine-tuning and prompt engineering, GuirlVG achieves state-of-the-art performance on three GUI-VG benchmarks while requiring only 5.2K training samples—dramatically fewer than traditional SFT methods.

## Method Summary
GuirlVG employs reinforcement fine-tuning on a multimodal large language model (Qwen2.5-VL) for GUI visual grounding. The approach systematically decomposes the GRPO framework into three core components: format reward, accuracy reward, and KL penalty. A key innovation is the Adversarial KL Factor, which dynamically adjusts the KL penalty to stabilize training. The method combines LoRA-based fine-tuning with prompt engineering to optimize performance. The authors conduct extensive empirical exploration to identify optimal configurations for group size, reward scaling, and other hyperparameters, demonstrating that this RL-based approach can achieve superior results with significantly less training data than traditional supervised fine-tuning methods.

## Key Results
- Achieves 7.7% improvement over SFT on ScreenSpot benchmark
- Delivers 17.2% improvement on ScreenSpotPro benchmark
- Reaches 91.9% accuracy on ScreenSpotV2 with only 5.2K training samples

## Why This Works (Mechanism)
GuirlVG's effectiveness stems from reinforcement learning's ability to explore the solution space more efficiently than supervised fine-tuning. By decomposing the GRPO framework into format reward, accuracy reward, and KL penalty components, the method can dynamically balance exploration and exploitation. The Adversarial KL Factor specifically addresses training instability by adaptively adjusting the KL penalty, preventing catastrophic forgetting while allowing the model to diverge from its pretrained behavior when beneficial. This approach is particularly effective for GUI visual grounding because it can handle the complex, multimodal nature of GUI elements without requiring massive labeled datasets.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - provides a framework for optimizing models based on reward signals rather than fixed labels; Quick check - verify reward functions are properly aligned with task objectives
- **Group Relative Policy Optimization (GRPO)**: Why needed - enables efficient policy updates using relative advantage estimates; Quick check - confirm group size selection doesn't introduce bias
- **KL Penalty Dynamics**: Why needed - prevents model collapse while allowing necessary adaptation; Quick check - monitor KL divergence during training to ensure stability
- **LoRA Fine-tuning**: Why needed - enables efficient parameter updates without full fine-tuning; Quick check - validate that LoRA matrices capture relevant task-specific patterns
- **Prompt Engineering**: Why needed - optimizes model behavior for GUI understanding tasks; Quick check - test prompts across different GUI element types and layouts

## Architecture Onboarding

**Component Map:**
Qwen2.5-VL (MLLM) -> LoRA Adapter -> GRPO RL Loop -> Format/Accuracy Rewards + Adversarial KL Factor

**Critical Path:**
GUI screenshot input → MLLM encoding → LoRA-modified policy → Action selection → Reward computation → Policy update via GRPO

**Design Tradeoffs:**
- Smaller training sets vs. comprehensive SFT coverage
- Dynamic KL penalty vs. fixed regularization
- Reinforcement exploration vs. supervised convergence speed

**Failure Signatures:**
- KL divergence spikes indicating training instability
- Reward stagnation suggesting exploration issues
- Performance degradation on certain GUI element types

**First Experiments:**
1. Baseline evaluation on ScreenSpot with pretrained Qwen2.5-VL
2. GRPO with fixed KL penalty to establish performance floor
3. Full GuirlVG pipeline with Adversarial KL Factor on ScreenSpotPro

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does GuirlVG's RFT approach generalize effectively to other MLLM architectures beyond Qwen2.5-VL?
- Basis in paper: [explicit] Limitations section states: "we focused on a specific multimodal large language model, Qwen2.5-VL, and did not extend our experiments to other models, such as Qwen2.5-VL."
- Why unresolved: The empirical findings may be architecture-specific, tied to Qwen2.5-VL's pretrained capabilities or training dynamics.
- What evidence would resolve it: Replicating GuirlVG's RFT pipeline on alternative MLLMs (e.g., InternVL, LLaVA) and comparing performance trajectories.

### Open Question 2
- Question: How does GuirlVG's data efficiency and performance scale with larger model sizes (32B+ parameters)?
- Basis in paper: [explicit] Limitations section notes: "computational resource constraints prevented us from exploring larger-scale models, such as those with 32B or 72B parameters."
- Why unresolved: The relationship between model scale and RFT effectiveness for GUI-VG remains unknown; larger models may require different hyperparameter configurations.
- What evidence would resolve it: Training GuirlVG on 32B and 72B parameter models with identical configurations and benchmarking on ScreenSpot variants.

### Open Question 3
- Question: Why does increasing group size from 6 to 8 degrade performance when theory suggests larger groups should provide better baseline estimates?
- Basis in paper: [inferred] Table 6 shows group size 8 achieves 83.9% vs. 87.4% for group size 6. The paper notes this is "counterintuitive" since "larger groups theoretically provide better baseline estimates."
- Why unresolved: The paper empirically observes the phenomenon but does not investigate the underlying mechanism or validate whether this is task-specific.
- What evidence would resolve it: Ablation studies analyzing advantage estimate variance across group sizes, or testing on non-GUI tasks to determine if the effect generalizes.

## Limitations
- Results are specific to Qwen2.5-VL architecture and may not transfer to other MLLMs
- Computational constraints prevented testing on larger models (32B+ parameters)
- The counterintuitive group size performance relationship lacks mechanistic explanation

## Confidence

**Claims about data efficiency gains:** Medium - results are strong on tested benchmarks but lack external validation
**Claims about Adversarial KL Factor effectiveness:** Medium - component shows promise but ablation clarity is limited  
**Claims about RFT being a scalable SFT alternative:** Low-Medium - theoretical advantages exist but practical deployment evidence is incomplete

## Next Checks
1. Test GuirlVG on a held-out dataset of GUI screenshots from mobile apps not represented in the training distribution to assess true generalization
2. Conduct controlled ablation experiments varying only the KL penalty mechanism while keeping LoRA and prompt tuning constant
3. Measure end-to-end task success rates when GuirlVG is integrated into a complete GUI agent pipeline handling multi-step interactions