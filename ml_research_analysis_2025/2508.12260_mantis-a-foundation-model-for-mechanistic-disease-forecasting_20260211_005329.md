---
ver: rpa2
title: 'Mantis: A Foundation Model for Mechanistic Disease Forecasting'
arxiv_id: '2508.12260'
source_url: https://arxiv.org/abs/2508.12260
tags:
- mantis
- data
- uniform
- transmission
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mantis is a simulation-grounded foundation model for infectious
  disease forecasting that achieves accurate out-of-the-box predictions across diverse
  diseases and transmission modes. Trained entirely on synthetic mechanistic simulations
  rather than real-world data, Mantis delivers competitive forecasting performance
  across six real-world diseases including COVID-19, influenza, dengue, and scarlet
  fever.
---

# Mantis: A Foundation Model for Mechanistic Disease Forecasting

## Quick Facts
- **arXiv ID:** 2508.12260
- **Source URL:** https://arxiv.org/abs/2508.12260
- **Reference count:** 40
- **Primary result:** Simulation-pretrained foundation model achieves accurate out-of-the-box infectious disease forecasts across diverse diseases and transmission modes

## Executive Summary
Mantis is a foundation model for infectious disease forecasting that achieves accurate predictions across diverse diseases and transmission modes without requiring retraining or disease-specific tuning. Trained entirely on synthetic mechanistic simulations rather than real-world data, Mantis demonstrates that fundamental contagion dynamics can be learned from simulation rather than historical patterns. The model achieves competitive performance on six real-world diseases including COVID-19, influenza, dengue, and scarlet fever, ranking among the top two models for point forecast accuracy and probabilistic metrics across all evaluation tasks.

## Method Summary
Mantis is trained on 400M+ days of synthetic data generated from stochastic SEAIR compartmental models, including human-to-human, vector-borne, and environmental transmission routes. The model employs a hybrid CNN-Transformer architecture with 16 layers and 1024 dimensions, incorporating an Epidemic Pattern Memory Bank of 256 learned pattern vectors. The training pipeline includes comprehensive observation noise modeling (underreporting, delays, day-of-week effects) to ensure robustness to real-world surveillance data. The model is trained using quantile regression with pinball loss across 23 quantiles, optimized via AdamW with OneCycle scheduling for one epoch, and evaluated on real-world datasets using Mean Absolute Error and Weighted Interval Score metrics.

## Key Results
- Achieves competitive or top-ranked performance across six real-world diseases including COVID-19, influenza, dengue, and scarlet fever
- Maintains accuracy at 8-week forecast horizons, exceeding typical 1-4 week limits of existing systems
- Generalizes to diseases with transmission mechanisms absent from training data, including vector-borne (dengue) and environmental (scarlet fever) routes
- Outperforms or ranks among top two models for point forecast accuracy and probabilistic metrics across all evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Purely synthetic mechanistic simulations can serve as a superior pretraining signal for real-world forecasting compared to limited real-world historical data.
- **Mechanism:** The model learns "fundamental contagion dynamics" (e.g., SEAIR transitions, seasonality) from a massive, diverse simulation space (~400M days) rather than memorizing sparse historical trajectories. This allows the model to recognize structural epidemic patterns in novel real-world data without fine-tuning.
- **Core assumption:** The parameter space of the mechanistic simulators sufficiently covers the distribution of real-world outbreak dynamics, including those of novel pathogens.
- **Evidence anchors:** "Mantis... trained entirely on synthetic mechanistic simulations... demonstrating that it can capture fundamental contagion dynamics rather than memorizing disease-specific patterns." When trained only on real-world COVID-19 data, forecast error increased by 262% compared to the simulation-pretrained Mantis model.

### Mechanism 2
- **Claim:** High architectural capacity (specifically hybrid CNN-Transformer with memory) is necessary to exploit the diversity of simulation data.
- **Mechanism:** The "Epidemic Pattern Memory Bank" stores prototypical outbreak trajectories learned from simulations. A simpler architecture (e.g., LSTM) fails to effectively map the high-dimensional simulation space to the forecast target, resulting in underfitting despite abundant data.
- **Core assumption:** The complexity of the mapping between "observed time series" and "future trajectory" requires a large receptive field and explicit memory retrieval, not just sequential processing.
- **Evidence anchors:** "Despite identical training data and procedures, the reduced architectural capacity [LSTM] led to substantially degraded performance... MAE was 89% higher." The pattern bank consists of K=256 learnable pattern vectors [retrieved] using attention weights.

### Mechanism 3
- **Claim:** Explicit simulation of observation noise (underreporting, delays) enables the model to be robust to low-quality real-world surveillance data.
- **Mechanism:** The training pipeline includes an "Observation Model" that corrupts the mechanistic "ground truth" with realistic artifacts (weekend effects, reporting lags). This trains the model to de-noise inputs at inference time, making it effective on messy real-world data where conventional models might overfit to reporting artifacts.
- **Core assumption:** Real-world data errors are structurally similar to the noise models defined in the simulator (e.g., log-normal multiplicative noise, gamma-distributed delays).
- **Evidence anchors:** "True epidemic dynamics are converted to observed case counts through a comprehensive noise injection pipeline that simulates real-world surveillance artifacts." "Mantis achieved competitive performance... even in settings with limited historical data [and data quality issues]."

## Foundational Learning

- **Concept: Compartmental Models (SEAIR)**
  - **Why needed here:** The entire training corpus is generated via SEAIR (Susceptible-Exposed-Asymptomatic-Infectious-Recovered) differential equations. You must understand these transitions to debug the simulator.
  - **Quick check question:** Can you explain why an "Exposed" (E) compartment is necessary for modeling diseases like COVID-19, and how the "Asymptomatic" (A) compartment affects the force of infection?

- **Concept: Quantile Regression / Pinball Loss**
  - **Why needed here:** Mantis is probabilistic. It outputs 23 quantiles, not a single point. The loss function optimizes these specific intervals.
  - **Quick check question:** If the 0.95 quantile forecast is 100 cases, but the true value is 150 cases, how does the pinball loss penalize this compared to if the 0.95 quantile forecast was 200?

- **Concept: Simulation-Grounded Neural Networks (SGNN)**
  - **Why needed here:** This distinguishes the approach from standard supervised learning. The "ground truth" is not historical fact, but a mechanistic hypothesis.
  - **Quick check question:** What is the primary risk of training a model exclusively on synthetic data (Simulation-to-Real gap), and how does the paper claim to mitigate it?

## Architecture Onboarding

- **Component map:** Input: Time series + Static features (Population, Disease type embedding) -> Multi-scale CNN (kernel sizes 3, 7, 15, 31) -> Hybrid CNN-Transformer Blocks (16 layers) -> Epidemic Pattern Memory (Attention-based retrieval from 256 learned patterns) -> Autoregressive GRU with Cross-Attention to Encoder -> Multi-head Quantile Output (23 quantiles)

- **Critical path:** The Epidemic Pattern Memory Bank is the core differentiator. The model maps input dynamics to 256 latent "archetype" outbreaks and uses these to inform the decoder.

- **Design tradeoffs:**
  - Synthetic vs. Real: Trading data fidelity for infinite volume and mechanism diversity
  - Complexity: The 150M parameter size is heavy for a time-series model but justified by the complexity of the simulation distribution
  - Autoregressive Decoder: Allows for long-horizon (8-week) forecast stability but risks error accumulation if the initial state is misinterpreted

- **Failure signatures:**
  - Temporal Misalignment: The paper notes a failure mode where Mantis forecasts the correct shape but incorrect timing (e.g., predicting a peak 18 weeks early). This suggests the model captures the dynamics but misses external triggers (covariates)
  - Distribution Shift: Performance degrades if the real disease mechanism (e.g., chronic Hepatitis B) fundamentally contradicts the simulator's assumptions (e.g., acute infection only), though the model shows surprising resilience here

- **First 3 experiments:**
  1. Baseline Sanity Check: Run the provided pretrained Mantis model on the COVID-19 dataset. Verify that performance (MAE/WIS) matches the paper's claim of outperforming the naive baseline without any retraining.
  2. Observation Ablation: Retrain Mantis on simulation data without the Observation Model (noise injection). Evaluate on real data to confirm the performance drop, validating the need for noise robustness.
  3. Memory Visualization: Extract the weights of the 256 memory vectors. Cluster them to see if the model has learned distinct archetypes (e.g., "sharp peak," "slow burn," "multi-wave") and correlate these clusters with real-world disease behaviors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific epidemic regimes (e.g., complex chronic diseases) require explicit representation in training simulations, or can sufficiently diverse simulations approximate arbitrary contagion dynamics?
- Basis in paper: The Discussion states, "Future work should investigate when simulation-trained models fail and whether certain epidemic regimes require explicit representation in training, or whether sufficiently diverse simulations can approximate arbitrary contagion dynamics."
- Why unresolved: While Mantis generalized to Hepatitis B (a chronic disease absent from training), the authors note that the specific Hepatitis B dataset had simple dynamics, leaving the boundaries of generalization to complex novel regimes unmapped.
- What evidence would resolve it: Evaluations on real-world diseases with complex dynamics (e.g., multi-stage progression) that are strictly absent from the simulator's parameter space.

### Open Question 2
- Question: Is the scale of 400 million simulated days necessary, or can comparable performance be achieved with fewer, but more strategically designed, simulations?
- Basis in paper: The Discussion asks, "Whether this scale is necessary or whether comparable performance could be achieved with fewer but more strategically designed simulations remains to be seen."
- Why unresolved: The authors generated data continuously until convergence but did not ablate the volume or "strategic design" of the data against the model's resulting accuracy.
- What evidence would resolve it: Ablation studies comparing the current model's performance against versions trained on subsets of the data (e.g., 10M or 100M days) or data curated for specific epidemic features.

### Open Question 3
- Question: How do systematic biases in the underlying mechanistic simulators propagate to the trained model's real-world forecasts?
- Basis in paper: The Discussion notes that "If the simulation framework systematically misrepresents certain disease dynamics... these biases will propagate," but the extent of this inheritance is not quantified.
- Why unresolved: The paper validates overall accuracy but does not isolate whether specific simulator artifacts (e.g., assumptions about intervention efficacy) cause correlated errors in the neural network's predictions.
- What evidence would resolve it: Stress tests comparing model outputs against ground-truths in scenarios where the training simulator is known to systematically deviate from real-world dynamics.

## Limitations

- Fundamental assumption that synthetic mechanistic simulations can adequately represent the full diversity of real-world disease dynamics may not hold for rare phenomena like super-spreading events or abrupt policy changes
- 150M parameter architecture represents significant computational overhead compared to simpler time-series models, raising scalability concerns for resource-constrained settings
- Performance on diseases with different natural histories (e.g., chronic vs. acute) provides some validation but doesn't guarantee coverage of all possible disease behaviors

## Confidence

- **High Confidence (5/5):** The core claim that Mantis achieves accurate out-of-box predictions across diverse diseases is well-supported by empirical results showing competitive or top-ranked performance across six real-world diseases and multiple evaluation metrics
- **Medium Confidence (3/5):** The claim that Mantis generalizes to diseases with transmission mechanisms absent from training data is supported by successful forecasts for vector-borne and environmental diseases, but systematic exploration of edge cases is lacking
- **Low Confidence (2/5):** The assertion that Mantis maintains accuracy at 8-week forecast horizons is impressive but least validated, as the paper doesn't compare against other models specifically at this horizon

## Next Checks

1. **Simulation Coverage Validation:** Systematically test Mantis on synthetic diseases with transmission mechanisms explicitly outside the training distribution (e.g., chronic diseases, multi-host systems, or diseases with extreme seasonal forcing) to identify the boundaries of its generalization capability and quantify the simulation-to-real gap.

2. **Long-Horizon Robustness Study:** Conduct a controlled comparison of Mantis against state-of-the-art models specifically at 4-week, 6-week, and 8-week horizons using standardized datasets, measuring not just accuracy but also calibration quality and temporal alignment to determine if the claimed advantage at longer horizons is statistically significant.

3. **Computational Efficiency Analysis:** Evaluate Mantis's performance relative to its computational footprint by comparing its MAE/WIS metrics per million parameters against simpler architectures (e.g., LSTM, GRU, or shallow CNN models) on the same tasks, establishing whether the architectural complexity is justified by performance gains or represents diminishing returns.