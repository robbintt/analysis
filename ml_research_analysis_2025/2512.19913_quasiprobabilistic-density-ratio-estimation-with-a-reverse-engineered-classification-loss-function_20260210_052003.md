---
ver: rpa2
title: Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification
  Loss Function
arxiv_id: '2512.19913'
source_url: https://arxiv.org/abs/2512.19913
tags:
- pull
- lrevert
- target
- rosmmr
- rosmmc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of density ratio estimation in
  quasiprobabilistic settings where probability densities can be negative, extending
  beyond traditional probabilistic methods. Existing classifier-based approaches struggle
  because their loss functions define transformations that are discontinuous or not
  surjective to negative density ratio values.
---

# Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function

## Quick Facts
- arXiv ID: 2512.19913
- Source URL: https://arxiv.org/abs/2512.19913
- Reference count: 40
- Primary result: State-of-the-art quasiprobabilistic density ratio estimation using REVERT loss, achieving Sliced-Wasserstein distance of 0.301±0.022 on LHC di-Higgs production problem

## Executive Summary
This paper addresses the challenge of density ratio estimation in quasiprobabilistic settings where probability densities can be negative, extending beyond traditional probabilistic methods. Existing classifier-based approaches struggle because their loss functions define transformations that are discontinuous or not surjective to negative density ratio values. To solve this, the authors reverse-engineer a convex loss function (REVERT) that provides a continuous, surjective transformation between classifier outputs and all real-valued density ratios. Applied to a particle physics problem involving di-Higgs production at the LHC, the method achieves state-of-the-art results with lower Sliced-Wasserstein distances compared to existing approaches.

## Method Summary
The paper introduces a novel convex loss function (REVERT) for classifier-based density ratio estimation that works for both probabilistic and quasiprobabilistic settings. The method reverses the typical approach by starting with a desired transformation T(s) = (1-2s)/(s(1-s)) that maps classifier outputs from (0,1) to all real numbers, then deriving the corresponding loss function L(s,y) = ys - (1-y)(log(s) + log(1-s)). This ensures the learned classifier outputs can represent negative density ratios. The approach uses an extended Sliced-Wasserstein distance for evaluation that is compatible with signed measures. The method is applied to a particle physics problem involving di-Higgs production at the LHC, using a 16-feature input space describing jet and muon kinematics.

## Key Results
- Achieves Sliced-Wasserstein distance of 0.301±0.022 on LHC di-Higgs dataset, significantly outperforming existing methods (baseline ~13.45)
- Demonstrates strong performance across multiple metrics including χ² scores and Tsallis relative entropy
- Shows that the reverse-engineered loss function provides state-of-the-art results for both probabilistic and quasiprobabilistic density ratio estimation
- Validates effectiveness through ablation studies comparing MLP vs RoSMM architectures with REVERT loss

## Why This Works (Mechanism)

### Mechanism 1: Homeomorphic Transformation to All Real Numbers
The REVERT loss function enables learning of quasiprobabilistic density ratios by establishing a homeomorphic mapping between bounded classifier outputs and all real-valued density ratios. Standard binary classification losses like BCE transform outputs through T(s) = s/(1-s), which only maps to positive real numbers. The REVERT transformation T(s) = 1/s + 1/(s-1) = (1-2s)/(s(1-s)) is a homeomorphism from (0,1) to ℝ, enabling representation of negative density ratios that arise in quasiprobabilistic settings.

### Mechanism 2: Reverse-Engineered Convexity Guarantees
Starting from a desired transformation T, one can derive a loss function whose risk minimization yields T as the ratio trick transformation, with guaranteed convexity ensuring unique optimal solutions. For loss functions of the form L(s,y) = ys + (1-y)g(s), the Euler-Lagrange equation yields r*(x) = -g'(s*(x))/f'(s*(x)). By choosing g(s) = ∫-T(s)ds for a monotonic transformation T, the Lagrangian becomes convex in s (when q(x|Y=0) ≥ 0), guaranteeing the solution is the unique risk minimizer.

### Mechanism 3: Extended Sliced-Wasserstein Distance for Quasiprobability Evaluation
Standard Wasserstein distances can be extended to signed measures through Hahn-Jordan-like decompositions, providing a valid metric for evaluating quasiprobabilistic density ratio estimates. For signed measures μ = μ+ - μ- and ν = ν+ - ν-, the extended W₁ distance is W₁(μ,ν) = W₁(μ+ + ν-, ν+ + μ-). This is computed by decomposing samples into positive/negative weight components, combining them appropriately, and computing standard Sliced-Wasserstein distance on the resulting non-negative distributions.

## Foundational Learning

- **Concept: Classifier-based Density Ratio Estimation ("Likelihood Ratio Trick")**
  - Why needed here: The entire approach builds on the insight that binary classifiers implicitly learn density ratios. Understanding this relationship is essential to grasp why changing the loss function changes the transformation between classifier output and density ratio.
  - Quick check question: Given a binary classifier s(x) trained with BCE loss on balanced data, what is the formula relating s(x) to the density ratio?

- **Concept: Quasiprobability Distributions (Signed Measures)**
  - Why needed here: The central challenge is that density ratios can be negative in quasiprobabilistic settings. Standard classification outputs (bounded to [0,1] or positive reals) cannot represent these negative values.
  - Quick check question: What physical or mathematical phenomena can produce quasiprobability distributions with negative values?

- **Concept: Euler-Lagrange Equations and Variational Optimization**
  - Why needed here: The paper derives loss functions by solving for the function that minimizes expected risk. Understanding that risk minimization can be formulated as a calculus of variations problem clarifies why the transformation T relates to derivatives of the loss components.
  - Quick check question: For a functional R[s] = ∫L(x,s(x))dx, what condition must s* satisfy to be an extremum?

## Architecture Onboarding

- **Component map:** Input(16) -> Dense(128)+ReLU -> Dense(256)+ReLU -> Dense(128)+ReLU -> Dense(1)+Sigmoid -> T(s) transformation
- **Critical path:**
  1. Ensure training data has explicit positive/negative weight labels
  2. Train classifier with REVERT loss: L(s,y) = ys - (1-y)(log(s) + log(1-s))
  3. Transform sigmoid outputs via T(s) = (1-2s)/(s(1-s)) to obtain density ratio estimates
  4. Evaluate using extended Sliced-Wasserstein distance

- **Design tradeoffs:**
  - Sigmoid output bounds model to (0,1), ensuring numerical stability but requiring careful initialization to avoid saturation
  - RoSMM architecture can improve performance by separating positive/negative components, but requires pre-training sub-models
  - MLP with REVERT is simpler but may struggle if negative/positive regions have complex boundaries

- **Failure signatures:**
  - Classifier outputs clustering near 0 or 1: gradient vanishing, transformation T approaches ±∞
  - Validation loss not decreasing: learning rate too high or initialization poor
  - Sliced-Wasserstein distance similar to reference (~13.45): model not learning meaningful ratios

- **First 3 experiments:**
  1. Train MLP with BCE loss on probabilistic (positive weights only) data subset; verify χ² scores near 1.0
  2. Train same architecture with REVERT loss on quasiprobabilistic data; verify Sliced-Wasserstein distance drops below 0.5
  3. Compare MLP+REVERT vs RoSMMr+REVERT on held-out test set; expect similar performance with RoSMMr slightly better on Sliced-Wasserstein metric

## Open Questions the Paper Calls Out

### Open Question 1
Can a valid extended Sliced-$p$-Wasserstein metric be defined for signed measures where $p=2$, allowing for the use of more efficient computational techniques? The paper notes this is only a valid distance for p=1, preventing use of various efficient techniques developed for computing Sliced 2-Wasserstein distances.

### Open Question 2
Does the REVERT loss function provide performance benefits or stability in standard probabilistic density ratio estimation compared to Binary Cross-Entropy? The introduction claims the loss is "well-suited for both probabilistic and quasiprobabilistic density ratio estimation," but the experimental evaluation is restricted solely to the quasiprobabilistic di-Higgs dataset.

### Open Question 3
How does the optimization landscape and convergence behavior change if the reference distribution $q(x|Y=0)$ is allowed to contain negative values? The theoretical guarantees (uniqueness of the minimizer) rely on this convexity; if the reference distribution contains negatives, the risk minimizer may not be unique or the model may fail to converge.

## Limitations
- The approach assumes the reference distribution q(x|Y=0) is nonnegative everywhere; if this assumption fails, the Lagrangian becomes non-convex and the reverse-engineered loss may not guarantee unique solutions
- The transformation T(s) is unbounded near s=0, 0.5, and 1, making the method sensitive to numerical precision and classifier output saturation
- The extended Sliced-Wasserstein distance computation requires explicit decomposition of signed measures into positive/negative parts, which may not be straightforward for all quasiprobabilistic data

## Confidence
- **High confidence**: The mathematical derivation of the REVERT loss from the Euler-Lagrange equation is rigorous and well-established. The homeomorphism property of T(s) = (1-2s)/(s(1-s)) mapping (0,1) to ℝ is mathematically proven.
- **Medium confidence**: Empirical performance claims on the LHC physics dataset, as these depend on specific Monte Carlo simulations not directly accessible and require domain expertise to validate.
- **Low confidence**: The claim that this is the first method to handle quasiprobabilistic density ratios via classifier-based approaches, as related work on unbounded density ratios may have explored similar territory.

## Next Checks
1. **Numerical stability verification**: Implement REVERT loss with clamped sigmoid outputs and verify that density ratio estimates remain finite across the full range of classifier outputs, particularly near s=0 and s=1.
2. **Distributional assumption test**: Analyze the reference distribution q(x|Y=0) for any negative regions; if found, document the breakdown of the convexity guarantee and measure the impact on estimation accuracy.
3. **Extended Wasserstein implementation**: Reproduce the signed measure decomposition and extended SW₁ distance calculation on synthetic quasiprobabilistic data with known ground truth to validate the metric computation.