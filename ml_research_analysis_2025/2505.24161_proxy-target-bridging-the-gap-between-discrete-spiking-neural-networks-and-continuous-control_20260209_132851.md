---
ver: rpa2
title: 'Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and
  Continuous Control'
arxiv_id: '2505.24161'
source_url: https://arxiv.org/abs/2505.24161
tags:
- proxy
- target
- network
- spiking
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental mismatch between the discrete
  dynamics of spiking neurons and the continuous target network soft update mechanism
  used in RL algorithms. To address this, the authors propose a proxy target framework
  that replaces the discrete spiking target actor with a continuous, differentiable
  proxy network.
---

# Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control

## Quick Facts
- arXiv ID: 2505.24161
- Source URL: https://arxiv.org/abs/2505.24161
- Reference count: 40
- Primary result: Introduces proxy target framework enabling simple LIF neurons to surpass ANN performance in continuous control tasks

## Executive Summary
This paper addresses a fundamental incompatibility between discrete spiking neural networks (SNNs) and continuous control reinforcement learning (RL) algorithms. In standard TD3 setups, the soft update mechanism for target networks assumes continuous gradients, but SNNs produce discrete spike events that disrupt smooth target tracking. The authors propose a proxy target framework where a continuous proxy network is trained to imitate the SNN actor through gradient-based updates. This proxy replaces the SNN in the target network role, enabling stable training while maintaining SNN efficiency at inference time. Experiments across multiple spiking neuron models and continuous control benchmarks show consistent stability improvements and up to 32% higher performance compared to vanilla SNN baselines.

## Method Summary
The core innovation is replacing the discrete SNN target actor with a continuous proxy network that implicitly imitates the online SNN actor. The proxy is updated via K gradient descent steps on a mean squared error loss between proxy and SNN outputs for each training batch. This proxy then participates in the soft target update mechanism alongside the critic network. The approach maintains energy efficiency by using the proxy only during training - the deployed policy remains purely spiking. The framework is evaluated across three spiking neuron models (LIF, CLIF, DN) on five MuJoCo continuous control tasks using TD3 with an ANN critic.

## Key Results
- Up to 32% performance improvement over vanilla SNN baselines across all tasks
- Simple LIF neurons achieve the highest overall performance, surpassing ANN baselines
- Consistent stability improvements across all spiking neuron models tested
- Framework maintains SNN energy efficiency by using proxy only during training

## Why This Works (Mechanism)
The fundamental issue is that discrete spiking events create non-differentiable jumps that conflict with continuous soft target updates in RL. The proxy target framework bridges this gap by providing a smooth, differentiable approximation of the SNN actor that can participate in the soft update mechanism. By training the proxy to imitate the SNN through gradient descent, the system maintains stable target tracking while preserving the energy efficiency of spiking computation at inference time.

## Foundational Learning

**Spiking Neuron Dynamics**
- Why needed: Understanding LIF/CLIF/DN models is essential for implementing the online actor
- Quick check: Verify membrane potential updates, spike generation, and reset mechanisms follow standard equations

**Surrogate Gradient Method**
- Why needed: Enables gradient-based training through discrete spike events
- Quick check: Confirm rectangular window width ω=0.5 is correctly applied to approximate gradients

**TD3 Target Network Updates**
- Why needed: The soft update mechanism (τ=5e-3) is the target of the proxy replacement
- Quick check: Ensure target critic updates follow standard polyak averaging

**Population Encoding/Decoding**
- Why needed: Converts continuous states to spike trains and vice versa
- Quick check: Verify 10 neurons per dimension for both encoder and decoder

## Architecture Onboarding

**Component Map**
State normalization -> Population encoder -> SNN actor (LIF/CLIF/DN) -> Population decoder -> Proxy target network -> TD3 critic update

**Critical Path**
Replay buffer -> Proxy update (K=3 steps) -> SNN actor update -> Critic update -> Target update (proxy + critic)

**Design Tradeoffs**
- Proxy complexity vs. imitation accuracy
- Update frequency vs. training stability
- Neuron model complexity vs. training difficulty

**Failure Signatures**
- High variance across seeds: proxy updates not converging or tracking SNN
- Flat learning curves: surrogate gradient window incorrectly applied
- Unstable training: proxy-SNN MSE not decreasing during training

**First Experiments**
1. Verify LIF dynamics with population encoding/decoding on a simple task
2. Test proxy imitation accuracy on fixed SNN outputs before RL integration
3. Validate TD3 with proxy target on a single MuJoCo task

## Open Questions the Paper Calls Out

**Open Question 1**
The authors explicitly state the work "still remains at the simulation level" and the next step involves "implementing it on edge devices and enabling decisions-making in the real world." Real-world deployment introduces sensor noise, latency, and hardware-specific constraints that the current proxy training mechanism may not account for.

**Open Question 2**
Experimental validation is strictly limited to TD3. Different RL algorithms manage target networks and policy updates differently; it is unclear if the implicit imitation loss is compatible with stochastic policies or different value estimation techniques.

**Open Question 3**
The finding that "simple LIF neuron achieves the highest overall performance" suggests complex dynamics may "unnecessarily increase training difficulty." It is unclear if this finding is specific to the MuJoCo benchmarks used or represents a fundamental shift where training stability outweighs representational capacity.

**Open Question 4**
While the paper emphasizes "no additional inference overhead," the method requires optimizing an extra proxy network during every training iteration. The energy cost of training could be substantially higher than vanilla SNN frameworks, potentially offsetting efficiency gains.

## Limitations

- Simulation-only results; real-world deployment on neuromorphic hardware untested
- Framework validated only on TD3 algorithm, not other RL methods
- Energy comparisons based on MAC/SOP estimates rather than direct measurements
- Proxy network architecture details partially unspecified

## Confidence

**High**: Spiking neuron dynamics and surrogate gradient implementation are well-detailed
**Medium**: Proxy architecture details (hidden layer configuration) and weight initialization unspecified
**Medium**: Integration with TD3 soft target update mechanism not fully described

## Next Checks

1. Verify that proxy actor gradient updates (K=3 steps, lr=1e-3) are applied before each TD3 actor/critic update and that proxy-SNN MSE tracks low during training
2. Confirm membrane potential leakage λ=0.75 and threshold V_th=0.5 are correctly implemented in LIF dynamics
3. Test different proxy network hidden layer configurations (e.g., (512,512) vs (800,600)) to assess sensitivity to architecture choices