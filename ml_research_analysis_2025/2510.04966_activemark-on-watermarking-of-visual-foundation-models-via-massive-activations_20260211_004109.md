---
ver: rpa2
title: 'ActiveMark: on watermarking of visual foundation models via massive activations'
arxiv_id: '2510.04966'
source_url: https://arxiv.org/abs/2510.04966
tags:
- watermarking
- watermarked
- watermark
- foundation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ActiveMark, a watermarking method for visual
  foundation models (VFMs) that embeds binary signatures into hidden representations
  of input images. The approach uses "massive activations" to identify expressive
  layers where watermarks can be embedded without affecting model performance.
---

# ActiveMark: on watermarking of visual foundation models via massive activations

## Quick Facts
- arXiv ID: 2510.04966
- Source URL: https://arxiv.org/abs/2510.04966
- Authors: Anna Chistyakova; Mikhail Pautov
- Reference count: 7
- Primary result: Watermarking method achieves >90% detection rates after fine-tuning and 20% pruning, with false positive rates below 10⁻⁶

## Executive Summary
ActiveMark introduces a novel watermarking technique for visual foundation models that leverages "massive activations" in transformer blocks to embed binary signatures into hidden representations. The method identifies expressive layers where high-magnitude activations emerge, then trains lightweight encoder and decoder networks to inject and extract watermarks while fine-tuning only the latter portion of the VFM. Experimental results demonstrate robust watermark detection across downstream tasks including classification and segmentation, maintaining effectiveness even after 20% pruning.

## Method Summary
The approach identifies "expressive blocks" in visual foundation models where massive activations occur, then embeds 32-bit watermarks into these layers. An encoder network modifies the first channel of hidden representations, while a decoder extracts the watermark from the final output. The method trains these networks jointly with the latter VFM layers using a loss function balancing representation fidelity and watermark recovery. Watermark detection uses statistical thresholding based on bit-error rates, achieving false positive rates below 10⁻⁶ through binomial hypothesis testing.

## Key Results
- Detection rates exceed 0.9 after fine-tuning on downstream tasks like classification and segmentation
- Maintains watermark detection after 20% pruning (0.495 at 40% pruning)
- Achieves false positive rates below 10⁻⁶ when tested against independent models
- Outperforms general-purpose watermarking techniques in robustness and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Massive activations in specific transformer blocks provide stable embedding locations that survive downstream fine-tuning
- Mechanism: Certain blocks in VFMs produce unusually high-magnitude activations that dominate subsequent representations. Embedding watermarks at these "expressive blocks" ensures the signal propagates strongly through remaining layers and persists after task-specific fine-tuning
- Core assumption: High-activation regions are functionally important enough to be preserved during fine-tuning, yet tolerant to modification without degrading model performance
- Evidence anchors:
  - [abstract]: "uses 'massive activations' to identify expressive layers where watermarks can be embedded without affecting model performance"
  - [section 3.2.4]: "We hypothesize that these high-activation regions are suitable for watermark embedding due to their huge impact on the image representations in the subsequent blocks"
  - [corpus]: "A Refined Analysis of Massive Activations in LLMs" confirms massive activations as an active research area

### Mechanism 2
- Claim: Jointly training encoder, decoder, and latter VFM layers enables watermark injection while preserving output embeddings
- Mechanism: The encoder modifies the first channel of hidden representation p(x) to embed message m. Modified representation flows through trainable latter layers q, and decoder extracts m' from final output. Loss L = ||q(x) – q̃(x)||₂ + λ||m – m'||₁ balances fidelity and recoverability
- Core assumption: Modifying a single channel of hidden representation is sufficient to encode 32-bit messages without disrupting primary embedding space
- Evidence anchors:
  - [section 3.2.1]: "the objective function is L(x, d, e, q̃) = ||q(x) – q̃(x)||₂ + λ||m – m'||₁"
  - [section 4.1]: "The output of the encoder... replaces the first feature channel of p(x)"

### Mechanism 3
- Claim: Statistical thresholding based on bit-error rates provides reliable ownership verification with bounded false positive/negative rates
- Mechanism: Detection uses bit-distance ρ(m, m') between embedded and extracted messages. Threshold τ is set via binomial hypothesis testing to ensure FPR < ε. With n=32 bits and τ=5, achieves FPR < 10⁻⁶
- Core assumption: Bit collisions in independent models follow random Bernoulli distribution with low per-bit match probability r
- Evidence anchors:
  - [section 3.2.3]: "FPR₁ = P[ρ(m, m'(g,x)) < τ] = Σ(binomial terms)... we set up an upper bound for FPR₁ as ε"
  - [section 3.3.1]: Clopper-Pearson confidence intervals bound r(Ω_f|x) and r(Ξ|x)

## Foundational Learning

- Concept: **Vision Transformers (ViT) and patch embeddings**
  - Why needed here: ActiveMark operates on ViT-based VFMs (CLIP, DINOv2); understanding token/patch structure is essential for identifying expressive blocks
  - Quick check question: Can you explain how a ViT processes an image into patches and transformer blocks?

- Concept: **Massive activations in foundation models**
  - Why needed here: The entire method hinges on selecting layers where massive activations emerge; you must recognize these patterns
  - Quick check question: What characterizes "massive activations" versus normal layer outputs?

- Concept: **Watermarking vs. fingerprinting for IP protection**
  - Why needed here: The paper positions ActiveMark against fingerprinting methods; understanding the distinction clarifies design choices
  - Quick check question: How does watermarking (modifying model) differ from fingerprinting (no modification)?

## Architecture Onboarding

- Component map: Frozen early layers (p) -> Trainable latter layers (q) -> Decoder (d)
- Critical path:
  1. Run activation profiling (100 images, top-5 magnitudes per block) to identify expressive block num
  2. Initialize encoder/decoder, freeze layers < num, set trainable layers ≥ num
  3. Train with loss L, alternating between embedding fidelity and message recovery
  4. Validate on downstream tasks (classification with 3 schedulers, segmentation) and pruning (20%/40%)

- Design tradeoffs:
  - Earlier embedding blocks: Higher robustness to fine-tuning but lower detection accuracy (Table 1: block 11 has R(τ=0)=0.0)
  - Later embedding blocks: Better detection but vulnerable to output-space modifications (block 24: R=0.0)
  - Larger τ: More false positives but higher true positive rate on perturbed models
  - Larger λ: Stronger watermark recovery but potential degradation in embedding quality

- Failure signatures:
  - Watermark not recovered after fine-tuning: Check if expressive block shifted during training; re-profile activations
  - High false positives on independent models: Threshold τ too high; recompute FPR bound with tighter ε
  - Model performance degrades: λ too large or encoder overwrites critical features in first channel

- First 3 experiments:
  1. Activation profiling: Pass 100 random images through unmodified VFM, plot average top-5 magnitudes per block. Confirm explosion at block 12 (CLIP) or 18 (DINOv2)
  2. Ablation on embedding block: Embed watermarks at blocks 10, 12, 15, 21, 23. Measure R(τ=0) and R(τ=4) after fine-tuning on classification task. Verify block 12 provides best tradeoff
  3. False positive calibration: Extract watermarks from 100 independent models (different architectures, non-watermarked VFMs). Compute empirical FPR at τ=5; compare to theoretical bound < 10⁻⁶

## Open Questions the Paper Calls Out
- How can the robustness of the watermark be preserved under aggressive model pruning (e.g., 40% sparsity) where the current method's detection rate drops significantly?
- Can the proposed watermarking strategy be effectively adapted to Large Language Models (LLMs) or multi-modal foundation models which also exhibit massive activations?
- Is ActiveMark robust against model extraction attacks, specifically knowledge distillation, where an adversary trains a student model to mimic the watermarked model without stealing weights directly?

## Limitations
- Reliance on specific activation patterns in transformer blocks may not transfer across architectures beyond CLIP and DINOv2
- Theoretical FPR bound (<10⁻⁶) assumes uniform bit collision probability, which may not hold in practice
- Computational overhead for watermark extraction remains unspecified beyond "negligible"

## Confidence
- **High Confidence:** Detection rate metrics on downstream tasks (R>0.9 for τ=5), false positive bounds (<10⁻⁶), and pruning robustness (20% pruning maintains detection)
- **Medium Confidence:** Generalizability to other VFM architectures, scalability to larger message sizes (>32 bits), and computational overhead claims
- **Low Confidence:** Long-term robustness against adversarial watermark removal techniques and performance on non-ImageNet datasets

## Next Checks
1. Test ActiveMark on additional VFM architectures (e.g., MAE, BEiT) to verify expressive block detection generalizes beyond CLIP/DINOv2
2. Measure actual computational overhead for watermark extraction across different hardware platforms and compare to claimed "negligible" impact
3. Evaluate watermark persistence under more aggressive fine-tuning scenarios (20+ epochs) and different learning rate schedules to identify practical robustness limits