---
ver: rpa2
title: 'PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models
  on Puzzle Solving'
arxiv_id: '2504.10885'
source_url: https://arxiv.org/abs/2504.10885
tags:
- puzzle
- lmms
- visual
- evaluation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PuzzleBench, a fully dynamic evaluation framework
  for Large Multimodal Models (LMMs) based on automatically generated puzzle-solving
  tasks. The framework, called Open-ended Visual Puzzle Generation (OVPG), addresses
  limitations of static benchmarks by enabling continuous dataset refreshing through
  random sampling, puzzle rule design, and visual content generation modules.
---

# PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving

## Quick Facts
- arXiv ID: 2504.10885
- Source URL: https://arxiv.org/abs/2504.10885
- Reference count: 40
- Proprietary models like Gemini-2.0-Flash achieve highest performance (51.9% average accuracy)

## Executive Summary
PuzzleBench introduces a fully dynamic evaluation framework for Large Multimodal Models (LMMs) based on automatically generated puzzle-solving tasks. The framework addresses limitations of static benchmarks through Open-ended Visual Puzzle Generation (OVPG), enabling continuous dataset refreshing via random sampling, puzzle rule design, and visual content generation modules. Built upon OVPG, PuzzleBench comprises 11,840 VQA samples across six puzzle tasks targeting three core LMM competencies: visual recognition, logical reasoning, and context understanding.

Comprehensive experiments with 14 LMMs reveal that current models struggle with fine-grained visual recognition and image understanding, while the framework effectively mitigates data contamination issues through complete dynamism. The evaluation framework provides a scalable approach to assessing LMM capabilities in puzzle-solving contexts, with proprietary models like Gemini-2.0-Flash achieving the highest performance at 51.9% average accuracy, though all models demonstrate substantial room for improvement.

## Method Summary
PuzzleBench employs the Open-ended Visual Puzzle Generation (OVPG) framework to create dynamic puzzle datasets. The system integrates three core modules: random sampling for puzzle configuration, puzzle rule design for task logic, and visual content generation for image creation. This architecture enables continuous dataset refreshing, addressing data contamination concerns inherent in static benchmarks. The framework generates 11,840 VQA samples across six puzzle tasks: Icon Connect, Hanzi Matrix, Word Search, Grid Sum, Jigsaw, and Difference Hunt, targeting visual recognition, logical reasoning, and context understanding competencies.

## Key Results
- Current LMMs demonstrate struggles with fine-grained visual recognition and image understanding
- Proprietary models outperform open-source alternatives, with Gemini-2.0-Flash achieving 51.9% average accuracy
- The framework effectively mitigates data contamination through complete dynamism
- All evaluated models show substantial room for improvement despite being tested on generated puzzles

## Why This Works (Mechanism)
The framework's effectiveness stems from its dynamic generation capability, which creates fresh puzzle datasets that prevent memorization and contamination. By automating puzzle creation through OVPG, the system ensures continuous variation in visual content and task logic, making it difficult for models to rely on memorized patterns. The modular architecture allows for scalable puzzle generation while maintaining task diversity across different cognitive competencies.

## Foundational Learning
- **Dynamic dataset generation**: Required to prevent model contamination and ensure fair evaluation; quick check: verify generation produces non-repetitive, diverse samples
- **Visual puzzle task design**: Essential for testing specific LMM capabilities; quick check: confirm tasks target distinct cognitive skills
- **VQA sample creation**: Critical for structured evaluation; quick check: validate question-answer pairs maintain logical consistency
- **Model contamination mitigation**: Necessary for benchmark integrity; quick check: track performance trends over time to detect contamination
- **Competency-based task categorization**: Important for systematic capability assessment; quick check: ensure tasks map to intended cognitive domains
- **Synthetic data validation**: Required to ensure generated puzzles maintain educational value; quick check: compare model performance on synthetic vs natural puzzles

## Architecture Onboarding

**Component Map:**
OVPG Generator -> Puzzle Dataset -> LMM Evaluation -> Performance Analysis

**Critical Path:**
Random sampling module → Puzzle rule design → Visual content generation → Dataset compilation → Model testing → Accuracy measurement

**Design Tradeoffs:**
- Synthetic vs natural puzzle generation: synthetic enables complete control and dynamic refreshing but may miss real-world complexity
- Task diversity vs evaluation consistency: broader task coverage provides comprehensive assessment but complicates performance comparison
- Proprietary vs open-source model inclusion: proprietary models achieve higher performance but limit reproducibility
- Static vs dynamic benchmark design: dynamic prevents contamination but requires more complex infrastructure

**Failure Signatures:**
- Model performance plateaus indicate potential contamination
- Inconsistent accuracy across similar puzzle types suggests task design flaws
- Generation artifacts in visual content reveal sampling module issues
- Low variance in puzzle configurations indicates insufficient randomness

**First Experiments:**
1. Test single puzzle type across multiple models to establish baseline performance
2. Compare model performance on synthetic vs human-designed puzzles of identical complexity
3. Run temporal analysis tracking model accuracy over successive dataset generations

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on synthetic data may not fully capture real-world puzzle-solving complexity
- Framework focuses on specific puzzle types, potentially overlooking other critical LMM skills
- Performance metrics reflect generated puzzle performance rather than natural challenges
- Current models show fundamental limitations in visual recognition that dynamic benchmarking alone cannot address

## Confidence

**High Confidence:**
- OVPG framework successfully generates diverse puzzle datasets dynamically
- Current LMMs show consistent struggles with fine-grained visual recognition tasks
- Proprietary models outperform open-source alternatives on average

**Medium Confidence:**
- PuzzleBench effectively mitigates data contamination concerns through complete dynamism
- The six selected puzzle tasks adequately represent core LMM competencies
- The 11,840 sample size provides statistically meaningful evaluation results

**Low Confidence:**
- The specific performance gap between proprietary and open-source models will persist as models evolve
- OVPG-generated puzzles will maintain relevance as LMM capabilities advance
- The framework's scalability to other puzzle domains beyond the current six tasks

## Next Checks
1. Cross-dataset validation: Test the same LMMs on both OVPG-generated puzzles and human-designed puzzles of similar complexity to quantify the synthetic-to-real performance gap and identify systematic differences in model behavior.

2. Temporal contamination analysis: Conduct longitudinal studies tracking model performance over 6-12 months as new models are trained, measuring actual contamination rates and the framework's ability to maintain benchmark integrity.

3. Generalization testing: Evaluate LMM performance on puzzles that combine multiple task types (e.g., integrating visual recognition with logical reasoning within single puzzles) to assess whether specialized performance translates to integrated reasoning capabilities.