---
ver: rpa2
title: 'Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via
  Model Pruning'
arxiv_id: '2505.06454'
source_url: https://arxiv.org/abs/2505.06454
tags:
- sponge
- pruning
- attacks
- energy
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates sponge attacks on sensing AI models deployed
  on resource-constrained IoT devices, demonstrating that such attacks significantly
  increase energy consumption and inference latency while maintaining accuracy. The
  authors show that model pruning, a widely used compression technique, effectively
  mitigates these attacks by reducing activation density and improving computational
  efficiency.
---

# Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning

## Quick Facts
- arXiv ID: 2505.06454
- Source URL: https://arxiv.org/abs/2505.06454
- Reference count: 23
- Primary result: Model pruning effectively mitigates sponge attacks by reducing activation density and improving computational efficiency in resource-constrained sensing AI.

## Executive Summary
This paper investigates sponge attacks targeting sensing AI models deployed on resource-constrained IoT devices. The authors demonstrate that these attacks significantly increase energy consumption and inference latency while maintaining accuracy, creating vulnerabilities in systems where energy and latency are critical constraints. They propose model pruning as an effective defense mechanism, showing that weight pruning outperforms neuron pruning in reducing the energy and latency overhead caused by sponge attacks. Experiments on wearable sensing datasets (UCI HAR and MotionSense) reveal that 10% pruning yields substantial improvements while preserving model accuracy.

## Method Summary
The study employs DNNs with Adam optimizer (lr=0.0001, batch_size=64, epochs=100) trained on UCI HAR and MotionSense datasets. Sponge attacks modify the loss function to L_sponge = L_task - λE(θ,x), where E sums L0 approximations of activations per layer (λ=1, σ=1e-5). The defense applies post-training weight and neuron pruning at 10-50% to both clean and sponge-poisoned models. Energy consumption is measured using CodeCarbon, and prediction latency is recorded in seconds. The approach is evaluated across varying sponge sample percentages (0-100%) to assess attack effectiveness and defense robustness.

## Key Results
- Sponge attacks increase energy consumption and latency by 1.5-2x while maintaining accuracy
- Weight pruning is more effective than neuron pruning for reducing sponge attack overhead
- 10% weight pruning provides substantial energy and latency reduction while preserving accuracy
- Pruning effectiveness increases with higher sponge attack severity

## Why This Works (Mechanism)
The mechanism works because sponge attacks force models to maintain high activation density across layers, which directly translates to increased computational load. Model pruning reduces this vulnerability by eliminating weights and neurons that contribute to unnecessary activations, thereby decreasing the computational graph complexity. Weight pruning is particularly effective because it directly removes the parameters responsible for high activations, while neuron pruning may leave redundant connections that still consume resources.

## Foundational Learning
- **Sponge Attack Mechanism**: Adversarial poisoning that increases activation density while maintaining accuracy - needed to understand the threat model and why traditional defenses fail
- **L0 Norm Approximation**: Mathematical function to penalize non-zero activations in the loss - needed to implement the sponge attack correctly
- **Model Pruning Techniques**: Post-training weight and neuron removal methods - needed to apply the defense mechanism effectively
- **CodeCarbon Energy Measurement**: Tool for estimating model energy consumption - needed to quantify attack impact and defense effectiveness
- **Sliding Window Processing**: Time-series data segmentation for HAR - needed to preprocess the wearable sensor datasets
- **Distributed Computing Constraints**: Resource limitations in IoT deployments - needed to contextualize the practical significance of the findings

## Architecture Onboarding
- **Component Map**: Data Preprocessing -> DNN Training -> Sponge Attack Injection -> Pruning Application -> Evaluation
- **Critical Path**: The attack-injection and pruning application steps are critical, as they directly determine whether the defense can mitigate the energy-latency overhead
- **Design Tradeoffs**: Weight pruning offers better performance but may be more sensitive to attack patterns; neuron pruning is more structured but less effective
- **Failure Signatures**: Sharp accuracy drops with pruning indicate incorrect pruning criteria or insufficient finetuning; zero energy readings suggest CodeCarbon precision limits
- **First Experiments**: 1) Train baseline model without attacks to establish reference metrics; 2) Apply sponge attack at 50% sample rate to verify effectiveness; 3) Test 10% weight pruning on clean model to confirm baseline defense capability

## Open Questions the Paper Calls Out
- **Hardware Validation**: How do sponge attacks and pruning-based defenses perform on physical resource-constrained hardware compared to simulation environments? The authors plan to investigate this on actual IoT devices due to CodeCarbon measurement precision issues.
- **Distributed Multimodal Learning**: Is model pruning effective against sponge attacks in distributed multimodal learning environments? The current study is limited to single-modal wearable sensing data.
- **Adaptive Attackers**: Can adversaries bypass pruning defenses by crafting attacks that anticipate or exploit the specific sparsity structure induced by pruning? The current defense is evaluated against general attacks, not attack-specific pruning resistance.

## Limitations
- Energy measurements via CodeCarbon suffer precision limitations on modern GPUs, potentially affecting quantitative comparisons
- Exact DNN architecture and L0 approximation function details are not fully specified, limiting exact reproduction
- The study focuses on single-modal data, leaving multimodal and distributed scenarios unexplored

## Confidence
- High confidence in the general sponge attack mechanism and its impact on activation density
- Medium confidence in the relative effectiveness of weight vs. neuron pruning given stated trends
- Low confidence in absolute energy consumption numbers reported via CodeCarbon without independent verification

## Next Checks
1. Reconstruct the exact L0 norm approximation function ℓ̂0(·) and verify it matches the energy penalty behavior described in the paper
2. Independently measure model energy consumption using hardware power meters or alternative profiling tools to cross-validate CodeCarbon results
3. Test post-pruning finetuning at various rates to assess whether accuracy preservation at 10% pruning is achievable without it