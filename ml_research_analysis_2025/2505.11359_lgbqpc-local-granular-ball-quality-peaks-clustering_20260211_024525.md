---
ver: rpa2
title: 'LGBQPC: Local Granular-Ball Quality Peaks Clustering'
arxiv_id: '2505.11359'
source_url: https://arxiv.org/abs/2505.11359
tags:
- clustering
- density
- quality
- gb-pojg
- definition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LGBQPC, a novel density-based clustering algorithm
  designed to address limitations of the GB-based density peaks clustering (GBDPC)
  method. GBDPC struggles with complex clustering tasks, particularly those involving
  non-uniform density distributions and complex manifold structures.
---

# LGBQPC: Local Granular-Ball Quality Peaks Clustering

## Quick Facts
- arXiv ID: 2505.11359
- Source URL: https://arxiv.org/abs/2505.11359
- Authors: Zihang Jia; Zhen Zhang; Witold Pedrycz
- Reference count: 40
- Primary result: Novel density-based clustering algorithm addressing GBDPC limitations on non-uniform density distributions and complex manifold structures

## Executive Summary
LGBQPC introduces a density-based clustering algorithm that overcomes limitations of GB-based density peaks clustering (GBDPC). The method employs comprehensive improvements in both granular-ball (GB) generation and clustering processes. In the GB generation phase, an enhanced method called GB-POJG+ is developed, which refines the original GB-POJG in four key aspects: objective function, termination criterion, abnormal GB definition, and granularity level adaptation strategy. This simplification reduces parameter requirements while maintaining high-quality GB generation. In the clustering phase, LGBQPC introduces two key innovations: relative GB quality for density estimation and geodesic distance for GB distance metric, both based on the GB k-nearest neighbor graph.

## Method Summary
LGBQPC operates in two phases: GB generation and clustering. The GB-POJG+ generator builds a binary tree of GBs using 2-division, terminates when |X| ≤ 3√n, detects abnormal GBs, applies granularity adaptation, and outputs the final GB set Φ. The clustering phase constructs a GB k-NN graph using distance D, computes relative quality RQ and geodesic distances via Dijkstra, ranks GBs by decision value DV = RQ × DR_G, and assigns cluster labels through relative nearest neighbor propagation. The method uses standardized features and evaluates performance using NMI and ARI metrics.

## Key Results
- LGBQPC demonstrates superior performance compared to state-of-the-art clustering algorithms on 40 benchmark datasets
- The method substantially improves performance on datasets with complex manifold structures and non-uniform density distributions
- Relative GB quality and geodesic distance metrics enable robust density estimation under varying data conditions

## Why This Works (Mechanism)

### Mechanism 1
Penalizing the number of granular-balls (GBs) in the objective function balances GB quality against computational overhead. The objective function J*(Φ, λ) = ΣQ(Ω_Xi) - λt adds a penalty term proportional to the number of generated GBs, preventing over-division while maintaining coverage and specificity tradeoffs from the Principle of Justifiable Granularity.

### Mechanism 2
Relative GB quality enables robust density estimation under non-uniform density distributions. RQ(Ω_Xi) = Q(Ω_Xi) / (1/k Σ Q(Ω_Xj)) normalizes GB quality against local neighborhood averages, allowing cluster centers to be identified based on locally higher density rather than requiring globally higher density.

### Mechanism 3
Geodesic distance over the GB k-NN graph captures manifold structure that Euclidean distance between GB centers misses. D_G(Ω_Xi, Ω_Xj) computes shortest path length on the k-NN graph, following the manifold surface rather than cutting through low-density regions, preserving cluster connectivity for elongated or curved structures.

## Foundational Learning

- **Density Peaks Clustering (DPC)**: LGBQPC inherits DPC's core logic (cluster centers have higher local density and large distance from higher-density points) but operates on GBs instead of raw instances. Quick check: Can you explain why DPC requires both high density AND large distance from higher-density points to identify cluster centers?

- **Granular-Ball Computing Paradigm**: The algorithm builds on representing data as multi-granularity hyperspherical regions (GBs) rather than points, trading some precision for efficiency and robustness. Quick check: Given a GB defined as Ω_X = (X, c_X, R_ave^X, R_max^X), what does the average radius tell you about the compactness of instances in X?

- **Principle of Justifiable Granularity (POJG)**: GB quality Q(Ω_X) = Q_C · Q_S balances coverage against specificity, providing an unsupervised quality metric. Quick check: If you increase the granularity level γ in the specificity function f_2(t) = exp(-γ·t), would you expect more or fewer GBs to be generated?

## Architecture Onboarding

- **Component map**: GB-POJG+ Generator -> k-NN Graph Builder -> Density/Distance Calculator -> Decision Value Ranker
- **Critical path**: GB quality → k-NN graph structure → geodesic distances → cluster center identification. Errors in GB generation propagate through all subsequent stages.
- **Design tradeoffs**: Higher λ → fewer GBs → faster but coarser granularity (risk: merging distinct clusters); Higher k → denser k-NN graph → more robust geodesic paths (risk: bridging separate clusters)
- **Failure signatures**: Too many singleton GBs (λ too low); all GBs assigned to one cluster (k-NN graph disconnected); poor performance on specific manifold types (k inappropriately tuned)
- **First 3 experiments**:
  1. Run LGBQPC on Flame (D1) and Spiral (D4) datasets; visualize GB boundaries and k-NN graph
  2. Vary λ ∈ {0, 0.1, 0.2, 0.3} × 3√n and k ∈ {3, 5, 10, 15} on D35 Abalone; plot NMI/ARI vs. parameters
  3. Compare full LGBQPC against v6 and v8 on datasets with known non-uniform density (D7, D8)

## Open Questions the Paper Calls Out

### Open Question 1
How can GB generation strategies and clustering methods be enhanced for unlabeled high-dimensional data? The conclusion states it's worthwhile to investigate enhanced GB generation strategies for high-dimensional data, as current GB computing methods face challenges with such data.

### Open Question 2
Can the optimal number of nearest neighbors k for GBs in LGBQPC be automatically determined without parameter tuning? The paper suggests exploring strategies to automatically determine optimal k, which currently requires manual grid search within {1,...,20} or {1,...,30}.

### Open Question 3
Can the penalty coefficient λ be determined adaptively rather than through parameter search? The paper notes that different datasets require different optimal λ values determined through grid search, but no adaptive mechanism is proposed.

## Limitations
- Parameter stability remains uncertain, with no systematic analysis showing whether λ values optimal for GB generation remain optimal across diverse datasets
- Scalability validation is weak, as runtime comparisons against baseline DPC on raw data are absent
- Generalizability beyond curated benchmarks is limited, with real-world data challenges not represented in the test suite

## Confidence
- **High confidence**: Theoretical improvements to GB generation (penalty mechanism, abnormal GB detection) are clearly specified and build directly on established GB quality metrics
- **Medium confidence**: Relative quality and geodesic distance modifications are conceptually sound but rely on GB-POJG+ output quality, creating pipeline dependencies not fully validated through ablation studies
- **Low confidence**: Claims about computational efficiency improvements are weakly supported due to absent runtime comparisons against non-GB clustering methods

## Next Checks
1. **Ablation study**: Run LGBQPC with only relative quality (Euclidean distance), only geodesic distance (raw quality), and full method on 5 datasets with varying densities to quantify independent contributions
2. **Scalability test**: Compare LGBQPC runtime against standard DPC on datasets ranging from n=500 to n=50,000 instances to verify claimed efficiency gains
3. **Noise robustness test**: Add varying levels of Gaussian noise to clean benchmark datasets and measure degradation in NMI/ARI to assess real-world applicability