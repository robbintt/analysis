---
ver: rpa2
title: Self-Consuming Generative Models with Adversarially Curated Data
arxiv_id: '2505.09768'
source_url: https://arxiv.org/abs/2505.09768
tags:
- data
- eert
- curation
- curated
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of adversarially curated data
  on self-consuming generative models, where synthetic data is iteratively used for
  retraining. It theoretically analyzes how such data affects model evolution and
  identifies conditions for robustness or vulnerability.
---

# Self-Consuming Generative Models with Adversarially Curated Data

## Quick Facts
- arXiv ID: 2505.09768
- Source URL: https://arxiv.org/abs/2505.09768
- Reference count: 40
- This paper investigates how adversarially curated data affects self-consuming generative models and designs attacks to disrupt model alignment with user preferences.

## Executive Summary
This paper examines self-consuming generative models that iteratively retrain on synthetic data curated via user preferences. The authors theoretically analyze how adversarial data curation impacts model convergence and identify conditions for robustness or vulnerability. They design gradient-based attacks to strategically flip preference labels and misalign competitor models from true user preferences. Experiments on CIFAR-10/100 and synthetic data demonstrate that these attacks effectively reduce expected rewards while partial defenses like adding real data fail to restore preference alignment.

## Method Summary
The authors study self-consuming generative models where synthetic samples are iteratively generated, curated based on learned reward models from user preferences, and used for retraining. They analyze convergence conditions based on the covariance between true and adversarial rewards. To attack competitor models, they design a bi-level optimization problem that flips preference labels to minimize this covariance, using implicit differentiation to compute gradients through the reward model training process. Experiments compare benign vs. adversarial curation across multiple iterations, testing both pure synthetic and mixed real-adversarial datasets.

## Key Results
- Gradient-based label-flipping attacks effectively reduce expected rewards in self-consuming loops (avg. reward drops from ~6.5 to ~5.6 on CIFAR-10)
- Negative covariance between true and adversarial rewards prevents convergence to preference-optimizing distributions
- Adding real data partially mitigates distributional drift but fails to defend against preference misalignment attacks
- Mixed data defenses align models with real data distribution rather than user preferences, even at 4:1 real:adversarial ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convergence to user-preference-optimizing distributions is governed by the covariance between user rewards and adversarial rewards.
- Mechanism: In a self-consuming loop with curated data, the expected reward at iteration t+1 satisfies: E_{p_{t+1}}[e^{r(x)}] ≥ E_{p_t}[e^{r(x)}] + g_var + g_cov, where g_cov := φ_t(K-1)/K · Cov_{p_t}[e^{r(x)}, e^{r̃(x)}]. When Cov ≥ 0 (adversarial and true rewards positively correlated), the model converges to maximize expected reward. When Cov < 0, convergence is not guaranteed and may oscillate or diverge.
- Core assumption: Bounded rewards (Assumption 3.2: finite r_min, r_max, r̃_min, r̃_max); asymptotic K → ∞ for Lemma 3.1; global optimum at each update step.
- Evidence anchors:
  - [section] Lemma 3.3 (Page 3) provides the lower/upper bounds showing covariance governs convergence.
  - [section] "When Cov_{p_t} ≥ 0, the expected reward increases... highlighting the model's inherent robustness against noise and adversarial attacks" (Page 4).
  - [corpus] Related work "Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation" (arXiv:2511.09002) extends Ferbach et al. (2024) which this paper builds upon.
- Break condition: Cov_{p_t}[e^{r(x)}, e^{r̃(x)}] < 0 persistently across iterations; or adversarial fraction φ_t exceeds a threshold where negative covariance dominates the variance term.

### Mechanism 2
- Claim: Gradient-based label-flipping attacks can strategically create negative covariance to misalign models from user preferences.
- Mechanism: The attacker solves a bi-level optimization: min_δ J(δ) := Cov_{p_t}[e^{R_θ(x)}, e^{R̃_θ(x)}] + α·dist(R_θ, R̃_θ), where R̃_θ is learned from perturbed preference data. Using implicit differentiation: dθ̃/dδ = -[H_{θ̃}L]^{-1}[d∇_{θ̃}L/dδ], the attacker computes gradients w.r.t. discrete label flips (relaxed to continuous), then projects back to flip top κ·n labels by magnitude.
- Core assumption: Attacker has access to samples from target model's distribution p_t; can collect genuine user preferences on own platform; success rate κ of label flips on target platform is known or estimated.
- Evidence anchors:
  - [section] Section 4.1 (Pages 5-6) describes gradient-based method with implicit function theorem.
  - [section] Figure 2 (Page 7) shows gradient-based attack causes model to generate predominantly least-favored classes (horse, ship, frog) vs. benign curation favoring airplanes.
  - [corpus] Related work "Preference poisoning attacks on reward model learning" (Wu et al., 2025, cited in paper) is the closest prior work but targets static reward models, not self-consuming loops.
- Break condition: Budget constraint κ·n too small; penalty α too high (attacks become detectable); model architecture too complex for efficient Hessian computation.

### Mechanism 3
- Claim: Adding real data partially mitigates distributional drift but does not defend against preference misalignment attacks.
- Mechanism: With mixed real and synthetic data (Eq. 3), E_{p_{t+1}}[e^{r(x)}] ≥ E_{p_data}[e^{r(x)}] + φ*(λ+1)Cov_min as t→∞. Real data anchors the model near p_data, but if Cov_min < 0 (adversarial rewards anti-correlated with true rewards), expected reward can still fall below baseline—the model aligns with real data distribution, not user preferences.
- Core assumption: Constant adversarial fraction φ_t = φ* across iterations; Cov_min is the minimum covariance across all iterations.
- Evidence anchors:
  - [section] Lemma 3.4 (Page 4) provides the theoretical bound for mixed data.
  - [section] Figure 5 (Page 8) shows mixed data (1:1, 4:1, 1:4 real:adversarial) aligns with p_data but not user-preferred distribution.
  - [corpus] Prior works Bertrand et al. (2024), Gerstgrasser et al. (2024) showed real data prevents collapse; this paper shows it's insufficient for adversarial curation.
- Break condition: Real data fraction too low; or Cov_min sufficiently negative that the lower bound becomes vacuous.

## Foundational Learning

- Concept: **Bradley-Terry Preference Model**
  - Why needed here: Core to understanding how users (benign or malicious) select among K samples—probability of selection is proportional to e^{r(x_k)}/Σ_j e^{r(x_j)}.
  - Quick check question: If two samples have rewards r(x₁)=2 and r(x₂)=1, what's P(user selects x₁)?

- Concept: **Bi-level Optimization with Implicit Differentiation**
  - Why needed here: The attack optimizes δ where R̃_θ(δ) is itself an optimization output; requires implicit function theorem to compute gradients through the inner optimization.
  - Quick check question: Why can't we simply backprop through the inner training loop directly?

- Concept: **Model Collapse in Self-Consuming Loops**
  - Why needed here: Prior work established that training on synthetic data alone leads to collapse; this paper extends to *curated* synthetic data and adversarial perturbations.
  - Quick check question: Why does curating synthetic data based on user preferences (Ferbach et al.) avoid collapse, while random synthetic data causes it?

## Architecture Onboarding

- Component map: [Target Platform] -> Generate p_t -> [Adversarial Platform] -> Sample x_i, z_i ~ p_t -> Collect preferences o_i -> Learn R_θ, compute attack δ* -> Curate (φ_t malicious) -> Deploy malicious users -> Train p_{t+1} on curated data -> Loop

- Critical path:
  1. **Reward model training**: Train R_θ from pairwise preference data D = {(x_i, z_i, o_i)} using MLE (Eq. 7).
  2. **Attack computation**: For gradient-based method, compute ∇_δ J(δ) via implicit differentiation through R̃_θ training; iterate M gradient steps; select top κ·n labels.
  3. **Curated data selection**: Generate N samples from p_t; select βN based on reward scores (R_θ or R̃_θ).
  4. **Model retraining**: Train p_{t+1} on mixture (1/(1+λ))·p_data + (λ/(1+λ))·curated synthetic.

- Design tradeoffs:
  - **Gradient vs. heuristic attacks**: Gradient-based more effective (lower avg. reward 5.59-5.65 vs. heuristic 5.47-5.60 on CIFAR-10) but requires Hessian computation (expensive for deep networks). Heuristics faster but less globally optimal.
  - **Batch size vs. covariance estimation accuracy**: Larger batches improve covariance estimates but increase memory/compute.
  - **Penalty α**: Higher α makes attacks stealthier but less effective; lower α increases misalignment but risks detection.

- Failure signatures:
  - Random attack shows high variance (Figure 6): sometimes reward increases, sometimes decreases—unreliable.
  - Heuristic attacks leave preferred classes partially intact (Figure 8: automobile proportion higher than gradient methods).
  - Mixed real data doesn't restore preference alignment even at 4:1 real:adversarial ratio (Figure 5).

- First 3 experiments:
  1. **Baseline reproduction**: Run benign curation on CIFAR-10 with known reward function r(x) = 10 - I(x); verify expected reward increases monotonically per Ferbach et al.
  2. **Attack validation**: Implement gradient-based attack with κ=0.2, measure E_{p_t}[r(x)] over 10 iterations; confirm decline vs. benign baseline.
  3. **Defense stress test**: Test mixed data defense at λ∈{0.25, 1, 4}; measure both (a) alignment with p_data and (b) alignment with user preferences—expect (a) to improve but (b) to remain degraded.

## Open Questions the Paper Calls Out

- Question: Can effective defense mechanisms be designed to protect self-consuming generative models against adversarial data curation attacks?
- Basis in paper: [explicit] The authors explicitly state in the conclusion: "developing effective defense mechanisms could be a promising direction for future work."
- Why unresolved: The experiments show that adding real data only partially mitigates adversarial effects by aligning the model to pdata rather than user preferences, and outlier detection risks removing genuine minority preferences.
- What evidence would resolve it: Development and empirical validation of defense algorithms that maintain model alignment with user preferences under adversarial curation across multiple datasets and attack scenarios.

- Question: How do the theoretical convergence guarantees translate when model updates do not reach global optimum due to optimization noise, local minima, or limited training budgets?
- Basis in paper: [explicit] The limitations section states: "our theoretical analysis assumes that each model update converges to the global optimum... such convergence may not always hold in practice due to optimization noise, local minima, or limited training budgets."
- Why unresolved: The theoretical bounds in Lemma 3.3 and 3.4 assume optimal convergence, but real-world training involves practical constraints that may affect attack effectiveness and model robustness differently.
- What evidence would resolve it: Empirical studies measuring the gap between theoretical predictions and observed behavior under various optimization configurations and training budgets.

- Question: How can defense mechanisms distinguish between adversarial curation and legitimate preferences from heterogeneous user subgroups?
- Basis in paper: [inferred] The discussion notes that outlier detection "can inadvertently remove genuine preferences" from minority groups, potentially introducing biases when users are heterogeneous.
- Why unresolved: The penalty term dist(Rθ,ẽRθ) makes adversarial behavior difficult to detect, and legitimate diverse preferences may appear anomalous relative to majority preferences.
- What evidence would resolve it: Analysis of false positive rates in detecting adversarial curation across diverse user populations with varying preference distributions.

## Limitations

- Theoretical analysis assumes global optimum convergence, which may not hold in practice due to optimization noise and local minima.
- Empirical results are limited to CIFAR-10/100 and synthetic 2D data, limiting generalizability to complex domains.
- Attack feasibility depends on practical constraints like sampling from target model's distribution and obtaining preference labels on adversarial platform.

## Confidence

- **High confidence**: Theoretical convergence bounds (Lemma 3.3, 3.4) and basic attack mechanism are well-founded mathematically.
- **Medium confidence**: Empirical attack results on CIFAR-10/100 are convincing but limited in scope; mixed data defense effectiveness needs more systematic testing.
- **Low confidence**: Generalization claims to other generative model architectures and real-world deployment scenarios lack supporting evidence.

## Next Checks

1. **Reproduce the core attack**: Implement gradient-based label flipping with κ=0.2 on CIFAR-10; verify that expected reward decreases monotonically over 10 iterations compared to benign baseline.
2. **Stress-test the defense**: Systematically vary λ∈{0.25, 0.5, 1, 2, 4} and measure both data alignment (p_data vs. p_t) and preference alignment (user rewards) to quantify the tradeoff.
3. **Scale to larger models**: Test the attack on a larger generative model (e.g., Stable Diffusion) to evaluate scalability of the implicit differentiation method and attack effectiveness on higher-dimensional data.