---
ver: rpa2
title: Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering
arxiv_id: '2508.21081'
source_url: https://arxiv.org/abs/2508.21081
tags:
- entity
- number
- similarity
- clustering
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of clustering and normalizing entity
  names found in SWIFT payment messages, where manual entry introduces variations
  and noise. They propose a hybrid approach combining string similarity, topic modeling,
  hierarchical clustering, and rule-based criteria to group entity variations.
---

# Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering

## Quick Facts
- arXiv ID: 2508.21081
- Source URL: https://arxiv.org/abs/2508.21081
- Reference count: 24
- Outperforms baseline rule-based approach (67.8% AMI) with 83.0% AMI using combined features

## Executive Summary
This paper addresses the problem of clustering and normalizing entity names found in SWIFT payment messages, where manual entry introduces variations and noise. The authors propose a hybrid approach combining string similarity, topic modeling, hierarchical clustering, and rule-based criteria to group entity variations. Testing on a real-world dataset of 2.55k unique entity strings against 1,668 gold clusters achieved 81.7% Adjusted Mutual Information (AMI) with string similarity features and 83.0% AMI when combined with LSA, outperforming a baseline rule-based approach of 67.8% AMI.

## Method Summary
The method uses a two-stage feature extraction process: first, TF-IDF followed by LSA for topic modeling, and second, an m×m string similarity matrix using Gestalt Pattern Matching. These features are concatenated and fed into agglomerative clustering with a custom rule-based stopping criterion that only allows clusters to merge if their first or second tokens share at least 75% similarity. The approach treats variations of the same entity as belonging to the same latent "topic" and uses both string similarity and LSA to capture relationships beyond simple token overlap.

## Key Results
- 81.7% AMI achieved using string similarity features alone on 2.55k entity strings
- 83.0% AMI achieved when combining string similarity with LSA features
- Outperforms baseline rule-based approach of 67.8% AMI
- LSA features peak at ~77.2% AMI when components explain ~90% of dataset variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: String similarity matrices act as targeted "topic models" for entity variations, outperforming traditional text analytics on short, unstructured financial text.
- Mechanism: An m×m matrix of Gestalt Pattern Matching scores between all pairs of alphabetized token sets captures whether two entity strings likely represent the same underlying entity—treating high-similarity pairs as belonging to the same latent "topic" (the true entity).
- Core assumption: Variations of the same entity share sufficient token overlap that similarity scores will be high, while distinct entities will not.
- Evidence anchors: [abstract] "applies agglomerative clustering with rules to stop merging clusters based on token similarity" and [section III.A] "Intuitively, variations of the same entity are expected to have a high similarity index, allowing us to treat those strings as topics in a loose sense."

### Mechanism 2
- Claim: LSA topic modeling supplements string similarity by capturing relationships beyond token overlap, such as shared company names across different-length strings.
- Mechanism: LSA reduces the term-document matrix to a lower-dimensional space where co-occurring terms cluster together, enabling matches between sparse strings (company name only) and richer strings (company + address) that pure similarity would miss.
- Core assumption: Entities sharing meaningful terms (e.g., company names) will co-occur in the same latent topics, and these topics are discoverable via SVD at appropriate dimensionality.
- Evidence anchors: [abstract] "outperforming a baseline rule-based approach of 67.8% AMI... 83.0% AMI when combined with LSA" and [section III.F.4] "instances where one variation contained just the company name and another the same company name in a much longer string... were far apart in terms of string similarity, but topic features brought them much closer."

### Mechanism 3
- Claim: Rule-based stopping criteria in agglomerative clustering preserve interpretability and control false positives by preventing merges that violate domain constraints.
- Mechanism: During hierarchical clustering, two clusters merge only if their first or second tokens achieve ≥75% similarity (first-with-first, second-with-second, or crosswise). This encodes the domain knowledge that SWIFT entity names prioritize the first tokens for identity.
- Core assumption: The first two tokens in SWIFT tags are the most informative for entity identity; later tokens (addresses, secondary details) are less discriminative and more likely to cause false positives.
- Evidence anchors: [abstract] "retains most of the interpretability found in rule-based systems" and [section III.C.2] "allowing two clusters to be merged into one... only when their first or second tokens were of sufficient (75%) similarity."

## Foundational Learning

- Concept: **Gestalt Pattern Matching / String Similarity**
  - Why needed here: Core feature extraction method; must understand how longest common subsequence algorithms produce similarity scores between 0-1.
  - Quick check question: Given strings "ABC CORPORATION LTD" and "ABC CORP LTD", would you expect a high or low similarity score? Why does alphabetizing tokens before comparison help?

- Concept: **Latent Semantic Analysis (LSA)**
  - Why needed here: Supplements string similarity; must understand SVD-based dimensionality reduction and how explained variance relates to topic quality.
  - Quick check question: If LSA components beyond 90% variance hurt performance in this paper, what might those later components represent? How would you diagnose this on a new dataset?

- Concept: **Agglomerative (Hierarchical) Clustering**
  - Why needed here: Enables clustering without knowing the number of clusters; must understand linkage criteria, stopping rules, and the trade-off between merge aggressiveness and cluster purity.
  - Quick check question: What happens to precision and recall if you lower the token-similarity threshold for merging from 75% to 50%? What if you raise it to 90%?

## Architecture Onboarding

- Component map: Preprocessing -> Feature Family 1 (TF-IDF → LSA) + Feature Family 2 (String Similarity Matrix) -> Concatenation -> Agglomerative Clustering with Rule-Based Stopping
- Critical path: The string similarity matrix computation is the bottleneck at O(m² × n³) complexity. For 2.5k entities this is tractable; for 100k+ entities, requires chunking (e.g., by first letter) or distributed parallelization.
- Design tradeoffs:
  - Precision vs. Recall: Lower similarity threshold → higher recall, lower precision; higher threshold → opposite
  - LSA-only vs. Combined: LSA alone peaks at ~90% variance; combined models benefit from higher (~97.5%) variance
  - Bigrams vs. Unigrams: Bigrams added noise on this dataset, but may help on larger corpora with more overlap
  - Interpretability vs. Automation: Rule-based stopping preserves interpretability but may miss valid entity variations
- Failure signatures:
  - Over-clustering (low precision): Distinct entities merged; check if first/second token rule is too permissive or if geographic/address tokens are dominating topics
  - Under-clustering (low recall): Same entity split across clusters; check if first/second tokens differ (nickname vs. full name) or if string similarity threshold is too high
  - Scalability failure: Runtime explosion on large datasets; check if similarity matrix is being computed globally instead of chunked
- First 3 experiments:
  1. Replicate the baseline: Cluster by first-two-token grouping only; measure AMI against a manually-labeled sample to establish your own baseline.
  2. Ablation test: Run (a) string similarity features alone, (b) LSA features alone, (c) combined features. Compare AMI to identify which feature family contributes most on your data.
  3. Threshold sensitivity: Vary the rule-based stopping threshold (50%, 75%, 90%) and plot precision/recall/AMI curves to find the optimal operating point for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neural embedding methods, such as paragraph vectors or lda2vec, substitute LSA to improve accuracy and scalability in SWIFT entity clustering?
- Basis in paper: [explicit] The authors explicitly list paragraph embeddings and lda2vec as potential candidates to replace LSA for better accuracy and scalability in the "Future Work" section.
- Why unresolved: The current work relied on algebraic LSA because LDA produced overly generic topics; modern neural approaches were identified as promising but not yet tested.
- What evidence would resolve it: A benchmark comparing the hybrid pipeline’s performance using Doc2Vec or lda2vec against the current LSA baseline on the same labeled dataset.

### Open Question 2
- Question: Do higher-order n-grams (bigrams) improve clustering performance when applied to larger datasets where the number of data points exceeds the feature dimensions?
- Basis in paper: [inferred] The paper concludes bigrams failed to improve results, but the authors note the dataset was small (m < dimensions), limiting the number of components and potentially invalidating the negative result for larger corpora.
- Why unresolved: The dimensionality constraint of the specific dataset (2.55k lines vs 12k bigram dimensions) prevented the model from utilizing sufficient topics, leaving the utility of bigrams inconclusive.
- What evidence would resolve it: Re-running the combined feature experiment on a larger dataset where data points significantly outnumber bigram dimensions, showing AMI improvements over unigrams.

### Open Question 3
- Question: What is the optimal trade-off between computational cost and accuracy when replacing the Gestalt Pattern Matching algorithm with less expensive string matching metrics?
- Basis in paper: [explicit] The authors state that selecting a less expensive string-matching algorithm is necessary for scaling but acknowledge that "it needs to be tested for trade-off in accuracy."
- Why unresolved: The current O(m² × n³) complexity of the similarity matrix is a bottleneck for large bank datasets, but the impact of switching to cheaper approximations on the 81.7% AMI is unknown.
- What evidence would resolve it: Performance metrics (runtime vs. AMI) comparing the current string similarity baseline against approximate nearest neighbor or simpler edit-distance algorithms on a large-scale dataset.

## Limitations

- Methodology relies heavily on domain-specific heuristics (first/second token importance) without strong empirical validation across different entity types
- String similarity approach assumes token overlap reliably indicates entity identity, which may fail for entities with common names or organizational structures
- LSA component selection (90-97.5% variance) appears somewhat arbitrary and may not generalize across different datasets

## Confidence

**High Confidence:** The hybrid approach combining string similarity and topic modeling improves clustering performance compared to baseline methods (AMI improvement from 67.8% to 83.0%).

**Medium Confidence:** The specific implementation details work well for the tested dataset, but the methodology's generalizability to other entity types, languages, or financial message formats remains uncertain.

**Low Confidence:** The claim that first and second tokens are "most important" for entity identity in SWIFT messages lacks systematic validation across diverse entity types.

## Next Checks

1. **Dataset Diversity Test:** Apply the method to a different financial messaging dataset (e.g., SEPA or ACH payments) with distinct entity naming conventions to assess generalizability beyond SWIFT messages.

2. **Ablation Study on Rule Thresholds:** Systematically vary the token similarity threshold (50%, 75%, 90%) and measure the precision-recall trade-off to determine optimal operating points for different use cases.

3. **Cross-Lingual Validation:** Test the approach on multilingual entity datasets to evaluate whether alphabetizing tokens and string similarity calculations work equally well across languages with different tokenization rules and character sets.