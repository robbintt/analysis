---
ver: rpa2
title: 'From Token to Action: State Machine Reasoning to Mitigate Overthinking in
  Information Retrieval'
arxiv_id: '2505.23059'
source_url: https://arxiv.org/abs/2505.23059
tags:
- reasoning
- query
- retrieval
- action
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: State Machine Reasoning (SMR) addresses overthinking in LLM-based
  retrieval by replacing token-level CoT reasoning with discrete IR-specific actions
  (REFINE, RERANK, STOP) over structured query-document states. This structured approach
  eliminates redundant reasoning and prevents intent drift by detecting semantic equivalence
  between states.
---

# From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval

## Quick Facts
- arXiv ID: 2505.23059
- Source URL: https://arxiv.org/abs/2505.23059
- Authors: Dohyeon Lee; Yeonseok Jeong; Seung-won Hwang
- Reference count: 19
- Improves nDCG@10 by up to 3.4% while reducing inference token usage by 74.4% compared to CoT baselines

## Executive Summary
State Machine Reasoning (SMR) addresses overthinking in LLM-based retrieval by replacing token-level Chain-of-Thought reasoning with discrete IR-specific actions (REFINE, RERANK, STOP) over structured query-document states. This structured approach eliminates redundant reasoning and prevents intent drift by detecting semantic equivalence between states. Evaluated on BEIR and BRIGHT benchmarks, SMR demonstrates both superior retrieval quality and computational efficiency through early stopping and targeted state transitions.

## Method Summary
SMR frames retrieval reasoning as a Markov Decision Process over query-document state tuples $(q_t, D_t)$, where each step applies one of three discrete actions: REFINE (query rewriting conditioned on retrieved documents), RERANK (document reordering with hallucination filtering), or STOP (termination). The system iteratively updates states based on LLM-selected actions, with early stopping triggered when consecutive states are semantically equivalent. The approach uses prompt-based action selection without training, supporting multiple retrievers (BM25, ReasonIR, GPL) and LLMs (Qwen2.5-32B, QwQ-32B) while maintaining retrieval quality through context-grounded query refinement and structural constraints on document reranking.

## Key Results
- Improves nDCG@10 by up to 3.4% across BRIGHT and BEIR benchmarks
- Reduces inference token usage by 74.4% compared to CoT and compressed CoT baselines
- Maintains query intent alignment above 0.9 throughout iterative refinement steps
- Generalizes across retrievers (BM25, ReasonIR, GPL) and LLMs (Qwen2.5-32B-Instruct, QwQ-32B) without task-specific tuning

## Why This Works (Mechanism)

### Mechanism 1: Structured State Transitions for Redundancy Detection
Operating over explicit query-document state tuples allows detection of semantically equivalent states, enabling early termination of redundant reasoning. Instead of token-level generation that lacks a mechanism to recognize repetition, SMR represents each reasoning step as a structured state $s_t = (q_t, D_t)$ where $q_t$ is the current query and $D_t$ is the ranked document list. When a transition produces no change to either component, the system recognizes equivalence and terminates via STOP action, preventing cycles.

### Mechanism 2: Discrete Action Space for Intent Preservation
Constrained IR-specific actions (REFINE, RERANK, STOP) prevent intent drift by grounding each step in retrieval-relevant operations rather than open-ended token generation. Each action has defined inputs and outputs: REFINE rewrites the query based on current documents, RERANK reorders documents without query modification, STOP terminates. This prevents the model from generating outputs that diverge from retrieval objectives.

### Mechanism 3: Context-Grounded Query Refinement
Conditioning query refinement on both the preceding query and retrieved documents anchors generation to retrieval context, reducing semantic drift. The REFINE action generates $q_{t+1} = \text{LLM}_\text{REFINE}(q_t, D_t)$, meaning each refinement is grounded in actual retrieved evidence rather than abstract reasoning. This creates a feedback loop where poor retrieval results inform better query formulations.

## Foundational Learning

- **Markov Decision Process (MDP) Formulation**
  - Why needed here: SMR frames reasoning as state transitions over $(q_t, D_t)$ tuples, directly borrowing from MDP concepts (states, actions, transitions).
  - Quick check question: Can you explain how SMR's state representation differs from standard MDP state representations in RL?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: SMR is positioned as an alternative to CoT; understanding CoT's token-level generation and "overthinking" problem is essential.
  - Quick check question: What specific inefficiency does SMR address that standard CoT reasoning exhibits in IR tasks?

- **Query Rewriting and Document Reranking in IR**
  - Why needed here: These are the two core retrieval operations SMR exposes as discrete actions.
  - Quick check question: How does SMR's REFINE action differ from traditional query expansion techniques?

## Architecture Onboarding

- **Component map**:
  - Initial state $(q_0, D_0)$ from user query and retriever
  - Policy model (LLM) evaluates state and outputs action
  - Action executor applies transformation (REFINE→new query + retrieval; RERANK→reordered list; STOP→return)
  - Validation layer filters hallucinated documents during RERANK, re-appends missing documents
  - Loop back to policy model unless STOP or max_steps=16 reached

- **Critical path**:
  1. Initial state $s_0 = (q_0, D_0)$ from user query and retriever
  2. Policy model evaluates $(q_t, D_t)$ and outputs action with reason
  3. Action executor applies transformation (REFINE→re-retrieve, append new docs; RERANK→reorder; STOP→return)
  4. Validation applied to RERANK outputs
  5. Loop to step 2 unless STOP or max_steps=16 reached

- **Design tradeoffs**:
  - **max_steps=16**: Caps computation but may truncate beneficial deep reasoning; paper shows 25% of queries use 12+ steps
  - **Prompt-based policy vs. learned controller**: Zero training cost but may be less optimal than learned policies
  - **Three actions only**: Simple but may miss edge cases requiring other operations (filtering, aggregation)

- **Failure signatures**:
  - **Hallucinated documents**: RERANK outputs documents not in $D_t$ → filtered automatically
  - **Missing documents**: RERANK omits documents from $D_t$ → re-appended in original order
  - **Invalid action format**: Temperature incrementally raised by 0.1 until valid JSON produced
  - **No convergence**: Hard cap at 16 steps enforced

- **First 3 experiments**:
  1. **Baseline comparison on single domain**: Run SMR vs. Rank1 and O1-Pruner on one BRIGHT subset (e.g., Bio) with BM25 retriever, measure nDCG@10 and token count.
  2. **Action distribution analysis**: Log REFINE vs. RERANK proportions across high-difficulty vs. low-difficulty queries to validate adaptive policy behavior.
  3. **Ablation on action space**: Compare full SMR against [RERANK or STOP only], [REFINE or STOP only], and random action selection to isolate policy contribution.

## Open Questions the Paper Calls Out

- **Would extending the maximum reasoning steps beyond 16 yield continued performance gains, or does retrieval quality plateau?**
  - Basis in paper: [explicit] "While we enforce a maximum of 16 steps due to computational constraints, the upward trend indicates that further improvements may be possible with extended actions."
  - Why unresolved: The upward nDCG@10 curve at step 16 suggests potential for additional gains, but resource constraints prevented exploration of longer trajectories.
  - What evidence would resolve it: Experiments varying max_steps (e.g., 24, 32, 48) on BRIGHT tasks, measuring both performance gains and token costs to identify the point of diminishing returns.

- **Can incorporating user interaction signals (click-through rates, engagement metrics) into the state representation enhance SMR's reasoning fidelity?**
  - Basis in paper: [explicit] "Our current state representation is limited to representing the query and the top-k retrieved documents, and does not incorporate user interaction signals such as click-through rates or engagement metrics."
  - Why unresolved: The current state tuple (q, D) lacks behavioral feedback that could signal document relevance more accurately than LLM judgment alone.
  - What evidence would resolve it: Augmenting the state representation with simulated or real user interaction features and comparing retrieval performance against the baseline SMR.

- **Would a learned action selection policy (via RL or supervised learning) outperform the prompt-based LLM judgment approach?**
  - Basis in paper: [inferred] The paper uses prompt-based action selection without comparing to learned alternatives, noting only that it is "transparent, easily modifiable" without claiming optimality.
  - Why unresolved: The prompt-based approach may not capture optimal decision boundaries for action selection, potentially leaving performance gains unrealized.
  - What evidence would resolve it: Training a policy network on SMR trajectories with reward signals from retrieval quality, then comparing against prompt-based selection on held-out queries.

## Limitations

- The three-action constraint may prove insufficient for complex retrieval scenarios requiring document filtering, aggregation, or multi-hop reasoning beyond query refinement and reranking.
- The zero-shot prompt-based policy approach may underperform learned policies that adapt to specific retriever-query distributions.
- The evaluation focuses primarily on reasoning-intensive subsets of BRIGHT, potentially limiting generalizability to broader IR tasks.

## Confidence

**High Confidence Claims**:
- The 74.4% token reduction compared to CoT and compressed CoT is well-supported by the structured state-action design and early stopping mechanism.
- The 3.4% nDCG@10 improvement on BRIGHT is directly measurable from the experimental results across multiple retrievers and LLMs.
- The mechanism of preventing intent drift through grounded query refinement is demonstrated through the maintained intent alignment scores above 0.9.

**Medium Confidence Claims**:
- The generalizability across retrievers (BM25, ReasonIR, GPL) is supported but limited to the specific implementations tested.
- The computational efficiency gains are well-demonstrated but depend on the specific hardware and inference configurations not fully detailed.
- The superiority over existing methods (Rank1, Rank-R1, O1-Pruner) is established but the baselines may not represent state-of-the-art.

**Low Confidence Claims**:
- The assertion that three actions are sufficient for all useful reasoning space in IR remains unproven and may break for complex multi-hop queries.
- The claim that SMR "eliminates" redundant reasoning assumes perfect state equivalence detection, which may fail on semantically similar but token-distinct states.
- The long-term robustness of the prompt-based policy without fine-tuning is speculative for production deployments.

## Next Checks

1. **Ablation Study on Action Space Sufficiency**: Implement and evaluate SMR variants with restricted action spaces (e.g., REFINE-only, RERANK-only) versus the full three-action setup on the Bio and DeepMath subsets of BRIGHT. Measure whether the full action space consistently outperforms restricted versions across different query difficulties, establishing whether all three actions are necessary or if one dominates.

2. **State Equivalence Detection Robustness**: Systematically introduce semantically equivalent but token-distinct queries (e.g., "What is X?" vs. "Explain X") and document orderings that preserve semantic ranking but alter presentation. Test whether SMR's early stopping mechanism correctly identifies these as equivalent states or prematurely terminates reasoning that could benefit from additional steps.

3. **Performance Under Extreme Initial Retrieval Failure**: Design adversarial queries where initial BM25 or ReasonIR retrieval returns zero relevant documents. Evaluate whether SMR's REFINE action can recover meaningful results through iterative refinement, or whether the grounding in poor initial documents leads to compounding degradation. Compare success rates against baseline methods under these conditions.