---
ver: rpa2
title: Autoformalizer with Tool Feedback
arxiv_id: '2510.06857'
source_url: https://arxiv.org/abs/2510.06857
tags:
- tool
- consistency
- arxiv
- check
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Autoformalizer with Tool Feedback (ATF) addresses the challenge
  of generating valid formal mathematical statements by integrating syntax and consistency
  checking tools into the formalization process. The method employs Lean 4 compilers
  for syntax validation and a multi-LLMs-as-judge approach for semantic consistency,
  allowing the model to iteratively refine statements based on tool feedback.
---

# Autoformalizer with Tool Feedback

## Quick Facts
- arXiv ID: 2510.06857
- Source URL: https://arxiv.org/abs/2510.06857
- Reference count: 38
- ATF-32B achieves 94.51% Pass@1 consistency on FormalMath-Lite, 89.78% on ProverBench, and 65.38% on CombiBench

## Executive Summary
Autoformalizer with Tool Feedback (ATF) addresses the challenge of generating valid formal mathematical statements by integrating syntax and consistency checking tools into the formalization process. The method employs Lean 4 compilers for syntax validation and a multi-LLMs-as-judge approach for semantic consistency, allowing the model to iteratively refine statements based on tool feedback. ATF is trained through a three-phase pipeline: cold-start on synthetic tool-calling data, expert iteration for formalization capability enhancement, and Direct Preference Optimization to reduce ineffective revisions. Experimental results show ATF-32B achieves significant improvements over baseline formalizers, with 9.1-29.13% gains across benchmarks.

## Method Summary
ATF trains a Qwen3-32B model through three phases: (1) cold-start SFT on synthetic tool-calling data generated by Claude-4-Sonnet from NuminaMath-1.5, (2) expert iteration with filtered successful trajectories (<8 revisions), and (3) DPO fine-tuning on revision preference pairs with revision difference ≥3. The approach uses Lean 4 compilers for syntax validation and an ensemble of QWQ-32B and Qwen3-32B models for semantic consistency checking, with inconsistency decisions defaulting to the more conservative judgment.

## Key Results
- ATF-32B achieves 94.51% Pass@1 consistency on FormalMath-Lite
- ATF-32B achieves 89.78% Pass@1 consistency on ProverBench
- ATF-32B achieves 65.38% Pass@1 consistency on CombiBench
- ATF-32B outperforms baselines by 9.1%, 10.08%, and 29.13% respectively
- Method successfully synthesizes 750K formal statements from NuminaMath-1.5 queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Externalizing validity checking to dedicated tools compensates for LLMs' limited formal language knowledge during generation.
- Mechanism: The model generates an initial formalization, receives concrete compiler error messages or semantic misalignment explanations from tools, and revises based on this feedback. This creates an error-correction loop where the model learns to map error patterns to fixes rather than needing perfect internal formalization knowledge.
- Core assumption: Models can learn to interpret and respond to tool feedback more reliably than they can generate correct formalizations in a single pass.
- Evidence anchors:
  - [abstract] "integrating Lean 4 compilers for syntax validation and a multi-LLMs-as-judge approach for semantic consistency, allowing the model to iteratively refine statements based on tool feedback"
  - [Section 3.1] Tool design separates syntax check (Lean 4 compiler) from consistency check (multi-LLM judge)
  - [Table 4] Without tools, consistency drops from 65.38% to 23.69% on CombiBench
- Break condition: If tool feedback is noisy, uninformative, or too slow, the refinement loop degrades. The paper notes consistency check success rate drops from 69.5% on first attempt to 8.8% on eighth attempt, suggesting diminishing returns.

### Mechanism 2
- Claim: Multi-LLM-as-judge ensemble reduces false positive rate in semantic consistency detection compared to single-model evaluation.
- Mechanism: Two models (QWQ-32B and Qwen3-32B) independently judge consistency; disagreement defaults to "inconsistent." This trades recall for precision, reducing false positives from ~9% to ~6%.
- Core assumption: The cost of rejecting valid formalizations (lower recall) is acceptable given the higher confidence in accepted outputs.
- Evidence anchors:
  - [Section 3.1.2] Table 1 shows ensemble FPR of 0.0579 vs. 0.0829-0.0950 for individual models
  - [Section 4.2] Human evaluation correlation of 0.746 validates tool reliability
  - [corpus] Related work (FormaRL, ReForm) addresses autoformalization but doesn't systematically benchmark LLM-as-judge reliability for subtle semantic errors
- Break condition: Ensemble voting fails when both models share systematic blind spots for specific error types. The 800-sample benchmark may not cover all failure modes.

### Mechanism 3
- Claim: DPO on revision trajectories reduces ineffective iterations without degrading final output quality.
- Mechanism: Training pairs are constructed from same-query trajectories with different revision counts (difference ≥3). The model learns to prefer shorter successful paths, internalizing more efficient revision strategies.
- Core assumption: Revision count correlates with inefficiency, not exploration value; shorter paths aren't systematically lower quality.
- Evidence anchors:
  - [Section 3.2] DPO uses Eq. 1 with NLL loss on chosen trajectories to prevent reward decline
  - [Table 4] DPO adds +1.5% consistency improvement over expert iteration alone on CombiBench
  - [corpus] Limited corpus evidence for DPO specifically in autoformalization; this appears to be a novel application
- Break condition: If shorter trajectories systematically miss edge cases that longer trajectories catch, DPO could reduce robustness. Paper doesn't analyze quality differences between short and long successful paths.

## Foundational Learning

- **Lean 4 syntax and type system**
  - Why needed here: Understanding compiler error messages and why statements fail to compile requires familiarity with Lean's dependent type theory, universe levels, and implicit argument handling.
  - Quick check question: Given a Lean error "type mismatch," can you distinguish between a type class instance problem vs. a universe level mismatch?

- **Semantic equivalence in formal mathematics**
  - Why needed here: The consistency check tool evaluates whether a formal statement captures the same mathematical meaning as natural language, which requires understanding how quantifiers, domain constraints, and logical structure translate.
  - Quick check question: If a problem states "prove that for all positive integers x, P(x) holds," would `∀x : ℕ, P(x)` be semantically equivalent? Why or why not?

- **Direct Preference Optimization (DPO)**
  - Why needed here: The third training phase uses DPO to shape revision behavior. Understanding the loss function (Eq. 1) and why NLL regularization is added requires familiarity with preference learning.
  - Quick check question: Why does DPO need a reference model, and what happens if the chosen and rejected trajectories are too similar?

## Architecture Onboarding

- Component map:
  - Syntax Check Tool: Pre-check stage (filters obvious errors) → Grouped Lean 4 execution (batch compilation) → Error message extraction
  - Consistency Check Tool: Informal-formal pair input → Dual LLM evaluation (QWQ-32B + Qwen3-32B) → Ensemble voting logic
  - ATF Model: Qwen3-32B base → SFT (cold-start + expert iteration) → DPO fine-tuning
  - Training Pipeline: Synthetic tool-calling data → Expert iteration with filtered trajectories → DPO pairs from self-sampling

- Critical path:
  1. Tool implementation must achieve <1s per syntax check (grouped execution achieves 0.808s vs 6.2s individual)
  2. Cold-start data must teach tool invocation format (tool calls wrapped in specific tags)
  3. Expert iteration requires filtering successful trajectories (revision attempts <8)
  4. DPO pair selection needs revision count difference ≥3

- Design tradeoffs:
  - **Ensemble strictness vs. recall**: Multi-LLM voting reduces FPR but increases FNR (40.33% false negative rate)
  - **Inference cost vs. quality**: More revision attempts improve results (Fig. 4a) but increase latency
  - **Training data filtering**: Decontamination (cosine similarity >0.8 removed) may exclude useful near-duplicates

- Failure signatures:
  - **Consecutive identical errors**: Model repeats same syntax error without progress (Section 3.2 mentions this motivates DPO)
  - **Consistency check rejection cascade**: After multiple revisions, success rate drops sharply (8.8% by 8th attempt)
  - **Tool call format drift**: Model generates malformed tool invocations if cold-start data insufficient

- First 3 experiments:
  1. **Validate syntax check tool latency**: Run grouped Lean 4 execution on 10K statements; verify average time <1s and error messages are parseable
  2. **Benchmark consistency check reliability**: Sample 100 informal-formal pairs, run both LLM judges independently, compare to human labels; target FPR <0.06
  3. **Cold-start data quality check**: Train base model on synthetic 24K trajectories, sample 100 generations; verify tool call format compliance >95% and at least one revision occurs in >50% of cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-LLMs-as-judge consistency check be improved to reduce the ~6% false positive rate while maintaining high precision?
- Basis in paper: [explicit] The authors note the ensemble vote achieves FPR below 6%, and acknowledge that the strictness results in "some sacrifices in recall" (Table 1 shows recall of 0.5967 for ensemble).
- Why unresolved: The paper benchmarks current LLM judges but does not propose methods to improve their discriminative ability for subtle semantic misalignments.
- What evidence would resolve it: Development of a consistency checker achieving both FPR < 3% and recall > 80% on the perturbation benchmark.

### Open Question 2
- Question: What causes the declining consistency check success rate across revision attempts (69.5% → 8.8%), and can models be trained to maintain effectiveness in later revisions?
- Basis in paper: [explicit] The authors observe: "the consistency check success rate consistently decreases with increasing revision attempts across all datasets from 69.5% on the first attempt to 8.8% on the 8th attempt" (Section 5.2).
- Why unresolved: The paper documents this phenomenon but does not investigate its causes or propose mitigation strategies.
- What evidence would resolve it: Analysis showing whether failures stem from exhausted strategies, compounding errors, or model uncertainty, plus training interventions that flatten this degradation curve.

### Open Question 3
- Question: Can the ATF framework generalize effectively to other formal languages (e.g., Isabelle, Coq) beyond Lean 4?
- Basis in paper: [inferred] The paper explicitly mentions "significant variations between different versions of formal languages (e.g. Lean 4 v.s. Lean 3), the trained formalizer often lacks generalizability across versions" but only evaluates on Lean 4.
- Why unresolved: The tool integration approach is language-agnostic in principle, but the training data and tool implementations are Lean 4-specific.
- What evidence would resolve it: ATF-style models trained with Isabelle or Coq tools achieving comparable syntactic validity and semantic consistency improvements over baselines.

## Limitations

- The consistency check tool has a 40.33% false negative rate, potentially rejecting valid formalizations
- The cold-start data generation relies on a single LLM (Claude-4-Sonnet) without systematic error analysis
- Expert iteration lacks clear stopping criteria and may overfit to specific error types
- The 10K DPO pairs from self-sampling may not capture diverse revision failure modes

## Confidence

**High Confidence:**
- ATF-32B achieves consistent improvements over baseline formalizers across all three benchmarks
- The three-phase training pipeline produces measurable gains
- Tool integration significantly improves consistency rates compared to no-tool baselines
- The method successfully synthesizes 750K formal statements from NuminaMath-1.5

**Medium Confidence:**
- Multi-LLM-as-judge ensemble reduces false positive rate while accepting higher false negative rate
- DPO on revision trajectories improves efficiency without degrading final output quality
- ATF-32B demonstrates better scaling properties than baselines
- The 9.1-29.13% improvement margins are statistically significant

**Low Confidence:**
- The method's generalization to domains beyond competition mathematics
- Long-term robustness of tool feedback mechanisms for unseen error patterns
- Whether the conservative consistency check design is optimal for practical deployment
- The 800-sample benchmark fully captures all types of semantic inconsistencies

## Next Checks

1. **Tool Reliability Audit**: Run 1,000 random informal-formal pairs through both syntax and consistency check tools independently, then conduct blind human evaluation to measure false positive and false negative rates, particularly focusing on edge cases where tools disagree.

2. **DPO Pair Quality Analysis**: Sample 100 successful ATF-32B revisions and compare them to the original natural language problems. Measure whether shorter revision paths (preferred by DPO) systematically miss important mathematical nuances compared to longer paths.

3. **Cross-Domain Transfer Test**: Apply ATF-32B to 100 problems from non-competition sources (textbooks, research papers, open problems) and measure Pass@1 rates. Compare performance degradation to baseline formalizers to quantify domain-specific learning.