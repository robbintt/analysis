---
ver: rpa2
title: Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object
  Recognition
arxiv_id: '2602.00360'
source_url: https://arxiv.org/abs/2602.00360
tags:
- sentiment
- image
- text
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEMSA, a novel approach for multimodal sentiment
  analysis that leverages textual cues derived from object recognition to improve
  sentiment classification. The method extracts object names from images and combines
  them with associated text data (TEMS) to address challenges posed by modality differences
  and contextual ambiguity.
---

# Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition

## Quick Facts
- arXiv ID: 2602.00360
- Source URL: https://arxiv.org/abs/2602.00360
- Reference count: 40
- Primary result: TEMS improves sentiment classification accuracy by 10-15 percentage points over unimodal approaches when all detected object names are included

## Executive Summary
This paper introduces TEMSA, a novel approach for multimodal sentiment analysis that leverages textual cues derived from object recognition to improve sentiment classification. The method extracts object names from images and combines them with associated text data (TEMS) to address challenges posed by modality differences and contextual ambiguity. Four experiments were conducted on two datasets (SIMPSoN and MVSA-Single), comparing image-only, text-only, and TEMS-based multimodal analysis. Results show that TEMS significantly outperforms individual modalities when all detected object names are included, with BERT-based TEMS achieving accuracy improvements of up to 10-15 percentage points over unimodal approaches. The approach demonstrates statistical significance (p < 0.05) and addresses data format dissimilarities by converting visual information to text. Limitations include relatively small dataset sizes, with future work planned for larger datasets and additional visual features.

## Method Summary
TEMSA converts visual content to textual cues by extracting object names from images using DETR and Faster R-CNN, then concatenating these names with caption/superimposed text to form TEMS. The approach addresses modality dissimilarity by unifying both modalities into a single textual representation processed by BERT. Object detection uses COCO (91 classes) and Visual Genome (200 categories) with ResNet101 backbone. TEMS preprocessing removes non-alphanumeric symbols, lowercases, tokenizes, and pads/truncates to 75 words for SIMPSoN and 41 for MVSA-Single, with up to 20 object names. Models include BERT-Base (lr=6e-06, Dense-1024-ReLU-Dropout-Softmax), BiLSTM with GloVe/BERT embeddings, and image models (VGG16/VGG19/ResNet50). Training uses 80/20 split, batch size 32, 10 epochs, Adam optimizer, dropout 0.1.

## Key Results
- BERT with TEMS achieved 79% accuracy on SIMPSoN vs 69% for text-only BERT
- BERT with TEMS achieved 84% accuracy on MVSA-Single vs 70% for text-only BERT
- Including all detected object names improved accuracy by 17 percentage points compared to single-object selection
- Results demonstrate statistical significance (p < 0.05) across all experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting visual information to textual format addresses modality dissimilarity in multimodal sentiment analysis.
- Mechanism: Object detection models (DETR, Faster R-CNN) extract object names from images, which are concatenated with caption/superimposed text to form TEMS. This unifies both modalities into a single textual representation that BERT processes.
- Core assumption: Object names carry sufficient sentiment-relevant information from images to supplement textual context.
- Evidence anchors: [abstract] "Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination TEMS." [section 1] "These additional details about the image could be valuable... to bridge the gap between these dissimilar modalities." [corpus] Related work (AdaptiSent, BERT-DINOv2 Approach) similarly addresses modality alignment through attention mechanisms, suggesting cross-modal integration is an active research direction, though TEMSA's text-conversion approach differs.
- Break condition: If images contain sentiment-critical visual features (colors, expressions, spatial relationships) that object names cannot capture, performance will plateau below vision-native multimodal methods.

### Mechanism 2
- Claim: BERT's self-attention mechanism creates relationships between detected object names and caption text, enabling joint sentiment inference.
- Mechanism: Self-attention computes attention weights for each word based on proximity to sentiment-related target words, allowing detected objects to contextualize caption text and vice versa.
- Core assumption: Object names and caption text share semantic relationships that BERT's pre-trained representations can exploit.
- Evidence anchors: [section 3.4] "Self-attention mechanisms enable BERT to simultaneously assess the relationships between all words in the input sequence. Thus, the detected objects' names make a relation with the caption or superimposed text." [results] BERT with TEMS achieved 79% (SIMPSoN) and 84% (MVSA-Single) accuracy vs. 69% and 70% for text-only BERT. [corpus] Corpus evidence is limited; no direct comparison studies validate self-attention specifically for object-text fusion in sentiment analysis.
- Break condition: If caption text and object names have no semantic overlap or contradictory sentiment signals, self-attention may amplify noise rather than signal.

### Mechanism 3
- Claim: Including all detected object names yields better sentiment classification than single-object selection.
- Mechanism: Aggregating multiple object names provides richer contextual information about image content, compensating for short or ambiguous captions.
- Core assumption: All detected objects contribute relevant sentiment information; no object acts as pure noise.
- Evidence anchors: [abstract] "Results demonstrate that only TEMS improves the results when considering all the object names." [Table 6 vs Table 7] All-object TEMS: 79% accuracy (SIMPSoN); single-object TEMS: 62% accuracy—a 17-point degradation. [corpus] Multi-Granular Multimodal Clue Fusion paper similarly emphasizes multi-level feature aggregation, supporting the principle that richer cues improve multimodal understanding.
- Break condition: If images contain many sentiment-irrelevant objects (background clutter), including all may dilute signal; filtering thresholds may be needed.

## Foundational Learning

- **Object Detection Fundamentals (DETR, Faster R-CNN)**
  - Why needed here: Understanding how object names are extracted, confidence thresholds, and detection limitations (e.g., 36% of SIMPSoN images had zero detected objects) is critical for diagnosing TEMS quality.
  - Quick check question: Given an image with no detected objects, what fallback strategy should TEMSA employ?

- **BERT Input Representation & Tokenization**
  - Why needed here: TEMS concatenation increases sequence length (55→75 words for SIMPSoN); understanding BERT's 512-token limit and padding/truncation strategy is essential for implementation.
  - Quick check question: If TEMS exceeds 512 tokens, which components should be prioritized—caption text or object names?

- **Multimodal Sentiment Labeling Schemes**
  - Why needed here: The paper uses three label types (Image, Text, Joint); understanding when labels align or contradict determines training data quality.
  - Quick check question: For MVSA-Single, why were image-positive/text-negative pairs excluded from Joint labels?

## Architecture Onboarding

- **Component map:**
  - P1: Object Detection Module (DETR + Faster R-CNN with ResNet101 backbone)
  - P2: TEMS Constructor (text preprocessing + object name concatenation)
  - P3: Image-only classification (VGG16/19, ResNet50, ViT) — baseline comparison
  - P4: Text/TEMS classification (BiLSTM with GloVe, BERT-Base) — primary models

- **Critical path:**
  1. Image input → Object detection → Extract all object names
  2. Preprocess caption text (remove non-alphanumeric, lowercase, tokenize)
  3. Concatenate: [caption text] + [object names] → TEMS
  4. TEMS → BERT with custom classification head (Dense 1024 → Dropout 0.1 → Softmax)
  5. Train with Joint labels, Adam optimizer (lr=6e-06), batch size 32, 10 epochs

- **Design tradeoffs:**
  - Text-conversion vs. visual feature fusion: TEMSA sacrifices visual feature richness (textures, colors, spatial layout) for modality unification simplicity.
  - All objects vs. filtered objects: Current design includes all; no sentiment-relevance filtering is applied.
  - Dataset size: SIMPSoN (2,830 samples) and MVSA-Single (2,486 joint-labeled samples) are small; generalization to larger datasets is unproven.

- **Failure signatures:**
  - Zero detected objects (36% of SIMPSoN): TEMS degrades to caption-only text.
  - Long captions with many objects exceeding token limits: Truncation may lose sentiment-critical content.
  - Contradictory sentiment signals (image positive, text negative): Current approach excludes these; model has no robustness to such cases.

- **First 3 experiments:**
  1. **Baseline validation:** Run image-only VGG16 and text-only BERT on your dataset to establish unimodal performance floors.
  2. **TEMS ablation:** Compare all-object TEMS vs. single-object TEMS to quantify the contribution of object quantity.
  3. **Empty-object handling:** Evaluate TEMSA performance on the subset of images with zero detected objects to assess fallback robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- The merging strategy for object names from DETR (91 classes) and Faster R-CNN (Visual Genome) is underspecified, potentially affecting TEMS consistency across runs
- 36% rate of zero-detected objects in SIMPSoN means nearly 1,000 samples rely solely on caption text, which may artificially inflate TEMS performance
- Absence of filtering for sentiment-irrelevant objects means TEMS could be diluted by background clutter

## Confidence

- **High confidence** in the finding that including all detected object names outperforms single-object selection (p < 0.05, 17-point accuracy gain), as this is directly supported by ablation results in Tables 6 and 7 with clear statistical significance.
- **Medium confidence** in the overall 10-15 percentage point improvement of TEMS over unimodal approaches, as results are consistent across two datasets but limited by relatively small sample sizes (n=2,486 and n=2,830) and lack of cross-dataset validation.
- **Low confidence** in the mechanism claim that self-attention creates meaningful relationships between object names and caption text, as this relies on BERT's general self-attention properties without direct experimental validation or ablation studies isolating attention patterns.

## Next Checks

1. **Object Detection Quality Audit**: Run DETR and Faster R-CNN on a stratified sample of 100 images from SIMPSoN and MVSA-Single, manually verify detected object relevance to sentiment, and measure inter-detector agreement to quantify noise in the TEMS pipeline.

2. **Empty-Object Performance Analysis**: Train and evaluate separate models on the subset of images with zero detected objects (36% of SIMPSoN) to determine whether TEMS degradation to caption-only text explains the reported performance gains.

3. **Cross-Dataset Generalization Test**: Apply the trained TEMSA model to an external multimodal sentiment dataset (e.g., ImageSentiment, MVSA-Multiple) without fine-tuning to assess whether performance improvements generalize beyond the original training domains.