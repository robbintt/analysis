---
ver: rpa2
title: Leverage Unlearning to Sanitize LLMs
arxiv_id: '2510.21322'
source_url: https://arxiv.org/abs/2510.21322
tags:
- data
- unlearning
- information
- sensitive
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes SANI, a machine unlearning approach to sanitize
  large language models (LLMs) by removing sensitive information from their memorization.
  The core method consists of two phases: an erasure phase that resets neurons in
  the last layer of the model to disrupt fine-grained memorization, followed by a
  repair phase that fine-tunes the model while avoiding memorization of blacklisted
  terms.'
---

# Leverage Unlearning to Sanitize LLMs

## Quick Facts
- arXiv ID: 2510.21322
- Source URL: https://arxiv.org/abs/2510.21322
- Authors: Antoine Boutet; Lucas Magnana
- Reference count: 40
- One-line primary result: SANI reduces sensitive information regurgitation from 0.25 to 0.05 after one epoch while maintaining model utility

## Executive Summary
SANI introduces a two-phase unlearning approach to sanitize LLMs by removing sensitive information from their memorization. The method consists of an erasure phase that resets neurons in the last layer to disrupt fine-grained memorization, followed by a repair phase that fine-tunes the model while avoiding memorization of blacklisted terms. Evaluated on medical records and confidential terms, SANI drastically reduces regurgitation after only one epoch, achieving privacy levels close to models that never memorized sensitive data. The approach outperforms baseline methods while maintaining utility, offering an effective solution for privacy preservation in LLMs.

## Method Summary
SANI applies a two-phase unlearning process to sanitize LLMs. First, the erasure phase randomly resets 50% of neurons in the final linear layer to disrupt memorization of sensitive information. Second, the repair phase fine-tunes the model using masked or causal language modeling while excluding blacklisted terms from mask selection, preventing re-memorization. The approach was evaluated on BERT models fine-tuned on medical records and pre-trained models with confidential terms, demonstrating significant reductions in regurgitation while maintaining task performance.

## Key Results
- For BERT fine-tuned on medical records, privacy increased by 0.2-0.32 after one epoch while maintaining utility
- Pre-trained BERT models showed confidential term regurgitation drop from 0.25 to 0.05 after one epoch
- SANI achieved 87% regurgitation reduction (470K → 60K) in just 1-2 epochs
- No impact on downstream emotion classification F1-score (≥0.90) while removing sensitive information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Resetting neurons in the last layer disrupts fine-grained memorization while preserving foundational knowledge.
- Mechanism: The erasure phase randomly resets 50% of neurons in the final linear layer. Since attention heads in earlier layers capture word dependencies and the last layer uses this for task-specific predictions, targeted reset disrupts specific memorized associations without destroying learned linguistic patterns.
- Core assumption: Fine-grained sensitive information is primarily encoded in the last layer's weights rather than distributed throughout the architecture.
- Evidence anchors: [abstract] "reset certain neurons in the last layers of the model to disrupt the memorization of fine-grained information" [section 3.2] "by targeting erasure only on the last layer, fine-grained memorisation is disrupted without affecting the model's learning foundation" [corpus] Neighbor papers on selective pruning (Pochinkov & Schoots 2024) support layer-targeted approaches but show mixed results on deeper interventions

### Mechanism 2
- Claim: Excluding blacklisted terms from mask selection during repair prevents re-memorization while maintaining model utility.
- Mechanism: During masked language modeling, 15% of non-blacklisted words are randomly selected for masking. Blacklisted terms can appear as context but are never prediction targets, preventing gradient updates that would reinforce their memorization while allowing the model to learn from surrounding context.
- Core assumption: Preventing a term from being a prediction target is sufficient to reduce its memorization; contextual exposure alone does not reinforce memorization.
- Evidence anchors: [abstract] "fine-tune the model while avoiding memorizing sensitive information" [section 3.3] "the masks defined during this specialization phase are randomly selected from the input data set... while excluding terms contained in the blacklist" [corpus] PPmlm-bert baseline from Boutet et al. 2025 implements similar protection from the start, achieving near-zero regurgitation

### Mechanism 3
- Claim: Frequently repeated sensitive terms are most susceptible to unlearning, enabling rapid sanitization.
- Mechanism: High-frequency terms in training data have stronger gradient signals and more distributed representations. The erasure phase disproportionately disrupts these reinforced patterns, causing their regurgitation to drop dramatically after just 1-2 epochs.
- Core assumption: Unlearning effectiveness correlates positively with original memorization strength.
- Evidence anchors: [abstract] "The most frequently repeated sensitive terms in training were most affected by unlearning" [section 4.3, Figure 5] "without unlearning... the greatest amount of regurgitation occurs for the most frequently repeated sensitive terms... After one epoch of unlearning, we observe that the regurgitation associated with the most frequently repeated sensitive information in training decreases drastically" [corpus] Evidence limited; neighbor papers don't directly address frequency-unlearning correlation

## Foundational Learning

- Concept: **Layer-wise functional specialization in Transformers**
  - Why needed here: SANI's erasure strategy depends on the assumption that last layers handle fine-grained task-specific patterns while earlier layers encode general linguistic knowledge.
  - Quick check question: Can you explain why resetting attention head weights would be more destructive than resetting the final linear projection layer?

- Concept: **Masked vs. Causal Language Modeling objectives**
  - Why needed here: SANI applies different repair strategies for BERT (bidirectional, masked prediction) versus GPT (autoregressive, sequential prediction), requiring understanding of how each objective shapes memorization.
  - Quick check question: In causal LM, why does padding out sensitive tokens reduce prediction quality more than excluding them from masks in masked LM?

- Concept: **Extractable vs. counterfactual memorization**
  - Why needed here: SANI targets extractable memorization (verbatim regurgitation) but the paper notes rare terms show slightly increased regurgitation post-unlearning, suggesting different memorization types respond differently.
  - Quick check question: Why might a model regurgitate a term it saw once during training but not a term it saw 100 times, and how does this complicate unlearning evaluation?

## Architecture Onboarding

- Component map: Input data -> NER model -> Blacklist generation -> Erasure module (last-layer neuron reset) -> Repair module (fine-tuning with blacklist-aware masking) -> Sanitized model

- Critical path:
  1. Define blacklist (direct/indirect identifiers or confidential terms)
  2. Apply erasure (single forward pass to reset last layer)
  3. Run repair fine-tuning (1-4 epochs depending on utility/privacy tradeoff)
  4. Evaluate privacy (regurgitation rate) and utility (task performance)

- Design tradeoffs:
  - **Erasure intensity**: 50% reset chosen empirically; higher reset increases privacy but requires more repair epochs
  - **Repair epochs**: 1 epoch achieves ~87% regurgitation reduction (470K → 60K), 2 epochs reaches ~94% reduction but costs more
  - **Blacklist granularity**: Single terms vs. n-grams; broader blacklists reduce regurgitation but may harm domain utility

- Failure signatures:
  - **Over-aggressive erasure**: Model utility drops below acceptable threshold; attention patterns remain but task head is destroyed
  - **Under-inclusive blacklist**: Indirect identifiers persist; membership inference attacks still succeed
  - **Rare term persistence**: Low-frequency sensitive terms show increased regurgitation post-unlearning due to reduced prediction vocabulary cardinality

- First 3 experiments:
  1. **Baseline replication**: Fine-tune BERT-base on N2C2 medical data (4-64 epochs), measure privacy/utility tradeoff with and without SANI erasure phase only
  2. **Ablation on reset percentage**: Test 25%, 50%, 75% last-layer neuron reset; plot privacy gain vs. repair epochs needed to restore utility to within 5% of original
  3. **Cross-domain transfer**: Apply SANI to BookCorpus-trained model with NER-identified confidential terms; verify downstream emotion classification F1-score remains ≥0.90 while regurgitation drops below 0.10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is SANI effective for removing model biases and neural backdoors?
- Basis in paper: [explicit] The conclusion states that "an interesting avenue for future work would be to use and evaluate SANI to remove the influence of biased data and for the removal of backdoors."
- Why unresolved: The current evaluation focuses strictly on removing explicit sensitive terms (identifiers/confidential info), whereas biases and backdoors often manifest as complex behavioral patterns rather than specific vocabulary.
- What evidence would resolve it: Successful application of the erasure and repair mechanism to eliminate specific embedded backdoors or measured social biases without compromising the model's primary utility.

### Open Question 2
- Question: Does SANI scale effectively to state-of-the-art LLMs with billions of parameters?
- Basis in paper: [explicit] The authors note they "did not consider larger versions of the model due to limited resources," limiting experiments to BERT-base (110M params) and GPT-2 small.
- Why unresolved: The dynamics of neuron reset and repair in massive, deeper architectures may differ significantly from the smaller models tested.
- What evidence would resolve it: Evaluation of SANI on larger models (e.g., Llama, GPT-3/4 scale) demonstrating that the 1-2 epoch repair convergence holds true.

### Open Question 3
- Question: How can the slight increase in regurgitation for rarely repeated terms be mitigated?
- Basis in paper: [inferred] Results show that for "very rarely repeated terms in the training data, the number of regurgitations increases slightly after unlearning," attributed to reduced prediction cardinality.
- Why unresolved: The current repair phase inadvertently increases the probability of predicting rare terms even while suppressing frequent ones.
- What evidence would resolve it: A modified repair loss function that uniformly reduces the probability of all blacklisted terms, regardless of their initial training frequency.

## Limitations
- Limited scalability testing to larger models (tested only on BERT-base and GPT-2, not state-of-the-art LLMs)
- Evaluation focuses on direct regurgitation rather than membership inference or adversarial extraction attacks
- Mechanistic assumptions about layer-wise memorization distribution lack comprehensive verification across architectures

## Confidence

**High Confidence**: Claims about SANI's architecture and two-phase methodology are well-specified and reproducible. The observed reduction in regurgitation from 0.25 to 0.05 after one epoch for pre-trained models is supported by direct measurements.

**Medium Confidence**: Results showing utility preservation (emotion classification F1-scores ≥0.90) and privacy-utility tradeoffs are convincing within tested domains but may not generalize to other tasks or larger models.

**Low Confidence**: Assertions about SANI's superiority over all baselines and its ability to handle indirect identifiers equally well as direct identifiers lack sufficient comparative analysis and edge case testing.

## Next Checks

1. **Layer Distribution Analysis**: Map memorization patterns across all transformer layers using integrated gradients or attention-based attribution to verify that sensitive information is indeed concentrated in the last layer before applying SANI.

2. **Adversarial Robustness Testing**: Evaluate SANI against membership inference attacks and paraphrase-based extraction attempts to determine whether reduced regurgitation translates to actual privacy improvements against sophisticated adversaries.

3. **Cross-Architecture Transfer**: Apply SANI to GPT-3.5-level models (e.g., OPT-66B) and decoder-only architectures to assess scalability and identify any architecture-specific limitations or parameter adjustments needed.