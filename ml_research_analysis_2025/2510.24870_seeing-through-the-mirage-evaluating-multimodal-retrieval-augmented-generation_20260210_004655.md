---
ver: rpa2
title: 'Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation'
arxiv_id: '2510.24870'
source_url: https://arxiv.org/abs/2510.24870
tags:
- claim
- information
- precision
- human
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIRAGE, a framework for evaluating multimodal
  retrieval-augmented generation (RAG) systems. The key insight is that evaluation
  should be claim-centric, decomposing generated text into subclaims and verifying
  them against source materials across modalities.
---

# Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2510.24870
- Source URL: https://arxiv.org/abs/2510.24870
- Authors: Alexander Martin; William Walden; Reno Kriz; Dengjia Zhang; Kate Sanders; Eugene Yang; Chihsheng Jin; Benjamin Van Durme
- Reference count: 40
- Primary result: MIRAGE framework shows strong alignment with human evaluation for multimodal RAG systems, outperforming text-centric metrics

## Executive Summary
This paper introduces MIRAGE, a comprehensive framework for evaluating multimodal retrieval-augmented generation (RAG) systems. The key innovation is a claim-centric evaluation approach that decomposes generated text into subclaims and verifies them against source materials across both text and visual modalities. MIRAGE consists of two core metrics: INFOF1 for measuring factuality and information coverage, and CITEF1 for measuring citation support and completeness. The framework demonstrates superior alignment with human quality judgments compared to existing text-based RAG metrics when adapted to multimodal settings.

## Method Summary
MIRAGE implements a two-stage evaluation process. First, generated text is decomposed into atomic subclaims using the Miratrix framework. Second, a VLM-LLM pipeline verifies each subclaim against retrieved multimodal sources, determining whether each claim is supported, contradicted, or not found in the evidence. INFOF1 measures the F1 score of information coverage and factuality, while CITEF1 evaluates citation support and completeness. The framework integrates seamlessly with existing RAG pipelines and can handle both textual and visual information sources. When compared against ALCE, ARGUE, and RAGAS (adapted for multimodal settings), MIRAGE shows significantly stronger correlation with human evaluation judgments of multimodal RAG quality.

## Key Results
- MIRAGE metrics (INFOF1 and CITEF1) show strong alignment with human evaluation judgments of multimodal RAG quality
- Text-centric metrics (ALCE, ARGUE, RAGAS) adapted to multimodal settings demonstrate poor correlation with human assessments
- The authors recommend using ROUGE, BERTScore, INFOF1, and CITEF1 for constrained evaluation, with RAGAS-Faithfulness for comprehensive assessment
- VLM-LLM claim verification requires careful calibration to handle modality-specific nuances and domain knowledge

## Why This Works (Mechanism)
The claim-centric approach works because it aligns evaluation with how humans assess multimodal information quality. By decomposing text into verifiable subclaims and checking each against source materials across modalities, MIRAGE captures both factual accuracy and proper citation practices. The VLM-LLM verification pipeline leverages the complementary strengths of vision-language models and large language models to handle the complexity of multimodal evidence. This decomposition allows for fine-grained assessment of where RAG systems succeed or fail, providing actionable feedback for system improvement. The framework's strength lies in its ability to handle the inherent complexity of multimodal information while maintaining rigorous verification standards.

## Foundational Learning
- Multimodal RAG systems combine text and visual information retrieval to enhance generation quality
  *Why needed:* Understanding the problem domain helps contextualize why standard text-only metrics fail for multimodal evaluation
  *Quick check:* Verify understanding of how RAG systems work and why multimodality adds complexity

- Claim decomposition separates generated text into atomic, verifiable statements
  *Why needed:* Enables fine-grained evaluation of factuality and citation completeness
  *Quick check:* Confirm ability to identify subclaims in sample text passages

- VLM-LLM verification pipelines use vision-language models for visual claims and LLMs for textual claims
  *Why needed:* Different model types excel at different modalities, requiring complementary verification approaches
  *Quick check:* Understand the distinction between VLM and LLM capabilities for verification tasks

- Citation completeness measures whether all source materials used in generation are properly acknowledged
  *Why needed:* Ensures transparency and traceability in RAG-generated content
  *Quick check:* Verify understanding of what constitutes proper citation in multimodal contexts

- Factuality metrics assess whether generated claims align with source evidence across modalities
  *Why needed:* Core requirement for trustworthy RAG systems in applications like education and research
  *Quick check:* Confirm understanding of how factuality differs from citation completeness

## Architecture Onboarding

Component map: Text Generation -> Claim Decomposition (Miratrix) -> VLM-LLM Verification -> INFOF1/CITEF1 Scoring

Critical path: The verification pipeline (VLM-LLM) is the critical path, as it directly determines the accuracy of both INFOF1 and CITEF1 metrics. Bottlenecks here propagate to all downstream evaluations.

Design tradeoffs: The framework trades computational efficiency for accuracy by using two separate model types (VLM and LLM) rather than attempting to handle all verification with a single model. This increases accuracy but requires more resources and careful orchestration.

Failure signatures: Poor calibration of VLM-LLM models leads to systematic over/under-verification of claims. Domain-specific jargon or complex visual relationships may cause verification failures even when human judgment would find claims valid. The claim decomposition process may struggle with implicit or nuanced claims that span multiple modalities.

First experiments:
1. Run MIRAGE on a small multimodal dataset with known ground truth to verify basic functionality
2. Test VLM-LLM calibration by varying confidence thresholds and measuring impact on metric scores
3. Compare MIRAGE output against human evaluation on a sample of claims to validate alignment

## Open Questions the Paper Calls Out
None

## Limitations
- VLM-LLM calibration requires careful tuning to handle modality-specific nuances and domain knowledge
- Claim decomposition introduces potential brittleness when dealing with nuanced or implicit claims spanning multiple modalities
- Framework may struggle with subtle visual cues or domain-specific terminology without additional fine-tuning

## Confidence
High confidence in MIRAGE's ability to assess multimodal factuality and citation quality based on strong alignment with human evaluation
Medium confidence in VLM-LLM calibration process due to potential domain-specific variations
Medium confidence in handling complex visual-textual relationships, particularly for specialized domains

## Next Checks
1. Conduct cross-domain validation using specialized datasets (medical, technical, legal) to assess VLM-LLM calibration requirements across different knowledge domains
2. Implement ablation studies comparing MIRAGE performance with and without claim decomposition to quantify the impact of this design choice
3. Test the framework's robustness against adversarial examples designed to exploit modality-specific weaknesses in the verification process