---
ver: rpa2
title: 'Delving into: the quantification of Ai-generated content on the internet (synthetic
  data)'
arxiv_id: '2504.08755'
source_url: https://arxiv.org/abs/2504.08755
tags:
- data
- generative
- delve
- chatgpt
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of quantifying the scale of\
  \ AI-generated content on the internet, which is increasingly saturated with such\
  \ material. The author proposes a novel method using linguistic markers\u2014specifically,\
  \ the frequency of phrases like \"delve into\" and \"explore\"\u2014that are commonly\
  \ used by ChatGPT."
---

# Delving into: the quantification of Ai-generated content on the internet (synthetic data)

## Quick Facts
- arXiv ID: 2504.08755
- Source URL: https://arxiv.org/abs/2504.08755
- Authors: Dirk HR Spennemann
- Reference count: 0
- Primary result: At least 30% of active web pages contain AI-generated content, with the actual proportion likely closer to 40%

## Executive Summary
This paper addresses the challenge of quantifying the scale of AI-generated content on the internet, which is increasingly saturated with such material. The author proposes a novel method using linguistic markers—specifically, the frequency of phrases like "delve into" and "explore"—that are commonly used by ChatGPT. By analyzing Google search data for webpages containing these phrases from January 2020 to March 2025, the study estimates the proportion of AI-generated text. The results indicate that at least 30% of active web pages contain AI-generated content, with the actual proportion likely closer to 40%. This finding highlights the growing prevalence of synthetic data and its potential implications, such as the risk of autophagous loops in AI training.

## Method Summary
The paper employs a novel approach to estimate AI-generated content prevalence by tracking linguistic markers commonly used by ChatGPT across Google search results from January 2020 to March 2025. The methodology analyzes the frequency of specific phrases like "delve into" and "explore," comparing pre- and post-ChatGPT release patterns. By establishing a baseline from academic publications and adjusting for natural vocabulary adoption rates, the author extrapolates from marker-containing pages to estimate the total proportion of AI-generated content on the web.

## Key Results
- At least 30% of active web pages contain AI-generated content
- The actual proportion is likely closer to 40%
- A dramatic increase in marker-containing webpages began in April 2023, four months after ChatGPT's public release
- Marker usage varies across GPT model versions, with GPT-4o showing reduced frequency of "delve into"

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Marker Proxy Detection
- Claim: Specific phrases favored by ChatGPT ("delve into", "explore") can serve as observable proxies for estimating AI-generated content prevalence at scale.
- Mechanism: LLMs exhibit characteristic vocabulary distributions inherited from training data and RLHF alignment. When users publish AI-generated text with minimal editing, these linguistic fingerprints propagate to the web. By measuring frequency changes in marker terms relative to a pre-AI baseline, the synthetic content proportion becomes estimable.
- Core assumption: The selected markers are genuinely distinctive to ChatGPT outputs, and users publish generated text without systematically editing out these phrases.
- Evidence anchors:
  - [abstract] "By analyzing the frequency of specific keywords commonly used by ChatGPT, this paper demonstrates that such linguistic markers can effectively be used to estimate the presence of generative AI content online."
  - [section] "It finds in roots in the observation that many of the conversations documented as part of datasets for various papers examining the abilities, biases and limitations of ChatGPT contained the phrase 'delve into' as part of the generative Ai response"
  - [corpus] Weak direct corpus validation. Related papers on AI detection exist (e.g., "Unknown Aware AI-Generated Content Attribution") but focus on image attribution or classifier-based methods, not keyword-frequency proxies.
- Break condition: If humans rapidly adopt these phrases into natural vocabulary (the paper attempts to control for this), or if future model versions systematically avoid these markers (which the paper observes with GPT-4o and 4.5).

### Mechanism 2: Temporal Discontinuity Detection
- Claim: A sharp, sustained increase in marker frequency immediately following ChatGPT's public release indicates causal adoption rather than organic trend continuation.
- Mechanism: The November 2022 ChatGPT release creates a natural experiment. Pre-release data (Jan 2020–Nov 2022) establishes baseline drift. Post-release divergence—particularly the April 2023 inflection after a 4-month experimentation lag—signals tool adoption driving content creation patterns.
- Core assumption: No alternative cause (cultural shift, different tool, search algorithm change) explains the timing and magnitude of the discontinuity.
- Evidence anchors:
  - [abstract] "The analysis revealed a dramatic increase in webpages containing these phrases starting in April 2023, coinciding with the public release of ChatGPT."
  - [section] "The time lag between the public release of ChatGPT on 30 November 2022 and dramatic increase in webpages containing the phrase 'delve into' from April 2023 onwards represents the experimentation phase of new technology leading to eventual adoption."
  - [corpus] No corpus papers validate this specific temporal discontinuity approach, though "AI-generated data contamination erodes pathological variability" discusses related contamination concerns in medical records with temporal implications.
- Break condition: If Google's indexing practices or search result composition changed substantially around this period (the paper acknowledges indexing artifacts causing troughs).

### Mechanism 3: Academic Baseline Subtraction
- Claim: Academic publication trends provide a conservative "human-only" vocabulary adoption rate to subtract from observed web-wide increases.
- Mechanism: Academic publishers rapidly instituted AI disclosure policies, reducing uncritical AI text adoption. By projecting pre-2023 academic growth forward (r=0.9961) and comparing to actual post-2023 values, the "excess" usage beyond natural vocabulary drift can be estimated, then applied to general web content.
- Core assumption: Academics neither use AI-generated text undetected nor accelerate their adoption of new phrases faster than the general public. The paper notes "even when assuming that the proportion of the general public incorporating the phrase 'delve into' their vocabulary is three times that of academics, the overall impact of that adjustment is only small."
- Evidence anchors:
  - [section] "We can draw on academic publications as a proxy, under the assumption that, by and large, academics are unlikely to uncritically adopt full sections of generative Ai created text, especially since publishers moved quickly to delegitimise this."
  - [section] "Based on the regression formula for that distribution (y = −2815508 + 1408⋅x), we can calculate the 'natural' progressive increase for the years 2023 and 2024, demonstrating an increase of 27.3% for 2023 and 28.1% for 2024."
  - [corpus] No corpus support for this specific methodology.
- Break condition: If academic preprint servers or smaller publishers lack effective AI-text policing, or if academics adopt AI-assisted writing while manually editing marker phrases out.

## Foundational Learning

- **Concept: Synthetic Data and Autophagous Loops**
  - Why needed here: The paper's core concern is that AI models training on AI-generated content create self-reinforcing degradation cycles ("model collapse"), where synthetic data dilutes the human-originated knowledge base.
  - Quick check question: If 40% of web content is AI-generated, what happens when the next generation of LLMs trains primarily on this post-2023 web corpus?

- **Concept: Marker Fingerprinting vs. Classifier Detection**
  - Why needed here: This paper proposes a lightweight, interpretable alternative to ML-based AI detectors (which have high false-positive rates and adversarial brittleness). Understanding tradeoffs between simple markers and complex classifiers is essential for practical detection systems.
  - Quick check question: Why might "delve into" be a useful marker despite being a valid English phrase that predates ChatGPT?

- **Concept: Index Artifacts and Data Quality**
  - Why needed here: The paper identifies Google indexing patterns (periodic troughs) as confounding signals. Recognizing that platform-level infrastructure creates artificial patterns in observational data is critical for valid inference.
  - Quick check question: If you observe monthly dips in a web-scraped dataset, what are three possible causes beyond genuine content decline?

## Architecture Onboarding

- **Component map:**
  - Google search data collection -> Baseline regression (2020-2022) -> Academic proxy adjustment -> Marker frequency extrapolation -> Total web proportion estimation

- **Critical path:**
  1. Select markers empirically validated as LLM-favored (requires known AI/human corpus comparison)
  2. Collect time-series data spanning pre- and post-model-release windows
  3. Quantify baseline drift rate from pre-release period
  4. Calculate excess usage post-release (observed minus projected baseline)
  5. Apply vocabulary-adoption adjustment using proxy population
  6. Map to total active web page estimates for proportion calculation

- **Design tradeoffs:**
  - Single marker vs. ensemble: Single markers are interpretable but noisy; ensembles increase coverage but dilute signal if markers have different adoption curves
  - Broad terms ("explore") vs. distinctive phrases ("delve into"): Broad terms capture more volume but introduce noise from unrelated usage
  - Google search counts vs. direct web scraping: Search counts are accessible but reflect indexing artifacts; direct scraping is cleaner but resource-intensive

- **Failure signatures:**
  - Periodic troughs in time-series (indexing artifacts; requires smoothing)
  - Marker drift across model versions (GPT-4o reduced "delve into" usage; marker selection must track model evolution)
  - Search algorithm changes confounding frequency interpretation
  - Human adoption of AI-favored phrases creating false positives

- **First 3 experiments:**
  1. **Marker validation:** Assemble a held-out corpus of 10,000 human-written and 10,000 ChatGPT-generated documents. Compute precision/recall for candidate markers ("delve into", "explore", "it's worth noting", "in summary"). Report ROC curves.
  2. **Marker stability across models:** Generate 5,000 responses each from GPT-3.5, GPT-4, GPT-4o, and GPT-4.5 to identical prompts. Quantify marker frequency drift to inform marker retirement schedules.
  3. **Negative control test:** Apply the methodology to a marker phrase *not* associated with ChatGPT (e.g., "furthermore") over the same time window. Confirm no April 2023 discontinuity, validating that observed effects are marker-specific rather than artifact.

## Open Questions the Paper Calls Out
None

## Limitations
- Google search count reliability: The observed periodic troughs suggest indexing artifacts may affect measurement precision
- Marker representativeness: The assumption that selected phrases are sufficiently distinctive to ChatGPT is critical but unproven
- Academic baseline as human proxy: Using academic publications assumes academics neither use AI undetected nor adopt new vocabulary faster than the general public

## Confidence
- High confidence: The methodology is internally consistent and the observed temporal discontinuity following ChatGPT's release is compelling
- Medium confidence: The core finding (30-40% AI-generated content) is plausible given the methodology, but depends heavily on unverified assumptions about marker representativeness and baseline estimation
- Low confidence: The exact percentage (30-40%) should be treated as an order-of-magnitude estimate rather than a precise measurement

## Next Checks
1. **Marker validation experiment:** Assemble a held-out corpus of 10,000 human-written and 10,000 ChatGPT-generated documents. Compute precision/recall for candidate markers ("delve into", "explore", "it's worth noting", "in summary"). Report ROC curves to quantify marker effectiveness and false positive rates.
2. **Cross-platform verification:** Replicate the methodology using Bing search data or direct web scraping of a representative sample of sites to verify that observed patterns are not Google-specific artifacts. Compare results across platforms.
3. **Temporal stability test:** Extend the time series analysis through 2025 with quarterly updates. Monitor whether marker frequencies plateau, continue increasing, or show new patterns that would indicate changing AI adoption behaviors or model evolution.