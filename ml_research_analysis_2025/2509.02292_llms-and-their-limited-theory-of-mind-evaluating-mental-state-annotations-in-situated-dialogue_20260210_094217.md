---
ver: rpa2
title: 'LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations
  in Situated Dialogue'
arxiv_id: '2509.02292'
source_url: https://arxiv.org/abs/2509.02292
tags:
- searcher
- belief
- dialogue
- llms
- beliefs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a two-step framework to evaluate large language
  models' (LLMs) ability to infer and track shared mental models (SMMs) in situated
  dialogue. LLMs were used both as annotators of belief, commitment, and goal states
  in the CReST corpus, and as discrepancy detectors when compared to human and ground-truth
  annotations.
---

# LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue

## Quick Facts
- arXiv ID: 2509.02292
- Source URL: https://arxiv.org/abs/2509.02292
- Authors: Katharine Kowalyshyn; Matthias Scheutz
- Reference count: 35
- Primary result: LLMs can annotate shared mental models in dialogue but struggle with spatial reasoning and grounding, revealing limited Theory of Mind capabilities.

## Executive Summary
This study introduces a two-step framework to evaluate large language models' (LLMs) ability to infer and track shared mental models (SMMs) in situated dialogue. LLMs were used both as annotators of belief, commitment, and goal states in the CReST corpus, and as discrepancy detectors when compared to human and ground-truth annotations. The evaluation framework identifies four discrepancy types (belief contradictions, omissions, unsupported beliefs, and false beliefs) and normalizes scores to assess annotation quality. Results show that while LLMs demonstrate surface-level Theory of Mind capabilities, they struggle with spatial reasoning and grounding beliefs in environmental context.

## Method Summary
The paper evaluates LLMs as annotators of shared mental models in situated team dialogue using a two-step pipeline. First, an LLM annotates each utterance in the CReST corpus with structured mental state information (beliefs, commitments, goals, second-order beliefs, common belief) using a JSON schema and explicit annotation prompt. Second, a secondary LLM detects discrepancies between naive annotations (LLM or human without video access) and ground truth annotations (human with headcam video). The framework classifies discrepancies into four types—belief contradictions, omissions, unsupported beliefs, and false beliefs—and computes weighted, normalized scores per utterance to enable cross-model and cross-dialogue comparison.

## Key Results
- Claude Sonnet 4 exhibited the highest discrepancy rates across all models, while o3-mini and Gemma 8.5B performed more conservatively.
- LLMs produce coherent annotations but systematically fail on spatial reasoning and prosodic cues due to lack of environmental grounding.
- Naive human annotators produced fewer unsupported beliefs but comparable error rates in other categories, highlighting both the potential and limitations of LLMs as annotators.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate structured mental state annotations from dialogue by following explicit annotation schemas with examples.
- Mechanism: The paper provides LLMs with a JSON schema for beliefs, goals, commitments, and second-order beliefs, along with few-shot examples. The LLM performs inference over dialogue history to populate fields based on linguistic patterns and task context.
- Core assumption: Mental state inferences can be reliably extracted from linguistic surface features without access to ground truth environmental state.
- Evidence anchors:
  - [abstract] "In the first step, an LLM generates annotations by identifying SMM elements within task-oriented dialogues"
  - [section 3.1] Describes annotation procedure with three groups and structured JSON output format
  - [corpus] Related work on LLMs for conversation tracking (Addlesee et al., 2023; Kong et al., 2024) supports feasibility
- Break condition: When spatial reasoning or prosodic cues are required but unavailable through text alone, annotations diverge from ground truth.

### Mechanism 2
- Claim: A secondary LLM can systematically classify discrepancies between annotation sets using a defined taxonomy.
- Mechanism: The discrepancy detection LLM receives two annotation sets and applies four discrepancy categories (belief contradictions, omissions, unsupported beliefs, false beliefs) with explicit definitions and examples in the prompt. It outputs structured JSON classifying each divergence.
- Core assumption: Discrepancy types are mutually distinguishable and the evaluating LLM can reliably apply the taxonomy without itself introducing classification errors.
- Evidence anchors:
  - [section 4.1] Defines four discrepancy types with severity ranking
  - [table 4] Human validation of D1 shows o3-mini achieved 87.6% accuracy in discrepancy identification
  - [corpus] No direct corpus evidence for this specific discrepancy taxonomy; related ToM benchmarks focus on different evaluation paradigms
- Break condition: When annotations are ambiguous or the ground truth itself contains uncertainty, the discrepancy classifier may produce false positives or miss subtle contradictions.

### Mechanism 3
- Claim: Severity-weighted discrepancy scores enable cross-model and cross-dialogue comparison of annotation quality.
- Mechanism: Raw discrepancy counts are weighted by severity (wb, wf, wu, wo), normalized by utterance count, then scaled to [0,1] range. Higher scores indicate better alignment with ground truth.
- Core assumption: All discrepancies of the same type have equivalent severity within a task context, and utterance-level normalization adequately controls for dialogue length.
- Evidence anchors:
  - [section 4.2] Equations 1-3 define the weighted scoring mechanism
  - [table 3] Shows normalized scores across models and dialogues, revealing Claude Sonnet 4 consistently underperforms (0.000-0.229) while o3-mini scores higher (0.770-1.000)
  - [corpus] No corpus evidence for this specific metric; appears novel to this work
- Break condition: If task context makes certain discrepancy types more critical than others but weights are set uniformly (all wx=1 in this study), the metric may not reflect operational impact.

## Foundational Learning

- Concept: Shared Mental Models (SMMs)
  - Why needed here: The entire annotation framework targets SMM elements—beliefs, goals, commitments that team members hold about the task and each other. Without understanding SMM theory, the annotation schema appears arbitrary.
  - Quick check question: Can you explain why tracking both first-order beliefs ("Searcher believes X") and second-order beliefs ("Searcher believes that Director believes X") matters for team coherence?

- Concept: Theory of Mind (ToM) in computational systems
  - Why needed here: The paper frames dialogue annotation as a proxy task for evaluating ToM-like capabilities in LLMs. Understanding what ToM entails in humans clarifies what the paper is and isn't claiming about LLM cognition.
  - Quick check question: Why might an LLM that correctly annotates beliefs still fail if deployed as an actual team member making real-time interventions?

- Concept: Grounded vs. ungrounded language processing
  - Why needed here: The central finding is that LLMs produce coherent annotations but fail on spatial reasoning and prosodic cues because they lack environmental grounding. This distinction explains the discrepancy between surface-level success and deeper failures.
  - Quick check question: If you gave the LLM access to the headcam video in addition to transcripts, which discrepancy types would you expect to decrease most?

## Architecture Onboarding

- Component map: Dialogue transcripts from CReST corpus -> LLM annotation stage -> Ground truth layer -> Discrepancy detection stage -> Evaluation layer

- Critical path:
  1. Prompt engineering for annotation schema (see Appendix A—explicit verb constraints, JSON structure, perspective-taking instructions)
  2. Discrepancy taxonomy definition and prompt for detection LLM
  3. Severity weight assignment (paper uses uniform weights but notes task-specific tuning is possible)

- Design tradeoffs:
  - Model selection breadth vs. compute cost: Paper tests only 3 models to limit environmental impact; may miss performance variation in other architectures
  - Naive vs. informed annotation: LLMs and naive humans lack video access, creating ecologically valid comparison but guaranteeing spatial errors
  - Automated vs. human discrepancy validation: Only D1 received human validation of discrepancy detection accuracy; other dialogues rely on LLM evaluator

- Failure signatures:
  - High unsupported belief counts: Claude Sonnet 4 produced 231 unsupported beliefs in D1 alone—indicates overgeneralization or hallucination tendency
  - Near-zero omissions with high contradictions: Gemma 8.5B had 0-1 omissions per dialogue but 114-191 belief contradictions—suggests overcommitment to inferences
  - Spatial ambiguity sensitivity: D3 and D5 consistently produced highest discrepancy counts across all annotators

- First 3 experiments:
  1. Replicate annotation on a single dialogue using the provided prompt (Appendix A) with your target LLM. Compare JSON structure and field population against example outputs to validate prompt transfer.
  2. Run discrepancy detection on your annotations vs. ground truth (if available) or vs. another model's annotations. Check classification accuracy against the four discrepancy types manually for 20-30 utterances.
  3. Vary severity weights (wx parameters) based on a hypothetical task context where false beliefs are critical but omissions are tolerable. Re-score and observe how model rankings change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be improved to accurately ground spatial beliefs and disambiguate prosodic cues in situated team dialogue, and would multimodal inputs (e.g., visual feeds) reduce discrepancy rates?
- Basis in paper: [explicit] The authors explicitly state that LLMs "systematically err in scenarios requiring spatial reasoning or disambiguation of prosodic cues," and note that naive annotators lacked access to the video ground truth.
- Why unresolved: The study isolated textual dialogue from environmental context to simulate "naive" inference; it did not test whether providing visual grounding would improve spatial belief accuracy.
- What evidence would resolve it: A controlled experiment where LLM annotators receive synchronized headcam video and dialogue, with discrepancy rates compared to the text-only baseline.

### Open Question 2
- Question: How does the choice of severity weights in the discrepancy metric affect model rankings, and is the equal-weighting scheme (wx = 1) appropriate for all team-coherence tasks?
- Basis in paper: [explicit] The authors define a weighted discrepancy score and state that "weights may be set differently depending on the corpus and task used," but apply uniform weights without sensitivity analysis.
- Why unresolved: Different task contexts (e.g., safety-critical coordination vs. low-stakes collaboration) may tolerate omissions but not false beliefs; it is unclear whether model performance rankings are robust to weight changes.
- What evidence would resolve it: A parametric analysis varying weights (e.g., emphasizing false beliefs vs. omissions) across the same annotated dialogues, reporting changes in normalized scores and rankings.

### Open Question 3
- Question: Can LLMs be extended from passive annotation to active verbal intervention that corrects divergent beliefs and restores team coherence in real-time dialogue?
- Basis in paper: [explicit] The introduction asks if LLMs could "verbally intervene to correct human misrepresentations and reestablish coherence," but the study only evaluates annotation, not intervention.
- Why unresolved: The two-step framework detects discrepancies offline; it does not test whether LLMs can generate timely, context-appropriate interventions or whether such interventions improve team performance.
- What evidence would resolve it: A human-subject experiment where an LLM monitors team dialogue, generates belief-correction prompts when discrepancies exceed a threshold, and measures task performance and subjective coherence compared to a no-intervention control.

### Open Question 4
- Question: What cognitive or architectural mechanisms are required for LLMs to integrate spatial, temporal, and epistemic reasoning in a way that approximates human-like ToM in grounded teamwork?
- Basis in paper: [inferred] The conclusion states that LLMs "approximate understanding through patterns in language rather than through participation in the physical and social worlds" and that enabling coherent teammates will require "cognitively grounded models capable of representing belief, perspective, and intention in context."
- Why unresolved: The paper characterizes the epistemological gap but does not propose or evaluate specific mechanisms (e.g., world-state models, Theory of Mind modules) that could bridge it.
- What evidence would resolve it: Comparative evaluation of LLMs augmented with explicit spatial-state tracking or ToM-inspired modules on the CReST-style annotation task, analyzing whether such augmentations reduce specific discrepancy types.

## Limitations
- Ground truth dependency: The evaluation framework relies on human annotations with video access as gold standard, assuming these are error-free and complete.
- Corpus size constraints: With only 6 dialogues (1,142 utterances), the statistical power to generalize findings across dialogue types and task contexts is limited.
- Spatial reasoning blind spot: The framework cannot distinguish whether failures stem from LLM reasoning capabilities or from absence of multimodal input.

## Confidence
- Medium confidence: LLMs demonstrate surface-level Theory of Mind capabilities but struggle with spatial reasoning and environmental grounding.
- Medium confidence: Claude Sonnet 4 exhibits the highest discrepancy rates while o3-mini and Gemma 8.5B perform more conservatively.
- Low confidence: The weighted discrepancy scoring system enables meaningful cross-model and cross-dialogue comparison.

## Next Checks
1. Compute Cohen's kappa for ground truth annotations across the three human annotators to establish baseline reliability of the gold standard.
2. Systematically vary the annotation prompt parameters (temperature, persona, few-shot examples) for a single model across multiple dialogues to quantify the impact of prompt engineering on discrepancy rates.
3. Re-run the annotation pipeline with LLMs given access to dialogue transcripts plus spatial metadata or simplified environmental descriptions to isolate whether discrepancies stem from reasoning limitations or input constraints.