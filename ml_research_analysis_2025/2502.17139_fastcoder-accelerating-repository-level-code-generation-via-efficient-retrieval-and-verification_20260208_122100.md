---
ver: rpa2
title: 'FastCoder: Accelerating Repository-level Code Generation via Efficient Retrieval
  and Verification'
arxiv_id: '2502.17139'
source_url: https://arxiv.org/abs/2502.17139
tags:
- code
- generation
- retrieval
- draft
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of slow inference in repository-level
  code generation using large language models (LLMs). Existing approaches focus on
  correctness but neglect efficiency, and existing inference acceleration methods
  are not tailored to the unique syntactic and semantic characteristics of code.
---

# FastCoder: Accelerating Repository-level Code Generation via Efficient Retrieval and Verification

## Quick Facts
- arXiv ID: 2502.17139
- Source URL: https://arxiv.org/abs/2502.17139
- Reference count: 36
- Primary result: Achieves up to 2.53x speedup on repository-level code generation while preserving output quality

## Executive Summary
FastCoder addresses the challenge of slow inference in repository-level code generation using large language models (LLMs). While existing approaches focus on correctness, FastCoder introduces an efficient retrieval-based acceleration approach that leverages multi-source datastores, dynamic retrieval timing, parallel retrieval, and a context- and LLM preference-aware cache. The method achieves significant speedups by constructing draft sequences using a weighted Trie and employing tree attention to avoid redundant computation during verification.

## Method Summary
FastCoder accelerates repository-level code generation through a draft-then-verify paradigm. It retrieves candidate sequences from a multi-source datastore (combining general and project-specific knowledge), verifies them using the target LLM with tree attention, and employs a dynamic cache that adapts to local context and LLM preferences. The system uses parallel retrieval, constructs weighted Tries from retrieved candidates, and verifies multiple candidates simultaneously while sharing computation for common prefixes. Key innovations include dynamic retrieval timing that skips expensive lookups at predictable failure points and a cache that captures both retrieved and generated sequences to improve future draft quality.

## Key Results
- Achieves up to 2.53x speedup compared to autoregressive decoding on repository-level code generation tasks (DevEval and RepoEval datasets)
- Outperforms state-of-the-art approaches by up to 88% in terms of latency reduction
- On standalone code generation (HumanEval dataset), achieves up to 2.54x acceleration
- Can be integrated with existing correctness-focused approaches (RepoCoder and RLCoder) to achieve over 2.6x speedup while preserving output quality

## Why This Works (Mechanism)

### Mechanism 1: Parallel Draft Verification
FastCoder reduces inference latency by retrieving and verifying draft tokens in parallel rather than generating them autoregressively. It uses a retrieval-based draft-then-verify paradigm where candidate sequences are retrieved via suffix matching and verified using tree attention, which masks attention to prevent candidate branches from interacting while sharing computation for common prefixes. This works because verification (even with tree attention overhead) is significantly faster than standard autoregressive generation when the average acceptance length is sufficiently high.

### Mechanism 2: Context- and LLM Preference-aware Cache
A static datastore is insufficient for code; FastCoder adds a dynamic cache that significantly improves draft acceptance rates by adapting to local context and LLM preferences. The cache stores verified sequences (both retrieved tokens that the LLM accepted and tokens the LLM generated itself), addressing the "localness" of code where token sequences frequently reappear and capturing the specific stylistic preferences of the target LLM that static external data cannot predict.

### Mechanism 3: Dynamic Retrieval Timing
Not every token position benefits from retrieval; FastCoder dynamically skips retrieval at "hard" positions to reduce overhead without sacrificing quality. It identifies that retrieval often fails at the first non-whitespace token of a line and probabilistically skips retrieval at these positions, maintaining a Missing Table to remember suffixes that previously yielded no results, avoiding redundant searches.

## Foundational Learning

**Concept: Speculative Decoding (Draft-then-Verify)**
- **Why needed here:** This is the fundamental paradigm FastCoder builds upon. The speedup comes from the LLM verifying $k$ tokens in roughly the time it takes to generate $1$.
- **Quick check question:** If the target LLM rejects the 3rd token in a draft sequence of length 5, how many tokens are ultimately accepted?

**Concept: Trie (Prefix Tree) & Tree Attention**
- **Why needed here:** FastCoder retrieves multiple candidate drafts and merges them into a Trie to verify them efficiently without batching, using a specialized attention mask.
- **Quick check question:** Why is a Trie structure better than a simple list for verifying multiple candidate code completions in parallel?

**Concept: N-gram / Suffix Matching**
- **Why needed here:** The retrieval mechanism relies on matching the end of the current context (suffix) against a database of previous code, distinct from semantic vector search used in RAG.
- **Quick check question:** How does FastCoder's retrieval strategy differ from standard semantic search used in RAG (like embedding cosine similarity)?

## Architecture Onboarding

**Component map:**
Datastore ($D_c$ Common + $D_r$ Repository) -> Retriever (suffix matching, Missing Table, Skip Token logic) -> Draft Constructor (weighted Trie) -> LLM Verifier (Tree Attention) -> Cache (Context-LLM Preference-aware)

**Critical path:**
1. Receive context $s$
2. Check Cache for hits
3. If miss → Check Missing Table → Apply Skip Token logic → Parallel Search $D_c$ & $D_r$
4. Construct Weighted Trie from results
5. Verify via LLM with Tree Attention
6. Update Cache and Missing Table based on results

**Design tradeoffs:**
- Retrieval vs. Generation Cost: If retrieval is too slow, it negates speedup. FastCoder keeps $D_c$ smaller than baselines like REST to mitigate this.
- Skip Probability ($p$): Setting $p$ too high saves cost but lowers acceptance length; setting it too low wastes time on failed lookups.

**Failure signatures:**
- Low Speedup (<1.5x): Likely caused by low acceptance length or excessive retrieval latency
- High Memory Usage: Cache might be growing unbounded if threshold $l$ is misconfigured
- Correctness Drop: Theoretically impossible if verification is implemented correctly, but bugs in tree attention masks could cause divergence

**First 3 experiments:**
1. Ablation on Cache: Run with and without Context-LLM Cache on DevEval to isolate gain in Average Acceptance Length
2. Timing Analysis: Profile system to ensure Retrieval Time is lower than Generation Time saved; verify impact of Missing Table
3. Integration Test: Integrate FastCoder with correctness-focused RAG method (like RepoCoder) to confirm it maintains pass@1 while reducing latency

## Open Questions the Paper Calls Out

**Open Question 1:** How does FastCoder's performance and resource consumption scale when applied to Code LLMs significantly larger than 13B parameters (e.g., 30B–70B scale)? The authors explicitly note they do not provide results on larger models due to resource constraints, limiting evaluation to models between 1.3B and 13B parameters.

**Open Question 2:** To what extent does the "skip token" heuristic (skipping the first non-whitespace token) generalize to programming languages with different syntactic structures than Python? The heuristic relies on Python-specific patterns, and all experiments are conducted exclusively on Python repositories.

**Open Question 3:** Can a learned policy for retrieval timing outperform the current probabilistic heuristic? The authors use a fixed probability for skipping retrieval because "the introduction of intricate judgment processes may incur additional computational overhead," but a lightweight learned classifier might predict retrieval failure more accurately.

## Limitations

- Tree attention implementation details are sparse, requiring non-trivial modifications to standard LLM architectures that aren't fully specified
- Datastore construction from The Stack is described ambiguously, with conflicting descriptions of the sampling strategy
- Heuristic-dependent components (skip tokens, cache activation threshold) lack systematic justification and sensitivity analysis
- Evaluation is limited to Python repositories, raising questions about generalizability to other programming languages

## Confidence

**High Confidence:** The draft-then-verify paradigm, multi-source retrieval, and cache mechanism are well-supported by both theoretical arguments and experimental evidence. Core speedup claims (up to 2.53x) are consistently demonstrated across multiple datasets.

**Medium Confidence:** Dynamic retrieval timing and weighted Trie construction are plausible based on the paper's reasoning, but evidence is primarily heuristic. The "localness" assumption about code repetition is reasonable but not universally proven.

**Low Confidence:** Tree attention implementation details are insufficient for reproduction. The paper claims this is crucial for efficiency but doesn't provide enough technical depth to verify the implementation.

## Next Checks

1. **Tree Attention Implementation Verification:** Implement the tree attention mask as described and measure the actual computational overhead compared to standard attention. Verify that it achieves the claimed efficiency gains for verifying multiple candidate drafts simultaneously.

2. **Parameter Sensitivity Analysis:** Systematically vary the skip probability $p$, cache activation threshold $l$, and retrieval parameters ($n_{max}$, $\alpha$, $\beta$) across different codebases to assess the robustness of FastCoder's performance and identify optimal configurations.

3. **Cross-Domain Generalization:** Test FastCoder on codebases from different programming languages, domains (web development vs. scientific computing), and team sizes to validate whether the "localness" and "LLM preference" assumptions hold beyond the Python repositories used in the experiments.