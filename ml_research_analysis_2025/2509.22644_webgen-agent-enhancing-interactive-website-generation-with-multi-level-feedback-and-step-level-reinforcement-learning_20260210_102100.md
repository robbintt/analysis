---
ver: rpa2
title: 'WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback
  and Step-Level Reinforcement Learning'
arxiv_id: '2509.22644'
source_url: https://arxiv.org/abs/2509.22644
tags:
- screenshot
- gui-agent
- feedback
- website
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "WebGen-Agent improves website code generation by using multi-level\
  \ visual feedback\u2014screenshot descriptions and GUI-agent testing scores\u2014\
  to iteratively refine both appearance and functionality. It integrates these scores\
  \ with backtracking and select-best mechanisms, and further enhances model performance\
  \ through Step-GRPO training using step-level rewards from screenshots and GUI-agent\
  \ testing."
---

# WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.22644
- Source URL: https://arxiv.org/abs/2509.22644
- Reference count: 40
- Primary result: WebGen-Agent improves website code generation accuracy from 26.4% to 51.9% using multi-level visual feedback and Step-GRPO training

## Executive Summary
WebGen-Agent addresses the challenge of generating interactive websites by introducing a multi-level feedback system that combines visual and functional evaluation. The system iteratively refines code using screenshot-based appearance scores from a VLM and GUI-agent testing scores for functionality. Through backtracking and select-best mechanisms, it recovers from quality degradation during multi-step generation. The approach is further enhanced by Step-GRPO training that uses step-level rewards, improving model performance from 38.9% to 45.4% accuracy on the WebGen-Bench dataset.

## Method Summary
WebGen-Agent employs a multi-level feedback loop for interactive website generation. After each code edit, the system executes the code in a WebContainer environment and captures a screenshot. This screenshot is evaluated by Qwen2.5-VL-32B to generate an appearance score and suggestions. If the visual quality is sufficient, a GUI-agent tests the interactive elements and provides a functionality score. The system maintains a memory of all states and scores, enabling backtracking to high-scoring previous states after 5 consecutive failures. At task completion, the best-scoring state is selected. For training, Step-GRPO uses immediate step-level rewards (appearance + functionality scores) to provide dense supervision, outperforming outcome-only rewards.

## Key Results
- WebGen-Agent increases Claude-3.5-Sonnet accuracy from 26.4% to 51.9% and appearance score from 3.0 to 3.9
- Step-GRPO training improves Qwen2.5-Coder-7B-Instruct accuracy from 38.9% to 45.4% and appearance score from 3.4 to 3.7
- The system outperforms Bolt.diy baseline across multiple metrics on WebGen-Bench dataset
- Adding screenshot and GUI-agent feedback improves accuracy from 39.7% to 49.9% compared to execution-only feedback

## Why This Works (Mechanism)

### Mechanism 1: Multi-Modal Feedback Loop for Visual and Functional Alignment
The system decouples code generation from evaluation, using a VLM for aesthetic assessment and a GUI-agent for functional testing. After code execution, a screenshot generates an appearance score ($Score_{shot}$) and suggestions, while successful visual evaluation triggers GUI-agent testing for functionality score ($Score_{gui}$). This dual feedback drives iterative refinement, with ablation showing improvement from 39.7% to 49.9% accuracy when adding visual and functional feedback.

### Mechanism 2: Trajectory Optimization via Backtracking and Selection
The agent maintains memory of step states and scores, backtracking to the highest-scoring state after 5 consecutive execution errors. At completion, it selects the best step rather than defaulting to the final step. This addresses non-monotonic quality in multi-step generation, improving accuracy from 49.9% to 52.6% when adding these mechanisms.

### Mechanism 3: Dense Process Supervision via Step-GRPO
Step-GRPO uses immediate step-level rewards (appearance + functionality scores) instead of sparse final rewards, providing dense supervision for policy optimization. By standardizing immediate rewards and removing KL loss constraints, the method improves convergence, outperforming naive outcome GRPO (42.5% vs 45.4% accuracy).

## Foundational Learning

**Group Relative Policy Optimization (GRPO)**
- Why needed: WebGen-Agent modifies standard GRPO to create Step-GRPO, using group statistics for baseline estimation
- Quick check: In Equation 3, how is the advantage $\hat{A}$ computed differently in Step-GRPO compared to standard GRPO?

**Vision-Language Models (VLMs) as Critics**
- Why needed: The system relies on VLMs not just to "see" but to "grade" and "suggest" website quality
- Quick check: Why does the paper use Qwen2.5-VL-32B instead of the coding LLM itself for visual feedback?

**Automated GUI Testing**
- Why needed: Functional validation requires automated interaction with generated websites
- Quick check: What specific prompt engineering ensures the GUI-agent covers the user's requirements comprehensively?

## Architecture Onboarding

**Component map:**
Instruction → Code Gen (Coding LLM) → Execution (WebContainer) → VLM Critique (Visual) → [If Visual OK] → GUI-Agent Test (Functional) → [If Functional Fail] → Refine Loop

**Critical path:**
The agent generates code, executes it, captures screenshots, and receives visual feedback from VLM. If visual quality is sufficient, GUI-agent testing follows. The loop continues until completion or failure, with backtracking triggered after 5 consecutive execution errors. Final selection picks the best-scoring state from the trajectory.

**Design tradeoffs:**
- Feedback Source: Uses Qwen2.5-VL-32B instead of GPT-4o for cost efficiency, trading slight accuracy for computational savings
- Reward Density: Uses immediate step rewards rather than cumulative rewards, arguing website quality is state-dependent not path-dependent
- Model Scale: Step-GRPO demonstrated on 7B-8B models; scaling to larger models presents computational challenges

**Failure signatures:**
- Local Maximum Trap: Agent repeatedly fixes visual issues while ignoring functional regressions until GUI-agent runs
- Hallucinated Fixes: VLM suggests changes that the agent implements, but the VLM fails to detect, leading to repeated feedback

**First 3 experiments:**
1. Run agent on 10 samples with only execution feedback vs. full visual+GUI feedback; compare appearance scores to reproduce Table 2
2. Train small model using only $Score_{shot}$ vs. only $Score_{gui}$ vs. Sum; verify Sum provides best balance (Table 3)
3. Manually inspect 20 VLM/GUI-agent scores vs. human judgment to verify claimed 93-96% accuracy (Table 5)

## Open Questions the Paper Calls Out
None

## Limitations
- VLM and GUI-agent reliability are critical bottlenecks; accuracy validation was performed on only 20 samples
- Step-GRPO training demonstrated only on 7B-8B models; scaling to larger models presents significant computational challenges
- The feedback loop could be brittle if VLM hallucinates suggestions or GUI-agent cannot reliably interact with dynamic elements

## Confidence

**Multi-Modal Feedback Loop (Mechanism 1):** High confidence - Strong empirical support from ablation results showing clear performance gains
**Trajectory Optimization (Mechanism 2):** Medium confidence - Improvement demonstrated but non-monotonic quality assertion needs deeper validation
**Step-GRPO Training (Mechanism 3):** High confidence - Clear ablation results and connection to established stepwise feedback work

## Next Checks

1. Scale the GUI-agent testing reliability study by manually inspecting 100+ samples against human judgment to verify functional accuracy and identify failure modes
2. Test the VLM feedback loop for hallucination susceptibility by logging all VLM suggestions and manually verifying a sample to identify problematic cases
3. Benchmark Step-GRPO on larger models with a pilot study assessing computational cost and performance gains on 30B-parameter models