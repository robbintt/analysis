---
ver: rpa2
title: 'AIR: Post-training Data Selection for Reasoning via Attention Head Influence'
arxiv_id: '2512.13279'
source_url: https://arxiv.org/abs/2512.13279
tags:
- reasoning
- data
- which
- theta
- separation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AIR (Attention Influence for Reasoning),
  a mechanism-driven data selection framework for post-training reasoning in LLMs.
  The core idea is to identify reasoning-critical attention heads, construct a weakened
  reference model by disabling their influence, and quantify loss divergence to score
  data importance at both step and sample levels.
---

# AIR: Post-training Data Selection for Reasoning via Attention Head Influence

## Quick Facts
- arXiv ID: 2512.13279
- Source URL: https://arxiv.org/abs/2512.13279
- Authors: Jinrui Liu; Jeff Wu; Xuanguang Pan; Gavin Cheung; Shuai Ma; Chongyang Tao
- Reference count: 31
- Primary result: AIR framework improves reasoning accuracy by 15.89% over baseline using step-level weighted fine-tuning

## Executive Summary
This paper introduces AIR (Attention Influence for Reasoning), a mechanism-driven data selection framework for post-training reasoning in LLMs. The core idea is to identify reasoning-critical attention heads, construct a weakened reference model by disabling their influence, and quantify loss divergence to score data importance at both step and sample levels. This enables step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show AIR consistently improves reasoning accuracy, surpassing heuristic baselines and manual curation while achieving comparable performance to models trained on much larger datasets. The method establishes an efficient, interpretable approach for selecting high-value post-training data in reasoning distillation.

## Method Summary
AIR operates in three stages: (1) Retrieval Head Identification - computes a Retrieval Score Rh for each attention head by measuring token copying behavior from context, selecting top 5% as reasoning-critical; (2) Reference Model Construction - creates a weakened model by masking these heads with uniform attention distributions; (3) Influence Scoring - computes loss divergence between base and weakened models at token, step, and sample levels. For step-level fine-tuning, top 20% of reasoning steps receive doubled weights. For sample selection, data is ranked by relative loss divergence within categories. The framework was applied to Qwen2.5-32B-Instruct using s1 datasets, achieving state-of-the-art performance with minimal training data.

## Key Results
- Step-level weighted fine-tuning improves average accuracy from 65.43% to 70.32% (15.89% gain)
- Sample-level selection achieves comparable performance to R1-distill-Qwen-32B trained on 800K examples using only 1K selected examples
- AIR consistently outperforms manual curation and heuristic baselines across AIME 2024/2025, MATH500, and GPQA Diamond benchmarks
- Performance is robust to hyperparameter variations, with optimal settings at P=20% critical steps and α=2.0 weight multiplier

## Why This Works (Mechanism)

### Mechanism 1: Retrieval Head Identification for Reasoning Criticality Detection
- **Claim:** Specialized attention heads ("retrieval heads") that perform token-level copying operations can be identified and serve as proxies for reasoning-critical computation pathways.
- **Mechanism:** A retrieval score R_h is computed for each head by measuring the proportion of context tokens that satisfy two conditions: (C1) the generated token exists in the context, and (C2) the corresponding position receives maximal attention weight. Heads in the top δ percentile are classified as reasoning-critical.
- **Core assumption:** Retrieval heads are mechanistically responsible for maintaining factual integrity and supporting multi-step reasoning chains; their engagement correlates with reasoning step importance.
- **Evidence anchors:**
  - [Section 2.1]: "Formally, a retrieval head h serves to preserve contextual fidelity by enabling accurate, token-level transfer of information from the source text to the generated output."
  - [Section 2.1]: "Attention heads exhibiting a high Rh are definitively classified as reasoning-critical retrieval heads."
  - [Corpus]: Related work "AttentionInfluence" (arXiv:2505.07293) applies similar retrieval head concepts to pretraining data selection, suggesting the mechanism generalizes across training stages.

### Mechanism 2: Loss Divergence as Causal Influence Metric
- **Claim:** The loss gap between a strong base model and a weakened reference model (with retrieval heads masked) quantifies the causal importance of reasoning steps and samples.
- **Mechanism:** After identifying H_critical retrieval heads, they are masked by setting attention weights to uniform distributions during forward pass. The Attention Influence Score S = Σ_t Δℓ(x_t) / Σ_t ℓ(θ_base, x_t) measures relative loss increase, indicating how much a sample/step relies on the masked reasoning mechanisms.
- **Core assumption:** Masking retrieval heads selectively degrades reasoning capability while preserving other model functions; larger loss divergence indicates greater dependence on reasoning-critical mechanisms.
- **Evidence anchors:**
  - [Abstract]: "AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score."
  - [Section 2.2]: "A positive loss gap (Δℓ(x_t) > 0) signifies that the base model's performance on token x_t heavily relied on the functional retrieval mechanism."
  - [Corpus]: Limited direct validation in corpus; mechanism relies on mechanistic interpretability literature (Wu et al., 2024; Hua et al., 2025) cited within the paper.

### Mechanism 3: Dual-Level Weighted Supervision via Step and Sample Scoring
- **Claim:** Aggregating token-level influence scores to step-level and sample-level enables both fine-grained weighted SFT and global sample selection, improving distillation efficiency.
- **Mechanism:** Step-level scores S_step^(k) average loss divergence across tokens within each reasoning step; top P% receive amplified weights (1 + (α-1)) in SFT loss. Sample-level scores S_samp(x) use relative loss divergence to rank and select high-value samples from large pools.
- **Core assumption:** Reasoning steps with higher influence scores are more valuable for training; prioritizing them improves knowledge transfer without requiring proportional data scaling.
- **Evidence anchors:**
  - [Table 1]: "AIR-Step method substantially improves the average accuracy to 70.32%" compared to s1K-1.1 baseline (65.43%), demonstrating step-level weighting efficacy.
  - [Section 3.4]: "Models trained on only 1K examples selected by AIR even outperform R1-distill-Qwen-32B, which is trained on 800K examples."
  - [Corpus]: No direct corpus comparison for step-level weighting; sample-level selection aligns with broader influence-based data selection literature.

## Foundational Learning

- **Concept: Attention Head Function in Transformers**
  - **Why needed here:** AIR fundamentally operates on identifying and manipulating specific attention heads; understanding their role in information routing is prerequisite.
  - **Quick check question:** Can you explain how attention weights determine token-to-token information flow in a transformer layer?

- **Concept: Cross-Entropy Loss in Language Model Training**
  - **Why needed here:** The AIR score is computed as loss divergence (difference in cross-entropy between base and weakened models); interpreting this metric requires understanding what loss represents.
  - **Quick check question:** What does a higher cross-entropy loss indicate about a model's prediction quality for a given token?

- **Concept: Chain-of-Thought Reasoning Structure**
  - **Why needed here:** AIR segments reasoning into discrete steps (typically delimited by line breaks) and aggregates scores per-step; understanding CoT format is essential for proper segmentation.
  - **Quick check question:** How would you programmatically segment a multi-step reasoning trace into discrete steps for scoring?

## Architecture Onboarding

- **Component map:**
  1. Retrieval Head Detector -> Reference Model Constructor -> Influence Scorer -> Weighted SFT Trainer/Sample Selector
  2. Head identification → Reference model construction → Loss computation on candidate data → Score aggregation → (Branch A) Step-weighted SFT OR (Branch B) Sample selection → Fine-tuning

- **Critical path:**
  Head identification → Reference model construction → Loss computation on candidate data → Score aggregation → (Branch A) Step-weighted SFT OR (Branch B) Sample selection → Fine-tuning

- **Design tradeoffs:**
  - **δ (head selection threshold)**: Paper uses 0.05 (top 5%); lower values may miss important heads, higher may include noise
  - **P% (critical step ratio)**: Paper finds 20% optimal; higher ratios include redundant steps, degrading performance (Figure 2)
  - **α (weight multiplier)**: Paper uses 2.0; aggressive weighting (5x-10x) causes overfitting to specific fragments
  - **Sample-level vs step-level**: Sample-level scales to large pools but loses granularity; step-level requires more compute per sample

- **Failure signatures:**
  - Low loss divergence across all samples: May indicate δ too low or retrieval head identification failed
  - Performance degradation after weighting: Check if α is too high or P% too aggressive
  - Selected samples cluster too tightly (PCA visualization): May lack diversity; consider mixing with random samples
  - No improvement over random baseline: Verify retrieval heads are correctly identified; check if base model has reasoning capability to measure

- **First 3 experiments:**
  1. **Validate retrieval head identification**: Run R_h computation on a held-out reasoning dataset; manually inspect top-scoring heads to confirm they attend to copied tokens
  2. **Ablate masking strategy**: Compare uniform masking vs zeroing vs removing heads entirely to ensure loss divergence is due to reasoning degradation, not general perturbation
  3. **Hyperparameter sensitivity scan**: On a small validation set, test δ ∈ {0.03, 0.05, 0.10}, P% ∈ {10, 20, 30}, α ∈ {1.5, 2.0, 3.0} to characterize performance surface before large-scale training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:**
  Can the Attention Influence Score be normalized to enable meaningful cross-domain data selection?
- **Basis in paper:**
  [explicit] The paper states in Section 2.3 that "Comparisons of S_sample are typically performed only within individual domains... since loss values cannot be meaningfully compared across domains."
- **Why unresolved:**
  The current methodology requires maintaining category proportions from the original dataset (e.g., Math vs. Science) and selecting samples within those buckets, preventing a truly global ranking of data quality across heterogeneous tasks.
- **What evidence would resolve it:**
  A normalization technique (e.g., z-score scaling within domains) that allows AIR to select a globally optimal subset without pre-defining domain quotas, resulting in equal or better performance than the stratified approach.

### Open Question 2
- **Question:**
  How sensitive is the framework's performance to the percentage of attention heads masked ($\delta$)?
- **Basis in paper:**
  [inferred] Section 2.2 fixes the threshold $\delta$ at 0.05 (top 5% of heads) citing prior work, but the Hyperparameter Analysis in Section 4.1 only varies the critical step ratio ($P$) and weight multiplier ($\alpha$), leaving the robustness of the head selection threshold untested.
- **Why unresolved:**
  It is unclear if masking fewer heads (stricter definition of "retrieval head") or more heads (including partial reasoners) would change the loss divergence signal $\Delta \ell(x_t)$ and subsequent data selection quality.
- **What evidence would resolve it:**
  An ablation study showing the downstream reasoning accuracy of models trained on data selected with varying $\delta$ values (e.g., 1%, 10%, 20%).

### Open Question 3
- **Question:**
  Does the mechanism of "retrieval heads" used for AIR generalize to model architectures other than Qwen?
- **Basis in paper:**
  [inferred] The experiments are restricted to the Qwen2.5 family (7B for scoring, 32B for training). The identification of "retrieval heads" relies on specific attention dynamics that may differ in architectures like Llama or Mistral.
- **Why unresolved:**
  The paper does not demonstrate if the "retrieval head" identification criterion (Equation 2) successfully isolates reasoning-critical heads in non-Qwen models, or if the correlation between head masking and loss divergence holds universally.
- **What evidence would resolve it:**
  Application of the AIR framework to a Llama-3 or Mistral backbone, verifying if the identified heads perform "copy-paste" operations and if the resulting data selection improves reasoning benchmarks.

## Limitations

- **Limited head function validation:** The paper identifies retrieval heads via R_h score but provides limited empirical validation that these heads mechanistically support reasoning rather than simple factual recall
- **Architecture-specific findings:** All experiments use Qwen2.5 models; the generalization of retrieval head identification and influence scoring to other architectures (Llama, Mistral) remains untested
- **Cross-domain normalization challenge:** AIR scores cannot be meaningfully compared across domains, requiring manual category balancing that may not scale to heterogeneous data pools

## Confidence

- **High Confidence**: Step-level weighted fine-tuning improves reasoning accuracy (15.89% gain over baseline) - supported by direct experimental comparison and consistent across multiple benchmarks
- **Medium Confidence**: Sample-level selection achieves comparable performance to 800K-example training using only 1K selected examples - while results show this, the comparison relies on a single distillation baseline and may not generalize across model families
- **Medium Confidence**: Retrieval head identification via Rh score successfully identifies reasoning-critical heads - the mechanism is well-specified but limited empirical validation of head function beyond R_h computation
- **Low Confidence**: AIR scores correlate with reasoning step importance rather than superficial features like length - the paper demonstrates this via ablation but doesn't rule out all confounding factors

## Next Checks

1. **Head Function Validation via Targeted Ablation**: Select the top 5 retrieval heads identified by AIR, perform controlled ablation (zeroing vs uniform masking vs complete removal), and measure degradation specifically on multi-step reasoning tasks vs simple factual recall tasks. This would confirm whether these heads mechanistically support reasoning rather than general information retrieval.

2. **Cross-Architecture Head Transferability**: Apply the same AIR head identification procedure to a different architecture (e.g., Llama or Mistral family) and verify whether identified heads show similar loss divergence patterns when masked. This tests whether the retrieval head proxy assumption generalizes beyond Qwen models.

3. **Score Correlation with Reasoning Complexity**: Construct a controlled evaluation set with reasoning problems of varying step complexity (2-step, 5-step, 10-step chains) and measure AIR scores against human-annotated reasoning difficulty. This would validate that loss divergence captures reasoning complexity rather than just token copying behavior.