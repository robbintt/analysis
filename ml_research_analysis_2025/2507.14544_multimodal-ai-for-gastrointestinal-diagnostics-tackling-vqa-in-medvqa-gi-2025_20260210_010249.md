---
ver: rpa2
title: 'Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI
  2025'
arxiv_id: '2507.14544'
source_url: https://arxiv.org/abs/2507.14544
tags:
- visual
- question
- dataset
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach for visual question answering (VQA)
  in gastrointestinal endoscopy as part of the ImageCLEFmed MEDVQA 2025 Challenge.
  The authors adopt Florence-2, a large-scale multimodal foundation model, as the
  core VQA pipeline, leveraging its unified vision-language encoder-decoder architecture.
---

# Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025

## Quick Facts
- arXiv ID: 2507.14544
- Source URL: https://arxiv.org/abs/2507.14544
- Reference count: 31
- One-line primary result: Florence-2 with domain-specific augmentations achieves BLEU 0.16, ROUGE-L 0.88, METEOR 0.49 on private test set in MEDVQA-GI 2025.

## Executive Summary
This paper presents a multimodal approach for visual question answering in gastrointestinal endoscopy as part of the ImageCLEFmed MEDVQA 2025 Challenge. The authors employ Florence-2, a large-scale multimodal foundation model, as the core VQA pipeline, enhanced with domain-specific augmentations to preserve critical medical features in endoscopic images. The model is fine-tuned on a 1% stratified subset of the Kvasir-VQA dataset, with the vision backbone frozen and the language decoder trained using AdamW optimization. Results show strong performance on spatial and binary question types, with ablation studies confirming the value of domain-aware augmentations over heavy or no augmentation.

## Method Summary
The approach uses Florence-2-base-ft with its frozen ViT-L/14 vision encoder and fine-tuned multimodal encoder-decoder. Domain-specific augmentations are applied to preserve critical medical features in endoscopic images. The model is trained on a 1% stratified subset of Kvasir-VQA using AdamW optimization (learning rate 7.8e-6, cosine decay over 20 epochs, weight decay 0.1). Training uses mixed precision, gradient clipping, and early stopping. Evaluation metrics include BLEU, ROUGE-L, and METEOR scores.

## Key Results
- Fine-tuned augmentations achieved the best performance: BLEU 0.16, ROUGE-L 0.88, METEOR 0.49 on private test set
- Spatial and binary question types showed stronger performance than other question categories
- Ablation studies confirmed domain-aware augmentations outperform heavy or no augmentation strategies
- The approach demonstrates the potential of large multimodal models in medical VQA applications

## Why This Works (Mechanism)
Florence-2's unified vision-language architecture provides a strong foundation for multimodal understanding. The frozen vision encoder preserves general visual feature extraction capabilities while the fine-tuned decoder adapts to medical question-answering tasks. Domain-specific augmentations maintain critical endoscopic features that could be lost with generic transformations, improving the model's ability to focus on medically relevant information. The stratified sampling ensures representative coverage of the dataset's diversity, while the optimization strategy balances learning rate adaptation with regularization.

## Foundational Learning

**Vision Transformers (ViT)**: Why needed - enable patch-based visual feature extraction for multimodal models; Quick check - verify input resolution matches expected 896×896

**Multimodal Encoder-Decoder**: Why needed - combines visual and language understanding for generation tasks; Quick check - confirm output sequence length matches answer length requirements

**Domain-Specific Augmentation**: Why needed - preserves critical medical features that generic augmentations might distort; Quick check - validate augmentations don't remove or alter pathological regions

**Metric-Based Evaluation**: Why needed - provides standardized assessment of generated answer quality; Quick check - ensure all metrics are calculated correctly using official implementations

**Fine-tuning Strategy**: Why needed - balances transfer learning benefits with task-specific adaptation; Quick check - verify freezing strategy preserves pretrained capabilities

## Architecture Onboarding

Component Map: Image → ViT-L/14 Vision Encoder → Multimodal Encoder-Decoder → Text Output

Critical Path: Endoscopic image input → vision encoding → multimodal fusion → answer generation → metric evaluation

Design Tradeoffs: Freezing vision encoder limits task-specific feature adaptation but preserves general capabilities; small batch size constrains convergence but fits memory constraints; 1% subset enables rapid experimentation but may limit generalization.

Failure Signatures: Heavy augmentations cause ROUGE-L to drop to 0.48; low BLEU despite high ROUGE-L indicates semantic correctness but n-gram mismatch; poor performance on certain question types suggests limited reasoning capabilities.

First Experiments:
1. Baseline training without augmentations to establish performance floor
2. Standard augmentation application to verify general improvement
3. Domain-specific augmentation tuning to optimize medical feature preservation

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on 1% subset may not fully represent model performance on full dataset
- Freezing the vision encoder limits task-specific feature adaptation capabilities
- Performance differences between BLEU and ROUGE-L suggest limitations of n-gram metrics for medical VQA
- Generalizability to other GI datasets and clinical settings remains to be demonstrated

## Confidence

**High confidence**: Florence-2 as core model choice, domain augmentation superiority over alternatives, spatial/binary question performance advantage

**Medium confidence**: Absolute metric values, effectiveness of specific augmentation parameters, performance on challenge test sets

**Low confidence**: Generalizability to other medical domains, impact of frozen vision encoder strategy, optimal augmentation composition

## Next Checks
1. Re-run ablation study on augmentation strategies using baseline code to confirm domain-specific augmentation superiority
2. Construct 1% stratified subset following described splits and verify metric reproducibility
3. Submit trained model to MEDVQA-GI challenge (if accessible) to confirm reported private test scores