---
ver: rpa2
title: 'MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models
  through Activation Steering'
arxiv_id: '2510.04217'
source_url: https://arxiv.org/abs/2510.04217
tags:
- unlearning
- steering
- forget
- multimodal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal large language model
  (MLLM) unlearning, which aims to selectively erase designated information across
  vision-language modalities while preserving general utility. Existing training-based
  unlearning approaches are computationally expensive, irreversible, and often distort
  retained knowledge.
---

# MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering

## Quick Facts
- **arXiv ID:** 2510.04217
- **Source URL:** https://arxiv.org/abs/2510.04217
- **Reference count:** 40
- **Primary result:** Training-free test-time unlearning framework using activation steering outperforms baselines on LLaVA-1.5 and Qwen-2.5-VL

## Executive Summary
MLLMEraser introduces a novel training-free approach to multimodal large language model unlearning by leveraging activation steering. The method constructs a multimodal erasure direction through contrastive learning between adversarially perturbed knowledge-recall pairs and knowledge-erasure pairs, then applies input-aware steering that preserves retain-set utility while enforcing forgetting on designated content. Experiments demonstrate consistent outperformance over state-of-the-art baselines with minimal utility degradation and lower computational cost.

## Method Summary
The method constructs a multimodal erasure direction by contrasting adversarially perturbed knowledge-recall image-text pairs (D⁻) with knowledge-erasure counterparts (D⁺), computing d_erase as the mean difference in hidden activations. An input-aware steering function f(h) = W·P·h is learned via null-space projection onto retain-set activations, ensuring the steering minimally affects retained knowledge. At inference, the method injects λ·f(h) into hidden states at specified layers, achieving selective forgetting without parameter updates.

## Key Results
- Consistently outperforms state-of-the-art MLLM unlearning baselines on LLaVA-1.5 and Qwen-2.5-VL
- Achieves stronger forgetting performance with lower computational cost
- Maintains minimal utility degradation on retained knowledge and celebrity profiles
- Ablation shows multimodal direction construction and proper steering strength are critical

## Why This Works (Mechanism)

### Mechanism 1
Contrasting knowledge-recall against knowledge-erasure image-text pairs yields a multimodal steering vector encoding a "refusal direction" in activation space. The method constructs D⁻ (adversarial images + harmful prompts) and D⁺ (clean images + refusal-triggering prompts), computing d_erase = mean(h⁺) − mean(h⁻) at selected layers. This captures the semantic shift from recall → refusal across both visual and textual modalities. Break condition: If forget-set queries don't share activation structure with refusal behavior, steering fails.

### Mechanism 2
Null-space projection ensures steering vectors minimally affect retain-set activations. A direction-determining function f(h) = W·P·h is learned, where P projects into the left null space of H_r (retain-set activations). This guarantees W·P·H_r ≈ 0, collapsing f(h) to near-zero for retain inputs while mapping forget inputs toward d_erase. Break condition: If forget and retain activations are highly overlapping, the null-space constraint will over-constrain W.

### Mechanism 3
Adversarially perturbed images amplify the visual component of the erasure direction. PGD generates I′ maximizing log P(y | I′, Q) for harmful responses y ∈ Y_f. Pairing I′ with harmful Q creates a stronger knowledge-recall signal in D⁻ than clean images alone, making the contrastive difference more visually grounded. Break condition: If adversarial images trigger unrelated failure modes rather than targeted recall, the extracted direction becomes noisy.

## Foundational Learning

- **Activation Steering / Representation Engineering**: Shifting hidden activations via additive vectors without parameter updates. Quick check: Given two sets of activations {h⁺} and {h⁻}, how would you compute a steering vector, and where would you inject it during inference?

- **Null-Space Projection (Linear Algebra)**: Understanding why W·H_r ≈ 0 preserves retain-set behavior requires knowing that vectors in the left null space of H_r annihilate it. Quick check: If H_r has rank r < d, how would you compute a projection matrix P that maps any vector to the null space of H_r?

- **Adversarial Perturbations (PGD)**: Constructing D⁻ requires maximizing harmful response probability via bounded image perturbations. Quick check: Write the PGD update step for maximizing log P(y | I, Q) subject to ‖I − I_clean‖_∞ ≤ ε.

## Architecture Onboarding

- **Component map:** Data preparation (forget/retain/test sets) → Contrastive set construction (D⁻/D⁺) → Erasure direction extraction (d_erase) → Input-aware function learning (W, P) → Inference hook (λ·f(h) injection)

- **Critical path:** (1) Verify model has exploitable refusal behavior → (2) Generate adversarial images successfully → (3) Extract d_erase with clear separation → (4) Learn W with non-trivial null-space component → (5) Test steering on held-out forget/retain samples

- **Design tradeoffs:** λ (steering strength): Higher → stronger forgetting but risk of over-steering; paper uses λ ∈ [0.25, 0.3]. ε (perturbation budget): Moderate (16/255) works best; too large introduces noise. Layer selection: Paper steers all layers; partial-layer variants underperform. Linear vs. MLP f(h): Linear outperforms MLP (MLP overfits).

- **Failure signatures:** Forget accuracy remains high → d_erase may not capture refusal direction; check contrastive pair quality. Retain accuracy drops sharply → null-space constraint may be insufficient; verify P correctly computed. Model outputs gibberish → λ too large; reduce steering strength. Adversarial images ineffective → ε too small or PGD not converging; increase iterations or budget.

- **First 3 experiments:** (1) Sanity check: Verify d_erase alone steers forget queries toward refusal while degrading retain queries—confirms need for selective application. (2) Ablation: Compare text-only, image-only, and multimodal d_erase on forget-set ROUGE-L and retain-set accuracy—validates multimodal construction. (3) Hyperparameter sweep: Vary λ ∈ {0.1, 0.2, 0.3, 0.4} and ε ∈ {8, 16, 32, 64}/255; plot forget vs. retain trade-off to identify stable operating region.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can MLLMEraser effectively generalize to video–language models and embodied agents where temporal dependencies and interactive multimodal dynamics introduce new challenges for erasure direction construction? [explicit] Section 5 states: "For future work, we plan to extend MLLMEraser to video–language models and embodied agents" and Section 6 notes these scenarios involve "temporal dependencies and interactive multimodal dynamics."

- **Open Question 2:** Does the reliance on adversarially perturbed images and hand-crafted prompts limit MLLMEraser's generalization to diverse domains and subtle knowledge types beyond the tested privacy-sensitive benchmarks? [explicit] Section 6: "The construction of multimodal erasure directions relies on adversarially perturbed images and hand-crafted prompts, which may not generalize across domains or subtle knowledge types."

- **Open Question 3:** How can the direction-determining function f(h) be enriched beyond the current linear transformation to enable finer-grained, concept-specific steering without sacrificing the parameter-free advantage? [explicit] Section 5: "develop richer forms of the direction-determining function for finer-grained steering." Appendix I shows MLP variants underperform, suggesting the form of richness matters.

## Limitations
- Efficacy relies heavily on the existence of robust refusal behavior in the target MLLM, which may not generalize across model families or fine-tuning regimes
- Adversarial image generation requires careful hyperparameter tuning with minimal implementation details provided
- Null-space projection assumes retain-set activations span a clean orthogonal subspace to the forget-set; high semantic overlap could severely limit effectiveness
- Limited ablation studies on layer selection; paper steers all layers without systematic investigation

## Confidence
- **High confidence**: The core mechanism of activation steering through contrastive direction extraction is well-founded and supported by existing representation engineering literature
- **Medium confidence**: The multimodal direction construction and null-space projection implementation details are technically sound but lack comprehensive empirical validation across diverse MLLM architectures
- **Medium confidence**: The claim of "training-free" unlearning is accurate for the inference-time steering component, though constructing D⁻/D⁺ and learning W requires dataset preparation that may be computationally intensive

## Next Checks
1. **Layer sensitivity analysis**: Systematically vary which layers receive steering injection (e.g., only early, only late, all, alternating) and measure the trade-off between forgetting efficacy and utility preservation across the same evaluation metrics

2. **Cross-model generalizability test**: Apply the exact MLLMEraser pipeline to a different MLLM family (e.g., GPT-4V or Gemini) without architectural modifications and evaluate whether the same refusal behavior extraction and steering direction generalize

3. **Perturbation budget sensitivity**: Conduct a detailed ablation across ε values (4, 8, 16, 32, 64/255) while varying PGD iterations and step sizes, measuring both the strength of the extracted d_erase and the downstream unlearning performance to identify the optimal adversarial generation regime