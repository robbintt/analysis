---
ver: rpa2
title: 'JoyTTS: LLM-based Spoken Chatbot With Voice Cloning'
arxiv_id: '2507.02380'
source_url: https://arxiv.org/abs/2507.02380
tags:
- module
- joytts
- voice
- text
- cloning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JoyTTS is an end-to-end spoken chatbot that integrates large language
  models with text-to-speech technology, featuring voice cloning capabilities. It
  combines the MiniCPM-o and CosyVoice2 models, trained on 2000 hours of conversational
  data.
---

# JoyTTS: LLM-based Spoken Chatbot With Voice Cloning

## Quick Facts
- arXiv ID: 2507.02380
- Source URL: https://arxiv.org/abs/2507.02380
- Authors: Fangru Zhou; Jun Zhao; Guoxin Wang
- Reference count: 6
- Primary result: JoyTTS achieves SS=0.73 and WER=5.09 on seed-tts-zh dataset with 1.8s latency on NVIDIA 4090D

## Executive Summary
JoyTTS is an end-to-end spoken chatbot that integrates large language models with text-to-speech technology, featuring voice cloning capabilities. It combines the MiniCPM-o and CosyVoice2 models, trained on 2000 hours of conversational data. The model uses hidden states from the LLM-Chat module to enhance the TTS module's contextual understanding, enabling effective voice cloning. JoyTTS achieves a speaker similarity (SS) score of 0.73 and a word error rate (WER) of 5.09 on the seed-tts-zh dataset. It demonstrates low latency of 1.8 seconds on a single NVIDIA 4090D, making it suitable for real-time interactions. The project is open-source, with training code and models available for further development.

## Method Summary
JoyTTS employs a two-stage training approach to integrate LLM-Chat and LLM-TTS modules. Stage 1 trains each module independently - LLM-Chat (Qwen-7B) aligns hidden states with text labels, while LLM-TTS learns text-to-speech conversion. Stage 2 performs joint training using combined loss to optimize interactions between text generation and speech synthesis. The key innovation transfers hidden layer features from LLM-Chat (3584-dim) to TTS module via MLP projection (768-dim), enriching embeddings with semantic context. Voice cloning is achieved by conditioning TTS on pre-processed prompt audio embeddings that encode speaker identity. The system was trained on 2000 hours of conversational data from RedGPT and GeneratedChat0.4M datasets, with data augmentation including text splitting and punctuation insertion.

## Key Results
- Speaker Similarity (SS) score of 0.73, outperforming baseline gpt-sovits (0.55)
- Word Error Rate (WER) of 5.09 on seed-tts-zh dataset
- Low latency of 1.8 seconds on NVIDIA 4090D
- Voice cloning capability demonstrated through conditioning on prompt audio embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferring hidden states from LLM-Chat to TTS module improves contextual semantic understanding in speech synthesis.
- Mechanism: LLM-Chat outputs 3584-dim hidden features mapped through MLP to 768-dim, combined with text embeddings via TTS_embed = Emb(y_i) + MLP(h_i).
- Core assumption: Hidden states contain semantic information beyond token embeddings that improves prosody and coherence.
- Evidence anchors: Abstract states hidden layer features improve contextual understanding; section 1.2 emphasizes deeper semantic understanding; related LLM-TTS systems use similar representations.
- Break condition: If hidden states contain no additional semantic signal beyond token embeddings, the MLP projection adds computational overhead without return.

### Mechanism 2
- Claim: Voice cloning is achieved by conditioning TTS on pre-processed prompt audio embeddings that serve as speaker identity priors.
- Mechanism: During inference, LLM-Chat processes both prompt text and audio to generate TTS embeddings encoding speaker characteristics, used as prior knowledge for LLM-TTS.
- Core assumption: LLM-Chat can extract and encode speaker identity information from audio prompts into transferable representations.
- Evidence anchors: Section 1.2 describes prompt text/wave pre-processing for TTS embedding; SS score of 0.73 vs. gpt-sovits at 0.55 suggests effective voice cloning; related systems address speaker identity preservation.
- Break condition: If prompt audio quality is poor or embedding representation fails to capture vocal characteristics, cloned output won't match target speaker.

### Mechanism 3
- Claim: Two-stage training enables module specialization while allowing cross-module optimization.
- Mechanism: Stage 1 trains LLM-Chat and LLM-TTS independently, then Stage 2 performs joint training using combined loss (Loss = L_LLM-Chat + L_LLM-TTS).
- Core assumption: Independent pre-training establishes stable representations that joint training can fine-tune without destabilizing either module.
- Evidence anchors: Section 3 states separate training allows specialization without interference and joint training fine-tunes interactions; no direct corpus evidence validates this specific approach.
- Break condition: If Stage 1 is undertrained, joint training may destabilize; if overtrained, modules may not adapt to each other during joint fine-tuning.

## Foundational Learning

- **Hidden States / Intermediate Layer Representations**
  - Why needed here: The core innovation transfers LLM hidden states to TTS module; understanding what hidden states encode is essential.
  - Quick check question: If you extracted hidden states from layer 1 vs. layer 32 of a 32-layer LLM, which would contain more task-specific semantic information for TTS conditioning?

- **Speaker Similarity (SS) Metric**
  - Why needed here: Paper evaluates voice cloning using SS=0.73; understanding how this metric is computed is necessary to interpret results.
  - Quick check question: An SS score of 0.73 vs. 0.55—what does this quantitatively suggest about perceived speaker match, and what are the metric's limitations?

- **Mel Spectrogram**
  - Why needed here: Generator Module outputs Mel spectrograms before audio synthesis; understanding this intermediate representation is fundamental to neural TTS.
  - Quick check question: Why do TTS systems generate Mel spectrograms as intermediate representations rather than raw waveforms directly?

## Architecture Onboarding

- Component map:
  Tokenizer Module -> LLM-Chat Module (Qwen-7B) -> Projection Layer (MLP + Embedding) -> LLM-TTS Module (CosyVoice2-based) -> Generator Module

- Critical path:
  1. Input text/audio → Tokenizer → LLM-Chat
  2. LLM-Chat → hidden states (h_i) + text tokens (y_i)
  3. h_i (3584-dim) → MLP projection → 768-dim
  4. TTS_embed = Emb(y_i) + MLP(h_i)
  5. TTS_embed + prompt audio embedding → LLM-TTS → speech tokens
  6. Speech tokens → Generator → Mel spectrogram → audio

- Design tradeoffs:
  - CosyVoice2 vs. GPT-Sovits: Replaced MiniCPM-o's original TTS with CosyVoice2 specifically for voice cloning quality (SS improved from 0.55 to 0.73). Tradeoff: may introduce different latency or computational profile.
  - Prompt conditioning format: Using "Please repeat the following text" adds tokens but may improve instruction-following; alternative formats not explored.
  - WER 5.09 vs. CosyVoice2 standalone 1.45: End-to-end integration introduces content errors; pure TTS systems outperform on WER but lack chatbot capabilities.

- Failure signatures:
  - High WER with good SS: Text generation errors from LLM-Chat propagating to speech; check LLM-Chat training quality.
  - Low SS with low WER: Voice cloning conditioning failing; check prompt audio quality and embedding projection.
  - High latency >1.8s on 4090D: Bottleneck likely in autoregressive token generation; profile LLM-Chat and LLM-TTS token generation separately.

- First 3 experiments:
  1. Ablate hidden state transfer: Replace TTS_embed = Emb(y_i) + MLP(h_i) with TTS_embed = Emb(y_i) only; measure impact on SS, WER, and qualitative prosody naturalness.
  2. Vary prompt audio duration: Test voice cloning quality with 3s, 5s, 10s, and 30s prompt audio clips; plot SS vs. prompt length to identify minimum viable reference.
  3. Single-stage vs. two-stage training comparison: Train baseline with joint training from initialization; compare convergence speed, final SS, and WER to validate training design.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explicit emotion control inputs be integrated into the LLM-Chat module to automatically modulate the emotional tone of the TTS output?
- Basis in paper: The conclusion states, "Looking ahead, one promising area of development is the introduction of emotion control inputs within the LLM," to enable automatic modulation for optimal emotional tone.
- Why unresolved: Current architecture lacks defined mechanism for injecting or controlling emotional prosody.
- What evidence would resolve it: Modified architecture with emotion embedding interface, validated by MOS tests for emotional expressiveness and objective metrics for speaker similarity retention.

### Open Question 2
- Question: What specific factors in joint training or hidden state injection contribute to WER degradation compared to standalone TTS model?
- Basis in paper: JoyTTS achieves WER of 5.09 vs. CosyVoice2 standalone at 1.45; paper doesn't explain this disparity in content consistency.
- Why unresolved: Paper highlights improved speaker similarity but doesn't analyze why intelligibility (WER) suffered during LLM-TTS integration.
- What evidence would resolve it: Ablation study isolating TTS module with and without injected LLM hidden states to determine if semantic features introduce noise disrupting acoustic generation.

### Open Question 3
- Question: Does training on synthetic audio generated by target TTS architecture limit model's ability to generalize to real human speech nuances?
- Basis in paper: "Data Construction" notes 2000 hours converted to audio using CosyVoice2; relying on same model for data generation and final TTS could create feedback loop amplifying synthetic artifacts.
- Why unresolved: While ensuring data consistency, paper doesn't evaluate whether model learns to mimic synthetic characteristics rather than real human vocal diversity.
- What evidence would resolve it: Comparative evaluation of voice cloning performance using real human audio prompts versus synthetic prompts, analyzing for artifacts or reduction in naturalness.

## Limitations

- WER of 5.09 is significantly higher than standalone TTS systems (1.45), indicating end-to-end integration introduces content errors that aren't fully addressed.
- Voice cloning mechanism relies on LLM-Chat's ability to encode speaker identity from audio, but robustness to varying audio quality and environmental conditions isn't characterized.
- Two-stage training approach is theoretically sound but lacks empirical validation from the corpus or ablation studies within the paper.

## Confidence

- **High Confidence**: Architectural framework and module connections are clearly specified, with hidden state transfer mechanism having direct textual support from abstract and section 1.2.
- **Medium Confidence**: Voice cloning mechanism is described with supporting evidence (SS score improvement), but underlying audio-to-embedding encoding process lacks detailed validation.
- **Medium Confidence**: Two-stage training approach is logically justified but lacks direct empirical validation from the corpus or ablation studies.

## Next Checks

1. **Ablation study of hidden state transfer**: Remove MLP(h_i) component from TTS_embed = Emb(y_i) + MLP(h_i) and measure changes in SS, WER, and qualitative prosody naturalness to confirm hidden state transfer provides measurable benefit beyond text tokens alone.

2. **Voice cloning robustness testing**: Evaluate SS scores across systematically varied prompt audio conditions (different durations: 3s, 5s, 10s, 30s; different recording qualities; different speaker-to-microphone distances) to establish minimum viable reference and identify failure thresholds.

3. **Single-stage vs. two-stage training comparison**: Train identical model architecture using joint training from initialization (skipping independent pre-training) and compare convergence speed, final SS, and WER metrics to empirically validate whether two-stage approach provides measurable advantages.