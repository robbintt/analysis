---
ver: rpa2
title: 'CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework'
arxiv_id: '2504.12576'
source_url: https://arxiv.org/abs/2504.12576
tags:
- event
- multimodal
- tasks
- pre-training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CM3AE, a unified pre-training framework for
  RGB frames and event-voxel/frame data designed to enhance multimodal perception
  in event camera applications. The key innovation lies in integrating a multimodal
  fusion reconstruction module and a multimodal contrastive learning strategy within
  a dual-branch masked autoencoder architecture, enabling effective cross-modal information
  aggregation and global feature alignment.
---

# CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework

## Quick Facts
- arXiv ID: 2504.12576
- Source URL: https://arxiv.org/abs/2504.12576
- Reference count: 40
- Key outcome: Achieves 53.18% Top-1 accuracy on event-based action recognition and 53.40% on RGB-Event action recognition with unified pre-training framework

## Executive Summary
This paper introduces CM3AE, a unified pre-training framework for RGB frames and event camera data (voxels/frames) designed to enhance multimodal perception in event camera applications. The key innovation lies in integrating a multimodal fusion reconstruction module and a multimodal contrastive learning strategy within a dual-branch masked autoencoder architecture, enabling effective cross-modal information aggregation and global feature alignment. A large-scale dataset (REV2M) containing 2.5 million paired RGB-Event samples was constructed for training. Extensive experiments on five downstream tasks—including human action recognition, object detection, and visual object tracking—demonstrate that CM3AE outperforms existing pre-trained models with significant improvements across all tasks.

## Method Summary
CM3AE employs a dual-branch ViT-B/16 encoder architecture with 75% masking ratio, where half of the unmasked patches are positionally aligned between RGB and Event modalities to prevent information leakage. The framework integrates three key components: (1) a multimodal fusion reconstruction module that reconstructs RGB images from fused features, (2) a multimodal contrastive learning module that aligns features in shared latent space, and (3) a voxel encoder for processing event stream data. The combined loss function (L = Lm + Lf + Lcl) optimizes masked reconstruction, fusion reconstruction, and contrastive alignment simultaneously. The model is pre-trained on the REV2M dataset (2.5M samples) and demonstrates strong transfer performance across five downstream tasks.

## Key Results
- Achieves 53.18% Top-1 accuracy on event-based action recognition (HARDVS dataset)
- Achieves 53.40% Top-1 accuracy on RGB-Event action recognition, outperforming ImageNet pre-training by 19.3%
- Improves visual object tracking SR from 62.0% to 62.6% using pre-trained fusion module
- Shows strong few-shot learning capability (43.68% with 10% data vs 6.65% with ImageNet MAE)
- Outperforms state-of-the-art pre-trained models across human action recognition, object detection, and tracking tasks

## Why This Works (Mechanism)

### Mechanism 1: Dual-Branch Masked Autoencoder with Positional Alignment
- **Claim:** High masking ratio with partially aligned unmasked patches enables cross-modal learning while preventing information leakage.
- **Mechanism:** Mask 75% of patches in both RGB and Event inputs, ensuring half of the unmasked patches occupy identical spatial positions across modalities. This forces the model to learn from limited visible information while the aligned patches provide anchor points for cross-modal correspondence without revealing masked regions.
- **Core assumption:** Event and RGB modalities share meaningful spatial correspondence that can be exploited for representation learning.
- **Evidence anchors:**
  - [abstract]: "dual-branch masked autoencoder architecture, enabling effective cross-modal information aggregation"
  - [section 3.2]: "we ensure that half of the unmasked patches in the RGB and Event modalities are identical. This design prevents the multimodal fusion reconstruction module from leaking information about the masked patches"
  - [Table 3]: 75% masking ratio achieves 53.24% Top1 accuracy vs. 51.27% at 50% and 50.78% at 85%
  - [corpus]: Limited direct validation of this specific alignment mechanism; related papers focus on downstream applications

### Mechanism 2: Multimodal Fusion Reconstruction Module (MFRM)
- **Claim:** Reconstructing RGB images from fused multimodal features forces the model to extract and integrate complementary cross-modal information.
- **Mechanism:** Concatenate encoded features from multiple modalities (RGB+Event and RGB+Event+Voxel), pass through a fusion Transformer block, then reconstruct the original RGB from this fused representation. The reconstruction loss provides direct supervision for learning useful fusion.
- **Core assumption:** Successful RGB reconstruction from fused features requires learning genuine cross-modal complementarity rather than relying solely on single-modality information.
- **Evidence anchors:**
  - [abstract]: "reconstructs the original image from fused multi-modal features, explicitly enhancing the model's ability to aggregate cross-modal complementary information"
  - [Table 2]: Adding MFRM improves Event-based Top1 from 49.47% to 52.20% (2.73% gain) and RGB-Event from 51.06% to 51.96%
  - [Table 5]: Pre-trained fusion module improves RGB-Event tracking from 62.0% to 62.6% SR, suggesting transferable fusion capability
  - [corpus]: Multiple papers (Mamba-FETrack V2, Human Activity Recognition HAR) validate RGB-Event fusion value for tracking and recognition tasks

### Mechanism 3: Multimodal Contrastive Learning for Global Alignment
- **Claim:** Contrastive learning between modalities captures global semantic dependencies that pixel-level reconstruction cannot.
- **Mechanism:** Treat paired RGB-Event and RGB-Voxel samples as positive pairs, others as negatives. Normalize features and compute similarity with scaling factor, optimizing contrastive loss to align modalities in shared latent space.
- **Core assumption:** Local pixel reconstruction is insufficient for learning global semantic relationships; explicit cross-modal alignment provides complementary learning signal.
- **Evidence anchors:**
  - [abstract]: "align cross-modal feature representations in a shared latent space, which effectively enhances the model's capability for multi-modal understanding and capturing global dependencies"
  - [section 3.2]: "the reconstruction process mainly focuses on modeling local relationships... Therefore, we introduce the concept of multimodal contrastive learning"
  - [Table 2]: Adding MCL to DMA alone improves Event-based Top1 from 49.47% to 51.49%; combined with MFRM achieves 53.18%
  - [corpus]: Weak direct evidence for contrastive learning's specific contribution in RGB-Event settings

## Foundational Learning

- **Concept: Masked Image Modeling (MAE)**
  - Why needed here: CM3AE extends single-modal MAE to multimodal settings; understanding why high masking works is essential for debugging.
  - Quick check question: Why does 75% masking outperform 50% in this framework?

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: The architecture uses 16×16 patches, CLS tokens, and positional encodings across three input types.
  - Quick check question: How are 3D event voxels converted to token embeddings when they cannot use 2D patch division?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: MCL computes normalized similarity between modalities; understanding positive/negative sampling is critical.
  - Quick check question: In Eq. 5-6, what determines which samples are treated as negatives, and how does batch size affect learning?

## Architecture Onboarding

- **Component map:**
  RGB Image (224×224×3) → Patch Embed (196 patches) → Mask 75% → ViT-B/16 Encoder → 512-dim features
  Event Image (224×224×3) → Patch Embed (196 patches) → Mask 75% → ViT-B/16 Encoder → 512-dim features
  Event Voxel (variable) → Group/Tokenize → 12-layer Residual Encoder → 512-dim features
                           ↓
  Fusion Module (Transformer block): [RGB+Event] and [RGB+Event+Voxel]
                           ↓
  Decoders (8-layer): Reconstruct RGB/Event pixels + Fused RGB reconstruction
                           ↓
  MCL: Normalize features → Compute similarity → Contrastive loss

- **Critical path:**
  1. **Data preparation:** Ensure paired RGB-Event-Voxel samples are temporally aligned
  2. **Masking:** Apply 75% random masking with 50% positional overlap between RGB and Event unmasked patches
  3. **Encoding:** Parallel forward pass through three encoders (RGB, Event, Voxel)
  4. **Fusion reconstruction:** Two fusion paths (RGB+Event→RGB, RGB+Event+Voxel→RGB) with MSE loss
  5. **Contrastive learning:** Compute L2-normalized features and similarity for RGB-Event and RGB-Voxel pairs
  6. **Loss aggregation:** `L = Lm (MAE) + Lf (fusion) + Lcl (contrastive)`

- **Design tradeoffs:**
  - **Masking ratio:** 75% optimal (Table 3); lower ratios reduce learning challenge, higher ratios lose too much context
  - **Fusion module simplicity:** Single Transformer block enables direct transfer to downstream tasks but may limit fusion complexity
  - **Voxel encoding:** Fixed token count via sampling/replication normalizes variable-length inputs but may lose temporal density information
  - **Positional alignment:** 50% overlap balances cross-modal anchoring vs. preventing reconstruction shortcut

- **Failure signatures:**
  - **Fusion module shows no improvement:** Check if RGB reconstruction already succeeds from RGB features alone (fusion may be ignored)
  - **Contrastive loss plateaus early:** Verify positive pairs are genuinely aligned; inspect temporal synchronization
  - **Event-only performance poor:** Ensure Event encoder not dominated by RGB-optimized gradients
  - **Training instability at high LR:** Recommended LR=0.0002, weight decay=0.04; scale down if loss spikes

- **First 3 experiments:**
  1. **Baseline validation:** Train MAE on REV2M without MFRM/MCL to reproduce 50.23% Top1 (Table 1, row 9); this confirms data pipeline correctness
  2. **Component ablation:** Train four configurations (DMA only, DMA+MFRM, DMA+MCL, full CM3AE) and compare against Table 2 results; expect ~1-4% incremental gains per component
  3. **Few-shot sanity check:** Train downstream task with 10% data using CM3AE pre-trained weights; should achieve ~43.68% vs. 6.65% with ImageNet MAE (Table 4, RGB-Event action recognition), validating pre-training quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be adapted to process raw, asynchronous event streams directly, rather than relying on synchronous event frames or voxels?
- Basis in paper: [inferred] The methodology section explicitly processes inputs as "event images" and "event voxels" which involves voxelization. The introduction highlights event cameras' "high temporal resolution," which is often diluted during voxelization.
- Why unresolved: The current architecture utilizes standard ViT encoders requiring synchronous tensor inputs, potentially losing the asynchronous precision advantages of the sensor.
- What evidence would resolve it: Extension of the framework to handle point-based or asynchronous event representations without explicit voxelization, maintaining performance.

### Open Question 2
- Question: How sensitive is the model's performance to the specific constraint that 50% of unmasked patches must be aligned across modalities?
- Basis in paper: [inferred] The methodology states: "we ensure that half of the unmasked patches in the RGB and Event modalities are identical." The ablation study varies the total masking ratio but not this specific alignment ratio.
- Why unresolved: It is unclear if this 50% fixed ratio is a heuristic or an optimal balance between sharing information and maintaining modality-specific independence.
- What evidence would resolve it: An ablation study varying the alignment ratio (e.g., 0%, 25%, 75%, 100%) to observe the impact on reconstruction loss and downstream task accuracy.

### Open Question 3
- Question: Does the computational overhead of the dual-branch Transformer architecture negate the low-power and low-latency advantages of event cameras for edge deployment?
- Basis in paper: [inferred] The introduction lists "low power consumption" and "low latency" as key advantages of event cameras. The method employs a heavy dual-branch ViT-B/16 encoder and a 12-block voxel encoder.
- Why unresolved: The paper validates accuracy on GPU servers but does not report inference speed (FPS), parameter efficiency, or energy consumption relative to the sensor's capabilities.
- What evidence would resolve it: Analysis of latency and energy consumption metrics during inference on resource-constrained hardware typical for event camera applications.

## Limitations
- Voxelization algorithm not fully specified - only "14 points per voxel" mentioned without spatial/temporal binning details
- Learning rate schedule beyond base rate (2e-4) not detailed, affecting reproducibility
- Event-to-image conversion method not explicitly defined in methodology section
- Limited ablation evidence for contrastive learning's independent contribution

## Confidence

**High Confidence Claims:**
- Overall framework architecture (dual-branch MAE + fusion module + contrastive learning) is well-defined and implemented consistently
- 75% masking ratio with 50% positional overlap between RGB and Event patches is explicitly specified and validated
- Combined loss formulation (L = Lm + Lf + Lcl) is clearly presented
- Five downstream tasks and their evaluation metrics are well-documented

**Medium Confidence Claims:**
- Specific quantitative improvements attributed to each component (MFRM and MCL) are well-supported by ablation studies, but the ablation design doesn't isolate contrastive learning's contribution as cleanly as the fusion module
- Generalizability across different downstream tasks is demonstrated but relies heavily on RGB-Event fusion benefits

**Low Confidence Claims:**
- Specific contribution of contrastive learning relative to fusion alone is less well-validated, with limited ablation evidence showing incremental gains
- Voxel encoding approach's impact on performance is not thoroughly explored, as most experiments focus on RGB-Event combinations

## Next Checks
1. **Component Isolation Test**: Re-run the ablation study with DMA+MCL configuration (excluding MFRM) to better quantify contrastive learning's independent contribution, particularly on event-only tasks where fusion benefits may be limited.

2. **Temporal Alignment Validation**: Systematically vary the temporal synchronization window between RGB and event data during pre-training to measure sensitivity to temporal misalignment, which could reveal whether the model learns spurious correlations.

3. **Voxelization Algorithm Replication**: Implement multiple voxelization strategies (different spatial/temporal resolutions, different point sampling methods) to determine whether the specific "14 points per voxel" approach is critical to performance or if the framework is robust to voxelization choices.