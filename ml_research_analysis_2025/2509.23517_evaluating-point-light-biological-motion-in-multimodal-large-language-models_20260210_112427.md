---
ver: rpa2
title: Evaluating point-light biological motion in multimodal large language models
arxiv_id: '2509.23517'
source_url: https://arxiv.org/abs/2509.23517
tags:
- person
- action
- motion
- performance
- plds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ActPLD, the first benchmark for evaluating
  action understanding in multimodal large language models (MLLMs) using point-light
  displays (PLDs). The authors assess state-of-the-art models on both single-actor
  and social interaction PLDs, testing action classification and intention inference
  using 3-AFC and chain-of-thought description matching.
---

# Evaluating point-light biological motion in multimodal large language models

## Quick Facts
- arXiv ID: 2509.23517
- Source URL: https://arxiv.org/abs/2509.23517
- Reference count: 40
- Primary result: Current MLLMs achieve near-chance accuracy (28–41% single-actor, 25–49% social) on point-light biological motion understanding, while humans score ~93–96%.

## Executive Summary
This paper introduces ActPLD, the first benchmark for evaluating action understanding in multimodal large language models (MLLMs) using point-light displays (PLDs). The authors assess state-of-the-art models on both single-actor and social interaction PLDs, testing action classification and intention inference using 3-AFC and chain-of-thought description matching. Results show consistently low performance across models, with accuracy near or just above chance levels, indicating fundamental gaps in action processing and spatiotemporal understanding. Humans achieved near-ceiling accuracy (~93–96%), highlighting a significant performance gap. The study reveals that current MLLMs lack the structural and semantic abstraction needed to interpret human motion from minimal visual cues.

## Method Summary
The study benchmarks MLLMs on 228 actions (158 single-actor, 70 social) rendered as point-light displays from CMU MoCap data. Models are evaluated using 3-Alternative Forced Choice (3-AFC) accuracy and Chain-of-Thought (CoT) description matching. For each PLD video, 8 consecutive frames (128×128px) are extracted from the central portion and presented as either a 4×2 grid montage (for Gemini) or individual base64 images. The 3-AFC task presents one ground truth and two semantically distinct distractors. CoT evaluation uses an independent LLM (Gemini 2.5 Pro) to score description similarity. Human performance is established via Mechanical Turk with ~93–96% accuracy.

## Key Results
- MLLMs show near-chance accuracy on PLD interpretation (28–41% for single-actor, 25–49% for social interactions)
- Humans achieve near-ceiling performance (~93–96% accuracy) on the same tasks
- No correlation between 3-AFC and CoT accuracy for single-actor actions (ρ = –0.112, p = 0.730)
- Models fail to distinguish walking from "constellations" and other qualitatively implausible interpretations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PLDs isolate motion as the sole semantic cue by eliminating texture, background, and object affordances, exposing whether systems possess genuine action understanding versus reliance on contextual shortcuts.
- **Mechanism:** By presenting only sparse dots at key body joints moving over time, PLDs remove all visual confounds that MLLMs typically leverage (scene context, object recognition, texture patterns), forcing any inference to derive purely from kinematic trajectory integration.
- **Core assumption:** Action understanding can be meaningfully separated from scene understanding and object affordances.
- **Evidence anchors:**
  - [abstract] "Since PLDs isolate body motion as the sole source of meaning, they represent key stimuli for testing the constraints of action understanding in these systems."
  - [section 1.1] "PLDs provide a deeper lens into action understanding by isolating motion... unconfounded by texture, background, or object affordances."
- **Break condition:** If models have developed motion-centric representations independent of context, PLD performance should approach human levels; near-chance accuracy confirms the absence of such representations.

### Mechanism 2
- **Claim:** Current MLLM architectures fail at biological motion understanding because their frame-level encoding and shallow temporal fusion cannot reconstruct the 4D spatiotemporal structure (3D body configuration + temporal dynamics) from sparse 2D projections.
- **Mechanism:** MLLMs tokenize frames independently via modality encoders, then fuse them through limited mechanisms (cross-attention, adapters). This loses joint trajectory coherence across time—humans perceive PLDs by integrating dot motions into a unified body schema; MLLMs process dots as unconnected pixel patterns without kinematic priors.
- **Core assumption:** Biological motion perception requires maintaining coherent joint relationships across time, not just per-frame feature extraction.
- **Evidence anchors:**
  - [abstract] "Results show consistently low performance... indicating fundamental gaps in action processing and spatiotemporal understanding."
  - [section 1.4] "These failures are attributed to aggressive, potentially lossy visual compression needed for processing and shallow temporal fusion that disrupts the sequence of the videos."
- **Break condition:** If architectures incorporate recurrent temporal memory or trajectory-aware attention, dot sequences may become interpretable as structured motion.

### Mechanism 3
- **Claim:** Human PLD perception relies on embodied priors—internal body schema and perception-action coupling developed through motor experience—which are entirely absent in MLLMs trained only on passive visual-text data.
- **Mechanism:** Humans map observed kinematics onto their own motor representations, enabling inference of intention, emotion, and identity from sparse cues. MLLMs lack sensorimotor experience; their "knowledge" of actions is purely linguistic-statistical, without grounded understanding of how bodies move or why certain trajectories imply specific intentions.
- **Core assumption:** Embodied experience provides irreplaceable inductive biases for interpreting biological motion.
- **Evidence anchors:**
  - [abstract] "This ability emerges early in development and is largely attributed to human embodied experience."
  - [section 1.1] "The underlying process is often attributed to a common mapping between perception and action in the human brain... MLLMs lack the temporal, embodied, and structural priors that humans use to understand actions."
- **Break condition:** If models are augmented with motor-derived priors (e.g., via sensorimotor simulation or kinematic constraints), grounding may improve even without physical embodiment.

## Foundational Learning

- **Concept: Point-Light Displays (PLDs)**
  - **Why needed here:** PLDs are the experimental stimulus that isolates motion understanding; grasping their design (dots at joints, no context) is essential to interpreting why they expose MLLM limitations.
  - **Quick check question:** What three visual elements are intentionally removed in a PLD compared to a standard video?

- **Concept: Spatiotemporal Fusion in MLLMs**
  - **Why needed here:** The paper identifies temporal fusion as a key bottleneck; understanding how current architectures sample and integrate frames clarifies why 4D understanding fails.
  - **Quick check question:** What is the typical frame sampling range for current MLLMs (~4–16 frames), and why might this be insufficient for PLD interpretation?

- **Concept: 3-Alternative Forced Choice (3-AFC)**
  - **Why needed here:** The benchmark quantifies performance via 3-AFC accuracy; knowing what chance performance represents (33.3%) enables meaningful comparison to human results (~93–96%).
  - **Quick check question:** In a 3-AFC paradigm, what accuracy baseline indicates random guessing?

## Architecture Onboarding

- **Component map:** Modality Encoder -> Modality Interface -> LLM Core -> Evaluation Layer
- **Critical path:**
  1. Extract 8 consecutive frames (128×128 px) from PLD video (central portion, trimmed to avoid artifacts)
  2. Encode frames via modality encoder (individually or as montage for Gemini)
  3. Prompt LLM with frames + 3-AFC options or CoT request
  4. Parse model response for selected label or generate description
  5. Score accuracy (3-AFC) or similarity to ground-truth description (CoT matching via independent LLM)

- **Design tradeoffs:**
  - **Frame count (8 frames):** Balances computational cost vs. temporal coverage; may be insufficient for extended or subtle actions
  - **3-AFC vs. CoT evaluation:** 3-AFC enables objective accuracy comparison; CoT captures reasoning quality but requires LLM-based scoring with potential bias
  - **PLD vs. natural video stimuli:** PLDs isolate motion but are ecologically rare; natural videos provide context but conflate multiple cues

- **Failure signatures:**
  - **Near-chance 3-AFC accuracy** (28–41% single-actor, 25–49% social) indicates failure to extract semantic content from kinematics
  - **Null 3-AFC/CoT correlation for single actors** (ρ = –0.112, p = 0.730) suggests reasoning is decoupled from correct perceptual judgment
  - **Qualitative misclassifications** (e.g., walkers mistaken for constellations) reveal absence of body schema priors

- **First 3 experiments:**
  1. **Frame count ablation:** Test performance with 8, 16, and 32 frames to isolate whether temporal undersampling drives failure
  2. **Stimulus degradation gradient:** Compare accuracy across PLD → stick figure → skeleton overlay → natural video to quantify context contribution
  3. **Joint trajectory augmentation:** Provide textual joint position sequences alongside frames to test whether explicit kinematic scaffolding improves inference

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating structured "body schema" priors—such as skeletal models or joint dependency constraints—into MLLMs enable them to better infer invisible joints and limb lengths from sparse point-light displays?
- **Basis in paper:** [explicit] Section 4.1 ("Body schema problem") states that current MLLMs lack the inductive prior for a human body model that humans possess, and asks if architectures can be developed to represent body models from sparse observations.
- **Why unresolved:** Current models treat PLDs as collections of independent pixel trajectories rather than a unified biological structure, lacking the "body schema" humans use to reconstruct 3D forms.
- **What evidence would resolve it:** Demonstrating that models equipped with kinematic constraints or body-related attention modules achieve significantly higher accuracy on ActPLD than standard architectures.

### Open Question 2
- **Question:** Which training paradigm is more effective for biological motion understanding: direct training on sparse motion data (risking overfitting) or compositional training on other low-fidelity modalities like stick figures and sensor data?
- **Basis in paper:** [explicit] Section 4.1 ("Training on...") explicitly contrasts these two potential approaches (a vs. b) as avenues to accommodate generalization.
- **Why unresolved:** It is currently unknown if the failure on PLDs is due to a lack of exposure to sparse data or a fundamental inability to generalize compositionally from standard training sets.
- **What evidence would resolve it:** Comparative benchmarking of models trained via these two distinct strategies on the ActPLD dataset.

### Open Question 3
- **Question:** Does integrating motor-derived priors via reinforcement learning or movement trajectory simulations provide the necessary experiential grounding to improve reasoning about action dynamics?
- **Basis in paper:** [explicit] Section 4.1 ("Modeling internal states") suggests that integrating motor features via simulation could add experiential grounding currently missing in MLLMs.
- **Why unresolved:** MLLMs currently lack internal bodily states and motor experience, which the authors identify as a cause for poor reasoning on intention and causality.
- **What evidence would resolve it:** Improved performance on intention inference tasks by models that have undergone motor-simulation or reinforcement learning training.

### Open Question 4
- **Question:** Can training models on prediction tasks, such as anticipating action outcomes or engaging in counterfactual simulations, successfully internalize the temporal dynamics required for biological motion understanding?
- **Basis in paper:** [explicit] Section 4.1 ("Training for prediction") proposes that training on prediction tasks could help models internalize action dynamics.
- **Why unresolved:** Current technical limitations include "shallow temporal fusion" and downsampling, which disrupt the sequence of videos and prevent the understanding of ongoing dynamics.
- **What evidence would resolve it:** Evaluation of models fine-tuned on next-frame prediction or outcome anticipation tasks showing superior performance on the ActPLD benchmark.

## Limitations

- **Ecological validity concerns:** PLDs are artificial stimuli rarely encountered in natural vision, limiting generalizability to real-world action understanding
- **Frame sampling protocol:** Extracting only 8 consecutive frames may undersample longer or more temporally extended actions
- **LLM-based scoring bias:** Chain-of-Thought evaluation relies on another LLM for description matching, introducing potential scorer bias
- **Reproducibility barriers:** Unknown exact system prompts and distractor sampling methodology for 3-AFC tasks

## Confidence

- **High confidence:** The fundamental claim that current MLLMs show near-chance performance on PLD interpretation is well-supported by the systematic benchmarking approach and clear accuracy metrics
- **Medium confidence:** The attribution of failures to spatiotemporal fusion limitations and lack of embodied priors is mechanistically plausible but not directly proven by the experimental design
- **Low confidence:** The generalizability of PLD-specific findings to broader biological motion understanding without further validation on naturalistic action videos

## Next Checks

1. **Frame count ablation:** Systematically test performance across 8, 16, and 32 frames to determine whether temporal undersampling drives the observed failures
2. **Context gradient evaluation:** Compare accuracy across PLD → stick figure → skeleton overlay → natural video to quantify how much performance improves with added visual context
3. **Joint trajectory augmentation:** Provide explicit joint position sequences alongside frames to test whether explicit kinematic scaffolding enables better action inference