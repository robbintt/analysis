---
ver: rpa2
title: 'BalDRO: A Distributionally Robust Optimization based Framework for Large Language
  Model Unlearning'
arxiv_id: '2601.09172'
source_url: https://arxiv.org/abs/2601.09172
tags:
- unlearning
- forget
- baldro
- baldro-g
- baldro-dv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sample-wise imbalance in
  LLM unlearning, where different samples exhibit widely varying unlearning difficulty,
  leading to asynchronous forgetting. To solve this, the authors propose BalDRO, a
  Distributionally Robust Optimization-based framework that formulates unlearning
  as a min-sup process.
---

# BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning

## Quick Facts
- arXiv ID: 2601.09172
- Source URL: https://arxiv.org/abs/2601.09172
- Authors: Pengyang Shao, Naixin Zhai, Lei Chen, Yonghui Yang, Fengbin Zhu, Xun Yang, Meng Wang
- Reference count: 40
- Primary result: BalDRO improves forget quality by >20% over NPO while maintaining model utility through distributionally robust optimization targeting hard-to-unlearn samples

## Executive Summary
BalDRO addresses sample-wise imbalance in LLM unlearning, where heterogeneous forgetting difficulty causes asynchronous forgetting. The framework formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution emphasizing hard-to-unlearn samples, while an outer step updates model parameters under this distribution. BalDRO is instantiated through two efficient variants - BalDRO-G (discrete GroupDRO-based approximation) and BalDRO-DV (continuous Donsker-Varadhan dual method). Experiments on TOFU and MUSE benchmarks show BalDRO significantly improves both forgetting quality and model utility over existing methods.

## Method Summary
BalDRO wraps base unlearning objectives (NPO, SimNPO, SatImp) with distributionally robust optimization to address sample-wise imbalance. The framework operates through two variants: BalDRO-G selects the top-50% highest-loss samples per batch for backpropagation (O(n log n) partial sort), while BalDRO-DV computes a smooth log-sum-exp reweighting: β·log(1/n Σ exp(ℓ_f(z_i;θ)/β)). Both variants target the inner supremum of a min-sup formulation that emphasizes hard-to-unlearn samples within a KL-divergence uncertainty set. The method is trained with hyperparameter search over learning rates (1e-5 to 1e-4), batch sizes (8-32), temperature β (1.0-10.0), and retain coefficient λ (0.25-2.0), with best results at β=2.0 and λ=1.0.

## Key Results
- BalDRO-DV with β=2.0 improves forget quality (FQ) from 0.77 to 0.99 on TOFU 1% forget ratio
- Both BalDRO-G and BalDRO-DV variants achieve >20% improvement in forget quality over NPO baseline
- Model utility (MU) is maintained or improved while significantly enhancing forget quality across TOFU and MUSE benchmarks
- BalDRO outperforms other unlearning methods including Iterative Unlearning, RRU, and CURE on standard metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The min–sup formulation adaptively emphasizes hard-to-unlearn samples by identifying a worst-case forget distribution within a KL-divergence uncertainty set.
- Mechanism: The inner supremum solves for Q* that maximizes expected loss under distributional perturbation (Eq. 10). This yields exponential reweighting: samples with higher forget loss receive larger probability mass. The outer minimization then updates parameters against this adversarial distribution, preventing premature convergence on easy samples.
- Core assumption: Forgetting difficulty heterogeneity is the primary driver of asynchronous unlearning; correcting this imbalance yields synchronized convergence.
- Evidence anchors:
  - [abstract] "an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while the outer step updates model parameters under this distribution"
  - [Section 4.1, Eq. 6-7] Formal definition of balanced unlearning objective with KL constraint
  - [corpus] "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization" shows DRO stabilizes alignment under noisy preference data, supporting the principle that adversarial distribution optimization handles heterogeneity
- Break condition: If samples have uniform forgetting difficulty, the adversarial distribution collapses to uniform (β → ∞ regime) and BalDRO provides no benefit over base methods.

### Mechanism 2
- Claim: BalDRO-DV converts the intractable distributional optimization into a smooth log-sum-exp objective that can be integrated into standard gradient-based training with O(n) overhead.
- Mechanism: Via Lagrangian relaxation of the KL constraint and the Donsker–Varadhan dual representation, the inner supremum becomes β·log E[exp(ℓ_f/β)] (Eq. 12). The exponential term exp(ℓ_f/β) adaptively scales gradients: harder samples receive higher effective learning rates, already-forgotten samples are downweighted.
- Core assumption: The temperature parameter β correctly calibrates the tradeoff between worst-case focus and gradient stability.
- Evidence anchors:
  - [Section 4.2.2, Eq. 12] "minimizing Eq. (11) leads to the DV dual formulation"
  - [Section 4.3.1] "The only additional cost arises from the log-sum-exp term... three lightweight element-wise operations... linear complexity O(n)"
  - [corpus] Weak direct evidence for DV specifically in unlearning; corpus shows DRO applied to preference optimization but not the dual formulation
- Break condition: If β is too small, the log-sum-exp collapses to max-loss, causing gradient instability from single-sample domination. If β is too large, weighting becomes uniform and balancing effect disappears.

### Mechanism 3
- Claim: BalDRO-G approximates the continuous DRO via discrete group selection, focusing optimization on the highest-loss subset (top-50%) to enforce progress on under-forgotten regions.
- Mechanism: Partition forget set into groups, compute per-group expected loss, select the group with maximum loss for backpropagation. This is a discrete instantiation of the inner supremum where Q assigns all probability mass to the worst-performing group.
- Core assumption: Group-level approximation (rather than sample-level) sufficiently captures distributional imbalance while avoiding single-sample noise.
- Evidence anchors:
  - [Section 4.2.1, Eq. 8] "optimize the maximum expected loss among them"
  - [Section 5.3.2] "β = 2 or 5 produces the most reliable and consistent improvements... excessively large β dilutes the effect"
  - [corpus] No direct corpus evidence for GroupDRO in LLM unlearning; GroupDRO is established in general ML but application here is novel
- Break condition: If group partition is poorly defined (e.g., groups contain mixed difficulty samples), the approximation fails to isolate hard samples. Percentile-based grouping mitigates this but requires sufficient batch size.

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: BalDRO's core theoretical foundation; you must understand min–sup formulations, uncertainty sets, and why worst-case optimization helps under distributional heterogeneity.
  - Quick check question: Given a dataset with varying sample difficulty, why would minimizing expected loss under the empirical distribution fail compared to minimizing worst-case expected loss?

- Concept: KL-divergence constrained optimization and Lagrangian duality
  - Why needed here: Required to derive the closed-form adversarial distribution Q* and understand how the temperature β controls the robustness-accuracy tradeoff.
  - Quick check question: In the Lagrangian relaxation of the KL constraint (Eq. 9), what happens to the optimal Q* as β → 0 and β → ∞?

- Concept: Gradient-based LLM unlearning (NPO, SimNPO)
  - Why needed here: BalDRO is a plugin framework that modifies existing unlearning objectives; you need to understand what NPO optimizes and why it suffers from asynchronous forgetting.
  - Quick check question: Why does NPO introduce a reference model π_ref, and what problem does SimNPO solve by removing it?

## Architecture Onboarding

- Component map:
  Forward pass on forget batch → compute per-sample base unlearning losses → Apply BalDRO-G (select high-loss subset) OR BalDRO-DV (compute log-sum-exp reweighting) → Combine with retain loss: ℓ_all = ℓ_f_balanced + λ·ℓ_r → Backward pass and parameter update

- Critical path:
  1. Forward pass on forget batch → compute per-sample base unlearning losses
  2. Apply BalDRO-G (select high-loss subset) OR BalDRO-DV (compute log-sum-exp reweighting)
  3. Combine with retain loss: ℓ_all = ℓ_f_balanced + λ·ℓ_r
  4. Backward pass and parameter update

- Design tradeoffs:
  - **BalDRO-G vs BalDRO-DV**: G is more interpretable (explicit hard-sample focus) but discontinuous; DV is fully differentiable and simpler to implement (just log-sum-exp) but requires β tuning
  - **β selection**: Lower β = more aggressive hard-sample focus (risk of instability); higher β = closer to uniform weighting (less balancing effect)
  - **Apply DRO to retain set?**: Section 5.3.3 shows this brings negligible benefit; retain samples are naturally balanced

- Failure signatures:
  - Forget quality plateaus early with high variance across samples → β too large, insufficient hard-sample emphasis
  - Training diverges with spiky losses → β too small, single samples dominating gradients
  - Model utility drops significantly → λ too small, retain signal overwhelmed
  - NPO baseline already performs well → limited headroom for BalDRO improvement (Section 5.2.1 notes NPO has "strong baseline, limiting potential gains")

- First 3 experiments:
  1. **Sanity check**: Replicate NPO baseline on TOFU 1% forget ratio, verify FQ ≈ 0.76 and MU ≈ 0.58 (Table 1). Then add BalDRO-DV with β=2.0, λ=1.0 and confirm FQ > 0.90.
  2. **β sensitivity sweep**: Fix λ=1.0, sweep β ∈ {0.5, 1.0, 2.0, 5.0, 10.0} on TOFU 5% forget ratio. Plot FQ vs β to verify peak around β=2–5 (Figure 4 pattern).
  3. **Variant comparison**: Compare BalDRO-G (top-50%) vs BalDRO-DV (β=2.0) on MUSE Books domain, focusing on KM-Df and PL metrics. Expect DV to show slightly better FQ, comparable KM-Dr (Table 2 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the loss function be redesigned to reduce the computational cost of LLM unlearning while maintaining the balance between forget quality and model utility?
- Basis in paper: [explicit] The conclusion states: "In the future, we plan to further investigate the balance between forget quality and general utility by redesigning the loss function to reduce cost of LLM unlearning."
- Why unresolved: The current BalDRO framework introduces computational overhead (log-sum-exp operations, group selection), and the trade-off between unlearning effectiveness and efficiency remains unoptimized.
- What evidence would resolve it: Experiments comparing modified loss formulations (e.g., simplified DRO approximations, hybrid objectives) measuring both performance metrics and training time/FLOPs across benchmarks.

### Open Question 2
- Question: How does BalDRO scale to significantly larger LLMs (e.g., 70B+ parameters) and larger forget sets beyond 10% of training data?
- Basis in paper: [inferred] All experiments use LLaMA-2-7B as the backbone, with forget ratios limited to 1%, 5%, and 10%. The paper does not address whether the sample-wise imbalance dynamics and DRO effectiveness persist at larger scales.
- Why unresolved: Larger models may exhibit different forgetting dynamics; DRO's worst-case optimization could become more computationally expensive or numerically unstable at scale.
- What evidence would resolve it: Experiments on models such as LLaMA-2-70B or larger, with forget sets comprising 20-50% of data, comparing BalDRO variants against baselines.

### Open Question 3
- Question: Would alternative divergence measures (beyond KL divergence) for the DRO uncertainty set yield better unlearning performance or stability?
- Basis in paper: [explicit] The paper states: "Regarding the choice of divergence D, any divergence that enables a valid min–sup DRO formulation could in principle be used; BalDRO is not restricted to a specific distance measure."
- Why unresolved: KL divergence was chosen for its closed-form DV dual, but alternatives like Wasserstein or χ² divergence might handle hard samples differently or provide better robustness guarantees.
- What evidence would resolve it: Comparative experiments using alternative divergences with their respective dual formulations, evaluated on TOFU/MUSE benchmarks.

### Open Question 4
- Question: Can the temperature parameter β be jointly optimized with model parameters rather than fixed, and would this improve performance without sacrificing stability?
- Basis in paper: [inferred] The paper states: "Although β can in principle be optimized jointly, we adopt a fixed-β variant for better stability and computational simplicity," acknowledging this as a design choice rather than a fundamental limitation.
- Why unresolved: Adaptive β could dynamically adjust the robustness radius during training, potentially better matching the evolving distribution of sample difficulties.
- What evidence would resolve it: Experiments with gradient-based β optimization (e.g., bilevel optimization, meta-learning approaches) comparing stability and final performance against fixed-β baselines.

## Limitations

- Critical implementation details missing: Exact training duration, reference model initialization strategy, and specific AdamW hyperparameters are not specified, limiting faithful reproduction
- Strong NPO baseline performance: NPO's strong baseline performance (FQ≈0.77) may limit the generalizability of BalDRO's improvements to other scenarios
- Limited comparative analysis: BalDRO-G lacks direct comparison against other discrete DRO approximations, and the claim about negligible benefit from applying DRO to retain sets is asserted but not empirically validated

## Confidence

- **High confidence**: The core mechanism of using adversarial distribution to target hard-to-unlearn samples is theoretically sound and well-supported by the min-sup formulation (Section 4.1, Eq. 6-7)
- **Medium confidence**: The DV dual approximation (BalDRO-DV) and its computational efficiency claims are reasonable but rely on standard DRO theory not extensively validated in the unlearning context
- **Medium confidence**: Empirical improvements over NPO (20%+ FQ gains) are well-demonstrated on TOFU and MUSE, but the strong baseline performance of NPO may limit generalizability

## Next Checks

1. **Implementation verification**: Reproduce NPO baseline on TOFU 1% forget ratio and confirm FQ ≈ 0.76, then add BalDRO-DV with β=2.0 to verify FQ > 0.90
2. **Generalization test**: Apply BalDRO to a third unlearning scenario (e.g., different model architecture or task) to assess robustness beyond TOFU/MUSE
3. **Edge case analysis**: Test BalDRO when forgetting difficulty is uniform across samples (simulated balanced dataset) to confirm it gracefully defaults to standard NPO behavior