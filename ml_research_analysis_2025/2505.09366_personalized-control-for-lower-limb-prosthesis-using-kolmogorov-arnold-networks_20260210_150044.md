---
ver: rpa2
title: Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks
arxiv_id: '2505.09366'
source_url: https://arxiv.org/abs/2505.09366
tags:
- data
- fkan
- training
- activation
- pooled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Kolmogorov-Arnold Networks (KANs)
  with learnable activation functions for personalized control in lower-limb prostheses,
  comparing user-specific versus pooled training data for turn intent prediction.
  The study evaluated Multilayer Perceptrons (MLP), KANs, convolutional neural networks
  (CNNs), and fractional KANs (FKANs) using IMU data from five individuals with lower-limb
  amputation.
---

# Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2505.09366
- Source URL: https://arxiv.org/abs/2505.09366
- Reference count: 40
- Primary result: Learnable activation functions in KANs did not significantly improve performance over MLPs/CNNs for turn intent prediction in lower-limb prostheses.

## Executive Summary
This study investigates the application of Kolmogorov-Arnold Networks (KANs) with learnable activation functions for personalized control in lower-limb prostheses. The research compares user-specific versus pooled training data for turn intent prediction using IMU data from five individuals with lower-limb amputation. Four model architectures were evaluated: MLPs, KANs, CNNs, and fractional KANs (FKANs). The study reveals that while learnable activation functions did not yield significant improvements, training on user-specific data produced superior results for conventional ML models, whereas deep learning models performed comparably with pooled data.

## Method Summary
The study employed IMU data (6 channels: 3-axis acceleration and 3-axis angular velocity at 120Hz) from five participants with lower-limb amputation. Data preprocessing included 7-point moving average filtering and sliding window segmentation (10-30 samples with 50% overlap). A weighted cross-entropy loss function addressed class imbalance between straight walking and turning activities. Bayesian optimization selected hyperparameters from specified ranges. Models were trained with both user-specific and pooled datasets, and performance was evaluated using macro-average F1-score. The study compared conventional ML approaches (MLP, KAN) with deep learning methods (CNN, FKAN).

## Key Results
- Learnable activation functions in KAN and FKAN did not yield significant improvement compared to MLP and CNN, respectively
- Training on user-specific data yielded superior results compared to pooled data for ML models (p < 0.05)
- No significant difference was observed between user-specific and pooled training for DL models
- FKAN achieved the highest F1-score for one participant in both individual (98%) and pooled datasets (90%)

## Why This Works (Mechanism)

### Mechanism 1
Learnable activation functions (B-splines) in Kolmogorov-Arnold Networks did not yield significant improvements over static activations for this specific turn-intent classification task. The classification of "Straight Walking," "Pre-Turn," and "Turning" does not require the high-order function approximation capabilities of KANs; simpler decision boundaries suffice. The added parameters of B-splines may introduce overfitting risks without providing representational advantages in this low-complexity task with limited data.

### Mechanism 2
Deep Learning models maintain performance when trained on pooled data (multiple participants), whereas conventional Machine Learning models require user-specific data for optimal performance. DL models utilize automatic feature extraction that can identify generalized gait patterns across different subjects, effectively leveraging increased data volume. In contrast, conventional models rely more heavily on specific data distributions, and exposure to diverse gait patterns from multiple participants may introduce noise rather than signal for these shallower architectures.

### Mechanism 3
Class weighting in the loss function mitigates the natural imbalance of straight walking vs. turning data. By applying weights inversely proportional to class frequency, the penalty for misclassifying minority classes (turning) is increased, forcing the model to learn discriminative features for these critical control moments. This ensures the model actually learns the turn intent despite the rarity of turning events.

## Foundational Learning

- **Concept: Kolmogorov-Arnold Representation**
  - Why needed here: This paper tests the core theoretical premise of KANs—that learning activation functions on edges (unlike fixed nodes in MLPs)—improves function approximation.
  - Quick check question: How does the placement of the activation function differ between an MLP and a KAN?

- **Concept: Sliding Window Segmentation**
  - Why needed here: The control system has a strict real-time constraint (300ms). Understanding how time-series IMU data is chunked into windows (10-30 samples) is vital for input preprocessing.
  - Quick check question: Why is the window size limited to 30 samples (250ms) in this specific prosthesis application?

- **Concept: Class Imbalance & Macro F1-Score**
  - Why needed here: "Turning" is a rare event compared to walking. Optimizing for accuracy is misleading here; the macro F1-score is the metric that ensures the model actually learns the turn intent.
  - Quick check question: Why would a model with 95% accuracy be considered a failure for this specific prosthesis controller?

## Architecture Onboarding

- **Component map:** IMU Data -> Sliding Window Segmentation -> Feature Extraction -> Weighted Loss Calculation -> Classification
- **Critical path:** Data ingestion -> Windowing -> Feature Extraction (Critical difference between MLP/KAN vs CNN/FKAN) -> Weighted Loss Calculation
- **Design tradeoffs:**
  - **Accuracy vs. Inference Speed:** KAN inference time is approximately 1000x slower than MLP, potentially violating the 300ms real-time constraint.
  - **Personalization vs. Generalization:** User-specific training yields better results for ML models but requires data collection per patient, while pooled training offers "good enough" performance without individual calibration.
- **Failure signatures:**
  - **KAN Overfitting:** High training performance but lower test performance compared to simpler MLPs on small datasets.
  - **Pooled Data Failure:** Significant drop in performance for MLP/KAN when exposed to inter-subject variability.
  - **Inference Bottleneck:** Exceeding the 300ms window due to computational cost of B-spline evaluation in KANs.
- **First 3 experiments:**
  1. **Baseline Replication:** Implement MLP and KAN baselines to verify KAN does not significantly outperform MLP on a 3-class classification task.
  2. **Data Ablation:** Train a CNN on Subject A01 data only vs. pooled dataset to verify if DL models generalize across subjects.
  3. **Latency Profiling:** Measure actual inference time of FKAN vs. CNN on target hardware to validate if F1-score gains justify potential latency costs.

## Open Questions the Paper Calls Out
- Do learnable activation functions in KANs provide significant performance advantages over static activation functions in more complex locomotion tasks with larger datasets?
- Can KAN-based models be sufficiently optimized to meet the strict real-time constraints required for embedded prosthetic control systems?
- Does the equivalence between pooled training and user-specific training hold for deep learning models when applied to a larger, more diverse cohort of amputees?

## Limitations
- The study relies on data from only 5 participants, restricting generalizability to broader amputee populations.
- Real-time deployment feasibility is not validated; KAN inference is reported as ~1000x slower than MLPs, raising concerns about meeting the 300ms control window.
- The lack of improvement with learnable activation functions may be due to insufficient task complexity rather than a fundamental limitation of the approach.

## Confidence
- **High Confidence:** The finding that ML models (MLP/KAN) benefit from user-specific training while DL models (CNN/FKAN) perform comparably with pooled data, supported by statistical significance testing (p < 0.05).
- **Medium Confidence:** The conclusion that learnable activation functions did not improve performance, limited by small dataset size and may not generalize to more complex control tasks.
- **Medium Confidence:** The class imbalance mitigation strategy effectively improved model performance, though its impact is difficult to isolate from other experimental variables.

## Next Checks
1. **Dataset Size Sensitivity Analysis:** Test whether the observed performance gap between user-specific and pooled training for ML models persists as the number of participants increases from 5 to 10 or 20.
2. **Real-Time Deployment Benchmark:** Measure the actual inference latency of the best-performing CNN and FKAN models on the target prosthetic microcontroller to verify compliance with the 300ms control window.
3. **Task Complexity Scaling:** Apply the same model comparison methodology to a more complex gait classification task to determine if learnable activation functions show advantages in higher-complexity scenarios.