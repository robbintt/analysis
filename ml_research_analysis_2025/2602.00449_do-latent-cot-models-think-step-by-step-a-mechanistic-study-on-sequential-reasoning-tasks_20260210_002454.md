---
ver: rpa2
title: Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential
  Reasoning Tasks
arxiv_id: '2602.00449'
source_url: https://arxiv.org/abs/2602.00449
tags:
- latent
- reasoning
- task
- final
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies CODI, a latent CoT model that compresses explicit\
  \ reasoning traces into short continuous thought vectors, and examines whether it\
  \ performs genuine step-by-step computation or shortcuts. Using logit-lens decoding,\
  \ linear probes, attention analysis, and activation patching on polynomial-iteration\
  \ tasks, the authors find that for 2\u20133 hops CODI does form and maintain intermediate\
  \ bridge states in the latent channel, but for longer hops (n\u22654) it typically\
  \ encodes only the final one or two intermediates and fuses them with the last input\
  \ at the answer readout position."
---

# Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks

## Quick Facts
- **arXiv ID**: 2602.00449
- **Source URL**: https://arxiv.org/abs/2602.00449
- **Reference count**: 40
- **Primary result**: CODI achieves high accuracy on 2–3 hop tasks by forming and maintaining intermediate bridge states in latent thought vectors, but for 4+ hops it concentrates computation on late intermediates and fuses them with the final input, with performance collapsing under prime moduli.

## Executive Summary
This paper investigates whether latent continuous-thought (CoT) models perform genuine step-by-step reasoning or shortcut computation. Using CODI, a latent CoT model that compresses explicit reasoning traces into short continuous thought vectors, the authors examine its behavior on polynomial-iteration sequential reasoning tasks. Through logit-lens decoding, linear probes, attention analysis, and activation patching, they find that while CODI faithfully maintains intermediate states for short sequences (2–3 hops), it defaults to a "partial latent reasoning" strategy for longer sequences (4+ hops), concentrating on late intermediates. This behavior is tied to the algebraic compressibility of the task, with the pattern breaking down under prime moduli.

## Method Summary
The study uses polynomial-iteration tasks where states are updated via s_t = s_{t-1} * x_t + b (mod m). The teacher model generates explicit CoT traces, while the student (CODI) uses continuous latent thought vectors inserted between [BoT] and [EoT] markers. Training employs cross-entropy on final answers plus ℓ1 feature-space distillation aligning teacher/student representations at the [Ans] boundary. The model is a 3-layer, 2-head GPT-2-style transformer with 6 latent tokens, trained with AdamW (lr=3×10⁻⁴) for 1,000 epochs on a curriculum of sequence lengths 1 to n+1.

## Key Results
- On 2–3 hop tasks, CODI forms and maintains the full set of intermediate bridge states in the latent channel.
- For 4+ hop tasks, CODI exhibits partial latent reasoning, concentrating on only the final one or two intermediates.
- The transition from faithful to partial reasoning is governed by task compressibility, breaking down under prime moduli where performance drops sharply.
- The explicit CoT teacher objective, rather than distillation alone, drives the late-bottleneck mechanism.

## Why This Works (Mechanism)

### Mechanism 1: Two-Route Computation with Late Fusion
- **Claim**: On short sequential tasks (2–3 hops), the model achieves high accuracy by maintaining a strict division of labor: the latent channel builds intermediate states while a direct pathway routes the final input, fusing them only at the output boundary.
- **Mechanism**: The latent path forms bridge states (e.g., s_2) maintained across latent thought positions, while attention heads implement a copy-like route transferring the final input token (x_{n+1}) directly to the answer readout position, with predictions arising via late fusion of these two streams.
- **Break condition**: This mechanism degrades as task depth increases (n ≥ 4) or when the task structure prevents compression (prime moduli).

### Mechanism 2: Partial Latent Rollout (Late Bottleneck)
- **Claim**: For longer sequential tasks (n ≥ 4), the model defaults to a "partial" strategy that concentrates compute on late intermediates rather than executing a full step-by-step rollout.
- **Mechanism**: The model stops maintaining the full sequence of intermediate states (s_1 ... s_{n-2}), collapses the latent trajectory to focus on forming and refining only the final one or two intermediates (s_{n-1}, s_n), then fuses this late intermediate with the directly-routed final input at the answer readout.
- **Break condition**: If the task requires faithful dependence on the full history (e.g., prime moduli), this partial strategy fails to generalize, leading to accuracy collapse.

### Mechanism 3: Compressibility-Dependent Generalization
- **Claim**: The emergence of faithful vs. partial reasoning mechanisms is governed by the algebraic "compressibility" of the task, specifically the presence of many-to-one mappings in the state update function.
- **Mechanism**: In composite rings (e.g., mod 50), multiplication by non-units creates d-to-1 contractions that erase information about early inputs, allowing the model to exploit this structure by allocating its limited latent budget to the few updates that matter. Under prime fields (e.g., mod 41), updates are permutations that preserve full history, preventing the late bottleneck strategy from working.
- **Break condition**: Task shifts that increase effective memory length (e.g., switching from composite to prime modulus) without increasing model capacity cause the mechanism to fail.

## Foundational Learning

- **Concept: Logit Lens Decoding**
  - **Why needed here**: This is the primary tool to "see" inside the latent channel. It projects intermediate hidden states back to the vocabulary space, allowing us to verify if a specific bridge state (e.g., s_2) is actually represented at a given latent step.
  - **Quick check question**: If you apply the unembedding matrix to the residual stream at latent step ℓ_3, does the resulting probability distribution peak at the ground-truth intermediate state s_3?

- **Concept: Activation Patching (Causal Tracing)**
  - **Why needed here**: Correlation (probing) is not causation. Patching is required to prove that the representations identified by the logit lens are functionally used to compute the answer, rather than being epiphenomenal.
  - **Quick check question**: If you corrupt an input token (e.g., change x_2) and the model fails, can you restore the correct answer by patching in the clean activation from a specific latent step?

- **Concept: Ring Theory (Units and Zero Divisors)**
  - **Why needed here**: To understand why the mechanism fails on prime moduli, one must grasp the algebraic difference: in Z_p (prime), multiplication is invertible (preserves info); in Z_m (composite), it can be non-invertible (destroys info).
  - **Quick check question**: In a modulus 50 system, does multiplying by 2 preserve or compress the state space? (Answer: It compresses it by a factor of gcd(2, 50) = 2).

## Architecture Onboarding

- **Component map**: Inputs -> [BoT] -> Latent Steps (ℓ_1 ... ℓ_p) -> [EoT] -> [Ans] -> Output
- **Critical path**: Inputs are embedded, initial bridge state is formed at [BoT], latent steps maintain/evolve the bridge state (or collapse to late states), attention heads copy the final input x_{n+1} directly to [Ans], and at [Ans] the latent state and direct input are fused to predict s_{n+1}.
- **Design tradeoffs**: Fixed latent count (p) forces aggressive compression but increasing p does not reliably induce deeper rollouts; distillation objective aligns results but doesn't guarantee replicating the process, hence shortcuts.
- **Failure signatures**: Prime moduli drop (composite 50: ~91% accuracy, prime 41: <10%); vanishing early intermediates (logit lens shows high probability for late states but near-zero for early states in long sequences); direct-path dominance (ablating latent channel has minimal effect if task can be solved via direct input route).
- **First 3 experiments**:
  1. Reproduce the modulus split: Train CODI student on polynomial iteration tasks with modulus 50 (composite) and modulus 41 (prime), plot accuracy to confirm the generalization gap.
  2. Trace the latent rollout: Use logit lens to decode probability of correct intermediate state s_k at each latent step ℓ_i for a 5-hop task, verify only s_4 and s_5 are decodable.
  3. Patch the direct route: Corrupt final input token x_{n+1}, attempt to recover accuracy by patching clean activation at [Ans] position but not latent steps, confirming existence of copy-like direct pathway.

## Open Questions the Paper Calls Out

- **Question**: Does the compressibility dependence observed in CODI (including the prime–composite split) generalize across different latent-CoT objectives and architectures?
  - **Basis in paper**: "Looking ahead, a natural next step is to test whether the compressibility dependence we observe (including the prime–composite split) persists across latent-CoT objectives and architectures."
  - **Why unresolved**: The study only analyzes CODI on polynomial-iteration tasks; other latent-CoT models (e.g., COCONUT, PaCoT) may exhibit different mechanistic behaviors.
  - **What evidence would resolve it**: Replicate the mechanistic analysis (logit-lens, probing, patching) on alternative latent-CoT architectures across compressible and incompressible task variants.

- **Question**: How can latent-CoT objectives be designed to induce faithful full rollouts rather than late-bottleneck shortcuts on longer-horizon tasks?
  - **Basis in paper**: The paper finds that increasing latent-step count does not reliably elicit deeper rollouts; the partial pathway "collapses under harder optimization" and "teacher-guided compression" drives late-bottleneck behavior.
  - **Why unresolved**: The paper identifies the failure mode but does not propose or validate alternative training objectives that enforce step-by-step latent computation.
  - **What evidence would resolve it**: Develop new regularization or supervision schemes and show via mechanistic analysis that intermediate states become sequentially decodable across all hops.

- **Question**: Do the mechanistic signatures observed in synthetic polynomial tasks transfer to naturalistic multi-step reasoning tasks?
  - **Basis in paper**: "Extending our mechanistic toolbox to more sequential and naturalistic datasets may further clarify when latent reasoning implements faithful multi-step computation versus shortcut- or compression-driven strategies."
  - **Why unresolved**: Polynomial-iteration tasks have ground-truth intermediates and known algebraic structure; real-world tasks may have messier dependencies and no clear compressibility signal.
  - **What evidence would resolve it**: Apply logit-lens, probing, and patching to latent-CoT models trained on natural language reasoning benchmarks and test for bridge-state formation and late-fusion patterns.

## Limitations

- The study is limited to synthetic polynomial-iteration tasks with known ground-truth intermediates, which may not capture the complexity of real-world reasoning tasks.
- The mechanistic analysis relies heavily on logit-lens decoding and probing, which establish correlation but require more extensive activation patching to confirm causal necessity.
- The findings are specific to the CODI architecture and may not generalize to other latent-CoT models or objective functions.

## Confidence

- **High confidence**: Basic empirical findings about decodability of intermediate states and the modulus split producing predicted accuracy patterns are consistently reproducible.
- **Medium confidence**: Interpretations of "late fusion" and "partial rollout" mechanisms are supported by correlation patterns but would benefit from more extensive causal analysis through ablation and activation patching.
- **Low confidence**: The universality of compressibility dependence across different task families and architectures remains uncertain without broader testing.

## Next Checks

1. Run activation patching experiments to establish causal necessity of latent intermediate states for final predictions, particularly on 3-hop tasks where partial reasoning first emerges.
2. Test the generalization of compressibility findings to non-modular sequential tasks (e.g., arithmetic chains with non-cyclic operations) to determine if the phenomenon is task-specific.
3. Implement and compare alternative distillation objectives (e.g., matching teacher's attention distributions rather than residual states) to determine if the partial reasoning pattern is driven by the ℓ1 feature alignment objective.