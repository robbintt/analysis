---
ver: rpa2
title: "Interpretable Kernel Representation Learning at Scale: A Unified Framework\
  \ Utilizing Nystr\xF6m Approximation"
arxiv_id: '2509.24467'
source_url: https://arxiv.org/abs/2509.24467
tags:
- learning
- kernel
- neural
- conference
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces KREPES, a scalable kernel-based framework\
  \ for representation learning using Nystr\xF6m approximation. It enables large-scale\
  \ self-supervised and unsupervised learning with diverse loss functions, while providing\
  \ principled interpretability via representer landmarks."
---

# Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nyström Approximation

## Quick Facts
- arXiv ID: 2509.24467
- Source URL: https://arxiv.org/abs/2509.24467
- Reference count: 40
- Achieves up to 90.31% accuracy on CIFAR-10 using far fewer parameters (500k-900k vs 5M-22M) than neural networks

## Executive Summary
This work introduces KREPES, a scalable kernel-based framework for representation learning using Nyström approximation. It enables large-scale self-supervised and unsupervised learning with diverse loss functions, while providing principled interpretability via representer landmarks. KREPES performs competitively with neural networks on large image and tabular datasets, achieving up to 90.31% accuracy on CIFAR-10, while using far fewer parameters (500k–900k vs 5M–22M). The method leverages second-order preconditioning and efficient landmark selection to improve optimization. Notably, interpretability is immediate: influential landmarks and concept activation vectors allow explaining representations without post-hoc analysis. This bridges the gap between kernel methods' theoretical benefits and the data demands of modern foundation models.

## Method Summary
KREPES uses Nyström approximation to scale kernel-based representation learning to large datasets. The framework computes a kernel matrix between data points and a small set of landmarks (typically 2000), then learns a projection matrix and bias on top of this approximated kernel space. The approach supports multiple self-supervised and unsupervised losses (Barlow Twins, SimCLR, VICReg, BYOL, KPCA, KAE) and uses Principal Component Initialization based on the landmark kernel's eigendecomposition. Optimization employs Adam with optional loss-specific Generalized Gauss-Newton (GGN) preconditioning via conjugate gradient. The method achieves competitive accuracy while using orders of magnitude fewer parameters than neural networks.

## Key Results
- Achieves 90.31% accuracy on CIFAR-10 with 2000 landmarks and Barlow Twins loss
- Uses 500k–900k parameters versus 5M–22M for comparable neural networks
- Maintains interpretability through landmark analysis without post-hoc methods
- Scales to 1M samples while preserving kernel methods' theoretical benefits

## Why This Works (Mechanism)

### Mechanism 1: Nyström Landmark Subsampling Reduces Kernel Complexity
- Replaces full n×n kernel matrix with m≪n landmark-based approximation
- Projects full RKHS onto span of landmark feature maps, reducing parameters from n×p×h to m×p×h
- Requires m = O(log n) landmarks sampled via leverage scores or kmeans++ for statistical accuracy

### Mechanism 2: Principal Component Initialization Aligns Parameters with Kernel Spectrum
- Initializes projection matrix using eigendecomposition of landmark kernel matrix
- Positions parameters in high-variance directions of kernel space before gradient descent
- Improves convergence and final accuracy compared to random initialization

### Mechanism 3: Loss-Specific GGN Preconditioning Exploits Curvature
- Uses Generalized Gauss-Newton Hessian approximation as preconditioner
- For BT loss, treats as nonlinear least squares; for SimCLR, uses block-diagonal Q
- Rescales gradients along high-curvature directions, flattening eigenvalue spectrum

## Foundational Learning

- **Representer Theorem**: Guarantees optimal f lies in span of {φ(x̃ʲᵢ)}, enabling finite parameterization and interpretability via landmark coefficients
  - Quick check: Can you explain why adding Tikhonov regularization ||W||²_H forces W^⊥ = 0?

- **Nyström Approximation**: Core mechanism for scaling; approximates K_nn ≈ K_nm K_mm^† K_mn, enabling linear-time operations
  - Quick check: What is the rank of the Nyström approximation if K_mm has h non-zero eigenvalues?

- **Conjugate Gradient for Implicit Hessian-Vector Products**: Required for preconditioning without forming full Hessian
  - Quick check: How many JVP and VJP operations are needed per CG iteration?

## Architecture Onboarding

- **Component map**: Kernel computation -> Landmark selection -> Parameter initialization -> Training loop -> Interpretability extraction
- **Critical path**: 1) Preprocess data → compute K_nm and K_mm (parallelizable) 2) Select m landmarks (kmeans++ default) 3) Initialize Ã via PCI 4) Iterate: forward pass → loss → gradient → precondition (optional) → update 5) Extract representations Z = K_nm Ã + γ
- **Design tradeoffs**: m (landmarks) vs memory/time, h (representation dim) vs capacity, preconditioner type (GGN vs general), leverage score vs kmeans++ landmark selection
- **Failure signatures**: Degraded accuracy with uniform landmark sampling → switch to leverage scores or kmeans++; slow convergence with random initialization → verify PCI; CG not converging → check regularization λ
- **First 3 experiments**: 1) Baseline validation: KREPES with BT loss on CIFAR-10, m=2000, no preconditioner (verify ~89%) 2) Ablation on landmark selection: Compare uniform, kmeans++, leverage scores on MNIST/CoverType (report downstream accuracy and κ) 3) Interpretability check: For test sample, compute top-5 influential landmarks and verify semantic similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hyper-parameters for diverse self-supervised kernel losses be selected without compromising unsupervised nature by relying on labeled validation data?
- Basis: Authors acknowledge using labeled subsets for hyper-parameter sweeps compromises self-supervised premise, but no superior method exists
- What evidence would resolve: Theoretical/heuristic framework for setting regularization coefficients without ground-truth labels

### Open Question 2
- Question: Can KREPES achieve competitive performance using standard kernels (RBF, Laplacian) without relying on eNTKs from pre-trained neural architectures?
- Basis: Appendix A notes eNTKs by far have better performance than standard kernels in experiments
- Why unresolved: Heavy reliance on eNTKs suggests representation power may come from neural feature extraction rather than kernel learning dynamics
- What evidence would resolve: Comparative experiments showing standard kernels matching eNTK performance on CIFAR-10 without transfer learning

### Open Question 3
- Question: Does storage and processing of K_nm remain tractable when scaling to billion-sample foundation models?
- Basis: Paper scales to 1M samples but "foundation models" require orders of magnitude more data
- Why unresolved: Linearity in n may still impose prohibitive memory constraints at billion-scale compared to mini-batch neural network training
- What evidence would resolve: Performance metrics on datasets exceeding 10M-100M samples or streaming/block-wise update mechanism

### Open Question 4
- Question: Do theoretical Nyström guarantees extend to non-convex, spectral, or contrastive losses implemented in KREPES?
- Basis: Section 3 highlights shift from closed-form solutions to gradient-based optimization for SSL losses
- Why unresolved: Standard Nyström guarantees assume specific loss structures; non-convex SSL objectives may void these guarantees
- What evidence would resolve: Convergence analysis or generalization bounds for gradient-based optimization of Nyström-approximated self-supervised losses

## Limitations

- Scalability depends on efficient landmark selection and kernel computation, but implementation details for parallelized eNTK computation are not provided
- GGN preconditioning computational cost may become prohibitive for very high-dimensional representations (h > 500)
- Interpretability claims assume influential landmarks directly correspond to human-interpretable concepts, which may not hold for abstract representations

## Confidence

- **High confidence**: Kernel complexity reduction via Nyström approximation, Principal Component Initialization effectiveness, parameter efficiency claims
- **Medium confidence**: Loss-specific GGN preconditioning benefits, scalability to billion-scale datasets, interpretability through landmark analysis
- **Low confidence**: Claims about immediate interpretability without post-hoc analysis, assertion that kernel methods can match neural network performance across all data regimes

## Next Checks

1. **Landmark quality validation**: Measure class coverage κ for different landmark selection methods (uniform, kmeans++, leverage scores) on CIFAR-10 and verify kmeans++ achieves κ ≈ 0.9 while uniform sampling drops to κ ≈ 0.7

2. **Preconditioner computational cost**: Benchmark wall-clock time per epoch for GGN preconditioning versus general preconditioner on CIFAR-10 with h=250, and verify GGN provides superior accuracy despite higher computational cost

3. **Interpretability verification**: For a held-out test sample from CIFAR-10, compute the top-5 influential landmarks and verify they belong to the same semantic class or share visually similar concepts, confirming the interpretability mechanism works as described