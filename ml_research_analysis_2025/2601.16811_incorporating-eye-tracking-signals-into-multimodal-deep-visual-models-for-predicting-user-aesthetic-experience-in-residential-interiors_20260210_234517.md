---
ver: rpa2
title: Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting
  User Aesthetic Experience In Residential Interiors
arxiv_id: '2601.16811'
source_url: https://arxiv.org/abs/2601.16811
tags:
- video
- eye-tracking
- visual
- aesthetic
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-branch CNN-LSTM framework that combines
  video and eye-tracking data to predict users' aesthetic evaluations of residential
  interior spaces. The method treats eye-tracking as privileged information during
  training, enabling deployment using video input alone.
---

# Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors

## Quick Facts
- arXiv ID: 2601.16811
- Source URL: https://arxiv.org/abs/2601.16811
- Reference count: 0
- This paper presents a dual-branch CNN-LSTM framework that combines video and eye-tracking data to predict users' aesthetic evaluations of residential interior spaces, achieving 72.2% accuracy on objective dimensions and 66.8% on subjective dimensions.

## Executive Summary
This paper introduces a dual-branch CNN-LSTM framework that leverages eye-tracking data as privileged information during training to predict users' aesthetic evaluations of residential interior spaces. The method employs a Temporal Frame Attention Branch for video and pupil-response processing, and a Spatial Gaze-Guided Attention Branch for video and gaze-heatmap processing, with cross-modal feature recalibration via Multimodal Transfer Modules. The approach demonstrates that eye-tracking signals can significantly improve prediction accuracy for both objective (e.g., light, organization) and subjective (e.g., relaxation, comfort) aesthetic dimensions, while enabling deployment using video input alone without requiring eye-tracking at inference time.

## Method Summary
The method combines video sequences with synchronized eye-tracking data (pupil responses and gaze heatmaps) to predict 15 binary aesthetic dimensions for residential interiors. A three-stage training procedure first pretrains the Temporal Frame Attention Branch using video and pupil-response images (encoded via Gramian Angular Field and Markov Transition Field transforms), then transfers LSTM hidden-to-hidden weights to the Spatial Gaze-Guided Attention Branch, and finally jointly fine-tunes both branches with cross-modal MMTM fusion. The framework achieves privileged information learning, where models trained with eye-tracking retain comparable performance when deployed with visual input alone.

## Key Results
- The multimodal model achieves 72.2% accuracy on objective aesthetic dimensions and 66.8% on subjective dimensions.
- Video-only baselines are outperformed by models trained with eye-tracking but deployed without it (72.8% vs 72.2% objective accuracy).
- Ablation experiments show pupil responses contribute most to objective assessments, while combining gaze and visual cues enhances subjective evaluations.
- Temporal attention patterns vary by dimension, with behavioral decisions showing early peaks while organizational assessments show progressive accumulation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Eye-tracking signals function as privileged information that improves representation learning even when absent at inference time.
- **Mechanism:** During training, synchronized gaze data provides ground-truth indicators of where humans attend and how they cognitively respond. The network learns to approximate these attention patterns implicitly through visual feature extraction, effectively distilling the "teacher signal" from eye-tracking into video-only representations.
- **Core assumption:** Video features contain sufficient information to approximate attention patterns learned from gaze, given proper supervision during training.
- **Evidence anchors:**
  - [abstract] "models trained with eye-tracking retain comparable performance when deployed with visual input alone"
  - [section 5.3] Model trained with full multimodal input but tested on video-only reached 72.8% (objective) and 67.0% (subjective), exceeding video-only baseline
- **Break condition:** If video features lack spatial resolution or temporal granularity to capture what gaze reveals, privileged information transfer will degrade significantly at inference.

### Mechanism 2
- **Claim:** Pupil responses encode cognitive load and affective response signals particularly informative for objective aesthetic assessments.
- **Mechanism:** Task-evoked pupillary responses are transformed into image representations via GAF and MTF transforms, preserving temporal dynamics in CNN-compatible format. These pupil-derived features correlate with physiological arousal and cognitive processing during aesthetic evaluation.
- **Core assumption:** Pupil dilation patterns reflect systematic cognitive responses to aesthetic stimuli that generalize across participants and can be learned by convolutional encoders.
- **Evidence anchors:**
  - [abstract] "pupil responses contribute most to objective assessments"
  - [section 5.3] Excluding pupil data reduced objective accuracy from 72.2% to 67.9%, a larger drop than removing gaze heatmaps
- **Break condition:** If pupil responses are dominated by individual physiological variation rather than stimulus-evoked responses, cross-subject generalization will fail.

### Mechanism 3
- **Claim:** Dual-branch spatial-temporal specialization enables task-adaptive attention allocation matching human perception patterns.
- **Mechanism:** The Temporal Frame Attention Branch captures time-sensitive dynamics (e.g., first-impression effects for behavioral decisions), while the Spatial Gaze-Guided Attention Branch models stable spatial cues (e.g., material recognition for naturalness). MMTM modules perform channel-wise recalibration across modalities, allowing each branch to specialize while sharing cross-modal context.
- **Core assumption:** Different aesthetic dimensions require fundamentally different attention strategies—some requiring early temporal integration, others requiring progressive spatial accumulation.
- **Evidence anchors:**
  - [section 5.4] Temporal attention for "Leave-Enter" shows early peak; organizational assessment shows progressive accumulation across 80 seconds
  - [section 5.4] Grad-CAM shows task-specific spatial attention: complexity judgments spread across layout; naturalness attends to wooden furniture and plants
- **Break condition:** If aesthetic dimensions share similar temporal-spatial attention patterns, branch specialization provides no benefit over unified architecture.

## Foundational Learning

- **Concept: Privileged Information (Learning Using Privileged Information - LUPI)**
  - Why needed here: Understanding why gaze data can be withheld at deployment requires grasping the LUPI paradigm—additional information available during training that shapes the hypothesis space but is unavailable at inference.
  - Quick check question: Can you explain why a model trained with gaze would outperform a video-only model trained from scratch, even when both receive identical inputs at test time?

- **Concept: Gramian Angular Field (GAF) and Markov Transition Field (MTF) transforms**
  - Why needed here: The paper encodes 1D pupil signals as 2D images for CNN processing. Understanding these transforms is necessary to implement or modify the pupil pathway.
  - Quick check question: Why would GAF/MTF be preferred over simply plotting pupil diameter over time as a spectrogram or raw sequence?

- **Concept: Multimodal Transfer Module (MMTM)**
  - Why needed here: Cross-modal feature recalibration is central to the Spatial branch. MMTM performs global average pooling, concatenation, and channel-wise attention—understanding this enables debugging fusion failures.
  - Quick check question: How does MMTM differ from simple feature concatenation, and what inductive bias does it introduce?

## Architecture Onboarding

- **Component map:**
  - *Temporal Frame Attention Branch:* Video → 3-layer shared CNN → task-specific Conv → [concat with pupil pathway] → Shared LSTM → task-specific LSTM
  - *Pupil Pathway:* Pupil-response images (GAF+MTF) → 2-layer shared Conv → task-specific Conv → [concat with video] → shared LSTM
  - *Spatial Gaze-Guided Attention Branch:* Video → 3-layer shared CNN → task-specific Conv → MMTM fusion with attention maps → Residual connection → shared LSTM → task-specific LSTM
  - *Attention Map Stream:* Gaze heatmaps (1-second windows, Gaussian blur σ≈1°) → 3-layer shared CNN → MMTM
  - *Output:* Concatenate both branches → 2 FC layers with sigmoid → 15 binary predictions

- **Critical path:** Temporal branch pretraining (Stage 1) → Weight transfer to Spatial branch LSTM hidden-to-hidden weights (Stage 2) → Joint fine-tuning with 30% frozen weights (Stage 3). If Stage 1 fails to converge, the entire transfer pipeline collapses.

- **Design tradeoffs:**
  - *Three-stage training vs. end-to-end:* Staged training stabilizes temporal priors but increases engineering complexity and training time.
  - *Binary classification per dimension vs. regression:* Treating aesthetic ratings as binary simplifies learning but discards ordinal information from 7-point Likert scales.
  - *Subject-level normalization vs. raw scores:* Normalization reduces individual bias but may remove meaningful individual variation in aesthetic sensitivity.

- **Failure signatures:**
  - *Large gap between multimodal train and video-only test performance:* Indicates privileged information not successfully distilled into video features.
  - *Temporal branch fails to show task-specific attention patterns:* Suggests LSTM not learning meaningful dynamics; check sequence length vs. LSTM capacity.
  - *Subjective dimensions significantly underperform objective:* May indicate gaze-pupil contribution is insufficient; consider additional physiological signals.

- **First 3 experiments:**
  1. **Baseline replication without staged training:** Train both branches end-to-end from random initialization to isolate the contribution of weight transfer.
  2. **Pupil-only vs. gaze-only ablation per dimension:** Run fine-grained ablation on each of the 15 aesthetic dimensions to identify which signals matter for which dimensions—beyond the aggregate objective/subjective split reported.
  3. **Cross-subject generalization test:** Train on N-1 participants, test on held-out participant, to quantify individual variation in eye-tracking patterns and assess whether normalization adequately controls for subject-specific bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the privileged information approach generalize across cultural contexts and diverse architectural styles beyond Taiwanese townhouses?
- Basis in paper: [inferred] The dataset uses only 28 participants viewing 16 videos of Taiwanese townhouses in four specific styles (Modern, Nordic, Wabi-Sabi, MUJI), with no evaluation of cross-cultural applicability.
- Why unresolved: Cultural background strongly influences aesthetic preferences, and the model's learned representations may be specific to East Asian residential design norms.
- What evidence would resolve it: Testing the trained model on interior design videos from different cultural contexts (e.g., European, Middle Eastern) and architectural typologies (e.g., apartments, commercial spaces) with new participant cohorts.

### Open Question 2
- Question: What latent representations are learned through eye-tracking supervision that enable improved video-only inference?
- Basis in paper: [inferred] The paper demonstrates that models trained with eye-tracking retain performance when deployed with video alone (72.8% vs 72.2% objective accuracy), but does not analyze what features or attention patterns the privileged information imparts.
- Why unresolved: The mechanism by which gaze signals improve visual feature learning during training remains unexplained.
- What evidence would resolve it: Probing experiments comparing intermediate representations between multimodal-trained and video-only models, or transfer learning studies examining which layers benefit most from eye-tracking supervision.

### Open Question 3
- Question: How does the temporal accumulation of aesthetic perception vary across different evaluation dimensions and individual differences?
- Basis in paper: [explicit] The paper notes "Temporal attention shows a complementary pattern" with early peaks for behavioral decisions versus progressive accumulation for organizational tasks, but does not fully characterize individual variation.
- Why unresolved: The model uses aggregate eye-tracking data, potentially obscuring individual differences in how aesthetic judgments form over the 80-second viewing period.
- What evidence would resolve it: Participant-level analysis of temporal attention patterns correlated with individual aesthetic ratings and demographic factors.

## Limitations
- The dataset size (224 video sequences) is relatively small, potentially limiting generalization across diverse interior spaces and participant populations.
- Binary classification approach discards ordinal information from the original 7-point Likert scales, potentially losing nuanced aesthetic judgments.
- Subject-level normalization may not fully eliminate individual bias in eye-tracking patterns and aesthetic preferences.

## Confidence
- **High Confidence:** The privileged information framework and its ability to enable video-only inference - directly demonstrated through ablation study.
- **Medium Confidence:** The superiority of pupil responses for objective assessments and the role of gaze-heatmap combinations for subjective evaluations - supported by ablation results but interpretation could benefit from additional experiments.
- **Low Confidence:** The claim that branch specialization (temporal vs. spatial) provides optimal attention allocation - shows reasonable results but lacks comparison against unified architectures.

## Next Checks
1. **Cross-subject generalization test:** Train on N-1 participants and test on held-out participants to quantify individual variation in eye-tracking patterns and validate whether subject-level normalization adequately controls for person-specific bias.
2. **Fine-grained ablation per dimension:** Run detailed ablation studies on each of the 15 aesthetic dimensions (rather than aggregate objective/subjective splits) to identify which signals (pupil, gaze, or both) matter most for each specific dimension.
3. **Alternative attention mechanisms comparison:** Replace the dual-branch architecture with a unified attention mechanism (e.g., transformer-based or single-branch CNN-LSTM) to test whether branch specialization provides statistically significant benefits over simpler architectures.