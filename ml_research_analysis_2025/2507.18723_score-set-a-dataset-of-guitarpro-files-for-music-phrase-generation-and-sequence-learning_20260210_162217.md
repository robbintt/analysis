---
ver: rpa2
title: 'SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence
  Learning'
arxiv_id: '2507.18723'
source_url: https://arxiv.org/abs/2507.18723
tags:
- guitar
- note
- standard
- dataset
- guit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SCORE-SET, a new dataset of guitar tablature
  files (.gp5) designed to enable music generation and sequence modeling for guitar-specific
  expressive techniques. Existing symbolic datasets lack realistic guitar performance
  nuances like bends, slides, palm mutes, and harmonics, limiting model expressiveness.
---

# SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence Learning

## Quick Facts
- arXiv ID: 2507.18723
- Source URL: https://arxiv.org/abs/2507.18723
- Reference count: 2
- One-line primary result: A new guitar tablature dataset (.gp5) with 198,656 notes enriched with guitar-specific expressive techniques for sequence modeling and music generation

## Executive Summary
The SCORE-SET dataset addresses a critical gap in symbolic music datasets by providing guitar tablatures with realistic expressive techniques like bends, slides, palm mutes, and harmonics. Existing datasets lack the nuanced performance characteristics essential for guitar-specific modeling. The author created this dataset by transcribing piano MIDI files from MAESTRO and GiantMIDI into guitar tablatures and enriching them with guitar articulations based on observed frequency ratios from modern metal transcriptions. This approach enables more authentic guitar performance representation for downstream generative modeling tasks.

## Method Summary
The dataset was created through a pipeline that converts piano MIDI files into guitar tablatures with realistic fingerings and expressive markings. MIDI files were first quantized to a 4/4 time signature with sixteenth-note minimum duration. Each MIDI note was mapped to a string-fret combination using heuristics that prioritize lower fret positions and minimize string skipping. Expression statistics were calculated from manually transcribed modern metal songs and used to stochastically augment the dataset with realistic articulation frequencies. The final output was exported to .gp5 format using PyGuitarPro, resulting in 198,656 notes with guitar-specific performance characteristics.

## Key Results
- Dataset contains 198,656 notes with realistic guitar expression statistics
- Approximately 25.39% of notes feature accents based on metal-derived frequency ratios
- Successfully captures guitar-specific techniques including bends, slides, palm mutes, harmonics, vibrato, hammer-ons, dead notes, and tremolo bar

## Why This Works (Mechanism)

### Mechanism 1
Mapping piano MIDI to string-fret positions enables guitar-specific representation learning through a custom script that converts MIDI pitch to pitch class, then assigns string-fret combinations using heuristics prioritizing lower fret positions and minimizing string skipping. The core assumption is that these heuristics produce realistically playable fingerings reflecting actual guitar performance. Evidence comes from the algorithm description in Section 2.3 and the abstract's mention of adapting MIDI to rhythm guitar tracks. This could break if heuristics produce unplayable or musically unnatural fingerings at scale.

### Mechanism 2
Expression annotation via observed frequency ratios produces performance-aware training data by using manual transcription of metal songs to derive expression frequency statistics, which guide stochastic augmentation of piano-derived sequences. The core assumption is that expression frequencies from one genre transfer meaningfully to classical/piano repertoire. Evidence includes the expression statistics calculation in Section 3 and the random accent addition following derived ratios. This could fail if expression patterns are genre-specific, producing stylistically incoherent training data.

### Mechanism 3
Quantization to 16th-note grid preserves rhythmic integrity while standardizing temporal representation by converting MIDI ticks to musical beats with a 16th-note floor, snapping off-beat notes to nearest beat, and aligning all to 4/4 time. The core assumption is that sixteenth-note granularity captures essential rhythmic variation without losing critical timing information. Evidence comes from Section 2.2's description of quantization parameters and the abstract's emphasis on sequence modeling suitability. This could break if faster idiomatic guitar passages are quantized away, limiting model expressiveness on virtuosic material.

## Foundational Learning

- **String-fret ambiguity**: A single MIDI pitch maps to multiple valid positions on a guitar neck; choosing the "right" one requires understanding playability constraints. Quick check: Can you explain why the same note (e.g., E4) can be played on multiple string-fret combinations, and what factors determine which a guitarist would choose?

- **Expressive techniques in symbolic representation**: Bends, slides, and palm mutes aren't standard MIDI events; understanding how they're encoded in .gp5 format is essential for parsing and modeling this dataset. Quick check: How would you represent a quarter-tone bend in a MIDI-like token sequence versus a tablature format?

- **Quantization trade-offs**: Converting continuous timing to discrete grids loses information; understanding this helps evaluate whether the 16th-note floor is appropriate for your use case. Quick check: What rhythmic patterns would be lost or distorted if you quantized all notes to an eighth-note grid?

## Architecture Onboarding

- **Component map**: MAESTRO/GiantMIDI MIDI files → pitch/timing extraction → fretboard mapper (custom script) → expression injector (stochastic augmentation) → quantization layer (tick-to-beat alignment) → PyGuitarPro serialization → .gp5 export

- **Critical path**: MIDI parsing → string-fret assignment → expression tagging → quantization → .gp5 export. Errors in fretboard mapping cascade into unplayable sequences; expression injection depends on clean note boundaries.

- **Design tradeoffs**: Expression realism vs. genre fidelity (metal-derived statistics may not suit classical repertoire), quantization granularity vs. temporal precision (16th-note floor simplifies modeling but loses detail), automation vs. accuracy (fully automated pipeline trades manual verification for scale).

- **Failure signatures**: Physically impossible fret stretches in output sequences, expression techniques applied in musically inappropriate contexts (e.g., palm mutes on sustained melodic lines), time signature corruption from dynamic measure extension.

- **First 3 experiments**:
  1. Playability validation: Sample 100 random sequences; manually verify fret positions are physically playable without excessive hand movement.
  2. Expression coherence check: Train a simple language model on the dataset; generate samples and evaluate whether expression placements align with musical phrasing boundaries.
  3. Genre sensitivity test: Compare expression distributions between metal-derived source statistics and a held-out classical guitar corpus; quantify distribution shift.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided. The analysis identifies several implicit open questions regarding genre transfer validity, the effectiveness of random expression placement, and the impact of octave adjustments on melodic fidelity.

## Limitations
- Exact heuristics for string-fret assignment are not fully specified, making reproduction difficult
- Expression frequency ratios are only visualized as a bar chart, not provided as numeric values
- Genre transfer assumption (metal expression patterns applied to classical piano-derived sequences) lacks empirical validation

## Confidence
- **High confidence**: The dataset construction pipeline (MIDI → quantization → string-fret mapping → expression augmentation → .gp5 export) is clearly described and methodologically sound
- **Medium confidence**: The string-fret assignment heuristics will produce playable fingerings, but without exact algorithmic details, some sequences may be suboptimal or unplayable
- **Low confidence**: The expression augmentation will produce musically appropriate results, as the genre transfer assumption (metal → classical) is not empirically validated

## Next Checks
1. Playability validation: Sample 100 random sequences from the dataset; manually verify fret positions are physically playable without excessive hand movement or impossible stretches
2. Expression coherence check: Train a simple language model on the dataset; generate samples and evaluate whether expression placements align with musical phrasing boundaries and sound stylistically appropriate
3. Genre sensitivity test: Compare expression distributions between the metal-derived source statistics and a held-out classical guitar corpus; quantify the distribution shift to assess genre transfer validity