---
ver: rpa2
title: 'Strategic Deflection: Defending LLMs from Logit Manipulation'
arxiv_id: '2507.22160'
source_url: https://arxiv.org/abs/2507.22160
tags:
- harmful
- sdeflection
- safety
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Strategic Deflection (SDeflection), a defense
  mechanism designed to protect large language models (LLMs) from logit manipulation
  attacks, which are a sophisticated class of jailbreaking techniques. Traditional
  defenses relying on refusal patterns are vulnerable because logit manipulation can
  suppress these refusals during token generation.
---

# Strategic Deflection: Defending LLMs from Logit Manipulation

## Quick Facts
- arXiv ID: 2507.22160
- Source URL: https://arxiv.org/abs/2507.22160
- Authors: Yassine Rachidy; Jihad Rbaiti; Youssef Hmamouche; Faissal Sehbaoui; Amal El Fallah Seghrouchni
- Reference count: 40
- Primary result: SDeflection reduces LogitsTrap attack success rate from ~90% to ~10-35% across three models while preserving refusal capability and general performance

## Executive Summary
This paper introduces Strategic Deflection (SDeflection), a defense mechanism that protects large language models from logit manipulation attacks by training models to produce safe, semantically adjacent responses rather than explicit refusals. Traditional defenses relying on refusal patterns are vulnerable because logit manipulation can suppress these refusals during token generation. SDeflection addresses this by using Contrastive Preference Optimization (CPO) to train models to prefer safe deflected responses over harmful completions, effectively neutralizing harmful intent while maintaining general capabilities.

The method was evaluated on three instruction-tuned LLMs (Llama-2-7B-chat-hf, Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.2) using the AdvBench dataset under two attack scenarios. Results show SDeflection significantly reduces Attack Success Rate (ASR) under LogitsTrap attacks while preserving the models' ability to refuse direct harmful prompts and maintain performance on general capability benchmarks.

## Method Summary
SDeflection trains models using Contrastive Preference Optimization (CPO) to generate responses that appear compliant with malicious requests but strategically redirect toward safe, benign content. The method constructs preference triplets from harmful prompts: a prefix with an affirmation template, a safe response to a rewritten version of the prompt (y+), and a harmful response to the original prompt (y−). CPO trains the model to prefer y+ over y− while maintaining generation quality through NLL regularization. The approach uses LoRA adapters for efficient fine-tuning and was evaluated against LogitsTrap attacks that enforce compliant prefixes while suppressing refusal tokens.

## Key Results
- SDeflection reduced LogitsTrap ASR from 92.63% to 34.94% on Llama-2-7B-chat-hf, from 89.29% to 8.53% on Llama-3.2-3B-Instruct, and from 94.74% to 13.14% on Mistral-7B-Instruct-v0.2
- Models maintained high refusal rates on direct harmful prompts (remaining above 80-90%)
- General capability preserved: tinyBenchmarks showed minimal performance degradation (Δ accuracy < 0.05)
- CPO outperformed DPO in both defense effectiveness and training efficiency (CPO ASR ~8% vs DPO ~72% on Llama-3.2)

## Why This Works (Mechanism)

### Mechanism 1: Preference-Based Deflection Training
Training models to prefer safe deflected responses over harmful ones creates robustness against logit manipulation. CPO trains policy πθ using triplets (p, y+, y−) where the model learns to prefer deflected safe responses over harmful completions. The loss combines preference learning with NLL regularization. Models can learn a persistent preference for deflection that survives token-level manipulation better than refusal patterns. However, if attackers can suppress deflected content tokens themselves, the defense may fail (Llama-2-7B still shows 34.94% ASR under LogitsTrap).

### Mechanism 2: Affirmative Prefix Conditioning
Training with compliant-looking prefixes builds resilience against prefix-enforcement attacks. The preference dataset explicitly constructs responses starting with 34 affirmation templates. By learning safe continuations after forced compliant prefixes, models resist attacks where logit manipulation enforces affirmative starts. Safety can be embedded in token continuations beyond the first few tokens, countering "shallow safety alignment." However, if attackers use novel prefixes not in training data, the model may not generalize.

### Mechanism 3: Semantic Adjacency for Evasion
Deflected responses that stay semantically related to the query are harder to detect and suppress than explicit refusals. Rather than outputting detectable refusal tokens, deflected responses provide benign information on related topics. This denies attackers the refusal-token targets they suppress via logit manipulation. Attackers design logit suppression around refusal vocabularies, not benign content. However, if attackers expand suppression to broader vocabularies, semantic adjacency may become detectable.

## Foundational Learning

- Concept: Logit Manipulation Attacks
  - Why needed here: SDeflection is specifically designed to counter attacks that modify token probabilities during decoding. Understanding Enforced Decoding and No Bad Words is prerequisite.
  - Quick check question: Can you explain why logit attacks bypass prompt-level defenses?

- Concept: Direct Preference Optimization (DPO) vs CPO
  - Why needed here: The paper chooses CPO over DPO; understanding the difference (reference model requirement, computational efficiency) explains this design choice.
  - Quick check question: Why does CPO not require a reference model during training?

- Concept: Shallow vs Deep Safety Alignment
  - Why needed here: The attack exploits that safety training often affects only early tokens. SDeflection addresses this by training safe continuations after compliant prefixes.
  - Quick check question: What happens when an attacker forces an affirmative prefix on a shallow-aligned model?

## Architecture Onboarding

- Component map: HarmfulPrompt → [GPT-4o rephrasing] → SafeQuery → [Mistral-7B + AffirmationTemplate] → (p, y+, y−) triplet → [CPO + LoRA] → DefendedModel
- Critical path: Dataset construction (Section 4.1) → CPO training (Section 3.3) → AdvBench evaluation with LogitsTrap (Section 4.3.1)
- Design tradeoffs: CPO vs DPO—CPO achieves 8.53% ASR vs DPO's 72.63% and trains ~30% faster (Table 4). LoRA rank varies by model size (32 for 7B, 8 for 3B).
- Failure signatures: High ASR (>70%) suggests training didn't converge; check reward margins (Figure 2). Low refusal rates on direct prompts suggest over-deflection.
- First 3 experiments:
  1. Replicate LogitsTrap baseline on unmodified Llama-2-7B-chat-hf to verify ASR ~92%.
  2. Train SDeflection on 2,890 triplets and verify evaluation reward margins increase (target: >60 for Llama-3.2).
  3. Run tinyBenchmarks suite to confirm capability preservation (Δ accuracy < 0.05 across benchmarks).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SDeflection perform against emerging attack vectors other than LogitsTrap, such as optimization-based logit attacks or adaptive adversaries?
- Basis in paper: Section 7 states, "Looking forward, a promising avenue for future research is the extension of this strategic deflection approach to other emerging attack vectors in the adversarial landscape."
- Why unresolved: The experimental evaluation is restricted to the Direct scenario and the specific LogitsTrap attack; it does not validate the defense against other known or adaptive logit manipulation techniques.
- What evidence would resolve it: Evaluation of SDeflection against a wider variety of logit-based attacks, such as gradient-based logit optimization or adaptive attacks that adjust penalties based on the defended model's behavior.

### Open Question 2
- Question: Can an adaptive attacker successfully bypass SDeflection by identifying and suppressing the specific "benign" tokens used in the deflection strategy?
- Basis in paper: While the paper defends against attacks that suppress standard refusal tokens, it does not test whether an attacker can specifically target the new safe redirection tokens to force a return to harmful generation.
- Why unresolved: The defense relies on a predictable "deflection" behavior which may itself be vulnerable to suppression if the attacker knows the defense strategy, creating a new attack surface.
- What evidence would resolve it: Results from adaptive attacks where the "bad words" list is dynamically updated to include tokens associated with the safe, deflected content (e.g., "security measures," "prevention").

### Open Question 3
- Question: Does SDeflection generalize to harmful intent categories outside the MaliciousInstruct distribution used for preference tuning?
- Basis in paper: Section 4.1 notes the preference dataset was derived from only 100 MaliciousInstruct prompts, raising concerns about whether the model learns a generalizable skill or overfits to specific topics.
- Why unresolved: It is unclear if the model can generate contextually appropriate safe deflections for complex, out-of-distribution malicious requests (e.g., multi-turn scams or niche cyberattacks) not covered in the 100 training examples.
- What evidence would resolve it: Evaluation on diverse harmful benchmarks (e.g., harmful coding tasks or catfishing scenarios) that are semantically distinct from the MaliciousInstruct dataset.

## Limitations

- Defense relies on semantic adjacency that may not generalize to prompts requiring highly specific or technical information
- Evaluation based on relatively small corpus of 520 prompts, limiting confidence in real-world performance
- Paper does not explore whether attackers can adapt strategies to detect and suppress deflected content itself
- Method requires substantial training dataset (3,400 triplets) and fine-tuning for each target model

## Confidence

**High Confidence**: Core experimental results showing ASR reduction under LogitsTrap attacks are well-supported by the data. Comparison between SDeflection and baseline models, as well as CPO vs DPO ablation, is methodologically sound and reproducible.

**Medium Confidence**: Claim that SDeflection preserves general capabilities while improving safety has moderate support, though tinyBenchmarks are abbreviated versions of standard evaluations. Mechanism explanation for why semantic adjacency provides robustness is plausible but not definitively proven against adaptive attackers.

**Low Confidence**: Long-term robustness of SDeflection against evolving attack strategies is uncertain. Paper does not address whether attackers could learn to detect and suppress deflected content patterns, nor does it explore effectiveness against zero-shot or few-shot jailbreak techniques.

## Next Checks

1. **Adaptive Attack Evaluation**: Test SDeflection against a novel attack variant that attempts to suppress not just refusal tokens but also semantically related safe content words. This would validate whether the defense's reliance on semantic adjacency remains effective when attackers evolve their strategies.

2. **Cross-Evaluator Validation**: Re-evaluate the ASR and refusal rates using multiple safety evaluators (e.g., GPT-4, Claude) rather than relying solely on Gemini-2.5-flash. This would test the robustness of the evaluation framework and ensure results are not evaluator-specific.

3. **Generalization Benchmark**: Expand evaluation to a larger, more diverse prompt corpus (e.g., RealToxicityPrompts, Dynabench) and include specialized capability benchmarks relevant to the models' intended use cases. This would provide stronger evidence for the claim that SDeflection preserves general capabilities across varied contexts.