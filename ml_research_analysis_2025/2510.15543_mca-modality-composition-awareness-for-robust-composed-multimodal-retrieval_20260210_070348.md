---
ver: rpa2
title: 'MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval'
arxiv_id: '2510.15543'
source_url: https://arxiv.org/abs/2510.15543
tags:
- retrieval
- modality
- composed
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We identify that multimodal large language models (MLLMs) trained
  with conventional contrastive learning are prone to modality shortcut, leading to
  poor robustness under distribution shifts. We propose modality composition awareness
  (MCA), which includes two objectives: a preference loss that enforces multimodal
  embeddings to outperform unimodal counterparts, and a composition regularization
  that aligns multimodal embeddings with prototypes composed from unimodal parts.'
---

# MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval

## Quick Facts
- arXiv ID: 2510.15543
- Source URL: https://arxiv.org/abs/2510.15543
- Reference count: 18
- Improves out-of-distribution retrieval performance by +5.9% while maintaining in-domain accuracy

## Executive Summary
This paper identifies modality shortcut learning in multimodal large language models (MLLMs) trained with contrastive learning, where models over-rely on a single dominant modality and ignore complementary signals. The authors propose Modality Composition Awareness (MCA), which includes two objectives: a preference loss that enforces multimodal embeddings to outperform unimodal counterparts, and a composition regularization that aligns multimodal embeddings with prototypes composed from unimodal parts. Experiments show MCA significantly improves out-of-distribution retrieval performance while maintaining in-domain accuracy, demonstrating that explicit modeling of modality composition is crucial for robust multimodal retrieval.

## Method Summary
MCA addresses modality shortcut learning in MLLMs for composed multimodal retrieval by introducing two regularization objectives. The method trains a unified MLLM encoder (Qwen2-VL-2B-Instruct) with LoRA fine-tuning (rank 8) using a composite loss: contrastive loss plus MCP (Modality Composition Preference) and MCR (Modality Composition Regularization) losses. MCP enforces that composed embeddings are more discriminative than unimodal counterparts, while MCR aligns composed embeddings with prototypes mixed from unimodal embeddings using gated fusion. The training uses AdamW optimizer (lr=2e-5), batch size 1024, and runs for 2000 steps on 8×A100 40G GPUs.

## Key Results
- MCA improves OOD retrieval performance by +5.9% while maintaining IND accuracy
- Ablation studies show both MCP and MCR components are essential for performance gains
- The method demonstrates robustness to distribution shifts across multiple OOD benchmarks
- Gated fusion mixer performs best among tested architectures for composition

## Why This Works (Mechanism)
MCA works by explicitly modeling the structural relationships between composed representations and their unimodal counterparts. The preference loss ensures the model leverages information from all modalities rather than shortcutting to a dominant one, while the composition regularization enforces consistency between composed embeddings and their constituent parts. This dual approach prevents the model from ignoring complementary information and encourages it to learn meaningful cross-modal interactions.

## Foundational Learning
**Modality Shortcut Learning**: Models trained with contrastive learning often over-rely on a single dominant modality, ignoring complementary information from other modalities. This is needed because standard contrastive learning doesn't explicitly encourage multimodal integration. Quick check: Compare unimodal vs multimodal performance on composed inputs.

**Composition Regularization**: Aligning composed embeddings with prototypes mixed from unimodal parts ensures the model maintains consistency with individual modality signals. This is needed to prevent the composed representation from diverging too far from its components. Quick check: Measure embedding similarity between composed and mixed prototypes.

**Preference Loss**: Enforcing that composed embeddings outperform unimodal counterparts in discrimination tasks ensures the model actually leverages multiple modalities. This is needed to guarantee multimodal integration provides value beyond individual modalities. Quick check: Verify composed embeddings have higher retrieval accuracy than unimodal variants.

## Architecture Onboarding

**Component Map**: Qwen2-VL-2B-Instruct -> LoRA Adapter -> Gated Fusion Mixer -> MCP Loss/MCR Loss -> Final Embedding

**Critical Path**: Input (text+image) -> Unified Encoder -> LoRA Adaptation -> Mixer Composition -> Regularized Loss Computation -> Output Embedding

**Design Tradeoffs**: Simple mixers (mean pooling, gated fusion, MFB) vs complex architectures; explicit composition awareness vs implicit learning; regularization strength vs performance preservation.

**Failure Signatures**: OOD degradation at low resolution with small MCA weight; minor IND performance drops due to regularization bias-variance tradeoff; inconsistent gradients when mixing noisy low-res representations.

**3 First Experiments**:
1. Test MCP loss alone (α>0, β=0) on OOD benchmarks to isolate preference effect
2. Test MCR loss alone (α=0, β>0) to measure composition regularization impact
3. Vary mixer architecture (mean pooling vs gated fusion) while keeping losses constant

## Open Questions the Paper Calls Out

**Open Question 1**: How does MCA's effectiveness scale when extended beyond two modalities (text+image) to settings with three or more modalities? The paper notes that as modality count increases, so does the risk of collapsing to a single dominant modality, but experiments only validate text+image composition.

**Open Question 2**: What is the optimal design of the mixer module for composing unimodal embeddings into prototypes? The paper only explores simple mixers and suggests richer architectures could yield better performance, but systematic comparison remains unexplored.

**Open Question 3**: Can the weighting coefficients (α, β) be determined automatically based on input characteristics rather than manual tuning? The paper shows strong interaction between input resolution and optimal weighting, suggesting dynamic adaptation could improve robustness.

## Limitations
- Gated fusion mixer architecture details are incompletely specified, creating reproducibility challenges
- Only three simple mixer architectures were tested, leaving rich design space unexplored
- Weighting coefficients require manual tuning with no principled mechanism for automatic adaptation

## Confidence
**High confidence**: Experimental findings showing MCA improves OOD performance (+5.9%) while maintaining IND accuracy are well-supported by results tables and ablation studies.

**Medium confidence**: Theoretical framing of modality composition awareness as explicit modeling approach is sound, but practical impact depends on unspecified architectural details of gated fusion mixer.

**Medium confidence**: Failure mode analysis regarding low-resolution inputs and small MCA weights is logically consistent but requires empirical validation across different model scales.

## Next Checks
1. Implement and test multiple variants of gated fusion mixer architecture to identify which configurations consistently reproduce reported OOD improvements
2. Conduct experiments with varying pooling strategies for extracting final embeddings from MLLM hidden states to determine optimal approach
3. Perform systematic sensitivity analysis across wider range of α and β values (including 0.1-1.0 range for degraded inputs) to establish robust hyperparameter guidelines