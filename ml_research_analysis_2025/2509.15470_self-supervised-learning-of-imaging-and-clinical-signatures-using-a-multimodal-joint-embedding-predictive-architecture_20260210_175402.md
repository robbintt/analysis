---
ver: rpa2
title: Self-supervised learning of imaging and clinical signatures using a multimodal
  joint-embedding predictive architecture
arxiv_id: '2509.15470'
source_url: https://arxiv.org/abs/2509.15470
tags:
- jepa
- dataset
- data
- multimodal
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of developing multimodal models
  for pulmonary nodule diagnosis, which is limited by the scarcity of labeled data
  and the tendency for these models to overfit on the training distribution. The authors
  propose a novel approach that leverages self-supervised learning from longitudinal
  and multimodal archives to address these challenges.
---

# Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture

## Quick Facts
- **arXiv ID**: 2509.15470
- **Source URL**: https://arxiv.org/abs/2509.15470
- **Reference count**: 0
- **Primary result**: JEPA pretraining improves internal validation (0.91 AUC) but underperforms external validation (0.72 AUC) compared to imaging-only baseline

## Executive Summary
This study addresses the challenge of developing multimodal models for pulmonary nodule diagnosis, which is limited by the scarcity of labeled data and the tendency for these models to overfit on the training distribution. The authors propose a novel approach that leverages self-supervised learning from longitudinal and multimodal archives to address these challenges. They curate an unlabeled set of patients with CT scans and linked electronic health records from their home institution to power joint embedding predictive architecture (JEPA) pretraining. After supervised finetuning, their approach outperforms an unregularized multimodal model and imaging-only model in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC), but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC). The authors also develop a synthetic environment to characterize the context in which JEPA may underperform. This work innovates an approach that leverages unlabeled multimodal medical archives to improve predictive models and demonstrates its advantages and limitations in pulmonary nodule diagnosis.

## Method Summary
The authors develop a self-supervised learning approach using joint embedding predictive architecture (JEPA) for multimodal pulmonary nodule diagnosis. They curate an unlabeled dataset of patients with CT scans and linked electronic health records from their institution, then pretrain a JEPA model to learn representations from this multimodal data. After pretraining, the model undergoes supervised finetuning on a labeled dataset for pulmonary nodule classification. The approach is compared against unregularized multimodal models and imaging-only models in both internal and external validation cohorts.

## Key Results
- JEPA pretraining achieves 0.91 AUC on internal validation, outperforming unregularized multimodal (0.88 AUC) and imaging-only (0.73 AUC) models
- On external validation, JEPA achieves 0.72 AUC, underperforming the imaging-only baseline (0.75 AUC)
- Synthetic environment analysis characterizes conditions where JEPA may underperform
- Self-supervised learning from unlabeled multimodal archives shows promise for improving multimodal predictive models

## Why This Works (Mechanism)
The approach works by leveraging abundant unlabeled multimodal data to learn rich representations that capture both imaging and clinical features before supervised finetuning. The JEPA architecture enables the model to learn joint embeddings that can effectively represent the relationship between CT imaging data and clinical information from electronic health records. This pretraining phase provides regularization that helps prevent overfitting to the training distribution, which is particularly valuable when labeled data is scarce. The synthetic environment analysis reveals that JEPA's performance depends on the alignment between pretraining and target distributions, explaining why it excels internally but struggles externally.

## Foundational Learning
- **Joint embedding predictive architecture (JEPA)**: A self-supervised learning framework that learns to predict future states from current observations in a shared embedding space. Needed to handle multimodal data effectively; check by verifying the model can reconstruct or predict one modality from another.
- **Multimodal pretraining**: Training models on multiple data types simultaneously to learn shared representations. Needed to leverage the complementary information in CT scans and clinical records; check by comparing performance with unimodal pretraining.
- **Self-supervised learning**: Learning from unlabeled data by creating proxy tasks. Needed to overcome labeled data scarcity; check by measuring improvement over supervised-only baselines.
- **Domain adaptation challenges**: The gap between training and deployment distributions. Needed to understand external validation underperformance; check by analyzing feature distribution shifts between cohorts.

## Architecture Onboarding

**Component Map**: CT Images -> JEPA Encoder -> Joint Embedding Space <- Clinical Data from EHR

**Critical Path**: Unlabeled multimodal data collection → JEPA pretraining → Supervised finetuning → Internal/external validation

**Design Tradeoffs**: The approach trades potential overfitting to the training institution's data distribution for improved performance on internal validation. Using self-supervised pretraining requires substantial unlabeled data but reduces dependency on labeled examples.

**Failure Signatures**: Significant performance degradation on external validation (0.72 vs 0.75 AUC) indicates overfitting to the training distribution. The model may fail when the relationship between imaging and clinical features differs across institutions.

**3 First Experiments**:
1. Vary the size of the unlabeled pretraining dataset to quantify the relationship between pretraining data volume and performance
2. Test different JEPA architectures (e.g., varying embedding dimensions, attention mechanisms) to optimize representation learning
3. Implement domain adaptation techniques during finetuning to improve external validation performance

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance degradation in external validation (0.72 AUC vs 0.75 for imaging-only baseline) indicates limited generalizability
- Single-institution pretraining data may create bias in learned representations
- Theoretical synthetic environment analysis lacks extensive empirical validation across diverse distributions
- Does not establish clear superiority over existing state-of-the-art approaches

## Confidence
- **High confidence**: Internal validation results showing JEPA outperforms unregularized multimodal and imaging-only models
- **Medium confidence**: Synthetic environment analysis providing theoretical characterization of JEPA limitations
- **Low confidence**: External validation results and generalizability claims due to significant performance drop

## Next Checks
1. **External validation across multiple institutions**: Replicate the study using multimodal data from 3-5 diverse healthcare systems to assess true generalizability and identify whether the performance gap in external cohorts can be mitigated.

2. **Ablation studies on pretraining data diversity**: Systematically vary the size, diversity, and characteristics of the unlabeled pretraining dataset to quantify how these factors impact both internal and external validation performance.

3. **Longitudinal performance monitoring**: Track model performance over time as more labeled data becomes available to determine whether the self-supervised pretraining provides sustained advantages or if supervised learning alone eventually catches up.