---
ver: rpa2
title: 'Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models'
arxiv_id: '2507.18171'
source_url: https://arxiv.org/abs/2507.18171
tags:
- tokens
- sticky
- attention
- token
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a novel type of problematic tokens\u2014\
  sticky tokens\u2014in text embedding models that can significantly degrade performance.\
  \ When repeatedly inserted, these tokens pull sentence similarities toward the mean\
  \ embedding similarity, undermining downstream tasks like retrieval and clustering."
---

# Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models

## Quick Facts
- arXiv ID: 2507.18171
- Source URL: https://arxiv.org/abs/2507.18171
- Authors: Kexin Chen; Dongxia Wang; Yi Liu; Haonan Zhang; Wenhai Wang
- Reference count: 40
- One-line primary result: Identified sticky tokens that degrade embedding quality by pulling similarities toward the mean, affecting clustering and retrieval tasks.

## Executive Summary
This paper identifies a novel type of problematic tokens—sticky tokens—in text embedding models that can significantly degrade performance. When repeatedly inserted, these tokens pull sentence similarities toward the mean embedding similarity, undermining downstream tasks like retrieval and clustering. The authors introduce Sticky Token Detector (STD), a method that filters sentence pairs and tokens, shortlists candidates via sticky scoring, and validates them with an adaptive threshold. Applied to 40 checkpoints across 14 model families, STD uncovers 868 sticky tokens, many stemming from special or unused vocabulary entries and multilingual subword fragments. Performance tests show sticky tokens cause up to 50% degradation in clustering and retrieval tasks, and attention-layer analysis reveals their disproportionate influence. The findings highlight tokenization robustness as a key issue for future embedding model design.

## Method Summary
The Sticky Token Detector (STD) pipeline identifies tokens that pull sentence similarity toward the mean when repeatedly inserted. It operates in four steps: (1) filter sentence pairs with similarity below the mean token embedding similarity, (2) filter undecodable or unreachable tokens, (3) compute sticky scores for remaining tokens using sampled sentence pairs and insertion operations (prefix, suffix, random), and (4) validate candidates against an adaptive threshold. The method uses 8 token insertions per test and evaluates candidates on downstream tasks to confirm their impact.

## Key Results
- 868 sticky tokens identified across 40 checkpoints spanning 14 model families
- Up to 50% performance degradation in clustering and retrieval tasks when sticky tokens are present
- Attention analysis shows sticky tokens receive disproportionately high weights in intermediate layers
- Most sticky tokens originate from special/unused vocabulary entries and fragmented multilingual subwords

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain tokens in a vocabulary, when repeatedly inserted into text, bias the computed embedding similarity toward the average similarity of the model's entire token embedding space.
- Mechanism: Text embedding models map tokens into an anisotropic (non-uniform) embedding space where most vectors occupy a narrow cone. Some tokens, particularly rare or under-trained ones (e.g., special tokens, multilingual fragments), have embeddings that lie near the "center of mass" of this distribution. When such tokens are repeatedly appended to any input, the pooled sequence embedding is pulled toward this central tendency, compressing the variance of pairwise similarities across all sentence pairs. This is detected by the paper's Sticky Token Detector (STD) which measures the change in similarity $|Sim(s_1, I(s_2, t, n)) - u|$, where $u$ is the mean pairwise token similarity.
- Core assumption: The model's token embedding space is anisotropic, meaning embeddings are not uniformly distributed around the origin but instead cluster in a specific region of the vector space.
- Evidence anchors:
  - [abstract] "These tokens... pull sentence similarity toward a certain value... often stemming from special or unused entries... as well as fragmented subwords..."
  - [Section 3.1] "...repeatedly adding the anomalous token lucrarea consistently 'pulls' the pairwise similarity to a value near the median of the distribution, which also aligns with the mean pairwise similarity among token embeddings..."
  - [Appendix C / Page 14] "We propose a conjecture... the anisotropy of the model embedding space, indicating that word representations occupy narrow cone-shaped regions... Sticky tokens tend to pull a sentence toward a specific focal point..."
- Break condition: This mechanism would not apply to models with perfectly isotropic (uniformly distributed) embedding spaces, as there would be no single "mean" direction for tokens to pull embeddings toward.

### Mechanism 2
- Claim: Sticky tokens disproportionately dominate the attention mechanism in intermediate Transformer layers, leading to a final sequence representation that is overly influenced by the sticky token rather than the semantic content of the original text.
- Mechanism: The Transformer's self-attention mechanism assigns weights to tokens based on their relevance. Analysis reveals that sticky tokens receive abnormally high attention weights compared to normal tokens, particularly in later layers. This disproportionate attention causes the model to aggregate information primarily from the sticky token's embedding, amplifying its centralizing effect on the final pooled vector. The paper shows this via the distribution of attention weights and the increasing Wasserstein distance between attention patterns of sticky vs. normal tokens across layers.
- Core assumption: The final embedding of a sequence is a function of the attention-weighted aggregation of token representations, so tokens with higher attention weights have a greater influence on the final output.
- Evidence anchors:
  - [abstract] "Attention-layer analysis reveals that sticky tokens disproportionately dominate the model's internal representations..."
  - [Section 5.4] "As illustrated in Figure 4 left, when sticky tokens are appended to sentences, their attention weights in intermediate layers concentrate disproportionately in high-value ranges... This suggests that sticky tokens dominate the model's attention..."
- Break condition: This mechanism assumes a standard Transformer architecture where attention weights are used for weighted aggregation. It may not apply to models with fundamentally different aggregation schemes (e.g., weighted averaging independent of attention).

### Mechanism 3
- Claim: The presence of sticky tokens is a direct artifact of tokenization choices and pre-training data coverage, where certain vocabulary entries are under-trained or lack sufficient semantic context.
- Mechanism: Subword tokenizers like BPE or SentencePiece create vocabularies that may include special tokens, unused tokens, or fragments from multilingual corpora. If these tokens appear very rarely or in meaningless contexts during the massive but finite pre-training of the embedding model, their learned representations remain under-specified. Instead of capturing a distinct semantic, their embedding becomes a "default" or "average" vector. When these tokens are forced into a sequence at inference time, they exert their under-trained, averaging influence on the result.
- Core assumption: Token embeddings are learned from data, and tokens with insufficient or non-representative training data will have poorly defined or "default" embeddings.
- Evidence anchors:
  - [abstract] "...these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora."
  - [Section 2] "For example, 'under-trained' tokens in LLMs... exhibit abnormal behaviors due to incomplete or skewed pre-training coverage."
  - [Section 5.2] "Our analysis reveals that these tokens often stem from special or unused entries in the vocabulary, as well as fragmented subwords from multiple languages... suggesting that tokenizer design and pre-training coverage both play important roles."
- Break condition: This mechanism presumes a fixed vocabulary learned via statistical methods (like BPE) on imperfect data. It may be less relevant for models with dynamically generated vocabularies or perfectly balanced, context-rich training data for every token.

## Foundational Learning

- Concept: **Anisotropy in Embedding Spaces**
  - Why needed here: Understanding that embedding models produce vectors clustered in a narrow cone, rather than being uniformly spread out, is the geometric foundation for why a "mean" similarity exists and why tokens can pull embeddings toward it. Without this, the centralizing behavior of sticky tokens is mathematically inexplicable.
  - Quick check question: If an embedding model produced perfectly isotropic vectors (uniformly distributed on a unit sphere), what would the average cosine similarity between random token embeddings be, and how would this affect the "sticky token" mechanism?

- Concept: **Subword Tokenization (BPE, WordPiece, Unigram)**
  - Why needed here: The origin of sticky tokens is linked to the tokenization process itself. Understanding how tokenizers break words into subword units, create vocabularies from frequency statistics, and handle multilingual text is essential for diagnosing *why* specific tokens (like multilingual fragments or special tokens) become under-trained or semantically void.
  - Quick check question: How might a Byte-Pair Encoding (BPE) tokenizer create a "fragmented subword" token from a multilingual corpus, and why would such a token be prone to poor training?

- Concept: **Self-Attention Mechanism in Transformers**
  - Why needed here: The paper's evidence for the sticky token effect relies on analyzing attention weights. To interpret the results (e.g., "disproportionate dominance" in attention), one must understand how attention scores are calculated ($Q K^T / \sqrt{d}$) and how they are used to create a weighted sum of value vectors. The mechanism is incomplete without this link between token anomalies and model internals.
  - Quick check question: In a self-attention layer, if a specific token consistently receives an attention weight close to 1.0 from all other tokens in the sequence, what is the practical effect on that token's contribution to the final output representation of the sequence?

## Architecture Onboarding

- Component Map: Tokenizer -> Embedding Layer -> Transformer Encoder -> Pooling Layer -> Downstream Task Head
- Critical Path: The **Tokenization -> Embedding Lookup -> Self-Attention -> Pooling** pipeline is the critical path. An anomalous token from the tokenizer enters via its flawed embedding, its influence is magnified through the attention layers, and its averaging effect is finalized at the pooling stage, producing a degraded sentence embedding.
- Design Tradeoffs:
  - **Vocabulary Size vs. Token Coverage**: A larger vocabulary supports more languages and rare words but increases the chance of creating under-trained tokens (sticky token candidates).
  - **Tokenizer Training Data vs. Pre-training Data**: A mismatch can create tokens that exist in the vocabulary but are never meaningfully seen during model pre-training, leading to under-specified embeddings.
  - **Detection Threshold ($\epsilon$) Selection**: In STD, a low threshold is strict (fewer false positives, higher risk of missing sticky tokens), while a high threshold is permissive (more comprehensive, higher risk of flagging normal tokens).
- Failure Signatures:
  - **Clustering/Retrieval Failure**: Intra-cluster similarity becomes artificially high and uniform. Inter-cluster similarity is reduced. Nearest-neighbor retrieval returns semantically unrelated documents that happen to contain the same sticky token.
  - **Embedding Distribution Anomaly**: A large fraction of sentence embeddings cluster tightly around a single point in vector space (the "mean" of the anisotropic cone).
  - **Attention Map Anomaly**: Visualizations of self-attention heads show a single token (the sticky token) receiving a majority of the attention weight across almost all heads and layers.
- First 3 Experiments:
  1. **Sticky Token Scan**: Implement and run the STD pipeline from the paper on a chosen text embedding model (e.g., `all-MiniLM-L6-v2`). Use a standard STS dataset. Generate the list of validated sticky tokens and report the count and common examples (special tokens, multilingual fragments).
  2. **Similarity Perturbation Test**: Select 10 validated sticky tokens and 10 normal tokens. For a fixed set of 1,000 sentence pairs, calculate the baseline cosine similarity. Then, append each sticky/normal token $n=8$ times to one sentence in each pair and measure the change in similarity. Plot the distributions of $\Delta Sim$ for sticky vs. normal tokens to visually confirm the "pulling to the mean" effect.
  3. **Downstream Task Degradation A/B Test**: Take a small retrieval dataset (e.g., SciFact). Create two versions of the corpus: a baseline version and a "poisoned" version where a validated sticky token is appended to all documents. Evaluate retrieval performance (e.g., NDCG@10) on both corpora using a clean query set. Report the performance drop to quantify the real-world impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sticky token phenomenon manifest in text embedding models that possess isotropic embedding spaces or non-Gaussian similarity distributions?
- Basis in paper: [Explicit] The authors note in the Limitations section that their definition assumes tokens pull similarity toward the token embedding mean, but it "remains unclear whether these models [with non-Gaussian distributions or isotropic spaces] exhibit abnormal features akin to sticky token properties."
- Why unresolved: The proposed detection method (STD) relies on calculating the mean of a presumably Gaussian distribution of token similarities. Models with isotropic spaces (where mean similarity is near zero) or uniform distributions break this assumption, making it unclear if "sticky" behavior is even possible or detectable using current definitions.
- What evidence would resolve it: Applying the detection logic (or a modified version thereof) to models known to have isotropic embedding spaces (e.g., specific whitened BERT models) and observing if specific tokens still artificially manipulate sentence similarity scores.

### Open Question 2
- Question: What are the most effective concrete mitigation strategies, such as tokenizer retraining or embedding space regularization, to neutralize the impact of sticky tokens without degrading model performance?
- Basis in paper: [Explicit] The authors explicitly state in the Limitations and Conclusion that while they identify the anomalous phenomenon and its impact, "we do not propose concrete solutions to mitigate sticky tokens."
- Why unresolved: The paper identifies the problem (tokenizer design issues, unused tokens) but stops short of testing solutions. Proposed solutions like "Tokenizer Sanitization" are mentioned as potential strategies in Appendix H.2, but their efficacy and potential side-effects (e.g., the impact of vocabulary pruning on embedding layer parameters) are not evaluated.
- What evidence would resolve it: A comparative study where identified sticky tokens are removed from the vocabulary or their embeddings are regularized during fine-tuning, followed by a re-evaluation of downstream task performance (e.g., retrieval accuracy on NFCorpus).

### Open Question 3
- Question: How robust are text embedding models to sticky tokens when they are inserted using complex adversarial strategies, such as context-aware placement or interleaving, rather than fixed repetition?
- Basis in paper: [Explicit] The authors acknowledge in the Limitations section that their analysis "did not extend to more complex adversarial scenarios," restricting experiments to fixed positions (prefix, suffix, random) with predefined repetition counts.
- Why unresolved: Real-world adversarial attacks (e.g., poisoning a RAG database) are unlikely to use naive, repetitive insertion. It is unknown if "smarter" insertion strategies amplify the "mean-pulling" effect or if the model's attention mechanism filters them out more effectively than naive repetition.
- What evidence would resolve it: Experiments utilizing optimization techniques (e.g., gradient-based search) to find the optimal placement and frequency of sticky tokens to maximize the degradation of similarity metrics.

## Limitations

- The sticky token phenomenon relies on the assumption of anisotropic embedding spaces, which may not hold for all models.
- The detection method's threshold sensitivity could lead to false positives or negatives depending on model-specific distributions.
- The evaluation uses artificial token insertion (8 times) which may not reflect realistic usage patterns.
- The causal link between attention mechanisms and final embedding degradation is not definitively established.

## Confidence

- **High Confidence**: The existence of tokens that degrade embedding quality when inserted is well-supported by the 40-model study and downstream task evaluations. The STD method is clearly defined and reproducible.
- **Medium Confidence**: The geometric explanation (anisotropic space + central tendency) is plausible and supported by the data, but relies on unvalidated assumptions about the universal nature of embedding space geometry. The attention-based mechanism is supported by empirical observations but lacks definitive causal proof.
- **Low Confidence**: The exact prevalence of sticky tokens in *naturally occurring* text (as opposed to artificially inserted tokens) is not quantified. The long-term impact on models exposed to sticky tokens during fine-tuning is unknown.

## Next Checks

1. **Real-World Occurrence Analysis**: Instead of artificially inserting tokens 8 times, analyze a large corpus of real text to identify naturally occurring instances of candidate sticky tokens. Measure the actual frequency and distribution of these tokens, and quantify the degradation in embedding quality when they appear in their natural frequency versus when they are artificially inserted. This will validate if the phenomenon is a laboratory artifact or a genuine concern for deployed models.

2. **Cross-Modal Sticky Token Transfer**: Test whether sticky tokens identified in text embedding models also exhibit similar "pulling" effects in other embedding modalities, such as image or audio embeddings from multimodal models (e.g., CLIP, AudioCLIP). If the phenomenon is specific to text embeddings, it suggests a unique property of language representation; if it's cross-modal, it points to a more fundamental issue in how embedding models learn and represent information.

3. **Fine-tuning Robustness Study**: Fine-tune a text embedding model on a corpus where sticky tokens are deliberately inserted at a low frequency (e.g., 0.1% of tokens). Evaluate the model's performance on downstream tasks before and after fine-tuning. If the model learns to ignore or mitigate the influence of sticky tokens, it suggests that the effect can be "trained out." If performance degrades further, it confirms that sticky tokens are a persistent problem that requires architectural or training procedure changes, not just detection.