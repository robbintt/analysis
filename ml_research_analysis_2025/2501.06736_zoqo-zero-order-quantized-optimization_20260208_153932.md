---
ver: rpa2
title: 'ZOQO: Zero-Order Quantized Optimization'
arxiv_id: '2501.06736'
source_url: https://arxiv.org/abs/2501.06736
tags:
- quantized
- zoqo
- quantization
- optimization
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZOQO, a zero-order quantized optimization
  method designed to train models with quantized parameters and operations in resource-constrained
  environments. The key innovation is integrating zero-order optimization (eliminating
  backpropagation) with quantized training, maintaining parameter quantization throughout
  without requiring full-precision gradient calculations.
---

# ZOQO: Zero-Order Quantized Optimization

## Quick Facts
- **arXiv ID**: 2501.06736
- **Source URL**: https://arxiv.org/abs/2501.06736
- **Reference count**: 38
- **Primary result**: ZOQO achieves competitive performance to full-precision methods for black-box adversarial attacks and LLM fine-tuning while reducing memory usage by approximately 50% through quantization

## Executive Summary
ZOQO introduces a zero-order quantized optimization method that enables training models with quantized parameters and operations in resource-constrained environments. The method integrates zero-order optimization with quantized training, maintaining parameter quantization throughout without requiring full-precision gradient calculations. By adapting the zero-sign stochastic gradient descent (ZO-SignSGD) approach with quantized noise injection and scaled learning rates, ZOQO demonstrates competitive performance compared to full-precision methods in two applications: black-box adversarial attacks and fine-tuning of large language models for sentiment analysis.

## Method Summary
ZOQO builds on zero-order optimization by replacing gradient computation with finite-difference approximations using quantized noise. The method quantizes parameters to uniform grids and injects noise sampled from discrete distributions instead of Gaussian noise. Learning rates are scaled to integer multiples of the quantization step size, ensuring parameters remain on the quantization grid throughout training. The update direction is determined by the sign of loss differences from two perturbed forward passes, eliminating the need for backpropagation. For LLM fine-tuning, ZOQO incorporates LoRA adapters with layer-wise quantization range computation.

## Key Results
- On MNIST and CIFAR-10 adversarial attacks, ZOQO achieves 0.00 failure rate at 8-bit quantization, comparable to full-precision methods
- For LLM fine-tuning with LoRA on SST2 task, ZOQO maintains 64.34% accuracy at 4-bit quantization versus 89.68% at 8-bit
- Memory usage simulations show approximately 50% reduction compared to full-precision training

## Why This Works (Mechanism)

### Mechanism 1
Replacing Gaussian noise with quantized noise preserves gradient sign estimation quality while enabling fully discrete operations. Instead of sampling from continuous N(0, μ²), noise is sampled from discrete set B = {−ms, (−m+1)s, ..., ms} where m = max{⌊μ/s⌋, 1}. This makes noise injection a simple integer addition of two quantized vectors, keeping all intermediate values on the quantization grid.

### Mechanism 2
Quantizing the learning rate to integer multiples of the quantization scale maintains parameters on-grid throughout training. The adjusted learning rate ηq = max{⌊η/s⌋, 1}·s ensures that every parameter update moves by exactly an integer number of quantization steps. Combined with sign-only updates (uniform magnitude), this guarantees quantization is preserved without rounding operations.

### Mechanism 3
Using only the sign of loss differences between two perturbed queries provides sufficient optimization signal without gradient computation. At each step, compute sign(ℓ(x+u) − ℓ(x−u)) · sign(u) to estimate gradient direction. This requires only two forward passes and a comparison—no backward pass, no gradient storage. The loss function itself can remain in floating point since only its sign matters.

## Foundational Learning

- **Concept: Zero-Order Optimization**
  - Why needed here: ZOQO builds on ZO-SignSGD; understanding how gradient-free optimization estimates gradients via finite differences is essential.
  - Quick check question: Can you explain why the Randomized Gradient Estimation formula âf = (1/q) Σ[ℓ(x + μui) − ℓ(x − μui)] / (2μ) · ui approximates a gradient?

- **Concept: Uniform Quantization**
  - Why needed here: The method relies on uniform mid-tread quantization with scale s = (Rmax − Rmin)/(2^b − 1); understanding quantization grids is necessary for implementing the noise and learning rate adaptations.
  - Quick check question: Given parameters in range [−1, 1] and 4-bit quantization, what is the quantization scale s and what values can quantized parameters take?

- **Concept: SignSGD / ZO-SignSGD**
  - Why needed here: ZOQO adapts ZO-SignSGD; the core insight is that update direction (sign) often matters more than magnitude.
  - Quick check question: Why might sign-based optimization be more robust to quantization than full gradient methods?

## Architecture Onboarding

- **Component map**: Quantization Module → Noise Sampler → Sign Estimator → Parameter Updater → Clamp
- **Critical path**: Initialize → Quantize parameters → [Sample noise → Forward passes → Compare losses → Compute sign → Update parameters → Clamp] (loop)
- **Design tradeoffs**:
  - Higher bit-width (b=8) → better accuracy but more memory; lower (b=4) → aggressive compression but degraded performance
  - Single query (q=1) → 2 forward passes per step, minimal memory; more queries → better gradient estimate but higher compute
  - Uniform vs. discretized Gaussian noise: Uniform is simpler but may harm convergence; discretized Gaussian better approximates RGE but requires precomputation
- **Failure signatures**:
  - Parameters escaping quantization grid → check learning rate quantization ηq is correctly computed
  - Degraded accuracy at low bits → verify noise distribution spans multiple quantization levels (m > 1)
  - High variance in training → may need more queries (q > 1) or larger ZO step size μ
- **First 3 experiments**:
  1. Replicate MNIST adversarial attack (Table I, b=8) with SignHunter + ZOQO to validate implementation matches paper's 0.00 failure rate.
  2. Run memory simulation (Table V) comparing FP, QAT, and ZOQO on toy 3-layer network to verify ~50% memory reduction claim.
  3. Fine-tune OPT-1.3b with LoRA on SST2 at 8-bit quantization to confirm accuracy in 89-92% range before testing 4-bit regime.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can rigorous convergence guarantees be established for ZOQO in convex and bounded settings, and under what precise conditions does it converge efficiently?
- Basis in paper: "Future work includes developing a rigorous convergence theory for ZOQO in convex and bounded cases, focusing on establishing clear theoretical guarantees."
- Why unresolved: The paper provides empirical validation but lacks formal convergence proofs; theoretical understanding of how quantization noise interacts with zero-order sign estimation remains undeveloped.
- What evidence would resolve it: Formal proofs bounding convergence rates under specific assumptions (convexity, Lipschitz continuity, quantization levels), with validation against empirical trajectories.

### Open Question 2
- Question: Can the sign of loss differences be estimated without computing exact loss values, potentially using comparison-based approaches inspired by human feedback models?
- Basis in paper: "One possible direction to explore is using tools from human comparison feedback training... A possible direction might be taking inspiration from human feedback models."
- Why unresolved: The current method still requires FP loss calculations; whether coarse comparison mechanisms could replace exact loss evaluation is unexplored.
- What evidence would resolve it: Experiments showing that approximate comparison oracles (without exact loss values) maintain competitive performance, with analysis of required comparison precision.

### Open Question 3
- Question: Would adaptive or non-uniform quantization schemes improve ZOQO's performance, particularly for aggressive low-bit regimes?
- Basis in paper: "Although we use uniform quantization for simplicity, more sophisticated methods could be applied" and "Further work will employ advanced quantization techniques, such as adaptive quantization."
- Why unresolved: Layer-wise uniform quantization was used, but whether per-parameter or gradient-aware adaptive quantization could mitigate the 4-bit performance degradation remains unknown.
- What evidence would resolve it: Comparative experiments with adaptive quantization methods showing improved accuracy at low bit-widths, particularly for LLM fine-tuning.

### Open Question 4
- Question: How does ZOQO perform in distributed optimization settings or reinforcement learning, where communication constraints and environment interactions align with its quantized zero-order nature?
- Basis in paper: "We will also explore ZOQO's applicability in other machine learning areas, including reinforcement learning and distributed optimization."
- Why unresolved: Current experiments only cover adversarial attacks and LLM fine-tuning; the method's applicability beyond supervised learning tasks is untested.
- What evidence would resolve it: Successful application of ZOQO to RL policy optimization or distributed training scenarios, with analysis of communication/memory savings versus performance trade-offs.

## Limitations

- Performance degradation at aggressive quantization (4-bit) for LLM fine-tuning suggests fundamental limits to ZOQO's effectiveness, with accuracy dropping from 89.68% at 8-bit to 64.34% at 4-bit.
- The method inherits zero-order optimization's query inefficiency - requiring two forward passes per iteration, which may limit applicability where computational budget rather than memory is the constraint.
- Memory usage simulations show ~50% reduction but this is not validated on actual hardware with real memory measurements.

## Confidence

- **High confidence**: The mechanism for maintaining quantization through integer-scaled learning rates and sign-only updates is well-founded and directly implementable.
- **Medium confidence**: The claim of ~50% memory reduction through quantization is supported by simulation but not validated on actual hardware with real memory measurements.
- **Medium confidence**: Competitive performance on adversarial attacks is demonstrated, though comparisons use specific baselines (SignHunter) that may not represent the full state of the art.

## Next Checks

1. **Memory validation**: Implement ZOQO on actual hardware with memory profiling to verify the claimed 50% reduction versus full-precision training, accounting for real-world quantization overheads and cache effects.

2. **Noise distribution sensitivity**: Systematically evaluate how different noise discretization schemes (uniform vs discretized Gaussian) affect convergence across multiple tasks, measuring both accuracy and query efficiency.

3. **Hyperparameter stability**: Conduct ablation studies varying learning rate, ZO step size μ, and quantization levels to identify stable operating regimes and quantify sensitivity to initialization and data distribution.