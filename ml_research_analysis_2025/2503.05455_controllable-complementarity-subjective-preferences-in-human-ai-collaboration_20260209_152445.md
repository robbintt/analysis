---
ver: rpa2
title: 'Controllable Complementarity: Subjective Preferences in Human-AI Collaboration'
arxiv_id: '2503.05455'
source_url: https://arxiv.org/abs/2503.05455
tags:
- human
- preferences
- behavior
- each
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding human subjective
  preferences in human-AI collaboration beyond just objective task performance. The
  authors introduce Behavior Shaping (BS), a reinforcement learning method that enables
  explicit human control over AI behavior by conditioning policies on interpretable
  behavioral weights.
---

# Controllable Complementarity: Subjective Preferences in Human-AI Collaboration

## Quick Facts
- arXiv ID: 2503.05455
- Source URL: https://arxiv.org/abs/2503.05455
- Authors: Chase McDonald; Cleotilde Gonzalez
- Reference count: 16
- One-line primary result: Humans strongly prefer controllable AI partners over fixed or hidden-setting partners in collaborative tasks, with preferences moderated by how closely AI adheres to specified settings and tendency to choose predictable behaviors over optimal outcomes.

## Executive Summary
This paper addresses the problem of understanding human subjective preferences in human-AI collaboration beyond just objective task performance. The authors introduce Behavior Shaping (BS), a reinforcement learning method that enables explicit human control over AI behavior by conditioning policies on interpretable behavioral weights. In Experiment 1, they validate BS against standard self-play policies and find that BS achieves significantly higher objective performance and is subjectively preferred by humans in most Overcooked layouts (p < 0.001). In Experiment 2, participants showed strong subjective preferences for controllable AI partners over fixed or hidden-setting partners (p < 0.001), with preferences further moderated by how closely the AI adhered to specified settings (p < 0.001). Additionally, participants consistently chose weight combinations that reduced behavioral entropy, suggesting a preference for predictability over optimal objective outcomes. These findings demonstrate that controllability and predictability are key factors in human subjective preferences, extending the concept of human-AI complementarity beyond objective performance to include subjective experiences.

## Method Summary
The paper introduces Behavior Shaping (BS), a reinforcement learning algorithm that conditions policies on interpretable behavioral weights to enable explicit human control over AI behavior. Using the Overcooked collaborative cooking environment, the authors implement BS with three behavioral reward functions (delivery act, onion-in-pot, plating) and compare it against self-play baseline policies. The method trains conditional policies that learn different behaviors based on weight values, which humans can manipulate at test time. Two experiments evaluate the approach: Experiment 1 compares BS against self-play baseline across five layouts, measuring objective performance and subjective preferences; Experiment 2 tests controllability by comparing controllable AI partners against fixed and hidden-setting conditions, measuring adherence to settings and subjective preferences.

## Key Results
- BS achieves significantly higher objective performance than self-play baseline across 4/5 Overcooked layouts (p < 0.001)
- Humans show strong subjective preference for controllable AI partners over fixed or hidden-setting partners (p < 0.001)
- Preferences are moderated by how closely AI adheres to specified settings (p < 0.001 interaction effect)
- Participants consistently choose weight combinations that reduce behavioral entropy, indicating preference for predictability over optimal objective outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning policies on interpretable behavioral weights during training enables explicit human control at deployment time.
- Mechanism: Policies learn π(a|o, ω) where ω represents behavioral weights sampled from distributions Dψ. Each ω corresponds to auxiliary reward functions ψ that shape behavior. At test time, humans can set ω manually to elicit specific behaviors.
- Core assumption: Behavioral reward functions are human-interpretable and the policy learns meaningful behavioral responses to weight variations.
- Evidence anchors:
  - [abstract] "Behavior Shaping (BS), a reinforcement learning algorithm that allows humans explicit control over AI behavior"
  - [section 3.2] "If a policy learns to maximize its reward conditioned on different values of these weights, it will learn different behaviors when the weights are manipulated"
  - [corpus] Weak direct corpus evidence; related work on human-AI trade-offs mentions control but not this specific mechanism.
- Break condition: If behavioral reward functions are not sufficiently distinct or don't map to meaningful behaviors, weight manipulation produces no useful behavioral change (observed in Forced Coordination layout).

### Mechanism 2
- Claim: Random sampling of behavioral weights during training simulates a diverse partner population, improving zero-shot coordination with humans.
- Mechanism: By sampling ω ~ Dψ each episode, a single conditional policy encounters diverse behavioral contexts. This implicitly captures characteristics that population-based methods achieve with multiple policies.
- Core assumption: Sufficient diversity in behavioral reward functions creates training variation that transfers to human partners.
- Evidence anchors:
  - [section 3.2] "we represent an arbitrarily diverse population of agents in a single conditional policy"
  - [section 4] "BS was able to simulate diversity that resulted in more effective partners, relative to SP" (significant score improvements on 4/5 layouts, p < 0.001)
  - [corpus] Related work on zero-shot coordination (Strouse et al.) uses population-based approaches; BS offers a single-policy alternative.
- Break condition: When behavioral mechanisms don't create meaningful inter-agent diversity (e.g., Forced Coordination where one agent handles all subtasks), robustness gains fail.

### Mechanism 3
- Claim: Humans prefer controllable AI partners, with preferences moderated by adherence to specified settings and reduced behavioral entropy.
- Mechanism: Controllability enables humans to reduce behavioral entropy (predictability). Subjective preference is amplified when AI closely follows settings; deviations are penalized more heavily when humans specified them.
- Core assumption: Humans value predictability and subjective experience alongside—or sometimes above—objective performance.
- Evidence anchors:
  - [abstract] "participants consistently chose weight combinations that reduced behavioral entropy, suggesting a preference for predictability over optimal objective outcomes"
  - [section 5, Table 1] Interaction term between "Followed Settings" and Controllable condition significant for effectiveness (p < 0.001) and enjoyability (p < 0.001)
  - [corpus] "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates" supports predictability as a measurable predictor of teammate quality.
- Break condition: If AI fails to adhere to settings (low "Followed Settings" rating), the subjective preference advantage for controllability diminishes.

## Foundational Learning

- Concept: Contextual Markov Decision Processes (CMDPs)
  - Why needed here: BS is formalized as inducing a CMDP where ω values represent context that alters both observation and reward.
  - Quick check question: How does conditioning on context differ from standard MDP observation conditioning?

- Concept: Zero-Shot Coordination
  - Why needed here: The paper evaluates whether BS-trained policies generalize to unseen human partners without additional fine-tuning.
  - Quick check question: What properties make a policy robust to diverse unseen partners?

- Concept: Behavioral Reward Shaping
  - Why needed here: Understanding how auxiliary reward signals ψ guide behavior without altering the environment's extrinsic reward R.
  - Quick check question: How do behavioral rewards differ fundamentally from environment rewards in BS?

## Architecture Onboarding

- Component map:
  Conditional Policy π(a|o, ω) -> Behavioral Reward Functions ψ₁...ψₖ -> Weight Distributions Dψ -> PPO Training Loop with augmented reward r' = R + Σψₖ(s, a|ωₖ)

- Critical path:
  1. Define domain-appropriate, interpretable behavioral reward functions
  2. Initialize weight distributions (start with N(0,1) as baseline)
  3. Train with PPO, sampling fresh ω per episode
  4. Discretize weights for human control (e.g., Discourage/Neutral/Encourage)
  5. Expose controls to users; measure adherence and preference

- Design tradeoffs:
  - K (number of behavioral dimensions): More control granularity vs. interpretability burden
  - Dψ breadth: Wider sampling = more diversity but potential training instability
  - Behavioral reward selection: Must be both human-interpretable AND behaviorally impactful

- Failure signatures:
  - No score improvement over baseline: Behavioral rewards may not create meaningful diversity
  - Low "Followed Settings" ratings: Policy not responding to ω changes—check training convergence
  - Users avoid control despite availability: Controls may be confusing or behaviors not distinguishable

- First 3 experiments:
  1. Validate BS vs self-play baseline across multiple layouts (objective score + subjective preference)
  2. Verify weight sensitivity: Confirm different ω values produce observable behavioral differences
  3. A/B test controllability: Compare Controllable vs Fixed vs Hidden conditions to isolate control effect on preference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can behavioral reward functions be designed or automated to ensure robustness across diverse collaborative structures?
- Basis in paper: [explicit] The authors note that the failure to improve performance in the "Forced Coordination" layout "underscores the importance of appropriate behavioral reward selection."
- Why unresolved: The current implementation relies on hand-defined rewards ($\psi$) which failed to induce diverse behavior in layouts with high role asymmetry, limiting the generalizability of the Behavior Shaping method.
- What evidence would resolve it: Successful application of BS in asymmetric environments using automated skill discovery or learned reward functions that adapt to role constraints.

### Open Question 2
- Question: Do human preferences for controllability persist in high-stakes or safety-critical environments?
- Basis in paper: [explicit] The authors explicitly limit their scope in the Introduction, stating "particularly in low-stakes settings... the incorporation of humans... may then be to satisfy some subjective preferences."
- Why unresolved: The study utilizes a low-stakes game (Overcooked); it is unknown if users would prioritize subjective feelings of control over guaranteed objective optimality in scenarios where failure carries significant consequences.
- What evidence would resolve it: A user study in a domain with real-world costs (e.g., financial loss or safety risks) showing user preference for controllability remains statistically significant despite performance trade-offs.

### Open Question 3
- Question: To what extent will humans accept objective performance degradations to maximize predictability?
- Basis in paper: [inferred] While RQ2 asks how humans balance preferences, the results show participants frequently chose weight combinations that reduced behavioral entropy (predictability) over those with higher expected scores.
- Why unresolved: The paper identifies the preference for predictability but does not quantify the precise "exchange rate" users apply when trading objective score for subjective predictability.
- What evidence would resolve it: Experiments that systematically vary the score penalty for predictable behavior to identify the threshold at which users abandon predictability for performance.

## Limitations
- Behavioral reward functions are task-specific to Overcooked and may not transfer to other domains without careful redesign
- Control condition shows lower adherence when humans actively specify behavior, suggesting potential mismatches between human expectations and learned behaviors
- Study population (MTurk workers) may not represent all user demographics, limiting external validity

## Confidence

- High confidence: BS achieves higher objective performance than self-play baseline (p < 0.001) and is subjectively preferred by humans (p < 0.001)
- Medium confidence: Preference for controllability is moderated by adherence to settings (p < 0.001), though effect size and practical significance require further validation
- Medium confidence: Humans consistently choose weight combinations that reduce behavioral entropy, though interpretation as preference for predictability over optimal outcomes requires additional behavioral studies

## Next Checks

1. Test BS across multiple domains beyond Overcooked (e.g., StarCraft II, Dota 2) to validate cross-domain applicability of behavioral weight conditioning
2. Conduct ablation studies removing individual behavioral reward functions to identify which dimensions most strongly drive subjective preference
3. Implement real-time weight adjustment during human-AI interaction to measure how dynamic controllability affects preference compared to pre-specified settings