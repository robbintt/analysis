---
ver: rpa2
title: Intrinsic training dynamics of deep neural networks
arxiv_id: '2508.07370'
source_url: https://arxiv.org/abs/2508.07370
tags:
- intrinsic
- property
- networks
- theorem
- diag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper studies when gradient flows on high-dimensional parameters\
  \ \u03B8 can be reduced to intrinsic gradient flows on lower-dimensional variables\
  \ z = \u03D5(\u03B8). It introduces the notion of intrinsic dynamic property, which\
  \ requires that the metric M(\u03B8) = \u2202\u03D5(\u03B8)\u2202\u03D5(\u03B8)\u22A4\
  \ can be expressed as K(z) along the trajectory."
---

# Intrinsic training dynamics of deep neural networks

## Quick Facts
- **arXiv ID:** 2508.07370
- **Source URL:** https://arxiv.org/abs/2508.07370
- **Reference count:** 40
- **Primary result:** Gradient flows on ReLU networks of any depth reduce to intrinsic flows for all initializations, while linear networks require relaxed balanced initializations.

## Executive Summary
This paper studies when high-dimensional gradient flows on neural network parameters can be reduced to lower-dimensional intrinsic dynamics. The authors introduce the concept of intrinsic dynamic property, requiring that the metric M(θ) = ∂ϕ(θ)∂ϕ(θ)ᵀ can be expressed as K(z) along the trajectory. For ReLU networks of arbitrary depth, this reduction works for all initializations due to a Frobenius property ensuring unique recoverability. For linear networks, the property only holds under relaxed balanced initializations (Uᵀᵢ₊₁Uᵢ₊₁ - UᵢUᵀᵢ = λᵢI), which constrain the conservation laws. The work provides a unified framework for understanding dimensionality reduction in neural network training dynamics through Riemannian geometry.

## Method Summary
The paper analyzes continuous gradient flows θ̇(t) = -∇ℓ(θ(t)) where ℓ(θ) = f(ϕ(θ)). It characterizes when these flows can be reduced to intrinsic dynamics ż(t) = -K(z(t))∇f(z(t)) on lower-dimensional variables z = ϕ(θ). For ReLU networks, the path-lifting parametrization satisfies a Frobenius property ensuring intrinsic recoverability for any initialization. For linear networks, the paper proves relaxed balanced initializations are necessary and sufficient for the intrinsic metric property. The analysis uses conservation laws h(θ) (functions constant along trajectories) and checks whether θ can be uniquely reconstructed from z and h(θ₀). For infinitely deep linear networks (neural ODEs), the paper derives closed-form expressions for the intrinsic dynamics under relaxed balanced conditions.

## Key Results
- ReLU networks of any depth exhibit the intrinsic metric property for all initializations due to the Frobenius property
- Linear networks only exhibit the intrinsic metric property under relaxed balanced initializations
- Relaxed balanced conditions are both necessary and sufficient for the intrinsic metric property in linear networks
- Conservation laws h(θ) = Uᵀᵢ₊₁Uᵢ₊₁ - UᵢUᵀᵢ remain constant during training for both architectures
- Closed-form intrinsic dynamics are derived for infinitely deep linear networks (neural ODEs) under relaxed balanced conditions

## Why This Works (Mechanism)

### Mechanism 1: Conservation Laws Constrain Trajectories to Lower-Dimensional Manifolds
- **Claim:** Gradient flow trajectories are confined to level sets defined by conservation laws, enabling reduction from high-dimensional θ to lower-dimensional z.
- **Mechanism:** Functions h(θ) remain constant during training, confining trajectories to manifolds. If loss factorizes as ℓ = f ∘ ϕ, dynamics can be described by evolution of z(t) and constants h(θ₀).
- **Core assumption:** Loss factorizes as ℓ(θ) = f(ϕ(θ)) and conservation laws are exhaustive for the architecture.
- **Evidence anchors:** Abstract mentions conservation laws; Proposition 2.9 lists conservation laws for different architectures; related corpus discusses Riemannian geometry of gradient flows.
- **Break condition:** If conservation laws are incomplete or initialization lacks sufficient laws, trajectory cannot be uniquely recovered from z and h.

### Mechanism 2: Intrinsic Recoverability via the Frobenius Property (ReLU Networks)
- **Claim:** For ReLU networks, parameters θ can be uniquely reconstructed from z and conserved quantities h(θ₀) for any initialization.
- **Mechanism:** Parametrization ϕReLU satisfies Frobenius property (Lie algebra closure), ensuring ker ∂ϕ ∩ ker ∂h = {0}. This guarantees θ = Γ(ϕ(θ), h(θ)), establishing intrinsic recoverability.
- **Core assumption:** Network is general ReLU DAG with non-zero weights.
- **Evidence anchors:** Abstract states property holds for ReLU networks at any initialization; Theorem 3.8 proves Frobenius property; related corpus focuses on linear/diagonal networks.
- **Break condition:** If Frobenius property fails (as for linear networks) or weights become zero, recovery mechanism breaks.

### Mechanism 3: Relaxed Balanced Conditions for Linear Networks
- **Claim:** Linear networks exhibit intrinsic metric property only with relaxed balanced initializations.
- **Mechanism:** Relaxed balanced condition (Uᵀᵢ₊₁Uᵢ₊₁ - UᵢUᵀᵢ = λᵢI) ensures metric kernel inclusion criterion holds. Under this condition, dynamic matrices can be expressed as polynomials of end-to-end matrix Z_L.
- **Core assumption:** Layers have full rank and metric must be expressible via Z alone.
- **Evidence anchors:** Abstract states property holds only for relaxed balanced initializations; Theorem 4.4 proves necessity and sufficiency; related corpus discusses optimization in linear settings.
- **Break condition:** If S = UᵀU - VᵀV ≠ λI, metric depends on θ details not captured by Z, preventing closed-form intrinsic dynamic description.

## Foundational Learning

**Concept: Gradient Flow**
- **Why needed here:** Paper models training as continuous ODE θ̇(t) = -∇ℓ(θ(t)) rather than discrete steps. Understanding continuous limit is required to analyze flow on manifolds.
- **Quick check question:** Can you explain why the paper focuses on θ̇ rather than θₜ₊₁?

**Concept: Riemannian Metric & Manifolds**
- **Why needed here:** Reduced dynamics framed as Riemannian gradient flow where metric tensor K⁻¹(z) determines geometry of update step. Metric defines "distance" and "gradient" on curved space.
- **Quick check question:** How does M(θ) = ∂ϕ(θ)∂ϕ(θ)ᵀ relate to standard Euclidean metric in parameter space?

**Concept: Lie Brackets & Frobenius Theorem**
- **Why needed here:** Paper uses Frobenius property (integrability of vector field distributions) to prove intrinsic recoverability for ReLU networks. Requires checking if Lie brackets of gradient fields remain within span of those fields.
- **Quick check question:** What does it mean for a distribution to be "involutive" or satisfy Frobenius property when finding submanifold?

## Architecture Onboarding

**Component map:**
- θ (High-dim): Network weights (e.g., U, V for 2-layer)
- ϕ (Lifting/Parametrization): Mapping from weights to effective parameter (e.g., product UVᵀ or path-lifting)
- z (Low-dim): Intrinsic variable z = ϕ(θ)
- M(θ) (Path Kernel): Metric induced by parametrization, M = ∂ϕ∂ϕᵀ
- h(θ) (Conservation Laws): Functions constant on trajectories (e.g., differences of weight norms)

**Critical path:**
1. Identify parametrization ϕ for your architecture (Linear → Product, ReLU → Path-lifting)
2. Determine conservation laws h (Proposition 2.9)
3. Check if ker ∂ϕ ∩ ker ∂h = {0} (Intrinsic Recoverability)
   - Yes: (ReLU case) Construct Γ to recover θ
   - No: (Linear case) Check for Relaxed Balanced Initialization (UᵀU - VᵀV = λI)
4. If conditions met, formulate dynamic ż = -K_θ₀(z)∇f(z)

**Design tradeoffs:**
- ϕReLU (Path-lifting) vs. ϕLin (Product):
  - ϕReLU: Robust (works for any init) but yields z in manifold with dimension d' ≈ D (less reduction)
  - ϕLin: Aggressive reduction (d' ≪ D) but brittle (requires specific Relaxed Balanced initialization)
- Initialization Strategy: Relaxed balanced init allows closed-form analysis of linear networks but restricts reachable trajectories.

**Failure signatures:**
- Linear Nets: Unbalanced initialization causes trajectory Z(t) to not follow simple Riemannian flow described by polynomial metric K(z)
- ReLU Nets: If weights vanish (become 0), domain assumption (R\{0})ᴰ violated, potentially breaking Frobenius property

**First 3 experiments:**
1. **Verify Conservation Laws:** Train 2-layer linear network on MNIST and plot h(θ(t)) = UᵀU - VᵀV over time. Verify it remains constant (up to numerical error).
2. **Compare Trajectories (Linear):** Train two 2-layer linear networks: one with balanced init (UᵀU = VᵀV) and one random unbalanced init. Plot singular values of Z(t) = UVᵀ over time. Check if balanced trajectory matches theoretical dynamic from Theorem 4.3 (Equation 15).
3. **Neural ODE Validation:** Implement linear Neural ODE (infinite depth). Initialize with relaxed balanced condition. Compare trajectory of Z₁(t) against closed-form integral expression in Theorem 4.8.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the intrinsic metric property hold for two-layer linear networks when hidden layer width r exceeds max(n, m)?
- **Basis in paper:** Page 9 explicitly states: "The case when r > max(n, m) is still an open question."
- **Why unresolved:** Theorem 4.4 establishes necessity for r ≤ max(n, m), but rank arguments and kernel inclusions change when hidden dimension dominates input/output dimensions.
- **What evidence would resolve it:** Proof that kernel inclusion condition (7) holds or fails for r > max(n, m), or counterexample showing metric depends on more than just z.

### Open Question 2
- **Question:** Can closed-form expressions for intrinsic metric K_θ₀ be derived for general deep ReLU networks (depth L > 3) or non-scalar inputs?
- **Basis in paper:** Proposition 3.10 provides first closed-form formula only for 3-layer ReLU MLP with scalar input/output, despite Theorem 3.8 proving property exists for any depth.
- **Why unresolved:** Implicit equations (10) for scaling factors become intractable as number of layers and conservation laws increases.
- **What evidence would resolve it:** Deriving general explicit formula for metric K valid for L ≥ 4, or proving implicit characterization is most compact form possible.

### Open Question 3
- **Question:** Is there reduced-dimensional representation capable of capturing training dynamics of linear networks initialized outside relaxed balanced conditions?
- **Basis in paper:** Theorem 4.4 shows intrinsic metric property fails for generic linear network initializations, suggesting standard product parameterization ϕLin is insufficient.
- **Why unresolved:** Paper focuses on characterizing when standard parameterization works; doesn't explore alternative parameterizations for non-balanced cases.
- **What evidence would resolve it:** Identification of different lifting function ψ(θ) or higher-dimensional manifold on which gradient flow is intrinsic for generic linear network initializations.

### Open Question 4
- **Question:** Does intrinsic recoverability property extend to neural networks with smooth activations (e.g., sigmoid, tanh)?
- **Basis in paper:** Theorem 3.8 relies on path-lifting ϕReLU being monomial map to prove Frobenius property, structure not shared by smooth activation functions.
- **Why unresolved:** Lie bracket closure mechanism used for ReLU depends on specific algebraic structure of piecewise monomials, absent in smooth architectures.
- **What evidence would resolve it:** Verification of Lie bracket closure (Frobenius property) for lifting associated with smooth networks, or proof that conservation laws are insufficient for intrinsic recoverability in these cases.

## Limitations
- Theoretical scope limited to loss functions factorizing as f ∘ ϕ, excluding data-dependent cases like f(Wx)
- Initialization sensitivity: relaxed balanced conditions represent measure-zero subset of all possible initializations
- Domain restrictions: ReLU analysis requires non-zero weights, which may not hold during training
- Closed-form solutions for infinitely deep networks involve complex integrals difficult to verify numerically

## Confidence
- **High confidence:** Conservation laws for linear and ReLU networks are well-established; characterization of relaxed balanced conditions as necessary and sufficient is mathematically rigorous
- **Medium confidence:** Extension of conservation laws to general ReLU networks via path-lifting is novel but relies on abstract algebraic machinery that may have subtle implementation issues
- **Low confidence:** Closed-form solution for infinitely deep linear networks (Theorem 4.8) involves complex integrals difficult to verify without explicit computation

## Next Checks
1. **Numerical verification of conservation laws:** Implement 2-layer linear network with both balanced and unbalanced initializations. Track h(θ(t)) = UᵀU - VᵀV during training to verify persistence of conservation laws.

2. **Relaxed balanced condition necessity test:** For 2-layer linear network with r ≤ max(n,m), train with non-relaxed balanced initialization and verify that M(θ) cannot be expressed solely via z = UVᵀ and h(θ₀).

3. **Neural ODE closed-form validation:** Implement linear neural ODE with relaxed balanced initialization. Compare numerical trajectory of Z₁(t) against theoretical closed-form integral expression from Theorem 4.8.