---
ver: rpa2
title: 'EZYer: A simulacrum of high school with generative agent'
arxiv_id: '2512.02561'
source_url: https://arxiv.org/abs/2512.02561
tags:
- content
- ezyer
- student
- teacher
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EZYer is a generative agent system designed to automate the creation
  of LaTeX Beamer courseware and interactive academic notes for high school mathematics.
  It integrates a Teacher Module that generates structured materials and exercises,
  a Student Module that simulates classroom interaction through multiple AI roles,
  and a Controller that ensures content validity through filtering and scoring.
---

# EZYer: A simulacrum of high school with generative agent

## Quick Facts
- **arXiv ID**: 2512.02561
- **Source URL**: https://arxiv.org/abs/2512.02561
- **Reference count**: 40
- **Primary result**: Generative agent system that automates creation of LaTeX Beamer courseware and interactive academic notes for high school mathematics

## Executive Summary
EZYer is a generative agent system designed to automate the creation of LaTeX Beamer courseware and interactive academic notes for high school mathematics. It integrates a Teacher Module that generates structured materials and exercises, a Student Module that simulates classroom interaction through multiple AI roles, and a Controller that ensures content validity through filtering and scoring. The system was evaluated using five large language models across 100 generated Beamer and Notes documents, with results showing strong performance in content accuracy (average 4.74/5) and knowledge coverage (average 4.70/5), demonstrating its potential for practical educational applications.

## Method Summary
EZYer employs a multi-agent architecture with three main components working in concert. The Teacher Module generates structured educational content including Beamer presentations and interactive notes based on user-specified topics and difficulty levels. The Student Module simulates classroom interaction through multiple AI roles including students, presenters, and instructors to create dynamic learning scenarios. A Controller component orchestrates the system, validating content through filtering and scoring mechanisms before final output. The system was evaluated using five different large language models to generate 100 documents each for both Beamer and Notes formats, with automated scoring measuring content accuracy and knowledge coverage.

## Key Results
- Generated LaTeX Beamer courseware with average accuracy score of 4.74/5 across 100 documents
- Created interactive academic notes achieving average knowledge coverage score of 4.70/5
- Demonstrated effective integration of multi-agent simulation for classroom interaction scenarios

## Why This Works (Mechanism)
The system leverages multi-agent simulation where different AI roles (teacher, students, presenters) interact to generate pedagogically sound content. The Teacher Module provides structured content generation while the Student Module creates realistic classroom dynamics through role-playing scenarios. The Controller ensures quality through systematic filtering and scoring of generated materials, maintaining educational standards throughout the content creation process.

## Foundational Learning
- **LaTeX Beamer framework**: Essential for generating professional academic presentations; needed for standardized courseware output
  - Quick check: Can system generate valid .tex files that compile without errors
- **Multi-agent systems**: Enables realistic classroom simulation through coordinated AI roles; needed for interactive learning scenarios
  - Quick check: Can different agent roles maintain coherent conversations and interactions
- **Automated content validation**: Critical for maintaining educational quality; needed to ensure accuracy and coverage standards
  - Quick check: Do generated materials pass validation thresholds consistently
- **Large language model orchestration**: Required for coordinating multiple specialized tasks; needed to manage different content generation stages
  - Quick check: Can system switch between different LLM capabilities effectively
- **Educational content structure**: Necessary for pedagogically sound materials; needed to ensure learning objectives are met
  - Quick check: Does generated content follow logical progression and include appropriate exercises

## Architecture Onboarding

**Component Map:**
Controller -> Teacher Module -> Student Module -> Output Generator

**Critical Path:**
User request → Controller routing → Teacher Module content generation → Student Module interaction simulation → Content validation → Final output generation

**Design Tradeoffs:**
- Automated LLM evaluation vs. human expert assessment (speed vs. accuracy)
- Pre-defined agent roles vs. flexible role adaptation (consistency vs. adaptability)
- Standardized LaTeX output vs. multiple format support (professional quality vs. accessibility)

**Failure Signatures:**
- Inconsistent content quality across different mathematical topics
- Breakdown in agent role interactions during complex scenarios
- Validation failures for edge cases in mathematical concepts
- Compilation errors in generated LaTeX code

**First Experiments:**
1. Generate simple algebra materials and verify LaTeX compilation
2. Test basic agent interactions with simple math problems
3. Evaluate content accuracy for fundamental mathematical concepts

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Evaluation relies entirely on automated LLM scoring without human expert validation
- Limited sample size of 100 documents may not capture full range of edge cases
- System restricted to high school mathematics with no evidence of cross-subject performance
- No assessment of long-term reliability or real-world classroom implementation challenges

## Confidence

**High confidence**: System architecture and technical implementation details are well-described and reproducible

**Medium confidence**: Reported performance metrics are internally consistent but unverified by independent human assessment

**Medium confidence**: Modular design approach appears sound, though practical effectiveness requires further validation

## Next Checks
1. Conduct blind human expert evaluation of a subset of generated materials to validate LLM-based scoring accuracy and identify potential systematic biases
2. Test system performance across diverse mathematical topics and difficulty levels to assess robustness and identify failure modes
3. Implement real-world classroom pilot testing with actual high school students to evaluate practical utility, engagement, and learning outcomes beyond content generation metrics