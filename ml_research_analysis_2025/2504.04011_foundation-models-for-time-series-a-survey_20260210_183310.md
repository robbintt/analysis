---
ver: rpa2
title: 'Foundation Models for Time Series: A Survey'
arxiv_id: '2504.04011'
source_url: https://arxiv.org/abs/2504.04011
tags:
- time
- series
- data
- forecasting
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive taxonomy of transformer-based
  foundation models for time series analysis, addressing the growing complexity in
  this rapidly evolving field. The authors systematically categorize models across
  multiple dimensions including architecture (encoder-only, decoder-only, encoder-decoder,
  and non-transformer), patching strategies (patch-based vs non-patch-based), objective
  functions (MSE, NLL, Huber), and task scope (univariate vs multivariate, probabilistic
  vs deterministic).
---

# Foundation Models for Time Series: A Survey

## Quick Facts
- arXiv ID: 2504.04011
- Source URL: https://arxiv.org/abs/2504.04011
- Authors: Siva Rama Krishna Kottapalli; Karthik Hubli; Sandeep Chandrashekhara; Garima Jain; Sunayana Hubli; Gayathri Botla; Ramesh Doddaiah
- Reference count: 40
- Primary result: Comprehensive taxonomy of transformer-based foundation models for time series analysis

## Executive Summary
This survey provides a systematic classification framework for foundation models in time series analysis, addressing the growing complexity in this rapidly evolving field. The authors categorize models across multiple dimensions including architecture (encoder-only, decoder-only, encoder-decoder, non-transformer), patching strategies (patch-based vs non-patch-based), objective functions (MSE, NLL, Huber), and task scope (univariate vs multivariate, probabilistic vs deterministic). The survey analyzes 17 prominent models, detailing their specific characteristics and performance trade-offs while identifying critical challenges in scalability, interpretability, and generalization across diverse datasets.

## Method Summary
The survey systematically reviews foundation models for time series by examining their architectural design, training objectives, and patching strategies. The authors classify models based on their core components including tokenization approaches, positional encoding schemes, attention mechanisms, and task-specific heads. The analysis focuses on both lightweight architectures and large-scale foundation models, comparing their computational efficiency against predictive accuracy. The survey methodology involves comprehensive literature review and technical analysis of published model specifications, though implementation details and source code access were not available for verification.

## Key Results
- Systematic taxonomy categorizing 17 transformer-based foundation models across multiple dimensions
- Clear distinction between patch-based and non-patch-based approaches for computational efficiency
- Analysis of trade-offs between probabilistic and deterministic forecasting capabilities
- Identification of scalability challenges and generalization limitations across diverse time series datasets

## Why This Works (Mechanism)

### Mechanism 1: Self-Attention Enables Long-Range Dependency Modeling
- Claim: Self-attention captures dependencies across distant time steps better than recurrent processing
- Mechanism: Self-attention computes pairwise relationships between all time steps simultaneously via Query-Key-Value matrices
- Core assumption: Temporal dependencies span variable distances and are not confined to local windows
- Evidence anchors: [abstract] patch-based representations; [section 2.2.1] self-attention computes pairwise relationships
- Break condition: If time series exhibits only short-term dependencies, recurrent models may achieve comparable performance with lower computational cost

### Mechanism 2: Patching Reduces Sequence Complexity While Preserving Local Semantics
- Claim: Segmenting time series into fixed-length patches improves computational efficiency while retaining local pattern information
- Mechanism: Patching aggregates adjacent time steps into tokens, reducing sequence length and self-attention's quadratic complexity
- Core assumption: Local temporal patterns within patches carry meaningful structure
- Evidence anchors: [abstract] patch-based representations; [section 4.2.1] fixed-length "windows" treated as individual tokens
- Break condition: If optimal pattern length varies significantly across datasets, fixed patch sizes may miss critical boundaries

### Mechanism 3: Pretraining on Heterogeneous Data Enables Cross-Domain Transfer
- Claim: Pretraining on diverse time series domains enables generalization to unseen datasets with minimal fine-tuning
- Mechanism: Pretraining exposes models to varied temporal patterns across domains, learning generalizable representations
- Core assumption: Different domains share underlying structural properties capturable in unified representations
- Evidence anchors: [abstract] lightweight architectures vs large-scale foundation models; [section 2.4.1] vast datasets capture generalized patterns
- Break condition: If target domain exhibits fundamentally different temporal dynamics not represented in pretraining data

## Foundational Learning

- Concept: Self-Attention Mechanics (Q/K/V computation, scaled dot-product)
  - Why needed here: Essential for debugging attention patterns and interpreting model focus areas
  - Quick check question: Given a sequence of 100 time steps, how does self-attention differ from RNN processing in terms of which past steps influence the current output?

- Concept: Pretraining-Fine-Tuning Paradigm
  - Why needed here: Foundation models operate in two stages; understanding transfer from general patterns to domain specifics informs training strategies
  - Quick check question: If you have only 1,000 labeled samples for a new forecasting task, should you train from scratch or fine-tune a pretrained model?

- Concept: Probabilistic vs. Deterministic Forecasting
  - Why needed here: Choosing between point forecasts and distribution forecasts depends on whether uncertainty quantification is required
  - Quick check question: For inventory management where stockouts are costly but overstocking is cheap, would you prefer probabilistic or deterministic forecasts?

## Architecture Onboarding

- Component map: Input Time Series → [Patching (optional)] → Token Embedding → Positional Encoding → Transformer Layers (Encoder/Decoder/Both) → Task Head (Forecasting/Classification/Anomaly) → Loss Function (MSE/NLL/Huber) → Output

- Critical path:
  1. Determine task type (forecasting horizon, classification, anomaly detection)
  2. Assess data characteristics (univariate vs. multivariate, sequence length, sampling regularity)
  3. Select architecture family (encoder-only for representation learning, decoder-only for autoregressive generation, encoder-decoder for seq2seq)
  4. Choose patching strategy based on sequence length and computational budget
  5. Select objective function aligned with output requirements (probabilistic vs. deterministic)

- Design tradeoffs:
  | Dimension | Choice A | Choice B | Trade-off |
  |-----------|----------|----------|-----------|
  | Architecture | Decoder-only | Encoder-only | Decoder enables autoregressive generation; encoder better for representation tasks |
  | Patching | Patch-based | Non-patch | Patching reduces O(n²) complexity but may lose fine-grained patterns |
  | Output | Probabilistic | Deterministic | Probabilistic provides uncertainty but requires distributional assumptions |
  | Scale | Large (1B+ params) | Lightweight (<50M) | Large models generalize better; lightweight deployable on edge |

- Failure signatures:
  - Model fails to capture seasonality: Likely insufficient context length or positional encoding issue
  - Good training loss, poor validation: Overfitting; reduce model capacity or increase data augmentation
  - Erratic long-horizon forecasts: Error accumulation in autoregressive decoding; consider teacher forcing or shorter horizons
  - Poor transfer to new domain: Pretraining data mismatch; collect domain-specific fine-tuning data

- First 3 experiments:
  1. **Baseline comparison**: Run a patch-based model (e.g., PatchTST-style) vs. non-patch model on ETTh1 with identical hyperparameters to isolate patching impact
  2. **Context length ablation**: Train with varying lookback windows (96, 336, 512 steps) to identify minimum context needed for target horizon
  3. **Fine-tuning data budget**: Fine-tune a pretrained model (e.g., MOMENT or TimesFM) with varying percentages of domain data (10%, 30%, 100%) to quantify transfer efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications can enable point-forecasting models to provide probabilistic outputs with uncertainty quantification without significant computational overhead?
- Basis in paper: Multiple models only generate point forecasts without distribution heads; TimesFM lacks explicit probabilistic support
- Why unresolved: Adding distribution heads changes training objectives and inference complexity; trade-off between probabilistic capability and efficiency unexplored
- What evidence would resolve it: Comparative benchmarks of modified architectures measuring uncertainty calibration metrics and inference latency

### Open Question 2
- Question: How can foundation models handle variable context lengths and multi-resolution inputs without requiring separate model instances?
- Basis in paper: TTM requires different models for different context lengths; Timer-XL doesn't incorporate multi-resolution patches
- Why unresolved: Current tokenization assumes fixed configurations; flexible mechanisms require architectural innovations
- What evidence would resolve it: Single model demonstrating consistent performance across varying context lengths (96, 192, 336, 720 time steps)

### Open Question 3
- Question: What mechanisms can effectively integrate multi-modal inputs into time series foundation models while preserving temporal modeling fidelity?
- Basis in paper: Future integration of tabular or textual data would unlock potential; TimeGPT relies solely on historical load data
- Why unresolved: Cross-modal fusion architectures for time series remain underexplored; aligning embeddings across modalities is non-trivial
- What evidence would resolve it: Benchmark results on datasets with paired multi-modal inputs showing improved forecasting accuracy

### Open Question 4
- Question: Can iterative error accumulation in autoregressive decoder-only architectures be mitigated for long-horizon forecasting?
- Basis in paper: Timer-XL's iterative generation may lead to error accumulation and inflexibility in output length
- Why unresolved: Autoregressive generation inherently compounds prediction errors; direct multi-step alternatives sacrifice generative flexibility
- What evidence would resolve it: Comparative error analysis across forecast horizons measuring error propagation rates

## Limitations

- Taxonomy framework may face temporal limitations as foundation models evolve rapidly
- Classification assumes clear boundaries that emerging hybrid models may blur
- Analysis relies on published specifications without source code access
- Patch-based vs. non-patch-based distinction may oversimplify sequence processing strategies

## Confidence

- **High confidence**: Taxonomy framework for categorizing foundation models (architecture types, patching strategies, objective functions)
- **Medium confidence**: Claims about model performance trade-offs between computational efficiency and accuracy
- **Low confidence**: Predictions about future research directions and scalability challenges

## Next Checks

1. **Cross-validation of taxonomy**: Apply the proposed classification framework to emerging models from NeurIPS 2024 or ICLR 2025 to test taxonomy extensibility and identify classification edge cases

2. **Benchmarking reproducibility study**: Select 3-5 representative models from different architectural categories and reproduce their results on standard benchmarks (ETTh1, Weather) using consistent evaluation protocols

3. **Generalization validation**: Conduct controlled experiments comparing transfer learning performance of pretrained foundation models versus training from scratch across domains with varying degrees of similarity to pretraining data