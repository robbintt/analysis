---
ver: rpa2
title: 'Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech
  Synthesis'
arxiv_id: '2601.13802'
source_url: https://arxiv.org/abs/2601.13802
tags:
- arabic
- speech
- data
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Habibi, the first open-source framework for
  unified-dialectal Arabic speech synthesis. It addresses the challenge of supporting
  diverse Arabic dialects with limited data and without requiring diacritization.
---

# Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis

## Quick Facts
- arXiv ID: 2601.13802
- Source URL: https://arxiv.org/abs/2601.13802
- Reference count: 38
- Primary result: First open-source unified-dialectal Arabic TTS framework supporting 20+ variants, outperforming ElevenLabs v3 on major dialects

## Executive Summary
Habibi introduces the first open-source framework for unified-dialectal Arabic speech synthesis, addressing the challenge of supporting diverse Arabic dialects with limited data without requiring diacritization. The approach uses linguistically-informed curriculum learning, starting from Modern Standard Arabic (MSA) and progressively adapting to dialectal data, leveraging existing ASR corpora. The framework supports over 20 Arabic variants and 12 regional identifiers, achieving performance close to specialized dialect models and outperforming the leading commercial TTS service, ElevenLabs' Eleven v3, on major dialect test sets.

## Method Summary
Habibi uses a two-stage curriculum learning approach: Stage 1 SFT on MSA-only data (~920 hours) until UTMOS converges, then Stage 2 SFT on dialectal data (1,857 hours total) with regional identifier tokens. The model builds on F5-TTS, a non-autoregressive flow-matching architecture, initialized from 95K-hour Chinese-English pre-training. Data is filtered by CPS thresholds and denoised via band-split RNN with 0.618 mixing ratio. Training runs for 200K updates on 8× H100 GPUs. The unified model uses `i⟨text⟩` format for regional identifiers, which can be omitted at inference while maintaining dialect awareness.

## Key Results
- WER-O ranging from 7.88% (MSA) to 50.30% (MAR), WER-S from 7.74% (MSA) to 44.05% (MAR)
- Unified model performance close to specialized dialect models with single-model convenience
- Outperforms ElevenLabs Eleven v3 on major dialect test sets
- Strong in-context learning capabilities demonstrated through reference audio retrieval
- Supports 20+ Arabic variants and 12 regional identifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage curriculum learning (MSA → dialectal) improves convergence and final quality over direct fine-tuning or training from scratch.
- Mechanism: Stage 1 transfers text-to-speech modality mapping from Chinese/English pre-training to Arabic's basic phonological structures using MSA as standardized intermediate representation. Stage 2 introduces dialectal variation once foundational mappings stabilize.
- Core assumption: MSA provides linguistically stable intermediate representation that dialects share at phonological level.
- Evidence anchors: [section 2.4] MSA favors model to grasp fundamental grammatical norms; [section 3.5, Table 5] Curriculum approach achieves WER-S 10.20 vs. 10.83 direct fine-tuning and 47.54 scratch training on Saudi Arabic.

### Mechanism 2
- Claim: In-context learning from reference audio-transcription pairs enables dialectal feature capture at inference without explicit diacritization.
- Mechanism: Model retrieves text-to-acoustic matching information from provided reference pair, implicitly encoding dialect-specific pronunciation patterns.
- Core assumption: Reference audio contains sufficient dialectal acoustic markers that generalize to target utterance.
- Evidence anchors: [section 2.5] Models can implicitly retrieve and leverage text-to-acoustic matching information; [section 3.6, Table 6] Removing reference context degrades WER-O on unified model from 7.71→13.77 (MSA), 40.02→42.42 (MAR).

### Mechanism 3
- Claim: Regional identifier tokens during training improve dialect-aware generation even when omitted at inference.
- Mechanism: Explicit dialect labels force model to internalize dialect-specific distributional patterns during training, creating separable representations that persist without explicit prompting.
- Core assumption: Dialect labels align with learnable acoustic-linguistic clusters in model's representation space.
- Evidence anchors: [section 2.5] Training with identifiers likely helps model better understand dialect-related patterns; [section 3.8, Table 8] ID-aware inference consistently improves WER-O over plain text.

## Foundational Learning

- Concept: **Flow-matching TTS architecture (F5-TTS backbone)**
  - Why needed here: Paper builds on F5-TTS, non-autoregressive model operating directly on mel spectrograms and raw text. Understanding flow matching advantages over AR/hybrid systems is prerequisite to interpreting training choices.
  - Quick check question: Can you explain why flow-matching model avoids cascading errors compared to AR systems with separate audio/text encoders?

- Concept: **Arabic diglossia and dialect variation**
  - Why needed here: Entire problem framing depends on MSA (formal) vs. spoken dialects having different lexical, phonological, and grammatical conventions. Without this context, curriculum design rationale is opaque.
  - Quick check question: Why might Moroccan Darija (MAR) be harder to model than Egyptian (EGY) given MSA pre-training?

- Concept: **Zero-shot TTS evaluation protocols**
  - Why needed here: Benchmark requires paired reference/target from same speaker. Understanding speaker similarity (SIM), WER with multilingual vs. dialect-specific ASR, and UTMOS naturalness scoring is essential for interpreting results.
  - Quick check question: Why does paper report both WER-O (multilingual ASR) and WER-S (dialect-specific ASR)?

## Architecture Onboarding

- Component map: F5-TTS (flow-matching, mel-spectrogram input) -> Raw character sequences with optional regional identifier tokens (format: `i⟨c₁ c₂ ... cₙ⟩`) -> Mel-spectrogram processing with band-split RNN denoising -> Output audio

- Critical path:
  1. Initialize from ZH-EN F5-TTS weights
  2. Stage 1 SFT on MSA-only data until UTMOS converges
  3. Stage 2 SFT on dialectal data (unified or specialized)
  4. Inference: provide reference audio + transcription + (optional) regional identifier

- Design tradeoffs:
  - **Unified vs. specialized models**: Unified models trade ~5-15% WER degradation for single-model convenience (Table 3); specialized models excel on in-domain data but degrade on out-of-distribution
  - **Denoising ratio (0.618)**: Mixed noisy/denoised sampling balances robustness to real-world audio vs. clean reference quality (Table 7)
  - **Training duration**: 200K updates chosen for convergence across dialects, but some dialects show UTMOS degradation past 100K (Table 4)

- Failure signatures:
  - **High WER-S with low WER-O**: Model generates fluent but dialect-incorrect speech (multilingual ASR "corrects" dialect features)
  - **Low SIM with high UTMOS**: Model prioritizes naturalness over speaker fidelity (observed in ElevenLabs comparison)
  - **MAR persistent failure**: WER-O 40-50% even with curriculum (Table 4), suggesting fundamental dialect-MSA mismatch

- First 3 experiments:
  1. **Baseline ablation**: Train unified model without MSA stage (direct ZH-EN → dialectal) and compare WER-O/WER-S on SAU test set to Table 5 results
  2. **Identifier injection test**: Train unified model with identifiers, then evaluate with mismatched identifiers (e.g., MAR text with EGY identifier) to probe representation separability
  3. **Data quality scaling**: Train MAR-specialized model with varying proportions of DarijaTTS-clean vs. Darija-S2T per Table 9 protocol to isolate length vs. quality effects

## Open Questions the Paper Calls Out

- **Open Question 1**: How can unified-dialectal framework be adapted to support code-switching scenarios involving Arabic mixed with English, French, or Spanish?
  - Basis: Authors explicitly state current model does not support code-switching scenarios common in real-world multilingual environments
  - Why unresolved: Model training and vocabulary focus exclusively on Arabic script and phonology, lacking mechanisms for multilingual linguistic shifts
  - What evidence would resolve it: Successful synthesis and evaluation on benchmark of mixed-language utterances with seamless transitions

- **Open Question 2**: What is minimum data scale or specific training strategy required to acquire Arabic dialectal capabilities without degrading original Chinese and English performance of base model?
  - Basis: Limitations note work does not identify minimum data scale or strategies to preserve original Chinese/English performance after Arabic dialectal training
  - Why unresolved: Trade-off regarding retention of pre-trained base model's original linguistic competencies (catastrophic forgetting) has not been quantified
  - What evidence would resolve it: Ablation studies reporting WER and speaker similarity on Chinese/English test sets throughout Arabic fine-tuning

- **Open Question 3**: Does unified model possess capacity for zero-shot transfer to dialects not explicitly included in fine-tuning data?
  - Basis: Authors identify model's capacity for zero-shot transfer to excluded dialects remains unexplored
  - Why unresolved: Undetermined if learned "unified" representation generalizes sufficiently to capture nuances of entirely excluded dialectal varieties
  - What evidence would resolve it: Evaluation on test sets from excluded Arabic varieties showing performance comparable to in-distribution dialects

- **Open Question 4**: Can model architecture specifically tailored for unified dialectal modeling outperform current general-purpose F5-TTS backbone?
  - Basis: Paper states study keeps with existing general-purpose TTS design without exploring tailored model structures for unified dialectal modeling
  - Why unresolved: Current success relies on generic flow-matching architecture; unknown if dialect-specific inductive biases could improve handling of dialect fusion
  - What evidence would resolve it: Comparative study measuring delta in WER-S and naturalness between current backbone and modified architecture trained on same data

## Limitations

- Performance gap between MSA (7.88%) and Moroccan Darija (50.30%) reveals fundamental modeling challenges that curriculum approach only partially mitigates
- Heavy dependency on data quality and duration distribution, with short clean utterances improving MAR-specific performance but unified model performing better on combined data
- Single-trial commercial benchmark comparison limits assessment of systematic performance differences versus random variation

## Confidence

- **High Confidence**: Curriculum learning framework demonstrably improves convergence and final quality over direct fine-tuning or training from scratch (systematic ablation experiments in Table 5)
- **Medium Confidence**: In-context learning through reference audio effectively captures dialectal features and improves WER compared to zero-shot inference without references (consistent WER degradation when references removed)
- **Low Confidence**: Regional identifier tokens consistently improve WER-O across dialects (consistent improvements but varying effect sizes, no demonstration of truly separable representations)

## Next Checks

1. **Cross-dialect identifier transfer**: Train unified model with identifiers, then evaluate with mismatched identifiers (e.g., MAR text with EGY identifier) to test whether identifier tokens create truly separable dialect representations versus soft conditioning

2. **Curriculum stage duration optimization**: Systematically vary duration of Stage 1 MSA training to identify optimal stopping points where UTMOS plateaus but before naturalness degrades from ASR-domain overfitting

3. **MAR-specific data quality scaling**: Following Table 9 methodology, train MAR-specialized models with controlled proportions of DarijaTTS-clean versus Darija-S2T data to isolate whether performance improvements stem from utterance length or audio quality