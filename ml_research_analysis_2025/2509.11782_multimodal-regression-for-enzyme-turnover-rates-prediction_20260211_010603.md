---
ver: rpa2
title: Multimodal Regression for Enzyme Turnover Rates Prediction
arxiv_id: '2509.11782'
source_url: https://arxiv.org/abs/2509.11782
tags:
- enzyme
- protein
- prediction
- substrate
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting enzyme turnover
  rates, which are critical for understanding enzyme kinetics but remain experimentally
  scarce. The authors propose a multimodal framework called ProKcat that integrates
  enzyme sequences, substrate structures, and environmental factors (temperature)
  to predict kcat values.
---

# Multimodal Regression for Enzyme Turnover Rates Prediction

## Quick Facts
- **arXiv ID:** 2509.11782
- **Source URL:** https://arxiv.org/abs/2509.11782
- **Reference count:** 18
- **Primary result:** ProKcat-M achieves RMSE 0.71, PCC 0.88, MAE 0.48, R² 0.74 on enzyme turnover rate prediction

## Executive Summary
This paper addresses the challenge of predicting enzyme turnover rates (kcat), which are critical for understanding enzyme kinetics but experimentally scarce. The authors propose ProKcat, a multimodal framework that integrates enzyme sequences, substrate structures, and environmental factors to predict kcat values. The model combines pre-trained protein language models (ESM-2) and CNNs for enzyme sequences, GNNs for substrate structures, and attention mechanisms to enhance interactions between enzyme and substrate representations. Additionally, they employ Kolmogorov-Arnold Networks for symbolic regression to learn interpretable mathematical formulas governing kcat. ProKcat-M outperforms state-of-the-art models with an RMSE of 0.71, while ProKcat-K provides interpretable symbolic formulas with comparable performance.

## Method Summary
ProKcat integrates multiple data modalities to predict enzyme turnover rates. The architecture uses ESM-2 (650M) embeddings and CNNs to process enzyme sequences, GAT networks for substrate structures from SMILES strings, and ECFP fingerprints. A soft alignment attention mechanism explicitly models interactions between enzyme and substrate representations, mimicking the lock-and-key theory of enzyme specificity. The model incorporates temperature (T) and its reciprocal (1/T) as inputs based on the Arrhenius equation. ProKcat-M uses an MLP regressor for optimal performance (RMSE 0.71, R² 0.74), while ProKcat-K employs KANs for symbolic regression, producing human-readable mathematical formulas that explicitly show temperature dependence.

## Key Results
- ProKcat-M achieves state-of-the-art performance with RMSE 0.71, PCC 0.88, MAE 0.48, and R² 0.74
- The attention module is critical: removing it increases RMSE from 0.71 to 0.89 and drops R² from 0.74 to 0.67
- ProKcat-K produces interpretable symbolic formulas, though with slightly lower performance (RMSE 0.99, R² 0.59) due to overfitting
- The learned symbolic formula explicitly recovers the linear dependence on 1/T, validating the temperature-informed design

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Interaction via Soft Alignment
The performance gains stem from explicitly modeling interactions between enzyme residues and substrate atoms rather than processing them independently. A soft alignment matrix computes interaction strengths, creating compound-to-protein and protein-to-compound features that mimic the lock-and-key theory. Ablation studies confirm this: removing the attention module increases RMSE from 0.71 to 0.89 and drops R² from 0.74 to 0.67.

### Mechanism 2: Physics-Informed Input Engineering
Including 1/T as an explicit input feature allows the model to leverage the Arrhenius equation governing reaction rates. This biases the regression function toward the known linear relationship between ln k and 1/T. The learned symbolic formula explicitly recovers this linear dependence on 1/T, validating the approach.

### Mechanism 3: Symbolic Regression for Interpretable Distillation
Replacing MLPs with Kolmogorov-Arnold Networks at the regression head yields human-readable mathematical formulas while maintaining competitive accuracy. By constraining network depth and projecting features to low dimensions (3D), the network approximates the regression function as a composition of simple symbolic operations. This is the first time such an equation has been derived in DL-based kcat prediction.

## Foundational Learning

- **Concept: Michaelis-Menten Kinetics & Arrhenius Equation**
  - **Why needed here:** Understanding that kcat is a rate constant related to temperature via the Arrhenius equation is necessary to interpret the input features (1/T) and the model's output formula.
  - **Quick check question:** Why would a model include both T and 1/T as inputs instead of just T?

- **Concept: Representation Learning (PLMs & GNNs)**
  - **Why needed here:** The system relies on ESM-2 (Protein Language Model) and GAT (Graph Attention Network) to convert raw sequences and SMILES strings into numerical vectors.
  - **Quick check question:** Why does ESM-2 provide a "pre-trained" advantage over a CNN trained from scratch on this specific dataset?

- **Concept: Attention Mechanisms**
  - **Why needed here:** The core improvement over baselines is the "Enzyme-Substrate Attention Module." You must understand how Query/Key/Value or soft-alignment matrices create weighted sums to grasp how the model "focuses" on relevant binding sites.
  - **Quick check question:** In Equation 4, what does the matrix dimension R^(N_v × L_p) represent physically regarding the inputs?

## Architecture Onboarding

- **Component map:** Enzyme Sequence (ESM-2 + CNN) -> Substrate SMILES (GAT) -> Enzyme-Substrate Attention Module -> Global Average Pooling + Concatenation -> MLP/KAN Regressor
- **Critical path:** The Attention Module is the primary driver of performance. The ablation study shows that removing it causes a performance crash comparable to removing the enzyme input entirely.
- **Design tradeoffs:**
  - ProKcat-M vs. ProKcat-K: The MLP head achieves superior metrics (R²=0.74) but is a black box. The KAN head offers an explicit formula and faster inference (1ms vs 3.6ms) but suffers from overfitting.
  - Latent Dimension (d): The paper identifies d=32 as optimal. Larger dimensions degraded performance, likely due to overfitting on the ~10k sample dataset.
- **Failure signatures:**
  - KAN Overfitting: The paper notes a gap between train (RMSE 0.66) and test (RMSE 0.99) for deeper KANs. Restrict depth to 2 layers and reduce input dimensions to 3 to mitigate this.
  - Attention Noise: Without the specific alignment network (MLP layers transforming h_f), the attention weights may not converge, resulting in performance similar to the "w/o attention" baseline.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run ProKcat-M with and without the attention module. Verify the RMSE gap approximates the reported 0.71 vs 0.89.
  2. **Hyperparameter Scan:** Train ProKcat-M with latent dimensions d ∈ {16, 32, 64, 128}. Confirm that d=32 provides the minimum test RMSE.
  3. **Symbolic Extraction:** Implement the 2-layer KAN with the reduced 3-dimensional input. Compare the inference speed and symbolic output against the 2-layer MLP to verify the efficiency claim.

## Open Questions the Paper Calls Out
- **Question 1:** How can the architecture or regularization of Kolmogorov-Arnold Networks (KANs) be optimized to mitigate overfitting when applied to high-dimensional, sparse datasets like enzyme kinetics?
  - **Basis in paper:** [explicit] The authors state that a "key limitation of the KAN model is its sensitivity to overfitting," observing significant disparities between training and testing metrics in deeper models.
- **Question 2:** Can the "black-box" nature of the learned enzyme and substrate embeddings be resolved to derive fully interpretable symbolic formulas?
  - **Basis in paper:** [explicit] The authors note that the input feature vectors are acquired from a "black-box model, implying the inability to explicitly define a formula expressing the representations and their associations with the input enzyme sequences."
- **Question 3:** How does the performance of ProKcat generalize to out-of-distribution enzyme classes or extreme temperature ranges not well-represented in the training set?
  - **Basis in paper:** [inferred] The paper acknowledges data scarcity (17k parameters vs. millions of sequences) and uses oversampling to fix temperature imbalance, but relies on a random 90/10 split rather than a scaffold or temporal split.

## Limitations
- The symbolic regression capability via KANs remains experimentally validated only on this specific dataset; generalizability to broader enzyme classes or reaction types is unknown
- The cross-modal attention mechanism's effectiveness depends heavily on the quality of the learned embeddings and may not generalize if the underlying PLM or GNN representations change significantly
- Temperature extrapolation beyond the training range could produce physically impossible predictions despite the Arrhenius-informed input engineering

## Confidence
- **High Confidence:** The core performance metrics of ProKcat-M (RMSE 0.71, PCC 0.88, R² 0.74) and the ablation study demonstrating the attention module's contribution are well-supported by the presented evidence
- **Medium Confidence:** The mechanism of temperature-informed feature engineering is theoretically sound and partially validated by the learned symbolic formula, but its effectiveness in extrapolation scenarios remains uncertain
- **Low Confidence:** The KAN-based symbolic regression claims are novel and promising, but the limited evidence (novelty claim, train-test gap concerns) and lack of broader validation make this mechanism the least certain

## Next Checks
1. **Cross-Dataset Generalization:** Test ProKcat-M on a held-out subset of SABIO-RK not used in training to assess true generalization beyond the BRENDA-dominated training set
2. **Temperature Extrapolation Stress Test:** Evaluate model predictions on enzyme-substrate pairs at temperatures 10-20°C outside the training range to quantify physical plausibility breakdown
3. **Attention Mechanism Dissection:** Perform a qualitative analysis of the attention weights on known enzyme-substrate pairs with well-characterized active sites to verify the model focuses on biologically meaningful regions