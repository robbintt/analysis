---
ver: rpa2
title: 'EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery
  Videos'
arxiv_id: '2503.18755'
source_url: https://arxiv.org/abs/2503.18755
tags:
- hand
- segmentation
- tool
- surgical
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EgoSurgery-HTS is a new dataset for egocentric hand-tool segmentation
  in open surgery videos, featuring pixel-wise annotations for 14 surgical tool types,
  four hand types, and hand-tool interactions across 19,496 frames. It extends the
  EgoSurgery dataset with detailed segmentation labels, enabling advanced analysis
  of surgical scenes.
---

# EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos

## Quick Facts
- **arXiv ID**: 2503.18755
- **Source URL**: https://arxiv.org/abs/2503.18755
- **Reference count**: 35
- **Primary result**: New dataset enabling significant improvements in surgical hand-tool segmentation compared to transfer from daily-activity datasets

## Executive Summary
EgoSurgery-HTS is a large-scale dataset for egocentric hand-tool segmentation in open surgery videos, containing pixel-wise annotations for 14 surgical tool types, four hand types, and hand-tool interactions across 19,496 frames. The dataset extends the EgoSurgery corpus with detailed segmentation labels generated using SAM-assisted annotation followed by manual correction. Experiments demonstrate substantial performance improvements over existing datasets, with mAP scores reaching up to 63.8% for hand detection and 55.2% for hand-tool detection using state-of-the-art models.

## Method Summary
The dataset was created by applying SAM to existing EgoSurgery bounding box annotations for tools and hands, generating initial segmentation proposals that were then manually reviewed and corrected. This approach leverages the Segment Anything Model's promptable segmentation capability to reduce annotation costs while maintaining label quality. The resulting annotations cover 50,383 tool instances, 57,173 hand instances, and 41,605 hand-tool segmentations across 19,496 frames from 21 surgical videos. The dataset enables three segmentation tasks: tool instance segmentation, hand instance segmentation, and hand-tool interaction segmentation.

## Key Results
- Mask R-CNN achieves 63.8% box mAP / 61.9% mask mAP for hand segmentation
- QueryInst achieves 55.2% box mAP / 49.4% mask mAP for hand-tool detection
- Models trained on EgoSurgery-HTS significantly outperform transfer from daily-activity datasets (EgoHands: 8.3% vs. 54.0% hand mAP; VISOR-HOS: 13.0% vs. 55.2% hand-tool mAP)
- Per-tool mAP strongly correlates with appearance frequency, indicating class imbalance challenges

## Why This Works (Mechanism)

### Mechanism 1
- Domain-specific pixel-level annotations enable substantially better hand-tool segmentation performance in surgical environments compared to transfer from daily-activity datasets.
- Surgical scenes exhibit unique visual characteristics—specular reflections from instruments, blood/tissue occlusion, variable lighting from surgical lamps, and specific tool-hand configurations—that create a domain shift from everyday hand-object datasets.
- Performance gap validation: QueryInst trained on EgoHands achieves only 8.3% box mAP / 6.3% mask mAP for hand segmentation vs. 54.0% / 50.7% when trained on EgoSurgery-HTS.

### Mechanism 2
- Leveraging SAM with existing bounding box annotations produces viable pixel-level segmentation labels after manual correction, reducing annotation cost while maintaining label quality.
- SAM's promptable segmentation capability accepts bounding boxes as spatial prompts, generating initial mask proposals that annotators manually review and correct rather than creating masks from scratch.
- Dataset scale: 50,383 tool instances, 57,173 hand instances, and 41,605 hand-tool segmentations across 19,496 frames.

### Mechanism 3
- Different segmentation architectures exhibit task-specific strengths due to their inductive biases—Mask R-CNN excels at hand detection while QueryInst and Mask2Former better handle tool complexity.
- Mask R-CNN's two-stage detection-then-segmentation pipeline with ROI features appears well-suited for the relatively consistent hand morphology, while Mask2Former's masked attention mechanism handles per-pixel tool segmentation (40.9% mask mAP vs. QueryInst's 36.7%).

## Foundational Learning

- **Instance Segmentation vs. Semantic Segmentation**
  - Why needed here: EgoSurgery-HTS provides instance-level masks (distinguishing individual tool copies) not just semantic class labels. Understanding this distinction is critical for interpreting the mAP metrics and model selection.
  - Quick check question: Given an image with two scalpel instances, would semantic segmentation output one combined scalpel mask or two separate masks with instance IDs?

- **Domain Shift / Transfer Learning**
  - Why needed here: The paper's central claim is that daily-activity hand datasets transfer poorly to surgery. Understanding why—appearance differences, motion patterns, occlusion types—enables proper assessment of the dataset's contribution.
  - Quick check question: If a model trained on kitchen videos achieves 50% hand mAP on a cooking dataset but 10% on surgical videos, what factors might explain this performance drop?

- **Mean Average Precision (mAP) at IoU Thresholds**
  - Why needed here: The paper reports box mAP and mask mAP using COCO metrics averaged over IoU thresholds 0.5-0.95. Understanding that high mAP requires both accurate localization and precise boundaries prevents misinterpreting modest scores as failures.
  - Quick check question: If a hand detection achieves 70% AP@0.5IoU but only 40% AP@0.95IoU, what does this indicate about the detection quality?

## Architecture Onboarding

- **Component map**:
  Input: Egocentric surgical video frames (RGB) -> Backbone: ResNet-50 (shared across evaluated models) -> Model branches: Mask R-CNN / QueryInst / Mask2Former / SOLOv2 -> Outputs: Tool instances, Hand instances, Hand-tool pairs

- **Critical path**:
  1. Frame extraction from EgoSurgery video corpus
  2. SAM-based mask generation using existing bounding boxes
  3. Manual review and correction of generated masks
  4. Hand-tool interaction labeling via IoU matching between hand and tool masks
  5. Video-split train/test division to prevent overfitting to specific procedures
  6. Model fine-tuning from MS-COCO pre-trained weights

- **Design tradeoffs**:
  - Annotation completeness vs. cost: SAM-assisted labeling trades some label noise for 19K+ annotated frames
  - Video-split vs. random-split evaluation: Video-split prevents leakage but reduces training diversity
  - Class granularity: 14 tool types capture diversity but create class imbalance
  - Hand ownership labeling: Distinguishing own/other hands enables first-person analysis but adds complexity

- **Failure signatures**:
  - Low-frequency tool confusion: Models misclassify tools with similar shapes and rare appearance
  - Other-hand detection failures: "Other hands" are less distinct, yielding higher confusion rates
  - Imaginary tool predictions: Models hallucinate tools in challenging lighting/texture conditions
  - Domain transfer collapse: Models trained on EgoHands/VISOR produce near-random predictions on surgical frames

- **First 3 experiments**:
  1. Baseline establishment: Train all four architectures (Mask R-CNN, QueryInst, Mask2Former, SOLOv2) with ResNet-50 backbone on EgoSurgery-HTS using video-split protocol; report box and mask mAP for each task.
  2. Domain transfer validation: Train QueryInst on EgoHands and VISOR-HOS, evaluate on EgoSurgery-HTS test set to replicate the 8.3% and 13.0% mAP gaps.
  3. Class imbalance ablation: Train Mask2Former on tool segmentation with (a) original data, (b) oversampled rare classes, and (c) merged rare classes into "other tools"; measure per-class AP.

## Open Questions the Paper Calls Out

- **How can model performance be improved to reach the level required for critical surgical applications?**
  - The authors state "Despite promising results, mAP results are still not acceptable for critical surgery applications" with current accuracy levels not sufficient for real-world surgical use.

- **How can the challenge of segmenting heavily imbalanced tool classes be addressed?**
  - The authors identify that "The unbalance in tool's appearance offers a great disparity in mAP prediction for each tool" and plan to focus on minimizing challenges of segmenting heavily imbalanced data.

- **Can expanding the dataset with more diverse environments and tool instances improve model robustness?**
  - The authors plan to "collect more tool instances to equilibrate the tool distribution and increase the robustness of the model through additions of different egocentric open surgery environment."

## Limitations

- **Annotation quality uncertainty**: SAM-assisted annotation approach's effectiveness depends on manual correction rates, which aren't quantified in the paper.
- **Class imbalance challenges**: The dataset shows pronounced class imbalance with some tools appearing significantly more frequently than others, leading to poor performance on underrepresented classes.
- **Limited environment diversity**: The dataset is limited to 21 videos from 8 surgeons performing 10 procedures, potentially limiting generalizability to other surgical environments.

## Confidence

- **High Confidence**: The domain transfer performance gap (daily-activity vs. surgical datasets) is well-demonstrated with clear quantitative evidence.
- **Medium Confidence**: SAM-assisted annotation mechanism is plausible but lacks validation metrics for correction effort and label quality.
- **Low Confidence**: Architectural performance differences are presented without systematic hyperparameter sweeps to rule out tuning effects.

## Next Checks

1. Obtain and analyze the EgoSurgery-HTS dataset to verify annotation quality, particularly the manual correction rates for SAM-generated masks and per-class instance counts.
2. Conduct controlled experiments comparing SAM-generated annotations versus fully manual annotations on a subset to quantify the trade-off between annotation cost and label quality.
3. Perform systematic hyperparameter sweeps across all four architectures using the same training protocol to determine if performance gaps reflect true architectural advantages or optimization differences.