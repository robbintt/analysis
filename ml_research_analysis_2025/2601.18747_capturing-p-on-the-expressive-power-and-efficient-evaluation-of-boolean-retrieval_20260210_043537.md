---
ver: rpa2
title: 'Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval'
arxiv_id: '2601.18747'
source_url: https://arxiv.org/abs/2601.18747
tags:
- retrieval
- query
- standard
- engine
- boolean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modern information retrieval systems are being pushed to support
  complex neuro-symbolic reasoning tasks, but standard architectures face efficiency
  dilemmas. Iterator-based engines struggle with nested Boolean logic graphs, while
  recursive approaches suffer from prohibitive memory costs when handling broad logical
  exclusions.
---

# Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval

## Quick Facts
- arXiv ID: 2601.18747
- Source URL: https://arxiv.org/abs/2601.18747
- Reference count: 29
- Primary result: ComputePN algorithm achieves polynomial-time evaluation of complex Boolean queries over inverted indices, demonstrated by executing 500-node DAGs with arithmetic constraints in 0.8s on MS MARCO corpus

## Executive Summary
Modern information retrieval systems face a critical efficiency dilemma: iterator-based engines struggle with nested Boolean logic graphs while recursive approaches suffer from prohibitive memory costs when handling broad logical exclusions. This paper formalizes a Retrieval Language (LùëÖ) based on Directed Acyclic Graphs (DAGs) and proves it captures the complexity class P, establishing that retrieval engines can evaluate any polynomial-time property directly over their index. The proposed ComputePN algorithm achieves this by combining native DAG traversal with a memory-efficient Positive-Negative response mechanism, eliminating both the Universal Scan penalty of negation and the exponential blowup of tree-based iterators. Stress tests on the MS MARCO corpus demonstrate that while standard engines fail with combinatorial explosion (>10,000 clauses) for complex arithmetic constraints, ComputePN executes equivalent logic circuits in 0.8 seconds, validating native DAG evaluation over static tree expansion.

## Method Summary
The paper introduces ComputePN, an algorithm that evaluates complex Boolean queries structured as DAGs over inverted indices. The core innovation is the PN-Response representation, which wraps posting list sets with a POS/NEG flag to defer materialization of negations until the final step. The algorithm performs a topological sort of the DAG, memoizing results at each node to avoid redundant computation. Boolean operations (AND, OR, NOT) are redefined to operate on these PN-Response tuples, ensuring intermediate results never exceed the size of active posting lists. Arithmetic constraints are compiled into logic gates within the Query DAG, with numerical fields pre-indexed in bit-sliced format to enable bitwise Boolean operations to simulate arithmetic. The method achieves polynomial-time evaluation by reducing the Circuit Value Problem to retrieval queries.

## Key Results
- ComputePN executes 500-node DAG queries with arithmetic constraints in 0.8 seconds on MS MARCO corpus
- System handles queries with >10,000 clauses that cause combinatorial explosion in standard engines
- Formal proof establishes LùëÖ captures complexity class P via reduction from Circuit Value Problem
- PN-Response mechanism eliminates universal scan penalty, ensuring intermediate set sizes remain bounded

## Why This Works (Mechanism)

### Mechanism 1: Positive-Negative Response Algebra
The proposed algebra avoids the "Universal Scan" penalty typically associated with negation by deferring materialization until the final step. Instead of materializing the result of a negation (e.g., $U \setminus A$), which requires scanning the entire document universe, the algorithm wraps results in a tuple $\langle S, type \rangle$. If a set is negated, only the boolean `type` flag is flipped (POS $\leftrightarrow$ NEG). Operations like Intersection and Union are redefined to operate on these tuples, ensuring the intermediate set size $S$ never exceeds the size of the active posting lists. The core assumption is that the majority of logical constraints involve sparse sets; dense intermediate results can be treated as logical complements without materializing them until the very end. If the final result of a query is a dense set (e.g., "all documents NOT containing X"), the system must materialize the universe $U \setminus S_{root}$, potentially incurring the scan cost the mechanism seeks to avoid.

### Mechanism 2: DAG Structural Memoization
Evaluating queries as Directed Acyclic Graphs (DAGs) with memoization prevents the exponential runtime blowup found in standard iterator-based engines. Standard engines (DAAT) "unroll" shared logic into trees, creating redundant iterator instances for repeated sub-expressions. The proposed algorithm topologically sorts the DAG and uses a memoization table $M$ to store the result of each node exactly once. Re-use of sub-expressions is handled by reference, changing complexity from exponential $O(2^{|Q|})$ to polynomial $O(|V| \cdot |U_{active}|)$. The core assumption is that complex reasoning queries naturally contain shared sub-expressions (re-convergent logic) that benefit from caching. If the query logic is purely linear (no shared nodes), the overhead of maintaining the memoization map might exceed a simple iterator stack, though the paper suggests the break-even point is low.

### Mechanism 3: Circuit Value Reduction (Capturing P)
The system can execute arbitrary polynomial-time computations (Class P) by reducing them to Boolean circuits evaluated over the index. The paper proves P-Hardness by reduction from the Circuit Value Problem (CVP). Arithmetic constraints (e.g., "citations > 100") are compiled into logic gates (Adders/Comparators) within the Query DAG. The retrieval engine executes this circuit via the PN-Algebra, effectively treating the index as a SIMD processor. The core assumption is that numerical fields are pre-indexed in a binary-decomposed format (bit-sliced) to allow bitwise Boolean operations to simulate arithmetic. If the arithmetic logic requires loops or unbounded recursion (Turing Complete), the DAG representation fails, and the query cannot be expressed.

## Foundational Learning

- **Concept: Inverted Index & Posting Lists**
  - Why needed here: The entire paper optimizes the intersection, union, and negation of these lists. Understanding the difference between "materializing" a list (TAAT) and "streaming" it (DAAT) is the core distinction the paper exploits.
  - Quick check question: Can you explain why a standard iterator cannot traverse two paths in a logic graph simultaneously without cloning?

- **Concept: Complexity Class P vs. NP**
  - Why needed here: The paper claims the system "Captures P," meaning it can solve any problem solvable in polynomial time. Understanding this sets the boundary of what the engine can and cannot do (e.g., it cannot solve NP-Complete problems efficiently).
  - Quick check question: Why is the Circuit Value Problem (CVP) used as the benchmark for P-Completeness rather than Boolean Satisfiability (SAT)?

- **Concept: Bit-Sliced Indexing**
  - Why needed here: To perform arithmetic (sums, comparisons) using only Boolean operators, numeric values must be stored as bits across different terms (e.g., `BIT_0`, `BIT_1`).
  - Quick check question: How would you represent the integer "5" (binary 101) in a bit-sliced inverted index?

## Architecture Onboarding

- **Component map:** Natural Language (User) -> LLM Planner -> Query DAG (Logic/Arithmetic) + Ranking Profile -> ComputePN Engine (Topological Sort -> PN-Algebra Evaluation) -> Inverted Index (Text) + Bit-Sliced Fields (Numeric) -> Candidate Set $S_{valid}$ -> Ranker -> Final Results

- **Critical path:** The implementation of the **PN-Evaluation Algebra** (Section 4.2). Incorrectly implementing the "POS $\vee$ NEG" or "NEG $\wedge$ NEG" merge rules will break the sparsity guarantee and cause memory explosions.

- **Design tradeoffs:**
  - **Latency vs. Complexity:** The system trades the low latency of simple keyword search for the high latency of complex logic evaluation (0.8s reported for 500-node DAG).
  - **Index Size vs. Arithmetic Capability:** Supporting arithmetic requires bit-slicing numeric fields, which increases index size and maintenance complexity compared to standard single-value fields.

- **Failure signatures:**
  - **Tree-Expansion Bottleneck:** Runtime spikes exponentially; iterator count exceeds system limits (e.g., Lucene `maxClauseCount`).
  - **Universal Scan:** CPU/IO saturation on full corpus scan when handling broad exclusions (e.g., `A OR NOT B`) without PN logic.

- **First 3 experiments:**
  1. **Validate PN-Merge:** Implement the 4 variants of the AND/OR operations (POS/POS, POS/NEG, NEG/POS, NEG/NEG) and verify that the output set size strictly equals the intersection/union of input sets, never the universe size.
  2. **DAG vs. Tree Stress Test:** Construct a query with deep re-convergent logic (e.g., Fibonacci-style dependencies). Compare runtime of `ComputePN` (linear) vs. a standard recursive unroller (exponential).
  3. **Arithmetic Circuit Execution:** Index a small set of documents with bit-sliced integers. Implement a 4-bit Ripple-Carry Adder in the Query DAG and query for documents where `Field_A + Field_B > Constant`. Verify correctness against a brute-force filter.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Boolean-mapped arithmetic framework be extended into a canonical, high-performance feature representation for continuous floating-point constraints?
- Basis in paper: [explicit] The authors state in "Future Directions" that extending Boolean-mapped arithmetic to handle floating-point constraints and vector operations is the "immediate next step" to realizing the index as a logic processor.
- Why unresolved: The current proof-of-concept focuses on integer arithmetic (e.g., ripple-carry adders) and bit-sliced representations; it does not define a scheme for efficiently encoding or evaluating continuous values within the DAG structure.
- What evidence would resolve it: A formal encoding scheme for continuous values in $\mathcal{L}_R$ and benchmarks showing ComputePN evaluating floating-point thresholds without significant overhead.

### Open Question 2
- Question: Can "HyperNodes" effectively unify dense vector retrieval with symbolic constraints without sacrificing the strict complexity guarantees of ComputePN?
- Basis in paper: [inferred] Section 6.4 proposes "HyperNodes" (macros) to integrate vector search into the DAG, noting that naive compilation yields "unmanageably large DAGs," but provides no implementation or evaluation of this mechanism.
- Why unresolved: It is unclear if macro-expansion of vector operations preserves the sparsity invariant (output-sensitivity) that prevents the "Universal Scan" penalty, or if it re-introduces computational bottlenecks.
- What evidence would resolve it: An algorithmic definition of the HyperNode expansion and empirical latency data comparing native ComputePN execution against hybrid vector-symbolic queries.

### Open Question 3
- Question: What is the reliability and error rate of LLMs when compiling complex natural language intent into the syntactically strict Query DAGs required by $\mathcal{L}_R$?
- Basis in paper: [inferred] The architecture relies on the assumption that LLMs can act as "Intelligent Gateways" to compile logic (e.g., arithmetic constraints) into DAGs, but the paper only demonstrates a single successful prompt instance.
- Why unresolved: The paper does not analyze the failure modes of the compilation step, such as syntax errors in the generated DAG or logical hallucinations, which could break the end-to-end pipeline.
- What evidence would resolve it: A user study or benchmark suite evaluating the success rate of various LLMs in generating valid Query DAGs for complex arithmetic and logical prompts.

## Limitations
- The paper assumes all numerical fields can be pre-indexed in bit-sliced format, but does not address the overhead of maintaining such indices for frequently updated datasets.
- The claim of "0.8s for 500-node DAG" is presented without sensitivity analysis across different query topologies or hardware configurations.
- No explicit handling of distributed execution is discussed, leaving scalability beyond a single machine uncertain.

## Confidence
- **High Confidence:** The formal proof that LùëÖ captures P via Circuit Value Problem reduction is mathematically sound and well-documented.
- **Medium Confidence:** The PN-Response algebra avoids universal scan penalties in theory, but empirical validation against real-world query distributions is limited.
- **Low Confidence:** The scalability claims for arithmetic circuits lack detailed benchmarks across varying bit-widths and circuit complexities.

## Next Checks
1. **Query Distribution Validation:** Test the system with a broader range of query topologies (linear chains, deep re-convergence, random DAGs) to confirm consistent polynomial scaling.
2. **Index Maintenance Overhead:** Measure the cost of maintaining bit-sliced indices under realistic update patterns to assess practical deployment feasibility.
3. **Universal Scan Edge Cases:** Construct adversarial queries that force dense final sets (e.g., `NOT (A OR B OR C...)`) and verify the system gracefully falls back to universe materialization without performance collapse.