---
ver: rpa2
title: Adapting Feature Attenuation to NLP
arxiv_id: '2601.00965'
source_url: https://arxiv.org/abs/2601.00965
tags:
- costarr
- bert
- classification
- base
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates open-set recognition for text classifiers,\
  \ focusing on the challenge of identifying inputs from unseen categories\u2014a\
  \ common real-world scenario for deployed NLP systems. The authors adapt the COSTARR\
  \ framework, originally designed for computer vision, to transformer models (BERT\
  \ and GPT-2) for classifying 176 arXiv subject areas."
---

# Adapting Feature Attenuation to NLP

## Quick Facts
- arXiv ID: 2601.00965
- Source URL: https://arxiv.org/abs/2601.00965
- Reference count: 1
- Key outcome: COSTARR performs similarly to MSP and MaxLogit for OSR on text, with no statistically significant advantage

## Executive Summary
This work investigates open-set recognition for text classifiers, focusing on the challenge of identifying inputs from unseen categories—a common real-world scenario for deployed NLP systems. The authors adapt the COSTARR framework, originally designed for computer vision, to transformer models (BERT and GPT-2) for classifying 176 arXiv subject areas. The adapted COSTARR score, which leverages pre- and post-classification features, is benchmarked against Maximum Softmax Probability (MSP), MaxLogit, and a temperature-scaled free-energy score. Results show that COSTARR performs similarly to MSP and MaxLogit, with no statistically significant advantage, while the free-energy score underperforms. The study highlights the difficulty of applying vision-centric OSR methods to NLP and suggests that model capacity and task-specific strategies may be more critical than scoring techniques for robust open-set text classification.

## Method Summary
The authors adapt COSTARR, a feature attenuation-based OSR framework from computer vision, to transformer models for open-set text classification. The method involves fine-tuning BERT (base) and GPT-2 on 132 known arXiv categories, extracting [CLS] embeddings as features, and computing post-classification features via element-wise multiplication with classification head weights. The COSTARR score concatenates pre- and post-attenuation features, computes cosine similarity to class-wise mean embeddings, and multiplies by a normalized maximum logit. This is compared against MSP, MaxLogit, and temperature-scaled free-energy scores. Evaluation uses OOSA (threshold-dependent accuracy) and AUOSCR (area under trade-off curve) metrics on a held-out test set containing 44 unknown categories.

## Key Results
- COSTARR performs similarly to MSP and MaxLogit, with no statistically significant advantage (OOSA: 0.319-0.319 BERT, 0.311-0.303 GPT-2)
- Temperature-scaled free-energy score underperforms substantially (OOSA: 0.172-0.174)
- All methods achieve OOSA < 0.4, suggesting model capacity limitations with 176 classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining pre- and post-attenuation features may provide complementary signals for detecting unknown inputs in transformers.
- Mechanism: The classification layer's weight vector W_j performs a dot product with features F(x), but low-weighted dimensions are attenuated (suppressed). COSTARR concatenates F(x) (pre-attenuation) with H_j = F(x) ⊙ W_j (post-attenuation) and computes similarity to class-wise mean embeddings.
- Core assumption: Information suppressed by the classification head retains novelty-detection signal that is recoverable via cosine similarity to known class centroids.
- Evidence anchors:
  - [abstract] "COSTARR framework... leverages pre- and post-classification features"
  - [section 3.4] Formal definition: C_j(x) := Concat(F(x), H_j), Sim_j(x) computed via cosine similarity to µ_Cj
  - [corpus] Weak evidence—related OSR work (arXiv:2404.10370) focuses on feature diversity for vision, not direct transfer to NLP
- Break condition: If classification head weights are dense (few near-zero values), attenuation effects diminish; Figure 1 shows white zones where weights approach zero, but this may not generalize across transformer architectures.

### Mechanism 2
- Claim: MaxLogit and MSP serve as strong baselines because transformer logits already encode class-discriminative signal that degrades gracefully on out-of-distribution inputs.
- Mechanism: MaxLogit takes the maximum pre-softmax activation; MSP applies softmax normalization first. Both threshold on the model's confidence signal without requiring feature-space analysis.
- Core assumption: Overconfidence on unknowns is partially mitigated by using raw logits rather than normalized probabilities.
- Evidence anchors:
  - [abstract] "COSTARR performs similarly to MSP and MaxLogit, with no statistically significant advantage"
  - [section 4] MaxLogit OOSA: 0.319 (BERT), 0.311 (GPT-2); COSTARR: 0.319 (BERT), 0.303 (GPT-2)—differences within noise
  - [corpus] No corpus papers directly compare logit-based OSR on text transformers
- Break condition: When class count is high (176 classes) and semantic overlap exists between known/unknown categories, all confidence-based methods degrade—OOSA never exceeds 0.4 for any method.

### Mechanism 3
- Claim: Temperature-scaled free-energy scoring underperforms in high-class-count NLP settings.
- Mechanism: Energy E(x;f) = -T log(Σ exp(f_k(x)/T)) aggregates all logit magnitudes; thresholding on -E(x) assumes in-distribution samples have lower energy. This works in vision but lags in this text classification setting.
- Core assumption: Energy landscape differentiates known from unknown in transformer embedding space similarly to CNN feature space.
- Evidence anchors:
  - [abstract] "free-energy score underperforms"
  - [section 4] Free-energy OOSA: 0.172 (BERT), 0.174 (GPT-2)—substantially below MaxLogit/COSTARR/MSP
  - [corpus] No corpus evidence for energy-based OSR on text; vision-origin method (Liu et al., 2020) cited but not validated for NLP
- Break condition: High class counts (176) may compress energy distributions, reducing separability; temperature tuning may require domain-specific calibration.

## Foundational Learning

- **Open-Set Recognition (OSR)**
  - Why needed here: Core problem being solved—classifiers must both label known classes and reject unknown inputs without retraining.
  - Quick check question: Given a 176-class classifier trained on arXiv categories, what should it output when given a paper from a new domain not in training data?

- **Feature Attenuation**
  - Why needed here: The hypothesis driving COSTARR—understanding why classification layers suppress information and how to recover it.
  - Quick check question: Why might the dot product F(x) · W_j discard information useful for detecting novel inputs?

- **Transformer Feature Extraction**
  - Why needed here: Practical implementation requires extracting [CLS] embeddings and classification head weights from BERT/GPT-2.
  - Quick check question: In BERT (base), which layer's output is used as the deep feature F(x) for COSTARR computation?

## Architecture Onboarding

- **Component map:**
  Input -> BERT/GPT-2 backbone -> [CLS] embedding (F(x)) -> Classification head (W_j) -> H_j = F(x) ⊙ W_j -> Concat(F(x), H_j) -> Cosine similarity to class means -> COSTARR score

- **Critical path:**
  1. Fine-tune transformer on 132 known categories (75% of 176 classes)
  2. Compute class-wise mean embeddings µ_Cj from training data
  3. At inference: extract F(x), compute H_m for predicted class m, concatenate, compute cosine similarity, multiply by normalized max logit
  4. Threshold on validation set containing held-out unknowns

- **Design tradeoffs:**
  - Model capacity vs. scoring complexity: Results suggest BERT/GPT-2 may be underpowered for 176-class OSR—OOSA < 0.4 for all methods. Larger backbones (GPT-4, DeepSeek-V3) may change conclusions.
  - COSTARR vs. MaxLogit: COSTARR more stable across thresholds (declines after 0.5–0.6 vs. MSP's drop after 0.2), but no accuracy gain. Implementation complexity not justified on current evidence.

- **Failure signatures:**
  - All methods score < 0.4 OOSA: Indicates backbone capacity issue, not scoring method failure
  - Free-energy scores near 0.17: Break condition for energy-based methods on high-class-count text tasks
  - Similar known/unknown categories (e.g., math vs. statistics): Semantic overlap causes false positives regardless of method

- **First 3 experiments:**
  1. Replicate on 8-class subset to validate preliminary finding (OOSA 0.752, AUOSCR 0.682) and confirm class-count scaling hypothesis.
  2. Test COSTARR on larger backbone (e.g., Llama-7B, Mistral) to isolate model capacity from scoring effectiveness.
  3. Ablate pre- vs. post-attenuation components: Run COSTARR using only F(x) or only H_j to determine if concatenation provides additive signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the feature attenuation hypothesis hold empirically for Transformer architectures, or is it specific to CNNs?
- Basis in paper: [explicit] "We also plan to conduct ablation studies to determine whether the feature attenuation hypothesis holds empirically in LLMs."
- Why unresolved: The authors successfully ported the COSTARR framework but note that layer normalization and attention mechanisms in LLMs may suppress features differently than in computer vision models, limiting theoretical understanding.
- What evidence would resolve it: Ablation studies analyzing feature suppression levels across internal layers of Transformers compared against CNN baselines to verify if the attenuation phenomenon exists.

### Open Question 2
- Question: Would COSTARR yield statistically significant improvements over MaxLogit or MSP if applied to state-of-the-art Large Language Models (LLMs)?
- Basis in paper: [explicit] "We would like to test our framework on larger and more powerful models such as GPT-4 and Deepseek-V3 to see if the performance improves."
- Why unresolved: The study utilized "modest" models (BERT base, GPT-2) and concluded that model capacity, rather than scoring technique, might be the primary limiting factor for OSR performance.
- What evidence would resolve it: Replicating the 176-class arXiv classification experiment using COSTARR on high-capacity models (e.g., GPT-4) to observe if scoring differentiation emerges.

### Open Question 3
- Question: Does the difficulty of open-set recognition in this context stem primarily from the high number of classes (176) or the semantic density of the categories?
- Basis in paper: [inferred] The authors note that preliminary experiments with 8 categories achieved an OOSA of 0.752, whereas the 176-class task dropped to ~0.31, and they call for testing on "alternative datasets."
- Why unresolved: The drastic performance drop suggests the task complexity scales with class count, but it remains unclear if the specific "arXiv" domain (where categories overlap significantly) is the main obstacle.
- What evidence would resolve it: Benchmarking the methods on datasets with varying class counts and semantic overlap to isolate the impact of class density on OSR scores.

## Limitations

- All OSR methods achieve OOSA < 0.4, suggesting model capacity limitations rather than scoring technique ineffectiveness
- Missing hyperparameter details (temperature, GNL normalization, training parameters) prevent faithful reproduction
- No ablation studies to validate the feature attenuation mechanism or determine if pre/post-feature concatenation provides additive benefit

## Confidence

- Medium: COSTARR vs. MSP/MaxLogit performance equivalence - supported by direct comparison but limited by missing hyperparameter details
- Medium: Free-energy underperformance in high-class NLP settings - clear experimental result but no investigation of why or temperature sensitivity
- Low: Feature attenuation mechanism validity - theoretical framework exists but lacks empirical validation in transformer context
- High: Model capacity is the limiting factor - all methods scoring < 0.4 OOSA strongly indicates this, though the paper doesn't explicitly state this conclusion

## Next Checks

1. **Ablation study**: Run COSTARR using only F(x) or only H_j components to determine if concatenation provides additive signal beyond individual features. This would validate the mechanism claim about complementary information.

2. **Scale experiment**: Test on a subset of 8 categories where paper reports OOSA ~0.75. If performance matches, this confirms class-count scaling hypothesis and isolates model capacity from scoring effectiveness.

3. **Hyperparameter sensitivity**: Systematically vary temperature T for free-energy scoring and GNL normalization parameters. Document how these affect OSR performance to understand whether poor free-energy results are methodological or parameter-dependent.