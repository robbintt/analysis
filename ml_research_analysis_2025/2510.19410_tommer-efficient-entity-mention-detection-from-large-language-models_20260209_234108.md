---
ver: rpa2
title: ToMMeR -- Efficient Entity Mention Detection from Large Language Models
arxiv_id: '2510.19410'
source_url: https://arxiv.org/abs/2510.19410
tags:
- entity
- tommer
- mention
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ToMMeR is a lightweight probing architecture that efficiently extracts
  mention detection capabilities from early layers of large language models, requiring
  fewer than 300K parameters and no schema input, prompting, or text generation. Trained
  solely on span boundaries from LLM-labeled data, ToMMeR achieves 93% recall and
  92% precision across 13 diverse NER benchmarks, validated by an LLM-as-judge approach.
---

# ToMMeR -- Efficient Entity Mention Detection from Large Language Models

## Quick Facts
- arXiv ID: 2510.19410
- Source URL: https://arxiv.org/abs/2510.19410
- Authors: Victor Morand; Nadi Tomeh; Josiane Mothe; Benjamin Piwowarski
- Reference count: 39
- ToMMeR achieves 93% recall and 92% precision on mention detection across 13 diverse NER benchmarks using <300K parameters

## Executive Summary
ToMMeR is a lightweight probing architecture that efficiently extracts mention detection capabilities from early layers of large language models. The approach requires no schema input, prompting, or text generation, and achieves high performance with fewer than 300K parameters. By training solely on span boundaries from LLM-labeled data, ToMMeR demonstrates that mention detection capabilities are naturally present in early LLM layers and can be reliably extracted across diverse model architectures.

## Method Summary
ToMMeR employs a lightweight probing architecture that attaches to early layers of large language models to detect entity mentions. The probe is trained exclusively on span boundary information from LLM-labeled data, avoiding the need for schema-specific training or generation capabilities. The system uses a binary classification approach to identify mention boundaries, achieving high performance with minimal parameter overhead compared to full NER systems.

## Key Results
- Achieves 93% recall and 92% precision on mention detection across 13 diverse NER benchmarks
- Cross-model analysis shows consistent mention boundary predictions across diverse LLM architectures (14M-15B parameters) with DICE >75% agreement
- When extended with span classification heads, achieves competitive full NER performance (80-87% F1) on standard benchmarks
- Demonstrates that mention detection emerges naturally from language modeling rather than being dataset-specific

## Why This Works (Mechanism)
The paper demonstrates that mention detection capabilities are inherently present in early layers of large language models through the consistent emergence of boundary prediction patterns across diverse architectures. The probe's effectiveness stems from leveraging these pre-existing capabilities rather than training from scratch, with the binary classification approach focusing on the fundamental task of identifying entity boundaries without requiring full schema understanding.

## Foundational Learning
- Large Language Model Layer Analysis: Understanding early layer representations and their semantic capabilities
  - Why needed: To identify optimal probing points for mention detection
  - Quick check: Verify activation patterns align with entity boundary positions

- Binary Classification for Boundary Detection: Simplified task formulation focusing on span identification
  - Why needed: Reduces complexity while maintaining core mention detection capability
- Span Classification Extension: Adding type prediction capability to binary mention detection
  - Why needed: Enables transition from mention detection to complete NER pipelines

## Architecture Onboarding

**Component Map:**
Input Text -> LLM Early Layers -> ToMMeR Probe -> Binary Classification -> Mention Boundaries

**Critical Path:**
The critical path flows from input text through LLM layers to the probe's binary classification layer. The probe's lightweight design (<300K parameters) ensures minimal computational overhead while maintaining high accuracy in boundary detection.

**Design Tradeoffs:**
The architecture trades full schema understanding for efficiency by focusing solely on mention boundaries rather than complete entity typing. This simplification enables high performance with minimal parameters but requires additional components for complete NER tasks.

**Failure Signatures:**
Performance degradation occurs when mention boundaries are ambiguous or when entities span complex syntactic structures. The binary classification approach may struggle with nested entities or overlapping mentions that require more sophisticated context understanding.

**First 3 Experiments to Run:**
1. Test cross-architectural consistency by applying ToMMeR probes trained on one LLM to another LLM's layers
2. Evaluate probe performance degradation as a function of distance from early layers
3. Assess the impact of different binary classification thresholds on precision-recall tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- The LLM-as-judge validation approach introduces potential circularity since the same models that generate training data are used for evaluation
- Full NER pipeline performance (80-87% F1) shows ToMMeR is competitive but not state-of-the-art
- The claim that mention detection capabilities are "naturally present" lacks mechanistic explanation

## Confidence

**Confidence labels:**
- Mention detection performance claims: High
- Cross-LLM consistency findings: High  
- Full NER pipeline performance: Medium
- LLM-as-judge validation reliability: Medium

## Next Checks

1. Conduct human evaluation on the full validation set to verify LLM-as-judge reliability across all confidence levels
2. Test ToMMeR's cross-lingual generalization on non-English NER benchmarks to assess language model universality claims
3. Analyze probe performance degradation as a function of distance from early layers to identify optimal probing depth across different LLM architectures