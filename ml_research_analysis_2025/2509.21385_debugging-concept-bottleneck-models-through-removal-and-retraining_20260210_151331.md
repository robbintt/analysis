---
ver: rpa2
title: Debugging Concept Bottleneck Models through Removal and Retraining
arxiv_id: '2509.21385'
source_url: https://arxiv.org/abs/2509.21385
tags:
- concepts
- concept
- spurious
- tail
- retraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents CBDebug, a general interpretable debugging
  framework for concept bottleneck models (CBMs) that enables domain experts to remove
  undesired concepts and retrain models for better alignment with expert reasoning.
  The framework consists of two steps: (1) Removal, where experts identify and remove
  spurious concepts, and (2) Retraining, where CBDebug converts concept-level feedback
  into sample-level auxiliary labels and applies reweighting and augmentation to reduce
  reliance on undesired concepts.'
---

# Debugging Concept Bottleneck Models through Removal and Retraining

## Quick Facts
- arXiv ID: 2509.21385
- Source URL: https://arxiv.org/abs/2509.21385
- Reference count: 34
- Key outcome: CBDebug improves worst-group accuracy up to 26% while maintaining average performance through concept removal and retraining

## Executive Summary
This paper presents CBDebug, a framework for debugging concept bottleneck models (CBMs) by enabling domain experts to remove undesired concepts and retrain models for better alignment with expert reasoning. The framework addresses the problem that CBMs can still learn spurious correlations even when explicitly trained to use specified concepts. CBDebug works by first having experts identify and remove spurious concepts from the inference layer, then converting this concept-level feedback into sample-level auxiliary labels for reweighting and augmentation-based retraining.

## Method Summary
CBDebug consists of three stages: (1) Label - extract concept activations for spurious concepts across training samples, (2) Reweight - apply permutation weighting with K-fold cross-validation to compute sample weights that approximate the unconfounded distribution, and (3) Augment - selectively augment bias-aligned samples using CutMix (for ProtoPNets) or Mixup (for VLM-CBMs) with concept banks, then retrain the CBM. The method is evaluated on PIP-Net and Post-hoc CBM architectures across multiple benchmarks including Waterbirds, MetaShift, CelebA, and ISIC, showing significant improvements in worst-group accuracy while maintaining average performance.

## Key Results
- CBDebug achieves up to 26% improvement in worst-group accuracy over original models
- The framework outperforms prior retraining methods and baselines across both real and automated expert feedback scenarios
- Significant improvements observed on Waterbirds (79.4% vs 74.2% for reweight-only), MetaShift (74.6% vs 70.8%), and CelebA (96.6% vs 96.2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept-level expert feedback can be converted into effective sample-level auxiliary labels for bias mitigation.
- Mechanism: CBDebug collects concept extractor activation scores for spurious concepts across all training samples, using these real-valued scores as approximate auxiliary labels that indicate which samples rely on undesired concepts.
- Core assumption: Concept activations reliably indicate the degree to which each sample depends on marked spurious concepts.
- Evidence anchors: The interpretability of CBMs allows converting concept-level feedback to sample-level labels; however, direct validation of score-to-reliance mapping is limited.

### Mechanism 2
- Claim: Permutation weighting approximates the counterfactual distribution where spurious concepts have no effect on the label.
- Mechanism: Two datasets are constructed - one with original confounded data and one with randomly permuted labels to break correlation. A binary predictor learns to distinguish these distributions, with sample weights reflecting the odds of belonging to the unconfounded distribution.
- Core assumption: Label permutation adequately simulates the unconfounded distribution and the predictor can reliably estimate membership probabilities.
- Evidence anchors: Formal derivation using K-fold cross-validation; prior validation of permutation weighting in bias mitigation work.

### Mechanism 3
- Claim: Targeted augmentation of bias-aligned samples reduces shortcut reliance more robustly than reweighting alone.
- Mechanism: Samples with low weights (strongly relying on spurious concepts) are preferentially augmented using CutMix or Mixup to increase support for underrepresented groups without relying solely on upweighting.
- Core assumption: Augmentation successfully breaks spurious correlations while preserving true labels and concept banks contain representative images.
- Evidence anchors: CBDebug (combining reweight + augment) outperforms reweight-only on PIP-Net Waterbirds; synthetic concept banks provided by DISC.

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)** - Two-stage prediction (concept extraction → inference layer). Why needed: Understanding CBM architecture is essential to grasp where feedback enters and how retraining propagates.
  - Quick check: Can you explain why removing a concept from the inference layer might not eliminate its influence on predictions?

- **Concept: Spurious Correlations and Shortcut Learning** - Background predicting bird type in Waterbirds dataset. Why needed: Motivation for reweighting and augmentation is unclear without this concept.
  - Quick check: In the Waterbirds dataset, what is the spurious attribute and which subgroup would have the lowest accuracy if the model relies on it?

- **Concept: Distributionally Robust Optimization and Reweighting** - Importance weighting to achieve robustness. Why needed: Clarifies why we reweight samples based on group membership or estimated membership.
  - Quick check: Why does upweighting minority-group samples help worst-group accuracy but potentially harm average accuracy?

## Architecture Onboarding

- **Component map**: Concept Extractor (φ) -> Inference Layer (h) -> Label Module -> Reweight Module -> Augment Module -> Retraining Loop
- **Critical path**: 1. Train original CBM → 2. Expert marks C_spur → 3. Compute bV for all training samples → 4. Run permutation weighting to get U → 5. Compute augmentation probabilities → 6. Augment training set → 7. Fine-tune CBM with weighted loss → 8. Evaluate on worst-group accuracy
- **Design tradeoffs**: 
  - Reweight vs. Augment vs. Both: Reweighting alone can be unstable; augmentation alone may not fully correct distribution
  - Full fine-tuning vs. linear layer only: PIP-Net fine-tunes entire model (allows concept discovery but risks instability); Post-hoc CBM freezes backbone (safer but limits adaptation)
  - Real vs. automated feedback: Real users provide nuanced judgments but are costly; LLM automation scales but may over/under-mark concepts
- **Failure signatures**: 
  - Worst-group accuracy drops after removal (Post-hoc CBM) - indicates limited concept set caused heavy reliance on spurious concepts
  - High variance in reweight-only results - suggests unstable weight estimation for minority groups
  - Augmentation introduces new spurious concepts - check concept bank quality and filter generated images
- **First 3 experiments**:
  1. Baseline sanity check: Run removal-only on both PIP-Net and Post-hoc CBM with Waterbirds
  2. Ablation of components: Compare Reweight Only, Augment Only, and CBDebug (both) on Waterbirds and MetaShift
  3. Automated feedback validation: Run LLM-based concept marking on CelebA or ISIC and compare against real-user feedback

## Open Questions the Paper Calls Out

- **Can multimodal foundation models effectively automate the debugging process for patch-based concept bottleneck models like PIP-Net?** The authors note exploration of this is left to future work, as current automation experiments were limited to text-based concepts in Post-hoc CBMs.

- **How does the composition and quality of the initial concept bank impact the effectiveness and stability of CBDebug?** The paper notes this exploration is left to future work, using a specific curation strategy combining synthetic and LLM-curated concepts without testing sensitivity to initialization choices.

- **How robust is CBDebug to noisy or imperfect feedback from non-expert users compared to graduate-level experts?** The small user study showed variable agreement (44.8% to 97.9%), suggesting sensitivity to specific feedback, but broader validation with diverse expertise levels remains unexplored.

## Limitations
- Concept activation scores may not reliably indicate sample-level reliance on spurious concepts without direct validation
- Permutation weighting assumes label permutation adequately simulates unconfounded distribution, which may not hold for complex multi-concept interactions
- Automated expert feedback using LLMs introduces significant variability and may over-remove legitimate concepts

## Confidence
- **High confidence**: Core framework design (removal → reweighting → augmentation) is sound and empirically validated on multiple benchmarks
- **Medium confidence**: Theoretical justification for permutation weighting as counterfactual estimator is solid but needs more rigorous validation across diverse spurious correlation structures
- **Low confidence**: Automated expert feedback pipeline using LLMs introduces significant variability and potential over-removal of legitimate concepts

## Next Checks
1. **Concept activation validation**: Systematically verify that concept extractor activation scores correlate with actual shortcut reliance by analyzing samples where the model makes predictions based on spurious concepts versus task-relevant features

2. **Permutation weight calibration**: Evaluate whether the permutation weighting estimator accurately estimates membership probabilities in the unconfounded distribution by testing on synthetic data with known confounding structures

3. **Concept bank quality assessment**: Quantitatively evaluate the coverage and quality of synthetic concept banks across different domains to ensure augmentation doesn't introduce new spurious correlations or fail to represent important subgroups