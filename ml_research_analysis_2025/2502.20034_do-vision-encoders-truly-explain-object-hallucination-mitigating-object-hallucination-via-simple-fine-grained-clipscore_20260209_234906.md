---
ver: rpa2
title: 'Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object
  Hallucination via Simple Fine-Grained CLIPScore'
arxiv_id: '2502.20034'
source_url: https://arxiv.org/abs/2502.20034
tags:
- f-clipscore
- clipscore
- hallucination
- object
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether object hallucination in vision-language
  models stems from limited vision encoder capacity. The authors propose Fine-grained
  CLIPScore (F-CLIPScore), a metric that incorporates noun-level text embeddings alongside
  full-sentence embeddings to improve hallucination detection.
---

# Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore

## Quick Facts
- **arXiv ID**: 2502.20034
- **Source URL**: https://arxiv.org/abs/2502.20034
- **Reference count**: 23
- **Primary result**: F-CLIPScore achieves 62.2% accuracy on OHD-Caps benchmark without training, representing a 39.6% improvement over conventional CLIPScore

## Executive Summary
This paper challenges the assumption that object hallucination in vision-language models (LVLMs) stems from limited vision encoder capacity. The authors propose Fine-grained CLIPScore (F-CLIPScore), which evaluates noun-level image-text alignment alongside full-sentence embeddings. They demonstrate that F-CLIPScore substantially improves hallucination detection and, when used for data filtering during LVLM pretraining, reduces hallucination rates by 4.9% on POPE accuracy. The results suggest that vision encoder capacity is not the primary bottleneck, as enhanced textual granularity alone can detect and mitigate object hallucination.

## Method Summary
The authors propose Fine-grained CLIPScore (F-CLIPScore) as a metric that computes CLIPScore for each noun extracted from a caption individually, then averages these with the full-sentence CLIPScore. F-CLIPScore is used both for discriminative hallucination detection (OHD-Caps benchmark) and data filtering during LVLM pretraining. The filtering approach removes image-caption pairs with low F-CLIPScore scores from pretraining data, reducing exposure to misaligned training examples. The method leverages existing CLIP models without requiring additional training, though it can be incorporated into training objectives with a weighted loss term.

## Key Results
- F-CLIPScore achieves 62.2% accuracy on OHD-Caps benchmark, a 39.6% improvement over conventional CLIPScore
- Data filtering with F-CLIPScore during LVLM pretraining improves POPE accuracy by 4.9%
- Optimal filtering ratio is 30% (retaining top 70% of data), with non-linear performance trends
- OHD-Caps training primarily reshapes text embeddings while vision embeddings remain largely unchanged, suggesting vision encoder capacity is not the bottleneck

## Why This Works (Mechanism)

### Mechanism 1: Noun-Level Granularity in Image-Text Alignment
Object hallucination detection improves when evaluating individual noun-image correspondence rather than only sentence-level alignment. F-CLIPScore extracts nouns using spaCy, computes CLIPScore for each noun independently against the image, and averages these with full-sentence CLIPScore. This forces explicit verification of each object mentioned.

### Mechanism 2: Training Data Curation via Hallucination-Aware Filtering
Removing image-caption pairs with low F-CLIPScore from pretraining data reduces hallucination in trained LVLMs. F-CLIPScore serves as a quality filter—samples scoring below a threshold are excluded from alignment pretraining, reducing exposure to misaligned or hallucinated training examples.

### Mechanism 3: Text-Side Representation Changes Dominate Hallucination Discrimination
Fine-tuning for hallucination detection primarily reshapes text embeddings, not vision embeddings—suggesting vision encoder capacity is not the bottleneck. When comparing original CLIP-L to OHD-Caps-trained CLIP-L, text embeddings show significant divergence between hallucinated vs. clean captions, while vision embeddings remain largely unchanged.

## Foundational Learning

- **Concept: CLIPScore and Image-Text Alignment**
  - Why needed here: F-CLIPScore modifies standard CLIPScore; understanding the baseline metric is essential for grasping the improvement.
  - Quick check question: Given an image and caption, what does CLIPScore compute, and why might sentence-level aggregation miss object-level misalignment?

- **Concept: Object Hallucination in LVLMs**
  - Why needed here: The paper specifically targets this failure mode; distinguishing discriminative vs. generative hallucination settings matters for interpreting results.
  - Quick check question: What is the difference between evaluating hallucination via caption selection (OHD-Caps) vs. free-form caption generation (POPE)?

- **Concept: LVLM Architecture Components (Vision Encoder, LLM, Adapter)**
  - Why needed here: The paper's central argument is about which component causes hallucination.
  - Quick check question: Which component does the paper argue is NOT the primary cause of object hallucination, and what evidence supports this?

## Architecture Onboarding

- **Component map**: spaCy parser -> CLIP vision encoder -> CLIP text encoder -> F-CLIPScore computation -> (optional) data filtering
- **Critical path**: 1) Parse caption → extract noun list, 2) Encode image once via CLIP vision encoder, 3) Encode full caption + each noun via CLIP text encoder, 4) Compute cosine similarities, average per Eq. 1, 5) (If filtering) Sort samples by F-CLIPScore, remove bottom x%
- **Design tradeoffs**: Parser choice: spaCy is fast but English-centric; multilingual generalization not validated. Filtering rate: 30% removal (top 70% retained) worked best; performance is non-linear with filtering amount. Noun vs. noun phrase vs. verb: Nouns performed best; noun phrases were intermediate; verbs showed minimal improvement.
- **Failure signatures**: Standard CLIPScore adds nonexistent objects; OHD-Caps-trained CLIP replaces existing objects. Random noun replacement causes expected accuracy decline—validates metric isn't gaming the benchmark. Over-filtering removes legitimate samples.
- **First 3 experiments**: 1) Reproduce OHD-Caps evaluation: Run F-CLIPScore on the test set with OpenAI CLIP-L; confirm ~62% accuracy vs. ~22% baseline CLIPScore. 2) Ablate noun replacement: Replace extracted nouns with random Wikipedia nouns at 0.2–1.0 rates; verify accuracy declines as expected. 3) Filter and retrain: Apply 30% F-CLIPScore filtering to LLaVA-Pretrain (558k → ~391k samples); run alignment training; evaluate POPE accuracy gain (~4.9% expected).

## Open Questions the Paper Calls Out

### Open Question 1
Can F-CLIPScore be effectively integrated into generative decoding or training objectives for free-form caption generation, rather than discriminative evaluation? The authors note this remains future work, as their study focuses on discriminative, retrieval-style evaluation. Evidence would include demonstrating modified decoding strategies or training losses incorporating F-CLIPScore that reduce hallucination rates on generative benchmarks like CHAIR.

### Open Question 2
What is the optimal data filtering ratio when using F-CLIPScore for LVLM pretraining, and how does it trade off with computational cost? The authors observe non-linear performance trends across filtering rates, with 30% optimal, but characterizing this relationship across datasets and LVLM scales remains unresolved. Evidence would include systematic sweeps of filtering ratios with compute-budget analyses.

### Open Question 3
Can F-CLIPScore generalize to multilingual settings given its reliance on English-centric noun parsing? The authors call for evaluating effectiveness on multilingual datasets, noting parsing accuracy may vary across languages. Evidence would include benchmarking on multilingual image-text datasets with language-specific parsers.

### Open Question 4
If vision encoder capacity is not the primary cause of object hallucination, what specific textual or cross-modal alignment factors are the main contributors? While the paper concludes vision encoder capacity is not the bottleneck, the precise mechanism—whether priors, attention patterns, or embedding space structure—remains unclear. Evidence would include ablation studies isolating textual priors, cross-attention distributions, and adapter representations.

## Limitations
- F-CLIPScore effectiveness relies on English-centric spaCy noun parsing, with no validation of multilingual generalization
- Core argument about vision encoder capacity is demonstrated in discriminative retrieval-style evaluation, not free-form generation
- Optimal data filtering ratio shows non-linear behavior and may be dataset-dependent rather than universal

## Confidence
- **High Confidence**: F-CLIPScore achieves 62.2% accuracy on OHD-Caps benchmark (empirical results directly measured)
- **Medium Confidence**: Vision encoder capacity is not the primary bottleneck for object hallucination (inference based on embedding space analysis, needs further validation in generative settings)
- **Medium Confidence**: Data filtering with F-CLIPScore improves LVLM pretraining (empirical results show 4.9% POPE improvement, but optimal filtering rate is non-linear and dataset-dependent)

## Next Checks
1. **Cross-linguistic validation**: Test F-CLIPScore with multilingual noun extraction on non-English caption datasets to verify the noun-level granularity mechanism generalizes beyond English.
2. **Generative setting replication**: Evaluate whether the vision encoder capacity hypothesis holds when testing free-form caption generation using established hallucination benchmarks like POPE across multiple LVLM architectures.
3. **Optimal filtering rate exploration**: Systematically test filtering thresholds across 10-90% ranges on multiple pretraining datasets to characterize the non-linear relationship between data curation intensity and hallucination mitigation.