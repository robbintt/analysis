---
ver: rpa2
title: 'GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive
  Proxy in Diffusion Transformers'
arxiv_id: '2512.03451'
source_url: https://arxiv.org/abs/2512.03451
tags:
- reuse
- video
- proxy
- wan2
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GalaxyDiT, a training-free method to accelerate
  video generation by selectively reusing computation across denoising steps in diffusion
  transformers. The key insight is that different video models have distinct internal
  dynamics, requiring model-specific proxy selection for optimal reuse decisions.
---

# GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers

## Quick Facts
- **arXiv ID**: 2512.03451
- **Source URL**: https://arxiv.org/abs/2512.03451
- **Reference count**: 28
- **Primary result**: 1.87× and 2.37× speedups with 0.97% and 0.72% drops on VBench-2.0 while maintaining 5-10 dB higher PSNR than prior approaches

## Executive Summary
GalaxyDiT introduces a training-free method to accelerate video generation in diffusion transformers by selectively reusing computation across denoising steps. The key innovation is identifying model-specific proxy statistics through rank-order correlation analysis, enabling optimal skip decisions for each model. The method also introduces guidance-aligned reuse to synchronize conditional and unconditional passes in classifier-free guidance, eliminating visual artifacts. Evaluated on Wan2.1-1.3B, Wan2.1-14B, and Cosmos-Predict2-2B, GalaxyDiT achieves significant speedups while maintaining superior visual fidelity compared to prior approaches.

## Method Summary
GalaxyDiT accelerates DiT video generation by caching and reusing residuals (y_out - y_in) across denoising steps. The method first identifies the optimal proxy statistic for each model through Spearman rank correlation analysis between 8 candidate proxies from the first DiT block and an oracle measuring residual change between steps. At inference, a reuse metric accumulates normalized proxy differences, and when below a threshold (after 20% warm-up), cached residuals are reused for both conditional and unconditional passes in classifier-free guidance. This guidance-aligned reuse ensures synchronized noise predictions, eliminating artifacts from misaligned timesteps.

## Key Results
- Achieves 1.87× speedup on Wan2.1-1.3B with only 0.97% drop on VBench-2.0
- Achieves 2.37× speedup on Wan2.1-14B with only 0.72% drop on VBench-2.0
- Maintains 5-10 dB higher PSNR than prior approaches while using only 2.88 GB memory for residuals (vs 173 GB for RADiT)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Proxy Selection via Rank Correlation
GalaxyDiT extracts 8 candidate proxies from the first DiT block and uses Spearman's rank correlation coefficient to identify which proxy best preserves the oracle's ordering of step importance for each model. Different models require different optimal proxies—Cosmos-Predict2 uses mlp_out (ρ=0.82), Wan2.1-1.3B uses cross_attn_out (ρ=0.89), and Wan2.1-14B uses attn_out (ρ=0.65).

### Mechanism 2: Guidance-Aligned Reuse
By computing only the conditional pass's proxy at each step and using it to determine reuse for both conditional and unconditional passes, GalaxyDiT ensures synchronized noise predictions in classifier-free guidance. This eliminates visual artifacts that arise when combining conditionally-computed and unconditionally-reused predictions from different noise levels.

### Mechanism 3: Threshold-Based Residual Caching
GalaxyDiT uses cumulative normalized proxy difference as a reuse metric, accumulating ||proxy_S-1 - proxy_S||_1 / ||proxy_S||_1 across steps. When this metric stays below a threshold and step > 20%, residuals from the previous step are reused, providing significant speedups with minimal quality degradation.

## Foundational Learning

- **Diffusion Transformers (DiTs)**: Understanding DiT architecture is essential since GalaxyDiT operates on residual computations (y_out - y_in) within DiT blocks. Quick check: Can you explain why residual caching is more natural for DiTs than for U-Net diffusion models?

- **Classifier-Free Guidance (CFG)**: CFG combines two noise predictions: ε_combined = (1+g)×ε_conditional - g×ε_unconditional. The guidance alignment innovation depends on understanding that misaligned timesteps in these predictions cause artifacts. Quick check: If conditional pass computes at step t but unconditional reuses from step t-2, what happens in the CFG combination formula?

- **Spearman's Rank Correlation**: The proxy selection methodology uses rank-order preservation rather than absolute value matching. A proxy with ρ=0.89 means it correctly ranks step importance 89% of the time relative to the oracle. Quick check: Why is Spearman's ρ more appropriate here than Pearson correlation or MSE between proxy and oracle?

## Architecture Onboarding

- **Component map**: Input: Noised latent x_t, condition c → First DiT Block (partial compute) → Extract proxy → Reuse metric accumulator → Decision gate (metric < threshold AND step > 20%?) → Reuse path or Compute path → CFG → Sampler → x_{t-1}

- **Critical path**: The first DiT block computation for proxy extraction is on the critical path for every step, completing before reuse decisions can be made. This computation is <1% of full step cost.

- **Design tradeoffs**: Higher thresholds enable more aggressive reuse (2.37× speedup vs 1.30×) but lower PSNR. Memory usage is 2.88 GB for 14B model vs 173 GB for RADiT. Excluding first 20% of steps improves ρ from 0.176 to 0.754 but reduces speedup ceiling.

- **Failure signatures**: Wrong proxy selection causes incorrect skip decisions and severe quality drops. Non-aligned CFG produces visual artifacts—blurry textures, color desaturation, missing details. Over-aggressive thresholds cause PSNR to drop rapidly above ~70% reuse rate.

- **First 3 experiments**:
  1. Run 8 proxy candidates through Spearman correlation analysis on a held-out model variant (e.g., CogVideoX) to verify if a clear optimal proxy emerges (ρ > 0.7).
  2. Implement both CFG-aligned and CFG-agnostic reuse on the target model to measure PSNR/SSIM/LPIPS gap at 50% reuse rate.
  3. Run inference at 5-7 threshold values from 0.01 to 0.5 on 100 diverse prompts to plot speedup vs PSNR and identify diminishing returns.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the methodology raises several important unresolved issues regarding generalization across model families, theoretical justification for proxy selection, and optimal threshold determination.

## Limitations
- Model family dependency means the approach may not generalize well across different architectures or training dynamics
- Single-threshold abstraction assumes uniform step importance distribution that may not hold for complex prompts
- CFG assumption scope is limited to specific guidance formulations used in tested models

## Confidence
- **High Confidence**: Speedup claims (1.87×, 2.37×) and VBench-2.0 score maintenance (0.97%, 0.72% drops) are well-supported by experimental setup
- **Medium Confidence**: Guidance alignment mechanism's artifact elimination is demonstrated but relies on subjective visual comparisons
- **Low Confidence**: Generalization to other model families beyond three tested architectures is speculative

## Next Checks
1. **Cross-Architecture Proxy Analysis**: Apply 8-proxy Spearman correlation analysis to a new DiT model family (e.g., CogVideoX or Open-Sora) to verify whether optimal proxy location changes systematically with architecture depth or training objective.

2. **CFG Sensitivity Across Guidance Scales**: Test GalaxyDiT's guidance alignment mechanism across CFG scales (g=1.0 to g=10.0) to measure whether PSNR/LPIPS gap between aligned and non-aligned approaches scales with guidance strength.

3. **Early-Step Reuse Feasibility**: Systematically relax the 20% exclusion rule by testing reuse starting at 10%, 15%, and 25% of denoising steps to determine if the 20% heuristic is overly conservative and identify true inflection point where early-step reuse becomes detrimental.