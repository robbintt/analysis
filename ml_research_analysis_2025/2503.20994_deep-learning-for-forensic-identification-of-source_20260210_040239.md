---
ver: rpa2
title: Deep Learning for Forensic Identification of Source
arxiv_id: '2503.20994'
source_url: https://arxiv.org/abs/2503.20994
tags:
- contrastive
- cartridge
- learning
- networks
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the common-but-unknown source problem in forensics
  by comparing cartridge casings to determine if they were fired from the same firearm.
  The authors use contrastive neural networks trained on 3D topography scans of 9mm
  cartridge casings to learn similarity scores between casings.
---

# Deep Learning for Forensic Identification of Source

## Quick Facts
- arXiv ID: 2503.20994
- Source URL: https://arxiv.org/abs/2503.20994
- Authors: Cole Patten; Christopher Saunders; Michael Puthawala
- Reference count: 36
- Primary result: Contrastive learning achieved ROC AUC of 0.892 on NBIDE dataset, outperforming CMC's 0.867

## Executive Summary
This paper addresses the common-but-unknown source problem in forensic ballistics by using contrastive neural networks to determine if cartridge casings were fired from the same firearm. The authors train a residual network on 3D topography scans of breech face impressions using supervised contrastive loss, achieving state-of-the-art performance on the NBIDE benchmark. An ablation study varying network architecture shows moderate robustness to design changes. The work demonstrates that deep learning can effectively learn similarity metrics for cartridge casing comparison, outperforming traditional statistical methods.

## Method Summary
The method uses a residual neural network trained with supervised contrastive loss (SupCon) on 3D topography scans of breech face impressions from cartridge casings. Images are preprocessed through cropping, resizing to 224×224, conversion to polar coordinates (377×60), and normalization. Cyclic padding along the angular dimension provides rotational invariance without data augmentation. The network learns embeddings where dot products serve as similarity scores between paired casings. Training uses the E3 dataset (2,967 casings) and evaluation uses the NBIDE dataset (144 casings), with performance measured by ROC AUC.

## Key Results
- Contrastive learning achieved ROC AUC of 0.892 on NBIDE dataset
- Outperformed state-of-the-art CMC algorithm (ROC AUC 0.867)
- 4 of 7 model architectures tested outperformed CMC
- Optimal model had 79,536 parameters with width n=16
- Training completed in 12 hours versus CMC's 9 days for full pairwise comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive learning produces forensic similarity scores that outperform traditional correlation-based methods when trained on sufficiently diverse cartridge casing data.
- Mechanism: The network learns an embedding function f that minimizes distance between same-source casings while maximizing distance between different-source casings via the SupCon loss function. This creates a metric space where dot products serve as similarity scores.
- Core assumption: Breech face impressions contain distinguishable firearm-specific toolmarks that can be learned from the E3 dataset and generalized to unseen firearms in NBIDE.
- Evidence anchors:
  - [abstract]: "When trained on the E3 dataset of 2967 cartridge casings, contrastive learning achieved an ROC AUC of 0.892. The CMC algorithm achieved 0.867."
  - [section 3.3]: "4 of our 7 model architectures outperformed the CMC algorithm"
  - [corpus]: Limited direct support—no comparable forensic contrastive learning papers found in corpus; related work on contrastive diffusion for time series (CDNet) and network incident correlation exists but addresses different domains.
- Break condition: If the training dataset lacks sufficient intra-class variance (e.g., Popstat with only 2 exemplars per firearm), the network over-optimizes for dissimilarity and performance degrades.

### Mechanism 2
- Claim: Architectural rotational invariance through cyclic padding eliminates the need for data augmentation during training.
- Mechanism: Images converted to polar coordinates (60×377 grid) receive cyclic padding along the angular dimension before each convolutional layer, ensuring x₁ and x* (rotated version) produce equivalent representations without learned invariance.
- Core assumption: Interpolation error from polar conversion is less detrimental than the optimization burden of learning rotational invariance through augmentation.
- Evidence anchors:
  - [section 2.2.1]: "We found it to be more effective to convert our images to polar coordinates and add cyclic padding along the angular dimension before each convolutional layer."
  - [section 2.2.1]: "With this method, however, there is no need to optimize for rotational invariance, because it is already accounted for."
  - [corpus]: No corpus evidence found for this architectural pattern in forensic applications.
- Break condition: If polar interpolation introduces artifacts at radii with sparse original pixels, edge features may be corrupted.

### Mechanism 3
- Claim: Model capacity must scale appropriately with dataset size—over-parameterization on small forensic datasets reduces generalization.
- Mechanism: The reference model (79,536 parameters, ratio ~26.8 params/sample) outperforms larger variants (Width=64 with 1.2M parameters) because the parameter-to-data ratio prevents overfitting while maintaining sufficient representational capacity.
- Core assumption: Deep learning scaling laws apply to forensic domains where training data is inherently limited.
- Evidence anchors:
  - [section 3.3]: "With a training dataset of 2967 samples, we are not surprised that a model with 79,536 parameters outperforms one with 1,205,244."
  - [section 3.3]: "Reducing the number of parameters in our model resulted in worse generalization capability."
  - [corpus]: Weak support—corpus contains unrelated work on "Cartridges" for LLM KV cache optimization, not forensic applications.
- Break condition: With larger training datasets, the optimal parameter count would increase; the current optimum is specific to n≈3K samples.

## Foundational Learning

- Concept: Residual Networks (ResNet) with Skip Connections
  - Why needed here: Enables training of deeper architectures (11-20 layers) for feature extraction from 3D topography scans without vanishing gradients.
  - Quick check question: Can you sketch how a skip connection (y = b(x) + x) changes gradient flow during backpropagation compared to a plain feedforward network?

- Concept: Supervised Contrastive Loss (SupCon)
  - Why needed here: Provides the objective function that learns embeddings where same-firearm casings cluster together without requiring a fixed gallery of known firearms.
  - Quick check question: Given the loss formula in Equation 2, what happens to the loss when all same-class embeddings are identical but inter-class distances are small?

- Concept: Common-but-Unknown Source Problem
  - Why needed here: This forensic paradigm requires verification (same source?) rather than classification (which source?), making traditional closed-set approaches inapplicable.
  - Quick check question: Why would a classifier trained on firearms {F1, F2, ..., Fk} fail when cartridge C was fired by F_{k+1}?

## Architecture Onboarding

- Component map: Input: 224×224 breech face scan → Polar conversion (60×377) → Normalization (μ=0, σ=1) → ResNet backbone with cyclic padding → L2-normalized embedding → Similarity = dot product of paired embeddings

- Critical path:
  1. **Preprocessing alignment**: Follow CMC pipeline for breech face extraction (Figure 5b), then polar conversion
  2. **Cyclic padding implementation**: Before each Conv2d, pad angular dimension: [x₃₇₆, x₃₇₇, x₁, ..., x₃₇₇, x₁, x₂]
  3. **Training loop**: 20K epochs on E3 (2,967 casings from 297 firearms), monitor ROC AUC on NBIDE every 20 epochs
  4. **Inference**: Pairwise embedding extraction, similarity = f(x_a) · f(x_b) / τ

- Design tradeoffs:
  - **Width vs. Depth**: Reference (n=16, 11 layers) beats Double Block (20 layers) and Width=64 (1.2M params)—prefer balanced capacity
  - **Speed**: Training overhead (12 hours) vs. CMC inference (9 days for 144 casings)—contrastive learning wins for batch comparisons
  - **Precision/Recall profile**: Contrastive achieves near-100% TPR at moderate FPR; CMC achieves low FPR at moderate TPR (Figure 7)

- Failure signatures:
  - **Insufficient intra-class samples**: Popstat (2 exemplars/firearm) caused optimization to over-penalize similarity—training loss decreased but test ROC AUC dropped
  - **Small training set on same-domain test**: Leave-two-out on NBIDE (120 train/24 test) failed to beat CMC—insufficient diversity
  - **Over-smoothing**: Width=8 (20K params) underperformed—insufficient capacity for breech face feature extraction

- First 3 experiments:
  1. **Baseline replication**: Train reference model on E3, evaluate on NBIDE—target ROC AUC = 0.892 ± 0.015 (5 runs)
  2. **Ablation sweep**: Vary width ∈ {8, 16, 32, 64} with fixed depth—confirm optimal capacity at n=16, plot params vs. AUC curve
  3. **Threshold analysis**: Compute precision-recall curves at multiple similarity thresholds—identify operating points where contrastive outperforms CMC (low threshold) vs. where CMC excels (high threshold)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating firing pin impressions alongside breech face scans improve the accuracy of contrastive neural networks for forensic source identification?
- Basis in paper: [explicit] The authors state, "We suspect that contrastive networks would achieve a greater ROC AUC if they were provided with both the breech face and the firing pin impression as input."
- Why unresolved: The current study crops out the firing pin impression to maintain parity with the CMC algorithm, which operates exclusively on the breech face.
- What evidence would resolve it: A comparative study where networks are trained on fused inputs (breech face + firing pin) versus breech face alone, measuring the change in ROC AUC.

### Open Question 2
- Question: How does the ratio of parameters to training sample size impact performance, and would larger datasets enable the successful use of deeper architectures?
- Basis in paper: [explicit] The authors note that their model is limited by the dataset size, stating, "The introduction of a larger dataset of high-quality 3D cartridge casing scans would likely allow for the training of neural networks to a much higher degree of accuracy."
- Why unresolved: The study found that larger models (e.g., Width=64) performed worse than the Reference Model, likely due to the small training set (2,967 samples) causing overfitting.
- What evidence would resolve it: Training the larger architectures (Width=32, Width=64) on a significantly expanded dataset to observe if performance scales positively with model capacity.

### Open Question 3
- Question: What is the optimal training duration and stopping criterion to balance the bias-variance tradeoff in this forensic application?
- Basis in paper: [explicit] The authors mention, "In this work, we trained neural networks until they had overfit... To determine where in the training process to stop, the traditional machine learning motif of 'bias-variance tradeoff' must be tended too."
- Why unresolved: The current methodology involves training for a fixed 20,000 epochs to find the maximum ROC AUC, rather than determining a generalizable stopping point for deployment.
- What evidence would resolve it: An analysis of validation loss curves and ROC AUC over time to identify the epoch range where generalization performance peaks before overfitting occurs.

## Limitations

- The study lacks critical experimental details including optimizer choice, learning rate schedules, batch size, and temperature parameter for SupCon loss, making exact replication difficult.
- The comparison to CMC uses different datasets for training (E3) and testing (NBIDE), which may not fully represent real-world forensic scenarios where both methods would be evaluated on identical datasets.
- The claim that this method is "more efficient" than CMC requires scrutiny—while training takes 12 hours versus CMC's 9 days for full pairwise comparison, the paper doesn't address real-world deployment considerations like computational resources or operational workflows.

## Confidence

- **High Confidence**: The contrastive learning approach outperforms CMC on NBIDE when properly trained on E3 (ROC AUC 0.892 vs 0.867). This finding is well-supported by the experimental results and ablation study.
- **Medium Confidence**: The architectural design choice of using polar coordinates with cyclic padding is effective, though this specific approach appears novel without extensive validation across different forensic domains.
- **Low Confidence**: The claim that this method is "more efficient" than CMC requires scrutiny—while training takes 12 hours versus CMC's 9 days for full pairwise comparison, the paper doesn't address real-world deployment considerations like computational resources or operational workflows.

## Next Checks

1. Replicate the ablation study with explicit parameter sweeps across width ∈ {8, 16, 32, 64} and depth ∈ {11, 15, 20} layers, measuring ROC AUC on held-out validation sets to confirm the optimal capacity point at ~80K parameters.

2. Test the robustness claim by training on the full NBIDE dataset (leave-two-out cross-validation across all 144 casings) to verify whether the contrastive method maintains its advantage when training data is limited to the same domain as testing.

3. Implement a direct statistical comparison of AUC distributions (e.g., DeLong's test) between contrastive learning and CMC across multiple random seeds to establish whether the 0.025 AUC difference is statistically significant.