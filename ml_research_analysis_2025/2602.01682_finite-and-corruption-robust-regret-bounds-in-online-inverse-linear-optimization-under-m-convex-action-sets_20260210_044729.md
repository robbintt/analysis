---
ver: rpa2
title: Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization
  under M-Convex Action Sets
arxiv_id: '2602.01682'
source_url: https://arxiv.org/abs/2602.01682
tags:
- regret
- bound
- page
- m-convex
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online inverse linear optimization with M-convex
  action sets, where a learner sequentially infers an agent's hidden objective vector
  from observed optimal actions over changing feasible sets. The learner's performance
  is measured by regret, the cumulative gap between the agent's optimal values and
  those achieved by the learner's recommendations.
---

# Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets

## Quick Facts
- arXiv ID: 2602.01682
- Source URL: https://arxiv.org/abs/2602.01682
- Reference count: 11
- Primary result: Achieves finite regret bound O(d log d) for online inverse linear optimization under M-convex action sets, resolving polynomial regret question

## Executive Summary
This paper studies online inverse linear optimization where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over changing M-convex feasible sets. The learner's performance is measured by regret, the cumulative gap between the agent's optimal values and those achieved by the learner's recommendations. The key technical contributions are: (1) achieving finite regret bound O(d log d) under M-convex action sets by combining structural characterization of optimal solutions with geometric volume arguments, (2) extending the algorithm to handle adversarial corruptions with regret bound O((C+1)d log d) without prior knowledge of corruption count C, and (3) establishing a regret lower bound of Ω(d) showing the upper bound is tight up to a log d factor. The results apply to broad classes of combinatorial optimization problems over matroids and integer lattices.

## Method Summary
The algorithm maintains a constraint set At of index orderings implied by past observations, then computes ŵt as the center of gravity of polytope Pt = {w ∈ [0,1]^d : w(i) ≥ w(j) ∀(i,j) ∈ At}. It recommends x̂t ∈ argmax{⟨ŵt, x⟩ : x ∈ Xt} and updates At+1 based on observed optimal actions. For corruption robustness, the algorithm restarts when the directed graph ([d], At+1) has a cycle, detected via DFS/topological sort. The M-convex structure enables the finite regret bound through a key volume argument: when the learner's recommendation differs from the agent's optimal action, the constraint set At expands to cut off at least a 1/e fraction of the remaining volume in [0,1]^d, ensuring at most O(d log d) mistakes.

## Key Results
- Achieves finite regret bound O(d log d) for online inverse linear optimization under M-convex action sets
- Extends to handle adversarial corruptions with regret bound O((C+1)d log d) without prior knowledge of C
- Establishes regret lower bound of Ω(d), showing the O(d log d) upper bound is tight up to a log d factor

## Why This Works (Mechanism)
The finite regret bound exploits M-convex structure through two key mechanisms: First, M-convexity ensures that if x* is optimal for w* and x* - ei + ej ∈ X, then w*(i) ≥ w*(j). This creates a directed graph constraint system that shrinks the feasible polytope P by at least a 1/e fraction whenever the learner makes a mistake. Second, the center-of-gravity recommendation strategy guarantees that any new constraint cuts off a constant fraction of the remaining volume, limiting mistakes to O(d log d). The corruption robustness works by detecting when observed feedback creates cycles in the implied ordering graph, which can only happen if adversarial corruptions have occurred.

## Foundational Learning
- M-convex functions: Submodular set functions extended to integer lattices; needed for modeling discrete combinatorial optimization problems. Quick check: verify matroid rank function is M-convex.
- Online inverse optimization: Learner infers hidden parameters from observed optimal decisions; needed to model decision-making under uncertainty. Quick check: implement basic inverse optimization for linear programs.
- Center of gravity method: Chooses decision as centroid of feasible region; needed for geometric volume arguments. Quick check: compute center of gravity for 2D polytope.
- Submodular volume argument: Shows constraints cut off constant fraction of remaining volume; needed for finite regret proof. Quick check: verify 1/e volume reduction for random halfspace cuts.

## Architecture Onboarding
**Component map:** Oracle access -> Membership queries -> At update -> Center-of-gravity computation -> Recommendation -> Regret calculation
**Critical path:** Oracle queries → At update → Center-of-gravity computation → Recommendation → Regret accumulation
**Design tradeoffs:** Exact center-of-gravity computation is #P-hard, requiring approximation (Lovász-Vempala 2006); cycle detection adds O(d²) overhead but enables corruption robustness without prior knowledge of C.
**Failure signatures:** Regret exceeds O(d log d) due to poor center-of-gravity approximation or numerical precision issues in tie-breaking; corruption detection fails due to false positives from floating-point errors.
**First experiments:** 1) Test M-convex membership oracle on uniform matroid with varying constraints; 2) Verify center-of-gravity computation reduces volume by 1/e factor on mistakes; 3) Validate cycle detection on synthetic corrupted sequences.

## Open Questions the Paper Calls Out
None

## Limitations
- Center-of-gravity computation is #P-hard in general, requiring careful approximation
- Algorithm assumes exact membership oracle queries and perfect cycle detection
- Numerical precision issues could cause false positives in corruption detection when w* components are close

## Confidence
- Theoretical regret bounds (O(d log d) uncorrupted, O((C+1)d log d) corrupted): High
- Algorithmic practicality: Medium (due to #P-hard center-of-gravity computation)
- Ease of reproducing benchmark results: Low (missing test instances and hyperparameters)

## Next Checks
1. Verify the cycle-detection restart mechanism on synthetic corrupted sequences; confirm that detected cycles correspond to true adversarial corruptions, not numerical artifacts.
2. Empirically measure Vol(Pt) shrinkage over time for different M-convex sets (e.g., uniform matroid) to confirm the (1-1/e) volume reduction rate per mistake.
3. Test the algorithm on combinatorial optimization instances (e.g., shortest path, spanning tree) with controlled w* and compare achieved regret to the O(d log d) bound.