---
ver: rpa2
title: An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection
arxiv_id: '2601.14305'
source_url: https://arxiv.org/abs/2601.14305
tags:
- detection
- accuracy
- data
- decision
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting anomalies in IoT
  networks while ensuring model interpretability and low computational overhead for
  resource-constrained devices. It proposes an optimized Decision Tree-based framework
  enhanced with explainable AI (XAI) techniques including SHAP values and Morris sensitivity
  analysis to provide both local and global feature importance.
---

# An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection

## Quick Facts
- arXiv ID: 2601.14305
- Source URL: https://arxiv.org/abs/2601.14305
- Reference count: 15
- Primary result: 99.91% accuracy on WUSTL-IoT with interpretable Decision Tree and XAI

## Executive Summary
This paper presents an optimized Decision Tree-based framework for IoT anomaly detection that balances high performance with model interpretability and low computational overhead. The approach combines recursive feature elimination, class imbalance handling through random oversampling, and explainable AI techniques (SHAP and Morris sensitivity analysis) to provide both local and global feature importance. The framework achieves 99.91% accuracy on the WUSTL-IoT dataset while maintaining computational efficiency suitable for resource-constrained edge devices.

## Method Summary
The framework preprocesses the WUSTL-IoT dataset through duplicate removal, label encoding, IQR outlier capping, and z-score normalization. Feature selection combines chi-square filtering, Pearson correlation with Bonferroni correction, and recursive feature elimination. Class imbalance is addressed through random oversampling to balance training data. Gaussian noise augmentation enhances model robustness. A Decision Tree classifier (max_depth=10, min_samples_split=4) serves as the core model, with SHAP and Morris sensitivity analysis providing interpretability. The approach achieves high accuracy while maintaining computational efficiency for edge deployment.

## Key Results
- Achieves 99.91% accuracy, 99.51% F1-score, and 99.60% Cohen Kappa on WUSTL-IoT dataset
- Identifies SrcMac as the most influential feature for anomaly detection
- Maintains high cross-validation stability with 98.93% mean accuracy
- Successfully handles class imbalance through random oversampling to 34,272 balanced samples
- Balances high detection performance with interpretability and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
Recursive feature elimination combined with statistical filtering improves detection accuracy while reducing computational overhead. The framework applies chi-square independence testing (p < 0.05 threshold) followed by Pearson correlation with Bonferroni correction to eliminate irrelevant features. RFE with cross-validation then selects the most predictive subset. This sequential filtering reduces dimensionality before classifier training, lowering inference cost. Core assumption: The removed features contain minimal signal for distinguishing Normal, Spoofing, and Data Alteration classes in deployment environments similar to WUSTL-IoT.

### Mechanism 2
Random oversampling of minority attack classes enables balanced learning without synthetic sample generation artifacts. Training set minority classes (Spoofing: 895, Data Alteration: 735) are resampled with replacement to match the majority class count (11,424 each), yielding 34,272 balanced samples. This prevents the decision tree from biasing toward Normal predictions. Core assumption: Oversampling does not introduce overfitting to repeated instances; the Gaussian noise injection (15% std) mitigates memorization.

### Mechanism 3
Dual XAI methods (SHAP + Morris sensitivity) provide complementary local and global interpretability without significantly impacting inference latency. SHAP computes per-prediction feature attributions using Shapley values; Morris sensitivity perturbs inputs globally to measure output variance. Both identified SrcMac, SIntPkt, DIntPkt as top predictors, converging on consistent importance rankings. Core assumption: Stakeholders require both instance-level explanations (for alert triage) and global feature rankings (for system auditing); computational overhead of XAI is acceptable offline or asynchronously.

## Foundational Learning

- Concept: **Decision Tree Induction and Pruning**
  - Why needed here: The core classifier is a depth-limited tree (max_depth=10). Understanding how trees split on features (e.g., SrcMac thresholds) is essential for interpreting why certain traffic is flagged.
  - Quick check question: Given a decision tree with max_depth=10 and min_samples_split=4, what is the maximum number of leaf nodes possible, and how does this constraint affect overfitting on noisy IoT data?

- Concept: **SHAP (SHapley Additive exPlanations) Values**
  - Why needed here: The framework relies on SHAP for local interpretability. Understanding additive feature attribution helps validate whether SrcMac dominance is genuine or an artifact.
  - Quick check question: For a binary classification decision tree, if SHAP assigns SrcMac a value of +0.7 for a given prediction, what does this signify about SrcMac's contribution to moving the prediction toward the anomaly class?

- Concept: **Morris Sensitivity Analysis (Elementary Effects Method)**
  - Why needed here: Global sensitivity analysis identifies which features most affect output variance across the input space. This complements SHAP's local view.
  - Quick check question: Morris method computes μ* (mean absolute elementary effect) and σ (standard deviation of effects). What does a high μ* with low σ indicate about a feature's influence?

## Architecture Onboarding

- Component map: Raw data -> Duplicate removal -> Label encoding -> IQR outlier capping -> Z-score normalization -> Chi-square filter -> Pearson correlation + Bonferroni -> RFE with CV -> Random oversampling -> Gaussian noise injection -> Decision Tree training -> SHAP + Morris sensitivity analysis

- Critical path:
  1. Raw WUSTL-IoT CSV -> preprocessing -> 41 features reduced to ~10-15 selected features
  2. Balanced training data -> decision tree training -> serialized model
  3. Inference: preprocessed sample -> tree traversal -> class prediction
  4. On-demand: prediction + SHAP values -> explanation output

- Design tradeoffs:
  - **Accuracy vs. Latency**: Decision Tree chosen over Random Forest/CatBoost for ~10-20× faster inference, accepting ~0.5% lower accuracy than ensemble alternatives
  - **Interpretability vs. Complexity**: Single tree enables direct rule extraction; ensembles would require surrogate models
  - **Oversampling vs. Synthetic Generation**: Random oversampling preserves real sample distributions but risks overfitting; SMOTE not tested
  - **Fixed vs. Adaptive Feature Set**: Feature selection is static; concept drift in deployment may require periodic retraining

- Failure signatures:
  - High false positives on new SrcMac values: Model may over-rely on SrcMac; if deployment introduces new MAC patterns, predictions may degrade
  - Class imbalance in production: Test set stratification matches training; real-world attack ratios may differ, requiring threshold tuning
  - XAI latency spikes: SHAP on large batches may exceed edge device memory; Morris pre-computation recommended for global insights

- First 3 experiments:
  1. **Baseline replication**: Reproduce the 99.91% test accuracy on WUSTL-IoT with the specified preprocessing and Decision Tree hyperparameters; log confusion matrix per class to verify Spoofing/Data Alteration recall
  2. **Feature ablation**: Remove SrcMac and retrain; measure accuracy drop to quantify dependency on the top feature
  3. **Noise robustness test**: Increase Gaussian noise injection from 15% to 30% std during training; evaluate whether cross-validation stability (std=0.0003) degrades

## Open Questions the Paper Calls Out
None

## Limitations
- Static feature selection may not adapt to concept drift in evolving IoT environments
- Performance validated only on single dataset without cross-dataset generalization testing
- Computational overhead of SHAP explanations for real-time edge deployment not quantified
- Random oversampling may introduce overfitting without synthetic sample diversity

## Confidence

- **High Confidence**: Model architecture design, preprocessing pipeline steps, and dataset characteristics are clearly specified and reproducible
- **Medium Confidence**: Performance metrics are well-documented but lack confidence intervals or statistical significance testing against baseline methods
- **Low Confidence**: Claims about real-time deployment feasibility, XAI computational overhead, and robustness to concept drift are not empirically validated

## Next Checks

1. **Cross-dataset generalization**: Evaluate the trained model on alternative IoT intrusion detection datasets (e.g., UNSW-NB15, CICIDS2017) to assess performance degradation and feature importance stability across domains.

2. **Edge deployment profiling**: Measure inference latency and memory consumption on actual edge hardware (e.g., Raspberry Pi 4) for both prediction and SHAP explanation generation to verify resource constraints claims.

3. **Concept drift simulation**: Apply gradual feature distribution shifts to the WUSTL-IoT test set and measure accuracy decay over time to quantify model robustness beyond the static validation period.