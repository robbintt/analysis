---
ver: rpa2
title: 'Deep Learning Meets Oversampling: A Learning Framework to Handle Imbalanced
  Classification'
arxiv_id: '2502.06878'
source_url: https://arxiv.org/abs/2502.06878
tags:
- data
- oversampling
- learning
- function
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses class imbalance in classification tasks by
  proposing a novel deep learning-based oversampling framework called AutoSMOTE. The
  framework generates synthetic minority samples in a data-driven manner through a
  composition of discrete decision criteria, including oversampling participation,
  k-nearest neighbors selection, and aggregation function choice.
---

# Deep Learning Meets Oversampling: A Learning Framework to Handle Imbalanced Classification

## Quick Facts
- arXiv ID: 2502.06878
- Source URL: https://arxiv.org/abs/2502.06878
- Reference count: 40
- Primary result: AutoSMOTEcohort achieves best overall ranking (3.8) on 8 imbalanced classification datasets

## Executive Summary
This paper introduces AutoSMOTE, a deep learning-based oversampling framework that addresses class imbalance by making the oversampling process learnable through discrete decision criteria. The framework generates synthetic minority samples by composing three discrete decisions: oversampling participation, k-nearest neighbor selection, and aggregation function choice. Two variants are proposed - AutoSMOTEself for instance-level decisions and AutoSMOTEcohort for group-level decisions. The method uses MLPs with Gumbel-Softmax to maintain differentiability while making discrete choices. Experimental results show superior performance over state-of-the-art methods with favorable runtime efficiency.

## Method Summary
AutoSMOTE formulates oversampling as a composition of three learnable discrete decisions: whether to oversample a minority instance, how many k-nearest neighbors to use (1-6), and which aggregation function to apply (linear, min, max, sum, average, weighted average). These decisions are implemented using small MLPs that output logits, which are then converted to differentiable categorical samples via Gumbel-Softmax. The framework operates end-to-end with a classifier MLP, jointly optimizing both the oversampling decisions and classification performance. AutoSMOTEcohort extends this by grouping minority instances and applying shared decisions to each cohort, reducing hypothesis complexity. Training uses Adam optimizer (LR=0.05) with categorical cross-entropy loss over 200 epochs.

## Key Results
- AutoSMOTEcohort achieves the best overall ranking (3.8) across 8 datasets
- Competitive F1-scores across all datasets compared to state-of-the-art oversampling methods
- Lower VC dimension and better generalization compared to standard MLPs
- Favorable runtime efficiency compared to complex oversampling approaches
- MLP baseline reaches near-zero training error (overfitting), while AutoSMOTE variants maintain better test performance

## Why This Works (Mechanism)

### Mechanism 1: Discrete Decision Criteria with Differentiable Approximation
Converting oversampling into learnable discrete decisions reduces the parameter search space while maintaining end-to-end trainability. Three discrete decisions are modeled per instance: oversampling participation (binary), k-nearest neighbor count (1-6), and aggregation function selection (6 options). Each decision uses an MLP to produce logits, then Gumbel-Softmax sampling provides differentiable gradients via the reparameterization trick.

### Mechanism 2: VC Dimension Reduction for Generalization
Constraining the hypothesis class through discrete compositions yields lower VC dimension than unconstrained MLPs, improving generalization under imbalanced conditions. Theorem 2 establishes that H_AutoSMOTEcohort ⊂ H_AutoSMOTEself ⊂ H_MLP, implying ordered VC dimensions. Lower VC dimension reduces the generalization error bound term O(√(VCdim·log(N)/N)).

### Mechanism 3: Cohort Grouping for Shared Decision Patterns
Grouping minority instances and applying shared decisions captures broader patterns while further constraining hypothesis complexity. AutoSMOTEcohort learns group assignments via MLP, then applies the same decision criteria to all instances within a group. This pools statistical strength across similar instances.

## Foundational Learning

- Concept: **SMOTE (Synthetic Minority Over-sampling Technique)**
  - Why needed here: AutoSMOTE builds on SMOTE's linear interpolation as one aggregation option; understanding the baseline clarifies what's being improved.
  - Quick check question: Given minority sample x and neighbor x*, can you write the SMOTE interpolation formula?

- Concept: **Gumbel-Softmax Reparameterization**
  - Why needed here: Critical for making discrete decisions differentiable; without this, backpropagation through categorical sampling is impossible.
  - Quick check question: Why does adding Gumbel noise to logits before softmax enable gradient flow?

- Concept: **VC Dimension and Generalization Bounds**
  - Why needed here: Theoretical justification for why constrained decision sets generalize better than unconstrained MLPs on imbalanced data.
  - Quick check question: If Model A has VCdim=100 and Model B has VCdim=50 with equal training error, which has lower generalization error bound?

## Architecture Onboarding

- Component map: Minority batch → [MLP_1: participation] → Gumbel-Softmax → binary mask → [MLP_2: k-selection] → Gumbel-Softmax → k value → [MLP_3: aggregation] → Gumbel-Softmax → agg function → [Optional: MLP_group for cohort variant] → Masked instances + k-NN lookup + agg function → Synthetic samples → Original batch ∪ Synthetic samples → Classifier MLP → Loss → Joint backprop

- Critical path: The Gumbel-Softmax temperature τ controls discreteness. Too high → soft/blurry decisions. Too low → gradients vanish. Paper uses τ ∈ (0, ∞) but doesn't specify annealing schedule.

- Design tradeoffs:
  - AutoSMOTEself vs cohort: Instance-level offers finer control but higher VC dimension; cohort reduces complexity but risks over-generalization
  - Predefined decisions vs learned: Current design uses 6 aggregation functions; extending this set increases expressiveness but also search space
  - Assumption: Temperature scheduling strategy not detailed; may require tuning per dataset

- Failure signatures:
  - Synthetic samples collapse to duplicates → participation decisions all "no" or aggregation collapsing
  - Training error doesn't decrease → Gumbel-Softmax temperature too low (gradient vanishing)
  - Large gap between train/test performance → VC dimension still too high; consider more cohorts or fewer decision options

- First 3 experiments:
  1. **Sanity check on a toy imbalanced dataset (IR=10)**: Verify that AutoSMOTE can recover known minority regions. Compare AutoSMOTEself vs AutoSMOTEcohort on 2D synthetic data where ground truth decision boundaries are visible.
  2. **Ablation on temperature τ**: Fix all hyperparameters, vary τ ∈ {0.1, 0.5, 1.0, 2.0, 5.0}. Monitor gradient magnitudes and decision entropy. Identify range where decisions are discrete enough but gradients still flow.
  3. **Cohort count sensitivity**: On the Glass dataset (214 instances, 6 classes, high imbalance), vary cohort count from 1-7 as paper suggests. Plot F1-score vs cohort count to identify if there's a clear optimum or if performance is robust to this hyperparameter.

## Open Questions the Paper Calls Out

- Can the framework incorporate additional interpretable design criteria beyond the current three (oversampling participation, k-NN selection, and aggregation function) to enhance representation expressiveness without increasing training instability?
- Can AutoSMOTE effectively generalize to high-dimensional unstructured data domains like image or text classification where semantic continuity differs from tabular data?
- Is there a theoretical or adaptive mechanism to determine the optimal number of cohorts in AutoSMOTEcohort dynamically, rather than relying on manual hyperparameter search?

## Limitations

- The theoretical claims hinge on the assumption that training errors are approximately equal across methods, which isn't empirically validated
- The cohort grouping mechanism lacks detailed specification of the MLP-based group assignment process
- Temperature scheduling for Gumbel-Softmax, critical for training stability, is not addressed
- The predefined decision space (6 aggregation functions, k=1-6) may constrain the framework's ability to discover optimal oversampling strategies

## Confidence

- **High confidence:** Empirical performance comparisons on 8 datasets showing AutoSMOTEcohort achieves best overall ranking (3.8) with competitive F1-scores across benchmarks
- **Medium confidence:** Theoretical VC dimension analysis establishing ordered generalization bounds, though this depends on the training error equality assumption
- **Medium confidence:** Mechanism of discrete decision criteria with Gumbel-Softmax approximation, though practical temperature tuning requirements are not specified

## Next Checks

1. **Temperature sensitivity analysis:** Systematically vary Gumbel-Softmax temperature (τ ∈ {0.1, 0.5, 1.0, 2.0, 5.0}) and monitor gradient flow and decision entropy to identify optimal range
2. **Cohort count sensitivity on Glass dataset:** Vary cohort count from 1-7 on the 214-instance Glass dataset to identify performance trends and potential overfitting
3. **Training error validation:** Measure and compare training errors across AutoSMOTE variants and baseline MLPs to validate the theoretical assumption underlying VC dimension comparisons