---
ver: rpa2
title: 'Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge'
arxiv_id: '2508.06709'
source_url: https://arxiv.org/abs/2508.06709
tags:
- self-bias
- claude
- scores
- completions
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a statistical method to quantify self-bias\u2014\
  the tendency of LLMs to assign higher scores to their own outputs compared to others\u2014\
  when serving as evaluators. The core innovation is a regression framework that controls\
  \ for completion quality (via human reference scores) and systematically separates\
  \ genuine performance differences from evaluator favoritism."
---

# Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2508.06709
- Source URL: https://arxiv.org/abs/2508.06709
- Reference count: 40
- One-line primary result: A regression framework detects self-bias in LLM-as-a-judge evaluations, with GPT-4o and Claude 3.5 Sonnet showing significant positive self-bias while Llama 3 8B shows negative self-bias on certain dimensions.

## Executive Summary
This paper introduces a statistical method to quantify self-bias—the tendency of LLMs to assign higher scores to their own outputs compared to others—when serving as evaluators. The core innovation is a regression framework that controls for completion quality (via human reference scores) and systematically separates genuine performance differences from evaluator favoritism. The method explicitly models self-bias and family-bias coefficients, enabling formal statistical inference with confidence intervals. On a large dataset spanning nine LLM judges and six evaluation dimensions, the authors find that models like GPT-4o and Claude 3.5 Sonnet exhibit significant positive self-bias, while Llama 3 8B shows negative self-bias on certain dimensions. Family-bias is also detected, with GPT and Claude models favoring outputs from their own model families.

## Method Summary
The method uses a regression framework where judge ratings are decomposed into alignment with reference scores, judge-specific intercepts, self-bias offsets (when judge evaluates own output), and family-bias offsets (when judge evaluates same-family output). The regression equation S̃idmj = α + δj + βj·Sidm + γj·1j(m) + λF(j)·1F(j)(F(m)) + ηd + ϵidmj models judge ratings (S̃) for dimension d, prompt i, and model m, with Sidm as the reference score (human-aggregated ratings). Self-bias is captured by γj (the coefficient on indicator 1j(m)), while family-bias is captured by λF(j) (coefficient on indicator 1F(j)(F(m))). The model uses robust standard errors and 90% confidence intervals for inference, with significance assessed at α=0.10.

## Key Results
- GPT-4o and Claude 3.5 Sonnet exhibit significant positive self-bias (γj > 0) across multiple evaluation dimensions
- Llama 3 8B shows negative self-bias (γj < 0) on certain dimensions like faithfulness
- Family-bias detected: Claude and GPT models favor same-family outputs (λF(j) > 0), while Llama and Mistral models show no significant family-bias
- Self-bias persists across robustness checks including length control, alternative reference scores, and ordinal logit regression
- Small bias magnitudes can meaningfully affect model rankings in LLM-as-a-judge evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regression framework isolates self-bias by controlling for genuine completion quality through reference scores, preventing conflation of true performance differences with evaluator favoritism.
- Mechanism: A linear model (Equation 1) decomposes judge ratings into: alignment with reference scores (βj), judge-specific intercepts (δj), self-bias offset (γj), and family-bias offset (λF(j)). The self-bias coefficient captures the vertical shift in scores when j=m (judge evaluating own output), conditioned on the reference score being held constant.
- Core assumption: Human reference scores provide an unbiased ground-truth measure of completion quality; systematic biases (e.g., length preference) are shared between human and judge ratings.
- Evidence anchors:
  - [abstract]: "Our method models the difference in the scoring distribution that LLM-as-a-judge assigns to its own completions compared to other models, while accounting for the underlying quality of the completions provided by an independent, third-party judge"
  - [section 3, Figure 1]: Demonstrates three scenarios where self-bias is masked or exaggerated without quality control
  - [corpus]: "Bridging Human and LLM Judgments" (arXiv:2508.12792) addresses similar alignment challenges but uses a different bridging framework
- Break condition: If human annotators systematically differ from the true quality dimension (e.g., prefer verbosity while judges do not), self-bias estimates become unreliable. The paper acknowledges this in limitations.

### Mechanism 2
- Claim: Family-bias detection requires isolating same-family effects while zeroing out self-evaluation cases to avoid double-counting.
- Mechanism: The indicator 1F(j)(F(m)) activates when judge and model share a family but is explicitly set to 0 when j=m. This creates a hierarchical bias structure: γj captures own-output favoritism, λF(j) captures same-family-other-model favoritism.
- Core assumption: Models within a family share a characteristic "evaluation lens" (architecture, training data, or style) that produces systematic scoring patterns.
- Evidence anchors:
  - [section 3]: "To isolate self-bias from family favoritism, we set 1F(j)(F(j)) = 0"
  - [section 5.2]: "Claude and GPT judges tend to give higher scores to completions of other models within the same family"
  - [corpus]: "Silencer" (arXiv:2505.20738) validates inflated performance on self-generated benchmarks but doesn't address family-level effects
- Break condition: Family definitions must be meaningful; if models share only superficial characteristics, family-bias estimates will be noisy. The paper uses vendor-based family groupings (Claude, GPT, Llama, Mistral).

### Mechanism 3
- Claim: Robustness checks using alternative reference sources (cross-family LLM averages) can validate that findings aren't artifacts of human annotation idiosyncrasies.
- Mechanism: For each model family, exclude all models from that family, compute average ratings from the remaining families as alternative reference scores. This breaks circularity while maintaining independence.
- Core assumption: Aggregated cross-family LLM ratings approximate ground-truth quality sufficiently for bias detection.
- Evidence anchors:
  - [section 6.2]: "Under this alternative reference scoring scheme, our estimates of self- and family-bias remain qualitatively similar"
  - [Figure 9]: Shows consistent patterns across human, Claude, Llama 3, GPT, and Mistral reference scores
  - [corpus]: Corpus evidence on cross-judge reliability is sparse; no direct validation of this specific approach found
- Break condition: If all model families share a common bias direction, cross-family averaging will not provide an independent reference. The paper doesn't test this explicitly.

## Foundational Learning

- Concept: **Ordinary Least Squares (OLS) with Robust Standard Errors**
  - Why needed here: The regression model estimates bias coefficients, and White/Huber-robust standard errors ensure valid inference even under heteroskedasticity (variance differs across completion quality levels).
  - Quick check question: Why would judge rating variance differ for low-quality vs. high-quality completions, and why does this matter for confidence intervals?

- Concept: **Indicator Variables and Interaction Effects**
  - Why needed here: The self-bias (1j(m)) and family-bias (1F(j)(F(m)) terms are indicator functions that partition the data into treatment and control groups within the regression.
  - Quick check question: If you fit the model without the self-bias indicator but kept family-bias, what systematic pattern would remain in the residuals for self-evaluation cases?

- Concept: **Likert Scale Ordinality vs. Cardinality**
  - Why needed here: The paper converts ordinal ratings (1-5, 1-7 scales) to numerical scores; this assumes roughly equal intervals between scale points. Robustness checks with ordinal logit regression test this assumption.
  - Quick check question: What distortions could arise if the psychological distance between "somewhat helpful" and "very helpful" differs from "very helpful" to "above and beyond"?

## Architecture Onboarding

- Component map: 596 prompts → 9 models → 6 dimensions → 3 human raters → 9 LLM judge scores → Reference score aggregation → OLS regression with bias indicators → Bias coefficient estimation with robust SEs → 90% CI inference

- Critical path: 1. Collect completions from all models → 2. Obtain human annotations → 3. Aggregate to reference scores → 4. Collect LLM-as-judge ratings → 5. Fit regression per judge → 6. Extract γj and λF(j) with CIs → 7. Run robustness variants → 8. Report significant bias directions

- Design tradeoffs:
  - **Human vs. LLM reference scores**: Human annotations assumed unbiased but introduce subjective variability; LLM references avoid this but risk circularity if not carefully excluded
  - **Pooled vs. per-dimension models**: Pooling increases sample size but masks dimension-specific bias patterns (e.g., Llama 3 8B's negative faithfulness bias)
  - **90% vs. 95% CI**: Paper uses 10% significance level, accepting more false positives for detection power given small effect magnitudes

- Failure signatures:
  - **Negative self-bias** (observed in Llama 3 8B): Models excessively critical of own outputs, potentially due to instruction-tuning on human-preference data
  - **High human-LLM correlation but high self-bias**: Alignment with humans doesn't preclude self-favoritism (GPT-4o shows both ρ>0.4 correlation and significant γj)
  - **Dimension-specific bias reversals**: Faithfulness shows different bias patterns than helpfulness—don't aggregate blindly

- First 3 experiments:
  1. Replicate the main regression on a held-out prompt set to test generalization; check if γj confidence intervals remain stable
  2. Run the analysis with pairwise comparison judgments (A vs. B) instead of absolute scores; verify if self-bias manifests as higher win rates
  3. Intervene by adding explicit instructions to LLM-as-judge prompts: "Rate impartially regardless of which model generated the output"; measure bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms cause negative self-bias in models like Llama 3 8B, and can white-box analysis of training data and processes explain this behavior?
- Basis in paper: [explicit] The paper states as future work: "studying the cause and extend of negative self-bias using a white-box LLM-as-a-judge, where we know the training data and process."
- Why unresolved: The current study uses black-box models without access to training data or methodology, making causal analysis impossible.
- What evidence would resolve it: Training data analysis and ablation studies on open models showing which training components correlate with negative vs. positive self-bias.

### Open Question 2
- Question: Does data contamination in training datasets explain why models show different self-bias levels across task types (e.g., summarization vs. open-ended QA)?
- Basis in paper: [explicit] The authors observe higher self-bias for open-ended QA than summarization and note: "This introduces the possibility of data contamination... Unfortunately we cannot test this hypothesis, as we need the training data and the instruction-tuning methodology for each of these models."
- Why unresolved: Training data for proprietary models is unavailable; even for open models, isolating contamination effects is methodologically challenging.
- What evidence would resolve it: Controlled experiments comparing self-bias on known-contaminated vs. uncontaminated evaluation sets using models with documented training corpora.

### Open Question 3
- Question: Can this regression-based framework be generalized to quantify other bias types in LLM-as-a-judge, such as positional bias, verbosity bias, or style bias?
- Basis in paper: [explicit] The conclusions state: "Our proposed approach can be applied to measure other types of biases, as long as there is a distinct control-group of completions where the bias does not apply."
- Why unresolved: The paper only validates the method for self-bias and family-bias; other bias types have different structural characteristics that may require methodological adaptation.
- What evidence would resolve it: Successful application of the framework to quantify positional, verbosity, or style bias with validation against ground-truth manipulation experiments.

### Open Question 4
- Question: Why do GPT and Claude model families exhibit positive family-bias while Llama and Mistral families do not?
- Basis in paper: [inferred] The results show asymmetry: "Claude and GPT judges tend to give higher scores to completions of other models within the same family... Llama and Mistral models do not exhibit such bias." The paper does not investigate mechanisms underlying this family-specific pattern.
- Why unresolved: Without access to training data, architecture details, and fine-tuning procedures, determining whether this stems from training corpus overlap, shared evaluation heuristics, or architectural similarity remains speculative.
- What evidence would resolve it: Cross-family experiments with controlled training configurations, or analysis of embedding-space similarity between family members' outputs correlated with bias magnitude.

## Limitations
- Reference score validity: Human ratings are assumed unbiased measures of completion quality, but systematic preference differences (e.g., verbosity) between humans and judges could distort bias estimates
- Family definition arbitrariness: Model family groupings are based on vendor attribution, which may not capture true architectural or training similarities that drive family-bias
- Cross-family reference robustness: Alternative reference scores from LLM averages show consistent patterns but haven't been validated against ground-truth quality

## Confidence
- **Self-bias detection framework**: High - The regression design effectively isolates bias while controlling for quality, with robustness across multiple reference sources
- **Specific bias magnitudes**: Medium - Effect sizes are small and may be sensitive to reference score normalization and scale transformations
- **Family-bias interpretation**: Medium - The hierarchical bias structure is well-defined, but the psychological mechanism linking family membership to scoring patterns remains speculative

## Next Checks
1. **Human reliability audit**: Compute inter-annotator agreement (ICC/Krippendorff's alpha) per dimension; test whether low-agreement dimensions show higher bias variance
2. **Adversarial prompting intervention**: Modify LLM-as-a-judge prompts with explicit impartiality instructions; measure bias reduction and test if effects generalize across model families
3. **Counterfactual simulation**: Generate synthetic datasets with known bias parameters; verify that the regression framework recovers these parameters accurately across different sample sizes and quality distributions