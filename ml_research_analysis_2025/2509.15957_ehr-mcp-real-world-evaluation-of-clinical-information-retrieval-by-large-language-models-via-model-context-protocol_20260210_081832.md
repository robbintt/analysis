---
ver: rpa2
title: 'EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large
  Language Models via Model Context Protocol'
arxiv_id: '2509.15957'
source_url: https://arxiv.org/abs/2509.15957
tags:
- patient
- clinical
- date
- data
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the capability of a large language model\
  \ (GPT-4.1) to autonomously retrieve clinically relevant information from an electronic\
  \ health record (EHR) using the Model Context Protocol (MCP) in a real hospital\
  \ setting. Custom MCP tools were developed to interface with the hospital\u2019\
  s data warehouse, and a LangGraph ReAct agent was used to manage tool selection\
  \ and execution for six clinically relevant tasks derived from infection control\
  \ team use cases."
---

# EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol

## Quick Facts
- **arXiv ID**: 2509.15957
- **Source URL**: https://arxiv.org/abs/2509.15957
- **Reference count**: 36
- **Primary result**: GPT-4.1 achieved near-perfect accuracy for simple EHR data retrieval tasks and high accuracy for multi-step reasoning tasks using MCP tools

## Executive Summary
This study evaluated the capability of GPT-4.1 to autonomously retrieve clinically relevant information from electronic health records using the Model Context Protocol (MCP) in a real hospital setting. Custom MCP tools were developed to interface with the hospital's data warehouse, and a LangGraph ReAct agent was used to manage tool selection and execution for six clinically relevant tasks. The framework demonstrated strong technical feasibility for MCP-based EHR access, with the LLM consistently selecting and invoking the correct tools across all task types.

Results showed near-perfect accuracy for simple retrieval tasks and high accuracy for complex multi-step reasoning tasks. However, errors primarily occurred due to incorrect argument specification and misinterpretation of tool responses, especially in time-dependent queries. While the study provides a secure, reproducible infrastructure for clinical data access, challenges remain in handling complex queries and ensuring robustness in ambiguous situations.

## Method Summary
The study developed custom MCP tools to interface with a hospital data warehouse and implemented a LangGraph ReAct agent to manage tool selection and execution. Six clinically relevant tasks were derived from infection control team use cases, including simple data retrieval (latest body weight, antibiotic lists) and complex reasoning tasks (creatinine clearance calculation). GPT-4.1 was evaluated on its ability to select appropriate tools, invoke them correctly, and interpret their responses. The evaluation focused on technical execution accuracy rather than clinical correctness of retrieved information.

## Key Results
- Near-perfect accuracy (99-100%) for simple data retrieval tasks such as fetching latest body weight and listing antibiotics
- High accuracy (85-95%) for multi-step reasoning tasks requiring complex tool interactions
- Errors primarily due to incorrect argument specification and misinterpretation of tool responses, particularly in time-dependent queries
- LLM consistently selected appropriate tools across all task types, demonstrating robust tool selection capabilities

## Why This Works (Mechanism)
The EHR-MCP framework succeeds by providing structured tool interfaces that enable the LLM to interact with complex clinical data systems in a controlled, interpretable manner. The LangGraph ReAct agent architecture allows the model to reason about tool selection and execution steps, while the MCP protocol ensures standardized communication between the LLM and hospital data systems. This approach bridges the gap between natural language understanding and structured data retrieval, enabling the LLM to perform clinically relevant tasks through a series of well-defined API calls.

## Foundational Learning
**Model Context Protocol (MCP)**: Standardized interface protocol for LLM-tool communication
- *Why needed*: Enables consistent, secure interaction between LLMs and external systems
- *Quick check*: Verify tool registration and argument validation work correctly

**LangGraph ReAct Agent**: Framework combining reasoning and action for tool use
- *Why needed*: Manages complex tool selection and execution sequences
- *Quick check*: Test agent's ability to handle multi-step reasoning tasks

**Tool Selection Accuracy**: LLM's ability to choose correct tools for given tasks
- *Why needed*: Critical for effective task completion in clinical settings
- *Quick check*: Measure tool selection accuracy across task complexity levels

**Argument Specification**: Process of providing correct parameters to invoked tools
- *Why needed*: Ensures accurate data retrieval from clinical systems
- *Quick check*: Validate argument parsing and error handling

**Response Interpretation**: LLM's ability to understand and use tool output
- *Why needed*: Enables multi-step reasoning and complex task completion
- *Quick check*: Test interpretation accuracy with varied response formats

## Architecture Onboarding

**Component Map**: User Query -> LangGraph ReAct Agent -> MCP Tool Registry -> Hospital Data Warehouse -> Response

**Critical Path**: User Query → Tool Selection → Argument Specification → Tool Invocation → Response Interpretation → Final Answer

**Design Tradeoffs**: The framework prioritizes security and reproducibility over flexibility, using predefined MCP tools rather than allowing arbitrary API access. This limits task scope but ensures controlled data access and auditable workflows.

**Failure Signatures**: Common failures include incorrect tool arguments, misinterpretation of tool responses (especially for time-dependent data), and inability to handle ambiguous or complex clinical queries requiring external knowledge.

**3 First Experiments**:
1. Test tool selection accuracy on simple retrieval tasks with varied input formats
2. Evaluate argument specification accuracy across different tool types
3. Assess response interpretation accuracy for complex multi-step tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Single hospital setting with specific data warehouse structure limits generalizability
- Tool invocation accuracy doesn't guarantee clinical accuracy of retrieved information
- Challenges remain with complex, time-dependent queries and ambiguous situations

## Confidence

**High**: Technical feasibility of MCP-based EHR access demonstrated
**High**: Tool selection accuracy consistently achieved across task types
**Medium**: Claims about practical utility in clinical workflows (limited task scope)
**Low**: Broader assertions about clinical safety and generalizability beyond tested environment

## Next Checks
1. External validation across multiple hospital systems with different EHR architectures
2. Clinical accuracy assessment by domain experts comparing LLM-retrieved information against gold-standard documentation
3. Longitudinal testing in live clinical environment to evaluate reliability and safety under realistic conditions