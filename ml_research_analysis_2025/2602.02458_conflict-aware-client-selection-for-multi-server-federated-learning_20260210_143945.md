---
ver: rpa2
title: Conflict-Aware Client Selection for Multi-Server Federated Learning
arxiv_id: '2602.02458'
source_url: https://arxiv.org/abs/2602.02458
tags:
- client
- learning
- conflict
- selection
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of client selection conflicts
  in multi-server federated learning (FL), where overlapping client coverage across
  servers leads to resource contention, bandwidth conflicts, and training failures.
  To resolve this, the authors propose a decentralized reinforcement learning framework
  with conflict risk prediction (RL-CRP).
---

# Conflict-Aware Client Selection for Multi-Server Federated Learning

## Quick Facts
- arXiv ID: 2602.02458
- Source URL: https://arxiv.org/abs/2602.02458
- Reference count: 33
- Primary result: RL-CRP achieves 67.68% IID and 61.32% non-IID accuracy on CIFAR-10, outperforming baselines

## Executive Summary
This paper addresses client selection conflicts in multi-server federated learning where overlapping server coverage leads to resource contention and training failures. The authors propose RL-CRP, a decentralized reinforcement learning framework with conflict risk prediction using a categorical hidden Markov model. The system predicts which clients are likely to be selected by other servers and uses fairness-aware reward shaping to promote balanced long-term participation. Experiments show significant improvements in accuracy and conflict reduction compared to baseline methods.

## Method Summary
RL-CRP implements a decentralized multi-server FL system where each server independently selects clients using soft actor-critic (SAC) reinforcement learning. The key innovation is a conflict risk prediction (CRP) module that uses a categorical hidden Markov model to estimate conflict probabilities from sparse historical selection data. Each server observes local latencies and predicted conflicts, then selects client subsets to maximize a reward combining negative latency, negative conflict penalty, and fairness term. The fairness term encourages balanced participation across clients using the inverse coefficient of variation. Bandwidth allocation uses a water-filling algorithm based on channel quality. The framework operates without inter-server communication, relying on accurate conflict predictions for implicit coordination.

## Key Results
- RL-CRP achieves 67.68% test accuracy under IID data partition and 61.32% under non-IID partition on CIFAR-10
- Outperforms baseline methods including FedAvg and ENSAC in both accuracy and conflict reduction
- Demonstrates effective fairness promotion while maintaining low conflict rates through careful reward tuning
- Shows scalability with increasing server count while maintaining performance advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorical hidden Markov models can estimate conflict probability from sparse, outdated client selection sequences.
- Mechanism: The CRP module computes forward probabilities from incomplete historical sequences, propagates hidden state distribution via transition matrix exponentiation, then estimates conflict probability using emission probability for conflict events. Incremental Baum-Welch updates model parameters as new observations arrive.
- Core assumption: Client selection patterns exhibit temporal structure captured by hidden states; historical behavior predicts near-future conflicts despite sparse observations.
- Evidence anchors: [abstract] "estimates the likelihood of client selection conflicts using a categorical hidden Markov model"; [section III-A] Equations 3-6 define forward algorithm, state propagation, and conflict probability computation; [corpus] Weak corpus support—neighbor papers address client selection but not HMM-based conflict prediction specifically
- Break condition: If client selection becomes purely random or state dynamics change abruptly, HMM predictions degrade; if observation sparsity exceeds model's capacity to interpolate, predictions become unreliable.

### Mechanism 2
- Claim: Fairness-aware reward shaping improves long-term model accuracy by encouraging participation from underrepresented clients.
- Mechanism: The reward function r_m = -L_m - C_m + α·f includes a fairness term f = tanh(μ/(δ+ε)), where μ is average training rounds per client and δ is standard deviation. This approximates the inverse coefficient of variation, rewarding equitable participation. SAC maximizes cumulative discounted reward, learning policies that balance latency, conflict avoidance, and fairness.
- Core assumption: Balanced client participation improves model generalization; the tanh formulation provides sufficient gradient signal for RL optimization.
- Evidence anchors: [abstract] "fairness-aware reward mechanism to promote balanced long-term client participation"; [section III-B] Equation 15-16 define reward and fairness term; equations 17-19 define SAC loss functions; [corpus] [Balancing Client Participation in FL Using AoI] supports importance of participation balance for convergence under heterogeneity
- Break condition: If α is set too high, fairness dominates and increases conflicts; if too low, clients with poor connectivity never participate, degrading non-IID performance.

### Mechanism 3
- Claim: Decentralized decision-making without inter-server communication can reduce conflicts when paired with accurate conflict prediction.
- Mechanism: Each server maintains independent SAC policy and value networks, observing only local latency L_m and predicted conflict probabilities P_m. Servers select client subsets independently using learned policies. Conflict prediction module provides implicit coordination by estimating which clients other servers likely selected.
- Core assumption: Conflict predictions are sufficiently accurate that local decisions avoid contention without explicit server-to-server communication.
- Evidence anchors: [abstract] "decentralized reinforcement learning framework with conflict risk prediction"; [section III-B] "each server uses its policy network to select actions based on local states, enabling scalable and independent decision-making without inter-server communication"; [corpus] [FedOC: Multi-Server FL with Overlapping Client Relays] addresses overlapping coverage but uses relay-based approach, not decentralized RL
- Break condition: If prediction accuracy drops (e.g., rapid topology changes, new servers joining), decentralized decisions may increase rather than decrease conflicts.

## Foundational Learning

- Concept: Hidden Markov Models (forward-backward algorithm, Baum-Welch)
  - Why needed here: Core of conflict prediction; must understand state inference from partial observations and incremental parameter updates.
  - Quick check question: Given a sequence of binary observations [0,1,0] and a 2-state HMM with known parameters, can you compute the posterior probability that the next observation is 1?

- Concept: Soft Actor-Critic (SAC) reinforcement learning
  - Why needed here: SAC solves the client selection MDP; understanding entropy-regularized RL and automatic temperature adjustment is essential for debugging training.
  - Quick check question: Why does SAC use two Q-networks and a minimum operator in the Bellman backup?

- Concept: Multi-server federated learning architecture
  - Why needed here: Context for why conflicts arise—overlapping coverage regions create contention for shared clients.
  - Quick check question: In a system with M=3 servers and N=50 clients, if 20 clients are in overlapping regions, what factors determine maximum simultaneous conflict-free selections?

## Architecture Onboarding

- Component map:
  - CRP Module -> RL Agent -> Bandwidth Allocator -> FL Training Loop
  - Categorical HMM per server maintains transition matrix A, emission matrix B, initial distribution π
  - SAC agent with policy network π_ϕ, two Q-networks Q_θ1, Q_θ2, target networks
  - Water-filling algorithm assigns bandwidth to selected clients sorted by channel quality

- Critical path:
  1. Observe current latencies L_m → 2. Compute conflict probabilities P_m via HMM → 3. Sample client selection from policy π_ϕ(s_m) → 4. Allocate bandwidth via water-filling → 5. Execute FL round → 6. Compute reward → 7. Update SAC networks and HMM parameters

- Design tradeoffs:
  - Fairness vs. conflicts: Higher α improves accuracy but increases conflict count (Fig. 4a)
  - HMM complexity vs. prediction accuracy: More hidden states K may improve prediction but increase computation
  - Decentralization vs. optimality: No inter-server communication limits coordination but improves scalability

- Failure signatures:
  - Diverging Q-values: Check reward scale; latency/conflict terms may dominate fairness term
  - High conflict rate: HMM predictions may be stale; consider increasing update frequency or observation window
  - Slow convergence: SAC temperature η may need tuning; entropy target H affects exploration

- First 3 experiments:
  1. Reproduce IID vs. non-IID accuracy comparison (Fig. 2) with N=50, M=2 to validate implementation; expect ~67% IID, ~61% non-IID.
  2. Ablate fairness term (α=0) and measure conflict count; should match "RL-CRP w/o fairness" baseline with lowest conflicts but reduced accuracy.
  3. Scale server count M from 2 to 4 while holding clients constant; verify conflict increase trend (Fig. 4b) and confirm RL-CRP maintains advantage over ENSAC.

## Open Questions the Paper Calls Out

- Question: How can the RL-CRP framework be adapted for resource-intensive applications like Large Language Model (LLM) fine-tuning, where communication costs and parameter sizes differ significantly from the current CNN setup?
  - Basis in paper: [explicit] The conclusion explicitly states a future direction to "extend our method to... LLM systems."
  - Why unresolved: The current validation is limited to CNNs on CIFAR-10, whereas LLMs involve massive gradient updates that may alter the bandwidth contention dynamics and latency assumptions used in the current reward function.
  - What evidence would resolve it: Successful convergence and efficiency metrics from experiments applying RL-CRP to LLM fine-tuning tasks (e.g., using parameter-efficient methods like LoRA) in a multi-server environment.

- Question: How can the inherent trade-off between fairness and conflict frequency be optimized to prevent fairness mechanisms from destabilizing the communication network?
  - Basis in paper: [explicit] The results section observes that "incorporating fairness tends to increase client selection conflicts," noting a trade-off between model accuracy (via fairness) and system stability (via conflict avoidance).
  - Why unresolved: The current implementation improves accuracy by forcing participation of "worse" clients, which inherently increases contention; the paper does not propose a mechanism to decouple these two objectives.
  - What evidence would resolve it: A Pareto optimization analysis or a modified reward function that maintains high fairness metrics without incurring the observed spike in conflict counts.

- Question: Does the decentralized HMM-based prediction maintain accuracy in highly dynamic environments where client availability or mobility renders historical data obsolete?
  - Basis in paper: [inferred] The system model assumes a static set of clients associated with servers, and the HMM relies on sparse historical sequences. Real-world edge scenarios often involve high client mobility or churn, which could violate the Markov property assumptions used for prediction.
  - Why unresolved: The paper does not evaluate the framework's robustness against non-stationary client behaviors or rapid changes in server coverage topology.
  - What evidence would resolve it: Simulation results evaluating the conflict prediction accuracy and convergence speed under high client mobility or variable dropout rates.

## Limitations
- Sparse historical data sensitivity: HMM-based conflict prediction depends critically on quality and recency of historical selection sequences; performance degrades with rapid pattern changes
- Decentralized coordination effectiveness: Implicit coordination through shared conflict predictions may break down under certain conditions, potentially increasing conflicts
- Fairness term scaling sensitivity: Fixed fairness weight α=100 may not generalize across different deployment scenarios with varying network conditions

## Confidence
- High confidence: Conflict reduction effectiveness (67.68% IID, 61.32% non-IID accuracy) - supported by controlled CIFAR-10 experiments with clear baselines
- Medium confidence: HMM-based conflict prediction mechanism - theoretical formulation is sound, but sparse data effectiveness requires further validation
- Medium confidence: Decentralized coordination without communication - theoretically plausible but practical limitations under dynamic conditions
- Low confidence: Generalizability to real-world wireless conditions - simulation parameters are reasonable but don't capture full complexity of actual edge networks

## Next Checks
1. **Ablation study on historical data quality**: Vary the observation window d and data sparsity level to measure how HMM prediction accuracy degrades. Compare conflict rates when using complete vs. sparse vs. outdated selection histories.

2. **Cross-scenario generalization test**: Evaluate RL-CRP on a different dataset (e.g., FEMNIST) and with varying numbers of servers (M=3, M=4) to verify the claimed scalability and fairness benefits hold beyond the original CIFAR-10 setup.

3. **Real-time adaptation evaluation**: Simulate rapid topology changes (e.g., clients joining/leaving, new servers added) to test whether the incremental Baum-Welch updates maintain prediction accuracy and whether decentralized decisions continue to reduce conflicts under dynamic conditions.