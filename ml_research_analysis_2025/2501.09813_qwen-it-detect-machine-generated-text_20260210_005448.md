---
ver: rpa2
title: Qwen it detect machine-generated text?
arxiv_id: '2501.09813'
source_url: https://arxiv.org/abs/2501.09813
tags:
- text
- task
- training
- subtask
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of distinguishing between human-authored
  and machine-generated text, a task that has become increasingly challenging with
  the advancement of language models. The authors propose using large language models
  (LLMs) with fine-tuning for text detection.
---

# Qwen it detect machine-generated text?

## Quick Facts
- arXiv ID: 2501.09813
- Source URL: https://arxiv.org/abs/2501.09813
- Reference count: 4
- Primary result: Fine-tuned Qwen2.5-0.5B achieved 1st place in F1 Micro (0.8333) and 2nd in F1 Macro (0.8301) for monolingual machine-generated text detection

## Executive Summary
This paper tackles the challenging problem of distinguishing human-authored from machine-generated text using large language models. The authors propose a selective fine-tuning approach, freezing all but the last transformer layer and classification head for the monolingual task, and applying LoRA to a masked multilingual model. Their monolingual approach achieved top rankings in the competition, while the multilingual model performed less well, highlighting the difficulty of adapting detection to varied linguistic structures. The work demonstrates that parameter-efficient fine-tuning can be effective for detection tasks when applied to appropriately pre-trained models.

## Method Summary
The authors employed two different architectures for the monolingual and multilingual detection tasks. For monolingual English detection, they used Qwen2.5-0.5B with a causal architecture, fine-tuning only the last transformer layer and classification head while freezing all preceding layers. For the multilingual task, they used XLM-Roberta-Base with a masked architecture, applying LoRA fine-tuning to all linear layers. Both models used a maximum token length of 2048 (monolingual) or 512 (multilingual), with down-sampling to achieve 50-50 class balance for the monolingual task. Training employed AdamW optimizer with early stopping based on validation loss plateau, and the monolingual model used class-weighted cross-entropy.

## Key Results
- Monolingual model: 1st place F1 Micro (0.8333), 2nd place F1 Macro (0.8301) out of 36 teams
- Multilingual model: 24th place in both F1 Macro (0.66) and F1 Micro (0.67)
- Selective freezing achieved parameter efficiency with only 3.02% trainable parameters for monolingual task
- Down-sampling to balanced classes improved performance from 0.7783/0.7868 to 0.8301/0.8333 F1 scores
- Over-prediction tendency observed with 91% true positive rate vs 74% true negative rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning only the last transformer layer and classification head while freezing all preceding layers can achieve competitive performance on binary machine-generated text detection.
- **Mechanism:** Pre-trained LLM layers encode general linguistic representations; the final layer adapts these to task-specific decision boundaries without disrupting learned features.
- **Core assumption:** Frozen pre-trained representations transfer sufficiently to the detection task domain.
- **Evidence anchors:**
  - [abstract]: "Specifically, we use a causal model (Qwen2.5-0.5B) for the monolingual task, fine-tuning only the last layer and classification head."
  - [section 3.1]: "We froze all layers excepting the last one and the classification head ending up with 14,914,176 (3.02%) trainable parameters."
  - [corpus]: Limited direct corpus evidence for this specific freezing strategy in detection tasks.
- **Break condition:** When pre-training domain differs significantly from target task, or frozen representations lack task-relevant features.

### Mechanism 2
- **Claim:** Down-sampling the majority class to achieve balanced 50-50 distribution mitigates overfitting and improves generalization in imbalanced classification.
- **Mechanism:** Equal class exposure during training prevents the model from learning trivial majority-class prediction strategies.
- **Core assumption:** Minority class retains sufficient representative samples after majority-class reduction.
- **Evidence anchors:**
  - [section 3.1]: "Initially, the model displayed a tendency to overfit on the majority class (F1 Macro: 0.7783 and F1 Micro: 0.7868 on the final test set), leading us to down-sample the training set to achieve a balanced 50-50 distribution... This adjustment helped mitigate over-fitting."
  - [corpus]: Weak corpus evidence specifically for down-sampling in LLM fine-tuning contexts.
- **Break condition:** When minority samples are too few to represent true distribution, or test-time distribution differs substantially from balanced training.

### Mechanism 3
- **Claim:** Halting training when validation loss plateaus prevents overfitting while preserving learned generalization.
- **Mechanism:** Validation loss plateau indicates the model has extracted transferable patterns; continued training risks memorizing training-specific noise.
- **Core assumption:** Validation set is representative of test distribution and signal-to-noise ratio.
- **Evidence anchors:**
  - [section 3.1]: "By the third epoch, the validation loss had reached a plateau, suggesting that further training would not yield additional gains and could potentially lead to over-fitting. We therefore halted training after the third epoch."
  - [Table 2]: Training loss decreased (0.11 → 0.04) while validation loss plateaued (0.11 → 0.10 → 0.10).
  - [corpus]: Standard practice; limited specific evidence in this domain.
- **Break condition:** When validation set is too small or unrepresentative, producing noisy plateau signals.

## Foundational Learning

- **Concept: Causal vs. Masked Language Models**
  - Why needed here: The paper deploys Qwen2.5 (causal, left-to-right) for monolingual and XLM-Roberta (masked, bidirectional) for multilingual tasks.
  - Quick check question: Can you explain why causal models predict tokens autoregressively while masked models leverage bidirectional context?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Multilingual model uses LoRA to fine-tune with only 0.27% trainable parameters.
  - Quick check question: How does LoRA decompose weight updates into low-rank matrices, and why does this reduce memory overhead?

- **Concept: F1 Macro vs. F1 Micro**
  - Why needed here: Competition ranked teams on both metrics; the paper reports 0.8301 Macro vs. 0.8333 Micro for monolingual.
  - Quick check question: When would F1 Macro (class-averaged) diverge significantly from F1 Micro (global pooled), and what does divergence signal?

## Architecture Onboarding

- **Component map:** Raw text → Tokenizer (max 2048/512 tokens) → Frozen LLM backbone → Trainable last layer + classification head → Binary output (Human/Machine)

- **Critical path:**
  1. Data preparation → down-sample to 50-50 class balance
  2. Model selection → causal for monolingual, masked for multilingual
  3. Layer configuration → freeze all but last layer (or apply LoRA)
  4. Training → monitor validation loss, early stop at plateau
  5. Evaluation → F1 Macro and Micro on held-out test

- **Design tradeoffs:**
  - Selective freezing (3% params) vs. LoRA (0.27% params): Freezing is simpler; LoRA enables broader adaptation with fewer params but requires tuning rank/alpha.
  - Down-sampling vs. class-weighted loss: Down-sampling loses data; class weights preserve data but require careful tuning.
  - Token limit 2048 vs. 512: Higher captures more context but increases compute; paper shows train/test distribution mismatch (Figures 1-2).

- **Failure signatures:**
  - Over-prediction of positive class (confusion matrix: 44,808 predictions vs. 39,266 actual positives)
  - Validation loss plateau while training loss continues decreasing
  - Performance collapse on unseen sources (Mixset F1 ≈ 0.5; Baichuan lowest accuracy)

- **First 3 experiments:**
  1. Establish baseline with classification-head-only fine-tuning on balanced data; log train/valid loss curves.
  2. Ablate freezing strategy: compare last-layer-only vs. LoRA (r=4) on same validation split.
  3. Generalization probe: evaluate checkpoint on unseen sources (e.g., ChatGPT, Baichuan) to detect distribution shift early.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can integrating latent-space variables and high-level features like event transitions improve detection resilience in adversarial settings?
  - Basis in paper: [explicit] The authors state they intend to "utilize latent-space variables" and features like "event transitions" to test if they can improve "accuracy and resilience, especially under varied generation and adversarial settings."
  - Why unresolved: The current work focuses on standard fine-tuning; the proposed architectural additions have not yet been implemented or tested against adversarial attacks.
  - What evidence would resolve it: Benchmarking the enhanced model against paraphrase attacks or obfuscation methods to measure robustness improvements.

- **Open Question 2:** Can language-specific fine-tuning and data augmentation effectively mitigate overfitting in multilingual detection tasks?
  - Basis in paper: [explicit] The authors identify overfitting as the primary failure mode for the multilingual subtask and propose exploring "language-specific fine-tuning" and "data augmentation" as solutions.
  - Why unresolved: The submitted multilingual model performed poorly (24th place), and the efficacy of the proposed remediation strategies remains hypothetical.
  - What evidence would resolve it: Training curves and test set performance showing reduced divergence between training and validation loss following the application of these techniques.

- **Open Question 3:** Does the disparity between monolingual and multilingual performance stem from architectural differences (causal vs. masked) or dataset difficulty?
  - Basis in paper: [inferred] The paper attributes the performance gap to "varied languages," but the switch from a causal model (Qwen) in the monolingual task to a masked model (XLM) in the multilingual task introduces a confounding variable.
  - Why unresolved: Without comparing both architectures on the same data, it is unclear if the masked model was inherently less suitable or if the multilingual data was simply harder.
  - What evidence would resolve it: Ablation studies applying the fine-tuned Qwen (causal) architecture to the multilingual dataset to isolate the impact of the model type.

## Limitations

- The performance gap between monolingual (top-2) and multilingual (mid-tier) tasks suggests fundamental architectural limitations when adapting detection to diverse linguistic structures, but the paper doesn't empirically validate whether the bidirectional masking in XLM-Roberta-Base or the LoRA configuration are optimal for detection tasks.
- Limited ablation studies prevent isolation of which component drives success; the paper doesn't report baseline results without down-sampling, without freezing, or with full fine-tuning to quantify the parameter efficiency tradeoff.
- The attribution of multilingual performance degradation solely to linguistic diversity without empirical validation of architecture-specific limitations or hyperparameter optimization for the multilingual setting.

## Confidence

- **High confidence:** The selective freezing strategy (freezing all but last layer) and its implementation details are well-specified and reproducible. The achieved F1 scores on Subtask A are verifiable through competition results.
- **Medium confidence:** The down-sampling approach effectively mitigates overfitting, supported by pre/post metrics but lacking comparative analysis with alternative balancing methods.
- **Low confidence:** The attribution of multilingual performance degradation solely to linguistic diversity without empirical validation of architecture-specific limitations or hyperparameter optimization for the multilingual setting.

## Next Checks

1. **Ablation study:** Run experiments comparing three fine-tuning strategies on Subtask A - full fine-tuning, last-layer-only freezing (as reported), and LoRA (r=4) - to quantify the parameter efficiency tradeoff and isolate whether the freezing strategy or the Qwen2.5 architecture drives the performance gain.

2. **Generalization probe:** Evaluate the monolingual model on out-of-distribution sources not present in training (e.g., newer models like GPT-4, Claude) to assess whether the 91% human detection rate is consistent across generation paradigms or overfits to specific model signatures.

3. **Multilingual architecture comparison:** Replace XLM-Roberta-Base with a causal multilingual model (e.g., XGLM) using the same freezing strategy as the monolingual task to determine whether bidirectional masking inherently limits detection performance or if the architecture can be optimized for this task.