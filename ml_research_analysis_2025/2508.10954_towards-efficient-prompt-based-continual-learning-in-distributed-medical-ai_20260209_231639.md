---
ver: rpa2
title: Towards Efficient Prompt-based Continual Learning in Distributed Medical AI
arxiv_id: '2508.10954'
source_url: https://arxiv.org/abs/2508.10954
tags:
- prompt
- learning
- medical
- performance
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of continual learning in distributed
  medical AI, where models must be incrementally updated at each hospital using only
  local data while avoiding catastrophic forgetting. The core method idea is a prompt-based
  continual learning (PCL) approach featuring a unified prompt pool with minimal expansion:
  by expanding and freezing a small subset of prompts, the method reduces computational
  overhead, and a novel regularization term balances retention and adaptation.'
---

# Towards Efficient Prompt-based Continual Learning in Distributed Medical AI

## Quick Facts
- **arXiv ID:** 2508.10954
- **Source URL:** https://arxiv.org/abs/2508.10954
- **Reference count:** 39
- **Primary result:** Improves final classification accuracy by at least 10% and F1-score by 9 points over state-of-the-art approaches in distributed medical AI continual learning.

## Executive Summary
This paper proposes a prompt-based continual learning (PCL) method for distributed medical AI systems, where models are incrementally updated at each hospital using only local data while avoiding catastrophic forgetting. The approach features a unified prompt pool with minimal expansion strategy, freezing existing prompts while adding a small subset of new ones, and introducing a regularization term to balance retention and adaptation. Experiments on three diabetic retinopathy datasets demonstrate significant improvements in classification accuracy and F1-score while reducing computational overhead compared to existing methods.

## Method Summary
The method uses a frozen Dino-v2 backbone and replaces layer-wise prompt pools with a unified prompt pool of size 500 (dim 768). At each training stage, all existing prompts are frozen and 100 new prompts (20% expansion) are added. The model uses the [CLS] token as a query to select relevant prompts via cosine similarity. Training employs a combined loss of cross-entropy and a regularization term (λ=0.001) to maximize utilization of newly added prompts. The system operates under domain incremental learning (DIL) without task identity information during inference.

## Key Results
- Final classification accuracy improved by at least 10% over state-of-the-art approaches
- F1-score improved by 9 points compared to baseline methods
- Inference cost reduced with 44.17 GFLOPs versus 66.42 GFLOPs for dual-backbone approaches
- Successfully avoided catastrophic forgetting across three sequential hospital domains

## Why This Works (Mechanism)

### Mechanism 1: Unified Prompt Pool
- Claim: Consolidating layer-wise prompt pools into a unified prompt pool enhances the model's ability to capture fine-grained features in medical images.
- Mechanism: Merges all layer-wise prompt pools into a single shared pool, allowing prompts to be shared and optimized across layers by leveraging inter-layer similarities in standardized medical imaging protocols.
- Core assumption: Medical images collected under strict clinical protocols share sufficient similarity across network layers to benefit from a shared prompt representation.
- Evidence anchors: Unified prompt pool mentioned in abstract; integration shown to form tighter clusters in Figure 2a; weak corpus evidence for specific architecture.
- Break condition: If input modalities require radically different features at early vs. late layers that cannot be served by a shared pool, performance may degrade.

### Mechanism 2: Minimal Prompt Expansion
- Claim: Minimal prompt expansion combined with freezing strategy mitigates catastrophic forgetting while reducing computational overhead.
- Mechanism: Freezes all existing prompts and adds a small fixed number of new prompts (20% of initial count) at each stage, training only new prompts and classification head.
- Core assumption: Fixed expansion ratio is sufficient to absorb new domain knowledge without requiring dynamic allocation.
- Evidence anchors: Abstract mentions reduced computational overhead; section IV.B describes 20% heuristic expansion; consistent with general CL stability-plasticity goals.
- Break condition: If a new domain is significantly more complex or distinct, fixed 20% expansion may be insufficient, leading to underfitting.

### Mechanism 3: Consistency-Enforcing Regularization
- Claim: A consistency-enforcing regularization term ensures newly added prompts are effectively utilized during training.
- Mechanism: Introduces loss term L_s to maximize utilization of prompt vectors, preventing newly added prompts from being ignored by softmax weighting in favor of older, well-trained prompts.
- Core assumption: Standard softmax selection might converge on existing prompts too greedily, nullifying contribution of newly initialized prompts.
- Evidence anchors: Abstract mentions novel regularization term; section IV.B introduces L_s formulation; no specific corpus evidence for this exact loss term.
- Break condition: If regularization weight is too high, it might force use of irrelevant new prompts; if too low, new prompts remain dormant.

## Foundational Learning

- **Concept: [CLS] Token as a Query**
  - Why needed here: Removes need for separate, computationally expensive query network; uses [CLS] token already computed by ViT backbone to select relevant prompts.
  - Quick check question: Does the [CLS] token from an intermediate layer contain enough domain-specific information to select the correct prompt subset?

- **Concept: Frozen Backbone (Dino-v2)**
  - Why needed here: Relies on pre-trained foundation model to remain fixed, providing stable feature extractor so "learning" is offloaded entirely to lightweight prompts, ensuring data privacy and efficiency.
  - Quick check question: Are pre-trained features from general-purpose ViT (Dino-v2) sufficiently robust for subtle medical domain shifts without fine-tuning backbone weights?

- **Concept: Domain-Incremental Learning (DIL)**
  - Why needed here: System operates under DIL setting where task identity (which hospital/data source) is unknown during inference.
  - Quick check question: Can prompt selection mechanism correctly identify domain implicitly based on input image's [CLS] token without explicit task ID?

## Architecture Onboarding

- **Component map**: Image -> Frozen ViT layers -> [CLS] token extraction -> Prompt key similarity computation -> Softmax selection -> Prompt embedding -> Concatenation with input tokens -> Next layer
- **Critical path**:
  1. Image processed by frozen ViT layers
  2. At specific layers, extract [CLS] token (q_l)
  3. Compute similarity between q_l and all prompt keys (K)
  4. Softmax selects top prompts; weighted sum produces prompt embedding (φ_l)
  5. Concatenate φ_l with input tokens and proceed to next layer
- **Design tradeoffs**:
  - Unified vs. Layer-wise Pool: Unified pool reduces parameters and redundancy but assumes higher feature correlation across layers
  - Single vs. Double Backbone: Uses single backbone to cut FLOPs (44.17 vs 66.42 GFLOPs) but relies entirely on [CLS] token's quality compared to methods using separate query model
- **Failure signatures**:
  - Stagnant Accuracy: New tasks learned but old task accuracy drops (Catastrophic Forgetting) -> Check if prompts were accidentally unfrozen or if selection mechanism collapsed
  - Low New Task Performance: Model fails to adapt to new hospital -> Check if expansion ratio is too low or if regularization term (L_s) is suppressing gradient signal for new prompts
- **First 3 experiments**:
  1. Sanity Check (Backbone): Run baseline OS-Prompt++ with and without Dino-v2 backbone to isolate performance gain from stronger foundation model
  2. Ablation (Loss): Train with and without proposed regularization term L_s to verify impact on final accuracy and F1-score
  3. Hyperparameter Scan (Expansion): Test different prompt expansion ratios (10% vs 20% vs 30%) to confirm heuristic choice and observe sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Unified prompt pool assumes strong inter-layer similarity in standardized medical images, which may not generalize to multimodal or heterogeneous imaging protocols
- Fixed 20% expansion heuristic lacks theoretical grounding and could underperform in domains with significantly different feature distributions
- Regularization term L_s is not empirically validated in isolation, and its interaction with softmax prompt selection is not fully characterized
- Reliance on frozen Dino-v2 backbone presumes its features are universally robust across hospital-specific domain shifts

## Confidence

- **High confidence**: Core continual learning mechanism (freezing old prompts, adding new ones) and overall performance improvement (10%+ accuracy, 9-point F1 gain) are well-supported by experimental design and reported results
- **Medium confidence**: Unified prompt pool's benefits and necessity of regularization term L_s are plausible based on paper's reasoning and ablation studies, but lack direct ablation or comparative evidence in literature
- **Low confidence**: Claim that [CLS] token alone is sufficient for prompt selection across all layers is weakly supported; fixed expansion ratio (20%) is presented as heuristic without sensitivity analysis

## Next Checks
1. **Ablation of the unified prompt pool**: Train the same model with and without prompt pool integration (i.e., revert to layer-wise pools) on three DR datasets to quantify actual performance impact of unified approach
2. **Isolation of L_s regularization**: Train with and without regularization term, keeping all other hyperparameters fixed, to measure isolated effect on both accuracy and forgetting (BWT)
3. **Expansion ratio sensitivity**: Systematically vary prompt expansion ratio (e.g., 10%, 20%, 30%) and record impact on final accuracy, F1, and FLOPs to validate choice of 20% as optimal or merely adequate