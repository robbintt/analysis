---
ver: rpa2
title: Combating Biomedical Misinformation through Multi-modal Claim Detection and
  Evidence-based Verification
arxiv_id: '2509.13888'
source_url: https://arxiv.org/abs/2509.13888
tags:
- claim
- evidence
- claims
- detection
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CER (Combining Evidence and Reasoning) is a multi-modal biomedical
  fact-checking system that integrates claim detection, scientific evidence retrieval
  from PubMed, and LLM-based reasoning to verify claims from text, web pages, and
  videos. The system uses dense retrieval with SBERT embeddings to extract relevant
  biomedical abstracts, then employs an LLM (Meta-Llama-3.1-405B-Instruct) to generate
  justifications and preliminary veracity assessments, which are combined with a classification
  model (DeBERTa, BERT, or PubMedBERT) for final classification.
---

# Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification

## Quick Facts
- arXiv ID: 2509.13888
- Source URL: https://arxiv.org/abs/2509.13888
- Reference count: 26
- CER achieves F1-score improvements up to +7.69% (69.90% on HealthFC, 95.20% on BioASQ, 61.14% on SciFact) over baselines

## Executive Summary
CER is a multi-modal biomedical fact-checking system that integrates claim detection, scientific evidence retrieval from PubMed, and LLM-based reasoning to verify claims from text, web pages, and videos. The system uses dense retrieval with SBERT embeddings to extract relevant biomedical abstracts, then employs an LLM (Meta-Llama-3.1-405B-Instruct) to generate justifications and preliminary veracity assessments, which are combined with a classification model (DeBERTa, BERT, or PubMedBERT) for final classification. Evaluations on HealthFC, BioASQ, and SciFact benchmarks show state-of-the-art performance with F1-score improvements up to +7.69% (69.90% on HealthFC, 95.20% on BioASQ, 61.14% on SciFact in fine-tuned settings). CER also achieves perfect accuracy in detecting deepfake healthcare videos by identifying false claims within their content, outperforming traditional deepfake detectors. The system is released open-source for transparency and reproducibility.

## Method Summary
CER is a multi-modal biomedical fact-checking system that integrates claim detection, evidence retrieval from PubMed, and LLM-based reasoning. Claims are detected via Meta-Llama-3.1-405B-Instruct, then relevant biomedical abstracts are retrieved using SBERT embeddings and FAISS indexing. The top-3 evidence passages are concatenated with the claim and passed to the LLM to generate justifications. A fine-tuned classifier (DeBERTa-v3-large, BERT, or PubMedBERT) then produces the final true/false/NEI label. The system handles text, web pages, and video inputs, with video transcription via Whisper small-v3. Evaluations on HealthFC, BioASQ, and SciFact benchmarks show state-of-the-art performance, with additional perfect accuracy in deepfake video detection by identifying false claims within content.

## Key Results
- F1-score improvements up to +7.69% (69.90% on HealthFC, 95.20% on BioASQ, 61.14% on SciFact) over baselines
- Perfect accuracy in detecting deepfake healthcare videos by identifying false claims within content
- Open-source release for transparency and reproducibility

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Grounded Reasoning Pipeline
- Claims are encoded as SBERT embeddings and matched against a FAISS-indexed PubMed corpus to retrieve top-20 results, with top-3 evidence passages selected for LLM reasoning. This grounding reduces hallucinations while generating domain-appropriate justifications. Core assumption: Retrieved evidence is both relevant and sufficient for veracity assessment.

### Mechanism 2: Decoupled LLM Reasoning + Classifier Prediction
- The LLM outputs justifications and preliminary binary assessments, but the final true/false/NEI label is produced by a fine-tuned classifier (DeBERTa-v3-large, BERT, or PubMedBERT) that takes claim + evidence + justification as input. This separation improves classification over LLM-only prediction. Core assumption: Fine-tuned classifiers are more reliable for structured label prediction than LLM zero-shot outputs.

### Mechanism 3: Content-Based Deepfake Detection via Claim Falsity
- Videos are transcribed via Whisper, claims are detected and verified through the standard pipeline, and a video is labeled "fake" if any claim is classified as false. This approach achieves perfect accuracy on the 40-video benchmark. Core assumption: Deepfake healthcare videos contain factually false claims rather than factually accurate synthetic content.

## Foundational Learning

- **Dense Retrieval with SBERT/FAISS**: Core to evidence grounding; you must understand semantic similarity search to debug retrieval quality. Quick check: Why would a keyword-based search fail where SBERT succeeds for biomedical claims?
- **RAG (Retrieval-Augmented Generation)**: The system is a RAG pipeline; understanding context-window tradeoffs and grounding vs. hallucination is essential. Quick check: What happens to output quality if retrieved context exceeds the LLM's effective context length?
- **Fine-tuned Classification vs. Zero-Shot LLM Prediction**: The paper explicitly compares zero-shot vs. fine-tuned classifiers; the design choice affects deployment cost and accuracy. Quick check: On a constrained compute budget, when would you choose zero-shot over fine-tuning?

## Architecture Onboarding

- **Component map**: Input Layer (Text processor, HTML extractor, Whisper small-v3) -> Claim Detection (Meta-Llama-3.1-405B-Instruct) -> Evidence Retrieval (PubMed + SBERT + FAISS + top-20 + top-3) -> LLM Reasoning (Meta-Llama-3.1-405B-Instruct) -> Veracity Prediction (DeBERTa-v3-large / BERT / PubMedBERT)
- **Critical path**: 1. Preprocess input (transcribe video, extract HTML text) 2. Detect claims via LLM 3. Retrieve top-3 PubMed evidence 4. Generate LLM justification 5. Classify via fine-tuned model
- **Design tradeoffs**: Zero-shot vs. fine-tuned classifier (zero-shot faster, fine-tuned yields +12.6% F1 on HealthFC); DeBERTa vs. PubMedBERT (domain pretraining vs. task-specific fine-tuning); top-3 evidence cutoff (balances context length vs. coverage)
- **Failure signatures**: Low retrieval relevance → LLM justifications drift from evidence; Whisper transcription errors on medical terminology → claim detection misses or distortions; NEI over-prediction when evidence is sparse or classifier uncertainty is high
- **First 3 experiments**: 1. Retrieval ablation: Vary top-k (1, 3, 5, 10) and measure F1 on HealthFC 2. Justification ablation: Run classifier with and without LLM justification 3. Cross-dataset transfer: Train on BioASQ, test on SciFact

## Open Questions the Paper Calls Out

- **Can CER's claim-based deepfake detection maintain its perfect accuracy when scaled to larger and more diverse datasets of healthcare-related synthetic videos?**: The authors note in footnote 3 that the 40-video evaluation is preliminary due to limited dataset size. The videos may not represent the full diversity of generation methods, medical topics, or linguistic styles in real-world healthcare misinformation.
- **Would replacing the current dense retrieval system (SBERT embeddings) with BioBERT-based retrieval improve evidence retrieval quality and downstream fact-checking accuracy?**: The authors suggest future work with BioBERT-based retrieval for better domain coverage, but the trade-offs between semantic generality and domain specificity for evidence retrieval remain unquantified.
- **To what extent does CER generalize to novel biomedical claims outside its training distribution, such as emerging diseases or treatments absent from PubMed at inference time?**: While promising cross-dataset generalization is mentioned, all benchmarks contain claims grounded in existing literature. The system's behavior when evidence is genuinely unavailable or contradictory is not systematically tested.

## Limitations
- Deepfake detection perfect accuracy claim based on small 40-video dataset lacking external validation
- Missing fine-tuning hyperparameters, prompt templates, and train/validation/test splits block faithful reproduction
- Results benchmarked on three English-language biomedical datasets; performance on other domains, languages, or real-world noisy data remains untested

## Confidence
- **High**: CER's retrieval-grounded LLM reasoning pipeline and explicit decoupling of LLM justification generation from classifier-based final prediction
- **Medium**: Claims about state-of-the-art performance improvements (+7.69% F1 on HealthFC) credible but require independent replication
- **Low**: Deepfake detection mechanism's perfect accuracy not robust to dataset size, external validation, or real-world deployment scenarios

## Next Checks
1. Retrieval ablation: Systematically vary the number of retrieved evidence passages (top-1, 3, 5, 10) and measure F1-score on HealthFC
2. Justification contribution: Compare classifier performance with and without LLM-generated justifications
3. Cross-dataset generalization: Train on BioASQ and test on SciFact, and vice versa, to assess generalization across biomedical claim verification tasks