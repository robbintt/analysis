---
ver: rpa2
title: 'Breaking the ICE: Exploring promises and challenges of benchmarks for Inference
  Carbon & Energy estimation for LLMs'
arxiv_id: '2506.08727'
source_url: https://arxiv.org/abs/2506.08727
tags:
- carbon
- latency
- energy
- data
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents R-ICE, a framework that estimates prompt-level
  inference carbon emissions for LLMs using LLM benchmark data instead of intrusive
  monitoring or detailed architectural modeling. By leveraging HELM efficiency benchmarks,
  R-ICE employs regression models to predict minimum hardware deployment, prompt encoding
  latency, and per-token output latency, which are then combined to estimate energy
  consumption and carbon emissions.
---

# Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs

## Quick Facts
- arXiv ID: 2506.08727
- Source URL: https://arxiv.org/abs/2506.08727
- Authors: Samarth Sikand; Rohit Mehra; Priyavanshi Pathania; Nikhil Bamby; Vibhu Saujanya Sharma; Vikrant Kaulgud; Sanjay Podder; Adam P. Burden
- Reference count: 17
- Primary result: R-ICE estimates LLM inference carbon emissions with ~15% energy and ~23% latency prediction errors using benchmark data

## Executive Summary
R-ICE introduces a non-intrusive framework for estimating prompt-level inference carbon emissions of LLMs using regression models trained on HELM efficiency benchmarks. By decomposing latency into prompt encoding and per-token generation components, the framework predicts minimum hardware requirements, latency, and energy consumption without requiring access to deployment details. Validation on the HuggingFace LLM-Perf Leaderboard demonstrates practical accuracy for carbon-aware LLM routing and design decisions.

## Method Summary
R-ICE uses three regression models trained on HELM-Efficiency benchmark data: linear regression for minimum GPU deployment from model parameters (R²=0.99), random forest for prompt encoding latency from parameters and input tokens (R²=0.986), and polynomial regression for per-token output latency from parameters (R²=0.94). These predictions are combined with hardware specifications (TDP, utilization, PUE) to estimate energy consumption and carbon emissions using the formula CO₂eq = Σ(#hardware × Power × runtime × PUE × RCI). The framework validates predictions against 46 filtered data points from the HuggingFace LLM-Perf Leaderboard.

## Key Results
- Average prediction error of ~15% for energy consumption and ~23% for latency on LLM-Perf Leaderboard
- Strong linear relationship between model parameters and minimum GPU deployment (R²=0.99)
- Random Forest model achieves R²=0.986 for prompt encoding latency with 5% MAPE
- Framework enables carbon-aware LLM routing without intrusive monitoring or proprietary model details

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latency can be decomposed into prompt encoding time and per-token generation time, each predictable from model parameter count.
- Mechanism: R-ICE leverages HELM's cost model (Eq. 1: T = prompt encoding time(p) + (o−1)β) where encoding latency scales with input tokens and output latency scales linearly with generated tokens. Regression models trained on HELM efficiency benchmark data map model size (# parameters) to these latency components.
- Core assumption: Auto-regressive transformer latency follows a piecewise linear function of input tokens and linear function of output tokens, stable across deployment contexts.
- Evidence anchors: [abstract] "employs regression models to predict minimum hardware deployment, prompt encoding latency, and per-token output latency"; [section IV] "Prompt-encoding latency dataset consists of ~400 data points"; Random Forest achieved R²=0.986, MAPE=0.050
- Break condition: Optimizations like Flash Attention, quantization, or batching violate the singleton-batch, unoptimized inference assumption—predictions will underestimate real-world latency for optimized deployments.

### Mechanism 2
- Claim: Energy consumption can be estimated from predicted latency using hardware power specifications and datacenter efficiency metrics.
- Mechanism: The framework applies CO₂eq = Σ(# hardware × Power × runtime × PUE × RCI). Predicted E2E latency fills the runtime variable; TDP (Thermal Design Power) provides maximum power draw; utilization factor (default 0.26) scales to realistic consumption.
- Core assumption: Hardware runs at consistent utilization (~26%) and power efficiency (PUE=1.1) regardless of workload characteristics.
- Evidence anchors: [section IV] Formula explicitly stated; "default Hardware Utilization value is assumed to be 0.26... based on a study that estimated the footprint of the BLOOM model"; [section V] Validation on HuggingFace LLM-Perf Leaderboard showed 15.3% MAPE for energy
- Break condition: Custom hardware stacks, dynamic voltage scaling, or GPU-specific optimizations (e.g., Turbo modes) cause actual power draw to deviate significantly from TDP-based estimates.

### Mechanism 3
- Claim: Minimum hardware requirements scale linearly with model parameter count.
- Mechanism: Linear regression trained on <15 data points mapping model size to minimum A100-80GB GPUs required. R²=0.99 suggests strong linear relationship for memory-bound deployment planning.
- Core assumption: Memory requirements dominate hardware scaling; all models deploy on homogeneous A100-80GB units without model parallelism variations.
- Evidence anchors: [section IV.A.2] Table II shows sample points (6B→1 GPU, 52B→4 GPUs); "Coefficient of Determination (R²) was observed to be close to 1.0"; [section VI] "Datasets size to train regression models esp. # Devices and Per-output token is small"
- Break condition: Model architectures with different parameter-to-memory ratios (e.g., MoE models, sparse attention) will mispredict hardware requirements.

## Foundational Learning

- Concept: **Latency decomposition for autoregressive models**
  - Why needed here: Understanding why inference splits into prompt encoding (parallelizable) vs. token generation (sequential) explains why two separate regression models are required.
  - Quick check question: Given a 10B parameter model, why would doubling input tokens from 500 to 1000 have a different latency impact than doubling output tokens from 50 to 100?

- Concept: **TDP vs. actual power consumption**
  - Why needed here: R-ICE uses TDP as a power proxy; understanding this approximation's limitations is critical for interpreting error margins.
  - Quick check question: If a GPU has TDP=300W but runs at 40% utilization during inference, what is the estimated power draw? What factors might make this estimate wrong?

- Concept: **PUE and carbon intensity as multiplicative factors**
  - Why needed here: These translate energy (kWh) to carbon emissions (CO₂eq) and are the primary levers for datacenter-level optimization.
  - Quick check question: A workload consumes 0.01 kWh in a datacenter with PUE=1.5 and carbon intensity=400g CO₂/kWh. What are the total emissions? How would moving to PUE=1.1 affect this?

## Architecture Onboarding

- Component map: Regression Model 1 (Linear: #params → #GPUs) → Regression Model 2 (Random Forest: #params, input_tokens → encoding_latency) → Regression Model 3 (Polynomial: #params → per_token_latency) → Energy calculator (latency + TDP + utilization → Energy) → Carbon calculator (Energy + PUE + RCI → CO₂eq)

- Critical path: Input (# params, input tokens, max output tokens) → Models 1,2,3 run in parallel → Aggregate latency → Apply Eq. 2 scaling → Energy & carbon computation. Latency prediction is the bottleneck; errors propagate multiplicatively.

- Design tradeoffs:
  - Accuracy vs. intrusiveness: R-ICE trades ~15-23% error for zero deployment access vs. CodeCarbon's intrusive monitoring
  - Generalization vs. specificity: Default utilization/PUE values enable out-of-box use but sacrifice environment-specific accuracy
  - Model coverage: Limited to <10k context windows; excludes modern 100k+ context models

- Failure signatures:
  - Systematic underestimation for quantized models (R-ICE trained on unoptimized baselines)
  - High variance for models near GPU memory boundaries (deployment regression has few data points)
  - Large errors for batched inference (assumption: batch size = 1)

- First 3 experiments:
  1. **Baseline replication**: Run R-ICE predictions on 5 models from LLM-Perf Leaderboard not in training set; compare against reported energy. Target: MAPE <25%.
  2. **Break condition test**: Apply R-ICE to a quantized model (e.g., GPTQ-4bit variant) and unquantized baseline. Document error divergence.
  3. **Sensitivity analysis**: Vary utilization (0.15–0.40) and PUE (1.0–1.4) inputs; measure impact on final CO₂eq. Identify which parameter dominates error propagation.

## Open Questions the Paper Calls Out
- Can a hybrid approach incorporating diverse efficiency benchmarks improve the robustness of R-ICE against various inference optimizations?
- How does the assumption of singleton batch inference impact estimation accuracy in high-throughput production environments?
- Does the linear relationship between model size and latency hold for context windows exceeding 10,000 tokens?

## Limitations
- Limited training data for per-token latency (n≈10) and minimum device deployment (n<15) creates significant uncertainty
- Framework assumes homogeneous A100-80GB GPU deployments and unoptimized inference, limiting applicability to production environments
- Default hardware utilization (0.26) and PUE (1.1) values may not reflect diverse deployment contexts

## Confidence
- **High Confidence**: Latency decomposition mechanism is well-established and directly validated by strong R² values (>0.94) for latency prediction models
- **Medium Confidence**: Energy estimation methodology is common practice but known to have high variance; 15% MAPE is promising but validation dataset is limited
- **Low Confidence**: Minimum hardware deployment predictions due to extremely small training set (<15 points) and assumption of linear scaling with parameters

## Next Checks
1. **Out-of-distribution generalization test**: Apply R-ICE to models with context lengths >10k tokens and MoE architectures not represented in HELM benchmarks. Measure prediction error degradation and identify breaking points in the regression models' assumptions.

2. **Hardware optimization impact analysis**: Compare R-ICE predictions against measured energy consumption for quantized (4-bit/8-bit), speculative decoding, and batched inference deployments. Quantify systematic underestimation patterns and develop correction factors for optimized inference.

3. **Dataset size sensitivity study**: Systematically reduce training data for each regression model (prompt encoding, per-token latency, deployment) to determine minimum viable sample sizes. Identify which model component is most sensitive to data scarcity and recommend minimum benchmark requirements for reliable carbon estimation.