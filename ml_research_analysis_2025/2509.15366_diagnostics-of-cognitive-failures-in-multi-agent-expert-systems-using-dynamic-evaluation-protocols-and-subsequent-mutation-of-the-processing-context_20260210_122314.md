---
ver: rpa2
title: Diagnostics of cognitive failures in multi-agent expert systems using dynamic
  evaluation protocols and subsequent mutation of the processing context
arxiv_id: '2509.15366'
source_url: https://arxiv.org/abs/2509.15366
tags:
- expert
- agent
- diagnostic
- system
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a diagnostic framework for expert systems
  that not only evaluates but also facilitates the transfer of expert behaviour into
  LLM-powered agents. The framework integrates (i) curated golden datasets of expert
  annotations, (ii) silver datasets generated through controlled behavioural mutation,
  and (iii) an LLM-based Agent Judge that scores and prescribes targeted improvements.
---

# Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context

## Quick Facts
- arXiv ID: 2509.15366
- Source URL: https://arxiv.org/abs/2509.15366
- Reference count: 0
- Introduces a diagnostic framework for expert systems that transfers expert behavior into LLM-powered agents

## Executive Summary
This work presents a novel diagnostic framework for multi-agent expert systems that moves beyond static evaluation to actively refine LLM agent behavior. The framework combines curated golden datasets of expert annotations with silver datasets generated through controlled behavioral mutation, evaluated by an LLM-based Agent Judge. The system identifies cognitive failures such as biased phrasing, extraction drift, and tool misrouting, while prescribing targeted improvements that propagate across multiple system instances through vectorized recommendation maps. Demonstrated on a multi-agent recruiter-assistant system, the framework establishes a foundation for standardized, reproducible expert behavior transfer in stochastic, tool-augmented LLM agents.

## Method Summary
The framework integrates three core components: curated golden datasets containing expert annotations, silver datasets generated through controlled behavioral mutation of expert behavior, and an LLM-based Agent Judge that scores agent outputs and prescribes improvements. These prescriptions are embedded into a vectorized recommendation map, creating reusable improvement trajectories across system instances. The approach combines static evaluation with active refinement, allowing expert interventions to systematically propagate through the multi-agent system. The framework was demonstrated on a recruiter-assistant system where it uncovered specific cognitive failures and steered agents toward expert-level reasoning and style.

## Key Results
- Successfully identifies latent cognitive failures including biased phrasing, extraction drift, and tool misrouting
- Demonstrates effective transfer of expert behavior improvements across multiple system instances
- Establishes a foundation for standardized expert behavior transfer in stochastic, tool-augmented LLM agents

## Why This Works (Mechanism)
The framework succeeds by creating a closed-loop system where expert knowledge flows from static annotations through dynamic mutation and evaluation back into actionable improvement prescriptions. The Agent Judge serves as an intelligent intermediary that not only detects failures but prescribes specific, targeted interventions. The vectorized recommendation map acts as a knowledge repository that captures nuanced expert reasoning patterns in a reusable format. By combining curated expert data with controlled behavioral variation, the system can identify failure modes that emerge from the interaction between expert intent and LLM stochasticity.

## Foundational Learning
- **Golden Dataset Curation**: Essential for providing ground truth expert behavior; quick check: verify dataset diversity and coverage of edge cases
- **Behavioral Mutation Control**: Needed to systematically explore failure modes; quick check: ensure mutation preserves task relevance while introducing variation
- **LLM-based Agent Judge**: Critical for scalable evaluation; quick check: validate judge scoring against human expert consensus
- **Vectorized Recommendation Mapping**: Enables reusable improvement trajectories; quick check: test map effectiveness across different agent configurations
- **Cognitive Failure Taxonomy**: Provides framework for systematic diagnosis; quick check: ensure taxonomy captures both semantic and procedural failures
- **Expert Behavior Transfer**: Core capability for system improvement; quick check: measure transfer effectiveness across multiple system instances

## Architecture Onboarding

**Component Map**: Golden Dataset → Behavioral Mutation → Agent Execution → Agent Judge → Recommendation Map → Expert Intervention → System Update

**Critical Path**: The evaluation pipeline flows from expert-curated golden datasets through controlled behavioral mutation, agent execution, LLM-based scoring by the Agent Judge, and prescription generation. These prescriptions populate the vectorized recommendation map, which guides expert interventions that update the system. This creates a continuous improvement cycle where expert knowledge systematically propagates through the multi-agent system.

**Design Tradeoffs**: The framework trades computational overhead (multiple LLM evaluations) for comprehensive failure detection and improvement prescription. The reliance on LLM-based evaluation introduces potential circularity but enables scalable assessment of stochastic agent behavior. The framework prioritizes systematic improvement over real-time performance, making it suitable for development and refinement phases rather than production deployment.

**Failure Signatures**: The system identifies three primary failure categories: (1) Extraction Drift - gradual degradation in information extraction accuracy, (2) Tool Misrouting - incorrect routing of tasks to inappropriate system components, and (3) Biased Phrasing - systematic deviations from expert communication style. Each failure type produces distinct scoring patterns in the Agent Judge output, enabling targeted interventions.

**First 3 Experiments**:
1. Baseline evaluation: Run the complete pipeline on the recruiter-assistant system without improvements to establish failure baseline
2. Single-intervention test: Apply one expert prescription and measure improvement propagation across system instances
3. Cross-domain validation: Apply framework to a medical diagnosis domain to test generalizability

## Open Questions the Paper Calls Out
The paper highlights several open questions including: How can the framework scale to handle larger, more complex multi-agent systems? What is the optimal balance between mutation intensity and task relevance preservation? How can the framework adapt to domains with limited expert availability or rapidly evolving knowledge bases? Can the vectorized recommendation maps be made more interpretable for human experts?

## Limitations
- Limited to single application domain (recruitment), constraining generalizability to other multi-agent scenarios
- Reliance on LLM-based evaluation introduces potential circularity and hallucination risks
- Effectiveness depends heavily on quality and representativeness of golden dataset expert annotations
- Computational overhead may limit practical deployment in resource-constrained environments
- The framework requires substantial expert time investment for dataset curation and intervention cycles

## Confidence
- Diagnostic capabilities: Medium-High
- Transferability of expert behavior improvements: Medium
- Cross-domain applicability: Low

## Next Checks
1. **Cross-domain validation**: Apply the framework to at least two additional domains (e.g., medical diagnosis, financial analysis) to assess generalizability across different task types and knowledge domains

2. **Ablation study of Agent Judge reliability**: Conduct systematic testing where human experts independently evaluate the same agent behaviors, comparing agreement rates with the LLM-based Agent Judge to quantify hallucination risks and scoring consistency

3. **Long-term improvement persistence**: Implement a longitudinal study tracking whether improvements from expert interventions persist across multiple generations of system updates and whether performance degrades over time without continued expert input

4. **Mutation intensity optimization**: Systematically vary behavioral mutation parameters to identify the optimal balance between exploration of failure modes and preservation of task relevance, establishing quantitative guidelines for different application domains