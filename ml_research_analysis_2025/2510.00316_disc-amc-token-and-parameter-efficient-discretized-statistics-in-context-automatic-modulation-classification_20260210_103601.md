---
ver: rpa2
title: 'DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context
  Automatic Modulation Classification'
arxiv_id: '2510.00316'
source_url: https://arxiv.org/abs/2510.00316
tags:
- prompt
- accuracy
- modulation
- classification
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational inefficiency of large language
  model (LLM)-based automatic modulation classification (AMC) in wireless communication,
  which requires long prompts and large models. The authors propose DiSC-AMC, a token-
  and parameter-efficient method that discretizes higher-order statistics into compact
  symbolic tokens, prunes exemplar lists via a lightweight classifier, and enforces
  constrained label-only predictions.
---

# DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification

## Quick Facts
- **arXiv ID:** 2510.00316
- **Source URL:** https://arxiv.org/abs/2510.00316
- **Reference count:** 16
- **Primary result:** DiSC-AMC reduces prompt length and model size by more than half while maintaining competitive accuracy in AMC tasks

## Executive Summary
This work addresses the computational inefficiency of large language model (LLM)-based automatic modulation classification (AMC) in wireless communication, which requires long prompts and large models. The authors propose DiSC-AMC, a token- and parameter-efficient method that discretizes higher-order statistics into compact symbolic tokens, prunes exemplar lists via a lightweight classifier, and enforces constrained label-only predictions. Evaluated on synthetic AMC with ten modulation types, DiSC-AMC reduces prompt length and model size by more than half while maintaining competitive accuracy. Specifically, a 5B-parameter Gemini-2.5-Flash model achieved 45.5% accuracy, compared to 5.2% for a 7B baseline using the prior approach.

## Method Summary
DiSC-AMC introduces a novel approach to LLM-based AMC by converting higher-order statistical features of wireless signals into symbolic tokens, significantly reducing the computational burden. The method employs a lightweight classifier to prune exemplar lists, further optimizing the in-context learning process. By enforcing constrained label-only predictions, DiSC-AMC ensures focused and efficient inference. The approach was evaluated on synthetic AMC datasets with ten modulation types, demonstrating substantial improvements in both token and parameter efficiency compared to traditional methods.

## Key Results
- DiSC-AMC reduces prompt length and model size by more than half
- 5B-parameter Gemini-2.5-Flash model achieved 45.5% accuracy
- Baseline 7B model using prior approach achieved only 5.2% accuracy

## Why This Works (Mechanism)
DiSC-AMC works by discretizing complex higher-order statistics into compact symbolic tokens, which reduces the information density required for AMC. This discretization process, combined with exemplar list pruning via a lightweight classifier, minimizes the computational load while preserving essential classification features. The constrained label-only prediction mechanism ensures that the model focuses on the most relevant information, further enhancing efficiency. By optimizing both the input representation and the inference process, DiSC-AMC achieves significant improvements in token and parameter efficiency without sacrificing accuracy.

## Foundational Learning
- **Higher-order statistics in wireless signals:** Capture complex signal characteristics beyond basic features; quick check: verify statistical moments (skewness, kurtosis) are computed correctly
- **In-context learning for LLMs:** Enables adaptation without fine-tuning; quick check: ensure exemplar prompts are relevant and diverse
- **Symbolic token representation:** Converts continuous data into discrete symbols for efficient processing; quick check: validate tokenization preserves signal distinctions
- **Exemplar list pruning:** Reduces computational load by filtering irrelevant examples; quick check: confirm pruning criteria maintain classification accuracy
- **Constrained label-only predictions:** Focuses model output on essential classification results; quick check: ensure no extraneous information is included in predictions

## Architecture Onboarding

**Component Map:**
Higher-order statistics -> Symbolic tokenization -> Lightweight classifier pruning -> Constrained label-only prediction -> AMC classification

**Critical Path:**
The critical path involves converting higher-order statistics to symbolic tokens, pruning exemplars, and enforcing constrained predictions. Each step must maintain signal integrity while reducing computational complexity.

**Design Tradeoffs:**
- Tokenization granularity vs. information preservation
- Pruning aggressiveness vs. classification accuracy
- Model size vs. inference efficiency
- Synthetic vs. real-world data generalization

**Failure Signatures:**
- Loss of signal distinction in tokenization
- Over-pruning leading to classification errors
- Constrained predictions missing nuanced distinctions
- Poor generalization to unseen modulation types

**First Experiments:**
1. Test tokenization on synthetic AMC with varying signal-to-noise ratios
2. Evaluate pruning effectiveness across different exemplar list sizes
3. Compare constrained vs. unconstrained predictions on real-world AMC datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluated only on synthetic AMC data with ten modulation types
- Significant accuracy gap between proposed method and baseline raises questions about baseline conditions
- Limited real-world validation and unclear generalization across different AMC scenarios

## Confidence

**High confidence:**
- Core efficiency claims (reduced prompt length and model size by more than half)

**Medium confidence:**
- Accuracy claims given the significant gap between proposed 45.5% and baseline 5.2% results
- Practical applicability due to limited real-world validation

## Next Checks
1. Test DiSC-AMC on real-world over-the-air AMC datasets to assess robustness beyond synthetic conditions
2. Compare against multiple strong baselines using identical experimental protocols to validate the 5.2% baseline result
3. Evaluate DiSC-AMC's performance across varying numbers of exemplar prompts and different LLM model families to confirm scalability and generalizability