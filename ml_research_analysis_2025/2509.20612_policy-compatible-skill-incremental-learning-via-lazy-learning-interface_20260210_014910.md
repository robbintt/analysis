---
ver: rpa2
title: Policy Compatible Skill Incremental Learning via Lazy Learning Interface
arxiv_id: '2509.20612'
source_url: https://arxiv.org/abs/2509.20612
tags:
- skill
- learning
- policy
- sil-c
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles skill-policy compatibility challenges in skill
  incremental learning (SIL), where evolving skill sets can disrupt compatibility
  with downstream policies, limiting their reusability and generalization. The authors
  propose SIL-C, a novel framework that ensures skill-policy compatibility by introducing
  a bilateral lazy learning-based mapping interface.
---

# Policy Compatible Skill Incremental Learning via Lazy Learning Interface

## Quick Facts
- arXiv ID: 2509.20612
- Source URL: https://arxiv.org/abs/2509.20612
- Reference count: 40
- Primary result: Maintains skill-policy compatibility during incremental learning through bilateral lazy mapping interface

## Executive Summary
This paper addresses skill-policy compatibility challenges in skill incremental learning where evolving skill sets can disrupt downstream policy performance. The authors propose SIL-C, a framework that ensures compatibility by introducing a bilateral lazy learning-based mapping interface. This interface dynamically aligns subtask space from policies with skill space based on trajectory distribution similarity, enabling appropriate skill selection without requiring policy re-training. Evaluation across diverse SIL scenarios demonstrates that SIL-C maintains compatibility while ensuring efficiency throughout the learning process.

## Method Summary
SIL-C tackles skill incremental learning by inserting a bilateral lazy learning interface between high-level policies and low-level decoders. The method uses append-only prototype memories to store Gaussian distributions characterizing both skills and subtasks. At inference time, it performs trajectory similarity matching between subtasks and skills using Mahalanobis distance, deferring alignment decisions until runtime. The framework includes skill validation via OOD detection to prevent execution of misaligned skills, and skill hooking to remap incompatible subtasks to valid skills. The approach preserves existing training procedures while ensuring backward and forward compatibility across skill evolution phases.

## Key Results
- Maintains backward compatibility (BwSC) when policies are evaluated with updated skill sets
- Achieves robust performance under observation noise through skill validation mechanisms
- Demonstrates 10% AUC improvement over baseline methods in compatibility metrics
- Shows efficient inference with only ~1ms increase in runtime due to prototype matching

## Why This Works (Mechanism)

### Mechanism 1: Bilateral Lazy Mapping for Decoupled Alignment
The bilateral lazy learning interface decouples skill evolution from policy stability by treating alignment as a retrieval problem. Instead of training a neural network to map subtasks to skills, it stores raw data prototypes and defers decisions to inference time using distance metrics. This "Skill Hooking" mechanism allows policies to reuse skills without re-training. The core assumption is that subtask intentions and skill capabilities can be characterized by Gaussian prototypes in state-subgoal space, enabling reliable distance-based matching. Break condition occurs when skill and subtask distributions drift such that prototype overlap is minimal.

### Mechanism 2: Skill Validation via Out-of-Distribution Detection
The interface validates proposed subtasks against predicted subgoals to prevent execution of misaligned skills. It calculates Mahalanobis distance between policy-proposed skills and predicted requirements, rejecting skills that exceed a chi-square quantile threshold. The core assumption is that the validation threshold effectively separates feasible execution from hallucinated outputs. Break condition occurs when input noise exceeds expected variance, causing valid skills to be erroneously rejected.

### Mechanism 3: Append-Only Non-Destructive Memory
The framework uses append-only memory for prototypes to prevent forgetting and preserve backward compatibility. Unlike neural network weights that are overwritten during fine-tuning, the interface stores skill and subtask prototypes in a memory buffer that strictly grows. New skills are added as new keys while old keys remain untouched, ensuring old policies can reference original skill definitions. Break condition occurs when storage costs become prohibitive or retrieval speed degrades with thousands of skills.

## Foundational Learning

- **Concept: Skill Incremental Learning (SIL)**
  - Why needed here: This is the core problem setting where agents must expand skill libraries over time without breaking existing high-level policies
  - Quick check question: Can you distinguish between "Forward Compatibility" (new skills helping new tasks) and "Backward Compatibility" (new skills helping/not breaking old tasks)?

- **Concept: Instance-Based (Lazy) Learning**
  - Why needed here: SIL-C uses "lazy" learning by storing raw data prototypes and deferring decisions to inference time, avoiding structural adaptation of the policy
  - Quick check question: Why does deferring the mapping decision to inference time help avoid "structural adaptation" of the policy?

- **Concept: Mahalanobis Distance & Gaussian Prototypes**
  - Why needed here: The interface models subtasks and skills as distributions (Gaussians), using Mahalanobis distance to account for variance when determining if a subtask is inside a skill's domain
  - Quick check question: Why would Euclidean distance fail in the "Skill Validation" step when trying to detect out-of-distribution subtasks?

## Architecture Onboarding

- **Component map:**
  - $\pi_h$ (High-Level Policy) -> Interface ($I$) -> $\pi_l$ (Low-Level Decoder)
  - Interface contains Task-Side Module ($\Psi^h_s$), Skill-Side Modules ($\Psi^l_g, \Psi^l_s$)
  - Memories: Append-only buffers storing $(\mu, \Sigma)$ prototypes

- **Critical path:**
  1. Observe State: Agent receives $s$
  2. Policy Proposal: $\pi_h$ samples subtask $z_h$
  3. Goal Prediction: Interface predicts subgoal $g$ from $s$ (Task-side)
  4. Validation: Check if $z_h$ is compatible with $g$ (Skill-side)
  5. Execution: If valid, execute $z_h$; if invalid, find new skill $z_l$ compatible with $(s, g)$ and execute $z_l$

- **Design tradeoffs:**
  - Memory vs. Plasticity: Append-only design ensures 100% retention but requires growing memory
  - Mahalanobis vs. Euclidean: Mahalanobis is significantly better for BwSC (+10% AUC) but computationally heavier
  - Noise Sensitivity: Validation threshold $\delta_c$ is critical - too low rejects valid skills, too high accepts hallucinations

- **Failure signatures:**
  - "Policy Stall": Interface rejects every skill proposed by $\pi_h$, falling back to defaults or failing
  - "Catastrophic Drift": Baseline fails (BWT drops) but SIL-C maintains; if SIL-C also fails, check if "Skill Hooking" retrieves distinct skills
  - "Memory Explosion": Long runs cause inference latency spikes if skill clustering generates too many prototypes

- **First 3 experiments:**
  1. Reproduce the Compatibility Gap: Run baseline vs. SIL-C on Kitchen environment, plot BWT over 4 phases
  2. Noise Robustness Stress Test: Inject observation noise (scale Ã—3), verify skill validation filters noisy outputs
  3. Interface Ablation: Disable "Skill Hooking" (force $z_l = z_h$), observe drop in Backward Compatibility

## Open Questions the Paper Calls Out

- **Open Question 1**: How can skill clustering be adapted for diverse or highly noisy skill distributions where unsupervised methods fail?
  - Basis: Conclusion notes that while the method works with well-defined representations, future work must address settings where unsupervised clustering becomes less effective

- **Open Question 2**: Can incorporating minimal goal information during exploration significantly improve sample efficiency in the few-shot setting?
  - Basis: Conclusion suggests that leveraging minimal goal information for exploration can improve sample efficiency

- **Open Question 3**: How can the interface monitor skill reliability and distribution shifts during deployment to ensure safer execution?
  - Basis: Authors identify monitoring skill reliability and distribution shifts during deployment as necessary for safer execution

## Limitations

- The framework assumes Gaussian prototypes can adequately characterize subtask and skill distributions, which may not hold in high-dimensional or sparse reward environments
- Append-only memory design raises scalability concerns for truly open-ended learning with thousands of skills
- Fixed chi-square threshold for OOD detection may not adapt well to heterogeneous skill distributions
- Paper does not address potential adversarial scenarios where malicious policies could exploit the validation mechanism

## Confidence

- **High Confidence**: SIL problem formulation and general architecture of using lazy learning interface for compatibility are well-supported
- **Medium Confidence**: Implementation details of interface (subgoal binning, K-means clustering parameters) rely on external implementations and are not fully specified
- **Low Confidence**: Long-term scalability of append-only memory and robustness of validation threshold to extreme noise or distribution shift are not rigorously tested

## Next Checks

1. **Prototype Quality Analysis**: Conduct ablation study replacing Gaussian prototype fitting with alternative density estimation methods (e.g., normalizing flows), measure impact on BwSC to isolate contribution of Mahalanobis validation

2. **Memory Scaling Experiment**: Simulate open-ended learning scenario by expanding skill library to 1000+ skills, measure inference latency and matching accuracy to quantify trade-off between memory growth and performance

3. **Adversarial Policy Test**: Design controlled experiment where high-level policy generates subtasks just outside validation threshold, forcing interface into "hook and fail" loop to stress-test OOD detection and fallback mechanism robustness