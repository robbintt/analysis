---
ver: rpa2
title: Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large
  Language Models
arxiv_id: '2511.06793'
source_url: https://arxiv.org/abs/2511.06793
tags:
- influential
- unlearning
- knowledge
- neuron
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning in multimodal
  large language models (MLLMs), focusing on the need to selectively forget specific
  knowledge while preserving general model performance. Existing approaches struggle
  with inconsistent forgetting across modalities and disruption of general reasoning
  paths.
---

# Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2511.06793
- Source URL: https://arxiv.org/abs/2511.06793
- Authors: Kunhao Li; Wenhao Li; Di Wu; Lei Yang; Jun Bai; Ju Jia; Jason Xue
- Reference count: 40
- Key outcome: MIP-Editor achieves up to 87.75% forgetting rate and 54.26% improvement in general knowledge retention on multimodal tasks, while achieving 80.65% forgetting with 77.9% retention on textual tasks.

## Executive Summary
This paper introduces MIP-Editor, a framework for machine unlearning in multimodal large language models (MLLMs) that addresses the challenge of selectively forgetting specific knowledge while preserving general model performance. The key innovation is a dual-branch influential neuron path localization framework that captures cross-layer information flow better than point-wise neuron selection. By using inter-layer gradient-integrated and Fisher-integrated attribution scores, MIP-Editor identifies structured neuron paths for both text and vision modalities, then applies path-aware editing via representation misdirection unlearning to steer forget-set representations away from original semantics while reinforcing retain-set knowledge.

## Method Summary
MIP-Editor operates through a two-stage process: first, it locates influential neuron paths using inter-layer gradient integration (IGI) for text and inter-layer Fisher integration (IFI) for vision modalities, identifying one influential neuron per layer via greedy layer-wise search. Second, it applies path-aware editing through representation misdirection unlearning (RMisU), which steers forget-set representations toward random semantic directions while anchoring retain-set embeddings to original representations. The method uses LoRA adapters for fine-tuning only the pruned neurons, with the Adam optimizer at learning rate 2e-5 for 4 epochs.

## Key Results
- On multimodal tasks, MIP-Editor achieves up to 87.75% forgetting rate while maintaining 54.26% improvement in general knowledge retention
- On textual tasks, achieves 80.65% forgetting with 77.9% retention of general performance
- Demonstrates superior performance over point-wise neuron selection methods, with path-based selection achieving higher ROUGE-L while retaining fewer neurons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured neuron paths capture cross-layer information flow better than point-wise neuron selection for targeted unlearning.
- Mechanism: Inter-layer gradient integration (IGI) traces how gradients flow across sequential FFN layers for text inputs. Inter-layer Fisher integration (IFI) captures second-order importance signals for visual neurons. Greedy layer-wise search identifies one influential neuron per layer, forming a path that maximizes attribution scores.
- Core assumption: Knowledge in MLLMs is transmitted through structured, layer-wise neuron pathways rather than isolated neurons.
- Evidence anchors: Abstract states point-wise attribution methods fail to capture structured, layer-by-layer information flow. Equations 3-6 define IGI and IFI scores. Corpus reference to Modality-Aware Neuron Pruning for Unlearning in MLLMs addresses related point-wise neuron selection but lacks path-level modeling.

### Mechanism 2
- Claim: Pruning only neurons along influential paths preserves general reasoning paths that share neurons with the forget-set.
- Mechanism: After identifying influential paths, set activations of path neurons to zero. This blocks forget-set information flow while leaving non-path neurons untouched, including those supporting retain-set knowledge.
- Core assumption: Forget-set and retain-set knowledge partially overlap in shared neurons, but influential paths contain sufficient forget-specific signal for targeted removal.
- Evidence anchors: Abstract mentions pruning disrupts model's ability to generalize. Section 4.4 shows path-based selection achieves higher ROUGE-L than point-wise with fewer neurons retained. Corpus references to neuron editing (DEPN, MANU) use point-wise pruning without path awareness.

### Mechanism 3
- Claim: Representation misdirection steers forget-set embeddings toward random semantic space while anchoring retain-set embeddings to original representations.
- Mechanism: Sample random unit vector, scale by frozen model's hidden representation norm. Minimize L2 distance between current forget representations and this random target. Minimize L2 distance between current retain representations and frozen model's retain representations.
- Core assumption: Forcing forget-set representations into a randomized direction effectively erases semantic content without corrupting the embedding space for other inputs.
- Evidence anchors: Abstract states "steer forget-set representations away from original semantics while reinforcing retain-set knowledge." Section 3.2 defines RMisU loss formulation. Corpus reference to Energy-Regularized Sequential Model Editing on Hyperspheres explores related representation-space interventions.

## Foundational Learning

- Concept: Gradient-based attribution for neuron importance
  - Why needed here: IGI computes how scaling neuron activations from 0 to original values affects output probability, requiring understanding of gradient integration methods.
  - Quick check question: Given a simple 2-layer network, can you trace how changing layer-1 neuron activation affects layer-2 output gradients?

- Concept: Fisher Information Matrix for second-order optimization
  - Why needed here: IFI uses squared gradients as FIM diagonal approximation to estimate visual neuron importance, which handles high-dimensional, redundant vision features better than first-order gradients.
  - Quick check question: Why might Fisher-based importance be more stable than gradient-based importance for vision encoders with spatial redundancy?

- Concept: Representation-level vs. weight-level model editing
  - Why needed here: RMisU operates on hidden representations rather than directly modifying weights, which requires understanding how intermediate-layer perturbations propagate to outputs.
  - Quick check question: If you perturb layer-15 hidden states toward a random vector, how does this differ from directly modifying layer-15 weights?

## Architecture Onboarding

- Component map:
  - Text branch: LLM FFN layers → IGI path localization
  - Vision branch: Vision encoder FFN layers → IFI path localization
  - RMisU editing layer: Select one layer l for representation misdirection
  - Loss combiner: L_total = L_forget_RMisU + γ * L_retain_RMisU

- Critical path:
  1. Forward pass through frozen MLLM on forget/retain batches → cache hidden states
  2. Compute IGI/IFI scores via m-step Riemann approximation (gradient accumulation across layers)
  3. Greedy path selection: for each layer, pick neuron maximizing path score
  4. Zero activations of selected path neurons
  5. Fine-tune only path neuron weights using RMisU objective

- Design tradeoffs:
  - Top-k neurons per layer vs. single best neuron: Paper uses k=1 per layer; higher k may capture more knowledge but risks retain-set degradation
  - Riemann steps m: More steps → better gradient approximation but O(m) cost increase
  - Layer l for RMisU: Mid-to-late layers likely capture higher-level semantics; choice affects forgetting granularity
  - Scaling coefficient λ: Larger λ → stronger misdirection but potential embedding space distortion

- Failure signatures:
  - Retain-set accuracy collapses (<30%): Path localization too aggressive; reduce top-k or add retain-set neurons to protected set
  - Forget-set accuracy remains high (>30%): RMisU insufficient; increase λ or try alternative random direction sampling
  - Model generates gibberish: Random target direction corrupted; check u sampling from unit sphere
  - Text-only forgetting fails but multimodal works: Check IGI path quality separately from IFI; may need different m values

- First 3 experiments:
  1. Reproduce path vs. point-wise comparison on MLLMU-Bench 5% forget ratio: Plot ROUGE-L vs. top-k neurons for both strategies on forget/retain sets to validate path superiority.
  2. Ablate RMisU by replacing with standard gradient ascent: Compare forgetting rate and retain-set accuracy to quantify RMisU contribution.
  3. Cross-model transfer test: Locate influential paths on Qwen2.5-VL-3B, apply same neuron indices to LLaVA-1.5-7B → assess whether paths generalize across architectures (expect low transfer).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the effectiveness of machine unlearning in MLLMs be rigorously evaluated for textual generation tasks where standard semantic overlap metrics fail?
- **Basis in paper:** The authors note that for textual generation tasks, the performance gap is "less pronounced due to the limitations of Rouge-L, which measures only semantic overlap and may not capture forget-set-related differences effectively."
- **Why unresolved:** Current metrics do not distinguish between a model generating a fluent, semantically unrelated response versus one that retains factual remnants of the forget-set in a paraphrased manner.
- **What evidence would resolve it:** The development of a probing-based evaluation metric or benchmark specifically designed to detect residual factual knowledge in open-ended generation.

### Open Question 2
- **Question:** Does the dual-branch influential neuron path localization scale efficiently to MLLMs with significantly larger parameter counts (e.g., >70B) or alternative architectures?
- **Basis in paper:** Experiments are limited to 3B and 7B models, and the complexity analysis suggests a high computational cost that increases quadratically with layers and neurons.
- **Why unresolved:** The greedy search algorithm requires iterating over candidate neurons with gradient/Fisher integration, which may become computationally prohibitive for industrial-scale models.
- **What evidence would resolve it:** Performance benchmarks and wall-clock time measurements on 70B+ parameter models or Mixture-of-Experts (MoE) architectures.

### Open Question 3
- **Question:** How robust is the method when the "forget-set" and "retain-set" share highly overlapping influential neuron paths?
- **Basis in paper:** While the paper claims to decouple specific and general knowledge, Table 1 shows a notable performance drop in the retain-set, suggesting the pruning or misdirection disrupts shared reasoning paths.
- **Why unresolved:** The method assumes distinct paths can be located for specific vs. general knowledge, but in complex semantic domains, these representations may be inextricably linked.
- **What evidence would resolve it:** Experiments using datasets specifically designed with high semantic similarity between the forget-set and retain-set samples.

## Limitations
- Assumes knowledge propagates through structured neuron paths rather than being distributed across parallel pathways, which may not hold for all types of forget-set knowledge
- RMisU mechanism assumes steering forget-set representations toward random directions won't interfere with semantically related concepts that happen to align with the sampled random vector
- Does not validate whether influential paths can be reliably transferred across different MLLM architectures

## Confidence
- **High Confidence**: The mathematical formulation of IGI/IFI and the RMisU objective are well-specified and internally consistent. The experimental setup and evaluation metrics are clearly defined.
- **Medium Confidence**: The superiority of path-based selection over point-wise methods is demonstrated on specific benchmarks, but the general robustness across different unlearning scenarios remains to be tested.
- **Low Confidence**: The claim that influential paths can be reliably transferred across MLLM architectures is not experimentally verified and may not hold in practice.

## Next Checks
1. **Path Generalization Test**: Apply influential neuron paths identified in Qwen2.5-VL-3B to LLaVA-1.5-7B and measure forgetting performance to assess cross-architecture transfer.
2. **Parallel Path Sensitivity**: Evaluate MIP-Editor's performance when forget-set knowledge is distributed across multiple parallel neuron paths rather than concentrated in one dominant path.
3. **Random Direction Robustness**: Test whether RMisU consistently erases forget-set knowledge when random unit vectors are sampled multiple times, or if some directions inadvertently preserve semantic content.