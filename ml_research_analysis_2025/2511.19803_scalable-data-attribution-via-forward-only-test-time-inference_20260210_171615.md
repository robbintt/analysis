---
ver: rpa2
title: Scalable Data Attribution via Forward-Only Test-Time Inference
arxiv_id: '2511.19803'
source_url: https://arxiv.org/abs/2511.19803
tags:
- training
- attribution
- data
- influence
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently attributing model
  predictions to training data, a key task for model debugging, auditing, and data
  valuation. Traditional influence-function methods, while principled, are computationally
  prohibitive due to expensive backward passes or Hessian inversion at inference.
---

# Scalable Data Attribution via Forward-Only Test-Time Inference

## Quick Facts
- arXiv ID: 2511.19803
- Source URL: https://arxiv.org/abs/2511.19803
- Authors: Sibo Ma; Julian Nyarko
- Reference count: 35
- Primary result: Forward-only test-time inference method for data attribution matching or exceeding SOTA on MLP benchmarks while offering orders-of-magnitude lower inference cost

## Executive Summary
This paper addresses the challenge of efficiently attributing model predictions to training data, a key task for model debugging, auditing, and data valuation. Traditional influence-function methods, while principled, are computationally prohibitive due to expensive backward passes or Hessian inversion at inference. The authors propose a method that shifts all heavy computation to training-time simulation by perturbing each training example's influence through short-horizon gradient propagation, storing the resulting parameter responses. At inference, attributions are computed via two forward passes, eliminating the need for backward passes. The method targets the same first-order counterfactual as classical influence functions but achieves it with only forward evaluations.

## Method Summary
The proposed approach performs training-time simulation by perturbing each training example's influence through short-horizon gradient propagation and storing the resulting parameter responses. During inference, attributions are computed through two forward passes without requiring backward passes or Hessian inversion. This design shifts computational burden to training time while maintaining test-time efficiency, targeting the same first-order counterfactual approximations as classical influence functions but with significantly reduced inference complexity.

## Key Results
- Matches or exceeds state-of-the-art methods like TRAK on standard metrics (LOO and LDS) for MLP benchmarks
- Achieves orders-of-magnitude lower inference cost compared to traditional influence-function methods
- Demonstrates practical scalability for large pretrained models serving billions of queries

## Why This Works (Mechanism)
The method leverages the observation that influence functions can be approximated through parameter perturbations during training, which can be stored and reused at inference time. By performing short-horizon gradient propagation during training to capture each example's influence on model parameters, the approach enables fast forward-pass computation at test time. This trades training-time storage and computation for test-time efficiency, maintaining the first-order counterfactual approximation quality of classical influence functions while eliminating their computational bottlenecks.

## Foundational Learning
- **Influence functions**: Measure how upweighting or removing training points affects model predictions. Needed to understand the theoretical foundation of data attribution. Quick check: Verify that parameter perturbations during training capture the same counterfactuals as classical influence computations.
- **Forward vs backward passes**: Forward passes compute predictions from parameters, backward passes compute gradients. Understanding this distinction is crucial since the method eliminates backward passes at inference. Quick check: Confirm that two forward passes can reconstruct the influence without gradient computation.
- **Short-horizon gradient propagation**: Limited-depth gradient updates during training. This is the key mechanism for capturing training data influence efficiently. Quick check: Determine optimal horizon depth for balancing accuracy and computational cost.

## Architecture Onboarding
**Component map**: Training examples -> Short-horizon gradient propagation -> Parameter response storage -> Inference forward passes -> Attribution scores

**Critical path**: The method's performance bottleneck occurs during training when computing and storing parameter responses for each training example, which scales with dataset size. At inference, the two forward passes are computationally trivial.

**Design tradeoffs**: The approach trades training-time storage and computation for test-time efficiency. While this dramatically reduces inference cost, it requires storing parameter responses for all training examples, which could be prohibitive for very large datasets.

**Failure signatures**: Poor attribution quality may occur if the short-horizon gradient propagation depth is insufficient to capture long-range dependencies in the model's parameter space. Storage limitations during training could also prevent the method from scaling to massive datasets.

**First experiments**:
1. Verify that parameter perturbations during training capture the same counterfactuals as classical influence computations on a small MLP
2. Benchmark inference time improvement of the forward-only approach versus traditional influence functions
3. Test attribution quality degradation as training horizon depth decreases

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited to MLP benchmarks, leaving uncertainty about performance on complex architectures like transformers or vision models
- Storage requirements for parameter responses during training-time simulation could become prohibitive for large-scale models and datasets
- Method's sensitivity to hyperparameters controlling short-horizon gradient propagation depth and perturbation count remains unclear

## Confidence
- Scalability and inference efficiency (High): The forward-only approach with two passes is clearly more efficient than traditional influence-function methods requiring backward passes or Hessian inversion.
- Attribution quality matching SOTA (Medium): While results on MLP benchmarks are promising, limited model diversity and dataset complexity reduce confidence in generalizability.
- Storage requirements during training (Medium): The paper claims efficiency gains but doesn't provide detailed analysis of the memory overhead for storing parameter responses across large training sets.

## Next Checks
1. Benchmark the method on transformer architectures and vision models to assess scalability beyond MLPs, measuring both attribution quality and storage requirements.
2. Conduct ablation studies varying the short-horizon gradient propagation depth and perturbation count to understand sensitivity to these hyperparameters.
3. Test the method on real-world datasets with millions of training examples to evaluate practical memory constraints and inference time improvements compared to traditional influence functions.