---
ver: rpa2
title: 'Chinese Stock Prediction Based on a Multi-Modal Transformer Framework: Macro-Micro
  Information Fusion'
arxiv_id: '2501.16621'
source_url: https://arxiv.org/abs/2501.16621
tags:
- time
- data
- event
- impact
- market
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a Multi-Modal Transformer framework (MMF-Trans)
  to improve Chinese stock market prediction by integrating four data modalities:
  technical indicators, financial text, macro data, and event knowledge graphs. The
  framework employs a four-channel parallel encoder with specialized modules for each
  modality, a dynamic gated fusion mechanism for adaptive weight allocation, and a
  time-aligned mixed-frequency processing layer using three-stage position encoding
  to solve heterogeneous data alignment issues.'
---

# Chinese Stock Prediction Based on a Multi-Modal Transformer Framework: Macro-Micro Information Fusion

## Quick Facts
- **arXiv ID:** 2501.16621
- **Source URL:** https://arxiv.org/abs/2501.16621
- **Reference count:** 8
- **Primary result:** MMF-Trans reduces RMSE by 23.7% vs. baselines, improves event response prediction accuracy by 41.2%, and enhances Sharpe ratio by 32.6%

## Executive Summary
This paper introduces MMF-Trans, a multi-modal Transformer framework for Chinese stock prediction that integrates four data modalities: technical indicators, financial text, macro data, and event knowledge graphs. The framework employs a four-channel parallel encoder with specialized modules for each modality, dynamic gated fusion for adaptive weight allocation, and three-stage position encoding to solve heterogeneous data alignment issues. A graph attention-based event impact quantification module captures dynamic market responses to events. Experiments on CSI 300 constituent stocks show significant performance improvements over baseline models, with the theoretical contribution including convergence proof under Lipschitz continuity conditions.

## Method Summary
The MMF-Trans framework processes four parallel data channels: technical indicators (processed via DWT and TCN), financial text (encoded with FinBERT-Chinese and cross-attention), macro data (processed via MF-LSTM with interpolation), and event knowledge graphs (encoded via GAT). These modalities are fused through a dynamic gated mechanism that learns adaptive weights, then processed through a time-aligned Transformer with three-stage position encoding (calendar, event, decay). The model outputs both stock price regression and event response classification using dual-task learning with combined L2 and focal loss functions.

## Key Results
- Reduces RMSE by 23.7% compared to baseline models
- Improves event response prediction accuracy by 41.2%
- Enhances Sharpe ratio by 32.6%
- Outperforms state-of-the-art models including TFT, Informer, and Autoformer
- Achieves convergence proof under Lipschitz continuity conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic gated fusion adaptively prioritizes modalities based on market regime.
- **Mechanism:** The fusion module computes softmax-normalized weights α_k = exp(w_k^T h_k) / Σ_j exp(w_j^T h_j) over four encoded modalities. During high volatility, technical indicator weights may increase; during policy announcements, event/text weights rise. This enables context-sensitive information integration rather than static weighting.
- **Core assumption:** The optimal modality importance varies predictably with market conditions and can be learned via gradient descent from historical data.
- **Evidence anchors:**
  - [abstract] "A dynamic gated cross-modal fusion mechanism that adaptively learns the importance of different modalities through differentiable weight allocation"
  - [Section 2.3] Explicit formula for dynamic gated fusion with backpropagation-learned weights
  - [corpus] StockMem (FMR=0.48) uses memory-based event reflection; related but different fusion strategy. No direct corpus comparison of gated vs. static fusion.
- **Break condition:** If market regime shifts faster than gradient updates can adapt weights (e.g., black swan events outside training distribution), the gating may lag. If modalities are highly correlated, learned weights may become unstable.

### Mechanism 2
- **Claim:** Three-stage position encoding solves heterogeneous frequency alignment.
- **Mechanism:** Instead of standard positional encoding, the model uses PosEnc(t) = Σ_m γ_m · Enc_m(t) where: (1) Calendar Encoding captures weekly/seasonal periodicity via sin/cos; (2) Event Encoding marks policy release times via Gaussian kernels; (3) Decay Encoding models exponential decay of event impact over time. This allows the Transformer to jointly process minute-level, daily, quarterly, and monthly data.
- **Core assumption:** Event impacts follow a learnable exponential decay pattern; calendar periodicity is consistent across the training period.
- **Evidence anchors:**
  - [abstract] "A time-aligned mixed-frequency processing layer that uses an innovative position encoding method"
  - [Section 2.4] Detailed formulas for three-stage encoding with learnable decay coefficients
  - [corpus] Weak direct evidence—no corpus paper explicitly validates three-stage encoding. MiMIC (FMR=0.41) addresses multi-modal earnings calls but uses standard temporal alignment.
- **Break condition:** If event decay patterns are non-monotonic (e.g., delayed market reactions, second-order ripple effects), the exponential decay assumption fails. If calendar patterns shift (regulatory trading hour changes), calendar encoding may misalign.

### Mechanism 3
- **Claim:** Graph attention over event knowledge graphs quantifies political event propagation.
- **Mechanism:** Events are nodes in a knowledge graph; edges encode relationships (e.g., "policy A affects industry B"). GAT computes attention weights α_ij between event nodes, aggregating neighbor features to derive an "event impact coefficient" for each stock. Event2Vec algorithm learns event embeddings for impact quantification.
- **Core assumption:** Event-event relationships are stable and captureable via the knowledge graph structure; market reactions to events follow graph-propagation patterns.
- **Evidence anchors:**
  - [abstract] "A graph attention-based event impact quantification module that captures the dynamic impact of events on the market"
  - [Section 2.2.4] GAT formulas with LeakyReLU attention; Section 5.2 shows carbon neutrality policy impact (coefficient 0.92 for new energy, -0.78 for traditional energy, 63-day duration)
  - [corpus] Structured Event Representation (FMR=0.51) uses LLM-extracted event features for prediction, supporting event-based approaches but not graph-specific validation.
- **Break condition:** If novel event types lack graph edges (cold-start problem), GAT cannot propagate information. If event relationships are time-varying (policy reversals), static graph topology becomes stale.

## Foundational Learning

- **Wavelet Transform + Temporal Convolutional Networks (TCN):**
  - Why needed here: Technical indicator channel uses DWT to decompose price series into frequency sub-bands, then TCN for dilated temporal convolution. This captures multi-scale patterns that pure attention may miss.
  - Quick check question: Can you explain how wavelet decomposition separates high-frequency noise from low-frequency trends?

- **Cross-Attention for Multi-Modal Fusion:**
  - Why needed here: Financial text channel uses cross-attention where stock features query (Q) text features (K, V), allowing the model to attend to relevant portions of financial reports per stock.
  - Quick check question: Given Q from modality A and K,V from modality B, what does softmax(QK^T/√d_k)V compute?

- **Graph Attention Networks (GAT):**
  - Why needed here: Event knowledge channel requires learning which events influence which others; GAT learns edge importance α_ij via attention rather than fixed weights.
  - Quick check question: How does GAT differ from GCN in aggregating neighbor node information?

## Architecture Onboarding

- **Component map:**
  ```
  Input Layer: 4 parallel channels
    ├── Technical: DWT(4 levels) → Dilated TCN → h_T
    ├── Financial Text: FinBERT-Chinese → Cross-Attention → h_F
    ├── Macro Data: MF-LSTM + Linear Interpolation → h_M
    └── Event Graph: GAT encoder → h_E

  Fusion Layer:
    ├── Dynamic Gated Fusion: [h_T, h_F, h_M, h_E] → weighted sum
    ├── Time-Aligned Transformer: 6 layers, 8 heads
    └── Event Impact Propagation Network

  Output Layer:
    ├── Stock Price Regression (L2 loss)
    └── Event Response Classification (Focal Loss)
  ```

- **Critical path:**
  1. Data alignment: Mixed-frequency inputs must be interpolated to common timestamps before encoding.
  2. Event graph construction: Knowledge graph edges must exist for GAT to propagate; missing edges = zero impact signal.
  3. Gated fusion weights: If weights collapse to single modality (degenerate softmax), information from other channels is lost.

- **Design tradeoffs:**
  - **Complexity vs. interpretability:** 4-channel architecture + GAT + 3-stage encoding yields 8.5h training time (vs. 6.8h for TFT baseline) for 23.7% RMSE improvement.
  - **Event graph vs. direct text:** Knowledge graph provides structured propagation but requires manual/semi-automated graph construction; pure text (FinBERT-only) is simpler but misses cross-event relationships.
  - **Shared vs. separate decoders:** Dual-task optimization (regression + classification) shares representations but may suffer from gradient conflicts; separate decoders increase parameters.

- **Failure signatures:**
  - **Gate collapse:** All α_k converge to one modality → ablation shows w/o Text Fusion: +23.1% RMSE; w/o Event Graph: +15.4% RMSE.
  - **Position encoding mismatch:** If decay coefficient γ_m learns too fast, event impact vanishes prematurely; if too slow, noise accumulates.
  - **Graph sparsity:** For stocks with few event associations, h_E provides weak signal; check attention distribution across event nodes.

- **First 3 experiments:**
  1. **Single-modality baseline:** Train each channel independently (T-only, F-only, M-only, E-only) to establish per-modality performance ceiling and verify each encoder functions correctly.
  2. **Ablation by position encoding stage:** Remove Calendar/Event/Decay encoding one at a time; expect +7.7% RMSE (w/o Time Alignment) per paper's ablation table.
  3. **Event impact decay visualization:** Plot learned γ_m and predicted impact coefficients for known policy events (e.g., carbon neutrality announcement) against actual stock reactions to validate decay assumptions and detect systematic over/under-reaction.

## Open Questions the Paper Calls Out
- **Generalizability:** Apply the model to other financial markets to verify its generalizability (explicit).
- **Interpretability:** Study the interpretability of the model to make the model prediction results easier to understand and increase the transparency of the model (explicit).

## Limitations
- Event graph construction methodology is underspecified, making faithful reproduction uncertain
- Specific FinBERT-Chinese vocabulary and pre-training details not fully disclosed
- Weighting between regression and classification losses not provided
- Lacks out-of-sample robustness testing across different market regimes

## Confidence
- **High Confidence:** Technical indicator channel effectiveness (DWT + TCN is well-established), multi-task learning framework validity
- **Medium Confidence:** Dynamic gated fusion mechanism (theoretically sound but complex to implement correctly), three-stage position encoding benefits
- **Low Confidence:** Event graph construction and GAT implementation details, Event2Vec algorithm specifics

## Next Checks
1. **Event Graph Construction Validation:** Build a simplified event graph using entity co-occurrence and validate GAT performance against a text-only baseline
2. **Position Encoding Ablation Study:** Implement and compare all three position encoding stages separately to isolate their individual contributions
3. **Cross-Market Robustness Test:** Train the model on CSI 300 and test on a different Chinese index (e.g., Shenzhen Component) to evaluate generalization beyond the training distribution