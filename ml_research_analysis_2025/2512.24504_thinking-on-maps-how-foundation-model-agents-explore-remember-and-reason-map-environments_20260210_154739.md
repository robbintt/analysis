---
ver: rpa2
title: 'Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason
  Map Environments'
arxiv_id: '2512.24504'
source_url: https://arxiv.org/abs/2512.24504
tags:
- spatial
- memory
- reasoning
- exploration
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how foundation model agents explore, remember,
  and reason in symbolic map environments through interactive, experience-driven learning.
  Agents incrementally explore partially observable grid-based maps, receiving only
  local observations, and are then tested on spatial tasks including direction judgment,
  distance estimation, proximity judgment, POI density recognition, and path planning.
---

# Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments

## Quick Facts
- arXiv ID: 2512.24504
- Source URL: https://arxiv.org/abs/2512.24504
- Authors: Zhiwei Wei; Yuxing Liu; Hua Liao; Wenjia Xu
- Reference count: 17
- Key outcome: Structured memory representations substantially improve reasoning performance, especially on path planning and distance estimation, while exploration strategies have limited impact.

## Executive Summary
This study evaluates how foundation model agents explore, remember, and reason in symbolic map environments through interactive, experience-driven learning. Agents incrementally explore partially observable grid-based maps, receiving only local observations, and are then tested on spatial tasks including direction judgment, distance estimation, proximity judgment, POI density recognition, and path planning. The research systematically varies exploration strategies, memory representations (sequential, graph-based, map-based), and reasoning schemes. Results show that structured memory representations substantially improve reasoning performance, especially on path planning and distance estimation, while exploration strategies have a limited impact. Memory representation is more critical than exploration for consolidating spatial experience. Prompting and reasoning schemes further enhance multi-step inference, with advanced prompts benefiting weaker models most. The study concludes that scaling alone does not improve spatial reasoning; instead, mechanisms tailored to spatial representation and reasoning are essential for advancing foundation model agents' map-based spatial understanding.

## Method Summary
The study uses a 3-phase evaluation framework to test foundation model agents on spatial reasoning tasks in symbolic 20x20 grid environments derived from OpenStreetMap data for 15 cities. Agents explore with partial observability (5x5 window) using different strategies (Nearest-POI, Random-Visible, Task-Driven), employ various memory representations (Simple Dialogue, Node-Sequence, Graph, Map), and apply different reasoning schemes (Default Thought, Chain-of-Thought, Self-Consistency CoT, Tree-of-Thoughts). The five spatial reasoning tasks tested are direction judgment, distance estimation, proximity judgment, POI density recognition, and path planning. Performance is measured across five foundation models (GPT-5.2, Gemini-2.5-Pro, Claude-4.5, Qwen-3, DeepSeek-V3.2).

## Key Results
- Structured memory representations (especially Node-Sequence Memory) substantially improve reasoning performance compared to unstructured dialogue memory.
- Memory representation is more critical than exploration strategy for consolidating spatial experience and improving reasoning accuracy.
- Advanced reasoning schemes like Tree-of-Thoughts enhance multi-step inference, particularly benefiting weaker models.
- Scaling model size alone does not improve spatial reasoning; specialized mechanisms for spatial representation and reasoning are essential.

## Why This Works (Mechanism)

### Mechanism 1: Sequential Memory Consolidation over Unstructured Context
Structured Node-Sequence Memory (NSM) outperforms unstructured dialogue history because it explicitly preserves the temporal order and connectivity of local observations, mitigating the "forgetting" inherent in long dialogue contexts. The agent records a chronological sequence of visited nodes and routes taken, enabling cumulative spatial integration rather than relying on scattered context.

### Mechanism 2: Reasoning Schemes as Failure Repair for Heuristic Guessing
Tree-of-Thought prompting repairs spatial reasoning failures by shifting the model from single-path "heuristic guessing" to explicit candidate enumeration and verification. ToT forces the model to generate multiple candidate routes and evaluate them against criteria, leveraging structured memory to validate or reject hypotheses.

### Mechanism 3: Capability Saturation and the Scaling Ceiling
Spatial reasoning in map environments saturates with model parameter scaling alone; improvements require architectural inductive biases (memory/reasoning structures) rather than raw capacity. Beyond a certain threshold, foundation models stop improving on structure-intensive tasks via scaling because the bottleneck shifts from "reasoning power" to "availability of structured evidence."

## Foundational Learning

- **Concept:** Partially Observable Markov Decision Process (POMDP)
  - **Why needed here:** Agents operate with limited 5x5 view in 20x20 grid, requiring belief state maintenance through memory
  - **Quick check question:** Does the agent know the full map layout at t=0? (No, it must explore)

- **Concept:** Cognitive Maps vs. Topological Graphs
  - **Why needed here:** Paper distinguishes Sequential (trajectory), Graph (connectivity), and Map (metric) memory
  - **Quick check question:** Which memory type helps most with knowing "how far" vs "how to get there"?

- **Concept:** Tree-of-Thought (ToT) Reasoning
  - **Why needed here:** Phase III results depend on ToT to fix "premature commitment" failures
  - **Quick check question:** Does the model generate one answer path or multiple candidates to compare?

## Architecture Onboarding

- **Component map:** Environment (20x20 symbolic grid) -> Observation Interface (5x5 local window) -> Memory Module (SDM, NSM, GM, or MM) -> Agent Core (Foundation Model) -> Reasoning Layer (Prompt wrapper: DT, CoT, ToT)
- **Critical path:** 1. Exploration Phase: Agent moves (Nearest-POI/Random) → collects observations → updates External Memory 2. Consolidation Phase: Memory representation finalized 3. Reasoning Phase: Task prompt + Memory Context → Reasoning Scheme (ToT) → Answer
- **Design tradeoffs:** Memory Fidelity vs. Token Cost (NSM uses ~3x bits of Graph Memory), Generalization vs. Structure (GM excels at Path Planning but fails at Distance Estimation), Prompt Complexity (ToT helps weaker models but offers diminishing returns for stronger models)
- **Failure signatures:** Heuristic Guessing (answers immediately without memory evidence), Metric Collapse (fails distance/direction while acing path planning), Global Blindness (fails POI Density tasks regardless of configuration)
- **First 3 experiments:** 1. Sanity Check: Run "Nearest-POI" exploration on 5x5 map, compare SDM vs. NSM on simple Direction Judgment task 2. Stress Test: Run Default Thought vs. ToT on mid-tier model (Qwen/DeepSeek) using NSM for complex path planning 3. Scaling Verification: Run best configuration (NSM + ToT) on smallest vs. largest available model to confirm performance plateau

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on symbolic 20x20 grids from urban OSM data with fixed 5x5 observation window and narrow set of 5 reasoning tasks
- Claim that "scaling alone does not improve spatial reasoning" is based on limited model comparisons in symbolic domain and may not hold for architectures with built-in spatial inductive biases
- Performance ceiling for POI density recognition suggests fundamental limitation of local exploration for tasks requiring global aggregation, but paper doesn't explore hybrid approaches

## Confidence
- **High Confidence:** Structured memory (NSM) significantly outperforms unstructured memory (SDM) for spatial reasoning tasks
- **Medium Confidence:** Advanced reasoning schemes (ToT) repair failure modes like "premature commitment"
- **Low Confidence:** "Scaling alone does not improve spatial reasoning" is an extrapolation from limited symbolic domain comparisons

## Next Checks
1. **Cross-Domain Generalization Test:** Reproduce experiment using continuous, real-world satellite imagery or street-view panoramas instead of symbolic grids
2. **Architectural Ablation Study:** Implement agent using foundation model with spatial inductive biases (vision transformer) and compare with/without external memory/reasoning scaffolding
3. **Global Aggregation Experiment:** Design exploration strategy or hybrid agent to test if POI density recognition performance can be improved beyond local exploration limit