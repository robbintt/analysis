---
ver: rpa2
title: 'ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries'
arxiv_id: '2511.10855'
source_url: https://arxiv.org/abs/2511.10855
tags:
- pairwise
- program
- queries
- programs
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ExPairT-LLM is a new code selection algorithm that uses pairwise\
  \ queries to select the correct program from multiple LLM-generated candidates.\
  \ It introduces two new query types\u2014pairwise membership and pairwise equivalence\u2014\
  that are simpler for LLMs and enable robustness to errors through a tournament-based\
  \ refinement process."
---

# ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries

## Quick Facts
- arXiv ID: 2511.10855
- Source URL: https://arxiv.org/abs/2511.10855
- Reference count: 17
- Key outcome: ExPairT-LLM improves code selection pass@1 by +13.0% over state-of-the-art, using pairwise queries that are simpler for LLMs and robust to errors

## Executive Summary
ExPairT-LLM is a novel code selection algorithm that uses pairwise queries to identify the correct program from multiple LLM-generated candidates. The approach introduces two new query types—pairwise membership and pairwise equivalence—that are simpler for LLMs than direct multi-way selection. By clustering programs based on outputs and using a tournament-based refinement process, ExPairT-LLM achieves significant improvements in code selection accuracy while using only a small fraction of possible queries. The algorithm demonstrates +13.0% improvement in pass@1 over the state-of-the-art and up to +24.0% improvement for LLM reasoning models.

## Method Summary
ExPairT-LLM adapts exact learning theory to code selection by replacing traditional membership and equivalence queries with pairwise variants. The algorithm first clusters programs by their outputs on initial inputs, then uses pairwise membership queries in a tournament (Copeland's method) to select the best cluster. It then iteratively refines this selection through pairwise equivalence queries, validating any differentiating inputs provided by the LLM. The process continues until a pure cluster of equivalent programs is found, at which point the first program is returned as the solution.

## Key Results
- +13.0% improvement in pass@1 over B4 state-of-the-art on HumanEval
- Up to +24.0% improvement for LLM reasoning models (o1-mini, DeepSeek-R1)
- Only ~25 average queries versus O(|P|²) worst-case bound
- Both pairwise membership and equivalence queries are essential for performance

## Why This Works (Mechanism)

### Mechanism 1: Tournament-based Cluster Selection
Pairwise membership queries aggregated via Copeland's tournament method yield more robust cluster selection than single-shot LLM judgments. Instead of asking the LLM to select among all candidates, ExPairT-LLM clusters programs by outputs, then runs a round-robin tournament comparing output sets pairwise. This aggregates noisy judgments into a robust collective decision, leveraging LLMs' superior reliability at binary comparisons versus multi-way selection.

### Mechanism 2: Iterative Cluster Refinement
Iterative cluster refinement via differentiating inputs prevents returning incorrect programs from impure clusters. After selecting a cluster, ExPairT-LLM poses pairwise equivalence queries to identify non-equivalent programs. When the LLM finds programs that aren't equivalent and provides a differentiating input, the system validates this input by execution and uses it to split the cluster, continuing until all programs are verified equivalent.

### Mechanism 3: Execution-based Validation
Execution-based validation of LLM-provided differentiating inputs eliminates hallucinated counterexamples and grounds equivalence checking in objective program behavior. Every differentiating input is verified by executing the compared programs on it before acceptance, creating a verifiable handshake where the LLM proposes and execution disposes.

## Foundational Learning

- **Concept**: Exact Learning (Angluin 1987) - The algorithm adapts classical exact learning theory, specifically membership and equivalence queries, to LLM-appropriate pairwise variants. Quick check: In classical exact learning, what does an equivalence query return when the candidate concept is incorrect?

- **Concept**: Copeland's Method / Tournament Scoring - Cluster selection uses Copeland's method where each candidate earns points from head-to-head comparisons. Quick check: In a tournament with n clusters where one cluster is correct, what is the maximum possible score the correct cluster can achieve?

- **Concept**: Pass@1 Metric - The standard success-rate metric in code generation where a selection is successful if the returned program passes all test cases. Quick check: How is pass@1 computed for a code selection algorithm evaluated on a dataset of task-program-set pairs?

## Architecture Onboarding

- **Component map**: Input Handler -> Clustering Module -> Tournament Engine -> Equivalence Checker -> Refinement Controller -> Termination Decider
- **Critical path**: Initial clustering → Tournament selection → Equivalence check → [If impure: validate differentiating input → re-cluster] → [If pure: return first program in C*]
- **Design tradeoffs**: Query budget vs. robustness (worst-case O(|P|²) queries per type); initial input quality dependence; stateless assumption limitation; oracle selection impact
- **Failure signatures**: Tournament failure (low pairwise accuracy causes incorrect cluster selection); refinement stall (failure to find valid differentiating inputs); non-termination (nondeterministic programs break validation); query explosion (pathological cases approach worst-case bounds)
- **First 3 experiments**: 1) Query type ablation to verify individual contributions (+7.7% equivalence, +24.6% membership); 2) Oracle quality sweep to map pairwise accuracy to final pass@1; 3) Scalability profile measuring query count as candidate pool size scales

## Open Questions the Paper Calls Out

- **Open Question 1**: How can ExPairT-LLM be extended to handle stateful programs or those lacking explicit input-output mappings? The current algorithm relies entirely on comparing output vectors for given inputs, which fails for code with side effects or implicit returns.

- **Open Question 2**: How can the algorithm detect when no correct program exists in the candidate set? The current tournament mechanism selects a "winner" from available clusters even if all candidates are functionally incorrect.

- **Open Question 3**: How sensitive is the algorithm's performance to the quality and diversity of the initial inputs? If initial inputs fail to differentiate distinct semantic clusters, the tournament might select the wrong cluster before refinement can occur.

## Limitations

- The approach is limited to stateless, deterministic functions and cannot handle programs with I/O, side effects, or nondeterminism
- Performance depends on the quality of initial inputs and the reliability of LLM pairwise judgments
- The algorithm assumes at least one correct program exists in the candidate set
- Theoretical bounds assume pairwise accuracy above 0.5 but don't address random guessing regimes

## Confidence

- **High confidence**: +13.0% pass@1 improvement over B4 on HumanEval; both pairwise membership and equivalence queries are essential for performance; average query count is ~25 versus O(|P|²) worst-case
- **Medium confidence**: Theorem 5's lower bound predictions for cluster selection probability; claim that pairwise queries are "simpler for LLMs" (supported by p values but lacks cognitive load evidence)
- **Low confidence**: Generalization to programs with I/O/state; scalability beyond 25 candidates per task; performance on non-benchmark datasets

## Next Checks

1. **Oracle quality validation**: Systematically vary the LLM oracle across the full performance spectrum (0.5-1.0 pairwise accuracy) and measure how pass@1 correlates with Theorem 5 predictions, confirming the tournament aggregation advantage disappears near p=0.5

2. **Query type isolation**: Run ExPairT-LLM with membership queries only and equivalence queries only (separate ablations) to verify the individual contributions reported in the main ablation (+24.6% for membership, +7.7% for equivalence)

3. **Candidate pool scalability**: Evaluate ExPairT-LLM with 50, 100, and 200 candidate programs per task to measure query count growth and pass@1 degradation, testing the O(|P|²) worst-case bound and practical scalability limits