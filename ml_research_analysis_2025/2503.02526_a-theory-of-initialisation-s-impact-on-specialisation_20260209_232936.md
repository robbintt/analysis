---
ver: rpa2
title: A Theory of Initialisation's Impact on Specialisation
arxiv_id: '2503.02526'
source_url: https://arxiv.org/abs/2503.02526
tags:
- learning
- forgetting
- networks
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how weight initialization influences neuron
  specialization in neural networks. Using theoretical frameworks including deep linear
  networks and mean-field theory, the authors show that weight imbalance and high
  weight entropy can promote specialized solutions.
---

# A Theory of Initialisation's Impact on Specialisation

## Quick Facts
- **arXiv ID:** 2503.02526
- **Source URL:** https://arxiv.org/abs/2503.02526
- **Reference count:** 40
- **One-line primary result:** Weight initialization strongly controls neuron specialization, affecting catastrophic forgetting patterns and disentanglement in neural networks.

## Executive Summary
This paper presents a comprehensive theory showing how weight initialization controls neuron specialization in neural networks. Through theoretical analysis of deep linear networks and empirical validation on Beta-VAEs and continual learning tasks, the authors demonstrate that imbalanced initialization promotes specialized representations while high entropy initialization leads to shared representations. The work reveals that initialization schemes can fundamentally alter forgetting patterns in continual learning, with specialized networks exhibiting non-monotonic (Maslow's Hammer) forgetting profiles versus monotonic forgetting in non-specialized networks. This provides crucial insights for designing initialization schemes and understanding when regularization techniques like EWC will be effective.

## Method Summary
The paper investigates initialization's impact on specialization across three domains: theoretical deep linear networks with whitened Gaussian data, disentanglement in Beta-VAEs on 3DShapes dataset, and catastrophic forgetting in continual learning tasks. The key mechanism involves inducing "imbalance" where output layer weights are larger than input layer weights (λ = h² - w² > 0). For Beta-VAE experiments, they modify Xavier initialization with variable gain, setting gain g=0.3 for the first block and 1/g for the readout layer. Continual learning experiments use polar coordinate initialization to control scale and entropy. The authors measure specialization through entropy of hidden activations and readout weights, evaluate disentanglement using DCI scores, and assess forgetting through generalization error patterns.

## Key Results
- Initialization strongly controls catastrophic forgetting: specialized networks show non-monotonic (Maslow's Hammer) forgetting while non-specialized networks show monotonic forgetting
- Imbalanced initialization improves disentanglement in Beta-VAEs on 3DShapes dataset
- Elastic Weight Consolidation (EWC) regularization relies on specialization, failing to distinguish redundant weights in non-specialized networks
- The phase diagram of specialization can be predicted using deep linear network theory based on weight imbalance parameter λ

## Why This Works (Mechanism)
The mechanism relies on creating weight imbalance between input and output layers, where larger output weights amplify the effect of input weight patterns. This creates a "rich get richer" dynamic where initial asymmetries are magnified during training. In deep linear networks, this manifests as pathway dominance - when one pathway has sufficiently larger weights, it dominates the output regardless of input correlations. For ReLU networks, the silent alignment phase allows weights to align with dataset structure before growing, with imbalance controlling whether multiple pathways develop specialized or shared representations. The conservation of the λ parameter during early training maintains this initial bias throughout learning.

## Foundational Learning
- **Deep linear network dynamics**: Understanding weight evolution in networks without nonlinearities is crucial for predicting specialization patterns. Quick check: Verify that the silent alignment assumption holds by monitoring weight norms during early training.
- **Mean-field theory for neural networks**: Provides the mathematical framework for analyzing large-width networks and their learning dynamics. Quick check: Confirm that the Gaussian data assumption is reasonable for your application.
- **Catastrophic forgetting patterns**: Different forgetting profiles (monotonic vs. non-monotonic) emerge based on representation specialization. Quick check: Track forgetting curves across multiple tasks to identify the pattern.
- **Fisher information matrix**: EWC uses this to identify important weights, but its effectiveness depends on weight specialization. Quick check: Compute Fisher information distribution across weights to assess specialization level.
- **Disentanglement metrics (DCI)**: Quantifies the quality of learned representations in terms of disentanglement, completeness, and informativeness. Quick check: Compare DCI scores across different initialization schemes.
- **Xavier/Glorot initialization**: Standard initialization scheme that can be modified with gain parameters to control weight scale. Quick check: Verify that modified initialization maintains appropriate weight variance.

## Architecture Onboarding

**Component Map:** Whitened Gaussian Data → Deep Linear Network/ReLU Network → Output Readout Layer → Specialization Metrics (H_Q, H_h) → Forgetting/Disentanglement Performance

**Critical Path:** Initialization (gain/λ) → Weight Evolution (silent alignment phase) → Pathway Dominance/Specialization → Performance Metrics (DCI, Forgetting Profile)

**Design Tradeoffs:** Specialized representations reduce interference and improve disentanglement but may limit generalization to novel tasks. Shared representations provide flexibility but increase catastrophic forgetting. The optimal initialization depends on whether the application prioritizes robustness to forgetting or adaptability to new tasks.

**Failure Signatures:** No specialization despite theory suggests insufficient learning rate (silent alignment phase not reached) or inappropriate data distribution. EWC appearing ineffective likely indicates lack of specialization rather than EWC failure. Poor disentanglement despite imbalanced initialization may indicate architectural constraints preventing specialization.

**First Experiments:**
1. Implement deep linear network dynamics for two hidden neurons with varying λ to reproduce phase diagram showing pathway dominance
2. Train Beta-VAE on 3DShapes with modified initialization (g=0.3 vs g=1) to compare DCI scores
3. Setup Teacher-Student continual learning task with specialized vs. non-specialized initialization to observe monotonic vs. non-monotonic forgetting profiles

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the weight imbalance initialization strategy be extended to more realistic generative models such as the hidden manifold model or superstatistical generative models? The current theoretical frameworks rely on Gaussian input distributions and simplified teacher-student setups that don't capture structured data distributions present in real-world scenarios.

- **Open Question 2:** How does initialization-induced specialization relate to the emergence of compositional representations in neural networks? The relationship between sparse, specialized representations and compositional structure remains unexplored, though both involve structured representations.

- **Open Question 3:** What is the optimal trade-off between specialized and shared representations for multi-task learning performance? While both extremes are characterized, the intermediate regime and task-dependent optimal configurations remain unexplored.

## Limitations
- Theoretical framework relies on strong assumptions including noiseless Gaussian data and infinite-width networks
- ReLU-based experiments lack theoretical grounding, creating a gap between theory and empirical validation
- Claims about EWC requiring specialization are primarily demonstrated through negative results rather than positive evidence

## Confidence
- **High Confidence:** Linear network theory and phase transition predictions, MNIST forgetting profile validation
- **Medium Confidence:** Beta-VAE disentanglement results, generalization across architectures
- **Low Confidence:** Broader claims about continual learning applicability, Maslow's Hammer profile persistence

## Next Checks
1. **Scale sensitivity analysis:** Systematically vary initialization imbalance parameter λ across multiple orders of magnitude in both linear and ReLU networks to identify operational specialization regime
2. **Architectural generalization:** Test whether initialization-induced specialization patterns hold across different architectures (CNNs, Transformers) and learning paradigms beyond Beta-VAE and linear models
3. **Long-term forgetting dynamics:** Extend continual learning experiments beyond two tasks to verify whether non-monotonic forgetting pattern persists in multi-task scenarios and whether specialized networks maintain advantage