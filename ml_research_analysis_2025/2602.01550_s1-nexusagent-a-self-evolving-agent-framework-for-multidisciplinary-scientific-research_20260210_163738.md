---
ver: rpa2
title: 'S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific
  Research'
arxiv_id: '2602.01550'
source_url: https://arxiv.org/abs/2602.01550
tags:
- scientific
- agent
- s1-nexusagent
- execution
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S1-NexusAgent is a self-evolving agent framework for multidisciplinary
  scientific research, addressing limitations in long-horizon planning, tool orchestration,
  and continual learning in complex scientific workflows. It uses a hierarchical Plan-and-CodeAct
  paradigm with an inner-outer dual-loop architecture, combining high-level planning
  with code-driven tool execution and iterative refinement.
---

# S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research

## Quick Facts
- arXiv ID: 2602.01550
- Source URL: https://arxiv.org/abs/2602.01550
- Authors: S1-NexusAgent Team
- Reference count: 26
- Primary result: State-of-the-art scientific agent achieving up to 76.07% accuracy on Biomni-Eval and superior performance across biology, chemistry, and materials science benchmarks

## Executive Summary
S1-NexusAgent is a self-evolving agent framework designed for multidisciplinary scientific research. It addresses limitations in long-horizon planning, tool orchestration, and continual learning through a hierarchical Plan-and-CodeAct paradigm with an inner-outer dual-loop architecture. The system integrates thousands of scientific tools via Model Context Protocol, employs intent-aware dynamic tool retrieval, and uses object-reference-based sparse context management to handle large-scale data and long textual contexts. A Critic Agent evaluates execution trajectories and distills successful paths into reusable Scientific Skills, enabling continuous self-evolution. Evaluated on Biomni-Eval (biology), ChemBench (chemistry), and MatSciBench (materials science), S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness in long-horizon, tool-intensive scientific tasks.

## Method Summary
S1-NexusAgent uses a dual-loop architecture with outer-loop Planner Agent for high-level task decomposition and inner-loop CodeAct for sub-task execution with tool invocation. The system employs intent-aware dynamic tool retrieval through a three-stage process (intent recognition, domain filtering, semantic matching) and sparse context management with object references to prevent context explosion. Training involves two stages: supervised fine-tuning on 500 rejection-sampled trajectories, 1,500 CodeActInstruct samples, and 3,000 general dialogue samples, followed by GRPO-based reinforcement learning with asymmetric clipping and binary outcome rewards. The agent is evaluated on three scientific benchmarks: Biomni-Eval-1 (biology), ChemBench (chemistry), and MatSciBench (materials science).

## Key Results
- Achieves 76.07% average accuracy on Biomni-Eval-1, outperforming Claude-4.5-Sonnet by 1.52%
- Outperforms all baseline agents on ChemBench with 67.12% accuracy (vs. 62.64% for DeepSeek-V3)
- Demonstrates superior performance on MatSciBench with 79.00% accuracy
- Shows consistent improvement over existing scientific agents across all three benchmark domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The inner-outer dual-loop architecture enables stable long-horizon scientific task execution by decoupling global planning from local exploration.
- **Mechanism:** The outer loop (Planner Agent) maintains research objectives and performs task decomposition at the stage level, while the inner loop (CodeAct) executes sub-tasks through iterative trial-and-error with tool invocation. Feedback flows from inner to outer loop as structured results, not direct action triggers.
- **Core assumption:** Long-horizon scientific tasks require both strategic coherence (outer loop) and tactical flexibility (inner loop); neither alone suffices for complex workflows.
- **Evidence anchors:**
  - [abstract] "dual-loop architecture, decoupling global scientific planning from subtask-level tool execution"
  - [Section 3.3] "The outer loop, governed by the Planner Agent, operates at the task and stage levels... the inner loop, centered on CodeAct, operates at the sub-task and execution levels"
  - [corpus] Weak direct evidence; CASCADE and Recon-Act use multi-agent structures but without explicit inner-outer separation
- **Break condition:** If sub-task feedback becomes too noisy or infrequent, outer-loop replanning may degrade into reactive execution.

### Mechanism 2
- **Claim:** Intent-aware dynamic tool retrieval prevents context explosion while maintaining tool access for multi-domain research.
- **Mechanism:** Three-stage retrieval: (1) Intent Recognition parses queries into structured objectives; (2) domain-level filtering removes irrelevant tool categories; (3) semantic matching hot-plugs only relevant tools into the execution context.
- **Core assumption:** Large tool ecosystems cause reasoning interference; task-relevant subsets can be identified from intent alone.
- **Evidence anchors:**
  - [abstract] "intent-aware dynamic tool retrieval and hot-plug mechanisms"
  - [Section 3.2.1] "This process ensures that only the tools most relevant to the immediate scientific problem are loaded into the execution context"
  - [corpus] SciToolAgent uses knowledge graphs for tool integration but lacks dynamic hot-plugging
- **Break condition:** If intent recognition fails (ambiguous queries), domain filtering may exclude necessary tools.

### Mechanism 3
- **Claim:** Sparse context management preserves decision-relevant information while suppressing raw data growth across long trajectories.
- **Mechanism:** Four components: (1) object-level referencing via lazy loading of large artifacts; (2) sub-task isolation with independent contexts; (3) trace compression into high-value signals; (4) planner-aware augmentation feeding compressed results back to Planner.
- **Core assumption:** Scientific decision-making depends on metadata and summaries, not raw data presence in context.
- **Evidence anchors:**
  - [abstract] "object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression"
  - [Section 3.4] "This mechanism ensures that the context retains information about how to use the data, rather than the data itself"
  - [corpus] No direct corpus evidence for this specific combination of context strategies
- **Break condition:** If compression discards critical low-level details needed for later reasoning, trace reconstruction may be impossible.

## Foundational Learning

- **Concept: CodeAct paradigm**
  - **Why needed here:** The inner loop couples reasoning with executable code; understanding this is prerequisite to grasping how S1-NexusAgent invokes tools programmatically rather than via static API calls.
  - **Quick check question:** Can you explain why code execution as an action space enables error recovery that text-only action spaces cannot?

- **Concept: Hierarchical task decomposition**
  - **Why needed here:** The outer loop decomposes high-level research goals into sub-tasks; understanding planning granularity is essential for diagnosing replanning failures.
  - **Quick check question:** Given a multi-step biology workflow, can you identify where task decomposition should stop and execution should begin?

- **Concept: Context window economics**
  - **Why needed here:** The sparse context management system is designed to prevent context explosion; understanding token budgets and information density is prerequisite.
  - **Quick check question:** If a sub-task produces a 50KB genomics file, what information should be retained in context versus offloaded to object references?

## Architecture Onboarding

- **Component map:**
  Intent Recognition Agent → Tools Retrieval (DHP) → Pre-Planner → Planner Agent (outer loop) → CodeAct/Code Agent (inner loop, sandbox) → Critic Agent → Scientific Skills Repository

- **Critical path:**
  1. Query parsing (Intent Recognition)
  2. Tool filtering (DHP three-stage)
  3. Task decomposition (Pre-Planner → Planner)
  4. Sub-task execution with tool invocation (CodeAct in sandbox)
  5. Context compression and feedback (inner → outer)
  6. Trajectory evaluation (Critic) and skill distillation

- **Design tradeoffs:**
  - Dual-loop adds latency but improves long-horizon stability vs. monolithic loops
  - Hot-plugging reduces noise but risks excluding useful tools if intent is misclassified
  - Trace compression saves tokens but may lose reproducibility detail

- **Failure signatures:**
  - Outer loop stuck in replanning cycles → check Planner state updates
  - Inner loop exceeds iteration budget → inspect tool output format mismatches
  - Context overflow despite SCM → verify object referencing is active, not embedding raw data

- **First 3 experiments:**
  1. Run a single-domain task (e.g., ChemBench) with dual-loop enabled vs. only-inner-loop; compare success rate and iteration count.
  2. Test DHP with intentionally ambiguous intent strings; measure tool retrieval precision and downstream task accuracy.
  3. Profile context size growth across a 10+ sub-task workflow; verify SCM is compressing traces by inspecting Planner prompt length before/after compression.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can joint training of both inner-loop (CodeAct) and outer-loop (Planner) agents improve performance compared to training only the inner loop?
  - **Basis in paper:** [explicit] Section 4 states "joint learning of the inner-outer loop is left for future work" after explaining that current training focuses solely on the inner-loop CodeAct module.
  - **Why unresolved:** The dual-loop architecture creates interdependencies between planning and execution, but whether co-training yields synergistic improvements or destabilizes learning remains unknown.
  - **What evidence would resolve it:** Comparative experiments training both loops jointly (e.g., multi-agent RL) versus the current staged approach, evaluated on long-horizon scientific benchmarks.

- **Open Question 2:** Do Scientific Skills distilled from one domain (e.g., biology) transfer effectively to other scientific domains (e.g., chemistry, materials science)?
  - **Basis in paper:** [explicit] The conclusion identifies "cross-domain scientific skill transfer" as a key future research direction. The paper claims cross-domain potential but only evaluates each benchmark independently.
  - **Why unresolved:** The skill distillation mechanism (TE-SE) is described conceptually, but no experiments quantify whether skills learned in one discipline improve performance in another.
  - **What evidence would resolve it:** Transfer learning experiments where skills acquired on Biomni-Eval are applied to ChemBench/MatSciBench tasks, measuring performance gains against baseline agents without transferred skills.

- **Open Question 3:** Does the Trajectory-Evaluation-based Self-Evolution mechanism produce compounding capability improvements over extended periods of deployment?
  - **Basis in paper:** [inferred] Section 3.5 describes TE-SE as enabling "continuous self-evolution," but the experiments only report single-task performance, not longitudinal skill accumulation or compounding returns.
  - **Why unresolved:** The framework claims to distill reusable Scientific Skills, but whether these skills accumulate meaningfully over time—rather than plateauing or degrading—remains untested.
  - **What evidence would resolve it:** Longitudinal evaluation tracking agent performance over hundreds/thousands of sequential tasks, measuring whether accuracy improves as the skill repository grows.

## Limitations

- The "thousands of tools" claim lacks tool inventory or MCP configuration details, making reproduction of the tool ecosystem uncertain
- Sparse Context Management implementation is underspecified—no details on object-reference storage, compression prompts, or planner-augmentation formats
- RL training signals (reward shaping, trajectory scoring) are described qualitatively without concrete rubrics or thresholds

## Confidence

- **High:** The dual-loop architecture concept and its separation of planning from execution
- **Medium:** Intent-aware dynamic tool retrieval effectiveness given the three-stage filtering description
- **Medium:** SOTA benchmark results, though dependent on exact tool ecosystem and training details

## Next Checks

1. Implement the dual-loop architecture with mock tools and test whether decoupling improves long-horizon task stability versus a monolithic loop
2. Build a minimal MCP-based tool retrieval system and measure whether intent-aware filtering reduces context size without hurting task accuracy
3. Profile context growth across multi-step workflows to verify sparse context management prevents token explosion while preserving execution trace fidelity