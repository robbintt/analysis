---
ver: rpa2
title: 'How Scale Breaks "Normalized Stress" and KL Divergence: Rethinking Quality
  Metrics'
arxiv_id: '2510.08660'
source_url: https://arxiv.org/abs/2510.08660
tags:
- stress
- t-sne
- normalized
- scale
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper highlights a critical flaw in widely used quality metrics
  for dimensionality reduction (DR) visualization: both normalized stress and Kullback-Leibler
  (KL) divergence are highly sensitive to uniform scaling of the output embedding,
  despite scaling not changing the inherent relationships in the data. The authors
  show that this scale sensitivity can lead to incorrect conclusions, such as ranking
  a random embedding as better than established techniques like MDS or t-SNE.'
---

# How Scale Breaks "Normalized Stress" and KL Divergence: Rethinking Quality Metrics

## Quick Facts
- **arXiv ID:** 2510.08660
- **Source URL:** https://arxiv.org/abs/2510.08660
- **Reference count:** 40
- **Primary result:** Scale-sensitive metrics (normalized stress, KL divergence) can rank random embeddings above MDS/t-SNE; scale-invariant variants SNS and SNKL fix this issue.

## Executive Summary
This paper reveals a fundamental flaw in widely used quality metrics for dimensionality reduction (DR) visualization: normalized stress and Kullback-Leibler (KL) divergence are highly sensitive to uniform scaling of the output embedding, despite scaling not changing the inherent relationships in the data. This scale sensitivity can lead to incorrect conclusions, such as ranking random embeddings as better than established techniques like MDS or t-SNE. The authors introduce scale-invariant variants—scale-normalized stress (SNS) and scale-normalized KL divergence (SNKL)—which automatically adjust for scale by optimizing the scaling factor. Their experiments demonstrate that scale-sensitive metrics produce inconsistent and unreliable rankings, especially when embeddings are resized, while the proposed scale-invariant metrics consistently rank MDS and t-SNE above random embeddings and show stable behavior across varying scales.

## Method Summary
The authors demonstrate that normalized stress and KL divergence are sensitive to uniform scaling of DR embeddings. They introduce scale-normalized variants (SNS and SNKL) that optimize the scaling factor to eliminate this sensitivity. SNS has a closed-form solution for the optimal scale, while SNKL requires numerical optimization (golden-section search). The method is validated on 24 datasets comparing MDS, t-SNE, UMAP, LLE, Isomap, and Random embeddings, showing that SNS/SNKL produce more consistent rankings than traditional metrics.

## Key Results
- Normalized stress and KL divergence are sensitive to uniform scaling of DR embeddings
- SNS and SNKL provide scale-invariant alternatives with stable rankings
- Scale-sensitive metrics can rank random embeddings above MDS/t-SNE
- Re-evaluating prior studies using SNS changes technique rankings significantly
- SNS and SNKL are recommended as more reliable alternatives for comparing DR methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If normalized stress or KL divergence are used to evaluate DR embeddings, uniform scaling changes the metric value, potentially leading to incorrect conclusions where random noise outperforms valid algorithms.
- **Mechanism:** Raw stress is a quadratic function of the scaling factor α (Page 4, Section III.A). Normalized stress inherits this scale sensitivity. Different DR algorithms output embeddings at arbitrary default scales, so evaluating them without adjusting for scale is equivalent to comparing them at random points on their respective stress curves.
- **Core assumption:** The intrinsic quality of a projection should not change if the entire plot is uniformly resized.
- **Evidence anchors:** [abstract] "normalized stress... is sensitive to uniform scaling... despite this act not meaningfully changing anything about the projection." [section] Page 4 shows analytical expansion of Raw Stress proving it is variant under stretching/compression.
- **Break condition:** If the evaluation metric relies solely on rank-correlation rather than absolute distance differences, the mechanism breaks.

### Mechanism 2
- **Claim:** Calculating SNS by optimizing the scaling factor α restores expected metric behavior, ensuring MDS ranks higher than random projections.
- **Mechanism:** SNS is defined as the minimum normalized stress over all positive scaling factors α. A closed-form solution exists for the optimal α, interpreted as a correlation coefficient between high-dimensional and low-dimensional distances.
- **Core assumption:** There exists a unique optimal scale α that aligns the magnitude of low-dimensional embedding distances with high-dimensional data distances.
- **Evidence anchors:** [abstract] "scale-normalized stress (SNS)... automatically adjust for scale" [section] Page 5, Equation 4 and derivation of α = Σd_high·d_low / Σd_high².
- **Break condition:** If the embedding has no correlation with input data, optimization might result in a degenerate scale, though this simply results in a stable, high stress value.

### Mechanism 3
- **Claim:** Optimizing the scale factor for KL divergence (SNKL) using numerical search effectively neutralizes scale sensitivity in neighbor-preservation metrics.
- **Mechanism:** Unlike stress, optimal scale for KL divergence lacks a simple closed-form solution. The authors utilize the empirical observation that KL divergence is generally unimodal with respect to scale, allowing efficient 1D optimization to find the scale that minimizes divergence.
- **Core assumption:** The KL divergence function is unimodal with respect to scale for typical datasets, ensuring the local search finds the global minimum.
- **Evidence anchors:** [section] Page 7, Section IV.B: "KL divergence is continuous and empirically unimodal... minimum value for α can be found efficiently... using golden-section search." [abstract] "scale-normalized KL divergence (SNKL)... automatically adjust for scale by optimizing the scaling factor."
- **Break condition:** If the KL divergence curve is not unimodal (e.g., Gaussian KL divergence), the search may converge to a local minimum rather than the global optimum.

## Foundational Learning

- **Concept:** **Normalized Stress**
  - **Why needed here:** This is the baseline metric the paper critiques. Understanding that it measures the aggregate squared error between high-D and low-D distances (normalized by input magnitude) is necessary to see why "scale" breaks it.
  - **Quick check question:** If you double the size of a projection (multiply all coordinates by 2), does the normalized stress go up, down, or stay the same? (Paper says it changes, usually following a parabola).

- **Concept:** **KL Divergence (in t-SNE)**
  - **Why needed here:** The paper extends its critique from stress to KL divergence. You must understand that KL divergence here measures the difference between probability distributions of points in high-D vs low-D space to grasp the proposed SNKL fix.
  - **Quick check question:** Does KL divergence measure the difference in absolute distances or the difference in the probability of neighbors? (Paper implies it measures distribution similarity).

- **Concept:** **Scale Invariance**
  - **Why needed here:** The central thesis is that quality metrics *should* be scale-invariant (insensitive to stretching/shrinking) but currently are not.
  - **Quick check question:** Is the Euclidean distance between two points scale-invariant? (No, scaling coordinates changes the distance).

## Architecture Onboarding

- **Component map:**
  High-dimensional dataset X -> DR Algorithm (MDS, t-SNE, etc.) -> Embedding Y -> Evaluation Layer (Compute Metric) -> Optimization Layer (Compute Optimal Scale α) -> Scale-Normalized Score (SNS or SNKL)

- **Critical path:**
  1. Generate embedding Y using a standard DR library
  2. *Do not* report raw Normalized Stress
  3. Calculate the optimal scaling factor α using the covariance formula (for Stress) or golden-section search (for KL)
  4. Report the metric value at this optimal α

- **Design tradeoffs:**
  - SNS vs. SNKL: SNS has a closed-form solution (faster, O(N²)), while SNKL requires an iterative search (slower). SNS assesses global distance preservation; SNKL assesses local neighborhood preservation.
  - Rank vs. Distance: The paper notes rank-based metrics (Shepard Goodness) are naturally scale-invariant but less specific than distance-based metrics. SNS attempts to keep the specificity of distance metrics while adding invariance.

- **Failure signatures:**
  - Rank Inversion: If evaluation shows Random embedding has lower stress than MDS or t-SNE, scale-sensitive metrics are being used on differently scaled outputs (Figure 1).
  - Metric Instability: If adding small isotropic noise to an embedding causes the metric value to decrease (improvement) rather than increase, the metric is likely reacting to scale changes induced by the noise (Figure 5).

- **First 3 experiments:**
  1. Replicate Figure 1: Take a dataset (e.g., Iris), run MDS and Random projection. Plot Normalized Stress vs. Scale factor α for both. Observe the curves crossing to verify scale sensitivity.
  2. Implement SNS: Implement the closed-form α calculation (Equation on Page 5). Verify that SNS ranks MDS > Random regardless of the initial scale of the output.
  3. Re-rank Prior Work: Take existing embeddings from a source like Espadoto et al. (Page 10) and re-evaluate them using SNS to check if the ranking of algorithms (e.g., t-SNE vs. UMAP) changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hyperparameter settings and initialization conditions of dimensionality reduction (DR) techniques interact with scale sensitivity and the proposed normalization methods?
- Basis in paper: [explicit] The authors state in the Limitations section that the experiments "do not account for potential variations in the performance of DR techniques due to hyperparameter settings or initialization conditions, which can influence the quality of DR techniques."
- Why unresolved: The experiments in the paper use default parameters and fixed initializations to isolate the scale variable, leaving the relationship between tuning, scale, and metric stability unexplored.
- What evidence would resolve it: A sensitivity analysis comparing SNS and SNKL scores across various hyperparameter settings (e.g., perplexity in t-SNE, n-neighbors in UMAP) and random seeds.

### Open Question 2
- Question: Can a better-grounded experimental framework be designed to verify the effectiveness of quality metrics without relying on the assumption that optimized embeddings always score better than non-optimized ones?
- Basis in paper: [explicit] The paper notes that while plausible, the reliance on a "correct order" (e.g., MDS > Random) is a hypothesis, and "a better-grounded experiment can verify the effectiveness of quality metrics."
- Why unresolved: The current validation depends on relative rankings between established algorithms rather than a "true" ground truth measure of distortion or structure preservation.
- What evidence would resolve it: Development of a benchmark using synthetic data with known, controllable distortions to quantitatively measure the correlation between metric scores and actual structural fidelity.

### Open Question 3
- Question: Under what specific conditions does Kullback-Leibler (KL) divergence fail to exhibit a unique minimum with respect to scale, rendering the optimization of the scaling factor α unreliable?
- Basis in paper: [explicit] The authors note that "It is possible that KL divergence strictly decreases as α → ∞," contrary to their empirical findings, and provide a constructed counter-example in the supplemental material.
- Why unresolved: While the metric behaved as a unimodal function on all 16 datasets tested, the theoretical possibility of it strictly decreasing suggests the proposed minimization approach might fail on specific embeddings.
- What evidence would resolve it: A theoretical characterization of the input or embedding properties that guarantee unimodality, or the identification of real-world datasets where SNKL minimization fails.

## Limitations
- Experiments use default DR parameters and fixed initializations, not accounting for hyperparameter sensitivity
- Validation relies on relative rankings between established algorithms rather than absolute ground truth
- SNKL optimization assumes unimodality of KL divergence with respect to scale, which may not hold in all cases

## Confidence
- **High:** Scale sensitivity exists for normalized stress (analytical proof) and affects current evaluations
- **Medium:** SNS provides stable rankings and better interpretation
- **Medium:** SNKL effectively neutralizes scale sensitivity in practice
- **Low:** The extent to which scale sensitivity has distorted conclusions in all prior DR studies

## Next Checks
1. **Unimodality Verification:** Systematically test KL divergence curves for various datasets to empirically verify unimodality assumptions before applying SNKL optimization.
2. **Ranking Impact Quantification:** Take 3-5 published DR comparison studies and recalculate technique rankings using SNS vs. normalized stress to measure actual impact on conclusions.
3. **Metric Sensitivity Analysis:** Compare how sensitive different DR metrics (stress, KL, Shepard, trustworthiness) are to scale changes using the same perturbation protocol across multiple datasets.