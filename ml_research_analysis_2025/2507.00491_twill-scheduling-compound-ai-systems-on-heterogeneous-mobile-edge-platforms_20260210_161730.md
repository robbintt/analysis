---
ver: rpa2
title: 'Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms'
arxiv_id: '2507.00491'
source_url: https://arxiv.org/abs/2507.00491
tags:
- inference
- workload
- execution
- cluster
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Twill is a runtime framework for scheduling compound AI (cAI) workloads
  on heterogeneous mobile edge platforms. It addresses the challenge of concurrently
  executing DNNs, transformers, and LLMs with dynamic arrival patterns and varying
  computational diversity.
---

# Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms

## Quick Facts
- **arXiv ID:** 2507.00491
- **Source URL:** https://arxiv.org/abs/2507.00491
- **Reference count:** 40
- **Primary result:** Twill reduced inference latency by 54% on average compared to state-of-the-art edge AI inference techniques while honoring power constraints.

## Executive Summary
Twill is a runtime framework for scheduling compound AI (cAI) workloads on heterogeneous mobile edge platforms. It addresses the challenge of concurrently executing DNNs, transformers, and LLMs with dynamic arrival patterns and varying computational diversity. The core method uses model affinity-aware cluster mapping/migration, priority-aware task freezing/unfreezing, and DVFS to minimize inference latency within power budgets. Evaluated on Nvidia Jetson Orin NX with contemporary cAI workloads, Twill reduced inference latency by 54% on average compared to state-of-the-art edge AI inference techniques while honoring power constraints.

## Method Summary
Twill is a Python-based runtime framework that orchestrates compound AI workloads on heterogeneous mobile edge platforms. It consists of a Model Interpreter that parses ONNX models to generate "signature maps" based on layer-cluster affinity and a DLA compatibility matrix, and a Controller that executes an "Analyze-Decide-Deploy" loop using actuation knobs: affinity-cluster mapping, migration, priority-aware freezing, and DVFS scaling. The framework dynamically schedules tasks to minimize inference latency while respecting power budgets.

## Key Results
- Twill reduced inference latency by 54% on average compared to state-of-the-art edge AI inference techniques
- Successfully honored power constraints (10W TDP) while maintaining performance
- Demonstrated effectiveness across heterogeneous workloads including DNNs, Transformers, and LLMs

## Why This Works (Mechanism)

### Mechanism 1: Affinity-Aware Cluster Mapping and Migration
The framework parses ONNX models to generate a "signature map" against a "DLA compatibility matrix." If a model lacks support on the DLA, it is assigned to the GPU. Supported DNNs are mapped to the DLA but can be dynamically migrated to the GPU if the DLA is occupied or the GPU becomes idle. This minimizes memory overhead caused by hardware fallbacks.

### Mechanism 2: Priority-Aware Task Freezing
When a high-priority task arrives and all feasible clusters are busy, the controller "freezes" a running lower-priority task (saving state/queueing it) to immediately free up the required resource (typically GPU). The frozen task is resumed later. This allows urgent prompt-based tasks to meet latency targets without violating power budgets.

### Mechanism 3: Power-Budgeted DVFS Scaling
The controller measures power before and after task mapping. It calculates a new frequency using a linear model: $freq_{new} = \frac{TDP - P_{prec}}{P_{curr} - P_{prec}} \times freq_{curr}$. This scales performance up to the thermal limit, preventing TDP violations while maximizing throughput.

## Foundational Learning

- **Concept: DLA Compatibility and Fallbacks**
  - **Why needed here:** DLAs often lack support for complex Transformer operations, forcing those ops to run on the GPU. Understanding this is required to build the "Compatibility Matrix" used in Mechanism 1.
  - **Quick check question:** If a layer is unsupported on the DLA, does the inference crash, or does it automatically execute on the GPU?

- **Concept: Online Profiling vs. Design-Time Profiling**
  - **Why needed here:** Twill relies on *online* heuristics because cAI workloads are dynamic and unknown at design time.
  - **Quick check question:** Why can't we pre-compute the optimal schedule for a Compound AI system before deployment?

- **Concept: Task Preemption vs. Freezing**
  - **Why needed here:** Twill uses "freezing" (suspend execution) rather than full preemption. Knowing the distinction helps in understanding the implementation overhead.
  - **Quick check question:** Does freezing a task require saving the entire GPU memory state to CPU RAM, or just the execution command pointer?

## Architecture Onboarding

- **Component map:** Model Interpreter -> Signature Map -> Controller -> Analyze-Decide-Deploy loop -> Actuation Knobs (Map, Migrate, Freeze, DVFS)
- **Critical path:** The **Decide Phase**. This logic determines if a task is mapped, migrated, or frozen. Errors here directly cause the latency spikes seen in the "SoA" baselines.
- **Design tradeoffs:**
  - **Latency vs. Overhead:** The 15ms overhead of the Python-based controller is acceptable for complex workloads (Bert/LLM) but may be significant for tiny, high-frequency DNNs.
  - **Isolation vs. Utilization:** Twill maps "at most one model per cluster" to avoid contention, trading off potential parallelism for predictable latency.
- **Failure signatures:**
  - **Starvation:** Low-priority DNNs never unfreeze if high-priority LLM requests arrive continuously.
  - **Thrashing:** Frequent migration of a task between GPU and DLA due to "flapping" affinity decisions.
  - **TDP Violation:** The linear DVFS model fails if ambient temperature changes significantly, causing the derived frequency to overheat the chip.
- **First 3 experiments:**
  1. **Baseline Affinity Check:** Run a single Transformer (e.g., Bert-base) on DLA vs. GPU to confirm the paper's claim that DLA causes fallback overhead.
  2. **Scenario 2 Reproduction (Stress Test):** Simulate the arrival of a high-priority task while the system is fully loaded (2 DNNs running) to verify the freezing logic triggers before queueing.
  3. **Power Cap Validation:** Gradually reduce the TDP constraint below 10W to verify the DVFS scaling logic correctly throttles frequency without crashing the inference.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Twill be extended to efficiently orchestrate cAI systems composed of multiple concurrent Large Language Models (LLMs)?
- **Basis in paper:** The conclusion states, "Orchestrating multi-LLM cAI systems on mobile platforms is planned as our future work."
- **Why unresolved:** The current evaluation focuses on heterogeneous DNN/Transformer mixes with at most one LLM, whereas multiple concurrent generative LLMs impose distinct memory and compute constraints (e.g., KV cache contention) not fully explored.
- **What evidence would resolve it:** Evaluation of system throughput and latency on workloads featuring multiple concurrent generative LLMs compared to single-LLM mixes.

### Open Question 2
- **Question:** Can application priority be derived dynamically at runtime to replace the current reliance on static user-defined inputs?
- **Basis in paper:** Section III-C states that "Application priority is a user-defined parameter," and Section IV-A notes priorities are "configured based on cAI workload requirements."
- **Why unresolved:** Requiring manual priority configuration limits the framework's ability to autonomously adapt to dynamic scenarios where the relative importance of tasks may change unexpectedly.
- **What evidence would resolve it:** Implementation of an adaptive priority heuristic and a comparison showing it maintains Quality of Service (QoS) without manual tuning.

### Open Question 3
- **Question:** How does the affinity-aware mapping strategy generalize to hardware accelerators with native transformer support?
- **Basis in paper:** The framework relies on a "Compatibility matrix" specific to Jetson Orin NX, where DLAs lack transformer support.
- **Why unresolved:** Twill optimizes by migrating transformers to GPU due to DLA limitations; this heuristic may be suboptimal on NPUs or future DLAs that efficiently support transformer operations.
- **What evidence would resolve it:** Benchmarks on diverse hardware (e.g., Qualcomm Hexagon or Apple Neural Engine) showing migration logic adaptability.

## Limitations

- The exact composition of the DLA compatibility matrix (specific unsupported operator constraints) is referenced but not fully enumerated in the paper, requiring reverse engineering from Nvidia documentation.
- The linear power-frequency scaling model may not hold under dynamic thermal conditions or memory-bound workloads, potentially causing TDP violations despite the controller logic.
- The paper reports an average 54% latency reduction but does not disclose variance across different workload mixes, raising questions about worst-case performance.

## Confidence

- **High confidence** in the core scheduling mechanism (affinity-aware mapping, freezing) as it's algorithmically specified and core to the experimental results.
- **Medium confidence** in the DVFS power control effectiveness, as the linear model is a simplification and real-world non-linear power spikes could break the control loop.
- **Low confidence** in absolute performance numbers without access to the exact baselines (MapFormer, Tango, Band) and their implementations.

## Next Checks

1. **Compatibility Matrix Validation:** Run a single Transformer (Bert-base) on DLA vs. GPU to confirm the paper's claim that DLA causes fallback overhead (Figure 5). If DLA execution time is within 20% of GPU but with high variance, the compatibility matrix needs refinement.
2. **Starvation Scenario:** Simulate continuous arrival of high-priority LLM requests while low-priority DNNs are running. If DNNs remain frozen indefinitely, implement a timeout mechanism and measure the impact on average latency.
3. **Power Cap Stress Test:** Gradually reduce the TDP constraint from 10W to 5W and verify the DVFS scaling logic correctly throttles frequency. If the system crashes or violates TDP at any point, the linear model coefficients need recalibration.