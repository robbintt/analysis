---
ver: rpa2
title: 'On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal
  Clinical AI'
arxiv_id: '2508.00171'
source_url: https://arxiv.org/abs/2508.00171
tags:
- text
- image
- modality
- textual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a perturbation-based framework called Selective
  Modality Shifting (SMS) to quantify how much vision-language models (VLMs) rely
  on textual vs. visual inputs in medical diagnosis tasks.
---

# On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI

## Quick Facts
- **arXiv ID:** 2508.00171
- **Source URL:** https://arxiv.org/abs/2508.00171
- **Reference count:** 30
- **Primary result:** Perturbation-based framework (SMS) reveals VLMs rely on text over images for medical diagnosis, raising safety concerns.

## Executive Summary
This paper introduces a perturbation-based framework called Selective Modality Shifting (SMS) to quantify how much vision-language models (VLMs) rely on textual versus visual inputs in medical diagnosis tasks. By systematically swapping images or text between samples with opposite labels, SMS exposes modality-specific biases. Tested on MIMIC-CXR and FairVLMed datasets with six VLMs (four general-purpose, two medical-tuned), the method reveals strong text bias: accuracy and F1-scores drop significantly under text swaps while image swaps have minimal impact. Attention analysis confirms text tokens dominate over image tokens during generation, and calibration error increases under text shifts, indicating overconfident and unreliable predictions.

## Method Summary
The Selective Modality Shifting (SMS) framework constructs counterfactual inputs by pairing an image or text from a sample with a complementary input from a sample possessing an opposing ground-truth label. This systematic modality swapping acts as a causal probe to isolate reliance on specific input types. The method evaluates six VLMs (LLaVA, Qwen-2 VL, Llama 3.2, Janus-Pro, Med-LLaVA, MedGemma) on MIMIC-CXR (chest X-ray) and FairVLMed (ophthalmology) datasets using zero-shot prompting. Metrics include accuracy, F1, Negative Flip Rate (NFR), Expected Calibration Error (ECE), and attention-based modality analysis. SMS tests both text and image shifts to determine which modality the model relies on for predictions.

## Key Results
- Accuracy and F1-scores drop significantly under text swaps while image swaps have minimal impact
- Negative Flip Rate (NFR) values above 0.60 for text shifts confirm that correct predictions are often overturned when text is mismatched
- Attention analysis shows text tokens dominate over image tokens during generation
- Calibration error (ECE) increases under text shifts, indicating overconfident, unreliable predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Systematic modality swapping acts as a causal probe to isolate reliance on specific input types
- **Mechanism:** SMS constructs counterfactual inputs by pairing an image (I) or text (T) from a sample with a complementary input from a sample possessing an opposing ground-truth label (y). If the model's prediction flips to match the swapped modality rather than the held-constant modality, it quantifies causal dominance. A large deviation in prediction when text is swapped (while the image remains pathologic) indicates the model is ignoring visual features
- **Core assumption:** The model possesses the capacity to integrate both modalities; if it relies on a single modality, the other contains effectively zero information weight in the decision function fVLM
- **Evidence anchors:** [abstract] "...swapping images or text between samples with opposing labels, we expose modality-specific biases"; [section 2.2] "A large deviation in the predicted labels when only T is replaced indicates that the model relies heavily on the text modality"

### Mechanism 2
- **Claim:** Textual dominance is driven by attention allocation patterns that prioritize language tokens over visual tokens during decoding
- **Mechanism:** Inside the transformer architecture, cross-attention and self-attention layers assign weights to input tokens. The paper observes that text tokens receive higher and more variable attention weights compared to image tokens, which remain stable (low variance). This suggests the model effectively treats the image as a low-information background anchor, while deriving semantic logic primarily from the text prompt
- **Core assumption:** Attention weights serve as a faithful proxy for the functional importance of input features in the model's reasoning process
- **Evidence anchors:** [abstract] "...attention-based analysis... confirms that image content is often overshadowed by text details"; [section 2.3] "We decompose At into modality-specific components... to determine whether different output tokens exhibit distinct attention score patterns"

### Mechanism 3
- **Claim:** Textual bias induces miscalibration (overconfidence) specifically under modality conflict
- **Mechanism:** When a model relies heavily on text and the text is swapped (creating a conflict between the text prior and visual reality), the model maintains high confidence in the (now incorrect) textual prediction. This results in a high Expected Calibration Error (ECE) because the predicted probability (confidence) does not align with the empirical accuracy (the outcome is wrong)
- **Core assumption:** The model's "first-token" logits (for "Yes"/"No") accurately represent its internal certainty and final decision state
- **Evidence anchors:** [abstract] "Calibration error (ECE) increases under text shifts, indicating overconfident, unreliable predictions"; [section 2.4] "We assess the calibration of first-token probabilities... ECE... averages the weighted absolute difference between predicted confidence and empirical accuracy"

## Foundational Learning

- **Concept: Negative Flip Rate (NFR)**
  - **Why needed here:** Standard accuracy metrics hide how a model fails. NFR quantifies stability by measuring the fraction of correct predictions that flip to incorrect specifically when a perturbation (swap) is applied
  - **Quick check question:** If a model has 90% accuracy on clean data and 50% accuracy on perturbed data, why is NFR more informative than the simple accuracy drop for diagnosing bias?

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** This metric is crucial for deployment safety. It measures the gap between the model's confidence (e.g., "90% sure") and its actual accuracy
  - **Quick check question:** A model predicts "Cancer" with 99% confidence but is wrong. How does ECE capture this failure mode compared to a simple accuracy score?

- **Concept: Cross-Modal Attention in Transformers**
  - **Why needed here:** Understanding how VLMs fuse data requires knowing how the attention mechanism weighs image patches versus text tokens
  - **Quick check question:** In a multimodal transformer, if the attention weights for all image patches approach zero during the final layer decoding, what does this imply about the model's decision logic?

## Architecture Onboarding

- **Component map:** Input Processor -> SMS Controller (Diagnostic) -> VLM Core -> Evaluator
- **Critical path:** 1) Batch samples ensuring equal representation of positive/negative classes 2) Apply Text Shift: Replace text T in positive samples with text T' from negative samples (and vice versa) 3) Run inference to get ŷ 4) Compare ŷ against original predictions to calculate NFR 5) Extract logits for "Yes"/"No" tokens to calculate ECE
- **Design tradeoffs:** Zero-shot vs. Fine-tuning: The paper uses zero-shot to expose inherent model biases without the masking effect of task-specific training; Regex vs. Generation: The system uses Regular Expressions to parse free-form text answers (e.g., mapping "Yes, patient has..." to "Yes"). This is robust for evaluation but risks misclassification if the model uses obscure phrasing
- **Failure signatures:** Text Bias Signature: Accuracy drops >20% and NFR > 0.60 on Text Shift, while Image Shift causes minimal change; Hallucination Signature: Model generates specific clinical details (e.g., "consolidation in right lower lobe") that are present in the swapped text but absent in the image
- **First 3 experiments:** 1) Baseline Validation: Run the VLM on clean MIMIC-CXR/FairVLMed sets to establish unperturbed accuracy and ECE 2) SMS Text vs. Image Ablation: Run SMS swapping only text on Run A, and only images on Run B. Compare the performance deltas to confirm text dominance 3) Attention Visualization: Visualize attention maps for a subset of high-NFR samples to verify that the model is attending to text tokens while ignoring image regions relevant to the pathology

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the strong textual bias observed under zero-shot prompting persist when using few-shot prompting techniques or when evaluated on closed-source models like GPT-4o or Gemini?
- **Basis in paper:** [explicit] The conclusion explicitly states: "As future work, we plan to explore if similar conclusions hold for other prompting techniques such as few-shot prompting and more powerful closed models like the GPT or Gemini families"
- **Why unresolved:** The study deliberately limited its scope to zero-shot evaluation of six open-source VLMs. Few-shot prompting may provide in-context examples that better balance modalities, and proprietary models may have different training regimes affecting bias
- **What evidence would resolve it:** Applying the SMS framework to GPT-4o, Gemini, and other closed models across varying numbers of in-context examples, measuring NFR and calibration shifts under text/image swaps

### Open Question 2
- **Question:** Can architectural interventions or training modifications effectively mitigate the observed textual modality bias without degrading overall diagnostic performance?
- **Basis in paper:** [inferred] The paper demonstrates and quantifies text bias but does not propose or evaluate any mitigation strategies. The results section frames this as a safety concern requiring attention but stops at diagnosis rather than treatment
- **Why unresolved:** Understanding whether this bias is an inherent property of current VLM architectures, a consequence of training data composition, or an artifact of instruction-tuning remains unknown. No intervention experiments were conducted
- **What evidence would resolve it:** Experiments with modified training objectives (e.g., visual grounding losses), architecture changes (e.g., cross-modal attention constraints), or fine-tuning strategies that explicitly penalize text-only reliance, evaluated under SMS

### Open Question 3
- **Question:** How does the binary classification setup used in SMS generalize to multi-class diagnostic scenarios or open-ended report generation tasks where modality integration may manifest differently?
- **Basis in paper:** [inferred] The methodology explicitly defines the task as binary classification (y ∈ {0,1}) and uses regular expressions to map outputs to yes/no. The paper acknowledges models sometimes generate comprehensive answers that require post-processing, suggesting the binary framing may miss nuanced reasoning
- **Why unresolved:** Real clinical workflows often involve differential diagnosis across multiple conditions or generating detailed reports. The bias patterns observed in constrained binary decisions may not capture how models integrate modalities during free-form generation
- **What evidence would resolve it:** Extending SMS to multi-label classification (e.g., multiple thoracic findings) and generation tasks (e.g., report synthesis), measuring whether text dominance persists when models must articulate visual findings in natural language

## Limitations
- The SMS swapping mechanism assumes swapped modalities carry meaningful information; if swapped content is inherently ambiguous, observed bias could be an artifact
- The attention analysis depends on attention weights as proxies for functional importance, which may not hold for all transformer architectures
- Regex-based evaluation of free-form outputs introduces potential noise if the model uses unexpected phrasing, which could skew accuracy and NFR calculations

## Confidence
- **High Confidence:** The observation that accuracy and F1 scores drop significantly under text swaps while image swaps have minimal impact
- **Medium Confidence:** The claim that text tokens dominate attention over image tokens, though interpretation depends on architectural assumptions
- **Medium Confidence:** The link between text bias and miscalibration (increased ECE), though the specificity to text bias requires further isolation

## Next Checks
1. **Swap Ablation with Controlled Noise:** Replace text/image with semantically neutral or random content (e.g., "The patient has no findings" vs. random words) to confirm that the observed NFR is due to learned bias rather than content mismatch
2. **Attention Weight Correlation with Visual Features:** For high-NFR samples, quantify the correlation between attention weights on image tokens and the presence of visual pathology features (e.g., opacity regions in X-rays). Low/no correlation would strengthen the claim of visual neglect
3. **Alternative Modality Dominance Probes:** Implement a second diagnostic (e.g., ablation of text/image encoders during inference) to verify that SMS findings are not artifacts of the swapping mechanism