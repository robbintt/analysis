---
ver: rpa2
title: 'ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer
  Climate Change Queries'
arxiv_id: '2506.13796'
source_url: https://arxiv.org/abs/2506.13796
tags:
- climate
- change
- instruction
- data
- climatechat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of improving large language
  models (LLMs) for answering climate change queries. It introduces an automated method
  for constructing climate change instruction data by generating instructions from
  documents, enhancing diversity through web scraping, and collecting seed instructions.
---

# ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries

## Quick Facts
- arXiv ID: 2506.13796
- Source URL: https://arxiv.org/abs/2506.13796
- Reference count: 33
- Primary result: ClimateChat achieves 93.0% accuracy on climate objective tasks and 126.2 on subjective tasks

## Executive Summary
ClimateChat addresses the challenge of improving large language models for answering climate change queries through specialized instruction tuning. The study introduces an automated method for constructing climate change instruction data by generating instructions from documents, enhancing diversity through web scraping, and collecting seed instructions. Using this method, a dataset named ClimateChat-Corpus was created and used to fine-tune open-source LLMs, resulting in ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-answering tasks, achieving an average accuracy of 93.0% on objective tasks and a score of 126.2 on subjective tasks. The study highlights the importance of selecting a domain-specific base model and demonstrates the adaptability of ClimateChat to various climate change scientific discovery tasks.

## Method Summary
The study developed ClimateChat-Corpus through three strategies: Self-QA using GPT-4 to generate Q&A from authoritative climate documents, Web Scraping of StackExchange Q&A with sufficient upvotes, and Self-Instruct using seed tasks with few-shot GPT-4 generation. This corpus was mixed with general datasets (Alpaca-GPT4, Dolly, BELLE) and used to fine-tune the JiuZhou base model (Mistral-7B with geoscience continued pre-training) using LoRA. The model was evaluated on 105 multiple-choice questions across five climate disciplines and 45 GeoBench questions across six subjective criteria scored by GPT-4.

## Key Results
- ClimateChat achieves 93.0% average accuracy on 105 objective climate change multiple-choice questions
- On subjective tasks, ClimateChat scores 126.2 versus 120.3 for JiuZhouChat+RAG on 45 GeoBench questions
- ClimateMistral (general base model) showed 11 significant inaccuracies vs 1 for ClimateChat on 45 questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining is a prerequisite for effective instruction tuning in specialized domains.
- Mechanism: The base model JiuZhou underwent continued pre-training on geoscience literature before instruction tuning. This prior knowledge enables the model to accurately ground climate-specific instructions rather than hallucinate plausible-but-wrong answers.
- Core assumption: The geoscience corpus overlaps sufficiently with climate change knowledge to transfer.
- Evidence anchors:
  - [section] "JiuZhou... outperformed GPT-3.5 on the GeoBench objective tasks by leveraging a substantial corpus of geoscience literature for continued pre-training on Mistral-7B."
  - [section] Table 3: ClimateMistral (no domain pretraining) showed 11 significant inaccuracies vs 1 for ClimateChat on 45 questions.
  - [corpus] Related work on climate benchmarks (Climate-Eval arXiv:2505.18653) supports domain evaluation but doesn't test base model selection directly.
- Break condition: If the target domain has low lexical/conceptual overlap with the base model's pretraining corpus, instruction tuning may increase hallucination rather than reduce it.

### Mechanism 2
- Claim: Instruction data diversity (not just accuracy) drives generalization across task types.
- Mechanism: ClimateChat-Corpus combines three sources: (1) Self-QA from authoritative documents for accuracy, (2) Web Scraping from StackExchange for real-world phrasing, and (3) Self-Instruct for scaling diversity via LLM generation with filtering.
- Core assumption: GPT-4 can reliably filter its own hallucinations during Self-Instruct generation.
- Evidence anchors:
  - [abstract] "...generates instructions using facts and background knowledge from documents and enhances the diversity... through web scraping and the collection of seed instructions."
  - [section] "While this method alone may result in lower diversity, the diversity of instruction data is essential."
  - [corpus] No direct corpus comparison available—authors note "accessing open climate change instruction data is challenging."
- Break condition: If filtered Self-Instruct data still contains systematic biases or hallucinations, model may amplify them.

### Mechanism 3
- Claim: RAG and instruction tuning are complementary, not interchangeable.
- Mechanism: RAG provides external factual context for objective queries but does not improve deep domain comprehension for subjective tasks. Instruction tuning activates internal knowledge representations learned during pretraining.
- Core assumption: Subjective task evaluation (via GPT-4 scoring) validly measures domain comprehension.
- Evidence anchors:
  - [section] Table 4: JiuZhouChat + RAG achieves 93.2 on objective tasks (near ClimateChat's 93.0) but only 120.3 on subjective tasks vs ClimateChat's 126.2.
  - [section] "RAG... does not improve the model's deeper comprehension of the climate change domain."
  - [corpus] No related papers directly test RAG vs instruction tuning in climate domain.
- Break condition: If retrieval corpus is more comprehensive than pretraining knowledge, RAG may outperform instruction tuning on certain objective tasks.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables efficient fine-tuning by training low-rank decomposition matrices instead of full weights, critical for domain adaptation without massive compute.
  - Quick check question: Can you explain why LoRA reduces memory requirements compared to full fine-tuning?

- Concept: Instruction Tuning vs Continued Pre-training
  - Why needed here: The paper combines both—JiuZhou does continued pre-training on geoscience, then ClimateChat does instruction tuning. Understanding this distinction explains why base model selection matters.
  - Quick check question: What type of knowledge does each method primarily inject into the model?

- Concept: Hallucination in Domain-Specific LLMs
  - Why needed here: A central finding is that instruction tuning a general model (Mistral) on climate data increased hallucinations, while doing so on a domain-pretrained model (JiuZhou) reduced them.
  - Quick check question: Why might adding domain instructions to a model without domain knowledge increase factual errors?

## Architecture Onboarding

- Component map:
  - Data Pipeline: Wikipedia/Reports/Papers → Self-QA (GPT-4) → ClimateChat-Corpus
  - Data Pipeline: StackExchange → Filtering → ClimateChat-Corpus
  - Data Pipeline: Seed tasks → Self-Instruct (GPT-4 + filtering) → ClimateChat-Corpus
  - Model: Mistral-7B → JiuZhou (geoscience continued pre-training) → ClimateChat (LoRA instruction tuning)

- Critical path: Base model selection → Instruction data construction → LoRA training → Evaluation (objective + subjective)

- Design tradeoffs:
  - Self-QA: High accuracy, lower diversity vs Web Scraping: High diversity, variable quality
  - General vs specialized base model: General offers broader capabilities; specialized reduces hallucination risk
  - RAG-only vs instruction-tuned: RAG faster to deploy; instruction tuning better for subjective reasoning

- Failure signatures:
  - High hallucination rate on factual queries → Likely wrong base model (lacks domain knowledge)
  - Good objective scores, poor subjective scores → RAG-only approach without instruction tuning
  - Low diversity in outputs → Over-reliance on Self-QA without Web Scraping/Self-Instruct

- First 3 experiments:
  1. Reproduce the base model comparison: Train ClimateMistral (Mistral base + climate data) and measure hallucination rate against ClimateChat to validate the domain pretraining hypothesis.
  2. Ablate instruction data sources: Train three variants using only Self-QA, only Web Scraping, only Self-Instruct data to isolate diversity vs accuracy contributions.
  3. Test RAG + instruction tuning boundary: Systematically vary retrieval corpus size against ClimateChat to identify where RAG stops adding value for subjective tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- The study demonstrates base model selection is critical but doesn't test whether results generalize to other specialized domains
- The three-source instruction data construction lacks ablation studies showing individual source contributions
- Subjective task evaluation relies entirely on GPT-4 scoring, which may introduce circularity if GPT-4's knowledge overlaps with training data
- Critical implementation details like LoRA hyperparameters and data mixing ratios are unspecified

## Confidence
- **High confidence**: Domain pretraining + instruction tuning outperforms general model + instruction tuning for reducing hallucinations (supported by 11 vs 1 significant inaccuracies across 45 questions)
- **Medium confidence**: RAG and instruction tuning are complementary for climate QA (supported by differential performance on objective vs subjective tasks, though no ablation studies)
- **Low confidence**: The three-source instruction data construction method is optimal (no ablation studies, no comparison with alternative diversity strategies)

## Next Checks
1. Reproduce the ClimateMistral vs ClimateChat comparison on a held-out climate QA benchmark, then test whether similar patterns hold for other domain-specific instruction tuning tasks (e.g., medical, legal domains with appropriate base models)
2. Train three models each using only one source of ClimateChat-Corpus (Self-QA only, Web Scraping only, Self-Instruct only) to quantify the relative contribution of accuracy vs diversity to overall performance
3. Systematically vary retrieval corpus size and quality against ClimateChat performance to identify the exact point where additional retrieval data stops improving subjective task scores, validating the claim that RAG and instruction tuning serve complementary rather than substitutable roles