---
ver: rpa2
title: 'DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision'
arxiv_id: '2502.08005'
source_url: https://arxiv.org/abs/2502.08005
tags:
- score
- training
- diffusion
- teacher
- one-step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DiffRatio, a new framework for training one-step
  diffusion models that avoids teacher supervision by directly estimating the score
  difference between the student and data distributions via the gradient of a learned
  log density ratio. This approach eliminates the need for separate student score
  estimation, reduces gradient estimation bias, and uses a lightweight density-ratio
  network instead of two full score networks.
---

# DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision
## Quick Facts
- arXiv ID: 2502.08005
- Source URL: https://arxiv.org/abs/2502.08005
- Reference count: 40
- Key outcome: FID 2.39 on CIFAR-10 and 1.41 on ImageNet 512×512 with teacher-free one-step diffusion

## Executive Summary
DiffRatio introduces a novel framework for training one-step diffusion models without teacher supervision by directly estimating the score difference between student and data distributions via the gradient of a learned log density ratio. This approach eliminates the need for separate student score estimation, reduces gradient estimation bias, and uses a lightweight density-ratio network instead of two full score networks. Experiments on CIFAR-10 and ImageNet (64×64 and 512×512) show competitive one-step generation performance, outperforming most teacher-supervised distillation approaches.

## Method Summary
DiffRatio trains one-step diffusion models without teacher supervision by learning a density ratio classifier that distinguishes noisy samples from the student generator versus the data distribution. In Stage 1, a score network is pre-trained via diffusion score matching. In Stage 2, the generator is initialized from the pre-trained score weights with fixed diffusion time, and a density-ratio classifier is trained to distinguish between student and data samples. The generator is then updated using gradients derived from the learned density ratio, with DiJS divergence showing the best performance. This approach eliminates the need for teacher supervision while maintaining competitive generation quality.

## Key Results
- Achieves FID 2.39 on CIFAR-10 unconditional generation
- Achieves FID 1.41 on ImageNet 512×512 class-conditional generation
- Outperforms most teacher-supervised distillation approaches on one-step generation
- Enables principled inference-time parallel scaling using the learned density ratio

## Why This Works (Mechanism)
DiffRatio works by directly estimating the score difference between student and data distributions through the gradient of a learned log density ratio. This eliminates the need for separate student score estimation, reducing gradient estimation bias and computational overhead. The density-ratio network serves as a lightweight discriminator that provides stable gradients for generator updates. By fixing the diffusion time at initialization and using the learned density ratio for generator updates, DiffRatio achieves teacher-free training while maintaining generation quality comparable to or better than distillation-based approaches.

## Foundational Learning
- **Diffusion score matching**: Essential for understanding how noise schedules and score networks work in diffusion models. Quick check: Can you explain the difference between score matching and denoising score matching?
- **Divergence measures**: Understanding f-divergences (KL, JS, reverse KL) is crucial for grasping the theoretical foundation. Quick check: What's the key difference between KL and reverse KL divergence in the context of generative modeling?
- **Density ratio estimation**: Core to DiffRatio's approach of avoiding teacher supervision. Quick check: Why is estimating density ratios often more stable than estimating densities directly?
- **Pathwise gradients**: Critical for understanding how the generator is updated using the learned density ratio. Quick check: How does pathwise gradient estimation differ from score-based updates in traditional diffusion models?
- **VE noise schedules**: Understanding the variance-exploding schedule used in DiffRatio is important for implementation. Quick check: What are the advantages of using VE schedules over VP schedules in one-step diffusion?
- **U-Net architectures**: Essential for understanding the backbone used in both score and density-ratio networks. Quick check: How does the encoder-decoder structure in U-Nets facilitate density ratio estimation?

## Architecture Onboarding
**Component Map:** EDM Score Network (EDM U-Net) -> One-step Generator (fixed t_init=2.5) -> Density-Ratio Classifier (U-Net encoder + sigmoid) -> Generator Updates via DiJS gradients

**Critical Path:** Pre-trained score network initialization → Density ratio classifier training → Generator update via pathwise gradients → FID evaluation

**Design Tradeoffs:** Uses lightweight density-ratio network instead of full teacher-student score pair, reducing computational overhead while maintaining quality. Single-step classifier updates per iteration versus more complex alternating optimization schemes.

**Failure Signatures:** Mode collapse when training from random initialization, training instability when t_init is not properly fixed, poor convergence if VE schedule bounds are incorrect.

**First Experiments:**
1. Pre-train EDM score model on CIFAR-10 and verify it can generate reasonable samples
2. Train density-ratio classifier on pre-trained model and verify it can distinguish real from generated samples
3. Run full DiffRatio training pipeline on CIFAR-10 and verify FID improves from initialization

## Open Questions the Paper Calls Out
None

## Limitations
- Missing details on t' sampling distribution and exact classifier architecture specifications
- No explicit claims about speed/throughput or memory advantages relative to other one-step methods
- Scalability results depend on additional unquantified inference-time adjustments
- Single-step classifier updates per iteration not fully explored in ablation studies

## Confidence
**High confidence:** Core training pipeline (Stages 1-2), use of DiJS as best-performing f-divergence, stated FID/IS scores on CIFAR-10 and ImageNet, claim that DiffRatio removes teacher supervision while matching/exceeding distillation baselines.

**Medium confidence:** Scalability results via learned density ratio, claim that single-step classifier updates are sufficient.

**Low confidence:** Speed/throughput or memory advantage claims relative to other one-step methods.

## Next Checks
1. Replicate Stage 2 training on CIFAR-10 and confirm FID converges to ~2.4
2. Verify DiJS vs DiKL/DiRM performance through controlled ablation
3. Validate inference-time scaling by applying density-ratio-based scaling to converged model and measuring FID/IS improvement