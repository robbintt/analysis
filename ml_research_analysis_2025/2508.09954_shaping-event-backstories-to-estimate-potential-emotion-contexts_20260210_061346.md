---
ver: rpa2
title: Shaping Event Backstories to Estimate Potential Emotion Contexts
arxiv_id: '2508.09954'
source_url: https://arxiv.org/abs/2508.09954
tags:
- event
- emotion
- events
- chains
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ambiguous emotion analysis
  in event descriptions due to missing contextual information. The authors propose
  generating contextual backstories for events to disambiguate emotion interpretations.
---

# Shaping Event Backstories to Estimate Potential Emotion Contexts

## Quick Facts
- arXiv ID: 2508.09954
- Source URL: https://arxiv.org/abs/2508.09954
- Reference count: 40
- Method generates contextual backstories to disambiguate emotion interpretation in event descriptions

## Executive Summary
This paper addresses the problem of ambiguous emotion analysis in event descriptions due to missing contextual information. The authors propose generating contextual backstories for events to disambiguate emotion interpretations. Their method uses an LLM to create coherent event chains by iteratively prompting with story planning and revision steps. Through automatic and human evaluation, they demonstrate that generated contexts significantly enhance emotion interpretation clarity for human annotators and improve automatic emotion classification performance. The PCR method (Plan-Construct-Revise) shows the highest coherence scores (.84) and successfully disambiguates emotion interpretations for most events, particularly for emotions like relief and sadness. The study provides a specialized dataset for contextualized emotion analysis and offers potential applications in explainable AI.

## Method Summary
The method generates 1000 event descriptions using type/object attributes, then creates 13 backstories per event (one per emotion category) using three LLM generation methods: Baseline (single prompt), Plan-Construct (PC: plan then construct), and Plan-Construct-Revise (PCR: plan, construct, revise). Coherence is evaluated using a zero-shot shuffle test with Llama-3.1-8B-Instruct, while emotion analysis uses zero-shot classification with Llama-3.1-70B-Instruct. The PCR approach shows highest coherence (.84) and successfully disambiguates emotion interpretations for most events, particularly relief and sadness.

## Key Results
- PCR method achieves highest coherence scores (.84) compared to baseline (.75)
- Generated backstories significantly improve human annotation clarity for emotion interpretation
- Emotion trajectories show distinct patterns: relief increases steadily with final-event spike, while fear and pride peak earlier then decline
- Backstories successfully disambiguate most emotion categories, though boredom, trust, and no-emotion remain challenging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contextual backstories disambiguate emotion interpretation of ambiguous events by providing cognitive appraisal cues.
- **Mechanism:** The target event (e.g., "loudspeaker malfunctioned") is semantically neutral or ambiguous alone. Generated backstories establish narrative expectations (responsibility → guilt; anxiety release → relief; unpredictability → fear; proactive effort → pride). These narratives pre-activate cognitive appraisal patterns that shape how the final event is interpreted emotionally.
- **Core assumption:** Emotion responses to events depend on reader-inferred context; without explicit context, annotators project default assumptions that cause disagreement.
- **Evidence anchors:** [abstract] "adds reasonable contexts to event descriptions, which may better explain a particular situation"; [section] Table 1 shows identical target event evoking guilt, relief, fear, pride depending on backstory; [corpus] Related work (Troiano et al. 2023) links implicit emotion cues to appraisal theories.
- **Break condition:** Events with inherently low ambiguity (strong prior emotion associations) may show minimal disambiguation benefit from added context.

### Mechanism 2
- **Claim:** Iterative story planning with revision (PCR) produces more coherent narratives than single-prompt generation.
- **Mechanism:** The baseline consolidates all constraints into one prompt, causing the model to generate tightly event-linked but narratively shallow backstories. PC decomposes into planning (scenario outline) then construction (event sequence). PCR adds a revision step that evaluates coherence against the plan and adjusts. This decomposition reduces cognitive load on the model at each step, allowing better narrative flow.
- **Core assumption:** Coherence in short stories can be measured via sequence likelihood against shuffled permutations (shuffle test), and higher coherence correlates with better disambiguation performance.
- **Evidence anchors:** [section] Table 4: PCR coherence .84 vs baseline .75; PCR shows consistent improvement across emotion categories; [section] Appendix D notes baseline "link[s] the backstory very strongly to the last event and frequently reuse content"; [corpus] Razumovskaia et al. (2024) and Clark and Sood (2022) support story planning benefits.
- **Break condition:** Excessive revision iterations may increase emotion word leakage (PCR: 5% vs baseline: 2%), potentially biasing annotations.

### Mechanism 3
- **Claim:** Different emotion categories require different narrative trajectory patterns for successful elicitation.
- **Mechanism:** Complex emotions like relief require progressive narrative buildup (anxiety → resolution), where the final event triggers a sharp probability increase. Emotions like fear and pride may be largely established in the backstory itself, with the target event confirming or slightly diminishing the predicted emotion. The model implicitly learns these trajectory patterns from pretraining on narratives.
- **Core assumption:** LLMs encode emotion trajectory patterns from narrative pretraining data; zero-shot prompting can access these patterns when given emotion-conditioned generation instructions.
- **Evidence anchors:** [section] Figure 3 shows relief trajectory: steady increase with sharp spike at final event; fear/pride show rise then decline; [section] Section 4.3: "relief exhibits a steady increase... with the most significant surge occurring upon the introduction of the final event"; [corpus] Related work on emotion trajectories in dialogues (Labat et al. 2024) supports dynamic emotion patterns.
- **Break condition:** Some emotions (boredom, trust, no-emotion) remain difficult to elicit even with PCR, suggesting narrative patterns for these categories are either underrepresented in pretraining or inherently harder to construct.

## Foundational Learning

- **Concept: Appraisal theory of emotions**
  - Why needed here: The paper's approach implicitly relies on cognitive appraisals (responsibility, control, goal-congruence) being established through narrative context. Understanding that emotions arise from how events are interpreted (not just events themselves) explains why backstories work.
  - Quick check question: Can you explain why "loudspeaker malfunction" could evoke either guilt or relief depending on what happened before?

- **Concept: Chain-of-thought prompting / decomposition**
  - Why needed here: The PC and PCR methods use prompt decomposition (plan → construct → revise) rather than single-prompt generation. This is a standard LLM technique for complex tasks.
  - Quick check question: Why might asking an LLM to first outline a story plan before writing it produce better results than asking it to write the story directly?

- **Concept: Shuffle test for coherence evaluation**
  - Why needed here: The paper uses a zero-shot coherence metric based on comparing original sequence likelihood to shuffled permutations. Understanding this evaluation method is necessary to interpret the coherence results.
  - Quick check question: If an event chain has a coherence score of 0.9, what does that mean about its likelihood ranking among 30 shuffled variants?

## Architecture Onboarding

- **Component map:** Event generator -> Backstory generator (Baseline/PC/PCR) -> Coherence evaluator (Llama-3.1-8B) -> Emotion classifier (Llama-3.1-70B) -> Human annotation layer
- **Critical path:** The PCR Plan step (Prompt 2.1) is the key differentiator. It asks the model to first explain "a scenario in which it can be deduced... that you felt emotion" before generating events. This scenario reasoning step appears to anchor subsequent event generation.
- **Design tradeoffs:** Length vs. leakage: PCR produces longer backstories (79.2 tokens) but has higher emotion word leakage (5%) vs baseline (69.8 tokens, 2% leakage); Coherence vs. diversity: All methods show similar Jaccard diversity (~0.87-0.90), but PCR achieves higher coherence; Complexity vs. reproducibility: Multi-step PCR requires careful prompt engineering and may be more sensitive to model version changes.
- **Failure signatures:** Low coherence scores (<0.70) particularly for fear narratives across all methods; Emotion confusion patterns: anger↔sadness, guilt↔shame, trust→relief (Table 5, Tables 20-22); Failed emotion elicitation: boredom, trust, no-emotion categories show low human-system agreement; Excessive backstory-event coupling: Baseline method over-links to final event.
- **First 3 experiments:**
  1. Reproduce coherence evaluation: Run shuffle test on 50 event chains comparing baseline vs PCR. Verify coherence score distributions match Table 4 (PCR should show ~0.84 mean vs baseline ~0.75).
  2. Validate emotion trajectory patterns: For 20 PCR-generated relief chains and 20 fear chains, plot p(e|sentences 1-5) trajectories. Confirm relief shows increasing trajectory with final-event spike; fear shows earlier peak with decline.
  3. Test annotation clarity improvement: Have 3 annotators label emotions for 10 events alone and then with PCR backstories. Calculate Fleiss' kappa for events-only vs. chains. Expect subset selection to show 10/13 backstories improving clarity (per Figure 2).

## Open Questions the Paper Calls Out

- **Can the generated backstory methodology be effectively adapted to improve emotion recognition in interactive dialogue systems?**
  - Basis in paper: [explicit] Conclusion states "future work should investigate how our narratives can be tailored for various applications, such as improving dialogue systems."
  - Why unresolved: The current study only validates the approach on isolated, static event descriptions rather than dynamic conversational contexts.
  - What evidence would resolve it: Integrating the context generation framework into a conversational agent and evaluating performance on a standard dialogue emotion recognition benchmark.

- **How do demographic differences influence the interpretation of emotions in these contextualized narratives?**
  - Basis in paper: [explicit] Conclusion suggests the dataset allows researchers to explore "the impact of demographic differences on contextualized emotion interpretations."
  - Why unresolved: The study aggregates human annotations without analyzing how specific demographic backgrounds might correlate with differing interpretations.
  - What evidence would resolve it: A controlled annotation study collecting annotator demographics to perform a correlation analysis between demographic factors and emotion label selection.

- **Can the variation in generated backstories be utilized to distinguish between events that are inherently unemotional versus those that are ambiguous?**
  - Basis in paper: [explicit] Conclusion identifies the need for "further distinction between such types of events" and calls it "a starting point for future work."
  - Why unresolved: The paper observes that backstories can "charge" unemotional events but does not formalize a method to detect the event type a priori.
  - What evidence would resolve it: Developing a classifier that uses the variance of generated backstories (e.g., emotion entropy) as features to predict the intrinsic emotionality of the seed event.

## Limitations

- The study uses artificially generated events and backstories rather than naturally occurring narratives, raising questions about ecological validity.
- The zero-shot evaluation approach depends heavily on the specific LLM model and prompt templates used.
- The shuffle test coherence metric, though innovative, is based on likelihood comparisons that may not fully capture human notions of narrative coherence.
- The study focuses on English-language narratives and doesn't address cross-cultural variations in emotion interpretation or potential biases in LLM training data.

## Confidence

- **High Confidence**: The PCR method produces more coherent narratives than baseline (Table 4 coherence scores .84 vs .75), and generated backstories significantly improve human annotation clarity for emotion interpretation (Figure 2 subset selection showing 10/13 backstories improving clarity). The observed emotion trajectory patterns for relief (increasing with final-event spike) and fear/pride (earlier peak with decline) are consistently replicated across chains.
- **Medium Confidence**: The claim that context disambiguation works through cognitive appraisal establishment is supported by qualitative examples but lacks direct validation. The emotion leakage rates (PCR: 5%, Baseline: 2%) are reported but their impact on annotation bias is not quantified. The failure modes for boredom, trust, and no-emotion categories suggest systematic limitations but don't explain the underlying causes.
- **Low Confidence**: The automatic emotion classification improvements (higher F1 scores with backstories) may reflect prompt template effects rather than genuine disambiguation, as the same LLM generates both backstories and performs classification. The shuffle test's ability to capture "true" narrative coherence remains unproven against human judgments.

## Next Checks

1. **Ecological Validity Test**: Collect naturally occurring event descriptions from social media or personal narratives, generate backstories using PCR, and measure annotation clarity improvement with human annotators. Compare results to the current study's controlled setup to assess generalization.

2. **Cross-Model Validation**: Repeat the core experiments using different LLM models (e.g., GPT-4, Claude) for both backstory generation and emotion classification. Measure coherence scores, annotation clarity, and classification performance to determine if results depend on the specific Llama-3.1 model.

3. **Bias and Leakage Analysis**: Conduct a systematic study of emotion word leakage effects by varying leakage rates (0%, 2%, 5%, 10%) across backstory sets. Measure how different leakage levels impact human annotation bias and automatic classification accuracy to quantify the tradeoff between context richness and annotation purity.