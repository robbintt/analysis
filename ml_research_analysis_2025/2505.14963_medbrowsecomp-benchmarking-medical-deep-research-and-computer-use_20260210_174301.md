---
ver: rpa2
title: 'MedBrowseComp: Benchmarking Medical Deep Research and Computer Use'
arxiv_id: '2505.14963'
source_url: https://arxiv.org/abs/2505.14963
tags:
- answer
- should
- question
- information
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedBrowseComp introduces the first benchmark targeting multi-hop,
  live medical fact retrieval from heterogeneous knowledge bases. It contains over
  1,000 human-curated questions reflecting real clinical scenarios.
---

# MedBrowseComp: Benchmarking Medical Deep Research and Computer Use

## Quick Facts
- arXiv ID: 2505.14963
- Source URL: https://arxiv.org/abs/2505.14963
- Reference count: 40
- Primary result: State-of-the-art deep research and computer use agents achieve as low as 10% accuracy on multi-hop medical fact retrieval tasks.

## Executive Summary
MedBrowseComp introduces the first benchmark for evaluating multi-hop, live medical fact retrieval from heterogeneous knowledge bases. The benchmark contains over 1,000 human-curated questions reflecting real clinical scenarios, requiring agents to navigate between sources like HemOnc.org, ClinicalTrials.gov, and FDA Orange Book PDFs. Evaluations reveal that even state-of-the-art deep research and computer use agents struggle with accuracy as low as 10%, exposing a major capability gap in LLM-based medical information seeking.

## Method Summary
The benchmark uses three heterogeneous medical knowledge bases (HemOnc.org, ClinicalTrials.gov, FDA Orange Book) to generate over 1,000 multi-hop questions requiring information synthesis across sources. Agents are evaluated using GPT-4.1 mini as an automated judge comparing responses against ground truth. The evaluation includes both deep research agents (API-based) and computer use agents (GUI-based) running in Docker containers with Firefox.

## Key Results
- Multi-hop performance decays monotonically with hop count, with accuracy dropping below 20% for 4-5 hop tasks
- O3 deepresearch achieves 25.5/50 correct answers (51%), a 34% relative gain over O3 search (19/50)
- Computer use agents show significant improvement when initialized at high-certainty entry points (PMID extraction accuracy increases from 11.57% to 42.98%)

## Why This Works (Mechanism)

### Mechanism 1
Iterative "Deep Research" agents appear to outperform single-shot search by enabling self-correction across reasoning steps. Models utilizing "deep research" modes likely allocate test-time compute to verify intermediate findings before synthesis, whereas single-shot search risks propagating errors from the first retrieved context. The mechanism is *iterative refinement* rather than linear extraction.

### Mechanism 2
Grounding Computer Use Agents (CUA) in high-certainty "entry points" significantly reduces navigation errors. By initializing the agent at a verified domain hub (e.g., HemOnc.org) rather than a generic search engine, the system reduces the *search entropy*. The agent expends fewer tokens filtering noise and encounters fewer "aimless wandering" loops.

### Mechanism 3
Performance decays monotonically with "hop" count due to compounding extraction noise. Multi-hop tasks require maintaining a coherent state across distinct webpages. Visual or context noise accumulates at each step (pagination, PDF parsing, ads), diluting the "reasoning signal" required for the next hop.

## Foundational Learning

- **Concept: Multi-hop Retrieval vs. RAG**
  - Why needed here: The paper distinguishes between simple retrieval (finding one fact) and multi-hop synthesis (finding a fact to find another fact). Understanding this distinction is critical for diagnosing why RAG performance plateaus on complex queries.
  - Quick check question: Can the system answer "Who is the PI of the trial that tested the drug approved in 2023?" (Requires finding the drug, then the trial, then the PI).

- **Concept: Context Window Economics (Text vs. Pixels)**
  - Why needed here: CUA agents consume massive token budgets (screenshots) compared to text-based APIs. An engineer must understand that visual navigation is computationally expensive and caps the depth of navigation.
  - Quick check question: Why does a GUI agent hit the 200k token limit faster than a text-based browser agent performing the same task?

- **Concept: LLM-as-Judge Evaluation**
  - Why needed here: The benchmark relies on an automated judge (GPT-4.1 mini) rather than human review for scale. Understanding the prompt structure of this judge is vital for reproducing the benchmark results.
  - Quick check question: Does the evaluation script check for semantic equivalence or exact string match?

## Architecture Onboarding

- **Component map:** Dataset Generation (HemOnc.org + ClinicalTrials.gov + FDA Orange Book) → Curated Questions → Execution Layer (Docker containers with Firefox or API integration) → Evaluation Layer (LLM-as-Judge)
- **Critical path:** 1. Prompt Encoding (converting multi-hop question to agent format) → 2. Tool Allocation (deciding between search, browse, or read_pdf) → 3. Extraction (parsing specific datum from final source)
- **Design tradeoffs:** CUA (Visual) vs. Deep Research (Text/API) - CUA is currently less efficient (expensive, lower accuracy on deep tasks) but more generalizable to non-API environments. Parametric Memory vs. Live Retrieval - Relying on training data causes hallucinations on new medical trials; relying on live search introduces navigation noise.
- **Failure signatures:** Greedy Extraction (grabbing first date rather than specific field), Tool Quota Exhaustion (using all tool calls verifying trial exists), Semantic Drift (retrieving valid PMID for related but incorrect paper)
- **First 3 experiments:** 1. Baseline Run (execute MedBrowseComp-50 subset using standard RAG pipeline to establish personal baseline and verify hop decay curve), 2. Entry Point Ablation (run CUA tasks with Google Search start page vs. HemOnc.org URL, measure delta in token usage and accuracy), 3. Judge Validation (manually review 10 random model outputs and compare human assessment against LLM-Judge script to calibrate trust)

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark composition may not fully characterize distribution of difficulty and clinical relevance across question types
- Evaluation focuses on narrow oncology domain, limiting generalizability to broader medical domains
- LLM-as-Judge reliability on borderline cases (semantic equivalence vs. exact match) not fully characterized

## Confidence
- Multi-hop performance decay is monotonic: **High**
- Deep Research agents outperform single-shot search: **High**
- Entry point initialization improves CUA performance: **High**

## Next Checks
1. **Cross-Domain Generalization:** Run the MedBrowseComp benchmark with a non-oncology medical domain (e.g., cardiology or infectious disease) to test if the "hop decay" and accuracy trends hold.
2. **Judge Calibration on Edge Cases:** Manually review 50 random model outputs where the LLM-as-Judge score was between 0.4 and 0.6 to identify systematic biases or blind spots in the evaluation metric.
3. **State-Management Intervention:** Implement a simple "memory pruning" module for the CUA agent that aggressively filters non-relevant tokens from previous hops and measure its impact on accuracy for 4+ hop tasks.