---
ver: rpa2
title: Template-Based Financial Report Generation in Agentic and Decomposed Information
  Retrieval
arxiv_id: '2504.14233'
source_url: https://arxiv.org/abs/2504.14233
tags:
- financial
- report
- reports
- agenticir
- earnings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares two LLM-based approaches\u2014AgenticIR and\
  \ DecomposedIR\u2014for generating structured financial reports from earnings call\
  \ transcripts. AgenticIR uses collaborative agents prompted with a full template,\
  \ while DecomposedIR applies prompt chaining to break down the template into subqueries\
  \ for detailed retrieval and generation."
---

# Template-Based Financial Report Generation in Agentic and Decomposed Information Retrieval

## Quick Facts
- **arXiv ID:** 2504.14233
- **Source URL:** https://arxiv.org/abs/2504.14233
- **Reference count:** 38
- **Primary result:** DecomposedIR statistically significantly outperforms AgenticIR with 27% average improvement in coverage metrics for financial report generation from earnings call transcripts.

## Executive Summary
This study compares two LLM-based approaches—AgenticIR and DecomposedIR—for generating structured financial reports from earnings call transcripts. AgenticIR uses collaborative agents prompted with a full template, while DecomposedIR applies prompt chaining to break down the template into subqueries for detailed retrieval and generation. Experiments on financial and weather-domain datasets show that DecomposedIR statistically significantly outperforms AgenticIR in providing broader and more detailed coverage, with improvements of 27% on average across key evaluation metrics. Self-reflection mechanisms improve performance but reduce readability. The findings highlight the effectiveness of prompt decomposition over agentic frameworks in template-based report generation, offering practical insights for real-world applications.

## Method Summary
The study evaluates two information retrieval approaches for financial report generation: AgenticIR and DecomposedIR. AgenticIR employs a multi-agent framework where agents collaboratively process a complete report template. DecomposedIR uses prompt chaining to decompose the template into smaller subqueries, each handled by specialized modules for information retrieval and generation. Both approaches utilize GPT-4o-mini as the underlying LLM. The evaluation employs three metrics: DecompEval for coverage assessment, G-Eval for factuality evaluation, and automatic readability metrics (FKGL, ARI). Experiments compare performance across synthetic and real-world datasets in both financial (earnings call transcripts) and weather domains.

## Key Results
- DecomposedIR statistically significantly outperforms AgenticIR in providing broader and more detailed coverage across evaluation metrics
- DecomposedIR achieves a 27% average improvement compared to AgenticIR in key performance metrics
- Self-reflection mechanisms improve evaluation scores but consistently reduce readability through increased text complexity and document length

## Why This Works (Mechanism)
The superior performance of DecomposedIR stems from its ability to break down complex templates into manageable subqueries, allowing more focused and detailed information retrieval. This decomposition enables specialized processing of each template component, resulting in more comprehensive coverage compared to the holistic approach of AgenticIR. The modular nature of DecomposedIR allows for targeted retrieval and generation, while AgenticIR's multi-agent coordination introduces potential communication overhead and reduced precision in handling individual template elements.

## Foundational Learning
- **Prompt Decomposition:** Breaking complex templates into smaller subqueries enables focused information retrieval and more detailed responses. Why needed: Complex templates overwhelm single-prompt approaches. Quick check: Compare performance on decomposed vs. monolithic prompts.
- **Multi-agent vs. Prompt Chaining:** Different architectural approaches to structured report generation. Why needed: Understanding trade-offs between coordination overhead and modular precision. Quick check: Measure performance gap between AgenticIR and DecomposedIR.
- **Self-reflection Mechanisms:** Iterative improvement processes that enhance content accuracy but may increase complexity. Why needed: Balancing quality improvement with readability preservation. Quick check: Track readability metrics when self-reflection is enabled.
- **Template-based Generation:** Structured approaches for consistent report creation from unstructured source data. Why needed: Ensures completeness and format compliance in financial reporting. Quick check: Verify template coverage in generated outputs.
- **Information Retrieval Evaluation:** Metrics measuring coverage breadth and factuality in generated content. Why needed: Objective assessment of information completeness. Quick check: Compare DecompEval scores across approaches.
- **Automated Readability Assessment:** Tools measuring linguistic complexity and accessibility. Why needed: Ensuring generated content remains usable by target audiences. Quick check: Monitor FKGL and ARI scores across experiments.

## Architecture Onboarding

**Component Map:** Template -> [AgenticIR: Multi-agent Coordinator -> Report Generator] OR [DecomposedIR: Prompt Decomposer -> Subquery Processors -> Report Assembler]

**Critical Path:** Input Template → Information Retrieval → Content Generation → Report Assembly → Evaluation

**Design Tradeoffs:** AgenticIR prioritizes coordination and holistic understanding but suffers from complexity overhead; DecomposedIR prioritizes modular precision and focused retrieval but requires careful prompt engineering for decomposition

**Failure Signatures:** AgenticIR failures manifest as incomplete coverage and coordination breakdowns; DecomposedIR failures appear as fragmented outputs and poor inter-module coherence

**3 First Experiments:** 1) Compare coverage metrics (DecompEval) between approaches on synthetic financial templates, 2) Evaluate factuality scores (G-Eval) across weather domain documents, 3) Measure readability impact (FKGL/ARI) when enabling self-reflection mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating Query-Focused or Aspect-based Summarization methods into the multi-agent framework close the performance gap with the decomposed prompting approach?
- **Basis in paper:** The conclusion states that future directions should investigate "exploring the enhancement of multi-agents with Query-Focused Summarization methods or Aspect-based Summarization in template-based report generation."
- **Why unresolved:** The current study only compared the baseline agentic framework against decomposed prompting without implementing these specific advanced summarization techniques.
- **What evidence would resolve it:** Experimental results showing an enhanced AgenticIR system achieving statistical parity or superiority over DecomposedIR in coverage metrics (DecompEval/G-Eval).

### Open Question 2
- **Question:** Is it possible to design self-reflection mechanisms that improve factual coverage without simultaneously increasing text complexity and reducing readability?
- **Basis in paper:** [inferred] The results (Table 2) show that while self-reflection improves evaluation scores, it consistently increases FKGL and ARI scores, leading to "longer documents" and reduced readability.
- **Why unresolved:** The paper identifies this trade-off but does not propose or test methods to balance detail-oriented reflection with linguistic simplicity.
- **What evidence would resolve it:** A modified self-reflection prompt or filtering step that maintains high G-Eval scores while keeping readability metrics stable compared to the non-reflection baseline.

### Open Question 3
- **Question:** Does the superior performance of DecomposedIR generalize to financial report generation using local or smaller open-source LLMs, given the study relied solely on GPT-4o-mini?
- **Basis in paper:** [inferred] The methodology section notes, "For the AgenticIR and DecomposedIR, we used GPT-4o-mini... as the LLM," leaving the performance dependency on model scale and proprietary architecture untested.
- **Why unresolved:** DecomposedIR relies on prompt chaining and decomposition quality, which may vary significantly in smaller models that struggle with complex instruction following compared to GPT-4o-mini.
- **What evidence would resolve it:** A replication of the experiments using open-source models (e.g., Llama-3-8B or Mistral) to verify if the 27% improvement margin holds across different model architectures.

## Limitations
- The study relies on synthetic and limited real-world datasets, focusing primarily on earnings call transcripts and a small subset of weather news articles
- Performance improvements measured through automated metrics may not fully capture nuanced quality aspects valued in professional financial reporting contexts
- Self-reflection mechanisms that improve performance metrics consistently reduce readability, indicating unresolved trade-offs
- The study does not assess performance on other business document types or longer time-series financial data
- Computational costs and latency implications of the more complex DecomposedIR approach compared to AgenticIR were not evaluated

## Confidence
- **High confidence** in the core finding that DecomposedIR outperforms AgenticIR with statistical significance and 27% average improvement across metrics
- **High confidence** in the observation that self-reflection improves performance but reduces readability, consistently observed across multiple evaluation dimensions
- **Medium confidence** in the practical implications for real-world applications given the limited scope of tested document types and absence of user studies or deployment assessments in actual professional settings

## Next Checks
1. Conduct user studies with financial analysts to assess whether quantitative improvements translate to perceived quality and utility in professional contexts
2. Test both approaches across additional business document types including annual reports, investor presentations, and regulatory filings to evaluate domain generalization
3. Measure and compare computational costs, processing times, and resource requirements between the two approaches under realistic deployment scenarios to assess practical feasibility