---
ver: rpa2
title: Measuring Time-Series Dataset Similarity using Wasserstein Distance
arxiv_id: '2507.22189'
source_url: https://arxiv.org/abs/2507.22189
tags:
- datasets
- distance
- time-series
- dataset
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to measure the similarity between
  time-series datasets using the Wasserstein distance. The authors model each dataset
  as a multivariate normal distribution and compute the Wasserstein distance between
  these distributions as a measure of dataset similarity.
---

# Measuring Time-Series Dataset Similarity using Wasserstein Distance

## Quick Facts
- **arXiv ID:** 2507.22189
- **Source URL:** https://arxiv.org/abs/2507.22189
- **Reference count:** 18
- **Primary result:** Wasserstein distance between MVNs correlates (>0.60) with foundation model inference loss on target datasets.

## Executive Summary
This paper introduces a novel method for measuring similarity between time-series datasets using the Wasserstein distance between multivariate normal distributions (MVNs). Each dataset is modeled as an MVN, with the Wasserstein distance serving as a similarity metric. The approach handles datasets of different lengths and captures internal time series relationships. Experiments on 30 real-world datasets demonstrate that this distance is more informative than traditional measures for clustering and predicts foundation model performance with high correlation (>0.60) in both out-of-distribution and transfer learning scenarios.

## Method Summary
The method involves sampling 20,000 windows of length 48 from each dataset, normalizing them, and estimating MVN parameters via maximum likelihood. The 2-Wasserstein distance between these distributions is computed using the closed-form expression for MVNs, which involves mean vectors, covariance matrices, and their matrix geometric mean. This distance serves as a quantitative measure of dataset similarity, with experimental validation showing strong correlation with foundation model performance metrics.

## Key Results
- Wasserstein distance captures meaningful clusters and subclusters of similar time-series datasets
- Achieves Pearson correlation >0.60 between distance and foundation model inference loss
- Outperforms Euclidean distance and Dynamic Time Warping for dataset similarity measurement
- Shows effectiveness in both out-of-distribution and transfer learning scenarios

## Why This Works (Mechanism)
The Wasserstein distance provides a geometrically meaningful measure of difference between probability distributions. For multivariate normal distributions, it has a closed-form solution that incorporates both mean differences and covariance structure. By modeling each time-series dataset as an MVN, the method captures not just central tendencies but also the spread and correlation structure within the data. The high correlation with model performance suggests that datasets with similar statistical properties (as measured by this distance) pose similar challenges to foundation models.

## Foundational Learning

**Multivariate Normal Distribution**
- Why needed: Forms the basis for modeling each dataset as a single statistical entity
- Quick check: Verify that sample mean and covariance matrices are correctly computed

**Wasserstein Distance (Earth Mover's Distance)**
- Why needed: Provides a geometrically meaningful measure of distribution difference
- Quick check: Confirm that the closed-form expression for MVN Wasserstein distance is correctly implemented

**Matrix Geometric Mean**
- Why needed: Required for computing the trace term in the Wasserstein distance formula
- Quick check: Ensure matrix square root operations are numerically stable

## Architecture Onboarding

**Component Map**
Sampling -> Normalization -> MVN Parameter Estimation -> Wasserstein Distance Calculation -> Correlation Analysis

**Critical Path**
The most computationally intensive step is sampling and parameter estimation (N=20,000, L=48), followed by the matrix operations in the Wasserstein distance calculation.

**Design Tradeoffs**
- Modeling choice: Single MVN vs. mixture models - favors simplicity over capturing multimodality
- Sampling strategy: Fixed window size vs. adaptive - favors consistency over flexibility
- Distance metric: Wasserstein vs. other divergences - favors geometric interpretation over computational simplicity

**Failure Signatures**
- Singular covariance matrices indicate insufficient samples or highly correlated data
- Negative eigenvalues in matrix operations suggest numerical instability
- Poor correlations with model performance may indicate MVN assumption violations

**3 First Experiments**
1. Verify the closed-form Wasserstein distance calculation on synthetic MVN pairs with known parameters
2. Test the method on simple synthetic time series with controlled similarity/dissimilarity
3. Apply the method to datasets with known relationships to validate clustering performance

## Open Questions the Paper Calls Out
1. What factors cause negative correlation between Wasserstein distance and inference loss on outlier datasets?
2. How does the method perform on non-i.i.d. time-series data that violates the distribution assumptions?
3. Can extending the single MVN assumption to Gaussian Mixture Models improve performance on multi-modal datasets?

## Limitations
- Relies on the MVN assumption which may not hold for complex real-world time series
- Computationally expensive sampling requirement (20,000 windows of length 48)
- Limited evaluation on datasets with strong temporal dependencies or non-stationarity

## Confidence
- **High Confidence:** Mathematical formulation of Wasserstein distance for MVNs is well-established
- **Medium Confidence:** Claim of being "more informative" than alternatives requires context-dependent interpretation
- **Low Confidence:** Generalizability to non-Gaussian, non-stationary, or multimodal time series is unproven

## Next Checks
1. Test method robustness on synthetic non-Gaussian datasets with known ground truth similarities
2. Perform sensitivity analysis by varying sampling parameters (N, L) and covariance regularization
3. Compare MVN-based Wasserstein distance against GMM-based approaches on multi-modal datasets