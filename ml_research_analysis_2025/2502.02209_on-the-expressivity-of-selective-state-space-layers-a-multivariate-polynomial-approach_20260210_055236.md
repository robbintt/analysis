---
ver: rpa2
title: 'On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial
  Approach'
arxiv_id: '2502.02209'
source_url: https://arxiv.org/abs/2502.02209
tags:
- layers
- mamba
- arxiv
- layer
- polynomial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the expressivity of selective state-space
  layers (S6), the core component of Mamba architectures. Using multivariate polynomials,
  the authors prove that S6 layers surpass linear transformers in expressiveness,
  particularly for long sequences.
---

# On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial Approach

## Quick Facts
- arXiv ID: 2502.02209
- Source URL: https://arxiv.org/abs/2502.02209
- Reference count: 40
- Key outcome: Proves S6 layers surpass linear transformers in expressiveness using multivariate polynomials, showing a single S6 layer can represent high-degree polynomials while linear attention requires logarithmic depth.

## Executive Summary
This paper establishes that selective state-space (S6) layers, the core component of Mamba architectures, are theoretically more expressive than linear attention models. Using multivariate polynomials as a measure of expressivity, the authors prove that a single S6 layer can represent polynomials with degree proportional to sequence length, while linear attention layers are limited to constant degree. Crucially, they show this superior expressivity doesn't compromise generalization through the first length-agnostic generalization bound for S6 layers. Empirical validation on synthetic datasets confirms that S6 layers effectively learn high-degree polynomials where linear attention struggles, establishing S6 as theoretically superior for long-range tasks.

## Method Summary
The paper analyzes S6 layers through the lens of multivariate polynomial expressivity. It introduces simplified variants of S6 using polynomial approximations for the softplus and exp functions, making theoretical analysis tractable while maintaining empirical performance. The analysis involves calculating the degree of polynomials that can be represented by different architectures, proving that S6's recurrence structure enables accumulation of products across positions, creating high-degree terms. The paper also derives a Rademacher complexity-based generalization bound that remains length-agnostic, and validates the theoretical findings on synthetic polynomial regression tasks and standard benchmarks.

## Key Results
- A single S6 layer can represent multivariate polynomials with degree up to L+3, while linear attention layers are limited to degree 3 regardless of sequence length
- S6 layers achieve superior expressivity without compromising generalization, providing the first length-agnostic generalization bound
- Empirical validation shows S6 layers outperform linear attention on synthetic polynomial tasks, particularly for high-degree polynomials
- Simplified S6 variants (polynomial approximations) perform comparably to full S6 models (0.7-2.4% degradation) while enabling theoretical analysis

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Degree Scaling with Sequence Length
The S6 recurrence h_t = Ā_t h_{t-1} + B̄_t x_t accumulates products across positions through Π_{k=j+1}^t Ā_k, creating polynomial terms involving many input elements. This enables degree proportional to sequence length L, while linear attention computes degree-3 polynomials per layer regardless of length. The expressivity advantage diminishes if sequence length is small or tasks don't require high-degree computations.

### Mechanism 2: Input-Dependent State Transmissions Enable Selective Accumulation
The selective mechanism (data-dependent Ā_t, B̄_t, C_t) conditions information flow on content, allowing S6 to selectively accumulate or discard information. Unlike fixed SSMs, S6 computes B_t = S_B X_{*t}, C_t = S_C X_{*t}, Δ_t = softplus(S_Δ X_{*t}) at each timestep. This allows amplification of important tokens while suppressing irrelevant ones. The overhead may not provide benefits if tasks require uniform information integration.

### Mechanism 3: Length-Agnostic Generalization via Norm-Based Bounds
Despite superior expressivity, S6 layers maintain generalization bounds that don't degrade with sequence length. The Rademacher complexity bound scales with O(1/√m · D²Γ(w) · K/(K-1)²) where L doesn't appear in the dominant term. The proof partitions the function class by norm-based criteria. The bound becomes loose if trained models violate the K < 1 assumption or if norms grow with sequence length during training.

## Foundational Learning

- Concept: **Multivariate Polynomial Expressivity**
  - Why needed here: The paper uses maximum polynomial degree as a proxy for how many tokens can jointly influence an output. Understanding that composing polynomials multiplies degrees (degree-3 composed N times → degree 3^N) explains why transformers need O(log L) depth.
  - Quick check question: Given two functions f(x) = x³ and g(y) = y², what is the degree of g(f(x))?

- Concept: **State-Space Model Recurrence with Discretization**
  - Why needed here: S6 builds on continuous-time SSMs discretized via Δ parameter. The recurrence h_t = Ā_t h_{t-1} + B̄_t x_t is the computational core, with Ā = exp(ΔA) controlling memory decay.
  - Quick check question: If Ā = 0.9 and h_0 = 0, what is h_3 after inputs x_1=1, x_2=0, x_3=1?

- Concept: **Rademacher Complexity for Generalization Bounds**
  - Why needed here: The paper uses Rademacher complexity to bound the generalization gap. Understanding that it measures function class complexity via correlation with random noise explains why norm constraints lead to tighter bounds.
  - Quick check question: Would a larger function class (more parameters, higher capacity) typically have higher or lower Rademacher complexity?

## Architecture Onboarding

- Component map: Input Projection (Linear(U) → Conv1D → SiLU) → S6 Core (A, S_B, S_C, S_Δ producing time-variant B_t, C_t, Δ_t) → Discretization (Ā_t = exp(Δ_t · A), B̄_t = Δ_t · B_t) → Recurrence (h_t = Ā_t h_{t-1} + B̄_t x_t) → Output (y^k = C_t h_t → Linear projection)

- Critical path: 1) Input X enters with shape (L, D); 2) S_Δ, S_B, S_C project input to produce per-timestep Δ_t, B_t, C_t; 3) Discretization converts continuous A, B to discrete Ā_t, B̄_t; 4) Parallel scan computes recurrent state efficiently; 5) Output projection produces (L, D) result

- Design tradeoffs: State dimension N: Larger N increases capacity but linearly increases memory/compute; Simplified vs full S6: Polynomial approximations work nearly as well (0.7-2.4% degradation) while enabling theoretical analysis; Position encoding: S6 can work without PE (Table 2 shows 84.8% vs 83.1% accuracy on synthetic tasks)

- Failure signatures: If ||Ā|| approaches or exceeds 1, the model may fail to capture long-range dependencies (states don't decay); If K ≥ 1 in the generalization bound, the bound becomes vacuous; Copying tasks: SSMs struggle with exact copying compared to transformers

- First 3 experiments: 1) Polynomial regression sanity check: Generate synthetic data y = P(x) with random coefficients and known degree. Compare single-layer S6 vs single-layer attention on sequences of varying length L; 2) Length scaling validation: Train simplified S6 variant from Eq. 5 on WikiText-103 with varying context lengths (256, 512, 1024). Monitor if perplexity scales gracefully without explosion; 3) State transition norm monitoring: During training, log ||Ā_t||_max across positions. Verify it stays bounded below 1 (satisfying K < 1 assumption)

## Open Questions the Paper Calls Out

- Question: Does the proven expressivity gap persist when analyzing the full complexity of standard Softmax-based Transformers rather than simplified linear attention variants?
  - Basis: Appendix D states analyzing full complexity of architectures is an "important direction for future research" as the work relies on simplified models.
  - Why unresolved: The proofs rely on linear attention or polynomial approximations, omitting Softmax normalization and SiLU activations present in standard models.
  - What evidence would resolve it: Theoretical proofs or empirical scaling laws demonstrating the relationship between S6 and Softmax-based Transformers on the same polynomial tasks.

- Question: Can the measure of multivariate polynomial degree be formally connected to established expressivity metrics like Rademacher complexity or VC dimension?
  - Basis: Appendix D explicitly notes it "does not formally connect these measures to widely-used expressivity metrics in the literature."
  - Why unresolved: The paper uses polynomial degree as a proxy for expressivity but leaves the relationship to traditional generalization metrics undefined.
  - What evidence would resolve it: Deriving a mathematical mapping or inequality that relates the degree of representable polynomials to the Rademacher complexity of the layer.

- Question: How do optimization dynamics and implicit biases affect the realizability of these high-degree polynomial functions in large-scale trained models?
  - Basis: Appendix D highlights that "LLMs... involve additional factors" such as optimization challenges and implicit biases which are "beyond the scope of this study."
  - Why unresolved: The paper establishes what functions the layers can represent (capacity), but does not prove that gradient-based optimization will successfully learn these functions.
  - What evidence would resolve it: An analysis of the loss landscapes and gradient descent trajectories for S6 models when fitting the high-degree polynomials discussed in the theory.

## Limitations
- The theoretical analysis relies on polynomial approximations of softplus and exp functions, which may accumulate approximation error in practice
- The generalization bound assumes ||Ā_d^k||_max < K < 1 for all d,k, which may not hold for all trained models
- The paper uses maximum polynomial degree as the primary measure of expressivity, which is a proxy that may not capture all aspects of task-relevant expressivity

## Confidence
- **High Confidence**: The core theoretical result that S6 can represent higher-degree polynomials than linear attention is well-established given the recurrence structure
- **Medium Confidence**: The empirical validation showing simplified variants perform nearly as well as full S6 is convincing but limited to specific tasks
- **Low Confidence**: The generalization bound's practical tightness is uncertain, as it relies on assumptions that may not hold for all trained models

## Next Checks
1. **Taylor Approximation Sensitivity**: Systematically vary the Taylor approximation degree in simplified S6 variants (Eq.5) and measure the trade-off between theoretical tractability and empirical performance to validate whether simplifications preserve essential behavior

2. **State Norm Monitoring**: During training of full S6 models, log ||Ā_t||_max across positions and over training epochs to verify the K < 1 assumption, and investigate correlations with performance degradation

3. **Expressivity vs Task Alignment**: Design synthetic tasks requiring different polynomial structures (high-degree sparse polynomials, products of distant tokens) and measure whether S6's theoretical advantage translates to practical gains, validating if the polynomial expressivity metric captures task-relevant properties