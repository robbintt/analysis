---
ver: rpa2
title: 'UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction'
arxiv_id: '2509.06883'
source_url: https://arxiv.org/abs/2509.06883
tags:
- claim
- claims
- post
- meteor
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We explore various methods for extracting check-worthy claims from
  social media posts, focusing on fine-tuning and prompting approaches with large
  language models. Our primary finding is that fine-tuning a FLAN-T5 model achieves
  the best METEOR score of 0.5569 on the validation set.
---

# UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction

## Quick Facts
- arXiv ID: 2509.06883
- Source URL: https://arxiv.org/abs/2509.06883
- Reference count: 31
- We find that fine-tuning a FLAN-T5 model achieves the best METEOR score of 0.5569 on the validation set, but prompting methods can produce higher-quality claims despite lower scores.

## Executive Summary
This paper explores methods for extracting check-worthy claims from social media posts, comparing fine-tuning and prompting approaches with large language models. The authors find that fine-tuning FLAN-T5-Large achieves the highest METEOR score (0.5569), while prompting methods like keyword-based few-shot prompting combined with self-refinement can produce more practically useful claims despite lower metric scores. The study reveals that multiple self-refinement iterations lead to hallucinations and verbosity, and that many gold-standard claims omit critical verification details. The work highlights a fundamental disconnect between automated metric optimization and practical fact-checking utility.

## Method Summary
The paper employs two primary approaches: fine-tuning and prompting. For fine-tuning, they use FLAN-T5-Large (783M parameters) trained for 10 epochs on the CheckThat! 2025 Task 2 English dataset with a task-specific prompt prepended to each example. For prompting, they test zero-shot, few-shot, and keyword-based few-shot methods with various large language models including LLaMA 3.3 70B and DeepSeek-R1-Distill-Llama-8b. Self-refinement is used as an optional post-processing step, with findings showing that single iterations improve quality while multiple iterations introduce hallucinations. The primary evaluation metric is METEOR score using NLTK implementation, with BERTScore and ROUGE as secondary metrics.

## Key Results
- Fine-tuning FLAN-T5-Large achieves the best METEOR score of 0.5569 on the validation set
- Single self-refinement iteration improves claims, but multiple iterations cause hallucinations and verbosity
- Gold-standard claims often omit critical verification details (e.g., missing $174,000 salary figure in Biden example)
- KBFP prompting with LLaMA 3.3 70B achieves 0.29 METEOR, while fine-tuned FLAN-T5-Base with LoRA achieves 0.52
- Claimify method achieves BERTScore F1 of 0.82 despite lower METEOR scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning smaller models on task-specific data can outperform larger prompted models on metric optimization by internalizing dataset-specific output patterns.
- Mechanism: Fine-tuning exposes the model to repeated examples of the exact input-output mapping required, allowing it to learn implicit patterns in gold-standard annotations that are not easily expressible via prompting alone.
- Core assumption: Gold-standard claims represent the target output distribution; metric alignment correlates with task success.
- Evidence anchors:
  - [abstract] "Our best METEOR score is achieved by fine-tuning a FLAN-T5 model."
  - [Section 2.1] "This approach's strength lies in its ability to internalize extraction patterns not easily expressible via prompting alone. It achieved an average validation-set METEOR score of 0.5569."
  - [corpus] Claim Extraction paper (arxiv 2502.04955) similarly finds fine-tuned summarization models competitive with LLMs for claim extraction tasks.

### Mechanism 2
- Claim: Iterative self-refinement beyond a single iteration degrades output quality through hallucination accumulation and verbosity escalation.
- Mechanism: Self-refinement feeds the model's own output back as input with refinement instructions. Multiple iterations cause the model to elaborate on gaps with plausible but ungrounded content.
- Core assumption: Model's self-feedback mechanism accurately identifies deficiencies; refinement improves rather than drifts from source content.
- Evidence anchors:
  - [abstract] "We find that multiple iterations of self-refinement often lead to hallucinations and verbosity."
  - [Section 2.5] "We find that repeated applications of Self-Refine do not improve the results... multiple iterations tend to introduce verbosity and hallucinated facts not grounded in the original post."

### Mechanism 3
- Claim: METEOR score optimization diverges from practical fact-checking utility because the metric rewards surface-level similarity to potentially suboptimal gold references.
- Mechanism: METEOR computes n-gram overlap between predicted and gold claims. If gold claims omit critical verification details, predictions capturing those details score lower despite being more actionable.
- Core assumption: Gold-standard claims are high-quality references; n-gram overlap correlates with semantic correctness.
- Evidence anchors:
  - [abstract] "We observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower."
  - [Section 2.5] "It's worth noting that the gold-standard claims themselves can have shortcomings... This omits key details like the senator's actual salary ($174,000), which may be necessary for verification."

## Foundational Learning

- Concept: METEOR metric
  - Why needed here: The paper's primary ranking criterion; understanding what it measures (and doesn't) is essential for interpreting results and the metric-task mismatch finding.
  - Quick check question: Given two claims—"Biden owns a large house" vs. "Biden's $174K salary seems insufficient for his estate"—which would score higher against the gold claim "Joe Biden lives in a large estate bought on a senator's salary"?

- Concept: Few-shot prompting with keyword-based example selection
  - Why needed here: One of the higher-performing prompting approaches (Section 2.5); selecting relevant examples via keyword matching improves alignment between demonstrations and target input.
  - Quick check question: Why might keyword-matched few-shot examples outperform randomly selected examples for claim extraction from political posts?

- Concept: Hallucination in generative refinement
  - Why needed here: Critical failure mode identified in the paper—self-refinement iterations introduce ungrounded content, making this a key risk to monitor in any iterative generation pipeline.
  - Quick check question: What signal in the refinement output would indicate the model is adding information not present in the original social media post?

## Architecture Onboarding

- Component map: Input post → prompt/template selection → LLM inference → optional refinement → claim output. For fine-tuning: training data → gradient updates → fine-tuned model → inference.

- Critical path: The pipeline flows from raw social media post through prompt selection (zero-shot, few-shot, or keyword-based), LLM inference, optional single-step self-refinement, and final claim output.

- Design tradeoffs:
  - Fine-tuning (higher METEOR, requires compute/days of training) vs. prompting (lower METEOR, potentially higher practical utility, no training)
  - Single-refinement (concise claims) vs. multi-iteration (hallucination risk)
  - Smaller fine-tuned models (FLAN-T5-Large: 0.5569 METEOR) vs. larger prompted models (LLaMA 3.3 70B with KBFP: 0.29 METEOR)

- Failure signatures:
  - Hallucinated claims containing specific figures or assertions absent from source (e.g., "significantly exceeding the cumulative value" elaboration)
  - Verbose outputs from over-refinement
  - Missing key verification details due to gold-label alignment
  - Prefix artifacts like "The essential primary claim is..." from instruction-tuned models

- First 3 experiments:
  1. Reproduce the fine-tuning vs. KBFP prompting comparison on a held-out sample, manually annotating which outputs are more actionable for fact-checkers (not just METEOR).
  2. Ablate self-refinement iterations (0, 1, 2, 3) on 50 examples and quantify hallucination rate via human review.
  3. Test whether BERTScore or human evaluation correlates better with practical utility than METEOR, using the paper's observation that Claimify achieved BERTScore F1 of 0.82 despite lower METEOR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can evaluation metrics be developed that better capture the utility of extracted claims for human fact-checkers than n-gram overlap metrics like METEOR?
- Basis in paper: [explicit] The authors state that "differences significantly impact utility for human fact-checkers" but "are not well captured by the METEOR metric, which is limited to overlap with gold-standard claims."
- Why unresolved: The paper identifies the gap but does not propose or test alternative metrics; METEOR remains the competition's official metric.
- What evidence would resolve it: A human evaluation study correlating alternative metrics (e.g., fact-checker usefulness ratings) with automated scores, or a new metric validated against fact-checker judgments.

### Open Question 2
- Question: What criteria should define high-quality gold-standard claims, and how can dataset annotation be improved to include all details necessary for verification?
- Basis in paper: [explicit] "Our analysis reveals that many gold-standard claims are suboptimal, sometimes omitting critical details necessary for verification." The authors give the Biden house example where the salary figure ($174,000) is omitted.
- Why unresolved: The paper critiques existing gold claims but does not propose concrete annotation guidelines or quality criteria.
- What evidence would resolve it: Re-annotation of a subset with refined guidelines, showing improved inter-annotator agreement and higher correlation with fact-checker utility.

### Open Question 3
- Question: Why do multiple iterations of self-refinement lead to hallucination and verbosity in claim extraction, and what constraints or stopping criteria could prevent this?
- Basis in paper: [explicit] "We find that multiple iterations of self-refinement often lead to hallucinations and verbosity."
- Why unresolved: The paper observes the pattern but does not analyze the mechanism (e.g., feedback prompt design, model drift) or test mitigations.
- What evidence would resolve it: A controlled study varying iteration count and feedback criteria, measuring hallucination rates and claim quality at each step.

### Open Question 4
- Question: Can claim extraction performance be improved by incorporating methods that infer or retrieve missing multi-modal content (images/videos) referenced in social media posts?
- Basis in paper: [explicit] "Many gold claims refer to information not available to participants, such as photos or videos included with the original post but removed during pre-processing."
- Why unresolved: The authors "accept that claims cannot be based on unseen data" but do not explore retrieval or inference approaches.
- What evidence would resolve it: Experiments using image retrieval or caption generation on held-out multimodal data, comparing METEOR and human utility to text-only baselines.

## Limitations
- Limited hyperparameter tuning was performed, using standard T5 settings without systematic exploration of learning rates or regularization
- METEOR metric evaluation reveals disconnect between automated scoring and practical utility, but lacks human evaluation data
- Dataset access limitations prevent independent verification as CheckThat! 2025 Task 2 English data appears competition-restricted
- Gold-standard claim quality analysis is qualitative rather than systematic
- Computational resources required for fine-tuning (4 days on NVIDIA 4060) create reproducibility barriers

## Confidence

- High confidence: Fine-tuning FLAN-T5-Large achieves the best METEOR score (0.5569); multiple self-refinement iterations degrade quality through hallucinations and verbosity; gold-standard claims often omit critical verification details.
- Medium confidence: Fine-tuned models outperform prompted models on METEOR due to internalization of dataset-specific patterns; METEOR optimization diverges from practical fact-checking utility; keyword-based few-shot prompting provides meaningful improvement over random selection.
- Low confidence: The practical superiority of prompting methods for human fact-checkers despite lower METEOR scores; the optimal number of self-refinement iterations beyond which degradation occurs; whether alternative metrics like BERTScore better capture claim quality.

## Next Checks

1. **Human Evaluation Validation**: Conduct blind comparison between fine-tuned FLAN-T5 outputs (highest METEOR) and top-performing prompting outputs (e.g., KBFP with LLaMA 3.3 70B) using 50 random validation examples rated by professional fact-checkers for actionability and completeness, measuring inter-annotator agreement and correlation with METEOR scores.

2. **Gold Standard Quality Audit**: Systematically analyze 100 gold-standard claims to identify missing verification-critical details (numerical values, dates, specific entities) using automated information extraction to quantify the prevalence and impact of these omissions on downstream fact-checking workflows.

3. **Metric Correlation Study**: Compare METEOR, BERTScore F1 (0.82 achieved by Claimify), and ROUGE against human-rated claim quality scores across the same 50 examples to determine which automated metric best predicts practical utility for fact-checkers, including statistical significance testing of correlations.