---
ver: rpa2
title: 'NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation'
arxiv_id: '2506.08360'
source_url: https://arxiv.org/abs/2506.08360
tags:
- nysact
- kfac
- foof
- methods
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NysAct, a scalable first-order gradient preconditioning
  method that balances the efficiency of first-order methods with the strong convergence
  and generalization of second-order methods. NysAct approximates the activation covariance
  matrix using an eigenvalue-shifted Nystrom method, significantly reducing computational
  and memory costs while maintaining minimal impact on test accuracy.
---

# NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation

## Quick Facts
- arXiv ID: 2506.08360
- Source URL: https://arxiv.org/abs/2506.08360
- Authors: Hyunseok Seung; Jaewoo Lee; Hyunsuk Ko
- Reference count: 40
- Primary result: Scalable first-order gradient preconditioning method achieving 75.62% top-1 accuracy on ImageNet ResNet-50 with O(1/T) convergence

## Executive Summary
NysAct introduces a scalable first-order gradient preconditioning method that approximates the Fisher Information Matrix using an eigenvalue-shifted Nyström approximation of the activation covariance matrix. By leveraging activation-only preconditioning (dropping the pre-activation gradient term) and low-rank Nyström sketching, NysAct achieves convergence rates comparable to second-order methods while maintaining first-order computational complexity. The method demonstrates superior performance on CIFAR and ImageNet benchmarks compared to both first-order methods (SGD, Adam) and second-order methods (KFAC, Shampoo), requiring significantly less computational resources than existing second-order methods.

## Method Summary
NysAct approximates the Fisher Information Matrix using only the activation covariance matrix A, leveraging prior empirical findings that the pre-activation gradient covariance P contributes minimally to KFAC's performance. The method employs an eigenvalue-shifted Nyström approximation to reduce the O(d³) matrix inversion cost to O(r³ + d·r²) where r ≪ d is the rank parameter. A stable inversion pipeline using eigenvalue shifting and Cholesky decomposition ensures numerical stability during training. The optimizer maintains exponential moving averages of sketched activations and preconditioner inverses, updating them at configurable frequencies to balance adaptation speed with computational cost.

## Key Results
- Achieves 75.62% top-1 accuracy on ImageNet ResNet-50, outperforming SGD (75.35%), KFAC (74.35%), and Adam (73.74%)
- Demonstrates 1.6% accuracy improvement over KFAC on CIFAR-10 with ResNet-32 while using significantly less computation
- Maintains O(1/T) convergence rate for non-convex optimization problems
- Requires only 1.00× memory relative to SGD vs. KFAC's 1.02× on ImageNet experiments
- Shows faster training times compared to KFAC and EVA while achieving comparable or better accuracy

## Why This Works (Mechanism)

### Mechanism 1: Activation-Only Preconditioning
NysAct drops the pre-activation gradient covariance P from KFAC's Fisher approximation, using only the activation covariance A as preconditioner. This reduces per-layer inversion complexity from O(d³_out + d³_in) to O(d³_in) while preserving most of KFAC's convergence benefits. The core assumption is that activation covariance sufficiently captures the curvature information needed for effective preconditioning.

### Mechanism 2: Nyström Low-Rank Approximation with Sketching
The method approximates the full activation covariance matrix using rank-r Nyström approximation with either subcolumn sampling or Gaussian sketching. This reduces storage from O(d²) to O(d·r) and inversion cost from O(d³) to O(r³ + d·r²). The approach exploits the spectral decay properties of activation covariance matrices, enabling accurate low-rank representation.

### Mechanism 3: Eigenvalue-Shifted Nyström for Numerical Stability
Standard Nyström inversion suffers from numerical instability when eigenvalues are near-zero. NysAct applies eigenvalue shifting combined with Cholesky decomposition to ensure positive definiteness before inversion. This stability mechanism enables training on architectures where KFAC fails due to numerical issues, particularly in attention-based models.

## Foundational Learning

- **Fisher Information Matrix (FIM) and Kronecker-Factored Approximate Curvature (KFAC)**: Understanding how KFAC approximates FIM via Kronecker factors A ⊗ P is essential to see what NysAct preserves and discards. Quick check: Can you explain why KFAC decomposes F into A ⊗ P and what computational advantage this provides over full FIM inversion?

- **Nyström Approximation for Low-Rank Matrix Approximation**: The core efficiency gain comes from applying Nyström approximation to activation covariance. Quick check: Given a matrix A and sampling matrix S selecting r columns, write the Nyström approximation A_nys and explain why it requires only O(nr) storage.

- **Preconditioned Gradient Descent and Convergence Analysis**: Understanding how preconditioning affects condition number and convergence rate helps explain why activation-only preconditioning is effective. Quick check: How does the condition number of the preconditioned Hessian affect convergence rate, and what role does the eigenvalue bound (λ_L, λ_U) play in Theorem 5.5?

## Architecture Onboarding

- **Component map**: Forward pass computes activations a^(l) → Sketch activations: A·S stored in ÊC (EMA buffer) → Every τ_inv steps: compute stable inverse (C_nys)^(-1) via Cholesky → SVD pipeline → Backward pass computes gradients G → Update: θ ← θ + momentum · G · (C_nys)^(-1)

- **Critical path**: Forward pass → Activation sketching → EMA update → Inverse computation (periodic) → Backward pass → Preconditioned gradient application

- **Design tradeoffs**: Rank r balances accuracy vs. cost (higher r improves accuracy but increases O(r³) inversion cost); Sampling method choice (subcolumn more stable for large-scale, Gaussian simpler but shows more variance); Update frequency τ_inv balances adaptation speed vs. compute cost; Damping ρ controls regularization level

- **Failure signatures**: NaN gradients during early training (damping ρ too low); No convergence improvement over SGD (rank r too low or update frequency too high); Memory OOM on large layers (rank r too high); Training divergence on attention architectures (numerical instability in inverse computation)

- **First 3 experiments**: 1) Baseline CIFAR-10 ResNet-32: Replicate 93%+ test accuracy within 100 epochs vs. SGD, Adam, KFAC; 2) CIFAR-100 rank sensitivity: ResNet-110 with r∈{5,10,15,20} to identify knee point of accuracy vs. wall-clock time; 3) ImageNet ResNet-50: 100 epochs with r=50, τ_inv=5 to verify 75.62% top-1 accuracy with competitive training time

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises implicit ones about the universal effectiveness of activation-only preconditioning across architecture types and the optimal strategy for controlling approximation error ε to achieve faster convergence rates.

## Limitations
- Activation-only preconditioning may not be universally effective across all architecture types, particularly for transformers and RNNs where gradient-pre-activation coupling might be more important
- The method's scalability claims are limited to ResNet-50 and DeiT-S architectures, with limited validation on extremely large-scale models
- Eigenvalue-shifted Nyström stability relies on proper damping parameter selection, but the paper provides limited guidance on choosing ρ for different activation scales and layer types

## Confidence
- **High Confidence**: O(1/T) convergence proof, CIFAR-10/100 experimental results, memory complexity analysis
- **Medium Confidence**: ImageNet results (single run, no seed specification), scalability claims (limited to ResNet-50 and DeiT-S)
- **Low Confidence**: Universal applicability across architectures, hyperparameter robustness beyond tested ranges

## Next Checks
1. **Architecture Generalization Test**: Run NysAct on ViT-B/16 and BERT-base using CIFAR-100 and GLUE benchmarks to verify activation-only preconditioning works beyond CNNs
2. **Rank Sensitivity at Scale**: Train ResNet-101 on ImageNet with varying ranks (r=10, 25, 50, 100) and measure the Pareto frontier of accuracy vs. wall-clock time to identify optimal rank scaling
3. **Cross-Dataset Robustness**: Apply NysAct to non-standard datasets (e.g., Food-101, Pets) and compare against KFAC/Adam to test if the method's advantages generalize beyond standard vision benchmarks