---
ver: rpa2
title: Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions
arxiv_id: '2510.20102'
source_url: https://arxiv.org/abs/2510.20102
tags:
- detection
- anomaly
- hcla
- agent
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HCLA framework introduces a multi-agent LLM-based system for
  detecting anomalous digital asset transactions, enabling non-expert users to interactively
  query and interpret results. It combines a Parsing Agent (ChatGPT) to convert natural
  language queries into structured JSON, a Detection Agent (XGBoost) for anomaly scoring,
  and an Explanation Agent (Gemini) for narrative rationales.
---

# Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions

## Quick Facts
- arXiv ID: 2510.20102
- Source URL: https://arxiv.org/abs/2510.20102
- Reference count: 3
- Primary result: XGBoost baseline achieves Accuracy 0.9159, Precision 0.9317, Recall 0.9159, F1 0.9209 on Bitcoin mixing dataset

## Executive Summary
The HCLA framework introduces a multi-agent LLM-based system for detecting anomalous digital asset transactions, enabling non-expert users to interactively query and interpret results. It combines a Parsing Agent (ChatGPT) to convert natural language queries into structured JSON, a Detection Agent (XGBoost) for anomaly scoring, and an Explanation Agent (Gemini) for narrative rationales. Tested on a Bitcoin mixing dataset (Wasabi Wallet, 2020–2024), the baseline XGBoost model achieved Accuracy 0.9159, Precision 0.9317, Recall 0.9159, and F1 0.9209. The human-in-the-loop approach improved accessibility, interpretability, and user trust compared to static dashboards, with LLM explanations rated significantly higher in clarity and trust in a simulated expert panel (n=32, p < .001).

## Method Summary
The system employs a three-agent architecture via Gradio: Parsing Agent (ChatGPT) converts natural language to JSON, Detection Agent (XGBoost) computes anomaly probabilities using temporal/transactional/graph features, and Explanation Agent (Gemini) generates narrative rationales. The framework uses JSON-based communication between modules and was trained on a Wasabi Wallet dataset (2020–2022) with testing on 2023–2024 data. The approach enables non-expert users to interact with complex anomaly detection through conversational interfaces while maintaining technical accuracy through the XGBoost backend.

## Key Results
- XGBoost baseline achieved Accuracy 0.9159, Precision 0.9317, Recall 0.9159, F1 0.9209 on test data
- Human-in-the-loop approach improved accessibility and interpretability compared to static dashboards
- LLM explanations rated significantly higher in clarity and trust (n=32, p < .001) in simulated expert panel

## Why This Works (Mechanism)

### Mechanism 1: Semantic-to-Schema Translation
Non-expert users can trigger complex anomaly detection by inputting natural language, assuming the Parsing Agent accurately maps linguistic intent to the detector's required JSON schema. A Parsing Agent (ChatGPT) receives unstructured text, extracts entities (dates, addresses, values) via prompt templates, and emits a structured JSON object that the XGBoost agent consumes. Core assumption: The LLM can deterministically resolve linguistic ambiguity into valid JSON fields that match the underlying feature store (e.g., mapping "past week" to a 7-day window integer). Break condition: If the user uses domain slang not defined in the prompt, the JSON schema may form incorrectly, causing the detection agent to fail or misinterpret the input.

### Mechanism 2: Post-hoc Explanation Decoupling
Interpretability is improved by decoupling the numerical detection logic from the narrative generation logic, assuming the explanation agent is strictly grounded in the features provided. The system separates the Detection Agent (XGBoost), which calculates probability scores, from the Explanation Agent (Gemini), which consumes the score and top feature weights to generate a narrative rationale. Core assumption: The explanation LLM will adhere to the prompt constraints and refrain from hallucinating factors (e.g., geopolitical events) that were not present in the XGBoost feature set. Break condition: If the explanation agent produces plausible but ungrounded reasoning (hallucination), user trust may increase while actual audit accuracy decreases.

### Mechanism 3: Stateful Interactive Refinement
Users can refine anomaly detection iteratively through conversation, assuming the controller maintains intra-session state to update queries without restarting the pipeline. A central controller buffers the previous JSON schema and detection results. When a user issues a follow-up ("Only show high-value"), the system updates the JSON filter rather than re-running the full parsing phase from scratch. Core assumption: The system can semantically merge new constraints with existing ones (e.g., adding a filter to an existing time range) without contradiction. Break condition: If context window limits are reached or the prompt logic fails to merge constraints, the system may lose the original query parameters, resetting the analysis unexpectedly.

## Foundational Learning

### Concept: Gradient Boosting (XGBoost)
Why needed here: This serves as the "Detection Agent." Engineers must understand that XGBoost handles the actual binary classification (anomaly vs. normal) based on tabular features, while LLMs only handle the interface. Quick check question: Can XGBoost process raw natural language text directly, or does it require pre-extracted numerical features?

### Concept: JSON Schema Validation
Why needed here: The architecture relies on JSON as the "API" between agents. Understanding how to define and validate schemas is critical to prevent the Detection Agent from crashing on malformed LLM output. Quick check question: If the Parsing Agent outputs a date in the format "Sept 20th" but the JSON schema requires "YYYY-MM-DD", where does the failure occur?

### Concept: Bitcoin Mixing / Wasabi Wallet
Why needed here: The dataset consists of CoinJoin transactions. Understanding that "mixing" obscures transaction trails is necessary to interpret why specific features (e.g., graph connectivity, timing) are predictors of anomaly. Quick check question: Why would a transaction with multiple inputs and outputs of equal value (CoinJoin) be flagged as an anomaly in this specific dataset context?

## Architecture Onboarding

### Component map:
Frontend (Gradio Web UI) -> Controller (Central orchestration) -> Parsing Agent (ChatGPT) -> Detection Agent (XGBoost) -> Explanation Agent (Gemini)

### Critical path:
1. User inputs text
2. Parsing Agent extracts JSON (Crucial step: JSON validation)
3. XGBoost predicts on extracted features
4. Explanation Agent generates rationale

### Design tradeoffs:
- Latency vs. Quality: LLM calls introduce "2-3s delay" compared to instant static dashboards, though the Abstract mentions "< 2 m" (likely minutes or a typo)
- Modularity vs. Complexity: Using separate agents allows swapping XGBoost for Graph Neural Network but requires maintaining consistent JSON schemas across three different models

### Failure signatures:
- Inconsistent Terminology: LLMs may confuse "cluster" vs "wallet" references
- Malformed JSON: Parsing Agent returns conversational text instead of JSON
- Context Drift: User tries to compare two different wallets, and the controller mixes up the context

### First 3 experiments:
1. **JSON Robustness Test**: Feed the Parsing Agent 50 distinct variations of the same query (slang, typos, different date formats) to measure schema extraction accuracy
2. **Explanation Faithfulness**: Run the detector on a known "normal" transaction and verify if the Explanation Agent can successfully articulate why it is normal (low risk) rather than forcing a "suspicious" narrative
3. **Latency Profiling**: Measure the end-to-end latency of the "Refinement" loop to resolve the discrepancy between the reported "2-3s" delay and "< 2 m" query time

## Open Questions the Paper Calls Out

### Open Question 1
Does domain-specific fine-tuning of the Explanation Agent significantly reduce semantic drift compared to generic LLMs? Basis in paper: The authors state that generic LLMs occasionally confuse blockchain terminology (e.g., "cluster" vs. "wallet") and propose fine-tuning as a necessary future step. Why unresolved: The current prototype relies solely on off-the-shelf models (ChatGPT, Gemini) without specialized financial corpus training. What evidence would resolve it: A comparative analysis measuring terminology accuracy and hallucination rates between base and fine-tuned models.

### Open Question 2
Can the framework maintain low latency when adapted for real-time, high-frequency blockchain streams? Basis in paper: The paper identifies computational cost and latency as a limitation, noting the current 2–3 second delay hinders real-time monitoring capabilities. Why unresolved: The evaluation relies on batch-processed data; the architecture currently lacks the asynchronous orchestration needed for continuous streams. What evidence would resolve it: Performance benchmarks demonstrating sub-second response times under high-throughput transaction loads.

### Open Question 3
Do the observed improvements in interpretability and trust generalize to non-expert, non-academic populations? Basis in paper: The authors acknowledge that the study relies on a "Micro-Expert Panel" (n=32), which limits external validity regarding general user populations. Why unresolved: Participants held Master's degrees in AI-related fields, potentially biasing the high ratings for comprehension and trust. What evidence would resolve it: Results from a large-scale, IRB-approved user study involving diverse, non-technical participants.

## Limitations

- Human-centered benefits (trust/clarity improvements) based on small simulated expert panel (n=32) rather than large-scale user study with real non-experts
- Dataset provenance unclear—exact labeling criteria for Wasabi Wallet anomalous transactions not specified
- LLM latency claims appear inconsistent (2-3s vs "<2 m" per query), suggesting potential measurement or reporting issues
- Lacks systematic evaluation of parsing robustness against linguistic variation, adversarial queries, or context drift scenarios

## Confidence

- **High**: Technical architecture (3-agent system design), baseline XGBoost performance metrics, and feature engineering pipeline
- **Medium**: LLM parsing accuracy and explanation faithfulness—described but no quantitative error analysis or hallucination detection
- **Low**: Claims about user trust improvements and accessibility gains—based on limited panel study without comparison to alternative interfaces or broader demographic sampling

## Next Checks

1. **Parsing Robustness Benchmark**: Systematically test the Parsing Agent with 100+ natural language variations (including typos, slang, and ambiguous phrasing) and measure JSON schema extraction accuracy against ground-truth structured queries
2. **Hallucination Audit**: Create a controlled test set of known normal transactions and verify whether the Explanation Agent generates truthful "low-risk" rationales versus artificially inflated suspicious narratives
3. **Latency Reconciliation**: Conduct end-to-end timing measurements across all three LLM calls (parsing, detection, explanation) to resolve the discrepancy between reported 2-3s and "<2 m" query times, including batch vs. single-query scenarios