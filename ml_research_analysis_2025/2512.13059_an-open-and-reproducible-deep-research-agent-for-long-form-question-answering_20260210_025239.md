---
ver: rpa2
title: An Open and Reproducible Deep Research Agent for Long-Form Question Answering
arxiv_id: '2512.13059'
source_url: https://arxiv.org/abs/2512.13059
tags:
- search
- answer
- research
- question
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an open and reproducible deep research system
  for long-form question answering, which won the text-to-text track of the MMU-RAG
  competition at NeurIPS 2025. The system integrates an open-source large language
  model with an open web search API to perform iterative retrieval, reasoning, and
  synthesis in real-world open-domain settings.
---

# An Open and Reproducible Deep Research Agent for Long-Form Question Answering

## Quick Facts
- arXiv ID: 2512.13059
- Source URL: https://arxiv.org/abs/2512.13059
- Reference count: 29
- Won text-to-text track of MMU-RAG competition at NeurIPS 2025 with clarity 8.18 and insightfulness 7.50

## Executive Summary
This work presents an open and reproducible deep research system for long-form question answering that won the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system integrates an open-source large language model with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, the authors apply preference tuning using LLM-as-a-judge feedback that evaluates clarity, insightfulness, and factuality. The experimental results show that the proposed method consistently improves answer quality across all three metrics, with the tuned model achieving clarity of 8.18 and insightfulness of 7.50 compared to 6.71 and 6.52 for the baseline.

## Method Summary
The deep research agent employs a Qwen3-Next-80B-A3B-Thinking model fine-tuned with DPO + LoRA (α=16, rank=16, β=0.5) to perform iterative retrieval-reasoning cycles. The system uses a two-stage retrieval pipeline: initial retrieval via DeepResearchGym API followed by Qwen3-Reranker-0.6B to select top-10 documents. A summarizer (Qwen3-Next-80B-A3B-Thinking) extracts query-relevant information with citation markers. The research agent decides when to issue follow-up queries (max 5 searches) based on reasoning about information sufficiency. Preference tuning uses OpenAI's o3-mini to score 20 answer samples per question on clarity, insightfulness, and factuality, constructing preference pairs with score gaps θ∈{0.3,0.5,0.7} for DPO training.

## Key Results
- Tuned model achieves clarity of 8.18 and insightfulness of 7.50 compared to baseline 6.71 and 6.52
- Factuality improves marginally from 43.4 to 43.8 (KPR-based metric)
- Iterative search behavior increases from 1.03 to 1.20 average searches per question
- Citation error rate increases slightly from 0.06 to 0.09 after tuning

## Why This Works (Mechanism)

### Mechanism 1
- Iterative retrieval-reasoning loops improve answer quality by allowing the model to identify and fill information gaps dynamically.
- The research agent begins with an initial search, then reasons about whether additional searches are needed. When encountering unfamiliar terms or insufficient information, it issues follow-up queries.
- Core assumption: The model can accurately identify when information is insufficient and formulate useful follow-up queries.
- Evidence anchors:
  - [Section 2.2]: "The research agent manages the overall reasoning process... determines whether additional searches are needed through reasoning, and issues a new query when necessary."
  - [Section 3.1]: The tuned model performs more searches on average (1.20 vs 1.03), suggesting learned search behavior.
  - [Corpus]: RAG-BioQA (arXiv:2510.01612) demonstrates similar retrieval-augmented approaches for long-form biomedical QA.

### Mechanism 2
- LLM-as-a-judge preference tuning shapes reasoning behavior toward clearer, more insightful outputs.
- The authors generate 20 answer samples per question using an untuned model, then use OpenAI's o3-mini to score clarity, insightfulness, and factuality. Preference pairs are constructed from highest vs. lowest scored responses.
- Core assumption: The judge LLM's preferences align with human judgments of answer quality.
- Evidence anchors:
  - [Abstract]: "To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects."
  - [Section 4.2]: Clarity improved +1.47 and insightfulness +0.98 after tuning; factuality improved only marginally (+0.41).
  - [Corpus]: Limited direct corpus evidence for LLM-as-a-judge DPO specifically in deep research agents.

### Mechanism 3
- Reranking and summarization modules reduce context noise and improve citation accuracy.
- Retrieved documents are reranked by Qwen3-Reranker-0.6B to select the most relevant N documents. A summarizer (Qwen3-Next-80B) then extracts query-relevant information with embedded citation markers.
- Core assumption: The summarizer faithfully represents source content without introducing hallucinations.
- Evidence anchors:
  - [Section 2.1]: "The summarizer extracts information relevant to the given query and previous reasoning steps, generating a concise summary... instruct the model to include citation markers."
  - [Section 4.2]: Citation error rate increased slightly (+0.03) after tuning, suggesting a tradeoff between reasoning quality and citation formatting.
  - [Corpus]: FinLFQA (arXiv:2510.06426) examines attribution in long-form QA, noting hallucination remains a challenge even with retrieval augmentation.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: The paper uses DPO with LoRA to fine-tune the research agent on preference pairs without training a separate reward model.
  - Quick check question: Can you explain how DPO differs from PPO-based RLHF in terms of what is optimized?

- Concept: **Agentic Tool Use / Function Calling**
  - Why needed here: The research agent must decide when to call the search tool and what query to issue. This requires understanding tool-augmented generation patterns.
  - Quick check question: How would you design a prompt to teach an LLM when to call a search API versus answering directly?

- Concept: **Reranking in Retrieval Pipelines**
  - Why needed here: The system uses a two-stage retrieval (initial retrieval → reranking) to improve relevance before summarization.
  - Quick check question: Why might a cross-encoder reranker outperform the initial retrieval model's relevance scoring?

## Architecture Onboarding

- Component map:
Question -> [Search Tool: API -> Reranker -> Summarizer] -> [Research Agent: Qwen3-Next-80B-DPO] -> Answer
                 ↑_______________________________________|
                     (Iterative feedback loop, max 5 searches)

- Critical path: The research agent's ability to recognize information gaps and issue targeted follow-up queries determines answer completeness. If this loop fails, the answer will be shallow regardless of retrieval quality.

- Design tradeoffs:
  - More search iterations → better coverage but higher latency and cost
  - Higher score-gap threshold (θ) → cleaner preference pairs but fewer training samples
  - Larger summarizer model → better extraction but higher inference cost
  - The paper notes factuality improved less than clarity/insightfulness, possibly due to noisy training signals

- Failure signatures:
  - Low search count (<1.0) with shallow answers → agent not learning to iterate
  - High citation error rate → summarizer or agent misaligning citations
  - Factuality lagging behind clarity/insightfulness → judge signals too noisy for this metric

- First 3 experiments:
  1. **Baseline replication**: Run the vanilla (untuned) model on 20 questions from Researchy Questions. Measure clarity, insightfulness, factuality, and search count. Compare to paper's Table 1 baseline (Clarity: 6.71, Insightfulness: 6.52, Factuality: 43.4).
  2. **Ablate preference threshold**: Train three models with θ ∈ {0.3, 0.5, 0.7} and compare. The paper shows all outperform baseline with marginal differences, but θ=0.3 yields the most training data.
  3. **Ablate iterative search**: Force single-search inference (disable follow-up queries) and measure quality drop. This isolates the contribution of the iterative reasoning loop versus the preference tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the training pipeline be modified to achieve significant improvements in factuality comparable to those observed in clarity and insightfulness?
- Basis in paper: [explicit] The authors state that factuality training is "more difficult" because it "relies on noisy user-click–based training signals" and is available only for a subset of the data, resulting in only marginal gains (+0.41) compared to clarity (+1.47).
- Why unresolved: The current work identifies the noise in training signals as a bottleneck but does not propose or test alternative data generation methods to overcome this limitation.
- What evidence would resolve it: Experimental results showing that incorporating high-quality, verified factuality signals (e.g., human-annotated or consistency-checked data) into the DPO phase significantly closes the performance gap between factuality and other metrics.

### Open Question 2
- Question: Does optimizing for semantic quality metrics (clarity, insightfulness) via preference tuning inherently trade off with citation formatting accuracy?
- Basis in paper: [inferred] While Table 1 shows substantial improvements in answer quality, it also shows the tuned model suffers a slight increase in citation_error_rate (0.06 to 0.09).
- Why unresolved: The paper reports the regression in citation accuracy but does not investigate if the DPO optimization shifts model capacity away from structural constraints toward semantic fluency.
- What evidence would resolve it: An ablation study comparing citation error rates when the preference model is explicitly penalized for citation errors versus the current setup.

### Open Question 3
- Question: To what extent do the improvements measured by LLM-as-a-judge (o3-mini) generalize to human evaluations?
- Basis in paper: [inferred] The system relies entirely on OpenAI's o3-mini for constructing preference pairs and evaluation, without validating if o3-mini's definition of "clarity" or "insightfulness" aligns with human preferences.
- Why unresolved: LLM judges often exhibit specific biases; high scores from o3-mini may indicate alignment with the judge's quirks rather than genuine improvement in research quality.
- What evidence would resolve it: A human evaluation study comparing the "Vanilla" and "Ours" models to validate the correlation between o3-mini scores and human judgment.

## Limitations

- Retrieval quality dependency: The system's performance hinges on the DeepResearchGym API's retrieval quality and the RAG agent's ability to extract relevant passages. If the underlying search corpus (ClueWeb22-A) is noisy or the retrieval model underperforms, the iterative loop cannot compensate.
- LLM-as-judge alignment: The preference tuning relies on o3-mini's judgments aligning with human preferences. While the paper reports consistent gains in clarity and insightfulness, factuality improvement is marginal (+0.41), suggesting the judge's factuality signals may be noisier or harder to optimize.
- Reproducibility constraints: Key components (DeepResearchGym API, MMU-RAG validation set) are not publicly documented, creating barriers to faithful reproduction.

## Confidence

- **High confidence**: The iterative retrieval-reasoning mechanism (Mechanism 1) is well-supported by quantitative evidence (1.20 vs 1.03 average searches) and aligns with established RAG paradigms. The architecture and training pipeline are clearly specified.
- **Medium confidence**: The LLM-as-a-judge preference tuning (Mechanism 2) improves clarity and insightfulness reliably, but the marginal factuality gain raises questions about judge alignment. The paper's ablation of preference thresholds (θ∈{0.3,0.5,0.7}) shows robustness but does not explore judge reliability.
- **Low confidence**: The citation accuracy claims (Section 4.2) are undermined by the slight increase in citation error rate (+0.03) after tuning. The paper does not diagnose whether this stems from the summarizer, agent, or judge preferences.

## Next Checks

1. **Judge alignment validation**: Run a small-scale human evaluation (n=50) comparing o3-mini's clarity/insightfulness/factuality scores against human ratings on 100 Researchy Questions. Compute inter-annotator agreement and judge-human correlation to quantify alignment reliability.

2. **Factuality signal denoising**: Filter preference pairs where o3-mini's factuality scores conflict (e.g., Δfactuality < 0.2) and retrain the DPO model. Compare factuality gains to the original paper's +0.41 improvement.

3. **Search loop ablation**: Force single-search inference on 200 Researchy Questions and measure quality drop. Quantify the marginal value of iterative retrieval versus preference tuning by comparing absolute gains in clarity/insightfulness/factuality.