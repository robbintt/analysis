---
ver: rpa2
title: Evaluating Large Language Models as Expert Annotators
arxiv_id: '2508.07827'
source_url: https://arxiv.org/abs/2508.07827
tags:
- annotation
- reasoning
- entity
- llms
- party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates whether top-performing large language models
  can serve as expert annotators for specialized domains like finance, law, and biomedicine.
  Experiments compare individual LLMs with inference-time techniques (chain-of-thought,
  self-refine, self-consistency) against human expert annotations, and assess reasoning
  models versus non-reasoning models.
---

# Evaluating Large Language Models as Expert Annotators

## Quick Facts
- arXiv ID: 2508.07827
- Source URL: https://arxiv.org/abs/2508.07827
- Authors: Yu-Min Tseng; Wei-Lin Chen; Chung-Chi Chen; Hsin-Hsi Chen
- Reference count: 15
- Primary result: LLMs fall short of human experts for high-stakes, domain-specific annotation tasks

## Executive Summary
This paper evaluates whether top-performing large language models can serve as expert annotators for specialized domains like finance, law, and biomedicine. Experiments compare individual LLMs with inference-time techniques against human expert annotations, and assess reasoning models versus non-reasoning models. Results show that inference-time techniques provide only marginal or negative performance gains, and reasoning models do not demonstrate statistically significant improvements. A multi-agent discussion framework is introduced, where LLMs iteratively refine annotations through peer discussion, leading to improved accuracy and agreement. However, certain models (e.g., Claude 3.7 Sonnet) rarely change initial annotations, limiting consensus gains. Overall, the study highlights that current LLMs still fall short of human experts for high-stakes, domain-specific annotation tasks.

## Method Summary
The study uses 5 datasets (REFinD, FOMC, CUAD, FoDS, CODA-19) with 200 instances each, covering finance, law, and biomedicine domains. Six models are tested: Gemini-1.5-Pro, Gemini-2.0-Flash, Claude-3-Opus, GPT-4o, Claude-3.7-Sonnet with thinking, and o3-mini medium. Inference-time techniques include vanilla prompting, chain-of-thought ("Let's think step by step"), self-consistency (5 samples, temp=0.7, majority vote), and self-refine (3-step). A multi-agent discussion framework uses 3 LLMs with max 2 rounds of iterative refinement through peer discussion. Performance is measured against human expert annotations using accuracy, Fleiss' Kappa for agreement, and McNemar's test for statistical significance (p < 0.05).

## Key Results
- Inference-time techniques (CoT, self-consistency, self-refine) show only marginal or negative performance gains in expert domains
- Reasoning models do not demonstrate statistically significant improvements over non-reasoning models
- Multi-agent discussion framework improves accuracy and agreement through iterative peer reasoning
- Claude 3.7 Sonnet with thinking rarely changes initial annotations, limiting consensus gains

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Discussion Improves Consensus via Peer Reasoning Exposure
- Exposing LLMs to peer annotations and justifications increases both accuracy and inter-annotator agreement compared to single-model annotation
- When agents receive a compiled "Discussion History" containing other agents' labels and reasoning chains, they can identify gaps in their own reasoning and correct errors
- The iterative check-discuss-re-annotate cycle enables convergence toward correct labels that no single agent initially produced
- Core assumption: Agents can recognize valid counter-arguments and update incorrect positions when presented with better reasoning
- Evidence anchors: [abstract] LLMs engage in discussions by considering others' annotations and justifications; [section 4.2] accuracy and Fleiss' Kappa steadily increase with each round
- Break condition: If all agents share the same systematic misconception or if no agent initially produces the correct annotation, discussion cannot recover the correct label

### Mechanism 2: Inference-Time Techniques Fail Due to Domain Context Misinterpretation
- Chain-of-thought, self-refine, and self-consistency provide marginal or negative gains for expert annotation, contrary to general-domain NLP tasks
- Inference-time techniques elicit reasoning chains that assume the model correctly interprets the task; for specialized domains, models misinterpret annotation guidelines or instance-specific context
- Extended reasoning amplifies rather than corrects these misinterpretations
- Core assumption: Models lack sufficient parametric domain knowledge to ground their reasoning in correct interpretations of specialized annotation guidelines
- Evidence anchors: [abstract] inference-time techniques show only marginal or even negative performance gains; [section 3.2] models might not accurately interpret specialized annotation guidelines
- Break condition: If the model fundamentally lacks domain knowledge, more compute cannot compensate

### Mechanism 3: Reasoning Model Overconfidence Limits Collaborative Gains
- Reasoning models (e.g., Claude 3.7 Sonnet with thinking) rarely change initial annotations during multi-agent discussion, even when peers provide correct labels
- Extended reasoning produces internally coherent justifications that increase model confidence and persuasiveness, making the model resistant to peer correction
- This "strong self-consistency" prevents collaborative refinement
- Core assumption: Reasoning models encode higher certainty in their outputs due to the apparent logical coherence of extended chains, even when wrong
- Evidence anchors: [abstract] Claude 3.7 Sonnet with thinking rarely changes its initial annotations; [section 4.4] hypothesized explanations include overconfidence and greater persuasiveness
- Break condition: If the reasoning model's initial annotation is correct, this self-consistency is beneficial; if incorrect, it blocks consensus and may mislead other agents

## Foundational Learning

- **Concept**: Inference-time compute techniques (CoT, self-consistency, self-refine)
  - Why needed here: These are the baseline interventions tested; understanding their failure modes in expert domains is central to the paper's contribution
  - Quick check question: Can you explain why self-consistency (majority vote over sampled reasoning paths) might fail when all sampled paths share the same systematic error?

- **Concept**: Inter-annotator agreement metrics (Fleiss' Kappa)
  - Why needed here: The multi-agent framework evaluates both accuracy and consensus; Kappa quantifies whether agents converge independently of chance agreement
  - Quick check question: What does a Fleiss' Kappa of 0.9 indicate about agent behavior across discussion rounds?

- **Concept**: Statistical significance testing (McNemar's test)
  - Why needed here: The paper uses McNemar's test to assess whether reasoning models significantly outperform non-reasoning models; understanding this helps interpret the "not statistically significant" claim
  - Quick check question: Why is McNemar's test appropriate for comparing two classifiers on the same labeled dataset?

## Architecture Onboarding

- **Component map**: Input layer (task description + guideline + instance) -> Agent layer (3 LLMs) -> Discussion loop (Check consensus → Compile Discussion History → Re-annotate) -> Output layer (Final annotation via consensus or majority vote)

- **Critical path**:
  1. Each agent independently generates initial annotation with CoT prompting
  2. If no consensus, compile all reasoning into Discussion History
  3. Agents re-annotate with access to peer reasoning
  4. Repeat until consensus or max rounds; fallback to majority vote

- **Design tradeoffs**:
  - Agent composition: More reasoning models increase upper-bound accuracy but may reduce consensus convergence due to self-consistency; non-reasoning models are more flexible but lower ceiling
  - Max discussion rounds: 2 rounds capture most consensus cases; additional rounds provide diminishing returns and can stall when all agents resist change
  - Group size: 3 agents ensures majority vote resolution but increases API cost linearly

- **Failure signatures**:
  - Persistent disagreement across rounds (all agents refuse to change)
  - Reasoning model incorrectly persuades non-reasoning agents toward wrong label
  - Correct label absent from all initial predictions (irrecoverable case)

- **First 3 experiments**:
  1. Baseline check: Run vanilla prompting vs. CoT on a held-out subset of each dataset to reproduce the reported negative/marginal gains; log per-dataset accuracy deltas
  2. Ablation on agent composition: Test 3 configurations (0, 1, 2 reasoning models) on the same instances; measure accuracy, agreement, and consensus rate per round
  3. Upper bound analysis: For a sample where discussion fails to reach consensus, manually inspect whether the correct label was present in any initial prediction; quantify the gap to theoretical upper bound

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms cause reasoning models (e.g., Claude 3.7 Sonnet with thinking) to exhibit strong self-consistency, rarely updating annotations even when presented with correct peer reasoning?
- Basis in paper: Section 4.4 states Claude 3.7 Sonnet with thinking rarely changes its initial annotations and hypothesizes overconfidence or greater persuasiveness as explanations
- Why unresolved: The paper identifies the behavior but does not experimentally isolate whether overconfidence, persuasiveness, or other factors drive this pattern
- What evidence would resolve it: Ablation studies controlling for confidence calibration and persuasiveness of model outputs in controlled multi-agent settings

### Open Question 2
- Question: Can few-shot prompting, domain-specific fine-tuning, or retrieval-augmented generation (RAG) close the performance gap between LLMs and human expert annotators?
- Basis in paper: Limitations section states future directions could explore few-shot settings, domain-specific or fine-tuned LLMs, and retrieval-augmented generation methods
- Why unresolved: The study deliberately uses zero-shot, general-purpose chatbot LLMs to assess out-of-the-box capability
- What evidence would resolve it: Comparative experiments augmenting the same annotation tasks with few-shot examples, domain-adapted models, or external knowledge retrieval

### Open Question 3
- Question: Why do inference-time techniques (CoT, self-refine, self-consistency) show marginal or negative gains for expert annotation, contrary to their effectiveness in general-domain tasks?
- Basis in paper: Section 3.2 notes performance declines and speculates that models might not accurately interpret specialized annotation guidelines, but does not test this hypothesis
- Why unresolved: The paper reports the phenomenon without diagnostic experiments isolating the cause
- What evidence would resolve it: Error analysis comparing guideline comprehension vs. reasoning quality, or controlled experiments with simplified vs. complex guidelines

### Open Question 4
- Question: How do findings generalize to natural language generation (NLG) annotation tasks beyond the multiple-choice NLU tasks studied?
- Basis in paper: Limitations section notes we primarily focus on natural language understanding tasks with fixed label space; natural language generation tasks could be further incorporated
- Why unresolved: All five datasets use classification-style labels; open-ended annotation requires different evaluation methodology
- What evidence would resolve it: Extending the experimental framework to NLG tasks (e.g., summarization, entity extraction) with appropriate metrics for free-form annotations

## Limitations
- Limited dataset diversity (5 domains only) restricts generalizability claims about LLM expert annotation capabilities
- Sampling methodology unspecified—potential bias in instance selection affects reproducibility
- No explicit cost analysis comparing LLM annotation pipelines versus human expert annotation

## Confidence
- **High**: Inference-time techniques (CoT, self-consistency, self-refine) provide marginal/negative gains in expert domains—directly supported by quantitative results across all 5 datasets
- **Medium**: Multi-agent discussion improves consensus and accuracy—mechanism plausible but relies on suggestive evidence from related work on multi-agent reasoning
- **Medium**: Reasoning models' resistance to peer correction due to overconfidence—observed behavior but causal mechanisms require further validation

## Next Checks
1. Dataset sampling validation: Replicate experiments with stratified sampling across dataset difficulty levels to confirm negative results aren't sampling artifacts
2. Cross-domain generalization: Test the multi-agent framework on one additional expert domain (e.g., chemistry or engineering) to assess external validity
3. Reasoning model ablation: Systematically vary the number of reasoning models in multi-agent groups (0, 1, 2, 3) to quantify the consensus-degrading effect and identify optimal composition