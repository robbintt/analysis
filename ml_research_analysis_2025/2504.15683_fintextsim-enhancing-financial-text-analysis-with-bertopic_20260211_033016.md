---
ver: rpa2
title: 'FinTextSim: Enhancing Financial Text Analysis with BERTopic'
arxiv_id: '2504.15683'
source_url: https://arxiv.org/abs/2504.15683
tags:
- topic
- fintextsim
- topics
- financial
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FinTextSim, a domain-specific sentence transformer
  finetuned for financial text analysis, and evaluates its performance in combination
  with BERTopic for analyzing 10-K filings. Compared to the widely used all-MiniLM-L6-v2
  (AM) model, FinTextSim increases intratopic similarity by 81% (0.9972 vs.
---

# FinTextSim: Enhancing Financial Text Analysis with BERTopic

## Quick Facts
- **arXiv ID:** 2504.15683
- **Source URL:** https://arxiv.org/abs/2504.15683
- **Reference count:** 40
- **Primary result:** FinTextSim increases intra-topic similarity by 81% and reduces inter-topic similarity by 100% compared to all-MiniLM-L6-v2 when used with BERTopic for 10-K analysis.

## Executive Summary
This paper introduces FinTextSim, a domain-specific sentence transformer fine-tuned for financial text analysis, and evaluates its performance in combination with BERTopic for analyzing S&P 500 10-K filings. The model demonstrates significant improvements over general-purpose embeddings, achieving perfect topic-precision (1.0) compared to 0.311-0.684 for the baseline, while also increasing intra-topic similarity by 81% and reducing inter-topic similarity by 100%. These results highlight the importance of domain-specific fine-tuning for contextual embeddings in financial text analysis.

## Method Summary
FinTextSim is trained by fine-tuning ModernBERT (base) on a keyword-labeled financial corpus using Adaptive Circle Loss, with sentences labeled when containing ≥2 keywords from a single financial topic domain. The model is then used as input to BERTopic, which applies UMAP dimensionality reduction followed by HDBSCAN clustering, with c-TF-IDF used for topic extraction. The training data consists of S&P 500 10-K reports from 2016-2022, with labels generated through dictionary matching against 14 financial topics.

## Key Results
- FinTextSim increases intra-topic similarity by 81% (0.9972 vs. 0.5498) and reduces inter-topic similarity by 100% (0.0002 vs. 0.4647) compared to all-MiniLM-L6-v2
- Topic-precision improves from 0.311-0.684 to perfect 1.0 with FinTextSim
- BERTopic forms clear and distinct economic topic clusters only when paired with FinTextSim
- FinTextSim generates significantly fewer outliers (184,470) compared to all-MiniLM-L6-v2 (226,605)

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Representation Tuning
Fine-tuning on a keyword-labeled financial corpus aligns vector representations with economic semantic boundaries. Adaptive Circle Loss dynamically adjusts penalties based on distance from optimal similarity scores, pulling domain-specific keywords tighter together while pushing non-related concepts further apart. Core assumption: the keyword dictionary accurately captures financial topics and substring matching is sufficient for generating training labels.

### Mechanism 2: Resolution of Semantic Ambiguity
General models misclassify financial text because they rely on linguistic patterns common in general English, conflating distinct financial concepts. Training on sentences with ≥2 keywords from a single topic domain forces the embedding space to isolate distinct economic themes. Core assumption: financial sentences predominantly discuss a single dominant topic, allowing for unambiguous label assignment.

### Mechanism 3: Improved Density-Based Clustering
Higher quality input embeddings simplify the downstream clustering task by reducing dimensionality reduction burden. Well-separated embeddings with high intra-topic density allow UMAP to preserve distinct structures more effectively, resulting in clearer topic clusters. Core assumption: UMAP hyperparameters are optimal for capturing the global structure of both general and domain-specific embeddings.

## Foundational Learning

- **Concept:** Sentence-BERT (Siamese Networks)
  - **Why needed here:** You must understand how text is converted into fixed-size vectors (embeddings) and how "similarity" is calculated (Cosine Similarity) to grasp how FinTextSim improves upon standard BERT.
  - **Quick check question:** How does a siamese network architecture determine if two sentences are semantically similar?

- **Concept:** Metric Learning (Circle Loss)
  - **Why needed here:** The paper selects Adaptive Circle Loss over Triplet Loss. Understanding this helps explain why the model achieves better separation (optimizing within a circular decision boundary rather than just minimizing distance).
  - **Quick check question:** Why might Circle Loss offer better optimization for clustering than standard Triplet Loss?

- **Concept:** Topic Coherence vs. Precision
  - **Why needed here:** The paper argues standard NPMI coherence is misleading for finance. You need to distinguish between "statistically probable word groupings" (Coherence) and "economically relevant topics" (Precision).
  - **Quick check question:** Why does the paper weight NPMI by "Topic-Precision"?

## Architecture Onboarding

- **Component map:** Raw text (Item 7/7A of 10-Ks) -> Keyword-matrix generation (substring matching against 14 financial topics) -> ModernBERT (Base) -> Mean Pooling -> Normalization -> Adaptive Circle Loss (Scale: 5→16, Margin: 0.25→0.1) -> UMAP (Dim Reduction) -> HDBSCAN (Clustering) -> c-TF-IDF (Topic Extraction)

- **Critical path:** The Keyword Labeling Logic. The entire model's performance relies on the quality of the automated labels generated by the keyword list. If this step introduces noise, the fine-tuning optimizes for error.

- **Design tradeoffs:** Dictionary vs. Manual Labeling (scalability vs. accuracy); Sentence-level vs. Document-level (fits transformer limits vs. losing cross-sentence context).

- **Failure signatures:** High Coherence, Low Precision (linguistically valid but economically meaningless topics); Outlier Overload (>30% classified as noise); Topic Overlap (intertopic similarity >0.1 indicates general-purpose behavior).

- **First 3 experiments:**
  1. Visual Validation: Run UMAP on test set embeddings using min_dist=0 and n_neighbors=100. Confirm if 14 colored topics form distinct islands (FinTextSim) or a single blob (AM).
  2. Precision Threshold Test: Calculate Topic-Precision on held-out 2023 10-Ks to check for temporal overfitting.
  3. Ablation on Loss Function: Retrain using standard Triplet Loss vs. Adaptive Circle Loss to quantify specific performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FinTextSim maintain high performance when applied to financial documents other than 10-K filings?
- Basis in paper: Section 4.4 states, "the extent to which FinTextSim generalizes beyond 10-K reports remains an open question."
- Why unresolved: The model was trained and evaluated exclusively on S&P 500 10-K reports.
- What evidence would resolve it: Benchmarking against general models on diverse financial sources like news articles or conference call transcripts.

### Open Question 2
- Question: How can domain-specific topic quality be objectively measured when standard coherence metrics contradict human judgment?
- Basis in paper: The authors note that NPMI gave higher scores to misclassified topics and conclude there is a "necessity for new coherence or topic quality measures."
- Why unresolved: Financial terms often stand alone; standard sliding-window coherence fails to capture this "true" quality.
- What evidence would resolve it: A proposed metric that aligns with "topic-precision" and expert evaluation better than raw NPMI scores.

### Open Question 3
- Question: To what extent does hyperparameter tuning for dimensionality reduction and clustering improve BERTopic results with contextual embeddings?
- Basis in paper: The authors write, "we plan to investigate the impact of hyperparameter tuning for both dimensionality reduction and clustering techniques on contextual embeddings."
- Why unresolved: The current study fixed these hyperparameters to isolate the effect of the embedding model.
- What evidence would resolve it: An ablation study optimizing UMAP and HDBSCAN parameters to determine if they further enhance clarity of financial topics.

## Limitations

- The core experimental claims rely on proprietary keyword lists and training data that are not fully disclosed
- Performance gains are benchmarked against a single general-purpose model with no ablation studies testing relative contributions
- Reported metrics are highly sensitive to specific clustering parameters which may not generalize to different datasets
- The single-label assumption for financial sentences could break down with more complex, multi-topic financial reporting

## Confidence

- **High Confidence:** The mathematical framework for Adaptive Circle Loss and its theoretical advantages over Triplet Loss; the BERTopic clustering methodology and its dependency on input embedding quality
- **Medium Confidence:** The domain-specific performance gains given the lack of direct comparison to other domain-adapted models and sensitivity of metrics to clustering hyperparameters
- **Low Confidence:** The generalizability of the keyword-based labeling approach across different financial reporting periods and industries; the robustness of the Topic-Precision metric as an evaluation standard

## Next Checks

1. **Temporal Validation:** Apply FinTextSim to 2023 10-K filings (data not used in training) to test for temporal overfitting and generalization to newer financial terminology
2. **Ablation Study:** Systematically vary UMAP and HDBSCAN parameters to quantify how much of the performance gain is due to FinTextSim's embeddings versus clustering configuration
3. **Cross-Industry Test:** Evaluate FinTextSim on 10-Ks from non-S&P 500 companies to assess whether domain adaptation is specific to large-cap U.S. companies or generalizes across the financial reporting landscape