---
ver: rpa2
title: Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices
arxiv_id: '2510.06093'
source_url: https://arxiv.org/abs/2510.06093
tags:
- alignment
- algorithmic
- classical
- llm-based
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared classical AI and LLM-based algorithmic decision-makers
  for aligning health insurance decisions with three risk tolerance profiles (0.0,
  0.5, 1.0). The classical AI model used case-based reasoning with Bayesian inference,
  while the LLM-based models (GPT-4 and GPT-5) employed weighted self-consistency
  with zero-shot prompting.
---

# Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices

## Quick Facts
- arXiv ID: 2510.06093
- Source URL: https://arxiv.org/abs/2510.06093
- Reference count: 5
- Classical AI and LLM-based models achieve ~89% alignment accuracy for health insurance decisions across three risk tolerance profiles

## Executive Summary
This study compares classical AI (case-based reasoning with Bayesian inference) and LLM-based algorithmic decision-makers (GPT-4, GPT-5) for aligning health insurance decisions with individual risk tolerance profiles. Both approaches achieve similar overall alignment accuracy (~89%), with classical AI showing slightly better consistency across risk levels. The highest alignment occurs for extreme risk profiles (0.0 and 1.0), while moderate risk (0.5) proves more challenging for both methods. The findings suggest that despite fundamentally different mechanisms—interpretability and fine-grained control versus prompt-based personalization—both approaches effectively capture risk-averse decision-making patterns.

## Method Summary
The study uses a health insurance dataset with 17,400 probes (5,000 train, 1,000 test) featuring contextual features and four plan options per probe. Three synthetic targets represent risk tolerance levels: Alex (0.0), Brie (0.5), and Chad (1.0). The classical AI model uses Molineaux et al.'s case-based reasoning with offline case base construction (Monte Carlo simulations, Bayesian analysis) and online nearest-neighbor retrieval. LLM-based models employ zero-shot prompting with weighted self-consistency: N=5 positive samples using target-aligned prompts (+1 votes) and N=5 negative samples using inverse-target prompts (-1 votes), with final decisions via weighted majority vote at temperature=0.7.

## Key Results
- Both classical AI and LLM models achieve ~89% overall alignment accuracy
- Highest alignment for extreme risk profiles (0.0 and 1.0), lower for moderate risk (0.5)
- Classical AI shows slightly better consistency across all risk levels
- Weighted self-consistency with positive/negative sampling improves LLM alignment stability
- Dataset and code are publicly available at specified GitHub repositories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighted self-consistency with positive/negative sampling improves LLM alignment stability.
- **Mechanism:** Generate N=5 positive samples using target-aligned prompts (+1 votes) and N=5 negative samples using inverse-target prompts (−1 votes). Final decision via weighted majority vote reduces stochasticity while preserving context sensitivity.
- **Core assumption:** The model's response distribution meaningfully shifts when prompted with different persona attributes, and opposing prompts provide informative negative signal.
- **Evidence anchors:**
  - [abstract] "LLM-based models (GPT-4 and GPT-5) employed weighted self-consistency with zero-shot prompting"
  - [Section 3.3, Algorithm 2] Full pseudocode shows positive sampling loop assigning +1, negative sampling assigning −1, with final argmax aggregation
  - [corpus] Ravichandran et al. (ALIGN, 2025) corroborates prompt-based attribute alignment for personalized LLM decision-making

### Mechanism 2
- **Claim:** Case-based reasoning with learned similarity weights enables granular, continuous alignment.
- **Mechanism:** Offline phase builds case base with decision analytics (Monte Carlo, Bayesian evaluation). Online phase retrieves k-nearest neighbors using learned weights, computes weighted average of neighbor targets, selects decision minimizing distance to target attribute value.
- **Core assumption:** Past decisions and their analytics generalize to new probes; similarity metric captures decision-relevant structure.
- **Evidence anchors:**
  - [Section 3.2] "builds a case base by generating candidate decisions...evaluating them with Monte Carlo simulations, Bayesian reasoning"
  - [Section 4.1] "classical AI algorithmic DM can operationalize risk tolerance by defining targets at every 0.1 interval between 0 and 1"
  - [corpus] No direct corpus papers on CBR for DMA; Molineaux et al. (2024) cited but not in neighbor set

### Mechanism 3
- **Claim:** Extreme risk profiles (0.0, 1.0) are easier to align than moderate profiles (0.5).
- **Mechanism:** Extreme profiles have clearer linguistic and structural markers (e.g., "minimize cost at all costs" vs. "maximize coverage"), reducing ambiguity in both prompt interpretation and case similarity matching.
- **Core assumption:** The dataset ground truth reflects consistent decision patterns for extreme profiles; moderate profiles involve more trade-offs without clear signals.
- **Evidence anchors:**
  - [abstract] "highest alignment for extreme risk profiles (0.0 and 1.0) and lower performance for moderate risk (0.5)"
  - [Section 4.1] "describing a moderate risk target (0.5) without overlapping linguistic cues from the extreme targets...is inherently difficult"
  - [corpus] Mainali & Weber (2025) on cognitive attributes in financial decision-making notes attributes exist on continuums, extremes are easier to characterize

## Foundational Learning

- **Concept: Decision-Maker Alignment (DMA)**
  - **Why needed here:** DMA shifts focus from universal value alignment (e.g., harmlessness) to aligning with individual cognitive attributes like risk tolerance, essential for personalized, high-stakes decision support.
  - **Quick check question:** Can you explain why RLHF-based "helpfulness" doesn't guarantee alignment with a specific user's risk preferences?

- **Concept: Weighted Self-Consistency**
  - **Why needed here:** LLMs are stochastic; self-consistency sampling aggregates multiple outputs to improve reliability. Weighted version uses positive/negative prompts to steer toward target while dampening opposing outputs.
  - **Quick check question:** If you run the same prompt 10 times with temperature 0.7, why might self-consistency outperform single-sample inference?

- **Concept: Case-Based Reasoning (CBR) for Alignment**
  - **Why needed here:** CBR provides interpretability and fine-grained control by retrieving similar past cases, enabling alignment at any target attribute value rather than discrete prompt categories.
  - **Quick check question:** What happens to CBR alignment accuracy if the case base has few examples near the target attribute value?

## Architecture Onboarding

- **Component map:**
  Classical AI DM: Offline trainer → Case base (probes, decisions, analytics, targets) → Similarity weight learner → Online retriever → Decision selector
  LLM-based DM: Prompt formatter → Target/inverse persona prompts → LLM API (GPT-4/GPT-5, temp=0.7) → Response parser → Vote aggregator → Final decision

- **Critical path:**
  1. Dataset preparation: 5,000 training probes, 1,000 test probes, three targets (Alex=0.0, Brie=0.5, Chad=1.0)
  2. Classical: Train case base → learn similarity weights → for each test probe, retrieve neighbors, estimate target, select aligned decision
  3. LLM: For each test probe, generate 5 positive + 5 negative samples → aggregate votes → return argmax

- **Design tradeoffs:**
  - Classical: Higher upfront engineering cost, but fine-grained control (0.1 intervals), interpretable similarity weights
  - LLM: Lower engineering cost (prompt-only), but limited by natural language expressiveness for moderate attributes, API dependency

- **Failure signatures:**
  - Classical: Low alignment for underrepresented attribute regions (sparse case base)
  - LLM: High variance across runs if temperature too high; poor moderate-profile alignment regardless of sampling
  - Both: Near-identical accuracy on extremes, divergence on moderate—check per-target breakdown first

- **First 3 experiments:**
  1. **Reproduce baseline:** Run both architectures on 1,000 test probes, confirm ~89% overall accuracy and per-target breakdown (Alex ~high, Chad ~high, Brie ~lower)
  2. **Ablate sampling:** For LLM, reduce N from 5 to 1 for positive/negative samples; measure alignment drop, especially for moderate target
  3. **Probe granularity:** For classical, test targets at 0.1 intervals (0.0, 0.1, ..., 1.0) on a subset; plot alignment vs. target value to verify smooth degradation around 0.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning or providing domain-specific training data to LLM-based decision-makers improve alignment for moderate risk profiles, or would models continue to rely primarily on internal priors and pretraining biases?
- Basis in paper: [explicit] Authors state: "This raises an important question: if we were to fine-tune or provide high-quality, domain-specific training data to the LLM, particularly for the moderate-risk target, which exhibited the lowest alignment, would its performance improve, or would it continue to depend primarily on its internal priors and pretraining biases?"
- Why unresolved: The study only evaluated zero-shot prompting; no fine-tuning experiments were conducted. All LLM-based DMs used pre-trained models without domain-specific training data.
- What evidence would resolve it: Experiments comparing zero-shot performance against fine-tuned models on the same health insurance dataset, particularly measuring alignment improvement for the moderate risk tolerance (0.5) target.

### Open Question 2
- Question: Does the difficulty in aligning LLMs to moderate cognitive attributes stem from prompt design limitations or from inherent expressive boundaries of natural language itself?
- Basis in paper: [explicit] Authors state: "describing a moderate risk target (0.5) without overlapping linguistic cues from the extreme targets (0.0 and 1.0) is inherently difficult, raising the question of whether this limitation stems from the prompt itself or from the expressive boundaries of natural language."
- Why unresolved: Current prompts for moderate risk may inadvertently share linguistic features with extreme risk prompts, and no systematic analysis of prompt linguistic overlap has been conducted.
- What evidence would resolve it: Development and testing of novel prompting strategies that use distinct linguistic constructions for intermediate attribute levels, followed by analysis of whether alignment improves independently of prompt wording.

### Open Question 3
- Question: How would varying parameters within the weighted self-consistency framework (sample counts, temperature, negative sampling allocations) affect alignment performance across different risk profiles?
- Basis in paper: [explicit] Authors state: "we did not vary parameters within the weighted self-consistency framework, which could reveal more nuanced relationships between prompt design and decision alignment."
- Why unresolved: The study used fixed parameters (N=5 positive samples, N=5 negative samples split 2-3 or 3-2, temperature=0.7) without exploring alternatives.
- What evidence would resolve it: Ablation studies systematically varying self-consistency parameters and measuring their impact on alignment accuracy, particularly for the moderate risk target.

### Open Question 4
- Question: Can alignment methods developed for risk tolerance generalize to other cognitive attributes such as ambiguity aversion, temporal discounting, and self-control that characterize real-world human decision-making?
- Basis in paper: [explicit] Authors state: "while our experiment focused solely on risk tolerance, human decision-making is shaped by a rich interplay of cognitive attributes, such as ambiguity aversion, temporal discounting, and self-control, that exist on a continuum rather than at extremes."
- Why unresolved: The dataset and experiments only operationalized risk tolerance; no other cognitive attributes were tested.
- What evidence would resolve it: Creation of benchmark datasets annotated for multiple cognitive attributes and evaluation of both classical AI and LLM-based DMs on these expanded attribute sets.

## Limitations

- Moderate risk profile (0.5) alignment challenges may reflect dataset or prompt limitations rather than fundamental model constraints
- Exact prompt templates for zero-shot prompting are unspecified, making verification difficult
- Classical AI hyperparameters (Monte Carlo iterations, Bayesian priors, neighbor count) are not detailed
- No analysis of individual decision-level errors or systematic error patterns
- Only one cognitive attribute (risk tolerance) was tested, limiting generalizability

## Confidence

- **High confidence:** Overall alignment accuracy (~89%) and extreme profile performance (0.0, 1.0) - directly reported results with clear metrics
- **Medium confidence:** Mechanism explanations for weighted self-consistency and case-based reasoning - supported by citations but lack full implementation details
- **Medium confidence:** Claim that both approaches capture risk-averse patterns comparably - based on aggregate metrics without individual decision analysis
- **Low confidence:** The fundamental difficulty of aligning with moderate risk profiles - could be mechanism-driven or artifact of prompt design/limited training coverage

## Next Checks

1. **Per-Decision Error Analysis:** Extract individual probe decisions where models failed on the moderate risk profile (0.5). Categorize errors by type (plan selection, attribute weighting, linguistic ambiguity) to determine if failures are systematic or random.

2. **Prompt Sensitivity Test:** Implement a small-scale ablation study varying prompt templates for the 0.5 risk profile while keeping all else constant. Test whether clearer linguistic separation from extremes improves alignment without changing model architecture.

3. **Case Base Coverage Analysis:** For the classical AI model, plot the distribution of training cases across the 0.0-1.0 risk continuum. Measure alignment accuracy as a function of target proximity to nearest training examples to verify the coverage assumption.