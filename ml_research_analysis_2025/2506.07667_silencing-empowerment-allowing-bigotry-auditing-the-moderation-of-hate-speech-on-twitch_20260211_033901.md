---
ver: rpa2
title: 'Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech
  on Twitch'
arxiv_id: '2506.07667'
source_url: https://arxiv.org/abs/2506.07667
tags:
- twitch
- moderation
- content
- automod
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Twitch\u2019s AutoMod system only flags 22% of hateful content\
  \ and struggles with implicit hate, blocking just 6-7% on datasets designed to test\
  \ context understanding. It over-moderates benign, pedagogical, or empowering content\
  \ using sensitive words, with up to 98.5% blocked."
---

# Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch

## Quick Facts
- **arXiv ID:** 2506.07667
- **Source URL:** https://arxiv.org/abs/2506.07667
- **Reference count:** 40
- **Primary result:** Twitch's AutoMod system only flags 22% of hateful content and struggles with implicit hate, blocking just 6-7% on datasets designed to test context understanding.

## Executive Summary
This audit reveals Twitch's AutoMod system has severe limitations in detecting hate speech, particularly implicit hate conveyed through stereotypes and coded language. The system demonstrates a heavy reliance on explicit slur detection, achieving 100% removal when slurs are added to otherwise undetected implicit hate, but only 4% recall under minor semantic-preserving perturbations. AutoMod over-moderates benign content using sensitive words, blocking up to 98.5% of pedagogical or empowering discussions. The audit also uncovers a pre-filtering layer that creates inconsistent protection across target groups, with performance varying significantly by protected category.

## Method Summary
The audit employed a three-bot pipeline (messenger, receiver, PubSub) to systematically test AutoMod's moderation decisions on 1.7 million messages across four datasets. Using Twitch's Developer Console with OAuth2.0 authentication, messages were sent at ~0.43 msg/s with AutoMod configured at maximum filtering level (α=4). The methodology captured both held messages via PubSub automodqueue events and successfully delivered messages, allowing measurement of false positives and negatives across four filter categories (Disability, SSG, Misogyny, RER) and multiple target groups.

## Key Results
- AutoMod's overall accuracy (55%) and precision (56%) lag far behind state-of-the-art models
- The system achieves only 6-7% recall on implicit hate datasets, compared to 100% when explicit slurs are added
- Performance varies dramatically across target groups due to pre-filtering effects (37.4% for Black-related hate vs much lower for other groups)
- AutoMod is extremely brittle to minor perturbations, with recall dropping from 100% to 4% under semantic-preserving modifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AutoMod relies primarily on explicit slur detection rather than contextual understanding of hate speech.
- **Mechanism:** The system appears to match input text against keyword lists or pattern-based rules for specific slur terms. When slurs are absent, implicit hate—conveyed through stereotypes, insinuations, or coded language—bypasses detection entirely.
- **Core assumption:** The audit infers this mechanism from behavioral observation; internal architecture is not disclosed.
- **Evidence anchors:**
  - [abstract]: "Contextual addition of slurs to these messages results in 100% removal, revealing AutoMod's reliance on slurs as a moderation signal."
  - [section §5]: Counterfactual analysis shows 110 false negatives become 100% detectable when slurs are inserted.
  - [corpus]: Hartmann et al. (2025) find similar keyword reliance in commercial moderation APIs.

### Mechanism 2
- **Claim:** A service-level pre-filter blocks certain high-sensitivity terms before content reaches channel-level AutoMod filters.
- **Mechanism:** A hardcoded blocklist appears to operate at a layer above AutoMod, removing messages containing specific terms (e.g., the n-word variants) without exposing them to filter configuration options.
- **Core assumption:** This is inferred from messages that never appear in receiver bot logs or PubSub events.
- **Evidence anchors:**
  - [section §4.1]: "We observe a third category of messages that were not detected by either the receiver or the pubsub bot... suspected that these messages are caught at the Service Level."
  - [section §4.2]: 54.8% of SSG-related hate examples are pre-filtered; 37.4% of Black-related hate recall is attributable to pre-filtering.

### Mechanism 3
- **Claim:** AutoMod processes messages independently without conversation-level context or memory.
- **Mechanism:** Each message is evaluated in isolation. Interleaving the same 100 hateful examples with 100 benign examples produces identical moderation decisions as sending hateful examples alone.
- **Core assumption:** The experiment tests only message-order context; longer-range conversational context was not evaluated.
- **Evidence anchors:**
  - [section §4.2]: "We observed that changing the order of input messages does not change the moderation decision for any example."

## Foundational Learning

- **Concept:** Implicit vs. Explicit Hate Speech
  - **Why needed here:** The paper's central finding is AutoMod's near-total failure on implicit hate (6-7% recall). Understanding this distinction is essential to interpret why slur-based systems fail.
  - **Quick check question:** "Why does the statement 'certain neighborhoods have always been known for crime' potentially constitute hate speech even without slurs?"

- **Concept:** Black-Box Algorithm Auditing
  - **Why needed here:** The paper's methodology demonstrates how to evaluate a system with only input-output access, a critical skill for platform accountability research.
  - **Quick check question:** "What can you infer about a system's decision boundary if it classifies 'X' as benign but 'X with a typo' as hateful?"

- **Concept:** Pre-filtering vs. Contextual Filtering
  - **Why needed here:** Twitch's multi-layer architecture (service-level blocklists + channel-level AutoMod) creates differential protection across target groups and reduces streamer control.
  - **Quick check question:** "Why might pre-filtering the n-word improve recall for anti-Black hate but harm pedagogical discussions about racism?"

## Architecture Onboarding

- **Component map:**
  1. Service-Level Pre-Filter: Undocumented hardcoded blocklist that removes messages before channel-level processing
  2. AutoMod Engine: Configurable filter system with four sub-filters (Disability, SSG, Misogyny, RER) and 5-level strictness settings (α=0–4)
  3. PubSub Event System: Real-time notification of moderation decisions with internal category labels
  4. Chat Filter: User-facing toggle that mirrors AutoMod α=4 behavior for Discrimination & Slurs category

- **Critical path:**
  Message → Service-Level Pre-Filter (if matched, dropped) → AutoMod Engine (evaluated against active filters at level α) → Decision (held or posted) → PubSub notification (if held) + Chat delivery (if passed)

- **Design tradeoffs:**
  - Speed vs. Accuracy: Real-time chat latency constraints may limit model complexity; AutoMod processes ~0.43 msg/s in audit
  - Configurability vs. Consistency: Per-streamer filter settings create inconsistent user experiences across channels
  - Over-blocking vs. Under-blocking: Setting α=4 blocks 98.5% of pedagogical content; α=2 still blocks 89.5%

- **Failure signatures:**
  - High false negatives on stereotype-based hate without explicit slurs
  - High false positives on reclaimed/educational uses of sensitive terms
  - Extreme brittleness to spacing, punctuation, or phonetic perturbations (recall drops from 100% to 4%)
  - Inconsistent pre-filtering creates disparate protection across target groups

- **First 3 experiments:**
  1. Implicit Hate Probe: Test 100 stereotype-based statements from IHC/ToxiGen with no slurs across all four filters at α=4 to establish baseline implicit hate recall.
  2. Context Sensitivity Test: Submit pairs where (a) a slur is used pedagogically and (b) the same slur is used hatefully to measure context-awareness gap.
  3. Perturbation Robustness Audit: Apply the six perturbation methods to 50 blocked examples and measure which still trigger moderation to characterize rule brittleness.

## Open Questions the Paper Calls Out

- **Question:** How does AutoMod's moderation performance compare to other deployed commercial content moderation systems (e.g., on Reddit, Discord, or emerging APIs like Mistral's)?
  - **Basis in paper:** [explicit] The authors state in the Limitations: "we only consider a single platform in our study, and do not compare AutoMod to any other deployed text moderation." They also note a concurrent study evaluating other moderation APIs.
  - **Why unresolved:** This audit was siloed to Twitch due to its specific configurable tools; a cross-platform comparison requires equivalent audit access and methodology adaptation.
  - **What evidence would resolve it:** A replicated audit methodology applied to other platforms' automated moderation tools, reporting comparative metrics (recall, precision, F1) on standardized hate speech datasets.

- **Question:** What is the impact of Twitch's pre-filtering blocklist on the equitable moderation of hate speech across different protected groups?
  - **Basis in paper:** [inferred] The paper observes that pre-filtering (e.g., of the n-word) causes disparate performance, leading to higher recall for hate targeting Black folks versus other groups like Jewish or Muslim folks (§D.7). This suggests blocklist design may introduce bias.
  - **Why unresolved:** The internal composition and design rationale for the pre-filtering blocklist is opaque, and its interaction with the AutoMod filters creates a complex, two-tiered system whose fairness properties are unclear.
  - **What evidence would resolve it:** An analysis of the pre-filtering blocklist's contents, coupled with ablation studies measuring group-wise recall with and without pre-filtering enabled.

- **Question:** Can state-of-the-art, context-aware language models be effectively integrated into low-latency, real-time moderation pipelines for live streaming platforms?
  - **Basis in paper:** [explicit] The authors demonstrate a large performance gap between AutoMod and SoTA LLMs (Figure 2) and conclude that AutoMod's "lack of contextual understanding" is a key failure mode. They suggest this as a core area for improvement.
  - **Why unresolved:** While SoTA models perform better, their computational cost and latency may be prohibitive for real-time chat moderation at scale, which requires millisecond-level responses.
  - **What evidence would resolve it:** A prototype implementation of a SoTA model (e.g., a fine-tuned Llama variant) within a Twitch-like chat pipeline, measuring both moderation performance (recall/precision) and end-to-end latency under realistic load.

## Limitations
- The audit methodology relies on controlled, siloed environments that may not reflect real-world chat dynamics with human moderators and community norms
- The internal composition of Twitch's pre-filtering blocklist remains completely opaque, limiting attribution of moderation decisions
- Dataset limitations mean results may not generalize to all languages, dialects, or platform-specific slang used on Twitch

## Confidence
- **High Confidence:** AutoMod's poor performance on implicit hate (6-7% recall) and its brittleness to semantic-preserving perturbations (4% recall under minor modifications)
- **Medium Confidence:** The inference that AutoMod relies primarily on keyword matching rather than contextual understanding
- **Low Confidence:** Precise quantification of pre-filtering effects, as messages caught at this layer leave no trace in AutoMod's decision logs

## Next Checks
1. **Contextual Pair Testing:** Submit matched pairs of messages where one uses a sensitive term pedagogically and another uses it hatefully. Measure whether AutoMod's decisions vary based on surrounding context or additional clarifying text.

2. **Perturbation Gradient Analysis:** Systematically apply increasingly subtle perturbations (letter spacing, punctuation, phonetic replacements) to blocked messages and measure the recall degradation curve.

3. **Cross-Platform Comparison:** Audit similar commercial moderation APIs (Perspective API, Facebook's Hate Speech Detection, etc.) using the same test datasets to establish whether AutoMod's performance is typical or particularly deficient among industry offerings.