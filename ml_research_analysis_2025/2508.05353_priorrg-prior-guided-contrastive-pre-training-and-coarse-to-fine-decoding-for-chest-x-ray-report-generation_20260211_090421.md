---
ver: rpa2
title: 'PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding
  for Chest X-ray Report Generation'
arxiv_id: '2508.05353'
source_url: https://arxiv.org/abs/2508.05353
tags:
- report
- clinical
- generation
- priorrg
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating accurate and
  context-aware chest X-ray reports by incorporating patient-specific prior knowledge,
  including clinical context and prior images. The authors propose PriorRG, a framework
  that uses a two-stage training pipeline: Stage 1 introduces a prior-guided contrastive
  pre-training scheme to leverage clinical context for spatiotemporal feature extraction,
  and Stage 2 employs a prior-aware coarse-to-fine decoding to progressively integrate
  prior knowledge with visual cues.'
---

# PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation

## Quick Facts
- arXiv ID: 2508.05353
- Source URL: https://arxiv.org/abs/2508.05353
- Reference count: 19
- Primary result: 3.6% BLEU-4 improvement over state-of-the-art on MIMIC-CXR

## Executive Summary
This paper introduces PriorRG, a framework for generating accurate and context-aware chest X-ray reports by incorporating patient-specific prior knowledge. The approach uses a two-stage training pipeline: Stage 1 employs prior-guided contrastive pre-training to leverage clinical context for spatiotemporal feature extraction, while Stage 2 uses prior-aware coarse-to-fine decoding to progressively integrate prior knowledge with visual cues. Experimental results demonstrate significant performance improvements over existing methods, with 3.6% BLEU-4 and 3.8% F1 score gains on MIMIC-CXR, and 5.9% BLEU-1 improvement on MIMIC-ABN.

## Method Summary
PriorRG introduces a two-stage training pipeline to address chest X-ray report generation. Stage 1 uses prior-guided contrastive pre-training that leverages clinical context (indications/history) to guide spatiotemporal feature extraction from current and prior chest X-rays. This is achieved through Perceiver-based cross-attention between clinical text representations and visual features. Stage 2 employs a prior-aware coarse-to-fine decoding strategy where a DistilGPT2 generator progressively integrates patient-specific prior knowledge with visual features, first using condensed clinical context, then spatiotemporal features from prior images, and finally fine-grained visual details.

## Key Results
- Achieves 3.6% improvement in BLEU-4 score and 3.8% gain in F1 score on MIMIC-CXR dataset
- Shows 5.9% improvement in BLEU-1 score on MIMIC-ABN dataset
- Outperforms state-of-the-art methods including MRT, KGPT, and MLRG
- Ablation studies demonstrate the effectiveness of coarse-to-fine decoding strategy

## Why This Works (Mechanism)

### Mechanism 1: Prior-Guided Semantic Alignment (Stage 1)
Injecting clinical context during pre-training forces visual features to align with diagnostic intent rather than just visual similarity. The Perceiver resampler compresses clinical text into latent representations that guide spatiotemporal visual feature extraction via cross-attention, ensuring the vision encoder prioritizes features relevant to specific patient symptoms. This reduces ambiguity in image interpretation by constraining the visual search space using reliable semantic priors from clinical context.

### Mechanism 2: Spatiotemporal Grounding for Progression Tracking
Explicitly modeling differences between current and prior images reduces hallucinations regarding disease progression. The Spatiotemporal Fusion Network injects temporal embeddings and uses cross-attention between current and prior image features to create a delta representation that grounds temporal claims in actual visual changes. This ensures statements about progression (e.g., "new device") are based on real visual differences rather than language priors.

### Mechanism 3: Hierarchical Coarse-to-Fine Decoding (Stage 2)
Decoding in stages—first high-level clinical context, then temporal changes, finally fine-grained visual details—improves fluency and clinical coherence. The generator receives a concatenated sequence ordered as [Condensed Context] + [Spatiotemporal Features] + [Hierarchical Visual Features], mimicking a radiologist's workflow of checking history, comparing priors, and inspecting details. This specific ordering matters for the LLM's generation logic.

## Foundational Learning

- **Vision-Language Contrastive Learning (CLIP-style)**: Stage 1 pre-training relies on instance-wise cross-modal alignment. You must understand how matching positive pairs (image+report) and pushing apart negative pairs in a batch creates meaningful joint embeddings. Quick check: How does the model handle multiple positive pairs (multi-view images) within a batch during contrastive loss calculation?

- **The Perceiver Resampler (or Latent Transformer)**: The paper uses a "Perceiver" to bridge different modalities and compress them into fixed-size latent arrays (N=128). Understanding cross-attention between small latent queries and large input sequences is vital for understanding how clinical context fuses into visual features. Quick check: What is the role of the learnable latent embedding Ē_lat in Equations 3 and 4?

- **Attention-Enhanced Layer Fusion**: Standard vision encoders often only output the final layer, but PriorRG uses CBAM-based fusion of all hidden states to capture both low-level (edges/textures) and high-level (semantics) features. Quick check: Why would using only the last hidden state (LastOnly in Table 3) result in performance drop compared to fusing hierarchical hidden states?

## Architecture Onboarding

- **Component map**: RAD-DINO (Vision, frozen) -> CXR-BERT (Text, fine-tuned) -> Spatiotemporal Fusion Network (STF) -> Perceiver blocks -> Attention-enhanced Layer Fusion (ALF) -> DistilGPT2 (LLM, fine-tuned)

- **Critical path**: Inputs (Current Image + Prior Image + Clinical Text) -> Feature Extraction (RAD-DINO + CXR-BERT) -> Stage 1 (STF fusion + Perceiver cross-attention + Contrastive loss) -> Stage 2 (ALF extracts hierarchical features + Perceiver creates context sequence + DistilGPT2 generates report)

- **Design tradeoffs**: The model handles missing priors by falling back to simpler paths, requiring conditional logic in STF block. Using DistilGPT2 and frozen RAD-DINO reduces GPU memory (7.68GB) compared to full LLM fine-tuning, but may limit reasoning capability.

- **Failure signatures**: The model might simplify "mildly enlarged" to "enlarged" due to lack of fine-grained attribute modeling. If Stage 1 alignment fails, the generator may rely too heavily on language priors rather than specific visual evidence.

- **First 3 experiments**:
  1. Verify Stage 1 alignment by running medical image-text retrieval task (MIMIC-5x200) to ensure prior-guided pre-training improves Cat-Precision@K
  2. Ablate priors by running Stage 2 with 0% Prior Images and 0% Clinical Context to establish performance floor and isolate contribution of "Prior" components
  3. Sanity check generation by inspecting "Coarse-to-Fine" vs. "Fine-to-Coarse" outputs qualitatively to confirm ordering produces more fluent/accurate text

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes prior images are geometrically and temporally aligned with current images; no analysis of performance with different views or time gaps
- Effectiveness depends on quality and relevance of indication/history text; no reported performance when clinical context is noisy or missing
- Limited fine-grained attribute modeling leads to oversimplification (e.g., "mildly enlarged" → "enlarged")

## Confidence
- **High Confidence**: Architectural design and experimental methodology are sound; two-stage pipeline well-defined; ablation studies provide strong evidence for coarse-to-fine strategy
- **Medium Confidence**: Performance gains are convincing but ablation studies could be more granular; contribution of clinical context vs. prior images not fully isolated
- **Low Confidence**: Robustness to missing/noisy priors only briefly mentioned; no quantitative analysis validating fallback mechanisms

## Next Checks
1. Systematically evaluate performance degradation when prior images are from different views or long time gaps; report quantitative metrics for each scenario
2. Run Stage 2 with clinical context completely removed (not just prior images) to isolate its contribution; compare against baseline using only visual features
3. Perform detailed error analysis on generated reports focusing on attribute modeling ("mildly" vs. "severely") and temporal claims; quantify frequency and severity of hallucinations