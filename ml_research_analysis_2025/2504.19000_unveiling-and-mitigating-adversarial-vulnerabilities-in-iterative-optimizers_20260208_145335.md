---
ver: rpa2
title: Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers
arxiv_id: '2504.19000'
source_url: https://arxiv.org/abs/2504.19000
tags:
- adversarial
- iterative
- optimizers
- examples
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the vulnerability of iterative optimizers to
  adversarial attacks, showing that such optimizers can be affected by carefully crafted
  input perturbations that modify the optimization objective surface and alter the
  minima sought. The authors demonstrate that many iterative optimizers are end-to-end
  differentiable and sensitive to small input variations, properties traditionally
  associated with ML models like deep neural networks.
---

# Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers

## Quick Facts
- **arXiv ID:** 2504.19000
- **Source URL:** https://arxiv.org/abs/2504.19000
- **Reference count:** 40
- **Primary result:** Iterative optimizers are end-to-end differentiable and sensitive to adversarial perturbations, but can be robustlyified through adversarial training of unfolded architectures.

## Executive Summary
This paper reveals that widely used iterative optimizers in signal processing and communications are vulnerable to adversarial attacks that alter their optimization objectives and cause them to converge to incorrect solutions. The authors demonstrate that these optimizers possess properties traditionally associated with neural networks—end-to-end differentiability and sensitivity to small input perturbations. Through the deep unfolding framework, they convert iterative algorithms into trainable architectures and show that standard unfolding can actually increase vulnerability. The key mitigation strategy involves adversarial training of these unfolded optimizers, which significantly improves robustness at a relatively minor cost to clean-data performance.

## Method Summary
The authors analyze adversarial vulnerability in iterative optimizers by studying how input perturbations reshape the optimization objective surface. They leverage the deep unfolding framework to convert iterative algorithms (proximal gradient descent, ADMM) into trainable architectures by fixing iteration count T and learning per-iteration hyperparameters. The mitigation approach uses adversarial training via a minimax objective where optimizers are trained to minimize worst-case loss over perturbed inputs. The theoretical analysis rigorously characterizes how learned parameters affect Lipschitz continuity, which governs adversarial sensitivity. The approach is validated across three case studies: sparse recovery, robust principal component analysis, and hybrid beamforming.

## Key Results
- Iterative optimizers across multiple case studies (sparse recovery, RPCA, hybrid beamforming) are vulnerable to BIM, NIFGSM, and CW attacks
- Standard unfolding (LISTA) can increase vulnerability compared to original iterative optimizers
- Adversarial training of unfolded optimizers reduces adversarial distortion by 50-75% with <10% clean performance degradation
- Enhanced robustness correlates with improved Lipschitz continuity in the learned unfolded architectures

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Perturbations Reshape the Optimization Objective Surface
- Claim: Carefully crafted input perturbations alter the optimization objective surface, causing the optimizer to converge to a different minimum than intended.
- Mechanism: When perturbation δ is added to input x, the optimizer operates on L_adv(x,s) = L_op(x+δ, s) instead of L_op(x,s). The iterative descent steps are computed on this corrupted surface, leading the optimizer to seek s*_adv = argmin L_adv(x,s) rather than s* = argmin L_op(x,s). For linear objectives with ℓ₂ matching, Proposition 1 provides a lower bound on output deviation: ||s*_adv - s*||²₂ ≥ ||(A^T A)^{-1}A^T||₂ · ε.
- Core assumption: The optimizer is end-to-end differentiable (satisfies P1) and small input perturbations can significantly alter outputs (satisfies P2).
- Evidence anchors:
  - [abstract] "attacking iterative optimizers effectively alters the optimization objective surface in a manner that modifies the minima sought"
  - [Page 3, Section III-B, Eq. 7-8] Formal definition of corrupted loss surface L_adv and resulting altered optimum
  - [corpus] Weak direct corpus support; neighbor papers focus on neural network vulnerabilities rather than optimizer-specific mechanisms
- Break condition: If the optimization objective is Lipschitz with constant sufficiently small relative to ε, then output deviation is bounded and attacks may be ineffective.

### Mechanism 2: Learned Hyperparameters Control Lipschitz Continuity in Unfolded Optimizers
- Claim: The hyperparameters learned through deep unfolding directly determine the Lipschitz constant of the optimizer, which governs adversarial sensitivity.
- Mechanism: For proximal gradient descent with T iterations, the Lipschitz constant is C(θ) = Σ_{t=0}^{T-1} [Π_{j=t+1}^{T-1} ||M^j||₂] · μ_t ||B^t||₂ (Proposition 2). For ADMM, C(θ) = ||B*_T||₂ · Π_{i=0}^T [1 + 2||M^T||₂(|μ^{T-1}|+1) · Π_{j=i-1}^{T-2} |μ^j|] (Proposition 3). The matrices M^t and B^t depend on step sizes μ_t and regularization parameters ρ_t, which become trainable via unfolding.
- Core assumption: The proximal operator is non-expansive in Euclidean space; iterative composition of non-expansive maps with learned parameters preserves Lipschitz structure.
- Evidence anchors:
  - [Page 4, Proposition 2] Complete Lipschitz constant derivation for proximal GD
  - [Page 4, Proposition 3] Complete Lipschitz constant derivation for ADMM
  - [Page 8, Fig. 15] Empirical confirmation that adversarial training reduces normalized Lipschitz constant to ~0.85-0.95 of standard LISTA
- Break condition: If learned parameters cause ||M^t||₂ > 1 consistently, the Lipschitz constant may grow exponentially with T, making robustness impossible without architectural constraints.

### Mechanism 3: Adversarial Training Implicitly Regularizes the Lipschitz Constant
- Claim: Training unfolded optimizers with a minimax objective (adversarial training) improves robustness primarily by inducing parameter configurations with smaller Lipschitz constants.
- Mechanism: Adversarial training solves θ*_adv = argmin_θ max_{δ∈Δ_p(ε)} (1/|D|) Σ L(f_θ, x+δ, s) (Eq. 14). This forces parameters to minimize worst-case loss, which empirically correlates with reduced Lipschitz constant. The trade-off is modest: clean-data performance degrades by ~0.01-0.02 in normalized MSE while adversarial distortion drops by ~50-70% (Figs. 8-9, 13-14).
- Core assumption: The relationship between adversarial robustness and Lipschitz continuity (established in prior work [37]) extends to unfolded optimizers; parameter updates during adversarial training have sufficient gradient signal to modify the Lipschitz constant.
- Evidence anchors:
  - [Page 5, Section III-C, Eq. 14] Formal adversarial training objective
  - [Page 6-8, Figs. 8-9, 11-14] Consistent pattern across BIM, NIFGSM, and CW attacks showing Robust-LISTA/LADMM achieves 50-75% lower adversarial distortion with <10% clean performance degradation
  - [corpus] [Adversarial Training of Reward Models] corroborates adversarial training as general robustification strategy, though in different domain
- Break condition: If ε is too large or the attack space Δ_p(ε) permits perturbations that fundamentally change the problem structure (e.g., make optimization problem infeasible), adversarial training may fail or degrade clean performance unacceptably.

## Foundational Learning

- Concept: **Proximal operators and proximal gradient descent**
  - Why needed here: The paper's theoretical analysis (Propositions 2-3) and primary case study (ISTA/LISTA) rely on understanding how proximal operators enable gradient-based optimization with non-smooth regularizers like ℓ₁.
  - Quick check question: Can you explain why Prox_h(y) = argmin_z {1/2||y-z||² + h(z)} is non-expansive, and how this property is used in the Lipschitz constant proof?

- Concept: **Lipschitz continuity and adversarial robustness**
  - Why needed here: The core theoretical contribution links Lipschitz constant C to adversarial sensitivity: for C-Lipschitz mappings, output deviation is bounded by C·||δ||. Understanding this relationship is essential for interpreting Propositions 2-3 and Figure 15.
  - Quick check question: Given a mapping f with Lipschitz constant C=5, what is the maximum output perturbation achievable with input perturbation δ where ||δ||₂ = 0.1?

- Concept: **Deep unfolding / algorithm unrolling**
  - Why needed here: The paper's mitigation strategy converts iterative optimizers into ML models by fixing iteration count T and learning hyperparameters. Understanding LISTA as the canonical example clarifies how unfolding enables adversarial training.
  - Quick check question: How does fixing T iterations and parameterizing each iteration's step size μ_t and regularization ρ_t convert an iterative algorithm into a trainable neural network architecture?

## Architecture Onboarding

- Component map:
  ```
  Input x → [Iterative Optimizer Module]
              ├── Each iteration t: s_{t+1} = g_{θ_t}(s_t; x)
              ├── Proximal GD: s_{t+1} = Prox_{μ_t·ρ_t·φ}(M^t·s_t + B^t·x)
              └── ADMM: primal update (13a), auxiliary update (13b), dual update (13c)
           → Output f_θ(x) after T iterations
  ```
  For robust training:
  ```
  Clean x → [Perturbation Generator: BIM/NIFGSM/CW] → x+δ → [Iterative Optimizer] → Loss L
  Training loop: min_θ max_δ L(f_θ, x+δ, s)
  ```

- Critical path:
  1. Define optimization objective L_op(x,s) for your task
  2. Select appropriate iterative solver (proximal GD for ℓ₁ regularized; ADMM for constrained formulations)
  3. Unroll T iterations, parameterizing θ_t = {μ_t, ρ_t, M^t, B^t} per iteration
  4. Implement adversarial attack module (start with BIM for simplicity)
  5. Train with adversarial objective (Eq. 14)

- Design tradeoffs:
  - **T (iteration count)**: Higher T improves convergence but increases Lipschitz constant multiplicatively (see product terms in Props. 2-3) and computational cost
  - **ε (attack budget)**: Larger ε in training improves robustness but degrades clean performance; Figure 9 shows ~2x degradation from ε=0.01 to ε=0.08
  - **Attack type for training**: BIM/NIFGSM cheaper than CW; paper shows consistent robustness across attack types (Figs. 8, 11, 12)
  - **Standard vs. adversarial training**: Standard unfolding (LISTA) can *increase* vulnerability (Fig. 8 shows LISTA worse than ISTA); adversarial training essential for robustness

- Failure signatures:
  - Standard unfolded optimizer showing *higher* sensitivity than original iterative optimizer (LISTA > ISTA in Fig. 8): indicates parameters learned for performance alone can increase Lipschitz constant
  - Lipschitz constant >1.5× baseline after adversarial training (Fig. 15): suggests insufficient adversarial epochs or learning rate issues
  - Large gap between clean and adversarial performance (>10× difference in distortion): may indicate ε training value mismatched with deployment threat model
  - Convergence failure during training: step sizes μ_t may be too large; constrain ||M^t||₂ < 1

- First 3 experiments:
  1. **Reproduce vulnerability demonstration**: Implement ISTA for sparse recovery (Case Study I) with BIM attack at ε=0.025. Verify that input perturbation visually imperceptible (Fig. 2) causes output deviation ||s*_adv - s*||₂ > 0.5. This confirms the paper's core claim.
  2. **Compare standard vs. adversarial unfolding**: Train LISTA with T=5 using (a) standard ERM (Eq. 6) and (b) adversarial training (Eq. 14) with BIM at ε=0.025. Measure clean distortion and adversarial distortion. Expected: adversarial-trained version should achieve ~0.02 clean distortion vs ~0.07 for standard LISTA under attack (Fig. 8).
  3. **Lipschitz constant analysis**: Compute C(θ) for both trained models using Proposition 2 formula. Correlate with empirical robustness. This validates the theoretical mechanism and provides debugging signal for future architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the vulnerability of iterative optimizers and the effectiveness of unfolding-based mitigation extend to general non-convex optimization problems?
- Basis in paper: [inferred] The paper focuses primarily on convex inference rules (Section I) and a specific non-convex RPCA case, noting that convex methods are often used for non-convex setups via surrogate objectives, but does not generalize findings to all non-convex landscapes.
- Why unresolved: The theoretical proofs linking Lipschitz constants to learned parameters rely on the properties of proximal operators and convex regularizers, which may not hold for arbitrary non-convex functions.
- What evidence would resolve it: Empirical evaluation of adversarial sensitivity in unconstrained non-convex solvers or theoretical analysis of robustness in non-convex loss landscapes.

### Open Question 2
- Question: Can iterative optimizers maintain robustness against black-box attacks where the adversary lacks access to the optimizer's gradients?
- Basis in paper: [inferred] The study relies on white-box attacks (FGSM, BIM, CW) which require gradient access (Section II-B), while the discussion on "sophisticated jamming" (Section III-D) implies real-world threats may operate with limited information.
- Why unresolved: The paper demonstrates sensitivity to gradient-based perturbations but does not test transferability of attacks or robustness against decision-based/score-based attacks.
- What evidence would resolve it: Numerical experiments applying black-box attack algorithms (e.g., ZOO, boundary attack) on the trained optimizers.

### Open Question 3
- Question: Can inherent robustness be achieved through specific objective function design rather than adversarial hyperparameter training?
- Basis in paper: [explicit] Section III-D states that "iterative optimizers can possibly cope with adversarial attacks by proper selection of the objective used for the considered task," but the paper focuses on adversarial training.
- Why unresolved: The proposed mitigation strategy relies on altering hyperparameters via adversarial training (Eq. 14) rather than analyzing or modifying the structural form of the loss function $L_{op}$ itself.
- What evidence would resolve it: A comparative analysis showing that specific mathematical formulations of optimization objectives yield lower Lipschitz constants or naturally resist input perturbations without adversarial training.

## Limitations
- The theoretical Lipschitz bounds assume specific operator structures that may not generalize to all iterative optimizers
- Clean-data performance degradation from adversarial training (~0.01-0.02 normalized MSE) may be problematic for high-precision applications
- The relationship between Lipschitz continuity and robustness, while established in neural networks, requires validation for the broader class of iterative optimizers

## Confidence

- **Adversarial vulnerability of iterative optimizers:** High (strong empirical support across multiple attack types and case studies)
- **Lipschitz continuity as the governing mechanism:** Medium (theoretical derivation complete but assumptions narrow)
- **Adversarial training as effective mitigation:** High (consistent results across sparse recovery, RPCA, and beamforming)

## Next Checks

1. Test the Lipschitz-based robustness framework on non-convex iterative optimizers (e.g., stochastic gradient descent variants) to assess generalizability
2. Implement hyperparameter sensitivity analysis for the unfolded optimizers to identify which parameters most strongly influence both performance and robustness
3. Evaluate the approach on a real-world signal processing task (e.g., wireless communication with actual channel data) to validate practical deployment considerations