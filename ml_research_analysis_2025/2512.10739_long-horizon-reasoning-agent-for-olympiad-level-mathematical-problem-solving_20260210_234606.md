---
ver: rpa2
title: Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving
arxiv_id: '2512.10739'
source_url: https://arxiv.org/abs/2512.10739
tags:
- reasoning
- points
- lemma
- step
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Intern-S1-MO extends large reasoning models' reasoning depth by
  introducing a hierarchical multi-agent framework that conducts multi-round reasoning
  using a compact lemma memory, enabling the exploration of reasoning spaces unconstrained
  by context length limits. By decomposing complex problems into sub-lemmas and maintaining
  a structured memory repository, it effectively breaks through the 64K context constraints
  for IMO-level math problems.
---

# Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving

## Quick Facts
- arXiv ID: 2512.10739
- Source URL: https://arxiv.org/abs/2512.10739
- Reference count: 40
- Key outcome: Achieves 26/35 points on non-geometry IMO2025 problems (silver medal level) and 102/126 points on CMO2025 (gold medal level) using hierarchical multi-agent reasoning framework

## Executive Summary
Intern-S1-MO introduces a hierarchical multi-agent framework that extends large reasoning models' reasoning depth by introducing lemma-based memory compression. The system decomposes complex mathematical problems into sub-lemmas and maintains a structured memory repository, enabling exploration of reasoning spaces unconstrained by context length limits. By employing theorem verification for intermediate lemmas and process verification for final proof completion, the framework achieves state-of-the-art performance on multiple Olympiad-level benchmarks including AIME2025 (96.6% pass@1) and HMMT2025 (95% pass@1), surpassing advanced models like Gemini 2.5 Pro.

## Method Summary
The method employs a multi-agent system built on Intern-S1 that conducts multi-round reasoning using a compact lemma memory. The system uses a reasoner agent to generate reasoning traces and candidate lemmas, a summarizer agent to extract validated lemmas into a structured repository, and theorem/process verifiers to certify intermediate lemmas and final proofs. The framework is trained using OREAL-H, an RL framework that employs lemma dependency graphs for credit assignment and conjugate reward modeling. The system operates within 64K context per round but achieves ~512K effective token exploration through lemma compression across up to 8 reasoning rounds.

## Key Results
- Achieves 26/35 points on non-geometry IMO2025 problems, matching silver medalist performance
- Obtains 102/126 points in official CMO2025 participation, reaching gold medal level
- Sets state-of-the-art results on AIME2025 (96.6% pass@1) and HMMT2025 (95% pass@1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lemma-based memory compression enables reasoning beyond single-context limits by storing only validated intermediate results.
- Mechanism: After each reasoning round, a summarizer agent extracts proven lemmas into a structured repository. Subsequent rounds receive the problem + accumulated lemma library rather than full reasoning history. This allows ~512K effective token exploration within 64K context per round.
- Core assumption: Intermediate reasoning contains sufficient redundancy that compression preserves critical logical structure.
- Evidence anchors:
  - [abstract] "By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages"
  - [section 3] "Such facts enable us to extract only the essential components that drive progress... storing them in a structured lemma library"
  - [corpus] Related work on step-by-step coding (SBSC) demonstrates multi-turn programmatic reasoning benefits, but lacks explicit memory compression mechanisms—weak comparative signal.
- Break condition: If lemma summarization loses critical logical dependencies or introduces errors, subsequent rounds will build on flawed foundations.

### Mechanism 2
- Claim: Hierarchical decomposition with theorem verification mitigates error propagation through confidence-weighted lemma acceptance.
- Mechanism: A theorem verifier performs n parallel verifications per lemma; the pass ratio becomes a confidence score. Low-confidence lemmas can be flagged or rejected before entering the memory repository.
- Core assumption: Lemma verification is tractable where full problem verification is not.
- Evidence anchors:
  - [abstract] "employs a theorem verifier to certify intermediate lemmas"
  - [section 3] "We address this by integrating a theorem verifier that uses parallel sampling to compute confidence scores for each lemma"
  - [corpus] No direct corpus evidence on theorem verification efficacy in multi-agent settings—gap in external validation.
- Break condition: If theorem verifier false-positive rate is high, erroneous lemmas propagate and corrupt downstream reasoning.

### Mechanism 3
- Claim: Process verifier feedback enables iterative solution refinement through explicit error localization.
- Mechanism: The process verifier identifies step indices containing logical fallacies. The policy model receives this feedback and revises the solution in a loop until verification passes or max iterations reached.
- Core assumption: Process-level feedback is more informative than binary outcome rewards for proof correction.
- Evidence anchors:
  - [abstract] "process verifier for final proof completion"
  - [section 3] "the verifier serves two main functions: (1) enhancing robustness through test time scaling... (2) providing high-quality feedback signals for iterative revision"
  - [corpus] Process-aware RL approaches (Prover-Verifier Games) are mentioned as related work but noted as not well-suited for complex reasoning scenarios—suggests this is an adaptation, not a proven transfer.
- Break condition: If process verifier incorrectly identifies error locations, revision loops may degrade rather than improve solutions.

## Foundational Learning

- Concept: **Hierarchical Markov Decision Processes (HMDP)**
  - Why needed here: The system models reasoning as alternating between high-level meta-actions (extract lemmas, commit answer) and low-level token generation. Understanding HMDP formulation is prerequisite for grasping the RL training objective.
  - Quick check question: Can you explain why per-round advantage estimation requires both a high-level critic and token-level policy gradient aggregation?

- Concept: **Beta-Bernoulli Conjugate Priors**
  - Why needed here: OREAL-H uses Bayesian reward modeling to denoise process verification signals. Understanding conjugate priors explains why the reward is P(p₁ > p₀) rather than raw pass rates.
  - Quick check question: Why does the paper use Beta(k+1, n-k+1) as the posterior rather than the maximum likelihood estimate k/n?

- Concept: **Lemma Dependency Graphs**
  - Why needed here: Credit assignment across reasoning rounds uses graph structure where lemma values propagate from terminal success nodes. This is the mechanism for progress-conditioned advantage computation.
  - Quick check question: In equation (5), why is lemma value defined recursively via successor lemmas rather than assigned directly?

## Architecture Onboarding

- Component map:
  Reasoner Agent -> Summarizer Agent -> Theorem Verifier -> Memory Update -> [Next Round] Reasoner (+ Memory) -> ... -> Process Verifier -> Revision Loop -> Final Answer

- Critical path: Reasoner → Summarizer → Theorem Verifier → Memory Update → [Next Round] Reasoner (+ Memory) → ... → Process Verifier → Revision Loop → Final Answer

- Design tradeoffs:
  - More verification rounds (n=4 in paper) improve signal fidelity but increase latency
  - Longer reasoning rounds (max 64K tokens) enable deeper exploration but risk premature conclusion bias
  - Assumption: The paper claims 8 rounds sufficient but acknowledges this is a budget constraint, not a theoretical limit

- Failure signatures:
  - Empty lemma extraction across multiple rounds (summarizer failing)
  - High variance in theorem verifier confidence scores (verification instability)
  - Process verifier flagging early steps repeatedly (systematic reasoning errors not caught by theorem verifier)

- First 3 experiments:
  1. **Ablation baseline**: Run single-round agent on a subset of problems (no multi-round, no verifiers) to establish floor performance—expected ~70-80% of full system per Table 2.
  2. **Memory isolation test**: Provide oracle lemmas from successful trajectories to fresh reasoner instances; measure whether performance recovers to verify memory mechanism is causal.
  3. **Verification stress test**: Introduce synthetic errors into lemma library; measure theorem verifier detection rate and downstream impact on final answer correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Intern-S1-MO's multi-round hierarchical reasoning framework be extended to effectively solve geometry problems, which currently require visual reasoning capabilities absent from the system?
- Basis in paper: [explicit] The authors state "For CNMO2025 and IMO2025, we only evaluate the non-geometric parts" and "We test it on the 5 non-geometry problems of IMO2025." All experimental evaluations explicitly exclude geometry.
- Why unresolved: The system relies on text-based lemma representations and lacks visual reasoning modules needed for geometric constructions and spatial reasoning integral to Olympiad geometry.
- What evidence would resolve it: Extending Intern-S1-MO with geometric formalization or diagram understanding capabilities and evaluating on full IMO2025 geometry problems.

### Open Question 2
- Question: How does the performance-efficiency tradeoff of Intern-S1-MO's multi-round exploration compare to simply scaling model context length, particularly as hardware advances enable longer contexts?
- Basis in paper: [inferred] The paper positions the framework as a solution to context limits ("breaking through the 64K context constraints"), but does not empirically compare against baseline models with equivalent token budgets using extended context windows.
- Why unresolved: Without controlled comparisons of token-matched experiments, it remains unclear whether architectural innovation or raw context scaling drives the observed gains.
- What evidence would resolve it: Ablation experiments comparing Intern-S1-MO at various inference budgets against single-round models with proportionally scaled context lengths on identical problem sets.

### Open Question 3
- Question: How effectively can the lemma dependency graph and conjugate reward modeling approaches transfer to domains beyond mathematical reasoning, such as formal verification, scientific discovery, or long-horizon planning?
- Basis in paper: [explicit] The conclusion states "We wish the work paves the way for future research that adopts LRMs for mathematical research," implicitly inviting domain transfer. The hierarchical MDP formulation is described generally.
- Why unresolved: The framework is specifically designed around mathematical lemmas, theorem verification, and process validation paradigms that may not directly map to other structured reasoning domains.
- What evidence would resolve it: Applying OREAL-H and the multi-agent lemma architecture to tasks like code verification, legal reasoning, or scientific hypothesis generation with domain-appropriate verification mechanisms.

## Limitations

- The system's performance claims are limited to non-geometry problems, with geometry excluded from all reported benchmarks
- Key mechanisms (lemma compression, verification signal quality) lack detailed ablation studies to quantify individual contributions
- The framework's generalizability to domains beyond mathematical reasoning remains untested

## Confidence

- **High confidence**: The empirical results on standard benchmarks (AIME2025, HMMT2025) are verifiable and represent clear state-of-the-art performance
- **Medium confidence**: The theoretical framework (OREAL-H, lemma dependency graphs, conjugate reward modeling) is mathematically sound but implementation details and hyperparameter sensitivity are not fully explored
- **Low confidence**: Claims about breaking through 64K context limits and achieving silver/gold medal performance are based on a single system evaluation without systematic ablation or comparison to alternative long-context approaches

## Next Checks

1. **Ablation of lemma memory**: Run the system with full reasoning trace retention instead of lemma compression on a subset of problems. Compare final performance to quantify the actual contribution of the memory mechanism versus simple context extension.

2. **Verification robustness test**: Systematically inject synthetic errors at different lemma stages and measure detection rates by the theorem verifier. Track downstream impact on final answer correctness to establish the verifier's practical utility.

3. **Cross-domain generalization**: Apply the Intern-S1-MO framework to a non-mathematical long-horizon reasoning task (e.g., multi-step scientific problem solving or complex code generation). Evaluate whether the hierarchical decomposition and lemma memory mechanisms transfer effectively.