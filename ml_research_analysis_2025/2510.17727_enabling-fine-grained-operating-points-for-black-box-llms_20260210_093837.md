---
ver: rpa2
title: Enabling Fine-Grained Operating Points for Black-Box LLMs
arxiv_id: '2510.17727'
source_url: https://arxiv.org/abs/2510.17727
tags:
- input
- probabilities
- output
- your
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Enabling Fine-Grained Operating Points for Black-Box LLMs

## Quick Facts
- arXiv ID: 2510.17727
- Source URL: https://arxiv.org/abs/2510.17727
- Authors: Ege Beyazit; KL Navaneet; Prashant Mathur; Roi Blanco; Vidit Bansal; Karim Bouyarmane
- Reference count: 40
- Key outcome: Noise-injection methods increase verbalized probability cardinality from 16-23 to 18,673+ while preserving PR/ROC curve quality

## Executive Summary
This paper addresses the fundamental limitation that black-box LLMs produce verbalized probabilities with extremely low cardinality (16-23 unique values), severely constraining the number of usable operating points on PR/ROC curves. The authors propose noise-injection methods that increase cardinality to near-dataset size while preserving classification performance. Their approach works entirely in the black-box setting without access to model internals, enabling precise operating point selection for practical applications requiring specific precision or recall constraints.

## Method Summary
The authors identify that LLM verbalized probabilities exhibit severe rounding bias, preferring values ending in "0" or "5", resulting in low cardinality outputs. They propose two main approaches: an unsupervised method that injects bounded uniform noise to increase cardinality while preserving ranking, and supervised variants that learn to calibrate outputs and inject appropriate noise simultaneously. The supervised approach uses an MLP to map verbalized probabilities and noise samples to calibrated outputs, with regularization encouraging higher cardinality. Both methods operate entirely on black-box outputs without requiring model access.

## Key Results
- Verbalized probabilities have cardinality of only 16-23 unique values across GPT-3.5, GPT-4, and Llama-2
- Ours-Unsup increases cardinality to 18,673 while maintaining comparable PRAUC (0.72 vs 0.73)
- Ours-Sup-1call achieves 8 percentage point PRAUC improvement over baseline on combined dataset
- The methods provide 10× efficiency improvement over sampling baselines while achieving similar granularity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit rounding bias in verbalized probability outputs, preferring values ending in "0" or "5"
- Mechanism: Training on human-generated data where humans prefer round numbers creates many-to-one mapping from internal probabilities to low-cardinality verbalized outputs
- Core assumption: Rounding behavior is an artifact of human-like expression patterns learned during training
- Evidence: LLMs choose "0" and "5" endings most frequently regardless of task; token probabilities show 41,671 unique values vs. 16-23 for verbalized

### Mechanism 2
- Claim: Adding continuous noise to verbalized probabilities increases output cardinality without degrading PR/ROC performance
- Mechanism: Bounded noise injection interpolates existing operating points while preserving ranking order
- Core assumption: Original verbalized probabilities maintain monotonic relationship with true class probabilities
- Evidence: Ours-Unsup achieves cardinality of 18,673 vs. 15 for baseline while maintaining comparable PRAUC

### Mechanism 3
- Claim: Supervised learning with noise injection jointly corrects miscalibration and increases cardinality
- Mechanism: MLP learns to transform (verbalized probability, noise sample) → calibrated probability with regularization encouraging noise
- Core assumption: Miscalibration patterns are consistent and learnable from verbalized probability alone
- Evidence: Ours-Sup-2call achieves 8 percentage point PRAUC improvement; ablations show noise injection is necessary for cardinality increase

## Foundational Learning

- Concept: **ROC/PR Curves and Operating Points**
  - Why needed here: Entire paper frames problem around selecting operating points on these curves to meet business constraints
  - Quick check: If you increase classification threshold from 0.5 to 0.8, what generally happens to precision and recall?

- Concept: **Black-Box LLM Setting**
  - Why needed here: Paper explicitly assumes no access to weights, logits, or token probabilities—only textual output
  - Quick check: Why can't you apply standard Platt scaling or temperature calibration directly to black-box LLM outputs?

- Concept: **Cardinality of a Distribution**
  - Why needed here: Core problem is verbalized probabilities have extremely low cardinality (16-23 unique values)
  - Quick check: If a model outputs only 10 unique probability values across 10,000 samples, how many distinct operating points can you select?

## Architecture Onboarding

- Component map: Input Instance → Black-box LLM → Verbalized Probability → Ours-Unsup or Ours-Sup → Noisy/Calibrated Prediction → PR/ROC Curve

- Critical path: Unsupervised variant (Ours-Unsup) is simplest entry point—requires no training, only computing upper bound for noise injection based on unique verbalized probabilities

- Design tradeoffs:
  - Unsupervised vs. Supervised: Unsupervised preserves ranking perfectly but cannot correct miscalibration; supervised can improve PRAUC but requires labels and tuning
  - Noise distribution: Uniform noise guarantees ranking preservation; Gaussian noise requires learned scale
  - Number of LLM calls: Ours-Sup-1call uses 1 call; Ours-Sup-2call uses 2 for 10× efficiency vs. sampling methods needing 20 calls

- Failure signatures:
  - Non-monotonic PR curves indicate original verbalized probabilities had severe ranking inversions
  - Significant PRAUC degradation suggests λ is too small or learning rate poorly tuned
  - Low cardinality after method indicates noise generation implementation bug

- First 3 experiments:
  1. Reproduce Table 1: Prompt target LLM with baseline prompt, collect verbalized probabilities, compute cardinality + operational granularity
  2. Implement Ours-Unsup: Add bounded uniform noise, verify cardinality increases to ~dataset size, PR curves interpolate without degradation
  3. Compare to sampling baseline: Run Sample-Prob (20 samples per instance) on subset, compare cardinality, granularity, and PRAUC vs. Ours-Sup-1call

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can modeling the noise variable $z$ as a prior for a generative model eliminate the tradeoff between operational granularity and classification performance?
- Basis in paper: Authors suggest future work involves "modeling z as a prior to a generative model, and fitting a classifier to the representations" to resolve tradeoff
- Why unresolved: Current additive noise approaches necessitate tradeoff; generative approach hypothesized to offer optimization without performance loss
- What evidence would resolve it: Empirical results comparing generative classifier performance against proposed MLP-based methods

### Open Question 2
- Question: How does the proposed noise-injection framework perform in multi-class classification settings?
- Basis in paper: Authors note they "limit our experiments to binary classification tasks" despite analysis suggesting extension is possible
- Why unresolved: Unclear if granularity improvements and calibration stability hold with multiple classes and overlapping decision boundaries
- What evidence would resolve it: Experiments applying methods to standard multi-class datasets and reporting operational granularity and AUC metrics

### Open Question 3
- Question: Can fine-tuning black-box LLMs natively generate high-cardinality scores, thereby overcoming "rounding bias" without post-hoc noise injection?
- Basis in paper: Authors suggest "with greater availability of fine-tuning capabilities... it might be possible to overcome both these limitations by training them to provide diverse verbalized scores"
- Why unresolved: Current methods rely on post-processing frozen outputs; unknown if rounding bias is fundamental constraint or training artifact
- What evidence would resolve it: Study where LLM is fine-tuned with loss function penalizing low-cardinality outputs, measuring resulting verbalized probability distribution

## Limitations

- The rounding bias hypothesis lacks direct corpus validation—observed preference for "0" and "5" endings is documented but origins remain theoretical
- All mechanisms assume original verbalized probabilities maintain reasonable monotonic relationships with true class probabilities
- Experiments validate performance across multiple datasets but only using three LLMs (GPT-3.5, GPT-4, Llama-2)
- Supervised variants require careful hyperparameter tuning (λ) with no systematic exploration of sensitivity across applications

## Confidence

- High Confidence: Verbalized probabilities have extremely low cardinality (16-23 unique values) is well-established and reproducible
- Medium Confidence: Supervised learning approach demonstrates consistent PRAUC improvements, though magnitude varies significantly by task
- Low Confidence: Specific hypothesis about rounding bias origins (human training data preference) lacks direct evidence

## Next Checks

1. **Mechanism Validation Study**: Conduct controlled experiment comparing verbalized probability cardinality when LLMs are trained on human-generated vs. synthetic data to test whether rounding behavior stems from human-like expression patterns

2. **Break Condition Analysis**: Systematically identify conditions under which noise interpolation degrades PR/ROC curves by testing on datasets known to produce highly non-monotonic curves with standard prompting

3. **Cross-Model Generalizability Test**: Apply method to diverse set of 10+ LLMs spanning different sizes, architectures, and training paradigms to quantify how cardinality increase and PRAUC improvement correlate with model characteristics