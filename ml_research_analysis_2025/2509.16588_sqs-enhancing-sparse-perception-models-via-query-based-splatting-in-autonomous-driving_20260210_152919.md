---
ver: rpa2
title: 'SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous
  Driving'
arxiv_id: '2509.16588'
source_url: https://arxiv.org/abs/2509.16588
tags:
- pre-training
- gaussian
- arxiv
- queries
- occupancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SQS, a novel query-based splatting pre-training
  framework for sparse perception models (SPMs) in autonomous driving. Unlike prior
  pre-training methods that rely on dense BEV or volumetric representations, SQS leverages
  3D Gaussian splatting to predict 3D Gaussians from sparse queries, enabling self-supervised
  learning through multi-view image and depth reconstruction.
---

# SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving

## Quick Facts
- arXiv ID: 2509.16588
- Source URL: https://arxiv.org/abs/2509.16588
- Reference count: 40
- Primary result: SQS improves mIoU by +1.3 and NDS by +1.0 on nuScenes benchmarks

## Executive Summary
This paper introduces SQS, a novel query-based splatting pre-training framework for sparse perception models (SPMs) in autonomous driving. Unlike prior pre-training methods that rely on dense BEV or volumetric representations, SQS leverages 3D Gaussian splatting to predict 3D Gaussians from sparse queries, enabling self-supervised learning through multi-view image and depth reconstruction. The pre-trained Gaussian queries are integrated into downstream SPMs via query interaction mechanisms, enhancing performance in tasks such as 3D object detection and semantic occupancy prediction. Extensive experiments on nuScenes benchmarks demonstrate that SQS significantly improves mIoU by +1.3 and NDS by +1.0, outperforming state-of-the-art pre-training approaches.

## Method Summary
SQS pre-trains sparse perception models by predicting 3D Gaussian parameters from multi-view images, then rendering these Gaussians to reconstruct both RGB images and depth maps. The framework uses a transformer decoder with deformable cross-attention to map image features to Gaussian parameters (position, scale, rotation, opacity, color). During fine-tuning, the pre-trained backbone is loaded and frozen, while task-specific queries interact with pre-trained Gaussian queries via k-NN and local attention to enhance downstream performance. The approach is validated on nuScenes for 3D object detection and semantic occupancy prediction tasks.

## Key Results
- SQS improves semantic occupancy prediction mIoU by +1.3 and object detection NDS by +1.0 on nuScenes
- Data efficiency gains: +3.7 mIoU improvement when using only 10% of fine-tuning data
- Depth supervision during pre-training is critical: depth-only improves mIoU (+2.1) while RGB-only hurts it (-3.0)

## Why This Works (Mechanism)

### Mechanism 1: Depth-Grounded Geometry Learning
The performance gains in occupancy prediction appear contingent on depth supervision during pre-training, suggesting that geometric grounding is more critical than texture reconstruction for SPMs. The model predicts 3D Gaussians (position, scale, rotation) which are rasterized into depth maps. By minimizing the L1 loss against LiDAR depth maps, the sparse queries are forced to learn accurate 3D spatial distributions.

### Mechanism 2: Query Interaction as Geometric Memory
Downstream performance likely improves because the Query Interaction module allows task-specific queries to "attend" to pre-trained geometric anchors, effectively functioning as a retrieval-augmented generation step. During fine-tuning, the pre-trained model is frozen. Task queries use k-NN based on 3D position to find the most relevant pre-trained Gaussian queries and aggregate features via Local Attention.

### Mechanism 3: Data Efficiency via Sparse Representation
The framework likely enhances data efficiency by learning generalizable geometric priors (surfaces, shapes) from unlabeled data, reducing the burden on the downstream task to learn these from scratch. By pre-training on the full dataset without annotations, the model learns a "universal" set of 3D Gaussians. Fine-tuning on smaller subsets then only requires adapting these universal shapes to specific semantic classes.

## Foundational Learning

**Concept: 3D Gaussian Splatting (3DGS)**
- Why needed here: SQS replaces dense BEV grids with 3D Gaussians. You must understand that a "Gaussian" here is a differentiable volumetric primitive defined by position (μ), scale (S), rotation (R), and opacity (α).
- Quick check question: Can you explain why 3DGS is generally faster to render than NeRF? (Hint: Rasterization vs Ray-marching).

**Concept: Deformable/Cross Attention**
- Why needed here: The core of the "Sparse" architecture. The queries do not see the whole image; they sample features at specific 2D projections of their 3D reference points.
- Quick check question: If a 3D reference point is occluded in one view, how does the attention mechanism handle it? (Ideally, it should look at other views or rely on multi-view aggregation).

**Concept: Sparse Convolution**
- Why needed here: The paper mentions using 3D sparse convolution across Gaussian queries to reduce memory cost.
- Quick check question: Why is standard 3D convolution inefficient for sparse points (or Gaussians) in autonomous driving scenes?

## Architecture Onboarding

**Component map:** ResNet + FPN (Multi-scale image features) -> Gaussian Transformer Decoder -> 3D Gaussian Parameters -> Splatting Renderer -> Query Interaction (k-NN + Local Attention) -> Task Decoder

**Critical path:**
1. Initialization: Gaussian Queries (Qg) are initialized as learnable anchors
2. Pre-training: Qg interact with image features to predict Gaussian params; Loss is L1(Depth) + L1(RGB)
3. Fine-tuning: Load backbone weights. For every task query qt, find k nearest qg (from frozen pre-trained model). Update qt via Local Attention

**Design tradeoffs:**
- Query Count (K): Higher K (25,600 in paper) improves reconstruction detail but increases memory/compute during the interaction step
- Interaction Overhead: The paper explicitly lists "extra computation burden" as a limitation, as the frozen pre-trained model must run inference alongside the downstream model

**Failure signatures:**
- RGB Overfitting: Excessive reliance on RGB loss (ignoring depth) leads to negative transfer
- Opacity Collapse: If the opacity threshold α_thresh is set too low, the k-NN may pull features from "ghost" Gaussians that don't represent physical surfaces

**First 3 experiments:**
1. Depth vs. RGB Ablation: Re-run pre-training with only Depth loss vs. only RGB loss on a small split to verify the geometric prior hypothesis locally
2. k-NN Sensitivity: Vary the number of neighbors k in the Query Interaction module to find the sweet spot between local geometric context and noise
3. Rendering Visualization: Before fine-tuning, visualize the rendered depth maps from the pre-trained Gaussians to ensure they capture scene structure and not just noise

## Open Questions the Paper Calls Out
1. How can explicit semantic information be integrated during the pre-training stage to better distinguish pre-trained Gaussian queries for diverse downstream tasks?
2. How can the computational overhead and memory consumption of the plug-in query interaction module be reduced for real-time application?
3. Can SQS be effectively adapted to enhance query-based end-to-end autonomous driving models such as SparseAD or GaussianAD?

## Limitations
- The exact MLP architectures for Gaussian heads and query interaction hyperparameters are not specified, limiting reproducibility
- Performance claims depend on high-quality LiDAR depth maps; sparse or noisy depth coverage could degrade geometric learning
- The query interaction mechanism adds computational overhead during inference, which is not addressed for real-time deployment

## Confidence
- High confidence: The core mechanism of learning 3D Gaussian queries from multi-view depth via splatting is technically sound and supported by the ablation showing depth supervision is essential
- Medium confidence: The downstream gains in mIoU (+1.3) and NDS (+1.0) are plausible given the architecture, but depend on specific choices of query interaction hyperparameters which are not fully specified
- Low confidence: The exact reproducibility of the reported results is limited by missing architectural and hyperparameter details, particularly around the MLP designs and the query interaction module

## Next Checks
1. Depth supervision ablation: Re-run pre-training on a small nuScenes split with only depth loss vs. only RGB loss to verify the geometric prior hypothesis and quantify its impact on occupancy prediction
2. Query interaction sensitivity: Systematically vary k (k-NN neighbor count) and α_thresh (opacity filter) in the fine-tuning phase to identify the optimal settings for mIoU/NDS and assess the robustness of the interaction module
3. Rendering validation: Before fine-tuning, visualize the rendered depth maps from the pre-trained Gaussians against LiDAR ground truth to confirm they capture meaningful scene structure and are not overfitting to noise