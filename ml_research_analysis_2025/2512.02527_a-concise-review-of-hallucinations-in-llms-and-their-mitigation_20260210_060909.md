---
ver: rpa2
title: A Concise Review of Hallucinations in LLMs and their Mitigation
arxiv_id: '2512.02527'
source_url: https://arxiv.org/abs/2512.02527
tags:
- hallucinations
- llms
- hallucination
- language
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review provides a concise overview of hallucinations in large
  language models (LLMs), their causes, and mitigation strategies. Hallucinations
  are defined as model-generated outputs that lack factual grounding, such as false
  statements or fabricated references.
---

# A Concise Review of Hallucinations in LLMs and their Mitigation

## Quick Facts
- arXiv ID: 2512.02527
- Source URL: https://arxiv.org/abs/2512.02527
- Reference count: 37
- Primary result: This review provides a concise overview of hallucinations in large language models (LLMs), their causes, and mitigation strategies.

## Executive Summary
This review examines hallucinations in large language models (LLMs)—outputs that lack factual grounding such as false statements or fabricated references. The paper categorizes existing approaches into detection, fine-tuning, knowledge integration, and user-centered methods. Detection techniques include black-box hallucination identifiers and early detection systems. Mitigation strategies involve domain-specific fine-tuning, retrieval-augmented generation (RAG), and reinforcement learning from human feedback (RLHF). Evaluation benchmarks like TruthfulQA and FaithDial are highlighted. The review concludes that while complete elimination of hallucinations is unlikely, a combination of these strategies can significantly improve model reliability.

## Method Summary
The paper synthesizes existing research on LLM hallucinations and their mitigation without conducting new experiments. It reviews detection approaches (black-box hallucination identifiers and early detection systems), mitigation strategies (domain-specific fine-tuning, retrieval-augmented generation, RLHF, and formal methods), and evaluation benchmarks (TruthfulQA for veracity, FaithDial for dialogue consistency). The review draws from 37 references spanning detection techniques, fine-tuning approaches, knowledge integration methods, and user-centered strategies.

## Key Results
- Hallucinations are defined as model-generated outputs lacking factual grounding, including false statements, logical inconsistencies, and fabricated references
- Four main mitigation strategies are identified: detection methods, fine-tuning with domain-specific data, knowledge integration via RAG, and user-centered approaches including formal methods
- Evaluation benchmarks like TruthfulQA and FaithDial provide standardized ways to measure hallucination reduction
- Complete elimination of hallucinations is unlikely, but a combination of strategies can significantly improve model reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) can reduce hallucinations by grounding outputs in external validated sources.
- Mechanism: RAG retrieves relevant documents from external knowledge bases at inference time and conditions generation on this retrieved context, providing factual anchors that constrain the model's output space.
- Core assumption: The retrieved documents are themselves accurate and relevant to the query.
- Evidence anchors:
  - [abstract] "mitigation strategies including... retrieval-augmented generation"
  - [section V] "Incorporating or attaching external knowledge bases in the model via RAG technique makes it less prone to hallucinations by ensuring that all of the model's replies are backed up by some credible sources."
  - [corpus] Related work "Retrieval Augmentation Reduces Hallucination in Conversation" (Shuster et al., 2021) is cited in references.
- Break condition: When retrieval fails to find relevant documents, or when retrieved documents contain conflicting or outdated information.

### Mechanism 2
- Claim: Reinforcement Learning from Human Feedback (RLHF) can align model outputs toward truthfulness by learning from human preferences.
- Mechanism: Humans rate model outputs on factual accuracy; a reward model learns these preferences; the LLM is optimized via reinforcement learning to maximize this reward, internalizing patterns associated with truthful responses.
- Core assumption: Human annotators can reliably identify hallucinations and their preferences generalize to unseen queries.
- Evidence anchors:
  - [section V] Table I shows RLHF-based methods with metrics: "RLHF... Improved factual behavior" and "RLHF-V... 34.8% hallucination reduction"
  - [section V] "RLHF has found its application in improving the alignment of large language models (LLMs) towards human preferences... fine-tuning of RL models is pertinent as it assists in democratizing the generative actions of the models in line with what is supposed to be output"
- Break condition: If reward hacking occurs (model learns to sound truthful without being truthful), or if human feedback is inconsistent/noisy.

### Mechanism 3
- Claim: Domain-specific fine-tuning on curated datasets can reduce hallucinations by narrowing the model's knowledge scope to areas where training data is high-quality.
- Mechanism: Fine-tuning on domain-specific, factually-verified data deepens accuracy within that domain while reducing the model's tendency to generate plausible-but-wrong information from sparse training signals.
- Core assumption: The fine-tuning dataset is factually accurate and representative of the target domain's query distribution.
- Evidence anchors:
  - [abstract] "fine-tuning with domain-specific data" listed as mitigation strategy
  - [section V] "fine-tuning in which LLMs undergo training with a narrow corpus of data and enables the models to produce outputs that are more consistent with facts"
  - [section VII] "With specialized knowledge (e.g. medical or financial domains), hallucinations can be further controlled by training LLaMA and other models using domain-specific datasets"
- Break condition: If fine-tuning data contains hallucinations or biases, these are amplified.

## Foundational Learning

- Concept: **Parametric vs. Non-parametric Knowledge**
  - Why needed here: Hallucinations arise from models relying purely on parametric knowledge (weights) which can be incomplete or contaminated. Understanding this distinction is essential for grasping why RAG and external knowledge integration help.
  - Quick check question: Can you explain why an LLM might confidently state a falsehood about a recent event, and how retrieval augmentation would change this?

- Concept: **Distribution Shift and Out-of-Distribution Generalization**
  - Why needed here: Hallucinations often occur when inputs fall outside the training distribution or when the model encounters ambiguous prompts. Recognizing this helps in designing robust prompts and detection systems.
  - Quick check question: If a model was trained on data from 2022, what type of hallucination risk increases when queried about 2024 events?

- Concept: **Reward Modeling and RLHF Pipeline**
  - Why needed here: RLHF is a core mitigation strategy discussed. Understanding the feedback loop (collect preferences → train reward model → optimize policy) is necessary to evaluate its effectiveness and limitations.
  - Quick check question: In RLHF, what could go wrong if the reward model learns to prefer confident-sounding answers over correct ones?

## Architecture Onboarding

- Component map:
```
┌─────────────────────────────────────────────────────────────┐
│                    HALLUCINATION STACK                       │
├─────────────────────────────────────────────────────────────┤
│  INPUT LAYER                                                 │
│  ├── User prompt (may be ambiguous → hallucination trigger) │
│  └── Context/documents (if RAG-enabled)                     │
├─────────────────────────────────────────────────────────────┤
│  MODEL CORE                                                  │
│  ├── Pre-trained weights (parametric knowledge source)      │
│  ├── Fine-tuning adapter (domain-specific adjustments)      │
│  └── RLHF alignment layer (preference-optimized behavior)   │
├─────────────────────────────────────────────────────────────┤
│  OUTPUT LAYER                                                │
│  ├── Generated text                                         │
│  └── Post-processing/detection (HILL, SelfCheckGPT, etc.)   │
└─────────────────────────────────────────────────────────────┘
```

- Critical path: Prompt design → (optional) retrieval augmentation → model inference → hallucination detection → output filtering. Failures can originate at any stage, but detection typically happens post-hoc.

- Design tradeoffs:
  - **RAG vs. pure fine-tuning**: RAG provides updatable knowledge but adds latency and retrieval complexity. Fine-tuning is faster at inference but knowledge is frozen and may become stale.
  - **RLHF intensity**: Stronger alignment reduces hallucinations but may reduce model capability/creativity.
  - **Detection strictness**: Aggressive hallucination filtering may block correct but unusual outputs; permissive filtering lets hallucinations through.

- Failure signatures:
  - **Factual hallucination**: Model generates specific, confident claims that are verifiably false (e.g., fabricated citations, wrong dates).
  - **Logical inconsistency**: Model output contradicts itself within the same response.
  - **Fabrication in structured tasks**: Made-up entities, non-existent API references, invented legal precedents.

- First 3 experiments:
  1. **Baseline hallucination rate measurement**: Run your target model on TruthfulQA or domain-specific factual queries. Measure base hallucination rate before any mitigation.
  2. **RAG integration test**: Implement basic retrieval augmentation using a verified knowledge base (e.g., company documentation, medical guidelines). Compare hallucination rates on the same query set against baseline.
  3. **Prompt engineering A/B test**: Design two prompt templates—one with explicit factuality constraints ("Only state information you are certain about; say 'I don't know' if uncertain") and one without. Measure differences in hallucination frequency and refusal rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hallucination detection be performed effectively in real-time during model inference without degrading latency?
- Basis in paper: [explicit] The authors explicitly call for the development of "real-time systems for the detection of hallucination" capable of identifying and correcting outputs during inference rather than post-hoc.
- Why unresolved: Current detection methods often require significant computation or external verification, which hinders their application in interactive, high-speed environments like chatbots.
- What evidence would resolve it: A framework capable of flagging factual inconsistencies on the fly with negligible impact on tokens-per-second generation speed.

### Open Question 2
- Question: To what extent can formal methods provide verifiable reliability guarantees for LLMs in safety-critical domains?
- Basis in paper: [explicit] The paper identifies a "crucial need to conduct more studies about hallucination mitigation using formal methods" to offer model reliability guarantees in high-stakes scenarios.
- Why unresolved: Translating the probabilistic, open-ended nature of LLMs into the rigid logic required for formal verification remains a theoretical and engineering challenge.
- What evidence would resolve it: Demonstration of a logic-based prompting or architecture constraint that mathematically ensures output validity for specific logical tasks.

### Open Question 3
- Question: Does fine-tuning LLMs on new knowledge inevitably increase hallucination rates due to conflicts with internal parametric knowledge?
- Basis in paper: [inferred] While reviewing fine-tuning, the text highlights [26], posing the problem of whether "new knowledge made available... might not create hallucinatory effects" if it conflicts with the model's existing data.
- Why unresolved: The trade-off between updating a model's knowledge base and maintaining its internal consistency is not fully understood.
- What evidence would resolve it: Comparative studies measuring hallucination frequency in models fine-tuned on conflicting versus complementary datasets.

## Limitations

- The review synthesizes existing research but lacks empirical validation of its own claims
- Specific effectiveness metrics for mitigation strategies are rarely quantified within the paper itself
- The paper does not address computational costs, latency implications, or scalability challenges of RAG and RLHF approaches
- Does not explore adversarial scenarios where models might deliberately exploit mitigation mechanisms

## Confidence

- **High Confidence**: The categorization of hallucination types and the general taxonomy of mitigation strategies are well-supported by the literature cited.
- **Medium Confidence**: Claims about RLHF effectiveness and RAG grounding are plausible based on related work, but the paper provides limited direct evidence for these mechanisms in isolation.
- **Low Confidence**: Specific performance improvements (e.g., exact hallucination reduction percentages) are not independently verified within the review.

## Next Checks

1. **Reproduce RAG grounding effectiveness**: Implement RAG with a verified knowledge base and measure hallucination reduction on domain-specific queries compared to a baseline model.
2. **Evaluate RLHF alignment fidelity**: Fine-tune a model using RLHF on human preference data focused on factual correctness, then test for reward hacking or overfitting by probing edge cases.
3. **Benchmark detection robustness**: Test multiple hallucination detection tools (e.g., HILL, SelfCheckGPT) on a curated dataset of known hallucinations to assess precision and recall trade-offs.