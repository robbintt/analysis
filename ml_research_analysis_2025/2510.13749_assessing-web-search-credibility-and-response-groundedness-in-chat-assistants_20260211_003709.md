---
ver: rpa2
title: Assessing Web Search Credibility and Response Groundedness in Chat Assistants
arxiv_id: '2510.13749'
source_url: https://arxiv.org/abs/2510.13749
tags:
- sources
- chat
- assistants
- claims
- credibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a methodology for evaluating chat assistants\u2019\
  \ web search behavior in fact-checking contexts, focusing on source credibility\
  \ and response groundedness. Using 100 claims across five misinformation-prone topics,\
  \ the study assesses GPT-4o, GPT-5, Perplexity, and Qwen Chat."
---

# Assessing Web Search Credibility and Response Groundedness in Chat Assistants

## Quick Facts
- arXiv ID: 2510.13749
- Source URL: https://arxiv.org/abs/2510.13749
- Reference count: 40
- Primary result: Introduces methodology to evaluate chat assistants' web search behavior, distinguishing between overall and credible grounding

## Executive Summary
This paper introduces a methodology for evaluating chat assistants' web search behavior in fact-checking contexts, focusing on source credibility and response groundedness. Using 100 claims across five misinformation-prone topics, the study assesses GPT-4o, GPT-5, Perplexity, and Qwen Chat. Perplexity achieved the highest source credibility (86% credible sources), while GPT-4o showed elevated citation of non-credible sources (4.55% on Russia-Ukraine War topics). All assistants demonstrated high overall groundedness, but credible grounding varied significantly. The methodology distinguishes between overall and credible grounding, revealing that assistants can appear well-grounded while relying on low-credibility evidence.

## Method Summary
The methodology evaluates web-search-enabled chat assistants using 100 claims across five topics (Health, Climate, U.S. Politics, Local, Russia-Ukraine) sourced from fact-checking databases. Claims are paraphrased and tested with two user roles: Fact-Checker seeking verification and Claim Believer seeking confirmation of false information. Responses are collected via Selenium automation, with source credibility classified using MBFC ratings and fact-checking organization lists. Groundedness is evaluated using a VERIFY-adapted pipeline that segments responses into atomic units (including a new "Reported Claim" type), decontextualizes them, retrieves evidence chunks from cited sources, and judges support using Llama 3.3 70B. Scores are computed separately for overall, credible, and non-credible grounding.

## Key Results
- Perplexity achieved the highest source credibility (86% credible sources)
- GPT-4o showed elevated citation of non-credible sources (4.55% on Russia-Ukraine War topics)
- All assistants demonstrated high overall groundedness, but credible groundedness varied significantly
- Qwen Chat showed fragile behavior when citing unreliable sources, with high within-response reliance on non-credible evidence
- The methodology revealed that assistants can appear well-grounded while relying on low-credibility evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-framing user simulation exposes assistant vulnerability to query presuppositions.
- Mechanism: By presenting identical claims under a skeptical "Fact-Checker" and a credulous "Claim Believer" framing, the methodology measures whether query phrasing influences source selection. Fact-checker prompts elicit verification behavior; claim-believer prompts embed false presuppositions that may bias retrieval toward confirmatory, lower-credibility sources.
- Core assumption: Assistants' retrieval and citation behavior is sensitive to user intent signals in prompts.
- Evidence anchors: [abstract] "each claim is tested against the assistants by modeling two distinct user roles: a fact-checker, seeking to verify the claim, and a claim believer, seeking confirmation of false information." [Section 3.2] "This distinction is important since prior work shows that queries with false presuppositions make AI systems more likely to accept misinformation."
- Break condition: If assistants ignore presuppositions and retrieve the same sources regardless of framing, the contrast between user roles disappears.

### Mechanism 2
- Claim: Credibility-weighted groundedness reveals a false-sense-of-reliability problem masked by standard groundedness scores.
- Mechanism: Standard groundedness measures whether statements are supported by any cited source. This methodology separates credible groundedness (supported by high-credibility sources) from non-credible groundedness (supported by low-credibility sources). A model can appear highly grounded while relying primarily on unreliable evidence.
- Core assumption: Source credibility can be reliably categorized using external databases (MBFC, fact-checking organization lists).
- Evidence anchors: [abstract] "The methodology distinguishes between overall and credible grounding, revealing that assistants can appear well-grounded while relying on low-credibility evidence." [Section 5.1] "A key distinction between our method and VERIFY lies in how we incorporate source credibility... we assess whether the evidence comes from credible or unverifiable sources using MBFC ratings."
- Break condition: If source credibility ratings are inconsistent or incomplete (many unrated domains), the credible/non-credible distinction becomes noisy or uninformative.

### Mechanism 3
- Claim: Atomic unit decomposition with source-aware verification isolates specific claims tied to specific evidence quality.
- Mechanism: Responses are segmented into atomic units (facts, claims, reported claims), decontextualized into self-contained statements, matched to evidence chunks from cited sources, and judged as supported, contradicted, or unverifiable. Critically, each unit is evaluated against credible and non-credible sources separately.
- Core assumption: Atomic units can be accurately extracted, decontextualized, and matched to evidence using LLM-based components without introducing systematic bias.
- Evidence anchors: [Section 5.1] "Our approach differs in two key aspects. First, we constrain evaluation strictly to the sources that the assistants explicitly cite... Second, we incorporate source credibility into the evaluation." [Section 5.1] "We introduce a new unit type, Reported Claim, capturing statements that attribute information to external sources."
- Break condition: If the extraction or judgment model systematically over- or under-supports units, or if reported claims are misclassified, groundedness scores become artifacts of the evaluation model rather than the assistant's behavior.

## Foundational Learning

- Concept: Groundedness vs. Factuality
  - Why needed here: Groundedness measures whether a response is supported by cited evidence, not whether it is true. An assistant can be perfectly grounded in a false or unreliable source.
  - Quick check question: If an assistant cites a conspiracy blog and accurately summarizes its claims, is it grounded? Is it factual?

- Concept: Source Credibility Rating Systems (e.g., MBFC)
  - Why needed here: The methodology relies on external credibility ratings to classify sources. Understanding how these ratings are constructed, their coverage gaps, and their limitations is essential for interpreting results.
  - Quick check question: What happens to the analysis if a cited domain has no MBFC rating?

- Concept: False Presuppositions in Queries
  - Why needed here: The claim-believer framing tests whether assistants are susceptible to queries that presuppose false information. Prior work shows this can bias retrieval.
  - Quick check question: Does asking "Why did X happen?" when X never happened make an assistant more likely to hallucinate or retrieve poor sources?

## Architecture Onboarding

- Component map: Claim Curation -> Prompt Templates -> Response Collection -> Source Credibility Analysis -> Groundedness Evaluation
- Critical path: Claim curation → Prompting → Response collection → Citation extraction → Domain classification → Unit decomposition → Evidence matching → Credibility-aware judgment → Score aggregation
- Design tradeoffs:
  - Web interface vs. API: Uses real user interfaces to capture deployed behavior, but introduces variability and scraping complexity
  - Constrained evidence (cited sources only) vs. open retrieval: More faithful to what the assistant actually used, but may miss contradictions from uncited high-quality sources
  - LLM-as-judge vs. human evaluation: Scalable but may inherit model biases; reported-claim category added to reduce misclassification
- Failure signatures:
  - High unrated domains: Indicates source coverage gaps; undermines credible grounding metric
  - Large gap between overall and credible groundedness: Signals that the assistant is grounding in unreliable sources
  - Fragile grounding (Qwen pattern): Low average non-credible grounding but high within-response reliance when any non-credible source appears—suggests cascade failures
  - Topic-specific spikes in non-credibility (e.g., Russia-Ukraine for GPT-4o): Indicates domain-specific retrieval vulnerabilities
- First 3 experiments:
  1. Replicate with different claim sets: Test whether credibility and grounding patterns hold across new claims within the same topics and across additional topics (e.g., financial misinformation)
  2. Ablate the reported-claim label: Run the groundedness pipeline with and without the new unit type to quantify its impact on hallucination scores and supported-claim rates
  3. Vary the credibility rating source: Replace or augment MBFC with alternative ratings (e.g., NewsGuard) to assess sensitivity of results to the credibility classification system

## Open Questions the Paper Calls Out

- Does the systematic activation of reasoning or "thinking" modes in LLMs consistently reduce the citation of non-credible sources across different misinformation topics?
  - Basis in paper: The authors observe that GPT-5's thinking mode improved credibility (raising CR from 69.8% to 85.8%) but state that a "systematic investigation of how reasoning capabilities interact with web search remains for future research" as the mode was only automatically triggered in 10% of cases.
  - Why unresolved: The study did not manually control the "thinking" setting; the improvement was observational based on the model's autonomous decision to activate the mode, leaving the causal link between reasoning and source selection unconfirmed across all topics.
  - What evidence would resolve it: A controlled ablation study where the "thinking" feature is forcibly enabled versus disabled for the same set of claims, measuring the resulting delta in Non-Credibility Rates (NCR) and Credible Groundedness (CG).

- Do web-search-enabled assistants exhibit similar credibility and grounding performance when processing claims in non-English languages?
  - Basis in paper: The authors list "English-only Instructions" as a limitation, noting that "sources retrieved and cited by the assistants may appear in a variety of languages" and explicitly identify extending experiments to other languages as a "promising direction for future work."
  - Why unresolved: The current methodology relies on English prompts and Media Bias/Fact Check (MBFC) ratings which are skewed toward English domains; it is unknown if assistants retrieve credible local sources or hallucinate support in low-resource languages.
  - What evidence would resolve it: Replicating the 100-claim evaluation using translated claims in languages with different media landscapes (e.g., Slovak, Chinese) and analyzing the availability of credibility ratings for the cited local domains.

- How does source credibility change during multi-turn interactions with real users compared to single-turn template simulations?
  - Basis in paper: The authors acknowledge that automated templates "cannot capture the full diversity and nuance of human interactions, such as multi-turn conversations or follow-up clarifications," and identify large-scale user studies as an important future step.
  - Why unresolved: The study strictly used "Fact-Checker" and "Claim Believer" templates; it is unclear if a real user's follow-up challenges or requests for better sources would successfully steer the assistant away from low-credibility citations.
  - What evidence would resolve it: A user study measuring the "Credibility Rate" trajectory over 3-5 turn conversations where human participants actively verify or challenge the assistant's initial responses.

## Limitations

- The methodology relies on source credibility ratings from MBFC and fact-checking organization lists, which have coverage gaps (many domains unrated)
- The evaluation uses LLM-based components (unit extraction, decontextualization, judgment) which may inherit model biases
- The study is limited to English-language prompts and sources, though assistants may retrieve non-English content
- The dual-framing effect on source selection is theoretically grounded but not directly validated with corpus evidence

## Confidence

- High: Source credibility analysis reveals systematic differences between assistants (Perplexity highest CR, GPT-4o elevated NCR on Russia-Ukraine topics)
- Medium: Overall and credible groundedness distinction is methodologically sound and reveals a false-sense-of-reliability problem
- Low: Direct validation of dual-framing effect on source selection is absent from corpus; reported-claim refinement impact is unquantified

## Next Checks

1. Replicate with different claim sets: Test whether credibility and grounding patterns hold across new claims within the same topics and across additional topics (e.g., financial misinformation)
2. Ablate the reported-claim label: Run the groundedness pipeline with and without the new unit type to quantify its impact on hallucination scores and supported-claim rates
3. Vary the credibility rating source: Replace or augment MBFC with alternative ratings (e.g., NewsGuard) to assess sensitivity of results to the credibility classification system