---
ver: rpa2
title: Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning
  for Diffusion and Flow Models
arxiv_id: '2511.21122'
source_url: https://arxiv.org/abs/2511.21122
tags:
- pruning
- diffusion
- distribution
- flow
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EntPruner, an entropy-guided automatic progressive
  pruning framework for diffusion and flow models. The core innovation is Conditional
  Entropy Deviation (CED), a generative-specific metric that quantifies how much removing
  each block disrupts the learned conditional distribution by measuring changes in
  entropy.
---

# Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models

## Quick Facts
- arXiv ID: 2511.21122
- Source URL: https://arxiv.org/abs/2511.21122
- Reference count: 40
- Key outcome: Entropy-guided automatic progressive pruning framework for diffusion and flow models achieving up to 2.22× inference speedup while maintaining generation quality

## Executive Summary
This paper introduces EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. The core innovation is Conditional Entropy Deviation (CED), a generative-specific metric that quantifies how much removing each block disrupts the learned conditional distribution by measuring changes in entropy. Unlike discriminative pruning methods, CED captures both drift toward randomness and mode collapse, enabling importance ranking that preserves distributional quality and diversity. The framework employs zero-shot adaptive pruning that automatically determines when and how much to prune during training using NTK condition numbers and ZiCo scores, avoiding catastrophic forgetting from one-shot pruning.

## Method Summary
EntPruner introduces Conditional Entropy Deviation (CED) as a generative-specific metric for identifying critical layers during pruning. CED measures how removing each block affects the conditional distribution by analyzing entropy changes in the output distribution. The framework uses a zero-shot adaptive pruning approach that determines pruning schedules automatically based on training dynamics, employing Neural Tangent Kernel (NTK) condition numbers and ZiCo scores to guide the process. This avoids the catastrophic forgetting associated with one-shot pruning while maintaining generation quality through entropy-aware importance ranking.

## Key Results
- Achieved up to 2.22× inference speedup on DiT and SiT models
- Demonstrated 43.04% FID improvement on Flowers dataset
- Showed 48.16% improvement over LD-Pruner on ImageNet

## Why This Works (Mechanism)
EntPruner works by introducing Conditional Entropy Deviation (CED), which measures the distributional impact of removing each block through entropy changes. This metric captures both the drift toward randomness and potential mode collapse that occurs when critical layers are pruned. By using entropy as a proxy for distributional quality, CED can identify which layers are essential for maintaining the learned conditional distribution. The zero-shot adaptive pruning approach then uses NTK condition numbers and ZiCo scores to determine optimal pruning timing and extent, preventing catastrophic forgetting while preserving generation quality.

## Foundational Learning

**Conditional Entropy**: Measures uncertainty in output given input - needed to quantify how block removal affects distribution; quick check: verify entropy increases when removing critical layers.

**Neural Tangent Kernel (NTK)**: Tracks model trainability during pruning - needed to prevent catastrophic forgetting; quick check: monitor NTK condition numbers for stability during pruning.

**ZiCo Score**: Measures convergence quality - needed to ensure pruning doesn't degrade final performance; quick check: validate ZiCo scores remain stable after pruning.

**Entropy-based Metrics**: Alternative to traditional weight-based importance measures - needed for distributional preservation; quick check: compare CED scores against weight magnitude rankings.

**Progressive vs One-shot Pruning**: Iterative approach prevents catastrophic forgetting - needed for stable training; quick check: monitor performance degradation between pruning stages.

## Architecture Onboarding

**Component Map**: Input Data -> CED Scoring -> NTK/ZiCo Voting -> Pruning Decision -> Output Data

**Critical Path**: CED calculation → NTK condition monitoring → ZiCo score evaluation → Pruning execution

**Design Tradeoffs**: CED provides distributional awareness but requires entropy computation overhead; adaptive pruning avoids catastrophic forgetting but needs dynamic threshold tuning

**Failure Signatures**: Performance degradation indicates incorrect CED scoring; training instability suggests premature pruning; mode collapse reveals insufficient preservation of critical layers

**First Experiments**:
1. Apply CED to a single DiT block and measure entropy change
2. Test NTK condition number sensitivity to different pruning rates
3. Validate ZiCo score correlation with final generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Conditional Entropy Deviation (CED) be effectively adapted for U-Net-based diffusion architectures, or is it inherently dependent on the sequential block structure of Vision Transformers?
- Basis in paper: [explicit] The Conclusion states: "Future work could explore extending CED to other generative architectures beyond transformer-based diffusion models."
- Why unresolved: The current validation is limited to DiT and SiT (Transformer backbones). U-Nets utilize skip connections and distinct encoder-decoder stages, which may fundamentally alter how entropy deviation propagates when a block is removed.
- What evidence would resolve it: Empirical results applying EntPruner to U-Net backbones (e.g., Stable Diffusion) and comparing the correlation between CED scores and generation quality degradation.

### Open Question 2
- Question: To what extent does the assumption that the output distribution $p(x)$ is Gaussian limit the accuracy of CED for complex, multi-modal generation tasks?
- Basis in paper: [inferred] Section 3.3 states: "For tractability, we assume that p(x) follows a Gaussian distribution." This simplifies the entropy calculation but may misrepresent the true distribution geometry.
- Why unresolved: Real-world generative distributions are often complex and multi-modal. A Gaussian approximation might underestimate distribution deviation in modes with heavy tails or complex correlations, leading to the accidental pruning of critical blocks.
- What evidence would resolve it: Ablation studies comparing the current CED formulation against non-parametric entropy estimators on datasets with high distributional complexity.

### Open Question 3
- Question: Does the equal weighting of NTK condition numbers ($K_\kappa$) and ZiCo scores ($K_{ZiCo}$) in the voting mechanism provide the optimal balance for determining the pruning schedule?
- Basis in paper: [inferred] Section 3.5 notes: "We assume both proxies are equally important... A common strategy is to apply a voting-based algorithm."
- Why unresolved: Treating trainability (NTK) and convergence (ZiCo) as equal contributors is a heuristic design choice. It is possible that one metric is significantly more predictive of final performance than the other, or that the optimal weighting changes as pruning progresses.
- What evidence would resolve it: A sensitivity analysis evaluating different weighted combinations of these metrics against the final FID scores and convergence times.

## Limitations

- CED metric may have limited generalizability to extremely diverse generative tasks beyond tested image generation benchmarks
- Performance on high-resolution generation or video diffusion models remains unexplored
- Computational overhead of entropy calculations during training could become significant for very large models

## Confidence

- High: Effectiveness of CED in identifying critical blocks for distributional preservation
- Medium: Automatic determination of pruning thresholds using NTK-based condition numbers
- Medium: Zero-shot adaptive pruning mechanism effectiveness in avoiding catastrophic forgetting

## Next Checks

1. **Cross-domain generalization test**: Apply EntPruner to diffusion models for video generation and text-to-image tasks to validate CED's effectiveness beyond static image datasets.

2. **Scalability analysis**: Evaluate the framework's performance and computational overhead on state-of-the-art diffusion models with 1B+ parameters to assess practical deployment viability.

3. **Dynamic adaptation study**: Implement online monitoring of NTK condition numbers during training to validate the automatic threshold determination mechanism across varying learning rate schedules and optimization strategies.