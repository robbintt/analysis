---
ver: rpa2
title: Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity
  Recognition
arxiv_id: '2601.10043'
source_url: https://arxiv.org/abs/2601.10043
tags:
- instruction
- financial
- lora
- entity
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to financial named entity
  recognition (NER) using Llama 3 8B with instruction fine-tuning and LoRA. The method
  converts annotated sentences into instruction-input-output triples and fine-tunes
  only low-rank matrices, achieving a micro-F1 score of 0.894.
---

# Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition

## Quick Facts
- arXiv ID: 2601.10043
- Source URL: https://arxiv.org/abs/2601.10043
- Reference count: 0
- Primary result: 0.894 micro-F1 on financial NER using Llama 3 8B + LoRA

## Executive Summary
This paper presents a novel approach to financial named entity recognition (NER) using Llama 3 8B with instruction fine-tuning and LoRA. The method converts annotated sentences into instruction-input-output triples and fine-tunes only low-rank matrices, achieving a micro-F1 score of 0.894. This outperforms strong baselines including Qwen3-8B and Baichuan2-7B by about 6 percentage points. The approach demonstrates that combining instruction tuning with parameter-efficient fine-tuning enables state-of-the-art performance on domain-sensitive NER while maintaining computational efficiency.

## Method Summary
The method fine-tunes Llama 3 8B with LoRA adapters (rank 8, alpha 16, dropout 0) on financial NER data. Annotated sentences are converted into instruction-input-output triples using a fixed template, transforming the task into conditional generation. The model is trained for 3 epochs with AdamW optimizer (lr=5e-5), batch size 4, gradient accumulation 6, and bf16 precision. This parameter-efficient approach updates only low-rank matrices while freezing base weights, achieving strong performance on 7 financial entity types with limited training data (1,693 samples).

## Key Results
- Achieved 0.894 micro-F1 score on financial NER task
- Outperformed Qwen3-8B (0.833) and Baichuan2-7B (0.792) by ~6 percentage points
- Maintained computational efficiency by fine-tuning only low-rank matrices instead of full weights
- Demonstrated effectiveness on small dataset (1,693 samples) with class imbalance challenges

## Why This Works (Mechanism)

### Mechanism 1: Instruction Formatting for Task Disambiguation
Converting annotated NER data into instruction-input-output triples enables the model to learn task-following behavior rather than memorizing label patterns. Each sentence is paired with a fixed instruction prompt and JSON-style output, framing extraction as conditional generation. This encourages the model to interpret task semantics, not just surface patterns.

### Mechanism 2: LoRA for Efficient Domain Adaptation
Low-rank adaptation matrices inserted into attention projections can capture domain-specific transformations without updating the full weight set. LoRA freezes base weights and learns two small matrices (A Ã— B) per targeted layer; the low-rank constraint forces the update to capture only essential adaptations, reducing overfitting risk on small corpora.

### Mechanism 3: Llama 3 Architectural Foundations
Llama 3 8B's tokenizer scale (128K vocabulary) and grouped-query attention improve handling of financial terminology and inference efficiency. A larger vocabulary reduces subword fragmentation for domain-specific terms; GQA lowers memory overhead during inference without sacrificing expressiveness.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Understanding how rank-constrained updates work is essential to reason about what the model can and cannot learn during fine-tuning
  - Quick check question: If you increase LoRA rank from 8 to 32, what tradeoffs do you expect in memory, overfitting, and expressiveness?

- **Instruction Tuning Paradigm**
  - Why needed here: The method frames NER as conditional generation; you must understand how prompts shape model behavior
  - Quick check question: How does an instruction-tuned model differ in inference from a standard sequence-labeling BERT model?

- **Entity Recognition as Structured Generation**
  - Why needed here: Outputs are JSON-formatted entity lists; parsing and evaluation require understanding generative NER vs. token classification
  - Quick check question: What post-processing steps are needed to convert a generative JSON output into span-level annotations for evaluation?

## Architecture Onboarding

- **Component map:** Raw sentence -> Instruction template -> Tokenized input (max 400 tokens) -> JSON output
- **Critical path:** 1) Convert annotations to instruction-input-output triples using fixed template; 2) Initialize Llama 3 8B with frozen weights; attach LoRA adapters; 3) Fine-tune LoRA matrices only; monitor training loss convergence; 4) Evaluate using micro-averaged precision, recall, F1 across 7 entity types
- **Design tradeoffs:** Rank r=8 balances expressiveness with overfitting risk on 1,693 samples; LoRA dropout=0 preserves full adaptation capacity but may reduce regularization; 3 epochs chosen to avoid overfitting; bf16 precision speeds training but requires hardware support
- **Failure signatures:** Training loss plateaus early -> check learning rate or rank sufficiency; high precision, low recall -> model is conservative; consider threshold adjustment or data augmentation; poor performance on rare entity types -> may need class-balanced sampling or macro-F1 monitoring; JSON parsing errors at inference -> validate output format during training
- **First 3 experiments:** 1) Baseline sanity check: Run zero-shot and few-shot inference with unmodified Llama 3 8B to quantify the pre-fine-tuning gap; 2) Rank ablation: Compare r=4, 8, 16 on validation F1 to verify rank-8 is not under- or over-parameterized; 3) Per-entity error analysis: Compute per-class F1 to identify weak categories and inspect failure modes before scaling data or architecture

## Open Questions the Paper Calls Out

### Open Question 1
Does retrieval-augmented instruction tuning (RA-IT) improve financial NER performance when combined with LoRA adaptation? Authors state they "have not investigated retrieval-augmented techniques" and suggest "LoRA can be combined with RA-IT or dynamic adapters to achieve even better performance." The paper only evaluates standard instruction tuning without retrieving semantically similar examples to prepend to instructions.

### Open Question 2
How robust is the instruction-tuned model to out-of-distribution financial text, such as informal social media posts or bilingual documents? Authors acknowledge models "might have a hard time with out-of-distribution text, such as informal social media posts" and note they "ignore multilingual and cross-lingual cases." The training corpus contains only formal financial reports and news, with no evaluation on diverse text genres or languages.

### Open Question 3
How does the model perform on rare entity types when evaluated using macro-F1 instead of micro-averaged metrics? Authors note they "assess only on micro-averaged metrics; macro-F1 would be more punitive to errors on rare classes" such as Product (226 samples) and Location (256 samples). Micro-F1 dominates evaluation, potentially masking poor performance on underrepresented entity categories.

### Open Question 4
Can automated or diverse instruction template generation improve generalization compared to the handcrafted template used in this study? Authors state "our instruction template is handcrafted which means it will be less robust for the model to autonomously create diverse instructions." A single fixed prompt format may limit the model's ability to generalize across varied task phrasings.

## Limitations

- Dataset provenance and composition not specified, creating uncertainty about reproducibility
- Critical implementation details missing (specific LoRA target modules, exact train/test split ratio)
- Base model specification ambiguity (Llama 3 base vs. instruct variant not clarified)
- No ablation studies on architectural features or instruction template variations

## Confidence

**High confidence**: The micro-F1 score of 0.894 represents a genuine performance improvement over baseline models (Qwen3-8B at 0.833 and Baichuan2-7B at 0.792).

**Medium confidence**: The mechanism by which instruction formatting improves task generalization is theoretically sound but lacks direct empirical validation in this paper.

**Low confidence**: The assertion that Llama 3's architectural features are decisive factors in the performance gain is speculative without ablation studies.

## Next Checks

1. **Dataset accessibility validation**: Obtain or recreate the exact financial corpus used in the study to verify that the 1,693 sentences can be reproduced. If unavailable, construct a comparable dataset from financial news sources and measure performance delta.

2. **Rank sensitivity analysis**: Conduct controlled experiments varying LoRA rank from r=4 to r=32 while keeping all other hyperparameters constant. Measure micro-F1, training stability, and parameter efficiency to determine if r=8 represents an optimal tradeoff.

3. **Per-entity robustness testing**: Implement class-balanced sampling during fine-tuning and evaluate per-entity F1 scores for all seven entity types. Identify whether rare entities (Product, Location) consistently underperform and test whether weighted loss functions or synthetic data augmentation improve their recognition accuracy.