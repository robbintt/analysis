---
ver: rpa2
title: 'Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception'
arxiv_id: '2503.06866'
source_url: https://arxiv.org/abs/2503.06866
tags:
- task
- safety
- risk
- planning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating large language
  models (LLMs) with structured safety perception for robotic task planning in dynamic
  environments. Existing LLM-based planners struggle to incorporate real-time hazard
  detection, often relying on static rules or predefined constraints that fail in
  open-world scenarios.
---

# Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception

## Quick Facts
- arXiv ID: 2503.06866
- Source URL: https://arxiv.org/abs/2503.06866
- Authors: Wanjing Huang; Tongjie Pan; Yalan Ye
- Reference count: 36
- Key outcome: Achieves 91.39% recall in hazard detection with Graphormer-augmented LLM task planning, significantly outperforming static rule-based and LLM-only baselines in AI2-THOR environment

## Executive Summary
This paper introduces a Graphormer-enhanced risk-aware task planning framework that integrates large language models with structured safety perception for robotic task planning in dynamic environments. The proposed approach constructs a dynamic spatio-semantic safety graph to model environmental risks and enable adaptive task modifications, addressing the limitations of static rule-based safety systems that fail in open-world scenarios. The framework demonstrates strong performance in hazard detection and task completion while maintaining the flexibility of LLM-based planning.

## Method Summary
The proposed framework combines Graphormer models with LLM-based task planning to create a dynamic safety perception system for robotic task planning. The core innovation involves constructing a spatio-semantic safety graph that encodes environmental risks using Graphormer's attention mechanisms, which capture spatial relationships and contextual information about potential hazards. When the system detects high-risk interactions through the safety graph, it triggers LLM-based replanning to modify tasks and ensure safe execution. The approach is evaluated in the AI2-THOR simulation environment, where it demonstrates superior performance compared to static rule-based and pure LLM baselines in both safety detection and task success rates.

## Key Results
- Achieves 91.39% recall in hazard detection with 29.27% precision in AI2-THOR environment
- Maintains task success rates up to 92.5% while handling safety-critical situations
- Significantly outperforms static rule-based and LLM-only baselines in both safety perception and task completion metrics

## Why This Works (Mechanism)
The framework works by leveraging Graphormer's ability to encode complex spatial and contextual relationships into a structured graph representation. This allows the system to continuously update safety assessments during task execution rather than relying on static rules. The Graphormer model processes the spatio-semantic safety graph to identify potential hazards by analyzing the relationships between objects, their spatial configurations, and contextual factors. When a high-risk interaction is detected, the LLM component can reason about the situation and generate alternative task plans that avoid the identified hazards while maintaining task objectives.

## Foundational Learning
- Graphormer models: Needed for encoding spatial and contextual relationships in safety graphs; quick check involves verifying attention mechanism effectiveness on spatial data
- Spatio-semantic safety graphs: Required for dynamic hazard representation; quick check involves validating graph construction accuracy across different environmental configurations
- LLM-based replanning: Essential for adaptive task modification; quick check involves testing reasoning consistency across safety-critical scenarios

## Architecture Onboarding

Component map: Environment sensors -> Safety Graph Construction -> Graphormer Encoding -> Hazard Detection -> LLM Replanning -> Task Execution

Critical path: Real-time sensor data flows through the safety graph construction module, where Graphormer encodes spatial and contextual relationships. The hazard detection component analyzes the encoded graph to identify risks, triggering LLM-based replanning when necessary. The replanned tasks are then executed in the environment.

Design tradeoffs: The system balances safety detection accuracy against computational overhead, with Graphormer providing rich contextual understanding but requiring significant processing resources. The LLM component offers flexible reasoning but may introduce latency in replanning decisions.

Failure signatures: Performance degradation occurs when Graphormer fails to capture novel hazard configurations, when LLM-based replanning produces inconsistent safety assessments, or when sensor noise corrupts the safety graph construction process.

First experiments:
1. Test hazard detection accuracy on known safety-critical scenarios with varying object configurations
2. Measure replanning latency under different computational loads and safety graph complexities
3. Evaluate task success rates when safety thresholds are dynamically adjusted during execution

## Open Questions the Paper Calls Out
None

## Limitations
- High false positive rate (70.73%) could lead to unnecessary replanning and reduced efficiency
- Performance evaluation limited to AI2-THOR simulation environment without real-world robot testing
- Reliance on predefined semantic labels may not capture novel or context-dependent hazards in dynamic environments

## Confidence
- Safety graph effectiveness in hazard detection: Medium confidence (strong simulation results but limited real-world validation)
- Dynamic replanning capability: Medium confidence (demonstrated in controlled scenarios but scalability untested)
- Outperformance of static baselines: High confidence (statistically significant results with clear baselines)
- Generalizability to open-world robotics: Low confidence (no evaluation beyond simulated environment)

## Next Checks
1. Deploy the framework on a physical robot platform with real-time sensor data to evaluate performance degradation under realistic noise, latency, and perception errors
2. Conduct ablation studies removing the Graphormer component to quantify its contribution to safety performance versus computational overhead
3. Test the system's response to adversarial environmental modifications designed to trigger false positive hazard detections and measure replanning efficiency impact