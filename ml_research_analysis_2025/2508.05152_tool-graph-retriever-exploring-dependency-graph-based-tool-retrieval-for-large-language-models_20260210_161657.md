---
ver: rpa2
title: 'Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for
  Large Language Models'
arxiv_id: '2508.05152'
source_url: https://arxiv.org/abs/2508.05152
tags:
- tool
- tools
- graph
- retrieval
- dependency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Tool Graph Retriever (TGR), a method to improve
  tool retrieval for large language models by leveraging dependencies between tools.
  Instead of treating tools independently, TGR constructs a tool dependency graph
  and uses graph convolution to encode these relationships into tool representations,
  leading to more effective retrieval.
---

# Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models

## Quick Facts
- arXiv ID: 2508.05152
- Source URL: https://arxiv.org/abs/2508.05152
- Authors: Linfeng Gao; Yaoxiang Wang; Minlong Peng; Jialong Tang; Yuzhe Shang; Mingming Sun; Jinsong Su
- Reference count: 2
- Key outcome: TGR improves tool retrieval by modeling dependencies via graph convolution, achieving significant gains on API-Bank and ToolBench benchmarks.

## Executive Summary
Tool Graph Retriever (TGR) addresses a critical limitation in LLM tool retrieval: the failure to identify prerequisite tools that are semantically distant from user queries but functionally necessary. By constructing a tool dependency graph and applying graph convolution to encode these relationships, TGR significantly improves retrieval performance over baseline semantic-only methods. The approach demonstrates substantial gains in both standard metrics (Recall@k, NDCG@k) and workflow-relevant metrics (Pass Rate@k), validating the importance of modeling tool dependencies for effective LLM tool use.

## Method Summary
TGR introduces a novel approach to tool retrieval that leverages dependencies between tools through graph convolution. The method constructs a tool dependency graph using a BERT-based discriminator trained on a custom dataset (TDI300K), then applies parameter-free graph convolution to update tool representations. These updated embeddings are used for online retrieval via cosine similarity. The system uses a two-stage training strategy for the discriminator: pretraining on synthetic data for general understanding, followed by finetuning on real-world data to handle sparse dependencies. This architecture enables the retrieval of prerequisite tools that semantic-only methods miss, addressing a fundamental limitation in current LLM tool retrieval systems.

## Key Results
- On API-Bank, TGR improves Recall@5 from 0.659 to 0.736 and Pass Rate@5 from 0.479 to 0.576.
- On ToolBench, TGR boosts Recall@5 from 0.709 to 0.761 and Pass Rate@5 from 0.460 to 0.595.
- Case studies show TGR successfully retrieves prerequisite tools (e.g., "GetUserToken" for "DeleteAccount") that baseline methods miss, validating the importance of modeling tool dependencies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Propagating relevance scores through a dependency graph allows semantically dissimilar but functionally necessary tools to be retrieved.
- **Mechanism**: The model constructs a directed graph where edges represent dependencies. Graph convolution aggregates neighbor features, meaning the representation of a prerequisite tool absorbs contextual information from the tools that depend on it. This aligns the prerequisite tool's embedding closer to the user query's intent.
- **Core assumption**: The semantic gap between a user query and a prerequisite tool's description is bridgeable via graph proximity to a relevant downstream tool.
- **Evidence anchors**: [abstract] "TGR constructs a tool dependency graph and uses graph convolution to encode these relationships into tool representations." [section 4.6, Table 6] Case study shows "GetUserToken" (semantically irrelevant to query) is retrieved because "DeleteAccount" depends on it.
- **Break condition**: If tools have circular dependencies or the graph density is too low (isolated nodes), the convolution fails to propagate sufficient relevance signals.

### Mechanism 2
- **Claim**: A discriminator trained on a two-stage pipeline (synthetic pretraining â†’ real finetuning) is required to accurately identify sparse dependencies.
- **Mechanism**: Real-world tool dependencies are rare. The model first pretrains on a synthetic, balanced dataset (TDI300K derived from CodeSearchNet) to learn the general concept of "dependency." It then finetunes on a smaller, imbalanced real-world set using a category-specific loss to handle sparsity without overfitting.
- **Core assumption**: Synthetic dependencies generated from code functions transfer sufficiently to identify logical dependencies in natural language API documents.
- **Evidence anchors**: [section 3.1] "We adopt a two-stage strategy... pretraining enables the discriminator to understand tool functions... finetuning enhances its ability to identify tool dependencies." [section 4.2, Table 3] Discriminator achieves 0.817 F1 on the test set.
- **Break condition**: If the synthetic data distribution diverges significantly from actual API logic, the discriminator will hallucinate edges, introducing noise into the graph.

### Mechanism 3
- **Claim**: Decoupling graph topology from the query encoding allows for offline graph processing, reducing online latency while improving completeness.
- **Mechanism**: The tool graph is static relative to the tool corpus. By pre-computing the Graph Convolution operation to update tool embeddings, the online retrieval step remains a simple similarity search (Query vs. Updated Tool Embedding).
- **Core assumption**: Tool dependencies are stable and do not change dynamically based on the specific user query context.
- **Evidence anchors**: [section 3.3] "Finally, these updated tool representations are employed for online retrieval... we compute the similarities between the embeddings of queries and tools." [section 3.2] Equation 3 describes the offline update of the embedding matrix X.
- **Break condition**: If the retrieval corpus changes dynamically (tools added/removed frequently), the graph must be re-convolved, potentially creating an update bottleneck.

## Foundational Learning

- **Concept**: **Graph Convolutional Networks (GCN)**
  - **Why needed here**: Understanding how TGR updates tool representations requires knowing how GCNs aggregate neighbor node features to create context-aware embeddings.
  - **Quick check question**: If Node A connects to Node B, does a single GCN layer cause Node A's final representation to depend on Node B's *original* features or its *updated* features?

- **Concept**: **Class Imbalance & Cost-Sensitive Learning**
  - **Why needed here**: The paper notes dependencies are "sparse," necessitating a specific loss function and two-stage training to prevent the model from defaulting to "no dependency."
  - **Quick check question**: Why would a standard cross-entropy loss fail if 95% of tool pairs have "no dependency"?

- **Concept**: **Recall@k vs. Pass Rate@k**
  - **Why needed here**: Standard retrieval metrics (Recall) measure individual tool accuracy, while Pass Rate measures the system's ability to retrieve the *entire* set of required tools, which is critical for agentic workflows.
  - **Quick check question**: If a query requires tools {A, B} and the system retrieves {A, C}, what is the Pass Rate@5?

## Architecture Onboarding

- **Component map**:
  - TDI300K Generator -> Dependency Discriminator -> Graph Builder -> Tool Encoder -> Graph Retriever

- **Critical path**: The **Discriminator Accuracy**. If the discriminator produces false positives (fake dependencies), the GCN will convolve noise, potentially diluting the semantic signal of correct tools.

- **Design tradeoffs**:
  - **Graph Density vs. Noise**: The paper (Section 4.5) shows higher density improves recall, but unverified edges from a weak discriminator could degrade performance.
  - **Efficiency**: The paper removes learnable weights from GCN to speed up retrieval, potentially limiting the model's capacity to learn complex graph dynamics compared to a full trainable GNN.

- **Failure signatures**:
  - **Low Pass Rate / High Recall**: The system retrieves the "main" tool but misses prerequisites (the exact baseline problem TGR solves; if this persists, the discriminator is failing to identify edges).
  - **High Latency on Updates**: Adding new tools requires re-running the O(N^2) dependency check against all existing tools.

- **First 3 experiments**:
  1. **Discriminator Baseline**: Evaluate the finetuned discriminator on the held-out test set (Table 3 metrics) to ensure edge generation is reliable before building the graph.
  2. **Ablation on Graph Source**: Compare retrieval performance using `+TGR-d` (automated graph) vs. `+TGR-m` (manual graph) on a subset of data to quantify the upper bound of the system.
  3. **Density Analysis**: Replicate Figure 4 to verify that your specific tool domain has sufficient connectivity for graph methods to provide a signal boost over semantic-only retrieval.

## Open Questions the Paper Calls Out

- **Question**: How can the generalization capability of Tool Graph Retriever be further enhanced to perform robustly across diverse, unseen tool ecosystems?
- **Basis in paper**: [explicit] The Conclusion explicitly lists "how to further enhance the generalization of our TGR" as a primary focus for future research.
- **Why unresolved**: The experiments are confined to specific datasets (API-Bank, ToolBench-I1), and the authors do not demonstrate the model's transferability to entirely new tool domains without retraining.
- **What evidence would resolve it**: Benchmark results showing sustained high Pass Rates when applying the trained TGR to novel tool libraries distinct from the training data.

- **Question**: Can the time complexity of the graph construction phase be reduced below O(N^2) to support scalable application?
- **Basis in paper**: [explicit] The Limitations section identifies the O(N^2) complexity required for constructing the dependency graph as a bottleneck.
- **Why unresolved**: The current methodology requires comparing every tool pair to identify dependencies, which becomes computationally prohibitive as the candidate tool set grows.
- **What evidence would resolve it**: An algorithm incorporating filtering rules or heuristics that lowers the construction time complexity (e.g., to linear or log-linear time) while maintaining retrieval quality.

- **Question**: Would utilizing more advanced or efficient graph networks significantly improve the quality of tool representations over the standard graph convolution currently used?
- **Basis in paper**: [explicit] The Conclusion states the intent to "try some efficient graph networks to obtain better tool representations" in future work.
- **Why unresolved**: The current implementation uses a simplified graph convolution; the potential benefits of attention mechanisms or deeper GNN architectures remain unexplored.
- **What evidence would resolve it**: Ablation studies comparing the current GCN approach against architectures like GAT or GraphSAGE on the TDI300K dataset.

## Limitations

- The core method relies on a custom dataset (TDI300K) and a dependency discriminator whose accuracy directly limits retrieval performance. Without access to the dataset, exact reproduction is challenging.
- The O(N^2) complexity for constructing the dependency graph creates a scalability bottleneck for large tool sets, requiring optimization for real-world deployment.
- The paper demonstrates strong performance on specific benchmarks but does not extensively validate generalizability to entirely new tool domains or highly dynamic tool ecosystems.

## Confidence

- **High Confidence**: The architectural design of TGR (Graph Convolution on a dependency graph) is sound and well-motivated. The improvement in standard retrieval metrics (Recall@k, NDCG@k) is a direct and verifiable outcome of the method.
- **Medium Confidence**: The two-stage training strategy for the discriminator is logically justified for handling class imbalance, but the extent to which synthetic data (CodeSearchNet) truly represents real API dependencies is an assumption that requires validation.
- **Low Confidence**: The absolute performance numbers are contingent on the quality of the unpublished TDI300K dataset and the specific manual annotations used for finetuning. Minor variations in these inputs could lead to significant performance differences.

## Next Checks

1. **Discriminator Quality Audit**: Evaluate the finetuned dependency discriminator on the held-out test set (Table 3 metrics) to ensure the F1 score is close to the reported 0.817. This validates the edge generation process before building the graph.

2. **Manual vs. Automated Graph Ablation**: On a small, controlled subset of the data where manual dependencies are known, compare the retrieval performance of TGR using the automated graph (`+TGR-d`) against the manual graph (`+TGR-m`). This quantifies the upper bound of the system and the impact of potential noise in the automated graph.

3. **Graph Density Sensitivity Analysis**: Replicate the analysis in Figure 4 for your specific tool domain. Verify that the density of the generated dependency graph is within a range (e.g., 40-80% connectivity) where graph methods are known to provide a performance boost over semantic-only retrieval. If the graph is too sparse, the convolution will have little effect; if too dense, it may introduce noise.