---
ver: rpa2
title: 'Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced
  Ensembles and XAI'
arxiv_id: '2512.01333'
source_url: https://arxiv.org/abs/2512.01333
tags:
- stroke
- prediction
- learning
- machine
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an interpretable machine learning framework
  for stroke risk prediction using ensemble modeling and explainable AI (XAI). The
  approach combined Random Forest, ExtraTrees, and XGBoost models optimized via hyperparameter
  tuning and trained on balanced datasets using Random Over-Sampling (ROS) to address
  class imbalance.
---

# Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI

## Quick Facts
- arXiv ID: 2512.01333
- Source URL: https://arxiv.org/abs/2512.01333
- Reference count: 25
- Primary result: Achieved 99.09% accuracy on stroke prediction using ROS-balanced ensemble with XAI

## Executive Summary
This study presents a machine learning framework for stroke risk prediction that combines ensemble modeling with explainable AI techniques. The approach addresses class imbalance through Random Over-Sampling (ROS) and achieves high predictive accuracy while maintaining interpretability for clinical decision-making. The framework integrates multiple classifiers optimized through hyperparameter tuning and validated using stratified cross-validation.

## Method Summary
The framework employs a systematic ML pipeline beginning with exploratory data analysis and missing value imputation using KNN and iterative methods for complex cases, and mean/median/mode for simpler scenarios. Features undergo standardization, outlier removal, and dimensionality reduction before ROS balancing. Ten models are evaluated using 5-fold stratified cross-validation with grid search optimization. The ensemble combines Random Forest, ExtraTrees, and XGBoost via soft voting, with LIME providing interpretability by identifying age, hypertension, and glucose levels as key predictors.

## Key Results
- Ensemble model achieved 99.09% accuracy on Stroke Prediction Dataset (SPD)
- Model achieved 84.04% accuracy on SDP dataset
- LIME identified age, hypertension, and glucose levels as top clinical predictors
- High F1 scores across both datasets demonstrate strong predictive performance

## Why This Works (Mechanism)
The framework succeeds by addressing stroke prediction's inherent challenges: class imbalance, need for interpretability, and complex feature interactions. ROS balancing ensures minority class representation during training, while ensemble methods combine diverse model strengths. The soft voting approach leverages probabilistic outputs from RF, ET, and XGBoost, creating robust predictions. LIME interpretation reveals clinically relevant features, enabling trust in model decisions.

## Foundational Learning
- **Random Over-Sampling (ROS)**: Resamples minority class to balance datasets, preventing bias toward majority class predictions
  - Why needed: Stroke datasets typically have severe class imbalance (few stroke cases vs. many non-stroke cases)
  - Quick check: Verify class distribution before/after ROS application
- **Stratified Cross-Validation**: Preserves class proportions across folds, ensuring representative training/testing splits
  - Why needed: Prevents information leakage between folds and maintains statistical validity
  - Quick check: Confirm each fold maintains original class ratio
- **Soft Voting Ensemble**: Combines model probabilities rather than hard predictions, creating smoother decision boundaries
  - Why needed: Captures consensus across diverse models, improving robustness
  - Quick check: Compare soft vs. hard voting performance
- **LIME (Local Interpretable Model-Agnostic Explanations)**: Generates local approximations to explain individual predictions
  - Why needed: Provides clinically actionable insights for model decisions
  - Quick check: Verify feature importance rankings match clinical expectations
- **Hyperparameter Tuning with Grid Search**: Systematically explores parameter space to optimize model performance
  - Why needed: Prevents suboptimal default parameters and maximizes predictive accuracy
  - Quick check: Compare tuned vs. default model performance
- **Feature Standardization**: Normalizes feature scales to prevent dominance by high-magnitude variables
  - Why needed: Ensures fair contribution from all features during model training
  - Quick check: Verify feature distributions have similar scales

## Architecture Onboarding

**Component Map:**
Data Loading -> Preprocessing (Imputation, Standardization, Outlier Removal) -> Dimensionality Reduction -> ROS Balancing -> 5-Fold Stratified CV with Grid Search -> Individual Model Training (10 models) -> Ensemble Soft Voting -> LIME Interpretation -> Clinical Validation

**Critical Path:**
ROS Balancing -> Ensemble Soft Voting -> LIME Interpretation

**Design Tradeoffs:**
- ROS vs. SMOTE: ROS chosen for simplicity, though may cause overfitting
- Soft vs. Hard Voting: Soft voting provides better calibration but requires probability estimates
- LIME vs. SHAP: LIME selected for local interpretability, SHAP might provide global insights

**Failure Signatures:**
- Near-perfect accuracy (99.09%) suggests potential data leakage or insufficient test separation
- Large accuracy gap between datasets (15.05%) indicates possible overfitting to SPD features
- Missing dataset specifications prevent reproducibility

**3 First Experiments:**
1. Verify ROS application occurs only within training folds of CV, not globally before splitting
2. Compare ensemble performance with individual base models to confirm added value
3. Test LIME explanations on edge cases to validate clinical relevance

## Open Questions the Paper Calls Out
None

## Limitations
- Exact dataset sources for SPD and SDP remain unspecified, preventing direct replication
- Ensemble weighting strategy and decision threshold optimization lack precise technical details
- Near-perfect accuracy on SPD raises concerns about potential data leakage
- Feature engineering and dimensionality reduction steps are mentioned but not detailed

## Confidence
- Stroke risk prediction methodology: **High** (standard ML practices)
- Performance metrics (accuracy, F1): **Medium** (potential leakage concerns)
- XAI interpretability results: **Medium** (LIME methodology is clear, but clinical validation missing)
- Ensemble model optimization: **Low** (incomplete weight specification and threshold details)

## Next Checks
1. Verify ROS application within 5-fold CV training folds only, not globally before splitting
2. Obtain and validate both dataset sources to confirm feature distributions match descriptions
3. Replicate on an independent external stroke dataset to test generalizability beyond the two used datasets