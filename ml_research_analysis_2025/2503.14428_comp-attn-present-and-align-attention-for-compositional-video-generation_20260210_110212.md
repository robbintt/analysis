---
ver: rpa2
title: 'Comp-Attn: Present-and-Align Attention for Compositional Video Generation'
arxiv_id: '2503.14428'
source_url: https://arxiv.org/abs/2503.14428
tags:
- subject
- arxiv
- attention
- video
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating compositional
  videos with multiple subjects and complex inter-subject relationships in text-to-video
  (T2V) generation. The key problem is that existing methods fail to simultaneously
  ensure subject presence and accurate inter-subject relations.
---

# Comp-Attn: Present-and-Align Attention for Compositional Video Generation

## Quick Facts
- arXiv ID: 2503.14428
- Source URL: https://arxiv.org/abs/2503.14428
- Reference count: 40
- 15.7%/11.7% improvement on Wan2.1-T2V-14B/Wan2.2-T2V-A14B using training-free method

## Executive Summary
This paper introduces Comp-Attn, a training-free method for compositional text-to-video generation that addresses the dual challenge of ensuring subject presence and accurate inter-subject relationships. The approach uses a "Present-and-Align" paradigm combining Subject-aware Condition Interpolation (SCI) for presence reinforcement and Layout-forcing Attention Modulation (LAM) for relational alignment. Tested on T2V-CompBench, the method achieves significant performance gains while adding only 5% inference overhead.

## Method Summary
Comp-Attn operates through two complementary mechanisms: SCI reinforces subject-specific semantics by computing saliency scores and restoration vectors for each subject, then applies time-weighted interpolation to modify prompt embeddings during the early denoising timesteps. LAM uses LLM-planned layouts to guide attention distributions, computing cross-attention maps and applying IoU-based modulation to ensure subjects appear in their correct spatial relationships. The method is training-free and works by modifying the denoising process of existing T2V models.

## Key Results
- 15.7% improvement on Wan2.1-T2V-14B and 11.7% on Wan2.2-T2V-A14B on T2V-CompBench
- Only 5% increase in inference time compared to baseline models
- Strong performance on VBench and T2I-CompBench, demonstrating cross-domain applicability
- Effective across different video generation architectures without retraining

## Why This Works (Mechanism)
The method succeeds by addressing the fundamental tension in compositional generation: ensuring all subjects appear while maintaining their spatial relationships. SCI tackles the presence problem by amplifying subject-specific features through learned restoration vectors, while LAM resolves relational accuracy by forcing the attention mechanism to respect layout constraints derived from LLM planning. The two-stage approach ensures neither presence nor alignment is sacrificed for the other.

## Foundational Learning

**Subject Saliency Scoring (s_k)**: Measures how well a subject is represented in the prompt embedding using attention weights. Needed to identify which subjects require enhancement. Quick check: Verify s_k values fall between 0 and 1 for all subjects.

**Cross-Attention Modulation**: Dynamically adjusts attention distributions based on layout mismatch. Needed to enforce spatial relationships between subjects. Quick check: Monitor IoU scores to ensure they're in reasonable ranges (0.3-0.7).

**Time-Weighted Interpolation**: Gradually applies restoration vectors during early denoising steps. Needed to smoothly integrate enhanced subject features without introducing artifacts. Quick check: Confirm interpolation weights decrease appropriately from 1 to 0 over the 20% timestep window.

## Architecture Onboarding

**Component Map**: Text Encoder -> SCI Module -> LAM Module -> Video Denoiser

**Critical Path**: Text prompt → subject extraction → LLM layout planning → SCI enhancement → LAM modulation → video generation

**Design Tradeoffs**: Training-free approach vs. LLM dependency; early timestep application vs. potential feature loss; simple IoU-based modulation vs. more complex spatial matching

**Failure Signatures**: Low IoU causing excessive modulation that degrades visual quality; SCI failing for subjects with high saliency scores that still get omitted; inconsistent layout generation across different LLM prompts

**First Experiments**: 1) Test SCI alone on simple single-subject prompts to verify presence enhancement; 2) Test LAM alone with perfect layouts to verify alignment capability; 3) Run end-to-end with 2 subjects to validate the complete Present-and-Align pipeline

## Open Questions the Paper Calls Out

None specified in the paper.

## Limitations

- LLM dependency for layout planning introduces potential variability without detailed prompt specifications
- Performance claims depend on specific text encoders and subject identification methods not fully detailed
- No ablation studies on optimal number of keyframe layouts or IoU modulation thresholds
- Computational overhead claim of "only 5%" lacks specific hardware/implementation details

## Confidence

**Performance improvements (15.7%/11.7%)**: High - Empirical results on established benchmarks with clear methodology
**Training-free effectiveness**: Medium - Method is training-free but LLM dependency and unspecified prompt details create uncertainty
**Scalability across architectures**: Low-Medium - Results shown on Wan2.1/Wan2.2 and VBench, but no comprehensive ablation on different base models

## Next Checks

1. Implement the full method using paper's equations and parameters (τ=0.2, first 20% timesteps, 4 keyframe layouts) on a different compositional T2V model to verify 5% overhead claim and baseline improvement
2. Conduct ablation studies varying the number of LLM-generated keyframe layouts (2, 4, 6) and measure impact on IoU scores and final compositional quality metrics
3. Analyze the distribution of IoU scores between LLM-planned and adaptive layouts across the test set to determine if E_k=1-IoU_k modulation strategy is consistently effective or requires adaptive thresholding