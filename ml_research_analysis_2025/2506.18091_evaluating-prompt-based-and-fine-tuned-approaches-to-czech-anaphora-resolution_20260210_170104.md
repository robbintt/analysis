---
ver: rpa2
title: Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution
arxiv_id: '2506.18091'
source_url: https://arxiv.org/abs/2506.18091
tags:
- anaphora
- accuracy
- sentence
- czech
- resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates prompt-based and fine-tuned approaches for
  anaphora resolution in Czech, a morphologically rich language. It compares large
  language models (LLMs) using prompting strategies with fine-tuned mT5 and Mistral
  models on a dataset from the Prague Dependency Treebank.
---

# Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution

## Quick Facts
- **arXiv ID:** 2506.18091
- **Source URL:** https://arxiv.org/abs/2506.18091
- **Reference count:** 17
- **Key outcome:** Fine-tuned mT5-large achieves 88% accuracy on Czech anaphora resolution, outperforming prompt-based approaches (74.5% with Mistral Large 2) due to better handling of morphological agreement patterns.

## Executive Summary
This paper evaluates fine-tuned and prompt-based approaches for anaphora resolution in Czech, a morphologically rich language. The authors compare large language models using prompting strategies with fine-tuned mT5 and Mistral models on a dataset from the Prague Dependency Treebank. Fine-tuned models, particularly mT5-large, significantly outperform prompted models, achieving up to 88% accuracy while requiring fewer computational resources. The study demonstrates that fine-tuning remains a competitive strategy for anaphora resolution in low-resource languages, though prompting offers flexibility with less labeled data. Grammatical anaphora proves systematically easier than textual anaphora for both approaches.

## Method Summary
The study evaluates anaphora resolution using two approaches: fine-tuning encoder-decoder models (mT5-small/base/large and Mistral 0.2 with LoRA) and prompting large language models (Mistral Large 2, Llama 3.1/3.2, Aya 23, Gemma 2). The task involves identifying antecedents for pronominal anaphora marked with `<ana></ana>` tags in Czech sentences from PDTC 1.0 dataset (56,207 sentences). Fine-tuning uses seq2seq format with tagged input-output pairs, while prompting employs three strategies: Yes/No classification, QA-style, and tagging formats in zero/one/three-shot settings. Evaluation uses relaxed span-level accuracy measuring syntactic root containment and continuity.

## Key Results
- Fine-tuned mT5-large achieves 88% accuracy, significantly outperforming prompted models (74.5% with Mistral Large 2)
- Grammatical anaphora resolution (91.7% accuracy) is substantially easier than textual anaphora (82.9% accuracy)
- Fine-tuning mT5-base (83% accuracy) outperforms all prompted models despite being 100x smaller than the largest prompted model (123B parameters)
- Prompt format significantly impacts performance, with conversational QA-style outperforming structured tagging approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-specific fine-tuning outperforms prompting for structured anaphora resolution in morphologically rich languages
- **Mechanism:** Fine-tuning directly optimizes model weights for the specific input-output mapping (sentence with `<ana>` tags → sentence with `<ant>` tags), allowing the model to internalize Czech morphological patterns rather than relying on in-context inference
- **Core assumption:** The performance gap persists because prompting cannot efficiently encode the complex gender/case/number agreement patterns that fine-tuning learns during gradient updates
- **Evidence anchors:** Fine-tuned models achieve 88% accuracy vs 74.5% for prompted models; even compact mT5-base outperforms all prompted models
- **Break condition:** If labeled data drops below ~5,000 examples, fine-tuning advantage may diminish

### Mechanism 2
- **Claim:** Grammatical anaphora is systematically easier to resolve than textual anaphora for both approaches
- **Mechanism:** Grammatical anaphora follows syntactic patterns (e.g., relative pronouns referring to noun phrases) that are well-represented in training data and require less semantic reasoning. Textual anaphora requires world knowledge and discourse understanding
- **Core assumption:** The difficulty differential stems from semantic reasoning requirements, not just data imbalance
- **Evidence anchors:** mT5-large shows 91.7% accuracy for grammatical vs 82.9% for textual anaphora; textual anaphora proved more difficult across all models
- **Break condition:** If semantic reasoning capabilities of LLMs improve substantially, textual anaphora gap may narrow

### Mechanism 3
- **Claim:** Prompt format significantly impacts performance, with conversational question-answering outperforming structured tagging
- **Mechanism:** Instruction-tuned models are optimized for dialogue formats, making open-ended questions more compatible with their training distribution than strict output formatting tasks
- **Core assumption:** Format compatibility matters more than task equivalence for prompted models
- **Evidence anchors:** Results mostly fit the conversational nature of models, showing best performance with open-ended questions; tagging experiment showed significant drop in performance
- **Break condition:** Models specifically instruction-tuned for structured output tasks may reverse this pattern

## Foundational Learning

- **Concept:** Anaphora vs. Coreference
  - **Why needed here:** The paper focuses on single anaphor-antecedent pairs, not full coreference chains; misunderstanding this limits comprehension of the task scope
  - **Quick check question:** Can you explain why resolving "he" referring to "Tom" is anaphora resolution, but grouping all mentions of Tom across a document is coreference resolution?

- **Concept:** Morphologically Rich Languages
  - **Why needed here:** Czech encodes gender, case, and number in word forms; this creates constraints that models must learn (e.g., pronoun-antecedent gender agreement)
  - **Quick check question:** Why might "která" (which-feminine) only refer to "budova" (building-feminine) but not to a masculine noun in Czech?

- **Concept:** Encoder-Decoder vs. Decoder-Only Architectures
  - **Why needed here:** mT5 (encoder-decoder) was fine-tuned differently than Mistral (decoder-only with LoRA); architectural choice affects training strategy
  - **Quick check question:** Why might bidirectional attention in mT5's encoder help with identifying antecedents that appear after the anaphor?

## Architecture Onboarding

- **Component map:** XML preprocessing → Train/val/test split → Fine-tuning path (mT5/Mistral) OR Prompting path (LLM with templates) → Relaxed span accuracy evaluation
- **Critical path:** 1) Dataset preparation from PDTC 1.0 → train/validation/test splits; 2) Fine-tuning: format as tagged input → tagged output pairs; 3) Prompting: select template type (Yes/No, QA, or Tagging); 4) Evaluation using root-token matching
- **Design tradeoffs:** Fine-tuning offers higher accuracy (88% vs 74.5%), lower inference cost, requires labeled data and training infrastructure vs Prompting requires no training, rapid prototyping, higher inference cost, lower accuracy, output format instability
- **Failure signatures:** Low accuracy on textual anaphora with demonstrative pronouns (67.7%); anaphor-in-antecedent cases (71.3%); spoken Czech (83.5%); tagging prompt format causes severe performance drops
- **First 3 experiments:** 1) Replicate mT5-base fine-tuning to establish baseline (~83% accuracy); 2) Test prompt robustness: compare zero-shot vs. three-shot QA format on textual anaphora subset; 3) Ablation study: evaluate whether antecedent distance affects fine-tuned vs. prompted models differently

## Open Questions the Paper Calls Out

- **Open Question 1:** How does integrating an automated mention detector affect performance when transitioning from single-pair resolution to full coreference resolution?
  - **Basis in paper:** The authors state they "plan to extend this research by including a mention detector in the pipeline to convert the fine-tuned model into a coreference resolution solution"
  - **Why unresolved:** Current study evaluates only pre-segmented single anaphor-antecedent pairs, bypassing the mention detection step required for end-to-end coreference
  - **What evidence would resolve it:** Evaluating the end-to-end system using standard coreference metrics (e.g., MUC, B-cubed, CEAF) against established baselines

- **Open Question 2:** Does a mixture of experts (MoE) architecture routed by pronoun category yield higher accuracy than a monolithic fine-tuned model?
  - **Basis in paper:** Future work suggests "employing a mixture of experts approach to divide the anaphora resolution model into expert networks along with a pronoun category classifier"
  - **Why unresolved:** Current experiments use standard fine-tuning, leaving the potential benefits of specialized sub-networks for different pronoun types unexplored
  - **What evidence would resolve it:** Comparative accuracy results on the test set between current mT5-large model and a newly trained MoE model

- **Open Question 3:** Does incorporating multilingual training data improve monolingual Czech anaphora resolution performance?
  - **Basis in paper:** The authors list "the inclusion of multilingual data into the training process" as a direction worth exploring
  - **Why unresolved:** Fine-tuning experiments relied exclusively on the monolingual Prague Dependency Treebank
  - **What evidence would resolve it:** An ablation study comparing models trained solely on Czech data against models fine-tuned on combined multilingual anaphora datasets

## Limitations

- The study uses English-language prompts for a morphologically rich Slavic language, potentially limiting cross-linguistic transfer effectiveness
- The relaxed span accuracy metric accepts approximate matches, which may overestimate real-world performance where precise span boundaries matter
- Dataset underrepresentation of spoken Czech (7.8%) and specific pronoun categories like demonstrative textual anaphora (n=90) raises generalization concerns

## Confidence

- **High confidence:** Fine-tuning mT5-large achieves significantly higher accuracy than prompted models (88% vs 74.5%) for Czech anaphora resolution
- **Medium confidence:** Grammatical anaphora is systematically easier than textual anaphora across both approaches, with the gap reflecting semantic reasoning requirements
- **Medium confidence:** Prompt format choice (QA vs tagging) substantially impacts performance for instruction-tuned models, with conversational formats performing better

## Next Checks

1. **Cross-linguistic robustness test:** Evaluate whether English-language prompts maintain performance when translated to Czech, or whether prompt translation introduces systematic errors in morphologically complex contexts
2. **Error analysis stratification:** Analyze whether the 11.5% gap between fine-tuned (88%) and prompted (74.5%) models comes primarily from agreement pattern failures, discourse-level reasoning, or output formatting issues
3. **Data efficiency experiment:** Systematically vary training data size (1K, 5K, 10K examples) to identify the crossover point where prompting becomes competitive with fine-tuning for Czech anaphora resolution