---
ver: rpa2
title: Pre-training under infinite compute
arxiv_id: '2509.14786'
source_url: https://arxiv.org/abs/2509.14786
tags:
- data
- scaling
- count
- parameter
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies language model pre-training under fixed data\
  \ and unlimited compute budgets. The authors show that standard approaches like\
  \ increasing epochs or parameter count eventually overfit, but can be improved significantly\
  \ by tuning regularization (finding optimal weight decay is 30\xD7 larger than standard\
  \ practice)."
---

# Pre-training under infinite compute

## Quick Facts
- arXiv ID: 2509.14786
- Source URL: https://arxiv.org/abs/2509.14786
- Authors: Konwoo Kim; Suhas Kotha; Percy Liang; Tatsunori Hashimoto
- Reference count: 40
- Primary result: Achieves same validation loss with 5.17× less data using joint scaling of parameters and ensemble size

## Executive Summary
This paper studies language model pre-training under fixed data constraints and unlimited compute budgets, challenging the conventional wisdom that scaling parameters requires proportionally more data. The authors demonstrate that standard approaches like increasing epochs or parameter count eventually overfit, but can be dramatically improved by aggressive regularization tuning (finding optimal weight decay is 30× larger than standard practice). With proper regularization, loss scales monotonically with parameter count, enabling significant data efficiency gains.

The key insight is that ensemble methods achieve better performance than single-model parameter scaling because independently trained models learn complementary representations of the data. Combining both parameter scaling and ensembling (joint scaling) further improves results. Their best recipe achieves the same validation loss as the baseline using 5.17× less data, and these gains persist at larger scales, transfer to downstream tasks with a 9% improvement on benchmarks, and can be realized in smaller models via distillation with 17.5× data efficiency in continued pre-training.

## Method Summary
The authors fix a data budget (200M-1.6B tokens) and explore pre-training recipes under unlimited compute. They use Llama-style transformer models (150M-1.4B parameters) with AdamW optimizer and cosine learning rate schedule. The core methodology involves coordinate descent to jointly tune learning rate, epoch count, and weight decay per model size. They evaluate three recipes: (1) Regularized - aggressive weight decay (0.8-3.2) enables monotonic scaling, (2) Ensemble - K independently trained models with logit averaging, (3) Joint scaling - combining both approaches. Validation loss is fitted to power laws L = A/N^α + E to estimate asymptotes as N→∞.

## Key Results
- Optimal weight decay is 30× larger than standard practice (0.1→3.2), enabling monotonic loss scaling in over-parameterized models
- K=3 ensemble outperforms the asymptote of the regularized recipe (N→∞, K=1)
- Self-distillation with mixed real/synthetic data improves over the teacher model by implicitly performing ensembling
- Joint scaling recipe achieves same validation loss as baseline using 5.17× less data
- Data efficiency gains transfer to downstream tasks with 9% improvement on benchmarks and 17.5× data efficiency in continued pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggressive weight decay (up to 30× standard) enables monotonic loss scaling in over-parameterized models by preventing overfitting under data constraints.
- Mechanism: High regularization constrains the effective capacity of over-parameterized networks, allowing them to generalize even when parameter-to-token ratios far exceed Chinchilla recommendations (up to 140×). The paper notes training dynamics shift: "loss for runs with high weight decay decreases much more slowly at the start of training, but quickly decreases near the end."
- Core assumption: The loss landscape has sufficiently convex structure that coordinate descent on hyperparameters finds locally optimal regions that transfer across scales.
- Evidence anchors:
  - [abstract]: "finding that the optimal weight decay is 30× larger than standard practice"
  - [Section 3, Figure 3]: Shows monotone power-law scaling (≈1/N) after jointly tuning weight decay, learning rate, and epoch count across parameter counts
  - [corpus]: Weak direct evidence—neighbor papers discuss compute-optimal scaling but not regularization dynamics under data constraints
- Break condition: If the hyperparameter landscape becomes non-convex or if regularization cannot compensate for extreme overfitting (e.g., orders of magnitude more parameters than tokens without any regularization tuning).

### Mechanism 2
- Claim: Ensembling independently trained models achieves lower loss asymptotes than single-model parameter scaling because different models learn distinct "views" of the data.
- Mechanism: Under the "multi-view" hypothesis (citing Allen-Zhu & Li 2023), each ensemble member independently converges to different feature representations from the same data due to random initialization and data order. Logit averaging combines these complementary representations, whereas a single model is biased toward learning only one feature set.
- Core assumption: The data has intrinsic multi-view structure where optimal classification requires combining multiple features.
- Evidence anchors:
  - [Section 4.2, Figure 4]: K=3 ensemble outperforms the asymptote of the regularized recipe (N→∞, K=1)
  - [Section 4.2]: "training a single model is biased towards only learning one feature, whereas each member of an ensemble happens to learn different features when independently trained"
  - [corpus]: No direct corpus validation of multi-view hypothesis in LLM pre-training specifically
- Break condition: If data lacks multi-view structure, or if ensemble members converge to identical representations despite different random seeds.

### Mechanism 3
- Claim: Self-distillation with mixed real/synthetic data improves over the teacher model by implicitly performing ensembling.
- Mechanism: Training a student on teacher-generated data mixed with original data can be interpreted as implicit ensembling between the teacher's knowledge and the student's fresh initialization. The paper emphasizes that mixing real data is critical: "without mixing in real pre-training data, self-distillation is substantially worse than the teacher model."
- Core assumption: The mixing ratio and data quality allow the student to integrate teacher knowledge without model collapse.
- Evidence anchors:
  - [Section 6.2, Figure 9]: 300M student outperforms 300M teacher (green star vs. blue point)
  - [Appendix E.3, Table 4]: Self-distillation with 1:1 mixing achieves loss 3.44 vs. teacher 3.71; without mixing, loss degrades to 4.07
  - [corpus]: Related work on synthetic data augmentation cited (Allen-Zahu & Li 2024, Maini et al. 2024) but no direct mechanistic validation
- Break condition: If synthetic data dominates without real data mixing, causing model collapse; or if teacher quality is too low to provide useful signal.

## Foundational Learning

- Concept: **Power-law scaling with asymptotes**
  - Why needed here: The paper evaluates recipes by fitting power laws L = A/N^α + E and comparing asymptotes E, which estimate best possible loss as N→∞.
  - Quick check question: Can you explain why comparing asymptotes is preferable to comparing loss at fixed compute when compute is unconstrained?

- Concept: **Coordinate descent for hyperparameter tuning**
  - Why needed here: The "locally optimal" hyperparameter search iteratively improves each dimension (learning rate, epoch count, weight decay) until no neighbor improves loss.
  - Quick check question: What assumption about the loss landscape makes coordinate descent likely to find global optima?

- Concept: **Ensemble logit averaging**
  - Why needed here: The ensembling recipe averages logits via exp((1/K) Σ log P_i(x)), which is mathematically equivalent to geometric mean of probabilities.
  - Quick check question: Why might logit averaging outperform weight averaging (model soups) for pre-trained models?

## Architecture Onboarding

- Component map:
  - **Pre-training loop**: AdamW optimizer with cosine LR schedule, weight decay (tuned 0.1–3.2), batch size 64 (smaller is better)
  - **Scaling axes**: Parameter count N (150M–1.4B), ensemble member count K (1–8+), epoch count E (powers of 2)
  - **Distillation pipeline**: Teacher generates synthetic data → mix with real data at ratio 1:3 to 1:9 → train student from scratch
  - **Evaluation**: Validation loss (held-out 4M tokens) + downstream benchmarks (PIQA, SciQ, ARC-Easy)

- Critical path:
  1. Fix token budget D (e.g., 200M tokens)
  2. For each parameter count N, run coordinate descent to find locally optimal (LR, epochs, weight decay)
  3. Fit power law to estimate asymptote E
  4. For ensembles: repeat with K members, fit power law in K, then in N
  5. For distillation: generate teacher data, tune mixing ratio, train student

- Design tradeoffs:
  - **Parameter scaling vs. ensemble scaling**: Ensembles achieve lower asymptotes but require K× inference compute; distillation can compress to single model with 83% benefit retention
  - **High vs. low weight decay**: High decay (1.6–3.2) enables monotone scaling but slows early training; low decay (0.1) overfits quickly
  - **Epoch count**: More epochs help smaller models but require higher regularization; optimal epochs decrease as N increases

- Failure signatures:
  - Validation loss increases with parameter count → insufficient regularization or wrong epoch count for that N
  - Ensemble asymptote higher than single-model asymptote → hyperparameters tuned for K=1, not K→∞ (need 2× epochs, 0.5× weight decay)
  - Self-distillation underperforms teacher → missing real data in mixture or wrong mixing ratio
  - Power law fit unreliable → need more parameter counts or check run-to-run variance (paper reports ±0.02 loss across seeds)

- First 3 experiments:
  1. **Baseline check**: Train 300M model on 200M tokens with default weight decay (0.1), vary epochs (1, 4, 8, 16). Expect overfitting at high epochs. Then re-run with tuned weight decay (1.6) and compare.
  2. **Ensemble vs. parameter scaling**: Train two 300M models vs. one 600M model (both with optimal hyperparameters). Compare validation losses to verify K=2 ensemble outperforms N=2×.
  3. **Self-distillation validation**: Take best 300M teacher, generate synthetic data, train student with 1:1 and 1:3 real:synthetic mixing. Verify student ≥ teacher only when real data is mixed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the data efficiency of Mixture-of-Experts (MoE) architectures stem from their structural sparsity or merely from dropout-like regularization effects?
- Basis in paper: [explicit] Appendix C.3.1 hypothesizes that MoE benefits likely derive from the dropout aspect rather than the sparsity aspect, stating, "We hope future experiments can settle this intuition more concretely."
- Why unresolved: The paper notes that jointly trained ensembles (analogous to MoE gradient steps) did not significantly outperform single models in early experiments, suggesting the mechanism is not yet fully understood.
- What evidence would resolve it: Large-scale ablations comparing MoE against dense models with matched dropout rates and parameter counts in the infinite compute regime.

### Open Question 2
- Question: How do alternative training objectives (e.g., diffusion) and architectures perform when evaluated via the infinite-compute asymptote rather than fixed compute budgets?
- Basis in paper: [explicit] Section 9 suggests the "success of such simple methods encourages us to revisit basic decisions such as objective [Ni et al., 2025]... architecture [Gladstone et al., 2025], and data augmentation."
- Why unresolved: Current literature optimizes these dimensions for compute-constrained regimes; their performance limits and asymptotic behaviors under data constraints remain unquantified.
- What evidence would resolve it: Applying the paper's power-law fitting methodology to diffusion or energy-based models to estimate their loss asymptotes compared to autoregressive baselines.

### Open Question 3
- Question: Does the observed 5.17× data efficiency improvement of the joint scaling recipe persist or diminish when scaling to trillions of tokens?
- Basis in paper: [inferred] Section 5.4 notes that the data scaling laws are "bound to be noisy" and rely on extrapolation from 200M–1.6B tokens to predict that efficiency gains "persist at higher token budgets."
- Why unresolved: While extrapolation suggests constant efficiency, asymptotic statistics imply different algorithms might converge to similar error rates under infinite data, potentially closing the gap at larger scales.
- What evidence would resolve it: Empirical validation of the joint scaling recipe at token counts orders of magnitude larger than the 200M seed tokens used in the main analysis.

## Limitations
- Scale extrapolation validity: Core claims rely heavily on power-law fits to models up to 1.4B parameters, with infinite scaling remaining theoretical
- Hyperparameter landscape smoothness: Coordinate descent assumes smooth loss surface for scale-invariant optimization, not rigorously proven
- Multi-view hypothesis evidence: Ensemble improvements attributed to feature diversity but lack direct mechanistic validation
- Downstream task transfer: 9% improvement shown on small benchmarks; larger evaluation suites would strengthen claims

## Confidence
- **High confidence**: Empirical observations about weight decay requirements (30× standard), monotonic scaling behavior under proper regularization, and basic ensemble improvements
- **Medium confidence**: Power-law fitting methodology and asymptote comparisons (reasonable but rely on statistical assumptions), self-distillation mechanism explanation (plausible but under-explained)
- **Low confidence**: Infinite compute extrapolation claims and multi-view hypothesis as primary explanation for ensemble gains (lack direct mechanistic validation)

## Next Checks
1. **Cross-scale hyperparameter transfer test**: Take hyperparameters optimized for 300M parameters and evaluate them directly on 1.4B parameters without any re-tuning. Measure the degradation in validation loss to quantify how much coordinate descent is actually finding scale-invariant optima versus scale-specific local minima.

2. **Representation diversity analysis**: For K=3 ensembles, compute centered kernel alignment (CKA) or similar similarity metrics between model representations. Verify that ensemble members learn genuinely different features rather than converging to similar representations despite different seeds.

3. **Extreme parameter scaling experiment**: Train a single model with 10× the parameters of the largest reported model (14B) on the same data budget. Measure whether the validation loss continues the observed monotonic trend or whether overfitting eventually overwhelms even aggressive regularization, testing the claimed infinite scaling regime.