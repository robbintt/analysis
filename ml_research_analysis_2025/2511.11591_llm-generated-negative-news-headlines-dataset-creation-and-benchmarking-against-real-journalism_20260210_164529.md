---
ver: rpa2
title: 'LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against
  Real Journalism'
arxiv_id: '2511.11591'
source_url: https://arxiv.org/abs/2511.11591
tags:
- headlines
- news
- dataset
- generated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study generated a dataset of 467 unique negative news headlines
  using an LLM and benchmarked it against real news datasets. Headlines were created
  across 58 categories using iterative prompts and validated by human experts.
---

# LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism

## Quick Facts
- arXiv ID: 2511.11591
- Source URL: https://arxiv.org/abs/2511.11591
- Authors: Olusola Babalola; Bolanle Ojokoh; Olutayo Boyinbode
- Reference count: 40
- Key outcome: Generated 467 unique negative news headlines using LLM, showing high semantic similarity (BERTScore F1 = 0.51) and low perplexity (1.16) compared to real news, but lacking proper nouns (p < 0.05).

## Executive Summary
This study demonstrates that large language models can generate synthetic negative news headlines that closely match real journalism in terms of semantic similarity, fluency, and readability. The dataset was created using iterative prompting across 58 categories, with human validation ensuring quality and negativity bias. While the generated headlines show high linguistic quality, they exhibit a notable absence of proper nouns compared to real news, suggesting a fundamental difference in style or a constraint of the generation process.

## Method Summary
The study used LLaMA 3 (70B parameters) to generate negative news headlines through a two-stage process: first allowing the model to determine categories, then prompting 35 predefined categories. The generation employed iterative prompts with constraints for negative valence, ~8-word length, and 30-50 headlines per batch. The resulting 467 unique headlines were validated by human experts and benchmarked against real news datasets (MN-DS with 10,821 headlines and Newsnow.com with 66 headlines) using metrics including BERTScore, perplexity, readability scores, and POS profiling.

## Key Results
- Semantic similarity between synthetic and real headlines: BERTScore F1 = 0.51
- Synthetic headlines show significantly lower proper noun usage than real news (p < 0.05)
- Generated headlines have higher readability scores (Grade 12.78) than real news (Grade 9.8)
- Lower standard deviation in semantic similarity indicates reduced linguistic diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative, constrained prompting allows LLMs to generate synthetic text that mimics the statistical distribution of real journalism in embedding space, despite not being derived from actual events.
- **Mechanism:** The system uses iterative prompts to force the LLM (LLaMA 3) to cover diverse categories, generating a corpus with high semantic overlap with real news because the model has learned the structure of news reporting during pre-training.
- **Core assumption:** Semantic similarity (BERTScore) and low perplexity are valid proxies for "realism" or utility in NLP training, even if the specific entities (proper nouns) are missing.
- **Evidence anchors:**
  - [abstract] "The synthetic headlines closely matched real news in semantic similarity (BERTScore F1 = 0.51)..."
  - [section 4.5] "The results indicate that the generated headlines exhibit a statistically significant higher average F1 score compared to the real-time dataset."
- **Break condition:** If the downstream task requires grounded entity recognition, this synthetic data fails because it lacks the specific token relationships found in real reporting.

### Mechanism 2
- **Claim:** LLMs acting as "generators" for negative news apply a generalization filter that systematically suppresses specific named entities (proper nouns), resulting in a distinct stylometric signature compared to human journalism.
- **Mechanism:** When prompted for "negative news" without specific context, the model defaults to generic scenarios rather than fabricating specific false attributions, likely due to safety fine-tuning or lack of access to real-time facts.
- **Core assumption:** The absence of proper nouns is a structural constraint of the generation process (or safety alignment), not merely a random artifact.
- **Evidence anchors:**
  - [abstract] "POS analysis revealed a significant lack of proper nouns compared to real news (p < 0.05)."
  - [section 4.1] "The divergence in proper noun usage is particularly noteworthy... reference headlines incorporate substantially more proper nouns than the main dataset."
- **Break condition:** If the prompt explicitly injects entities, this "genericness" mechanism would likely break, potentially violating safety filters or producing hallucinations.

### Mechanism 3
- **Claim:** Synthetic datasets generated for a specific valence (negative) exhibit lower semantic variability than organic datasets, effectively narrowing the distribution of the training data.
- **Mechanism:** By enforcing "negative valence" via prompt constraints, the system filters out the natural variance found in real news corpora, resulting in a "cleaner" but less diverse dataset.
- **Core assumption:** Lower standard deviation in semantic similarity implies reduced linguistic diversity, which may limit the robustness of models trained solely on this data.
- **Evidence anchors:**
  - [section 4.2] "The synthetic dataset headlines demonstrated the lowest standard deviation (0.0694)... In contrast, MN-DS short and MN-DS showed slightly higher standard deviations."
- **Break condition:** If the goal is to train a model to distinguish subtle shifts in tone, this homogenized dataset might fail to provide the necessary contrastive examples.

## Foundational Learning

- **Concept: BERTScore & Semantic Equivalence**
  - **Why needed here:** The paper relies on BERTScore F1 to claim the synthetic data "matches" real news. Unlike BLEU (which looks for exact word matches), BERTScore uses embeddings to capture meaning.
  - **Quick check question:** If two headlines share no words (e.g., "CEO quits" vs "Boss leaves job"), would BLEU or BERTScore show a higher match?

- **Concept: Perplexity & Fluency**
  - **Why needed here:** The paper uses perplexity (1.16 for synthetic vs 1.57 for real) to argue the synthetic text is highly fluent. Low perplexity means the model finds the text "predictable" or well-formed.
  - **Quick check question:** Does a low perplexity score guarantee the text is factually accurate, or only that it is grammatically fluid?

- **Concept: POS (Part-of-Speech) Profiling**
  - **Why needed here:** The paper's most critical finding comes from POS profiling, specifically the deficit of PROPN (Proper Nouns). Understanding POS tags is essential to diagnose why the synthetic data feels "generic."
  - **Quick check question:** Which POS tag would you analyze to determine if a text is describing specific people and places vs. general concepts?

## Architecture Onboarding

- **Component map:** Master Prompt -> LLaMA 3 (LLM) -> Iterative Generation -> Deduplication -> Human Validation -> 467 Headlines
- **Critical path:** 1. Define Master Prompt (Intent + Constraints) -> 2. Execute Multi-step Generation (Stage 1: LLM categories; Stage 2: Forced categories) -> 3. Curate (Deduplicate 510 -> 467) -> 4. Benchmark (Compare against MN-DS and Realtime sets)
- **Design tradeoffs:**
  - **Control vs. Diversity:** The paper enforces negative sentiment (Control) which reduces standard deviation/semantic variety (Diversity)
  - **Fluency vs. Specificity:** The model achieves high fluency (low perplexity) at the cost of specific entity usage (low PROPN score)
  - **N-gram vs. Embedding Eval:** The paper rejects BLEU/ROUGE (n-gram) in favor of BERTScore (embeddings) to evaluate "realism," trading precision for semantic flexibility
- **Failure signatures:**
  - **The "Generic" Signature:** Generated headlines contain high Noun/Adjective ratios but near-zero Proper Nouns (e.g., "Violence increases in region" vs "Riots in Paris")
  - **The "Complexity" Drift:** Synthetic headlines show higher readability scores (Grade 12.78) than real news (Grade 9.8), indicating a drift toward formal or complex phrasing not typical of standard headlines
- **First 3 experiments:**
  1. **Entity Injection Test:** Rerun the generation pipeline with prompts forcing specific named entities (e.g., "Include a country name") to verify if the PROPN deficit is a hard constraint or a prompting artifact
  2. **Classifier Robustness Test:** Train a sentiment classifier on this synthetic data and test it on the MN-DS real data to see if the "homogenized" semantic distribution hampers real-world accuracy
  3. **Stylometric Overfitting Check:** Use a simple binary classifier to distinguish Synthetic vs. Real headlines using only POS ratios; if successful, the synthetic data has a detectable "accent" that needs correction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the significant absence of proper nouns in LLM-generated negative headlines caused by safety alignment mechanisms?
- **Basis in paper:** [explicit] The authors note the "glaring absence" of proper nouns (0.5% vs 15.4% in real news) and speculate that "putting a proper noun in a synthetic negative news would be unacceptable."
- **Why unresolved:** The study quantified the lack of named entities but did not perform ablation studies to determine if this results from safety filters or inherent model limitations.
- **What evidence would resolve it:** A comparative study generating headlines using both safety-aligned and base (unaligned) models to see if proper noun usage increases.

### Open Question 2
- **Question:** Does removing length constraints alter the morphological similarity between synthetic and real headlines?
- **Basis in paper:** [explicit] The conclusion states the authors are "unable to inform about whether the morphology e.g. the length of unconstrained LLM-generated news headlines matches real headlines" due to their methodology.
- **Why unresolved:** The generation process artificially constrained headlines to an average of 8 words, preventing the observation of natural variance in length and structure.
- **What evidence would resolve it:** Generating a new dataset without token limits and comparing the resulting morphological distribution against a corpus of unconstrained real headlines.

### Open Question 3
- **Question:** Can stylometric analysis reveal "robotic" tendencies or homogeneity in synthetic headlines that semantic metrics failed to capture?
- **Basis in paper:** [explicit] The authors state they are "currently unable to undertake this examination" regarding whether the headlines "exhibit robotic tendencies, indicative of being generated in a consistent yet synthetic style."
- **Why unresolved:** The evaluation relied on semantic similarity, perplexity, and POS profiles, but did not assess deeper stylistic variance or "authorship" patterns.
- **What evidence would resolve it:** Applying stylometric feature extraction algorithms to measure the variance of writing style within the synthetic dataset compared to human-written sources.

## Limitations

- The generation process relies on iterative prompt engineering without disclosing exact prompt texts, making exact reproduction challenging
- The synthetic dataset lacks proper nouns, suggesting a fundamental limitation in generating entity-specific content that could be a prompting artifact or deeper constraint
- The dataset's homogenized distribution (lowest standard deviation among comparison sets) may limit its robustness for training diverse NLP models

## Confidence

- **High Confidence:** The observation that synthetic headlines have significantly lower proper noun usage compared to real news (p < 0.05) is well-supported by POS analysis and is a clear, measurable finding
- **Medium Confidence:** Claims about semantic similarity matching (BERTScore F1 = 0.51) and fluency (perplexity = 1.16) are based on established metrics, though the practical significance for downstream applications remains uncertain
- **Low Confidence:** The assertion that the synthetic dataset can effectively substitute for real news in training NLP models is speculative, given the stylistic differences and potential distributional limitations identified in the analysis

## Next Checks

1. **Entity Injection Test:** Modify the generation pipeline to explicitly require proper nouns in prompts (e.g., "Include at least one country name and one organization") and measure changes in POS distribution and semantic similarity scores

2. **Classifier Transferability Test:** Train a sentiment classifier on the synthetic dataset and evaluate its performance on real news data (MN-DS) to assess whether the homogenized semantic distribution impacts real-world accuracy

3. **Stylometric Discrimination Test:** Train a simple classifier to distinguish synthetic from real headlines using only POS ratios. If successful (accuracy > 80%), this confirms the synthetic data has a detectable "accent" that requires correction before practical deployment