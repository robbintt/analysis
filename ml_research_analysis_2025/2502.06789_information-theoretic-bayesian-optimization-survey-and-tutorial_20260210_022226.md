---
ver: rpa2
title: 'Information-theoretic Bayesian Optimization: Survey and Tutorial'
arxiv_id: '2502.06789'
source_url: https://arxiv.org/abs/2502.06789
tags:
- optimization
- information
- bayesian
- entropy
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of information-theoretic
  Bayesian optimization methods, which use concepts like entropy and mutual information
  to guide optimization processes. The survey covers the evolution of these methods
  from early approaches like IAGO to recent developments such as Alpha Entropy Search
  and Joint Entropy Search.
---

# Information-theoretic Bayesian Optimization: Survey and Tutorial

## Quick Facts
- arXiv ID: 2502.06789
- Source URL: https://arxiv.org/abs/2502.06789
- Reference count: 40
- This survey comprehensively covers information-theoretic Bayesian optimization methods that use entropy and mutual information to guide optimization processes.

## Executive Summary
This paper provides a comprehensive survey of information-theoretic Bayesian optimization methods that use concepts like entropy and mutual information to guide optimization processes. These methods evolved from early approaches like IAGO to recent developments such as Alpha Entropy Search and Joint Entropy Search. Information-theoretic acquisition functions are shown to outperform traditional methods by considering global information from the entire predictive distribution rather than just local information. The paper explores adaptations to complex scenarios including multi-objective, constrained, parallel, and high-dimensional optimization problems.

## Method Summary
Information-theoretic Bayesian optimization methods compute acquisition functions based on maximizing expected information gain about the optimum location or value. The core mechanism involves computing mutual information I(X, Y) between candidate evaluation points and a random variable representing the optimum (location x* or value y*). These methods typically outperform traditional acquisition functions by leveraging the full posterior distribution over functions. Key approaches include PES (targets p(x*), uses Expectation Propagation), MES (targets p(y*), analytical), and JES (targets joint distribution p(x*,y*), moment matching). The methods achieve tractability through reformulation strategies like exploiting mutual information symmetry or targeting univariate distributions.

## Key Results
- Information-theoretic acquisition functions typically outperform traditional methods by maximizing expected information gain about the optimum
- Global information processing from the full predictive distribution enables better exploration-exploitation balance than local criteria
- Tractability is achieved by reformulating intractable integrals through symmetries, bounds, or different target random variables
- These methods can be adapted to complex scenarios including multi-objective, constrained, parallel, and high-dimensional optimization problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information-theoretic acquisition functions outperform traditional methods by maximizing expected information gain about the optimum rather than local improvement metrics.
- Mechanism: The acquisition function computes mutual information I(X, Y) between a candidate evaluation point and a random variable representing the optimum (either its location x* or value y*). Points that maximize expected reduction in entropy of p(x*|D) or p(y*|D) are selected, leveraging the symmetric property I(X,Y) = I(Y,X) to enable tractable approximations.
- Core assumption: The surrogate model's posterior distribution accurately captures uncertainty about the objective function, and entropy reduction correlates with finding the global optimum.
- Evidence anchors: [abstract] "information theoretical acquisition functions, whose performance typically outperforms the rest of acquisition functions"; [section 4] PES exploits mutual information symmetry to simplify approximations; MES achieves analytical tractability by targeting p(y*) instead of p(x*).

### Mechanism 2
- Claim: Global information processing from the full predictive distribution enables better exploration-exploitation balance than local criteria.
- Mechanism: Unlike Expected Improvement, which evaluates utility based solely on the predictive distribution at a single point (μ(x), σ²(x)), information-theoretic methods integrate information from the entire posterior over functions. This is achieved through Monte Carlo sampling of function paths, optimization of samples to estimate p(x*), and computation of how hypothetical observations would shift this distribution.
- Core assumption: The posterior distribution over the optimum location/value can be approximated sufficiently well (via Expectation Propagation, Monte Carlo, or analytical bounds) to guide meaningful queries.
- Evidence anchors: [section 2] "Expected Improvement...tends to exploit more than explore, as it is based on local information, not taking into account regions where the uncertainty is high"; [section 4] IAGO builds empirical distribution p(x*|D) using s Monte Carlo samples from GP.

### Mechanism 3
- Claim: Tractability is achieved by reformulating intractable integrals through symmetries, bounds, or different target random variables.
- Mechanism: Three strategies reduce computational complexity: (1) PES uses mutual information symmetry to flip conditioning order; (2) MES targets univariate p(y*) instead of multivariate p(x*), enabling closed-form expressions with truncated Gaussians; (3) JES combines both but requires moment-matching approximations. FITBO further reduces cost by parameterizing the minimum as f(x) = η + ½g(x)².
- Core assumption: Approximations (Expectation Propagation, Monte Carlo, moment matching) introduce bounded error that does not reverse the ranking of candidate points.
- Evidence anchors: [section 4] MES formula (Eq. 20) achieves analytical form using truncated Gaussian properties; JES uses lower bound via moment matching; [section 4.1] PESMOC adds non-Gaussian constraint factors, increasing implementation difficulty.

## Foundational Learning

- Concept: **Mutual Information and Entropy**
  - Why needed here: Core quantity optimized by information-theoretic acquisition functions; understanding H(X), H(X|Y), and I(X,Y) is essential to interpret why these methods work.
  - Quick check question: Given two random variables with I(X,Y) = 0, what can you conclude about their relationship, and how would this affect an entropy search acquisition function?

- Concept: **Gaussian Process Posterior Predictive Distribution**
  - Why needed here: Provides the probabilistic model p(y|D,x) from which all information-theoretic quantities are derived; the entropy H(p(y|D,x)) is a direct term in acquisition functions.
  - Quick check question: Write the posterior predictive mean μ(x*) and variance σ²(x*) for a GP conditioned on data D, and explain how observation noise affects the entropy term.

- Concept: **Variational Inference and Expectation Propagation**
  - Why needed here: Most information-theoretic acquisition functions require approximating intractable integrals; ES and PES explicitly rely on EP for Gaussian approximations.
  - Quick check question: When would you prefer a Monte Carlo approximation over Expectation Propagation for entropy search, and what tradeoffs are involved?

## Architecture Onboarding

- Component map:
  Data D = {(xi, yi)} → GP Surrogate → Posterior p(f|D) → p(x*|D) or p(y*|D)
         ↓
  Acquisition Function α(x) = H[current] - E[H[posterior after observing x]]
         ↓
  Optimize α(x) → Select x_next → Evaluate f(x_next) → Update D

- Critical path:
  1. Implement GP surrogate with kernel selection (lengthscales critical for entropy estimates)
  2. Choose target random variable (optimum location vs. value vs. joint)
  3. Implement acquisition function approximation (start with MES for simplicity)
  4. Optimize acquisition function (LBFGS with multi-start recommended)
  5. Update GP and iterate

- Design tradeoffs:
  | Method | Target | Approximation | Complexity | Best For |
  |--------|--------|---------------|------------|----------|
  | PES | p(x*) | Expectation Propagation | O(N⁴) | Low-dim, high accuracy |
  | MES | p(y*) | Analytical + MC | O(K × N) | General use, easier implementation |
  | JES | p(x*, y*) | Moment matching | Moderate | When location AND value matter |
  | AES | p(x*, y*) | α-divergence ensemble | Higher | Robustness across problem types |

- Failure signatures:
  - Acquisition function returns NaN/Inf: Check GP kernel hyperparameters (degenerate covariance matrix)
  - Exploration stalls in local region: Acquisition function may be exploiting too heavily; verify entropy computation is not dominated by numerical errors
  - Computational time explosion: EP-based methods scale poorly; switch to MES or reduce discretization resolution
  - Poor performance in >8 dimensions: Information-theoretic methods struggle with curse of dimensionality; consider dimension reduction or alternative surrogates

- First 3 experiments:
  1. **Benchmark MES vs. EI on Branin function**: 50 iterations, 5 random seeds. MES should show better exploration of multi-modal structure; measure simple regret over time.
  2. **Compare PES and MES computational cost**: Profile wall-clock time per iteration as dataset size grows (N=10, 50, 100, 200). Expect PES to scale as O(N⁴), MES as O(K×N³) for GP prediction.
  3. **Test MES on constrained optimization (2D)**: Add a single black-box constraint; implement constrained MES using the indicator function approach. Verify feasibility rate improves over unconstrained MES with rejection sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generalized entropy measures, such as Sharma-Mittal entropy, outperform standard Shannon entropy in the formulation of acquisition functions for specific black-box scenarios?
- Basis in paper: [explicit] The conclusion suggests exploring "generalized notions of entropy, like the Sharma-Mittal entropy, to develop more general acquisition functions."
- Why unresolved: Current methods predominantly rely on Shannon entropy or $\alpha$-divergences; the potential benefits of other generalized entropies remain unexplored.
- Evidence: Empirical benchmarks comparing the convergence speed and robustness of Sharma-Mittal-based acquisitions against Max-value Entropy Search (MES) on standard test functions.

### Open Question 2
- Question: What are the formal theoretical upper bounds on cumulative regret for recent acquisition functions like Joint Entropy Search (JES)?
- Basis in paper: [explicit] The paper identifies the need to "study theoretical guarantees of convergence of these acquisitions, to determine for example upper bounds on the cumulative regret."
- Why unresolved: While information-theoretic methods show strong empirical performance, rigorous theoretical guarantees regarding regret bounds are currently sparse or non-existent for complex acquisitions.
- Evidence: A mathematical proof establishing sub-linear regret bounds for JES or Alpha Entropy Search (AES) under standard assumptions (e.g., Gaussian process priors).

### Open Question 3
- Question: Can information compression techniques be successfully integrated to allow information-theoretic Bayesian optimization to compete with metaheuristics in problems with cheaper evaluations?
- Basis in paper: [explicit] The author lists "enhancing scalability through information compression techniques of the evaluations to compete with metaheuristics" as a key future direction.
- Why unresolved: Information-theoretic methods are computationally intensive (often $O(N^4)$ or requiring Monte Carlo), limiting their application to problems where evaluations are cheap enough to use faster metaheuristics.
- Evidence: A scalable algorithm implementation that utilizes compression to reduce computational overhead while maintaining an advantage in sample efficiency over evolutionary strategies.

## Limitations

- High computational complexity, particularly for PES which scales as O(N⁴), limiting application to problems with expensive evaluations
- Poor scalability to high-dimensional problems (>8 dimensions) where entropy approximations become unreliable and curse of dimensionality effects dominate
- Reliance on accurate GP surrogate models - when the objective function violates GP assumptions (non-stationarity, discontinuities), theoretical guarantees may not hold

## Confidence

- **High confidence**: The theoretical foundation of mutual information as an acquisition function - supported by established information theory and the paper's mathematical derivations
- **Medium confidence**: The global vs. local information processing advantage - while intuitively compelling and mentioned in the paper, direct empirical evidence comparing information-theoretic methods to local methods across diverse benchmarks is limited in the survey
- **Medium confidence**: The tractability through reformulation strategies - the paper describes these strategies but provides limited quantitative analysis of approximation quality and computational tradeoffs

## Next Checks

1. **Scalability experiment**: Implement MES and PES on synthetic functions with increasing dimensionality (2D to 10D), measuring both optimization performance and wall-clock time per iteration. Verify the claimed breakdown around 8 dimensions.

2. **Approximation accuracy study**: For a 1D benchmark function, compute ground truth mutual information (via brute-force integration) and compare against PES (EP approximation), MES (analytical), and JES (moment matching) approximations. Quantify approximation error and correlation with optimization performance.

3. **Robustness to GP misspecification**: Apply information-theoretic BO to objective functions with discontinuities or non-stationary behavior. Compare performance degradation against EI, which may be more robust to model misspecification due to its local nature.