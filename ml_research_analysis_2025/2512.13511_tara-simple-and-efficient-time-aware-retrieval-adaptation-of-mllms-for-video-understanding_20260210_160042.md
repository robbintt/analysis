---
ver: rpa2
title: 'TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video
  Understanding'
arxiv_id: '2512.13511'
source_url: https://arxiv.org/abs/2512.13511
tags:
- video
- tara
- retrieval
- arxiv
- chiral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TARA, a simple method to adapt MLLMs for time-aware
  video-text retrieval without using any video data. It does so by fine-tuning on
  a small text-only dataset augmented with time-aware triplets mined from Ego4D.
---

# TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding

## Quick Facts
- **arXiv ID:** 2512.13511
- **Source URL:** https://arxiv.org/abs/2512.13511
- **Reference count:** 40
- **Primary result:** TARA adapts MLLMs for time-aware video-text retrieval using only text data, achieving state-of-the-art chiral benchmark performance.

## Executive Summary
TARA introduces a novel approach to adapt Multimodal Large Language Models (MLLMs) for time-aware video-text retrieval without requiring any video data during fine-tuning. The method leverages Explicit One-word Limitation (EOL) prompts to extract unified video-text embeddings from generative MLLMs, then fine-tunes on a small text-only dataset augmented with time-aware triplets mined from Ego4D. By using contrastive learning with chiral hard negatives (temporally opposite actions), TARA achieves superior performance on time-sensitive retrieval tasks while maintaining strong results on standard benchmarks. The approach demonstrates that a small proportion of high-quality temporal data can effectively adapt large models for directional time understanding.

## Method Summary
TARA adapts Tarsier-7B MLLM for time-aware video-text retrieval through text-only fine-tuning. The method uses EOL prompts ("Summarize the video in one word:") to extract unified embeddings from the LLM's last token hidden state. Training uses 10K text triplets: 9K from NLI (static-biased) and 1K from Ego4D with time-aware hard negatives. Hard negatives are generated by replacing chiral verbs with antonyms (e.g., "puts food on dish" → "takes food off dish"). The vision encoder and projection layer are frozen during fine-tuning, with only the LLM backbone updated using contrastive loss. Inference uses 16 uniformly spaced frames per video.

## Key Results
- Outperforms all existing models on the new chiral benchmark measuring time-sensitivity across SSv2, EPIC, and Charades datasets
- Achieves strong results on standard retrieval benchmarks (MSRVTT, DiDeMo) while specializing in temporal awareness
- Demonstrates improved negation understanding and verb/adverb recognition capabilities
- Shows that only 10K text samples (with 1K time-aware) are sufficient for effective temporal adaptation

## Why This Works (Mechanism)

### Mechanism 1: EOL Prompts Dissolve Modality Gap
EOL prompts force MLLMs to condense semantic meaning into single-token embeddings, creating comparable video-text representations. This eliminates the modality gap that typically separates video and text embeddings, enabling unified retrieval.

### Mechanism 2: Contrastive Learning with Chiral Hard Negatives
Training with temporally opposite actions (e.g., "climbing up" vs "climbing down") forces the model to encode directional semantics. The contrastive loss pulls temporally consistent pairs together while pushing chiral pairs apart.

### Mechanism 3: Targeted Temporal Data Composition
The 9K:1K split between static NLI and time-aware Ego4D triplets focuses the model's attention on temporal distinctions while maintaining general retrieval capability. Performance plateaus beyond 10K samples, suggesting this ratio is optimal.

## Foundational Learning

**Concept: Contrastive Learning with Hard Negatives**
- Why needed: TARA's training objective relies on distinguishing temporally opposite actions through embedding geometry
- Quick check: Given triplet (anchor: "opening door", positive: "door is being opened", negative: "closing door"), which loss component pushes the negative away from the anchor?

**Concept: Modality Gap in Multimodal Embeddings**
- Why needed: EOL prompts specifically address separation between video and text embedding spaces
- Quick check: What would retrieval performance look like if the modality gap persisted after EOL prompting?

**Concept: Chiral Actions and Temporal Consistency**
- Why needed: The entire benchmark and training data construction revolves around identifying temporally opposite actions
- Quick check: Is "walking forward" vs. "walking backward" a chiral pair? What about "lifting box" vs. "dropping box"?

## Architecture Onboarding

**Component map:**
Input Video (16 frames) → Vision Encoder (frozen) → Projection Layer (frozen) → LLM Backbone (fine-tuned) ← EOL Prompt Template → Last Token Hidden State → Embedding Vector

**Critical path:**
1. Frame sampling (F=16 uniformly spaced frames)
2. EOL prompt construction ("Summarize the video in one word:")
3. Last token extraction from LLM output
4. Cosine similarity computation for retrieval

**Design tradeoffs:**
- Data composition (α=0.1): Higher Ego4D fraction improves chiral accuracy on SSv2 but reduces generalization to EPIC/Charades
- Training scale: 10K samples vs. 275K full NLI; smaller dataset prevents drift from base model
- Freezing vision/projection: Preserves pre-trained visual features but limits temporal adaptation to LLM's attention

**Failure signatures:**
- Retrieval confuses "pushing left-to-right" with "pushing right-to-left" → insufficient chiral verb coverage
- Performance drops on non-chiral splits → overfitting to temporal distinctions
- Subject anonymization causes unrealistic text → subject replacement module failures

**First 3 experiments:**

1. **Baseline check:** Extract embeddings from base Tarsier-7B without TARA fine-tuning using EOL prompt. Verify modality gap exists and measure baseline chiral retrieval accuracy.

2. **Ablation on temporal data:** Train TARA variants with α ∈ {0.0, 0.1, 0.5, 1.0}. Plot chiral vs. non-chiral accuracy tradeoff to identify optimal configuration.

3. **Hard negative quality audit:** Manually inspect 50 generated temporal antonyms from Ego4D pipeline. Check chiral verb inversion, context preservation, and subject replacement quality.

## Open Questions the Paper Calls Out

**Open Question 1: Universal Encoder Extension**
The paper suggests investigating ways to include videos and other modalities in the training set to obtain a truly universal encoder. This remains unresolved because the text-only approach may limit full visual grounding of temporal concepts compared to multimodal training.

**Open Question 2: Two-Stage Retrieval Setup**
The authors propose exploring TARA in a two-stage (retrieval-and-reranking) setup to further boost performance. This is untested as the paper focuses on single-stage embedding extraction and retrieval.

**Open Question 3: Composed Video Retrieval Gap**
While TARA outperforms zero-shot baselines, it falls short of specialized models fine-tuned on WebVid-CoVR for composed video retrieval tasks. This performance gap needs further investigation to determine if text-only adaptation lacks necessary visual grounding.

## Limitations

- **Temporal Generalization Gap:** The method's ability to generalize to unseen temporal actions beyond the 35 unique chiral verbs remains untested
- **Modality-Specific Transfer Uncertainty:** It's unclear whether improvements come from temporal adaptation versus continued pre-training on video data
- **Subject Anonymization Artifacts:** Automated subject replacement could introduce semantic inconsistencies between anchor and hard negative

## Confidence

**High Confidence:** Experimental results showing TARA's superior performance on chiral benchmark are well-documented and reproducible
**Medium Confidence:** Mechanism of text-only triplet transfer to video retrieval is plausible but not rigorously validated
**Low Confidence:** Claim that EOL prompts "dissolve the modality gap" lacks direct empirical validation

## Next Checks

1. **Temporal Generalization Test:** Create held-out test set using chiral verb pairs not in Ego4D training data to measure generalization beyond known verb pairs

2. **Transfer Mechanism Validation:** Train TARA with frozen Tarsier-7B (including vision encoder) to isolate whether temporal adaptation occurs in LLM attention versus frozen vision components

3. **Subject Replacement Quality Audit:** Manually evaluate 100 hard negative triplets generated through subject anonymization pipeline to measure semantic inconsistency rate and impact on retrieval performance