---
ver: rpa2
title: 'Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance
  and Order Sensitivity'
arxiv_id: '2510.08648'
source_url: https://arxiv.org/abs/2510.08648
tags:
- curvature
- layer
- drift
- commutator
- attn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WILSON provides a minimal, post-hoc diagnostic suite for detecting
  invariance failures and order sensitivity in Transformers without retraining. It
  uses inverse-free curvature maps computed via JVPs and Hutchinson probes to localize
  regions where operator reordering or fusion risks output drift, alongside activation
  commutators to flag order-sensitive module pairs.
---

# Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity

## Quick Facts
- arXiv ID: 2510.08648
- Source URL: https://arxiv.org/abs/2510.08648
- Authors: Edward Y. Chang; Ethan Y. Chang
- Reference count: 36
- One-line primary result: WILSON provides a minimal, post-hoc diagnostic suite for detecting invariance failures and order sensitivity in Transformers without retraining, targeting 75% ROC AUC and 65% Spearman correlation.

## Executive Summary
WILSON introduces inverse-free curvature maps computed via JVPs and Hutchinson probes to detect invariance failures and order sensitivity in Transformers. The method uses discrete holonomy loops on a (position, layer) grid, comparing two transport paths ending at the same point to estimate curvature without matrix inversion. It also introduces activation commutators to flag order-sensitive module pairs and gauge-stable logging for reproducible CI integration.

## Method Summary
WILSON computes inverse-free curvature κ_inv using Jacobian-vector products (JVPs) with Rademacher probes to estimate transport operator differences without matrix inversion. The method samples targets and neighbors across positions and layers, computes commutators between submodules to predict reordering safety, and applies whitening plus Procrustes alignment for gauge-stable logging. The complete pipeline emits CSV artifacts (holonomy.csv, commutator.csv, ir.csv, gauge_stats.csv) with thresholds that guide safe fusion, reordering, and parallelization decisions while maintaining ≤20% runtime overhead.

## Key Results
- Targets 75% ROC AUC for predicting invariance breaks using curvature metrics
- Aims for 65% Spearman correlation between commutators and output drift
- Maintains ≤20% runtime overhead at default settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse-free holonomy computed via JVPs and Hutchinson probes can localize regions of order sensitivity in Transformers without matrix inversion.
- Mechanism: Forms discrete "loops" on a (position, layer) grid by composing vertical (layer-to-layer) and horizontal (attention-based) transport operators. Compares two paths ending at the same point: `T_layer * T_attn` vs. `T_attn * T_layer`. Uses squared norm difference averaged over random Rademacher probes to estimate curvature via `κ_inv^2 = E_v[||Av||^2] = ||A||_F^2` (Eq. 1), where `A = T_layer * T_attn - T_attn * T_layer`.
- Core assumption: The Hutchinson estimator with 6–12 probes sufficiently approximates the true Frobenius norm (concentration relies on stable rank; see Appendix A.3). Sampled targets and neighbors capture the critical edges for holonomy.
- Evidence anchors: [abstract] "inverse-free curvature map... computed via JVPs and Hutchinson probes to localize regions where operator reordering or fusion risks output drift"; [section 4.2.2] "we estimate the expectation with r Rademacher probes via JVPs... avoiding ill-posed inverses from LN/softmax/MLP".
- Break condition: If probes `r` are too few or `A` has low stable rank, estimation variance overwhelms signal. Also breaks if attention sparsity causes critical edges to be unsampled.

### Mechanism 2
- Claim: Activation commutators `Δ_A,B = ||A∘B(X) - B∘A(X)||_F` predict output drift under reordering/fusion of submodules.
- Mechanism: On a calibration batch, compute both orderings of two submodules (e.g., heads, sublayers) and measure representation-space deviation. High `Δ` flags order-sensitive pairs; low `Δ` indicates safe candidates for fusion/reorder/parallelization. Targets Spearman ρ≥0.65 between `Δ` and actual drift `δ`.
- Core assumption: Calibration batch inputs are representative of deployment distribution. `Δ` correlates linearly or monotonically with true output drift.
- Evidence anchors: [abstract] "activation commutators to flag order-sensitive module pairs"; [section 4.1] "Large Δ_A,B suggests reordering/fusing A,B risks output drift, while small Δ_A,B flags candidates for safe fusion".
- Break condition: If calibration batch is unrepresentative, or if floating-point nondeterminism introduces noise larger than the commutator signal.

### Mechanism 3
- Claim: Orthogonal gauge fixing (whitening + Procrustes) stabilizes cross-seed feature comparisons without altering curvature computations.
- Mechanism: Before logging, features are whitened (`H Σ^{-1/2}`) and Procrustes-aligned to a reference seed. Curvature `κ_inv` is computed in the native basis (Frobenius norm is orthogonal-invariant), but downstream logs use the fixed gauge for reproducibility.
- Core assumption: The relevant gauge group is approximately `O(d)`. Procrustes alignment preserves semantic structure while reducing variance.
- Evidence anchors: [section 4.3] "apply whitening and Procrustes for logging and cross-seed comparability, not when computing κ_inv"; [section 5, E2] "gauge-stable logging... variance ratios and Kendall-τ, pre/post gauge-fix".
- Break condition: If true gauge freedom exceeds `O(d)` or whitening is numerically unstable (near-zero covariance eigenvalues).

## Foundational Learning

- Concept: Jacobian-vector products (JVPs) as matrix-free directional derivatives
  - Why needed here: Core to estimating transports and holonomy without materializing full Jacobians.
  - Quick check question: Given `f: R^d -> R^d` and a probe `v`, what is `JVP(f, x, v)` mathematically?

- Concept: Hutchinson trace/energy estimators and concentration
  - Why needed here: Underlies the inverse-free curvature estimate and its variance.
  - Quick check question: Why does `E_v[||Av||^2] = ||A||_F^2` hold for `v` with `E[vv^T] = I`?

- Concept: Frobenius norm and orthogonal invariance
  - Why needed here: Curvature and commutator metrics are Frobenius norms, preserved under orthogonal transforms.
  - Quick check question: Show that `||Q A Q^T||_F = ||A||_F` for `Q ∈ O(d)`.

## Architecture Onboarding

- Component map: Input batch -> sample targets/neighbors -> frozen-softmax scan (propose hotspots) -> full JVP confirm on hotspots -> compute `κ_inv` -> compute `Δ_A,B` -> gauge-fix logging -> emit CSVs -> CI gate thresholds
- Critical path: Input batch -> sample targets/neighbors -> frozen-softmax scan (propose hotspots) -> full JVP confirm on hotspots -> compute `κ_inv` -> compute `Δ_A,B` -> gauge-fix logging -> emit CSVs -> CI gate thresholds
- Design tradeoffs:
  - Accuracy vs. cost: More probes/targets/neighbors (`r,k,m`) improves coverage but increases overhead (target ≤20%).
  - Scan vs. confirm: Frozen-softmax is cheap but approximate; full JVPs are accurate but expensive (confirm only hotspots).
  - Granularity: Module-level commutators (heads vs. sublayers) trade specificity for compute.
- Failure signatures:
  - High `κ_max` with low output drift: False positives; over-sensitive representation regions.
  - Low curvature but high drift: Missed long-range interactions or sampling gaps.
  - High cross-seed variance in logging: Gauge fix not applied or ineffective.
  - High overhead: `r,k,m` too large; reduce defaults or use scan-only mode.
- First 3 experiments:
  1. Unit test: On a synthetic linear model with known commutativity, verify `κ_inv ≈ 0` and `Δ_A,B ≈ 0`.
  2. Ablation: Sweep `(r,k,m)` on a 7B model calibration batch; plot overhead vs. ROC AUC for predicting invariance failures (target AUC≥0.75).
  3. Cross-seed stability: Compute `κ_inv` and `Δ_A,B` on 3–5 seeds; apply gauge fix and report variance ratios and Kendall-τ (target variance ratio ≤0.6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does geometric curvature (κ_inv) predict semantic anchoring success in the Unified Cognitive Consciousness Theory (UCCT), and can it guide anchor placement?
- Basis in paper: [explicit] Sections 1 and 8.2 state that empirical validation of the hypothesized synergies between WILSON diagnostics and UCCT remains future work.
- Why unresolved: While the authors hypothesize that low-curvature regions correspond to stable patterns for anchoring, no empirical correlation has been established.
- What evidence would resolve it: A shared evaluation protocol computing both WILSON metrics and UCCT anchoring strength (S) on identical tasks (e.g., code synthesis), demonstrating a significant correlation (R² > 0.5).

### Open Question 2
- Question: Can the inverse-free holonomy and commutator diagnostics be extended to encoder-decoder models, state-space models (SSMs), and mixture-of-experts (MoE) architectures?
- Basis in paper: [explicit] Section 8.2 lists applying the diagnostics to encoder-decoder and state-space models as future work; Section 8.1 notes current methods target decoder-only Transformers.
- Why unresolved: The current formalism relies on specific residual stream structures and transport definitions that may not map directly to SSM memory states or encoder-decoder crossings.
- What evidence would resolve it: Successful adaptation of the discrete transport definitions to alternative architectures, achieving the target ROC AUC (≥ 0.75) for invariance failure prediction.

### Open Question 3
- Question: Can a formal proof be established for the gauge invariance of the proposed orthogonal gauge-fixing pipeline (whiten plus Procrustes)?
- Basis in paper: [explicit] Section 8.1 explicitly states that a prior proof attempt for gauge invariance had a flaw, leaving a formal proof to future work.
- Why unresolved: The paper currently relies on empirical unit tests and variance reduction metrics rather than a theoretical guarantee of invariance under the O(d) group.
- What evidence would resolve it: A mathematical derivation proving that the logging pipeline produces gauge-invariant quantities, or a delineation of the specific conditions required for stability.

## Limitations

- The core claims that κ_inv and Δ_A,B reliably predict invariance failures and output drift respectively are weakly supported, with no direct corpus evidence and only internal metrics as validation targets.
- The Hutchinson estimator concentration properties and specific application to Transformers lack corpus validation, creating uncertainty in the inverse-free curvature approach.
- The orthogonal gauge fixing pipeline's effectiveness in reducing cross-seed variance for Transformer feature spaces has not been empirically validated.

## Confidence

- **High Confidence**: The orthogonal invariance of Frobenius norms and the mathematical framework of gauge fixing are well-established. The CSV logging infrastructure and CI integration approach are straightforward engineering implementations.
- **Medium Confidence**: The Hutchinson estimator concentration properties and the general approach of using JVPs for curvature estimation are sound, but their specific application here lacks corpus validation.
- **Low Confidence**: The core claims that κ_inv and Δ_A,B reliably predict invariance failures and output drift respectively are weakly supported, with no direct corpus evidence and only internal metrics (AUC≥0.75, ρ≥0.65) as validation targets.

## Next Checks

1. **Empirical Validation of Curvature Estimation**: Run WILSON on a synthetic Transformer variant with known position-sensitive regions (e.g., modified attention patterns) and verify that κ_inv correctly localizes these regions with the claimed AUC≥0.75. This directly tests Mechanism 1.

2. **Commutator-Drift Correlation Study**: For a 7B model, compute Δ_A,B across all attention head pairs and sublayer pairs on a calibration batch. Perform safe fusions/reorders and measure actual output drift δ. Report the empirical Spearman ρ and compare against the 0.65 target, validating Mechanism 2.

3. **Cross-Seed Stability with Gauge Fix**: Compute κ_inv and Δ_A,B on 5 different random seeds of the same model. Apply the whitening + Procrustes pipeline and report the cross-seed variance ratios and Kendall-τ values. Verify they meet the ≤0.6 and ≥0.8 targets, validating Mechanism 3.