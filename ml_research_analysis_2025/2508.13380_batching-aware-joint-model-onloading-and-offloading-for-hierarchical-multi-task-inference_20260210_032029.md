---
ver: rpa2
title: Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task
  Inference
arxiv_id: '2508.13380'
source_url: https://arxiv.org/abs/2508.13380
tags:
- edge
- task
- offloading
- onloading
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maximizing inference accuracy
  in hierarchical multi-task learning systems by jointly optimizing model onloading
  (which models to deploy) and task offloading (where to execute tasks) across clients,
  edge servers, and the cloud. The authors formulate this as a mixed-integer nonlinear
  program and propose J3O, an alternating optimization algorithm that decomposes the
  problem into greedy submodular model selection and constrained linear programming
  for task routing.
---

# Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference

## Quick Facts
- arXiv ID: 2508.13380
- Source URL: https://arxiv.org/abs/2508.13380
- Reference count: 35
- Authors: Seohyeon Cha; Kevin Chan; Gustavo de Veciana; Haris Vikalo
- One-line primary result: J3O achieves over 97% of optimal accuracy while incurring less than 15% of the runtime required by the optimal solver

## Executive Summary
This paper addresses the problem of maximizing inference accuracy in hierarchical multi-task learning systems by jointly optimizing model onloading (which models to deploy) and task offloading (where to execute tasks) across clients, edge servers, and the cloud. The authors formulate this as a mixed-integer nonlinear program and propose J3O, an alternating optimization algorithm that decomposes the problem into greedy submodular model selection and constrained linear programming for task routing. They extend this to BAJ3O to incorporate batching at the edge for improved GPU utilization. Experiments on three multi-task benchmarks (Taskonomy, DomainNet, Cityscape3D) show that J3O consistently achieves over 97% of the optimal accuracy while incurring less than 15% of the runtime required by the optimal solver.

## Method Summary
The authors formulate the joint model onloading and task offloading problem as a mixed-integer nonlinear program to maximize system-wide average inference accuracy under memory, compute, and communication constraints. They propose J3O, an alternating optimization algorithm that (1) greedily selects models to on-load at each tier using submodular maximization with Lagrangian relaxation, and (2) determines optimal offloading via constrained linear programming. BAJ3O extends J3O by incorporating batching constraints at the edge through surrogate linearization of the non-linear batching latency. The algorithm iterates between these two steps until convergence, achieving near-optimal solutions with significantly reduced computational complexity compared to solving the full MINLP.

## Key Results
- J3O achieves over 97% of the optimal accuracy while incurring less than 15% of the runtime required by the optimal solver
- BAJ3O further improves performance by effectively managing batching constraints, outperforming heuristic baselines
- The system maintains near-optimal accuracy across varying client heterogeneity and batching configurations
- J3O converges in less than 10 iterations on average across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Alternating Optimization of Coupled Variables
Decomposing the joint mixed-integer nonlinear program into model onloading (integer) and task offloading (continuous) subproblems allows for tractable near-optimal solutions. The algorithm iteratively fixes one variable set to solve for the other, leveraging the structure where hard combinatorial choices can be separated from flow allocation problems.

### Mechanism 2: Submodularity of Multi-Task Model Selection
The objective of selecting multi-task models to maximize weighted accuracy exhibits diminishing returns, enabling greedy selection to provide provable approximation guarantees. The utility of a model set is defined by the maximum accuracy it provides for a set of tasks, and adding models that cover already-covered tasks yields zero marginal gain.

### Mechanism 3: Batching Linearization via Surrogate Constraints
Linearizing the non-linear latency cost of batching allows the framework to optimize for GPU efficiency without solving intractable integer programming problems. The batching latency involving a non-convex ℓ₀-norm term is approximated using first-order Taylor expansion, turning the constraint into a linear inequality.

## Foundational Learning

- **Submodular Function Maximization**
  - Why needed here: This is the mathematical property that justifies using a fast greedy algorithm for model selection instead of exhaustive search
  - Quick check question: If you have models A and B, is the accuracy gain of adding B to the set {A} always less than or equal to adding B to the empty set? Why?

- **Alternating Optimization (AO)**
  - Why needed here: The core J3O algorithm relies on AO to decouple the integer (model) and continuous (routing) variables
  - Quick check question: Does Alternating Optimization guarantee convergence to the global optimum? What property of the subproblems ensures the objective improves monotonically?

- **Lagrangian Relaxation**
  - Why needed here: The paper handles complex compute constraints by moving them into the objective as penalty terms, decoupling the selection process across nodes
  - Quick check question: How does the "dual update" step (updating α) force the solution to respect the compute budget over iterations?

## Architecture Onboarding

- **Component map**: Client Node -> Edge Server -> Cloud
- **Critical path**: 
  1. Input: Task loads λ, Resource budgets (μ, β, κ), Model Library accuracies
  2. Step 1 (Onloading): Run Greedy-LR to select which models live on Client vs. Edge
  3. Step 2 (Offloading): Run LP solver to route traffic (Client → Edge → Cloud)
  4. Step 3 (Batching): (If BAJ3O) Adjust for batching window Tb and iterate
  5. Output: Config deployed to devices

- **Design tradeoffs**:
  - Optimality vs. Speed: J3O achieves ~97% of MINLP optimality but runs ~15× faster. Is strict optimality required for your SLA?
  - Batch Size (Tb): Larger Tb amortizes GPU kernel launch overhead (better throughput) but increases tail latency
  - Model Granularity: A library of many small single-task models vs. few large multi-task models

- **Failure signatures**:
  - Infeasibility: LP solver returns infeasible if communication budget κ is too low for the incoming query volume
  - Thrashing: Model onloading decisions fluctuate wildly between iterations if task loads λ are non-stationary
  - Latency Spike: If the batching surrogate underestimates setup cost, queues build up at the edge, violating the 2Tb latency bound

- **First 3 experiments**:
  1. Baseline Convergence: Reproduce Figure 3. Run J3O on the Taskonomy dataset and plot the objective value vs. iteration count. Verify convergence happens in <10 iterations
  2. Batching Sensitivity (BAJ3O): Reproduce Figure 7. Vary the batching window Tb and plot the accuracy vs. latency trade-off. Confirm that smaller Tb forces the optimizer to favor local execution
  3. Stress Test Communication: Reduce the offloading budget κe and observe at what point the system performance collapses compared to the "Full Local" baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How does J3O/BAJ3O perform under time-varying task demands and noisy load estimates in real-time deployment? The paper states this can be extended but doesn't characterize performance degradation or overhead costs.

- **Open Question 2**: Does relaxing the single-edge assignment constraint yield significant accuracy gains, and can the algorithm tractably handle multi-edge client associations? The current formulation restricts each client to exactly one edge server.

- **Open Question 3**: How sensitive is BAJ3O to errors in the batching setup cost estimation (νe_m), and does the surrogate linearization hold under extreme batching window (Tb) settings? The paper only tests Tb ∈ {0.5s, varying range} with estimated νe_m.

## Limitations

- The evaluation is limited to synthetic task distributions and may not reflect real-world temporal correlation or bursty traffic patterns
- The submodularity assumption may break down if dynamic task interference or negative transfer effects are significant
- The linearization of batching constraints assumes linear latency scaling, which may not hold under memory bandwidth saturation

## Confidence

**High Confidence**: The alternating optimization framework (J3O) provides tractable near-optimal solutions for the decomposed problem, supported by the monotonic improvement proof and experimental convergence in <10 iterations.

**Medium Confidence**: The submodular model selection provides approximation guarantees for the specific coverage function defined, but the static accuracy assumption may not hold in practice with dynamic task interference.

**Medium Confidence**: The batching linearization via surrogate constraints effectively approximates the batching overhead within tested operating ranges, but may underestimate setup costs under extreme load conditions.

## Next Checks

1. **Model Library Sensitivity**: Test J3O performance across different model library compositions (single-task vs. multi-task models) to quantify the impact of library design on achievable accuracy.

2. **Temporal Traffic Patterns**: Evaluate the system under bursty arrival processes and temporally correlated task demands to assess performance degradation from the synthetic Dirichlet distribution used in experiments.

3. **Cross-Device Generalization**: Validate the learned model placements and routing policies on heterogeneous device configurations not seen during training to test robustness to hardware variations.