---
ver: rpa2
title: Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement
  Fine-Tuning
arxiv_id: '2601.16419'
source_url: https://arxiv.org/abs/2601.16419
tags:
- domain
- knowledge
- learning
- shot
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement fine-tuning framework for multimodal
  large language models (MLLMs) to internalize domain knowledge at the optimization
  level rather than through prompts or captions. The method introduces domain-aware
  constraints and reward shaping to guide the model toward domain-consistent behaviors
  like rotation invariance and symmetry consistency.
---

# Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning

## Quick Facts
- **arXiv ID:** 2601.16419
- **Source URL:** https://arxiv.org/abs/2601.16419
- **Reference count:** 15
- **Primary result:** Domain knowledge injection via optimization-level constraints improves MLLM performance by up to 12.8% over supervised fine-tuning in few-shot remote sensing tasks.

## Executive Summary
This paper introduces a reinforcement fine-tuning framework for multimodal large language models (MLLMs) that internalizes domain knowledge at the optimization level rather than through prompts or captions. The method converts domain-specific constraints (like rotation invariance for remote sensing and symmetry consistency for medical imaging) into differentiable regularization terms that guide the model's policy distribution. Extensive experiments demonstrate consistent improvements across few-shot settings, with the approach outperforming both prompt-based and supervised methods by enforcing consistency at the distribution level.

## Method Summary
The framework uses Domain-Aware GRPO (Group Relative Policy Optimization) to inject domain knowledge through two components: a domain-aware constraint that minimizes KL divergence between original and transformed input distributions, and reward shaping that reweights advantages based on Jensen-Shannon divergence consistency. Domain transformations are applied to inputs (rotations for remote sensing, symmetry operations for medical imaging) while keeping prompts identical, creating a domain-support distribution that the model learns to align with. The method is tested on Qwen2.5-VL-3B across remote sensing datasets (UCM, AID, RSICD, WHURS19, PatternNet, NWPU) and medical imaging from MedMNIST v2, showing superior performance in 1-8 shot settings.

## Key Results
- Domain-aware constraint improves remote sensing accuracy by 12.8% over supervised fine-tuning in 1-shot settings
- Enforcing invariance at distribution level (DC) outperforms output-level constraints (OC) by 3+ points in 1-shot scenarios
- Distribution-level consistency enforcement shows significant gains over baseline methods across all few-shot experiments

## Why This Works (Mechanism)

### Mechanism 1: Optimization-Level Knowledge Internalization
The framework bypasses the semantic bottleneck of language by converting domain knowledge into geometric constraints on the policy distribution. Instead of telling the model "this image is rotation-invariant," the loss function forces the output distribution of original and rotated images to converge. This directly encodes abstract domain priors as optimization constraints rather than semantic instructions. The method succeeds where prompt-based approaches fail because abstract concepts like "invariance" cannot be reliably mapped to weight updates via language tokens alone.

### Mechanism 2: Distribution-Level Consistency Enforcement
The method constructs a "domain-support distribution" π^D_θ by passing transformed inputs through the model, then minimizes KL divergence between this distribution and the original policy π_θ. This regularizes the internal reasoning process rather than just final text output, making it more robust than enforcing consistency on greedy decodes. Distribution-level enforcement handles semantic equivalence better than output-level constraints because it regularizes the full probability distribution rather than a single prediction.

### Mechanism 3: Domain-Aware Advantage Reweighting
Sample efficiency improves when optimization focuses on samples that naturally exhibit high domain consistency. The method calculates Jensen-Shannon divergence between original and domain-support distributions for each sample, scaling reinforcement learning advantages by (1 - D_i) to down-weight inconsistent samples. This ensures the model learns primarily from samples where domain transformations produce consistent behavior, providing cleaner learning signals for domain adaptation.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: This is the base RL engine that estimates advantages by comparing outputs within a group, removing the need for a value model (critic)
  - Quick check: Can you explain how GRPO estimates the baseline advantage without a learned value function? (Answer: It uses the mean/std of rewards within the sampled group)

- **Concept: Divergence Measures (KL vs. JS)**
  - Why needed: The method relies on measuring "distance" between probability distributions to enforce constraints and shape rewards
  - Quick check: Why does the paper use KL for the constraint loss but JS for the reward shaping advantage? (Answer: KL is asymmetric and strictly penalizes probability mass going to zero; JS is symmetric and bounded, making it more stable as a multiplicative scalar)

- **Concept: MLLM Architecture (Vision Encoder + LLM)**
  - Why needed: You must understand how to manipulate inputs to the vision encoder while keeping LLM prompts identical to generate comparable distributions
  - Quick check: If you rotate the input image, do you need to change the text prompt? (Answer: No, the prompt text stays identical to allow the divergence comparison between π_θ and π^D_θ)

## Architecture Onboarding

- **Component map:** Input image + prompt -> Vision Encoder -> Policy Model π_θ -> Logits -> Distribution π_θ; Transform Branch: Input image -> Domain Transformation (rotation/symmetry) -> Vision Encoder -> Support Model π^D_θ -> Logits -> Distribution π^D_θ; Divergence Module: Computes D_KL(π^D_θ||π_θ) for constraint loss and D_JS(π^D_θ||π_θ) for advantage reweighting; Optimizer: Updates θ using combined GRPO objective + Domain Loss

- **Critical path:** The implementation of the `domain_support_distribution`. You must ensure that the forward pass of the transformed image uses the exact same model weights (θ) as the original image to compute the divergence correctly before the update step.

- **Design tradeoffs:**
  - KL vs JS: The paper explicitly notes that using KL for reward shaping destabilizes training, while using JS for constraints is suboptimal. Adhering to the specific configuration (KL for DC, JS for DR) is crucial
  - Data Augmentation vs. Distribution Constraint: Standard data augmentation yields marginal gains. The "Domain Constraint" (forcing consistency between distributions) is the active ingredient, not the augmentation itself

- **Failure signatures:**
  - Performance Degradation: If adding the domain constraint lowers accuracy, check if the transformation breaks the semantic label (e.g., rotating digit '6' to look like '9')
  - Collapse to Zero Divergence: If the model outputs uniform probabilities to satisfy the consistency constraint, the KL penalty weight β or domain loss weight may be too high

- **First 3 experiments:**
  1. **Sanity Check (Table 1 Reproduction):** Verify that simply prompting Qwen-VL with domain descriptions does not improve accuracy, establishing the need for optimization-level intervention
  2. **Constraint vs. Output (Table 7 Reproduction):** Compare enforcing rotation invariance via proposed Distribution Constraint (DC) vs. naive Output Constraint (penalizing different text outputs). Confirm DC wins
  3. **Divergence Ablation (Table 8 Reproduction):** Run a sweep using KL-KL vs. KL-JS to validate the authors' claim that bounded JS divergence is superior for reward scaling

## Open Questions the Paper Calls Out

- **Question:** Can this framework effectively integrate non-geometric domain priors (e.g., functional, causal, or hierarchical constraints) that cannot be expressed via simple input-space transformations?
  - Basis in paper: The authors state the formulation is "general" but only instantiate it using geometric transformations: rotation invariance and symmetry consistency
  - Why unresolved: The current method constructs π_D by applying visual transforms to the input. Abstract knowledge (e.g., "tumor volume correlates with malignancy") lacks a direct visual transformation operator, making it unclear how to encode such priors into the proposed constraint loss
  - What evidence would resolve it: Successful application of the framework to a domain defined by semantic or logical rules rather than spatial invariance by defining a corresponding constraint mechanism

## Limitations
- The method relies on domain properties that can be expressed as differentiable transformations, limiting applicability to domains where such properties are well-defined
- Performance gains in few-shot settings are impressive but the paper doesn't extensively explore how benefits scale with more training data
- The advantage reweighting mechanism using JS divergence is supported by ablation but lacks strong external validation

## Confidence
- Optimization-level knowledge internalization: High
- Distribution-level consistency superiority: High
- Domain-aware advantage reweighting effectiveness: Medium
- Generalizability to arbitrary domains: Low

## Next Checks
1. Test the method on domains where invariance constraints are ambiguous (e.g., medical imaging where transformations might change diagnostic features)
2. Compare JS divergence reweighting against alternative sample selection strategies in controlled ablation
3. Evaluate scaling behavior as training data increases from few-shot to full dataset scenarios