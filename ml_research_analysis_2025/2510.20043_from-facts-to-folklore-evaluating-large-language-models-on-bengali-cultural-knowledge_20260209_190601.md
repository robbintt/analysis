---
ver: rpa2
title: 'From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural
  Knowledge'
arxiv_id: '2510.20043'
source_url: https://arxiv.org/abs/2510.20043
tags:
- bengali
- context
- language
- cultural
- terms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BLanCK, a dataset to evaluate large language
  models (LLMs) on Bengali cultural knowledge. The dataset includes 5,265 terms from
  Wikipedia across 16 semantic categories, grouped into cultural, non-cultural, and
  miscellaneous domains, along with contextual descriptions and normalized monthly
  page views.
---

# From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge

## Quick Facts
- arXiv ID: 2510.20043
- Source URL: https://arxiv.org/abs/2510.20043
- Reference count: 12
- LLMs perform better on non-cultural vs. cultural Bengali topics, revealing significant knowledge gaps

## Executive Summary
This work introduces BLanCK, a dataset to evaluate large language models (LLMs) on Bengali cultural knowledge. The dataset includes 5,265 terms from Wikipedia across 16 semantic categories, grouped into cultural, non-cultural, and miscellaneous domains, along with contextual descriptions and normalized monthly page views. LLMs were tested on question answering and masked prediction tasks, both with and without context. Results show that model performance improves with term popularity and significantly with context, but all models perform better on non-cultural topics, revealing a gap in understanding Bengali culture and tradition. Closed-source models generally outperformed open-source ones, indicating limitations in the latter's Bengali knowledge.

## Method Summary
The BLanCK dataset was constructed by recursively scraping Bengali Wikipedia starting from the Bangladesh page, extracting 5,265 terms and their context descriptions. Terms were clustered into 16 categories using Community Detection Algorithm, with manual assignment. Monthly page views for 2024 were normalized as popularity scores. Questions were generated using Qwen-2.5-32B where terms were answers. For masked prediction, multi-word terms were expanded by masking each word separately. Six LLMs were evaluated via API at temperature=0 on QA and masked prediction tasks with/without context. QA accuracy was measured via substring matching, while masked prediction used MRR@5.

## Key Results
- All models perform significantly better on non-cultural topics than cultural ones, revealing knowledge gaps in Bengali cultural understanding
- Model performance improves substantially with context provision across all tasks and models
- Closed-source models (GPT-4o, Gemini 2.0 Flash) outperform open-source models (Llama3 70B, DeepSeek V3) on Bengali cultural knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual information significantly improves model performance on Bengali cultural knowledge tasks.
- Mechanism: Providing Wikipedia-derived context alongside questions bridges the parametric knowledge gap, allowing models to retrieve relevant information non-parametrically rather than relying solely on encoded weights that underrepresent Bengali cultural terms.
- Core assumption: The provided context accurately contains the answer, and models can attend to and extract this information correctly.
- Evidence anchors:
  - [abstract] "...performance improves substantially across all models when context is provided, emphasizing context-aware architectures..."
  - [section] "So, Bengali requires context regardless of term's popularity."
  - [corpus] CRaFT (arXiv:2510.14014) similarly emphasizes contextual evaluation for cultural reasoning.
- Break condition: If contexts are noisy, irrelevant, or contradictory, performance may degrade.

### Mechanism 2
- Claim: Model performance correlates with term popularity as measured by Wikipedia page views.
- Mechanism: Frequently viewed Wikipedia pages are likely overrepresented in pre-training corpora, creating stronger parametric representations for popular terms. Less popular but culturally significant terms receive weaker representations.
- Core assumption: Training data curation for evaluated LLMs correlates with web traffic patterns.
- Evidence anchors:
  - [abstract] "Results show that model performance improves with term popularity..."
  - [section] "Figure 2 shows that accuracy increases with term popularity... indicating LLMs have better knowledge of more popular terms."
  - [corpus] Mallen et al. (2022) and Maekawa et al. (2024), cited in the paper, document similar frequency-dependent gaps in English.
- Break condition: Correlation may weaken if models are fine-tuned on cultural corpora or if popularity metrics don't reflect training distributions.

### Mechanism 3
- Claim: Closed-source models outperform open-source models on Bengali cultural knowledge tasks.
- Mechanism: Closed-source models may benefit from proprietary training data with better multilingual coverage, larger capacity, or more sophisticated instruction tuning. Open-source models exhibit weaker semantic alignment for Bengali.
- Core assumption: Performance differences stem from training data and scale rather than architecture alone.
- Evidence anchors:
  - [abstract] "Closed-source models generally outperformed open-source ones, indicating limitations in the latter's Bengali knowledge."
  - [section] "The poor performance of open-sourced models points to issues with semantic alignment and deep reasoning."
  - [corpus] Corpus evidence is weak for direct open/closed comparisons; BLUCK (arXiv:2505.21092) evaluates Bengali cultural knowledge but does not focus on this split.
- Break condition: Targeted Bengali fine-tuning could narrow this gap.

## Foundational Learning

- Concept: **Parametric vs. Non-Parametric Memory in LLMs**
  - Why needed here: The study contrasts zero-shot (parametric) performance against context-augmented (non-parametric) performance.
  - Quick check question: Why does providing context "bypass" a model's parametric knowledge gap for low-resource cultural terms?

- Concept: **Low-Resource Language Challenges in NLP**
  - Why needed here: Bengali is widely spoken yet underrepresented in computational resources—central to the study's motivation.
  - Quick check question: What makes a language "low-resource" even with millions of speakers?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper evaluates context-aware architectures, a core RAG intervention.
  - Quick check question: How does RAG address parametric memory limitations, and what are its failure modes?

## Architecture Onboarding

- **Component map:**
  1. Wikipedia scraping → 5,265 terms → 16-category clustering → context extraction → Qwen-2.5-32B question generation → popularity normalization
  2. QA + Masked Prediction tasks × with/without context × 6 models
  3. Accuracy (QA), MRR (masked prediction) metrics

- **Critical path:**
  1. Scrape culturally diverse Bengali terms from Wikipedia
  2. Generate QA pairs with/without context
  3. Evaluate at temperature=0 to isolate knowledge
  4. Analyze performance gaps between cultural/non-cultural domains

- **Design tradeoffs:**
  - Wikipedia-only data vs. broader cultural sources
  - Substring matching vs. semantic answer evaluation
  - Multilingual models vs. potential monolingual Bengali models

- **Failure signatures:**
  - Inconsistent API responses at scale (Mistral Small 3.1 failed for masked prediction)
  - Substring matching edge cases with morphological variants
  - Limited representation of informal cultural knowledge

- **First experiments:**
  1. Evaluate substring matching accuracy by manual review of 100 QA samples
  2. Test context-augmented vs. zero-shot performance on 50 randomly selected terms
  3. Compare performance across cultural vs. non-cultural categories for all models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can monolingual Bengali models outperform current multilingual LLMs in capturing cultural nuances and specific language capabilities?
- Basis in paper: [explicit] The Limitations section states that future investigations could benefit from a deeper exploration of monolingual Bengali models to more precisely evaluate their language-specific capabilities.
- Why unresolved: This study exclusively evaluated multilingual models (e.g., GPT-4o, Llama, Gemini), leaving the performance gap compared to dedicated monolingual architectures untested.
- What evidence would resolve it: A benchmark comparison of monolingual Bengali models against the multilingual models tested on the BLanCK dataset.

### Open Question 2
- Question: Does the observed performance gap between cultural and non-cultural knowledge persist in generative NLP tasks like summarization and next sentence prediction?
- Basis in paper: [explicit] The authors note that expanding the evaluation scope to encompass additional NLP tasks—such as summarization and next sentence prediction—offers a promising direction for future research.
- Why unresolved: The current evaluation was restricted to question answering and masked token prediction, limiting the understanding of how cultural deficits affect generative coherence.
- What evidence would resolve it: Evaluation of the same models on the BLanCK dataset using metrics suitable for summarization and next sentence prediction tasks.

### Open Question 3
- Question: How does LLM performance on Bengali cultural knowledge change when evaluated on informal data sources compared to the formal Wikipedia entries used in this study?
- Basis in paper: [inferred] The paper notes the dataset relies exclusively on Wikipedia, characterized by formal language, and mentions alternative data sources like social media were not considered.
- Why unresolved: Cultural knowledge is often transmitted through informal channels; the current dataset may not capture the colloquial nuances or diverse linguistic registers found in everyday cultural discourse.
- What evidence would resolve it: A comparative analysis of model performance on a dataset derived from social media platforms versus the Wikipedia-based BLanCK dataset.

## Limitations

- Relies on Wikipedia coverage which may underrepresent certain cultural domains and exhibits editorial biases
- Substring matching for QA evaluation may not capture semantic correctness of answers
- Lacks comparison with other low-resource languages to contextualize Bengali-specific performance gaps
- Closed-source vs. open-source comparison lacks detailed architectural analysis to attribute performance differences

## Confidence

- **High confidence**: Context significantly improves performance across all models (consistent results across tasks and models)
- **Medium confidence**: Closed-source models generally outperform open-source models (observed results but lacks detailed architectural analysis)
- **Medium confidence**: Correlation between term popularity and model performance (statistically observed but requires validation of training data composition assumptions)

## Next Checks

1. **Substring Matching Validation**: Manually review 100 randomly sampled QA results to assess whether substring matching accurately reflects semantic correctness, particularly for morphological variants and paraphrased answers.

2. **Cross-Lingual Comparison**: Evaluate the same models on cultural knowledge benchmarks for another low-resource language (e.g., Polish using the recently published benchmark) to determine if Bengali-specific factors contribute to observed performance gaps.

3. **Training Data Analysis**: Investigate the pre-training corpora of evaluated models to empirically verify whether Wikipedia page view popularity correlates with term representation frequency, testing the core assumption underlying the popularity-performance relationship.