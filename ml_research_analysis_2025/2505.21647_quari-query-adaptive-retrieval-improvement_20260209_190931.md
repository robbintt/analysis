---
ver: rpa2
title: 'QuARI: Query Adaptive Retrieval Improvement'
arxiv_id: '2505.21647'
source_url: https://arxiv.org/abs/2505.21647
tags:
- retrieval
- quari
- query
- image
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QuARI, a query-adaptive retrieval framework
  that dynamically generates query-specific transformations for improving image retrieval
  performance. The method uses a transformer-based hypernetwork to predict low-rank
  projection matrices conditioned on each query, allowing for fine-grained adaptation
  of visual features without significant computational overhead.
---

# QuARI: Query Adaptive Retrieval Improvement

## Quick Facts
- arXiv ID: 2505.21647
- Source URL: https://arxiv.org/abs/2505.21647
- Reference count: 40
- Key outcome: Query-adaptive retrieval framework using transformer-based hypernetwork to predict low-rank projection matrices, achieving 5.3-19.7 mAP@1k improvements over strong baselines on ILIAS and INQUIRE benchmarks.

## Executive Summary
This paper introduces QuARI, a query-adaptive retrieval framework that dynamically generates query-specific transformations for improving image retrieval performance. The method uses a transformer-based hypernetwork to predict low-rank projection matrices conditioned on each query, allowing for fine-grained adaptation of visual features without significant computational overhead. QuARI is trained with semi-positive samples and noise injection to encourage robust feature transformations that improve retrieval accuracy. Experiments on challenging benchmarks (ILIAS and INQUIRE) show substantial improvements over strong baselines including CLIP, SigLIP, and fine-tuned models, with gains ranging from 5.3 to 19.7 mAP@1k depending on the task and backbone.

## Method Summary
QuARI uses a frozen VLM backbone to encode images and queries, then applies a transformer hypernetwork to generate per-query low-rank projection matrices. The hypernetwork takes zero-initialized tokens, iteratively refines them over L steps conditioned on the query embedding, and decodes them to form a rank-r projection matrix T. This matrix transforms all database embeddings before computing cosine similarities for retrieval. The method is trained with semi-positive samples (top-2 neighbors from 100 nearest neighbors) and noise injection on query embeddings to bridge text-image modality gaps. Training uses contrastive loss with softmax-weighted semi-positive targets.

## Key Results
- Achieves 5.3-19.7 mAP@1k improvements over CLIP, SigLIP, and fine-tuned models on ILIAS and INQUIRE benchmarks
- Outperforms traditional re-ranking approaches by orders of magnitude in speed while achieving higher accuracy
- Maintains <10ms inference time per query while providing substantial accuracy gains
- Successfully handles both image-to-image and text-to-image retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank query-conditioned projections specialize the embedding space to emphasize query-relevant subspaces while suppressing irrelevant dimensions, enabling fine-grained discrimination without re-encoding images.
- Mechanism: A transformer hypernetwork consumes the query embedding and outputs a rank-64 projection matrix T = Σᵤⱼvⱼᵀ via iterative token refinement. This linear transformation is applied to all database embeddings, rotating/scaling the feature space so dimensions useful for the specific query have higher discriminative variance. The low-rank constraint (r=64) limits overfitting and computational cost while preserving expressiveness.
- Core assumption: Pretrained VLM embeddings contain sufficient information for fine-grained discrimination but are distributed across many dimensions; appropriate subspace selection can isolate task-relevant features without needing new representations.
- Evidence anchors:
  - [abstract] "uses a transformer-based hypernetwork to predict low-rank projection matrices conditioned on each query, allowing for fine-grained adaptation of visual features"
  - [Section 3.2] "To constrain computation and promote generalization, we parameterize T as a low-rank matrix of rank r = 64"
  - [corpus] Patch-wise Retrieval addresses instance-level matching via patch-level features, supporting the need for fine-grained specialization, though does not validate low-rank projections directly.

### Mechanism 2
- Claim: Training with semi-positive samples prevents the transformation from overfitting to exact query-target pairs, encouraging ranked similarity gradients rather than binary match/non-match behavior.
- Mechanism: For each target, 100 nearest neighbors are retrieved using backbone embeddings; top-2 are designated semi-positives with softmax-derived target similarities as soft labels. The contrastive loss assigns weight αᵢⱼ=1 for positives, αᵢⱼ=wᵢⱼ for semi-positives, and 0 for negatives. This teaches the model to produce graded similarity scores that respect visual similarity.
- Core assumption: Nearest neighbors in the original embedding space provide meaningful "almost matches" that should be ranked higher than dissimilar images, even if they are not exact matches.
- Evidence anchors:
  - [abstract] "trained with semi-positive samples and noise injection to encourage robust feature transformations that improve retrieval accuracy"
  - [Section 3.3] "semi-positive samples discourages the behavior of overfitting to training query-target pairs, and encourages lower ranked images that seem visually similar to the target to be returned with higher similarity"
  - [corpus] No direct corpus validation; related work focuses on composed retrieval and temporal structures, not semi-positive mining.

### Mechanism 3
- Claim: Noise injection on query embeddings during training bridges the text-image modality gap, enabling image-to-image retrieval without requiring image queries during training.
- Mechanism: Query embeddings are perturbed: qᵢ ← qᵢ + U[0,1]×N[0,1], smoothing the mapping from query space to transformation space. This regularizer prevents overfitting to exact text embeddings and improves generalization to image queries, which occupy a different manifold region.
- Core assumption: Text and image query embeddings share neighborhood structure in the joint space, and noise approximates cross-modal variation.
- Evidence anchors:
  - [abstract] "trained with semi-positive samples and noise injection"
  - [Section 3.3] "add noise to every query embedding to help bridge the text-image modality gap. We borrow this formulation from LinCIR. This acts as a regularizer that also allows high performance with image queries, training only on text-image datasets."
  - [corpus] CoLLM and IDMR address multimodal queries but do not specifically validate noise injection for cross-modal retrieval.

## Foundational Learning

- Concept: Hypernetworks (networks that generate weights of another network)
  - Why needed here: QuARI uses a transformer hypernetwork to generate query-specific projection matrices dynamically.
  - Quick check question: Can you explain how a hypernetwork differs from standard meta-learning approaches that fine-tune a base model?

- Concept: Low-rank matrix decomposition (T = UVᵀ)
  - Why needed here: The projection matrix is parameterized as rank-64 to constrain capacity and computation.
  - Quick check question: What is the computational benefit of applying a low-rank transformation compared to a full-rank matrix?

- Concept: Contrastive learning with soft labels
  - Why needed here: Semi-positive samples use soft similarity targets rather than binary positives/negatives.
  - Quick check question: How does using soft labels for semi-positives differ from standard InfoNCE loss?

## Architecture Onboarding

- Component map:
  1. Frozen backbone encoder (CLIP/SigLIP) → produces query embedding q ∈ Rᴱ and database embeddings D
  2. Query encoder MLP → conditions control token on q
  3. Token bank → 2r zero-initialized tokens (U-tokens and V-tokens)
  4. Transformer backbone (4 layers) → iteratively refines tokens over L steps with sinusoidal positional encodings
  5. MLP decoders → decode refined tokens to uⱼ, vⱼ ∈ Rᴱ
  6. Projection matrix assembly → T = ΣMLPᵤ(uⱼ)·MLPᵥ(vⱼ)ᵀ
  7. Retrieval → cosine similarity between transformed query q′ and transformed database D′

- Critical path: Query embedding → control token conditioning → L-step token refinement → matrix decoding → database transformation → similarity computation. The iterative refinement (L steps) is the computational bottleneck at inference.

- Design tradeoffs:
  - Rank r=64: Lower rank reduces overfitting and compute but may limit expressiveness for complex queries.
  - Iterative refinement steps L: More steps improve transformation quality but increase latency; ablation shows 6.8–8.5 mAP drop without iteration.
  - Semi-positive count: Using 2 semi-positives from 100 neighbors balances training stability and gradient quality; too many may dilute signal.

- Failure signatures:
  - Transformation collapse: T approaches zero matrix → retrieval degrades to backbone performance. Check token magnitude after refinement.
  - Overfitting to text queries: Image-to-image retrieval underperforms text-to-image → inspect noise injection strength and modality gap.
  - Semi-positive noise: Unrelated neighbors selected as semi-positives → verify backbone embedding quality and neighbor relevance.

- First 3 experiments:
  1. Baseline validation: Run frozen CLIP/SigLIP retrieval on ILIAS/INQUIRE, then add QuARI to confirm reported mAP gains (5–20 points). Verify inference time is <10ms per query.
  2. Ablation on rank: Train QuARI with r∈{16, 32, 64, 128} on a subset of training data; plot mAP vs. rank to validate r=64 choice.
  3. Semi-positive analysis: Visualize top-10 neighbors before/after transformation for sample queries; confirm ground truth moves closer and semi-positives maintain graded similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-linear transformation strategies improve retrieval accuracy over QuARI's linear projections while maintaining computational tractability?
- Basis in paper: [explicit] The Limitations section states: "Exploring non-linear transformation strategies to overcome representational limitations while maintaining computational tractability is a promising avenue for future research."
- Why unresolved: Linear transformations are computationally efficient but restrict expressiveness; non-linear alternatives have not been explored.
- What evidence would resolve it: Comparisons of retrieval accuracy and inference time between QuARI and a non-linear variant on ILIAS and INQUIRE benchmarks.

### Open Question 2
- Question: How sensitive is QuARI's performance to the choice of low-rank dimension (r=64)?
- Basis in paper: [inferred] The paper fixes r=64 "to constrain computation and promote generalization" but provides no ablation or justification for this specific value.
- Why unresolved: The rank parameter trades off expressiveness against computational cost; optimal values may vary across tasks or backbone sizes.
- What evidence would resolve it: Ablation experiments varying r (e.g., 16, 32, 64, 128, 256) reporting mAP and inference time.

### Open Question 3
- Question: How well does QuARI generalize to retrieval domains beyond instance-level and ecological image search?
- Basis in paper: [inferred] Evaluation is limited to ILIAS (instance retrieval) and INQUIRE (ecological queries); training data includes only COCO, CC12M, and BioTrove.
- Why unresolved: It is unclear whether query-adaptive linear projections transfer effectively to domains with different fine-grained discrimination requirements (e.g., medical imaging, satellite imagery).
- What evidence would resolve it: Cross-domain evaluation on additional retrieval benchmarks (e.g., ROxford, RParis, fashion retrieval) without domain-specific retraining.

### Open Question 4
- Question: What is the impact of the number of iterative refinement steps (L) on accuracy and latency?
- Basis in paper: [inferred] The method refines tokens over L denoising steps, and ablation shows removing iterative generation causes a 6.8–8.5 mAP drop, but the optimal L is not explored.
- Why unresolved: More steps may improve transformation quality but increase latency; the current L is not specified or analyzed.
- What evidence would resolve it: Ablation over L (e.g., 1, 2, 4, 8, 16 steps) reporting mAP@1k and inference time per query.

## Limitations

- The method cannot recover discriminative information absent from the base model embeddings, limiting its effectiveness when the VLM fails to encode subtle visual attributes.
- Semi-positive mining assumes nearest neighbors in the original space are semantically relevant, which may introduce harmful gradients if the backbone embedding space contains artifacts.
- The training procedure requires extensive precomputed embeddings (100M images for ILIAS), creating significant barriers to replication and adaptation to new domains.

## Confidence

- **High confidence**: Retrieval performance gains (mAP@1k improvements of 5.3-19.7 on ILIAS and INQUIRE), ablation of core components (no iteration, no noise injection, no semi-positives), and speed advantage over re-ranking baselines (inference <10ms vs. 10,000× slower).
- **Medium confidence**: Cross-modal generalization (image-to-image retrieval performance with text-only training) and noise injection's role in bridging modality gaps—supported by ablation but lacking detailed analysis of failure modes.
- **Low confidence**: Sensitivity to rank choice (r=64) and hypernetwork architecture—no ablation on these design decisions, and reported gains may be contingent on specific hyperparameter settings.

## Next Checks

1. **Ablation on hypernetwork capacity**: Train QuARI with varying ranks (16, 32, 64, 128) and iteration counts (1, 2, 4, 8) on a subset of ILIAS to quantify the impact of architectural choices on mAP@1k. Verify that r=64 and L≥4 are near-optimal, not just coincidentally effective.

2. **Cross-modal robustness test**: Evaluate QuARI on image-to-image retrieval when trained with and without noise injection, and compare performance degradation when query embeddings are drawn from a disjoint distribution (e.g., Flickr30k instead of MS COCO). This isolates whether noise injection truly bridges modality gaps or merely regularizes training.

3. **Semi-positive mining reliability**: For 100 random ILIAS queries, visualize top-10 neighbors before and after transformation. Compute the overlap between ground truth targets and top-100 neighbors to quantify the quality of semi-positive samples. If overlap is low (<10%), the semi-positive strategy may introduce harmful gradients.