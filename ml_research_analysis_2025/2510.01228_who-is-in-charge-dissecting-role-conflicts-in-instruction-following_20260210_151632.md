---
ver: rpa2
title: Who is In Charge? Dissecting Role Conflicts in Instruction Following
arxiv_id: '2510.01228'
source_url: https://arxiv.org/abs/2510.01228
tags:
- system
- user
- social
- steering
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work mechanistically analyzes how large language models handle
  hierarchical instruction conflicts. It introduces linear probing to localize conflict-decision
  signals, finding they are encoded early in model layers and form distinct subspaces
  for system-user versus social conflicts.
---

# Who is In Charge? Dissecting Role Conflicts in Instruction Following

## Quick Facts
- **arXiv ID**: 2510.01228
- **Source URL**: https://arxiv.org/abs/2510.01228
- **Reference count**: 40
- **Primary result**: Large language models detect system-user instruction conflicts early but resolve them inconsistently, while social cues trigger stronger, more reliable compliance.

## Executive Summary
This work mechanistically analyzes how large language models handle hierarchical instruction conflicts. It introduces linear probing to localize conflict-decision signals, finding they are encoded early in model layers and form distinct subspaces for system-user versus social conflicts. Logit Attribution reveals stronger internal conflict detection for system-user cases but inconsistent resolution, while social cues drive consistent primary constraint compliance. Steering experiments show that vectors derived from social-bias representations unexpectedly amplify instruction-following behavior in a role-agnostic manner rather than restoring system authority. These findings demonstrate that system instructions are internally detectable but externally fragile, highlighting the need for lightweight, hierarchy-sensitive alignment methods.

## Method Summary
The paper uses Llama-3.1-8B-Instruct to analyze conflict resolution in instruction-following. It extracts hidden states at the final prompt token across all layers and positions, trains linear probes to classify compliance decisions (primary/secondary/neither), and uses Logit Attribution to measure internal conflict detection via attention-weighted contributions. Steering experiments inject vectors derived from consensus vs. system-user representation differences at Layer 12 MLP output. The evaluation uses an augmented benchmark with 120K prompts across five constraint types and four role-conflict framings.

## Key Results
- Conflict-resolution decisions are encoded linearly in early transformer layers (Layer 10-12), forming separable subspaces for system-user and social conflicts.
- Models show stronger internal conflict detection for system-user cases (25.61% show opposing logit contributions) but inconsistent resolution, while social cues resolve smoothly.
- Steering with social-bias vectors amplifies instruction-following behavior in a role-agnostic manner rather than restoring system authority.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conflict-resolution decisions are encoded linearly in early transformer layers, forming separable subspaces for distinct hierarchy types.
- **Mechanism**: Hidden states at the final prompt token contain linearly separable features corresponding to "obey primary," "obey secondary," or "neither." This signal peaks around layer 10–12 and degrades in later layers, suggesting the decision is "locked in" during early processing.
- **Core assumption**: The linear probe's high AUC implies the model relies on these directions for the decision, rather than the directions being correlational artifacts.
- **Evidence anchors**: High AUC > 0.89 in early layers; probe performance rises sharply around layer 10.
- **Break condition**: If ablating these early-layer directions does not change behavioral output, the "causal encoding" claim fails.

### Mechanism 2
- **Claim**: System-User hierarchies and Social hierarchies are processed in orthogonal representational subspaces, preventing simple transfer of "obedience" signals.
- **Mechanism**: Linear probes trained on system-user conflicts learn weight vectors nearly orthogonal (cos ≈ 0.02) to those trained on social conflicts (e.g., CEO-Intern).
- **Core assumption**: Orthogonality in probe weights reflects distinct processing pathways, not just distinct training data distributions.
- **Evidence anchors**: Probe weights show low cosine similarity between system-user and social conflict directions.
- **Break condition**: If a single "hierarchy vector" simultaneously boosts both system and social obedience, the orthogonality hypothesis is invalid.

### Mechanism 3
- **Claim**: Models detect system-user conflicts internally (Logit Attribution shows opposing forces) but fail to resolve them consistently, whereas social cues resolve smoothly.
- **Mechanism**: In system-user conflicts, attention heads attribute opposing logit contributions (positive vs. negative signs) to the constraints, indicating internal "fighting." However, the system constraint rarely wins. In social conflicts, the "dominant" cue creates consistent positive attribution with less internal opposition.
- **Core assumption**: Signed logit contribution shares accurately reflect the model's internal conflict state.
- **Evidence anchors**: System-user conflicts exhibit substantially more conflict detection with opposing signed contributions.
- **Break condition**: If downstream layers trivially "smooth out" these opposing signals, the attribution signature is an epiphenomenon.

## Foundational Learning

- **Linear Probing (Read-Only Interpretability)**: Locates where in the 32-layer stack the model "decides" who to obey. *Why needed*: Without this, interventions are blind. *Quick check*: If you train a probe on Layer 30 and it fails, does that mean the information isn't there, or that it's non-linear?

- **Logit Attribution (Causal Tracing)**: Distinguishes between "the model didn't see the conflict" and "the model saw the conflict but chose the user anyway" by observing opposing logit forces. *Why needed*: To understand internal conflict detection. *Quick check*: If SA is positive and SB is negative, is the model complying with Role A or Role B?

- **Activation Steering (Vector Injection)**: Tests if amplifying "social authority" directions can rescue "system authority." *Why needed*: To validate subspace relationships and test intervention strategies. *Quick check*: Why might adding a "social authority" vector to a "system prompt" latent state fail to make the model obey the system?

## Architecture Onboarding

- **Component map**: Input (System Instr, User Instr, Task) -> Early Layers (L1-L12: "Conflict Encoder") -> Mid-Late Layers (L12+: "Resolution/Generation") -> Output

- **Critical path**: The system-user conflict signal is detectable at L12 MLP output on the final prompt token. This is the intervention point.

- **Design tradeoffs**:
  - *System vs. Social*: System instructions are internally detected but behaviorally fragile. Social cues are "super-biases" that override internal conflict detection.
  - *Steering Location*: Intervening at L12 MLP yields "role-agnostic" instruction following rather than hierarchy restoration.

- **Failure signatures**:
  - **Role-Agnostic Amplification**: Steering makes the model follow the User instruction even harder when User is dominant, failing to restore System priority.
  - **Random Control Effects**: High-magnitude random vectors also disrupt fluency and instruction following, suggesting sensitivity to norm perturbations.

- **First 3 experiments**:
  1. Reproduce Linear Probes: Extract hidden states at Layer 12 MLP output on final prompt token. Train 3-class linear classifier. Verify AUC > 0.89.
  2. Logit Attribution Conflict Check: Run on System-User conflict prompt. Calculate SA, SB. Confirm opposite signs (conflict) but check which won.
  3. Steering Ablation: Construct v_steer = μ_consensus - μ_system-user at Layer 12 MLP output. Inject with α=5. Verify if model obeys System more, or just follows any instruction more aggressively.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can attention-based interventions selectively enforce system hierarchy without causing role-agnostic amplification?
- **Basis in paper**: [Explicit] Authors suggest adapting attention modification to boost system instructions while suppressing user instructions.
- **Why unresolved**: Current steering vectors caused role-agnostic amplification rather than specifically restoring system authority.
- **What evidence would resolve it**: Successful attention-modification techniques that increase "Primary Win" rate in System-User conflicts without degrading fluency.

### Open Question 2
- **Question**: Can mappings be learned between the orthogonal subspaces of social-role and system-user conflicts to transfer compliance?
- **Basis in paper**: [Explicit] Authors suggest learning mappings between contrastive subspaces (system-user ↔ CEO-intern) while preserving conflict detection.
- **Why unresolved**: Conflict types form distinct, orthogonal subspaces (cosine similarity ≈ 0), preventing simple subtraction methods from transferring the "super-bias" of social compliance to system instructions.
- **What evidence would resolve it**: A learned transformation that maps high-compliance social-consensus representation onto system-user representation, resulting in behavioral obedience rates for system prompts comparable to social prompts.

### Open Question 3
- **Question**: Why does random vector steering amplify instruction-following behavior?
- **Basis in paper**: [Inferred] Authors note a "striking and unexpected result" where "even random control steering increases instruction-following when scaled."
- **Why unresolved**: This suggests the model's "instruction-following" feature might be aligned with high-magnitude activations generally, or that the residual stream is highly sensitive to norm increases in specific dimensions.
- **What evidence would resolve it**: Ablation studies identifying specific dimensions or features responsible for general compliance, showing whether the effect is due to vector magnitude or alignment with a specific "obedience" direction.

## Limitations
- The claim that early-layer conflict signals are causal for behavioral compliance is supported by correlation but not definitively proven; ablation studies would be needed.
- The orthogonality of system-user vs. social conflict subspaces is based on probe weight similarity, which may conflate distinct data distributions with truly distinct processing pathways.
- Logit Attribution assumes signed share metrics accurately reflect internal conflict states, but downstream smoothing could render these signals epiphenomenal.

## Confidence
- **High confidence**: The existence of early-layer conflict signals detectable by linear probes (Mechanism 1) and the orthogonality of probe weights between system-user and social conflicts (Mechanism 2).
- **Medium confidence**: The claim that internal conflict detection (via Logit Attribution) does not guarantee consistent resolution, as this relies on interpreting signed logit contributions as conflict indicators.
- **Low confidence**: The steering vector's failure to restore system authority is interpreted as role-agnostic amplification, but the mechanism (why social-bias vectors boost any instruction following) remains unclear.

## Next Checks
1. **Probe Ablation**: Test whether ablating the identified early-layer conflict directions (Layer 12 MLP output) changes the model's behavioral compliance in system-user conflicts.
2. **Orthogonality Stress Test**: Verify if a linear combination of system-user and social conflict probe vectors can achieve high AUC on both conflict types, challenging the subspace orthogonality claim.
3. **Steering Attribution**: Analyze the steering vector's semantic content (e.g., via cosine similarity to known social-bias or instruction-following directions) to explain why it amplifies compliance in a role-agnostic manner.