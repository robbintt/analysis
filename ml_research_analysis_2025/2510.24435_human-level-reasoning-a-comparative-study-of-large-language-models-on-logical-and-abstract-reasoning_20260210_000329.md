---
ver: rpa2
title: 'Human-Level Reasoning: A Comparative Study of Large Language Models on Logical
  and Abstract Reasoning'
arxiv_id: '2510.24435'
source_url: https://arxiv.org/abs/2510.24435
tags:
- llms
- question
- reasoning
- correct
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the logical and abstract reasoning abilities
  of 15 large language models (LLMs) against human performance using 8 custom-designed
  reasoning questions. Results showed that while LLMs excelled in straightforward
  pattern recognition and basic calculations, they struggled with abstract reasoning,
  non-standard problem formats, and applying common-sense knowledge.
---

# Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning

## Quick Facts
- arXiv ID: 2510.24435
- Source URL: https://arxiv.org/abs/2510.24435
- Reference count: 0
- LLMs achieved 73.4% average accuracy vs 69.6% for humans on 8 custom reasoning questions

## Executive Summary
This study compared the logical and abstract reasoning abilities of 15 large language models (LLMs) against human performance using 8 custom-designed reasoning questions. Results showed that while LLMs excelled in straightforward pattern recognition and basic calculations, they struggled with abstract reasoning, non-standard problem formats, and applying common-sense knowledge. The average LLM score was 73.4% compared to 69.6% for humans overall, though human participants outperformed LLMs on several individual questions. The study highlights persistent limitations in LLMs' reasoning capabilities despite advances in natural language processing, particularly their difficulty adapting to unconventional problems and integrating disparate information.

## Method Summary
The study evaluated 15 LLMs and 80 human participants (54 students, 26 professors) on 8 custom-designed logical and abstract reasoning questions presented in Portuguese. Each LLM was tested twice with identical prompts, and the highest-scoring instance was selected. Questions covered deductive, inductive, abductive, and abstract reasoning types. Scoring used a qualitative rubric: 10 points for correct answers, 5 points for correct reasoning with incorrect answers, and 0 otherwise. Human participants were stratified by educational level, and their average performance served as a baseline for comparison with LLM results.

## Key Results
- LLMs achieved 73.4% average accuracy vs 69.6% for humans overall
- LLMs excelled at straightforward pattern recognition (100% on Caesar cipher) but struggled with abstract reasoning (26.7% on language alternation)
- Question 5 (month logic) was most challenging: only 6.7% LLM success vs 31.2% human success
- Question 4 (arithmetic with missing correct answer) revealed format rigidity: 40% of LLMs selected incorrect options
- Gemini 2.5 Pro was the only LLM to correctly identify the concatenation pattern in Question 5

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pattern recognition and statistical association enable LLMs to solve well-defined logical problems but fail when problems require integrating multiple disparate rules or recognizing non-obvious patterns.
- **Mechanism:** LLMs leverage statistical correlations from training data to recognize patterns (e.g., Caesar cipher, day abbreviations, binary addition). When problems map to familiar formats, performance is high. When problems require combining multiple rules (e.g., concatenation of squared position + letter count) or recognizing novel patterns, the statistical associations are insufficient.
- **Core assumption:** Success correlates with problem format similarity to training distribution; failure occurs when solution requires rule integration beyond surface patterns.
- **Evidence anchors:**
  - [abstract]: "LLMs excelled in straightforward pattern recognition and basic calculations, they struggled with abstract reasoning, non-standard problem formats, and applying common-sense knowledge"
  - [section 4.1]: Question 5 (most complex) had only 6.7% LLM success vs 31.2% human success; LLMs attempted polynomial fitting instead of recognizing concatenation pattern
  - [corpus]: "Navigating Semantic Relations" paper confirms LLMs struggle with deeper cognitive skills like common-sense understanding

### Mechanism 2
- **Claim:** LLMs struggle with unconventional problem formats (missing correct answers, hidden rules) because they lack mechanisms to question premise validity or detect meta-patterns.
- **Mechanism:** When presented with multiple-choice questions without a valid answer (Question 4: 3 + 3 × 5 = 18, not in options), 40% of LLMs still selected an incorrect option rather than identifying the premise error. Similarly, when rules require detecting patterns across languages (Question 7: even increment → English, odd → Portuguese), LLMs scored only 26.7%.
- **Core assumption:** LLMs optimize for providing answers within the constraints presented, rather than critically evaluating those constraints.
- **Evidence anchors:**
  - [section 4.1]: "The correct result of the calculation is 18, which was deliberately excluded from the provided answer choices... Several human respondents correctly performed the calculation but selected one of the available alternatives, a behavior also observed in some LLMs"
  - [section 4.1]: Question 7 with language alternation based on parity: LLMs scored 26.7% vs humans at 73.7%
  - [corpus]: "RuozhiBench" paper (weak relevance - different domain) addresses logical fallacies and misleading premises

### Mechanism 3
- **Claim:** LLMs exhibit inconsistent reasoning strategies, attempting analytical solutions (polynomial fitting, numerical substitution) even when simpler pattern-based solutions exist.
- **Mechanism:** When faced with novel patterns, LLMs default to complex analytical approaches rather than exploring simpler rule combinations. For Question 5, 47% of LLMs attempted polynomial solutions (7n² + 11n - 1) while only one LLM (Gemini 2.5 Pro) recognized the concatenation pattern.
- **Core assumption:** LLMs' training on mathematical problem-solving creates bias toward analytical solutions over pattern exploration.
- **Evidence anchors:**
  - [section 4.2]: "The most prevalent solution proposed by the LLMs (47% of the responses) involved calculating the value from a polynomial function"
  - [section 4.1]: Only Gemini 2.5 Pro identified concatenation pattern; DeepSeek "presented the most possibilities, attempting to substitute letters for numbers, match values from the ASCII table, search for a pattern with prime numbers"
  - [corpus]: "Hybrid Models for Natural Language Reasoning" paper explores compositionality vs. systematic generalization - directly relevant but not cited

## Foundational Learning

- **Concept: Deductive vs. Inductive vs. Abductive Reasoning**
  - **Why needed here:** The paper classifies questions by reasoning type (Q8 = abductive, Q1/Q3/Q4/Q7 = deductive, Q6 = inductive). Understanding these distinctions is essential for interpreting why LLMs succeed on some tasks and fail on others.
  - **Quick check question:** Given the pattern "SUN + 1 = MON, WED + 2 = FRI," is predicting "TUE + 2 = THU" an example of deduction, induction, or abduction?

- **Concept: Transformer Attention and Pattern Completion**
  - **Why needed here:** LLM performance differences (e.g., Question 6: 100% vs Question 7: 26.7%) reflect how attention mechanisms surface training distribution patterns. Understanding this helps predict where models will struggle.
  - **Quick check question:** Why would attention-based pattern completion succeed on Caesar cipher (Q3) but fail on language alternation based on parity (Q7)?

- **Concept: Evaluation Methodology: Qualitative vs. Quantitative Assessment**
  - **Why needed here:** The paper uses custom questions and qualitative scoring (correct answer = 10 points, correct reasoning = 5 points) rather than standard benchmarks. Understanding tradeoffs is critical for interpreting results.
  - **Quick check question:** What are the advantages and limitations of using custom-designed questions versus standardized benchmarks like Winograd Schema Challenge?

## Architecture Onboarding

- **Component map:** Input layer (8 custom reasoning questions in Portuguese) -> Evaluation framework (qualitative scoring: 10/5/0) -> Test subjects (15 LLMs + 80 humans) -> Question taxonomy (deductive/inductive/abductive/abstract) -> Comparison analysis

- **Critical path:**
  1. Present question with prompt: "Solve... and elucidate the reasoning process"
  2. Run two instances per LLM, select highest-scoring instance
  3. Score responses: correct answer (10) / correct reasoning only (5) / incorrect (0)
  4. Compare against human baseline (students vs. professors stratification)
  5. Analyze reasoning strategies (polynomial vs. pattern-based approaches)

- **Design tradeoffs:**
  - Custom questions allow controlled evaluation but reduce comparability with other studies
  - Qualitative scoring captures reasoning validity but introduces subjectivity
  - Two-instance testing improves consistency but increases computational cost
  - No feedback given to LLMs prevents gaming but removes learning opportunity

- **Failure signatures:**
  - **Analytical overreach:** LLM attempting polynomial/complex solutions when simple patterns exist (Q5: 47% attempted polynomial)
  - **Format rigidity:** Selecting incorrect option rather than identifying missing correct answer (Q4: 40% chose option C)
  - **Pattern blindness:** Failing to detect multi-dimensional rules (Q7: 26.7% LLM accuracy vs 73.7% human)
  - **Language confusion:** Defaulting to dominant language when alternation rule applies

- **First 3 experiments:**
  1. **Replicate with Chain-of-Thought prompting:** The paper notes "Some models possess Reasoning capabilities... but this was not utilized because not all models have this feature." Run same 8 questions with explicit CoT prompting on models that support it (GPT-o3-mini, DeepSeek R1, Claude 3.7 Sonnet) to measure improvement on Q5 and Q7.
  2. **Scaffolded Question 5 variants:** Present April value (missing in original) to models that proposed polynomial solutions. Test whether additional data point shifts strategy toward concatenation pattern vs. overfitting polynomial.
  3. **Cross-lingual parity:** Run English-only and Portuguese-only versions of Q7 to isolate whether failure is due to language switching or parity-rule detection. Compare performance to diagnose root cause of 26.7% accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does the application of Chain-of-Thought (CoT) prompting improve performance on these specific logical reasoning challenges compared to standard prompting?
  - **Basis in paper:** [explicit] The conclusion states, "Some models possess Reasoning capabilities (also known as Chain-of-Thought), but this was not utilized because not all models have this feature. However, it represents an important evaluation for future work."
  - **Why unresolved:** The study utilized a standard prompt ("Solve... and elucidate") without explicitly triggering step-by-step reasoning features (CoT) available in some models, leaving the potential performance gain from these features unmeasured.
  - **What evidence would resolve it:** A replication of the study evaluating the same models (e.g., DeepSeek R1 or GPT-o1) with CoT enabled versus disabled, measuring the score differential on the abstract reasoning questions (specifically Q5 and Q7).

- **Open Question 2:** Does providing intermediate ground-truth values (e.g., the value for April) shift the problem-solving strategy of LLMs from analytical derivation to the intended pattern recognition?
  - **Basis in paper:** [explicit] The text suggests, "for the fifth question, in the models that proposed an analytical solution... it would be interesting to inform them of the value for the month of April and verify the change in the solution approach."
  - **Why unresolved:** LLMs frequently failed Q5 by deriving incorrect polynomial functions (e.g., $7n^2 + 11n-1$) rather than identifying the concatenation pattern; it is unknown if additional data points would correct this "analytical" bias.
  - **What evidence would resolve it:** Running a follow-up experiment on Q5 where the prompt includes the derived value for April (167) and observing if models converge on the correct concatenation logic (position squared + letter count) rather than refining their polynomial formulas.

- **Open Question 3:** How does the average performance of LLMs across multiple stochastic runs compare to the "best-of-two" performance reported in this study?
  - **Basis in paper:** [inferred] The methodology notes that "two instances of each LLM were evaluated" and "The instance exhibiting the highest number of correct answers was selected," which may represent an optimistic upper bound rather than reliable performance.
  - **Why unresolved:** By selecting the highest-scoring instance, the study may have masked the inconsistency of LLMs, making their "human-level" comparison favorable against the average score of the human cohort.
  - **What evidence would resolve it:** A benchmark report including pass@k rates (e.g., pass@5 or pass@10) or the standard deviation of scores across 10+ instances per model to establish statistical reliability.

## Limitations

- **Custom questions reduce comparability:** The study uses 8 custom-designed questions rather than standardized benchmarks, limiting comparison with other research
- **Qualitative scoring introduces subjectivity:** The 10/5/0 scoring system requires subjective assessment of reasoning quality
- **Two-instance testing masks variability:** Selecting the highest-scoring instance may overstate LLM reliability

## Confidence

- **LLM Performance Patterns (High):** The differential success rates across question types (100% on Caesar cipher vs. 26.7% on language alternation) are well-supported by the data and consistent with known LLM limitations
- **Human vs. LLM Comparison (Medium):** While the average scores (73.4% LLM vs. 69.6% human) are reported, the custom nature of questions and qualitative scoring methodology introduce uncertainty in direct comparisons
- **Mechanism Explanations (Medium):** The proposed mechanisms (pattern recognition bias, format rigidity, analytical overreach) are plausible but would benefit from additional experimental validation through controlled variations

## Next Checks

1. **Chain-of-Thought Validation:** Re-run all 8 questions with explicit chain-of-thought prompting on models that support it (GPT-o3-mini, DeepSeek R1, Claude 3.7 Sonnet). Measure improvement specifically on Question 5 (month logic) and Question 7 (language alternation) to determine if reasoning capabilities are underutilized rather than absent.

2. **Format Compliance Testing:** Create variant versions of Question 1 (day abbreviations) and Question 7 (language alternation) that explicitly require 3-letter abbreviations or language specification. Test whether poor performance stems from format misunderstanding or genuine reasoning deficits.

3. **Cross-Domain Generalization:** Apply the same 8-question framework to smaller, more specialized models (e.g., Mistral 7B, Gemma 2) to determine whether performance degradation follows predictable scaling patterns or reveals architecture-specific limitations in abstract reasoning capabilities.