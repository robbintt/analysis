---
ver: rpa2
title: 'ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search'
arxiv_id: '2601.23232'
source_url: https://arxiv.org/abs/2601.23232
tags:
- video
- shot
- description
- frame
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ShotFinder, a benchmark for open-domain video
  shot retrieval, and proposes an imagination-driven retrieval method. The benchmark
  evaluates models using shot descriptions and five constraint dimensions (temporal,
  color, style, audio, resolution) across 1,210 YouTube video shots from 20 topics.
---

# ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search

## Quick Facts
- **arXiv ID:** 2601.23232
- **Source URL:** https://arxiv.org/abs/2601.23232
- **Reference count:** 32
- **Key outcome:** Significant performance gap exists between models (26.9%) and humans (88.5%) on open-domain video shot retrieval with temporal, color, style, audio, and resolution constraints.

## Executive Summary
ShotFinder introduces a benchmark for open-domain video shot retrieval from YouTube using text descriptions with five constraint dimensions. The method employs a three-stage pipeline: video imagination for query expansion, web-based retrieval, and description-guided temporal localization. Experiments reveal that while temporal localization is relatively tractable, color and style constraints remain major challenges, with current models achieving only 26.9% overall accuracy compared to 88.5% for humans.

## Method Summary
The ShotFinder pipeline consists of three stages: (1) Generator uses video imagination via LLM to expand shot-level descriptions into search queries that capture broader video context rather than local semantics, (2) Retriever uses search APIs to find candidate YouTube videos (2 queries × 2 URLs each), and (3) Localizer performs adaptive frame sampling (64/128/192 frames based on video length) followed by MLLM-based temporal grounding with audio support. The evaluation uses MLLM-assisted judgment comparing retrieved frames against ground truth.

## Key Results
- Human baseline: 88.5% accuracy on the ShotFinder benchmark
- Best model (GPT-5.2): 26.9% overall accuracy, showing a 61.6-point gap
- Temporal constraints: Most tractable, with models achieving up to 30.6% accuracy
- Color and Style constraints: Major challenges, with models achieving only 15.7% on Color
- Increasing frame sampling from 16-32-48 to 64-128-192 improves overall performance by 4.4 points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Video imagination improves retrieval recall by lifting shot-level descriptions to video-level search semantics.
- **Mechanism:** Instead of directly summarizing shot descriptions into keywords, the model first infers the broader video context (theme, genre, production type) that would plausibly contain the described shot. This generates queries matching how videos are actually titled and tagged on platforms, rather than matching isolated visual details.
- **Core assumption:** Video platform metadata (titles, tags, descriptions) encodes higher-level semantic categories that are more searchable than granular shot-level attributes.
- **Evidence anchors:** [abstract] "query expansion via video imagination"; [Section 4.2] "limited to local semantics of the shot"; [corpus] Query expansion survey confirms LLM-based QE helps with vocabulary mismatch.

### Mechanism 2
- **Claim:** Staged pipeline decomposition reduces the information imbalance between partial shot knowledge and full-video search targets.
- **Mechanism:** The task is decomposed into (1) semantic inference (what video might contain this), (2) web-scale retrieval (find candidate containers), and (3) fine-grained localization (verify the shot exists within). Each stage narrows uncertainty with different information: semantic priors → platform-scale statistics → precise visual matching.
- **Core assumption:** Search engines have sufficient coverage of YouTube content such that the target video appears in top candidates if queries are reasonable.
- **Evidence anchors:** [Section 3.1] "Information imbalance"; [Section 4.1] "staged design effectively combines shot-level descriptions"; [corpus] FactIR shows staged evidence gathering improves complex verification tasks.

### Mechanism 3
- **Claim:** Adaptive frame sampling with MLLM verification captures temporal structure better than fixed-rate sampling for varying video lengths.
- **Mechanism:** Videos are sampled at different rates based on duration (64 frames for <3min, 128 for 3–10min, 192 for >10min). MLLM then performs joint reasoning over frames plus audio to identify the matching moment. The keyframe-based evaluation approximates shot content without requiring full video processing.
- **Core assumption:** Uniform sampling within duration buckets adequately captures shot boundaries; MLLM can reliably perform cross-modal temporal reasoning.
- **Evidence anchors:** [Section 4.4] "adaptive frame sampling strategy"; [Figure 4C] "increasing the number of extracted frames consistently improves overall performance"; [corpus] MADTempo confirms frame density matters for temporal reasoning.

## Foundational Learning

- **Video Temporal Grounding (VTG):**
  - Why needed here: The localizer stage must identify which temporal segment matches the description among hundreds of frames.
  - Quick check question: Can you explain why moment detection in known videos differs fundamentally from open-domain retrieval where the video must first be discovered?

- **Query Expansion / Relevance Feedback:**
  - Why needed here: Shot descriptions are too specific for effective keyword search; imagination-based expansion bridges this vocabulary gap.
  - Quick check question: How does "imagining the container video" differ from standard pseudo-relevance feedback in text IR?

- **Multimodal Alignment (Vision-Language-Audio):**
  - Why needed here: Constraints span visual (color, style), temporal (action sequence), and audio modalities that must be jointly reasoned about.
  - Quick check question: Why does CLIP-based evaluation fail for this task while MLLM-assisted evaluation succeeds?

## Architecture Onboarding

- **Component map:** Input: Shot description + constraints → Generator: LLM imagination → Retriever: Search API → Localizer: Adaptive frame sampling + MLLM → Verifier: MLLM judgment → Output: Matching frame ID or N/A

- **Critical path:** Generator query quality determines whether the target video enters the candidate pool. If retrieval fails (Section G.2.2 shows this accounts for significant errors), downstream stages cannot recover.

- **Design tradeoffs:**
  - More queries (M) increases recall but costs more API calls and downloads
  - More frames sampled (X-Y-Z) improves temporal precision but hits context length limits
  - MLLM verification is expensive but necessary; CLIP similarity is cheap but unreliable for fine-grained constraints

- **Failure signatures:**
  - Generator errors: MLLM misinterprets description, generates queries for wrong video category
  - Retriever errors: Correct queries but no matching videos in search results, or only irrelevant videos returned
  - Localizer errors: Correct video retrieved but sampling misses the key moment, or MLLM issues false positive/negative judgment
  - Pattern: Temporal and Audio constraints are more tractable; Color and Style are hardest (Table 1)

- **First 3 experiments:**
  1. **Ablate imagination:** Replace video imagination with direct keyword extraction. Measure recall drop to quantify the mechanism's contribution.
  2. **Scale frame sampling:** Run the 16-32-48 vs 64-128-192 comparison on a held-out subset to determine optimal cost/accuracy tradeoff for your compute budget.
  3. **Error taxonomy analysis:** Manually classify 50 failures by pipeline stage (Generator/Retriever/Localizer) and constraint type to identify which constraint-category combinations are most brittle and where architectural improvements would yield highest returns.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-factor constraint combinations be effectively handled in open-domain video shot retrieval, beyond the single-factor design used in ShotFinder?
- **Basis in paper:** [explicit] The authors state in Section 3.2.1: "Multi-constraint shot retrieval is a more complex constraint problem, which we regard as a future research direction." They further note in Limitations that "Multi-factor narrative editing, which is a more complex and realistic challenge, requires addressing combined constraints in a holistic manner."
- **Why unresolved:** The single-factor design was chosen for control and attribution clarity; combining constraints introduces attribution ambiguity and complex interdependencies that the current benchmark and methods do not address.
- **What evidence would resolve it:** A benchmark extension with multi-constraint samples and a method that can jointly reason over combined temporal, color, style, audio, and resolution constraints with disentangled performance attribution.

### Open Question 2
- **Question:** What architectural or training improvements can close the substantial gap in Color and Style constraint retrieval, where even the best models (e.g., GPT-5.2 at 15.7% on Color) severely underperform relative to humans?
- **Basis in paper:** [explicit] Results in Table 1 and Section 5.2 state that "Color and Style remain the major challenges for current models," with GPT-5.2 achieving only 15.7% on Color versus 88.5% human average.
- **Why unresolved:** Increasing frame density alone does not resolve fine-grained visual attributes (Section 5.3), suggesting deeper issues in how models encode and match color/style semantics across retrieval and localization stages.
- **What evidence would resolve it:** Targeted ablations isolating the retrieval versus localization failure modes for Color/Style, coupled with improved visual-text alignment mechanisms showing consistent gains on these subtasks.

### Open Question 3
- **Question:** Can multi-turn interactive search strategies improve retrieval recall over the single-turn query expansion approach used in ShotFinder?
- **Basis in paper:** [explicit] The Limitations section states: "For query generation, we employed single-turn interactions, but we aim to expand to multi-turn interactions in future iterations."
- **Why unresolved:** Single-turn queries may miss high-level video metadata concepts; iterative refinement based on initial results could bridge the granularity gap, but the optimal interaction pattern remains unexplored.
- **What evidence would resolve it:** Experiments comparing single-turn vs. multi-turn retrieval pipelines on ShotFinder, measuring recall gains and identifying which constraint types benefit most from interactive refinement.

## Limitations

- **Model availability:** The paper relies heavily on proprietary models (Gemini-3-Pro, GPT-5.2) that are not publicly available, limiting reproducibility of the claimed performance results.
- **Dataset curation:** The 1,210-sample dataset was generated using Gemini-3-Pro with human verification, making it resource-intensive and potentially unreproducible without the curated dataset being released.
- **Evaluation methodology:** MLLM-assisted evaluation depends on specific prompt configurations and model versions, which may not generalize across different implementations or time periods.

## Confidence

- **High confidence:** The staged pipeline architecture (imagination → retrieval → localization) is logically sound and addresses the fundamental information imbalance problem. The observation that temporal constraints are more tractable than color/style constraints is well-supported by the empirical results.
- **Medium confidence:** The effectiveness of video imagination as query expansion depends critically on the specific prompting strategy, which is not fully detailed. The MLLM-based evaluation methodology is reasonable but may have systematic biases.
- **Low confidence:** Claims about the specific performance of GPT-5.2 and Gemini-3-Pro are difficult to verify given these models' limited availability. The cost-effectiveness claims (7.2x cheaper than video captioning) assume specific API pricing that may not hold across different providers or time periods.

## Next Checks

1. **Mechanism isolation experiment:** Implement ablation studies comparing direct keyword extraction vs. video imagination for query generation, measuring the specific contribution of the imagination mechanism to overall recall.

2. **Cross-model evaluation consistency:** Test whether the MLLM-as-judge evaluation produces consistent results across different model versions (e.g., GPT-4 vs GPT-4V vs Qwen-VL) and whether human judgments align with model predictions across all constraint types.

3. **Generalization stress test:** Evaluate the pipeline on videos from categories not represented in the training set (e.g., non-YouTube sources, different languages, or niche content) to assess whether the search and localization components can handle distributional shift beyond the curated dataset.