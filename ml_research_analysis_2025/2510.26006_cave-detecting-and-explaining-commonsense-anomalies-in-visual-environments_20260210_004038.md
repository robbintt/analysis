---
ver: rpa2
title: 'CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments'
arxiv_id: '2510.26006'
source_url: https://arxiv.org/abs/2510.26006
tags:
- anomaly
- anomalies
- image
- figure
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAVE, the first real-world visual anomaly
  benchmark designed to evaluate Vision-Language Models' (VLMs) ability to detect
  and explain commonsense anomalies. Unlike prior synthetic benchmarks, CAVE contains
  361 real-world images with 334 human-annotated anomalies, spanning diverse categories
  like entity presence/absence, attribute, spatial relations, uniformity breaches,
  and textual anomalies.
---

# CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments

## Quick Facts
- arXiv ID: 2510.26006
- Source URL: https://arxiv.org/abs/2510.26006
- Authors: Rishika Bhagwatkar; Syrielle Montariol; Angelika Romanou; Beatriz Borges; Irina Rish; Antoine Bosselut
- Reference count: 40
- First real-world visual anomaly benchmark with 361 images and 334 human-annotated anomalies

## Executive Summary
CAVE introduces the first real-world visual anomaly benchmark designed to evaluate Vision-Language Models' (VLMs) ability to detect and explain commonsense anomalies. The benchmark contains 361 real-world images with 334 human-annotated anomalies spanning diverse categories including entity presence/absence, attribute, spatial relations, uniformity breaches, and textual anomalies. Through three open-ended tasks—anomaly description, explanation, and justification—CAVE provides a comprehensive framework for assessing both visual perception and commonsense reasoning capabilities. Experiments with 8 state-of-the-art VLMs reveal significant performance gaps, with even the best model (GPT-4o) achieving only 57% F1-score on anomaly detection, highlighting the challenges in visual commonsense understanding.

## Method Summary
CAVE employs a cognitively-grounded approach to visual anomaly detection, focusing on real-world scenarios rather than synthetic data. The benchmark utilizes human annotators to identify and categorize anomalies across five main types: entity presence/absence, attribute, spatial relations, uniformity breaches, and textual anomalies. Each anomaly is accompanied by detailed annotations including severity ratings, surprisal levels, and complexity assessments. The evaluation framework incorporates three distinct tasks: describing the anomaly, explaining why it's anomalous, and justifying the explanation. This multi-task approach enables comprehensive assessment of both detection accuracy and reasoning capabilities. The benchmark tests multiple state-of-the-art VLMs using various prompting strategies, including zero-shot, few-shot, and Chain-of-Thought approaches.

## Key Results
- GPT-4o achieves only 57% F1-score on anomaly detection, demonstrating significant performance gaps in VLM capabilities
- VLMs perform better on surprising and severe anomalies compared to complex spatial reasoning and pattern detection tasks
- Models show particular difficulty with spatial relations and uniformity breaches, indicating challenges in visual commonsense reasoning
- Advanced prompting strategies provide only marginal improvements, suggesting fundamental limitations in current VLM architectures

## Why This Works (Mechanism)
CAVE works by grounding visual anomaly detection in real-world commonsense reasoning rather than synthetic patterns. The benchmark leverages human cognitive processes to identify anomalies that violate everyday expectations, forcing VLMs to engage with nuanced visual and contextual understanding. By incorporating multiple evaluation dimensions—detection, explanation, and justification—the framework captures both the technical accuracy and the reasoning depth required for true commonsense understanding. The human-annotated nature of the anomalies ensures ecological validity and relevance to real-world applications.

## Foundational Learning
The benchmark's effectiveness stems from its foundation in cognitive science principles, particularly the concept of surprisal in anomaly detection. By incorporating human-rated surprisal levels and severity ratings, CAVE aligns with established theories of human perception and attention. The benchmark builds on previous work in visual reasoning while addressing the gap between synthetic anomaly detection and real-world applications. The categorization scheme draws from cognitive frameworks for organizing visual information and detecting violations of expected patterns.

## Architecture Onboarding
CAVE is designed to be architecture-agnostic, allowing evaluation across different VLM architectures without requiring model-specific modifications. The benchmark uses standard image inputs with accompanying textual prompts, making it compatible with both transformer-based and other VLM architectures. The evaluation framework supports various prompting strategies, enabling researchers to test different architectural approaches to the same tasks. The multi-task evaluation structure provides insights into how different architectural components contribute to anomaly detection versus explanation capabilities.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of commonsense reasoning in VLMs, particularly how models can generalize from limited examples to novel anomaly types. It questions whether current architectural approaches can bridge the gap between pattern recognition and true commonsense understanding. The authors highlight the need for better evaluation metrics that capture the qualitative aspects of explanations beyond simple accuracy measures. They also raise questions about how to effectively incorporate temporal reasoning for dynamic anomaly detection scenarios.

## Limitations
The benchmark's reliance on human annotators introduces potential subjectivity in anomaly identification and categorization. The current scope may not fully capture the diversity of real-world scenarios, potentially limiting generalizability to all types of commonsense anomalies. The evaluation framework's dependence on textual prompts may not fully represent multimodal interaction capabilities. Additionally, the benchmark may favor certain types of commonsense knowledge over others, potentially biasing the evaluation of VLM capabilities.

## Confidence
High confidence in the benchmark's validity due to its foundation in human cognitive processes and comprehensive evaluation methodology. The use of real-world images and human-annotated anomalies provides strong ecological validity. The multi-task evaluation approach and diverse anomaly categories support robust assessment of VLM capabilities. However, some uncertainty remains regarding the generalizability of results across different cultural contexts and real-world applications.

## Next Checks
Further investigation needed into the cultural biases inherent in human-annotated anomalies and their impact on VLM evaluation. Additional work required to expand the benchmark with more diverse scenarios and anomaly types. Need to explore alternative evaluation metrics that better capture the quality of explanations beyond traditional accuracy measures. Future research should examine the relationship between VLM performance on CAVE and their practical utility in real-world applications. Additional analysis needed on how different prompting strategies affect performance across various anomaly categories.