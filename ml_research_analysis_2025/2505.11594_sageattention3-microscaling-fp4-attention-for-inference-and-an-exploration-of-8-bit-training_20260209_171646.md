---
ver: rpa2
title: 'SageAttention3: Microscaling FP4 Attention for Inference and An Exploration
  of 8-Bit Training'
arxiv_id: '2505.11594'
source_url: https://arxiv.org/abs/2505.11594
tags:
- attention
- quantization
- sagebwd
- should
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents two main contributions: First, it introduces\
  \ SageAttention3, the first microscaling FP4 attention implementation that achieves\
  \ 1038 TOPS on RTX5090 GPUs, representing a 5\xD7 speedup over the fastest FlashAttention\
  \ on the same hardware. The implementation uses NVFP4 format with microscaling quantization\
  \ and two-level scaling for the attention map to preserve accuracy."
---

# SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training

## Quick Facts
- arXiv ID: 2505.11594
- Source URL: https://arxiv.org/abs/2505.11594
- Authors: Jintao Zhang; Jia Wei; Pengle Zhang; Xiaoming Xu; Haofeng Huang; Haoxu Wang; Kai Jiang; Jianfei Chen; Jun Zhu
- Reference count: 40
- Primary result: First microscaling FP4 attention implementation achieving 5× speedup over FlashAttention on RTX5090 GPUs with controlled accuracy loss

## Executive Summary
This paper introduces SageAttention3, the first microscaling FP4 attention implementation for Blackwell GPUs, and SageBwd, the first trainable 8-bit attention for training tasks. The microscaling approach uses NVFP4 format with 1×16 block granularity and two-level scaling for the attention map to achieve 1038 TOPS on RTX5090 while maintaining high accuracy. For training, the work proposes keeping only one matrix multiplication in FP16 while quantizing the rest to INT8, enabling 1.67× speedup on RTX4090 for fine-tuning tasks across multiple benchmarks.

## Method Summary
The microscaling FP4 approach partitions Q, K, V matrices into 1×16 blocks, each with its own FP8 scale factor, achieving ~8× speedup over FP16. The two-level quantization strategy first normalizes each row of the attention map eP to [0, 448×6] via per-token scaling, then applies standard FP4 microscaling to the normalized values. For training, SageBwd keeps the dO·V^T matrix multiplication in FP16 while quantizing the remaining five backward matrix multiplications to INT8, addressing gradient error accumulation that occurs when using uniform 8-bit quantization.

## Key Results
- 5× speedup over FlashAttention2 on RTX5090 for inference tasks
- 1038 TOPS achieved on RTX5090 with NVFP4 microscaling
- Lossless fine-tuning performance across GSM8K, DROP, MMLU, and HellaSwag datasets using 8-bit attention
- 1.67× speedup on RTX4090 for training with INT8 attention (forward + backward)

## Why This Works (Mechanism)

### Mechanism 1: Microscaling FP4 Quantization for Matrix Multiplications
Applying FP4 microscaling to QK^T and PV matrix multiplications achieves ~8× speedup over FP16 with controlled accuracy loss. The mechanism partitions matrices into 1×16 blocks, each with its own FP8 scale factor (NVFP4 format, E2M1 data type). This granularity localizes outlier effects within small blocks rather than allowing them to degrade global quantization. The FP4MM hardware instruction performs dequantize-multiply-requantize in a single operation.

### Mechanism 2: Two-Level Quantization for Attention Map P
A two-level scaling strategy enables accurate FP4 quantization of the softmax output eP. First, normalize each row of eP to [0, 448×6] via per-token scaling (s_P1), storing this in FP32. Second, apply standard FP4 microscaling to the normalized eP2, producing FP8 scale factor s_P2 and FP4 values. During PV multiplication, the output is rescaled by s_P1. This exploits E4M3 FP8's full representable range (127 values vs 35 in direct quantization).

### Mechanism 3: Selective Precision for Backward Pass (SageBwd)
Keeping only the dO·V^T matrix multiplication in FP16 while quantizing the remaining four backward matmuls to INT8 achieves lossless fine-tuning performance. The gradient dS = P ◦ (dP - D) depends directly on dP = dO·V^T. Quantization error in dP propagates to dS, which accumulates across sequence blocks during FlashAttention's recurrent backward pass, compounding error in dQ and dK for long sequences.

## Foundational Learning

- **FlashAttention Tiling and Online Softmax**: Understanding block-based computation (Q, K, V divided into B_q × D and B_kv × D blocks) and online softmax (computing max and sum incrementally without materializing full N×N attention matrix) is essential to follow how two-level quantization integrates with softmax computation.
  - Quick check: Can you explain why FlashAttention avoids storing the full N×N attention matrix and how online softmax enables this?

- **Quantization Granularity and Scale Factors**: The paper relies on understanding per-tensor vs per-token vs per-block (microscaling) quantization. The 1×16 block structure with FP8 scale factors is central to NVFP4's accuracy advantage.
  - Quick check: If you quantize a 1024×64 matrix with per-tensor vs per-1×16-block scaling, how many scale factors does each approach require and which better handles outliers?

- **Tensor Core Precision and Throughput**: The 5× speedup claim rests on FP4 Tensor Cores delivering ~1600 TOPS vs ~200 TOPS for FP16 on RTX5090. Understanding that different precision modes have different hardware throughput is critical for evaluating tradeoffs.
  - Quick check: Why can't we simply use FP4 everywhere, and what hardware constraint forces scale factors to be E4M3 FP8 rather than FP32?

## Architecture Onboarding

- **Component map:**
  Input (Q, K, V in FP16) → [Smoothing K: K ← K - mean(K)] → [Smoothing Q: q̄ = mean(Q), quantize Q - q̄] → [Per-block Quantization ϕ]: Q, K, V → (âQ, s_Q), (âK, s_K), (âV, s_V) → [FP4MM]: S = FP4MM(âQ, s_Q, âK, s_K) + GEMV(q̄, K^T) → [Online Softmax + Two-level Quantization]: m = running_max, eP = exp(S - m), s_P1 = rowmax(eP) / (448×6), eP2 = eP / s_P1, (âP, s_P2) = ϕ(eP2) → [FP4MM with rescale]: O = FP4MM(âP, s_P2, âV, s_V) × s_P1 → [Accumulate and normalize]: O = O / running_sum → Output (O in FP16)

- **Critical path:** The two-level quantization of eP (lines 9-10 in Algorithm 1) is the most fragile component—incorrect s_P1 computation or range normalization causes cascading accuracy failure. For training (SageBwd), the backward pass's dO·V^T precision decision is the critical accuracy gate.

- **Design tradeoffs:**
  - **NVFP4 vs MXFP4:** NVFP4 uses 1×16 blocks with E4M3 FP8 scales; MXFP4 uses 1×32 blocks with E8M0 scales. NVFP4 trades finer granularity (more scale factors) for accuracy; MXFP4 trades coarser granularity for simpler implementation.
  - **Speed vs accuracy:** Full FP4 achieves 1038 TOPS but 99.55% cosine similarity; keeping sensitive layers in FP16 reduces accumulated error at speed cost.
  - **Training vs inference:** SageBwd INT8 achieves 1.67× speedup on RTX4090 for forward+backward but shows slower pretraining convergence; fine-tuning matches BF16.

- **Failure signatures:**
  - **Direct eP quantization without two-level:** Visual artifacts in video/image generation (Figure 12c shows severe degradation)
  - **Quantizing dO·V^T in backward:** dQ cosine similarity drops to ~97.5%, fine-tuning may diverge on long-sequence tasks
  - **Pretraining with INT8 attention:** Loss convergence is slower than BF16 (Figure 8a); may not reach same final loss within budget
  - **OOM on long sequences:** FlashAttention2 baselines OOM at 8K+ tokens on RTX5090 (Figure 4); SageAttention3 handles 32K

- **First 3 experiments:**
  1. **Kernel benchmark reproduction:** Run SageAttention3 vs FlashAttention2 on RTX5090 with varying sequence lengths (1K-32K) and head dimensions (64, 128) to reproduce Figure 4 speedup curves. Measure TOPS and verify ~5× claim.
  2. **Two-level quantization ablation:** On CogVideoX-2B layers, compare direct eP quantization vs two-level quantization, measuring cosine similarity, L1 error, and RMSE of output O. Should reproduce Table 1(b) showing ~6% cosine similarity gain.
  3. **Fine-tuning convergence test:** Fine-tune Qwen2.5-3B on GSM8K for 700 steps using SageBwd vs BF16. Compare final accuracy and loss curves to verify Figure 8(c) and Table 3 results showing <0.5% accuracy difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the convergence speed degradation observed in 8-bit attention during pretraining be mitigated to match BF16 performance?
- Basis in paper: [explicit] The authors state in the Conclusion and Results that SageBwd "exhibits slower convergence in pretraining tasks," restricting its applicability.
- Why unresolved: The paper demonstrates the problem exists but does not propose or test solutions to recover the convergence rate for pretraining workloads.
- What evidence would resolve it: A modified 8-bit backward attention mechanism that achieves training loss curves equivalent to BF16 on pretraining benchmarks (e.g., FineWeb-Edu).

### Open Question 2
- Question: What specific optimization strategies are required to bridge the performance gap between the current SageBwd Triton implementation and its theoretical speed upper bound?
- Basis in paper: [explicit] The authors explicitly note that the "noticeable gap between its current speed and theoretical upper bounds" may be caused by "suboptimal Triton kernel implementations."
- Why unresolved: The paper provides the current speedup data but leaves the optimization of the Triton kernels to future work.
- What evidence would resolve it: An updated implementation demonstrating end-to-end training latency that approaches the theoretical peak TOPS on supported GPUs.

### Open Question 3
- Question: Can the dO V^T matrix multiplication, currently kept in FP16 to prevent gradient error accumulation, be safely quantized?
- Basis in paper: [inferred] The methodology identifies dO V^T as accuracy-sensitive and maintains it in FP16, representing a bottleneck in the fully quantized pipeline.
- Why unresolved: The authors show that quantizing this step hurts accuracy but do not explore if smoothing or advanced quantization could enable full 8-bit execution.
- What evidence would resolve it: A technique that allows dO V^T to be computed in 8-bit while maintaining the gradient cosine similarity and downstream task performance reported in the paper.

## Limitations

- The microscaling approach's effectiveness depends heavily on the spatial locality of outlier values in activation distributions, which may not hold for attention matrices with different statistical properties
- The two-level quantization mechanism for eP is specifically designed for E4M3 FP8 format and may not generalize to other quantization schemes
- The claim of "lossless" fine-tuning is supported by specific benchmarks but may not generalize to all tasks or longer sequences

## Confidence

- **High confidence:** The 5× speedup claim on RTX5090 is well-supported by kernel benchmarks (Figure 4) and hardware specifications (1038 TOPS vs 200 TOPS for FP16). The two-level quantization mechanism's accuracy improvements (Table 1b, Figure 3) are directly measurable through cosine similarity and error metrics.
- **Medium confidence:** The training results (SageBwd) show strong finetuning performance but weaker pretraining convergence (Figure 8a). The claim of "lossless" fine-tuning is supported by GSM8K/DROP/MMLU results but may not generalize to all tasks or longer sequences.
- **Low confidence:** The generalization of microscaling beyond the tested video generation and language models remains uncertain. The paper doesn't extensively test different model architectures or attention distributions that might break the locality assumptions.

## Next Checks

1. **Generalization test:** Apply SageAttention3 to a different attention-based model (e.g., Vision Transformer) and measure accuracy degradation vs. FlashAttention2 across varying sequence lengths to verify the locality assumptions hold for non-text data.

2. **Pretraining stability experiment:** Run a controlled pretraining study with varying sequence lengths using SageBwd vs BF16, measuring not just convergence speed but final task performance after fine-tuning to quantify long-term gradient quality impact.

3. **Two-level quantization ablation:** Systematically vary the normalization factor (448×6) and test different FP8 formats (E8M0 vs E4M3) to understand the sensitivity of the two-level approach to hardware constraints and identify the minimum viable precision requirements.