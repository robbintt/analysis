---
ver: rpa2
title: Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information
arxiv_id: '2512.03074'
source_url: https://arxiv.org/abs/2512.03074
tags:
- fairness
- sensitive
- graph
- eosp
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-agnostic fairness regularization framework
  for Graph Neural Networks (GNNs) with incomplete sensitive attribute information.
  The method integrates equal opportunity and statistical parity as differentiable
  regularization terms during training, enabling fair learning even when sensitive
  attributes are only available for labeled nodes.
---

# Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information

## Quick Facts
- **arXiv ID**: 2512.03074
- **Source URL**: https://arxiv.org/abs/2512.03074
- **Reference count**: 40
- **Primary result**: Model-agnostic framework achieving significant bias mitigation in GNNs with incomplete sensitive attribute information while maintaining competitive accuracy

## Executive Summary
This paper introduces a model-agnostic fairness regularization framework for Graph Neural Networks (GNNs) that addresses the challenge of incomplete sensitive attribute information. The method integrates differentiable statistical parity and equal opportunity regularization terms into the training objective, enabling fair learning even when sensitive attributes are only available for a subset of labeled nodes. By operating on continuous probability predictions rather than discrete classifications, the framework provides effective gradients for bias mitigation during backpropagation. The approach demonstrates strong performance across five real-world datasets, achieving favorable fairness-accuracy trade-offs through Bayesian hyperparameter optimization.

## Method Summary
The framework extends standard GNN training by adding fairness regularization terms to the loss function. It computes differentiable statistical parity loss (L_SP) and equal opportunity loss (L_EO) using continuous probability predictions over labeled nodes with known sensitive attributes. These losses enforce independence between predictions and sensitive attributes (statistical parity) and balance true positive rates across groups (equal opportunity). The total loss combines prediction loss with weighted fairness terms: L_total = L_pred + α·L_EO + β·L_SP. Hyperparameters α and β are optimized using Bayesian optimization (Optuna) to navigate the fairness-accuracy trade-off space. The method is model-agnostic and can be applied to any standard GNN architecture.

## Key Results
- Achieved near-perfect statistical parity (ΔSP = 0.16) on NBA dataset with 50% labeled sensitive attributes
- Outperformed baseline models on German dataset with BACC = 77.53% and ΔSP = 0.15
- Successfully navigated fairness-accuracy trade-off through Bayesian optimization, identifying optimal configurations within 15-25 trials

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiable statistical parity regularization constrains prediction independence from sensitive attributes during gradient descent
- Mechanism: Replaces discrete statistical parity evaluation with continuous approximation using predicted probabilities p_i. The loss L_SP = |mean(p|s=1) - mean(p|s=0)| is computed only over labeled nodes with known sensitive attributes, providing gradients that penalize divergent mean predictions across groups
- Core assumption: The labeled subset's sensitive group distributions approximate the full graph's fairness properties
- Break condition: When labeled nodes are not representative of sensitive group distributions, the proxy loss may optimize for the wrong objective

### Mechanism 2
- Claim: Equal opportunity regularization enforces balanced true positive rates without requiring uniform decisions across groups
- Mechanism: The loss L_EO = |mean(p|y=1,s=1) - mean(p|y=1,s=0)| operates only on positive-class labeled nodes within each sensitive group, targeting error distribution fairness rather than outcome equality
- Core assumption: Positive class labels y=1 are reliably available for the labeled subset, with sufficient positive examples in both sensitive groups
- Break condition: When either P_1 or P_0 has very few samples, the mean probability becomes unstable, causing gradient noise or overfitting to sparse positive examples

### Mechanism 3
- Claim: Bayesian hyperparameter optimization successfully navigates the multi-objective fairness-accuracy trade-off space
- Mechanism: Uses Optuna to optimize α and β coefficients against a hybrid validation score (BACC + fairness penalties). The paper shows 15-25 trials can identify configurations achieving strong fairness gains with minimal accuracy loss
- Core assumption: The validation set's fairness-accuracy trade-off generalizes to test data
- Break condition: When validation set is too small or not representative, selected α,β may overfit to validation noise

## Foundational Learning

- Concept: Statistical Parity vs Equal Opportunity fairness definitions
  - Why needed here: Framework combines two distinct fairness notions with different inductive biases. Understanding their trade-offs is essential for interpreting α/β tuning and explaining why both are needed
  - Quick check question: If a dataset has 80% positive labels in group A and 20% in group B, which fairness metric is easier to satisfy without sacrificing accuracy?

- Concept: Semi-supervised node classification in GNNs
  - Why needed here: Method operates where only a subset of nodes have labels (both class and sensitive attributes). Message-passing mechanism propagates information from labeled to unlabeled nodes, and fairness regularizer only applies to labeled subset
  - Quick check question: In a graph with 10,000 nodes but only 500 labeled, how does the model make predictions for the 9,500 unlabeled nodes?

- Concept: Regularization as soft constraint enforcement
  - Why needed here: Framework adds fairness terms to loss function rather than hard constraints. This allows gradient-based optimization to balance competing objectives but provides no guarantee of exact fairness
  - Quick check question: If α=1.0 and β=1.0 but fairness metrics remain at 5% disparity after training, what are two possible causes?

## Architecture Onboarding

- Component map:
  - Base GNN encoder (f_θ) -> Classifier head (g_ϕ) -> EOSP regularization module -> Training loop with combined loss
  - Hyperparameter optimizer (Bayesian search) -> Validation score evaluation

- Critical path:
  1. Verify sensitive attribute availability matches the labeled set (not full graph)
  2. Implement group mask construction for D_0, D_1 (SP) and P_0, P_1 (EO)
  3. Add regularizer computation to loss function before backward pass
  4. Run Bayesian optimization with at least 15 trials on validation set
  5. Monitor both fairness metrics and accuracy on held-out test set

- Design tradeoffs:
  - α/β magnitude: Higher values enforce stronger fairness but risk accuracy collapse; paper finds α∈[0.01, 0.1], β∈[0.01, 1.0] effective
  - Labeled set size: More labeled sensitive attributes improve fairness optimization but may not be available; experiments show gains even at 20% coverage
  - Base GNN choice: Model-agnostic by design, but different GNNs have different bias amplification properties

- Failure signatures:
  - NaN losses during training: Check for empty group masks (P_1 or P_0 has zero elements when positive class is rare in one group)
  - Fairness worsens with EOSP: Likely hyperparameter mismatch; run extended Bayesian optimization (>25 trials) or check validation/test distribution shift
  - Accuracy drops >5% with minimal fairness gain: α,β too high; reduce regularization strength or use staged training
  - High variance across runs: Fairness objectives create non-convex landscape; increase random seeds or use ensemble selection

- First 3 experiments:
  1. Apply GCN-EOSP to German dataset with α=0.1, β=0.1; verify L_SP and L_EO decrease during training while BACC stays within 2% of baseline
  2. Fix α=0.1, vary β∈[0.01, 0.1, 0.5, 1.0] on Bail dataset; plot ΔSP, ΔEO, and BACC to confirm gentle constraints (β≈0.01) can achieve strong fairness
  3. On NBA dataset, compare GCN-EOSP fairness metrics at 20%, 30%, 50% labeled sensitive attributes; verify claims of near-perfect statistical parity at 50% labels and assess degradation at lower coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the EOSP framework be effectively generalized to handle non-binary sensitive attributes or weighted graph structures?
- Basis in paper: Authors state the framework can be extended to weighted graphs and non-binary sensitive attributes, but current formulation only handles binary attributes
- Why unresolved: Current mathematical formulation relies on binary partitions D_0 and D_1 that don't directly translate to continuous or multi-class settings
- What evidence would resolve it: Theoretical extension of loss functions for multi-class sensitive attributes and empirical validation on datasets with non-binary protected groups

### Open Question 2
- Question: What specific factors cause EOSP regularizer to increase statistical parity disparity for GraphSAGE on German dataset compared to baseline?
- Basis in paper: Observed that for SAGE-EOSP on German dataset, ΔSP increased from 6.98 to 11.63
- Why unresolved: Paper notes anomaly and improves via extended optimization but doesn't isolate whether failure was due to model architecture, dataset size, or interaction between sampling strategy and fairness constraint
- What evidence would resolve it: Ablation study isolating GNN aggregation mechanism and dataset size to identify conditions causing regularization failure

### Open Question 3
- Question: How robust is the proposed method to noise or errors within the limited set of available sensitive attribute labels?
- Basis in paper: Framework relies on labeled training set to approximate fairness constraints, assuming available attributes are ground truth
- Why unresolved: In privacy-constrained scenarios, limited data might be self-reported or erroneous, potentially amplifying actual bias if regularization enforces fairness with respect to wrong groups
- What evidence would resolve it: Experiments evaluating degradation of fairness metrics as label noise ratio in sensitive attributes increases

## Limitations
- Performance heavily depends on representativeness of labeled subset for both class labels and sensitive attributes
- Method assumes binary sensitive attributes and binary classification, limiting applicability to multi-class or continuous fairness constraints
- Bayesian hyperparameter optimization requires multiple training runs that may be computationally expensive for large graphs

## Confidence
- **High confidence**: Differentiability of regularization terms and their integration into standard GNN training
- **Medium confidence**: Bayesian optimization effectively navigates fairness-accuracy trade-off space
- **Low confidence**: Labeled subset's representativeness assumption for full-graph fairness guarantees

## Next Checks
1. **Sensitivity analysis on labeled subset size**: Systematically vary percentage of nodes with known sensitive attributes (10%, 20%, 30%, 50%) and measure fairness metric degradation to validate partial coverage assumption
2. **Stability test across random seeds**: Run German dataset experiment with 10 different random seeds while keeping hyperparameters fixed to quantify variance in fairness-accuracy trade-offs
3. **Edge construction ablation**: Compare fairness outcomes when using different graph construction strategies (k-NN vs. similarity thresholds) on tabular datasets to isolate impact of graph topology on bias amplification