---
ver: rpa2
title: Simulating multiple human perspectives in socio-ecological systems using large
  language models
arxiv_id: '2507.17680'
source_url: https://arxiv.org/abs/2507.17680
tags:
- user
- simulation
- system
- https
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HoPeS (Human-Oriented Perspective Shifting) framework uses
  LLM-driven agents and structured simulation protocols to enable users to step into
  the perspectives of diverse stakeholders in socio-ecological systems. A prototype
  system integrating a perspective-taking simulation with an AI reflective companion
  was developed to explore institutional dynamics and land use change.
---

# Simulating multiple human perspectives in socio-ecological systems using large language models

## Quick Facts
- arXiv ID: 2507.17680
- Source URL: https://arxiv.org/abs/2507.17680
- Reference count: 14
- Key outcome: HoPeS framework enables users to adopt diverse stakeholder perspectives in socio-ecological simulations, revealing policy-implementation misalignments and supporting interdisciplinary collaboration.

## Executive Summary
This paper introduces the HoPeS (Human-Oriented Perspective Shifting) framework, which leverages large language models to simulate multiple stakeholder perspectives in socio-ecological systems. The framework enables users to step into roles such as researchers, policymakers, and interest groups, experiencing the complexity of land-use governance firsthand. By coupling narrative role-playing with numerical environmental models, the system reveals how competing stakeholder interests can lead to policy-implementation misalignments. A prototype integrating a perspective-taking simulation with an AI reflective companion was developed to explore institutional dynamics and land use change.

## Method Summary
The method employs LLM-driven agents to represent various stakeholders within a socio-ecological system, coupled with a structured simulation protocol. Users interact with the system through two Streamlit interfaces: one for simulation initialization and agent output display, and another for data exploration and decision submission. The backend uses LangChain/LangGraph for LLM agents and a Java-based CRAFTY land use model via Py4J. The simulation runs in phases, with users submitting natural language decisions while agents respond dynamically. An AI "Reflective Companion" guides users through phases of reflection, transition, and integration across perspectives, aiming to transform role-playing experiences into integrated systemic understanding.

## Key Results
- Users experienced the misalignment between policy recommendations and implementation due to competing stakeholder interests
- The simulation supported both narrative-driven and numerical experimentation
- The framework showed potential for enhancing interdisciplinary collaboration and system understanding through perspective pluralism

## Why This Works (Mechanism)

### Mechanism 1
LLMs appear to simulate stakeholder perspectives via natural language, enabling a narrative richness unattainable in traditional rule-based Agent-Based Models (ABMs). The system injects persona prompts into LLMs, which then generate context-dependent arguments and decisions, allowing users to interact via "situated knowledge" rather than abstract logic. This relies on LLMs reliably emulating human reasoning and stakeholder biases when provided with specific personas.

### Mechanism 2
The structured simulation protocol (Contextualization -> Reflection -> Transition) likely transforms raw role-playing experience into integrated systemic understanding. An AI "Reflective Companion" guides the user through phases, actively prompting synthesis of insights to facilitate knowledge transfer between disparate perspectives. This assumes users can hold and integrate conflicting perspectives without "perspective pollution."

### Mechanism 3
Bridging narrative outputs with numerical environmental models potentially reveals the causal misalignment between policy recommendations and implementation. The "Research Supplier" analyzes numerical data and generates text-based recommendations, while the "High-Level Institution" weighs these against competing text-based advocacies. The resulting numerical policy output often diverges from optimal recommendations, modeling real-world governance compromise.

## Foundational Learning

- **Situated Knowledge**
  - Why needed here: The core theory (Haraway) is that knowledge depends on the observer's position, explaining why the system forces users to "shift" perspectives rather than just observing data.
  - Quick check question: How does changing the user's role from "Observer" to "Researcher" alter their perception of the system's efficiency?

- **Socio-Ecological Systems (SES)**
  - Why needed here: This domain context requires understanding that human institutions and natural land-use processes are coupled; changes in one affect the other.
  - Quick check question: In the simulation, does a change in "budget allocation" (social) immediately alter "meat supply" (ecological), or is there a time lag?

- **LLM Role Alignment**
  - Why needed here: A critical engineering challenge. The system relies on LLMs acting as specific stakeholders with distinct agendas, not as helpful assistants.
  - Quick check question: If an LLM agent breaks character and offers a balanced, neutral view when it should be advocating for a specific interest, is the simulation valid?

## Architecture Onboarding

- **Component map**: User -> Streamlit Interface I/II -> LangChain/LangGraph (LLM Agents) <-> Py4J -> CRAFTY Model; AI Reflective Companion guides transitions
- **Critical path**: 1) User configures simulation (selects role) 2) Backend runs land-use model until "policy time lag" expires 3) LLM agents (and User) activated to propose policies 4) User utilizes AI Assistants to analyze data and draft reports 5) High-Level Institution (LLM) aggregates inputs and sets numerical policy 6) Policy feeds back into land-use model
- **Design tradeoffs**: Trades precision of hard-coded ABM rules for narrative richness of LLMs, accepting hallucination risks to gain social realism; automation vs. control in AI assistant report drafting
- **Failure signatures**: Hallucination (AI assistant claims "significant fluctuations" in data where none exist); Role Drift (Researcher agent fails to distinguish between "mean values across times" and specific policy targets); Tool Failure (code generation for data visualization fails)
- **First 3 experiments**: 1) Observer Baseline: Run with user as "None" (Observer) to calibrate expected behavior 2) Researcher Intervention: Step into "Research Supplier" role; attempt to maximize a specific metric 3) Adversarial Framing: As Research Supplier, intentionally use "emotional" or "manipulative" framing to test if High-Level Institution shifts budget allocation more aggressively

## Open Questions the Paper Calls Out

### Open Question 1
How do LLMs weigh narrative contributions of varying lengths within a single prompt when aggregating inputs from multiple agents? The paper notes this remains unresolved, raising concerns that shorter human inputs may be overlooked compared to lengthy AI-generated outputs. Evidence would come from controlled experiments varying argument lengths while holding semantic content constant.

### Open Question 2
How can the alignment of LLM agents with specific role personas be effectively evaluated in simulation contexts? The paper notes there is no established framework for this, hindering role alignment. Evidence would come from developing and validating a standardized evaluation protocol correlating LLM behavior with expert-defined role profiles.

### Open Question 3
How can simulation incentive systems be designed to encourage engagement without prioritizing "winning" over integrated system understanding? The paper asks how to avoid stimulating users to outweigh winning or losing over understanding. Evidence would come from comparative user studies measuring reflection depth and system insight in gamified vs. non-gamified protocols.

### Open Question 4
What mechanisms effectively support users in blending distinct perspectives to form a holistic understanding without "perspective pollution"? The paper highlights this methodological gap. Evidence would come from cognitive psychology studies tracking how different transition protocols affect user bias and synthesis capabilities.

## Limitations
- LLM hallucination and role drift can produce factually incorrect analysis and out-of-character outputs
- The coupling between narrative outputs and numerical models depends on tool-calling interfaces prone to hallucination
- User capacity for integrating conflicting perspectives without cognitive overload remains unverified

## Confidence

- **High Confidence**: The framework's structural design and integration of narrative and numerical elements are well-documented and theoretically sound.
- **Medium Confidence**: LLM agents' ability to simulate stakeholder perspectives is plausible but their long-term role fidelity is unverified.
- **Low Confidence**: User ability to achieve meaningful perspective integration and system effectiveness in real-world collaboration remain speculative without empirical validation.

## Next Checks

1. **Role Fidelity Test**: Run a 10-cycle simulation with fixed LLM agents and monitor adherence to personas using automated text analysis to detect role drift.

2. **Tool-Calling Validation**: Implement a sanity-check layer that cross-verifies AI-generated data analyses against raw model outputs to quantify hallucination rates.

3. **Cognitive Integration Assessment**: Conduct a user study with pre- and post-simulation surveys measuring perspective integration and system understanding, comparing results across different user expertise levels.