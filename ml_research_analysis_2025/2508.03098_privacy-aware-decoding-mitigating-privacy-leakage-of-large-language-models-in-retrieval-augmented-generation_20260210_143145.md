---
ver: rpa2
title: 'Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models
  in Retrieval-Augmented Generation'
arxiv_id: '2508.03098'
source_url: https://arxiv.org/abs/2508.03098
tags:
- privacy
- noise
- decoding
- generation
- repeat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy leakage in retrieval-augmented generation
  (RAG) systems, where attackers can extract private information from retrieved contexts
  through carefully crafted prompts. The authors propose Privacy-Aware Decoding (PAD),
  a decoding-time defense that adaptively injects calibrated Gaussian noise into token
  logits during generation.
---

# Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.03098
- Source URL: https://arxiv.org/abs/2508.03098
- Authors: Haoran Wang; Xiongxiao Xu; Baixiang Huang; Kai Shu
- Reference count: 36
- Primary result: Proposed Privacy-Aware Decoding (PAD) reduces private information leakage by 67.8% on HealthCareMagic and 69.0% on iCliniq while maintaining competitive perplexity

## Executive Summary
This paper addresses privacy leakage in retrieval-augmented generation (RAG) systems, where attackers can extract private information from retrieved contexts through carefully crafted prompts. The authors propose Privacy-Aware Decoding (PAD), a decoding-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD uses confidence-based screening to identify high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A Rényi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, providing explicit per-response (ε, δ)-DP guarantees for sensitive outputs.

The method is model-agnostic and requires no retraining or modification to the retrieval infrastructure. Experiments on three real-world datasets (HealthCareMagic, iCliniq, and Enron Mail) demonstrate that PAD substantially reduces private information leakage while preserving response utility. Across both Pythia-6.9B and Llama2-7B models, PAD achieves significant reductions in privacy leakage while maintaining competitive perplexity across all datasets, outperforming existing retrieval- and post-processing-based defenses.

## Method Summary
Privacy-Aware Decoding (PAD) is a decoding-time defense that mitigates privacy leakage in RAG systems by adaptively injecting calibrated Gaussian noise into token logits during generation. The approach consists of three key components: confidence-based screening identifies high-risk tokens that could reveal private information, efficient sensitivity estimation minimizes unnecessary noise injection, and context-aware noise calibration balances privacy protection with generation quality. A Rényi Differential Privacy accountant tracks cumulative privacy loss to provide explicit (ε, δ)-DP guarantees per response. The method operates entirely at inference time without requiring model retraining or infrastructure modifications, making it model-agnostic and practical for deployment.

## Key Results
- PAD reduces privacy leakage by 67.8% on HealthCareMagic and 69.0% on iCliniq datasets
- Maintains competitive perplexity across all tested datasets
- Outperforms existing retrieval- and post-processing-based defenses
- Demonstrates effectiveness across both Pythia-6.9B and Llama2-7B model architectures

## Why This Works (Mechanism)
PAD works by injecting calibrated Gaussian noise into token logits during the decoding process, with the amount of noise dynamically adjusted based on the sensitivity of each token to private information. The confidence-based screening mechanism identifies tokens that are likely to reveal sensitive information, allowing targeted noise injection rather than uniform noise application. The context-aware calibration ensures that noise levels are appropriate for the specific generation context, preserving utility while maximizing privacy protection. The Rényi Differential Privacy accountant provides rigorous mathematical guarantees by tracking cumulative privacy loss across the generation sequence, ensuring that the final output meets specified privacy thresholds.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG)**: Combines retrieval systems with language models to enhance responses with external knowledge; needed to understand the specific privacy challenges in context-dependent generation systems.
- **Rényi Differential Privacy (RDP)**: A mathematical framework for quantifying privacy loss that provides tighter composition bounds than traditional differential privacy; needed to rigorously track cumulative privacy guarantees during token generation.
- **Logit-based noise injection**: Technique of adding noise to the raw output scores before softmax normalization; needed to understand how PAD modifies generation probabilities without retraining models.
- **Confidence-based screening**: Method of identifying high-risk tokens based on their generation confidence scores; needed to understand how PAD selectively targets privacy-sensitive tokens.
- **Sensitivity estimation**: Process of measuring how much individual tokens contribute to potential privacy leakage; needed to calibrate noise levels appropriately.
- **Gaussian mechanism**: Adding Gaussian noise to achieve differential privacy; needed to understand the specific noise injection approach used in PAD.

## Architecture Onboarding

**Component Map**: Retrieval System -> Language Model -> PAD Decoder -> Output

**Critical Path**: Token generation sequence where PAD intercepts logits after model prediction but before softmax, applies noise injection, and passes modified logits to the next generation step.

**Design Tradeoffs**: PAD trades some generation quality for privacy protection, requiring careful calibration of noise parameters to maintain utility. The method prioritizes model-agnostic deployment over potentially higher performance from model-specific adaptations. Runtime overhead is introduced by the privacy screening and noise calibration processes, but this is offset by avoiding expensive retraining requirements.

**Failure Signatures**: If noise calibration is too conservative, privacy leakage may persist; if too aggressive, generation quality degrades significantly. Poor sensitivity estimation can lead to either over-protection of non-sensitive tokens or under-protection of truly private information. RDP accounting errors could result in false privacy guarantees.

**First Experiments**: 1) Test PAD's effectiveness on synthetic datasets with known privacy vulnerabilities. 2) Evaluate the impact of different noise calibration parameters on the privacy-utility tradeoff curve. 3) Benchmark runtime overhead compared to standard decoding with the same language models.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may not generalize beyond the specific healthcare and email datasets tested
- RDP accounting complexity could limit scalability for very long generation sequences
- Performance in non-domain-specific applications is not thoroughly evaluated
- Does not address all possible adversarial attack vectors, particularly sophisticated prompt engineering techniques

## Confidence

High-Medium confidence in privacy leakage reduction effectiveness (well-supported by experimental results)
High confidence in model-agnostic implementation (clearly demonstrated across two different model architectures)
Medium confidence in utility preservation (competitive perplexity reported, but qualitative assessment of response quality is limited)
Medium confidence in RDP accounting accuracy (theoretical framework is sound, but practical verification is limited)

## Next Checks

1. Test PAD's effectiveness against more sophisticated adversarial attacks, including multi-turn conversation scenarios and prompt injection techniques not covered in the current evaluation
2. Evaluate the approach on additional datasets spanning different domains (e.g., financial, legal, educational) to assess generalizability
3. Conduct user studies to empirically measure the trade-off between privacy protection and response utility from human evaluators' perspectives