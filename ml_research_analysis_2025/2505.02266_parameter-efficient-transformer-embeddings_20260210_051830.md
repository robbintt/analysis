---
ver: rpa2
title: Parameter-Efficient Transformer Embeddings
arxiv_id: '2505.02266'
source_url: https://arxiv.org/abs/2505.02266
tags:
- embedding
- token
- fourier
- embeddings
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient transformer embedding
  method that replaces traditional large embedding tables with a deterministic Fourier
  expansion of normalized token IDs, followed by a lightweight multilayer perceptron
  (MLP) to capture higher-order interactions. The approach leverages the statistical
  structure in Byte-Pair Encoding token IDs, mapping them to continuous values and
  expanding them using Fourier basis functions.
---

# Parameter-Efficient Transformer Embeddings

## Quick Facts
- arXiv ID: 2505.02266
- Source URL: https://arxiv.org/abs/2505.02266
- Authors: Henry Ndubuaku; Mouad Talhi
- Reference count: 21
- One-line primary result: Replaces learned embedding tables with Fourier expansion of normalized token IDs, reducing parameters from 8.9M to 1.1M while maintaining competitive NLI and STS-B performance

## Executive Summary
This paper introduces a parameter-efficient transformer embedding method that eliminates learned embedding tables by using a deterministic Fourier expansion of normalized Byte-Pair Encoding token IDs. The approach maps token IDs to continuous values and expands them using Fourier basis functions, with a lightweight MLP learning residual corrections. The method achieves competitive performance on natural language inference and sentence similarity tasks while dramatically reducing model parameters and eliminating the need for dropout. Experimental results show the approach trains faster and operates effectively without the overfitting concerns typical of large embedding tables.

## Method Summary
The method normalizes BPE token IDs to [-1, 1] and expands them using Fourier basis functions (sin/cos pairs at increasing frequencies) to generate initial embeddings. A lightweight MLP then learns residual corrections through a skip connection, forming the final embedding E(p) = MLP(T(p)) + T(p). The approach leverages BPE's frequency-based ID ordering to provide statistical structure, uses the Fourier basis for smooth continuous mapping, and employs residual learning for token-specific refinement. Models are trained with contrastive loss on SNLI/MNLI datasets and evaluated zero-shot on STS-B, achieving competitive results with significantly fewer parameters than traditional transformers.

## Key Results
- Parameter reduction from 8.9M to 1.1M for basic transformer configuration
- Maintains competitive performance on SNLI/MNLI entailment and STS-B similarity tasks
- Eliminates need for dropout while maintaining performance
- Trains faster due to fused CUDA kernel implementation

## Why This Works (Mechanism)

### Mechanism 1: BPE Frequency Ordering Provides Exploitable Statistical Structure
BPE assigns lower IDs to frequent tokens and higher IDs to rare tokens. Normalizing these IDs to [-1, 1] preserves relative ordering while enabling smooth function approximation through Fourier basis functions. Lower-order terms capture broad statistical trends while higher-order terms encode finer distinctions. This works because frequency-based ordering contains structure that correlates with desired semantic embeddings.

### Mechanism 2: MLP Refines Deterministic Basis via Residual Learning
The Fourier expansion provides a deterministic base representation T(p). The MLP learns to approximate the residual function H(z) = f(φ⁻¹(z)) - z, where f is the target embedding function. The residual connection E(p) = MLP(T(p)) + T(p) frames learning as small corrections rather than full embedding construction, making it easier to learn smooth adjustments to the fixed basis.

### Mechanism 3: Continuous Normalization Provides Implicit Regularization
The deterministic continuous mapping from token IDs to [-1, 1] constrains embeddings to follow smooth trajectories in embedding space, limiting the effective capacity available for overfitting. This constraint provides regularizing effects that eliminate the need for dropout, as the smoothness constraint is beneficial rather than overly restrictive for semantic representation.

## Foundational Learning

- **Concept: Fourier Basis Functions and Function Approximation**
  - Why needed here: The core innovation replaces learned embeddings with Fourier series expansion. Understanding how sine/cosine functions at different frequencies compose to approximate arbitrary functions is essential for grasping why this works.
  - Quick check question: Given a normalized token ID x ∈ [-1, 1], what does the k-th Fourier component sin(kπx) capture that sin(2πx) does not?

- **Concept: Residual Learning and Skip Connections**
  - Why needed here: The architecture uses E(p) = MLP(T(p)) + T(p), framing the MLP as learning corrections to the Fourier basis rather than full embeddings.
  - Quick check question: If the Fourier basis were already optimal, what would the ideal MLP output be, and how does the residual formulation make this easy to learn?

- **Concept: Subword Tokenization (BPE) and Frequency-Based ID Assignment**
  - Why needed here: The method explicitly exploits that BPE assigns lower IDs to frequent tokens. Without understanding this ordering, the normalization and Fourier expansion would operate on arbitrary indices with no statistical structure.
  - Quick check question: In a BPE vocabulary where "the" has ID 5 and "photosynthesis" has ID 28,000, what does normalizing to [-1, 1] and applying low-frequency Fourier terms capture about these tokens' relative properties?

## Architecture Onboarding

- **Component map:** Input Token IDs -> Normalization x = 2p/(V-1) - 1 -> Fourier Expansion Ti(p) = sin/cos((⌊i/2⌋+1)πx) -> MLP Refinement -> Residual Addition E(p) = MLP(T(p)) + T(p) -> Downstream Transformer with Rotary Positional Encoding

- **Critical path:** 1) Verify BPE tokenizer produces frequency-ordered IDs 2) Implement fused CUDA kernel for normalization + Fourier expansion 3) Configure MLP size (linear layer vs. full FFN) 4) Remove dropout from embedding pathway entirely

- **Design tradeoffs:** MLP size vs. parameter savings (larger MLP approaches traditional embedding parameters); d_model vs. vocabulary capacity (smaller d_model limits Fourier terms); layers vs. embedding importance (2+ layers match baseline, single-layer shows larger gaps)

- **Failure signatures:** Training loss diverges or plateaus high (check dropout disabled); adjacent tokens cluster excessively (vocabulary too large for d_model); worse than baseline with equal parameters (under-parameterized, try 2+ layers); no training speed improvement (custom CUDA kernel not fused)

- **First 3 experiments:** 1) Ablation on MLP presence: Fourier-only vs. Fourier + linear vs. Fourier + full MLP on SNLI validation 2) Vocabulary size scaling: 10k, 30k, 100k tokens to identify near-collision thresholds 3) Dropout sensitivity check: Add dropout at 0.1, 0.3 to embedding pathway and measure STS-B degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Fourier embedding approach perform on token-level classification tasks (e.g., NER, POS-tagging) and generative language modeling?
- Basis in paper: [explicit] The authors state: "The effectiveness of PETE embeddings on a broader range of NLP tasks, such as generative modeling, token-level classiﬁcation (e.g., NER, POS-tagging), or complex reasoning tasks, has not yet been evaluated."
- Why unresolved: Experiments were limited to natural language inference and sentence similarity; token-level tasks may require different embedding properties that the deterministic mapping does not preserve.
- What evidence would resolve it: Evaluate PETE on standard NER and POS-tagging benchmarks (e.g., CoNLL-2003, Penn Treebank) and on generative perplexity metrics.

### Open Question 2
- Question: Can the MLP component effectively separate near-colliding base representations when vocabularies scale to hundreds of thousands of tokens?
- Basis in paper: [explicit] The authors acknowledge: "As the normalized token ID space [−1, 1] becomes densely populated, the initial Fourier representations of distinct tokens could become very close... its ability to effectively separate a vast number of near-colliding base representations requires further investigation at scale."
- Why unresolved: Current experiments used a fixed vocabulary (30,522 tokens); the mathematical spacing between adjacent tokens shrinks as V grows, potentially overwhelming the MLP's capacity.
- What evidence would resolve it: Scale experiments to vocabularies of 100k+ tokens and analyze embedding similarity distributions; measure performance degradation as vocabulary size increases.

### Open Question 3
- Question: How well does the deterministic Fourier expansion capture discrete lexical phenomena such as morphological variants, homonyms, and polysemy?
- Basis in paper: [explicit] The authors note: "the current study does not explicitly investigate how well the deterministic Fourier expansion captures discrete lexical phenomena such as morphological variants, homonyms, or polysemy, which might be implicitly handled by traditional learned embeddings."
- Why unresolved: The continuous, frequency-based mapping may smooth over lexical distinctions that learned embeddings capture through independent optimization.
- What evidence would resolve it: Design probes or targeted evaluations measuring polysemy resolution and morphological similarity in the embedding space.

### Open Question 4
- Question: Do alternative polynomial bases (Chebyshev, Legendre) offer theoretical or practical advantages over Fourier for embedding generation?
- Basis in paper: [explicit] The authors propose: "Another promising direction is the exploration of alternative polynomial bases (e.g., Chebyshev or Legendre) for embedding mechanisms."
- Why unresolved: Fourier was chosen for GPU/TPU compatibility, but the authors acknowledge Chebyshev and Legendre may have theoretical advantages that were not explored.
- What evidence would resolve it: Implement and benchmark alternative bases with controlled comparisons on identical tasks and model sizes.

## Limitations

- The method's dependence on BPE's frequency-based ID ordering is untested—performance may degrade significantly with randomized IDs or alternative tokenization schemes
- Vocabulary scaling limits are unclear, with potential near-collision issues for vocabularies exceeding tens of thousands of tokens
- Claims about dropout-free training rely on theoretical analogies rather than direct empirical validation across different architectures and tasks

## Confidence

- **High Confidence:** Parameter reduction claims (8.9M → 1.1M) are directly verifiable from model configurations and well-established baseline values. Training speed improvements with custom CUDA kernels are measurable and independent of model performance.
- **Medium Confidence:** Task performance on SNLI/MNLI and STS-B is reported with standard metrics, but lacks significance testing, variance measures, or comparison to contemporary small models.
- **Low Confidence:** The three proposed mechanisms (statistical structure exploitation, residual learning refinement, and implicit regularization) are theoretically motivated but lack direct empirical validation through targeted ablation studies.

## Next Checks

1. **BPE ID Structure Dependency Test:** Train identical models using randomized token ID assignments versus frequency-ordered BPE IDs to quantify dependence on BPE's statistical structure.

2. **Vocabulary Scaling Boundary Analysis:** Systematically vary vocabulary size (10k, 30k, 100k tokens) while keeping d_model fixed to identify threshold where near-collision effects overwhelm MLP separation capacity.

3. **Dropout-Free Training Validation:** Replicate polynomial approximation experiments by training models with dropout applied to Fourier embeddings at 0.1, 0.3, and 0.5 to empirically verify smooth function approximation breaks down with dropout.