---
ver: rpa2
title: Evaluation Report on MCP Servers
arxiv_id: '2504.11094'
source_url: https://arxiv.org/abs/2504.11094
tags:
- search
- server
- figure
- comment
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness and efficiency of Model
  Context Protocol (MCP) servers for AI-driven data retrieval tasks. The study uses
  MCPBench to compare various MCP servers across web search and database search tasks,
  measuring accuracy, time consumption, and token usage.
---

# Evaluation Report on MCP Servers

## Quick Facts
- **arXiv ID**: 2504.11094
- **Source URL**: https://arxiv.org/abs/2504.11094
- **Reference count**: 21
- **Key outcome**: This paper evaluates the effectiveness and efficiency of Model Context Protocol (MCP) servers for AI-driven data retrieval tasks, finding significant performance differences among servers with no inherent MCP advantage over direct function calls.

## Executive Summary
This paper evaluates MCP servers for web search and database search tasks using the MCPBench framework. The study measures accuracy, time consumption, and token usage across various MCP servers, revealing substantial performance differences. Bing Web Search achieved the highest accuracy at 64%, while DuckDuckGo lagged at 10%. Time efficiency varied widely, with top performers completing tasks in under 15 seconds while Exa Search took 231 seconds. The research demonstrates that MCP servers can significantly improve accuracy through declarative interfaces that shift SQL generation from LLMs to specialized components within servers, as shown by the XiYan MCP server's 22 percentage point accuracy improvement.

## Method Summary
The evaluation uses MCPBench to test MCP servers across web search and database search tasks. Web search datasets include Frames (100 English samples), News Chinese (100 samples), and Knowledge Chinese (100 samples). Database search uses Car bi (355 synthetic samples) and SQL EVAL (256 samples). Evaluation metrics include accuracy (via DeepSeek-v3 LLM grader), time consumption (end-to-end latency), and token consumption. MCP servers run in SSE mode with 30-second timeout on a Singapore-based VM (dual-core CPU, 2GB RAM) with MySQL 9.2 and PostgreSQL 15.8. Standardized prompts are provided for web search tasks, while database search prompts remain unspecified.

## Key Results
- Bing Web Search achieved highest accuracy at 64%, while DuckDuckGo only reached 10%
- End-to-end time varied significantly: Bing and Brave completed in under 15 seconds, while Exa Search took 231 seconds
- Accuracy improved by 22 percentage points when using natural language processing interfaces instead of SQL queries in the XiYan MCP server

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A **declarative interface** (natural language input) in an MCP server can improve accuracy over a raw SQL interface.
- Mechanism: A raw SQL interface (`execute sql`) requires the calling LLM to generate a correct and precise SQL query. A declarative interface (`get data`) allows the LLM to provide the original question in natural language. An internal text-to-SQL model within the MCP server then handles the complex translation to SQL. This offloads the difficult and error-prone code generation task from the general-purpose LLM to a specialized model within the server.
- Core assumption: The internal text-to-SQL model is more accurate at generating SQL for the specific database than the calling LLM.
- Evidence anchors:
  - [section 5.3] The XiYan MCP Server improved accuracy by 22 percentage points on PostgreSQL by using a text-to-SQL model instead of requiring the LLM to write SQL.
  - [section 7] The conclusion states "...transitioning from SQL-based queries to natural language processing in the XiYan MCP server resulted in a noteworthy increase in accuracy...".
- Break condition: If the internal text-to-SQL model performs poorly or the database schema is too complex for it to understand, this mechanism will fail to provide a benefit.

### Mechanism 2
- Claim: The **quality and format of search results** returned by an MCP server directly influence the final answer's accuracy.
- Mechanism: MCP servers return results in different formats: raw lists of URLs, summarized answers, or full webpage content. A server that pre-processes and summarizes results (e.g., BochaAI) gives the LLM a direct, reasoned answer, reducing its workload. A server returning a raw list of URLs (e.g., Brave Search, DuckDuckGo) forces the LLM to perform all the synthesis and reasoning, increasing the cognitive load and potential for error.
- Core assumption: The summarization performed by the MCP server is itself accurate.
- Evidence anchors:
  - [section 6.1] The case study contrasts BochaAI (which summarized and gave the answer "Crimson Tide") with Brave Search (which provided a list of Wikipedia pages, leading to an incorrect answer) and Qwen Web Search (which analyzed incorrectly).
- Break condition: If the MCP's internal summarization is incorrect (as with Qwen Web Search in the case study), the LLM has no raw data to correct the error, making it nearly impossible to get the correct answer.

### Mechanism 3
- Claim: Using a **standardized MCP protocol** does not inherently improve accuracy over direct function calls or plugin-based tool use.
- Mechanism: The paper compares MCP servers against native function calls and finds them to be competitive. The MCP protocol standardizes the *interface* for tool usage but does not improve the underlying quality of the tool itself. Performance is driven by the search engine's quality, the API's features, and result processing, not the MCP wrapper.
- Core assumption: The underlying tools being compared are the primary determinant of performance.
- Evidence anchors:
  - [abstract] The abstract states "using MCPs does not demonstrate a noticeable improvement compared to function call."
  - [section 5.2] The function call `Qwen Web Search` achieved 55.52% accuracy, outperforming several MCP servers.
  - [corpus] Related work [5] (MCPToolBench++) also focuses on general tool use, but the primary paper's finding is the lack of an *MCP-specific* performance advantage.
- Break condition: This mechanism suggests there is no inherent "magic" to MCP. If an implementation assumes using MCP will automatically yield better results than a well-tuned function call, that assumption is incorrect.

## Foundational Learning

- Concept: **Model Context Protocol (MCP)**
  - Why needed here: This is the central subject of the paper. It is an open protocol that standardizes how AI models connect to external tools (servers) for tasks like web search and database queries. It replaces ad-hoc function calls with a standard client-server architecture.
  - Quick check question: What is the primary problem MCP is designed to solve for LLMs? (Answer: Standardizing the connection to external data sources and tools).

- Concept: **Declarative vs. Imperative Tool Interface**
  - Why needed here: This is a key concept for improving database MCP performance. An imperative interface (e.g., `execute sql`) requires the LLM to specify *how* to get the data. A declarative interface (e.g., `get data`) allows the LLM to specify *what* they want in natural language, letting the system figure out the "how."
  - Quick check question: In a database MCP, why is an interface that accepts a natural language question considered an improvement over one that requires a SQL query? (Answer: It shifts the complex task of SQL generation from the calling LLM to a potentially specialized component within the server).

- Concept: **End-to-End Evaluation (Accuracy of Valid Samples)**
  - Why needed here: The paper evaluates MCP servers not just on tool output, but on the final correctness of the LLM's answer (accuracy), along with time and token consumption. This holistic view is critical because a tool can be "working" but the overall system can still fail.
  - Quick check question: Why is the "accuracy of valid samples" a more complete metric than just checking if the server returned a 200 OK response? (Answer: It measures the final correctness of the answer produced by the LLM, capturing the effectiveness of the entire pipeline, not just the tool in isolation).

## Architecture Onboarding

- **Component map:**
  - LLM/AI Model -> MCP Client -> MCP Server -> Backend Resource

- **Critical path:**
  1. A question is fed to the LLM.
  2. The LLM, guided by a system prompt, decides to use an MCP tool and generates tool parameters.
  3. The MCP client sends this request to the running MCP server.
  4. The MCP server translates this request, calls its backend, and gets a result.
  5. The MCP server may process the raw result or return it as-is to the LLM.
  6. The LLM receives the tool's output and generates a final answer for the user.

- **Design tradeoffs:**
  - **Raw vs. Processed Results:** Returning raw results gives the LLM more data but requires more reasoning. Returning a processed summary simplifies the task but introduces a single point of failure if the server's processing is wrong.
  - **Generic vs. Specialized Server:** A generic `execute sql` MCP is flexible but places high demands on the LLM. A specialized `get data` MCP with an internal text-to-SQL model can be more accurate but is more complex to build.

- **Failure signatures:**
  - **Low Accuracy with Raw SQL MCP:** The LLM consistently generates incorrect SQL queries.
  - **Incorrect Answers with Summary-First MCP:** The server provides a confident but wrong summary, and the LLM cannot self-correct.
  - **Timeouts or High Latency:** The end-to-end time exceeds reasonable limits due to an inefficient server or backend.

- **First 3 experiments:**
  1. **Establish a Baseline:** Implement a task (e.g., web search) using a direct function call. Measure accuracy, time, and token usage.
  2. **Evaluate an Off-the-Shelf MCP:** Select a relevant MCP server from a registry and run the same task using MCP. Compare performance against your baseline.
  3. **Test Interface Impact:** For a database task, compare a raw SQL MCP (`execute sql`) against a natural language MCP (`get data`) on the same dataset to quantify the accuracy difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness and safety of "world-changing" MCPs (e.g., code commits, state modifications) be reliably evaluated given the difficulty in accessing the underlying status of target data sources?
- Basis in paper: [explicit] Section 2 states that evaluating world-changing MCPs presents challenges due to the difficulty in accessing data status (e.g., GitHub datasets), which restricted the study to data fetching tasks.
- Why unresolved: The authors explicitly excluded state-altering tasks from the MCPBench framework, so no methodology currently exists in this work to verify if requested actions were successfully executed in external systems.
- What evidence would resolve it: An evaluation framework incorporating verification loops or sandbox environments that can track and confirm state changes in external APIs.

### Open Question 2
- Question: What is the optimal level of interface abstraction for MCP tools to balance reducing LLM reasoning burden against the risk of propagating server-side processing errors?
- Basis in paper: [inferred] Section 6.1 notes that while processed results (BochaAI) help LLMs, they remove the ability to correct errors, whereas raw results (Brave Search) rely too heavily on the LLM.
- Why unresolved: The paper demonstrates a trade-off but does not identify a "hybrid" or optimal interface design that maximizes accuracy by leveraging both server-side summarization and LLM verification.
- What evidence would resolve it: Comparative experiments on MCP interfaces that return both summarized answers and raw context chunks to measure error recovery rates.

### Open Question 3
- Question: Do the observed latency and accuracy differences stem primarily from the MCP protocol implementation overhead or the underlying search engine performance?
- Basis in paper: [inferred] The Executive Summary notes that using MCPs "does not demonstrate a noticeable improvement compared to function call," suggesting protocol overhead or implementation inefficiencies may exist.
- Why unresolved: The study measured end-to-end performance but did not isolate the specific latency introduced by the MCP server wrapper versus the inherent latency of the tool's API (e.g., Bing vs. Exa).
- What evidence would resolve it: Ablation studies measuring the time consumption of the MCP serialization/deserialization layer separate from the external API call duration.

## Limitations

- The evaluation was constrained to a single VM in Singapore with limited resources (dual-core CPU, 2GB RAM), which may not reflect real-world deployment conditions
- Network latency and API rate limits significantly affected sample collection, with incomplete samples due to disruptions
- The study relies on a single LLM grader (DeepSeek-v3) for accuracy assessment, introducing potential grader-specific biases

## Confidence

**High Confidence Claims:**
- Bing Web Search achieves highest accuracy (64%) among evaluated web search MCP servers
- Accuracy can be improved by 22 percentage points through natural language processing interfaces instead of SQL queries
- End-to-end time varies significantly across MCP servers, with Bing and Brave completing in under 15 seconds versus Exa Search at 231 seconds
- Function calls and MCP servers show competitive performance, with no inherent MCP advantage

**Medium Confidence Claims:**
- Raw SQL interfaces create higher cognitive load for LLMs compared to declarative interfaces
- Pre-processed summaries from MCP servers can improve answer accuracy when correct
- The declarative interface mechanism works better than imperative SQL generation for general-purpose LLMs

**Low Confidence Claims:**
- MCP protocol itself does not improve accuracy over direct function calls (based on limited comparison)
- Natural language processing is universally better than SQL for all database tasks (tested only on specific datasets)

## Next Checks

1. **Cross-Grader Validation**: Re-run the evaluation using multiple LLM graders (e.g., GPT-4, Claude, Llama) to assess grader bias and establish inter-rater reliability for accuracy scores across all MCP servers.

2. **Resource-Constrained Deployment Test**: Deploy the same MCP servers on varied infrastructure (local machines, cloud instances with different specifications) to quantify the impact of hardware limitations on performance metrics, particularly focusing on the 231-second latency observed with Exa Search.

3. **Cost-Performance Analysis**: Measure API costs and computational resource usage for each MCP server configuration alongside performance metrics to determine cost-effectiveness trade-offs, particularly comparing high-accuracy servers (Bing at 64%) against lower-cost alternatives.