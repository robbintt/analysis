---
ver: rpa2
title: 'Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional
  Climate Models with a Shared Encoder and Multi-Decoder Architecture'
arxiv_id: '2506.22447'
source_url: https://arxiv.org/abs/2506.22447
tags:
- downscaling
- variables
- climate
- multi-variable
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of high-resolution climate downscaling,
  where the goal is to generate Regional Climate Model (RCM)-scale data from coarse
  Global Climate Model (GCM) outputs. The authors propose a multi-variable Vision
  Transformer (ViT) architecture with a shared encoder and variable-specific decoders
  (1EMD) to jointly predict three key climate variables: surface temperature, wind
  speed, and geopotential height at 500 hPa.'
---

# Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture

## Quick Facts
- arXiv ID: 2506.22447
- Source URL: https://arxiv.org/abs/2506.22447
- Authors: Fabio Merizzi; Harilaos Loukos
- Reference count: 40
- Key outcome: Multi-variable ViT with shared encoder and variable-specific decoders outperforms single-variable baselines in climate downscaling with 25% faster inference

## Executive Summary
This paper addresses high-resolution climate downscaling by proposing a multi-variable Vision Transformer (ViT) architecture that jointly predicts three climate variables (surface temperature, wind speed, geopotential height) from coarse Global Climate Model (GCM) outputs. The approach uses a shared encoder to capture cross-variable interactions while employing variable-specific decoders to prevent interference artifacts. Experiments on European climate data from 2006 to 2099 demonstrate consistent performance improvements over single-variable baselines, with up to 12.3% reduction in MSE for geopotential height and improved structural similarity metrics.

## Method Summary
The method uses a shared ViT encoder that processes concatenated 8×8 patches from three climate variables, followed by six transformer layers with self-attention to capture spatial dependencies. Each variable has its own decoder branch consisting of three bilinear upsampling blocks with residual connections, allowing specialized reconstruction while sharing the encoded representation. The architecture omits skip connections based on the ViT's constant spatial resolution throughout the encoder. Training uses joint MSE loss with AdamW optimization over 400 epochs, processing daily climate data from EURO-CORDEX MPI-ESM-LR GCM to ICTP-RegCM4-6 RCM outputs.

## Key Results
- 1EMD model consistently outperforms single-variable baselines across all three climate variables
- Up to 12.3% reduction in MSE for geopotential height at 500 hPa
- Improved SSIM values indicating better structural preservation in downscaled outputs
- Approximately 25% reduction in inference time compared to running independent models
- Eliminates geographic artifacts in geopotential height that appear in single-decoder approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared encoder representations enable positive cross-variable knowledge transfer
- Mechanism: The ViT encoder processes all three climate variables jointly through patch embeddings and self-attention, allowing the model to learn inter-variable spatial dependencies (e.g., how temperature gradients relate to pressure patterns) that single-variable models cannot capture
- Core assumption: Atmospheric variables share underlying physical dynamics that can be encoded in a common representation space
- Evidence anchors: Multi-variable approach achieves positive cross-variable knowledge transfer and consistently outperforms single-variable baselines

### Mechanism 2
- Claim: Variable-specific decoders prevent cross-variable interference artifacts
- Mechanism: Each decoder specializes in reconstructing one variable's spatial characteristics. This decoupling prevents high-frequency patterns from one variable (e.g., wind speed's localized features) from bleeding into smoother fields (e.g., geopotential height)
- Core assumption: Different climate variables require different reconstruction priors; a shared decoder cannot simultaneously optimize for all
- Evidence anchors: When using a single decoder to reconstruct all variables jointly, these differences can cause interference... high-frequency patterns from one variable (e.g., wind) can bleed into the smoother reconstructions

### Mechanism 3
- Claim: Omitting skip connections improves multi-decoder modularity without significant accuracy loss
- Mechanism: The ViT encoder maintains constant spatial resolution throughout, so output tokens preserve spatial structure. Without skip connections, each decoder operates independently on the encoder output, simplifying multi-decoder routing and reducing task coupling
- Core assumption: ViT encoders provide sufficiently rich representations that skip connections are not critical for spatial detail recovery
- Evidence anchors: Unlike CNN encoders, which typically reduce spatial resolution through downsampling, our ViT encoder preserves a constant resolution throughout... reducing the need for skip connections

## Foundational Learning

- Concept: Vision Transformer (ViT) patch embedding and self-attention
  - Why needed here: The backbone encoder divides climate fields into 8×8 patches and processes them as sequences. Understanding how self-attention captures long-range spatial dependencies is essential for debugging encoder outputs
  - Quick check question: Given a 504×432 input image with 8×8 patches, how many tokens does the encoder process, and what spatial information is preserved in each token?

- Concept: Multi-task learning with shared representations
  - Why needed here: The 1EMD architecture is fundamentally a multi-task model. Understanding when shared representations help vs. hurt is critical for variable selection and architecture decisions
  - Quick check question: If adding a fourth variable (precipitation) degrades performance on existing variables, what does this suggest about task affinity?

- Concept: Normalization for multi-variable loss balancing
  - Why needed here: MSE loss is averaged across variables with different units (K, m/s, m). Min-max normalization to [0,1] ensures equal optimization priority
  - Quick check question: If temperature has variance 100× larger than wind speed without normalization, which variable would dominate the gradient updates?

## Architecture Onboarding

- Component map:
  - Input preprocessing: Bilinear upsampling of GCM inputs → concatenation → padding to 432×504 → min-max normalization to [0,1]
  - Patch embedding: 8×8×3 patches → 3402 tokens → 256-dim linear projection + positional encoding
  - Encoder: 6 transformer blocks (LayerNorm → 8-head attention → residual → LayerNorm → MLP [512→256] → residual)
  - Decoder branches (×3): Reshape tokens to 54×63×256 → 3× upsampling blocks (bilinear 2× + residual conv) → zero-init final conv

- Critical path:
  1. Verify input alignment: GCM and RCM must be on same grid via bilinear interpolation before training
  2. Check normalization: Each variable normalized to [0,1] using training data statistics only
  3. Monitor per-variable loss: If one variable's loss plateaus while others improve, consider loss re-weighting

- Design tradeoffs:
  - Skip connections: Omitted for multi-decoder simplicity vs. potential spatial detail loss
  - Patch size (8×8): Larger patches reduce computation but may blur fine-scale features
  - Number of decoders: More variables → more decoders → higher parameter count but better specialization

- Failure signatures:
  - Cross-variable leakage: Unexpected spatial patterns (e.g., coastlines in zg500) indicate decoder sharing is problematic → switch from 1E1D to 1EMD
  - Training instability: Divergent per-variable losses suggest normalization issues or learning rate too high
  - Blurry outputs: May indicate insufficient decoder capacity or patch size too large

- First 3 experiments:
  1. Reproduce single-variable baseline for one variable (e.g., tas) to validate backbone implementation; compare MSE to reported 4.37e-4
  2. Train 1E1D model and inspect zg500 outputs for geographic artifacts; confirms cross-variable interference mechanism
  3. Train 1EMD model with same hyperparameters; verify MSE improvement across all three variables and check inference time reduction

## Open Questions the Paper Calls Out

- Question: To what extent can the number of input variables be increased in the 1EMD architecture before the benefits of shared representations are outweighed by optimization difficulties or interference?
  - Basis in paper: The conclusion explicitly lists "how many variables can be effectively modeled together" as an open question
  - Why unresolved: The study only tested three variables (tas, sfcWind, zg500), so the scaling limits of the multi-decoder approach remain unknown
  - What evidence would resolve it: Experiments adding progressively more variables (e.g., precipitation, pressure) to the shared encoder and measuring the point where performance degrades or training destabilizes

- Question: Which specific meteorological variables should be grouped together to maximize positive transfer learning, particularly when mixing smooth and high-frequency spatial fields?
  - Basis in paper: The conclusion asks "which variables should be selected to maximize performance"
  - Why unresolved: The authors selected three variables with diverse spatial characteristics, but did not test if this specific heterogeneity was optimal or if other combinations would yield better results
  - What evidence would resolve it: An ablation study comparing different variable groupings (e.g., surface-only vs. mixed-level) to quantify cross-variable transfer gains

- Question: Can the reconstruction of fine-scale details be significantly improved by replacing the standard MSE loss with adversarial or physics-informed constraints?
  - Basis in paper: The conclusion asks "whether more sophisticated loss functions could further enhance the model's capabilities"
  - Why unresolved: The authors relied solely on MSE, which tends to produce blurry outputs, leaving the potential benefits of perceptual or physics-based losses unexplored
  - What evidence would resolve it: Comparative training runs using GAN or diffusion-based loss functions to evaluate improvements in high-frequency spatial variability

## Limitations

- The attention head count discrepancy between main text (8 heads) and figure caption (6 heads) creates uncertainty about the encoder's actual architecture
- The computational efficiency claim (25% inference time reduction) only compares multi-variable vs single-variable models, not against other potential architectures
- The study's evaluation period (2006-2099) may not adequately capture long-term climate change trends or performance under different forcing scenarios

## Confidence

- **High Confidence**: The 1EMD architecture consistently outperforms single-variable baselines across all three climate variables with measurable improvements in MSE and SSIM
- **Medium Confidence**: The mechanism of cross-variable knowledge transfer through shared encoder representations is plausible but lacks direct experimental validation
- **Low Confidence**: The claim that omitting skip connections does not significantly impact accuracy relies on indirect reasoning rather than systematic comparison

## Next Checks

1. **Ablation Study on Encoder Architecture**: Train variants of the 1EMD model with different attention head counts (6 vs 8) and with/without skip connections to quantify their individual contributions to the reported performance improvements

2. **Temporal Generalization Test**: Evaluate the 1EMD model on RCM data from different climate forcing scenarios (e.g., RCP4.5) and time periods outside the training range to assess its robustness to varying climate conditions and potential temporal drift

3. **Cross-Variable Transfer Analysis**: Design a controlled experiment where one variable is trained with known ground truth while others are trained with noisy labels, then measure if the "clean" variable's performance improves when sharing the encoder versus using separate encoders