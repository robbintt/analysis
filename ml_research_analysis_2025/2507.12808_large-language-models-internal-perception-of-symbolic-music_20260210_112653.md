---
ver: rpa2
title: Large Language Models' Internal Perception of Symbolic Music
arxiv_id: '2507.12808'
source_url: https://arxiv.org/abs/2507.12808
tags:
- music
- musical
- symbolic
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) implicitly
  model symbolic music by generating a dataset of MIDI files from textual prompts
  describing genre and style combinations. The authors generate 16,250 LLM-created
  MIDI files without explicit musical training and use this dataset to train neural
  networks for music classification and melody completion tasks.
---

# Large Language Models' Internal Perception of Symbolic Music

## Quick Facts
- arXiv ID: 2507.12808
- Source URL: https://arxiv.org/abs/2507.12808
- Reference count: 2
- Key outcome: LLM-generated MIDI data enables above-chance performance on music classification and melody completion, but falls significantly short of supervised models.

## Executive Summary
This paper investigates how large language models (LLMs) implicitly model symbolic music by generating a dataset of MIDI files from textual prompts describing genre and style combinations. The authors generate 16,250 LLM-created MIDI files without explicit musical training and use this dataset to train neural networks for music classification and melody completion tasks. Their experiments show that while CNNs trained on this LLM-generated dataset outperform chance in genre and style classification, their performance remains significantly below supervised models trained on real music datasets. Similarly, transformer models trained on LLM-generated data achieve above-chance melody completion but fall far short of state-of-the-art benchmarks. These results demonstrate LLMs' ability to capture rudimentary musical structures through text-based learning, though explicit musical grounding remains necessary for high performance, revealing both the potential and limitations of using LLM-generated symbolic music data for downstream tasks.

## Method Summary
The authors create an LLM-generated symbolic music dataset by prompting GPT-4 with genre and style combinations to output JSON-encoded MIDI data with specific structural constraints (4 tracks, 8 bars, fixed tempo). They generate 16,250 files across 13 genres and 25 styles. This dataset is then used to train a CNN for genre/style classification and a transformer for melody completion. The CNN processes 4-channel piano rolls (128Ã—128) through two convolutional layers and dense layers, while the transformer uses an encoder-decoder architecture with 2 layers and 4 attention heads. Both models are evaluated against real music datasets (TOP-MAGD, MASD, PiRhDy) to assess their ability to learn from LLM-generated data.

## Key Results
- CNN trained on LLM-MIDI outperforms melody2vec and tonnetz on style classification (F1-score 27.9 vs 24.8) but falls significantly below supervised baselines (~65%+)
- Transformer trained on LLM-MIDI achieves above-chance melody completion (MAP 0.0285) but performs far below state-of-the-art systems (MAP 0.1341)
- LLM-generated data enables rudimentary learning of musical structures, but explicit musical grounding remains necessary for high performance

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Symbolic Mapping via Textual Descriptors
- Claim: Text-only pre-training appears sufficient for LLMs to infer basic musical syntax (e.g., pitch ranges, duration ticks) when prompted with genre/style descriptors, conditional on the model's prior exposure to textual discussions about music theory.
- Mechanism: The LLM maps linguistic concepts (e.g., "pop," "happy," "romantic") to structural token sequences in JSON format. It treats musical constraints (0-127 velocity, tick durations) as syntactic rules similar to code generation, relying on statistical associations between genre terms and structural patterns found in internet-scale text corpora.
- Core assumption: The statistical distribution of text descriptions of music correlates strongly enough with actual musical structures to generate valid symbolic outputs.
- Evidence anchors:
  - [abstract] "LLMs can infer rudimentary musical structures and temporal relationships from text."
  - [page 3] "We instructed the LLM to output a JSON string encoding 8-bar sequences... relying entirely on the LLM's interpretation of prompt relationships."
  - [corpus] Related work (e.g., Text2MIDI, MuseCoco) supports the feasibility of text-to-music mapping but typically uses specialized encoders; this mechanism tests raw generalization.
- Break condition: Mechanism fails if the prompt describes a genre or style combination with insufficient textual precedent in the training data (e.g., obscure fusion genres), resulting in hallucinated or syntactically invalid JSON.

### Mechanism 2: Supervised Distillation of Implicit Knowledge
- Claim: Training standard neural networks (CNNs/Transformers) on LLM-generated synthetic data can yield performance above random chance on real-world music tasks, conditional on the LLM capturing genre-distinctive statistical fingerprints.
- Mechanism: A "teacher" LLM generates a large dataset (16,250 files) reflecting its internal representation of musical patterns. A "student" CNN or Transformer then distills these patterns into discriminative weights. The student model learns to associate specific pianoroll configurations (spatial/temporal patterns) with genre labels derived from the prompt.
- Core assumption: The latent structure of LLM-generated MIDI files shares enough isomorphism with human-composed music to serve as a useful, albeit noisy, training signal.
- Evidence anchors:
  - [abstract] "CNNs trained on this LLM-generated dataset outperform chance in genre and style classification."
  - [page 6] "CNN trained on LLM-MIDI outperformed melody2vec and tonnetz on style classification... affirming that supervised training... provides richer clues."
  - [corpus] Corpus neighbors (e.g., CSyMR, WildScore) explore reasoning capabilities, reinforcing that structure extraction is a key active area, though this paper confirms baseline viability.
- Break condition: The signal-to-noise ratio is too low for complex tasks. Evidence shows performance "significantly below supervised models," indicating the mechanism breaks when high-fidelity musical nuance is required.

### Mechanism 3: Structural Constraint Satisfaction
- Claim: LLMs can adhere to strict structural constraints (e.g., 4-track instrumentation, specific pitch/duration ranges) defined in the prompt, reducing the need for post-processing cleaning.
- Mechanism: The prompt explicitly defines valid JSON schemas and value ranges (e.g., pitch 0-127, ticks 240/480/960). The LLM predicts tokens that maximize the probability of satisfying these explicit instructions, operating as a constrained decoder.
- Core assumption: The model's instruction-following capability generalizes to domain-specific schemas (MIDI JSON) it may not have seen frequently as a complete block.
- Evidence anchors:
  - [page 3] "The instruction explicitly demanded a pure JSON string as output... validated to ensure all four tracks were present."
  - [page 4] "Successful outputs were converted to MIDI files... addressing truncation issues."
  - [corpus] Not found in corpus signals explicitly for constraint satisfaction in MIDI, but implied by the success of the dataset generation pipeline.
- Break condition: Mechanism degrades with increased complexity (e.g., longer sequences > 1200 tokens, complex polyphony) where the LLM loses context or exceeds token limits.

## Foundational Learning

- **Concept: Symbolic Music Representation (MIDI/Pianoroll)**
  - Why needed here: The entire pipeline converts abstract musical intent into discrete (pitch, duration, velocity, start_time) tuples. Understanding that "60" is Middle C and "480" is a quarter note is required to debug the JSON outputs and interpret the CNN's pianoroll inputs.
  - Quick check question: If a JSON tuple is `[60, 240, 90, 0]`, what does the number 240 represent in the context of a 128 time-step sequence?

- **Concept: Cross-Modal Generalization (Text-to-Symbol)**
  - Why needed here: The core hypothesis is that text training alone teaches "musical" logic. You must distinguish between an LLM "knowing" music theory vs. merely retrieving statistical patterns of text associated with music.
  - Quick check question: Does the model's ability to generate a "sad" melody imply it understands emotion, or just that it correlates the word "sad" with minor key statistical patterns?

- **Concept: Synthetic Data Distillation**
  - Why needed here: The methodology uses a large "teacher" model to train smaller "student" models (CNN/Transformer). Understanding the risks of "model collapse" or amplifying hallucinations is critical when interpreting the results.
  - Quick check question: Why might a student model trained on LLM-generated data perform worse on real-world data than a model trained on a smaller set of human data?

## Architecture Onboarding

- **Component map**: Prompt Generator -> GPT-4 LLM -> JSON Validator/Parser -> MIDI Converter -> Pianoroll Tensor -> CNN/Transformer Student Models
- **Critical path**: The validity of the **Prompt Constraints** determines the success of the **Parser**. If the prompt is ambiguous, the LLM generates invalid JSON, breaking the pipeline before training data is created.
- **Design tradeoffs**:
  - **Fixed vs. Variable Constraints**: The authors fixed tempo (120 BPM), time signature (4/4), and 8-bar length to ensure valid JSON parsing.
  - *Tradeoff:* High validity/clean data vs. Low musical diversity/realism. This design choice artificially capped the "musicality" and upper-bound performance of the student models.
- **Failure signatures**:
  - **JSON Truncation**: Output cuts off mid-sequence (mitigated by 1200 max token limit).
  - **Hallucinated Constraints**: LLM invents values outside the 0-127 pitch range or invalid ticks.
  - **Semantic Drift**: Generated "Jazz" track lacks jazz-specific harmonic movements (e.g., ii-V-I), appearing statistically generic.
- **First 3 experiments**:
  1. **Zero-Shot Validation**: Run the generation pipeline with 50 random genre/style pairs. Manually verify JSON validity and listen to 5 converted MIDIs to confirm the "rudimentary structure" claim before training student models.
  2. **Baseline Classification**: Train the 2-layer CNN on the LLM-MIDI dataset and evaluate against the TOP-MAGD test set. Verify that accuracy is > chance (7.7%) but < supervised baselines (~65%+).
  3. **Ablation on Constraints**: Relax the strict JSON constraints (e.g., allow any duration) and observe if the generation success rate collapses or if the resulting MIDI file is musically incoherent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reducing structural constraints (e.g., allowing variable time signatures, longer sequences, expanded rhythmic values, broader instrumentation) significantly improve the musical quality and utility of LLM-generated symbolic music?
- Basis in paper: [explicit] "Future work will explore generating music with fewer constraints to better reflect the complexity of real musical datasets."
- Why unresolved: The current study deliberately restricted outputs to 8-bar sequences, 4/4 time, three drum instruments, and fixed tempo to ensure consistent JSON parsing, limiting expressiveness and realism.
- What evidence would resolve it: Generate datasets with relaxed constraints and evaluate whether downstream task performance improves, particularly for genres requiring complexity like free jazz or electronic music.

### Open Question 2
- Question: How does prompt formulation affect the fidelity and distinctiveness of LLM-generated symbolic music across genre and style combinations?
- Basis in paper: [inferred] The paper acknowledges that "a comprehensive understanding would ideally involve extensive ablation studies on prompts" but does not conduct them, leaving the role of prompt engineering unexplored.
- Why unresolved: Only a single prompt template was used ("[genre] song in [style] manner" plus mood), so the sensitivity of outputs to alternative phrasings, descriptions, or prompt complexity remains unknown.
- What evidence would resolve it: Systematic ablation across varied prompt structures measuring changes in classification accuracy, melody completion metrics, and human ratings.

### Open Question 3
- Question: Can incorporating musically informed similarity measures or training on longer sequences substantially narrow the performance gap between models trained on LLM-generated data and those trained on human-composed music?
- Basis in paper: [inferred] The authors note that "with refinements such as training on longer sequences or incorporating musically informed similarity measures, training a transformer model on LLM-generated dataset could potentially further narrow down the gap to established systems."
- Why unresolved: Experiments used short 8-bar sequences and cosine similarity for ranking, which may not capture musically meaningful continuations.
- What evidence would resolve it: Train transformer models on longer LLM-generated sequences and evaluate with music-theoretic similarity metrics, comparing against supervised baselines.

## Limitations
- Performance gap remains significant: Models trained on LLM-generated data achieve above-chance results but fall far below supervised models trained on human-composed music
- Structural constraints may have limited musical expressiveness: Fixed 8-bar length, 4/4 time signature, and 4-track instrumentation may have prevented learning of more complex musical patterns
- No exploration of prompt sensitivity: The study uses a single prompt template without investigating how different formulations affect output quality

## Confidence

- **High Confidence**: The empirical results showing CNN/Transformer models trained on LLM-MIDI outperform chance but underperform supervised models. The methodology for generating and validating the dataset is clearly specified.
- **Medium Confidence**: The claim that text-only training enables rudimentary musical structure learning. While the results support this, the mechanism by which linguistic patterns map to musical syntax remains somewhat speculative.
- **Low Confidence**: The assertion that LLM-generated data can serve as a viable substitute for human-annotated datasets in complex musical tasks. The significant performance gap suggests this mechanism breaks down under realistic conditions.

## Next Checks

1. **Cross-Genre Generalization Test**: Train the CNN on LLM-MIDI data from 10 genres, then test on held-out genres (3 unseen). Measure performance drop to quantify model's ability to generalize beyond its training distribution and identify genre-specific hallucinations.

2. **Human Evaluation of Musical Coherence**: Conduct a listening test with 20 participants rating 50 LLM-generated vs. 50 human-composed 8-bar sequences on musicality, coherence, and genre authenticity. Compute inter-rater reliability and compare to objective metrics to validate whether the structural validity translates to perceptual quality.

3. **Ablation on Prompt Complexity**: Generate datasets using (a) simple genre-only prompts, (b) genre+style prompts, and (c) genre+style+mood prompts. Train identical CNN models on each dataset and measure performance differences. This will isolate the contribution of prompt complexity to the quality of generated musical structures.