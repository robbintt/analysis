---
ver: rpa2
title: There Is More to Refusal in Large Language Models than a Single Direction
arxiv_id: '2602.02132'
source_url: https://arxiv.org/abs/2602.02132
tags:
- refusal
- directions
- prompts
- latents
- direction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether refusal and non-compliance behaviors\
  \ in large language models are mediated by a single activation-space direction,\
  \ as previously suggested, or whether distinct types of refusal\u2014such as safety,\
  \ incomplete requests, anthropomorphization, and over-refusal\u2014correspond to\
  \ different mechanisms. To address this, the authors compute refusal directions\
  \ for eleven categories of refusal and non-compliance using contrastive activation\
  \ differences, and apply both linear steering and sparse autoencoder (SAE)-based\
  \ interventions to test their causal roles."
---

# There Is More to Refusal in Large Language Models than a Single Direction

## Quick Facts
- arXiv ID: 2602.02132
- Source URL: https://arxiv.org/abs/2602.02132
- Reference count: 39
- Primary result: Refusal directions are geometrically distinct but steering any of them produces nearly identical behavior, acting as a shared one-dimensional control knob.

## Executive Summary
This paper investigates whether refusal behaviors in large language models are mediated by a single activation-space direction or multiple distinct mechanisms. Using contrastive activation differences across eleven refusal categories (safety, incomplete requests, anthropomorphization, over-refusal, etc.), the authors compute geometrically distinct refusal directions. However, linear steering along any of these directions produces nearly identical trade-offs between refusal and over-refusal rates. Sparse autoencoder analysis reveals a structured internal representation: a small reusable core of shared refusal latents supplemented by category-specific style latents. This explains why geometrically distinct directions behave similarly under linear intervention—they are different linear combinations of the same underlying refusal features, demonstrating that linear interpretability methods flatten mechanistic diversity into uniform behavioral control.

## Method Summary
The authors compute refusal directions as mean activation differences (refusal minus benign) at token position -2 in the residual stream. They evaluate steering performance by adding these directions to new prompts with varying strengths, measuring refusal and over-refusal rates using a WildGuard classifier. For SAE analysis, they use GemmaScope to decompose activations into latents, rank them by firing-rate separation scores, and reconstruct refusal directions from top latents. The study uses four datasets (WildGuardMix, SorryBench, CoCoNot, XSTest) to create 11 evaluation splits with balanced 32/32 refusal-benign prompt pairs. Models tested include Gemma-2-9b-it and Llama-3.1-8B-Instruct with steering applied at mid-layer resid_pre positions.

## Key Results
- Geometrically distinct refusal directions exist across categories (cosine similarities typically 0.4-0.6, some near-orthogonal)
- Steering along any refusal direction produces identical refusal/over-refusal trade-off curves
- SAE analysis reveals a small robust core (2.5-3.6% of latents) shared across all refusal categories
- Raw activation-space refusal directions align highly (0.85-0.97 cosine similarity) with SAE-based reconstructions

## Why This Works (Mechanism)

### Mechanism 1: The "One-Dimensional Knob" Effect
Linear steering along any refusal-related direction acts as a coarse, shared control mechanism that triggers refusal regardless of the specific semantic category. The intervention saturates a shared "refusal" subspace, forcing the model into a refusal state. Evidence shows steering produces identical behavior across different semantic categories, acting as a single control knob.

### Mechanism 2: The "Core + Tail" Latent Structure
Refusal is encoded by a sparse, reusable core of general "refusal" features supplemented by a long tail of category-specific features that dictate style. SAEs decompose the residual stream, revealing a strict intersection of active latents across all refusal categories (the core) and unique features for specific splits (the tail). Linear directions are weighted sums of these core features.

### Mechanism 3: Reconstruction of Activation Directions
Geometrically distinct refusal directions in raw activation space are different linear combinations of the same underlying SAE latent features. The paper demonstrates high cosine similarity (0.85-0.97) between raw contrastive directions and SAE-based reconstructions, implying the raw "direction" is an artifact of how sparse features sum up.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed: The paper relies on SAEs to decompose dense geometric directions into interpretable "latents." Without understanding SAEs (encoder/decoder, JumpReLU, L0 sparsity), the "Core + Tail" finding is opaque.
  - Quick check: If an SAE latent has a firing rate of 0.8 on "safety" prompts and 0.1 on "benign" prompts, what is its "refusal separation score" (Δ) according to Equation 4?

- **Concept: Contrastive Activation Addition (Steering)**
  - Why needed: The causal evidence comes from "steering" (x' = x + αr). You must understand how modifying the residual stream at a specific layer shifts model behavior.
  - Quick check: Why does the paper steer at the "decision state" (token position -2) rather than the first token of the response?

- **Concept: Residual Stream Geometry**
  - Why needed: The paper debates whether refusal is a "single direction." Understanding vector space concepts like cosine similarity, orthogonality, and projection is required to interpret Table 1 and the "geometrically distinct" findings.
  - Quick check: If two refusal directions have a cosine similarity of -0.062 (Table 1, Humanizing vs. Incomplete), what does that imply about their geometric relationship?

## Architecture Onboarding

- **Component map:** 4 datasets → 11 evaluation splits → Activation extraction at token -2 → Direction computation → Steering intervention OR SAE analysis → WildGuard classification
- **Critical path:**
  1. Prepare balanced 32/32 splits for a target category
  2. Extract activations at the decision token (position -2)
  3. Compute the mean-difference vector (Refusal Direction)
  4. If Steering: Add vector to new prompts at generation time
  5. If Analyzing: Project activations into SAE space → rank latents by separation score → ablate/steer specific latents
- **Design tradeoffs:**
  - Steering Strength (α): Higher α increases refusal of harmful prompts but also increases over-refusal of benign prompts
  - SAE Layer Selection: Layer 9 vs. 31 trades off between "early concept formation" and "final behavioral output"
  - Prompt Pool Size: Small pools (32) introduce noise but are computationally efficient
- **Failure signatures:**
  - "Refusal without Style": Steering works but style doesn't match intended split, indicating collapse into dominant "core"
  - Ablation Resistance: In Llama models, ablating refusal direction fails to suppress refusal, suggesting redundant mechanisms
- **First 3 experiments:**
  1. Replicate the "Core" Analysis: Train a linear classifier on SAE latent activations of identified "Core" latents to predict Refusal vs. Benign
  2. Style Ablation Test: Identify "tail" latents specific to "Humanizing" refusals, ablate only these, and check if refusal style changes without changing rate
  3. Cross-Model Transfer: Compute "Safety" direction on Gemma-2-9b and apply to Llama-3.1-8B or compare SAE latents directly

## Open Questions the Paper Calls Out
- Do the observed refusal mechanisms persist in larger models and across substantially different architectures?
- Do base (non-instruction-tuned) models exhibit the same structured internal representation of refusal?
- Can non-linear interventions in activation or latent space preserve mechanistic diversity while maintaining effective refusal control?
- What determines the boundary between the shared refusal core and the long tail of style-specific latents?

## Limitations
- Experiments limited to gemma-2-9b-it and Llama-3.1-8B-Instruct; scaling behavior and cross-architecture generalization remain untested
- Focus on steering at a single token position (-2) and layer leaves open whether refusal is a multi-layered, multi-step process
- The "single knob" model is plausible but not definitively proven; alternative mechanisms are not ruled out
- Over-refusal problem is identified but not solved; paper does not explore non-linear or more sophisticated interventions

## Confidence
- **High confidence:** Geometrically distinct refusal directions exist; steering any direction produces identical trade-off curves
- **Medium confidence:** SAE-based explanation for why distinct directions behave the same ("core + tail" hypothesis)
- **Low confidence:** Generalizability of "single knob" model to all types of refusal and non-compliance; assertion that findings definitively limit utility of linear interpretability methods

## Next Checks
1. **Temporal Steering Experiment:** Apply steering vector at multiple token positions during response generation to test if effect is maximal and consistent at -2 or varies across positions
2. **Cross-Model Core Validation:** Compare "Core" SAE latents for Safety category between Gemma-2-9b and Llama-3.1-8B to test for universal "refusal core"
3. **Adversarial Steering Test:** Design prompts where refusal style is semantically critical, apply steering, and check if style matches intended direction or defaults to generic refusal