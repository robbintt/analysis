---
ver: rpa2
title: Physics-informed neural networks to solve inverse problems in unbounded domains
arxiv_id: '2512.12074'
source_url: https://arxiv.org/abs/2512.12074
tags:
- domain
- error
- training
- networks
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of solving inverse problems for
  the Poisson equation in unbounded domains, where traditional boundary conditions
  are unavailable or impractical to obtain. The authors propose a novel methodology
  using Physics-Informed Neural Networks (PINNs) and Physics-Informed Kolmogorov-Arnold
  Networks (PIKANs) to tackle this problem.
---

# Physics-informed neural networks to solve inverse problems in unbounded domains

## Quick Facts
- arXiv ID: 2512.12074
- Source URL: https://arxiv.org/abs/2512.12074
- Authors: Gregorio Pérez-Bernal; Oscar Rincón-Cardeño; Silvana Montoya-Noguera; Nicolás Guarín-Zapata
- Reference count: 28
- Primary result: PINNs achieve one order of magnitude lower relative error than PIKANs while being 1000x faster for Poisson inverse problems in unbounded domains.

## Executive Summary
This work addresses the challenge of solving inverse problems for the Poisson equation in unbounded domains, where traditional boundary conditions are unavailable or impractical to obtain. The authors propose a novel methodology using Physics-Informed Neural Networks (PINNs) and Physics-Informed Kolmogorov-Arnold Networks (PIKANs) to tackle this problem. A key innovation is the development of a dual-network architecture that simultaneously learns the solution and the unknown parameter (e.g., thermal conductivity or permittivity) using a shared loss function. Additionally, they introduce a specialized sampling strategy using normal and exponential distributions to focus training points in regions of interest while allowing the network to generalize in unbounded domains.

The primary results show that PINNs significantly outperform PIKANs in both accuracy and computational efficiency. PINNs achieved a relative error one order of magnitude lower than PIKANs while being 1,000 times faster to train. The PINNs demonstrated strong generalization capabilities, accurately approximating solutions and parameters even outside the training domain. Experiments with varying levels of noise in observed data revealed that both PINNs and PIKANs exhibit a linear increase in error with noise, but PINNs consistently achieved lower absolute errors. These findings highlight PINNs as a more reliable and efficient tool for solving inverse problems in unbounded domains.

## Method Summary
The paper proposes solving inverse Poisson problems in unbounded domains using dual-network PINNs that simultaneously learn the solution u and parameter k. The method uses a two-stage training approach (Adam followed by L-BFGS) and introduces specialized sampling strategies (normal and exponential distributions) to focus on regions of interest while maintaining generalization in unbounded domains. The dual-network architecture prevents bias between solution and parameter estimation, with a combined loss function incorporating PDE residuals, observation MSE for u, and observation MSE for k.

## Key Results
- PINNs achieved relative error one order of magnitude lower than PIKANs for parameter estimation
- PINNs were 1,000 times faster to train than PIKANs on the same problem
- Both PINNs and PIKANs showed linear error growth with noise, but PINNs maintained lower absolute errors
- PINNs demonstrated strong generalization, accurately approximating solutions and parameters outside training domains

## Why This Works (Mechanism)
The dual-network architecture prevents the solution u from biasing the parameter k estimation by maintaining separate networks with shared loss. The specialized sampling strategy using normal/exponential distributions focuses training points in regions of interest while allowing the network to generalize in unbounded domains. The two-stage optimization (Adam → L-BFGS) provides robust convergence followed by refinement. PINNs' standard MLP structure with tanh activation proves more stable than PIKANs' polynomial-based structure in unbounded domains, avoiding numerical instability issues.

## Foundational Learning
- **Physics-Informed Neural Networks (PINNs):** Neural networks trained to satisfy PDEs by incorporating physics constraints into the loss function. Needed because traditional PINNs struggle with unbounded domains due to uniform sampling limitations.
- **Dual-network architecture:** Separate networks for solution u and parameter k with shared loss. Needed to prevent solution u from biasing parameter k estimation.
- **Kolmogorov-Arnold Networks (KANs):** Alternative to MLPs using learnable activation functions for improved interpretability. Needed for potential transparency but showed instability in unbounded domains.
- **Normal/Exponential sampling:** Distribution-based point selection focusing on regions of interest. Needed to maintain generalization in unbounded domains where uniform sampling fails.
- **L-BFGS optimization:** Quasi-Newton method for fine-tuning neural network weights. Needed for achieving high accuracy after initial Adam training convergence.

## Architecture Onboarding

- **Component map:**
  - Input Layer: Takes coordinates $(x, y)$.
  - Dual Networks: Two separate networks.
    - $NN_u$: A standard MLP with `tanh` activation, predicting the solution $u$.
    - $NN_k$: A similar MLP with `tanh` activation, predicting the parameter $k$.
  - Physics Engine: A module that uses automatic differentiation to compute the gradients of the predicted $u$ and $k$ to evaluate the PDE residual $\nabla \cdot (k\nabla \hat{u}) - f$.
  - Loss Aggregator: Computes the total loss by summing the PDE residual loss, the MSE for observations of $u$, and the MSE for observations of $k$. For semi-infinite domains, it adds a boundary condition loss.

- **Critical path:**
  1. Define the PDE (Poisson equation) and its source term $f$.
  2. Implement the dual network architecture in a framework like PyTorch.
  3. Implement the physics-informed loss function, including the PDE residual term.
  4. Implement the sampling strategy for training points (normal/exponential) and observation points (uniform).
  5. Execute the two-stage training: Adam for robust convergence, then L-BFGS for refinement.

- **Design tradeoffs:**
  - **Dual vs. Single Network:** The dual-network design prevents bias but increases the number of parameters to train. A single network with two outputs is simpler but may suffer from the solution for $u$ biasing the solution for $k$.
  - **PINNs vs. PIKANs:** The paper strongly concludes that for this problem, PINNs are vastly superior in both speed (~1000x faster) and accuracy (one order of magnitude lower error). PIKANs offer potential interpretability but at a significant computational cost and with stability issues in unbounded domains.
  - **Sampling Strategy:** The proposed distribution-based sampling is key for unbounded domains but assumes the solution stabilizes at infinity. It may not be suitable for problems with dynamics at infinity.

- **Failure signatures:**
  - **PIKAN Instability:** PIKANs exhibited larger, more dispersed errors, especially in parameter approximation ($k$), and struggled near the edges of the computational domain. Their polynomial-based structure may cause numerical instability as the domain approaches infinity.
  - **PINN Overfitting:** The largest PINN configuration (64, 128) showed increased error due to overfitting, highlighting that more parameters are not always better.
  - **Domain Mismatch:** The method relies on the solution stabilizing at infinity. Failure will occur if this assumption is violated.

- **First 3 experiments:**
  1. **Reproduce Infinite Domain Baseline:** Implement the dual-network PINN for the infinite domain problem with the specified architecture (e.g., (16,32)), Adam (15k epochs) + L-BFGS (1.5k epochs), and normal distribution sampling. Measure relative error for $k$ and training time to establish a baseline.
  2. **Ablation on Sampling:** Compare the proposed normal distribution sampling for the infinite domain against a uniform distribution over the same domain. Quantify the impact on training convergence speed and final relative error.
  3. **Noise Robustness Test:** Retrain the best-performing PINN model while injecting 5%, 10%, and 15% white noise into the observation data for $u$ and $k$. Plot the relative error of the recovered $k$ against the noise percentage to confirm the observed linear relationship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural modifications to Physics-Informed Kolmogorov-Arnold Networks (PIKANs), such as alternative basis functions, mitigate the numerical instability observed in unbounded domains?
- Basis in paper: [explicit] The Conclusion states that PIKANs exhibited limitations related to their "polynomial-based structure," which "can lead to numerical instability and error growth" as domains approach infinity.
- Why unresolved: The current work only tested standard B-splines, which failed to perform effectively in unbounded settings compared to PINNs.
- What evidence would resolve it: A study comparing the performance of PIKANs using different spline bases (e.g., Chebyshev polynomials) or adaptive grids against PINNs on the same unbounded inverse problems.

### Open Question 2
- Question: Does the proposed normal and exponential sampling strategy remain effective for inverse problems where the solution exhibits oscillatory or non-decaying behavior at infinity?
- Basis in paper: [explicit] The Conclusion notes that the sampling strategy is "particularly suitable for problems whose solutions stabilize as we leave the zone of interest," implying potential limitations for other solution types.
- Why unresolved: The verification of the methodology relied entirely on manufactured solutions that stabilize at domain boundaries.
- What evidence would resolve it: Application of the sampling strategy to inverse problems with known non-decaying or highly oscillatory asymptotic behavior.

### Open Question 3
- Question: For which specific scientific applications does the interpretability of PIKANs justify their significantly higher computational cost compared to PINNs?
- Basis in paper: [explicit] Section 4.4 explicitly asks, "Whether this interpretability gain justifies the additional training time depends heavily on the application," while noting that extracting actionable insights from deep spline compositions remains challenging.
- Why unresolved: The paper demonstrates that PINNs are orders of magnitude faster and more accurate, leaving the practical utility of PIKANs uncertain beyond theoretical transparency.
- What evidence would resolve it: Case studies in scientific machine learning where the spline-level transparency of a trained PIKAN leads to the discovery of a physical law or relationship that a PINN obscured.

### Open Question 4
- Question: How does the dual-network architecture and specific sampling strategy perform when applied to non-linear or hyperbolic PDEs?
- Basis in paper: [inferred] The paper focuses exclusively on the linear, elliptic Poisson equation (Methodology), leaving the generalizability of the unbounded domain methodology to more complex dynamics untested.
- Why unresolved: Inverse problems involving wave propagation or fluid dynamics often behave differently than potential fields, and the dual-network loss balancing may require different weighting.
- What evidence would resolve it: Experiments applying the dual-network methodology to inverse problems for the Helmholtz equation or Navier-Stokes equations in unbounded domains.

## Limitations
- Results are based on synthetic data from known analytic solutions rather than real-world experimental measurements
- Superior performance of PINNs demonstrated only for Poisson equation in unbounded domains
- Sampling strategy effectiveness relies on assumption that solutions stabilize at infinity
- Computational cost advantage of PINNs may not generalize to other PDE types

## Confidence

- **High confidence:** PINNs demonstrate superior accuracy and computational efficiency compared to PIKANs for Poisson inverse problems in unbounded domains
- **Medium confidence:** The proposed dual-network architecture and specialized sampling strategy are effective for this specific problem class
- **Medium confidence:** The linear relationship between noise levels and error rates is observed but based on limited noise percentages

## Next Checks
1. Validate the PINN methodology on a real-world dataset with experimental measurements rather than synthetic data to assess practical applicability
2. Test the approach on different types of PDEs (e.g., Helmholtz or convection-diffusion equations) to evaluate generalizability beyond Poisson problems
3. Implement cross-validation with varying network architectures to determine optimal configurations and identify potential overfitting thresholds more systematically