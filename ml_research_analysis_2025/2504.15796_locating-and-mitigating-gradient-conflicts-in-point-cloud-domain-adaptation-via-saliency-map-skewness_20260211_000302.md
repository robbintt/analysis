---
ver: rpa2
title: Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation
  via Saliency Map Skewness
arxiv_id: '2504.15796'
source_url: https://arxiv.org/abs/2504.15796
tags:
- point
- domain
- gradient
- skewness
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gradient conflicts in point
  cloud unsupervised domain adaptation (UDA) by proposing a novel data sampling strategy
  based on saliency map skewness. The authors observe that not all gradients from
  self-supervision tasks are beneficial for classification, with some causing negative
  transfer.
---

# Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness

## Quick Facts
- arXiv ID: 2504.15796
- Source URL: https://arxiv.org/abs/2504.15796
- Authors: Jiaqi Tang; Yinsong Xu; Qingchao Chen
- Reference count: 40
- Primary result: SM-DSB improves UDA accuracy by filtering detrimental self-supervision gradients based on saliency map skewness

## Executive Summary
This paper addresses the problem of gradient conflicts in point cloud unsupervised domain adaptation (UDA) by proposing a novel data sampling strategy based on saliency map skewness. The authors observe that not all gradients from self-supervision tasks are beneficial for classification, with some causing negative transfer. To mitigate this, they introduce the Saliency Map-based Data Sampling Block (SM-DSB), which uses the skewness of 3D saliency maps to estimate gradient conflicts without requiring target labels. By dynamically filtering out samples whose self-supervision gradients are not beneficial for classification, the method improves performance across various point cloud UDA tasks. Extensive experiments show that SM-DSB achieves state-of-the-art results when integrated into mainstream point cloud UDA frameworks, demonstrating its effectiveness in reducing negative transfer and enhancing cross-domain feature learning. The approach is scalable, introduces modest computational overhead, and provides new insights into understanding UDA through back-propagation analysis.

## Method Summary
The method employs a two-step multi-task learning framework. In Step 1, the model jointly trains on source domain classification and self-supervision tasks on both domains. The key innovation is the SM-DSB module, which calculates saliency scores for each point based on classification loss gradients, then computes the skewness of the resulting saliency map distribution. Samples are ranked by skewness, and an adaptive threshold determines which samples have their self-supervision gradients masked. In Step 2, pseudo-labels are generated for the target domain, and the model is fine-tuned. The method uses hyperparameters α=1 and β=0.7, with batch sizes of 64 (DGCNN) or 128 (PointNet) and 40,000 steps (DGCNN) or 10,000 steps (PointNet).

## Key Results
- SM-DSB achieves state-of-the-art performance across multiple point cloud UDA benchmarks when integrated into mainstream frameworks
- The method demonstrates significant improvements in classification accuracy on target domains by mitigating negative transfer from conflicting gradients
- Experimental results validate the effectiveness of using saliency map skewness as a proxy for estimating gradient conflicts without requiring target labels

## Why This Works (Mechanism)

### Mechanism 1: Gradient Conflict Estimation via Skewness
The skewness of a point cloud's saliency map serves as a label-free proxy for estimating gradient conflicts between self-supervision and classification tasks. The method calculates a saliency score for each point based on the gradient of the classification loss relative to point distance. High positive skewness indicates the model relies on sparse "critical points" (shortcut learning), making it brittle to domain shifts. Samples with high skewness are likely to generate self-supervision gradients that conflict with the oracle classification gradients.

### Mechanism 2: Dynamic Gradient Filtering (SM-DSB)
Dynamically zeroing the self-supervision loss for high-skewness samples prevents negative transfer without requiring target domain labels. The "Selector" module sorts samples by skewness and sets an adaptive threshold based on a ratio β. Samples exceeding this threshold have their self-supervision gradients masked (λb=0). This isolates the shared encoder from detrimental auxiliary gradients while retaining beneficial self-supervision signals.

### Mechanism 3: Noise-Robust Target Sampling
Applying small perturbations to skewness scores derived from pseudo-labels mitigates the noise inherent in unsupervised target adaptation. In the second training stage (pseudo-labeling), target labels are noisy, leading to slightly inflated skewness scores. The method adds a Gaussian perturbation to these scores before thresholding to prevent the false rejection of samples that appear "high skewness" solely due to label noise.

## Foundational Learning

- **Concept: Gradient Surgery / Conflict in MTL**
  - Why needed: The paper builds on the premise that multi-task learning (classification + self-supervision) can induce negative transfer. Understanding that gradients from different tasks can point in opposing directions is essential to grasp why filtering is necessary.
  - Quick check: If two tasks have opposing gradients, does averaging them (standard MTL) move the model closer to the optimal minimum for both?

- **Concept: Saliency Maps in Point Clouds**
  - Why needed: The core metric (skewness) is derived from the saliency map. One must understand that this map visualizes which 3D points the model considers "important" for its current decision.
  - Quick check: Does a high saliency score for a point mean the model uses that point to identify the object, or that changing that point would confuse the model?

- **Concept: Statistical Skewness**
  - Why needed: The paper uses the third standardized moment (skewness) to quantify the shape of the "attention" distribution.
  - Quick check: Does a high positive skewness indicate a heavy concentration of low-value points with a few extreme high-value outliers, or a uniform distribution?

## Architecture Onboarding

- **Component map:**
  Encoder (Φf) -> SM-DSB (Measurer) -> SM-DSB (Selector) -> Classification Head (Φc), Self-Supervision Head (Φssl)

- **Critical path:**
  1. Forward pass (Source & Target)
  2. Compute Classification Loss (Lc) on Source
  3. Trigger Measurer: Backprop to input coordinates to generate Saliency Map → Calculate Skewness
  4. Compute Self-Supervision Loss (Lssl) on Source & Target
  5. Trigger Selector: Apply mask λ to Lssl (Zero out high skewness samples)
  6. Joint Backward (Encoder updates on masked Lssl + Lc)

- **Design tradeoffs:**
  - Sampling Ratio β: The paper sets β ≈ 0.7
    - Lower β: Filters more samples (safer against conflict, but less training data)
    - Higher β: Uses more data (more risk of negative transfer)
  - Rescaling Factor α: Used in saliency calculation. Paper finds low sensitivity; standard α=1 is stable

- **Failure signatures:**
  - Stagnation: If accuracy doesn't improve, check if β is too aggressive (filtering valid gradients)
  - High Variance: If training loss oscillates, the adaptive threshold τ might be unstable due to small batch sizes
  - Domain Drift: If target performance degrades specifically in the second stage, the pseudo-label noise perturbation (0.1) might be insufficient for the specific dataset noise level

- **First 3 experiments:**
  1. Sanity Check (Correlation): Replicate Figure 1. Plot skewness vs. cosine similarity of gradients on a held-out set to verify the proxy holds for your specific backbone
  2. Hyperparameter Sweep (β): Run ablation on the selection ratio (e.g., [0.5, 0.7, 0.9]) on a single domain pair (e.g., ModelNet → ScanNet) to find the stability range
  3. Integration Test: Plug SM-DSB into a baseline MTL framework (like DefRec or GAST) on PointDA-10. Compare standard MTL vs. MTL+SM-DSB to isolate the performance gain from conflict mitigation

## Open Questions the Paper Calls Out

### Open Question 1
What alternative metrics to skewness can estimate gradient conflicts with higher accuracy or lower computational cost?
The conclusion states, "In future work, we will extend to explore other metrics to estimate the gradient conflict, both accurately and fast." The current method relies solely on a third-order statistic (skewness), which serves as a proxy but may not capture all nuances of gradient interference or represent the optimal efficiency-accuracy trade-off.

### Open Question 2
Can the sample selection ratio β be determined adaptively rather than empirically to ensure stability across different dataset scales and training iterations?
The paper notes that the selection ratio β is highly correlated with iteration numbers and dataset scales, observing that "a single set of balanced ratios... does not necessarily generalize to optimal performance." The current framework requires manual tuning of β for different transfer directions, limiting the method's robustness and "plug-and-play" capability.

### Open Question 3
Is the causal link between saliency map skewness and gradient conflict robust across diverse self-supervision tasks (e.g., contrastive learning) or architectures (e.g., Transformers)?
While the authors demonstrate the method works with DefRec and GAST, the causal analysis (ANM) was performed on specific geometric tasks. The generalizability of "high skewness equals high conflict" to semantic or contrastive tasks on different backbones is unverified.

## Limitations
- The causal link between skewness and gradient utility is derived from a limited experimental scope and may not generalize across different network architectures or self-supervision tasks
- The assumption that a fixed percentage (β=0.7) of samples in each batch are detrimental is a heuristic that could vary with dataset characteristics and batch size
- The method requires careful tuning of hyperparameters, particularly β, which may need adjustment for different transfer directions

## Confidence
- **High**: SM-DSB improves UDA accuracy when integrated into mainstream frameworks (experimental evidence across multiple datasets)
- **Medium**: The skewness metric is a useful proxy for gradient conflict (supported by causal analysis, but with limited scope)
- **Low**: The specific value of β=0.7 is optimal for all scenarios (likely dataset and architecture dependent)

## Next Checks
1. **Architecture Generalization**: Test SM-DSB with a different point cloud backbone (e.g., PointNet++) to verify the skewness-gradient conflict relationship is not backbone-specific
2. **Skewness Noise Analysis**: Systematically vary the pseudo-label accuracy (e.g., by adjusting confidence threshold) to quantify how noise in target labels affects the skewness-based filtering and its correction
3. **Alternative Metrics**: Replace skewness with other distributional metrics (e.g., kurtosis, entropy) to determine if the performance gain is unique to skewness or a more general property of saliency map shape