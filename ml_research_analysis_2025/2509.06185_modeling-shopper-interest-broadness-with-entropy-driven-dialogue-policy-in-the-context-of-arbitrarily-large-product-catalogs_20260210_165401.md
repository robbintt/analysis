---
ver: rpa2
title: Modeling shopper interest broadness with entropy-driven dialogue policy in
  the context of arbitrarily large product catalogs
arxiv_id: '2509.06185'
source_url: https://arxiv.org/abs/2509.06185
tags:
- product
- entropy
- shopper
- dialogue
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an entropy-driven dialogue policy for conversational
  recommender systems in e-commerce. The core idea is to measure the entropy of retrieval
  score distributions to decide between exploration (asking clarifying questions)
  and exploitation (making recommendations).
---

# Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs

## Quick Facts
- arXiv ID: 2509.06185
- Source URL: https://arxiv.org/abs/2509.06185
- Reference count: 24
- The system uses entropy of retrieval score distributions to dynamically route between exploration (asking clarifying questions) and exploitation (making recommendations) in e-commerce CRS

## Executive Summary
This work presents an entropy-driven dialogue policy for conversational recommender systems in e-commerce. The core idea is to measure the entropy of retrieval score distributions to decide between exploration (asking clarifying questions) and exploitation (making recommendations). When a user query yields a highly skewed score distribution (low entropy), the system makes direct recommendations; when scores are spread out (high entropy), it asks clarifying questions. The approach uses a two-stage neural retrieval pipeline with a re-ranker to generate score distributions, enabling real-time catalog awareness without bloating context. The method improves shopper engagement, resulting in longer conversations compared to a baseline LLM-driven policy. This simple entropy-based routing strategy offers an effective way to dynamically balance exploration and exploitation in large product catalogs.

## Method Summary
The system implements a two-stage retrieval pipeline: first, E5 embeddings with HNSW indexing retrieve top-50 candidates, then a RoBERTa cross-encoder re-ranks them using click-through labels to produce calibrated relevance scores. The normalized entropy of these scores determines whether to explore (ask clarifying questions) or exploit (recommend products). The entropy threshold is bucketed into three merchant-configurable presets (educational, balanced, pushy) based on recall@10 plateaus observed at entropy ~0.3 and ~0.8. The approach enables catalog-aware routing without context bloat by using retrieval scores as a proxy for query broadness relative to the actual product catalog.

## Key Results
- Entropy-driven policy led to more engaging conversations compared to LLM-classifier baseline
- Three threshold presets (educational, balanced, pushy) allow merchant customization of exploration-exploitation balance
- A/B testing on real customers showed improved shopper engagement metrics
- Conversion rate improvement was statistically insignificant despite increased conversation length

## Why This Works (Mechanism)

### Mechanism 1: Entropy as a Catalog-Contextualized Query Broadness Proxy
Normalized entropy of retrieval score distributions serves as a reliable proxy for user intent specificity, enabling catalog-aware routing without context bloat. Low entropy (concentrated mass) indicates precise intent; high entropy (flat distribution) signals vagueness. The shape of the retrieval score distribution correlates with query specificity relative to the actual catalog.

### Mechanism 2: Two-Stage Neural Retrieval with Calibrated Re-Ranking
A two-stage retrieval pipeline (dense retrieval + cross-encoder reranking) provides sufficiently calibrated scores for entropy computation while remaining scalable to large catalogs. Stage 1 uses multilingual E5 embeddings with HNSW indexing for approximate nearest neighbor search. Stage 2 re-ranks top-50 candidates with a RoBERTa cross-encoder trained on binary cross-entropy with click-through labels, producing calibrated relevance scores.

### Mechanism 3: Threshold-Based Exploration-Exploitation Routing
A fixed entropy threshold, empirically derived from organic search recall patterns, effectively separates queries warranting discovery questions from those ready for recommendations. The system computes entropy for the user's query and routes: if entropy < τ_merchant, route to Recommendation; if entropy ≥ τ_merchant, route to Discovery. Thresholds are bucketed into three presets based on recall@10 plateaus observed at entropy ~0.3 and ~0.8.

## Foundational Learning

- **Concept: Normalized Shannon Entropy**
  - Why needed here: Core mathematical tool for quantifying distribution "peakedness" vs "flatness"; must understand how entropy ranges [0, log k] and why normalization enables threshold portability.
  - Quick check question: Given scores [0.9, 0.05, 0.05, 0.0, 0.0] for k=5, is entropy closer to 0 or 1?

- **Concept: Cross-Encoder vs Bi-Encoder Retrieval**
  - Why needed here: Understanding why Stage 1 (bi-encoder) is fast but approximate, while Stage 2 (cross-encoder) is slow but accurate and better calibrated.
  - Quick check question: Why can't we use only the bi-encoder scores for entropy computation?

- **Concept: Exploration-Exploitation Tradeoff in Bandits/Dialogue**
  - Why needed here: Frames the problem—entropy routing is a heuristic for when to gather information (explore) vs act on current beliefs (exploit).
  - Quick check question: If a user says "I need nails" at a nail supply store vs a beauty store, should entropy differ? Why?

## Architecture Onboarding

- **Component map:**
  User Message → Planning Node (LLM) → Sales opportunity detected? → Shopping-Assistant Workflow → Query Generator (LLM) → [Exploratory queries, Focused query] → Product Search System → Dense Retrieval (E5 + HNSW) → top-50 → Cross-Encoder Rerank (RoBERTa) → calibrated scores → Entropy Computation (B_focused) → Threshold Comparison (τ_merchant) → Discovery (clarifying Qs) or Recommendation (product suggestions) → Reply-Generation Node (LLM) → Response

- **Critical path:** The entropy computation depends entirely on reranker score quality. Debug output: log top-10 scores, entropy value, and routing decision for each query.

- **Design tradeoffs:**
  - k=50 candidates: Larger k improves entropy estimator convergence but increases reranker latency. Paper shows error converges reasonably by k=50.
  - Three threshold presets: Merchant-configurable aggressiveness trades discovery depth against conversion pressure; paper notes conversion impact was statistically insignificant.
  - Top-k entropy vs full-catalog entropy: Paper acknowledges top-k overestimates true broadness; empirical error analysis justifies approximation.

- **Failure signatures:**
  - Always routing to Discovery: Entropy persistently high → check reranker score distribution; may be miscalibrated toward uniformity.
  - Always routing to Recommendation: Entropy persistently low → reranker may be overconfident; verify BCE training labels quality.
  - Irrelevant recommendations: Retrieval working but ranking poor → inspect negative sampling strategy.
  - Cold-start failures: New products not retrieved → verify embeddings are computed on ingest and indexed immediately.

- **First 3 experiments:**
  1. Entropy calibration audit: Sample 100 queries, manually label specificity (vague/moderate/precise), compare with computed B_focused. Validate threshold boundaries at ~0.3 and ~0.8.
  2. Reranker score distribution analysis: Plot histogram of top-50 scores across 1K queries. Check for healthy variance vs pathological uniformity/polarization.
  3. A/B threshold sensitivity: Run concurrent tests with τ ∈ {0.25, 0.35, 0.45} on traffic subset. Measure conversation length, recommendation click-through, and user satisfaction signals.

## Open Questions the Paper Calls Out

### Open Question 1
How can a shopping assistant proactively initiate contact with shoppers based on browsing data while minimizing distraction risk? The current system is purely reactive; proactive engagement introduces new tradeoffs around intrusion, timing, and user experience that the present architecture does not address. A/B tests comparing proactive vs. reactive initiation with metrics on engagement, conversion, and user-reported distraction or annoyance would resolve this.

### Open Question 2
What are the formal convergence bounds for the top-k entropy estimator relative to full-catalog entropy? The paper acknowledges it does "not derive convergence bounds of this estimator" and relies only on empirical error analysis over organic search data. Formal analysis bounding estimation error as a function of k, catalog size N, and score distribution properties, validated empirically across varied catalogs, would strengthen confidence in the estimator's reliability.

### Open Question 3
What factors mediate the relationship between entropy-driven dialogue policies and actual conversion rate improvements? The authors observed "a slight increase" in conversion that "remained statistically insignificant," and they believe "there are way more factors involved" beyond dialogue policy alone. Causal mediation analysis or factorial experiments isolating dialogue policy from other conversion drivers would clarify the policy-to-conversion pathway.

### Open Question 4
How should entropy thresholds be personalized or adapted for different merchants, catalog structures, or shopper segments? The paper uses three fixed aggressiveness presets and acknowledges thresholds are "experimental and can be refined," but does not explore adaptive or personalized threshold selection. Experiments comparing static vs. merchant-adaptive vs. user-adaptive threshold strategies, measuring engagement and conversion across diverse merchant profiles, would identify optimal approaches.

## Limitations

- Catalog Entropy Estimation Bias: Top-k entropy computation overestimates true catalog entropy, but the paper lacks empirical quantification of this bias across diverse product domains.
- Threshold Calibration Generalization: Thresholds derived from storefront search recall patterns may not generalize to conversational queries, which could exhibit systematically different entropy distributions.
- Limited Conversion Impact: Despite improved engagement, conversion rate improvement was statistically insignificant, suggesting the policy-to-conversion pathway is indirect and potentially moderated by other factors.

## Confidence

- **High Confidence:** The two-stage retrieval architecture with calibrated reranking scores is well-established and technically sound. The entropy computation methodology is mathematically correct.
- **Medium Confidence:** The core claim that entropy correlates with query broadness and enables effective routing is supported by recall plateaus and engagement improvements, but the calibration method relies on implicit assumptions about query distribution similarity.
- **Low Confidence:** The claim that three fixed threshold presets optimally serve all merchants lacks empirical validation across diverse catalog types and merchant strategies.

## Next Checks

1. **Cross-Merchant Threshold Validation:** Run A/B tests with fixed thresholds (τ=0.25, 0.35, 0.45) across 3+ merchant types. Measure not just engagement but also conversion rate and customer satisfaction to identify optimal thresholds per domain.
2. **Entropy Bias Quantification:** Sample 1000 queries, retrieve full catalog matches (not just top-k), compute true catalog entropy, and compare with top-k estimates. Plot error distribution and correlate with routing accuracy.
3. **Score Calibration Audit:** For 100 queries, manually label query specificity (vague/moderate/precise), compare with computed entropy, and analyze reranker score distributions. Apply calibration techniques if scores show pathological uniformity or polarization.