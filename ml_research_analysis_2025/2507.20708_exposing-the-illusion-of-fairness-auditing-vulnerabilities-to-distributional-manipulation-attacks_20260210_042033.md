---
ver: rpa2
title: 'Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional
  Manipulation Attacks'
arxiv_id: '2507.20708'
source_url: https://arxiv.org/abs/2507.20708
tags:
- fairness
- dataset
- methods
- tests
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how data samples can be manipulated to
  artificially satisfy fairness criteria, specifically Disparate Impact, while remaining
  statistically indistinguishable from the original distribution. The authors introduce
  methods based on entropic and optimal transport projections to create minimally
  perturbed datasets that meet prescribed fairness constraints.
---

# Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks

## Quick Facts
- **arXiv ID:** 2507.20708
- **Source URL:** https://arxiv.org/abs/2507.20708
- **Reference count:** 40
- **Primary result:** Minimal distributional perturbations can artificially satisfy fairness constraints while evading standard statistical detection tests

## Executive Summary
This paper exposes a critical vulnerability in fairness auditing frameworks by demonstrating that datasets can be manipulated to appear fair while remaining statistically indistinguishable from the original distribution. The authors introduce mathematically sound methods based on entropic and optimal transport projections that can create minimally perturbed datasets satisfying prescribed fairness constraints, specifically Disparate Impact. Through experiments on seven benchmark datasets, they show that certain manipulation methods—particularly the Wasserstein-minimizing matching approach—can significantly improve fairness metrics without being detected by rigorous statistical tests, highlighting fundamental weaknesses in current auditing frameworks.

## Method Summary
The paper proposes multiple methods to manipulate empirical distributions under fairness constraints. The entropic projection approach reweights samples to minimize KL divergence while achieving target fairness metrics. The Wasserstein-based methods perturb feature vectors using gradient optimization or matching algorithms to satisfy constraints while minimizing transport cost. The MW(X,S,Ŷ) method iteratively swaps sample points to maximize fairness improvement per unit distance. These methods are evaluated by their ability to evade detection from seven statistical tests (KL, Wasserstein, MMD, KS) when compared against the original dataset.

## Key Results
- The Wasserstein-minimizing matching method (MW pX,S, ˆY q) consistently achieves the highest undetected fairness improvements across all tested datasets
- Entropic projection methods, while faster, are more easily detected by KL-based statistical tests
- Statistical detection power increases with sample size submitted for audit, with 20% samples being harder to manipulate undetected than 10% samples
- Some datasets with extreme original DI values (like BAF) are more vulnerable to undetected manipulation than others

## Why This Works (Mechanism)

### Mechanism 1: Entropic Projection for Minimal-KL Reweighting
- **Why needed here:** Creates a reweighted version of the original sample that appears fairer while minimizing information loss
- **Mechanism:** Solves an optimization problem to find weights λᵢ for each sample point, minimizing KL divergence subject to a linear constraint on a function Φ encoding the DI condition
- **Core assumption:** The empirical covariance matrix of the constraint function Φ is invertible
- **Evidence anchors:** Theorem 4.1 and Proposition 4.2 provide formal solution; Section 4.2 details the Φ function for DI
- **Break condition:** Fails when original sample has zero variance in relevant subgroups (singular covariance matrix)

### Mechanism 2: Wasserstein-Projection via Gradient-Based Perturbation
- **Why needed here:** Minimally perturbs individual feature vectors to shift classifier outputs and satisfy DI constraints
- **Mechanism:** Solves constrained optimization that moves each sample point by balancing squared-distance penalty and classifier output penalty, using iterative gradient updates
- **Core assumption:** Classifier f is differentiable with respect to inputs, allowing gradient-based optimization
- **Evidence anchors:** Theorem 5.1 proves optimal solution is push-forward map Tλ; Algorithm 3 outlines iterative procedure
- **Break condition:** Fails for highly discrete or categorical features where gradients are undefined

### Mechanism 3: Wasserstein-Minimizing Matching (MW pX,S, ˆY q)
- **Why needed here:** Achieves best tradeoff between DI improvement and undetectability through sample swaps
- **Mechanism:** Iteratively finds sample pairs whose swap yields largest DI increase per unit Wasserstein cost, preserving covariate structure
- **Core assumption:** Dataset contains sufficient diversity for beneficial swaps without large distributional shifts
- **Evidence anchors:** Table 4 and Figure 3 show consistent performance; Section 5.4 describes algorithm
- **Break condition:** Fails when required swaps for very low original DI cause detectable shifts in joint distribution

## Foundational Learning

- **Concept: Disparate Impact (DI)**
  - **Why needed here:** Core fairness metric being manipulated; defined as P(Ŷ=1|S=0) / P(Ŷ=1|S=1)
  - **Quick check:** For a dataset where 40% of protected group (S=0) receives positive outcome and 60% of non-protected group (S=1), DI = 0.4/0.6 = 0.67. This fails the 80% rule.

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** Quantifies information loss when one distribution approximates another; entropic projection minimizes this
  - **Quick check:** KL(Q, P) can be infinite if Q has support where P has zero probability, implying distributions are fundamentally incompatible

- **Concept: Wasserstein Distance (Optimal Transport)**
  - **Why needed here:** Measures minimal work to transform one distribution into another, considering geometry of space
  - **Quick check:** Moving 10 pounds of mass 5 units costs 50 units (for p=1). Analogous to changing a data point's feature values in a meaningful way

## Architecture Onboarding

- **Component map:** Audited Entity (Attacker) -> Audited Data (Qₜ) -> Auditor -> Statistical Tests -> Supervisory Authority (Defender with Qₙ)
- **Critical path:** 1) Manipulation: Choose method and target DI, compute minimal transformation 2) Submission: Draw random subsample (10%/20%) from Qₜ and submit for audit 3) Detection: Authority runs multiple statistical tests; all H₀ accepted means undetected manipulation
- **Design tradeoffs:** Target DI vs. Detectability (perfect fairness requires more aggressive manipulation), Sample Size (smaller hides better but may raise suspicion), Method Choice (Entropic fast but detected by KL tests; MW robust but computationally expensive)
- **Failure signatures:** Infinite KL divergence (manipulation creates data outside original support), High Wasserstein cost (large distributional shift), Failed multiple tests (any single test failure signals potential manipulation)
- **First 3 experiments:** 1) Replicate MW pX,S, ˆY q baseline on ADULT: increase DI from 0.30 to 0.80, measure costs, test 100 samples of 10% 2) Test sample size sensitivity: vary from 5% to 30%, plot highest undetected DI vs. sample size 3) Compare methods across EMP dataset: apply all 6 methods, create radar plot comparing KL and Wasserstein costs for DI=0.8

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed manipulation detection framework be effectively extended to non-tabular data (e.g., text, images) without relying on a specific descriptor space that the auditee can reverse-engineer?
- **Open Question 2:** Are the distributional manipulation methods theoretically applicable to individual fairness metrics, or are they strictly limited to global statistical measures?
- **Open Question 3:** Do the heuristic algorithms proposed for fair-washing, such as the Replace(S, Ŷ) method, guarantee a globally optimal solution for minimizing distributional shift?

## Limitations
- Theoretical guarantees rely on strong assumptions (invertible covariance matrices, differentiable classifiers, sufficient dataset diversity) that may not hold in practice
- Practical feasibility in real-world auditing scenarios is uncertain due to potential multiple audit rounds, additional context, or adaptive detection methods
- Methods are primarily validated on tabular data, with unclear applicability to other data modalities

## Confidence
- **High Confidence:** Mathematical formulations of entropic and optimal transport projections are sound with provided proofs
- **Medium Confidence:** Comparative effectiveness of different manipulation methods across datasets is well-demonstrated
- **Low Confidence:** Practical feasibility of these attacks in real-world auditing scenarios with adaptive defenses

## Next Checks
1. **Robustness Testing:** Evaluate method performance across different classifier architectures and with various feature types to assess generalizability
2. **Adaptive Detection:** Test whether combining statistical tests with additional audit techniques (e.g., examining prediction distributions within sensitive groups) would increase detection rates
3. **Practical Constraints:** Assess computational feasibility and detection risk when applying these methods to larger datasets (100K+ samples) and when audit samples are drawn from multiple disjoint subsets