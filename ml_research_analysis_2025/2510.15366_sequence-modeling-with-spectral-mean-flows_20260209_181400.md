---
ver: rpa2
title: Sequence Modeling with Spectral Mean Flows
arxiv_id: '2510.15366'
source_url: https://arxiv.org/abs/2510.15366
tags:
- page
- mean
- flow
- spectral
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces spectral mean flows, a novel approach to
  sequence modeling that leverages operator theory to represent and learn highly nonlinear
  and probabilistic state dynamics. The method embeds the full sequence distribution
  as a tensor in a product Hilbert space and defines a generative process as maximum
  mean discrepancy (MMD) gradient flow in the space of sequences.
---

# Sequence Modeling with Spectral Mean Flows

## Quick Facts
- arXiv ID: 2510.15366
- Source URL: https://arxiv.org/abs/2510.15366
- Reference count: 40
- This paper introduces spectral mean flows, a novel approach to sequence modeling that leverages operator theory to represent and learn highly nonlinear and probabilistic state dynamics.

## Executive Summary
This paper presents spectral mean flows, a novel approach to sequence modeling that leverages operator theory to represent and learn highly nonlinear and probabilistic state dynamics. The method embeds the full sequence distribution as a tensor in a product Hilbert space and defines a generative process as maximum mean discrepancy (MMD) gradient flow in the space of sequences. To overcome challenges with large tensors and slow sampling convergence, spectral mean flows introduce a scalable tensor network decomposition of sequence mean embeddings and extend MMD gradient flows to time-dependent Hilbert spaces.

## Method Summary
Spectral mean flows model sequence distributions by embedding them in a tensor product Hilbert space and defining a generative process as MMD gradient flow. The method uses tensor network decomposition to handle exponentially large tensors, time-dependent MMD flows connected to flow matching for faster convergence, and neural parameterization of spectral components for end-to-end learning. The approach is trained via conditional flow matching objectives and can generate sequences through ODE solvers.

## Key Results
- Competitive results across six time-series modeling datasets compared to baseline methods
- Demonstrated convergence of spectral mean flows at finite t=1 instead of asymptotic convergence
- Showed tractable computation of exponentially large tensor operations through tensor network decomposition
- Validated effectiveness on diverse datasets including Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor network decomposition of sequence mean embeddings makes exponentially large tensor computations tractable.
- Mechanism: Spectral decomposition of the conditional mean embedding operator U factorizes the hidden chain embedding into a tensor network, avoiding materialization of the full tensor in H^{⊗N}.
- Core assumption: The RKHS G is closed under pointwise multiplication (Assumption F), enabling eigenfunction algebra closure.
- Evidence anchors:
  - [Section 3.1] Equation (3.9) shows the decomposition; Corollary B.12 confirms inner products reduce to matrix-vector products.
  - [Appendix B.2] Proposition B.6 proves the decomposition under Assumption F (Sobolev spaces with s > d/2 satisfy this).
  - [Corpus] Weak direct evidence; related work on tensor trains and kernel methods exists but does not confirm this specific decomposition.
- Break condition: If the RKHS cannot form an algebra (Assumption F fails), the multiplicative closure used in the proof does not hold, and the decomposition is invalid.

### Mechanism 2
- Claim: MMD flows in time-dependent RKHS with flow matching achieve convergence at finite t=1 instead of asymptotically.
- Mechanism: By connecting time-dependent MMD flows to flow matching via the continuity equation, the vector field v_t matches a known gradient field u_t, ensuring p_t = q_t and p_1 ≈ π.
- Core assumption: The target vector field u_t must be a gradient field (curl-free); this holds for OT, VP, and VE flows (proven in Appendix B.4).
- Evidence anchors:
  - [Section 3.2] Equations (3.12)-(3.13) define the time-dependent flow; Propositions B.11-B.12 prove common flow matching fields are gradient fields.
  - [Figure 3] Synthetic experiment shows spectral mean flow converges at t=1 while standard MMD flows stagnate.
  - [Corpus] No direct corpus evidence on this specific extension; time-dependent kernel flows are discussed in Galashov et al. (2024) but without the flow matching connection.
- Break condition: If the flow matching vector field is not a gradient field, the equality v_t = u_t cannot hold, and the convergence guarantee fails.

### Mechanism 3
- Claim: Neural parameterization of spectral components enables end-to-end learning of the MMD flow.
- Mechanism: Complex-valued MLPs parameterize O_t b_i, S_t b_i, O_t w_{ij}, etc. (elements of H_t), with eigenvalues λ_i ∈ C and readout heads B, L, R, H. The flow matching objective trains the neural gradient field v_t(·; θ) to match u_t.
- Core assumption: Neural tangent kernel theory connects MLPs to Sobolev RKHS, justifying the parameterization (infinite-width limit norm-equivalence).
- Evidence anchors:
  - [Section 3.3] Equations (3.18)-(3.20) define the learning objective and parameterization; Appendix C.1 gives implementation details.
  - [Table 1] Competitive results across six datasets support the effectiveness of this parameterization.
  - [Corpus] Weak evidence; related operator-theoretic learning exists but does not validate this specific neural architecture.
- Break condition: If the MLP is non-differentiable (e.g., ReLU causes discontinuities), the gradient field v_t may be ill-defined, breaking the flow matching connection.

## Foundational Learning

- Concept: **Reproducing Kernel Hilbert Spaces (RKHS) and Mean Embeddings**
  - Why needed here: Core to representing distributions as vectors (μ_π ∈ H) and defining linear operators (CME operators U, O) for nonlinear dynamics.
  - Quick check question: Given a kernel k, can you explain why the mean embedding μ_π = E[φ(X)] is unique for a characteristic RKHS?

- Concept: **Maximum Mean Discrepancy (MMD) and Gradient Flows**
  - Why needed here: MMD provides a distance metric between distributions; gradient flows in this metric define the generative process.
  - Quick check question: Why does the MMD gradient flow v_t(x) = -∇_x⟨φ(x), μ_{p_t} - μ_π⟩_H move samples toward the target distribution?

- Concept: **Flow Matching and the Continuity Equation**
  - Why needed here: Bridges MMD flows to fast-sampling techniques by matching vector fields to known probability paths.
  - Quick check question: What condition on the vector field u_t ensures that matching it with v_t yields p_t = q_t?

## Architecture Onboarding

- Component map:
  - Shared feature extractor MLP(·, q, t) -> X × {o, s} × [0,1] -> C^{d_f} (complex-valued, differentiable with squared ReLU, RMSNorm)
  - Spectral parameters: λ -> C^r (eigenvalues), B, L, R, H -> C^{d_f × r} (readout heads with block-diagonal structure)
  - Tensor network contraction: Computes ⟨φ_t(x_1) ⊗ ... ⊗ φ_t(x_N), μ_{ρ,t}⟩ in O(Nr + r^2) space

- Critical path:
  1. Initialize spectral parameters using exponential parameterization (uniform on complex unit disk, r_min = ϵ, r_max = 1-ϵ)
  2. Forward pass: Compute h = MLP(x, q, t), then O_t b_i = [hB]_i, O_t w_{ij} = [L diag(h)R]_{ij}, O_t h_i = [hH]_i
  3. Contract tensor network (use opt_einsum) to compute inner products in Equation 3.17
  4. Compute vector field v_t via gradient of the inner product (double backpropagation)
  5. Loss: ||v_t(x_{1:N}; θ) - u_t(x_{1:N}^1)||^2 (conditional flow matching objective)

- Design tradeoffs:
  - Rank r: Higher r increases expressiveness but adds O(r^2) cost per sequence step; r = Ld_h/2 is a practical default
  - EVD vs. SVD: EVD captures long-range dependencies via oscillations (complex eigenvalues) but requires normal operator assumption; SVD is always valid but less expressive
  - MLP depth vs. width: Deeper MLPs improve approximation (NTK theory) but may destabilize training; L=10-16, d_h=64-96 works well

- Failure signatures:
  - Divergent training: Likely due to eigenvalue initialization near zero or unit circle boundary; check scaling (√2 for λ, √(2^{d-1}/r) for others)
  - Poor sampling quality: If MMD does not decrease by t=1, check if MLP uses non-differentiable activations (avoid standard ReLU)
  - Memory blow-up: If tensor contraction materializes intermediate r×r matrices, verify block-diagonal readout structure is implemented

- First 3 experiments:
  1. Tractability test (Figure 2): Evaluate inner product ⟨x_1 ⊗ ... ⊗ x_N, μ⟩ with and without tensor decomposition for N=2-8, d=32; verify exponential vs. linear memory scaling
  2. Convergence test (Figure 3): Run 2D checkerboard sampling with spectral mean flow vs. standard MMD flow; plot MMD over timesteps to confirm t=1 convergence
  3. Small-scale time-series: Train on Sines (10k sequences, N=24, 5 channels); compare context-FID and discriminative scores against baselines in Table 1 to validate full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can spectral mean flows be extended to condition generative processes on partial observations in a principled manner?
- Basis in paper: [explicit] Appendix D states the "current main limitation is the question of how to condition the generative process on partial observations" and suggests studying kernel Bayesian inference.
- Why unresolved: The current framework lacks a mechanism to compute the conditional mean embedding μ_{X_{n+1:N}|X_{1:n}} required for generation conditioned on history, which requires integrating kernel Bayes rules not yet present in the architecture.
- What evidence would resolve it: The successful integration of kernel Bayesian inference or Laplace transforms of CME operators into the spectral mean flow framework to handle partial observations without ad-hoc heuristics.

### Open Question 2
- Question: Can continuous-time conditional mean embedding (CME) operators effectively replace the discrete-time formulation for modeling irregularly sampled systems?
- Basis in paper: [explicit] Appendix D proposes exploring "continuous-time CME operators" and their spectral decomposition U = ∑ exp(s_i Δτ) h_i ⊗ g_i as a potentially valid approach for irregular sampling.
- Why unresolved: The current paper handles irregular time series using discrete-time operators with interval embeddings or interpolation, leaving the theoretical connection to continuous-time operator theory unexplored.
- What evidence would resolve it: A modification of the spectral decomposition theory to support continuous eigenvalues dependent on time intervals Δτ, demonstrating improved performance on irregularly sampled datasets.

### Open Question 3
- Question: How can the learned spectral components be directly identified to improve the interpretability of physical systems?
- Basis in paper: [inferred] Section 5.2 notes that "interpreting eigenfunctions is not direct due to our end-to-end design," limiting the ability to identify physical modes without explicit regularization.
- Why unresolved: While the architecture is grounded in operator theory, the neural parameterization obscures the physical meaning of the eigenfunctions, making it difficult to verify if the model captures true system dynamics.
- What evidence would resolve it: A method that successfully maps the learned neural parameters back to interpretable eigenfunctions that align with known physical modes of the systems being modeled.

## Limitations

- The theoretical foundation relies heavily on Assumption F (RKHS algebra closure), which may not hold for many practical kernels like Gaussian or Matérn
- The neural parameterization assumes MLP-to-Sobolev-RKHS norm-equivalence through NTK theory, but this infinite-width limit connection is not empirically verified
- The flow matching connection requires gradient fields, proven for OT, VP, and VE flows, but not systematically checked for all possible vector fields or practical datasets

## Confidence

- **High confidence**: The tensor network decomposition mechanism and its computational efficiency gains are well-supported by theoretical proofs and synthetic experiments
- **Medium confidence**: The flow matching convergence at t=1 is demonstrated on synthetic data but not comprehensively validated across all datasets and flow types
- **Medium confidence**: The neural parameterization and overall learning pipeline show competitive empirical results but rely on unproven NTK connections and lack ablation studies on architectural choices

## Next Checks

1. **Assumption F validation**: Test the tensor network decomposition with common kernels (Gaussian, Matérn) on synthetic data to verify if the RKHS algebra closure holds in practice
2. **Flow matching robustness**: Run the 2D checkerboard experiment with non-gradient vector fields (e.g., rotational flows) to confirm that spectral mean flows fail gracefully when the gradient field assumption is violated
3. **Architectural ablation**: Systematically vary the rank r and compare against CP/Tucker decompositions on a small time-series dataset to quantify the practical efficiency gains of the proposed tensor network approach