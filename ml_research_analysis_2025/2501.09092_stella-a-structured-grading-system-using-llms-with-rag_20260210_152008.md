---
ver: rpa2
title: 'SteLLA: A Structured Grading System Using LLMs with RAG'
arxiv_id: '2501.09092'
source_url: https://arxiv.org/abs/2501.09092
tags:
- student
- answer
- grading
- molecule
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SteLLA, a system for automated short-answer
  grading using large language models (LLMs) enhanced with Retrieval-Augmented Generation
  (RAG). SteLLA extracts structured evaluation questions from instructor-provided
  reference answers and rubrics, then uses an LLM to assess student responses against
  these questions, providing both overall grades and detailed feedback.
---

# SteLLA: A Structured Grading System Using LLMs with RAG

## Quick Facts
- arXiv ID: 2501.09092
- Source URL: https://arxiv.org/abs/2501.09092
- Reference count: 40
- Primary result: SteLLA achieves Cohen's Kappa of 0.6720 agreement with human graders on biology short-answer grading

## Executive Summary
SteLLA introduces a structured grading system that combines Retrieval-Augmented Generation (RAG) with large language models (LLMs) for automated short-answer grading. The system extracts structured evaluation questions from instructor-provided reference answers and rubrics, then uses an LLM to assess student responses against these questions, providing both overall grades and detailed feedback. Experiments on a real-world biology exam dataset demonstrate substantial agreement with human graders while delivering valuable breakdown grades and feedback.

## Method Summary
SteLLA employs a three-module architecture: (1) an R-RAG module that generates evaluation questions and gold answers from reference material, (2) an LLM evaluation module that performs QA-based grading using GPT-4 with 4-shot learning and clustering-based example selection, and (3) a scoring module that aggregates per-question scores into final grades. The system uses clustering (KMeans) to select representative student responses as few-shot examples, and grounds evaluation in instructor-provided reference answers rather than external knowledge bases to reduce hallucination.

## Key Results
- Achieves Cohen's Kappa of 0.6720 agreement with human graders
- Raw Agreement (Accuracy) of 0.8358 on 175 student responses
- Clustering-based few-shot selection improves Kappa by approximately 0.2 compared to random selection
- GPT-4 effectively identifies relevant facts but may over-infer implications from student responses

## Why This Works (Mechanism)

### Mechanism 1: QA-Based Semantic Understanding
Decomposing rubric points into explicit Evaluation Questions forces the LLM to assess specific semantic constraints rather than relying on surface-level text similarity. The system shifts from "compare text A to text B" to "does text B contain the answer to question A," inducing deeper semantic understanding.

### Mechanism 2: Instructor-Provided Grounding
R-RAG uses instructor-provided reference answers and rubrics as the retrieval corpus, constraining the LLM context to domain-specific facts rather than external knowledge bases. This grounding reduces hallucination frequency by anchoring evaluation in specific exam logic.

### Mechanism 3: Representative Few-Shot Selection
Clustering student responses and selecting cluster centroids as few-shot examples ensures the prompt examples represent the true distribution of student responses. This prevents outlier responses from dominating the prompt and provides a representative baseline for the LLM.

## Foundational Learning

### Concept: Question-Answering (QA) based Evaluation
**Why needed here:** SteLLA treats grading as a QA task where the system asks "Did the student answer Question X?" rather than classifying text similarity.
**Quick check question:** How does framing grading as a QA task differ from framing it as a semantic similarity task?

### Concept: Cohen's Kappa
**Why needed here:** The paper evaluates success using inter-rater reliability (Kappa) rather than just accuracy, because grading is subjective and class imbalances exist.
**Quick check question:** Why is Raw Agreement (Accuracy) insufficient for evaluating a grading system?

### Concept: In-context Learning (Few-Shot)
**Why needed here:** The system relies on providing the LLM with 4-6 examples of graded student work to "teach" it the grading style without fine-tuning.
**Quick check question:** What is the trade-off between increasing the number of shots and the context window limit?

## Architecture Onboarding

### Component map:
Reference Answer + Rubric -> R-RAG Module -> Evaluation Questions + Gold Answers -> LLM Evaluation Module -> Binary Scores + Justifications -> Scoring Module -> Final Grade + Feedback

### Critical path:
The generation of Evaluation Questions (EQs) is the most sensitive step; if the questions are misaligned with the rubric, the downstream LLM grading will be technically correct but pedagogically wrong.

### Design tradeoffs:
- **Control vs. Flexibility:** Binary grading per rubric point (0/1) rather than partial credit, simplifying logic but potentially missing nuance
- **Manual vs. Auto Question Gen:** While the architecture supports automated question generation, experiments used manual/instructor-reviewed questions to ensure quality

### Failure signatures:
- **Over-inference:** GPT-4 tends to infer "too much implication" from vague student text, assuming concepts not explicitly stated
- **Strict Literalism:** LLM may mark answers wrong if students explain concepts correctly but apply them to wrong molecules or contexts

### First 3 experiments:
1. **Validation of EQs:** Manually check if generated Evaluation Questions accurately map to rubric points before running full system
2. **Ablation on Shot Selection:** Compare Random Selection vs. Clustering Selection to verify the ~0.2 Kappa improvement
3. **Over-inference Test:** Run grading on "vague" student answers to measure false positives (GPT-4 assuming implications not in text)

## Open Questions the Paper Calls Out

### Open Question 1: Rubric-Free Question Generation
Can SteLLA generate reliable evaluation question-answer pairs when only a reference answer is available without instructor-provided rubrics? This addresses scalability when explicit rubrics are unavailable in real-world grading scenarios.

### Open Question 2: Mitigating LLM Over-Inference
How can LLM over-inference in grading tasks be systematically mitigated beyond prompt engineering? The paper identifies that prompt constraints alone fail to prevent LLMs from applying general knowledge that contradicts specific domain expectations.

### Open Question 3: Cross-Domain Generalizability
How does SteLLA perform across different academic domains, question complexity levels, and course levels beyond introductory biology? The current study's focus on a single biology exam limits understanding of broader applicability.

## Limitations

- The system relies on manually-reviewed evaluation questions rather than fully automated generation, creating scalability concerns
- Binary grading approach (0/1 per rubric point) may oversimplify nuanced student responses that partially address criteria
- Focus on a single introductory biology exam question limits generalizability to other subjects or question types
- Kappa score of 0.6720, while substantial, still indicates notable disagreement with human graders

## Confidence

- **High Confidence:** QA-based grading mechanism fundamentally shifts evaluation from similarity matching to semantic understanding
- **Medium Confidence:** Clustering-based few-shot selection improves kappa scores by ~0.2, though exact parameters remain unclear
- **Medium Confidence:** GPT-4's tendency to over-infer implications is well-documented but needs more systematic quantification

## Next Checks

1. **Question Generation Validation:** Implement automated evaluation question generation from rubric points and measure alignment quality against instructor-reviewed questions to assess scalability limitations.

2. **Cross-Domain Testing:** Apply SteLLA to short-answer questions from different subjects (e.g., humanities, social sciences) to evaluate generalizability beyond the biology domain.

3. **Partial Credit Extension:** Modify the scoring module to support partial credit per rubric point and compare agreement metrics against the binary system to assess whether nuanced grading improves human grader alignment.