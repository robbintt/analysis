---
ver: rpa2
title: Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?
arxiv_id: '2510.25064'
source_url: https://arxiv.org/abs/2510.25064
tags:
- items
- evidence
- difficulty
- item
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) can
  estimate the cognitive complexity of reading comprehension items by analyzing two
  dimensions: Evidence Scope (how much text is needed to answer) and Transformation
  Level (how much linguistic transformation is required). The authors created a benchmark
  dataset (RECO) of 776 items annotated along these dimensions and evaluated eight
  instruction-tuned LLMs on two classification tasks.'
---

# Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?

## Quick Facts
- **arXiv ID:** 2510.25064
- **Source URL:** https://arxiv.org/abs/2510.25064
- **Authors:** Seonjeong Hwang; Hyounghun Kim; Gary Geunbae Lee
- **Reference count:** 24
- **Primary result:** LLMs can estimate cognitive complexity of reading comprehension items with F1 scores up to 75.5 for Evidence Scope and 83.2 for Transformation Level.

## Executive Summary
This paper investigates whether large language models can estimate the cognitive complexity of reading comprehension items by analyzing two dimensions: Evidence Scope (how much text is needed to answer) and Transformation Level (how much linguistic transformation is required). The authors created a benchmark dataset (RECO) of 776 items annotated along these dimensions and evaluated eight instruction-tuned LLMs on two classification tasks. Results show that LLMs can approximate cognitive complexity with F1 scores up to 75.5 for Evidence Scope and 83.2 for Transformation Level, with open-source models like Qwen2.5-32B and Mistral-24B performing comparably to GPT-4o. However, LLMs often struggle to explicitly recognize the cognitive features underlying their own reasoning, revealing a gap between problem-solving ability and metacognitive awareness. The findings suggest LLMs have strong potential for estimating cognitive complexity in educational settings, though further work is needed to improve alignment with human judgments.

## Method Summary
The authors created the RECO benchmark dataset of 776 reading comprehension items annotated by experts along two cognitive dimensions: Evidence Scope (single/multi-sentence or insufficient) and Transformation Level (5-level taxonomy for single-sentence items, simplified to 3-level for multi-sentence). They evaluated eight instruction-tuned LLMs (Gemma2, Mistral, Qwen2.5, GPT-4o variants) using zero-shot, one-shot, and few-shot Chain-of-Thought prompting with greedy and self-consistency decoding. Performance was measured by micro-averaged F1 scores against human expert annotations, with human baselines of 87.9 for Evidence Scope and 85.9 for 3-level Transformation Level.

## Key Results
- GPT-4o achieved the highest F1 score of 75.5 for Evidence Scope classification with one-shot CoT prompting
- Mistral-24B (83.2) and Qwen2.5-32B (78.3) outperformed GPT-4o (78.1) in Transformation Level classification
- Larger open-source models (24B-32B parameters) performed comparably to GPT-4o on both tasks
- LLMs often struggle to explicitly recognize the cognitive features underlying their own reasoning

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Complexity as a Supervised Classification Task
- Claim: Instruction-tuned LLMs can approximate human expert judgments on cognitive complexity by framing the estimation as a text classification task over two defined dimensions: Evidence Scope (ES) and Transformation Level (TL).
- Mechanism: The system provides an LLM with a prompt containing the task definition, a reading passage, a statement, and its factuality label. The model is tasked to output one of several discrete labels (e.g., for Evidence Scope: `single`, `multi`, or `insufficient`). Performance is evaluated by computing the micro-averaged F1 score against the ground-truth labels in the RECO benchmark dataset.
- Core assumption: The LLM's pre-trained internal representations and reasoning capabilities are sufficiently aligned with the linguistically and cognitively defined categories that they can be elicited via prompt-based instructions without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "LLMs can approximate the cognitive complexity of items, indicating their potential as tools for prior difficulty analysis."
  - [section 5 Results] "GPT-4o achieves the highest F1 score (75.5) with a one-shot CoT prompt... In the TL classification task with the 3-level taxonomy, Mistral-24B (83.2) and Qwen2.5-32B (78.3) outperform GPT-4o (78.1)."
  - [corpus] Corpus evidence is weak for this specific mechanism. Related papers like "Exploring the Potential of LLMs for Estimating Reading Comprehension Question Difficulty" focus on direct difficulty prediction, not on classifying intermediate cognitive features.
- Break condition: Performance degrades if the cognitive categories are not clearly defined in the prompt or if the model lacks sufficient capacity (e.g., Mistral-7B shows unstable, lower performance).

### Mechanism 2: Reasoning Elicitation via CoT and Self-Consistency
- Claim: Advanced prompting strategies like Chain-of-Thought (CoT) and self-consistency decoding improve classification accuracy for cognitively complex items by encouraging explicit reasoning steps.
- Mechanism: CoT prompting instructs the model to generate intermediate reasoning steps before the final label. Self-consistency further improves this by sampling multiple reasoning paths (with non-zero temperature) and selecting the most frequent final answer, smoothing out stochastic errors.
- Core assumption: The task benefits from "thinking aloud," and the correct answer is more likely to be reached via consistent reasoning paths than a single direct generation.
- Evidence anchors:
  - [section 4 Experimental Setup] "Chain-of-Thought Prompting (CoT), which encourages step-by-step reasoning before prediction... In the CoT setting, we additionally apply self-consistency decoding."
  - [section 5 Results] "Self-consistency decoding improves results in several settings, particularly zero-shot CoT, but its gains are less stable than in ES classification."
  - [corpus] No direct corpus evidence was found for this specific combination of techniques applied to cognitive complexity estimation.
- Break condition: The benefit of more complex prompting is inconsistent. In larger models, few-shot prompting with CoT can degrade performance, potentially due to attention dispersion over longer contexts.

### Mechanism 3: The Gap Between Performance and Metacognition
- Claim: An LLM can solve a reading comprehension problem correctly while failing to explicitly identify the cognitive features (e.g., specific evidence sentences, phrase reordering) that underpin its own solution.
- Mechanism: This gap is revealed by decomposing the main classification task into fine-grained sub-tasks (e.g., evidence sentence counting, phrase reordering detection). While models achieve high F1 on the overall RC task, their performance on these introspective sub-tasks is significantly lower, indicating a lack of explicit metacognitive awareness.
- Core assumption: Accurate task completion does not guarantee that a model has an inspectable, human-aligned representation of the process it used.
- Evidence anchors:
  - [abstract] "LLMs often struggle to explicitly recognize the cognitive features underlying their own reasoning, revealing a gap between problem-solving ability and metacognitive awareness."
  - [section 6.1] "LLMs showed consistent performance on falsifiability classification (Sub-task 1.1) and paraphrase detection (Sub-task 2.2), but struggled with evidence sentence counting (Sub-task 1.2) and phrase reordering detection (Sub-task 2.3)."
  - [section 6.3 Case Study] "The case reveals a metacognitive failure: the model arrives at the correct factual judgment but fails to recognize the reasoning behavior it engaged in to reach that conclusion."
  - [corpus] The paper "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities" aligns with this concept of LLM blind spots in cognitive assessment.
- Break condition: This is not a mechanism that "breaks" but a persistent limitation. It suggests that LLM predictions for cognitive complexity, while useful, require external validation and cannot be fully trusted for self-explanation.

## Foundational Learning

- **Concept:** Evidence Scope (ES)
  - **Why needed here:** This is one of the two foundational cognitive dimensions in the paper's framework. It operationalizes cognitive burden based on the amount of text a learner must process.
  - **Quick check question:** Why would an item requiring "multi-sentence evidence" be considered more cognitively complex than one requiring "single-sentence evidence"?

- **Concept:** Transformation Level (TL)
  - **Why needed here:** This is the second core dimension. It captures the cognitive demand of mapping the question to the text, independent of the amount of text.
  - **Quick check question:** In the 5-level taxonomy, how does "Transformed Paraphrasing" differ from "Inference," and which imposes a higher cognitive load?

- **Concept:** Instruction Tuning
  - **Why needed here:** The paper's entire methodology relies on using *instruction-tuned* LLMs. Understanding this is key to knowing why these models can follow the complex prompts at all.
  - **Quick check question:** How does the process of instruction tuning differentiate a base LLM from one capable of performing the cognitive classification tasks described in this paper?

## Architecture Onboarding

- **Component map:** RECO Benchmark -> Prompt Template -> LLM Engine -> Decoding Strategy -> Evaluation Function
- **Critical path:** Data Curation & Annotation (most resource-intensive) -> Prompt Engineering (primary lever) -> Inference & Evaluation (final validation)
- **Design tradeoffs:**
  - Prompt Complexity vs. Stability: While Chain-of-Thought prompting can improve scores, it introduces variability. Simpler prompts are more stable but may have lower peak performance.
  - Model Size vs. Cost/Accessibility: Larger open-source models (e.g., Qwen2.5-32B) perform comparably to proprietary ones (GPT-4o) but require significant GPU resources. Smaller models are cheaper but underperform.
  - Task Specificity vs. Generalization: The framework is tailored to "detail" questions (TFNG). Generalizing to other RC types (e.g., "main idea") would require redefining the cognitive dimensions.
- **Failure signatures:**
  - Low Recall on Evidence Sentences: The model's self-reported evidence often omits sentences that humans consider crucial, indicating a "skimming" behavior.
  - Phrase Reordering Blindness: A consistent failure mode is misclassifying items with re-ordered phrases (e.g., labeling "Transformed Word Matching" as "Word Matching").
  - Context Overload: In few-shot settings with large models, performance can degrade, suggesting the model is "distracted" by the long context.
- **First 3 experiments:**
  1. Baseline Zero-Shot Evaluation: Establish a performance baseline by running zero-shot standard prompts for ES and TL classification across several model sizes (e.g., 7B, 24B, 32B).
  2. Prompting Ablation Study: Compare Standard Prompting vs. Chain-of-Thought vs. CoT with Self-Consistency, measuring the F1 score gain and computational cost for each strategy.
  3. Fine-Grained Error Analysis: Run the best-performing model on the sub-tasks defined in the paper (evidence counting, phrase reordering detection) to diagnose specific weaknesses rather than just overall accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can data augmentation and knowledge distillation effectively transfer cognitive complexity estimation capabilities from large (24B-32B) models to smaller, resource-efficient models?
  - **Basis in paper:** [explicit] The Limitations section highlights "opportunities for future work on data augmentation and knowledge distillation to build smaller yet effective models" because larger models currently outperform smaller ones.
  - **Why unresolved:** The current dataset scale (776 items) and budget constraints prevented supervised fine-tuning, leaving the potential for smaller models to learn these cognitive features unexplored.
  - **What evidence would resolve it:** A smaller model (e.g., 7B parameters) fine-tuned using synthetic data or distillation from a larger teacher model achieving F1 scores comparable to GPT-4o on the RECO benchmark.

- **Open Question 2:** Can LLMs estimate cognitive complexity for reading comprehension item types beyond factual detail questions, such as those targeting main ideas or author intent?
  - **Basis in paper:** [explicit] The authors state in the Limitations that their current dimensions do not generalize to all item types and call for "future research on additional factors across a broader set of RC item types."
  - **Why unresolved:** The study focused exclusively on True/False/Not Given items (factual details), as different item types involve distinct cognitive complexity factors not captured by the current Evidence Scope and Transformation Level dimensions.
  - **What evidence would resolve it:** The development of a new taxonomy and benchmark dataset for non-factual items where LLMs demonstrate significant correlation with human expert judgments.

- **Open Question 3:** Can cognitive complexity estimation frameworks be leveraged for difficulty-controlled item generation?
  - **Basis in paper:** [explicit] The Conclusion expresses the hope that findings will "encourage further research in... difficulty-controlled item generation."
  - **Why unresolved:** The paper focuses on the classification (estimation) of existing items rather than the generative task of creating items that meet specific cognitive criteria.
  - **What evidence would resolve it:** A generative system that produces reading comprehension items where the assigned cognitive complexity label matches the ground truth label assigned by human annotators or the classifier with high reliability.

## Limitations
- The framework is limited to factual detail questions and may not generalize to other RC item types like main idea or author intent questions
- The dataset size (776 items) is relatively small, limiting the potential for supervised fine-tuning of smaller models
- LLMs struggle to explicitly recognize the cognitive features underlying their own reasoning, revealing a gap between problem-solving ability and metacognitive awareness

## Confidence

| Claim | Confidence |
|---|---|
| LLMs can approximate cognitive complexity with reasonable accuracy | High |
| Open-source models perform comparably to GPT-4o | High |
| LLMs struggle with explicit metacognitive awareness | High |
| The framework generalizes to all RC item types | Low |
| Small models can match large models without fine-tuning | Low |

## Next Checks

1. **Reproduce main results**: Clone the repository and run the one-shot CoT experiments with Qwen2.5-32B-Instruct or Mistral-Small-24B-Instruct to verify F1 scores of ~73 for ES and ~78-83 for TL

2. **Verify failure modes**: Analyze the model's evidence sentence counting and phrase reordering detection performance to confirm systematic weaknesses in the 5-level TL task

3. **Test prompting sensitivity**: Run an ablation study comparing Standard Prompting vs. CoT vs. CoT with Self-Consistency to quantify performance gains and computational costs for each strategy