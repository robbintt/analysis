---
ver: rpa2
title: 'Bag of Coins: A Statistical Probe into Neural Confidence Structures'
arxiv_id: '2507.19774'
source_url: https://arxiv.org/abs/2507.19774
tags:
- calibration
- confidence
- detection
- score
- logit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Bag-of-Coins (BoC) probe, a non-parametric
  diagnostic tool that examines the internal coherence of neural network predictions
  by comparing softmax confidence to a geometry-derived coherence quantity. The probe
  is grounded in random utility theory and uses a statistical test to assess whether
  the top logit's dominance over random competitors aligns with the model's softmax
  confidence.
---

# Bag of Coins: A Statistical Probe into Neural Confidence Structures

## Quick Facts
- **arXiv ID**: 2507.19774
- **Source URL**: https://arxiv.org/abs/2507.19774
- **Reference count**: 16
- **Primary result**: BoC probe reveals architectural differences in uncertainty encoding (ViT shows strong ID/OOD separation in coherence gaps; ResNet/RoBERTa show overlap) but fails as practical calibrator/OOD detector

## Executive Summary
This paper introduces the Bag-of-Coins (BoC) probe, a non-parametric diagnostic tool that examines internal coherence of neural network predictions by comparing softmax confidence to a geometry-derived coherence quantity. The probe is grounded in random utility theory and uses a statistical test to assess whether the top logit's dominance over random competitors aligns with the model's softmax confidence. The study evaluates BoC across three architectures—ViT, ResNet, and RoBERTa—using both in-distribution and out-of-distribution datasets, finding strong architectural differences in how uncertainty is represented in logit space.

## Method Summary
BoC compares softmax confidence p̂ to an aggregate of pairwise Luce-style dominance probabilities q̄, where q̄ = (1/(C-1)) Σ π_ŷ≻j for predicted class ŷ. The coherence gap Δ = q̄ - p̂ reveals structural inconsistencies in logit geometry. For statistical testing, a binomial-tail p-value is computed from expected wins W* = round(kq̄) using Hoeffding's result that Binomial(k, p̂) maximizes the upper tail among all Poisson-binomial sums with mean kp̂. The method is deterministic, requires only logits, and produces both diagnostic (Δ) and scoring outputs (c_BoC = q̄ for calibration, s_BoC = 1 - p*_val for OOD detection).

## Key Results
- ViT exhibits strong ID/OOD separation in coherence gaps (ID ~0.1-0.2, OOD ~0.5-0.6) while ResNet and RoBERTa show substantial overlap near Δ≈0
- BoC improves calibration over uncalibrated models when base model is poorly calibrated (ViT: ECE 0.024 vs 0.180) but underperforms standard calibrators (ECE ~0.005)
- For OOD detection, BoC fails catastrophically across all architectures (AUROC 0.020–0.253) compared to standard methods (0.75–0.99)
- BoC is positioned as a research diagnostic tool revealing how architectures encode uncertainty, not as a production-ready method

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Luce-style Dominance Aggregation
- Claim: Aggregating pairwise softmax comparisons between the top logit and competitors yields a geometry-derived coherence quantity that probes internal logit structure
- Mechanism: For predicted class ŷ with logit z_ŷ, compute pairwise Luce probability π_ŷ≻j = exp(z_ŷ)/(exp(z_ŷ) + exp(z_j)) for each competitor j. Average these to get q̄ = (1/(C-1)) Σ π_ŷ≻j
- Core assumption: Logit geometry encodes meaningful uncertainty structure beyond what softmax confidence alone captures
- Evidence anchors: Abstract description of coherence gap, Section 6.1 connection to Gumbel-Softmax, weak corpus support for logit geometry probes
- Break condition: If all logits are nearly equal (uniform predictions), q̄ ≈ 0.5 regardless of class count, potentially losing discriminative signal

### Mechanism 2: Coherence Gap as Architectural Diagnostic
- Claim: The coherence gap Δ = q̄ - p̂ reveals architecture-dependent differences in how models encode uncertainty, with ViT showing strong ID/OOD separation while CNN/Transformer classifiers show overlap
- Mechanism: When Δ is large, the top logit dominates pairwise comparisons more strongly than softmax confidence suggests—a "confident delusion" state. The paper finds ViT's Δ shifts from ~0.1-0.2 (ID) to ~0.5-0.6 (OOD)
- Core assumption: Coherent confidence representation requires q̄ ≈ p̂; deviations indicate structural inconsistency
- Evidence anchors: Abstract showing ViT separation vs ResNet/RoBERTa overlap, Section 7.1 on structural differences, corpus neighbor on ViT NAS for OOD generalization
- Break condition: For well-calibrated models where softmax already reflects true uncertainty (ResNet/RoBERTa with ECE ~0.03-0.04), Δ provides little discriminative signal

### Mechanism 3: Binomial-tail Hypothesis Testing for Structural Scoring
- Claim: A binomial-tail p-value computed from expected wins W* = round(kq̄) provides a valid (super-uniform) test statistic under the null hypothesis H₀: q̄ = p̂
- Mechanism: The actual win count W from k trials is Poisson-binomial (different success probabilities per trial). Hoeffding's result proves Binomial(k, p̂) maximizes the upper tail among all Poisson-binomials with mean kp̂, ensuring p_val = Pr{Binomial(k, p̂) ≥ W} is super-uniform under H₀
- Core assumption: The null hypothesis q̄ = p̂ represents ideal coherence; observing higher wins than expected indicates structural inconsistency
- Evidence anchors: Section 6.2 on Hoeffding's result, Proposition 6.2 proving p_val validity, no direct corpus support for this specific construction
- Break condition: The derived OOD score s_BoC = 1 - p*_val catastrophically inverts rankings (AUROC 0.020-0.253), suggesting the p-value convention doesn't translate to useful ID/OOD scoring despite theoretical validity

## Foundational Learning

- Concept: Random Utility Theory and Gumbel-Softmax Connection
  - Why needed here: The entire BoC framework rests on interpreting softmax as the probability of maximum perturbed utility under Gumbel noise, which justifies pairwise Luce probabilities as the right primitive
  - Quick check question: Given two logits z_A = 2.0 and z_B = 1.0, compute both the softmax probability for class A (with C=2) and the Luce pairwise probability π_A≻B. Are they equal?

- Concept: Poisson-Binomial vs. Binomial Distributions
  - Why needed here: Understanding why W is Poisson-binomial (trials have different success probabilities π_ŷ≻j) but why the binomial tail still provides a valid upper bound is essential for the statistical validity proof
  - Quick check question: If you flip 3 coins with different biases (p₁=0.3, p₂=0.5, p₃=0.7), is the total number of heads Binomial(3, 0.5)? Why or why not?

- Concept: Expected Calibration Error (ECE) and Reliability Diagrams
  - Why needed here: The paper evaluates BoC primarily through ECE, and understanding binning-based calibration metrics is necessary to interpret why BoC improves ViT (ECE 0.024 vs 0.180) but degrades ResNet (0.069 vs 0.039)
  - Quick check question: A model has 100 predictions with confidence 0.8, of which 75 are correct. What is the calibration error in this bin?

## Architecture Onboarding

- Component map:
  Input -> Logit extraction -> Softmax computation -> Pairwise Luce module -> Aggregator -> Coherence gap -> P-value module -> Output scores

- Critical path:
  1. Extract logits from model (not softmax probabilities—logits are required)
  2. Identify top class ŷ and compute softmax confidence p̂
  3. Compute pairwise Luce probabilities (vectorized: exp(z_ŷ) / (exp(z_ŷ) + exp(z)))
  4. Average excluding self to get q̄
  5. Use Δ for diagnostics; use q̄ as calibration score if base model is poorly calibrated

- Design tradeoffs:
  - Deterministic vs Monte-Carlo: Deterministic q̄ is stable; Monte-Carlo with k trials introduces noise but converges to same value. Paper uses deterministic for all main results
  - Trial count k: Affects p-value granularity but not q̄. Paper uses k=100; ablations show k doesn't fix OOD inversion
  - Use case positioning: BoC is a diagnostic probe, not a production calibrator. Standard methods (temperature scaling, isotonic regression) outperform for calibration; Energy/MSP outperform for OOD detection

- Failure signatures:
  - OOD score inversion: s_BoC = 1 - p*_val yields AUROC < 0.5 across all architectures (0.020-0.253), indicating OOD samples receive higher "ID" scores than actual ID samples
  - Calibration degradation on well-calibrated models: When base ECE is already low (~0.03-0.04 for ResNet/RoBERTa), BoC increases ECE
  - No ID/OOD separation in ResNet/RoBERTa: Δ distributions heavily overlap near zero, unlike ViT's clear shift

- First 3 experiments:
  1. Implement deterministic BoC on a validation set: Compute q̄ for each sample and compare Δ distributions across ID vs known OOD. If no separation exists (like ResNet), BoC won't help for that architecture
  2. Calibration baseline comparison: Measure ECE before/after using c_BoC = q̄ as confidence. Only expect improvement if base ECE > 0.1 (as with ViT)
  3. Visualize confidence-coherence relationship: Plot mean Δ vs. p̂ on ID data. Paper shows consistent negative relationship across architectures—high confidence correlates with low Δ. Verify this pattern holds for your model before trusting Δ as a diagnostic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does ViT exhibit strong ID/OOD separation in coherence gaps while ResNet and RoBERTa show substantial overlap?
- Basis in paper: The authors note that ViT shows clear separation (ID ∼0.1–0.2, OOD ∼0.5–0.6) while ResNet and RoBERTa both cluster near ∆≈0, "indicating architecture-dependent uncertainty geometry"
- Why unresolved: The paper establishes the empirical phenomenon but does not investigate mechanistic causes—whether this stems from attention mechanisms, training procedures, or representational properties
- What evidence would resolve it: Systematic ablation across attention-based vs. convolutional architectures, controlled training comparisons, and analysis of intermediate layer logit geometries

### Open Question 2
- Question: Can alternative scoring functions leverage the coherence gap for reliable OOD detection?
- Basis in paper: The conclusion states: "Improving BoC as an OOD detector likely requires alternative scoring that leverages ∆ (or related geometry) more directly than the current binomial-tail p-value rule"
- Why unresolved: The binomial-tail p-value scoring catastrophically fails (AUROC 0.020–0.253) despite clear distributional separation in ∆ for ViT, suggesting the scoring transform discards discriminative information
- What evidence would resolve it: Developing and benchmarking scoring functions that directly use ∆ magnitude or related geometric summaries, rather than hypothesis-testing p-values

### Open Question 3
- Question: How would a Bag-of-Dice generalization (multinomial outcomes) extend coherence diagnostics?
- Basis in paper: The conclusion proposes: "generalizing BoC from 'coins' to 'dice' by replacing Bernoulli trials with categorical/multinomial outcomes, yielding a Bag-of-Dice (BoD) diagnostic"
- Why unresolved: The current binary framework pits the top logit against one competitor at a time, potentially missing higher-order interactions among multiple classes simultaneously
- What evidence would resolve it: Formal derivation of multinomial coherence tests and empirical comparison with BoC on the same benchmarks

## Limitations
- OOD detection fails catastrophically: BoC score s_BoC = 1 - p*_val systematically inverts ID/OOD rankings (AUROC 0.020-0.253) across all architectures
- Calibration only helps poorly calibrated models: For well-calibrated architectures (ResNet/RoBERTa with ECE ~0.03-0.04), BoC degrades performance
- Diagnostic power depends on architecture: Coherence gap Δ only shows strong ID/OOD separation for ViT, not for ResNet/RoBERTa

## Confidence
- **High Confidence**: The coherence gap mechanism and its mathematical formulation (Δ = q̄ - p̂) are well-defined and reproducible
- **Medium Confidence**: The statistical validity of the p-value construction is proven, but the practical utility of s_BoC is catastrophically negative
- **Low Confidence**: The claim that BoC reveals "fundamental" architectural differences lacks mechanistic explanation for why ViT behaves differently

## Next Checks
1. Validate OOD score inversion: Implement BoC on a simple synthetic dataset where you control the logit distributions for ID vs OOD samples. Verify that s_BoC = 1 - p*_val indeed ranks OOD samples higher than ID samples, confirming the systematic inversion observed in the paper.

2. Test coherence gap on well-calibrated models: Apply BoC to architectures known to be well-calibrated (e.g., properly temperature-scaled ResNets). Measure whether Δ distributions show the same ID/OOD separation as ViT or if the complete overlap observed in the paper persists.

3. Examine pairwise dominance patterns: For ViT specifically, analyze the distribution of individual π_ŷ≻j values (not just their average q̄) for ID vs OOD samples. Determine whether the strong separation comes from changes in the mean or from distributional shifts in the dominance probabilities themselves.