---
ver: rpa2
title: Neural Induction of Finite-State Transducers
arxiv_id: '2601.10918'
source_url: https://arxiv.org/abs/2601.10918
tags:
- state
- input
- transducers
- sink
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for inducing finite-state transducers
  (FSTs) from data using recurrent neural networks (RNNs) as intermediate models.
  The approach addresses the challenge of constructing accurate FSTs automatically
  by leveraging the theoretical correspondence between RNN hidden state dynamics and
  finite-state automata.
---

# Neural Induction of Finite-State Transducers

## Quick Facts
- arXiv ID: 2601.10918
- Source URL: https://arxiv.org/abs/2601.10918
- Reference count: 40
- Primary result: Neural induction method produces FSTs with up to 87% accuracy improvement over OSTIA on morphological tasks

## Executive Summary
This paper presents a novel method for automatically constructing finite-state transducers (FSTs) from string transduction data using recurrent neural networks. The approach leverages the theoretical correspondence between RNN hidden state dynamics and finite-state automata, training RNNs with a transduction objective and spectral norm penalty. By clustering hidden states and applying a state-splitting algorithm, the method extracts unweighted FSTs that achieve substantial accuracy improvements over classical algorithms like OSTIA, particularly on morphological inflection tasks. The induced FSTs also show competitive performance to expert-crafted transducers in many cases.

## Method Summary
The method involves training a single-layer Elman RNN with a transduction objective (predicting output symbols from hidden states) and spectral norm penalty (λ=0.1) to encourage finite-state-like dynamics. Hidden states are collected from both training and synthetic strings, clustered using k-means, and transitions are aggregated. A state-splitting algorithm with SVM or logistic regression resolves non-determinism. The pipeline includes CRPAlign for alignment, synthetic data generation via n-gram models or tag permutations, and FST minimization using Hopcroft's algorithm. Hyperparameters are tuned via Bayesian optimization with 50 runs.

## Key Results
- 87% accuracy improvement over OSTIA on average across inflection tasks
- FSTs achieve competitive performance to expert-crafted transducers in many cases
- Ablation studies show synthetic data generation improves accuracy by up to 8 points
- Method struggles with right-context dependencies in G2P and normalization tasks due to unidirectional RNN design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNN hidden states cluster into discrete regions that correspond to FST states
- Mechanism: After training with transduction objective, hidden state vectors form distinguishable clusters in continuous space. K-means clustering partitions these into discrete states. Transitions between clusters become FST transitions when aggregated across input symbols.
- Core assumption: RNN organizes hidden state space such that states with similar outgoing transitions occupy nearby regions
- Evidence anchors:
  - [abstract] "automatically constructing unweighted FSTs following the hidden state geometry learned by a recurrent neural network"
  - [section 3.1] "Early work on RNNs recognized an intuitive correspondence between RNNs and finite-state automata... Casey (1996) proved that an RNN which robustly learns to model a regular language must organize its state space to correspond with the target FSA"
  - [corpus] Neighbor paper "Exact Learning of Arithmetic with Differentiable Agents" uses Differentiable FSTs for similar extraction purposes (FMR=0.54)

### Mechanism 2
- Claim: Spectral norm penalty encourages finite-state-like dynamics (fixed-point attractors) in the RNN
- Mechanism: Adding L_SN penalizes large singular values in weight matrices, reducing the Lipschitz constant of the hidden state update. Smaller Lipschitz constants make recurrent dynamics contractive, pushing trajectories toward stable attractors that behave like discrete states.
- Core assumption: FST-like computation requires dynamics where small perturbations in hidden state do not cause divergent behavior
- Evidence anchors:
  - [section 3.3.2] "We hypothesized that by encouraging a small spectral norm, and thus a small Lipschitz constant for the hidden state update function, the RNN will tend to have finite-state-like dynamics (i.e. fixed point attractors), and we found empirically that this enabled more robust FST extraction"
  - [abstract] The method produces FSTs that generalize well, implying stable dynamics

### Mechanism 3
- Claim: Synthetic data generation expands input domain coverage, enabling extraction of transitions unseen in training data
- Mechanism: FSTs must explicitly encode all valid input sequences. Training data alone provides sparse coverage. Synthetic strings (generated via n-gram models or tag permutations) produce additional hidden state activations, revealing transitions that would otherwise be missing.
- Core assumption: RNN generalizes to synthetic inputs reasonably well, so their activations reflect meaningful state structure rather than noise
- Evidence anchors:
  - [section 3.3.3] "If we only collect activations for the strings in the training set, we are very unlikely to observe every possible transition"
  - [section 6.2 ablation] Removing synthetic data reduces accuracy by up to 8 points on inflection and G2P datasets

## Foundational Learning

- Concept: **Finite-State Transducers (FSTs)**
  - Why needed here: The entire method targets FST extraction. You must understand that FSTs are deterministic string-to-string rewriting machines with a finite set of states, an input alphabet, output alphabet, and transition function.
  - Quick check question: Given an FST that maps "run" → "running" and "walk" → "walking", what states and transitions are minimally required?

- Concept: **Spectral Norm and Lipschitz Continuity**
  - Why needed here: The spectral norm penalty is a core innovation. Understanding how it constrains the RNN's state dynamics is essential for tuning λ_SN and diagnosing extraction failures.
  - Quick check question: If a 2x2 weight matrix has singular values [3.0, 1.5], what is its spectral norm? How does adding 0.1 * L_SN affect the loss?

- Concept: **Clustering for State Quantization**
  - Why needed here: The method uses k-means to discretize continuous hidden states. You need to understand how cluster count (k) affects FST size and the tradeoff between under-clustering (merged distinct states) and over-clustering (excessive states).
  - Quick check question: If you have 1000 hidden state vectors and set k=10, what happens if two true FST states map to the same cluster? What if one true state splits across two clusters?

## Architecture Onboarding

- Component map:
  - Alignment module -> RNN encoder -> Activation collector -> Clustering module -> Transition aggregator -> State splitter -> FST minimizer

- Critical path:
  1. Alignment quality determines whether RNN receives meaningful supervision (bad alignment → unlearnable transduction)
  2. RNN training must converge with low spectral norm (high λ_SN → underfitting; low λ_SN → unstable states)
  3. Cluster count k must approximately match true FST state count (too few → merged states; too many → fragmentation)
  4. Splitting threshold λ_trans filters noise (too low → spurious splits; too high → non-determinism remains)

- Design tradeoffs:
  - Elman RNN vs. LSTM/GRU: Elman enforces Markov assumption; LSTMs complicate extraction but may model longer dependencies
  - CRPAlign vs. MED: CRPAlign uses global statistics; MED is faster but fails on disjoint alphabets (e.g., G2P)
  - SVM vs. logistic regression for splitting: Both tried via sweep; no clear winner reported
  - Synthetic data volume: More coverage improves generalization but increases computational cost

- Failure signatures:
  - Low accuracy on held-out test but high training accuracy: Overfitting or insufficient synthetic coverage
  - Non-deterministic FST after splitting: λ_trans too high; splitting failed to resolve conflicts
  - Very large FST (e.g., 280 vs. 30 expert states): Over-clustering or splitting cascade
  - Poor performance on G2P/normalization: Unidirectional RNN cannot handle right-context dependencies

- First 3 experiments:
  1. Reproduce on a single inflection dataset: Use a small dataset (e.g., mao with 145 training examples). Run full pipeline with default hyperparameters. Verify alignment outputs, visualize hidden state clusters, and compare extracted FST accuracy to Table 1.
  2. Ablate spectral norm penalty: Train RNN with λ_SN = 0 vs. λ_SN = 0.1. Measure cluster separation (e.g., silhouette score) and FST accuracy. Confirm penalty improves extraction robustness.
  3. Test synthetic data coverage: Extract FST using only training activations vs. training + synthetic. Report accuracy delta on test set, matching Figure 5 analysis.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance critically depends on hyperparameter sensitivity (λ_SN, cluster count k, synthetic data quality)
- Unidirectional RNN architecture cannot capture right-context dependencies, limiting G2P and normalization performance
- Exact hyperparameter specification for CRPAlign alignment is not provided, making reproduction challenging

## Confidence

- **High confidence**: Theoretical foundation linking RNN hidden states to FST structure (Mechanism 1) is well-supported by Casey (1996) and empirical clustering results
- **Medium confidence**: Synthetic data generation approach improves coverage but relies on assumptions about RNN's generalization to synthetic inputs
- **Low confidence**: State splitting mechanism's robustness across diverse datasets is less established; threshold λ_trans and classifier choice require per-dataset tuning

## Next Checks

1. **Cross-task hyperparameter transfer**: Test whether optimal hyperparameters (k, λ_SN, λ_trans) from inflection tasks transfer to G2P/normalization, or if task-specific tuning is necessary

2. **Bidirectional extension validation**: Implement a bidirectional variant (e.g., bimachine-inspired approach) for G2P and normalization tasks to quantify the impact of right-context modeling

3. **Alignment quality ablation**: Systematically compare CRPAlign vs. alternative aligners (e.g., expectation-maximization) on the same datasets to isolate alignment quality's contribution to final FST accuracy