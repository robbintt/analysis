---
ver: rpa2
title: Improving Text-to-Image Generation with Input-Side Inference-Time Scaling
arxiv_id: '2510.12041'
source_url: https://arxiv.org/abs/2510.12041
tags:
- image
- prompt
- rewriter
- alignment
- aesthetics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes input-side inference-time scaling for text-to-image
  generation, where a large language model rewrites user prompts to improve alignment,
  quality, and aesthetics without modifying the T2I backbone. The method uses iterative
  direct preference optimization trained on synthetic prompt-image pairs, with multimodal
  LLM judges providing rewards across image quality, text-image alignment, and aesthetics.
---

# Improving Text-to-Image Generation with Input-Side Inference-Time Scaling

## Quick Facts
- arXiv ID: 2510.12041
- Source URL: https://arxiv.org/abs/2510.12041
- Reference count: 40
- Primary result: GPT-4o win rate of 0.579 on Pick-a-Pic v2 using input-side prompt rewriting

## Executive Summary
This paper proposes input-side inference-time scaling for text-to-image generation, where a large language model rewrites user prompts to improve alignment, quality, and aesthetics without modifying the T2I backbone. The method uses iterative direct preference optimization trained on synthetic prompt-image pairs, with multimodal LLM judges providing rewards across image quality, text-image alignment, and aesthetics. Experiments show consistent improvements over original prompts and strong baselines across diverse T2I models and benchmarks.

## Method Summary
The approach trains an LLM prompt rewriter using iterative Direct Preference Optimization without supervised fine-tuning. For each user prompt, the rewriter generates n candidate rewrites, which are then used to synthesize images with a frozen T2I model. A multimodal LLM judge evaluates all pairwise comparisons across four reward dimensions (quality, general alignment, physical alignment, aesthetics), selecting chosen and rejected prompts based on cumulative voting. The rewriter is updated via DPO using these preference pairs, with LoRA adapters enabling efficient training. The method transfers effectively across different T2I backbones without retraining.

## Key Results
- GPT-4o win rate of 0.579 on Pick-a-Pic v2 v2
- Higher GenEval scores compared to original prompts
- Lower FID on MS COCO 30K
- Larger LLM rewriters yield better results
- Rewriters transfer effectively across different T2I backbones without retraining

## Why This Works (Mechanism)

### Mechanism 1: Iterative DPO Training Without Supervised Fine-Tuning
Training a prompt rewriter through iterative Direct Preference Optimization without SFT enables better generalization and avoids overfitting to model-specific prompt distributions. The rewriter generates n candidate rewrites per prompt, frozen T2I synthesizes images, MLLM judges perform pairwise comparisons across reward dimensions, chosen/rejected pairs update policy via DPO loss, and the process repeats over multiple rounds with updated reference policy. The assumption is that pointwise image rewards are too noisy for reliable scalar signals, but pairwise comparisons provide lower-variance supervision suitable for DPO's Bradley-Terry formulation.

### Mechanism 2: Decomposed Multi-Dimensional Reward with MLLM-as-Judge
Decomposing evaluation into orthogonal reward dimensions enables controllable trade-offs between alignment, quality, and aesthetics. Qwen2.5-VL-72B-Instruct evaluates four dimensions (quality, general alignment, physical alignment, aesthetics) via structured prompts, each comparison yields +1/0/-1 vote, and cumulative scores across all n(n-1) pairs select chosen/rejected prompts. The assumption is that MLLM judges can reliably distinguish subtle quality differences and their structured reasoning outputs correlate with human preferences.

### Mechanism 3: Cross-Backbone Transferability via Shared Training Distributions
A rewriter trained on one T2I backbone transfers to others without retraining because T2I models share common failure modes on underspecified prompts. T2I models trained on similar web-scale image-caption distributions means the rewriter learns universal refinement patterns (adding specificity, attributes, spatial relations) that improve any backbone conditioned on similar prompt distributions. The assumption is that T2I backbones encode similar prompt-to-visual mappings due to shared training data distributions (LAION, web scrapes).

## Foundational Learning

### Concept: Direct Preference Optimization (DPO)
Why needed: Core training algorithm that avoids explicit reward model training by directly optimizing policy from preference pairs using Bradley-Terry assumption.
Quick check: Can you explain why DPO's implicit reward formulation (β log π_θ/π_ref) might produce different optimization dynamics than PPO-style policy gradient with explicit reward model?

### Concept: Inference-Time Scaling Paradigm
Why needed: Understanding the computational trade-off between output-side scaling (best-of-N image sampling) and input-side scaling (best-of-N prompt refinement).
Quick check: Given T2I generation cost G and LLM refinement cost L, under what conditions is input-side scaling (n prompts → n images) more efficient than output-side scaling (1 prompt → n images)?

### Concept: Reward Hacking in Multi-Objective RLHF
Why needed: Critical for understanding why aesthetics reward can degrade alignment—the rewriter learns to game the aesthetic dimension at the cost of faithfulness.
Quick check: If the aesthetics judge systematically prefers ornate backgrounds, what failure mode would you expect in the aesthetics rewriter's outputs?

## Architecture Onboarding

### Component Map:
User Prompt → [Prompt Rewriter LLM + LoRA adapter] → [Frozen T2I Backbone] → [MLLM Judge: Qwen2.5-VL-72B] → [Preference Aggregator] → [DPO Loss with reference π_ref]

### Critical Path:
1. Data curation: ~60k prompts from Pick-a-Pic v2 + T2I-CompBench++ (text-only, no image pairs needed)
2. Reward prompt engineering: 4 structured judge prompts (Appendix H, Figures 7-11)
3. Training loop: 6 DPO rounds × 10k samples/round × 5 candidates/prompt × n(n-1)=20 comparisons/prompt
4. Hyperparameters: LoRA rank=64, β=0.1, lr=5e-6, batch_size=256, temperature=1.5

### Design Tradeoffs:
- General vs. Aesthetics rewriter: Alignment→aesthetics trade-off (Table 1: alignment 0.561→0.424 when adding aesthetics reward); choose based on application requirements
- DPO vs. GRPO: DPO chosen for pairwise compatibility; GRPO requires scalar rewards which paper claims are noisy for images
- LoRA vs. full finetuning: Figure 6 shows comparable performance; LoRA preferred for memory efficiency
- Candidate count n=5: More candidates increase comparison cost O(n²) but provide richer preference signal

### Failure Signatures:
- Reward dimension collapse: Aesthetics-dominated training produces ornate but misaligned outputs
- Training overfitting: Performance plateaus/degrades after round 6
- JanusPro anomaly: Table 9 shows rewriter fails on JanusPro GenEval
- Prompt length explosion: Figure 5 shows tokens grow from ~15→100+ across rounds

### First 3 Experiments:
1. Reward ablation: Train with single reward dimension, measure degradation on held-out dimension
2. Transfer validation: Train on FLUX.1-schnell, evaluate on SD-3.5-medium and JanusPro
3. Scaling curve: Sweep LLM backbone sizes (3B/8B/70B) on fixed T2I

## Open Questions the Paper Calls Out

### Open Question 1
How can the inherent trade-off between aesthetic enhancement and text-image alignment be explicitly controlled or balanced during rewriter training? The paper demonstrates that adding aesthetics reward improves aesthetic quality but degrades alignment scores (0.561→0.424), highlighting a trade-off that isn't fully characterized. A training framework with a controllable hyperparameter or adaptive mechanism that smoothly interpolates between alignment and aesthetics, validated through human preference studies across diverse prompt types, would resolve this.

### Open Question 2
Why do reasoning-oriented LLMs (e.g., DeepSeek-R1-Distilled) underperform standard instruction-tuned models in prompt rewriting for aesthetics? The paper reports that "Models tuned for explicit reasoning (e.g., DeepSeek-R1-Distilled series) underperform Llama-3-70B-Instruct overall, especially on aesthetics," but doesn't investigate whether this stems from reduced creative exploration, different token distributions, or misalignment with aesthetic reward signals. Controlled experiments comparing intermediate activations, prompt diversity metrics, and reward model scores would resolve this.

### Open Question 3
What is the mechanism enabling cross-backbone transferability, and under what conditions does it fail? The authors hypothesize transferability arises because T2I models are commonly trained on image-caption pairs from similar distributions, but the JanusPro anomaly suggests potential failure modes when T2I models have substantially different training distributions or evaluation biases. Systematic transfer experiments across T2I models with known training distribution differences would resolve this.

## Limitations

- Reward dimension composition: The 4-dimensional reward decomposition may not capture all relevant user preferences, and the orthogonality assumption between quality, alignment, and aesthetics may not hold in practice.
- MLLM judge reliability: The paper doesn't validate whether the specific MLLM judge's preferences align with human preferences across all reward dimensions.
- Transfer mechanism understanding: The claim that rewriters transfer across T2I backbones lacks theoretical grounding and the JanusPro failure case isn't fully explained.

## Confidence

**High confidence** - The iterative DPO training algorithm, paired comparison mechanism, and core experimental results are well-documented and reproducible.

**Medium confidence** - The claim that pairwise comparisons are superior to pointwise rewards for image evaluation is supported by relative performance but could benefit from direct controlled experiments.

**Low confidence** - The theoretical explanation for cross-backbone transfer and the exact conditions under which the method fails are not fully characterized.

## Next Checks

1. **Judge consistency validation** - Conduct human preference studies comparing MLLM judge rankings against human rankings for the same prompt-image pairs. Measure Kendall tau correlation across all four reward dimensions to quantify judge reliability and identify systematic biases.

2. **Transfer boundary analysis** - Systematically evaluate the rewriter on T2I backbones with increasingly divergent training distributions (e.g., SDXL, Midjourney, medical imaging models). Plot performance degradation as a function of domain distance to identify transfer limits and failure modes.

3. **Reward dimensionality sensitivity** - Perform a systematic ablation study sweeping different combinations of the four reward dimensions on a held-out test set. Quantify the Pareto frontier between alignment, quality, and aesthetics to provide principled guidance for reward mix selection.