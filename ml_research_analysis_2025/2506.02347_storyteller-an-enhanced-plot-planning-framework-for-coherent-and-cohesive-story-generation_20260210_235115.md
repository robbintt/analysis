---
ver: rpa2
title: 'STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive
  Story Generation'
arxiv_id: '2506.02347'
source_url: https://arxiv.org/abs/2506.02347
tags:
- elias
- story
- generation
- clara
- storyteller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces STORYTELLER, a plot-planning framework designed
  to enhance narrative coherence and logical consistency in long-form story generation.
  Inspired by human cognitive processes, it employs a plot node structure based on
  subject-verb-object triplets and integrates two dynamic modules: the STORYLINE and
  narrative entity knowledge graph (NEKG), which continuously interact during generation
  to maintain thematic continuity and structural soundness.'
---

# STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation

## Quick Facts
- arXiv ID: 2506.02347
- Source URL: https://arxiv.org/abs/2506.02347
- Authors: Jiaming Li; Yukun Chen; Ziqiang Liu; Minghuan Tan; Lei Zhang; Yunshui Li; Run Luo; Longze Chen; Jing Luo; Ahmadreza Argha; Hamid Alinejad-Rokny; Wei Zhou; Min Yang
- Reference count: 19
- Primary result: 84.33% average win rate in human preference evaluation on WRITING PROMPTS dataset

## Executive Summary
STORYTELLER is a plot-planning framework designed to enhance narrative coherence and logical consistency in long-form story generation. The framework introduces a novel plot node structure based on subject-verb-object triplets and integrates dynamic modules including the STORYLINE and narrative entity knowledge graph (NEKG). These components continuously interact during generation to maintain thematic continuity and structural soundness. The approach is inspired by human cognitive processes for story creation, aiming to address common challenges in AI-generated narratives such as logical inconsistencies and weak thematic continuity.

## Method Summary
The framework employs a structured approach to story generation using plot nodes organized as subject-verb-object triplets. Two key dynamic modules, the STORYLINE and NEKG, work together throughout the generation process to maintain coherence. The STORYLINE module tracks narrative progression while the NEKG manages relationships between narrative entities. The system also incorporates a Pseudo CPN Review mechanism to evaluate and refine generated content. This architecture enables continuous interaction between components, allowing for real-time adjustments to maintain narrative quality and logical flow.

## Key Results
- Achieved 84.33% average win rate in human preference evaluation against baselines
- Superior performance in creativity (74.7), coherence (72.8), engagement (66.5), and relevance (77.6)
- Demonstrated strong diversity with DistinctL-4 score of 3.856 and Diverse Verbs score of 0.95

## Why This Works (Mechanism)
The framework's effectiveness stems from its structured approach to narrative generation that mirrors human cognitive processes. By organizing plot elements as subject-verb-object triplets, the system creates a clear framework for tracking narrative relationships. The continuous interaction between the STORYLINE and NEKG modules allows for real-time maintenance of thematic continuity and structural integrity. The Pseudo CPN Review mechanism provides ongoing quality assessment and refinement. This multi-layered approach addresses common issues in AI-generated stories by maintaining logical consistency throughout the narrative while preserving creative flexibility.

## Foundational Learning
- Plot node structure (subject-verb-object triplets) - Needed for systematic narrative organization; Quick check: Verify triplet formation accuracy and completeness
- Narrative entity knowledge graph (NEKG) - Required for tracking entity relationships and maintaining consistency; Quick check: Test entity relationship preservation across story segments
- STORYLINE module - Essential for monitoring narrative progression and coherence; Quick check: Validate storyline continuity between generated segments
- Pseudo CPN Review - Necessary for quality assessment and refinement; Quick check: Evaluate review mechanism effectiveness in identifying logical inconsistencies
- Dynamic module interaction - Critical for real-time coherence maintenance; Quick check: Assess interaction frequency and impact on narrative quality
- Human evaluation framework - Important for measuring subjective narrative qualities; Quick check: Verify annotation consistency and reliability

## Architecture Onboarding

Component map:
Input prompt -> Plot node generation -> STORYLINE/NEKG modules -> Pseudo CPN Review -> Output generation

Critical path:
Input prompt → Plot node creation (SVO triplets) → STORYLINE module processing → NEKG relationship mapping → Pseudo CPN Review → Final story generation

Design tradeoffs:
- Structured approach vs. creative flexibility
- Real-time processing vs. computational efficiency
- Human evaluation vs. automated metrics
- Complexity of multi-module interaction vs. system maintainability

Failure signatures:
- Inconsistent entity relationships across story segments
- Logical gaps in narrative progression
- Reduced diversity in generated content
- Processing delays due to module interactions

First experiments:
1. Test plot node generation accuracy with varied input prompts
2. Validate NEKG relationship mapping with controlled entity sets
3. Evaluate STORYLINE module performance on narrative continuity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies on Amazon Mechanical Turk, potentially introducing variability in assessment quality
- Study focuses exclusively on WRITING PROMPTS dataset, limiting generalizability
- Computational efficiency and resource requirements not addressed

## Confidence
- Framework effectiveness in maintaining coherence: High
- Human evaluation results reliability: Medium
- Generalizability across domains: Low
- Computational efficiency: Not assessed

## Next Checks
1. Conduct cross-domain validation using different story datasets to test generalizability
2. Implement inter-annotator agreement analysis to validate human evaluation reliability
3. Perform computational efficiency benchmarking to assess practical deployment feasibility