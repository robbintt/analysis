---
ver: rpa2
title: Spectral Identifiability for Interpretable Probe Geometry
arxiv_id: '2511.16288'
source_url: https://arxiv.org/abs/2511.16288
tags:
- probe
- spectral
- fisher
- stability
- clipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of linear probes used to interpret
  neural representations, where accuracy can fluctuate unpredictably. The core contribution
  is the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition
  linking probe stability to eigengap geometry.
---

# Spectral Identifiability for Interpretable Probe Geometry

## Quick Facts
- **arXiv ID**: 2511.16288
- **Source URL**: https://arxiv.org/abs/2511.16288
- **Reference count**: 40
- **Primary result**: Spectral Identifiability Principle (SIP) links probe stability to eigengap geometry via Fisher error, predicting phase transitions in linear probe reliability.

## Executive Summary
This paper addresses the instability of linear probes used to interpret neural representations, where accuracy can fluctuate unpredictably. The core contribution is the Spectral Identifiability Principle (SIP), a verifiable Fisher-inspired condition linking probe stability to eigengap geometry. SIP states that when the Fisher estimation error is smaller than the eigengap separating task-relevant directions, the probe is stable; otherwise, instability emerges in a phase-transition manner. The theory connects eigengap, sample size, and misclassification risk through finite-sample reasoning. Controlled synthetic studies confirm the predicted phase transitions and demonstrate how spectral inspection can anticipate unreliable probes before they distort evaluation. In practice, tracking the ratio of Fisher error to eigengap provides a simple, interpretable diagnostic for probe reliability.

## Method Summary
The method validates SIP by generating synthetic data with controlled eigengaps and spectral properties. For Gaussian mixtures, the population Fisher operator Γ is analytically constructed, and empirical Γ̂ is computed from n samples. The top-k eigenspace is extracted via SVD, and the principal angle distance sin Θ between true and estimated subspaces is measured. In heavy-tailed regimes (Student-t), optimal clipping quantiles q⋆ are swept to minimize Δ/gap. The phase transition is verified by showing that probe accuracy collapses sharply when Δ ≈ gap. The approach requires tracking sin Θ, misclassification risk, and the scaling of Δ with n.

## Key Results
- SIP predicts sharp phase transitions in probe stability when Fisher error (Δ) approaches eigengap (gap(Γ))
- Sample complexity scales as n ∝ gap⁻² log d for stable subspace recovery
- Optimal clipping quantile q⋆ exists for heavy-tailed data that minimizes Δ/gap ratio
- Principal angle distance sin Θ(Û, U) ≤ Δ/gap provides a verifiable stability bound

## Why This Works (Mechanism)

### Mechanism 1: Subspace Recovery via Spectral Gap
- **Claim**: If the Fisher estimation error (Δ) is smaller than the spectral eigengap separating task-relevant directions, the probe's discriminative subspace remains stable.
- **Mechanism**: The mechanism relies on the Davis–Kahan sinΘ theorem. It bounds the angular deviation between the true subspace U and the estimated subspace Û by the ratio of the operator norm error to the spectral gap. When the gap dominates the noise, the estimated geometry concentrates around the true geometry.
- **Core assumption**: Assumption (S) requires a non-zero spectral gap λₖ - λₖ₊₁ > 0 separating the signal subspace from the bulk.
- **Evidence anchors**:
  - [abstract] "When the eigengap... is larger than the Fisher estimation error, the estimated subspace concentrates..."
  - [section 4.2] Theorem 4.1 states sin Θ(Û, U) ≤ Δ/gap(Γ)
  - [corpus] Related work "Spectral Thresholds for Identifiability..." confirms geometric mechanisms for stability thresholds.
- **Break condition**: If the eigenvalues collapse (gap → 0) or sample size is too small (Δ large), the bound explodes, and the subspace becomes unidentifiable.

### Mechanism 2: Phase Transition in Sample Complexity
- **Claim**: Probe stability exhibits a sharp phase transition dependent on sample size n, specifically scaling as n ≳ gap⁻² log d.
- **Mechanism**: Matrix concentration bounds (Matrix Bernstein) ensure the estimation error Δ scales as Õ(√log d / n). Since subspace recovery requires Δ < gap, inverting the inequality yields a critical sample size threshold. Crossing this threshold moves the system from an unstable to a stable regime.
- **Core assumption**: Assumption (V) requires bounded variance or sub-Gaussian tails to guarantee the √1/n concentration rate.
- **Evidence anchors**:
  - [abstract] "...instability emerges in a phase-transition manner."
  - [section 6.2] Figure 1(b) shows a sharp increase in misclassification risk near n ≈ 500 in synthetic studies.
  - [corpus] Corpus signals for "Spectral Thresholds..." support finite-sample phase transitions.
- **Break condition**: If data has heavy tails without clipping, variance may not concentrate at the expected rate, disrupting the predicted sample complexity.

### Mechanism 3: Heavy-Tail Stabilization via Clipping
- **Claim**: In heavy-tailed regimes, applying feature clipping can stabilize the probe by optimizing the ratio of estimation error to spectral gap.
- **Mechanism**: Clipping reduces the variance of the Fisher estimate (lowering Δ) but may distort the operator norm or effective gap. A "sweet spot" clipping quantile minimizes the ratio Δ/gap, balancing bias and variance to satisfy SIP where raw features would fail.
- **Core assumption**: Assumption (A5) implies clipping enforces the boundedness required for matrix concentration.
- **Evidence anchors**:
  - [section 6.3] "Clipping has negligible effect in Gaussian but stabilizes in heavy-tailed regimes."
  - [section 6.5] Identifies a "sweet spot" q⋆ where sin Θ is minimized for Student-t distributions.
  - [corpus] Corpus evidence for this specific clipping mechanism is weak; primarily supported by this paper's synthetic experiments.
- **Break condition**: Excessive clipping biases the Fisher operator significantly, potentially collapsing the eigengap or distorting the subspace geometry.

## Foundational Learning

- **Concept: Davis-Kahan Theorem**
  - **Why needed here**: This theorem is the mathematical engine of SIP. It provides the inequality linking perturbation magnitude (Δ) to subspace rotation (sin Θ), defining the stability condition.
  - **Quick check question**: If the spectral gap between two eigenvalues doubles while the perturbation noise stays constant, does the subspace become more or less stable?

- **Concept: Empirical Fisher Information Matrix**
  - **Why needed here**: The paper defines its "Fisher operator" as the uncentered second moment E[h(X)h(X)ᵀ]. Understanding how to estimate this from samples and its role in capturing discriminative geometry is central.
  - **Quick check question**: Does the paper's definition of the Fisher operator include label information directly, or is it derived solely from the representation h(X)?

- **Concept: Matrix Concentration (Bernstein Inequality)**
  - **Why needed here**: This explains why the error Δ decreases with more data. It provides the probabilistic guarantees that the empirical Fisher matrix is close to the population matrix.
  - **Quick check question**: How does the dimension d of the representation affect the sample complexity n required to bound the operator norm error?

## Architecture Onboarding

- **Component map**: Frozen representations h(x) ∈ ℝᵈ -> Empirical Fisher Operator Γ̂ = (1/n)Σh(Xᵢ)h(Xᵢ)ᵀ -> Top-k Eigensolver -> Gap: λₖ - λₖ₊₁, Error Proxy Δ -> Decision: Compare Ratio Δ/gap

- **Critical path**: The reliable estimation of the Eigengap and the Error Proxy. If the gap is indistinguishable from the estimation noise, the probe cannot be trusted regardless of its accuracy.

- **Design tradeoffs**:
  - **Splitting vs. Resampling**: Estimating Δ̂ robustly requires data splitting (reducing n for the main estimate) or bootstrapping (added compute cost)
  - **Clipping**: Aggressive clipping stabilizes variance (reduces Δ) but risks biasing the subspace structure (distorting Γ)
  - **Choice of k**: Increasing k captures more task dimensions but typically reduces the eigengap (smaller λₖ - λₖ₊₁), making the SIP condition harder to satisfy

- **Failure signatures**:
  - **Spectral Collapse**: λ̂ₖ ≈ λ̂ₖ₊₁ (gap ≈ 0)
  - **High Variance**: Δ̂ ≫ gap̂
  - **Symptom**: Probe accuracy fluctuates wildly across random seeds or data shuffles (Phase Transition)

- **First 3 experiments**:
  1. **Synthetic Validation**: Generate Gaussian data with a known ground truth subspace and gap. Verify that the phase transition in probe accuracy occurs exactly when Δ ≈ gap as predicted by Theorem 4.1
  2. **Heavy-Tail Stress Test**: Replace Gaussian data with Student-t data. Demonstrate failure of standard SIP, then show recovery by sweeping the clipping quantile q to find the optimal q⋆ that minimizes the ratio Δ̂/gap̂
  3. **Layer-wise Audit**: Extract features from different layers of a pre-trained model (e.g., BERT). Compute the SIP ratio Δ̂/gap̂ for each layer to identify which layers have stable linear separability and which are spectrally unreliable

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do spatial or temporal feature dependencies in complex architectures (CNNs, RNNs) alter Fisher geometry and eigengap estimation?
- **Basis in paper**: [explicit] "Adapting SIP to complex architectures remains an open direction. In convolutional or recurrent networks, feature dependencies across space or time may alter the Fisher geometry..."
- **Why unresolved**: Current theory assumes general vector representations without modeling specific structural dependencies found in modern deep networks.
- **What evidence would resolve it**: Theoretical bounds or empirical validation showing SIP's predictive power holds (or requires modification) in convolutional layers with local receptive fields.

### Open Question 2
- **Question**: Can robust spectral estimators or regularized eigengap surrogates stabilize SIP in heavy-tailed domains like healthcare or finance?
- **Basis in paper**: [explicit] "Another open question concerns non-Gaussian and heavy-tailed regimes... where Fisher estimates can be distorted by outliers. Developing robust spectral estimators... may improve stability."
- **Why unresolved**: Standard Fisher estimates are biased by outliers, and the current proposed solution (clipping) relies on finding a dataset-specific "sweet spot."
- **What evidence would resolve it**: A modified SIP framework incorporating robust covariance estimators that maintains phase-transition accuracy without manual parameter tuning.

### Open Question 3
- **Question**: Does the spectral phase transition persist in large-scale models like BERT or ResNet when using stochastic approximation for Fisher quantities?
- **Basis in paper**: [inferred] The paper validates SIP using "analytically tractable" synthetic data where quantities are "computed exactly" but calls for extending to "complex distributions and real datasets."
- **Why unresolved**: Real-world models introduce distributional shifts and require stochastic approximation (e.g., randomized SVD), potentially obscuring the sharp theoretical eigengap threshold.
- **What evidence would resolve it**: Layer-wise diagnostic plots from frozen large models showing the empirical ratio of Fisher error to eigengap successfully anticipating accuracy collapses.

## Limitations

- **High-dimensional fragility**: Eigengap estimation becomes unreliable when eigenvalues are close or the spectrum is flat
- **Synthetic validation only**: Current experiments limited to low-dimensional Gaussian and Student-t mixtures with known ground truth
- **Manual parameter tuning**: Optimal clipping requires finding dataset-specific "sweet spot" without clear heuristics

## Confidence

- **High confidence**: Phase transition behavior when Δ ≈ gap (synthetic Gaussian experiments), Davis-Kahan bound sin Θ ≤ Δ/gap, and sample complexity scaling n ∝ gap⁻² log d (supported by matrix concentration theory)
- **Medium confidence**: Heavy-tail stabilization via clipping (based on limited synthetic Student-t experiments without extensive real-world validation)
- **Low confidence**: Practical applicability to real neural representations with unknown spectral structure and unknown tail behavior, effectiveness of the SIP ratio as a universal diagnostic without extensive empirical benchmarking

## Next Checks

1. **Real-world spectral audit**: Apply SIP diagnostics to representations from multiple layers of pre-trained models (BERT, CLIP) across different tasks. Measure how often the Δ/gap ratio exceeds the stability threshold and correlate with probe instability in practice.

2. **Robustness to covariance misspecification**: Generate synthetic data where the true covariance has a slowly decaying spectrum (e.g., polynomial decay) rather than a clean gap. Test whether SIP still provides reliable stability predictions or produces false positives/negatives.

3. **Heavy-tail characterization**: Instead of assuming Student-t distributions, analyze the actual tail behavior of real representation vectors using robust statistical tests. Verify whether the optimal clipping quantile q⋆ from the theory aligns with the empirically optimal quantile for stabilizing probe geometry.