---
ver: rpa2
title: 'SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion'
arxiv_id: '2510.01456'
source_url: https://arxiv.org/abs/2510.01456
tags:
- scoped
- diffusion
- detection
- noise
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCOPED, a score-curvature-based OOD detection
  method for diffusion models. It computes a statistic combining the squared norm
  of the model's score and its Jacobian trace, requiring only a single diffusion model
  evaluation per timestep and leveraging Hutchinson's trace estimator for efficiency.
---

# SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion

## Quick Facts
- arXiv ID: 2510.01456
- Source URL: https://arxiv.org/abs/2510.01456
- Authors: Brett Barkley; Preston Culbertson; David Fridovich-Keel
- Reference count: 36
- Primary result: SCOPED achieves state-of-the-art OOD detection on vision and RL benchmarks using 10x fewer model evaluations than diffusion baselines

## Executive Summary
This paper introduces SCOPED, a score-curvature-based method for out-of-distribution detection using pre-trained diffusion models. Rather than expensive ODE trajectory integration, SCOPED computes a single statistic combining the squared norm of the score function and its Jacobian trace, requiring only one diffusion model evaluation per timestep. The method achieves state-of-the-art performance on vision benchmarks while using an order of magnitude fewer model evaluations than competing approaches, and successfully generalizes to reinforcement learning domains by detecting distribution shifts across tasks and policies.

## Method Summary
SCOPED computes a score-curvature statistic $T(x) = \frac{\|s(x)\|^2}{\kappa(x)}$ where $s(x)$ is the diffusion model's score and $\kappa(x)$ is the curvature (Jacobian trace). It uses Hutchinson's trace estimator for efficient curvature calculation, requiring only a single Jacobian-vector product. The method calibrates using KDE on in-distribution statistics to enable unsupervised detection. For vision tasks, it evaluates at $t=1$ and $t=300$ timesteps and takes the maximum negative log-likelihood as the anomaly score. For RL, it applies the same framework to state-action tuples.

## Key Results
- Achieves 0.84 average AUROC on vision benchmarks versus 0.78 for DiffPath with 10x fewer NFEs
- Successfully detects OOD in RL domains (DMC, D4RL) without task-specific training
- Shows robustness to diverse OOD datasets including texture, semantic, and task distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
The ratio of squared score norm to Jacobian trace (score-curvature) indicates whether a sample resides in the "typical set" of the training distribution. For in-distribution data in high dimensions, information geometry suggests the expected squared norm of the score should match the expected trace of the Jacobian. Deviations from this balance suggest the sample lies outside the typical set, characterizing it as OOD. Core assumption: the data distribution satisfies regularity conditions where the Fisher information relationship holds for in-distribution samples.

### Mechanism 2
Computational efficiency is achieved by replacing full ODE trajectory integration with single-step probing and stochastic trace estimation. Unlike path-based methods that require serial evaluation of denoising steps, SCOPED evaluates the score and its Jacobian at a single (or few) noise levels. It estimates the Jacobian trace using Hutchinson's estimator, which requires only a single Jacobian-vector product rather than computing the full Jacobian matrix, reducing complexity from quadratic to linear in dimension. Core assumption: the Hutchinson estimator provides a sufficiently low-variance estimate with very few probes.

### Mechanism 3
Unsupervised calibration using Kernel Density Estimation (KDE) on in-distribution score-curvature statistics enables robust OOD detection without OOD labels. Raw score-curvature ratios vary across datasets and noise levels. SCOPED constructs a reference distribution by fitting a KDE to the statistics computed on a hold-out set of in-distribution data. Test inputs are scored by their negative log-likelihood under this KDE, measuring how "strange" their geometry is relative to known data. Core assumption: the in-distribution statistics form a coherent density that can be modeled by KDE.

## Foundational Learning

- **Score Functions in Diffusion Models**: SCOPED relies on querying the score ($\nabla_x \log p_t(x)$) and its Jacobian. You must understand that diffusion models are trained to approximate this gradient of the log-density. Quick check: Does the score function point towards higher or lower probability density regions?

- **Typicality vs. Likelihood**: The paper critiques likelihood-based OOD detection (which can fail due to the "typical set" phenomenon). Understanding that high likelihood does not always imply a sample is "typical" (mode vs. mass) is central to the SCOPED hypothesis. Quick check: In high dimensions, does the mode of a distribution typically contain the most probability mass?

- **Hutchinson's Trace Estimator**: This is the engine of SCOPED's efficiency. It allows calculating the trace of the Jacobian (curvature) without computing the full Jacobian matrix. Quick check: How does using random projection vectors allow one to estimate the trace of a matrix without seeing the diagonal explicitly?

## Architecture Onboarding

- **Component map:** Input $x$ -> Noise Scheduler (add noise to get $x_t$) -> Score Network (predicts $s(x_t)$) -> Hutchinson Probe (computes JVP) -> Curvature $\kappa(x)$ -> Statistic $T(x)$ -> KDE Calibration (ID) -> Anomaly Score (test)

- **Critical path:** Setup: Train/load diffusion model. Select $t$ (e.g., via SNR analysis). Calibration: Pass ID batch $\to$ Noise $\to$ Score $\to$ JVP $\to$ $T(x)$ $\to$ Fit KDE. Inference: Input $x$ $\to$ Noise $\to$ Score $\to$ JVP $\to$ $T(x)$ $\to$ KDE Likelihood $\to$ Anomaly Score.

- **Design tradeoffs:** Timestep Selection: Early steps ($t=1$) preserve fine detail (good for texture shifts); mid steps ($t=300$) preserve coarse structure (good for semantic shifts). The paper uses the max score of both for robustness. Probes vs. Cost: Increasing Hutchinson probes reduces variance but increases cost linearly (1 JVP per probe). Paper finds 1 probe sufficient. Sign Correction: The statistic has a sign ambiguity; a global sign factor derived from the score is required for stability (Section D).

- **Failure signatures:** High NFEs: If you are integrating full trajectories, you aren't running SCOPED correctly. It should be single-step. Random Performance on RL: If training on mixed/noisy replay buffers (like "medium-replay" in D4RL), performance may drop. Prefer coherent expert data for training. Numerical Instability: If dividing by curvature $\kappa$, ensure $\epsilon$ is added to the denominator (Section 2.3).

- **First 3 experiments:** Efficiency Validation: Compare wall-clock time and NFEs between SCOPED (Single) and a path-based method (DiffPath) on CIFAR-10 vs SVHN to verify the order-of-magnitude speedup. Timestep Ablation: Run SCOPED on a validation set using only $t=1$, only $t=300$, and the combination (Max) to verify that the two-step approach provides superior robustness over any single step. RL Sanity Check: Train a diffusion model on "humanoid-stand" transitions and test OOD detection on "humanoid-walk". Verify that the SCOPED statistic distribution shifts as expected (ID near 1, OOD dispersed/higher), confirming the method works on proprioceptive data.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the SCOPED statistic be effectively adapted for autoregressive models? The conclusion states future work could "extend it to autoregressive models." Why unresolved: SCOPED relies on the score and curvature of diffusion models; the analogous geometric properties in autoregressive attention mechanisms are not defined. What evidence would resolve it: A formulation of SCOPED for autoregressive architectures that maintains competitive OOD detection performance and computational efficiency.

- **Open Question 2:** How can SCOPED be integrated into reinforcement learning exploration strategies? The abstract and conclusion propose using the method for "exploration in reinforcement learning." Why unresolved: The paper demonstrates detection capabilities post-hoc but does not implement the feedback loop required to guide an agent's policy during active exploration. What evidence would resolve it: An RL agent using SCOPED scores as intrinsic rewards or uncertainty weights that successfully improves sample efficiency or task discovery.

- **Open Question 3:** What principles should guide the curation of the "foundation dataset" to ensure optimal separability? Section 5.1 notes that "training on the most diverse dataset (medium-replay) does not produce the strongest detector," despite Section 6 calling for "dataset curation." Why unresolved: The paper presents counterintuitive evidence that higher diversity can degrade performance via entanglement, leaving the optimal dataset selection strategy unclear. What evidence would resolve it: A study correlating specific dataset characteristics (e.g., trajectory coherence vs. state-coverage diversity) with SCOPED's detection accuracy across tasks.

## Limitations

- The method relies heavily on the validity of typical set assumptions in high dimensions, which may not hold for low-dimensional manifolds or highly structured data distributions.
- The calibration process using KDE assumes in-distribution statistics form a coherent density, but this may fail if training data lacks diversity or contains noise.
- The sign correction mechanism, while necessary for stability, adds complexity that could introduce implementation errors.

## Confidence

- **High Confidence:** The computational efficiency claims (order-of-magnitude reduction in NFEs) are well-supported by the single-step evaluation design and Hutchinson estimator usage. The vision benchmark results showing superior AUROC are also highly credible given the established baselines.
- **Medium Confidence:** The RL domain generalization is promising but based on a limited set of tasks (DMC/D4RL). The performance degradation on noisy replay buffers suggests sensitivity to training data quality that needs further investigation.
- **Low Confidence:** The theoretical justification for the score-curvature ratio as a universal OOD detector across domains relies on strong information geometry assumptions that may not generalize to all data types.

## Next Checks

1. **Robustness to Manifold Structure:** Test SCOPED on synthetic data with known low-dimensional structure (e.g., Swiss roll, spheres) to verify the typical set assumptions break down as expected, and measure the degradation in detection performance.

2. **Cross-Domain Calibration Transfer:** Train the KDE calibration on vision data (CelebA) and evaluate directly on RL data (DMC) without retraining, to assess whether the score-curvature statistics transfer across domains or require domain-specific calibration.

3. **Trace Estimator Variance Analysis:** Systematically vary the number of Hutchinson probes (1, 5, 10, 50) on a challenging OOD detection task and measure the tradeoff between detection accuracy and computational cost to determine if 1 probe is truly sufficient across all scenarios.