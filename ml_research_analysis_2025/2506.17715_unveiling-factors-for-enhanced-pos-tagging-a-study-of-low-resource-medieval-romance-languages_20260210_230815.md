---
ver: rpa2
title: 'Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval
  Romance Languages'
arxiv_id: '2506.17715'
source_url: https://arxiv.org/abs/2506.17715
tags:
- medieval
- tagging
- languages
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates POS tagging for low-resource Medieval Romance
  languages using modern large language models. Experiments across seven datasets
  of Medieval Occitan, French, and Spanish texts show that fine-tuning consistently
  outperforms prompting approaches, with multilingual transfer learning improving
  performance by +0.86 percentage points.
---

# Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages

## Quick Facts
- **arXiv ID**: 2506.17715
- **Source URL**: https://arxiv.org/abs/2506.17715
- **Reference count**: 15
- **Primary result**: Fine-tuning outperforms prompting for POS tagging of Medieval Romance languages, with multilingual transfer learning providing +0.86 percentage points improvement.

## Executive Summary
This study investigates POS tagging for low-resource Medieval Romance languages using modern large language models. Experiments across seven datasets of Medieval Occitan, French, and Spanish texts demonstrate that fine-tuning consistently outperforms prompting approaches, with multilingual transfer learning improving performance by +0.86 percentage points. The research introduces two new Medieval Occitan datasets totaling 135,667 tokens and provides practical recommendations for model selection and optimization in historical language processing.

## Method Summary
The study evaluates POS tagging performance using seven historical language datasets from Medieval Occitan, French, and Spanish. Experiments compare fine-tuning against prompting approaches across different model sizes (8B vs 14B parameters) and training strategies (single-language vs multilingual transfer learning). The evaluation uses Universal Dependencies tagging scheme and measures accuracy across multiple decoding strategies and temperature settings.

## Key Results
- Fine-tuning consistently outperforms prompting approaches across all tested datasets
- Multilingual transfer learning improves performance by +0.86 percentage points
- Aya-8B model outperforms larger 14B-parameter alternatives, demonstrating pre-training quality matters more than model size
- Two new Medieval Occitan datasets totaling 135,667 tokens were introduced

## Why This Works (Mechanism)

### Mechanism 1: Multilingual Transfer for Data Scarcity
Pooling training data from related historical languages improves POS tagging accuracy for extremely low-resource target varieties. The model leverages shared etymological roots and structural similarities across Romance languages, allowing gradient signals from resource-rich pairs to support low-resource pairs via shared latent representations.

### Mechanism 2: Pre-training Fidelity over Scale
The composition of pre-training data (language coverage) is a stronger determinant of performance for historical languages than raw parameter count. Models pre-trained on diverse multilingual corpora possess better initial representations for non-standardized tokens and morphological variations found in medieval texts.

### Mechanism 3: Gradient-Based Adaptation for Orthographic Noise
Fine-tuning updates model weights to map the high-variance input space (e.g., "tepms", "tems" for 'time') to stable output classes (POS tags). In contrast, few-shot prompting relies on the frozen model's ability to pattern-match noisy inputs based on limited context.

## Foundational Learning

**Universal Dependencies (UD)**: The study evaluates performance based on the UD standard. Without understanding this schema, interpreting the class-specific failures (e.g., confusing AUX vs. VERB) is impossible.

**Cross-Lingual Transfer (N-to-1)**: This is the primary method used to boost performance on low-resource languages like Medieval Occitan using Spanish/French data.

**Decoding Strategies**: The paper highlights that optimal decoding changes based on the presence of examples (Zero-shot vs. Few-shot).

## Architecture Onboarding

**Component map**: Aya-8B (Primary recommendation), Gemma-2B (Efficiency), Llama-3.1-8B -> 7 Datasets (Occitan, French, Spanish) formatted as JSON word/upos pairs -> LoRA/Full Fine-tuning vs. In-Context Learning (Prompting)

**Critical path**:
1. Tokenizer Check: Verify vocabulary overlap for historical variants
2. Strategy Selection: Use Fine-tuning for robustness; fall back to Few-Shot (Greedy/Temp=0.3) if compute is unavailable
3. Data Pooling: If the target is low-resource, combine with related Romance datasets for transfer learning

**Design tradeoffs**:
- Size vs. Coverage: Do not use larger models (14B) unless they have specific Romance pre-training. Prefer Aya-8B
- Prompting vs. Fine-tuning: Fine-tuning offers higher average accuracy but requires labeled data
- Transfer Learning: While beneficial for Occitan, it hurt performance on specialized texts like Lapidaire

**Failure signatures**:
- Negative Transfer: Performance drops when combining datasets if the domain is too distinct
- Prompting Collapse: Zero-shot often fails to produce valid JSON or correct tags for ambiguous words like "que"
- Rare Class Blindness: The model fails on INTJ and PROPN due to underrepresentation

**First 3 experiments**:
1. Baseline Zero-Shot: Run Gemma2-9B with Temperature=0.9 on the target dataset
2. Single-Dataset Fine-tuning: Fine-tune Aya-8B on the target language only (1-to-1)
3. Multilingual Transfer: Fine-tune Aya-8B on the pooled Romance corpus (N-to-1)

## Open Questions the Paper Calls Out
None

## Limitations
- Limited number of annotated datasets (7 total) may not capture full linguistic diversity of historical varieties
- Focus on specific commercially available LLMs rather than exploring full landscape of potential models
- Challenges with morphological ambiguity and context-dependent tagging decisions for historical languages

## Confidence
**High Confidence (90-95%)**
- Fine-tuning consistently outperforms prompting approaches
- Multilingual transfer learning provides measurable benefits (+0.86 percentage points)
- Aya-8B outperforms larger models for this task

**Medium Confidence (70-85%)**
- Pre-training quality matters more than model size
- Fine-tuning is generally more effective than prompting for orthographic noise

**Low Confidence (50-65%)**
- Specific temperature and decoding strategy recommendations

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate the best-performing fine-tuned models on completely unseen Medieval Romance texts from different manuscript traditions.

2. **Parameter Sensitivity Analysis**: Systematically vary fine-tuning hyperparameters (learning rate, batch size, LoRA rank, epochs) to identify optimal configurations.

3. **Alternative Annotation Scheme Evaluation**: Re-annotate a subset of the data using a Medieval Romance-specific tagset to test whether UD tagging limits model performance.