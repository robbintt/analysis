---
ver: rpa2
title: 'TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and
  LLM Reasoning'
arxiv_id: '2508.08273'
source_url: https://arxiv.org/abs/2508.08273
tags:
- clinical
- reasoning
- explanation
- explanations
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TT-XAI improves clinical text classification and interpretability
  by distilling long EHR discharge notes into domain-relevant keyword sequences. Using
  Rakun and Med7, it extracts salient medical terms to enhance BERT performance and
  focus LIME explanations.
---

# TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning

## Quick Facts
- **arXiv ID:** 2508.08273
- **Source URL:** https://arxiv.org/abs/2508.08273
- **Reference count:** 0
- **Primary result:** Keyword-augmented explanations improve clinical text classification F1 by over 10 percentage points and explanation fidelity (deletion AUC) by about 10%.

## Executive Summary
TT-XAI introduces a novel approach for generating trustworthy explanations of clinical text classification decisions by distilling long EHR discharge notes into domain-relevant keyword sequences. The method combines Rakun graph-based keyword extraction with Med7 clinical NER to enhance ModernBERT classifier performance and focus LIME explanations. LLMs are then prompted with these keywords to generate structured, clinically relevant chain-of-thought explanations. Evaluation across automatic fidelity metrics, LLM scoring, and blinded human expert review consistently shows that keyword-augmented explanations are clearer, more concise, and clinically more useful than full-text-only approaches.

## Method Summary
The method extracts medical keywords from EHR discharge notes using Rakun (graph-based load centrality) and Med7 (clinical NER), then trains ModernBERT on these distilled keyword sequences. Focused LIME explanations are generated by restricting perturbations to the extracted keywords, and LLMs (LLaMA-3) are prompted with keywords to produce chain-of-thought reasoning. The approach is evaluated on MIMIC-IV kidney stone discharge summaries for binary classification of prolonged hospitalization, using deletion AUC for explanation fidelity, LLM self-scoring, and blinded human expert ratings.

## Key Results
- Classification F1 scores improve by over 10 percentage points using keyword-based inputs versus raw text
- Focused LIME explanations achieve deletion AUC of 0.668 versus 0.742 for standard LIME
- Human experts rated keyword-augmented explanations higher (3.15) versus full-text (2.58) on 1-5 scale
- LLM-generated explanations with keywords are rated clearer and more clinically relevant than those without

## Why This Works (Mechanism)

### Mechanism 1: Signal Distillation for Noise Reduction
Reducing raw EHR notes to concise keyword sequences enhances transformer classifier performance by isolating high-value predictive tokens. Clinical notes contain administrative boilerplate and redundant phrasing that dilute the gradient signal during fine-tuning. By training ModernBERT on these distilled sequences, the model attends primarily to domain-relevant terms. The core assumption is that the relevant clinical signal for prolonged hospitalization is concentrated in specific medical entities and keyphrases rather than narrative structure.

### Mechanism 2: Constrained Perturbation for Explanation Fidelity
Restricting LIME perturbations to a pre-defined "focus set" of keywords improves the faithfulness of local explanations. Standard LIME creates artificial samples by randomly masking tokens, often producing ungrammatical or semantically void sequences in long texts. TT-XAI restricts masking to only the extracted keywords/entities, maintaining semantic coherence during perturbation and allowing the surrogate model to better approximate the classifier's true decision boundary.

### Mechanism 3: Context-Grounded Steering of LLM Reasoning
Prepending extracted keywords to LLM prompts improves the clinical utility and clarity of chain-of-thought reasoning. LLMs can suffer from hallucination or "lost-in-the-middle" effects when processing long notes. By explicitly providing keywords as "Key Clinical Findings" in the prompt, the system grounds the LLM's reasoning process, forcing it to prioritize validated medical entities over generic text comprehension.

## Foundational Learning

- **Concept:** Perturbation-based Fidelity (Deletion AUC)
  - **Why needed here:** The paper relies on "deletion curves" to prove its explanations are better. You must understand that removing "important" tokens should cause the prediction probability to drop; a lower Area Under the Curve (AUC) means the explanation successfully identified the critical tokens.
  - **Quick check question:** If deleting the top-ranked tokens does *not* change the model's probability, is the explanation faithful?

- **Concept:** Graph-based Keyword Extraction (Load Centrality)
  - **Why needed here:** The paper moves beyond TF-IDF. You need to understand that Rakun treats text as a graph where words are nodes; "central" words are those that sit on the shortest paths between other words, capturing the "backbone" of the document's meaning.
  - **Quick check question:** Why might a graph-based approach capture "discharge status" better than simple word frequency?

- **Concept:** ModernBERT (Long-context Encoder)
  - **Why needed here:** The classifier isn't just standard BERT; it is ModernBERT, chosen for its ability to handle longer contexts (up to 1024 tokens in the study) more effectively.
  - **Quick check question:** Does distillation help more at short context windows or long ones, according to the results?

## Architecture Onboarding

- **Component map:** Input (MIMIC-IV Discharge Notes) -> Distillation (Rakun + Med7) -> Focus Set -> Classification (ModernBERT) -> Prediction; Focus Set -> Focused-LIME -> Feature Weights; Focus Set + Raw Note -> LLaMA-3 -> CoT Explanation; Evaluation (Deletion AUC + Blinded Experts)

- **Critical path:** The **Distillation Module**. If Rakun/Med7 misses a crucial clinical entity, the Classifier fails (low recall), the LIME explanation is incomplete, and the LLM hallucinates.

- **Design tradeoffs:**
  - **Recall vs. Precision:** Aggressive keyword merging (Rakun's `merge_threshold`) reduces noise but risks losing distinct medical nuances.
  - **Prompting Strategy:** The "Hybrid" prompt uses *both* keywords and full text. Using *only* keywords might lose context, but using *only* full text loses focus.

- **Failure signatures:**
  - **High Deletion AUC:** Indicates the extracted keywords are not actually what the model uses for prediction (Perturbation-Model mismatch).
  - **Low Expert Score:** LLM explanations are verbose but clinically invalid (hallucination).
  - **Performance Drop at 1024 tokens:** If distillation degrades at max context, it suggests the keyword extraction is diluting the signal with false positives.

- **First 3 experiments:**
  1. **Sanity Check:** Visualize the "Focus Set" against the raw text for 5 random notes. Verify Med7 captures drug dosages and Rakun captures conditions.
  2. **Ablation (Classifier):** Train ModernBERT on (a) Raw Text, (b) Keywords Only, (c) Hybrid. Compare F1 scores to confirm the "10 point gain" holds on your local data slice.
  3. **Fidelity Stress Test:** Run the deletion test on the Focused-LIME output. Does the probability drop to <0.2 when top 20% tokens are removed? If not, the focus set is likely misconfigured.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the keyword distillation approach generalize to clinical domains with different linguistic patterns, such as mental health or chronic disease management?
- **Basis in paper:** The evaluation is restricted to a single cohort of kidney stone cases (467 notes) from MIMIC-IV.
- **Why unresolved:** The vocabulary and structure of discharge notes vary significantly by medical specialty; Rakun's graph-based centrality might perform differently on less concrete or more narrative-heavy notes.
- **What evidence would resolve it:** Replicating the TT-XAI pipeline on a multi-specialty dataset (e.g., full MIMIC-III) to verify if the ~10% F1 improvement holds across diverse conditions.

### Open Question 2
- **Question:** To what extent does the keyword extraction process inadvertently discard subtle contextual signals required for accurate edge-case predictions?
- **Basis in paper:** The Discussion notes that keyword extraction "introduces a form of abstraction that may omit subtle contextual signals, especially in edge cases."
- **Why unresolved:** The aggregate metrics (F1, AUC) do not isolate cases where the distillation process removed negations or temporal modifiers critical to the correct label.
- **What evidence would resolve it:** A qualitative error analysis of false negatives specifically identifying instances where key context was lost during the Rakun/Med7 distillation phase.

### Open Question 3
- **Question:** How do LLM-generated chain-of-thought explanations compare to human expert-written rationales in terms of factual correctness and hallucination rates?
- **Basis in paper:** The Conclusion states a need to "compare LLM-generated explanations against those written by domain experts" to assess factual correctness.
- **Why unresolved:** Current evaluation relies on LLM self-assessment and expert scoring of *perceived quality*, rather than a direct comparison to a ground-truth explanation.
- **What evidence would resolve it:** A study quantifying the semantic overlap and factual consistency between the LLM's reasoning steps and a gold-standard set of expert explanations.

## Limitations

- Performance claims rely on a single dataset (MIMIC-IV kidney stone subset) with specific Rakun parameters that weren't extensively validated across different clinical domains
- Human expert evaluation involved only three reviewers, raising concerns about inter-rater reliability and potential bias
- LLM reasoning quality depends heavily on keyword extraction accuracy - errors in Rakun or Med7 directly propagate to hallucinated explanations

## Confidence

- **High Confidence**: Classification F1 improvement claims (>10 percentage points) are supported by direct comparison within the study using 5-fold cross-validation
- **Medium Confidence**: LIME explanation fidelity improvements are demonstrated through deletion AUC reduction but lack comparison to other attribution methods
- **Low Confidence**: Human expert preference scores (3.15 vs 2.58) are based on limited reviewer pool and lack statistical significance testing

## Next Checks

1. **Cross-domain Validation**: Apply TT-XAI to MIMIC-IV notes for a different condition (e.g., sepsis or pneumonia) using identical Rakun/Med7 parameters. Verify if the 10-point F1 improvement and 10% deletion AUC reduction generalize beyond kidney stones.

2. **Keyword Ablation Stress Test**: Systematically vary Rakun's merge threshold (0.8 to 1.5) and max keyword count (512 to 2,048). Measure classification performance degradation and human explanation scores to identify optimal parameter ranges.

3. **Alternative Attribution Method Comparison**: Implement Integrated Gradients or SHAP on the same ModernBERT model. Compare deletion AUC and human preference scores against focused LIME to establish if the restricted perturbation approach provides unique benefits.