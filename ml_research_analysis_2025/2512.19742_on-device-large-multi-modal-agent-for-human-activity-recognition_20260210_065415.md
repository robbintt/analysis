---
ver: rpa2
title: On-device Large Multi-modal Agent for Human Activity Recognition
arxiv_id: '2512.19742'
source_url: https://arxiv.org/abs/2512.19742
tags:
- data
- activity
- dataset
- ting
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating Large Language
  Models (LLMs) with Human Activity Recognition (HAR) using IMU sensor data, which
  requires bridging the modality gap between numerical sensor data and text-based
  language models. The proposed solution extracts statistical features from IMU time-series
  data and aligns them with LLMs to enable activity classification, reasoning, and
  question-answering capabilities.
---

# On-device Large Multi-modal Agent for Human Activity Recognition

## Quick Facts
- arXiv ID: 2512.19742
- Source URL: https://arxiv.org/abs/2512.19742
- Authors: Md Shakhrul Iman Siam; Ishtiaque Ahmed Showmik; Guanqun Song; Ting Zhu
- Reference count: 40
- Key outcome: Bridges modality gap between IMU sensor data and LLMs using statistical feature extraction and LoRA fine-tuning, achieving 77-83% accuracy on seen data and 61-75% on unseen data.

## Executive Summary
This paper addresses the challenge of integrating Large Language Models with Human Activity Recognition using IMU sensor data. The proposed solution extracts statistical features from time-series IMU data and aligns them with LLMs to enable activity classification, reasoning, and question-answering capabilities. The framework fine-tunes a smaller, mobile-friendly LLM using LoRA-based parameter-efficient tuning on instruction pairs derived from HAR datasets.

## Method Summary
The framework segments raw IMU data into 200-sample windows and calculates time-domain and frequency-domain statistical features. These features are formatted as structured text prompts for LLM processing. The Llama-3-8B model is fine-tuned using LoRA adapters (rank=128) on instruction-response pairs. The approach is evaluated across five datasets (HHAR, Shoaib, MotionSense, UCI HAR, WISDM) with both seen and unseen subject testing.

## Key Results
- Fine-tuned LLM achieves 77-83% classification accuracy on seen data and 61-75% on unseen data
- Outperforms traditional ML models (RF, DNN) on cross-user generalization tasks
- Demonstrates class-wise performance improvements, particularly for static activities like sitting and standing
- Shows LLMs' ability to generalize across unseen data distributions with interpretable reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
Statistical feature extraction acts as a modality bridge, converting high-dimensional time-series data into token-efficient, semantically meaningful text format for LLM processing. The pipeline segments raw IMU data into windows (N<sub>W</sub>=200) and calculates fixed sets of time-domain and frequency-domain features. These numerical statistics are then formatted as structured text, drastically reducing the token count compared to raw data. Core assumption: Selected statistical features preserve essential discriminative characteristics of human activities, allowing the LLM to perform accurate classification and reasoning without access to raw temporal waveform.

### Mechanism 2
Parameter-Efficient Fine-Tuning with LoRA adapts a frozen LLM's general reasoning capabilities to the specific domain of sensor-based activity recognition. Trainable low-rank decomposition matrices (LoRA adapters) are injected into the Llama-3-8B transformer layers. During fine-tuning on instruction-response pairs, only these adapter weights are updated, specializing the model for HAR while preserving its base knowledge. Core assumption: The pre-trained LLM possesses sufficient foundational reasoning and pattern-matching abilities that can be efficiently redirected to interpret statistical sensor signatures via low-rank updates.

### Mechanism 3
The LLM-based framework provides enhanced robustness to cross-user variability compared to traditional specialized models. Traditional models excel by learning data-specific patterns, making them brittle to distribution shifts. The fine-tuned LLM, leveraging its broader pre-training and reasoning, appears to classify based on more generalizable concepts of motion described by the statistical features, rather than overfitting to specific data distributions. Core assumption: The combination of a general-purpose LLM backbone and high-level statistical features creates a less data-specific, more conceptually robust classifier for human activity.

## Foundational Learning

- **Concept: Modality Gap & Alignment**
  - Why needed here: Core problem the paper solves - numerical sensor data and text tokens exist in different semantic spaces, requiring a "bridge" (statistical features) for LLM processing.
  - Quick check question: Why is feeding raw numerical IMU data directly into a text-based LLM ineffective?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: Enables "on-device" agent by freezing main weights and training small adapters, crucial for efficient model specialization.
  - Quick check question: What component of a pre-trained LLM does LoRA modify during the fine-tuning process?

- **Concept: Zero-Shot vs. Fine-Tuned Performance**
  - Why needed here: The paper evaluates base model's "zero-shot" capabilities before introducing fine-tuning, separating LLM's inherent reasoning from learned specialization.
  - Quick check question: What does it mean for the LLM to perform HAR in a "zero-shot" manner?

## Architecture Onboarding

- **Component map:** IMU Sensors -> Sliding window segmentation (200 samples) -> Statistical feature extraction -> Textual formatting -> Llama-3-8B + LoRA Adapters -> Activity classification and reasoning

- **Critical path:** The feature extraction and textualization process. If statistical features fail to capture the essence of the motion, the LLM will have no signal to reason upon, regardless of its capabilities.

- **Design tradeoffs:**
  - Token Efficiency vs. Temporal Detail: Using statistical summaries discards raw temporal waveform, losing fine-grained detail but making input feasible for on-device LLM
  - Generalization vs. Peak Accuracy: Trades some peak accuracy on familiar data for significantly better performance on unseen users/contexts
  - Model Size: Uses 8B parameter model as practical upper bound for mobile deployment, sacrificing potential power of larger models

- **Failure signatures:**
  - Hallucination: Model confidently provides detailed but factually incorrect reasoning for classification
  - Static Activity Confusion: Difficulty distinguishing between low-motion activities like "Sitting" vs. "Standing" due to similar statistical profiles
  - Modality Misalignment: Poor tokenization may cause LLM to interpret numerical values as linguistic tokens, leading to nonsensical outputs

- **First 3 experiments:**
  1. Feature Baseline: Implement statistical feature extraction pipeline on UCI HAR dataset and train Random Forest classifier to establish non-LLM performance baseline
  2. Zero-Shot Evaluation: Format test data into text prompt template and evaluate pre-trained Llama-3-8B's classification accuracy and reasoning quality without fine-tuning
  3. LoRA Fine-Tuning & Comparison: Fine-tune Llama-3-8B using LoRA on instruction dataset. Compare accuracy on held-out test set against Random Forest baseline, specifically checking performance on unseen subjects

## Open Questions the Paper Calls Out

- Can pre-trained sensor encoders outperform manual statistical feature extraction for aligning IMU data with LLMs in HAR tasks? Authors state future work will focus on leveraging pre-trained encoders to automatically extract features from sensor data.

- What domain adaptation techniques can effectively reduce the severe performance degradation in cross-dataset HAR scenarios? The paper notes performance gap between same-modality (~0.97) and cross-modality (~0.04-0.09) accuracy, highlighting the importance of domain adaptation.

- How can hallucination be mitigated in smaller, on-device LLMs during HAR reasoning and classification tasks? The paper observes significant hallucination issues in smaller models like Llama-3-8B, particularly for made-up reasoning or predictions.

- What is the optimal balance between model size, inference latency, and classification accuracy for real-time on-device HAR? Authors note larger models perform better but substantial size and resource requirements pose challenges for on-device implementation.

## Limitations
- Performance on unseen data (61-75% F1) indicates room for improvement, particularly for high-reliability applications
- Statistical feature extraction may miss subtle temporal patterns critical for fine-grained activity distinctions
- Evaluation focuses primarily on classification metrics without extensive analysis of reasoning quality or robustness to sensor noise and placement variations
- 8B model size may still be too large for widespread deployment on low-end smartphones or edge devices

## Confidence

- **High Confidence:** The fundamental approach of using statistical features as a modality bridge to LLMs is technically sound and well-supported by empirical results showing consistent performance improvements over baselines
- **Medium Confidence:** Cross-dataset generalization claims are supported by comparative results but would benefit from testing on truly diverse, out-of-distribution data
- **Medium Confidence:** Reasoning capabilities demonstrated through question-answering are promising but not extensively validated for correctness and consistency

## Next Checks

1. **Out-of-Distribution Testing:** Evaluate the model on sensor data from different device positions (wrist vs. pocket) and environmental conditions not represented in training data to assess true generalization limits

2. **Reasoning Quality Audit:** Conduct systematic evaluation of LLM's reasoning outputs for accuracy, consistency, and potential hallucination across all activity classes, comparing explanations against ground truth activity definitions

3. **Model Size-Performance Tradeoff:** Implement and compare framework using progressively smaller models (3B, 1B parameters) to identify practical lower bound for acceptable on-device performance