---
ver: rpa2
title: 'Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level
  Understandings'
arxiv_id: '2503.05620'
source_url: https://arxiv.org/abs/2503.05620
tags:
- dialogue
- arxiv
- learning
- intent
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for training small models on noisy
  LLM-generated labels for utterance-level dialogue tasks. The core method uses pairwise
  preference learning with intra-session ranking, leveraging the idea that comparing
  utterances within the same dialogue session reduces noise from LLM labeling errors.
---

# Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings

## Quick Facts
- arXiv ID: 2503.05620
- Source URL: https://arxiv.org/abs/2503.05620
- Authors: Xuanqing Liu; Luyang Kong; Wei Niu; Afshin Khashei; Belinda Zeng; Steve Johnson; Jon Jay; Davor Golac; Matt Pope
- Reference count: 26
- The paper proposes a framework for training small models on noisy LLM-generated labels for utterance-level dialogue tasks, showing 2-10% improvements over baselines.

## Executive Summary
This paper introduces a novel framework for training small models on noisy LLM-generated labels for utterance-level dialogue understanding tasks. The core innovation lies in using pairwise preference learning over intra-session dialogue pairs, which leverages the structural consistency within conversations to reduce labeling noise. By calibrating LLM scores and training with adaptive margins, the method achieves significant performance gains on sentiment detection, dialogue act classification, and dialogue state tracking tasks.

## Method Summary
The framework addresses the challenge of training small models on noisy LLM labels by reformulating the problem as pairwise preference learning. Instead of treating each utterance independently, it exploits the natural structure of dialogues by comparing utterances within the same session. The method involves calibrating LLM scores through ensemble techniques and KNN-based label smoothing, then training models to match these calibrated preferences under adaptive margins. This approach is particularly effective in low-data regimes where human-verified labels are scarce.

## Key Results
- Achieves 2-10% performance improvements over traditional knowledge distillation baselines
- Pairwise preference learning outperforms pointwise knowledge transfer, especially in low-data regimes
- Shows particular effectiveness when only small amounts of human-verified labels are available

## Why This Works (Mechanism)
The framework works by recognizing that dialogue sessions have inherent structural consistency - utterances within the same conversation share contextual relationships that can help identify and filter out labeling noise. By focusing on relative preferences between utterances in the same session rather than absolute labels, the method reduces the impact of individual LLM labeling errors. The calibration step ensures that the LLM's confidence scores are properly adjusted before being used for training, while the adaptive margin mechanism allows the model to learn more nuanced distinctions between utterances based on their relative importance.

## Foundational Learning

**LLM-based supervision**: Using large language models to generate training labels for smaller models. Needed because human labeling is expensive and LLMs can provide scalable supervision. Quick check: Verify that LLM-generated labels have reasonable accuracy compared to human labels on a validation set.

**Pairwise preference learning**: Training models to rank pairs of examples rather than predict absolute labels. Needed because relative judgments can be more reliable than absolute ones, especially in noisy settings. Quick check: Ensure that the pairwise ranking objective improves over pointwise classification on benchmark tasks.

**Dialogue session structure**: Leveraging the fact that utterances within the same conversation share contextual information. Needed because this structure provides additional constraints that can help filter noise. Quick check: Validate that intra-session comparisons consistently outperform inter-session comparisons.

**Label calibration**: Adjusting the confidence scores from LLMs to better reflect true probabilities. Needed because raw LLM scores are often poorly calibrated and can mislead training. Quick check: Test calibration methods on a held-out set to ensure improved reliability.

**Adaptive margins**: Using variable margins in pairwise ranking based on the confidence of preferences. Needed because not all pairwise comparisons should be treated equally. Quick check: Perform ablation studies with fixed vs. adaptive margins.

## Architecture Onboarding

Component map: LLM labeler -> Calibration module -> Pairwise comparator -> Small model trainer -> Evaluation

Critical path: The most important sequence is LLM labeler → Calibration → Pairwise comparator → Small model trainer. This path determines the quality of the final trained model.

Design tradeoffs: The framework trades computational complexity (pairwise training is more expensive than pointwise) for improved robustness to labeling noise. The calibration step adds overhead but is essential for quality pairwise comparisons.

Failure signatures: Poor calibration of LLM scores will lead to unreliable pairwise preferences and degraded model performance. Without proper intra-session selection, the method may introduce more noise than it removes.

First experiments:
1. Compare calibrated vs. uncalibrated pairwise training on a simple sentiment detection task
2. Test the impact of different margin values in the adaptive margin mechanism
3. Evaluate the framework on dialogues with varying lengths to assess scalability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Heavy reliance on LLM-generated labels introduces uncertainty about label quality
- Calibration methods require careful tuning and lack extensive validation across different LLM models
- Assumption that intra-session ranking reduces noise may not hold for all dialogue types
- Computational cost of pairwise training with large dialogue contexts is not fully explored

## Confidence

High confidence: Mathematical formulation and general framework design are sound and well-justified
Medium confidence: Effectiveness of calibration and adaptive margin mechanisms shows improvements but needs more ablation studies
Medium confidence: Low-data regime claims demonstrate improvements but absolute performance limits are unclear
Low confidence: Generalization claims to different dialogue types and domains beyond tested datasets

## Next Checks

1. Conduct ablation studies specifically isolating the impact of ensemble calibration versus alternative calibration approaches on pairwise preference quality
2. Test the framework on dialogues with varying turn lengths and topic structures to validate the intra-session ranking assumption across different conversation types
3. Evaluate computational efficiency and memory requirements for very long dialogues (100+ turns) to determine practical deployment constraints