---
ver: rpa2
title: Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient
  Fine-Tuning for Text Summarization
arxiv_id: '2509.01314'
source_url: https://arxiv.org/abs/2509.01314
tags:
- domain
- datasets
- adapters
- dataset
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization

## Quick Facts
- **arXiv ID:** 2509.01314
- **Source URL:** https://arxiv.org/abs/2509.01314
- **Reference count:** 18
- **Primary result:** Parameter-efficient fine-tuning enables 8B LLMs to outperform 70B models on specialized summarization tasks

## Executive Summary
This paper investigates whether smaller LLMs (Llama-3-8B-Instruct) can outperform much larger models (Llama-3-70B-Instruct) on domain-specific text summarization tasks through parameter-efficient fine-tuning (PEFT). The authors train six different PEFT methods across 14 datasets spanning scientific, medical, legal, and news domains, demonstrating that within-domain adapters consistently outperform both zero-shot and larger model baselines. They also show that cross-domain adapter transfer effectiveness can be predicted from distributional similarity metrics between source and target domains, enabling efficient adaptation without full fine-tuning.

## Method Summary
The study trains six PEFT variants (AdaLoRA, (IA)³, LoHA, LoKr, LoRA, OFT) on Llama-3-8B-Instruct across 14 domain-specific datasets (1000 samples each), then evaluates on four holdout validation sets (100 samples each). Each adapter is trained for 5 epochs with rank-64 decomposition, learning rate 0.0005, and bfloat16 precision. Adapter selection uses Borda Count ranking across five metrics (ROUGE, BERTScore, BLEU, METEOR, FActScore). The authors systematically test both within-domain performance and cross-domain transfer, analyzing how distributional similarity metrics predict adapter effectiveness across domains.

## Key Results
- Within-domain PEFT adapters (e.g., SciTLDR-AdaLoRA) outperform both Llama3-8B-Instruct zero-shot and Llama3-70B-Instruct across all four domains
- Cross-domain adapter transfer effectiveness correlates strongly with vocabulary overlap, TF-IDF similarity, and contextual embedding overlap between datasets
- Single adapters consistently outperform combinations for within-domain tasks; merging more than two adapters typically degrades performance
- AdaLoRA shows strongest overall performance, though domain-specific adapters (LoKr, OFT) excel in certain contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Within-domain PEFT adapters enable smaller LLMs (8B parameters) to outperform much larger models (70B parameters) on specialized summarization tasks.
- Mechanism: PEFT techniques modify only ~0.67–1.34% of model parameters via low-rank adaptations attached to self-attention blocks, injecting domain-specific knowledge while preserving the pretrained knowledge base—thus avoiding catastrophic forgetting while achieving specialization.
- Core assumption: Domain-specific patterns can be captured in low-dimensional subspaces without requiring full model retraining.
- Evidence anchors:
  - [abstract] "inference using Within-Domain Adapters can achieve better performance than Few-Shot as well as a much larger Llama-3-70B-Instruct"
  - [section 5.3, Table 6] Within-domain adapters (e.g., SciTLDR-AdaLoRA, CORD19-OFT) outperform both Llama3-8B-Instruct zero-shot and Llama3-70B-Instruct on ROUGE, BERTScore, BLEU, and METEOR across all four domains.
  - [corpus] Related work on adapter recycling (EigenLoRAx) and parameter-efficient methods (QuIC) supports the viability of low-rank adaptation, though not specifically for summarization.
- Break condition: Fails when target domain has no linguistic overlap with any available high-resource dataset, or when factual consistency (FActScore) is prioritized over fluency—some adapters showed lower FActScore than baselines.

### Mechanism 2
- Claim: Cross-domain transfer effectiveness is predictable from distributional similarity between source and target datasets.
- Mechanism: Adapters trained on datasets with higher vocabulary overlap, TF-IDF similarity, and contextual embedding overlap with the target domain yield better performance, suggesting that adapters encode transferable linguistic patterns rather than just surface-level features.
- Core assumption: Semantic and distributional similarity metrics approximate the relevant transferable structure across domains.
- Evidence anchors:
  - [abstract] "evaluate whether intrinsic linguistic commonalities between datasets can be leveraged for efficient domain adaptation"
  - [section 5.2, Table 5] Strong correlation reported between dataset similarity metrics (Vocab Overlap, TF-IDF Overlap, KL Divergence, Contextual Overlap) and adapter performance on holdout validation sets.
  - [corpus] Cross-domain recommendation work exists but does not directly validate this mechanism for text summarization.
- Break condition: Fails when similarity metrics don't capture task-relevant structure (e.g., when domain jargon differs but rhetorical structure is similar), or when adapters overfit to source-domain-specific patterns.

### Mechanism 3
- Claim: Single adapters outperform combinations for within-domain tasks due to reduced interference.
- Mechanism: Merging multiple adapters introduces conflicting adaptation signals that can degrade specialization; individual adapters maintain cleaner parameter updates aligned to specific data distributions.
- Core assumption: Adapter parameter spaces are not orthogonal and merging creates interference rather than complementary coverage.
- Evidence anchors:
  - [section 5.1] "across all domains, the top-3 ranked adapters are individual (single) adapters rather than combinations"
  - [section 5.2] "adapter configurations involving more than two components generally show diminishing returns or even degradation in scores"
  - [corpus] No direct corpus validation; related adapter composition work not found in neighbors.
- Break condition: May fail when target domain genuinely benefits from multi-source knowledge (e.g., medical-legal intersection), though the paper suggests strategic pair combinations can still work if carefully curated.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA) and PEFT variants**
  - Why needed here: The paper benchmarks six PEFT methods; understanding how they inject learnable parameters into frozen pretrained models is essential to interpret results.
  - Quick check question: Can you explain why updating 1% of parameters via low-rank decomposition might preserve more general capability than full fine-tuning?

- **Concept: Domain Shift and Transfer Learning**
  - Why needed here: The entire methodology rests on transferring knowledge from high-resource to low-resource domains; understanding distributional mismatch is critical.
  - Quick check question: If you train on scientific abstracts but test on legal case documents, what linguistic features might transfer vs. fail to transfer?

- **Concept: Summarization Evaluation Metrics (ROUGE, BERTScore, FActScore)**
  - Why needed here: The paper uses five metrics including FActScore for factual precision; interpreting tradeoffs (e.g., higher ROUGE but lower FActScore) requires metric literacy.
  - Quick check question: Why might an adapter improve ROUGE but not FActScore, and which metric matters more for medical summarization?

## Architecture Onboarding

- **Component map:** Llama-3-8B-Instruct (frozen) -> 6 PEFT modules (AdaLoRA, (IA)³, LoHA, LoKr, LoRA, OFT) -> Self-attention blocks -> Domain-specific adaptation
- **Critical path:**
  1. Choose domain and identify available high-resource datasets
  2. Train all 6 PEFTs on each dataset (5 epochs, rank-64, learning rate 0.0005)
  3. Select top-3 adapters per domain using Borda Count
  4. For within-domain inference: apply single best adapter to holdout set
  5. For cross-domain: test adapter combinations from other domains; prioritize pairs with highest similarity scores
- **Design tradeoffs:**
  - Single adapter vs. combination: Single preferred for within-domain; combinations risky beyond 2 adapters
  - AdaLoRA vs. others: AdaLoRA strongest overall but dataset-specific; LoKr/OFT better for certain domains
  - FActScore vs. other metrics: Adapters may sacrifice factual consistency for fluency; domain-dependent priority
- **Failure signatures:**
  - High perplexity during training but strong downstream performance (observed with (IA)³)—don't use perplexity as sole quality signal
  - Performance drops when combining >2 adapters due to interference
  - Cross-domain adapters underperform when source-target similarity is low (check TF-IDF/vocabulary overlap first)
- **First 3 experiments:**
  1. Replicate within-domain baseline: Train AdaLoRA on one dataset per domain, evaluate on respective holdout; verify ROUGE improvement over zero-shot Llama3-8B.
  2. Similarity-transfer validation: Compute TF-IDF and contextual overlap between your available training data and target domain; predict which cross-domain adapter will perform best, then test.
  3. Adapter combination stress test: Merge top-2 vs. top-3 adapters within one domain; confirm performance degradation pattern with 3+ adapters on your specific task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adapters be optimally selected, weighted, or merged to maximize performance in increasingly complex domain transfer scenarios?
- Basis in paper: [explicit] The authors state in the Limitations section that "further research is needed to understand how to best select, weight, or merge adapters for optimal performance across increasingly complex domain transfer scenarios."
- Why unresolved: The current study found that simply combining more than two PEFTs often led to performance degradation rather than improvement, and a methodology for effective merging remains undefined.
- What evidence would resolve it: A systematic study comparing various adapter merging algorithms (e.g., weight averaging, task arithmetic, learned weighting) demonstrating sustained performance gains when scaling beyond two adapters.

### Open Question 2
- Question: To what extent do the cross-domain adaptation capabilities of PEFTs generalize to architectures beyond Llama-3-8B and tasks other than text summarization?
- Basis in paper: [explicit] The authors acknowledge that their experiments "are limited to a single LLM" and suggest "Future work could explore generalization across a broader range of model architectures, tasks beyond summarization, and more linguistically and culturally diverse datasets."
- Why unresolved: The observed efficiency of smaller LLMs with adapters may be specific to the Llama-3 architecture or the structural constraints of text summarization, limiting the broader applicability of the findings.
- What evidence would resolve it: Reproduction of the experimental benchmark using different model families (e.g., GPT, Qwen) and distinct tasks (e.g., question answering, reasoning) showing similar within-domain and cross-domain performance trends.

### Open Question 3
- Question: How does adapter-based domain adaptation perform in truly underserved, low-resource linguistic contexts where domain boundaries do not align with high-resource analogs?
- Basis in paper: [explicit] The Ethical Statement notes that the chosen domains "may not comprehensively represent marginalized or linguistically diverse communities" and suggests future work should "investigate how adapter-based domain adaptation performs in truly underserved or low-resource linguistic contexts."
- Why unresolved: The current method relies on leveraging intrinsic similarities between high-resource and low-resource domains; this strategy may fail if a high-resource proxy does not exist for a specific marginalized language or domain.
- What evidence would resolve it: Experiments applying cross-domain adapters to low-resource languages (e.g., dialects without large training corpora) to determine if linguistic commonalities are sufficient for transfer without high-resource analogs.

## Limitations
- Generalizability uncertainty: The cross-domain transfer effectiveness relies on distributional similarity metrics that may not capture task-relevant structural patterns
- Adapter interference mechanisms: The paper identifies interference when combining >2 adapters but doesn't characterize the specific mechanisms or when strategic combinations might succeed
- Metric tradeoff ambiguity: The paper reports factual consistency degradation in some adapters but doesn't provide clear guidance on which metric should be prioritized based on domain requirements

## Confidence

**High Confidence:** The core finding that within-domain PEFT adapters enable 8B models to outperform 70B models on specialized summarization tasks. This is directly supported by quantitative comparisons across multiple metrics and domains, with clear baselines and statistical significance.

**Medium Confidence:** The predictability of cross-domain transfer effectiveness from distributional similarity metrics. While strong correlations are reported, the analysis doesn't account for potential confounders like dataset size differences, temporal factors, or task-specific structural similarities that may not be captured by the similarity metrics used.

**Low Confidence:** The general recommendation against adapter combinations beyond two components. The paper shows diminishing returns but doesn't systematically explore whether this is a universal pattern or specific to the domains and adapter types tested. Some domain pairs might benefit from multi-source knowledge despite the general trend.

## Next Checks
- **Validation Check 1: Structural Similarity Analysis:** Beyond lexical and distributional similarity, analyze whether structural similarity in summarization patterns (e.g., rhetorical structure, information ordering) better predicts cross-domain transfer success. This could involve parsing summaries to identify common structures and testing whether adapters trained on structurally similar but lexically different domains transfer better than predicted by current metrics.

- **Validation Check 2: Factual Consistency Optimization:** Systematically investigate whether adapters can be trained or tuned specifically to improve FActScore without sacrificing fluency metrics. This could involve adding factual consistency as a training objective, using fact-aware loss functions, or applying post-hoc factual consistency filtering to adapter outputs.

- **Validation Check 3: Domain-Specific Interference Patterns:** Characterize the specific mechanisms of adapter interference by analyzing parameter space overlap and gradient conflicts when combining adapters. This could involve visualizing adapter parameter distributions, measuring cosine similarity between adapter updates, or conducting ablation studies where specific adapter components are removed to identify which contributions cause interference.