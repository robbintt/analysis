---
ver: rpa2
title: Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many
  Relationships
arxiv_id: '2405.18770'
source_url: https://arxiv.org/abs/2405.18770
tags:
- image
- text
- multimodal
- augmentations
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the vulnerability of vision-language models
  to multimodal adversarial attacks, where both images and texts can be perturbed
  to mislead retrieval tasks. Prior defense methods focus on unimodal (image-only)
  attacks, overlooking the one-to-many relationship between images and texts, where
  multiple valid descriptions exist for each image and vice versa.
---

# Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships

## Quick Facts
- arXiv ID: 2405.18770
- Source URL: https://arxiv.org/abs/2405.18770
- Authors: Futa Waseda; Antonio Tejero-de-Pablos; Isao Echizen
- Reference count: 40
- Primary result: MAT+ improves multimodal adversarial robustness by 9-20% R@1 over unimodal defenses on standard benchmarks.

## Executive Summary
This work addresses the vulnerability of vision-language models (VLMs) to multimodal adversarial attacks, where both images and texts can be perturbed to mislead retrieval tasks. Prior defense methods focus on unimodal (image-only) attacks, overlooking the one-to-many relationship between images and texts, where multiple valid descriptions exist for each image and vice versa. To address this, the authors propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly improving robustness against multimodal attacks compared to unimodal defenses. They further enhance robustness by leveraging one-to-many relationships through augmentations, generating diverse yet well-aligned image-text pairs. Analysis reveals that cross-modal augmentations (e.g., image→text) outperform intra-modal ones, and text augmentations are more effective than image augmentations due to distribution shift challenges in high-dimensional image space. MAT and MAT+ consistently improve performance across image-text retrieval, visual grounding, and image captioning tasks.

## Method Summary
The proposed MAT framework implements multimodal adversarial training by solving a min-max optimization problem where both image and text inputs are perturbed during training. The attack generation follows a sequential approach: first applying BERT-Attack to perturb the text modality (1-token replacement with k=10 candidates), then using 2-step PGD to generate adversarial images that minimize cosine similarity with the perturbed text. The optimization uses InfoNCE loss and is applied to CLIP, ALBEF, and BLIP models. MAT+ extends this by incorporating one-to-many augmentations during training, specifically using cross-modal generation (I2T with InternVL/Human captions) to create diverse yet semantically aligned image-text pairs that better approximate the true data distribution. Training uses standard hyperparameters with 5k steps on Flickr30k and 10k steps on COCO.

## Key Results
- MAT improves multimodal adversarial robustness by 9-20% R@1 over unimodal defenses on Flickr30k and COCO
- Cross-modal augmentations (I2T) outperform intra-modal ones, with MAT+ achieving 5-15% higher R@1 than MAT
- Text augmentations are more effective than image augmentations due to distribution shift challenges in high-dimensional image space
- MAT+ maintains clean accuracy while providing robust defense, unlike stronger attacks that degrade clean performance

## Why This Works (Mechanism)

### Mechanism 1: Simultaneous Multimodal Perturbation (MAT)
- **Claim:** Robustness against multimodal attacks requires perturbing both image and text modalities during training; unimodal defenses are insufficient.
- **Mechanism:** By solving a multimodal min-max optimization problem where both the image (δ_I) and text (δ_T) are perturbed sequentially (Text → Image) in the inner loop, the model learns to align representations in a local neighborhood around the clean data points in both embedding spaces simultaneously.
- **Core assumption:** Adversarial vulnerability in Vision-Language Models (VLMs) is localized and can be patched by observing and training on the worst-case perturbations in the joint space.
- **Evidence anchors:**
  - [abstract] "MAT... incorporates adversarial perturbations in both image and text modalities... significantly outperforming existing unimodal defenses."
  - [section 4.2.1] "We generate adversarial examples (I', T') by sequentially perturbing the text and image modalities."
  - [corpus] "Q-MLLM" and "EigenShield" propose architectural or quantization defenses; this mechanism focuses on optimization/training data.
- **Break condition:** If the inner maximization fails to find strong attacks (e.g., insufficient PGD steps or failed text token swaps), the outer minimization trains on weak adversarial examples, yielding false robustness.

### Mechanism 2: True Data Distribution Approximation (MAT+)
- **Claim:** Deterministic 1:1 training pairs provide a sparse and insufficient approximation of the true data manifold (D*), limiting robustness.
- **Mechanism:** By modeling the inherent ambiguity of VL tasks through one-to-many (1:N) augmentations, MAT+ expands the support of the training distribution. This prevents the model from overfitting to a single "deterministic" caption for an image, smoothing the decision boundary for valid semantic variations.
- **Core assumption:** Valid image-text pairs exist in a dense manifold around the training data points that are currently un-sampled.
- **Evidence anchors:**
  - [abstract] "MAT is limited by deterministic one-to-one (1:1) image-text pairs... To address this, we... leverage one-to-many relationships."
  - [section 4.3.1] "For a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift."
  - [corpus] Neighbors focus on robustness via architectural changes; this mechanism highlights the causal role of data density and ambiguity modeling.
- **Break condition:** If the augmented pairs do not belong to the true distribution (Case 3: Distribution Shift) or are semantically misaligned (Case 1), the approximation of D* degrades, reducing robustness.

### Mechanism 3: Cross-Modal Alignment Preservation
- **Claim:** Cross-modal augmentations (e.g., Image → Text) are more effective for defense than intra-modal augmentations (e.g., Image → Image).
- **Mechanism:** Intra-modal augmentations (like RandAug) struggle to maintain semantic alignment with the paired modality while introducing diversity, often violating the alignment condition. Cross-modal generation (using a VLM to generate a caption for an image) naturally ensures the new pair is semantically aligned while providing diverse descriptions, thereby satisfying the conditions for effective approximation.
- **Core assumption:** The text modality is lower-dimensional and more structured than the image modality, making it easier to augment without causing distribution shift.
- **Evidence anchors:**
  - [abstract] "Analysis reveals that cross-modal augmentations (e.g., image→text) outperform intra-modal ones."
  - [section 6.1.2] "Cross-modal augmentation... outperforms intra-modal... by generating better-aligned pairs."
  - [corpus] Weak direct evidence in neighbors; this appears to be a specific causal finding of this paper regarding data strategy.
- **Break condition:** If the cross-modal generator (e.g., Stable Diffusion or InternVL) hallucinates or drifts from the image content, the "Alignment" property is violated, negating the mechanism's benefit.

## Foundational Learning

- **Concept: Min-Max Optimization (Adversarial Training)**
  - **Why needed here:** The core engine of MAT is solving $\min_\theta \max_\delta L(I+\delta_I, T+\delta_T)$. You must understand that the "max" creates the attack and the "min" updates the weights.
  - **Quick check question:** In the MAT formulation, does the "inner maximization" update the model weights or the input perturbations?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** The paper relies on CLIP and ALBEF, which use contrastive loss. The attack mechanism (minimizing similarity) and defense (maximizing similarity) operate directly on this loss landscape.
  - **Quick check question:** How does the "Cosine Similarity" relate to the InfoNCE loss used for training?

- **Concept: The 1-to-Many (1:N) Alignment Problem**
  - **Why needed here:** Standard classification assumes 1:1 (Image → Label). VLMs operate in a space where one image has many valid descriptions. Understanding this ambiguity is key to grasping why MAT+ works.
  - **Quick check question:** Why would training on only one caption per image make a model vulnerable to a perturbed version of a different valid caption?

## Architecture Onboarding

- **Component map:** Vision Encoder (ViT/CNN) -> Text Encoder (Transformer/BERT) -> Projectors (Linear layers) -> Shared latent space (R^{d_E}) -> Attackers (BERT-Attack → PGD) -> Augmentors (I2T/Stable Diffusion) -> Augmented Dataset

- **Critical path:**
  1. Load Batch → Generate Adv Text (T') using BERT-Attack (maximize divergence)
  2. Generate Adv Image (I') using PGD (minimize similarity with T')
  3. Forward pass (I', T') through model
  4. Compute Loss (InfoNCE) → Backprop (Outer Minimization)

- **Design tradeoffs:**
  - **Efficiency vs. Strength:** The paper uses 2-step PGD and sequential attacks (Text → Image) to reduce compute, trading off theoretical optimality for speed (vs. PGD-10)
  - **Alignment vs. Diversity:** Intra-modal augmentations offer diversity but break alignment. Cross-modal (I2T) offers better alignment but requires a capable generator (e.g., InternVL vs. Human)
  - **Synthetic vs. Human Data:** Human captions (I2T_{Human}) provide the best alignment/gap balance but are unscalable. Synthetic (I2T_{SD}) risks distribution shift

- **Failure signatures:**
  - **Clean Accuracy Collapse:** Using strong PGD-10 steps during training drops clean accuracy significantly (see Ablation 3-3)
  - **Zero Robustness:** If only unimodal AT is used (TeCoA), multimodal attacks (SGA) reduce R@1 to < 1%
  - **Distribution Shift:** Using T2I (Stable Diffusion) purely for image augmentation can lower robustness if the Fréchet distance (distribution gap) is too high

- **First 3 experiments:**
  1. **Baseline Check:** Train CLIP with standard Fine-tuning vs. TeCoA (Unimodal AT) vs. MAT (Multimodal AT) on Flickr30k. Measure R@1 under SGA attack. *Expected: MAT >> TeCoA*
  2. **Ablation on Perturbation Order:** Compare T → I vs. I → T generation. *Expected: Minimal difference, but T → I slightly preferred*
  3. **Augmentation Type Analysis:** Train MAT+ using basic EDA (intra-modal) vs. I2T (cross-modal). *Expected: I2T provides higher robustness due to better alignment*

## Open Questions the Paper Calls Out

- **Open Question 1:** How can image augmentation techniques be developed to achieve high diversity while minimizing distribution shift in high-dimensional image spaces?
  - **Basis in paper:** [explicit] The authors conclude that "developing image augmentations with high diversity yet minimal distribution gap is a promising direction for future research" because current image augmentations underperform compared to text augmentations (Page 8)
  - **Why unresolved:** The high dimensionality of image data makes it difficult to avoid distribution shifts when introducing diversity, a challenge not present in text augmentation
  - **What evidence would resolve it:** The creation of an image augmentation technique that achieves diversity metrics comparable to text augmentations (e.g., I2T(Human)) while maintaining a lower Fréchet distance (distribution gap) than existing methods like T2I(SD)

- **Open Question 2:** How can many-to-many (N:N) augmentation strategies be effectively formulated to enhance robustness without distorting the training data distribution?
  - **Basis in paper:** [explicit] The paper observes that while theoretically promising, N:N augmentations did not improve performance and notes they "require a dedicated methodology, which falls out of the scope of this work" (Page 8)
  - **Why unresolved:** The authors found that combining image and text augmentations leads to data where original pairs are a small minority (e.g., 12.5%), making the dataset prone to distortion if not carefully managed
  - **What evidence would resolve it:** A study demonstrating a specific N:N augmentation protocol that yields statistically significant robustness improvements over the 1:N (MAT+) baseline on standard benchmarks like COCO

- **Open Question 3:** Does solving the exact inner maximization for multimodal perturbations yield significantly better robustness than the proposed step-by-step approximation?
  - **Basis in paper:** [inferred] The authors state that "Directly solving the inner-maximization... is highly non-trivial" and rely on a practical, sequential "step-by-step perturbation" strategy (Page 4)
  - **Why unresolved:** While the approximation is efficient, the paper does not quantify the theoretical or empirical optimality gap between this sequential approach and a true simultaneous multimodal perturbation
  - **What evidence would resolve it:** A comparative analysis evaluating the robustness of models trained via simultaneous gradient-based updates versus the sequential text-then-image approach used in MAT

## Limitations
- Sequential (text-then-image) perturbation may not capture the true worst-case multimodal attacks
- Augmented pairs may introduce distribution shift despite claims of alignment preservation
- Cross-modal augmentation superiority not verified across different VL model architectures
- Theoretical justification for MAT+ relies on data density arguments without explicit manifold learning validation

## Confidence
- **High Confidence:** MAT outperforms unimodal defenses on standard benchmarks; unimodal defenses fail against multimodal attacks; MAT+ improves upon MAT
- **Medium Confidence:** Cross-modal augmentations outperform intra-modal ones; text augmentations are more effective than image augmentations due to distribution shift
- **Low Confidence:** MAT+ works by true data distribution approximation through one-to-many relationships (lacks quantitative manifold validation)

## Next Checks
1. Measure explicit distribution divergence (Fréchet distance, KL divergence) between original and augmented image-text pairs across all augmentation types to validate the claim that cross-modal augmentations maintain better alignment
2. Systematically compare MAT performance under different perturbation sequences (T→I, I→T, simultaneous) and attack strengths to determine whether the sequential approach is optimal or merely convenient
3. Test MAT/MAT+ on VL models beyond CLIP (e.g., ALBEF, BLIP) and on datasets with different semantic characteristics (e.g., more abstract concepts) to verify that cross-modal augmentation superiority is architecture-independent