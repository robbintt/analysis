---
ver: rpa2
title: 'QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence
  Completion Tasks'
arxiv_id: '2601.20731'
source_url: https://arxiv.org/abs/2601.20731
tags:
- subject
- across
- subjects
- sentiment
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) reproduce
  heterocisnormative biases by evaluating sentence completions for unmarked, queer-marked,
  and non-queer-marked subjects. Using the QueerGen framework, we analyzed sentiment,
  regard, toxicity, and prediction diversity across 14 models (MLMs and ARLMs).
---

# QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks

## Quick Facts
- arXiv ID: 2601.20731
- Source URL: https://arxiv.org/abs/2601.20731
- Reference count: 40
- This study shows that LLM architecture and alignment strategies significantly influence representational harms toward queer-marked subjects, with MLMs showing strongest bias and closed-access ARLMs redistributing harms toward unmarked subjects.

## Executive Summary
This study investigates how Large Language Models reproduce heterocisnormative biases through sentence completion tasks. Using the QueerGen framework, researchers evaluated 14 models (MLMs and ARLMs) across four dimensions: sentiment, regard, toxicity, and prediction diversity. MLMs exhibited the strongest bias, producing less favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Open-access ARLMs partially mitigated these patterns, while closed-access ARLMs redistributed harms, often toward unmarked subjects. The findings reveal that identity-related content significantly influences model behavior, demonstrating that alignment and model scale can redistribute but not eliminate representational harms.

## Method Summary
The QueerGen dataset contains 3,100 prompts constructed from 10 unmarked subjects, 30 identity markers (10 non-queer, 20 queer), and 10 sentence templates. MLMs predict masked tokens using full sentence context, while ARLMs generate completions through prompted single-word generation. Evaluation employs VADER sentiment scoring (word-level, -1 to 1), Hugging Face regard classifier (negative/neutral/positive), Perspective API toxicity (5 dimensions averaged), and lexical diversity (% unique predictions). Identity markers are masked during toxicity and regard evaluation to prevent artificial triggering. The study analyzes top-1 predictions across 14 models including BERT/RoBERTa, Llama, Gemma, DeepSeek, GPT-4, and Gemini variants.

## Key Results
- MLMs showed strongest heterocisnormative bias: less favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects
- Open-access ARLMs partially mitigated bias patterns compared to MLMs
- Closed-access ARLMs redistributed harms toward unmarked subjects rather than eliminating them
- Identity-related content significantly influences model behavior across all architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MLMs encode stronger heterocisnormative bias than ARLMs because bidirectional training amplifies corpus-level associations between identity markers and negative context
- **Core assumption:** Training data contains systematic co-occurrence patterns where queer identity terms appear in more negative or underrepresented contexts
- **Evidence anchors:** MLMs showed strongest bias producing less favorable sentiment, higher toxicity, and more negative regard; likely reflect architectural constraints, smaller model sizes, and biased training data
- **Break condition:** If training corpora were balanced for identity representation AND context valence, MLM bias should approach ARLM levels

### Mechanism 2
- **Claim:** RLHF-based safety alignment redistributes representational harms rather than eliminating them, often shifting toxicity toward unmarked subjects
- **Core assumption:** Safety fine-tuning operates as output suppression rather than representation correction
- **Evidence anchors:** Closed-access ARLMs redistributed harms toward unmarked subjects; this may indicate use of RLHF; no corpus papers specifically test redistribution effects
- **Break condition:** If closed-access models without RLHF showed similar redistribution patterns, mechanism would be confounded by other factors

### Mechanism 3
- **Claim:** Explicit identity marking activates narrower, more stereotyped vocabulary predictions compared to unmarked prompts
- **Core assumption:** The prediction space for marked identities is more constrained by training corpus associations
- **Evidence anchors:** Non-queer-marked subjects elicit most lexically diverse predictions; ARLMs yield higher prediction diversity than MLMs; marked subjects elicit broader vocabulary while unmarked prompts default to generic completions
- **Break condition:** If diversity differences disappeared when controlling for token frequency in training data

## Foundational Learning

- **Concept:** Masked Language Modeling vs. Autoregressive Language Modeling
  - **Why needed here:** The paper's central finding is that architecture type strongly predicts bias expression; understanding MLM bidirectional context versus ARLM sequential generation is essential
  - **Quick check question:** Given "The [MASK] person likes to ___", would BERT (MLM) or GPT (ARLM) leverage more context for prediction?

- **Concept:** Representational vs. Allocational Harms
  - **Why needed here:** The study operationalizes representational harms (misrepresentation, stereotyping, derogatory language); distinguishing these from allocational harms clarifies the evaluation scope
  - **Quick check question:** If an LLM assigns lower credit scores to queer individuals, is this representational or allocational harm?

- **Concept:** Linguistic Markedness Theory
  - **Why needed here:** The tripartite framework (unmarked, non-queer-marked, queer-marked) draws on sociolinguistic markedness; unmarked forms function as defaults carrying implicit normative weight
  - **Quick check question:** In "same-sex wedding" versus "wedding," which is marked and what normative assumption does the unmarked form encode?

## Architecture Onboarding

- **Component map:** Dataset Construction → Model inference (per architecture) → Mask identity markers during evaluation → Aggregate scores per (model, category) → Compare distributions across categories
- **Critical path:** Dataset → Model inference (per architecture) → Mask identity markers during evaluation → Aggregate scores per (model, category) → Compare distributions across categories
- **Design tradeoffs:** Word-level vs. sentence-level evaluation (chose word-level to avoid context dilution); Top-1 vs. top-5 predictions (top-1 may overstate bias; top-5 provides nuance but increases cost); Masking identity markers during toxicity evaluation (prevents artificial triggering but may miss identity-dependent harms)
- **Failure signatures:** MLMs: High toxicity variance across categories; negative sentiment clustering for queer-marked subjects; Closed-access ARLMs: Inverted toxicity pattern (unmarked > marked); constrained diversity for marked identities; All architectures: Low JSD (~0.39) between marked categories suggests shared stereotyped vocabulary
- **First 3 experiments:** 1) Replicate with top-5 predictions to assess whether observed bias is ranking artifact or distribution-level phenomenon; 2) Test prompt sensitivity: Compare Prompt 2 vs. Prompt 4 variants to isolate identity vs. prompt-structure confounds; 3) Extend to intersectional markers (e.g., "Black transgender woman") to evaluate whether current tripartite framework captures compounding effects

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific mechanisms in closed-access ARLMs (e.g., RLHF techniques, guardrails, training data filtering) cause the redistribution of representational harms toward unmarked subjects rather than eliminating them?
  - **Basis in paper:** Authors state in Discussion: "This may indicate the use of safety mechanisms such as Reinforcement Learning from Human Feedback (RLHF)... a trade-off consistent with alignment practices around sensitive identity terms."
  - **Why unresolved:** The study measures outcomes but does not examine underlying causes, as explicitly noted in Limitations: "our study also does not examine the underlying causes of the observed differences in model generations."
  - **What evidence would resolve it:** Ablation studies comparing models with different alignment techniques, or analysis of training data distributions and guardrail configurations across model versions.

- **Open Question 2:** How do intersectional dimensions such as race, class, and disability interact with gender and sexuality markers to compound or modify representational harms in LLM outputs?
  - **Basis in paper:** Limitations section states: "our analysis does not account for intersectional dimensions such as race, class, or disability, which are critical for understanding broader representational biases."
  - **Why unresolved:** The study design isolates gender and sexuality markers without intersectional combinations, and the dataset does not include intersectional identity markers.
  - **What evidence would resolve it:** Extending the QueerGen framework with intersectional marker combinations (e.g., "Black transgender woman") and analyzing whether harm patterns are additive, multiplicative, or qualitatively different.

- **Open Question 3:** How do bias patterns identified in English LLM outputs manifest in multilingual models or models trained primarily on non-English corpora?
  - **Basis in paper:** Limitations section states: "This study is limited to the English language. Social norms and identity expressions vary across languages and cultures, and models trained in or for other languages may encode different patterns of bias."
  - **Why unresolved:** The study tested only English sentence completions, and societal norms around gender and sexuality vary significantly across cultures.
  - **What evidence would resolve it:** Applying the QueerGen framework to multilingual models or culture-specific models, with culturally appropriate identity markers and templates.

## Limitations

- The tripartite framework may oversimplify complex identity categories and potentially mask intersectional effects
- The study relies on word-level scoring through masked prediction (MLMs) versus prompted completion (ARLs), creating architectural confounds that make direct comparison challenging
- The finding that closed-access ARLMs redistribute harms toward unmarked subjects requires cautious interpretation without access to model internals or training details
- The study is limited to English language and does not account for intersectional dimensions such as race, class, or disability

## Confidence

- **Confidence: Medium** - While the study demonstrates clear patterns of bias across model architectures, several methodological limitations affect interpretation including the tripartite framework's potential oversimplification and architectural confounds in evaluation methods
- **Confidence: Low-Medium** - The finding that closed-access ARLMs redistribute harms toward unmarked subjects is particularly concerning but requires cautious interpretation due to lack of access to model internals or training details
- **Confidence: Medium-High** - The architectural bias differences between MLMs and ARLMs are robust across multiple evaluation metrics and align with the mechanism that bidirectional context in MLMs amplifies corpus-level associations between identity markers and negative language

## Next Checks

1. **Test prompt sensitivity and intersectional effects:** Compare the current Prompt 2 against alternative formulations (e.g., Prompt 4) to determine whether observed bias patterns persist across prompt variations. Additionally, extend the framework to include intersectional markers (e.g., "Black transgender woman") to evaluate whether the tripartite framework captures compounding effects or masks intersectional harms.

2. **Validate with top-5 predictions and alternative toxicity classifiers:** Replicate the analysis using top-5 predictions rather than top-1 to assess whether bias is a ranking artifact or distribution-level phenomenon. Supplement Perspective API toxicity scores with alternative classifiers (e.g., Detoxify, HateXplain) to determine whether observed toxicity patterns are specific to Google's classifier or reflect genuine distributional differences.

3. **Cross-linguistic and cross-cultural validation:** Extend the QueerGen framework to non-English languages and diverse cultural contexts to determine whether heterocisnormative bias patterns generalize beyond Western English-language corpora. This would help distinguish whether observed patterns reflect universal linguistic associations or are artifacts of specific training data demographics.