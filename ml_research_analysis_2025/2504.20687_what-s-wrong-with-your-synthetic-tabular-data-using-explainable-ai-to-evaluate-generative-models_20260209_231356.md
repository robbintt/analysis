---
ver: rpa2
title: What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate
  Generative Models
arxiv_id: '2504.20687'
source_url: https://arxiv.org/abs/2504.20687
tags:
- data
- synthetic
- feature
- generative
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating synthetic tabular
  data quality, which is difficult due to the absence of direct performance measures
  and the complexity of assessing high-dimensional dependencies. Existing metrics
  often provide conflicting results and lack interpretability.
---

# What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models

## Quick Facts
- arXiv ID: 2504.20687
- Source URL: https://arxiv.org/abs/2504.20687
- Reference count: 40
- The paper proposes using XAI techniques on binary classifiers to evaluate synthetic tabular data quality by revealing why synthetic data are distinguishable from real data.

## Executive Summary
Evaluating synthetic tabular data quality is challenging due to the absence of direct performance measures and the complexity of assessing high-dimensional dependencies. Traditional metrics often provide conflicting results and lack interpretability, making it difficult to identify specific weaknesses in synthetic data. This paper addresses these challenges by proposing an XAI-based approach that leverages feature importance measures, partial dependence plots, Shapley values, and counterfactual explanations to reveal why synthetic data are distinguishable from real data.

The proposed methodology uses a binary classifier trained to distinguish real from synthetic data, then applies XAI techniques to understand the classifier's decision-making process. This approach uncovers unrealistic patterns, missing dependencies, and distributional inconsistencies in synthetic data that standard evaluation techniques overlook. Experiments on real datasets demonstrate that this method provides deeper insights for improving synthetic data quality and debugging generative models, offering a more interpretable and actionable evaluation framework.

## Method Summary
The authors propose using explainable AI (XAI) techniques on a binary classifier trained to distinguish real from synthetic data. The method involves training a classifier to identify whether data points are real or synthetic, then applying XAI tools such as feature importance measures, partial dependence plots, Shapley values, and counterfactual explanations to the classifier's predictions. By analyzing which features and patterns make the classifier successful, researchers can identify specific weaknesses in synthetic data generation, including unrealistic patterns, missing dependencies, and distributional inconsistencies. The approach provides interpretable insights that help debug generative models and improve synthetic data quality beyond what traditional evaluation metrics offer.

## Key Results
- XAI techniques applied to binary classifiers reveal specific patterns that make synthetic data distinguishable from real data
- The approach identifies unrealistic patterns, missing dependencies, and distributional inconsistencies overlooked by standard metrics
- Experiments demonstrate that this method provides deeper insights for improving synthetic data quality and debugging generative models

## Why This Works (Mechanism)
The approach works by exploiting the fact that if synthetic data can be distinguished from real data, there must be systematic differences that a classifier can learn. By applying XAI techniques to this classifier, researchers can identify the specific features, patterns, and relationships that make synthetic data detectable. These distinguishing characteristics often represent flaws in the synthetic data generation process, such as unrealistic distributions, missing correlations, or inconsistent patterns. The XAI methods provide interpretable explanations of these flaws, allowing researchers to understand exactly what aspects of their synthetic data need improvement.

## Foundational Learning

**Binary classification for data discrimination**: Train a classifier to distinguish real vs synthetic data; needed to identify systematic differences between data distributions; quick check: classifier accuracy significantly above random chance indicates detectable differences.

**Feature importance analysis**: Measure which features contribute most to classification decisions; needed to identify which variables contain distinguishing information; quick check: top features align with domain knowledge of data characteristics.

**Partial dependence plots**: Visualize how features influence predictions while averaging over other variables; needed to understand feature effects and interactions; quick check: plots reveal unrealistic feature relationships in synthetic data.

**Shapley value explanations**: Calculate feature contributions to individual predictions using game theory; needed for granular understanding of prediction drivers; quick check: Shapley values highlight specific data points with unrealistic feature combinations.

**Counterfactual explanations**: Generate alternative data points that would change classification outcomes; needed to understand what minimal changes would make synthetic data appear real; quick check: counterfactuals reveal unrealistic feature thresholds or relationships.

## Architecture Onboarding

**Component map**: Data preprocessing -> Binary classifier training -> XAI method application -> Feature importance analysis -> Partial dependence plots -> Shapley value computation -> Counterfactual explanation generation

**Critical path**: Real data + Synthetic data -> Binary classifier (e.g., Random Forest, XGBoost) -> XAI explanation (SHAP, LIME, PDP) -> Interpretability insights

**Design tradeoffs**: 
- Classifier choice affects explanation quality (tree-based models provide built-in feature importance but may overfit)
- XAI method selection impacts interpretability and computational cost (SHAP provides theoretically grounded explanations but is computationally expensive)
- Balancing explanation granularity with computational efficiency

**Failure signatures**: 
- Classifier performs at chance level (no distinguishable patterns found)
- XAI explanations are inconsistent across methods (potential overfitting or unreliable patterns)
- Feature importance concentrated on irrelevant or noisy features (data quality issues)

**First experiments**:
1. Train a simple Random Forest classifier on real vs synthetic data and evaluate classification accuracy
2. Apply SHAP values to identify top features driving classification decisions
3. Generate partial dependence plots for the top 3 features to visualize their effects on distinguishability

## Open Questions the Paper Calls Out

None

## Limitations
- Classifier performance may not correlate with practical data utility for downstream tasks
- Interpretability depends on the specific XAI method used, with different methods yielding varying insights
- The approach assumes distinguishing patterns indicate flaws, though some differences might reflect legitimate statistical variations

## Confidence

**High confidence**: General utility of XAI for synthetic data evaluation
**Medium confidence**: Specific methodological details and implementation
**Low confidence**: Transferability across different data domains and generative models

## Next Checks

1. Conduct experiments comparing XAI-based evaluation results with actual downstream task performance to verify correlation between distinguishability and practical utility
2. Test multiple XAI methods (e.g., LIME, SHAP variants, feature importance techniques) on the same datasets to assess consistency of insights
3. Apply the approach to synthetic data generated by different model architectures to evaluate robustness across various generative approaches