---
ver: rpa2
title: On shallow feedforward neural networks with inputs from a topological space
arxiv_id: '2504.02321'
source_url: https://arxiv.org/abs/2504.02321
tags:
- neural
- functions
- networks
- theorem
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces feedforward neural networks with inputs\
  \ from a topological space (TFNNs) and establishes their universal approximation\
  \ property. The main result shows that for any topological space X, a subclass A(X)\
  \ with the D-property, and a Tauber-Wiener activation function \u03C3, TFNNs can\
  \ approximate any continuous function on compact subsets of X arbitrarily well."
---

# On shallow feedforward neural networks with inputs from a topological space

## Quick Facts
- arXiv ID: 2504.02321
- Source URL: https://arxiv.org/abs/2504.02321
- Reference count: 38
- Introduces feedforward neural networks with inputs from topological spaces (TFNNs) and establishes their universal approximation property

## Executive Summary
This paper generalizes the universal approximation theorem to feedforward neural networks with inputs from arbitrary topological spaces. The key insight is replacing standard linear input transformations with a basic family of continuous functions from the input space to R. The D-property ensures this family, combined with Tauber-Wiener activation functions, is dense in the space of continuous functions. The work extends Kolmogorov's superposition theorem to compact metric spaces using superactivation functions that can approximate any univariate continuous function with a single neuron.

## Method Summary
The method establishes that feedforward neural networks with topological space inputs can approximate any continuous function on compact subsets if the basic family of input functions satisfies the D-property. The architecture uses continuous functions from the input space to R instead of standard linear transformations, with Tauber-Wiener activation functions ensuring universal approximation. The proof relies on Stone-Weierstrass theorem for locally convex spaces and constructs superactivation functions that are infinitely differentiable while maintaining universal approximation capability.

## Key Results
- TFNNs can approximate any continuous function on compact subsets of a topological space X if the basic family A(X) satisfies the D-property
- For locally convex topological vector spaces, continuous linear functionals form a basic family satisfying the D-property
- Superactivation functions can be constructed that are infinitely differentiable and allow any univariate continuous function to be approximated by a single neuron
- An approximative version of Kolmogorov's superposition theorem is derived for compact metric spaces using superactivation functions

## Why This Works (Mechanism)

### Mechanism 1
Feedforward networks can approximate functions on non-Euclidean topological spaces if the feature extraction family satisfies the "D-property." The architecture replaces standard linear weight multiplication with a basic family of functions A(X) from the input space X to R. If A(X) is dense in C(X) when composed with univariate continuous functions (the D-property), the network can generate the necessary variety of basis signals to approximate any target continuous function g.

### Mechanism 2
The universal approximation capability is preserved by using Tauber-Wiener (TW) activation functions. TW functions have spans dense in the space of continuous univariate functions. The proof chains this univariate density with the multivariate density provided by A(X): first approximate g via u_i(v_i(x)), then approximate u_i via σ.

### Mechanism 3
Infinite differentiability can be achieved without sacrificing universal approximation by constructing "superactivation" functions. A superactivation function σ is constructed algorithmically such that a single shifted/scaled instance of σ can approximate any univariate continuous function, allowing Kolmogorov's theorem to be approximated with fixed, smooth outer functions.

## Foundational Learning

- **Concept:** Topological Spaces and Density
  - **Why needed here:** Generalizes inputs from R^d to arbitrary topological spaces X. Understanding "open sets" and "compactness" is essential for grasping why the D-property ensures the network can "reach" all points in C(X).
  - **Quick check question:** Can you explain why the density of a set of functions in C(X) depends on the topology of uniform convergence on compact sets?

- **Concept:** Stone-Weierstrass Theorem
  - **Why needed here:** This theorem is the engine behind the D-property verification. The paper uses it to prove that exponential functions of linear functionals are dense in C(X) for locally convex spaces.
  - **Quick check question:** Does a subalgebra of C(X) that separates points and contains constants satisfy the conditions of the Stone-Weierstrass theorem?

- **Concept:** Kolmogorov Superposition Theorem (KST)
  - **Why needed here:** The paper uses KST as a primary application, showing how TFNNs can realize an approximate version for compact metric spaces.
  - **Quick check question:** How does KST relate multivariate functions to sums of univariate functions, and why is the smoothness of these univariate functions typically a problem?

## Architecture Onboarding

- **Component map:** Input X -> Basic Family A(X) -> Activation σ -> Linear Combination
- **Critical path:** Defining the basic family A(X) for your specific input space. The architecture fails if A(X) does not satisfy the D-property.
  - *Action:* If X is a locally convex topological vector space, use continuous linear functionals X* as A(X).
  - *Action:* If X is a product of metric spaces, use sums of continuous maps to [0,1].

- **Design tradeoffs:**
  - **Generality vs. Tractability:** While theory applies to any topological space, implementation requires computing functions in A(X). Linear functionals are easy; arbitrary dense subsets of C(X) may be hard to construct.
  - **Activation Choice:** Standard non-polynomial σ works, but Superactivation functions enable fixed-neuron architectures at the cost of complex, algorithmically defined activations.

- **Failure signatures:**
  - **Polynomial Activation:** If σ is a polynomial, universal approximation fails.
  - **Insufficient Feature Family:** If A(X) is too small, the D-property fails, and the network cannot approximate arbitrary g.
  - **Non-Locally Convex Input:** If X is not locally convex, using X* as A(X) might not satisfy the D-property.

- **First 3 experiments:**
  1. **Euclidean Baseline:** Set X = R^d and A(X) = L(R^d). Implement standard shallow network to recover classical Universal Approximation Theorem.
  2. **Infinite-Dimensional Input:** Let X be C([0,1]). Define A(X) as evaluation functionals δ_t(f) = f(t). Test approximation of functional G(f).
  3. **Superactivation Construction:** Implement algorithm from Proposition 2.1 to construct smooth superactivation function σ. Test if single neuron σ(wx - θ) can approximate both x² and sin(x) on [0,1].

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Universal Approximation Theorem for TFNNs be extended to neural networks with outputs in infinite-dimensional spaces?
  - **Basis in paper:** Explicit statement that current techniques are insufficient for infinite-dimensional output spaces.
  - **Why unresolved:** Current proofs rely on output space being the real line R, and mathematical tools don't transfer directly.
  - **What evidence would resolve it:** A theoretical proof demonstrating TFNNs are dense in the space of continuous functions mapping X to an infinite-dimensional Banach or Hilbert space.

- **Open Question 2:** Can the approximation results be generalized to hypercomplex-valued neural networks?
  - **Basis in paper:** Explicit mention hoping results will stimulate exploration of these neural networks.
  - **Why unresolved:** Paper restricts output space strictly to R, leaving validity for hypercomplex algebras unproven.
  - **What evidence would resolve it:** Deriving a variant of Theorem 2.1 that holds for activation functions and weight parameters defined over hypercomplex algebras.

- **Open Question 3:** What are the quantitative approximation rates (complexity bounds) for TFNNs on specific classes of topological spaces?
  - **Basis in paper:** Inferred from paper establishing qualitative results without quantitative estimates.
  - **Why unresolved:** Theorem 2.1 relies on Stone-Weierstrass theorem and density properties, which are existential rather than constructive.
  - **What evidence would resolve it:** Explicit bounds relating hidden layer width to approximation error ε and the dimension or geometry of compact set K ⊂ X.

## Limitations

- The D-property requirement is abstract and doesn't provide concrete constructions of A(X) for general topological spaces beyond specific cases.
- Superactivation function construction involves algorithmically dense sequences that are not explicitly specified, making numerical verification challenging.
- The paper establishes qualitative approximation results without providing quantitative bounds on the number of neurons required.

## Confidence

- **High confidence:** The mechanism by which Tauber-Wiener activation functions preserve universal approximation is well-established in the literature.
- **Medium confidence:** The general TFNN framework and D-property definition are mathematically rigorous, but practical applicability depends on finding suitable A(X) families.
- **Medium confidence:** The superactivation function construction is valid in principle, but numerical implementation would require careful handling of dense polynomial sequences.

## Next Checks

1. Implement the superactivation function σ(t) using a concrete enumeration of rational polynomials (e.g., truncated Taylor series) and test whether a single neuron can approximate both polynomial and transcendental functions on a bounded interval.

2. Verify the D-property for a concrete non-Euclidean space by selecting X = C([0,1]) and A(X) = {evaluation functionals δ_t}. Demonstrate that compositions σ(w·f(x) - θ) with f ∈ A(X) can approximate a target functional G(f).

3. Test the failure mode by implementing a shallow network with polynomial activation (e.g., σ(x) = x²) and show it cannot approximate non-polynomial continuous functions, confirming the necessity of non-polynomial TW functions.