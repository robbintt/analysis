---
ver: rpa2
title: Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot
  Class-Incremental Learning
arxiv_id: '2507.13739'
source_url: https://arxiv.org/abs/2507.13739
tags:
- synthetic
- images
- learning
- class
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates synthetic image replay
  strategies for Few-Shot Class-Incremental Learning (FSCIL). The authors examine
  three key aspects: quantity of synthetic images per class, generative strategy selection,
  and timing of synthetic image integration during base-session training.'
---

# Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2507.13739
- Source URL: https://arxiv.org/abs/2507.13739
- Authors: Junsu Kim; Yunhoe Ku; Seungryul Baek
- Reference count: 40
- Key outcome: Optimization-based synthetic replay (Textual Inversion) achieves 55.28% last-session accuracy on CUB-200, outperforming simpler generative approaches by embedding detailed class-specific semantic information

## Executive Summary
This paper systematically investigates synthetic image replay strategies for Few-Shot Class-Incremental Learning (FSCIL), examining quantity, generative strategy selection, and timing of synthetic image integration. Through extensive experiments on CUB-200 and miniImageNet, the authors demonstrate that optimization-based methods like Textual Inversion consistently outperform simpler generative approaches by embedding detailed class-specific semantic information. The study reveals that synthetic replay effectiveness depends on both quantity and semantic richness, not quantity alone, and that timing analysis uncovers dataset-dependent trade-offs between representational diversity and catastrophic forgetting mitigation.

## Method Summary
The authors systematically investigate synthetic image replay in FSCIL by testing three key aspects: quantity of synthetic images per class, generative strategy selection, and timing of synthetic image integration during base-session training. They employ a ResNet-18 backbone trained only on the base session, then incrementally add classes using real exemplars (1/class) plus synthetic buffers (5/class). The study compares Textual Inversion, prompt-based generation, GLIGEN, InstructPix2Pix, and DreamBooth+LoRA methods across CUB-200 and miniImageNet datasets, with synthetic images generated at 512×512 resolution using frozen Stable Diffusion v1.5.

## Key Results
- Textual Inversion achieves 55.28% last-session accuracy and 63.42% average accuracy on CUB-200
- Semantic richness matters more than quantity alone - poorly-aligned synthetic images degrade performance as count increases
- Dataset-dependent timing trade-offs: CUB-200 benefits from staged integration while miniImageNet performs best with real images only
- Optimization-based methods outperform naive prompting by explicitly encoding class-specific visual semantics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimization-based generation (Textual Inversion) outperforms naive prompting because it explicitly encodes class-specific visual semantics into learnable embeddings.
- Mechanism: Textual Inversion optimizes a unique token embedding per class using few-shot examples. This embedding captures distinctive visual features that generic class-name prompts cannot encode, enabling synthetic images that preserve class identity across incremental sessions.
- Core assumption: Few-shot examples sufficiently represent the visual distribution of each class for embedding optimization.
- Evidence anchors:
  - [abstract]: "optimization-based methods like Textual Inversion consistently outperform simpler generative approaches by embedding detailed class-specific semantic information"
  - [section 6.2, Table 2]: Textual Inversion achieves 55.28% last-session accuracy vs. 48.36% for class-name prompts on CUB-200
  - [corpus]: Related work "Beyond Synthetic Replays" explores diffusion features but uses different approach; limited direct comparison available
- Break condition: When few-shot examples are unrepresentative or when domain gap between synthetic and real images is too large (observed on miniImageNet's low-resolution images).

### Mechanism 2
- Claim: Increasing synthetic image quantity improves replay effectiveness only when images carry sufficient semantic alignment; quantity alone degrades performance.
- Mechanism: Naive methods generate images with weak semantic binding to target classes. Adding more such images introduces noise rather than useful signal, while semantically-aligned synthetic images (from Textual Inversion) show stable or improving accuracy with quantity.
- Core assumption: Semantic fidelity of synthetic images determines their replay utility more than raw count.
- Evidence anchors:
  - [abstract]: "effectiveness of the synthetic replay strongly depends on both the quantity and semantic richness of generated images, rather than quantity alone"
  - [section 6.1]: GLIGEN, InstructPix2Pix, and prompt-based methods show declining performance as synthetic count increases; Textual Inversion shows stable improvement
  - [corpus]: No direct corpus evidence on quantity-semantics interaction in this context
- Break condition: When synthetic images lack class-specific semantic grounding, adding more samples actively harms incremental learning.

### Mechanism 3
- Claim: Timing of synthetic integration during base training creates dataset-dependent trade-offs between representational diversity and catastrophic forgetting mitigation.
- Mechanism: Synthetic images serve dual roles—augmentation during base training (enhancing diversity) and replay during incremental sessions (mitigating forgetting). Early/large-scale synthetic integration improves base representations but may introduce domain shift; optimal balance depends on dataset characteristics.
- Core assumption: Synthetic images can augment backbone training without introducing harmful distribution shift.
- Evidence anchors:
  - [abstract]: "timing analysis uncovers dataset-dependent trade-offs between representational diversity and catastrophic forgetting mitigation"
  - [section 6.3, Table 4]: CUB-200 benefits from staged integration R(1st)→R+S(2nd) (57.46% AA); miniImageNet performs best with R(full) only (51.75% AA)
  - [corpus]: No corpus evidence on synthetic timing analysis
- Break condition: On low-resolution or visually complex datasets, synthetic images introduce domain shift that outweighs diversity benefits.

## Foundational Learning

- Concept: **Few-Shot Class-Incremental Learning (FSCIL) Protocol**
  - Why needed here: Defines the session structure (base → incremental), exemplar constraints, and evaluation metrics that all experiments build upon.
  - Quick check question: Why does the backbone freeze after the base session, and what constraint does this impose on incremental learning?

- Concept: **Textual Inversion in Diffusion Models**
  - Why needed here: Understanding how token embeddings capture visual concepts is essential for comprehending why this approach outperforms prompt engineering.
  - Quick check question: What gets optimized during Textual Inversion—the diffusion model weights or the text encoder embeddings?

- Concept: **Stability-Plasticity Dilemma in Replay Methods**
  - Why needed here: Synthetic replay attempts to balance retaining old knowledge (stability) while adapting to new classes (plasticity).
  - Quick check question: Why does increasing synthetic replay quantity with poorly-aligned images hurt rather than help stability?

## Architecture Onboarding

- Component map:
  ```
  [Stable Diffusion v1.5] → frozen generator for all strategies
       ↓
  [Textual Inversion] → optimized embeddings per class (5-7 tokens)
       ↓
  [Synthetic Image Generation] → 512×512 images, stored in buffer
       ↓
  [ResNet-18 Backbone] → trained base session only, frozen thereafter
       ↓
  [Prototype Classifier] → pre-defined prototypes, updated incrementally
  ```

- Critical path:
  1. **Base session**: Train ResNet-18 on real images (optionally + synthetic for augmentation timing experiments)
  2. **Per incremental class**: Optimize Textual Inversion embedding using 5-shot examples
  3. **Generate replay**: Create 5 synthetic images per class using optimized embeddings
  4. **Incremental session**: Update classifier using new samples + real buffer (1/class) + synthetic buffer (5/class)

- Design tradeoffs:
  - **Textual Inversion vs DreamBooth**: TI is parameter-efficient (embeddings only); DreamBooth+LoRA captures more visual detail but occasionally misses nuances
  - **Timing strategies**: Full synthetic integration (R+S(full)) minimizes forgetting; staged integration (R(1st)→R+S(2nd)) maximizes diversity
  - **Resolution sensitivity**: CUB-200 (224×224 pretraining) benefits more from synthetic augmentation than miniImageNet (84×84)

- Failure signatures:
  - **Accuracy drops as synthetic count increases** → Generation method lacks semantic grounding; switch to optimization-based approach
  - **High base accuracy, high PD** → Synthetic timing prioritized diversity over forgetting; try R+S(full) strategy
  - **MiniImageNet underperforms with synthetic** → Domain shift from 512→84 resolution; consider real-only or reduced synthetic

- First 3 experiments:
  1. Reproduce quantity ablation (Fig. 3): Vary synthetic images 1→5 per class on CUB-200, comparing Textual Inversion vs class-name prompt to verify semantic-quantity interaction.
  2. Reproduce timing analysis (Table 4): Test all four integration schedules on your target dataset to identify dataset-specific optimal timing.
  3. Qualitative validation: Generate samples with each strategy, visually confirm that Textual Inversion preserves class identity better than naive prompts (compare against reference few-shot images).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would combining synthetic image replay with additional domain-alignment techniques (e.g., feature distillation, regularization losses) affect FSCIL performance compared to synthetic replay alone?
- Basis in paper: [explicit] The authors state they "omitted these additional methods" typical of CL approaches, "limiting direct comparability with state-of-the-art methods."
- Why unresolved: The study intentionally isolated synthetic image effects by excluding commonly used domain-alignment mechanisms, leaving their synergistic potential unexplored.
- What evidence would resolve it: Experiments combining Textual Inversion synthetic replay with knowledge distillation or regularization losses, compared against both the isolated approach and state-of-the-art FSCIL methods.

### Open Question 2
- Question: Can adaptive hyperparameter schedules (learning rates, iteration counts) tailored to each generative strategy further improve synthetic replay effectiveness?
- Basis in paper: [explicit] The authors acknowledge "adapting these parameters could potentially enhance performance" but maintained fixed hyperparameters "to rigorously isolate synthetic image effects."
- Why unresolved: The controlled experimental design prioritized fair comparison over optimal per-method tuning, leaving potential gains from strategy-specific hyperparameter optimization unknown.
- What evidence would resolve it: Ablation studies with optimized hyperparameters for each generative strategy (e.g., Textual Inversion vs. DreamBooth), comparing against the fixed-configuration baseline.

### Open Question 3
- Question: What mechanisms determine the dataset-dependent trade-offs between representational diversity and catastrophic forgetting when integrating synthetic images during base training?
- Basis in paper: [explicit] The timing analysis revealed "dataset-dependent trade-offs" where CUB-200 benefited from two-stage integration while miniImageNet performed best with real images only.
- Why unresolved: The paper identifies the phenomenon but does not fully explain why lower-resolution, multi-object datasets like miniImageNet experience detrimental domain shifts from synthetic augmentation.
- What evidence would resolve it: Systematic analysis across datasets with controlled variations in resolution, object complexity, and semantic granularity, measuring both representation quality and forgetting rates.

## Limitations

- The study shows significant dataset dependency, with miniImageNet exhibiting weaker synthetic replay benefits due to resolution mismatch between generated (512×512) and original (84×84) images.
- Semantic-quantity interaction findings require validation across additional datasets and domain types beyond fine-grained and general image datasets.
- Timing analysis reveals dataset-specific trade-offs but lacks theoretical grounding for why certain schedules work better on CUB-200 versus miniImageNet.

## Confidence

- **High Confidence**: Semantic richness-quantity interaction (Mechanism 2) - supported by clear ablation patterns across multiple methods
- **Medium Confidence**: Textual Inversion superiority (Mechanism 1) - strong empirical evidence but limited comparison to newer generative approaches
- **Medium Confidence**: Timing strategy effectiveness (Mechanism 3) - dataset-dependent results require broader validation
- **Low Confidence**: Generalization across domains - current evidence limited to fine-grained and general image datasets only

## Next Checks

1. **Resolution Robustness Test**: Conduct controlled experiments varying synthetic image resolutions (224×224, 128×128, 84×84) on miniImageNet to quantify the domain shift penalty and identify optimal resolution for low-resolution datasets.

2. **Cross-Domain Generalization**: Apply the complete methodology (Textual Inversion + timing analysis) to non-natural image datasets (e.g., medical imaging, satellite imagery) to test whether semantic-quantity and timing findings generalize beyond the current domain.

3. **Longitudinal Retention Study**: Extend the FSCIL protocol to include additional incremental sessions (5-10 classes) to measure whether synthetic replay benefits persist over longer learning trajectories or if catastrophic forgetting re-emerges despite synthetic buffers.