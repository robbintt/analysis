---
ver: rpa2
title: Data Scaling Laws for Radiology Foundation Models
arxiv_id: '2509.12818'
source_url: https://arxiv.org/abs/2509.12818
tags:
- rad-dino
- arxiv
- report
- pretraining
- tube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies scaling laws for pretraining radiology foundation
  models, comparing CLIP-style (MedImageInsight) and DINOv2-style (RAD-DINO) encoders.
  Both are continually pretrained on up to 3.5M chest X-rays, with equal compute and
  evaluation.
---

# Data Scaling Laws for Radiology Foundation Models

## Quick Facts
- arXiv ID: 2509.12818
- Source URL: https://arxiv.org/abs/2509.12818
- Reference count: 20
- Pretraining radiology vision encoders on up to 3.5M CXRs shows CLIP-style (MedImageInsight) scales better for findings classification while DINOv2-style (RAD-DINO) excels at lines-and-tubes tasks; structured labels via UniCL improve CLIP performance.

## Executive Summary
This work systematically studies data scaling laws for radiology foundation models by continually pretraining two vision encoder paradigms—CLIP-style MedImageInsight (MI2) and DINOv2-style RAD-DINO—on increasing subsets of a large chest X-ray dataset. Both models are evaluated across multiple tasks including findings classification, lines/tubes classification, segmentation, and report generation, with equal compute budgets. The study reveals that MI2 scales more effectively for classification tasks while RAD-DINO is stronger for segmentation, with structured supervision via UniCL providing additional benefits for CLIP-style models.

## Method Summary
The authors continually pretrain open-weights MI2 (DA-ViT 360M) and RAD-DINO (ViT-B 87M) on increasing subsets (30k to 3.5M) of a 4M-image chest X-ray dataset from Mayo Clinic, using equal wall-clock time budgets. MI2 uses UniCL training (image-text + image-label contrastive), while RAD-DINO uses DINOv2 self-distillation. Evaluation uses frozen feature extraction with linear probes for classification/segmentation and MAIRA-2 for report generation. Labels (19 findings, 11 tubes) are extracted from reports using GPT-4o. Power-law scaling curves are fitted to quantify task-specific scaling behavior.

## Key Results
- MI2 scales about 3x better than RAD-DINO for findings classification tasks
- RAD-DINO outperforms MI2 on lines-and-tubes segmentation due to its self-supervised learning bias
- Adding structured labels via UniCL improves MI2 performance, especially for tube-related tasks
- As few as 30k in-domain samples are sufficient to surpass open-weights foundation models
- Center-specific pretraining yields significant gains, though performance degrades on out-of-domain benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Pretraining paradigm determines task-specific scaling efficiency for medical vision encoders. CLIP-style contrastive learning (MI2) aligns global image-text features, favoring classification and retrieval tasks. DINOv2-style self-distillation with masked objectives (RAD-DINO) learns curve-continuity features—representations preserving continuity along elongated structures—benefiting segmentation and tube-localization tasks. Core assumption: Task performance reflects the inductive biases of the pretraining objective, not just data scale. Evidence: MI2 scales three times better than RAD-DINO for findings classification, while RAD-DINO excels at tube-related tasks. Break condition: Tasks requiring both global semantic understanding and dense spatial precision need hybrid approaches.

### Mechanism 2
Center-specific continual pretraining enables in-domain adaptation with surprisingly few samples. Open-weight models train on heterogeneous, multi-source data. Pretraining on in-domain data reduces distribution shift from scanner protocols, population demographics, and labeling practices, improving feature relevance for downstream tasks. Core assumption: In-domain samples provide higher information density for the target distribution than general-purpose pretraining data. Evidence: MI2 with 30k samples significantly outperforms open-weights MI2 for rib fracture classification, though trends reverse on out-of-domain VinDR benchmarks. Break condition: Benefits diminish when target task distribution diverges significantly from pretraining domain.

### Mechanism 3
Structured label supervision via UniCL improves CLIP-style models even with abundant image-text pairs. UniCL extends contrastive learning to image-label pairs, treating categorical labels as text inputs. Explicit tube-presence labels provide spatial and semantic signals often missing or underspecified in free-text reports. Core assumption: Structured labels encode cleaner, more consistent supervision than extracted text representations alone. Evidence: MI2 with reports+tube labels outperforms MI2 with reports alone, especially for low-prevalence tubes, and UniCL closes the gap between MI2 and RAD-DINO for lines-and-tubes segmentation. Break condition: Noisy label extraction degrades gains, as GPT-based extraction has F1=0.94.

## Foundational Learning

- **Concept: CLIP vs. DINOv2 pretraining paradigms**
  - Why needed here: Understanding the inductive biases of contrastive (image-text) vs. self-supervised (image-only) learning explains task-specific scaling differences.
  - Quick check question: Can you explain why CLIP typically excels at classification while DINOv2 excels at segmentation?

- **Concept: Continual pretraining**
  - Why needed here: The paper uses continual pretraining (fine-tuning from open checkpoints) rather than training from scratch; understanding this distinction is critical for replication.
  - Quick check question: What is the difference between continual pretraining and fine-tuning on downstream tasks?

- **Concept: Power-law scaling**
  - Why needed here: The paper fits performance vs. dataset size to power-law curves; understanding scaling law assumptions helps interpret when trends hold or break.
  - Quick check question: What conditions cause power-law scaling to saturate or reverse?

## Architecture Onboarding

- **Component map:**
  - MI2: DA-ViT backbone (360M params) → UniCL training (image-report + image-label contrastive)
  - RAD-DINO: ViT-Base (87M params) → DINOv2 self-distillation with masked image modeling
  - Evaluation heads: Linear classifiers for frozen features; feature pyramid + linear head for segmentation; MAIRA-2 (LLM) for report generation

- **Critical path:**
  1. Data prep: CXR PNGs + reports → GPT-based label extraction (findings + tubes)
  2. Checkpoint loading: Open-weights MI2 or RAD-DINO
  3. Continual pretraining: Vary dataset sizes (30k–3.5M), equal wall-clock time
  4. Frozen evaluation: Extract multi-layer features → attention pooling → task heads

- **Design tradeoffs:**
  - MI2 vs. RAD-DINO: Classification vs. segmentation bias; DA-ViT vs. ViT-Base capacity
  - Reports-only vs. reports+labels: UniCL adds supervision but requires label extraction pipeline
  - Layer selection: Multi-layer features better for classification; final-layer better for generation

- **Failure signatures:**
  - Inverse scaling on out-of-domain benchmarks (VinDR) → catastrophic forgetting, domain shift
  - Small benchmark saturation → unclear scaling trends below 100k samples
  - Noisy labels from GPT extraction → label noise degrades rare-class performance

- **First 3 experiments:**
  1. Replicate findings classification scaling curve on a held-out split; verify ~3x slope difference between MI2 and RAD-DINO.
  2. Ablate UniCL label supervision: train MI2 with reports-only vs. reports+tube-labels; measure delta on tube classification.
  3. Test domain sensitivity: pretrain on subset from one scanner manufacturer, evaluate on another; quantify performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
What is the effective saturation point for scaling medical vision encoders on single-institution chest X-ray data? The study only scales to 3.5M samples, while extrapolated requirements of billions exceed current medical imaging dataset sizes.

### Open Question 2
To what extent do findings about CLIP vs. DINOv2 scaling generalize to medical imaging domains beyond chest X-rays? The study is restricted to chest X-rays and does not validate on other imaging modalities like CT, MRI, or pathology slides.

### Open Question 3
How robust are scaling laws under domain shift between training and evaluation distributions? RAD-DINO exhibits reversed scaling trends and catastrophic forgetting on out-of-domain VinDR benchmarks due to distribution shift.

### Open Question 4
Can data selection strategies (active learning, data filtering) improve scaling efficiency beyond brute-force data collection? The authors conclude that simply collecting more data is not effective and suggest prioritizing underrepresented tasks via data selection.

## Limitations

- Single-institution dataset may limit generalizability across different scanner protocols, populations, and institutional practices
- GPT-based label extraction introduces potential noise and systematic biases that could affect model performance
- Limited evaluation on out-of-domain benchmarks shows domain shift can reverse scaling benefits, suggesting narrow applicability
- The study focuses exclusively on chest X-rays without validating findings on other medical imaging modalities

## Confidence

- **High Confidence**: Task-specific scaling differences between MI2 and RAD-DINO paradigms are well-supported by controlled experiments
- **Medium Confidence**: The claim that 30k in-domain samples suffice to surpass open-weights models requires qualification by domain similarity
- **Low Confidence**: The mechanism by which UniCL structured labels improve performance lacks external validation

## Next Checks

1. **Domain Transfer Test**: Pretrain MI2 on Mayo Clinic data, then evaluate on completely independent datasets (different institutions, countries, scanner manufacturers). Measure whether the 30k-sample advantage persists or inverts, quantifying domain shift effects.

2. **Label Quality Ablation**: Replace GPT-extracted labels with manual annotations on a subset of data. Retrain MI2 with UniCL and compare performance against GPT labels to isolate the contribution of label quality versus quantity.

3. **Hybrid Pretraining Experiment**: Combine contrastive (MI2-style) and self-supervised (RAD-DINO-style) objectives in a single pretraining regime. Evaluate whether this hybrid approach can achieve the best of both paradigms across all task types, testing the hypothesis that task-specific scaling reflects fundamental pretraining biases.