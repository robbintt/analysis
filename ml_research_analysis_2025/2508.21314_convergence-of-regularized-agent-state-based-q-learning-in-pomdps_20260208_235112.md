---
ver: rpa2
title: Convergence of regularized agent-state-based Q-learning in POMDPs
arxiv_id: '2508.21314'
source_url: https://arxiv.org/abs/2508.21314
tags:
- state
- policy
- learning
- convergence
- regularized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves convergence of regularized Q-learning algorithms
  for partially observable Markov decision processes (POMDPs) using agent states rather
  than belief states. The key insight is that even when the agent state is not a sufficient
  statistic, Q-learning with policy regularization converges to a fixed point defined
  by an appropriately constructed regularized MDP.
---

# Convergence of regularized agent-state-based Q-learning in POMDPs

## Quick Facts
- arXiv ID: 2508.21314
- Source URL: https://arxiv.org/abs/2508.21314
- Reference count: 40
- This paper proves convergence of regularized Q-learning algorithms for partially observable Markov decision processes (POMDPs) using agent states rather than belief states.

## Executive Summary
This paper establishes convergence guarantees for regularized Q-learning in partially observable Markov decision processes when using agent states (like RNN hidden states or frame stacks) instead of belief states. The key insight is that by replacing the standard max operator with the gradient of a convex conjugate (via Legendre-Fenchel duality), the Bellman operator becomes a contraction mapping, ensuring convergence to a unique fixed point. The analysis shows that even when the agent state is not a sufficient statistic for the POMDP, the algorithm converges to a well-defined limit determined by the exploration policy's stationary distribution. The framework extends to periodic policies, which can capture cyclic behaviors better than stationary policies in certain environments.

## Method Summary
The method introduces Regularized Agent-State-based Q-Learning (RASQL) that replaces the standard max operator in Q-learning updates with a smoothed version derived from the convex conjugate of a regularizer (typically entropy). For agent state $z_t$ and action $a_t$, the update rule is $Q_{t+1}(z_t, a_t) \leftarrow (1-\alpha_t)Q_t(z_t, a_t) + \alpha_t[r_t + \gamma \Omega^*(Q_t(z_{t+1}, \cdot))]$, where $\Omega^*$ is the convex conjugate of the regularizer $\Omega$. The paper proves this converges to a fixed point defined by an artificial MDP constructed from the limiting distribution of the behavior policy. A periodic variant (RePASQL) extends this to cyclic policies by applying different Bellman operators in sequence.

## Key Results
- Proves convergence of RASQL to a unique fixed point defined by the limiting distribution of the behavior policy
- Shows the regularized Bellman operator is a contraction mapping via Legendre-Fenchel duality
- Extends convergence guarantees to periodic policies with cyclic fixed points
- Demonstrates convergence on a small 4-state POMDP with agent states as observations
- Establishes that the converged Q-values match theoretical limits derived from model parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Regularization stabilizes Q-learning in non-Markovian environments by transforming the Bellman update into a contraction mapping via convex duality.
- **Mechanism:** Standard Q-learning fails in POMDPs with agent states because the "max" operator is not a contraction in the non-Markovian setting. This paper replaces the hard max with the gradient of the convex conjugate ($\nabla \Omega^\star$), which acts as a smooth soft-max operator. This smoothing ensures the Bellman operator $\mathcal{B}^\Omega$ contracts to a unique fixed point, regardless of the underlying environment's partial observability.
- **Core assumption:** The regularizer $\Omega$ must be strongly convex and twice differentiable (e.g., entropy or KL-divergence) so that $\nabla \Omega^\star$ is Lipschitz continuous.
- **Evidence anchors:** [abstract] Mentions transforming the problem into a "well-behaved optimization framework via Legendre-Fenchel duality." [Section II-B] Defines the regularized Bellman operator and Proposition 1 establishes it as a contraction. [corpus] The paper "Regularized Gradient Temporal-Difference Learning" supports the general stability benefits of regularization in approximate settings.
- **Break condition:** If the regularization strength approaches zero (recovering standard Q-learning) or if the regularizer is non-convex, the contraction property is lost.

### Mechanism 2
- **Claim:** Convergence relies on the ergodicity of the behavior policy rather than the optimality of the agent state representation.
- **Mechanism:** Since the agent state $z_t$ is not a sufficient statistic, the system is non-Markovian. The algorithm sidesteps this by converging to the fixed point of an "artificial" MDP defined by the stationary distribution $\zeta^\mu$ of the exploration policy $\mu$. The Q-values effectively learn the expected value of actions conditioned on the agent state under this specific exploration distribution.
- **Core assumption:** The behavior policy must induce a Markov chain that converges to a limiting stationary distribution $\zeta^\mu$ where all state-action pairs are visited infinitely often.
- **Evidence anchors:** [Section IV] Theorem 1 explicitly constructs the reward $r^\mu$ and transition $P^\mu$ based on the limiting distribution $\zeta^\mu$. [Section I] Notes that standard DP techniques fail, necessitating this alternative fixed-point analysis.
- **Break condition:** If the behavior policy does not have full support or the environment+agent-state chain is non-ergodic, the Q-values will not converge to the defined fixed point.

### Mechanism 3
- **Claim:** Periodic policies enable convergence to cyclic fixed points, potentially capturing superior performance in environments with memory.
- **Mechanism:** The paper extends the stationary analysis to periodic policies by treating the system as having $L$ distinct time-homogenized phases. The update rule cycles through $L$ Bellman operators. Because the composition of these operators remains a contraction, the Q-estimates converge to a limit cycle (periodic fixed point) rather than a single stationary point.
- **Core assumption:** The underlying Markov chain must admit a limiting periodic distribution.
- **Evidence anchors:** [Section V] Defines the periodic Bellman operator $\mathcal{B}^\ell_\mu$ and Theorem 2 proves convergence. [Section I] States that periodic policies "may perform better when the agent state is not an information state."
- **Break condition:** If the period $L$ is misaligned with the environment's dynamics, or if the learning rates $\alpha^\ell_t$ are not synchronized with the phase $\ell$, the cyclic fixed point may not be found.

## Foundational Learning

- **Concept: Legendre-Fenchel Duality (Convex Conjugates)**
  - **Why needed here:** This is the mathematical engine that converts the regularization penalty $\Omega(\pi)$ into the smoothed value update $\Omega^\star(Q)$. Without understanding this, the appearance of "soft-max" in the update rule is opaque.
  - **Quick check question:** Can you explain why $\nabla \Omega^\star(q)$ yields a probability distribution (the policy) when $\Omega$ is the entropy function?

- **Concept: Agent State vs. Belief State**
  - **Why needed here:** The paper's central tension is that the agent (using RNNs or frame-stacking) does *not* know the true probability distribution over hidden states (Belief). Distinguishing "Agent State" (model-free recursive summary) from "Belief State" (sufficient statistic) is critical for understanding why standard MDP proofs fail here.
  - **Quick check question:** Why does the lack of a "sufficient statistic" prevent the use of standard Dynamic Programming decomposition?

- **Concept: Stochastic Approximation (Robbins-Monro conditions)**
  - **Why needed here:** The proof of convergence relies on standard stochastic approximation assumptions (learning rates $\sum \alpha = \infty$ and $\sum \alpha^2 < \infty$).
  - **Quick check question:** Why must the learning rate decrease slowly enough to sum to infinity, but fast enough to square-sum to a finite number?

## Architecture Onboarding

- **Component map:** POMDP Environment -> Agent State Generator ($\phi$) -> Behavior Policy ($\mu$) -> Regularized Q-Table -> Soft-Target Policy ($\pi$)

- **Critical path:** Sample $(z, a, r, z') \to$ Compute smoothed target $V(z') = \Omega^\star(Q(z', \cdot)) \to$ Update $Q(z, a)$ towards $r + \gamma V(z')$

- **Design tradeoffs:**
  - **Convergence vs. Optimality:** The architecture guarantees convergence to *a* fixed point $Q^\mu$ defined by the exploration policy, but explicitly **does not guarantee** convergence to the optimal POMDP policy.
  - **Agent State Complexity:** A richer agent state (e.g., larger RNN) better approximates a belief state but increases the variance of the stochastic approximation.

- **Failure signatures:**
  - **Divergence:** Caused by overly aggressive learning rates or lack of coverage from the behavior policy.
  - **Sub-optimal Convergence:** Converging to $Q^\mu$ which is "safe" but uninteresting, if $\mu$ is poorly designed or the agent state $z$ is insufficient to distinguish critical environmental modes.

- **First 3 experiments:**
  1. **Validation of Fixed Point:** Run RASQL on the 4-state POMDP provided in Section VI. Plot the empirical Q-iterates against the theoretical limit $Q^\mu$ calculated via Eq. (4) to verify the implementation of the Legendre-Fenchel transform.
  2. **Impact of Regularization ($\beta$):** Sweep the regularization coefficient $\beta$ (temperature). Verify that as $\beta \to 0$ (high regularization), the policy becomes uniform; as $\beta \to \infty$, the policy should approach deterministic behavior (and potentially become unstable/non-convergent).
  3. **Stationary vs. Periodic:** Implement RePASQL (Periodic) on an environment with clear cyclical dynamics. Compare the learned value limits $\{Q^\ell_\mu\}$ against the stationary limit to observe if periodicity captures additional value.

## Open Questions the Paper Calls Out

- **Question:** Can RASQL be modified to guarantee convergence to the optimal agent-state-based policy rather than a fixed point dependent on the behavioral policy?
  - **Basis in paper:** [explicit] The authors state in Section VII: "we cannot guarantee the convergence to the optimal agent-state-based solution and this largely depends on the choice of exploration policy."
  - **Why unresolved:** The analysis proves convergence to a fixed point $Q_\mu$ defined by the stationary distribution of the behavioral policy $\mu$, which does not necessarily correspond to the global optimum of the POMDP.
  - **What evidence would resolve it:** A proof of convergence to the global optimum, or an algorithm that adapts $\mu$ dynamically to close the performance gap.

## Limitations

- The convergence guarantees depend critically on the ergodicity of the exploration policy's stationary distribution, which may not hold in all POMDPs
- The algorithm converges to a fixed point defined by the behavior policy rather than the optimal policy, limiting practical utility
- The theoretical analysis assumes tabular Q-functions, leaving open whether guarantees extend to function approximation settings
- The periodic extension requires careful synchronization of learning rates with policy phases, which may be challenging to implement

## Confidence

- **High Confidence:** The contraction mapping argument via Legendre-Fenchel duality (Mechanism 1) is mathematically rigorous and well-supported by the convex analysis literature
- **Medium Confidence:** The convergence to the "artificial" MDP defined by the limiting distribution (Mechanism 2) is logically sound but depends on the non-trivial ergodicity assumption
- **Medium Confidence:** The periodic policy extension (Mechanism 3) follows from the stationary case but requires empirical validation to confirm practical benefits

## Next Checks

1. **Ergodicity Validation:** Implement the POMDP environment and verify that the Markov chain induced by the agent state and behavior policy has a unique stationary distribution. Test with multiple initial conditions and measure convergence of empirical state visitation frequencies.

2. **Regularization Strength Sweep:** Systematically vary the regularization coefficient β and measure both convergence speed and final Q-value accuracy. Identify the range where the contraction property holds and where it breaks down (β → 0 or β → ∞).

3. **Policy Sensitivity Analysis:** Replace the fixed behavior policy μ with different exploration strategies (e.g., ε-greedy with varying ε) and measure how the limiting Q-values Q^μ change. This would empirically demonstrate that the algorithm converges to the fixed point defined by the exploration policy rather than the optimal policy.