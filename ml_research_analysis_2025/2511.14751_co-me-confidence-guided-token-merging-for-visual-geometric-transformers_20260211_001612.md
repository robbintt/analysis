---
ver: rpa2
title: 'Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers'
arxiv_id: '2511.14751'
source_url: https://arxiv.org/abs/2511.14751
tags:
- vggt
- token
- confidence
- merging
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck of visual geometric
  transformers in 3D perception tasks by introducing Confidence-Guided Token Merging
  (Co-Me), which accelerates inference without retraining the base model. The method
  distills a lightweight confidence predictor from intermediate ViT features to rank
  tokens by uncertainty and selectively merge low-confidence ones, preserving spatial
  coverage while reducing computation in both attention and MLP modules.
---

# Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers

## Quick Facts
- **arXiv ID:** 2511.14751
- **Source URL:** https://arxiv.org/abs/2511.14751
- **Reference count:** 40
- **Primary result:** Achieves up to 11.3× and 7.2× speedup on VGGT and MapAnything respectively with minimal accuracy loss, enabling real-time 3D reconstruction at 3.5 FPS on edge devices.

## Executive Summary
This paper introduces Confidence-Guided Token Merging (Co-Me), a method to accelerate visual geometric transformers for 3D perception tasks without retraining the base model. The approach distills a lightweight confidence predictor from intermediate ViT features to rank tokens by uncertainty and selectively merge low-confidence ones, preserving spatial coverage while reducing computation. When applied to VGGT and MapAnything, Co-Me achieves significant speedups (11.3× and 7.2×) with minimal performance degradation, enabling real-time 3D reconstruction on edge devices at 3.5 FPS—1.5× faster than the original models. The method scales with sequence length and introduces negligible overhead (<1%) when using efficient attention kernels.

## Method Summary
Co-Me accelerates visual geometric transformers through a two-stage process. First, a confidence predictor is distilled from intermediate encoder features (specifically layer 15) using a logistic ranking loss to learn token importance ranking. Second, at inference time, tokens are grouped spatially and low-confidence groups are merged via averaging, with the merged token replicated after attention/MLP computations to maintain sequence length compatibility. An attention bias correction (log n) is applied to merged tokens to preserve attention distribution. The method achieves speedups by reducing both attention and MLP computations while maintaining spatial coverage better than pruning approaches.

## Key Results
- Achieves up to 11.3× and 7.2× speedup on VGGT and MapAnything respectively
- Maintains accuracy with minimal performance degradation across depth, pose, and point cloud metrics
- Enables real-time 3D reconstruction at 3.5 FPS on edge devices, 1.5× faster than original models
- Introduces negligible overhead (<1%) when using efficient attention kernels like FlexAttention

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Uncertainty Distillation
Intermediate encoder features contain sufficient signal to estimate token reliability before full inference completes. A lightweight confidence predictor module is distilled in a self-supervised manner to map intermediate ViT features to confidence rankings using logistic ranking loss. The geometry and texture cues required for reliability judgment are formed in encoder layers before the final prediction head. This predicted confidence outperforms similarity-based merging heuristics.

### Mechanism 2: Context-Preserving Token Averaging
Aggregating low-confidence tokens via averaging preserves spatial context better than pruning while reducing computational load. Tokens are grouped spatially, and if a group's average confidence is low, tokens are averaged into a single representative token. After attention/MLP computation, this token is replicated to restore original sequence length, maintaining compatibility with downstream heads. Low-confidence regions provide contextual cues necessary for global consistency without requiring high-resolution features.

### Mechanism 3: Attention Distribution Correction
Merging tokens distorts softmax attention weights, requiring log-scale bias correction to maintain original attention distribution. When n tokens are merged into one, a bias of log n is added to attention logits to prevent softmax from suppressing the concentrated weight relative to unmerged tokens. This ensures merged tokens inherit the collective attention mass of their constituents to interact proportionally with the rest of the sequence.

## Foundational Learning

- **Concept: Vision Transformer Quadratic Complexity** - Needed to understand why reducing token count linearly reduces MLP cost and quadratically reduces attention cost. Quick check: Why does the paper claim MLPs become the bottleneck when efficient attention (like FlashAttention/FlexAttention) is used?

- **Concept: Token Pruning vs. Merging** - Needed to understand the distinction between pruning (reducing sequence length) and merging (temporarily aggregating then restoring shape). Quick check: How does Co-Me ensure tensor shape remains compatible with the original model's prediction head after merging?

- **Concept: Self-Supervised Distillation** - Needed to understand how the confidence predictor is trained without human labels. Quick check: What specific "ground truth" signal does the confidence predictor learn to predict?

## Architecture Onboarding

- **Component map:** Frozen Backbone (VGGT/MapAnything) → Confidence Predictor (MLP + Attention + Conv2D at layer 15) → Merge/Split Module (CUDA kernel) → Bias Logic (log n addition)

- **Critical path:** The Merge-Split CUDA kernel is the implementation linchpin. If this kernel is slow, the overhead (>1%) destroys speedup benefits. The distillation quality determines if merging actually preserves accuracy.

- **Design tradeoffs:** Insertion Layer (too early = inaccurate confidence; too late = low acceleration potential; paper finds Layer 15 optimal), Group Size (larger = faster but more geometric loss; paper uses n=4), Merge Ratio (higher = faster but riskier; paper uses p=0.5 as default balance).

- **Failure signatures:** Thin Structure Loss (high-frequency, thin objects in low-confidence zones are over-smoothed or disappear), Attention Sink Effect (without log-bias correction, model effectively ignores merged background context, potentially destabilizing pose estimation).

- **First 3 experiments:** 1) Distillation Validation: Train predictor on TartanAir subset and verify ranking loss convergence, 2) Overhead Benchmark: Profile Merge+Split CUDA kernel in isolation ensuring <1% of total inference time, 3) Ablation on Bias: Run inference on RE10K with and without log n attention bias to quantify accuracy drop.

## Open Questions the Paper Calls Out
- How can Co-Me be adapted to merge tokens across the time dimension in streaming visual geometric transformers?
- Can Co-Me be utilized during the model training pipeline to improve training efficiency?
- How can the merging strategy be refined to preserve thin, high-frequency geometric structures?

## Limitations
- Architecture specification gaps leave critical implementation details underspecified, including confidence predictor architecture and block split points
- Cross-dataset generalization behavior remains untested on real-world datasets with different characteristics
- Efficiency claims assume use of FlexAttention or similar efficient kernels, with less pronounced speedups using standard PyTorch attention

## Confidence
**High Confidence:** The mechanism of confidence-guided token merging itself and its ability to preserve shape for downstream compatibility; superiority of averaging over pruning for preserving spatial context; necessity of attention bias correction (log n) to maintain attention distribution.

**Medium Confidence:** Generalization of intermediate feature confidence to token importance across different 3D perception tasks; claimed 11.3× and 7.2× speedups; effectiveness of logistic ranking loss versus regression for confidence distillation.

**Low Confidence:** Assumption that MLPs will remain the bottleneck as attention kernels continue to improve; optimal merge ratio (p=0.5) across all possible 3D perception tasks and datasets; claimed <1% overhead without detailed profiling methodology.

## Next Checks
1. **Distillation Quality Validation:** Train confidence predictor using both logistic ranking loss and MSE regression on TartanAir subset, comparing resulting mask IoU and downstream accuracy to validate ranking loss superiority.

2. **Kernel Overhead Benchmark:** Implement and profile Merge-Split CUDA kernel in isolation, measuring execution time percentage across different sequence lengths (32, 64, 128 frames) to verify <1% overhead on A100 GPU.

3. **Thin Structure Preservation Test:** Create synthetic test dataset with known thin structures in low-confidence regions, applying Co-Me with varying merge ratios and measuring structural similarity index (SSIM) and precision/recall of thin structure detection.