---
ver: rpa2
title: Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power
  Consumption Prediction
arxiv_id: '2511.06538'
source_url: https://arxiv.org/abs/2511.06538
tags:
- uncertainty
- energy
- quantile
- dropout
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses uncertainty quantification (UQ) in EV power
  consumption prediction using LSTM networks. The proposed method extends Bayesian
  anchored ensembles to LSTMs, incorporating a Student-t likelihood to jointly capture
  epistemic and aleatoric uncertainties.
---

# Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction

## Quick Facts
- arXiv ID: 2511.06538
- Source URL: https://arxiv.org/abs/2511.06538
- Authors: Ghazal Farhani; Taufiq Rahman; Kieran Humphries
- Reference count: 40
- Primary result: Anchors LSTMs with Student-t likelihoods achieve RMSE 3.36±1.10, MAE 2.21±0.89, R²=0.93±0.02 with well-calibrated intervals

## Executive Summary
This paper addresses uncertainty quantification in EV power consumption prediction using LSTM networks. The proposed method extends Bayesian anchored ensembles to LSTMs, incorporating a Student-t likelihood to jointly capture epistemic and aleatoric uncertainties. This approach provides calibrated prediction intervals while avoiding Monte Carlo sampling during inference, making it suitable for real-time deployment. The model achieves strong accuracy and well-calibrated uncertainty bands with near-nominal coverage.

## Method Summary
The method uses 4-layer LSTMs with gate-wise anchored ensembles (M=30 members) and Student-t output heads. Each ensemble member samples independent anchors from Gaussian priors for each LSTM gate, then trains via MAP estimation. The Student-t likelihood jointly models aleatoric uncertainty through learnable scales and epistemic uncertainty through ensemble variance. Inference is deterministic per member, aggregating for uncertainty estimates without Monte Carlo sampling.

## Key Results
- Achieves RMSE 3.36±1.10, MAE 2.21±0.89, R²=0.93±0.02 on test data
- Standardized variance ≈1.13 (near-ideal calibration) vs 0.06-0.36 for quantile baselines
- Prediction intervals are well-calibrated with near-nominal 90% coverage
- Maintains comparable or better log-scores than MC dropout and quantile regression baselines
- Outperforms baselines with sharper intervals at same coverage

## Why This Works (Mechanism)

### Mechanism 1: Anchored Ensembles for Epistemic Uncertainty
Anchored ensembles approximate Bayesian posterior diversity through deterministic inference, avoiding MC sampling latency. For each member m, draw an anchor weight W_anc^(m) ~ N(μp, Σp) from the prior. Train via MAP where the anchor replaces the prior mean in the L2 penalty term. Different anchors pull members toward different regions of weight space, creating posterior-like spread without test-time stochasticity. Core assumption: The Gaussian prior on weights meaningfully covers the parameter space; members converge to diverse solutions rather than identical minima. Break condition: If anchor variance σ²p is too small, all members converge near the same MAP solution, collapsing epistemic uncertainty estimates toward zero.

### Mechanism 2: Student-t Likelihood for Heavy-Tailed Residuals
The t-distribution's density has heavier tails than Gaussian (controlled by degrees of freedom ν). Outliers contribute less to the NLL gradient, preventing them from inflating the learned scale s(x) disproportionately. Prediction intervals have closed form: μ(x) ± t_(1-α/2,ν) · s(x). Core assumption: Residuals exhibit heavier tails than Gaussian; ν can be learned or set appropriately for the data distribution. Break condition: If residuals are actually near-Gaussian (light-tailed), the t-likelihood may produce unnecessarily wide intervals; if ν is fixed incorrectly, calibration degrades.

### Mechanism 3: Gate-wise Anchor Extension to LSTMs
LSTM weights are partitioned by gate (input, forget, output, candidate, head). Each gate block receives an independent Gaussian prior/anchor. The prior term ∑_g ||W_g - W_g,anc||² / (2σ²_g) regularizes each gate separately while the data loss couples all gates through the recurrent computation. Core assumption: Gate-wise factorization of the prior is a reasonable approximation; treating gates independently does not miss critical posterior correlations. Break condition: If strong posterior correlations exist between gates (e.g., input-forget gate coupling), the factorized prior may misrepresent true uncertainty.

## Foundational Learning

- **Epistemic vs Aleatoric Uncertainty**: The paper explicitly disentangles these—epistemic (model uncertainty from limited data) via ensemble variance, aleatoric (irreducible noise) via the t-distribution scale. Without this distinction, you cannot debug whether poor calibration stems from model inadequacy or inherent data noise. Quick check: Your ensemble predicts identical means across all members but with wide t-distribution scales. Which uncertainty type dominates?

- **MAP Estimation as Regularized MLE**: Understanding that L2 regularization ≡ Gaussian prior is essential. The "anchoring" trick is just MAP where each member has a different prior mean—this should click once you see L_prior = (1/2σ²)||W - W_anc||². Quick check: If you set σ²p → ∞, what happens to the anchor's influence on training?

- **LSTM Hidden State Propagation**: The paper assumes comfort with LSTM recurrence. The data term L_data depends on all gates through sequential hidden state updates—this coupling is why gate-wise priors are an approximation. Quick check: In a 4-layer LSTM with hidden_dim=32, how many scalar parameters are in the forget gate weights for layer 2?

## Architecture Onboarding

- Component map: Input Features (5) -> Normalized to [0,1] -> 4-Layer LSTM (hidden_dim=32 per layer) -> Final hidden state -> Dense -> Student-t Head (μ(x), s(x), [ν]) -> Anchored Ensemble (M=30 members, gate-wise anchors W_g,anc ~ N(0, 0.01·I)) -> Training Loss (t-NLL + ∑_g ||W_g - W_g,anc||² / (2σ²_g)) -> Inference (Mean prediction μ_ens = (1/M)∑_m μ^(m)(x), Epistemic var = (1/M)∑_m (μ^(m) - μ_ens)², Total uncertainty = epistemic + aleatoric)

- Critical path: 1) Normalize inputs to [0,1] — required for anchored variance σ²=0.01 to be sensible 2) Sample gate-wise anchors for each member before training 3) Train each member independently (300 epochs, Adam, lr=0.001) 4) At inference: single forward pass per member, aggregate for uncertainty 5) Validate calibration via binomial coverage test

- Design tradeoffs: M=30 members vs inference latency (linear cost; can reduce to M=5-10 with weight averaging if latency-critical); σ²_anchor=0.01 vs diversity (too small → underconfident epistemic UQ; too large → training instability); Learned vs fixed ν (learned adapts to data tail behavior but adds optimization complexity; fixed ν=5-10 is a common robust default)

- Failure signatures: StVar >> 1 (overconfident): Anchors too similar → reduce ensemble to near-deterministic; increase anchor variance; StVar << 1 (underconfident, overly wide bands): ν too small or quantile loss used instead of t-NLL; RMSE spikes on highway vs dyno data: Expected due to uncontrolled factors; if extreme, check feature drift; Training loss plateaus early: Anchor variance mismatch with feature scale; verify normalization

- First 3 experiments: 1) Single-member baseline: Train one anchored LSTM, plot residual histogram vs fitted t-distribution. Verify heavy-tailed assumption holds for your data. 2) Ensemble diversity diagnostic: After training M=30 members, compute coefficient of variation across member predictions on held-out data. Target: meaningful spread (not zero, not chaotic). 3) Calibration curve: Bin test samples by predicted uncertainty; plot empirical coverage vs nominal coverage (90%). Well-calibrated models should hug the diagonal.

## Open Questions the Paper Calls Out

None

## Limitations

- Data Generalization: Strong calibration demonstrated on held-out dynamometer data and single IONIQ 5 highway trip, but generalization to vehicles with significantly different powertrain architectures remains unverified.

- Anchor Diversity Dependency: Epistemic uncertainty estimates depend critically on anchor variance σ²_anchor=0.01. Optimal anchoring scales likely vary with feature normalization schemes and LSTM architecture depth.

- Student-t Degrees of Freedom: The paper does not clearly specify whether ν is learned or fixed, nor analyze sensitivity to this choice. Incorrect ν assumptions could compromise interval calibration.

## Confidence

- **High Confidence**: The core claim that anchored ensembles with Student-t likelihoods produce well-calibrated prediction intervals is strongly supported by empirical results (StVar≈1.13 vs 0.06-0.36 for baselines).

- **Medium Confidence**: Claims about real-world IONIQ 5 performance are based on a single vehicle and driving scenario. While results are promising, the limited test set size reduces confidence in robustness claims.

- **Low Confidence**: No direct comparison against pure deep ensembles for the same computational budget. The paper claims anchoring avoids test-time sampling but does not benchmark against full Bayesian inference.

## Next Checks

1. **Anchor Variance Sensitivity**: Systematically vary σ²_anchor across [0.001, 0.01, 0.1] and measure effects on ensemble diversity (coefficient of variation), epistemic uncertainty magnitude, and calibration. Document the relationship between anchor scale and prediction interval sharpness.

2. **Degrees of Freedom Analysis**: If using learned ν, plot its convergence during training and test set residuals against fitted t-distributions with different fixed ν values. Quantify calibration sensitivity to ν choices in the range [3, 30].

3. **Cross-Vehicle Transfer**: Test the IONIQ 5-trained model on data from other EV models (different manufacturers, battery sizes) without retraining. Measure degradation in RMSE and calibration to assess architecture generalization limits.