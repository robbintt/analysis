---
ver: rpa2
title: 'From Monolingual to Bilingual: Investigating Language Conditioning in Large
  Language Models for Psycholinguistic Tasks'
arxiv_id: '2508.02502'
source_url: https://arxiv.org/abs/2508.02502
tags:
- language
- dutch
- chinese
- bilingual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether Large Language Models (LLMs) encode\
  \ psycholinguistic knowledge across languages under different linguistic identities.\
  \ The authors evaluate two models\u2014Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct\u2014\
  on two psycholinguistic tasks: sound symbolism and word valence\u2014using monolingual\
  \ and bilingual prompts in English, Dutch, and Chinese."
---

# From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks

## Quick Facts
- arXiv ID: 2508.02502
- Source URL: https://arxiv.org/abs/2508.02502
- Reference count: 22
- Primary result: Both Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct adapt outputs based on prompted language identity, with psycholinguistic signals becoming more decodable in deeper layers.

## Executive Summary
This paper investigates whether Large Language Models encode psycholinguistic knowledge across languages under different linguistic identities. The authors evaluate two models—Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct—on sound symbolism and word valence tasks using monolingual and bilingual prompts in English, Dutch, and Chinese. They find that both models adapt their outputs based on prompted language identity, with Qwen showing greater sensitivity and sharper distinctions between Dutch and Chinese. Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers, with Chinese prompts yielding stronger and more stable valence representations than Dutch.

## Method Summary
The study uses two psycholinguistic tasks: sound symbolism (classifying pseudowords as round or spiky) and word valence (classifying words as positive or negative). The authors evaluate Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct with temperature=0 across four prompting conditions: Dutch monolingual, Chinese monolingual, Dutch-English bilingual, and Chinese-English bilingual. For probing analysis, hidden states are extracted from every 10th layer (8 checkpoints across 80 layers) and a frozen MLP classifier (input 512, hidden 256, output 2, ReLU, Adam, 200 epochs, 80/20 split) is trained to predict psycholinguistic properties.

## Key Results
- Both models adapt their outputs based on prompted language identity, though with markedly different magnitudes
- Qwen shows greater sensitivity to language identity with sharper distinctions between Dutch and Chinese responses
- Psycholinguistic signals become more linearly decodable in deeper transformer layers
- Chinese prompts yield stronger and more stable valence representations than Dutch in probing analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language identity in prompts conditions both output behavior and internal representations in LLMs for psycholinguistic tasks.
- **Mechanism:** Linguistic identity is embedded via persona-based system prompts (e.g., "You are a native speaker of Mandarin Chinese..."), which activate language-specific phonological and semantic priors within the model's representations, influencing judgments on tasks like sound symbolism (round vs. spiky) and word valence (positive vs. negative).
- **Core assumption:** The model has sufficiently internalized language-specific phonological and semantic associations during pre-training to be modulated by explicit identity prompts.
- **Evidence anchors:** [abstract]: "We find that both models adapt their outputs based on prompted language identity..."; [Section 4.3.2]: "Both models exhibit language-dependent behavior, though with markedly different magnitudes."
- **Break condition:** If the model lacks sufficient pre-training data in the target language or the psycholinguistic phenomenon is entirely absent from its training distribution, identity prompts will likely have minimal or noisy effect.

### Mechanism 2
- **Claim:** Psycholinguistic signals (sound symbolism, valence) become more linearly decodable in deeper transformer layers.
- **Mechanism:** Early layers process surface-level phonological and lexical features. As information propagates to deeper layers, it is integrated into more abstract, semantic, and affective representations, making the psycholinguistic properties more separable by a linear probe.
- **Core assumption:** The model's layer hierarchy naturally decomposes linguistic information, with abstract properties emerging in later layers.
- **Evidence anchors:** [abstract]: "Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers..."; [Section 5.2, Figure 3]: "Across all settings, accuracy increases with layer depth, indicating that sound-symbolic cues become more linearly decodable in deeper layers."
- **Break condition:** If a task relies solely on surface form (e.g., character n-grams) without deeper semantic integration, the signal may be decodable in early layers and not improve, or degrade, in deeper layers.

### Mechanism 3
- **Claim:** Bilingual conditioning can introduce representational interference, especially for lower-resource or less-familiar languages.
- **Mechanism:** Prompting with bilingual identity (e.g., "speaker of English and Dutch") engages representations for both languages. If the model has weaker priors for one language (e.g., Dutch in Llama), the stronger language's priors (English) may dominate or interfere, reducing performance compared to a focused monolingual prompt.
- **Core assumption:** The model's multilingual representations are not perfectly disentangled and can suffer from cross-lingual interference when jointly activated.
- **Evidence anchors:** [Section 4.3.1]: "...bilingual conditioning may weaken or blur language-specific cues."; [Section 5.2]: "...bilingual settings, especially Chinese bilingual, lag behind in early layers... suggesting that bilingual conditioning introduces representational diffusion or interference..."
- **Break condition:** If the model has strong, well-separated multilingual representations (e.g., Qwen for Chinese), bilingual conditioning may not cause interference and could even improve performance.

## Foundational Learning

### Concept: Sound Symbolism
- **Why needed here:** This is the core psycholinguistic phenomenon being tested. Understanding that certain sounds are non-arbitrarily associated with shapes or meanings (e.g., "bouba" = round) is essential for interpreting the task and results.
- **Quick check question:** Does the model associate the pseudoword "tiki" with a spiky or round shape, and how does this change if prompted in Chinese vs. Dutch?

### Concept: Linear Probing
- **Why needed here:** The method for evaluating internal representations. It tests whether a property is linearly encoded in hidden states, indicating it's a readily available feature.
- **Quick check question:** If a linear classifier trained on layer 60's hidden states can predict word valence with 90% accuracy, what does this imply about the representation at that layer?

### Concept: Cross-Lingual Transfer
- **Why needed here:** Explains performance differences between Llama (English-centric) and Qwen (multilingual). It's the principle that knowledge from a high-resource language (English) can be applied to or interfere with tasks in other languages.
- **Quick check question:** Why might Qwen, a multilingual model, show more variable and sometimes poorer performance on English-based psycholinguistic tasks compared to Llama?

## Architecture Onboarding

### Component map:
Prompting Module -> LLM Core (Llama/Qwen) -> Probing Module -> Evaluation Suite

### Critical path:
The prompt design is the most sensitive component. A poorly phrased persona or ambiguous task instruction will invalidate both behavioral and probing results. Ensure prompts are verbatim from the paper's appendix.

### Design tradeoffs:
- **Model Choice:** Llama-3.3-70B (English-centric) vs. Qwen2.5-72B-Instruct (multilingual). Llama offers a cleaner test of emergent multilingualism; Qwen tests a natively multilingual system.
- **Probing Classifier:** Using a simple MLP makes results easier to interpret but may underestimate the complexity of the encoded signal.

### Failure signatures:
- **Inconsistent Outputs:** Model generates responses not in the target language or format (e.g., "joyful" instead of "positive"). This is noted for Qwen under bilingual prompts.
- **Flat Probing Curves:** Probing accuracy fails to rise with layer depth, indicating the target property is not being encoded or the probing setup is flawed.
- **Random Guessing:** Accuracy hovers near 50%, suggesting the model has no relevant knowledge or the prompt failed to condition it.

### First 3 experiments:
1. **Reproduce Behavioral Baseline:** Run the sound symbolism task on both Llama and Qwen using only English monolingual prompts. Validate that Llama achieves ~70% and Qwen ~58% accuracy, as per Table 2.
2. **Implement Monolingual Probing:** Extract hidden states from layers 10, 30, 50, 70 of Llama for the English sound symbolism task. Train the MLP probe and plot accuracy vs. layer. Verify the increasing trend shown in Figure 3.
3. **Test Cross-Lingual Interference:** Run the word valence task on Qwen using both monolingual Dutch and bilingual Dutch-English prompts. Compare accuracy to confirm the large drop reported in Table 3 (from 21% to 67.5% bilingual, indicating a major shift).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do these findings generalize to languages with significantly different typological features, such as distinct scripts, phonological systems, or morphological complexity (e.g., Arabic, Hindi, Finnish)?
- **Basis in paper:** [explicit] The authors state in the Limitations section that focusing only on English, Dutch, and Chinese restricts generalizability to a broader range of linguistic families.
- **Why unresolved:** The current study is limited to three languages, leaving the impact of deeply divergent linguistic structures on LLM psycholinguistic encoding untested.
- **What evidence would resolve it:** Replicating the sound symbolism and valence experiments across a diverse set of languages with varying morphological and script characteristics.

### Open Question 2
- **Question:** Does language conditioning modulate model behavior in other psycholinguistic domains, such as semantic priming, lexical decision making, or metaphor comprehension?
- **Basis in paper:** [explicit] The authors note that the two tasks used represent only a "narrow slice" of the psycholinguistic space and suggest expanding to other phenomena.
- **Why unresolved:** It is currently unclear if the observed sensitivity to language identity is specific to sound-emotion associations or applies broadly to cognitive processing.
- **What evidence would resolve it:** Applying the same monolingual vs. bilingual prompting framework to a wider array of psycholinguistic tasks.

### Open Question 3
- **Question:** What is the precise mechanism causing bilingual prompting to degrade internal representation stability and probing accuracy for certain languages like Dutch?
- **Basis in paper:** [inferred] The probing results show that bilingual prompts can lead to "flatter probing curves" and "representational diffusion" (Section 5.3), suggesting interference.
- **Why unresolved:** The paper identifies the instability (e.g., Dutch-English performing worse than monolingual Dutch) but does not isolate the root cause, such as conflicting phonological priors or attention dilution.
- **What evidence would resolve it:** Analysis of cross-lingual neuron overlap or attention head activation patterns during bilingual versus monolingual inference.

## Limitations
- The study relies on Google Translate for initial dataset creation, introducing potential translation artifacts
- The paper does not explore alternative prompting strategies or test whether observed effects persist with different system prompt formulations
- The probing analysis assumes that increasing accuracy with layer depth specifically reflects psycholinguistic encoding rather than general semantic processing improvement

## Confidence
- **Behavioral adaptation to language identity:** High
- **Qwen showing greater sensitivity to language identity:** High
- **Layer-wise emergence of psycholinguistic signals:** Medium
- **Cross-lingual interference in bilingual prompts:** Medium

## Next Checks
1. **Prompt variation test:** Run the sound symbolism task with alternative persona prompts (e.g., "You are a linguist studying sound symbolism in Mandarin Chinese" vs. "You are a native Mandarin speaker") to determine whether the observed language conditioning is robust to prompt formulation.

2. **Semantic control probe:** Design a control probing task using words that have the same psycholinguistic property but no direct translation across languages (e.g., words with the same valence but different cultural associations) to test whether the probing classifier is learning true cross-linguistic psycholinguistic encoding versus surface lexical patterns.

3. **Resource analysis validation:** Examine the actual training data composition for Dutch and Chinese in both models to quantify whether the observed differences in language sensitivity (Qwen's superior Chinese performance vs. Llama's Dutch performance) align with relative training data volumes and quality.