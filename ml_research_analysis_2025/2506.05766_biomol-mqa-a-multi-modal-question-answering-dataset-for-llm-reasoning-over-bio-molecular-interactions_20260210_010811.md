---
ver: rpa2
title: 'BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over
  Bio-Molecular Interactions'
arxiv_id: '2506.05766'
source_url: https://arxiv.org/abs/2506.05766
tags:
- question
- graph
- questions
- information
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BioMol-MQA, a multi-modal question answering
  dataset for polypharmacy that combines knowledge graphs, text, and molecular structure
  data (SMILES) to test LLM reasoning capabilities. The dataset contains 1,683 questions
  requiring retrieval and reasoning over drug-drug and drug-protein interactions,
  with ground truth answers and associated context.
---

# BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions

## Quick Facts
- **arXiv ID**: 2506.05766
- **Source URL**: https://arxiv.org/abs/2506.05766
- **Reference count**: 40
- **Primary result**: Multi-modal QA dataset combining KG, text, and SMILES data for polypharmacy reasoning

## Executive Summary
This paper introduces BioMol-MQA, a multi-modal question answering dataset designed to test LLM reasoning capabilities over bio-molecular interactions in polypharmacy scenarios. The dataset combines knowledge graphs, text, and molecular structure data (SMILES) to create complex questions requiring retrieval and reasoning over drug-drug and drug-protein interactions. The dataset contains 1,683 questions with ground truth answers and associated context from DrugBank, aiming to provide a challenging benchmark for evaluating multi-modal reasoning in healthcare applications.

## Method Summary
The dataset construction process involved extracting drug-drug interactions from DrugBank, selecting 39 interaction pairs from 1,131 candidates based on shared protein targets and pathways, and generating 1,683 multi-modal questions. Each question requires reasoning over knowledge graph data, text descriptions, and SMILES molecular structures. The evaluation framework tests both zero-shot performance and context-augmented retrieval-augmented generation approaches, with exact match accuracy as the primary metric. The study compares various retrievers including BM25, Neo4j, and more complex models to assess their effectiveness in multi-modal biomedical retrieval tasks.

## Key Results
- Zero-shot LLM performance on BioMol-MQA achieves only 22% exact match accuracy
- Context-augmented retrieval improves performance significantly to 62% accuracy
- Simple retrievers like BM25 and Neo4j outperform more complex models on multi-modal retrieval tasks
- The dataset reveals substantial challenges in multi-modal biomedical reasoning requiring KG, text, and molecular structure integration

## Why This Works (Mechanism)
The dataset works by creating a realistic test bed for multi-modal reasoning where questions require synthesizing information across different data representations. By combining knowledge graphs (drug-protein interactions), text descriptions (drug properties and effects), and molecular structures (SMILES), the dataset forces models to integrate heterogeneous information sources. The retrieval-augmented generation approach demonstrates that while models struggle with zero-shot reasoning, providing relevant context significantly improves performance, suggesting that the bottleneck is information access rather than reasoning capability.

## Foundational Learning
- **Polypharmacy**: The study of drug interactions when multiple medications are taken simultaneously - needed to understand the clinical relevance and why interaction prediction matters in real-world healthcare scenarios
- **SMILES notation**: A linear textual representation of chemical structures - needed to understand how molecular data is encoded and processed in multi-modal systems
- **Knowledge Graph reasoning**: Methods for inferring relationships between entities in structured data - needed to understand how drug-protein and drug-drug interactions are represented and queried
- **Retrieval-augmented generation**: Techniques that combine information retrieval with language model generation - needed to understand the evaluation methodology and performance improvements with context

## Architecture Onboarding
**Component Map**: DrugBank DB -> Question Generation -> Multi-modal Dataset (KG + Text + SMILES) -> LLM Evaluation Framework -> Performance Metrics

**Critical Path**: Data extraction from DrugBank → Question generation with multi-modal context → Retriever selection and implementation → LLM evaluation with/without context → Performance analysis

**Design Tradeoffs**: The choice of exact match accuracy as the primary metric provides clear evaluation but may underestimate performance on complex reasoning tasks. The small sample of 39 interaction pairs from 1,131 candidates balances dataset manageability with representativeness, though this may limit generalizability.

**Failure Signatures**: Poor zero-shot performance (22%) indicates models lack sufficient domain knowledge for multi-modal biomedical reasoning. Simple retrievers outperforming complex models suggests current retrieval approaches may be over-engineered for this specific task type.

**First Experiments**:
1. Evaluate domain-specific biomedical LLMs (BioMedLM, BioBERT) on the dataset to compare with general LLMs
2. Test different retrieval strategies including hybrid approaches combining BM25 with graph-based methods
3. Analyze retrieval failures to identify whether limitations stem from dataset construction, retrieval mechanisms, or inherent reasoning complexity

## Open Questions the Paper Calls Out
None

## Limitations
- The small sample size of 39 interaction pairs from 1,131 candidates may introduce selection bias and limit generalizability
- Heavy reliance on DrugBank annotations constrains scope to well-characterized drugs, potentially excluding emerging compounds
- Exact match accuracy metric may underestimate performance, particularly for complex reasoning tasks
- Absence of human evaluation to validate whether retrieved context actually answers questions
- Limited comparison with specialized biomedical models beyond general LLMs

## Confidence
**High Confidence**: The dataset successfully combines three distinct modalities (KG, text, and molecular structures) in a unified format, and the claim about performance improvements with context retrieval is well-supported by the experimental results.

**Medium Confidence**: The assertion that BioMol-MQA represents a "challenging benchmark" is supported by the zero-shot performance results, though this conclusion depends on the assumption that 22% accuracy is indeed low for this task type.

**Low Confidence**: Claims about the dataset's comprehensiveness for polypharmacy research are questionable given the small sample size of interaction pairs and the heavy reliance on DrugBank annotations.

## Next Checks
1. **External Validation**: Test the dataset with additional biomedical LLMs beyond those used in the original study, including domain-specific models like BioMedLM or BioBERT, to determine if performance patterns hold across different model architectures.

2. **Coverage Analysis**: Conduct a systematic analysis of the 39 selected interaction pairs to assess their representativeness across different drug classes, therapeutic areas, and interaction types compared to the full set of 1,131 candidates.

3. **Human Evaluation Study**: Recruit clinical experts to evaluate a subset of retrieved contexts and generated answers to determine whether the exact match metric accurately reflects clinically meaningful performance, and to identify failure modes that automated metrics might miss.