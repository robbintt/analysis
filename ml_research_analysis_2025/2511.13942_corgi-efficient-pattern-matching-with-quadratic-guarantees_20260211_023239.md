---
ver: rpa2
title: 'CORGI: Efficient Pattern Matching With Quadratic Guarantees'
arxiv_id: '2511.13942'
source_url: https://arxiv.org/abs/2511.13942
tags:
- city
- matching
- memory
- corgi
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORGI (Collection-Oriented Relational Graph
  Iteration), a new pattern-matching algorithm for rule-based systems that addresses
  exponential memory blowups in RETE-based approaches when matching rules with many
  underconstrained variables. CORGI achieves quadratic time and space guarantees by
  maintaining a relation graph that narrows match candidates without committing partial
  matches to memory, and iteratively generates matches through backward graph traversal.
---

# CORGI: Efficient Pattern Matching With Quadratic Guarantees

## Quick Facts
- arXiv ID: 2511.13942
- Source URL: https://arxiv.org/abs/2511.13942
- Reference count: 40
- Key outcome: CORGI achieves quadratic time/space guarantees for rule-based pattern matching by eliminating β-memories, outperforming RETE implementations on combinatorial matching tasks.

## Executive Summary
CORGI (Collection-Oriented Relational Graph Iteration) is a new pattern-matching algorithm for rule-based systems that addresses exponential memory blowups in RETE-based approaches when matching rules with many underconstrained variables. Unlike RETE, which stores partial matches in β-memories leading to combinatorial explosion, CORGI maintains a relation graph with collections at edges and generates matches on-demand through backward traversal. The approach achieves quadratic time and space complexity for graph maintenance while enabling efficient match iteration.

## Method Summary
CORGI builds a forward-passing relation graph with α-nodes (filtering WMEs) and β-nodes (maintaining binary relation mappings) without storing partial matches. Instead of β-memories, edges carry collections of WME identifiers. A backward traversal iteratively generates matches on demand by following paths through β-node mappings and intersecting results where variables overlap. The implementation uses bit matrices for relation truth tables to optimize memory locality and performance.

## Key Results
- CORGI outperformed RETE implementations from SOAR and OPS5 on a combinatorial matching task
- Completed Valentine's task in 2 milliseconds or less regardless of number of matches required
- RETE implementations failed or took up to 665 seconds as matches increased
- Achieved O(KN²) time and space complexity for graph maintenance versus RETE's exponential blowup

## Why This Works (Mechanism)

### Mechanism 1: Collection-Oriented Edge Storage (Eliminates Combinatorial Explosion)
CORGI stores WME collections at graph edges rather than partial match tuples in β-memories. This avoids exponential memory growth because each β-node only maintains mappings between satisficing pairs (O(N²)) rather than all combinations of partial matches (O(N^K)). The final match enumeration is deferred and generated on-demand.

### Mechanism 2: Backward Graph Traversal for On-Demand Match Generation
Match candidates are generated iteratively via backward traversal starting from terminal nodes. The process follows paths backward through β-node mappings, intersecting results when multiple nodes share a variable pair. This lazy evaluation enables efficient match selection without materializing all possibilities upfront.

### Mechanism 3: Bit Matrix Implementation for Relation Truth Tables
Relations are implemented as bit matrices over input collections, enabling cache-efficient lookups without hash table overhead. Memory is amortized via doubling on growth, and deletions mark slots empty for reuse without data movement, maintaining contiguous data layout for better CPU cache locality.

## Foundational Learning

- **RETE Algorithm**: Understanding α-memories, β-memories, and conflict sets is essential since CORGI positions itself as a replacement. Quick check: Can you explain why storing partial matches in β-memories can lead to O(N^K) space complexity?

- **Production Systems**: Familiarity with match-execution cycles, working memory elements (WMEs), and conflict resolution strategies is assumed. Quick check: In a production system, what happens during the match phase versus the act phase?

- **Graph Traversal**: Understanding forward vs. backward traversal is crucial since CORGI uses forward graph construction followed by backward iteration. Quick check: If you have a directed acyclic graph representing variable dependencies, how would backward traversal from a sink node differ from forward traversal from sources?

## Architecture Onboarding

- **Component map**: Working Memory → α-nodes → β-nodes → Relation Graph → Match Iterator
- **Critical path**: 1) Initialize working memory with typed objects, 2) Forward pass: update α-filters → propagate collections → update relation mappings, 3) Backward pass: select binding → traverse backward through mappings → yield match, 4) Repeat via iterator until selection heuristic satisfied
- **Design tradeoffs**: Deferred vs. eager matching (quadratic guarantees vs. per-match latency), variable ordering for early culling, heuristic integration challenges
- **Failure signatures**: Full enumeration requirements regress to O(N^K) time, separable graphs generate all combinations, memory pressure from many relations
- **First 3 experiments**: 1) Replicate Valentine's task to verify correct matches without β-memory allocation, 2) Stress test with increasing Valentines (V=1-5) to confirm quadratic scaling via AIC model comparison, 3) Compare against RETE baseline to identify failure thresholds

## Open Questions the Paper Calls Out

- **Incremental Match Cycles**: How does CORGI perform across multiple match cycles with incremental working memory modifications in full production systems? The evaluation only measured the first match cycle, but real systems require sustained performance across many cycles.

- **Heuristic Integration**: Can heuristic or activation-based match selection biases be integrated into CORGI's relation graph iteration without compromising quadratic guarantees? The paper conjectures this is possible but provides no implementation or validation.

- **Fragmentation Comparison**: How does CORGI compare to collection-oriented RETE on patterns that cause β-memory fragmentation? The paper conjectures similar performance but lacked a collection-oriented RETE implementation for comparison.

## Limitations
- Quadratic memory guarantee applies only to graph maintenance; enumerating all matches still takes O(N^K) time if required
- Assumes production systems typically need only one match per cycle, which may not hold for applications requiring global optimization
- Implementation details for heuristic integration and variable ordering optimizations are conjectured but not demonstrated

## Confidence
- High confidence: Core mechanism eliminating β-memories and resulting quadratic space complexity for graph maintenance
- Medium confidence: Practical runtime benefits demonstrated in Valentine's task evaluation
- Low confidence: Conjecture about embedding heuristic biases in graph iteration order without implementation details

## Next Checks
1. **Implementation Verification**: Replicate the Valentine's task using the provided repository to confirm CORGI's O(KN²) scaling behavior and verify that match enumeration is indeed lazy/on-demand.

2. **Stress Test Full Enumeration**: Design a scenario requiring all matches simultaneously to quantify performance regression from O(KN²) to O(N^K) match generation time.

3. **Broader Benchmark Suite**: Evaluate CORGI on diverse pattern-matching tasks (e.g., subgraph isomorphism, temporal pattern matching) to assess generality beyond the Valentine's task.