---
ver: rpa2
title: 'MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis'
arxiv_id: '2508.01370'
source_url: https://arxiv.org/abs/2508.01370
tags:
- report
- market
- data
- each
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MaRGen is a multi-agent LLM framework that automates market research\
  \ by coordinating specialized agents for data querying, analysis, report writing,\
  \ and review. It learns professional methodologies from real consultants\u2019 materials\
  \ and uses iterative review cycles to improve report quality."
---

# MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis

## Quick Facts
- arXiv ID: 2508.01370
- Source URL: https://arxiv.org/abs/2508.01370
- Reference count: 40
- Generates comprehensive 6-page reports in 7 minutes at approximately $1 per report

## Executive Summary
MaRGen is a multi-agent LLM framework that automates end-to-end market research and report generation. The system coordinates specialized agents—Researcher (SQL querying), Writer (markdown-to-PDF generation), Reviewer (multimodal evaluation), and Retriever (domain knowledge injection)—to produce professional-quality reports from high-level client objectives. The framework demonstrates effective iterative improvement through automated review cycles, achieving perfect reviewer scores on clarity and layout within 4 rounds while maintaining cost efficiency at approximately $1 per report.

## Method Summary
MaRGen implements a multi-agent orchestration system where the Researcher iteratively queries a SQL database to gather market data, the Writer generates markdown with embedded Python code for visualizations that is converted to LaTeX and compiled to PDF, the Reviewer evaluates the PDF (as images) on clarity and layout using 1-10 scoring, and the Retriever injects domain knowledge by extracting hypothesis trees from consultant materials. The system uses iterative review cycles that terminate when scores reach 10/10 or after 4 rounds maximum. An LLM-based Judge performs pairwise comparisons to select the best report from parallel generations, with evaluation correlation of r=0.6 against human assessments.

## Key Results
- Iterative review cycles achieve perfect reviewer scores (10/10) on clarity and layout within 4 rounds
- Domain knowledge injection via Retriever agent significantly enhances report quality in pairwise comparisons
- LLM-based evaluation shows 0.6 Pearson correlation with human assessments
- Generates comprehensive 6-page reports in 7 minutes at approximately $1 per report

## Why This Works (Mechanism)

### Mechanism 1: Iterative Review Cycles
The Reviewer agent evaluates PDF reports (converted to images) on 1-10 scales for clarity and layout, providing specific textual feedback. The Writer conditions on feedback history plus previous drafts to generate improved markdown → LaTeX → PDF. This continues until scores reach threshold or max rounds. Evidence shows progressive improvement—clarity scores rise from ~7 to 10, layout from ~8 to 10 within 4 rounds (Figure 4), with later revisions winning pairwise comparisons even when round identity is hidden (Figure 5).

### Mechanism 2: Domain Knowledge Injection
The Retriever agent extracts hypothesis trees from unstructured consultant PowerPoint slides, validates via SQL execution, and structures them as few-shot examples in the Researcher's prompt. This guides SQL query formulation and reasoning patterns. Evidence shows reports with Retriever knowledge (indices 10-19) consistently win pairwise comparisons against those without (0-9) (Figure 8). The hypothesis tree extraction follows Barbara Minto's Pyramid Principle (hierarchical argument structure).

### Mechanism 3: LLM-Based Evaluation Proxy
Given N reports, an LLM performs K pairwise comparisons per pair, recording wins/losses/ties. Aggregated scores correlate with averaged human expert scores (Pearson r=0.6, p<0.01). This enables selecting the best report from parallel generations without human oversight. Evidence shows LLM pairwise vs. human score correlation (Figure 7) with six defined scoring criteria (non-triviality, justification, clarity, feasibility, balance, overall).

## Foundational Learning

- **Concept: Multi-agent orchestration with state persistence**
  - Why needed: Each agent maintains conversation history and passes structured outputs. Managing agent state, context windows, and handoff protocols is essential.
  - Quick check: Can you explain how the Researcher's query history H is passed to the Writer, and what format the Reviewer receives (hint: it's not PDF)?

- **Concept: Retrieval-Augmented Generation (RAG) with structured extraction**
  - Why needed: The Retriever extracts hypotheses, structures them into trees, and validates via SQL execution. This is RAG + structured reasoning + database validation.
  - Quick check: What is the purpose of the validation step in Eq. 10, where SQL queries are executed against the database?

- **Concept: Iterative refinement with termination criteria**
  - Why needed: The review cycle must balance improvement against cost. Understanding threshold-based termination, max-round limits, and score saturation patterns prevents runaway costs.
  - Quick check: What triggers termination in the review cycle (two conditions)?

## Architecture Onboarding

- **Component map:** Researcher → (SQL queries → DB → results loop) → research history H → Writer → (H + feedback history) → markdown → Python/figures → LaTeX → PDF → Reviewer → (PDF as images + criteria) → scores + feedback → back to Writer → Judge → (N parallel reports) → pairwise comparisons → select best

- **Critical path:**
  1. Researcher's SQL generation and interpretation loop (4-8 queries)
  2. Writer's figure code extraction and execution
  3. Review cycle (up to 4 rounds)
  4. Final report selection via Judge

- **Design tradeoffs:**
  - Temperature settings: 0.8 for generation (creativity), 0.1 for reviewing/scoring (consistency)
  - Individual vs. pairwise evaluation: Pairwise captures more variance but costs more comparisons
  - Few-shot vs. zero-shot: Few-shot with Retriever improves quality but requires domain materials

- **Failure signatures:**
  - Writer misinterprets reviewer feedback (e.g., increases DPI instead of resizing)
  - Reviewer references wrong figure/section in feedback
  - Retriever extracts irrelevant hypotheses from different industries
  - SQL queries return empty results due to exact name mismatches

- **First 3 experiments:**
  1. Run ablation comparing reports with vs. without Retriever knowledge on same prompts (replicate Figure 8 pattern for your domain).
  2. Validate LLM-human correlation in your specific domain: have 2+ humans score 20 reports, compute correlation with LLM pairwise scores.
  3. Trace review rounds: for 5 reports, manually inspect whether reviewer feedback is actionable and whether Writer correctly implements changes (identify systematic error patterns).

## Open Questions the Paper Calls Out

- **Question:** Can MaRGen generate market reports that are measurably equivalent or superior to those produced by professional human consultants in terms of strategic utility?
  - Basis: Authors explicitly state they have "not conducted evaluations on whether the market reports created by MaRGen are equivalent to or better than those created by humans."
  - Why unresolved: Current validation relies on LLM-based evaluation proxies rather than direct competitive benchmarking against expert human outputs.
  - What evidence would resolve it: A blind Turing-test style study where domain experts rate anonymous reports from both MaRGen and human consultants based on the same datasets.

- **Question:** To what extent does integrating a dedicated "Verifier" agent reduce factual inaccuracies and SQL hallucinations compared to the current Reviewer-Writer loop?
  - Basis: Paper mentions "preliminary analysis" on a Verifier agent and identifies "incorporating a data validation process" as necessary future work.
  - Why unresolved: Current system relies on Reviewer focused on clarity and layout, meaning data accuracy is not systematically validated within the autonomous loop.
  - What evidence would resolve it: An ablation study measuring the frequency of numerical errors or hallucinations in reports generated with and without the Verifier agent.

- **Question:** Does the iterative review process result in genuine semantic improvement, or does it lead to "reward hacking" where the Writer optimizes for the Reviewer's specific scoring heuristics?
  - Basis: Section 5.1 discusses concern that "the LLM simply increases the score because each new revision... is expected to be better."
  - Why unresolved: Rapid achievement of perfect scores (10/10) suggests evaluation metrics might be susceptible to optimization strategies that prioritize style over substance.
  - What evidence would resolve it: A study analyzing "insight density" or information gain per round, rather than just scalar score, to ensure revisions add value beyond formatting changes.

## Limitations
- Current validation relies on LLM-based evaluation proxies with moderate 0.6 correlation to human judgment, leaving substantial unexplained variance
- Framework's reliance on specific domain materials (PowerPoint slides) for knowledge injection creates a manual extraction and validation bottleneck
- SQL generation loop effectiveness depends heavily on specific database schema, with no systematic approach to schema variations

## Confidence

- **High:** Iterative review mechanism demonstrably improves scores (clarity from ~7 to 10, layout from ~8 to 10 within 4 rounds), supported by Figure 4 and pairwise comparisons in Figure 5
- **Medium:** Domain knowledge injection via Retriever significantly enhances report quality (Figure 8 pairwise comparisons), but manual effort for hypothesis tree extraction limits practical applicability
- **Medium:** LLM-based evaluation correlates with human judgment (r=0.6, p<0.01), but moderate correlation leaves substantial unexplained variance

## Next Checks

1. **Ablation Test:** Generate 20 reports on identical prompts—10 with Retriever domain knowledge, 10 without. Conduct blind pairwise comparisons and statistical tests to verify knowledge injection improves quality beyond random chance.

2. **Correlation Validation:** Have 3 human experts score 30 reports (different domains than original study) on the six criteria. Compute Pearson correlation between human average scores and LLM pairwise rankings to test generalizability.

3. **Robustness Test:** Apply the framework to 5 different database schemas (varying table structures, naming conventions). Measure SQL query success rates and report quality to identify systematic failure patterns and necessary prompt engineering adaptations.