---
ver: rpa2
title: 'Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on African
  American English'
arxiv_id: '2503.04099'
source_url: https://arxiv.org/abs/2503.04099
tags:
- language
- question
- prompts
- llms
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates dialectal disparities in large
  language models' reasoning accuracy and explanations between Standard American English
  (SAE) and African American English (AAE). The authors develop a linguistically-grounded
  LLM-based dialect conversion method that outperforms state-of-the-art approaches,
  validated by native AAE speakers.
---

# Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on African American English

## Quick Facts
- arXiv ID: 2503.04099
- Source URL: https://arxiv.org/abs/2503.04099
- Reference count: 40
- Key outcome: Systematic accuracy disparities exist between Standard American English and African American English prompts across seven tested LLMs

## Executive Summary
This paper reveals systematic performance gaps in large language models when processing African American English (AAE) versus Standard American English (SAE). The authors develop a linguistically-grounded LLM-based dialect conversion method that outperforms existing approaches, validated by native AAE speakers. Across seven different LLMs tested on MMLU and BigBench-Hard datasets, AAE prompts consistently show 6.8% to 18.2% accuracy drops. The study also finds that explanations for AAE prompts are linguistically simpler (higher Flesch Reading Ease scores) and exhibit different psychological patterns in language use, including more pronouns, social processes, and positive emotions. The authors identify effective mitigation strategies that can reduce these disparities by up to 7.9%.

## Method Summary
The authors evaluated dialectal disparities by converting 2,850 MMLU questions and 1,333 BigBench-Hard questions from SAE to AAE using 11 morphosyntactic features (copula deletion, habitual be, negative concord, etc.) via an LLM-based converter. They tested seven LLMs including GPT-4 Turbo, GPT-3.5, LLaMA 3.1 8B, LLaMA 3.2 3B, Qwen 2.5 3B, Gemma 2 9B, and Mistral 7B. Models were prompted to generate explanations in SAE followed by answer choices. The evaluation used multiple metrics: accuracy (parsed from multiple-choice responses), Flesch Reading Ease Score for readability, LIWC psychological markers for linguistic patterns, and consistency analysis (entropy and BERTScore across 10 generations). The study also tested mitigation strategies including contrastive explanations and in-context examples.

## Key Results
- Accuracy drops of 6.8% to 18.2% for AAE prompts across all tested LLMs and reasoning categories
- AAE explanations show higher readability scores (simpler language) and different linguistic patterns including more pronouns and positive emotions
- Combined mitigation strategies reduce accuracy gaps by up to 7.9%
- Consistency analysis reveals AAE prompts generate more variable outputs across multiple generations

## Why This Works (Mechanism)
The study demonstrates that LLMs exhibit systematic biases in processing dialectal variations, with AAE prompts receiving lower accuracy scores and generating explanations with distinct linguistic characteristics. The dialect conversion method captures key morphosyntactic features of AAE, allowing controlled comparison between language varieties. The LIWC analysis reveals that AAE explanations contain more social and emotional content, suggesting LLMs process these prompts differently at a linguistic level. The consistency analysis shows that AAE prompts produce more variable outputs, indicating less stable reasoning patterns for non-standard dialects.

## Foundational Learning
- Dialect conversion methodology: Rule-based transformation of SAE to AAE using 11 morphosyntactic features; needed to create controlled comparisons between language varieties; quick check: validate converted prompts with native speaker ratings (>7.97/10)
- LIWC psychological analysis: Tool for measuring linguistic patterns including pronouns, social/affective/cognitive/perceptual processes; needed to understand qualitative differences in explanations; quick check: verify LIWC categories sum to expected proportions per 1K tokens
- Consistency measurement: Entropy and BERTScore across multiple generations; needed to assess output stability; quick check: compare consistency scores between SAE and AAE within same model

## Architecture Onboarding

**Component Map:**
Dialect Converter (11-rule LLM) -> LLM Query Engine (SAE+AAE prompts) -> Answer Parser -> Metrics Calculator (Accuracy, FRES, LIWC, Consistency)

**Critical Path:**
Dialect conversion → LLM generation → Answer extraction → Metric computation → Statistical analysis

**Design Tradeoffs:**
- Used LLM-based dialect converter rather than rule-based to capture linguistic nuance, but introduces potential approximation errors
- Selected 11 specific morphosyntactic features balancing coverage and computational efficiency
- Chose temperature 0.7 for generation balancing creativity and consistency
- Limited to 50 questions per MMLU subject for computational feasibility

**Failure Signatures:**
- Unnatural AAE conversions (reality score <7/10) indicating conversion errors
- Answer parser missing embedded options suggesting format mismatch
- Low human agreement with automated parsing (<0.9 Cohen's kappa) indicating extraction issues

**First Experiments:**
1. Validate dialect converter output with native speakers on 100 randomly sampled questions
2. Test answer parser reliability by comparing automated extraction against human labels on 50 questions
3. Verify consistency measurement by comparing entropy scores across temperature variations

## Open Questions the Paper Calls Out
None

## Limitations
- Dialect conversion remains an approximation of authentic AAE use, potentially missing full linguistic complexity
- Sample size of 50 questions per subject may not capture full variability in dialectal effects
- LIWC analysis, designed for written text, may not fully capture spoken dialect nuances
- Mitigation strategies tested on limited subsets, limiting generalizability

## Confidence

**High Confidence:**
- Systematic accuracy disparities exist between SAE and AAE prompts (6.8-18.2% drops)

**Medium Confidence:**
- LIWC-based linguistic analysis shows significant differences in explanation patterns

**Low Confidence:**
- Effectiveness of mitigation strategies may not generalize beyond tested subsets

## Next Checks
1. Replicate findings with larger samples (100+ questions per subject) to verify scalability
2. Conduct blind human evaluation of explanation quality between SAE and AAE responses
3. Test generalizability by applying dialect conversion to other language varieties (British vs. American English)