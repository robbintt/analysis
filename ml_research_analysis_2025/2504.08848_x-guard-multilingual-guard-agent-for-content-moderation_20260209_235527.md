---
ver: rpa2
title: 'X-Guard: Multilingual Guard Agent for Content Moderation'
arxiv_id: '2504.08848'
source_url: https://arxiv.org/abs/2504.08848
tags:
- safety
- text
- label
- language
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The X-Guard agent was developed to address multilingual safety
  evaluation challenges for LLMs, focusing on low-resource languages and code-switching
  attacks. The method employs a two-stage architecture combining a custom-finetuned
  mBART-50 translation module with an X-Guard 3B evaluation model, trained using supervised
  fine-tuning and GRPO.
---

# X-Guard: Multilingual Guard Agent for Content Moderation

## Quick Facts
- arXiv ID: 2504.08848
- Source URL: https://arxiv.org/abs/2504.08848
- Reference count: 40
- 70.38% accuracy and 70.44% F1-score across 132 languages on 65K samples

## Executive Summary
X-Guard addresses critical safety evaluation challenges for large language models (LLMs) in multilingual contexts, particularly focusing on low-resource languages and code-switching attacks. The system employs a two-stage architecture combining a custom-finetuned mBART-50 translation module with an X-Guard 3B evaluation model, trained using supervised fine-tuning and GRPO. A jury of judges approach mitigates individual LLM biases, providing transparent reasoning in safety decisions. The agent demonstrates robust performance across 132 languages with 70.38% accuracy and successfully defends against Sandwich Attacks with 83% accuracy versus 62% for Llama-Guard-8B.

## Method Summary
X-Guard employs a two-stage architecture where input content is first processed through a custom-finetuned mBART-50 translation module, followed by evaluation using the X-Guard 3B model. The system utilizes supervised fine-tuning and GRPO for training, with a jury of judges approach to mitigate individual LLM biases. Safety decisions include transparent reasoning, and the system is designed to handle multilingual contexts, including low-resource languages and code-switching scenarios. The evaluation encompasses 65K samples across 132 languages, with particular attention to defending against adversarial attacks like Sandwich Attacks.

## Key Results
- 70.38% accuracy and 70.44% F1-score across 132 languages on 65K samples
- 97.20% accuracy on English-only data subset
- 83% accuracy defending against Sandwich Attacks versus 62% for Llama-Guard-8B
- Weighted F1-score of 52.37% for category classification across all languages

## Why This Works (Mechanism)
The two-stage architecture with translation followed by evaluation enables effective processing of multilingual content by first normalizing inputs into a common evaluation space. The jury of judges approach reduces individual model biases through consensus-based decision making, while the combination of supervised fine-tuning and GRPO training ensures robust learning from both labeled examples and reinforcement signals. The system's transparency in safety decisions allows for better interpretability and trust in moderation outcomes.

## Foundational Learning
- **Supervised Fine-Tuning**: Training on labeled safety datasets to establish baseline performance - needed to create initial safety evaluation capabilities; quick check: verify labeled dataset quality and coverage
- **GRPO Training**: Reinforcement learning with group relative policy optimization - needed to refine safety judgments through reward-based feedback; quick check: validate reward signal design and consistency
- **Code-Switching Detection**: Identifying mixed-language content patterns - needed to handle real-world multilingual communication; quick check: test with diverse language mixing scenarios
- **Jury Aggregation Methods**: Consensus-based decision making across multiple models - needed to reduce individual model biases; quick check: evaluate jury diversity and independence
- **Translation Module Fine-Tuning**: Adapting mBART-50 for safety-specific content - needed to ensure accurate cross-lingual understanding; quick check: assess translation quality on safety-relevant terminology
- **Sandwich Attack Defense**: Protecting against prompt injection attacks - needed to maintain safety evaluation integrity; quick check: test with various attack pattern variations

## Architecture Onboarding

**Component Map**: Input -> mBART-50 Translation -> X-Guard 3B Evaluation -> Jury Aggregation -> Safety Decision

**Critical Path**: Translation module → Safety evaluation → Jury consensus → Final decision with reasoning

**Design Tradeoffs**: The choice of 3B parameter model balances performance with computational efficiency, while the jury approach trades off single-model simplicity for improved bias mitigation and robustness.

**Failure Signatures**: Performance degradation on low-resource languages, inconsistent translations affecting safety evaluation, jury disagreement patterns indicating ambiguous content, vulnerability to coordinated adversarial attacks.

**Three First Experiments**:
1. Test translation accuracy on safety-critical terminology across target language families
2. Evaluate jury consensus reliability under varying attack intensities
3. Measure performance degradation when individual judge models are compromised

## Open Questions the Paper Calls Out
None

## Limitations
- Jury of three judge models may not be sufficiently diverse or robust against coordinated attacks
- Performance across 132 languages shows substantial variation, with only 70.38% overall accuracy
- Category classification performance remains weak at 52.37% weighted F1-score across all languages

## Confidence
- **High Confidence**: Technical architecture clearly described and empirical results showing 83% vs 62% superiority in Sandwich Attacks are verifiable
- **Medium Confidence**: Generalizability across 132 languages uncertain due to potential data imbalance and lack of per-language performance details
- **Medium Confidence**: Jury aggregation approach methodologically sound but implementation details and effectiveness need further validation

## Next Checks
1. Conduct per-language performance analysis to identify degraded performance patterns and systematic translation errors for specific language families
2. Test jury aggregation system against adversarial attacks designed to exploit consensus mechanisms through coordinated prompt engineering
3. Evaluate system performance on dynamic real-world content streams with varying context lengths and multimodal inputs to assess practical deployment robustness