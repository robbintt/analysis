---
ver: rpa2
title: Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image
  Diffusion Models
arxiv_id: '2505.02824'
source_url: https://arxiv.org/abs/2505.02824
tags:
- watermarked
- diffusion
- dataset
- samples
- ceat2i
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CEAT2I, the first copyright evasion attack
  specifically targeting dataset ownership verification (DOV) in text-to-image diffusion
  models. The core insight is that watermarked samples exhibit faster convergence
  in intermediate feature representations during fine-tuning, enabling reliable detection.
---

# Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2505.02824
- Source URL: https://arxiv.org/abs/2505.02824
- Reference count: 40
- Primary result: Introduces CEAT2I, first copyright evasion attack that reduces watermark success rate by up to 97.1% while maintaining high generation quality

## Executive Summary
This paper introduces CEAT2I, the first copyright evasion attack specifically targeting dataset ownership verification (DOV) in text-to-image diffusion models. The core insight is that watermarked samples exhibit faster convergence in intermediate feature representations during fine-tuning, enabling reliable detection. CEAT2I operates in three stages: (1) watermarked sample detection using feature deviation analysis, (2) trigger identification via word-level ablation of input prompts, and (3) watermark mitigation using closed-form concept erasure. Experiments across three datasets and four DOV methods show that CEAT2I reduces watermark success rate by up to 97.1% while maintaining high generation quality (CLIP similarity >89%). The method successfully evades both global and local watermarking attacks, demonstrating robustness even against adaptive defenses.

## Method Summary
CEAT2I is a three-stage pipeline designed to neutralize backdoor-based dataset ownership verification watermarks in text-to-image diffusion models. The attack begins by fine-tuning the model on watermarked data and detecting watermarked samples through feature deviation analysis at early epochs (T_e=30), exploiting the fact that watermarked samples converge faster. Identified watermarked samples are then processed through token ablation to pinpoint trigger words that activate the watermark. Finally, closed-form concept erasure modifies cross-attention weights to remove the watermark association while preserving generation quality. The method is evaluated across three datasets (Pokemon, Ossaili, Pranked03) and four DOV watermarking techniques, demonstrating significant reduction in watermark detectability while maintaining CLIP similarity scores above 89%.

## Key Results
- Reduces watermark success rate by up to 97.1% across all tested DOV methods
- Maintains high generation quality with CLIP similarity >89% post-mitigation
- Achieves 100% trigger identification accuracy across all DOV methods and datasets
- Successfully evades both global and local watermarking attacks
- Robust to adaptive defenses that modify watermark injection during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Watermarked samples exhibit faster convergence in intermediate feature representations during fine-tuning, enabling reliable detection via feature deviation analysis.
- Mechanism: Compute L2 distance between layer-wise activations of original and fine-tuned models at early epochs (T_e=30). Watermarked samples show larger feature deviations (L_f^i) across layers because trigger-target correlations create low-entropy, high-gain learning directions that are preferentially amplified early in training. A voting mechanism flags samples where deviation exceeds threshold α_1 across more than α_2 layers.
- Core assumption: The backdoor-based watermarking introduces discriminative trigger-target mappings that require less representational relevance (I(Z;Y)) to achieve target mutual information compared to benign samples.
- Evidence anchors:
  - [abstract]: "watermarked samples exhibit faster convergence in intermediate feature representations during fine-tuning, enabling reliable detection"
  - [Section IV-C, Eq. 3-4]: Formal definition of feature deviation L_f^i and detection threshold
  - [Fig. 4]: Empirical distribution showing watermarked samples have higher feature deviation than benign across 4 DOV methods
  - [corpus]: Weak direct corpus support; DATABench (arXiv:2507.05622) discusses dataset auditing but not feature-based detection specifically
- Break condition: Detection fails if watermarked and benign samples converge at similar rates (e.g., adaptive defense minimizing feature deviation per Eq. 11), or if watermarking rate is very low (γ < 5%).

### Mechanism 2
- Claim: Trigger tokens can be identified by iteratively ablating words from input prompts and detecting outlier shifts in feature representations.
- Mechanism: For each detected watermarked sample, tokenize the prompt y_w into L tokens. Create L variants by removing each token individually. Compute feature deviation L_tr^i at the second-to-last convolutional layer for each variant (Eq. 5). Tokens whose removal causes deviation exceeding μ+σ (statistical outlier threshold) are candidate triggers. Final trigger selected via frequency analysis across all watermarked samples.
- Core assumption: Trigger tokens dominate feature activations in watermarked T2I models, so their removal causes significant feature deviation compared to benign tokens.
- Evidence anchors:
  - [abstract]: "iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens"
  - [Section IV-D, Eq. 5-7]: Formal definition of token-level deviation and trigger selection
  - [Table V]: 100% trigger identification accuracy across 4 DOV methods and 3 datasets
  - [corpus]: AGATE (arXiv:2504.21044) uses OoD data as backdoor triggers but doesn't address trigger identification methods
- Break condition: Fails if multiple tokens jointly form the trigger without individual dominance, or if triggers are semantically similar to benign words (low discriminability).

### Mechanism 3
- Claim: Closed-form concept erasure on cross-attention weights removes watermark effects while preserving generation quality without additional fine-tuning.
- Mechanism: Define target embedding v*_w = W_ori × T(y_w \ŷ_tr) for watermarked text with trigger removed. Solve minimization problem (Eq. 9) to find new cross-attention weights W that: (1) maps watermarked texts to their trigger-free embeddings, (2) preserves original outputs for benign texts. Closed-form solution (Eq. 10) directly updates attention weights without iterative optimization.
- Core assumption: Cross-attention layers are the primary mechanism linking textual triggers to visual watermark outputs; modifying these weights suffices to break the association.
- Evidence anchors:
  - [abstract]: "closed-form concept erasure method to remove the injected watermarks"
  - [Section IV-E, Eq. 8-10]: Formal optimization problem and closed-form solution
  - [Table I]: CLIP similarity >89% maintained while WSR reduced by up to 97.1%
  - [corpus]: "Removing Watermarks with Partial Regeneration" (arXiv:2505.08234) addresses watermark removal but via regeneration, not weight editing
- Break condition: Fails if watermark associations are encoded in non-attention layers (e.g., FFN or ResNet blocks), or if triggers have distributed representations across multiple attention heads.

## Foundational Learning

- Concept: **Backdoor-based Dataset Ownership Verification (DOV)**
  - Why needed here: CEAT2I is designed to evade DOV; understanding how backdoor triggers associate text tokens with target outputs is prerequisite for understanding the attack surface.
  - Quick check question: Can you explain why DOV uses backdoor techniques rather than simple image watermarks for ownership verification?

- Concept: **Information Bottleneck Principle in Representation Learning**
  - Why needed here: The paper justifies faster convergence of watermarked samples via IB theory (I(Z;Y) - β·I(Z;X) minimization). Watermarked samples have lower conditional entropy H(X|Y), requiring less representational relevance.
  - Quick check question: Why would watermarked samples with deterministic trigger-target mappings require less I(Z;Y) than benign samples with diverse outputs?

- Concept: **Cross-Attention Mechanisms in Latent Diffusion Models**
  - Why needed here: CEAT2I's mitigation targets cross-attention weights specifically. Understanding how text embeddings c = T(y) condition the denoising network via attention is essential for implementing Eq. 10.
  - Quick check question: In Stable Diffusion's U-Net, which layers receive text conditioning, and why might cross-attention be more vulnerable to backdoor injection than self-attention?

## Architecture Onboarding

- Component map: Input: Watermarked dataset D_wm → Stage 1: Fine-tune model for T_e=30 epochs → Feature Deviation Analysis (all U-Net layers) → Output: D_w (watermarked samples), D_b (benign samples) → Stage 2: Continue fine-tuning to T_total=100 epochs → Token Ablation Analysis (second-to-last conv layer) → Output: ŷ_tr (identified trigger tokens) → Stage 3: Closed-form weight editing (cross-attention layers only) → Output: Watermark-free T2I model

- Critical path: **Feature deviation calculation at early epochs (T_e=30)** is the gating step. If detection accuracy drops (e.g., <80%), downstream trigger identification and mitigation degrade. The paper reports 95-100% detection accuracy across all DOV methods (Table II), but this is highly sensitive to threshold selection (α_1, α_2) and detection epoch timing (Fig. 6 shows peak at T_e=30).

- Design tradeoffs:
  - **Detection epoch T_e**: Earlier (T_e<30) → less convergence signal; later (T_e>30) → benign samples catch up, reducing discriminability
  - **Threshold α_1/α_2**: Higher → fewer false positives but more missed watermarks; lower → better recall but more noisy detection
  - **Layer selection for Stage 2**: Second-to-last conv layer maximizes benign/watermarked deviation difference (Fig. 8), but other layers may be more robust to adaptive defenses

- Failure signatures:
  - WSR remains >10%: Likely trigger misidentification; check token ablation deviation distribution for outliers
  - CLIP score drops >5%: Overly aggressive weight editing; benign samples may be misclassified as watermarked
  - Detection accuracy <80%: Check if adaptive defense (Eq. 11) is being used; feature deviation distribution may overlap between classes

- First 3 experiments:
  1. **Baseline detection validation**: Fine-tune Stable Diffusion v1.4 on Pokemon dataset with BadT2I-L watermark (γ=20%). Compute feature deviation distributions at T_e=30, verify separation between watermarked and benign samples matches Fig. 4.
  2. **Threshold sensitivity sweep**: Vary α_1 ∈ [0.28, 0.44] and α_2 ∈ [10, 19] per Fig. 7. Plot detection accuracy heatmap to confirm optimal region around (0.4, 15).
  3. **End-to-end attack validation**: Run full CEAT2I pipeline on all 4 DOV methods. Measure WSR and CLIP scores; compare against Table I baselines. Specifically check BadT2I-L (local watermark) where T2IShield fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CEAT2I framework be effectively adapted for text-to-video or text-to-3D generative models?
- Basis in paper: [explicit] Section VI states that while T2I dominates, the ecosystem is evolving, and the method has not been validated outside image generation tasks.
- Why unresolved: Architectural and cross-modal alignment mechanisms differ significantly between T2I and video/3D models.
- What evidence would resolve it: Successful application of convergence-based evasion to video or 3D benchmarks.

### Open Question 2
- Question: Can the three-stage pipeline be integrated into a single end-to-end framework to minimize computational overhead?
- Basis in paper: [explicit] Section VI identifies the extra resource consumption of feature extraction as a primary limitation.
- Why unresolved: The current method relies on a sequential process of detection, identification, and mitigation.
- What evidence would resolve it: A unified model that achieves comparable Watermark Success Rate (WSR) reduction with significantly lower latency.

### Open Question 3
- Question: How can dataset ownership verification (DOV) methods be redesigned to resist convergence-based detection?
- Basis in paper: [explicit] Section VI and the Conclusion call for the design of more robust DOV methods resistant to CEAT2I.
- Why unresolved: Current DOV methods create strong trigger-target correlations that cause faster convergence, which CEAT2I detects.
- What evidence would resolve it: A watermarking technique that maintains verification capabilities while normalizing feature convergence rates to match benign samples.

## Limitations
- Adaptive defense gap: Limited evaluation against defenses that could modify watermark injection to minimize feature deviation
- Closed-form solution generality: Assumes watermark associations are primarily in cross-attention, may not work for distributed watermarking schemes
- Dataset specificity: Evaluation on niche datasets; real-world heterogeneous datasets and lower watermarking rates remain untested

## Confidence

**High Confidence**: Core detection mechanism (Stage 1) and token ablation approach (Stage 2) are well-supported with 95-100% detection accuracy across all tested DOV methods and datasets.

**Medium Confidence**: Closed-form concept erasure (Stage 3) shows strong quantitative results but theoretical justification for cross-attention-only modification is less rigorous.

**Low Confidence**: Claims about effectiveness against adaptive defenses and sophisticated watermarking schemes are speculative without empirical evidence.

## Next Checks

1. **Adaptive Defense Evaluation**: Implement and evaluate against the proposed adaptive defense mechanism (Eq. 11) that modifies watermark injection during training to minimize feature deviation. Test whether CEAT2I's detection accuracy drops below the critical 80% threshold when attackers can adapt their watermarking strategy based on intermediate model checkpoints.

2. **Layer-wise Mitigation Analysis**: Extend the closed-form concept erasure to include additional layer types beyond cross-attention (e.g., self-attention, FFN layers, ResNet blocks). Compare WSR reduction and CLIP score maintenance across different layer combinations to determine whether cross-attention editing alone is sufficient or whether a more comprehensive approach is needed for complex watermarking schemes.

3. **Real-world Dataset Validation**: Evaluate CEAT2I on larger, more diverse datasets with realistic prompt distributions and image characteristics. Test with lower watermarking rates (γ < 10%) and varying dataset sizes to assess the method's robustness under practical conditions where watermark signals may be weaker and more distributed across the training corpus.