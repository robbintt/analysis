---
ver: rpa2
title: Physics-informed features in supervised machine learning
arxiv_id: '2504.17112'
source_url: https://arxiv.org/abs/2504.17112
tags:
- features
- feature
- learning
- machine
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a physics-informed approach to feature-based
  machine learning that enhances model interpretability and predictive performance.
  The key idea is to construct non-linear feature maps informed by physical laws and
  dimensional analysis, creating physics-informed features (PIFs) that are dimensionally
  homogeneous.
---

# Physics-informed features in supervised machine learning

## Quick Facts
- **arXiv ID:** 2504.17112
- **Source URL:** https://arxiv.org/abs/2504.17112
- **Reference count:** 3
- **Primary result:** Physics-informed features improve model interpretability and predictive performance in supervised learning tasks involving physical systems.

## Executive Summary
This paper introduces a physics-informed approach to feature-based machine learning that enhances both interpretability and predictive performance. The method constructs non-linear feature maps informed by physical laws and dimensional analysis, creating physics-informed features (PIFs) that are dimensionally homogeneous with the target labels. This approach addresses the limitation of standard feature-based machine learning where features represent different physical quantities with distinct meanings and dimensions, potentially compromising model explainability. The framework was validated through three synthetic experiments and one real-world application, demonstrating improved performance metrics and successful identification of governing physical mechanisms.

## Method Summary
The method constructs Physics-Informed Features (PIFs) by combining original physical features according to domain-specific laws, ensuring dimensional homogeneity with target labels. These PIFs are standardized and used in ridge regression with Tikhonov regularization. A greedy feature ranking algorithm identifies the most predictive PIFs based on coefficient magnitude and performance saturation. The approach maintains interpretability by recovering physical coefficients through de-standardization, enabling direct comparison with known physical equations when available.

## Key Results
- Physics-informed features consistently outperformed standard features across all experiments, with MAE reductions from 0.179 to 0.067 in the Bernoulli equation experiment at 10% noise.
- Feature ranking successfully identified the most predictive PIFs, which corresponded to correct physical equations in synthetic cases.
- In solar flare forecasting, skill scores improved (TSS from 0.590 to 0.642, HSS from 0.430 to 0.561), with magnetic helicity identified as the most predictive feature.
- The method demonstrated robustness to noise levels (10%, 30%, 50%) while maintaining performance advantages over standard approaches.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constructing dimensionally homogeneous features constrains the hypothesis space to physically meaningful combinations, reducing the effective search space for the learning algorithm.
- **Mechanism:** The paper constructs feature maps ϕ that combine original features according to physics-driven laws, producing Physics-Informed Features (PIFs) that share the same physical dimension as the target labels. This dimensional coherence means the learned coefficients β directly represent physically interpretable weights rather than arbitrary scaling factors applied to incompatible quantities.
- **Core assumption:** The underlying physical relationship governing the target variable can be expressed as a linear combination of dimensionally homogeneous non-linear feature maps.
- **Evidence anchors:**
  - [abstract] "constructs non-linear feature maps informed by physical laws and dimensional analysis. These maps enhance model interpretability..."
  - [Section 2] "Our goal was to address this limitation by building new feature maps ϕ that combine the original features according to physics-driven laws, resulting in dimensionally homogeneous, physics-informed features (PIFs)."
  - [corpus] Related work on physics-consistent ML (arxiv:2502.15755) similarly constrains outputs to physical manifolds, suggesting domain constraints are a broader effective strategy.
- **Break condition:** If the true physical mechanism involves quantities or relationships not representable through the constructed PIF combinations, or if the governing physics involves fundamentally different dimensional structures than assumed.

### Mechanism 2
- **Claim:** Physics-informed feature maps enable linear regression models to capture non-linear physical relationships while maintaining interpretable coefficients.
- **Mechanism:** Rather than applying a generic kernel function chosen without physical consideration, the method constructs explicit feature maps where each component represents a physically meaningful combination. The forward model becomes g(x) = Σᵢ βᵢ · ϕᵢ(x), where each ϕᵢ(x) is a physics-derived term. Ridge regression then solves for β̂ = (ΦᵀΦ + λI)⁻¹ΦᵀY. The de-standardization process (Remark 3.1) recovers the physical coefficients, enabling direct comparison to known physical equations.
- **Core assumption:** The complexity of the physical relationship is captured by the non-linearity in the feature maps, not by requiring non-linear combinations in the coefficient space.
- **Evidence anchors:**
  - [Section 2] "Machine learning here has therefore the two-fold task to compute the right values for β₁, β₂ and β₃, and to select the most predictive PIFs by means of some feature ranking algorithm."
  - [Section 3.1, Table 3] The de-standardized coefficients (0.998, 0.500, 1.005) closely match the Bernoulli equation coefficients (1, 0.5, 1).
  - [corpus] Physics-Informed Neural Networks (arxiv:2501.06572) incorporate physical laws into loss functions rather than feature maps—an alternative mechanism that may be complementary but serves similar goals.
- **Break condition:** If the target relationship requires higher-order interactions between PIFs themselves (e.g., products or compositions of physics-informed terms), the linear combination assumption fails.

### Mechanism 3
- **Claim:** Feature ranking applied to physics-informed features can identify governing physical mechanisms when the true equations are unknown.
- **Mechanism:** A greedy ranking algorithm (Algorithm 3.2) ranks SPIFs by regression coefficient magnitude, then recursively adds features until MAE/MSE saturation occurs. The highest-ranked features correspond to the most predictive physical combinations. In the solar flare experiment, PIF2 = Φ·I (magnetic flux × electric current, measuring magnetic helicity) emerged as most predictive—matching independent physics literature identifying helicity as central to flare energy budgets.
- **Core assumption:** The features most predictive of the target correspond to the governing physical mechanism, and less relevant physical combinations contribute diminishing returns.
- **Evidence anchors:**
  - [Section 1] "when these physical laws are not known, the application of feature ranking algorithms to the physics-informed features allows the selection of the feature maps that mostly impact the prediction process, which helps to identify the possible physical mechanism"
  - [Section 5] "the ranking algorithm applied to the PIFs in this application points out PIF2 = ΦI as the most predictive operation for flare forecasting... the product of the magnetic flux times the electric current is a measure of the so-called magnetic helicity"
  - [corpus] Weak corpus signal: Related work on symbolic regression (arxiv:2512.04204) discovers empirical laws through equation search, but the PIF approach differs by constraining search to dimensionally coherent physics combinations.
- **Break condition:** If multiple distinct physical mechanisms contribute comparably to the target, or if spurious correlations from limited data cause irrelevant PIFs to rank highly, discovery reliability degrades.

## Foundational Learning

- **Concept: Ridge Regression and Tikhonov Regularization**
  - **Why needed here:** The paper uses ridge regression as its primary learning algorithm. Understanding the regularization parameter λ's role in balancing fit vs. generalization is essential for reproducing results and diagnosing overfitting.
  - **Quick check question:** Can you explain why adding λI to XᵀX before inversion stabilizes the solution when features are correlated?

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** Proposition 2.1 establishes the formal connection between physics-informed feature maps and RKHS theory, showing that PIFs define a kernel K(x,r) = ⟨ϕ(x), ϕ(r)⟩. This grounds the method in established learning theory.
  - **Quick check question:** How does the representer theorem guarantee that the solution lies in the span of the kernel evaluations on training points?

- **Concept: Dimensional Analysis and Buckingham π Theorem**
  - **Why needed here:** Constructing PIFs requires systematically combining physical quantities to achieve dimensional homogeneity with the target. Dimensional analysis provides the principled basis for generating these combinations.
  - **Quick check question:** Given features with dimensions [M], [L], [T] and a target with dimension [M][L]²/[T]², how would you identify valid dimensionless groups?

## Architecture Onboarding

- **Component map:** Input Layer -> PIF Constructor -> Standardization Layer -> Learning Core -> De-standardization Layer -> Feature Ranker
- **Critical path:** The PIF construction step is the highest-leverage component. If PIFs don't include combinations close to the true physical relationship, no amount of regularization or ranking can recover it. Start by identifying the target dimension, then systematically enumerate dimensionally consistent combinations of input features.
- **Design tradeoffs:**
  - **PIF count vs. interpretability:** More PIFs increase coverage of possible physics but dilute ranking signals. The paper uses p ≈ m (same count as original features) but this is a design choice, not a constraint.
  - **Standardization necessity:** Remark 3.1 notes standardization reduces numerical issues but requires de-standardization to recover physical coefficients. Skipping standardization risks numerical instability with features spanning many orders of magnitude.
  - **Ridge vs. other learners:** The paper notes SVM regression produces similar results, suggesting the mechanism is learner-agnostic. Choose based on computational constraints and hyperparameter tuning familiarity.
- **Failure signatures:**
  - **PIFs perform worse than standard features:** Likely indicates PIFs miss the relevant physics. Check whether target dimension was correctly identified and whether domain knowledge was insufficient.
  - **Coefficients don't match known physics in synthetic experiments:** May indicate insufficient training data, excessive noise, or regularization parameter λ too large (over-regularizing toward zero).
  - **Feature ranking selects implausible combinations:** Check for multicollinearity among PIFs—highly correlated features make coefficient-based ranking unreliable.
- **First 3 experiments:**
  1. **Reproduce Bernoulli synthetic experiment:** Generate 1000 samples with 10% noise, construct 7 PIFs with pressure dimension, verify that ranking recovers PIF1-3 and de-standardized coefficients approximate (1, 0.5, 1). This validates the full pipeline with known ground truth.
  2. **Ablation study on PIF coverage:** Using the pulsar experiment framework, systematically vary which PIFs are included in training. Quantify performance degradation when the "correct" PIF is excluded (as done with PIF1* in Table 4) to understand robustness requirements.
  3. **Transfer to new domain:** Apply the framework to a physical system not covered in the paper (e.g., ideal gas law, circuit analysis) where ground truth is known. Document the PIF construction rationale and compare discovered coefficients to theoretical values. This tests whether the method generalizes beyond the paper's specific domains.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness depends on correctly identifying the target dimension and having sufficient domain knowledge to construct relevant PIFs.
- Performance can degrade if the true physical relationship involves interactions between PIFs themselves, violating the linear combination assumption.
- The greedy feature ranking algorithm may select spurious correlations in limited data scenarios where multiple physical mechanisms contribute comparably to the target.

## Confidence
- **Synthetic experiments with known ground truth:** High confidence - the method successfully recovered known physical equations with accurate coefficients.
- **Real-world solar flare forecasting:** Medium confidence - demonstrated improved performance metrics and identified physically meaningful features, though validation against independent methods is limited.
- **Generalization to new domains:** Medium-Low confidence - success in three physical domains suggests promise, but broader applicability requires further testing across diverse physical systems.

## Next Checks
1. Reproduce the Bernoulli synthetic experiment to verify the complete pipeline works with known ground truth.
2. Conduct an ablation study varying PIF coverage to understand robustness requirements.
3. Apply the framework to a new physical domain (e.g., ideal gas law) to test generalization beyond the paper's specific examples.