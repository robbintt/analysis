---
ver: rpa2
title: ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on Heart
  Sounds
arxiv_id: '2502.16914'
source_url: https://arxiv.org/abs/2502.16914
tags:
- heart
- data
- sounds
- audio
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ENACT-Heart, an ensemble-based method combining
  CNN and Vision Transformer (ViT) models for heart sound classification. The authors
  address the challenge of accurately diagnosing cardiovascular anomalies by converting
  audio heart sounds into visual representations (spectrograms and centroid graphs)
  and leveraging a Mixture of Experts (MoE) framework to integrate both models' predictions.
---

# ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on Heart Sounds

## Quick Facts
- arXiv ID: 2502.16914
- Source URL: https://arxiv.org/abs/2502.16914
- Reference count: 17
- Primary result: Ensemble of CNN and Vision Transformer achieves 97.52% accuracy on 5-class heart sound classification

## Executive Summary
ENACT-Heart introduces an ensemble method combining CNN and Vision Transformer (ViT) models for classifying heart sounds into five categories: artifact, extrahls, extrastole, murmur, and normal. The approach converts audio heart sounds into two visual modalities—spectrograms for ViT and centroid graphs for CNN—and integrates their predictions using a Mixture of Experts framework. With extensive data augmentation (Gaussian noise, 9 augmentations per file) and careful preprocessing (5-second segmentation, 195 Hz low-pass filtering), the method achieves state-of-the-art performance of 97.52% accuracy, surpassing individual model baselines.

## Method Summary
ENACT-Heart addresses heart sound classification by transforming audio recordings into visual representations and applying ensemble learning. Audio files (1-30 seconds) are segmented to 5-second clips with zero-padding, then augmented with Gaussian noise (9 versions per file). Two visual modalities are generated: spectrograms (with 195 Hz low-pass filter) for the ViT model and centroid graphs (waveform + spectral centroid overlay) for the CNN. Both models are trained independently (batch size 32, 50 epochs, Adam optimizer, learning rate 0.001) and their predictions combined via weighted ensemble (MoE), where weights are optimized through grid search (w_ViT ∈ {0, 0.05, ..., 1.0}). The ensemble achieves superior performance compared to individual models (ViT: 93.88%, CNN: 95.45%).

## Key Results
- Ensemble accuracy: 97.52% (primary metric)
- Precision: 0.98
- Recall: 0.97
- F1-score: 0.98

## Why This Works (Mechanism)
The ensemble leverages complementary strengths of CNN and Vision Transformer architectures. CNNs excel at capturing local spatial patterns in the centroid graphs, while ViTs capture long-range dependencies in spectrograms. The Mixture of Experts framework optimally weights these complementary perspectives, with data augmentation and careful preprocessing further enhancing robustness and generalization.

## Foundational Learning
- **Spectrogram generation**: Converts time-domain audio to time-frequency representation; needed for ViT input to capture frequency patterns over time
  - Quick check: Verify spectrogram dimensions match ViT patch size requirements
- **Spectral centroid calculation**: Measures "brightness" of sound; needed for centroid graph to highlight cardiac event characteristics
  - Quick check: Ensure centroid values normalized consistently across audio files
- **Data augmentation with Gaussian noise**: Increases dataset diversity; needed to improve model robustness to real-world variations
  - Quick check: Confirm augmentation applied uniformly across all audio files
- **Mixture of Experts ensemble**: Combines multiple model predictions; needed to leverage complementary model strengths
  - Quick check: Validate ensemble weights sum to 1 and optimal weight identified through grid search

## Architecture Onboarding

**Component map**: Audio files -> Segmentation/Zero-padding -> Augmentation -> Spectrogram/Centroid Graph Generation -> ViT/CNN Training -> Weighted Ensemble -> Final Classification

**Critical path**: Data preprocessing (segmentation, augmentation, visual generation) → Model training (ViT on spectrograms, CNN on centroid graphs) → Ensemble weight optimization → Final classification

**Design tradeoffs**: The choice of visual modalities (spectrograms vs centroid graphs) represents a tradeoff between capturing comprehensive frequency information (spectrograms) versus highlighting specific cardiac event features (centroid graphs). The ensemble framework mitigates this by combining both perspectives.

**Failure signatures**: Performance degradation may occur due to class imbalance in the PASCAL dataset, inconsistent audio preprocessing leading to shape mismatches, or suboptimal ensemble weights failing to properly combine model strengths.

**First experiments**:
1. Verify data preprocessing pipeline produces consistent input dimensions for both CNN and ViT
2. Train individual CNN and ViT models to establish baseline performance
3. Implement weighted ensemble with grid search over w_ViT values to find optimal combination

## Open Questions the Paper Calls Out
None

## Limitations
- Specific ViT architecture details (patch size, number of heads, pretrained weights) not specified
- CNN architecture details (filter counts/sizes, dropout rate, dense layer configuration) not provided
- Train/validation/test split ratios not disclosed
- Optimal ensemble weight (w_ViT) that achieved 97.52% accuracy not stated

## Confidence
- High confidence in ensemble concept and data augmentation strategy
- Medium confidence in exact model architectures and hyperparameters
- Medium confidence in reproducibility of 97.52% accuracy claim without further specification

## Next Checks
1. Confirm the optimal w_ViT value and corresponding ensemble accuracy from the reported grid search
2. Verify the train/validation/test split and ensure consistent data preprocessing across modalities
3. Test the robustness of the method to class imbalance by reporting per-class F1 scores and confusion matrices