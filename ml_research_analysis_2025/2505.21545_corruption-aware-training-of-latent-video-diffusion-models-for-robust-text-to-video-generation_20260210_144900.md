---
ver: rpa2
title: Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video
  Generation
arxiv_id: '2505.21545'
source_url: https://arxiv.org/abs/2505.21545
tags:
- corruption
- bcni
- noise
- video
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CAT-LVDM, the first corruption-aware training
  framework for Latent Video Diffusion Models (LVDMs) that improves robustness to
  noisy, web-scale video-text data. The core method involves structured, data-aligned
  noise injection: Batch-Centered Noise Injection (BCNI), which perturbs embeddings
  along intra-batch semantic directions to preserve temporal coherence, and Spectrum-Aware
  Contextual Noise (SACN), which injects noise along dominant spectral modes to enhance
  low-frequency smoothness.'
---

# Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation

## Quick Facts
- **arXiv ID:** 2505.21545
- **Source URL:** https://arxiv.org/abs/2505.21545
- **Reference count:** 40
- **Primary result:** Introduces CAT-LVDM, achieving 31.9% FVD improvement on caption-rich datasets via Batch-Centered Noise Injection (BCNI)

## Executive Summary
This paper introduces CAT-LVDM, the first corruption-aware training framework for Latent Video Diffusion Models (LVDMs) that improves robustness to noisy, web-scale video-text data. The core method involves structured, data-aligned noise injection: Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal coherence, and Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral modes to enhance low-frequency smoothness. On average, BCNI reduces FVD by 31.9% across caption-rich datasets (WebVid-2M, MSR-VTT, MSVD), while SACN achieves a 12.3% improvement on UCF-101. The approach is supported by theoretical analysis showing improved entropy, Wasserstein, and generalization bounds. Code and models are available at https://github.com/chikap421/catlvdm.

## Method Summary
CAT-LVDM implements structured noise injection during training of LVDMs by modifying the text embeddings that condition the U-Net. BCNI perturbs each embedding along its deviation from the batch mean, scaling noise by this magnitude to confine corruption to a low-rank semantic subspace. SACN performs SVD on the embedding matrix and injects noise weighted by singular values to target dominant spectral modes. Both methods aim to preserve semantic coherence while regularizing the model against noise. The approach integrates with the DEMO architecture, using VQGAN for video compression and OpenCLIP for text encoding.

## Key Results
- BCNI reduces FVD by 31.9% on average across WebVid-2M, MSR-VTT, and MSVD datasets
- SACN achieves 12.3% improvement on UCF-101 dataset
- Optimal corruption scale Ï found between 0.075-0.10 for most datasets
- Theoretical analysis shows improved entropy, Wasserstein, and generalization bounds

## Why This Works (Mechanism)

### Mechanism 1: Dimension-Reduced Corruption Complexity
Restricting corruption noise to a low-rank semantic subspace ($d$) rather than the full ambient embedding space ($D$) constrains error propagation and improves generalization bounds. By projecting noise onto a rank-$d$ subspace, the theoretical complexity of the error scales as $O(\rho^2 d)$ instead of $O(\rho^2 D)$, effectively "compressing" the negative impact of the noise.

### Mechanism 2: Semantic-Preserving Perturbation (BCNI)
BCNI perturbs embeddings along their deviation from the batch mean, preserving semantic identity better than isotropic noise while increasing local conditional entropy. This acts as an implicit regularizer for intra-batch variance by confining corruption to the semantic subspace.

### Mechanism 3: Low-Frequency Smoothing via Spectral Modes (SACN)
SACN injects noise along dominant spectral modes to enhance temporal coherence and low-frequency smoothness. By emphasizing low-frequency modes, the noise targets global, temporally coherent structures rather than high-frequency details, smoothing the score manifold.

## Foundational Learning

- **Concept:** Latent Video Diffusion Models (LVDMs)
  - **Why needed here:** The paper modifies the training of the score network within a latent diffusion framework
  - **Quick check question:** Can you explain why the forward process adds noise to latents and how the U-Net is conditioned on text embeddings $z$?

- **Concept:** Conditional Embedding Perturbation (CEP)
  - **Why needed here:** This is the baseline the paper argues against
  - **Quick check question:** How does adding Gaussian noise to a CLIP embedding act as a regularizer for the diffusion model?

- **Concept:** Wasserstein Distance & Score Drift
  - **Why needed here:** The theoretical justification relies on bounding how far the "corrupted" distribution deviates from the "clean" one
  - **Quick check question:** Why would a lower Wasserstein distance between the clean and corrupted conditional distributions imply better generation quality?

## Architecture Onboarding

- **Component map:** Text prompt $p$ -> OpenCLIP encoder -> Corruption Module (BCNI/SACN) -> U-Net (conditioned) -> Latent space denoising -> VQGAN decoder -> Video output
- **Critical path:** Input text prompt -> Encode to embeddings $z$ -> Corrupt with BCNI/SACN -> Condition U-Net -> Calculate diffusion loss predicting noise
- **Design tradeoffs:** Use BCNI for caption-heavy datasets, SACN for class-labeled datasets; optimal $\rho$ between 0.075-0.10; SACN requires SVD computation (more expensive)
- **Failure signatures:** Semantic drift at high $\rho$, temporal flickering with isotropic noise, motion collapse from misaligned spectral corruption
- **First 3 experiments:**
  1. Implement BCNI on a single batch and visualize corrupted vs clean embeddings
  2. Sweep $\rho \in [0.025, 0.20]$ on WebVid-2M subset and plot FVD
  3. Compare BCNI against Gaussian Noise at optimal $\rho$ on temporal consistency metrics

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive subspace selection or dynamic corruption scaling further improve performance compared to fixed corruption strengths?
- **Open Question 2:** How does corruption-aware training impact the generation quality and stability of long-form video generation?
- **Open Question 3:** Does the structured corruption paradigm transfer effectively to autoregressive video generation models?

## Limitations
- Theoretical claims rely on idealized assumptions about low-rank structure of semantic embeddings
- Computational overhead of SVD-based SACN may limit resource-constrained applications
- Empirical results focus on short-form video clips, leaving long-form generation effectiveness unproven

## Confidence
- **High Confidence:** Empirical FVD improvements (31.9% for BCNI, 12.3% for SACN) are well-supported by quantitative metrics
- **Medium Confidence:** Theoretical analysis connecting low-rank corruption to improved generalization bounds depends on embedding manifold assumptions
- **Medium Confidence:** Heuristic recommendations for choosing BCNI vs SACN based on dataset type are supported but lack rigorous validation

## Next Checks
1. Conduct ablation studies varying the intrinsic dimensionality $d$ of the corruption subspace to quantify scaling relationships
2. Evaluate transfer learning performance when models trained with CAT-LVDM are fine-tuned on datasets with different semantic distributions
3. Measure the computational overhead of SACN's SVD operation relative to generation quality gains across different hardware configurations