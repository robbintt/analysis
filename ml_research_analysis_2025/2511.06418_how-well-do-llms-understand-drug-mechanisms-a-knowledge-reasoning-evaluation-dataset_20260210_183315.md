---
ver: rpa2
title: How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation
  Dataset
arxiv_id: '2511.06418'
source_url: https://arxiv.org/abs/2511.06418
tags:
- drug
- llms
- mechanisms
- deep
- well
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel evaluation framework for assessing
  LLM understanding of drug mechanisms, using a dataset of counterfactual scenarios
  derived from biomedical interaction graphs. The framework tests both factual recall
  and reasoning under novel situations by altering drug-disease interaction chains.
---

# How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset

## Quick Facts
- arXiv ID: 2511.06418
- Source URL: https://arxiv.org/abs/2511.06418
- Authors: Sunil Mohan; Theofanis Karaletsos
- Reference count: 14
- Primary result: Reasoning-specialized models (o4-mini, Qwen3-4B-Thinking) match or exceed larger general models on drug mechanism understanding

## Executive Summary
This work introduces a novel evaluation framework for assessing LLM understanding of drug mechanisms, using a dataset of counterfactual scenarios derived from biomedical interaction graphs. The framework tests both factual recall and reasoning under novel situations by altering drug-disease interaction chains. Experiments with five LLMs (GPT-4o, o3/o3-mini/o4-mini, and Qwen3-4B-Thinking) show that o4-mini outperforms larger models like o3, and the small Qwen3-4B-Thinking model matches o4-mini's performance. Reasoning models consistently outperform general-purpose models. The study reveals that open-world settings (requiring knowledge recall) are more challenging than closed-world (knowledge provided), and that counterfactuals affecting internal links in reasoning chains are harder than surface-level ones. Overall, the results highlight both progress and limitations in LLM reasoning about drug mechanisms.

## Method Summary
The evaluation uses DrugMechCounterfactuals dataset derived from DrugMechDB (4,664 drug-disease MoA pairs with 14 entity types, 66 relation labels). Counterfactuals are generated by systematically modifying edges in directed acyclic graphs: Add-Link connects entities from different MoAs, Delete-Link removes edges, and Invert-Link flips direction-of-effect. Each type has "surface" (drug-adjacent) and "deep" (internal node) variants. Five LLMs are evaluated via API (GPT-4o, o3/o3-mini/o4-mini) or vLLM (Qwen3-4B-Thinking) with seed=42, temperature=1.0, reasoning_effort="medium" for reasoning models. Open-world requires knowledge recall while closed-world provides MoA context. Accuracy metrics include correct response selection, Interior Node Match, and Reduced Edge Match scores.

## Key Results
- o4-mini outperforms larger models (o3, 4o) and matches Qwen3-4B-Thinking despite smaller size
- Open-world accuracy (0.60-0.77) is consistently lower than closed-world (0.86-0.97) across all models
- Deep counterfactuals show 0.45-0.66 accuracy vs 0.76-0.89 for surface counterfactuals in open world
- Reasoning models consistently outperform general-purpose models on all counterfactual types

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Perturbation as Reasoning Probe
- Claim: Altering links in mechanistic reasoning chains reveals whether LLMs compose causal effects or pattern-match surface features
- Mechanism: Drug-disease MoAs are formalized as directed acyclic graphs, then systematically modified edges (add/delete/invert). Surface counterfactuals alter drug-adjacent links; deep counterfactuals modify interior chain nodes. Correct responses require tracing effect composition through entire chain
- Core assumption: Successful reasoning on synthetic counterfactuals indicates compositional understanding rather than retrieval
- Evidence anchors: Deep counterfactuals show 0.45-0.66 accuracy vs 0.76-0.89 for surface; accuracy degrades with MoA chain length
- Break condition: If models improve on deep counterfactuals through exposure to similar graph structures in training data, signal becomes memorization

### Mechanism 2: Open vs. Closed World Knowledge Integration
- Claim: Separating knowledge retrieval from compositional reasoning isolates failure modes in LLM mechanistic understanding
- Mechanism: Closed-world prompts provide MoA graph explicitly; open-world requires model to retrieve relevant biochemical interactions from parametric knowledge before reasoning. Performance gap measures retrieval-reasoning integration difficulty
- Core assumption: Parametric biomedical knowledge is sufficiently encoded in pretrained weights and accessible via natural language prompts
- Evidence anchors: Closed-world accuracy: 0.86-0.97; Open-world: 0.60-0.77 (Table 6)
- Break condition: If closed-world performance is low, model fails at reasoning task itself

### Mechanism 3: Small Reasoning-Optimized Models Match Larger Generalists
- Claim: Architecture and training objective specialization can compensate for parameter count in mechanistic reasoning tasks
- Mechanism: o4-mini and Qwen3-4B-Thinking are explicitly trained for chain-of-thought reasoning. Despite fewer parameters, they match or exceed o3 and GPT-4o on MoA composition tasks
- Core assumption: DrugMechDB-derived evaluation is representative of broader mechanistic reasoning
- Evidence anchors: o4-mini (0.76 open-world) vs o3 (0.71) vs 4o (0.60)
- Break condition: If small reasoning models plateau on more complex multi-hop biomedical chains, parameter scaling may dominate

## Foundational Learning

- Concept: **Directed Acyclic Graphs (DAGs) as Causal Chains**
  - Why needed here: MoAs are formalized as DAGs where nodes are biomedical entities and directed edges encode effect relationships. Understanding DAG traversal is prerequisite for reasoning about effect composition
  - Quick check question: Given Drug A → Protein B → Process C → Disease D, if the edge A→B is deleted, what happens to the therapeutic effect of A on D?

- Concept: **Abductive Reasoning**
  - Why needed here: Inferring a valid MoA that explains why a drug treats a disease is abductive - reasoning from effect to plausible cause. Counterfactual evaluation tests whether models can perform abduction under perturbation
  - Quick check question: If a drug suddenly stops working in a patient population, and you observe a new protein-protein interaction, how would you determine if this explains the drug failure?

- Concept: **Closed-World vs. Open-World Assumptions**
  - Why needed here: The evaluation explicitly varies whether knowledge is provided (closed) or must be retrieved (open). This distinction is critical for deploying LLMs in scientific settings
  - Quick check question: In a drug repurposing scenario with a novel compound, which setting (open/closed world) better reflects real constraints, and what failure modes would you expect?

## Architecture Onboarding

- Component map: DrugMechDB source -> Counterfactual generator -> MoA simulator -> LLM query interface -> Evaluation metrics
- Critical path: 1. Sample drug-disease pair from DrugMechDB 2. Generate counterfactual by modifying edge 3. Validate with MoA simulator 4. Query LLM with counterfactual scenario 5. Score response against reference MoA
- Design tradeoffs: Strict vs. relaxed accuracy (Partially Blocked responses may be correct if DrugMechDB is incomplete); Entity matching tolerance (approximate text match for nodes); Chain length confounds (longer MoAs show lower accuracy)
- Failure signatures: Surface pattern matching (correct for surface counterfactuals but fails on deep ones); Hallucinated chains ("Potentially very different MoAs" - 20-29% in open-world Add-Link); Syntactic shortcut exploitation (high negative-sample accuracy with low positive accuracy)
- First 3 experiments: 1. Baseline open-world vs. closed-world gap; 2. Chain depth ablation (bin deep counterfactuals by distance from drug 1-4); 3. Negative sample diagnostic (compare model justifications on incorrect negative responses)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the specific length or topological depth of a Mechanism of Action (MoA) chain causally determine LLM failure rates in deep counterfactual scenarios?
- Basis in paper: The authors investigate chain length but find results inconclusive due to noise
- Why unresolved: Current analysis showed downward trend but logistic regression coefficients were not significant
- What evidence would resolve it: Controlled ablation study varying MoA chain length/depth while strictly validating ground truth

### Open Question 2
- Question: Can Retrieval-Augmented Generation (RAG) effectively close the performance gap between open-world and closed-world reasoning for drug mechanisms?
- Basis in paper: The conclusion suggests combining pre-trained LLMs with actively updated knowledge base
- Why unresolved: Study evaluated static open-world and closed-world settings but did not test dynamic external retrieval
- What evidence would resolve it: Benchmarking using models equipped with medical knowledge retrievers

### Open Question 3
- Question: How can evaluation metrics be refined to distinguish between model hallucination and valid reasoning about redundant biological pathways not captured in ground truth graph?
- Basis in paper: Models sometimes selected "Partially Blocked" (marked incorrect) because they correctly identified alternative pathways not listed in DrugMechDB
- Why unresolved: Ground truth is incomplete; current metrics penalize models for valid reasoning extending beyond single curated causal chain
- What evidence would resolve it: Expert review of "incorrect" responses to validate if "Partially Blocked" answers reflect true biological redundancy

## Limitations

- DrugMechDB incompleteness may cause models to be penalized for valid reasoning about alternative pathways not captured in ground truth
- Counterfactual generation process could inadvertently create scenarios resembling training examples, particularly for surface-level perturbations
- Performance gaps may reflect data incompleteness rather than true reasoning limitations (55% of deep Delete/Invert positive responses return "Partially Blocked")

## Confidence

**High confidence**: The closed-world vs. open-world performance gap (0.86-0.97 vs 0.60-0.77) and depth-dependent accuracy degradation in deep counterfactuals are robust findings

**Medium confidence**: The superiority of reasoning-optimized models over larger general models is well-supported but requires additional controls

**Low confidence**: The claim that counterfactual evaluation reveals compositional understanding rather than pattern matching is plausible but not definitively proven

## Next Checks

1. **Data completeness audit**: Cross-validate DrugMechDB MoAs against external biomedical knowledge bases (DrugBank, ChEMBL) to quantify incompleteness and its impact on "Partially Blocked" response rates

2. **Counterfactual novelty analysis**: Implement embedding-based similarity checks between generated counterfactuals and potential training data (PubMed abstracts, clinical guidelines) to ensure perturbations are truly novel

3. **Depth-accuracy ablation study**: Perform fine-grained binning of deep counterfactuals by exact distance from drug (1-5 edges) and test whether accuracy follows smooth degradation curve or shows non-monotonic patterns suggesting other confounding factors