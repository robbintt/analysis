---
ver: rpa2
title: 'Enhancing Sports Strategy with Video Analytics and Data Mining: Automated
  Video-Based Analytics Framework for Tennis Doubles'
arxiv_id: '2507.02906'
source_url: https://arxiv.org/abs/2507.02906
tags:
- tennis
- shot
- player
- annotation
- doubles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research presents a comprehensive video-based analytics framework
  for tennis doubles, addressing the lack of automated analysis tools for this strategically
  complex sport. The framework introduces a standardised annotation methodology covering
  player positioning, shot types, court formations, and match outcomes, alongside
  a specialised annotation tool designed for tennis video labelling.
---

# Enhancing Sports Strategy with Video Analytics and Data Mining: Automated Video-Based Analytics Framework for Tennis Doubles

## Quick Facts
- arXiv ID: 2507.02906
- Source URL: https://arxiv.org/abs/2507.02906
- Reference count: 0
- Primary result: CNN-based models with transfer learning substantially outperform pose-based methods for predicting shot types, player positioning, and formations in tennis doubles.

## Executive Summary
This research introduces a comprehensive video-based analytics framework for tennis doubles, addressing the lack of automated analysis tools for this strategically complex sport. The framework standardizes annotation methodology covering player positioning, shot types, court formations, and match outcomes, alongside a specialized annotation tool for tennis video labeling. By integrating advanced machine learning techniques including GroundingDINO for precise player localization and YOLO-Pose for robust pose estimation, the approach significantly reduces manual annotation effort while improving data consistency and quality. Experimental results demonstrate that CNN-based models with transfer learning substantially outperform pose-based methods for predicting shot types, player positioning, and formations, effectively capturing complex visual and contextual features essential for doubles tennis analysis.

## Method Summary
The framework employs a two-stage detection pipeline: GroundingDINO performs player localization through natural language grounding (e.g., "red-shirt tennis player") followed by YOLO-Pose for 2D keypoint extraction. For classification tasks, the system uses ResNet-50 backbone CNNs with pre-trained ImageNet weights, fine-tuned with differential learning rates (1e-5 for backbone, 1e-4 for classifier head) using class-weighted Cross-Entropy loss. Single-Image CNN processes individual frames while Double-Image CNN concatenates features from two frames separated by n=10 frames to capture temporal context. Training uses AdamW optimizer with early stopping (patience 20 epochs) on 70/30 train/validation splits of 8 tennis doubles videos (NCAA and Professional/YouTube highlights), with 2 specific videos held out for testing.

## Key Results
- CNN-based models with transfer learning substantially outperform pose-based GCN methods for predicting shot types, player positioning, and formations
- GroundingDINO+YOLO-Pose achieves 64.3-100% detection across all four players vs YOLOv11+DeepSORT achieving 0.1-100% (with far players P3/P4 at 0.1-43.4%)
- Single-Image CNN achieves 75.75-100% AUC across professional/NCAA datasets versus Single-Pose GCN achieving 42.72-70.22% AUC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN-based models with transfer learning outperform pose-based GCN methods for tennis doubles classification tasks.
- Mechanism: Pre-trained ImageNet weights in ResNet-50 backbone provide rich, hierarchical visual feature representations that transfer to tennis-specific patterns (racket orientation, body positioning) with minimal fine-tuning, whereas GCNs must learn pose-to-label mappings from scratch on limited data (~2000 samples across 8 videos).
- Core assumption: Tennis shot classification depends more on holistic visual features (equipment, court context, player appearance) than on skeletal joint relationships alone.
- Evidence anchors:
  - [abstract] "CNN-based models with transfer learning substantially outperform pose-based methods for predicting shot types, player positioning, and formations"
  - [section] Tables 4.10 and 4.11 show Single-Image CNN achieving 75.75-100% AUC across professional/NCAA datasets versus Single-Pose GCN achieving 42.72-70.22% AUC
  - [corpus] Limited corpus support; related tennis analytics papers (e.g., Classification of Tennis Actions Using Deep Learning) discuss deep learning for action recognition but lack direct CNN-vs-GCN comparisons in doubles contexts
- Break condition: If training data exceeded ~10,000 diverse samples, GCN architectures might close the gap due to better parameter efficiency and explicit body-mechanics modeling.

### Mechanism 2
- Claim: Combining GroundingDINO (phrase grounding) with YOLO-Pose improves far-court player detection and enables identity tracking via natural language descriptions.
- Mechanism: GroundingDINO filters detections semantically using text prompts (e.g., "red-shirt tennis player"), reducing false positives before YOLO-Pose extracts keypoints. This two-stage pipeline handles the "four-player, variable-distance" challenge in doubles where standard tracking fails on occluded or distant players.
- Core assumption: Players can be reliably distinguished by visual descriptions (clothing, position) across frames, and semantic filtering reduces noise enough for accurate pose estimation.
- Evidence anchors:
  - [abstract] "GroundingDINO for precise player localisation through natural language grounding and YOLO-Pose for robust pose estimation"
  - [section] Table 4.4 shows GroundingDINO+YOLO-Pose achieving 64.3-100% detection across all four players vs YOLOv11+DeepSORT achieving 0.1-100% (with far players P3/P4 at 0.1-43.4%)
  - [corpus] Related work "Automated Tennis Player and Ball Tracking" uses YOLOv8 for player detection but does not integrate phrase grounding; corpus evidence for GroundingDINO-specific tennis gains is weak
- Break condition: If video quality degrades significantly (low-resolution NCAA footage) or occlusion exceeds ~20% of frames for far players, pose estimation accuracy drops substantially (Figure 4.16 shows occlusion failures).

### Mechanism 3
- Claim: Dual-input architectures (Double-Image CNN, Double-Pose GCN) improve predictions requiring temporal or partner context—specifically formation and outcome.
- Mechanism: Concatenating features from two images (hitting player + partner, or current + future frame n=10) provides contextual signals absent in single-frame views. Formation benefits from partner positioning cues; outcome benefits from follow-through dynamics visible in future frames.
- Core assumption: Ten-frame lookahead captures sufficient post-shot information to infer outcome; partner stance encodes formation type reliably.
- Evidence anchors:
  - [abstract] "CNN models effectively capture complex visual and contextual features essential for doubles tennis analysis"
  - [section] Tables 4.8 vs 4.9: Formation AUC improves from 97.79% (Single-Image) to 99.25% (Double-Image) for professional video; Outcome AUC improves from 56.32% to 69.41%
  - [corpus] No direct corpus evidence for dual-frame tennis architectures; "Silent Impact" and related papers focus on single-player sensor-based tracking
- Break condition: If temporal window n=10 frames is too short or too long for specific outcomes, or if partner is occluded, dual-input gains diminish. Shot direction unexpectedly performed worse in Double-Image vs Single-Image (72.43% vs 74.05% AUC), suggesting temporal context adds noise for this task.

## Foundational Learning

- Concept: **Transfer Learning with CNNs**
  - Why needed here: Dataset is small (8 videos, ~2000 annotated events), making training deep networks from scratch infeasible. Pre-trained ImageNet weights provide initialized feature extractors.
  - Quick check question: Can you explain why freezing early convolutional layers while fine-tuning later layers prevents overfitting on small datasets?

- Concept: **Visual/Phrase Grounding**
  - Why needed here: GroundingDINO enables zero-shot player detection via text prompts, critical when annotated tennis doubles data is scarce and player identities vary across matches.
  - Quick check question: How does phrase grounding differ from standard object detection, and why would "tennis player in red shirt near net" be more useful than a generic "person" class?

- Concept: **Graph Convolutional Networks for Skeleton Data**
  - Why needed here: Pose-based GCNs represent players as skeletal graphs (joints as nodes, bones as edges), which should theoretically capture body mechanics for shot classification efficiently.
  - Quick check question: Why might a GCN struggle to distinguish a serve from a smash using only a single static pose frame, versus a temporal sequence?

## Architecture Onboarding

- Component map:
  - React + Vite + DaisyUI + TailwindCSS -> Annotation UI, video navigation, label visualization
  - Flask -> API orchestration, model inference serving
  - GroundingDINO -> Player localization via phrase grounding
  - YOLO-Pose -> 2D keypoint extraction
  - ResNet-50 CNNs -> Shot/formation/outcome classification
  - FFmpeg -> Frame extraction, video handling
  - COCO format -> Data annotations

- Critical path:
  1. Player Annotation Page -> Define player descriptions + draw initial bounding boxes (10-30 frames)
  2. Training & Inference Page -> Fine-tune GroundingDINO on annotated frames -> Run inference across all frames -> Extract poses via YOLO-Pose
  3. Rally Analysis Page -> Mark net position, rally start/end frames, hitting moments
  4. Label Generation Page -> CNN models predict shot type, side, direction, formation, outcome
  5. Label Confirmation Page -> Human review/validation with tennis-rule enforcement

- Design tradeoffs:
  - Hot-loading vs Cold-start: Hot-loading keeps CNN models in memory (3.2 GB GPU, 20s startup, 54s inference) vs cold-start (0.8 GB GPU, 5s startup, 12+ min inference). Choose hot-loading for interactive use; cold-start for resource-constrained batch jobs.
  - Phrase grounding vs standard tracking: GroundingDINO+YOLO-Pose enables player identification but adds 10x processing time (1436s vs 130s for 952 frames) compared to YOLOv11+DeepSORT.
  - Single vs Dual-image CNN: Dual-image improves formation/outcome but adds complexity and degrades shot direction—deploy selectively per task.

- Failure signatures:
  - Far-player occlusion: P3/P4 detection drops to 0% in NCAA footage when near players block view (Figure 4.14, 4.16)
  - Low-resolution input: NCAA videos show 16% missing pose data for P4; consider court cropping or super-resolution preprocessing
  - Class imbalance: Shot types like "smash" and "lob" are rare; model uses class-weighted CrossEntropyLoss but still shows lower recall (55.48% for all shot types on NCAA)
  - Temporal window mismatch: n=10 frames may not suit all outcome predictions; tune per use case

- First 3 experiments:
  1. **Baseline reproduction**: Replicate Single-Image CNN on provided train/val split (70/30) with ResNet-50 ImageNet weights; validate shot type AUC > 85% on professional video before proceeding.
  2. **Ablation on temporal window**: Test n ∈ {5, 10, 15, 20} frames in Double-Image CNN for outcome prediction; measure AUC and latency tradeoffs.
  3. **Far-player detection enhancement**: Apply court cropping + brightness normalization to NCAA footage before GroundingDINO inference; compare P3/P4 detection rates against Table 4.4 baseline (64.3-69.8%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating ball and racket tracking (e.g., TrackNet) into a unified multi-branch model improve shot direction and outcome prediction compared to the current isolated player-only CNN approach?
- Basis in paper: [explicit] "One future direction could be creating a singular, integrated model consisting of multiple branches – ball, racket, pose, player positioning"
- Why unresolved: Current models operate in isolated silos; shot direction prediction remains suboptimal (AUC 72-74%), potentially due to missing ball trajectory information.
- What evidence would resolve it: Comparative experiments showing whether adding ball/racket branches to the CNN architecture improves shot direction AUC beyond 75%.

### Open Question 2
- Question: What is the optimal temporal window (n frames) for capturing shot follow-through mechanics in the Double-Image CNN architecture?
- Basis in paper: [inferred] The paper uses n=10 frames but notes "n = 10 frames worth of temporal information may be unsuitable" for shot direction prediction, which unexpectedly performed worse than Single-Image CNN.
- Why unresolved: No systematic exploration of different temporal offsets was conducted; the degradation in shot direction performance with added temporal frames suggests the window size may be suboptimal.
- What evidence would resolve it: Ablation study varying n (e.g., 5, 10, 15, 20 frames) and measuring impact on shot direction and outcome AUC.

### Open Question 3
- Question: Can transformer-based multimodal architectures that fuse pose, visual, and audio signals outperform the current CNN-based approach for tennis doubles analysis?
- Basis in paper: [explicit] "Employing transformer-based multimodal architectures, as demonstrated by Liu et al (2025), could particularly benefit shot type, shot direction, and outcome prediction."
- Why unresolved: Current dataset is limited (~2000 samples); transformer models typically require larger datasets. No experiments with audio or trajectory modalities were conducted.
- What evidence would resolve it: Training transformer-based models on expanded dataset with multimodal inputs, comparing against CNN baselines.

### Open Question 4
- Question: Can 3D pose estimation be effectively integrated into the tennis doubles analysis pipeline given current computational constraints and far-player occlusion issues?
- Basis in paper: [explicit] "3D pose estimation is not yet integrated into our annotation tool... In the future, with more optimisations and newer models, it will be more feasible"
- Why unresolved: Appendix shows MotionAGFormer exploration faced "extremely high computational overhead, long processing time and distorted 3D mapping for far players."
- What evidence would resolve it: Demonstrating 3D pose estimation with acceptable processing time per frame and improved accuracy for far-court players.

## Limitations

- Small dataset size (8 videos, ~2000 samples) limits generalizability and prevents training complex models from scratch
- GroundingDINO+YOLO-Pose combination lacks direct corpus validation for tennis-specific applications
- Far-player detection degrades significantly in NCAA footage due to occlusion and resolution issues

## Confidence

- CNN superiority over GCN methods: High confidence (well-supported by experimental results)
- Grounding-based player detection mechanism: Medium confidence (strong performance in professional footage but degrades in NCAA videos)
- Dual-input architecture benefits: Medium confidence (demonstrated benefits but temporal window selection appears arbitrary)
- Dataset generalizability: Medium confidence (small sample size limits broader applicability)

## Next Checks

1. **Temporal Window Ablation**: Test the Double-Image CNN architecture with multiple temporal window sizes (n=5, 10, 15, 20 frames) for outcome prediction to determine optimal lookahead duration and verify the n=10 selection is justified.

2. **Far-Player Enhancement**: Implement court cropping and brightness normalization preprocessing for NCAA footage, then re-run GroundingDINO inference to measure improvement in P3/P4 detection rates against the reported 64.3-69.8% baseline.

3. **Class Imbalance Impact**: Conduct experiments with synthetic minority oversampling or class-balanced sampling strategies for rare shot types (smash, lob) to quantify the extent to which current class-weighted loss mitigates recall degradation observed in Table 4.12.