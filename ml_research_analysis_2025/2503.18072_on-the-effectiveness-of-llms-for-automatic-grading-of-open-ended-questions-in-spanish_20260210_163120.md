---
ver: rpa2
title: On the effectiveness of LLMs for automatic grading of open-ended questions
  in Spanish
arxiv_id: '2503.18072'
source_url: https://arxiv.org/abs/2503.18072
tags:
- llms
- grading
- prompt
- prompts
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of large language models
  (LLMs) for automatic grading of open-ended questions in Spanish. A handcrafted dataset
  of 51 questions with three-tiered responses (excellent, acceptable, incorrect) was
  created, along with human-expert ratings as ground truth.
---

# On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish

## Quick Facts
- arXiv ID: 2503.18072
- Source URL: https://arxiv.org/abs/2503.18072
- Reference count: 35
- Primary result: Advanced LLMs achieve over 95% accuracy for three-level grading of Spanish open-ended questions using few-shot prompting

## Executive Summary
This study evaluates large language models (LLMs) for automatic grading of open-ended questions in Spanish using a handcrafted dataset of 51 questions with three-tiered responses (excellent, acceptable, incorrect). Multiple LLMs and prompting strategies were tested, with few-shot prompting emerging as the most effective approach, consistently achieving over 95% accuracy for three-level grading and over 98% for binary classification. The research reveals significant sensitivity to specific wording in prompts, particularly grading labels, suggesting potential biases that require careful prompt design for fair and consistent automated grading.

## Method Summary
The authors created a handcrafted dataset of 51 open-ended questions across five subjects, each with three graded responses (excellent, acceptable, incorrect) rated by human experts. They evaluated multiple LLMs including GPT-4o, Llama-3.3-70B, Llama-3.2-8B, and Llama-3.2-3B using different prompting strategies: short prompts, long prompts, chain-of-thought, and few-shot learning. Accuracy was measured against human-grounded truth across various configurations, with few-shot prompting showing the most consistent high performance.

## Key Results
- Few-shot prompting consistently achieved over 95% accuracy for three-level grading across all tested models
- Larger models (>70B parameters) required to reliably distinguish between "excellent" and "acceptable" tiers
- Binary (right/wrong) classification achieved over 98% accuracy, while three-tier grading showed ~95% accuracy
- Significant performance drops (>10%) observed when using synonyms for grading labels instead of specific terms like "Excelente"

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning via few-shot examples significantly outperforms rubric-based or instruction-heavy prompts for this specific grading task.
- **Mechanism:** Providing concrete input-output pairs allows the model to infer grading criteria through pattern matching rather than interpreting abstract definitions.
- **Core assumption:** The model possesses sufficient inherent language understanding in Spanish to map provided examples to new student responses.
- **Evidence anchors:** Abstract states "Few-shot prompting emerged as the most effective strategy"; section 5.1 shows "significant improvement in most cases"; corpus supports LLM viability for subjective evaluation.

### Mechanism 2
- **Claim:** Grading accuracy is strongly dependent on model scale, with larger parameter counts (>70B) required to distinguish between "Excellent" and "Acceptable" tiers effectively.
- **Mechanism:** Larger models possess more nuanced semantic representations of Spanish, enabling detection of subtle justifications that separate complete from merely correct answers.
- **Core assumption:** Human-expert labels are consistent and linguistic distinctions between tiers are learnable.
- **Evidence anchors:** Section 5.1 shows "Performance increases as model capacities/sizes grow"; section 5.4 reveals smaller models tend to over-grade; corpus lacks direct evidence for Spanish grading.

### Mechanism 3
- **Claim:** The model's grading logic is anchored to specific lexical tokens used for labels (e.g., "Excelente"), making it sensitive to synonym substitution in the prompt.
- **Mechanism:** The LLM forms semantic associations between input text and specific label tokens requested; changing these tokens disrupts learned associations.
- **Core assumption:** Pre-training data has stronger semantic associations for specific keyword pairs than others.
- **Evidence anchors:** Abstract notes "Results are notably sensitive to prompt styles"; section 5.2 shows simple word changes lead to worse results; corpus mentions prompt structure influences LLM responses.

## Foundational Learning

- **Concept: In-Context Learning (Few-Shot)**
  - **Why needed here:** This technique is the primary driver of high accuracy. Understanding it is crucial because it shifts "programming" from writing rules to curating examples.
  - **Quick check question:** If you have a new type of science question to grade, would you rather write a 200-word rubric or provide 3 example answers with scores?

- **Concept: Prompt Sensitivity/Lexical Bias**
  - **Why needed here:** The study highlights a critical failure mode where changing a single word in the output label causes performance drops.
  - **Quick check question:** Does the prompt "Score this as Good, Bad, or Ugly" work exactly the same as "Score this as Positive, Negative, or Neutral"?

- **Concept: Confusion Matrix Analysis**
  - **Why needed here:** Accuracy alone hides where the model fails. The paper reveals binary grading hits 98% while 3-tier grading confuses "Acceptable" with "Excellent."
  - **Quick check question:** If a model achieves 95% accuracy but all its errors involve mis-grading "Passable" work as "Fail," is it safe to deploy?

## Architecture Onboarding

- **Component map:** Student Answer + Question + Few-Shot Examples (Prompt) -> High-capability LLM (GPT-4o or Llama-3.3-70B) -> Promptfoo (or similar) for assertions and accuracy calculation against Ground Truth

- **Critical path:**
  1. Define specific grading labels (e.g., *Excelente*) and do not change them without re-testing
  2. Construct a "Golden Dataset" of human-graded answers (similar to the paper's 3-tier structure)
  3. Use Few-Shot prompting with examples that explicitly demonstrate distinction between adjacent tiers

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** Few-shot prompting requires ~4x more tokens than short prompts (Section 5.1), increasing latency and cost, but is necessary for >95% accuracy
  - **Granularity vs. Reliability:** Moving from 2 levels (Binary) to 3 levels drops accuracy from ~98% to ~95% (Section 5.4); system must tolerate this 3% reliability tax for nuanced feedback

- **Failure signatures:**
  - **Label Drift:** Replacing "Excelente" with "Sobresaliente" causes immediate performance degradation (Section 5.2)
  - **Over-grading:** Smaller models tend to rate "Acceptable" answers as "Excellent" (Section 5.4, Figure 7a)

- **First 3 experiments:**
  1. **Sanity Check (Binary):** Run 3-tier prompt but map results to Right/Wrong to verify alignment with >98% baseline
  2. **Token Sensitivity Test:** Duplicate best-performing prompt but swap grading labels for synonyms to quantify "prompt wording" penalty
  3. **Consistency Run:** Execute same 50-question set 5 times with default temperature to measure output variance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does modifying the temperature parameter influence the consistency and accuracy of automated grading outcomes?
- **Basis in paper:** Section 6 states future work involves analyzing consistency by varying temperature to determine its impact on grading consistency.
- **Why unresolved:** Current experiments used default parameters (typically temperature = 1), introducing randomness. It's unclear if lower temperatures would improve stability without sacrificing nuanced handling.
- **What evidence would resolve it:** Experimental results comparing accuracy and output variance across temperature settings (0.0 to 1.0) using same dataset and prompts.

### Open Question 2
- **Question:** Can LLMs maintain consistent grading accuracy when student responses undergo minor lexical changes that preserve semantic meaning?
- **Basis in paper:** Section 6 proposes extending sensitivity analysis to modifications in student responses without altering semantic meaning.
- **Why unresolved:** Study demonstrated sensitivity to specific words in grading prompts, but it's unknown if this sensitivity extends to wording of student's answer itself.
- **What evidence would resolve it:** Robustness test where original answers are paraphrased or rewritten using synonyms, followed by comparison of LLMs' grading consistency against original ground truth.

### Open Question 3
- **Question:** How does inter-rater reliability between LLMs and human teachers compare to reliability between human teachers themselves?
- **Basis in paper:** Section 6 suggests designing experiments to measure inter-rater reliability between automated systems and educators to illuminate the "glass ceiling" of performance.
- **Why unresolved:** Paper establishes high accuracy compared to "ground truth" but doesn't define baseline variability of human educators. Without knowing how often human teachers disagree on same answers, it's difficult to contextualize relative error rate of LLMs.
- **What evidence would resolve it:** Study involving multiple independent human educators grading same response set, followed by statistical comparison (e.g., Cohen's Kappa) of human-human vs. human-LLM agreement.

## Limitations

- **Dataset Scale and Representativeness:** Conclusions based on only 51 questions across 5 subjects, raising questions about generalizability to broader educational domains
- **Language and Cultural Specificity:** Results demonstrated specifically for Spanish grading; mechanisms may not translate directly to other languages or cultural educational contexts
- **Ground Truth Reliability:** Relies on human-expert ratings without discussing inter-rater reliability or consistency of "excellent/acceptable/incorrect" distinctions

## Confidence

**High Confidence** - Finding that few-shot prompting consistently outperforms other strategies is well-supported with quantitative results across multiple models showing significant improvements (abstract: "Few-shot prompting emerged as the most effective strategy"; section 5.1: "showing a significant improvement in most cases")

**Medium Confidence** - Claim about model scale requirements (>70B needed for 3-tier grading) is supported by performance patterns but lacks direct statistical analysis of scaling relationship; observation of over-grading by smaller models noted but not deeply explored

**Medium Confidence** - Prompt sensitivity findings demonstrated through controlled experiments, but underlying mechanism remains unexplained; observed >10% performance drops from synonym substitution suggest brittleness but don't establish whether this reflects model limitations or inherent subjectivity in grading

## Next Checks

1. **Inter-rater Reliability Study** - Conduct statistical analysis of agreement between multiple human graders on same dataset to establish whether "ground truth" labels contain systematic biases or inconsistencies that could explain model sensitivity to prompt wording

2. **Cross-linguistic Generalization Test** - Replicate experimental protocol with equivalent datasets in at least two other major languages (e.g., English, French) to determine whether few-shot prompting advantages and prompt sensitivity patterns hold across linguistic boundaries

3. **Edge Case Stress Test** - Systematically generate adversarial examples that sit precisely at boundaries between grading tiers (e.g., answers that are 60% complete vs 70% complete) to quantify where model's 95% accuracy breaks down and whether failures cluster around specific types of answer incompleteness