---
ver: rpa2
title: 'Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time
  Series Modeling'
arxiv_id: '2510.17106'
source_url: https://arxiv.org/abs/2510.17106
tags:
- fighter
- matrix
- transformer
- graph
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that the Transformer encoder is fundamentally
  equivalent to a Graph Convolutional Network (GCN), with the attention distribution
  matrix acting as a dynamic adjacency matrix and subsequent transformations performing
  graph convolution-like computations. During training, updates to value and feed-forward
  projections mirror GCN parameter updates, while query and key weights enable adaptive
  learning of the attention structure.
---

# Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling

## Quick Facts
- arXiv ID: 2510.17106
- Source URL: https://arxiv.org/abs/2510.17106
- Reference count: 40
- Primary result: Establishes Transformer encoders as equivalent to GCNs with dynamic adjacency matrices; proposes Fighter architecture achieving competitive performance

## Executive Summary
This paper establishes a fundamental equivalence between Transformer encoders and Graph Convolutional Networks (GCNs), reinterpreting the attention mechanism as a dynamic graph convolution operation. The authors show that the attention distribution matrix functions as a dynamic adjacency matrix, with subsequent transformations performing graph convolution-like computations. Based on this theoretical framework, they propose Fighter (Flexible Graph Convolutional Transformer), a streamlined architecture that removes redundant linear projections and incorporates multi-hop graph aggregation. Experiments on time series forecasting benchmarks demonstrate that Fighter achieves competitive performance while providing clearer mechanistic interpretability of its predictions.

## Method Summary
The Fighter architecture reinterprets the standard Transformer as a GCN where the attention matrix serves as a dynamic adjacency matrix. The method removes the second linear projection in the feed-forward network (deemed redundant) and introduces explicit multi-hop aggregation by raising the attention matrix to powers up to κ-1. This captures multi-scale temporal dependencies in a single layer rather than through deep stacking. The architecture uses a streamlined forward pass: compute attention A = softmax(QK^T/√d), raise to powers A^[κ], apply blkdiag replication, and project with a single weight matrix. Training involves standard optimization with batch_size=32 for 25 epochs, applying StandardScaler to time series data.

## Key Results
- Transformer encoders are fundamentally equivalent to GCNs with attention matrices as dynamic adjacency matrices
- Fighter achieves competitive performance on time series forecasting (Electricity, Weather) and text classification (AG News) benchmarks
- Optimal κ values balance local and global dependency modeling, with performance degrading at high κ due to over-smoothing
- The model provides interpretable predictions by treating temporal dependencies as graph edges

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Graph Topology via Attention Matrices
The Transformer's self-attention mechanism functions as a graph convolution where the sequence is treated as a fully connected graph with learned edge weights. The operation $Y = \text{softmax}(QK^T/\sqrt{d})V$ re-interprets the attention distribution matrix $A = \text{softmax}(QK^T/\sqrt{d})$ as a dynamic adjacency matrix determining connection strength between time steps. This matrix multiplied by $V$ performs feature aggregation analogous to GCN layer $H^{(l+1)} = \sigma(AH^{(l)}W^{(l)})$. The temporal dependencies in time series data are effectively modeled as spatial dependencies on a graph with data-dependent adjacency weights.

### Mechanism 2: Adaptive Structure Learning via Q/K Gradients
The learning dynamics of Query ($Q$) and Key ($K$) weights are responsible for learning the graph topology (edge weights), distinct from Value ($V$) and FFN weights which learn feature extraction. During backpropagation, gradients updating $W_Q$ and $W_K$ directly modify the adjacency matrix $A$, adapting the graph structure to input data. Updates to $W_V$ and FFN weights mirror GCN parameter updates, processing aggregated features. This separation suggests $W_Q$ and $W_K$ are primary drivers of topological adaptation.

### Mechanism 3: Multi-hop Aggregation via Streamlined Projections (Fighter)
Removing redundant linear projections and explicitly computing powers of the attention matrix ($A^\kappa$) enables efficient multi-hop information propagation. Fighter removes the second linear projection in the FFN ($W_{FFN}^{(2)}$) and raises the adjacency matrix to power $\kappa$ ($A^{[\kappa]}$). This operation aggregates information from $\kappa$-hop neighbors in a single layer, mirroring multi-layer GCN propagation without depth. Single layer with multi-hop aggregation is sufficient and more interpretable than deep layer-by-layer propagation for time series data.

## Foundational Learning

**Graph Convolutional Networks (GCNs)**
Why needed: The paper reinterprets the Transformer *as* a GCN. Understanding static adjacency ($A$) and weight ($W$) matrices is essential for grasping the dynamic adjacency matrix concept.
Quick check: Can you explain the difference between a static adjacency matrix (used in standard GCNs) and the dynamic one proposed here?

**Self-Attention Mechanics ($Q, K, V$)**
Why needed: This is the substrate being reinterpreted. Understanding how $QK^T$ computes similarity scores and how softmax normalizes them is crucial for seeing why this behaves like a weighted adjacency matrix.
Quick check: Why does the softmax function ensure the attention matrix is row-stochastic (sums to 1), and how does that relate to a weighted graph?

**Over-smoothing in GNNs**
Why needed: The Fighter mechanism relies on increasing the "hop" parameter $\kappa$. Understanding over-smoothing explains why this parameter cannot go to infinity and why performance degrades at high $\kappa$.
Quick check: What happens to feature representations of distinct nodes if you repeatedly apply the adjacency matrix without resetting or differentiating them?

## Architecture Onboarding

**Component map:**
Input sequence $X$ -> Q/K Generation (linear projections) -> Dynamic Adjacency (compute $A = \text{softmax}(QK^T)$) -> Multi-hop Expansion (compute $A^{[\kappa]}$) -> Streamlined Convolution (multiply expanded adjacency by features and single weight) -> Activation

**Critical path:** The computation of $A^{[\kappa]}$ and the blkdiag feature stacking. The elimination of $W_{FFN}^{(2)}$ changes the standard Transformer block structure significantly.

**Design tradeoffs:**
- Efficiency vs. Complexity: Removing FFN projection reduces parameters but computing matrix powers ($A^\kappa$) can be computationally heavy for long sequences
- $\kappa$ Sensitivity: Highly sensitive parameter; low $\kappa$ behaves like standard Transformer, optimal $\kappa$ captures long range, high $\kappa$ causes over-smoothing

**Failure signatures:**
- Uniform Attention: If $A^\kappa$ converges to a matrix where all values are equal, the model has over-smoothed (uniform heatmap in visualization)
- Divergence at High $\kappa$: Training loss may plateau or increase if $\kappa$ is set too high for the dataset's inherent temporal radius

**First 3 experiments:**
1. **Hop Sensitivity Sweep**: Run Fighter on "Weather" dataset with $\kappa \in \{1, 3, 6, 8\}$. Plot validation MSE to confirm $\kappa=1$ underperforms (too local), $\kappa=8$ degrades (over-smoothing), with optimal "sweet spot" in between
2. **Gradient Verification**: Implement simple 2-layer Transformer and 2-layer GCN. Feed identical data, inspect gradients of $W_V$ vs $W_{GCN}$ and $W_Q$ vs $A_{GCN}$ to verify "gradient mirroring" claim
3. **FFN Layer Ablation**: Run Fighter with and without "redundant" $W_{FFN}^{(2)}$ projection across multiple datasets to confirm if removing it maintains or improves performance while reducing parameters

## Open Questions the Paper Calls Out

**Open Question 1:** Can the established equivalence between Transformers and GCNs be generalized to other Transformer variants, such as Conformers in speech processing or Diffusion Transformers in generative vision modeling?
Basis: The conclusion explicitly states future work would extend this reinterpretation to Conformers and Diffusion Transformers.
Unresolved: The current theoretical derivation and empirical validation focus on standard time series forecasting. Architectures like Conformers integrate convolutions, and Diffusion Transformers operate on distinct generative principles; whether the attention distribution matrix functions as a dynamic adjacency matrix in these contexts remains unproven.

**Open Question 2:** How can the hop parameter $\kappa$ be determined adaptively for a given layer or input sequence to prevent over-smoothing while maximizing long-range dependency capture?
Basis: The paper analyzes $\kappa$ sensitivity (Figure 3), noting that while $\kappa > 3$ helps long-range dependencies, increasing it too much causes "over-smoothing effect" that degrades performance.
Unresolved: The current implementation requires $\kappa$ as a static hyperparameter. The optimal value varies based on dataset characteristics (e.g., Weather vs. AG News), suggesting a fixed value may be suboptimal for dynamic data patterns.

**Open Question 3:** Does removal of the second feed-forward linear projection ($W^{(\ell,2)}_{FFN}$) impair convergence speed or feature specialization in domains where long-range dependencies are not dominant?
Basis: The results section notes that on AG News text classification task, where "long-range dependencies are less dominant," Fighter's training loss decreases "slightly more slowly" than standard Transformer.
Unresolved: While the paper argues the projection is "redundant" because subsequent layers compensate, the observed slower convergence suggests this redundancy might serve a functional role in optimizing non-spatial features or local context processing in specific modalities.

## Limitations
- The equivalence between Transformers and GCNs relies on specific linearization and aggregation interpretation that may not capture all practical differences
- The claim about distinct gradient flows for topology vs feature learning is based on structural similarity rather than direct gradient analysis
- The Fighter architecture's effectiveness is demonstrated empirically but theoretical justification for removing FFN layer is relatively brief
- Optimal κ value appears dataset-dependent, requiring manual tuning rather than being learned or determined adaptively

## Confidence
- **High Confidence**: The dynamic adjacency matrix interpretation (Mechanism 1) is well-grounded in mathematical formulation and standard Transformer operations
- **Medium Confidence**: The adaptive structure learning via Q/K gradients (Mechanism 2) is theoretically sound but relies on unstated assumptions about gradient behavior
- **Medium Confidence**: The Fighter architecture's effectiveness (Mechanism 3) is demonstrated empirically but theoretical justification for removing FFN layer is brief, and optimal κ appears dataset-dependent

## Next Checks
1. **Gradient Verification**: Implement simple 2-layer Transformer and 2-layer GCN with forced or compared adjacency matrices. Inspect gradients of $W_V$ vs $W_{GCN}$ and $W_Q$ vs $A_{GCN}$ to verify "gradient mirroring" claim
2. **Hop Sensitivity Sweep**: Run Fighter on "Weather" dataset with $\kappa \in \{1, 3, 6, 8\}$. Plot validation MSE to confirm "sweet spot" and degradation at high $\kappa$ due to over-smoothing
3. **FFN Layer Ablation**: Run Fighter with and without "redundant" $W_{FFN}^{(2)}$ projection across multiple datasets to confirm if removing it maintains or improves performance while reducing parameters