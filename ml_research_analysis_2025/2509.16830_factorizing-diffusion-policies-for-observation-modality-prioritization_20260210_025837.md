---
ver: rpa2
title: Factorizing Diffusion Policies for Observation Modality Prioritization
arxiv_id: '2509.16830'
source_url: https://arxiv.org/abs/2509.16830
tags:
- diffusion
- logp
- arxiv
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of how diffusion policies for
  robotics can be improved by prioritizing certain observational modalities over others
  during training. The core idea is to factorize the conditioning process in diffusion
  models, learning a base policy on prioritized modalities and a residual policy on
  the full set, enabling flexibility in modality influence.
---

# Factorizing Diffusion Policies for Observation Modality Prioritization

## Quick Facts
- **arXiv ID:** 2509.16830
- **Source URL:** https://arxiv.org/abs/2509.16830
- **Reference count:** 40
- **Key outcome:** Improves sample efficiency by 15% and robustness to visual distractors/occlusions by 40% through factorization of conditioning in diffusion policies

## Executive Summary
This paper introduces Factorizing Diffusion Policies (FDP), a method that improves diffusion-based robot learning by prioritizing certain observational modalities during training. The approach decomposes the conditional score function using Bayes' rule, learning a base policy on high-priority modalities (like proprioception) and a residual policy on the full set. This factorization allows the model to explicitly learn the contribution of secondary modalities rather than implicitly combining all inputs. The method demonstrates significant improvements in both sample efficiency (15% higher success rates in low-data regimes) and robustness to distribution shifts (40% higher success under visual distractors or camera occlusions), with real-world experiments confirming these benefits.

## Method Summary
FDP addresses multimodal imitation learning for robotics by factorizing the conditioning process in diffusion models. The method trains a base policy on prioritized modalities (e.g., proprioception) using standard diffusion loss, then freezes it and trains a residual policy on all modalities to predict the noise residual. The core innovation is the mathematical decomposition of the score function via Bayes' rule, which allows the residual to learn the additional information provided by secondary modalities without overwriting the base policy. The architecture uses zero-initialized layers to connect the residual to the base, ensuring the model defaults to the base policy and cautiously integrates secondary information. This approach enables flexible modality influence and improves robustness to visual disturbances.

## Key Results
- Achieves 15% higher success rates in low-data regimes compared to standard diffusion policies
- Demonstrates 40% higher success rates under distribution shifts such as visual distractors or camera occlusions
- Shows improved performance and safety under visual disturbances in real-world experiments

## Why This Works (Mechanism)

### Mechanism 1: Score Decomposition via Bayes' Rule
The paper applies Bayes' theorem to decompose the conditional score function $\nabla_{x_t} \log p(x_t | y_{1:M})$ into a base score dependent on prioritized modalities $y_{1:k}$ and a residual score dependent on remaining modalities. This forces the network to explicitly model the addition of information from secondary sources rather than learning an implicit, potentially noisy mapping from all inputs at once. The core assumption is that the action distribution can be modeled as a base distribution conditioned on high-priority modalities, perturbed by a correction term from other modalities.

### Mechanism 2: Implicit Classifier Guidance via Residual Learning
Theorem 1 derives that the optimal residual model parameterizes the score of the likelihood of secondary modalities given the action and base modalities. The training objective forces the residual model to predict the noise residualâ€”the difference between ground truth noise and the base model's prediction. This prevents the residual from "overwriting" the base policy and limits its update to only what is necessary to explain the secondary observations. The core assumption is that the frozen base model provides a stable, reasonably accurate estimation of the denoising direction.

### Mechanism 3: Zero-Initialized Architectural Bias
Zero-initializing the connection layers between base and residual networks acts as a strong regularizer, ensuring the model defaults to the base policy and integrates secondary modalities cautiously. At initialization, FDP acts identically to the base policy. During training, gradients slowly update the connection layers to incorporate visual/tactile information only when it demonstrably reduces the loss. This architectural bias directly combats spurious correlations in high-dimensional modalities like visual distractors.

## Foundational Learning

- **Concept: Score-Based Generative Modeling (Diffusion)**
  - Why needed here: FDP manipulates the score function (gradient of log-probability density) rather than directly predicting actions. Understanding that diffusion models generate data by following these gradients is essential to grasp why "adding scores" corresponds to "combining modalities."
  - Quick check question: In a diffusion model, does the score $\nabla_x \log p(x)$ point toward regions of higher or lower data density?

- **Concept: Bayes' Rule and Conditional Probabilities**
  - Why needed here: The theoretical justification for FDP rests on decomposing a conditional probability $p(x|y_A, y_B)$. Without understanding how Bayes' rule splits joint dependencies, the architecture appears to be merely an engineering trick.
  - Quick check question: If $p(A|B,C)$ is your target, how can you express it using $p(A|B)$ and a term involving $C$?

- **Concept: Residual Learning (Adapters/ControlNet)**
  - Why needed here: The implementation of the residual policy draws heavily from concepts in residual networks and adapters. The core idea is that adding the output of a new branch to an existing (frozen) branch allows for fine-grained control without disrupting the pre-learned features of the base.
  - Quick check question: If you initialize the final layer of a residual branch to zero, what is the output of the combined network at step 0 of training?

## Architecture Onboarding

- **Component map:**
  - $\pi_{base}$ (DiT/UNet) -> Prioritized modalities (e.g., Proprioception) -> **Frozen** after training
  - $\pi_{res}$ (ViT) -> All modalities (Vision + Proprioception + Tactile) -> Predicts noise residual
  - Layer Z (Zero-initialized Conv/Linear) -> Projects residual outputs to base blocks
  - Composed Model = Base outputs + Residual outputs (block-wise addition)

- **Critical path:**
  1. Pre-train $\pi_{base}$ on prioritized modalities until validation loss plateaus (~700 epochs)
  2. Freeze all weights in $\pi_{base}$
  3. Train $\pi_{res}$ and Layer Z; loss calculated on composed noise prediction
  4. Sample actions using composed reverse diffusion process (~150ms latency)

- **Design tradeoffs:**
  - Latency vs. Robustness: FDP nearly triples inference latency (150ms vs 50ms) but provides 40% higher success under distribution shift
  - Rigidity vs. Flexibility: "Prioritization Order" is a hyperparameter; `prop>vision` is robust to occlusion, `vision>prop` is better for large workspaces but brittle to visual noise
  - Safety vs. Speed: Zero-initialization ensures safety but might slow convergence for tasks where secondary modality is critically important immediately

- **Failure signatures:**
  - Catastrophic Forgetting (Base): If $\pi_{base}$ is not strictly frozen, the combined loss can distort base features
  - Base Overfitting: If $\pi_{base}$ is trained too long (e.g., 2000 epochs vs 700), it may fit noise in proprioception, leaving little residual signal
  - Lazy Residual: If Layer Z is not zero-initialized or learning rates are too low, residual branch may fail to activate

- **First 3 experiments:**
  1. **Sanity Check (Base-Only):** Train and evaluate only $\pi_{base}$ on prioritized modality to verify it solves easy instances but fails at precision/localization
  2. **Ablation (Architecture):** Compare "Score Composition" vs "Block-wise Composition" on a task with heavy visual distractors to validate architectural choice
  3. **Robustness Stress Test:** Train FDP (prop>vision) and baseline DP-DiT; introduce color variation/occlusion during evaluation to verify FDP's success rate drops <10% while baseline collapses

## Open Questions the Paper Calls Out
None

## Limitations
- Exact implementation details of block-wise composition and zero-initialized layer Z dimensions are not fully specified, requiring code access for faithful reproduction
- Validation criteria for selecting optimal base model checkpoint (e.g., 700 epochs) are not consistently defined across all benchmarks
- Claim that zero-initialization is the primary reason for observed robustness is not rigorously isolated from other factors

## Confidence

**High confidence:** The core mechanism of score decomposition via Bayes' rule and its equivalence to the residual learning objective (Theorem 1) is mathematically rigorous and well-supported.

**Medium confidence:** Empirical results showing 15% sample efficiency gains and 40% robustness improvements are strong, but exact hyperparameter choices are partially omitted, leaving room for implementation variance.

**Low confidence:** The claim that zero-initialization is the sole or primary reason for the observed robustness is not rigorously isolated from other factors like prioritization order or frozen base model architecture.

## Next Checks
1. **Architectural fidelity test:** Implement both "Score Composition" and "Block-wise Composition" and compare their performance on a task with heavy visual distractors to validate the paper's architectural choice.

2. **Base model training duration ablation:** Systematically vary the base model training epochs (e.g., 300, 700, 1500, 2000) and measure the impact on residual learning and final task performance to confirm the 700-epoch sweet spot.

3. **Prioritization order swap experiment:** Train FDP with both `prop>vision` and `vision>prop` prioritization orders on the same task and evaluate performance under both clean and distractor conditions to verify the paper's claims about task-specific prioritization benefits.