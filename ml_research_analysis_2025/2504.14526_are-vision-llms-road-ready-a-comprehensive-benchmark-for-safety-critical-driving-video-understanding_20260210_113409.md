---
ver: rpa2
title: Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving
  Video Understanding
arxiv_id: '2504.14526'
source_url: https://arxiv.org/abs/2504.14526
tags:
- driving
- vllms
- reasoning
- dvbench
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DVBench, a comprehensive benchmark designed
  to evaluate Vision Large Language Models (VLLMs) in safety-critical driving video
  understanding. DVBench addresses the challenge of assessing VLLMs in autonomous
  driving scenarios by providing 10,000 multiple-choice questions derived from safety-critical
  driving videos.
---

# Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding

## Quick Facts
- arXiv ID: 2504.14526
- Source URL: https://arxiv.org/abs/2504.14526
- Authors: Tong Zeng; Longfeng Wu; Liang Shi; Dawei Zhou; Feng Guo
- Reference count: 40
- Primary result: No model exceeds 40% accuracy on safety-critical driving video understanding

## Executive Summary
This paper introduces DVBench, a comprehensive benchmark designed to evaluate Vision Large Language Models (VLLMs) in safety-critical driving video understanding. The benchmark addresses the challenge of assessing VLLMs in autonomous driving scenarios by providing 10,000 multiple-choice questions derived from safety-critical driving videos. The primary results show that no model achieves over 40% accuracy, highlighting significant limitations in understanding complex driving scenarios. Fine-tuning selected models with domain-specific data improves accuracy by 5.24 to 10.94 percentage points, demonstrating the necessity of targeted adaptation for mission-critical driving applications.

## Method Summary
DVBench evaluates VLLMs using 4,000 video clips (5 seconds each) from the SHRP 2 naturalistic driving study, annotated with 75 expert variables per video. The benchmark generates ~10,000 multiple-choice questions across 25 hierarchical ability levels aligned with driving scenario frameworks. Questions are evaluated using GroupEval, where models must correctly answer each question across N shuffled trials (N = number of choices). The method includes domain knowledge injection and fine-tuning on 2,880 QA pairs to improve performance.

## Key Results
- No VLLM achieves over 40% accuracy on safety-critical driving video understanding
- Fine-tuning improves accuracy by 5.24 to 10.94 percentage points
- GroupEval reveals position bias: models drop 9-25% accuracy compared to IndividualEval
- Models excel at perception (up to 76.7% on environmental conditions) but struggle with reasoning (10-30% on causal and behavioral tasks)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GroupEval exposes brittle model predictions that IndividualEval masks.
- **Mechanism:** Each MCQ is presented N times with correct answer position rotating while distractors shuffle randomly. Models must select correct answer across all trials.
- **Core assumption:** Position bias is primary failure mode in VLLM evaluation.
- **Evidence anchors:** [section 4.2] N-way rotation strategy; [section 4.3, Table 2] 9-25% accuracy drop from IndividualEval to GroupEval; [corpus] Novel approach among driving benchmarks.

### Mechanism 2
- **Claim:** Domain knowledge injection improves question interpretability for general-purpose VLLMs.
- **Mechanism:** Questions supplemented with definitions and context (e.g., "relation to junction") via manually crafted explanations.
- **Core assumption:** Performance gaps stem partially from terminology misunderstanding rather than pure visual reasoning failure.
- **Evidence anchors:** [section 4.4, Table 3] Qwen2-VL-7B improves from 10.6% to 25.1% (+14.5%) with domain knowledge; average improvement is +1.8%.

### Mechanism 3
- **Claim:** Domain-specific fine-tuning yields substantial accuracy gains, demonstrating adaptability but not sufficiency.
- **Mechanism:** Fine-tuning Qwen2-VL-7B on 2,880 human-annotated QA pairs improves GroupEval accuracy from 25.1% to 36.04% (+10.94pp).
- **Core assumption:** 2,880 QA pairs are representative of full test distribution.
- **Evidence anchors:** [abstract] Accuracy gains of 5.24 to 10.94 percentage points; [section 4.6, Table 5] Qwen2-VL-2B improves +5.24pp; Qwen2-VL-7B improves +10.94pp.

## Foundational Learning

- **Position Bias in LLMs**
  - **Why needed here:** GroupEval is designed explicitly to counteract VLLMs' tendency to favor specific answer positions (e.g., always selecting option A).
  - **Quick check question:** If a model answers "A" to a 4-choice question 60% of the time regardless of content, what does this suggest about its evaluation validity?

- **Hierarchical Ability Taxonomy (PEGASUS/NHTSA alignment)**
  - **Why needed here:** DVBench's 25 L3 abilities map to established ADS testing frameworks.
  - **Quick check question:** Why might a model score high on "Environmental Conditions" (perception) but low on "Causal & Responsibility" (reasoning)?

- **Zero-Shot vs. Fine-Tuned Evaluation**
  - **Why needed here:** The paper evaluates models in zero-shot settings first, then fine-tunes on domain data.
  - **Quick check question:** What are the tradeoffs of reporting only zero-shot performance for safety-critical applications?

## Architecture Onboarding

- **Component map:** Video Data (4,000 clips, 5 seconds, 432×324) -> Annotations (75 expert variables) -> Question Bank (10,000 MCQs, 25 abilities) -> Evaluation Framework (GroupEval with N-way rotation) -> Fine-Tuning Pipeline (2,880 QA pairs on Qwen2-VL)

- **Critical path:** 1) Video preprocessing (crop, resize, extract 5-second critical window) 2) Question generation via LLM-assisted rewriting + manual filtering 3) Zero-shot inference with standardized prompts 4) GroupEval scoring across 25 abilities 5) (Optional) Fine-tuning on DVBench training split

- **Design tradeoffs:**
  - GroupEval vs. IndividualEval: GroupEval is more rigorous but increases inference cost by ~N×
  - MCQ vs. Open-Ended: MCQs enable automated scoring but may not capture nuanced reasoning
  - Domain Knowledge: Helps some models (+14.5% for Qwen2-VL-7B) but hurts others (-4.9% for PLLaVA-13B)

- **Failure signatures:**
  - Low perception + low reasoning: Model lacks basic visual grounding
  - High perception + low reasoning: Model detects objects but cannot infer causality
  - Inconsistent GroupEval vs. IndividualEval: Indicates position bias or unstable predictions
  - Domain knowledge degradation: Model misinterprets added context

- **First 3 experiments:**
  1. Baseline GroupEval on Qwen2-VL-7B (zero-shot, no domain knowledge)
  2. Domain knowledge ablation: Same model with domain knowledge enabled
  3. Fine-tuning on DVBench train split: Fine-tune Qwen2-VL-7B on 2,880 QA pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does domain knowledge injection improve accuracy for some VLLMs while degrading performance for others?
- **Basis in paper:** [explicit] Table 3 and Section 4.4 report divergent effects of domain knowledge
- **Why unresolved:** The paper documents the phenomenon but doesn't isolate specific architectural or training differences
- **What evidence would resolve it:** Ablation studies varying domain knowledge format and length across architectures

### Open Question 2
- **Question:** Can VLLMs achieve reliable, consistent predictions across safety-critical driving scenarios with stricter consistency requirements?
- **Basis in paper:** [inferred] Gap between IndividualEval (up to 46.0%) and GroupEval (up to 35.7%) reveals inconsistent predictions
- **Why unresolved:** Paper quantifies consistency gap but doesn't identify whether it stems from positional bias or other factors
- **What evidence would resolve it:** Probing experiments testing whether models rely on visual features versus textual heuristics

### Open Question 3
- **Question:** What architectural or training innovations are needed to close the perception-reasoning gap?
- **Basis in paper:** [explicit] Section 4.5 states VLLMs excel in perception but struggle with reasoning
- **Why unresolved:** Paper benchmarks limitations but doesn't propose mechanisms for improving temporal reasoning or causal inference
- **What evidence would resolve it:** Comparing fine-tuning strategies that explicitly target reasoning tasks versus perception-only adaptation

## Limitations
- Benchmark relies on human-annotated MCQs which may not capture all safety-critical edge cases
- 2,880 fine-tuning QA pairs represent small fraction of full test distribution
- Domain knowledge injection shows inconsistent effects across models with some experiencing performance degradation

## Confidence
- **High Confidence:** Overall finding that VLLMs struggle with safety-critical driving understanding (GroupEval accuracy <40% for all models)
- **Medium Confidence:** Fine-tuning accuracy gains (+5.24 to +10.94 percentage points) depend on representativeness of training pairs
- **Medium Confidence:** Position bias mitigation mechanism via GroupEval is demonstrated empirically

## Next Checks
1. **Out-of-Distribution Testing:** Evaluate fine-tuned models on driving scenarios not represented in DVBench training data
2. **Ablation Study on Fine-Tuning Size:** Systematically vary the number of fine-tuning QA pairs (500, 1000, 2000, 2880)
3. **Cross-Benchmark Comparison:** Test DVBench-trained models on related driving benchmarks (ReasonDrive, CurricuVLM)