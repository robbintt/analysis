---
ver: rpa2
title: 'Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language
  Models'
arxiv_id: '2502.10835'
source_url: https://arxiv.org/abs/2502.10835
tags:
- attention
- arxiv
- layer
- back
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how large language models perform multi-hop
  reasoning and identifies key mechanisms underlying their success and failure. The
  authors introduce "logit flow," a method to trace how prediction logits propagate
  across layers and positions, revealing four stages in single-hop knowledge prediction:
  entity subject enrichment, entity attribute extraction, relation subject enrichment,
  and relation attribute extraction.'
---

# Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2502.10835
- Source URL: https://arxiv.org/abs/2502.10835
- Reference count: 23
- Single 1-layer transformer with back attention achieves accuracy comparable to 2-layer transformer on multi-hop reasoning tasks

## Executive Summary
This paper analyzes how large language models perform multi-hop reasoning and identifies key mechanisms underlying their success and failure. The authors introduce "logit flow," a method to trace how prediction logits propagate across layers and positions, revealing four stages in single-hop knowledge prediction: entity subject enrichment, entity attribute extraction, relation subject enrichment, and relation attribute extraction. Analyzing incorrect two-hop reasoning cases, they find that failures often occur when relation attribute extraction stages capture conflicting logits, reducing prediction accuracy. To address this, they propose "back attention," which allows lower layers to leverage higher-layer hidden states from different positions during attention computation. This approach improves a 1-layer transformer's performance to match that of a 2-layer transformer and, when applied to four large language models, increases accuracy across five reasoning datasets.

## Method Summary
The paper analyzes multi-hop reasoning in LLMs through two main approaches. First, it develops "logit flow" to trace how prediction logits propagate across layers and positions, identifying four distinct stages in single-hop knowledge prediction. Second, it introduces "back attention," a mechanism that allows lower layers to leverage higher-layer hidden states from different positions during attention computation. The method involves computing queries from lower-layer hidden states while deriving keys and values from higher-layer hidden states, enabling backward information flow. The approach is evaluated on both synthetic arithmetic datasets and real-world knowledge-based reasoning datasets.

## Key Results
- Single-hop factual prediction proceeds through four functionally distinct stages distributed across token positions and layer depths
- Two-hop reasoning failures primarily occur at relation attribute extraction stage due to conflicting logits reinforcing intermediate entities
- Back attention improves 1-layer transformer accuracy to match 2-layer transformer and increases accuracy across five reasoning datasets when applied to four LLMs
- Layer 6 is identified as the optimal placement for back attention modules in fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1: Four-Stage Single-Hop Knowledge Prediction via Layer-Position Specialization
Single-hop factual prediction ("Mozart's mother is → Maria") proceeds through four functionally distinct stages: entity subject enrichment (FFN neurons at entity position amplify features), entity attribute extraction (attention neurons propagate features), relation subject enrichment (FFN neurons at relation position prepare processing), and relation attribute extraction (attention and FFN neurons combine entity features with relation embeddings). This staged mechanism relies on functional specialization where FFN handles enrichment and attention handles extraction.

### Mechanism 2: Multi-Hop Failure via Late-Stage Conflicting Logit Interference
Two-hop reasoning failures primarily originate from the relation attribute extraction stage at the first relation position (r1), where high-layer information encoding "e1 features & r1 → e2" reinforces the intermediate entity (e2) more strongly than the correct final entity (e3). The temporal ordering matters because when bridge-entity features are extracted late (after layer 20), insufficient remaining layers exist to fully transform "e2" into "e3" via the second-hop parameters.

### Mechanism 3: Back Attention Enables Backward Feature Propagation
Computing attention queries from lower-layer hidden states while deriving keys and values from higher-layer hidden states allows shallow layers to aggregate task-relevant features computed deep in the network. This creates a backward information pathway where layer l can "see" what higher layers computed at all positions, learning which position-layer combinations are relevant for multi-hop reasoning.

## Foundational Learning

- **Concept: Residual Stream and Logit Lens** - Needed to understand how logit flow attributes final prediction logits to individual layer outputs through residual stream decomposition and logit lens interpretation. Quick check: Can you explain why residual stream formulation allows attributing final prediction logits to individual layer outputs?

- **Concept: FFN as Key-Value Memories** - Needed to interpret FFN outputs as weighted sums of subvalues activated by subkeys, extending Geva et al. (2020)'s interpretation. Quick check: Given F^l = Σ m_{i,k} f c2_k, what does a high importance score Imp(v^l) = log(p(s|v^l + h^{l-1})) - log(p(s|h^{l-1})) indicate about neuron v^l?

- **Concept: Autoregressive Attention Masking in Decoder-Only LLMs** - Needed to understand why r1 cannot access r2 token information and how back attention modifies this constraint. Quick check: Why can the last position attend to r1 and r2, but r1 cannot attend to r2? How does back attention modify this constraint?

## Architecture Onboarding

- **Component map:** Logit Flow Analyzer -> Four-Stage Predictor -> Back Attention Module -> Fine-tuning Interface
- **Critical path:** Entity features flow from e1 position lower layers → relation/last position deeper layers → unembedding; for two-hop failure, r1 high-layer features extracted late → direct e2 logit boost > e3 boost → wrong prediction; for back attention fix, higher-layer r1 features → back attention aggregation → lower-layer last position → activates "e2 → e3" parameters → correct prediction
- **Design tradeoffs:** Layer 6 achieves peak accuracy; 1-layer transformer + back attention (56.7% parameters of 2-layer) achieves comparable performance; training from scratch enables full integration while fine-tuning preserves base model
- **Failure signatures:** 42.4% of two-hop false predictions predict e2 (intermediate entity) instead of e3; relation position underutilization in two-hop queries; back attention at layers 0-5 shows high variance while layers 7+ show monotonic decline
- **First 3 experiments:**
  1. Reproduce logit flow visualization on single-hop queries: Take 50 "e1's r1 is → e2" examples, compute importance scores for top 300 attention neurons, visualize FFN inner products across layers/positions
  2. Ablate back attention layer placement: Fine-tune back attention on layers 4, 6, 8, 10, 12 of Llama3-8B using TwoHop dataset; plot accuracy curve to confirm layer-6 peak
  3. Intervention study on r1 features: Using activation patching, replace r1 high-layer (layers 25-31) features in false cases with features from correct cases; measure accuracy change

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the four-stage mechanism identified in single-hop knowledge prediction generalize to diverse reasoning domains beyond factual knowledge retrieval, such as symbolic reasoning or logical deduction? The analysis focuses on specific knowledge queries and other types of reasoning tasks might involve different mechanisms not captured in the analysis.

- **Open Question 2:** How does the simultaneous application of back attention to multiple layers affect model performance compared to the single-layer application tested? Experiments only fine-tuned a single back attention module, and it is unknown if adding modules to multiple layers would result in cumulative gains or interference.

- **Open Question 3:** Can the back attention mechanism effectively resolve reasoning failures in multi-hop queries involving more than two hops? The study focuses exclusively on two-hop reasoning, but the "hopping too late" hypothesis may compound in longer reasoning chains (3+ hops).

## Limitations

- Dataset filtering procedure (removing shortcut cases, capping e3 occurrences) is not fully specified, making exact reproduction difficult
- Neuron-level importance analysis depends heavily on the choice of top-300 attention neurons for inner-product weighting, which may not be robust across models
- Back attention introduces architectural complexity that may not generalize beyond the tested autoregressive decoder-only setting

## Confidence

- **High Confidence:** The four-stage single-hop prediction mechanism is well-supported by consistent neuron-level patterns across multiple LLMs and layer visualizations
- **Medium Confidence:** The multi-hop failure mechanism at r1 position is strongly evidenced by logit flow comparisons, but the causal role of conflicting logits needs direct intervention validation
- **Medium Confidence:** Back attention's effectiveness is demonstrated across multiple models and datasets, but the exact layer-6 sweet spot and training dynamics require more systematic exploration

## Next Checks

1. **Intervention Study on r1 Features:** Using activation patching, replace r1 high-layer (layers 25-31) features in false cases with features from correct cases and measure accuracy change to directly test whether conflicting logits at r1 cause two-hop failures.

2. **Cross-Model Neuron Pattern Replication:** Apply the logit flow analysis to additional decoder-only LLMs (e.g., GPT-2, OPT) to verify whether the four-stage prediction pattern and r1 failure mechanism are universal properties of transformer architectures.

3. **Back Attention Ablation at Layer 6:** Perform a controlled ablation study where back attention is added at layer 6 but with varying target layer selections (only layer 6 vs. layers 6-7 vs. layers 6-31) to understand whether the full stacked-higher-layer mechanism is necessary for the observed improvements.