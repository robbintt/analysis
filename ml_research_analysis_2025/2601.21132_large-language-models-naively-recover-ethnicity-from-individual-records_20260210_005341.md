---
ver: rpa2
title: Large Language Models Naively Recover Ethnicity from Individual Records
arxiv_id: '2601.21132'
source_url: https://arxiv.org/abs/2601.21132
tags:
- accuracy
- bisg
- gemini
- data
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that large language models can infer ethnicity
  from names with accuracy exceeding Bayesian Improved Surname Geocoding (BISG) without
  additional training data. Using Florida and North Carolina voter files, LLM-based
  classification achieved up to 84.7% accuracy compared to BISG's 68.2%, with notable
  improvements for Black and Hispanic voters.
---

# Large Language Models Naively Recover Ethnicity from Individual Records
## Quick Facts
- arXiv ID: 2601.21132
- Source URL: https://arxiv.org/abs/2601.21132
- Reference count: 2
- LLMs achieve 84.7% ethnicity classification accuracy vs BISG's 68.2% without training data

## Executive Summary
Large language models can infer ethnicity from names with accuracy exceeding the widely-used Bayesian Improved Surname Geocoding (BISG) method, without requiring labeled training data. Using Florida and North Carolina voter files, LLM-based classification achieved up to 84.7% accuracy compared to BISG's 68.2%, with notable improvements for Black and Hispanic voters. The approach extends beyond the United States, successfully classifying religious sects in Lebanon (64.3%), caste in India (74.0%), and recovering known population distributions across six countries. Including metadata like party registration improved accuracy to 86.7%. The method overcomes BISG's limitations of requiring labeled training data and being restricted to predefined categories, while also reducing income-based classification bias. Small transformer models fine-tuned on LLM labels can achieve BISG-level accuracy for local deployment.

## Method Summary
The study compares LLM-based ethnicity classification against BISG using voter files from Florida and North Carolina. A simple prompt template asks models to classify race/ethnicity based on full name and location, returning only one category. Multiple models were tested including Gemini 3 Flash, GPT-4o, and various open-source alternatives. Accuracy was measured against self-reported race data. The approach was extended to Lebanon for religious sect classification and India for caste inference. Aggregate voter rolls from six countries were used to recover known population distributions. Fine-tuning with LoRA on small transformer models demonstrated that local deployment achieving BISG-level accuracy is feasible.

## Key Results
- LLM-based classification achieved 84.7% accuracy vs BISG's 68.2% on US voter files
- Notable improvements for Black and Hispanic voters (8.6pp and 11.5pp gains respectively)
- Successfully classified religious sects in Lebanon (64.3%) and caste in India (74.0%)
- Small transformer models fine-tuned on LLM labels can match BISG accuracy for local deployment

## Why This Works (Mechanism)
LLMs appear to leverage patterns learned during pretraining about naming conventions across different ethnic groups and geographic regions. The models encode correlations between name components, surname distributions, and demographic characteristics from their training corpus. When provided with location metadata, they can disambiguate cases where names are shared across multiple ethnic groups by incorporating geographic naming patterns. This zero-shot capability suggests LLMs have internalized statistical relationships between names and demographics that BISG attempts to capture through explicit statistical modeling of surname and location distributions.

## Foundational Learning
- **Bayesian Improved Surname Geocoding (BISG)**: Statistical method that estimates race/ethnicity probabilities using surname distributions and geographic data; needed because it represents the current state-of-the-art baseline for demographic inference
- **Prompt engineering for classification**: Simple template-based prompting without few-shot examples; needed because it demonstrates LLMs can perform zero-shot classification tasks
- **Geospatial metadata integration**: Combining name with location information; needed because geography significantly improves classification accuracy
- **Temperature settings in LLM inference**: Temperature=0 for deterministic outputs; needed because reproducibility is critical for validation
- **Fine-tuning with LoRA**: Low-rank adaptation for efficient model customization; needed because it enables local deployment of LLM-derived models
- **Population distribution recovery**: Aggregating individual classifications to estimate demographic proportions; needed because it validates method's ability to capture real-world demographics

## Architecture Onboarding
- **Component map**: Prompt template -> LLM API -> Classification output -> Accuracy evaluation -> (optional) Fine-tuning pipeline
- **Critical path**: Name + location input → LLM classification → Accuracy validation vs ground truth
- **Design tradeoffs**: Zero-shot learning (no training data) vs. model API dependency; simple prompting vs. potential accuracy gains from few-shot examples
- **Failure signatures**: Non-deterministic outputs (temperature not set to 0), reduced accuracy with surname-only input, model version drift with proprietary APIs
- **3 first experiments**: 1) Test with full names vs. surnames only to quantify metadata impact; 2) Compare multiple model providers (Gemini, GPT, open-source) for consistency; 3) Validate classification on synthetic names to check for data leakage

## Open Questions the Paper Calls Out
- How do LLM-based methods perform on populations with less distinct naming patterns or higher rates of name ambiguity?
- What is the impact of training data composition on LLM inference capabilities for demographic classification?
- Can the accuracy gap between LLM and BISG methods be further reduced through prompt engineering or ensembling techniques?
- How does the method handle name changes due to marriage, naturalization, or other life events?

## Limitations
- Performance depends heavily on model selection, with significant variance between different LLMs
- Method requires internet access and API calls, creating deployment barriers for resource-constrained environments
- Results from proprietary models may not be reproducible as underlying models evolve
- Accuracy gains for minority groups may not transfer to populations with less distinct naming patterns
- The approach may perpetuate or amplify existing biases present in LLM training data

## Confidence
- Core claim (LLMs exceed BISG without training): High
- Religious sect/caste classification in non-US contexts: Medium
- Fine-tuned small transformers matching BISG accuracy: High

## Next Checks
1. Test the method on populations with ambiguous naming conventions (e.g., multicultural urban centers) to assess robustness beyond the studied geographic contexts
2. Conduct cross-validation using multiple model versions and providers to establish consistency as underlying LLMs are updated
3. Evaluate the privacy implications by testing whether LLMs can be prompted to reveal inference logic or training data leakage when given synthetic names