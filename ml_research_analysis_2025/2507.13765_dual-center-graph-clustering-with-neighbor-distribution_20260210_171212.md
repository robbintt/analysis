---
ver: rpa2
title: Dual-Center Graph Clustering with Neighbor Distribution
arxiv_id: '2507.13765'
source_url: https://arxiv.org/abs/2507.13765
tags:
- distribution
- neighbor
- graph
- clustering
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of graph clustering by proposing
  a Dual-Center Graph Clustering (DCGC) approach that leverages neighbor distribution
  properties. The core idea is to utilize neighbor distribution as a reliable supervision
  signal for mining hard negative samples in contrastive learning and to introduce
  neighbor distribution center alongside feature center to construct a dual-target
  distribution for dual-center optimization.
---

# Dual-Center Graph Clustering with Neighbor Distribution

## Quick Facts
- arXiv ID: 2507.13765
- Source URL: https://arxiv.org/abs/2507.13765
- Reference count: 40
- Primary result: 15% ACC and 23% NMI improvement on Wisconsin dataset vs baselines

## Executive Summary
This paper addresses the fundamental limitations of existing graph clustering methods that rely on single-center optimization in feature space. The authors propose Dual-Center Graph Clustering (DCGC), which introduces neighbor distribution as a reliable supervision signal for mining hard negative samples in contrastive learning. The method leverages both feature centers and neighbor distribution centers to construct a dual-target distribution for optimization. Experiments on six graph datasets demonstrate significant improvements over state-of-the-art baselines, with DCGC achieving 15% ACC and 23% NMI gains on the Wisconsin dataset.

## Method Summary
DCGC operates on graph data through an adaptive filterbank that generates node embeddings using low-pass, high-pass, and identity filters with learnable coefficients. The method employs two unshared MLPs to produce embeddings that undergo contrastive learning with hard negative mining based on neighbor distribution similarity. A dual-center optimization framework simultaneously minimizes distances between nodes and their feature centers while aligning neighbor distributions with their corresponding class distributions. The overall loss combines contrastive, reconstruction, and dual-center KL divergence terms, trained in two phases: 300 epochs of pretraining followed by 100 epochs of dual-center fine-tuning with periodic target distribution updates.

## Key Results
- DCGC achieves 15% improvement in ACC on Wisconsin dataset
- 23% improvement in NMI on Wisconsin dataset compared to baseline methods
- Consistent performance gains across all six benchmark datasets
- Demonstrates superiority over existing graph clustering approaches

## Why This Works (Mechanism)
The key innovation lies in using neighbor distribution as a reliable supervision signal that captures structural relationships between nodes. By mining hard negative samples through neighbor distribution similarity, the method focuses learning on the most challenging node pairs that traditional feature-based approaches might miss. The dual-center optimization simultaneously optimizes both feature space and neighbor distribution space, providing complementary guidance that prevents the limitations of single-center approaches. This dual perspective ensures that clusters are both structurally coherent and feature-distinct.

## Foundational Learning
- **Neighbor distribution**: The probability distribution of neighbors belonging to different classes; needed because it provides structural context that complements node features; quick check: verify neighbor distributions are non-uniform and informative
- **Hard negative mining**: Selecting difficult negative pairs for contrastive learning; needed to focus model capacity on challenging distinctions; quick check: confirm hard negatives have high neighbor similarity but low feature similarity
- **Dual-center optimization**: Simultaneously optimizing feature centers and distribution centers; needed to provide complementary supervision signals; quick check: ensure both centers converge during training
- **Adaptive filterbank**: Multi-scale graph filtering with learnable coefficients; needed to capture different structural patterns at multiple scales; quick check: verify learned coefficients differ meaningfully across datasets

## Architecture Onboarding

**Component map**: Adaptive Filterbank → Two MLPs → Contrastive Loss + Reconstruction → K-means → Neighbor Distributions → Hard Negative Mining → Dual-Center Optimization

**Critical path**: Filterbank outputs → MLP embeddings → Contrastive pretraining → K-means initialization → Neighbor distribution computation → Hard negative mining → Dual-center fine-tuning → Final clustering

**Design tradeoffs**: Single-center vs dual-center optimization provides complementary structural and feature guidance but increases complexity; neighbor-based hard negative mining is more reliable than feature-based methods but computationally intensive

**Failure signatures**: 
- Unstable training when neighbor distributions are too sparse (low-degree nodes)
- Poor clustering if K-means initialization is suboptimal
- Numerical instability in dual-center KL divergence with frequent target updates

**First experiments**:
1. Verify adaptive filterbank learns different coefficients across datasets by logging α1, α2, α3 values
2. Compare neighbor distribution similarity vs feature similarity for hard negative mining effectiveness
3. Test dual-center optimization with varying update frequencies (T=1, 5, 10) to find stability sweet spot

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter configurations (filterbank coefficients, threshold τ) not fully specified per dataset
- Computational complexity not analyzed for scalability to large graphs
- Dependence on K-means initialization quality for dual-center optimization
- Neighbor distribution reliability on sparse graphs with low-degree nodes

## Confidence
- **High confidence**: Theoretical framework connecting neighbor distributions to hard negative mining is well-developed
- **Medium confidence**: Experimental results show significant improvements, but exact hyperparameter configurations unclear  
- **Medium confidence**: Dual-center objective is novel, though advantages over single-center approaches could be more thoroughly analyzed

## Next Checks
1. Reproduce the adaptive filterbank with default t=2 and log the learned α coefficients across different datasets to verify stability
2. Perform ablation studies removing either the contrastive loss or dual-center component to quantify their individual contributions
3. Test on datasets with varying density (sparse vs dense graphs) to evaluate the robustness of neighbor distribution-based hard negative mining across graph structures