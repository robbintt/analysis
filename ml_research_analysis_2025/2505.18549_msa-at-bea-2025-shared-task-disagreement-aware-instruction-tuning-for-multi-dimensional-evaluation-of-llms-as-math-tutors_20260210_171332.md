---
ver: rpa2
title: 'MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional
  Evaluation of LLMs as Math Tutors'
arxiv_id: '2505.18549'
source_url: https://arxiv.org/abs/2505.18549
tags:
- across
- tutor
- task
- evaluation
- mistake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents MSA-M ATHEVAL, a unified system for evaluating
  AI tutor responses across four pedagogical dimensions in the BEA 2025 Shared Task.
  The approach uses LoRA-based fine-tuning of the math-specialized Mathstral-7B-v0.1
  model with a consistent training pipeline across all tracks.
---

# MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors

## Quick Facts
- arXiv ID: 2505.18549
- Source URL: https://arxiv.org/abs/2505.18549
- Reference count: 6
- Primary result: First place in Providing Guidance track, third in Actionability, fourth in Mistake Identification and Mistake Location

## Executive Summary
This work presents MSA-M ATHEVAL, a unified system for evaluating AI tutor responses across four pedagogical dimensions in the BEA 2025 Shared Task. The approach uses LoRA-based fine-tuning of the math-specialized Mathstral-7B-v0.1 model with a consistent training pipeline across all tracks. An ensemble-based inference strategy leverages model disagreement to improve minority-class prediction coverage, particularly for ambiguous cases labeled as "To some extent." The system achieved first place in Providing Guidance, third in Actionability, and fourth in both Mistake Identification and Mistake Location, demonstrating the effectiveness of scalable instruction tuning and disagreement-driven modeling for robust multi-dimensional evaluation of LLMs as educational tutors.

## Method Summary
MSA-M ATHEVAL fine-tunes the math-specialized Mathstral-7B-v0.1 model using LoRA (r=64, α=2.0) on query/value projections for efficient adaptation to pedagogical evaluation. The system processes MRBench dialogues through track-specific prompts and trains separate LoRA adapters for each of the four evaluation dimensions. Training uses a maximum of 500 steps with batch size 2, AdamW optimizer (lr=4e-5), and gradient clipping. An ensemble of five independently fine-tuned models per track employs disagreement-aware inference that preferentially preserves minority-class predictions when consensus is low, addressing class imbalance in the 3-way classification (Yes/To some extent/No).

## Key Results
- Achieved 1st place in Providing Guidance track
- Secured 3rd place in Actionability track
- Ranked 4th in both Mistake Identification and Mistake Location tracks
- Demonstrated effectiveness of ensemble disagreement-aware inference for minority-class recovery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA-based fine-tuning enables efficient domain adaptation of a math-specialized LLM without full parameter updates.
- **Mechanism:** Low-rank matrices (A ∈ Rd×r, B ∈ Rr×d) are injected into attention query and value projections. Only these adapters are trained while base weights remain frozen, reducing memory while preserving the model's pre-trained mathematical reasoning capabilities.
- **Core assumption:** The low-rank decomposition (r=64, α=2.0) has sufficient expressiveness to capture pedagogical evaluation patterns without catastrophically forgetting math reasoning abilities.
- **Evidence anchors:**
  - [Section 3.3]: "We used LoRA with a rank of r = 64, scaling factor α = 2.0... Adapters were injected into the attention query and value projections in each Transformer block."
  - [Section 6]: "LoRA significantly reduces memory and compute costs, its low-rank decomposition can constrain the model's expressiveness in capturing nuanced pedagogical feedback"
  - [corpus]: Weak—no direct corpus evidence on LoRA effectiveness for this specific task; related BEA 2025 papers use different approaches (MPNet ensembles, retrieval-augmented prompting).
- **Break condition:** If minority-class recall degrades significantly compared to full fine-tuning, the rank may be insufficient for capturing subtle pedagogical distinctions.

### Mechanism 2
- **Claim:** Disagreement-aware ensemble filtering improves minority-class prediction by selectively preserving "To some extent" labels when models lack consensus.
- **Mechanism:** Five independently fine-tuned models generate predictions. Rather than majority voting (which biases toward dominant "Yes" labels), the system analyzes class distribution and preferentially retains minority-class predictions proportional to development set frequencies.
- **Core assumption:** Model disagreement signals genuine ambiguity rather than model failure; ensemble predictions are sufficiently independent to provide meaningful diversity.
- **Evidence anchors:**
  - [Abstract]: "An ensemble-based inference strategy leverages model disagreement to improve minority-class prediction coverage, particularly for ambiguous cases labeled as 'To some extent.'"
  - [Section 3.4]: "If the ensemble disagrees, we analyze the class distribution and prefer predictions that preserve the relative frequency of 'To some extent' observed in the development set."
  - [Section 6]: "The benefit may diminish if the base models exhibit correlated predictions. Prior work shows that ensembles are most effective when model predictions are diverse and independent."
  - [corpus]: LeWiDi-2025 shared task explicitly addresses learning with disagreements, suggesting broader validity of this approach for ambiguous labels.
- **Break condition:** If all five models converge to identical predictions (high correlation), the ensemble provides no additional signal—verify variance across runs before relying on this mechanism.

### Mechanism 3
- **Claim:** Math-specialized instruction-tuned models transfer better to math tutoring evaluation than general-purpose LLMs.
- **Mechanism:** Mathstral-7B-v0.1 is pre-trained and instruction-tuned on mathematical reasoning tasks (GSM8K, MATH, MMLU-STEM), providing domain-relevant representations that generalize to evaluating math tutoring dialogues without task-specific architectural changes.
- **Core assumption:** Mathematical reasoning capability correlates with ability to evaluate pedagogical quality in math dialogues.
- **Evidence anchors:**
  - [Section 3.2]: "Mathstral-7B-v0.1 was selected based on its strong performance in math-specific benchmarks... It was instruction-tuned by Project Numina on mathematical reasoning tasks."
  - [Section 6]: "The specialization of Mathstral-7B-v0.1 to mathematical reasoning may hinder generalization to non-mathematical domains."
  - [corpus]: NeuralNexus and BD teams used different approaches (RAG, MPNet), making direct model comparison difficult—no clear evidence yet that Mathstral is optimal for this task.
- **Break condition:** Performance on non-math tutoring domains would likely degrade; test with other subjects before assuming generalization.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Understanding how parameter-efficient fine-tuning works is essential for reproducing results and debugging adapter issues. The paper assumes familiarity with attention projections and weight decomposition.
  - **Quick check question:** Can you explain why LoRA adapters are injected into query and value projections specifically, rather than all attention weights?

- **Concept: Macro-F1 vs Micro-F1 for Imbalanced Classes**
  - **Why needed here:** The entire ensemble strategy is motivated by macro-F1's equal weighting of all classes. Without understanding this, the disagreement filtering rationale is unclear.
  - **Quick check question:** If "Yes" labels are 60% of the data, why does optimizing for macro-F1 require special handling of "To some extent" predictions?

- **Concept: Ensemble Uncertainty and Model Diversity**
  - **Why needed here:** The disagreement mechanism assumes independent predictions across ensemble members. Understanding when ensembles fail (correlated errors) is critical for debugging.
  - **Quick check question:** What would happen to the disagreement-aware filtering if all five models were trained with identical seeds and hyperparameters?

## Architecture Onboarding

- **Component map:** MRBench dialogues → JSONL preprocessing → Mathstral-7B-v0.1 + LoRA adapters → Per-track fine-tuning → Ensemble inference with disagreement filtering → Macro-F1 evaluation

- **Critical path:** Data preprocessing → LoRA adapter initialization → Per-track fine-tuning (separate models) → Checkpoint selection at best dev macro-F1 → Ensemble inference with disagreement filtering → Submission

- **Design tradeoffs:**
  - **Single unified model vs track-specific adapters:** Paper uses same base model across all tracks but trains separate LoRA adapters per track—no shared parameters across dimensions.
  - **Ensemble size (5):** More models increase diversity but also inference cost; 5 chosen empirically but not ablated.
  - **Minority-class preservation:** Prioritizing "To some extent" recall may hurt precision on dominant classes—appropriate only when macro-F1 is the metric.

- **Failure signatures:**
  - All ensemble models predict "Yes" → disagreement filtering has no effect → check initialization diversity
  - Macro-F1 significantly lower than lenient-F1 → minority-class recall is failing → increase ensemble size or adjust filtering threshold
  - Training loss plateaus early → LoRA rank may be too low or learning rate too conservative
  - Model predicts only majority class → class imbalance not handled; consider weighted loss or oversampling

- **First 3 experiments:**
  1. **Baseline single model:** Train one LoRA adapter per track, evaluate on dev set to establish single-model macro-F1 before ensemble.
  2. **Ensemble size ablation:** Compare 3, 5, and 7 model ensembles to validate that disagreement filtering scales with diversity.
  3. **Label distribution analysis:** Compare predicted vs dev set label distributions for each track; if "To some extent" is underpredicted, the disagreement mechanism is not activating.

## Open Questions the Paper Calls Out

- **Question:** Can domain-specialized models like Mathstral-7B generalize their pedagogical evaluation capabilities to non-mathematical educational domains without catastrophic forgetting?
  - **Basis in paper:** [explicit] The conclusion states: "Future work will explore cross-domain generalization and dynamic calibration strategies to further enhance robustness." The limitations section also notes that "the specialization of Mathstral-7B-v0.1 to mathematical reasoning may hinder generalization to non-mathematical domains."
  - **Why unresolved:** The system was only evaluated on math tutoring dialogues from MRBench. No experiments were conducted on other subject domains (e.g., science, language arts) to test transfer.
  - **What evidence would resolve it:** Fine-tuning and evaluation of the same pipeline on datasets from non-mathematical tutoring domains, comparing performance against general-purpose base models.

- **Question:** How does the correlation between ensemble member predictions affect the effectiveness of disagreement-aware filtering for minority-class recovery?
  - **Basis in paper:** [explicit] The limitations section states: "the benefit may diminish if the base models exhibit correlated predictions. Prior work shows that ensembles are most effective when model predictions are diverse and independent, which may not always hold in practice."
  - **Why unresolved:** The paper does not analyze or report the degree of prediction correlation among the five ensemble members, nor how this correlation varies across tracks or label classes.
  - **What evidence would resolve it:** Quantitative analysis of inter-model prediction correlation coefficients, with ablation studies comparing ensemble performance under different diversity thresholds.

- **Question:** What is the optimal rank configuration for LoRA adapters when fine-tuning for nuanced pedagogical feedback classification?
  - **Basis in paper:** [inferred] The limitations section notes that "LoRA's low-rank decomposition can constrain the model's expressiveness in capturing nuanced pedagogical feedback," yet only a single configuration (r=64, α=2.0) was tested.
  - **Why unresolved:** No ablation study was conducted on LoRA rank or scaling factor, so it remains unclear whether higher ranks would improve capture of subtle distinctions like "To some extent" labels.
  - **What evidence would resolve it:** Systematic ablation across different rank values (e.g., r=16, 32, 64, 128, 256) with analysis of per-class F1 scores, particularly on the minority "To some extent" class.

- **Question:** Can task-specific hierarchical loss functions better capture the ordinal severity of pedagogical evaluation errors than macro-F1 optimization?
  - **Basis in paper:** [explicit] The limitations section states: "misclassifying a completely wrong tutor response as 'To some extent' is penalized equally to a more plausible confusion between 'Yes' and 'To some extent'... [lenient evaluation] does not fully capture the instructional severity of errors."
  - **Why unresolved:** The current training uses standard cross-entropy loss without incorporating ordinal relationships between labels or differential penalty weights for pedagogically critical errors.
  - **What evidence would resolve it:** Comparison of models trained with ordinal regression losses or cost-sensitive learning against the current approach, evaluated on both macro-F1 and a new pedagogically-weighted error metric.

## Limitations

- **Dataset Size Constraints:** With only 192 dialogues and 1,596 tutor responses, the model may be overfitting despite LoRA's parameter efficiency and aggressive 500-step training schedule.

- **Generalizability Limitations:** The approach is validated only on math tutoring dialogues (Bridge + MathDial), with no evidence of transfer to other pedagogical domains or subjects.

- **Model Diversity Uncertainty:** The ensemble strategy assumes independent predictions across five models, but the paper doesn't verify prediction correlation, which could render disagreement filtering ineffective.

## Confidence

- **High Confidence:** The technical implementation details (LoRA configuration, training hyperparameters, dataset preprocessing) are sufficiently specified for reproduction. The system's performance ranking (1st in Providing Guidance, 3rd in Actionability, 4th in others) is verifiable through shared task results.

- **Medium Confidence:** The claim that Mathstral's math specialization improves pedagogical evaluation is plausible but not directly validated against general-purpose LLMs on this specific task. The ensemble strategy's effectiveness depends on unverified assumptions about model diversity.

- **Low Confidence:** The exact disagreement filtering mechanism and its activation thresholds are underspecified, making it difficult to reproduce the ensemble's behavior precisely.

## Next Checks

1. **Ensemble Diversity Analysis:** Run all five models with identical seeds and hyperparameters to verify that prediction correlation is low enough for disagreement filtering to provide meaningful signal. Calculate pairwise prediction agreement rates.

2. **Label Distribution Validation:** Compare predicted vs actual label distributions for each track on the development set. Verify that "To some extent" recall improves with ensemble disagreement filtering compared to single-model baselines.

3. **Dataset Size Sensitivity:** Train the same pipeline on progressively smaller subsets of the training data (50%, 25%, 10%) to assess overfitting risk and determine the minimum viable dataset size for this approach.