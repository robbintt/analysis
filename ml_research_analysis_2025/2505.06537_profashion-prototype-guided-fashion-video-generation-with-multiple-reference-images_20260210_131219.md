---
ver: rpa2
title: 'ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference
  Images'
arxiv_id: '2505.06537'
source_url: https://arxiv.org/abs/2505.06537
tags:
- reference
- video
- fashion
- generation
- profashion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating fashion videos with
  consistent views when garments have patterns visible only from specific angles.
  The proposed solution, ProFashion, leverages multiple reference images and introduces
  a Pose-aware Prototype Aggregator (PPA) that selects and aggregates fine-grained
  and global reference features based on pose similarity.
---

# ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images

## Quick Facts
- arXiv ID: 2505.06537
- Source URL: https://arxiv.org/abs/2505.06537
- Reference count: 40
- One-line primary result: Multi-reference fashion video generation with pose-aware prototype aggregation improves view-consistent garment generation over single-reference baselines

## Executive Summary
This paper addresses the challenge of generating fashion videos with consistent garment patterns when the clothing has details visible only from specific angles. The proposed ProFashion method leverages multiple reference images and introduces a Pose-aware Prototype Aggregator (PPA) that intelligently selects and combines reference features based on pose similarity. This produces frame-wise prototypes that guide the denoising process without significant computational overhead. The method is evaluated on newly collected MRFashion-7K dataset and UBC Fashion dataset, showing significant improvements over single-reference baselines in both image-level and video-level metrics, as well as in human evaluations.

## Method Summary
ProFashion is a diffusion-based model that generates fashion videos conditioned on multiple reference images and a driving pose sequence. The method uses a two-stage training approach: first training on single frames, then fine-tuning on 16-frame clips. The core innovation is the Pose-aware Prototype Aggregator (PPA), which selects reference features based on pose similarity to create frame-wise prototypes. These prototypes are then used in a Flow-enhanced Prototype Instantiator (FPI) that incorporates human keypoint motion flow to improve temporal consistency. The model is built on Stable Diffusion v1.5 and uses DDIM inference with 35 steps.

## Key Results
- ProFashion achieves SSIM↑, PSNR↑, and LPIPS↓ improvements over single-reference baselines on MRFashion-7K and UBC Fashion datasets
- Video-level FVD↓ metrics show significant improvement in motion smoothness and consistency
- Human evaluations demonstrate better Character Authenticity, Clothing Detail, and Motion Fluency compared to baselines
- The pose-aware aggregation mechanism effectively reduces feature blending artifacts compared to simple averaging

## Why This Works (Mechanism)
The method works by addressing the core challenge of maintaining view-consistent garment patterns across different poses. When a garment has patterns visible only from specific angles, single-reference methods struggle to maintain consistency when the pose changes. ProFashion solves this by using multiple references and intelligently selecting the most relevant features for each frame based on pose similarity. The PPA creates frame-wise prototypes that capture the fine-grained details needed for each specific view, while the FPI uses motion flow to ensure temporal consistency. This approach allows the model to maintain pattern consistency across different poses without requiring all possible views in the references.

## Foundational Learning
- **Concept**: Cross-Attention in Latent Diffusion Models
  - Why needed here: The Reference Encoder and FPI use cross-attention to inject information from reference images (via prototypes) and driving poses into the denoising U-Net. Understanding how queries, keys, and values work is essential to grasp the prototype injection mechanism.
  - Quick check question: In a cross-attention layer within a denoising U-Net, what serves as the "query" and what typically serves as the "key" and "value" when conditioning on an external image embedding?

- **Concept**: Optical Flow and Motion Representation
  - Why needed here: The FPI module predicts a dense offset map supervised by keypoint flow. Understanding optical flow (per-pixel motion between frames) and sparse vs. dense representations is key to understanding how temporal consistency is enforced.
  - Quick check question: What is the difference between sparse optical flow (e.g., from keypoints) and dense optical flow, and why would a model predict the latter while being supervised by the former?

- **Concept**: Multi-Conditioning in Generative Models
  - Why needed here: The model is conditioned on multiple reference images and a driving pose sequence. Understanding how models fuse these different conditional inputs (e.g., via concatenation, averaging, or attention-based selection like PPA) is central to this work.
  - Quick check question: What are two common methods for fusing features from multiple conditioning inputs in a neural network, and what is a potential problem with simple averaging?

## Architecture Onboarding
- **Component Map**: Input -> Pose/Reference Encoders -> Reference Encoder (U-Net with self/cross-attention) -> PPA (pose-aware selector + aggregator) -> Fine-Grained & Global Prototypes -> FPI (Denoising U-Net) -> Flow-Enhanced Temporal Attention -> Denoised Latent -> VAE Decoder -> Video Output
- **Critical Path**: The correct functioning of PPA is the most critical. If the pose-aware selector creates an incorrect aggregation map, the wrong reference features will be combined, leading to flawed prototypes. These flawed prototypes then misguide the entire denoising process in FPI, which cannot be corrected by the motion flow module.
- **Design Tradeoffs**:
  - Number of Reference Images (Nr): More references provide more information but increase aggregation complexity and potential for conflicting features. The paper sets Nr=3.
  - Pose Similarity Metric: Simple dot-product and softmax may fail for nuanced poses. More complex metrics could be more robust but computationally heavier.
  - Dense Flow Prediction from Sparse Keypoints: This is an ill-posed problem requiring the model to hallucinate cloth texture motion based only on joint movement, favoring computational simplicity over perfect physical accuracy.
- **Failure Signatures**:
  - Hallucinated Patterns: Wrong reference selection by PPA may paint front-patterns onto back-views of garments
  - Text Distortion: Feature blending in aggregation causes blurring and distortion of text on garments
  - Temporal Flickering: Erratic keypoint flow or failed offset prediction causes jittery motion, especially in loose clothing areas
- **First 3 Experiments**:
  1. PPA Ablation vs. Baselines: Compare full model against same architecture with simple averaging or concatenation of reference features. Report SSIM, PSNR, and LPIPS.
  2. FPI Ablation with/without Motion Flow: Train versions with and without flow supervision. Evaluate FVD and conduct human evaluation on motion fluency.
  3. Generalization to Unseen Poses: Test on driving pose sequences significantly different from reference poses (e.g., 360-degree turn). Analyze if PPA degrades gracefully or fails completely.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the limitations of the current approach. One major limitation is the struggle to maintain textual details on garments due to the blending of reference features in the aggregation process. The model also has limitations with loose or flowing garments where keypoint flow may not accurately capture fabric dynamics. The current method is specifically designed for Nr=3 reference images, and it's unclear how well it would scale to more references. Additionally, the inference-time heuristic for reference selection based on keypoint orientation may not be necessary if the model has truly learned generalizable pose-to-reference mapping.

## Limitations
- Text generation on garments is a known failure mode due to feature blending in the aggregation process
- Model may struggle with loose or flowing garments where keypoint flow doesn't capture fabric dynamics
- Method is specifically tuned for Nr=3 reference images and scalability to more references is uncertain
- Architectural hyperparameters and exact loss weights are not fully specified, creating reproducibility gaps

## Confidence
- PPA effectiveness in reducing feature blending artifacts vs. simple averaging: High
- Overall improvement in image-level metrics (SSIM, PSNR, LPIPS): High
- Video-level FVD improvements and human evaluation results: Medium
- Computational efficiency claims: Medium
- Scalability to more than 3 reference images: Low

## Next Checks
1. Test PPA with Nr=4 or Nr=5 reference images to assess scalability and aggregation quality degradation
2. Evaluate model's generalization on videos with loose, flowing garments where keypoint flow may not capture fabric motion accurately
3. Conduct ablation study isolating impact of pose similarity metric (e.g., replace dot-product+softmax with cosine similarity or learned metric)