---
ver: rpa2
title: 'DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos'
arxiv_id: '2512.14217'
source_url: https://arxiv.org/abs/2512.14217
tags:
- depth
- video
- trajectory
- object
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DRAW2ACT introduces a depth-aware trajectory-conditioned video\
  \ generation framework for robotic manipulation. The method encodes multiple complementary\
  \ trajectory representations\u2014including depth-aware 3D trajectories, high-level\
  \ DINOv2 object features, and pixel-augmented text prompts\u2014into a diffusion\
  \ transformer to enable precise control over robot-object interactions."
---

# DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos

## Quick Facts
- arXiv ID: 2512.14217
- Source URL: https://arxiv.org/abs/2512.14217
- Reference count: 40
- Key outcome: 65.2% task success rate in simulation, outperforming baselines on video quality and trajectory accuracy

## Executive Summary
DRAW2ACT introduces a depth-aware trajectory-conditioned video generation framework for robotic manipulation. The method encodes multiple complementary trajectory representations—including depth-aware 3D trajectories, high-level DINOv2 object features, and pixel-augmented text prompts—into a diffusion transformer to enable precise control over robot-object interactions. The framework jointly generates spatially aligned RGB and depth videos, leveraging cross-modality attention and depth supervision to enhance spatio-temporal consistency. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks demonstrate superior visual fidelity, trajectory accuracy, and task success rates compared to existing baselines.

## Method Summary
DRAW2ACT conditions video generation on 3D trajectories extracted from object masks and depth maps, using three complementary representations: a depth-encoded reference frame, DINOv2 object features propagated along the trajectory, and text prompts augmented with pixel coordinates. These are injected into a diffusion transformer (CogVideoX-Fun-5B) via gated fusion blocks. RGB and depth videos are jointly generated by concatenating their latent sequences temporally and applying shared self-attention. A multimodal policy model regresses robot joint angles from the generated videos. The approach is trained on robotic manipulation datasets and evaluated on video quality, trajectory adherence, and task success in simulation.

## Key Results
- Achieves up to 65.2% task success rate in simulated manipulation tasks
- Outperforms baselines on trajectory accuracy (25.30 pixel error vs. 39.88 for first-frame-only)
- Demonstrates superior video quality via VBench metrics including Motion Smoothness and Subject Consistency

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Trajectory Representation Fusion
The framework encodes trajectories through multiple complementary channels—depth-coded visual reference, semantic features, and coordinate-augmented text—to capture depth, motion, object semantics, and shape. This multi-representation approach improves trajectory adherence compared to single-modality conditioning by ensuring no single signal type dominates.

### Mechanism 2: Object-Centric Semantic Feature Propagation with Gated Injection
DINOv2 features are extracted from the object region, propagated along the trajectory, and injected via learned sigmoid gates. This preserves object identity and reduces disappearance/deformation artifacts by selectively modulating relevant features per layer.

### Mechanism 3: RGB-Depth Cross-Modal Co-Generation
Joint generation of RGB and depth videos via shared self-attention across temporally concatenated sequences leverages depth supervision to enhance spatio-temporal consistency and geometric accuracy.

## Foundational Learning

- **Latent Diffusion Models (LDMs) and 3D Causal VAEs**: The method operates on compressed latent representations; understanding VAE compression and diffusion denoising is prerequisite to following control injection strategy.
  - Quick check: Can you explain why the VAE outputs 16 channels and compresses the temporal dimension from N frames to n frames?

- **Diffusion Transformer (DiT) Architectures**: The denoising network is implemented as a DiT; control signals are injected via attention mechanisms and residual fusion blocks.
  - Quick check: How does a DiT differ from a U-Net diffusion backbone in terms of where spatial conditioning can be injected?

- **Cross-Attention vs. Self-Attention for Multimodal Fusion**: Text conditioning uses cross-attention while RGB-depth fusion uses self-attention across concatenated sequences.
  - Quick check: Why would you use self-attention for RGB-depth fusion but cross-attention for text conditioning?

## Architecture Onboarding

- **Component map**: Grounded-SAM + TrackAnything → object masks → 2D trajectory → Video Depth Anything → depth values → 3D trajectory → VAE → z_ref_0 (reference frame); Object crop → DINOv2 → y_dino → DiT backbone → denoised latents → VAE decoder → RGB video V and depth video V_depth

- **Critical path**: Trajectory extraction quality determines downstream conditioning; DINOv2 feature propagation alignment affects object consistency; RGB-depth temporal alignment during concatenation prevents sync issues.

- **Design tradeoffs**: Temporal concatenation (vs. channel) avoids projection layers but limits per-modality specialization; relative depth simplifies interaction but may limit physical plausibility; single-object limitation noted for future work.

- **Failure signatures**: Object disappearance indicates DINOv2 propagation issues; trajectory deviation >30 pixels suggests depth encoding misalignment; gripper-object misalignment points to RGB-depth sync problems.

- **First 3 experiments**:
  1. Validate trajectory extraction pipeline by computing mask IoU against ground truth on 50 held-out frames
  2. Ablate DINOv2 gating by setting G=1 and comparing trajectory error
  3. Sweep depth supervision weight λ and measure LPIPS/SSIM to find quality plateau point

## Open Questions the Paper Calls Out
- **Multi-object manipulation extension**: Currently supports one object only; future work should address generation conditioned on multiple trajectories
- **Long-horizon controllability**: Model does not currently support long-horizon tasks; temporal architecture needs modification for consistency
- **Robustness to input noise**: Performance under noisy segmentation or depth estimation not analyzed; sensitivity to upstream failures untested

## Limitations
- Single-object manipulation only; multi-object support requires architectural changes
- Performance on out-of-distribution objects/environments untested
- DINOv2 gating mechanism contribution not fully validated through ablation

## Confidence
- **High Confidence**: RGB-depth co-generation mechanism and trajectory error measurements
- **Medium Confidence**: Orthogonal trajectory representation fusion hypothesis
- **Low Confidence**: DINOv2 gating mechanism necessity

## Next Checks
1. Evaluate DRAW2ACT on datasets with different object types, backgrounds, and camera viewpoints not seen during training
2. Replace learned sigmoid gates with fixed values (e.g., G=1) to isolate gating contribution
3. Compare Video Depth Anything outputs against LiDAR/structured-light depth sensors to quantify depth estimation errors and their impact on trajectory encoding accuracy