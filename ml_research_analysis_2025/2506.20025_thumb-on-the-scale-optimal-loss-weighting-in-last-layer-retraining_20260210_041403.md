---
ver: rpa2
title: 'Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining'
arxiv_id: '2506.20025'
source_url: https://arxiv.org/abs/2506.20025
tags:
- weighting
- class
- loss
- error
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes optimal loss weighting in last layer retraining\
  \ for imbalanced binary classification. The authors develop a theoretical framework\
  \ showing that loss weighting effectiveness depends on the overparameterization\
  \ ratio \u03B4=d/n, where d is latent dimension and n is sample size."
---

# Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining

## Quick Facts
- **arXiv ID:** 2506.20025
- **Source URL:** https://arxiv.org/abs/2506.20025
- **Reference count:** 40
- **Primary result:** Optimal loss weighting depends on overparameterization ratio δ=d/n; using effective latent dimension to compute weighting outperforms classical ratio of priors for balancing worst-class error in last layer retraining.

## Executive Summary
This paper analyzes optimal loss weighting for last layer retraining in imbalanced binary classification. The authors show that the effectiveness of loss weighting depends critically on the overparameterization ratio δ = d/n, where d is the effective latent dimension and n is the sample size. For square loss in the underparameterized regime (δ < 1), they derive a closed-form optimal weighting scheme ρ̃ that balances worst-class error, which outperforms the classical ratio of priors except when class separation is large. Experiments on CelebA and CIFAR10 datasets confirm that using the effective latent dimension (instead of full dimension) to compute ρ̃ accurately predicts the empirically optimal weighting and achieves better balanced accuracy than using the ratio of priors.

## Method Summary
The method involves fine-tuning a pretrained backbone on the target dataset, extracting latent features, and then retraining only the final linear layer with weighted square loss. The key innovation is computing the optimal weighting ρ̃ = (π₋/π₊) + [(π₋/π₊) - 1]·δ/(2π₊ - δ), where δ = d_eff/n and d_eff is the number of latent dimensions capturing 99% of variance via PCA. This weighting scheme corrects for finite-sample estimation error and places the decision boundary at the true midpoint between class distributions, balancing per-class error rates. The method is validated through synthetic experiments and real-world datasets (CelebA and CIFAR10) with artificially induced class imbalance.

## Key Results
- The optimal minority-class weight ρ̃ depends on the overparameterization ratio δ = d/n, not just class priors, adding a correction term that grows with δ
- Using effective latent dimension (PCA-based) rather than full dimension accurately predicts empirical optimal weight and achieves better balanced accuracy than ratio of priors
- For δ < 2π₊, ρ̃ computed from effective dimension tracks the empirical error crossover better than the classical ratio of priors
- The square loss assumption enables closed-form solutions, though the authors note it often matches cross-entropy performance in low-data regimes

## Why This Works (Mechanism)

### Mechanism 1
The optimal minority-class weight ρ̃ depends on the overparameterization ratio δ = d/n, not just class priors. The weighting scheme corrects for finite-sample estimation error: as δ increases (fewer samples per parameter), the decision boundary increasingly favors the majority class due to variance in the estimator. The optimal weight compensates by applying stronger minority-class weighting: ρ̃ = (π₋/π₊) + [(π₋/π₊) - 1]·δ/(2π₊ - δ). This formula adds a correction term to the classical ratio of priors that grows with δ.

### Mechanism 2
Setting the bias term b = 0 via ρ̃ equalizes per-class error rates. In unweighted ERM under class imbalance, the learned bias b* < 0 shifts the decision boundary toward the minority class, causing R₊ > R₋. By selecting ρ̃ that forces b* = 0, the classifier places the decision boundary at the true midpoint between class distributions, balancing the Q-function arguments for both classes and minimizing worst-class error.

### Mechanism 3
Using the effective latent dimension rather than the full dimension accurately predicts the empirical optimal weight. Pretrained backbones produce latent representations where most dimensions carry negligible variance. Computing δ using the full dimension underestimates the true overparameterization experienced during learning. By counting only dimensions capturing 99% of variance (3 for CelebA, 2-9 for CIFAR10 variants), the computed δ better reflects the actual learning dynamics and yields ρ̃ predictions matching empirical optima.

## Foundational Learning

- **Overparameterization ratio δ = d/n**
  - Why needed here: This ratio, not absolute model size, determines weighting effectiveness and optimal weight magnitude.
  - Quick check question: If you double both the latent dimension and the number of retraining samples, does δ change?

- **Worst-class error (WCE) as fairness metric**
  - Why needed here: The optimization target is minimizing max{R₊, R₋}, not overall accuracy; this drives the balanced-error solution.
  - Quick check question: Why is WCE preferred over average error for imbalanced classification?

- **Moreau envelope and proximal operators**
  - Why needed here: The theoretical reduction from (d+1)-dimensional optimization to four scalar equations relies on expressing expectations of convex losses via Moreau envelopes.
  - Quick check question: What property of the Moreau envelope allows it to simplify the CGMT auxiliary optimization?

## Architecture Onboarding

- **Component map:**
  Pretrained Backbone (frozen) -> Latent Features (d dimensions) -> Final Linear Layer (θ ∈ ℝᵈ, b ∈ ℝ) -> Weighted Square Loss -> Retrained Parameters

- **Critical path:**
  1. Extract latent features X ∈ ℝⁿˣᵈ from frozen backbone
  2. Compute effective dimension via PCA (99% variance threshold)
  3. Calculate δ = d_eff / n and derive ρ̃ using Theorem 2 formula
  4. Train final layer with weighted square loss: (1/n) Σᵢ ωᵢ(yᵢ(xᵢᵀθ + b) - 1)²

- **Design tradeoffs:**
  - Square loss vs. cross-entropy: Theory requires square loss for closed-form; authors note square loss often matches CE in low-data settings
  - PCA threshold choice: 99% variance is heuristic; tighter thresholds lower d_eff, increasing δ and thus ρ̃
  - Downsampling vs. weighting: Downsampling reduces n, increasing effective δ, which raises error for both classes; weighting preserves samples

- **Failure signatures:**
  - **δ ≥ 2π₊:** Per-class errors never cross; weighting cannot balance. Remedy: Increase n or reduce d_eff; consider ρ → ∞
  - **Large class separation (high s):** ρ̃ overcorrects; unweighted may outperform. Check if s² exceeds threshold in Equation 52
  - **Non-Gaussian latents:** ρ̃ prediction shifts from empirical optimum. Validate by sweeping ρ empirically

- **First 3 experiments:**
  1. Replicate synthetic validation: Generate class-conditional Gaussian data (d=100, n=500, π₊=0.2, s=1), sweep ρ ∈ [1, 20], confirm ρ̃ matches error crossover
  2. Effective dimension ablation: On CelebA with n=20, compare ρ̃ computed using (a) full dimension, (b) 99%-variance PCA dimension, (c) 95%-variance PCA dimension
  3. Break condition test: Create synthetic data with δ = 0.5, π₊ = 0.1; verify per-class errors never intersect as ρ increases

## Open Questions the Paper Calls Out

### Open Question 1
How can the effective latent dimension be estimated in a principled manner rather than relying on heuristic PCA-based variance thresholds? The paper uses PCA with a 99% variance threshold as an ad-hoc solution, but this choice is not theoretically motivated and may not generalize across architectures or tasks.

### Open Question 2
Can closed-form optimal weighting schemes similar to ρ̃ be derived for other convex losses (e.g., cross-entropy, logistic loss) in the underparameterized regime? The Moreau envelope for other losses does not admit closed-form expressions, making it unclear whether similar interpretable weighting formulas exist.

### Open Question 3
How does the optimal weighting scheme extend to class-conditional Gaussian models with non-isotropic covariance structures? The analysis is limited to isotropic noise, but real latent representations may exhibit anisotropic structure.

## Limitations

- The theoretical framework assumes Gaussian class-conditional distributions with isotropic noise, which may not hold for real-world latent representations from deep networks
- The PCA-based effective dimension calculation is heuristic and may poorly capture task-relevant subspaces when discriminative information lies in low-variance directions
- The square loss assumption, while theoretically convenient, may not perfectly align with cross-entropy performance in all regimes

## Confidence

- **High confidence:** The mathematical derivation of ρ̃ and its dependence on δ = d_eff/n is rigorous and well-validated by synthetic experiments
- **Medium confidence:** The empirical validation on CelebA and CIFAR10 confirms the theoretical predictions, but the dataset diversity is limited and the PCA-based effective dimension is heuristic
- **Low confidence:** The practical applicability of the exact formula ρ̃ in non-Gaussian or highly anisotropic latent spaces remains unproven

## Next Checks

1. Test ρ̃ prediction accuracy on datasets with known non-Gaussian latent structures (e.g., MNIST with manifold structure)
2. Compare PCA-based effective dimension with alternative dimensionality reduction methods (e.g., diffusion maps, UMAP) for computing ρ̃
3. Validate the framework's predictions under different loss functions (e.g., logistic loss) through extensive synthetic experiments