---
ver: rpa2
title: Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning
arxiv_id: '2508.19202'
source_url: https://arxiv.org/abs/2508.19202
tags:
- reasoning
- knowledge
- scientific
- llms
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating and improving
  large language models' scientific reasoning abilities, which require both deep domain
  knowledge and complex multi-step reasoning. The authors introduce SCIREAS, a unified
  suite of ten scientific benchmarks, and SCIREAS-PRO, a reasoning-focused subset
  that better distinguishes model capabilities.
---

# Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning

## Quick Facts
- arXiv ID: 2508.19202
- Source URL: https://arxiv.org/abs/2508.19202
- Authors: Alan Li; Yixin Liu; Arpan Sarkar; Doug Downey; Arman Cohan
- Reference count: 40
- Primary result: Base models given high-quality in-context knowledge ingredients can outperform reasoning-enhanced models by ≥10%, suggesting knowledge retrieval is the primary bottleneck for scientific reasoning.

## Executive Summary
This paper addresses the challenge of evaluating and improving large language models' scientific reasoning abilities, which require both deep domain knowledge and complex multi-step reasoning. The authors introduce SCIREAS, a unified suite of ten scientific benchmarks, and SCIREAS-PRO, a reasoning-focused subset that better distinguishes model capabilities. They propose KRUX, a framework that extracts knowledge ingredients from reasoning traces to study how knowledge and reasoning interact in scientific problem-solving. Through controlled experiments, they find that (1) retrieving relevant knowledge is a key bottleneck, with base models outperforming reasoning models once provided with high-quality in-context knowledge, (2) reasoning models benefit from external knowledge beyond reasoning enhancement, and (3) reasoning fine-tuning improves models' ability to surface relevant knowledge. Their findings are supported by post-training experiments showing their method is competitive with concurrent approaches.

## Method Summary
The authors develop SCIREAS as a unified evaluation harness for 10 scientific benchmarks (GPQA, MMLU-Pro, LabBench, etc.) with standardized prompts and scoring. They create SCIREAS-PRO by filtering for reasoning-intensive instances where performance gaps between low and high reasoning effort settings are largest. KRUX is a framework that extracts knowledge ingredients (KIs) from reasoning traces using an LLM extractor, then augments questions with these KIs for evaluation. They conduct controlled fine-tuning experiments on Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct, creating reasoning-enhanced variants by training on DeepSeek-R1 traces from synthetic datasets (SYNTHETIC-1-Math and SYNTHETIC-1-STEM). The framework disentangles knowledge retrieval from reasoning ability by comparing base and reasoning-enhanced models with and without in-context knowledge.

## Key Results
- Base models with high-quality KIs from strong reasoners outperform reasoning models without KIs by ≥10% across benchmarks
- Reasoning models consistently benefit from external in-context knowledge beyond their reasoning enhancement
- Reasoning fine-tuning improves a model's ability to surface relevant parametric knowledge during CoT generation
- KI extraction is robust to extractor choice (DeepSeek-R1 vs. Qwen3-30B-A3B-Thinking)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge retrieval from parametric memory is a primary bottleneck for scientific reasoning, not reasoning capacity itself.
- Mechanism: Base models given high-quality in-context "knowledge ingredients" (KIs) extracted from strong reasoners can outperform reasoning-enhanced models by ≥10%, suggesting the limiting factor is accessing/surfacing knowledge rather than applying it.
- Core assumption: KIs faithfully represent the knowledge needed without leaking reasoning steps or answers.
- Evidence anchors:
  - [abstract] "Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning"
  - [section 4.2.2, Table 2] Base models with R1 KIs outperform reasoning variants without KIs by ≥10% across benchmarks
  - [corpus] EAIRA (arXiv:2502.20309) similarly identifies reasoning and problem-solving capabilities as key evaluation dimensions for scientific assistants

### Mechanism 2
- Claim: Reasoning fine-tuning improves a model's ability to surface relevant parametric knowledge during CoT generation.
- Mechanism: -Math variants (fine-tuned only on math reasoning, no science data) generate KIs that boost base models more than the base's own KIs, despite both models showing similar parametric knowledge when directly probed—suggesting reasoning training improves knowledge recall organization rather than adding new facts.
- Core assumption: Synthetic knowledge probing questions accurately measure what models "know" parametrically.
- Evidence anchors:
  - [section 4.2.4] "KIs from -Math deliver significant boosts over those from the base models...the KIs are unlikely to have been newly acquired during fine-tuning"
  - [Table 4] Base and -Math models show similar knowledge recall (~72-82%) on KI-GPQA and KI-MMLU-Pro probing tasks

### Mechanism 3
- Claim: Reasoning models can leverage external in-context knowledge for additional gains beyond their reasoning enhancement.
- Mechanism: When both base and reasoning-enhanced models receive the same KIs from a strong reasoner, reasoning models consistently outperform base models, showing complementary benefits from reasoning training + external knowledge access.
- Core assumption: External KIs simulate realistic knowledge-augmented scenarios and don't artificially narrow the problem space.
- Evidence anchors:
  - [abstract] "Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement"
  - [section 4.2.3, Table 3] Reasoning variants w/ R1 KIs outperform base models w/ R1 KIs across GPQA, MMLU-Pro*, and LabBench*

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper studies how long CoT traces (enclosed in markers) interact with knowledge recall; understanding CoT structure is essential for interpreting KRUX results.
  - Quick check question: Can you explain why providing reasoning steps before an answer might change what knowledge a model surfaces?

- Concept: **Supervised Fine-tuning (SFT) for Reasoning Distillation**
  - Why needed here: The controlled experiments use SFT on DeepSeek-R1 traces to create reasoning-enhanced variants; understanding distillation helps interpret whether behavior changes come from training data or methodology.
  - Quick check question: What's the difference between training a model with CoT traces vs. training on final answers only?

- Concept: **Parametric vs. In-Context Knowledge**
  - Why needed here: KRUX explicitly separates knowledge stored in parameters from knowledge provided in-context; this distinction underpins all three main findings.
  - Quick check question: If you provide a fact in the prompt that contradicts what a model learned during pretraining, which source typically dominates the model's response?

## Architecture Onboarding

- Component map:
  SCIREAS (10 benchmarks) -> SCIREAS-PRO (reasoning-intensive subset) -> KRUX Pipeline (reasoning traces -> KIs -> augmented evaluation)

- Critical path:
  1. Run baseline evaluation on SCIREAS/SCIREAS-PRO
  2. Extract KIs from reasoning traces using the standardized prompt (Figure 12)
  3. Validate KI quality (task-agnostic, answer-agnostic, faithful to trace)
  4. Evaluate target models with KI-augmented prompts
  5. Compare base vs. reasoning-enhanced models with/without external KIs

- Design tradeoffs:
  - KI extraction depends on strong extractor model (DeepSeek-R1); weaker extractors may produce less generalizable KIs
  - SCIREAS-PRO filtering uses OpenAI reasoning-effort flags as proxies—proprietary factors may confound
  - Controlled SFT uses models <10B parameters; findings may not scale to frontier models

- Failure signatures:
  - KIs that reference specific question details or options indicate extraction prompt failure
  - No performance difference between w/ self KIs and w/ R1 KIs suggests KIs aren't providing useful knowledge
  - Base model outperforming reasoning model even with identical KIs may indicate reasoning training degraded parametric knowledge

- First 3 experiments:
  1. **Reproduce RQ1 baseline**: Evaluate Qwen2.5-7B-Instruct on GPQA with/without KIs extracted from DeepSeek-R1; confirm base + KIs outperforms reasoning-only variants
  2. **Validate KI extraction quality**: Manual review of 50 extracted KIs for task-agnosticity and answer-agnosticity using criteria in Appendix F.1
  3. **Test alternative extractors**: Compare KI quality when using Qwen3-30B-A3B-Thinking vs. DeepSeek-R1 as extractor (replicating Appendix F.2)

## Open Questions the Paper Calls Out
None

## Limitations
- KRUX framework's conclusions depend critically on KI extraction quality; if DeepSeek-R1 fails to extract genuinely relevant knowledge or includes reasoning scaffolding, observed gains could conflate knowledge access with implicit reasoning support
- Controlled fine-tuning experiments on <10B parameter models may not generalize to frontier reasoning models
- SCIREAS-PRO filtering using proprietary OpenAI reasoning-effort flags introduces potential confounders from factors outside authors' control

## Confidence
- **High confidence**: Base models with high-quality KIs outperforming reasoning models by ≥10% (directly measured across multiple benchmarks)
- **Medium confidence**: Reasoning fine-tuning improves knowledge recall organization (supported by synthetic probing but lacks independent validation)
- **Medium confidence**: Reasoning models benefit from external knowledge beyond reasoning enhancement (observed consistently but KI quality remains a concern)

## Next Checks
1. Replicate the core finding using an alternative KI extraction method (e.g., supervised extraction from human-annotated knowledge snippets) to test whether gains depend on the specific extraction approach
2. Test whether KI-augmented evaluation is robust to prompt ordering by running 5 random permutations of KIs per question and reporting mean ± std deviation
3. Validate the SCIREAS-PRO filtering by independently evaluating a subset of questions with human annotations of reasoning effort to confirm the correlation with OpenAI's proprietary flags