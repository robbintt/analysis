---
ver: rpa2
title: Federated Retrieval Augmented Generation for Multi-Product Question Answering
arxiv_id: '2501.14998'
source_url: https://arxiv.org/abs/2501.14998
tags:
- search
- domain
- retrieval
- product
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MKP-QA, a multi-product knowledge-augmented
  question-answering framework that uses probabilistic federated search to address
  the challenges of multi-domain product QA. Existing methods either query all domains
  indiscriminately (causing computational inefficiency and LLM hallucinations) or
  use rigid resource selection (limiting search scope).
---

# Federated Retrieval Augmented Generation for Multi-Product Question Answering

## Quick Facts
- **arXiv ID**: 2501.14998
- **Source URL**: https://arxiv.org/abs/2501.14998
- **Reference count**: 4
- **Primary result**: MKP-QA significantly improves retrieval accuracy and response quality for multi-product question answering using probabilistic federated search

## Executive Summary
This paper introduces MKP-QA, a multi-product knowledge-augmented question-answering framework that addresses the challenges of multi-domain product QA through probabilistic federated search. The framework uses a query-domain router to estimate relevance scores, a stochastic gating mechanism to balance exploration and exploitation of domains, and a bi-encoder retriever for document retrieval. Experiments on three Adobe product datasets demonstrate that MKP-QA significantly improves retrieval accuracy and response quality compared to baselines, particularly in cross-domain settings.

## Method Summary
MKP-QA employs a multi-step pipeline where queries are first routed through a BERT-based domain classifier to estimate domain relevance probabilities. A stochastic gating mechanism with adaptive entropy-based thresholds determines which domains to search, balancing exploration of uncertain domains with exploitation of high-confidence ones. A bi-encoder retriever using Sentence-BERT and symmetric InfoNCE loss retrieves relevant documents from the selected domains. The framework aggregates query-domain and query-document relevance scores to create unified document rankings, which are then fed to an LLM generator for response synthesis.

## Key Results
- MKP-QA achieves significant improvements in retrieval accuracy (Acc@Top1) compared to baselines
- Response quality metrics (relevancy and faithfulness) show substantial gains, especially for cross-domain queries
- The framework introduces new benchmark datasets for multi-product QA evaluation
- Performance improvements are particularly notable when handling queries requiring information from multiple product domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating query-domain and query-document relevance scores improves multi-domain retrieval accuracy compared to querying all domains indiscriminately or using rigid domain selection.
- Mechanism: The unified scoring function U(j,q,d) = p_j · s(q,d) multiplies the domain relevance likelihood with document similarity score, creating domain-aware document rankings.
- Core assumption: Query-domain relevance and query-document similarity provide complementary signals that better approximate true relevance when multiplied together.
- Evidence anchors: [abstract] "This method enhances multi-domain search quality by aggregating query-domain and query-passage probabilistic relevance."
- Break condition: If domain router predictions are systematically miscalibrated, the multiplicative scoring will amplify errors rather than mitigate them.

### Mechanism 2
- Claim: Stochastic gating with entropy-adaptive thresholds enables cross-domain query handling by balancing exploitation of high-confidence domains with exploration of potentially relevant but less certain domains.
- Mechanism: The adaptive threshold τ(q) = τ₀(1 - H(p)/log(m)) decreases when domain probability distribution has high entropy, making the Bernoulli gate more likely to activate additional domains.
- Core assumption: Query-domain router entropy correlates with genuine multi-domain relevance; high entropy indicates cross-domain queries rather than poor router performance.
- Evidence anchors: [Section 3.2] "To address the challenge of balancing exploitation of high-confidence domains with exploration of potentially relevant but less certain domains, we introduce a stochastic gating mechanism with adaptive threshold control."
- Break condition: If router entropy is high primarily due to poor training rather than genuine cross-domain queries, the mechanism will over-explore and retrieve noisy documents.

### Mechanism 3
- Claim: Symmetric contrastive learning (InfoNCE variant) for bi-encoder retriever improves multi-domain retrieval by learning domain-agnostic query-document matching.
- Mechanism: The retriever loss L_r = -(L_q2d + L_d2q)/2 trains embeddings to rank positive pairs higher than in-batch negatives bidirectionally.
- Core assumption: Domain-specific terminology can be handled through shared embedding space fine-tuning without requiring domain-specific retrievers.
- Evidence anchors: [Section 3.3] "We fine-tune the retriever model on our multi-domain dataset using a contrastive learning approach with a symmetric supervised variant of the InfoNCE loss"
- Break condition: If negative sampling strategy does not include sufficient hard cross-domain negatives, the retriever may learn superficial domain-specific shortcuts.

## Foundational Learning

- Concept: Federated Search
  - Why needed here: MKP-QA's core innovation is adapting federated search to RAG pipelines. Understanding that federated search involves resource selection + result merging is prerequisite to grasping why the paper combines domain routing with document retrieval.
  - Quick check question: In federated search, what two sub-problems must be solved? (Answer: Which sources to query / resource selection, and how to merge ranked results from different sources)

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: The bi-encoder retriever uses symmetric InfoNCE loss. You need to understand that contrastive learning pulls positive pairs together and pushes negative pairs apart in embedding space.
  - Quick check question: In InfoNCE loss, what happens when a negative sample has high similarity to the query? (Answer: The loss increases, and gradients push the query and negative embeddings apart)

- Concept: Exploration-Exploitation Trade-off
  - Why needed here: The stochastic gating mechanism explicitly frames domain selection as an exploration-exploitation problem, borrowing from reinforcement learning.
  - Quick check question: Why might pure exploitation (always selecting highest-probability domain) fail for cross-product queries? (Answer: Cross-product queries may require information from domains the router isn't confident about; pure exploitation would miss these)

## Architecture Onboarding

- Component map: Query → Router (domain probs) → Stochastic Gate (active domains) → Retriever (parallel search in active domains) → Aggregator (unified ranking) → LLM (generation)

- Critical path: Query → Router (domain probs) → Stochastic Gate (active domains) → Retriever (parallel search in active domains) → Aggregator (unified ranking) → LLM (generation)

- Design tradeoffs:
  - τ₀ base threshold: Higher = fewer domains searched (faster but may miss cross-domain info); lower = more domains (slower, more noise)
  - Top-k per domain: Larger k improves recall but increases LLM context length and latency
  - Unified vs. separate domain indexes: Unified simplifies deployment but requires domain tagging; separate indexes allow domain-specific tuning but increase operational complexity
  - Stochastic vs. deterministic gating: Stochastic enables exploration but introduces non-determinism in outputs; deterministic is reproducible but may miss edge cases

- Failure signatures:
  - Retrieval Acc@Top1 low but Faithfulness high: Router is selecting wrong domains but retriever finds relevant docs within selected domains → check router calibration
  - High retrieval accuracy but low response quality: Retrieved documents are relevant but LLM fails to synthesize → check prompt engineering or LLM capability
  - Cross-domain queries underperforming uni-domain: Stochastic gate threshold too high or entropy scaling not working → visualize τ(q) distribution
  - Latency exceeds 10s target: Check if too many domains activated per query; profile parallel retrieval efficiency

- First 3 experiments:
  1. Ablate stochastic gating: Replace Bernoulli sampling with deterministic top-N domain selection (compare τ₀ variants)
  2. Router calibration analysis: Plot router confidence (max probability) vs. actual domain correctness
  3. Cross-domain negative sampling impact: Train retriever with hard negatives from other domains vs. random negatives

## Open Questions the Paper Calls Out

- Can the multi-step MKP-QA framework be optimized to meet strict low-latency requirements necessary for real-time production deployment?
- How does the stochastic gating mechanism's exploration-exploitation balance perform as the number of product domains scales significantly beyond the three tested?
- To what extent do the automated LLM-based evaluation metrics (relevancy and faithfulness) correlate with actual human user satisfaction in a live setting?

## Limitations

- Router calibration uncertainty: No validation that the query-domain router is well-calibrated, which is critical for the stochastic gating mechanism
- Cross-domain negative sampling underspecification: The retriever training strategy for cross-domain queries is not fully detailed
- Dataset access limitations: The new multi-product QA benchmark datasets are not yet public, preventing independent verification

## Confidence

- High confidence: Retrieval accuracy improvements over baselines (Acc@Top1 gains are well-measured and significant)
- Medium confidence: Response quality improvements (relevancy and faithfulness scores rely on GPT-4 evaluation, which may not perfectly align with human judgment)
- Low confidence: Stochastic gating mechanism effectiveness (adaptive threshold and entropy scaling are theoretically sound but lack empirical ablation studies)

## Next Checks

1. Ablation study: Compare MKP-QA against deterministic domain selection to isolate the contribution of stochastic gating and adaptive thresholding
2. Router calibration analysis: Plot router confidence (max probability) vs. actual domain correctness across validation queries to detect systematic miscalibration
3. Cross-domain retrieval gap analysis: Measure and compare retrieval performance for cross-domain vs. single-domain queries to identify if the stochastic gate threshold is appropriately calibrated