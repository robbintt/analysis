---
ver: rpa2
title: A new initialisation to Control Gradients in Sinusoidal Neural network
arxiv_id: '2512.06427'
source_url: https://arxiv.org/abs/2512.06427
tags:
- initialization
- siren
- neural
- network
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new initialization scheme for sinusoidal
  neural networks (SIREN) that stabilizes gradient propagation in deep architectures.
  The key innovation is an explicit derivation of weight and bias initialization parameters
  based on fixed points of pre-activation and Jacobian distributions in the infinite-width
  limit.
---

# A new initialisation to Control Gradients in Sinusoidal Neural network

## Quick Facts
- arXiv ID: 2512.06427
- Source URL: https://arxiv.org/abs/2512.06427
- Authors: Andrea Combette; Antoine Venaille; Nelly Pustelnik
- Reference count: 40
- This paper proposes a new initialization scheme for sinusoidal neural networks (SIREN) that stabilizes gradient propagation in deep architectures.

## Executive Summary
This paper introduces a theoretically-grounded initialization scheme for sinusoidal neural networks (SIRENs) that addresses the fundamental instability problem in deep architectures. By deriving explicit weight and bias initialization parameters from fixed points of pre-activation and Jacobian distributions in the infinite-width limit, the method controls gradient scaling through unit variance in layer-to-layer Jacobians (σg = 1) and optionally constrains the pre-activation fixed point to zero (σa = 0). This approach prevents gradient explosion/vanishing and suppresses spurious high-frequency artifacts, achieving superior performance across function fitting, image reconstruction, audio denoising, video reconstruction, and physics-informed neural networks.

## Method Summary
The proposed initialization scheme for SIRENs uses closed-form parameters derived from theoretical analysis of infinite-width networks. For the first layer, weights are initialized with uniform distribution over [-ω₀/n₀, ω₀/n₀] where ω₀ controls the frequency scale, and biases are set to zero. For hidden layers (ℓ ≥ 2), weights follow uniform distribution over [-√3/√N, √3/√N] where N is the layer width, with zero biases. This initialization enforces unit variance in layer-to-layer Jacobians (σg = 1) and optionally constrains pre-activation variance to zero (σa = 0), which together stabilize gradient propagation and control the network's frequency spectrum. The method is implemented by overriding standard initialization in SIREN modules and training with Adam optimizer.

## Key Results
- The proposed initialization achieves significantly lower generalization error than original SIREN scheme across multiple domains while maintaining stable training dynamics even in very deep networks.
- Experimental validation shows the method prevents spurious high-frequency artifacts that degrade performance in competing approaches, with spectral energy confined below ω₀ when σa = 0.
- The initialization successfully controls gradient scaling by enforcing unit variance in layer-to-layer Jacobians, preventing exponential gradient explosion or vanishing with depth.

## Why This Works (Mechanism)

### Mechanism 1: Gradient Stability via Jacobian Variance Normalization
Enforcing unit variance in layer-to-layer Jacobians (σg = 1) prevents exponential gradient explosion or vanishing with depth. The Jacobian entries Jℓ = diag(cos(zℓ))Wℓ have variance σ²g = (c²w/6N)(1 + e^(-2σ²a)). Setting σg = 1 yields a constraint curve cb(cw) (Eq. 8). When satisfied, gradients scale as (σ²g)^(L-ℓ-1) ≈ 1 rather than exploding/decaying exponentially.

### Mechanism 2: Frequency Spectrum Control via Pre-activation Fixed Point
Setting the pre-activation fixed point σa = 0 suppresses emergence of spurious high-frequency components above the Nyquist frequency. Pre-activations zℓ ∼ N(0, σ²a) feed through sin(zℓ). When σa → 0, sin(zℓ) ≈ zℓ (near-linear), preventing spectral broadening from successive sine compositions. Theoretical spectrum shows energy confined below ω₀ when σa = 0 (Fig. 4).

### Mechanism 3: NTK Eigenvalue Scaling Determines Learning Speed
The initialization controls NTK eigenvalue scaling with depth, which governs how quickly different frequency components are learned. Mean NTK eigenvalue λ̄ ∝ Σ(σ²g)^ℓ. For σg = 1, λ̄ ∝ L (linear growth); for σg > 1, exponential explosion; for σg < 1, depth-independent plateau. Larger eigenvalues → faster convergence of associated eigenmodes (typically low frequencies).

## Foundational Learning

- **Edge of Chaos (EOC) initialization**: Why needed here: The paper frames its contribution as placing SIREN at EOC—where forward activations and backward gradients both remain stable. Understanding EOC clarifies why the (cw, cb) constraint curve matters. Quick check question: Can you explain why EOC initialization balances expressivity and trainability?

- **Neural Tangent Kernel (NTK) framework**: Why needed here: Section 4 uses NTK to explain training dynamics. The residual evolution u(t) = exp(-tKθ₀)u(0) shows how initialization determines which frequencies are learned first. Quick check question: In the NTK regime, what determines the learning speed of a given frequency component?

- **Spectral bias in neural networks**: Why needed here: The paper explicitly addresses spectral bias—low frequencies learned faster than high frequencies. SIREN's sin activations were designed to mitigate this, but depth introduces new spectral artifacts that the proposed initialization controls. Quick check question: Why do standard MLPs preferentially learn low-frequency components?

## Architecture Onboarding

- **Component map**: First layer (W₁, b₁) → Hidden layers (Wℓ, bℓ for ℓ ≥ 2) → Final linear layer
- **Critical path**: 
  1. Set ω₀ based on target signal's Nyquist frequency (ω₀ ≈ 2π × f_Nyquist recommended)
  2. Initialize with (cw, cb) = (√3, 0) for σa = 0
  3. Verify gradient norms remain stable across layers during first forward/backward pass
  4. Monitor for high-frequency artifacts in early training—these indicate spectral broadening

- **Design tradeoffs**:
  - σa = 0 vs σa = 1: σa = 0 gives better spectral control and generalization; σa = 1 preserves more nonlinearity but introduces noise in deep networks
  - Depth vs stability: Original SIREN degrades with depth; proposed method maintains performance but requires careful ω₀ tuning
  - Width vs approximation: Theoretical guarantees require large N; finite-width effects shown in Appendix C.1

- **Failure signatures**:
  - Spurious high-frequency noise in reconstructed signals → gradient explosion (σg > 1), check initialization
  - Slow training, blurred outputs → gradient vanishing (σg < 1), verify (cw, cb) values
  - Spectrum exceeds ω₀ → σa too large or depth too high without proper initialization
  - Training loss decreases but generalization error increases → spectral aliasing from σa = 1 initialization

- **First 3 experiments**:
  1. **Gradient scaling validation**: Initialize network with proposed scheme, measure ||∂xΨθ|| and layer-wise Jacobian variance across depths L ∈ {4, 8, 16, 32}. Confirm σg ≈ 1 and gradient norms remain constant (replicate Fig. 6 right panel).
  2. **Spectrum analysis**: Compute Fourier spectrum of Ψθ(x) on dense grid for different depths and initializations. Verify σa = 0 confines energy below ω₀ while Sitzmann initialization shows spectral broadening (replicate Fig. 4).
  3. **Image fitting generalization**: Train on 128×128 grid, evaluate on 512×512 grid. Compare training MSE vs generalization MSE for proposed vs Sitzmann initialization (replicate Fig. 2 methodology). Look for noise in high-resolution outputs as failure signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise theoretical mechanism by which the slow decay of pre-activation variance in the σa=0 scheme compensates for network non-linearities to prevent spectrum explosion or collapse in very deep SIRENs?
- Basis in paper: Page 8 states that the observed stability of the Fourier spectrum in the proposed initialization "appears to compensate the nonlinearities just enough to avoid both explosion and collapse... a behaviour that remains unexplained and calls for further investigation."
- Why unresolved: While the authors prove the convergence of the variance sequence, they currently lack a formal explanation for why this specific dynamic prevents the spectral broadening seen in other initialization schemes.
- What evidence would resolve it: A theoretical derivation linking the decay rate of pre-activations to the spectral radius of the network's Jacobian, or an analytical description of the energy redistribution across Fourier modes for this specific fixed point.

### Open Question 2
- Question: Can the initialization scheme be refined to achieve full dynamical isometry by constraining the distribution of singular values of the layer Jacobians?
- Basis in paper: Page 11 discusses broadening the perspective to "distribution of singular values," and Page 20 (Appendix B.1) notes that while the singular values are stable, "our initialization does not achieve full dynamical isometry, indicating that there remains room for improvement."
- Why unresolved: The current method successfully controls gradient variance but does not explicitly enforce the concentration of singular values around 1, which is known to optimize training stability.
- What evidence would resolve it: A modified closed-form initialization that constrains the weight distribution to ensure singular values concentrate near unity, demonstrated via empirical measurement of the end-to-end Jacobian spectrum.

### Open Question 3
- Question: How does the theoretical framework for this initialization extend to training dynamics governed by non-quadratic loss functions?
- Basis in paper: Page 11 states, "Although this study focuses on signal encoding with a quadratic loss, future work could extend the approach to more complex losses, including physics-informed settings."
- Why unresolved: The paper's theoretical analysis of the Neural Tangent Kernel (NTK) and training dynamics relies on the properties of the mean-squared error loss, leaving the behavior under alternative loss landscapes uncharacterized.
- What evidence would resolve it: An analysis of the NTK evolution or empirical convergence benchmarks for SIRENs using the proposed initialization on tasks requiring cross-entropy or complex physics-informed residual losses.

## Limitations
- The theoretical guarantees rely heavily on infinite-width assumptions that may not fully translate to practical finite-width implementations, particularly for the CLT-based approximations of pre-activation distributions.
- The empirical compensation mechanism that allows σa = 0 to work despite theoretical concerns about spectrum explosion is not fully explained, representing a gap in theoretical understanding.
- Limited external validation exists for the NTK-frequency relationship claims, with no corpus evidence specifically addressing spectral bias in SIREN architectures.

## Confidence

- **High confidence**: Gradient stability mechanism via Jacobian variance normalization (σg = 1), as this follows established EOC principles with direct empirical validation in Fig. 6.
- **Medium confidence**: Frequency spectrum control via σa = 0, supported by spectral analysis (Fig. 4) but with the unexplained empirical compensation mechanism.
- **Medium confidence**: NTK eigenvalue scaling determining learning speed, theoretically sound but lacking external corpus validation for SIREN-specific applications.

## Next Checks
1. **Width dependency study**: Test proposed initialization across varying widths (N = 64, 128, 256, 512) to quantify deviation from theoretical predictions and identify minimum width for stable gradient propagation.
2. **Finite-depth spectrum analysis**: Conduct detailed Fourier analysis on networks of depth L = 4, 8, 16, 32 with proposed initialization to empirically verify the theoretical spectrum bounds and identify any residual spectral broadening.
3. **Cross-domain generalization**: Apply the proposed initialization to non-image SIREN applications (e.g., audio denoising, PINNs) to validate that the theoretical framework generalizes beyond the primary experimental domain.