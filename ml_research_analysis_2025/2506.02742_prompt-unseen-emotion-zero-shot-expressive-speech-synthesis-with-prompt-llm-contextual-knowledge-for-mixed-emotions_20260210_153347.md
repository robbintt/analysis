---
ver: rpa2
title: 'Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM
  Contextual Knowledge for Mixed Emotions'
arxiv_id: '2506.02742'
source_url: https://arxiv.org/abs/2506.02742
tags:
- speech
- emotional
- emotion
- surprise
- mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Prompt-Unseen-Emotion (PUE), a zero-shot expressive
  speech synthesis approach that generates unseen emotional speech via emotion-guided
  prompt learning. PUE leverages an LLM-TTS architecture to learn emotional consistency
  between emotion-relevant prompts and speech, allowing the model to quantitatively
  capture different emotion weightings per utterance.
---

# Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions

## Quick Facts
- **arXiv ID:** 2506.02742
- **Source URL:** https://arxiv.org/abs/2506.02742
- **Reference count:** 39
- **Key outcome:** Zero-shot expressive speech synthesis for unseen and mixed emotions using emotion-guided prompt learning with LLM-TTS architecture.

## Executive Summary
Prompt-Unseen-Emotion (PUE) introduces a zero-shot approach to expressive speech synthesis that generates unseen emotional speech through emotion-guided prompt learning. The method leverages an LLM-TTS architecture to learn emotional consistency between emotion-relevant prompts and speech, enabling quantitative capture of different emotion weightings per utterance. During inference, mixed emotional speech can be generated by adjusting emotion proportions in prompts and leveraging LLM contextual knowledge. This allows flexible control over emotional styles without requiring additional training on new emotion categories.

## Method Summary
PUE employs an LLM-TTS architecture initialized from CosyVoice-300M-Instruct, training on the ESD dataset with 2 English speakers across 5 emotion categories. The model learns emotional consistency through KL divergence loss between predicted and target emotional token distributions. Prompts are formatted to specify emotion weightings (e.g., "α% happy, β% sad"), with training setting target emotions to 100 and others to 0. During inference, mixed emotions are generated by adjusting these proportions. The system uses a frozen pretrained flow-matching model and HiFi-GAN vocoder for speech synthesis, with training conducted for 1 epoch using dynamic batching on 4 GPUs.

## Key Results
- PUE outperforms baseline methods in speech intelligibility (WER) and subjective speech quality (MOS) for both single-emotion and mixed-emotion generation
- Successfully generates unseen emotions like outrage (surprise + anger), disappointment (surprise + sadness), and delight (surprise + happiness)
- Maintains emotional consistency and speech quality across different mixing ratios of emotions

## Why This Works (Mechanism)
The method works by leveraging LLM contextual knowledge to understand and generate appropriate emotional expressions based on weighted prompt descriptions. The emotion-guided prompt learning approach allows the model to capture the semantic relationships between different emotions and their combinations, enabling zero-shot generation of unseen emotion mixtures. The KL divergence loss ensures emotional token distributions align with target emotions, while the frozen flow-matching model preserves speech quality during inference.

## Foundational Learning
- **ESD dataset structure** (why needed: provides standardized emotional speech data for training and evaluation) - quick check: verify speaker IDs 0019/0013 and emotion category counts
- **CosyVoice-300M-Instruct architecture** (why needed: serves as initialization for LLM-TTS model) - quick check: confirm model component compatibility
- **KL divergence loss for emotion learning** (why needed: enables quantitative emotion weight control) - quick check: validate loss behavior across different emotion mixtures
- **Flow-matching model for speech synthesis** (why needed: generates high-quality speech from emotional embeddings) - quick check: test model inference with simple prompts
- **HiFi-GAN vocoder integration** (why needed: converts acoustic features to final speech waveform) - quick check: verify vocoder compatibility with flow-matching output
- **Mixed-emotion prompt formulation** (why needed: enables zero-shot generation of novel emotion combinations) - quick check: test prompt parsing and emotion weight application

## Architecture Onboarding

**Component map:** Text encoder -> LLM decoder -> Flow-matching model -> HiFi-GAN vocoder

**Critical path:** Prompt input → Text encoder → LLM decoder (emotion token generation) → Flow-matching model (acoustic features) → HiFi-GAN (waveform synthesis)

**Design tradeoffs:** Zero-shot learning vs. fine-tuning precision; prompt complexity vs. generation flexibility; emotional consistency vs. linguistic naturalness

**Failure signatures:** Mixed emotions not perceptible (model defaults to single emotion), high WER indicating content degradation, inconsistent emotion perception across listeners

**First experiments:**
1. Test single-emotion generation with 100% weightings to verify basic functionality
2. Generate mixed emotions with varying ratios (30/60/90%) to identify perceptibility thresholds
3. Compare WER and MOS across different mixing levels to establish quality baselines

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Missing critical training hyperparameters (optimizer, learning rate, scheduler specifics)
- Dynamic batching strategy not fully detailed
- Specific pretrained model checkpoints not explicitly provided
- Subjective evaluation procedures lack detail on listener recruitment and consistency measures

## Confidence
- **High confidence:** Overall methodology soundness, clear problem formulation, appropriate evaluation metrics
- **Medium confidence:** Training procedure reproducibility with reasonable assumptions, well-specified core technical approach
- **Low confidence:** Subjective evaluation setup detail, exact model checkpoint availability

## Next Checks
1. Verify the exact pretrained checkpoint URLs for the flow-matching model and HiFi-GAN vocoder used in CosyVoice-300M-Instruct
2. Implement a small-scale pilot study to test different mixing ratio levels (30%, 60%, 90%) for a subset of emotions
3. Conduct a pilot subjective test with a small group of listeners to validate MOS and AB test procedures before full-scale evaluation