---
ver: rpa2
title: 'XR: Cross-Modal Agents for Composed Image Retrieval'
arxiv_id: '2601.14245'
source_url: https://arxiv.org/abs/2601.14245
tags:
- retrieval
- image
- cross-modal
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XR introduces a training-free multi-agent framework for composed\
  \ image retrieval (CIR), where each query combines a reference image with textual\
  \ modifications. The framework orchestrates three specialized agents\u2014imagination\
  \ agents generate cross-modal target representations, similarity agents perform\
  \ coarse filtering via hybrid matching, and question agents verify factual consistency\
  \ for fine filtering."
---

# XR: Cross-Modal Agents for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2601.14245
- Source URL: https://arxiv.org/abs/2601.14245
- Reference count: 40
- Primary result: Training-free multi-agent framework achieves up to 38% gains over baselines on FashionIQ, CIRR, and CIRCO

## Executive Summary
XR introduces a training-free multi-agent framework for composed image retrieval (CIR), where each query combines a reference image with textual modifications. The framework orchestrates three specialized agents—imagination agents generate cross-modal target representations, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency for fine filtering. By progressively coordinating these agents, XR achieves up to 38% gains over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO benchmarks, demonstrating robust multimodal reasoning. Each module is essential, with ablation studies confirming their individual contributions to retrieval accuracy. The approach highlights the necessity of explicit cross-modality and multi-perspective verification in addressing edit-sensitive retrieval challenges.

## Method Summary
XR employs a three-stage pipeline for composed image retrieval. First, imagination agents (text and vision) generate target representations by predicting what the edited image should look like, producing captions from both perspectives. Second, coarse filtering uses CLIP-based similarity scoring across both modalities, with Reciprocal Rank Fusion (RRF) aggregating results to select top candidates. Third, fine filtering deploys question agents that generate verification questions from the modification text, with question-based scoring agents evaluating candidates against these predicates and re-ranking the results. The entire framework is training-free, relying on off-the-shelf MLLMs and CLIP models.

## Key Results
- Achieves up to 38% gains over training-free and training-based baselines on FashionIQ, CIRR, and CIRCO benchmarks
- Each agent component is essential, with ablation studies confirming individual contributions to retrieval accuracy
- Demonstrates robust multimodal reasoning through progressive coordination of imagination, similarity, and verification agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing an explicit "imagined" target representation bridges the modality gap between a reference image and text modification, serving as a more reliable anchor than raw embeddings.
- Mechanism: The system employs two imagination agents: a text agent ($A^i_t$) that generates a target caption conditioned on the reference caption and modification text, and a vision agent ($A^i_v$) that generates a caption conditioned on the reference image and modification text. These outputs ($C_t, C_v$) approximate the ideal target image from semantic and visual perspectives respectively.
- Core assumption: The Multimodal Large Language Model (MLLM) can accurately predict the visual outcome of a text edit (e.g., "change the color to red") without seeing the target.
- Evidence anchors:
  - [abstract]: "...imagination agents synthesize target representations through cross-modal generation..."
  - [section 3.3]: "...agents $A^i_t$ and $A^i_v$ are designed to imagine and approximate the ideal target image... combining both perspectives enables the model to adapt flexibly."
  - [corpus]: Related work (e.g., "Multimodal Reasoning Agent") supports the use of reasoning to generate synthetic intermediate targets for zero-shot retrieval.
- Break condition: If the modification text is highly abstract or subjective (e.g., "make it look happier"), the imagination agents may hallucinate details not present in the actual target.

### Mechanism 2
- Claim: Aggregating similarity scores using Reciprocal Rank Fusion (RRF) across cross-modal agents creates a more robust initial candidate list than single-vector similarity search.
- Mechanism: Text and vision similarity agents score candidates against the imagined targets ($C_t, C_v$). Instead of averaging scores directly, the system ranks candidates per modality and fuses them using RRF. This limits the dominance of one modality's score distribution.
- Core assumption: The correct target exists within the top ranks of at least one modality's similarity list.
- Evidence anchors:
  - [section 3.4]: "Reciprocal Rank Fusion (RRF) then aggregates these scores... ensuring higher-ranked candidates in either modality contribute more."
  - [figure 3a]: Shows RRF (z=60) outperforming direct score summation, indicating robustness to noisy candidates.
- Break condition: If the visual and textual signals are contradictory (e.g., text says "remove object" but visual search relies on that object's features), RRF may struggle to prioritize effectively.

### Mechanism 3
- Claim: Converting implicit modification requirements into explicit True/False verification questions reduces false positives that survive visual similarity matching.
- Mechanism: A question agent ($A^q$) parses the modification text into a set of atomic verification questions ($Q$) and expected answers ($A$). Question-based scoring agents then evaluate the top-$k'$ candidates by checking if the candidate's image and caption satisfy these predicates.
- Core assumption: User intent can be decomposed into a finite set of verifiable factual predicates.
- Evidence anchors:
  - [abstract]: "...question agents verify factual consistency through targeted reasoning for fine filtering."
  - [section 3.5]: "Unlike the similarity-based scoring agents, $A^q$ emphasizes the critical differences between a candidate image and the ideal target... enforcing explicit verification."
  - [corpus]: Neighbors like "SQUARE" emphasize reranking, but XR uniquely uses predicate-style questions for this step.
- Break condition: If the verification questions are redundant or fail to capture the core edit (e.g., asking about background when the edit is foreground texture), fine filtering provides no signal.

## Foundational Learning

- Concept: **Composed Image Retrieval (CIR)**
  - Why needed here: This is the task definition. You must understand that the query is a tuple (Reference Image + Modification Text), not just a text or image alone.
  - Quick check question: If a user provides an image of a red shirt and the text "blue", what is the system expected to retrieve?

- Concept: **Reciprocal Rank Fusion (RRF)**
  - Why needed here: Used in the coarse filtering stage. You need to know why rank-based fusion is preferred over score summation for multimodal data.
  - Quick check question: Why might averaging a CLIP visual score (0.0 to 1.0) with a text relevance score (0.0 to 100.0) fail without normalization?

- Concept: **MLLM Prompting (Captioning & VQA)**
  - Why needed here: The "agents" are primarily MLLMs (like InternVL) prompted for specific roles (imagine, verify). Understanding prompt engineering is critical for debugging these agents.
  - Quick check question: What is the difference in input context for the Text Imagination Agent vs. the Vision Imagination Agent?

## Architecture Onboarding

- Component map:
  1. **Input:** Reference Image ($I_r$), Modification Text ($T_m$).
  2. **Imagination Module:** Text Imagination Agent (outputs target caption $C_t$) + Vision Imagination Agent (outputs target caption $C_v$).
  3. **Coarse Filter:** Text/Vision Similarity Agents -> Compute 4 scores -> Reciprocal Rank Fusion (RRF) -> Select Top $k'$ (e.g., 100).
  4. **Fine Filter:** Question Agent (Generates $Q$) -> Text/Vision Question Agents (Answer $Q$ for candidates) -> Re-rank.
  5. **Output:** Final ranked list.

- Critical path: The **Imagination Module**. If $C_t$ and $C_v$ are inaccurate (hallucinated), the Coarse Filter retrieves garbage, and the Fine Filter has nothing to work with. The downstream verification relies entirely on the quality of the imagined proxy.

- Design tradeoffs:
  - **Latency vs. Accuracy ($k'$):** The paper sets $k'=100$ for fine filtering. Increasing $k'$ improves recall potential but linearly increases latency as the Question Agents must process more candidates (see Figure 5).
  - **Training-free adaptability:** The system uses off-the-shelf MLLMs (InternVL). This allows zero-shot deployment but inherits the biases and error rates of the frozen backbone.

- Failure signatures:
  - **Hallucination Loop:** Imagination agent describes a detail (e.g., "stripes") not in the target; Question Agent penalizes valid candidates lacking this hallucinated detail.
  - **Coarse Filter Bottleneck:** If RRF fails to surface the correct image in the top 100, the Fine Filter cannot recover it (hard cutoff).
  - **Subjectivity Mismatch:** Question Agents formulate binary True/False questions for vague modifications (e.g., "more professional"), leading to inconsistent scoring.

- First 3 experiments:
  1. **Ablation Sanity Check:** Run the pipeline with the Question Agent disabled to measure the delta in R@10 on FashionIQ. This confirms the specific contribution of fine filtering.
  2. **Latency Profiling:** Measure the inference time of the Imagination Agents vs. Question Agents on a single query to identify the primary computational bottleneck.
  3. **$k'$ Sensitivity:** Sweep $k'$ (e.g., 10, 50, 100, 200) on a validation set to find the point of diminishing returns for your specific hardware constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can XR's multi-agent framework generalize to temporal modalities such as video retrieval, where queries involve temporal modifications (e.g., "change the order of actions" or "make this scene longer")?
- Basis in paper: [explicit] Appendix D states: "The framework is currently tailored to image–text composition and has not yet been explored in settings involving richer modalities or temporal data."
- Why unresolved: The imagination and verification agents are designed for static images; temporal reasoning requires handling frame sequences, motion, and temporal consistency, which the current architecture does not address.
- What evidence would resolve it: Experiments extending XR to video CIR benchmarks (e.g., ActivityNet, MSR-VTT) with modified agents that synthesize temporal captions and verify frame-level or sequence-level constraints.

### Open Question 2
- Question: How do biases inherited from MLLM-generated captions and verification questions affect retrieval fairness across demographic groups or object categories?
- Basis in paper: [explicit] Appendix D notes: "Its reliance on captions and verification questions generated by large models can also introduce subtle biases, which may affect consistency."
- Why unresolved: The paper evaluates overall accuracy but does not analyze whether certain query types, attributes, or demographic representations are systematically disadvantaged by MLLM outputs.
- What evidence would resolve it: A fairness audit measuring retrieval performance stratified by protected attributes (e.g., gender, skin tone) or object categories, combined with bias mitigation experiments (e.g., debiased prompts, balanced sampling).

### Open Question 3
- Question: Does the binary True/False verification scheme in fine filtering limit retrieval precision compared to probabilistic or continuous scoring mechanisms?
- Basis in paper: [inferred] The question agents assign +1 or 0 scores (Section 3.5), but whether this discrete formulation captures nuanced partial matches is untested.
- Why unresolved: Binary scoring may discard candidates that partially satisfy modifications, especially for subjective or gradual attributes (e.g., "more formal," "slightly brighter").
- What evidence would resolve it: Ablations comparing True/False, Likert-scale (1–5), and continuous confidence scoring, measuring precision-recall trade-offs on fine-grained benchmarks like FashionIQ.

### Open Question 4
- Question: How does XR's latency scale with candidate pool sizes beyond 500 (e.g., 10K–1M images), and can approximate nearest-neighbor retrieval be integrated without degrading accuracy?
- Basis in paper: [inferred] Figure 5 shows near-linear latency up to k′=500, but real-world search engines index millions of images. The paper does not test or propose optimizations for such scales.
- Why unresolved: The coarse filtering stage evaluates all candidates individually; without approximate retrieval or indexing, computational cost becomes prohibitive at industrial scales.
- What evidence would resolve it: Experiments integrating ANN indexes (e.g., FAISS, HNSW) for coarse filtering, reporting latency-accuracy curves at 10K, 100K, and 1M candidate pools.

## Limitations
- The framework's performance critically depends on the Multimodal Large Language Model's (MLLM) ability to accurately imagine target representations from text edits, which may fail for highly abstract or subjective modifications
- The question agent's effectiveness is constrained by its ability to decompose user intent into verifiable predicates, potentially failing for vague modifications like "more professional"
- The coarse filtering stage uses a hard cutoff (top-100 candidates), meaning if the correct target falls outside this range, the fine filtering stage cannot recover it regardless of its verification quality

## Confidence
**High Confidence** (backed by ablation studies and benchmark results):
- The three-agent architecture (imagination, similarity, question) collectively improves retrieval performance by up to 38% over baselines
- Reciprocal Rank Fusion (RRF) provides more robust candidate aggregation than direct score summation
- Fine filtering through verification questions consistently improves R@10 scores across all three benchmarks

**Medium Confidence** (mechanisms appear sound but edge cases exist):
- The MLLM's ability to accurately imagine target representations for abstract modifications
- The question agent's decomposition of subjective edits into verifiable predicates
- The RRF cutoff parameter (z=60) being optimal across all query distributions

**Low Confidence** (unverified assumptions or potential failure modes):
- System behavior when reference and target images have completely different visual characteristics
- Performance consistency across domains with varying image-text alignment quality
- Scalability to extremely large candidate pools beyond the tested benchmarks

## Next Checks
1. **Hallucination Robustness Test**: Design a benchmark subset with highly abstract modification text (e.g., "more joyful," "less formal") and measure how often imagination agents generate plausible vs. hallucinated target descriptions. Compare retrieval accuracy between these two categories.

2. **Question Agent Predicate Coverage**: Manually annotate a sample of FashionIQ queries to identify which modifications cannot be decomposed into binary True/False predicates. Measure the correlation between predicate coverage and fine filtering effectiveness.

3. **Cutoff Sensitivity Analysis**: Systematically vary the RRF cutoff parameter (z=30, 60, 90, 120) and fine filtering threshold (k'=50, 100, 150) on validation sets. Identify at what point diminishing returns begin and whether optimal parameters vary by benchmark domain.