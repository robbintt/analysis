---
ver: rpa2
title: 'StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation
  via Long-range Spatial Dependencies'
arxiv_id: '2504.17401'
source_url: https://arxiv.org/abs/2504.17401
tags:
- disparity
- stereo
- stereomamba
- image
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StereoMamba addresses the challenge of real-time, accurate, and
  robust stereo disparity estimation in robot-assisted minimally invasive surgery
  (RAMIS), where existing deep learning methods struggle to balance accuracy, robustness,
  and inference speed. The proposed method introduces a Feature Extraction Mamba (FE-Mamba)
  module that leverages self-attention and cross-attention to capture long-range spatial
  dependencies within and across stereo image pairs, combined with a Multidimensional
  Feature Fusion (MFF) module for effective integration of multi-scale features.
---

# StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies

## Quick Facts
- arXiv ID: 2504.17401
- Source URL: https://arxiv.org/abs/2504.17401
- Authors: Xu Wang; Jialang Xu; Shuai Zhang; Baoru Huang; Danail Stoyanov; Evangelos B. Mazomenos
- Reference count: 30
- Primary result: Achieves 21.28 FPS real-time inference on 1280×1024 images while maintaining state-of-the-art accuracy on SCARED benchmark

## Executive Summary
StereoMamba addresses the challenge of real-time, accurate, and robust stereo disparity estimation in robot-assisted minimally invasive surgery (RAMIS), where existing deep learning methods struggle to balance accuracy, robustness, and inference speed. The proposed method introduces a Feature Extraction Mamba (FE-Mamba) module that leverages self-attention and cross-attention to capture long-range spatial dependencies within and across stereo image pairs, combined with a Multidimensional Feature Fusion (MFF) module for effective integration of multi-scale features. On the SCARED benchmark, StereoMamba achieves state-of-the-art performance with an EPE of 2.64 px and depth MAE of 2.55 mm, the second-best Bad2 of 41.49% and Bad3 of 26.99%, while maintaining real-time inference at 21.28 FPS for high-resolution images (1280×1024). Additionally, StereoMamba demonstrates strong zero-shot generalization on in-vivo datasets (RIS2017 and StereoMIS), achieving the best average SSIM of 0.8970 and PSNR of 16.0761 when comparing synthesized right images with actual ones.

## Method Summary
StereoMamba builds upon Mamba's Selective State Space Model (SSM) to create a stereo matching architecture that captures long-range spatial dependencies efficiently. The core innovation is the FE-Mamba module, which extracts features from both left and right images using parallel branches of self-attention (VMamba-style) and cross-attention (Mamba2-style) modules. These features are then fused through the MFF module to combine multi-scale self-attention outputs with cross-attention outputs. The fused features are used to construct a group-wise correlation cost volume, which is processed through a 3D convolutional decoder to produce disparity predictions at four different scales. The model is pre-trained on SceneFlow and fine-tuned on SCARED surgical datasets, achieving state-of-the-art accuracy while maintaining real-time inference speeds on high-resolution surgical images.

## Key Results
- Achieves state-of-the-art performance on SCARED benchmark with EPE of 2.64 px and depth MAE of 2.55 mm
- Maintains real-time inference at 21.28 FPS for 1280×1024 resolution images while achieving high accuracy
- Demonstrates strong zero-shot generalization on in-vivo datasets (RIS2017 and StereoMIS) with best average SSIM of 0.8970 and PSNR of 16.0761

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The paper proposes that Mamba's Selective State Space Model (SSM) provides a mathematically equivalent formulation to Transformer attention while maintaining linear computational complexity.
- **Mechanism:** The core insight is establishing that SSM's recurrent computation Y = (L ∘ CB^T) · x is formally identical to linear attention Y = (L ∘ QK^T) · V. The lower-triangular matrix L enforces causal ordering while the element-wise product with CB^T captures content-dependent interactions. This allows stereo matching to benefit from global context without the O(n²) cost of standard Transformers.
- **Core assumption:** Assumption: The mathematical equivalence holds empirically for 2D spatial data when scanning strategies (four-way in VMamba) are applied to convert images to sequences.
- **Evidence anchors:**
  - [section III.B.2]: "This equation is formally identical to Eq. 1, thus establishing a mathematically unified formulation of Mamba and Transformer architectures."
  - [abstract]: "FE-Mamba module that leverages self-attention and cross-attention to capture long-range spatial dependencies"
  - [corpus]: Limited direct evidence—corpus shows related stereo work (SurfSLAM, S²M²) but no SSM-specific comparisons.
- **Break condition:** If input sequences become too short (highly downsampled features), the recurrent structure's advantage over direct attention diminishes. Also, if hardware lacks optimized SSM kernels, practical speed gains may not materialize.

### Mechanism 2
- **Claim:** Introducing cross-image connections at the feature extraction stage (rather than only during cost volume construction) improves stereo correspondence quality.
- **Mechanism:** Traditional stereo pipelines extract features independently from left/right images before correlation. FE-Mamba's cross-attention component allows each image to query features from its stereo pair during extraction itself. This pre-aligns feature representations so that subsequent correlation operates on already partially-matched embeddings, reducing ambiguity in textureless or repetitive regions common in RAMIS.
- **Core assumption:** Assumption: Surgical scenes benefit from early cross-image feature exchange because tissue surfaces often have similar appearance across wide areas, making late-stage matching ambiguous.
- **Evidence anchors:**
  - [page 2]: "We propose that introducing these cross-image connections earlier—at the feature extraction stage—can improve the performance of stereo matching"
  - [table I]: StereoMamba achieves best mean depth MAE (2.55 mm) and EPE (2.64 px) vs. methods using only late correlation
  - [corpus]: GwcNet and ACVNet in comparisons use correlation-based volumes without early cross-attention.
- **Break condition:** If left/right images have significant non-overlapping regions or extreme viewpoint differences, cross-attention may introduce spurious correlations.

### Mechanism 3
- **Claim:** Multi-scale feature fusion with explicit self/cross-attention combination provides complementary information for robust disparity estimation.
- **Mechanism:** The MFF module fuses features from three self-attention scales (f₁, f₂, f₃) with the cross-attention output (f₄). Self-attention captures within-image context (useful for occlusion boundaries and local texture). Cross-attention captures across-image correspondence (useful for initial disparity hypotheses). The progressive upsampling and concatenation preserves fine spatial detail while incorporating global matching cues.
- **Core assumption:** The paper assumes that self and cross-attention capture fundamentally different information types that benefit from explicit fusion rather than sequential processing.
- **Evidence anchors:**
  - [table III]: Adding MFF to FE-Mamba reduces Bad2 from 3.57% to 3.48% on SceneFlow
  - [section III.C]: "To effectively fuse self-attention features (f₁, f₂, f₃) and cross-attention features (f₄), we propose the MFF module"
  - [corpus]: No direct comparisons from corpus on multi-scale fusion strategies.
- **Break condition:** If the fusion weights or upsampling are poorly calibrated, coarse cross-attention features may corrupt fine local detail, particularly at object boundaries.

## Foundational Learning

- **Concept: Stereo Matching and Cost Volumes**
  - **Why needed here:** StereoMamba's output feeds into a group-wise correlation cost volume. Understanding that disparity is found by minimizing matching cost across disparity candidates is essential.
  - **Quick check question:** Given a 1280×1024 image pair with max disparity 192, what is the dimensionality of a 4D cost volume using 32-channel features?

- **Concept: Selective State Space Models (SSM)**
  - **Why needed here:** FE-Mamba is built on Mamba/VMamba. Understanding that SSMs compress sequence history into a hidden state h(t) with content-dependent gating (selective mechanism) explains why they can model long-range dependencies efficiently.
  - **Quick check question:** Why does an SSM have O(n) complexity while standard Transformer attention has O(n²)?

- **Concept: Cross-Attention in Vision**
  - **Why needed here:** StereoMamba uses cross-attention to let left features query right features. Understanding that cross-attention computes attention between two different sequences (not within one) is critical for debugging feature extraction.
  - **Quick check question:** In cross-attention between left and right image features, which image provides the Query and which provides Key/Value?

## Architecture Onboarding

- **Component map:**
  Input (L/R images) -> Patch Embed (Conv k4 s4) -> FE-Mamba (parallel branches) -> MFF (fusion + upsample) -> Group-wise Correlation -> 4D Cost Volume -> Cost Aggregation (3D conv decoder) -> Disparity Regression (soft-argmin)

- **Critical path:**
  1. Initial patch embedding (resolution reduction is permanent)
  2. FE-Mamba cross-attention (couples left/right branches—bugs here cascade)
  3. Cost volume construction (group-wise correlation must match channel grouping)
  4. Soft-argmin regression (requires valid probability distribution)

- **Design tradeoffs:**
  - Removed downsampling from final VSS stack to preserve resolution for upsampling (Page 3: "low resolution feature output does not facilitate the subsequent upsampling process")
  - Modest FPS reduction (27.78→27.03 on SceneFlow) accepted for accuracy gains from MFF
  - Four disparity predictions at different scales for multi-scale supervision

- **Failure signatures:**
  - Near-zero disparity output: Check soft-argmin implementation or cost volume normalization
  - Left-right inconsistency: Cross-attention branch may not be receiving both inputs
  - Edge artifacts: MFF upsampling may have alignment issues with multi-scale features
  - Slow inference despite SSM: Verify causal masking is not disabled (removes parallelization)

- **First 3 experiments:**
  1. **Sanity check:** Run StereoMamba-base (no MFF) on SceneFlow subset. Verify EPE ~0.60 px and >25 FPS on 960×540. If FPS < 10, check CUDA kernel availability for SSM operations.
  2. **Cross-attention validation:** Ablate cross-attention by zeroing f₄ input to MFF. Expect degraded Bad2/3 metrics on SCARED (paper shows cross-attention provides measurable gain). If no change, verify branch connectivity.
  3. **Generalization test:** Train on SceneFlow only (no SCARED fine-tuning), test on RIS2017. Compare SSIM to table II values. Large degradation (>0.05 SSIM drop) suggests overfitting to synthetic data or insufficient cross-domain feature learning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can StereoMamba maintain real-time performance on the resource-constrained hardware typically available on actual surgical robotic platforms?
- **Basis:** [inferred] The implementation details specify the use of a high-end Nvidia RTX A6000 GPU (48GB memory), whereas RAMIS platforms often have limited onboard computing power compared to workstations.
- **Why unresolved:** The paper demonstrates real-time speeds (21.28 FPS) on high-end hardware but does not evaluate the model's efficiency or memory footprint on embedded systems.
- **What evidence would resolve it:** Benchmarking latency and accuracy on embedded surgical hardware (e.g., Nvidia Jetson) or measuring the memory bandwidth usage.

### Open Question 2
- **Question:** Does the frame-independent processing lead to temporal instability or flickering in the disparity maps during continuous surgical video streams?
- **Basis:** [inferred] The architecture focuses on "long-range spatial dependencies" within single pairs but does not explicitly model temporal dependencies or consistency across frames.
- **Why unresolved:** While the network processes video frames, the methodology treats them as independent stereo pairs, a common source of temporal flicker in video depth estimation.
- **What evidence would resolve it:** A quantitative evaluation of temporal consistency metrics (e.g., t-error) on continuous video sequences rather than static keyframes.

### Open Question 3
- **Question:** How does the model's performance degrade when facing severe surgical visual obstructions like heavy smoke or bleeding, which are less prevalent in the ex-vivo SCARED training data?
- **Basis:** [inferred] While zero-shot generalization is tested on RIS2017/StereoMIS, the paper highlights robustness mainly against "specular reflections and textureless regions" rather than complex dynamic occlusions.
- **Why unresolved:** The primary fine-tuning dataset (SCARED) is ex-vivo, potentially limiting the model's exposure to the full range of in-vivo visual artifacts.
- **What evidence would resolve it:** Evaluation on datasets specifically containing heavy smoke or blood artifacts to measure failure rates in these conditions.

## Limitations
- Mathematical equivalence between SSM and linear attention is theoretically established but not empirically validated for specific 2D spatial scanning strategies
- Cross-attention implementation details (particularly how A, B, C matrices are computed from image features) are underspecified
- Cost volume aggregation architecture beyond "4 outputs with 3D convolutions" is not detailed

## Confidence
- **High confidence**: EPE and depth MAE results on SCARED (these are direct measurements with standard metrics)
- **Medium confidence**: Zero-shot generalization performance (relies on image synthesis quality and assumes fair comparison methodology)
- **Medium confidence**: Real-time inference claims (FPS depends on hardware-specific optimizations not fully disclosed)

## Next Checks
1. **SSIM reproducibility check**: Replicate the zero-shot generalization experiment by training only on SceneFlow and testing on RIS2017. Compare SSIM to the reported 0.8970 value to assess true cross-domain generalization vs. dataset-specific tuning.
2. **Ablation of MFF module**: Remove the MFF module entirely and measure the impact on Bad2/Bad3 metrics on SCARED. This validates whether the explicit fusion of self and cross-attention features provides measurable benefit beyond just stacking attention modules.
3. **Complexity verification**: Profile memory and FLOPs for FE-Mamba at 1280×1024 resolution to verify that the claimed linear complexity translates to practical speed gains over Transformer-based alternatives under identical hardware conditions.