---
ver: rpa2
title: Investigating Model Editing for Unlearning in Large Language Models
arxiv_id: '2512.20794'
source_url: https://arxiv.org/abs/2512.20794
tags:
- editing
- unlearning
- information
- rome
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using model editing algorithms for machine
  unlearning in LLMs. The authors explore ROME, IKE, and WISE algorithms with three
  new target definitions for removal rather than alteration of information.
---

# Investigating Model Editing for Unlearning in Large Language Models

## Quick Facts
- **arXiv ID:** 2512.20794
- **Source URL:** https://arxiv.org/abs/2512.20794
- **Authors:** Shariqah Hossain; Lalana Kagal
- **Reference count:** 14
- **Primary result:** Model editing algorithms can achieve effective unlearning in LLMs, with IKE using incorrect targets showing the best balance of forgetting and utility.

## Executive Summary
This paper investigates the application of model editing algorithms to machine unlearning in large language models, exploring three algorithms (ROME, IKE, WISE) with three new target definitions focused on removal rather than alteration of information. Through experiments on the TOFU benchmark using Llama-2-7B, the authors demonstrate that model editing can be a viable approach to unlearning, though effectiveness depends significantly on both the algorithm and target definition chosen. The work shows that IKE with incorrect response targets achieves effective forgetting while maintaining high model utility, outperforming traditional unlearning methods, while ROME performs well with simple "dummy" targets but degrades significantly with complexity.

## Method Summary
The authors adapt model editing algorithms for unlearning by redirecting inputs to removal-oriented targets rather than altered information. They evaluate ROME, IKE, and WISE on the TOFU benchmark with three target definitions: "dummy" string, incorrect perturbed answers from TOFU, and GPT-4-generated avoidant responses. The experimental setup uses Llama-2-7B as the base model, with a train/test split of Forget 10 (10% of TOFU prompts) and Retain 90. They evaluate using Model Utility (harmonic mean of Truth Ratio, probability, ROUGE across Retain Set) and Forget Quality (KS test p-value comparing unlearned vs ground truth distributions, threshold 0.05). Ground truth is established using Llama-2-7B trained only on the remaining 90% of fictitious data.

## Key Results
- IKE with incorrect response targets achieves effective forgetting (Forget Quality p < 0.05) while maintaining high model utility (0.612)
- ROME with simple "dummy" targets achieves highest Forget Quality but suffers severe utility degradation (0.153)
- WISE struggles with Forget Quality (p = 0.0) because it does not apply edits during generation
- The choice of target definition significantly impacts unlearning effectiveness across all algorithms

## Why This Works (Mechanism)

### Mechanism 1: In-Context Behavioral Override (IKE)
IKE prepends demonstrations to the input prompt, mapping forget queries to specific incorrect answers in the context window. The model attends to this override, effectively masking the original ground truth without degrading underlying weights. This works by prioritizing in-context examples over stored weights during generation.

### Mechanism 2: Localized Weight Nullification (ROME)
ROME uses causal mediation analysis to locate specific MLP layers associated with a subject, then computes a rank-one update to map the subject key to a new value ("dummy"). This overwrites the MLP layer where the fact is stored with a null or low-information signal, effectively deleting the information.

### Mechanism 3: The Utility-Forgetting Tradeoff Gradient
Effective unlearning depends on balancing target complexity against algorithm's ability to localize changes. Simple targets allow high-forget quality but risk utility loss due to aggressive weight changes, while in-context methods handle complex targets better but may fail to remove latent knowledge.

## Foundational Learning

- **Causal Mediation Analysis**: Essential for understanding how ROME identifies specific layers responsible for storing facts to be unlearned. *Quick check:* Which layer does ROME identify as the storage location for factual associations?
- **In-Context Learning (ICL)**: IKE relies entirely on ICL. You must understand how demonstrations in a prompt steer model behavior without weight updates. *Quick check:* Does IKE modify model weights permanently or temporarily alter input context?
- **Forget Quality vs. Model Utility**: These are dual metrics for success. An engineer must understand that optimizing one often degrades the other. *Quick check:* If a model outputs "I don't know" to all questions, is it considered to have high Forget Quality?

## Architecture Onboarding

- **Component map:** Editor Core (ROME/IKE/WISE) -> Target Generator (Dummy/Incorrect/Avoidant) -> Benchmark (TOFU) -> Evaluation Metrics
- **Critical path:** Select editing algorithm -> Define New Target strategy -> Run edit on Forget 10 dataset -> Evaluate on TOFU metrics
- **Design tradeoffs:** ROME vs IKE: Choose ROME for strict statistical similarity to retrained model if utility loss is acceptable; choose IKE for practical deployment where model coherence is prioritized. Target Choice: "Dummy" for simple erasure; "Incorrect" for IKE to maintain natural flow; "Avoidant" currently unstable for ROME.
- **Failure signatures:** ROME Degradation: Model outputs gibberish or fails to produce outputs; reduce edit quantity or simplify target. WISE False Negative: High Model Utility but low Forget Quality; verify retrieval mechanism is activated. Low Forget Quality: Model still knows ground truth; ensure New Target is correctly mapped.
- **First 3 experiments:** 1) Baseline Reproduction (ROME+Dummy): Verify high Forget Quality / low Utility tradeoff. 2) Optimal Config Test (IKE+Incorrect): Confirm exceeds 0.05 Forget Quality threshold while maintaining >0.6 Model Utility. 3) Stress Test (ROME+Avoidant): Observe specific failure mode (degradation/gibberish).

## Open Questions the Paper Calls Out

### Open Question 1
How does the non-weight-modifying nature of IKE-based unlearning withstand adversarial extraction attacks compared to parameter-modifying methods? The paper evaluates Forget Quality using statistical distribution tests but does not simulate adversarial attacks or white-box access where latent knowledge might still be extracted.

### Open Question 2
Can modifications to the ROME algorithm or its hyperparameters prevent the observed catastrophic degradation of model utility when applying complex or high-volume edits? The authors experimented with hyperparameters but "were not able to restore reasonable performance," leaving the stability issue unresolved.

### Open Question 3
How can the WISE architecture be modified to effectively apply stored edits to generative outputs to ensure high Forget Quality? The current architecture stores edits in side memory but defaults to main memory for generation, creating a disconnect between unlearning intent and actual output.

## Limitations
- Narrow evaluation scope using only fictitious authors as unlearning targets, limiting generalizability to complex real-world knowledge
- WISE algorithm shows particularly poor performance (Forget Quality p = 0.0, Model Utility 0.253), suggesting fundamental incompatibility with evaluation framework
- No computational cost comparison between editing-based unlearning and traditional fine-tuning approaches

## Confidence
**High Confidence Claims:**
- ROME with simple "dummy" targets achieves statistically significant forgetting but suffers severe utility degradation
- IKE with incorrect response targets maintains high model utility while achieving effective forgetting
- Choice of target definition significantly impacts unlearning effectiveness across all algorithms
- Model editing can be a viable approach to unlearning in LLMs

**Medium Confidence Claims:**
- The observed tradeoff between Forget Quality and Model Utility is inherent to the unlearning problem
- Complex targets like "Avoidant" degrade ROME performance significantly
- The harmonic mean metric appropriately balances utility and forgetting considerations

**Low Confidence Claims:**
- Specific numerical values of Model Utility and Forget Quality metrics
- The claim that IKE "overrides retrieval mechanism via attention" rather than creating permanent knowledge modification
- Generalization of results beyond the TOFU benchmark and fictitious author domain

## Next Checks
1. **Algorithm-Agnostic Target Evaluation**: Test the three target definitions (Dummy, Incorrect, Avoidant) across all three algorithms on a held-out subset of TOFU to verify that target choice drives performance differences.
2. **Real-World Knowledge Removal**: Apply the most successful configuration (IKE with Incorrect targets) to remove actual factual knowledge from Llama-2-7B and evaluate both factual accuracy and generation quality.
3. **Utility-Forgetting Surface Analysis**: Systematically vary perturbation strength in IKE's incorrect targets and clamp norm factor in ROME to map the full tradeoff surface between Forget Quality and Model Utility.