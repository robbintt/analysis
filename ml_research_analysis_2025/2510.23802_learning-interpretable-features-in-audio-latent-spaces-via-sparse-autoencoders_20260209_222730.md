---
ver: rpa2
title: Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders
arxiv_id: '2510.23802'
source_url: https://arxiv.org/abs/2510.23802
tags:
- audio
- acoustic
- features
- arxiv
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of interpreting audio generative\
  \ models, which unlike text models, face difficulties due to audio\u2019s dense\
  \ nature and limited automatic feature characterization. The proposed framework\
  \ addresses this by training sparse autoencoders (SAEs) on audio autoencoder latent\
  \ spaces, then learning linear mappings from SAE features to human-interpretable\
  \ acoustic concepts: pitch, amplitude, and timbre."
---

# Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders

## Quick Facts
- arXiv ID: 2510.23802
- Source URL: https://arxiv.org/abs/2510.23802
- Reference count: 6
- Primary result: SAEs extract interpretable features from audio latents, enabling pitch control with 0.75-0.87 accuracy and timbre analysis with 0.17-0.46 accuracy

## Executive Summary
This work introduces a framework for interpreting audio generative models by applying sparse autoencoders (SAEs) to their latent spaces and learning linear mappings to human-interpretable acoustic concepts. Unlike text models, audio's dense nature and limited automatic feature characterization make interpretation challenging. The framework successfully extracts pitch, amplitude, and timbre features from both continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latents. Experiments show pitch is most linearly separable (0.75-0.87 accuracy), while timbre remains challenging (0.17-0.46). Analysis of DiffRhythm reveals a coarse-to-fine generation progression where pitch converges first, followed by timbre, while loudness remains unresolved by the final step.

## Method Summary
The method trains SAEs on pretrained audio autoencoder latent spaces using L1 regularization on hidden activations while maintaining reconstruction fidelity. Linear probes are then trained to map SAE features to acoustic concepts: pitch (via CREPE), amplitude (via RMS), and timbre (via spectral centroid). The framework applies to both continuous latents (DiffRhythm-VAE) and discrete latents (EnCodec, WavTokenizer). SAE hyperparameters are optimized through grid search over hidden dimensions (2048-16384) and sparsity coefficients (0.005-0.15). The resulting linear probe weights enable bidirectional control—classification accuracy validates feature interpretability while weight vectors enable targeted acoustic manipulation through linear perturbations in sparse space.

## Key Results
- Pitch classification accuracy: 0.75-0.87 across all models, demonstrating strong linear separability
- Timbre classification accuracy: 0.17-0.46, indicating fundamental challenges in linear representation
- DiffRhythm generation analysis reveals pitch converges around step 21, followed by timbre, while loudness remains unresolved
- EnCodec outperforms WavTokenizer for loudness classification (0.56-0.63 vs 0.17-0.46)
- Control vector experiments show isolated attribute changes while preserving non-targeted properties

## Why This Works (Mechanism)

### Mechanism 1: Sparse Decomposition of Superposed Audio Features
Audio autoencoders compress dense waveforms into latent codes where features are superposed. SAEs exploit this by finding sparse activation directions through L1 regularization that isolate underlying, disentangled features. This works because audio latents contain linearly superposed features that can be decomposed via sparse coding.

### Mechanism 2: Linear Decodability of Acoustic Properties from Sparse Features
After SAE decomposition, each acoustic property maps to a subset of sparse features. Linear classifiers trained on these features reveal which dimensions encode specific properties, with classification accuracy indicating decodability. This works because acoustic properties are approximately linearly separable in the sparse feature space.

### Mechanism 3: Bidirectional Control via Linear Probe Weight Vectors
The linear classifier weights directly indicate feature-to-acoustic-class contributions. Adding scaled probe weight vectors to SAE features shifts the representation toward specific acoustic classes. After RMSNorm renormalization, decoding through both SAE and audio decoder produces modified audio with isolated attribute changes. This works because linear perturbations in sparse space cause localized acoustic changes without catastrophic interference.

## Foundational Learning

- **Polysemantic Hypothesis and Superposition**: Explains why SAEs are necessary—neurons encode multiple features through superposition, requiring sparse decomposition to disentangle. Quick check: Why can't we directly interpret raw latent dimensions without sparse decomposition?

- **Audio Latent Space Types (Continuous vs. Discrete)**: Framework applies to VAE (DiffRhythm) and codec (EnCodec, WavTokenizer) latents with different sparsity characteristics. Quick check: How does WavTokenizer's high sparsity (0.993-0.999) affect SAE training compared to EnCodec?

- **Linear Probing and Probe Weights**: Linear probe weights serve dual purposes—classification accuracy validates feature interpretability, and weight vectors enable control. Quick check: If pitch probe accuracy is 0.87 but timbre is 0.17, what does this imply about control vector reliability for each?

## Architecture Onboarding

- **Component map**: Raw audio → encoder latents → SAE sparse features → linear probe predictions; Control: sparse features + control vector → renormalize → SAE decode → audio decode
- **Critical path**: Pretrained audio encoder → latent vectors → SAE encoder with ReLU + RMSNorm → sparse features → linear probes → acoustic predictions
- **Design tradeoffs**: Hidden dimension (4×-256× input) affects reconstruction quality vs. memory; sparsity λ (0.005-0.15) trades interpretability vs. information loss; discretization bins affect resolution vs. training samples
- **Failure signatures**: Timbre accuracy consistently low (0.17-0.46) across all models; loudness doesn't converge in DiffRhythm; out-of-distribution artifacts during control; reconstruction quality degrades at extreme sparsity/hidden dimension combinations
- **First 3 experiments**:
  1. SAE hyperparameter sweep: Grid search hidden dims {2048, 4096, 8192, 12288, 16384} × λ ∈ {0.005, 0.01, 0.05, 0.1, 0.15} on each encoder; track reconstruction loss and sparsity ratio
  2. Linear probe training and evaluation: Train probes for pitch (66 MIDI-aligned bins), loudness (20 linear bins), timbre (20 spectral centroid bins); plot accuracy vs. SAE sparsity to identify hierarchy
  3. Control vector intervention test: On held-out audio samples, apply control vectors with α ∈ {1, 10, 20, 30} for each property; measure attribute isolation via held-out probe predictions on modified audio

## Open Questions the Paper Calls Out

### Open Question 1
Can linear probes trained on SAE features accurately decode higher-level musical attributes such as rhythm, harmony, and instrument identity? The current study is limited to pitch, amplitude, and timbre; it is unknown if the linearity observed in basic acoustic properties holds for complex, temporal, or categorical musical concepts. Evidence would be successful training of linear classifiers for rhythm/harmony with accuracy significantly above chance.

### Open Question 2
Can control vectors derived from SAE features be used to guide the generative process during inference without compromising audio quality? The authors propose using interpretable features to directly guide generation behavior during inference. The current work demonstrates post-hoc manipulation but hasn't tested if these vectors can steer the generation process itself while maintaining fidelity to the prompt. Evidence would be a demonstration of inference-time intervention where specific acoustic properties are altered via control vectors without introducing artifacts.

### Open Question 3
Does the low linear separability of timbre (0.17-0.46 accuracy) result from the limitations of spectral centroid as a proxy, or is timbre inherently non-linear in these latent spaces? The paper notes timbre remains "challenging" while using a "simplified proxy" (spectral centroid). It's unclear if low probe accuracy is because latent space encodes timbre non-linearly, or if the chosen metric fails to capture multi-dimensional nature of timbre. Evidence would be retraining probes using multi-dimensional timbre descriptors to see if linear separability improves.

### Open Question 4
Is the observed failure of loudness to converge during DiffRhythm generation a limitation of the latent representation or an artifact of the probe methodology? The analysis reveals that while pitch and timbre converge, loudness "converges last and remains unresolved by the final step." The paper establishes the phenomenon but doesn't determine if the model fails to encode loudness information or if the linear probe and discretization method are insufficient. Evidence would be analyzing the latent space with non-linear probes or continuous regression targets for loudness.

## Limitations

- Timbre classification accuracy remains consistently low (0.17-0.46) across all models, suggesting fundamental challenges in linear representation
- Framework depends on pretrained audio encoders, limiting generalizability to other generative architectures
- Control mechanism reliability not thoroughly tested for simultaneous manipulation of multiple attributes
- Spectral centroid may be insufficient proxy for complex timbre properties

## Confidence

- **High Confidence**: SAE decomposition for feature isolation (Mechanism 1) is well-established and pitch classification results (0.75-0.87) provide strong empirical support
- **Medium Confidence**: Linear decodability claims (Mechanism 2) are supported but wide variance in accuracy suggests property-dependent limitations; control mechanism (Mechanism 3) is theoretically sound but lacks extensive validation
- **Low Confidence**: Claims about enabling "controllable manipulation and analysis of the music generation process" are supported by proof-of-concept but lack comprehensive evaluation of control reliability

## Next Checks

1. Apply the SAE framework to at least two additional audio generative models (e.g., Jukebox, AudioLDM) to test generalizability beyond the three models examined, comparing whether the same acoustic properties show similar linear separability patterns.

2. Systematically evaluate control vector effectiveness by measuring attribute isolation across multiple simultaneous manipulations, testing combinations like "increase pitch while maintaining timbre" and quantifying unintended attribute drift using held-out linear probes.

3. Replace spectral centroid with a more comprehensive timbre representation (e.g., Mel-frequency cepstral coefficients or learned timbre embeddings) and retrain linear probes to determine if timbre classification accuracy improves beyond the current 0.17-0.46 range.