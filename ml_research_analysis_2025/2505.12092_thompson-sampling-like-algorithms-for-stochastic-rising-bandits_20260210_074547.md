---
ver: rpa2
title: Thompson Sampling-like Algorithms for Stochastic Rising Bandits
arxiv_id: '2505.12092'
source_url: https://arxiv.org/abs/2505.12092
tags:
- have
- regret
- lemma
- algorithms
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Thompson Sampling-like algorithms for stochastic\
  \ rising rested bandits (SRRBs), where arm expected rewards increase as they are\
  \ pulled. The authors propose ET-Beta-SWTS and \u03B3-ET-SWGTS algorithms, which\
  \ incorporate sliding-window Thompson Sampling with forced exploration to handle\
  \ the dynamic nature of SRRBs."
---

# Thompson Sampling-like Algorithms for Stochastic Rising Bandits

## Quick Facts
- **arXiv ID:** 2505.12092
- **Source URL:** https://arxiv.org/abs/2505.12092
- **Reference count:** 40
- **Primary result:** Thompson Sampling-like algorithms (ET-Beta-SWTS and γ-ET-SWGTS) achieve sublinear regret for stochastic rising rested bandits when the complexity index σ is bounded

## Executive Summary
This paper addresses stochastic rising rested bandits (SRRBs), where arm expected rewards increase as they are pulled. The authors introduce two Thompson Sampling-like algorithms that combine forced exploration with sliding-window updates to handle the dynamic nature of these problems. The key insight is that rising bandits have a "strong regularity" property that allows Thompson Sampling to succeed when properly augmented with initial uniform exploration. The work provides the first Thompson Sampling analysis for this setting, introducing a novel complexity index σµ(T) that characterizes the difficulty of identifying the optimal arm. Theoretical regret bounds are validated through numerical simulations on both synthetic and real-world IMDB dataset experiments.

## Method Summary
The method combines three key mechanisms: (1) an Explore-Then (ET) phase where each arm is pulled Γ times initially to allow rising rewards to manifest, (2) sliding-window (SW) updates that limit history to the most recent τ rewards to reduce bias from outdated low-reward samples, and (3) Thompson Sampling (TS) posterior sampling from Beta or Gaussian distributions. The ET-Beta-SWTS algorithm uses Beta posteriors for Bernoulli rewards, while γ-ET-SWGTS uses Gaussian posteriors for Subgaussian rewards. The forced exploration parameter Γ is set to approximately ασ (where α≥1) to ensure the optimal arm is sufficiently identified, and the sliding window size τ is typically set to √T for robustness. The algorithms maintain window statistics (N_{i,t,τ} and S_{i,t,τ}) to enable efficient posterior updates.

## Key Results
- ET-Beta-SWTS and γ-ET-SWGTS achieve sublinear regret when complexity index σ is bounded, with bounds O(σ + log(T)/∆²)
- Thompson Sampling succeeds in dynamic environments due to the "strong regularity" of rising bandits, proven via novel Poisson-Binomial Stochastic Dominance arguments
- Sliding window variants show improved robustness to mis-specified Γ parameters compared to non-windowed versions
- Real-world IMDB experiments demonstrate superior performance over state-of-the-art algorithms in online model selection tasks

## Why This Works (Mechanism)

### Mechanism 1: Forced Exploration Phase (ET)
- **Claim:** Initial uniform arm pulls allow the "rising" reward functions to manifest their potential, mitigating the risk of prematurely discarding the optimal arm $i^*(T)$ due to low initial values.
- **Mechanism:** By forcing $\Gamma$ pulls of every arm, the algorithm ensures sufficient data is collected to distinguish the optimal arm's trajectory from suboptimal ones, effectively satisfying the complexity index $\sigma_\mu(T)$.
- **Core assumption:** The optimal arm's expected reward $\mu_{i^*(T)}(n)$ will eventually surpass suboptimal arms, and $\Gamma$ is chosen such that $\Gamma \approx \alpha \sigma$ for some $\alpha \ge 1$.
- **Evidence anchors:** Section 4 describes the forced exploration phase; Corollary 6.4 shows the regret bound depends on setting $\Gamma = \alpha \sigma$.
- **Break condition:** If $\Gamma$ is set significantly lower than $\sigma_\mu(T)$, the algorithm may suffer linear regret.

### Mechanism 2: Sliding Window Bias Reduction (SW)
- **Claim:** Limiting history to a window of size $\tau$ reduces bias introduced by non-stationary rewards.
- **Mechanism:** Old rewards (when the arm was "weak") have lower expected values than recent rewards. A sliding window discards these outdated samples, aligning the empirical mean closer to the current expected reward.
- **Core assumption:** Expected reward $\mu_i(n)$ is non-decreasing; older samples are pessimistically biased estimates.
- **Evidence anchors:** Section 4 introduces sliding window estimators; Appendix B analyzes sliding-window bounds.
- **Break condition:** If $\tau$ is too small, variance of posterior estimate becomes too high; if too large, bias persists.

### Mechanism 3: Thompson Sampling with Log-Concavity (TS)
- **Claim:** Thompson Sampling can be adapted for dynamic settings by leveraging "strong regularity" of rising bandits.
- **Mechanism:** Novel Poisson-Binomial Stochastic Dominance argument shows worst-case error for TS in dynamic environments is bounded by stationary case.
- **Core assumption:** Rewards are Bernoulli or Subgaussian, and suboptimality gaps are defined via complexity index.
- **Evidence anchors:** Section 5 introduces Lemma 5.2 (PB-Bin Stochastic Dominance); Abstract mentions "strong regularity."
- **Break condition:** Theoretical guarantees depend on complexity index $\sigma_\mu(T)$ being bounded.

## Foundational Learning

- **Concept: Rested vs. Restless Bandits**
  - **Why needed here:** The paper specifically targets Rested bandits, where arm evolution is triggered only by its own pulls. Standard non-stationary algorithms assume arms change regardless of pulls.
  - **Quick check question:** If an arm is not pulled for 100 rounds, does its expected reward change in a rested setting?

- **Concept: Thompson Sampling (TS)**
  - **Why needed here:** The core algorithm is a Bayesian heuristic. TS maintains a posterior distribution over expected reward and selects arm with highest sampled value, rather than UCB.
  - **Quick check question:** Does TS rely on constructing a deterministic confidence interval or a probability distribution to guide exploration?

- **Concept: Complexity Index ($\sigma_\mu(T)$)**
  - **Why needed here:** This novel metric defines the "hardness" of the rising bandit instance. It represents the number of pulls required to statistically distinguish the optimal arm.
  - **Quick check question:** If learning curves of two arms cross multiple times, would $\sigma_\mu(T)$ likely be high or low?

## Architecture Onboarding

- **Component map:** Buffer/Deque -> Accumulators (N_{i,t,τ}, S_{i,t,τ}) -> Sampler (Beta/Gaussian) -> Orchestrator (ET vs Commit phases)
- **Critical path:** 1) Initialization: Pull every arm Γ times. 2) Update: Add new reward, evict reward at t-τ. 3) Sample: Draw θ_{i,t,τ} from posterior. 4) Select: argmax θ_{i,t,τ}.
- **Design tradeoffs:**
  - Window size (τ): Small τ reduces bias but increases variance; τ=√T is typical.
  - Forced Exploration (Γ): High Γ guarantees sublinear regret for hard instances but increases constant regret cost.
  - Prior Choice: Beta priors exact for Bernoulli; Gaussian priors handle Subgaussian but require tuning γ.
- **Failure signatures:**
  - Linear Regret: If curve grows linearly instead of flattening, Γ was likely too small.
  - High Variance Jitter: If τ is too small, cumulative regret curve will be noisy.
- **First 3 experiments:**
  1. Baseline Comparison: Run ET-Beta-SWTS against R-ed-UCB and standard TS on synthetic 15-arm environment to validate standard TS fails and ET-Beta-SWTS succeeds.
  2. Hyperparameter Sensitivity (Γ): Sweep forced exploration parameter Γ (e.g., 100 to 5000) on synthetic environment to observe elbow where regret stabilizes.
  3. Real-world Validation: Apply algorithm to IMDB dataset experiment to verify performance in online model selection task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the regret lower bound be extended from deterministic to stochastic SRRB instances, and does the complexity index σ remain tight in the stochastic case?
- **Basis in paper:** [explicit] "First of all, we note that the lower bound is derived considering deterministic instances. Indeed, the complexity index σµ(T) is not affected by the possible reward stochasticity."
- **Why unresolved:** Theorem 6.5 provides a lower bound only for M^det_σ (deterministic subset), leaving the stochastic case unaddressed despite algorithms operating in stochastic settings.
- **What evidence would resolve it:** A formal lower bound proof for stochastic instances with noise variance λ², showing whether O(σ) dependence remains unavoidable.

### Open Question 2
- **Question:** Can the sliding window size τ be selected adaptively without prior knowledge of instance-dependent quantities σ'(T; τ) and ∆'_i(T; τ)?
- **Basis in paper:** [explicit] "As it is common in the Sliding-Window literature [52, 20], the best choice for the window-length is instance dependent."
- **Why unresolved:** Regret bounds depend on τ through σ'(T; τ), which requires knowledge of optimal arm's reward growth rate.
- **What evidence would resolve it:** An adaptive window selection rule with regret guarantees, or an impossibility result showing such adaptation degrades bounds.

### Open Question 3
- **Question:** Can minimax regret guarantees be achieved for SRRBs when σ grows with T, or does linear regret remain unavoidable without additional structure?
- **Basis in paper:** [explicit] "The worst-case regret over the class of all rising bandits (i.e., the minimax regret) degenerates to linear if no further structure is enforced, making the problem unlearnable in the minimax sense."
- **Why unresolved:** Paper provides instance-dependent bounds but notes minimax analysis fails; it remains unclear what minimal structural assumptions would enable sublinear minimax regret.
- **What evidence would resolve it:** Either a minimax lower bound for bounded σ scenarios, or identification of structural conditions enabling O(√T) minimax regret.

## Limitations
- The complexity index σµ(T) is instance-specific and unknown a priori, requiring careful tuning of the forced exploration parameter Γ
- The Poisson-Binomial Stochastic Dominance argument is novel but its applicability to broader bandit settings requires further validation
- Sliding window analysis shows improved robustness but lacks complete regret bounds for all parameter regimes
- Real-world IMDB experiments are limited to specific classifier architectures and may not generalize to all model selection scenarios

## Confidence

| Claim | Confidence |
|-------|------------|
| Core algorithmic framework is sound | High |
| Theoretical regret bounds are mathematically rigorous | Medium |
| Poisson-Binomial Stochastic Dominance technique is broadly applicable | Medium |
| Sliding window variants provide practical improvements | Low |
| Real-world IMDB results generalize well | Low |

## Next Checks
1. **Synthetic Environment Sensitivity:** Systematically vary the complexity index σ by generating arms with different rising rates and verify that the required Γ scales as predicted by theory, validating the core claim that forced exploration should match problem complexity.

2. **Distributional Robustness:** Test the algorithms beyond Bernoulli and Subgaussian rewards (e.g., Gaussian with unknown variance, or heavy-tailed distributions) to assess practical limitations of Beta and Gaussian posterior assumptions.

3. **Adaptive Complexity Estimation:** Implement a heuristic to estimate σµ(T) online and adaptively adjust Γ during execution, addressing the practical limitation of requiring prior knowledge of problem complexity and potentially improving real-world applicability.