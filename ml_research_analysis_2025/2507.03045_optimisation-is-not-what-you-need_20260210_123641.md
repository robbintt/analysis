---
ver: rpa2
title: Optimisation Is Not What You Need
arxiv_id: '2507.03045'
source_url: https://arxiv.org/abs/2507.03045
tags:
- problem
- optimisation
- methods
- forgetting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that catastrophic forgetting is inherent to optimisation
  methods, and therefore such methods cannot develop Artificial General Intelligence.
  The key insight is that weight-based optimisation methods will always modify weights
  to solve new problems, thereby losing the capability to solve previous ones.
---

# Optimisation Is Not What You Need

## Quick Facts
- arXiv ID: 2507.03045
- Source URL: https://arxiv.org/abs/2507.03045
- Reference count: 40
- Primary result: Proves catastrophic forgetting is inherent to optimization methods, preventing AGI development

## Executive Summary
This paper demonstrates that catastrophic forgetting is an unavoidable consequence of weight-based optimization methods. When a model trained on one task is trained on a new task, the optimization process necessarily modifies weights in a way that degrades performance on the original task. The paper proves this is not a side effect that can be mitigated, but a fundamental limitation arising from how optimization methods work. The author proposes world-modelling frameworks like Synthetic Cognition as alternatives that avoid these issues by building separate representation hierarchies rather than modifying shared weights.

## Method Summary
The paper establishes theoretical proofs showing that weight-based optimization methods inherently suffer from catastrophic forgetting (Theorem 3) and overfitting (Theorem 5). These proofs are based on formal definitions of weighted mappings and optimization problems. Empirical validation uses a world-modelling framework called "Unsupervised Cognition" to demonstrate that such approaches can maintain 95.32% accuracy on Wisconsin Breast Cancer after training on a different task (Pima Indians Diabetes), while standard optimization methods would show catastrophic forgetting.

## Key Results
- Weight-based optimization methods will always modify weights when trained on new problems, causing catastrophic forgetting of previous tasks
- Overfitting is an inherent property of optimization methods, as precisely solving the training problem degrades performance on similar but different test distributions
- World-modelling frameworks like Synthetic Cognition can avoid both catastrophic forgetting and overfitting by maintaining separate representation hierarchies for different tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted optimization methods inherently suffer from catastrophic forgetting when trained on new tasks.
- Mechanism: When a weighted mapping $M_w: I \rightarrow_w O$ is trained on a problem $Q$ to produce weights $w$, and then trained on a different problem $Q' \neq Q$ to produce weights $w'$, Theorem 3 proves that $w \neq w'$, which implies $M_w(L) \neq M_{w'}(L)$. The new weights overwrite the old, breaking the original mapping.
- Core assumption: The two problems $Q$ and $Q'$ are distinct ($Q' \neq Q$) and the training sets $L$ and $L'$ are disjoint.
- Evidence anchors:
  - [abstract] "This paper proves that catastrophic forgetting is inherent to optimisation methods...weight-based optimisation methods will always modify weights to solve new problems, thereby losing the capability to solve previous ones."
  - [section] Theorem 3 and its proof on page 4 explicitly demonstrate that for any optimization method based on weights, training on a new problem $Q' \neq Q$ will result in catastrophic forgetting.
  - [corpus] Related literature cited in the paper (e.g., McCloskey and Cohen [36], and various mitigation attempts [54, 60, 43]) consistently identifies catastrophic forgetting as a persistent, unsolved problem in neural networks.

### Mechanism 2
- Claim: Overfitting is an inherent property of weight-based optimization methods because precisely solving the training set's optimization problem necessarily degrades performance on slightly different test distributions.
- Mechanism: A weighted mapping $M_w$ trained to precisely solve the optimization problem $Q$ for a training set $L$ will produce outputs $y'_L$ that do not minimize the loss function for a new, albeit similar, test problem $Q' \approx Q$. Theorem 5 shows this is a direct consequence of the weights being tuned for $Q$ and not $Q'$.
- Core assumption: The test distribution is similar but not identical to the training distribution ($Q' \approx Q$).
- Evidence anchors:
  - [abstract] "Additionally, it addresses the problem of overfitting and discuss about other smaller problems that optimisation methods pose."
  - [section] Theorem 5 and its proof on page 5 formally demonstrate that for any weight-based optimization method, solving $Q$ means it cannot solve $Q'$.
  - [corpus] Corpus evidence on overfitting specifically is weak or missing for the neighbors, but the paper cites standard mitigation techniques like regularization [6], dropout [52] which only mitigate, not eliminate, the problem.

### Mechanism 3
- Claim: A world-modelling framework, specifically "Synthetic Cognition," can avoid catastrophic forgetting by maintaining separate representation hierarchies for different tasks.
- Mechanism: Instead of a single set of weights that is modified, this framework builds internal representations for each input. For different problems, it constructs "different representation hierarchies that do not overlap with each other" (Section VI-A). Learning for a new task adds new, separate structures rather than overwriting existing ones.
- Core assumption: The representational structures for different tasks are sufficiently distinct that they do not interfere with one another.
- Evidence anchors:
  - [abstract] "Empirical results show that world-modelling frameworks like Synthetic Cognition avoid both catastrophic forgetting and overfitting, maintaining 95.32% accuracy on Wisconsin Breast Cancer after training on a different task..."
  - [section] Section VI-A describes the experiment where an Unsupervised Cognition instance trained on Wisconsin Breast Cancer (95.32% accuracy) maintained the same accuracy after being trained on Pima Indians Diabetes.

## Foundational Learning

- Concept: Weighted Mappings and Optimization Problems
  - Why needed here: The entire theoretical argument rests on defining machine learning as a process that creates a mapping $M_w: I \rightarrow_w O$ by minimizing a loss function (Definitions 2, 4, 5).
  - Quick check question: Can you explain, in your own words, how an optimization method's goal of minimizing a loss function on a training set $L$ leads to a specific weighted mapping?

- Concept: Catastrophic Forgetting (Formal Definition)
  - Why needed here: This is the central problem the paper addresses. Understanding its formal definition (Definition 6) is critical to appreciating why the author argues it's inherent to optimization methods.
  - Quick check question: According to Definition 6, what is the key condition that must be met for catastrophic forgetting to have occurred after training on a new set $L'$?

- Concept: Overfitting (Formal Definition)
  - Why needed here: The paper provides a specific, formal definition of overfitting (Definition 7) linked directly to the optimization setup.
  - Quick check question: How does Definition 7 frame overfitting in terms of a model's performance on a new set of inputs $L'$ compared to its original training set $L$?

## Architecture Onboarding

- Component map:
  1. **Optimization Core**: The conventional ML pipeline (e.g., Neural Network, Transformer) defined as a weighted mapping $M_w$. Its function is to minimize a loss function $f$ on a training set $L$.
  2. **World-Modelling Alternative**: The proposed alternative framework (e.g., Synthetic Cognition). Its function is to build literal and abstract representations of the input domain.
  3. **Representation Hierarchies**: The internal structures within the world-modelling framework. Separate hierarchies are built for different problems, preventing interference.

- Critical path:
  1. **Identify the method's core**: Is the system based on a single set of weights ($M_w$) that are updated globally? If so, it's on the "optimization" path and will suffer from forgetting/overfitting.
  2. **Diagnose the task relationship**: Are new tasks related to old ones ($Q' \approx Q$) or distinct ($Q' \neq Q$)? Distinct tasks will break the old mapping on the optimization path (Theorem 3).
  3. **Select appropriate architecture**:
      - For single, fixed tasks: A standard optimization method is sufficient (Theorem 4).
      - For continual/AGI-like tasks: An optimization method is fundamentally insufficient. A world-modelling or modular approach is required to build separate representations.

- Design tradeoffs:
  - **Optimization Methods**: High performance on single, well-defined tasks. Simple to implement with existing libraries. Fundamentally limited in continual learning scenarios.
  - **World-Modelling/Modular Methods**: Potentially solves continual learning and overfitting. Requires abandoning the standard loss-function minimization paradigm. Implementation details for "Synthetic Cognition" are less mature than standard deep learning.

- Failure signatures:
  - **Catastrophic Forgetting**: Model's accuracy on a previously mastered task drops abruptly after training on a new one. Theorem 3 guarantees this for weight-based methods on new tasks.
  - **Overfitting**: Training accuracy continues to improve while validation/test accuracy stagnates or degrades. Theorem 5 guarantees this tension for weight-based methods.

- First 3 experiments:
  1. **Reproduce the forgetting result**: Train a simple multi-layer perceptron (MLP) on the Wisconsin Breast Cancer dataset until it achieves high accuracy. Then, continue training it on the Pima Indians Diabetes dataset. Measure its accuracy on the original Breast Cancer test set. It should drop significantly, empirically confirming Theorem 3.
  2. **Test the modular/hybrid approach**: Implement a simple modular system where two separate MLPs are trained, one for each dataset. A router directs inputs to the correct expert. Observe that this avoids catastrophic forgetting, aligning with the paper's suggestion of "interconnected optimization methods, each one focused on a single task."
  3. **Validate overfitting dynamics**: Train a deep network on a small dataset without regularization. Plot both training and validation loss curves. Observe the divergence, confirming the mechanism described in Theorem 5 and the "balance" discussed in Section V.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are world-modelling methods completely immune to catastrophic forgetting, or do corner cases exist where representation hierarchies from different tasks interfere with each other?
- Basis in paper: [explicit] The authors state: "Further research is necessary to explore if there are corner cases where these methods can suffer catastrophic forgetting, or if they are totally immune."
- Why unresolved: The catastrophic forgetting experiment tested only two simple numerical datasets (Wisconsin Breast Cancer and Pima Indians Diabetes), which may not expose overlapping representation issues that could occur with more complex or similar tasks.
- What evidence would resolve it: Systematic testing across diverse task pairs with varying degrees of similarity, complexity, and representational overlap, including high-dimensional data like images and text.

### Open Question 2
- Question: Can overfitting occur in world-modelling methods under specific conditions, such as when aggregative abstractions become overly specialized to training representations?
- Basis in paper: [explicit] The authors acknowledge: "Further research is necessary to explore if there are corner cases where these methods can suffer overfitting, or if they are totally immune."
- Why unresolved: The overfitting experiment only monitored accuracy and internal representation changes on a single dataset for 10 epochs, which may not capture subtler forms of overfitting that emerge with more complex data distributions or longer training.
- What evidence would resolve it: Testing on datasets with varying noise levels, training for extended epochs while monitoring generalization gaps, and analyzing whether aggregative abstractions can become overly specific.

### Open Question 3
- Question: Do sets of interconnected optimization methods (where each solves a single task) truly avoid catastrophic forgetting, or does interference emerge at the coordination level?
- Basis in paper: [explicit] The authors mention: "It would be also interesting to explore if these problems extrapolate to sets of optimisation methods" and reference modular approaches like JEPA.
- Why unresolved: The paper's theoretical proof addresses single optimization methods but does not formally analyze whether combining multiple single-task optimizers introduces new failure modes in task switching, knowledge sharing, or meta-level coordination.
- What evidence would resolve it: Formal analysis of multi-module systems and empirical evaluation of task-switching performance in modular architectures with shared components.

### Open Question 4
- Question: How do world-modelling methods compare to optimization methods in computational and memory efficiency at scale?
- Basis in paper: [inferred] The paper demonstrates accuracy preservation but does not discuss training time, inference speed, memory footprint, or scalability to large-scale problems.
- Why unresolved: Avoiding catastrophic forgetting and overfitting is only valuable if the alternative approach remains practically viable; the experimental scope uses small datasets that may not reveal scaling bottlenecks.
- What evidence would resolve it: Benchmarking world-modelling methods against optimization baselines on standardized large-scale tasks while measuring resource consumption, training duration, and inference latency.

## Limitations

- Theoretical framework limitations: The proofs rely on strict mathematical conditions (disjoint training sets, distinct problems) that may not hold in real-world applications where tasks have overlapping features.
- Empirical evidence scope: Validation is based on a single pair of datasets (Wisconsin Breast Cancer and Pima Indians Diabetes) without comprehensive testing across diverse domains or larger-scale scenarios.
- Implementation accessibility: Core empirical claims depend on "Unsupervised Cognition" (reference [23]), which is not fully described in the paper, making independent verification currently impossible.

## Confidence

**High confidence**: The mathematical proofs for Theorems 3 and 5 regarding catastrophic forgetting and overfitting in weight-based optimization methods are internally consistent and logically sound within their stated assumptions.

**Medium confidence**: The theoretical argument that optimization methods cannot achieve AGI due to inherent forgetting/overfitting properties, while compelling, makes assumptions about AGI requirements that may be debated.

**Low confidence**: The empirical demonstration of Synthetic Cognition's superiority, given its reliance on a single, non-reproducible experiment with limited scope.

## Next Checks

1. **Theoretical boundary conditions**: Systematically test the formal definitions of catastrophic forgetting and overfitting across edge cases where training and test distributions have varying degrees of overlap, to validate the theorems' applicability boundaries.

2. **Replication with alternative implementations**: Develop and test simplified modular approaches (e.g., separate models per task with routing mechanisms) to verify whether the paper's conclusions about weight-based methods hold across different architectural designs.

3. **Scalability assessment**: Evaluate the Synthetic Cognition framework on multi-task benchmarks with increasing complexity and dataset size to determine whether the reported benefits scale beyond the initial two-dataset validation.