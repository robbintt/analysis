---
ver: rpa2
title: Analyzing Latent Concepts in Code Language Models
arxiv_id: '2510.00476'
source_url: https://arxiv.org/abs/2510.00476
tags:
- code
- concept
- clusters
- latent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Code Concept Analysis (CoCoA), a global post-hoc
  interpretability framework that uncovers emergent lexical, syntactic, and semantic
  structures in code language models by clustering contextualized token embeddings
  into human-interpretable concept groups. The framework combines static analysis
  tool-based syntactic alignment with LLM-based annotation to scale labeling across
  abstraction levels.
---

# Analyzing Latent Concepts in Code Language Models

## Quick Facts
- arXiv ID: 2510.00476
- Source URL: https://arxiv.org/abs/2510.00476
- Reference count: 40
- Key outcome: CoCoA improves human interpretability of code model explanations by 37 percentage points using concept-augmented attributions vs. token-level Integrated Gradients.

## Executive Summary
This paper introduces Code Concept Analysis (CoCoA), a global post-hoc interpretability framework that uncovers emergent lexical, syntactic, and semantic structures in code language models by clustering contextualized token embeddings into human-interpretable concept groups. The framework combines static analysis tool-based syntactic alignment with LLM-based annotation to scale labeling across abstraction levels. Empirical evaluations across three models and tasks show that discovered concepts remain stable under semantic-preserving perturbations (average Cluster Sensitivity Index = 0.288) and evolve predictably with fine-tuning. In a user study on programming-language classification, concept-augmented explanations improved human-centric explainability by 37 percentage points compared to token-level attributions using Integrated Gradients. The study validates that mapping salient tokens to concept clusters yields more robust and interpretable explanations than traditional attribution methods.

## Method Summary
CoCoA extracts layer-wise token embeddings from code language models, applies K-Means clustering to discover concept groups, and uses a hybrid LLM-static analysis pipeline for scalable annotation. The framework then enhances local attributions by mapping salient tokens to their corresponding concept clusters. The approach was validated across three models (CodeBERT, UniXCoder, DeepSeekCoder) and three tasks, with stability measured through semantic-preserving perturbations and human evaluation of explanation quality.

## Key Results
- Concept clusters remain stable under semantic-preserving perturbations (CSI = 0.288)
- LLM-human hybrid annotation achieves 83.2% accuracy vs human labels
- Concept-augmented explanations improve human interpretability by 37 percentage points over token-level attributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Clustering contextualized token embeddings reveals emergent conceptual structure organized along lexical, syntactic, and semantic dimensions.
- **Mechanism:** K-Means partitions token representations from each model layer by minimizing intra-cluster variance. Similar tokens (e.g., comparison operators `<`, `>`, `==`) co-locate in representational space, forming interpretable groups that reflect how the model internally organizes code abstractions rather than imposed external taxonomies.
- **Core assumption:** Token embeddings encode functionally meaningful similarity; distance in latent space corresponds to conceptual relatedness.
- **Evidence anchors:**
  - [abstract] "clusters contextualized token embeddings into human-interpretable concept groups"
  - [Section 2.1] "K-Means partitions the data to minimize intra-cluster variance"
  - [corpus] Related work (SPARC, Concept-Based Mechanistic Interpretability) supports clustering-based concept discovery as viable, though code-specific validation remains limited.
- **Break condition:** If embeddings were random or dominated by tokenizer artifacts without semantic grounding, clusters would not exhibit consistent alignment with syntactic categories or stability under perturbations.

### Mechanism 2
- **Claim:** Hybrid annotation combining static analysis with LLM prompting yields scalable, semantically expressive concept labels.
- **Mechanism:** Tree-sitter-derived AST labels provide syntactic alignment verification. LLMs (GPT-4o, Gemini-2.0-Flash) generate semantic tags and descriptions via few-shot prompts. Manual filtering consolidates 1,724 raw labels into 43 canonical tags.
- **Core assumption:** LLMs can reliably infer functional roles from token lists and code context without task-specific supervision.
- **Evidence anchors:**
  - [abstract] "combines static analysis tool-based syntactic alignment with prompt-engineered large language models"
  - [Section 2.2] "83.2% of the semantic labels were deemed more accurate and informative than their human-generated counterparts"
  - [corpus] Limited direct corroboration; concept annotation for code remains underexplored in prior literature.
- **Break condition:** If LLM annotations were inconsistent or hallucinated functional roles, manual inspection would reveal low inter-annotator agreement or misaligned cluster descriptions.

### Mechanism 3
- **Claim:** Mapping attribution-salient tokens to latent concept clusters improves explanation coherence over raw token-level saliency.
- **Mechanism:** Integrated Gradients identifies top tokens contributing to predictions. A logistic regression classifier maps each token's representation to its precomputed cluster. The cluster's semantic label (e.g., "Mathematical Operations") contextualizes the token's role.
- **Core assumption:** Salient tokens correspond to semantically coherent clusters rather than isolated surface features.
- **Evidence anchors:**
  - [abstract] "concept-augmented explanations disambiguated token roles and improved human-centric explainability by 37 percentage points"
  - [Section 3.5] "only 24.3% of clusters could be satisfactorily explained using token-level attributions alone"
  - [corpus] Sparse Autoencoder and concept-based interpretability work supports concept-level explanations, though direct code-domain comparisons are absent.
- **Break condition:** If salient tokens mapped to incoherent or "Unclear Behavioral Role" clusters without semantic signal, user studies would show no improvement over baseline attributions.

## Foundational Learning

- **Concept: Contextualized Token Embeddings**
  - **Why needed here:** CoCoA operates on layer-wise token representations (not static embeddings). Understanding how context shapes these vectors is prerequisite to interpreting cluster semantics.
  - **Quick check question:** Can you explain why the token `<` might have different embeddings in `List<Integer>` vs. `if (a < b)`?

- **Concept: K-Means Clustering and Cluster Validation**
  - **Why needed here:** Concept discovery relies on K-Means partitioning. The Cluster Sensitivity Index (CSI) measures robustness to perturbations.
  - **Quick check question:** What does low CSI (e.g., 0.288) indicate about cluster stability under semantic-preserving transformations?

- **Concept: Attribution Methods (Integrated Gradients)**
  - **Why needed here:** The local attribution enhancement builds on IG. Understanding its axiomatic properties and limitations is necessary to interpret the 37-point improvement claim.
  - **Quick check question:** Why might IG highlight syntactic tokens like `<` without indicating whether it functions as a comparator or type delimiter?

## Architecture Onboarding

- **Component map:** Representation Extraction -> Concept Discovery -> Concept Annotation -> Attribution Enhancement
- **Critical path:** Representation extraction → clustering → annotation → classifier training → explanation generation. Errors in early stages (e.g., poor clustering parameters) propagate downstream, degrading both concept coherence and explanation quality.
- **Design tradeoffs:**
  - **K-Means vs. agglomerative clustering:** K-Means scales but assumes spherical clusters; agglomerative captures hierarchical structure but is computationally prohibitive for large datasets.
  - **Token frequency filtering:** Removing high-frequency tokens (e.g., `int`, `{`) prevents dominance but may discard meaningful structural signals.
  - **LLM annotation reliability:** Few-shot prompting improves consistency but introduces dependency on model quality; manual verification remains necessary.
- **Failure signatures:**
  - Clusters dominated by single tokens (poor diversity) → suggests K is too large or frequency filtering insufficient.
  - High proportion of "Unclear Behavioral Role" labels → indicates clustering parameters or annotation prompts need refinement.
  - Low inter-annotator agreement in user study → signals ambiguous cluster semantics or poor explanation generation.
- **First 3 experiments:**
  1. **Baseline clustering validation:** Run K-Means on CodeBERT layer 12 representations with K=350. Compute lexical/syntactic alignment scores. Verify clusters exhibit interpretable patterns (e.g., access modifiers grouping together).
  2. **Perturbation robustness test:** Apply identifier renaming, statement reordering, and scope reassignment. Compute CSI. Confirm average CSI ≈ 0.288 as reported.
  3. **Attribution enhancement pilot:** Fine-tune CodeBERT on language classification. Compute IG attributions for test snippets, map salient tokens to clusters, and conduct a small-scale human evaluation comparing raw vs. concept-augmented explanations. Target improvement >30 percentage points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can concept-aware training objectives improve model robustness compared to post-hoc analysis methods?
- Basis in paper: [explicit] Section 6 states future work will focus on "concept-aware training objectives that may improve both interpretability and robustness."
- Why unresolved: The current CoCoA framework is purely a post-hoc interpretability method applied to already trained models.
- What evidence would resolve it: Comparative benchmarks evaluating the semantic stability and interpretability of models trained with explicit concept supervision versus standard pre-training.

### Open Question 2
- Question: How can compositional latent clusters that blend multiple fine-grained concepts be probed at higher levels of granularity?
- Basis in paper: [explicit] Section 3.2 identifies clusters that "blend fine-grained concepts" and states "Probing these compositions at higher levels of granularity is an important future work direction."
- Why unresolved: Current alignment methods map clusters to single syntactic or semantic labels, struggling with multi-faceted abstractions.
- What evidence would resolve it: Development of new probing techniques or perturbation strategies that can decompose and quantify the interaction of concepts within a single cluster.

### Open Question 3
- Question: Can scalable clustering algorithms capture irregular concept boundaries in code models better than spherical clustering methods?
- Basis in paper: [explicit] Section 5 notes that K-Means "assumes spherical clusters and may miss irregular boundaries," suggesting "exploring more scalable alternatives" is a promising direction.
- Why unresolved: Agglomerative clustering was too computationally expensive (limiting coverage), while K-Means is a geometric approximation.
- What evidence would resolve it: Application of density-based or hierarchical clustering methods to larger datasets, comparing cluster coherence against the K-Means baseline.

### Open Question 4
- Question: Does concept augmentation stabilize local explanations for complex semantic tasks like compile error detection?
- Basis in paper: [inferred] Section 5 notes that "compile error detection posed challenges due to attribution instability," unlike the successful language classification task.
- Why unresolved: The user study validated the method on the simpler task of language classification, leaving the utility for deeper semantic debugging uncertain.
- What evidence would resolve it: A user study or stability analysis focused specifically on the compile error detection task, measuring if concept context reduces attribution noise.

## Limitations
- Framework relies on LLM-generated annotations with 16.8% potential error rate
- K-Means clustering assumes spherical geometry that may miss hierarchical or irregular concept structures
- 30k snippet sample from CodeNet represents limited diversity of programming languages and domains

## Confidence

**High Confidence:**
- Clustering contextualized token embeddings reveals emergent conceptual structure along lexical, syntactic, and semantic dimensions
- Semantic-preserving perturbations maintain cluster stability (CSI = 0.288 demonstrates measurable robustness)
- Fine-tuning leads to predictable concept evolution patterns

**Medium Confidence:**
- LLM-human hybrid annotation produces scalable and accurate concept labels (83.2% accuracy is promising but lacks direct comparison to alternative annotation methods)
- Concept-augmented explanations improve human interpretability by 37 percentage points (user study results are positive but based on a single task and may not generalize)

**Low Confidence:**
- The specific K=350 parameter is optimal for code concept discovery (no systematic comparison with other values presented)
- 15K token frequency threshold is appropriate (threshold selection appears arbitrary without sensitivity analysis)

## Next Checks

1. **Cross-Domain Generalization Test:** Apply CoCoA to a diverse set of programming languages and domains (e.g., assembly, Prolog, R) not represented in the original CodeNet sample to evaluate concept discovery robustness across language paradigms.

2. **Concept Coherence Validation:** Conduct a blind human evaluation where participants must assign semantic labels to clusters without seeing the CoCoA-generated labels, measuring inter-annotator agreement and comparing to the reported 83.2% accuracy.

3. **Perturbation Sensitivity Analysis:** Systematically vary perturbation types and magnitudes to determine whether CSI=0.288 represents a stable property of the discovered concepts or an artifact of the specific perturbations tested.