---
ver: rpa2
title: Instruction-tuned Self-Questioning Framework for Multimodal Reasoning
arxiv_id: '2509.21251'
source_url: https://arxiv.org/abs/2509.21251
tags:
- sub-questions
- reasoning
- main-question
- visual
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a self-questioning framework for improving\
  \ multimodal reasoning in vision-language tasks. The method uses a three-component\
  \ architecture\u2014Questioner, Answerer, and Reasoner\u2014all based on InstructBLIP\
  \ to iteratively generate sub-questions and sub-answers that help solve the main\
  \ visual question."
---

# Instruction-tuned Self-Questioning Framework for Multimodal Reasoning

## Quick Facts
- **arXiv ID**: 2509.21251
- **Source URL**: https://arxiv.org/abs/2509.21251
- **Reference count**: 10
- **Primary result**: Self-questioning framework improves multimodal reasoning accuracy on VQA-Introspect and A-OKVQA datasets by iteratively generating sub-questions and sub-answers

## Executive Summary
This paper introduces a self-questioning framework that improves multimodal reasoning by decomposing complex visual questions into simpler sub-questions. The method uses a three-component architecture—Questioner, Answerer, and Reasoner—all based on InstructBLIP to iteratively generate sub-questions and sub-answers that help solve the main visual question. Experiments show that using generated sub-questions as additional context improves performance over baseline models, with accuracy increasing as more sub-questions are generated. The framework addresses limitations of existing LLM-based methods by using a vision-language model that can extract fine-grained visual information, making the process more accurate and reproducible.

## Method Summary
The framework consists of three InstructBLIP-based components: a Questioner that generates sub-questions, an Answerer that responds to them, and a Reasoner that synthesizes the final answer. The Questioner and Reasoner have their Q-Former modules fine-tuned while the image encoder and LLM remain frozen. The process iterates up to three times, with each new sub-question instructed to address different information than previous ones. Sub-question/sub-answer pairs are then provided as context to the Reasoner, which uses instruction-tuned prompts to integrate this information and produce the final answer.

## Key Results
- Generated sub-questions improve VQA-Introspect accuracy from 85.53% to 86.84% over baseline
- Ground-truth sub-QAs yield 91.23% accuracy, showing potential for improvement with better sub-answer generation
- Accuracy increases with more sub-questions, reaching 91.67% with 4 sub-questions
- Framework demonstrates effectiveness in multi-step reasoning through qualitative examples

## Why This Works (Mechanism)

### Mechanism 1: Iterative Sub-Question Diversification
- **Claim**: Sequentially generating diverse sub-questions yields broader informational coverage for reasoning.
- **Mechanism**: The Questioner is prompted to produce a sub-question, then—on subsequent turns—instructed to "ask about different information than the following questions," incrementally expanding the evidence base for the Reasoner.
- **Core assumption**: Diversity of sub-questions correlates with marginal gains in relevant visual evidence retrieval.
- **Evidence anchors**:
  - [abstract]: "generating image-aware informative sub-questions and sub-answers iteratively"
  - [section 2.1]: "Create a question that asks about different information than the following questions. {sq_1}, ..., {sq_{i-1}}"
  - [corpus]: Socratic Questioning (FMR=0.71) also uses recursive self-questioning, suggesting convergence on iterative decomposition as a reasoning scaffold.
- **Break condition**: When sub-questions become redundant or the Answerer introduces errors that propagate to the Reasoner.

### Mechanism 2: Vision-Language Grounding Across All Modules
- **Claim**: Using a VLM (rather than a text-only LLM) for every module allows access to fine-grained visual features throughout the self-questioning loop.
- **Mechanism**: All three modules (Questioner, Answerer, Reasoner) instantiate InstructBLIP; only the Q-Former is fine-tuned while the image encoder and LLM remain frozen, preserving visual grounding without full-model retraining.
- **Core assumption**: A shared VLM backbone enables coherent visual reasoning across modules; text-only LLMs cannot substitute for visual feature access.
- **Evidence anchors**:
  - [abstract]: "fine-grained visual contents of images are not available using LLMs that cannot read visual information"
  - [section 1]: "utilizing a vision-language model (VLM) rather than a language-only model"
  - [corpus]: Weak direct evidence comparing VLM vs. LLM within self-questioning; related papers emphasize reasoning strategies rather than modality grounding.
- **Break condition**: When Q-Former fails to bridge vision and language effectively for a given task, or when visual features are insufficient for the required fine-grained distinctions.

### Mechanism 3: Contextual Aggregation via Instruction-Tuned Reasoner
- **Claim**: Providing sub-question/sub-answer pairs as explicit context improves the Reasoner's final inference by making intermediate reasoning steps visible.
- **Mechanism**: The Reasoner is instruction-tuned to incorporate sub-question/sub-answer pairs using a prompt that explicitly instructs it to use helpful Q&A results and ignore unhelpful ones, then fine-tuned on ground-truth sub-QAs.
- **Core assumption**: The Reasoner can learn to weight and integrate multiple evidence sources appropriately; ground-truth supervision signals the optimal integration pattern.
- **Evidence anchors**:
  - [abstract]: "Reasoner performs reasoning on the main-question considering the generated sub-question information"
  - [section 4.1]: Using generated sub-QAs improves over baseline (86.84% vs. 85.53%); using ground-truth sub-QAs yields 91.23%, indicating upper-bound potential.
  - [corpus]: Rethinking Information Synthesis in Multimodal QA (FMR=0.62) addresses multi-agent information synthesis, aligning with aggregation challenges.
- **Break condition**: When incorrect sub-answers (from the Answerer) mislead the Reasoner, as shown in Figure 4(b).

## Foundational Learning

- **Concept: InstructBLIP Architecture (Image Encoder + Q-Former + Frozen LLM)**
  - Why needed here: Understanding that only the Q-Former is trainable while other components remain frozen is essential for correctly implementing fine-tuning for Questioner and Reasoner.
  - Quick check question: Why does the method fine-tune only the Q-Former rather than the full model?

- **Concept: Instruction Design for Multi-Turn Generation**
  - Why needed here: The framework relies on carefully constructed prompts to enforce diversity across sub-questions and to instruct the Reasoner on conditional use of context.
  - Quick check question: What specific instruction ensures each new sub-question addresses different information than prior ones?

- **Concept: Error Propagation in Modular Pipelines**
  - Why needed here: The Answerer's mistakes can cascade to the Reasoner; recognizing this dependency is critical for diagnosing failures and prioritizing improvements.
  - Quick check question: In Figure 4(b), what is the root cause of the final incorrect inference?

## Architecture Onboarding

- **Component map**: Questioner -> Answerer -> Reasoner
- **Critical path**:
  1. Input image and main question to Questioner → generate sq_1
  2. Feed sq_1 to Answerer → generate sa_1
  3. Append sq_1 to diversity prompt → Questioner generates sq_2
  4. Repeat for N sub-questions (optimal N=3)
  5. Concatenate all sub-QA pairs with main question → Reasoner produces final answer

- **Design tradeoffs**:
  - Sub-question count: More sub-questions increase accuracy (up to 91.67% with 4) but linearly increase inference time.
  - Generated vs. ground-truth sub-QAs: Generated enables full automation; ground-truth provides ~4% higher accuracy (supervision signal).
  - Open-ended vs. multiple-choice evaluation: Open-ended evaluation penalizes synonym variation and Answerer errors; multiple-choice is more forgiving.

- **Failure signatures**:
  - Answerer errors propagate to Reasoner (Figure 4b shows incorrect "Yes" leading to wrong final inference).
  - Open-ended evaluation drops 4% below baseline (56.04% vs. 60.96%) due to synonym mismatches and incorrect sub-answers.
  - Redundant sub-questions fail to provide new information, limiting accuracy gains.

- **First 3 experiments**:
  1. Run InstructBLIP baseline (no sub-questions) on VQA-Introspect validation to establish a reference accuracy.
  2. Ablate the number of sub-questions (0→4) using ground-truth sub-QAs to isolate the effect of context quantity on Reasoner accuracy.
  3. Compare Reasoner performance with generated sub-QAs vs. ground-truth sub-QAs to quantify Answerer error impact and identify failure modes.

## Open Questions the Paper Calls Out

- **Transferability to other multimodal tasks**: The paper plans to apply the framework to visual commonsense reasoning or visual entailment tasks, but current validation is limited to VQA datasets. Benchmark results on VCR or SNLI-VE would demonstrate generalizability.

- **Answerer module robustness**: The paper identifies Answerer errors as a primary cause of performance degradation but doesn't propose specific improvements. Experiments with a fine-tuned or higher-capacity Answerer could quantify the impact of reduced error rates.

- **Inference latency optimization**: While the paper notes linear time increases with sub-question count, it arbitrarily sets 3 as optimal without exploring parallel generation or adaptive stopping criteria that might maintain accuracy with lower computational overhead.

## Limitations

- **Answerer error propagation**: The framework's dependence on the Answerer module creates a single point of failure that can cascade to the Reasoner, with errors directly impacting final accuracy.

- **Modality grounding assumption**: The claim that VLMs are essential over text-only LLMs lacks direct comparative evidence within the self-questioning framework.

- **Diversity vs. quantity uncertainty**: The paper lacks direct evidence showing that sub-question diversity alone drives performance gains versus simply increasing the number of reasoning steps.

## Confidence

- **High confidence**: Basic architecture and implementation using InstructBLIP are well-specified and reproducible.
- **Medium confidence**: Performance improvements are statistically significant but modest, with Answerer errors identified as a major limiting factor.
- **Low confidence**: Claims about inherent superiority of self-questioning over alternative reasoning strategies lack comparative analysis.

## Next Checks

1. **Ablation of sub-question diversity vs. quantity**: Compare diverse sub-questions (current approach), multiple identical sub-questions, and random sub-questions to isolate whether diversity or simply more reasoning steps drive accuracy gains.

2. **Error propagation quantification**: Measure Answerer accuracy independently and correlate with Reasoner performance drops to establish the error propagation coefficient and identify most harmful error types.

3. **Alternative reasoning strategies comparison**: Implement and evaluate direct question decomposition and external knowledge retrieval approaches using identical datasets to determine whether self-questioning provides unique advantages.