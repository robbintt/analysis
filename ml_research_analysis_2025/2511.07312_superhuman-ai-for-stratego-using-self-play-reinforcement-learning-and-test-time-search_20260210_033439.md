---
ver: rpa2
title: Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time
  Search
arxiv_id: '2511.07312'
source_url: https://arxiv.org/abs/2511.07312
tags:
- bomb
- piece
- game
- capt
- move
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Ataraxos, a reinforcement learning-based AI
  system for the board game Stratego, which has long been considered one of the most
  challenging benchmarks in artificial intelligence due to its massive hidden information
  state space. Ataraxos achieves a breakthrough result by defeating the most decorated
  Stratego player of all time with unprecedented margin (15 wins, 4 draws, 1 loss)
  while requiring only a few thousand dollars to train, compared to previous efforts
  costing millions.
---

# Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search

## Quick Facts
- arXiv ID: 2511.07312
- Source URL: https://arxiv.org/abs/2511.07312
- Authors: Samuel Sokota; Eugene Vinitsky; Hengyuan Hu; J. Zico Kolter; Gabriele Farina
- Reference count: 40
- Primary result: Defeated world champion 15-4-1 (wins-draws-losses) using reinforcement learning and test-time search

## Executive Summary
Ataraxos achieves superhuman performance in Stratego, a benchmark imperfect-information game with 10^33 possible hidden piece configurations, by combining self-play reinforcement learning with test-time search. The system defeats the most decorated Stratego player of all time with unprecedented margin while requiring only a few thousand dollars to train, compared to previous efforts costing millions. The key innovation is a coordinated interplay between regularization strength, policy update size, and policy strength that enables stable learning under imperfect information. This is combined with test-time search via belief networks that approximate hidden information, allowing Ataraxos to leverage additional computation during gameplay.

## Method Summary
Ataraxos uses two interdependent self-play reinforcement learning processes: a setup selection process (decoder-only transformer) and a move selection process (encoder-only transformer). Both processes share training signals but maintain separate architectures and hyperparameters to avoid tradeoffs. The system employs dynamic damping via coordinated regularization and update scheduling that adapts to policy strength, preventing entropy collapse and ensuring stable learning. Test-time search uses belief networks to sample opponent piece configurations, runs rollouts using the move network, and applies a single tabular policy update via magnetic mirror descent. The entire system is trained on 163 million self-play games (208 billion environment steps) using 16 H100 GPUs for one week.

## Key Results
- Defeated world champion 15-4-1 with 85% win rate, exceeding previous best by over 50 percentage points
- Achieved Elo rating of 2111 versus human baseline of 1650
- Required only a few thousand dollars to train, compared to previous efforts costing millions
- Test-time search provides consistent Elo gains across different opponent strengths

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Damping via Coordinated Regularization and Update Scheduling
Imperfect-information environments produce cyclical, divergent, or chaotic learning dynamics that can be stabilized by co-adapting regularization strength and policy update magnitude to policy strength. When the policy is weak, apply strong regularization (entropy maximization, reverse KL penalties) with aggressive updates. As policy strength increases, anneal regularization and reduce update sizes. This prevents both underdeveloped policies and entropy collapse.

### Mechanism 2: Interdependent Separation of Setup and Move Learning
Decomposing Stratego into two self-play processes (setup selection, move selection) that share training signal but maintain separate architectures and hyperparameters improves both sample efficiency and asymptotic performance. The setup process generates initial boards for the move process; the move process determines game outcomes used by both processes. This separation avoids architectural and hyperparameter tradeoffs.

### Mechanism 3: Test-Time Search as a Single Policy Update
Test-time search in high-hidden-information settings can be implemented as a single tabular policy update using rollouts from a learned belief model, avoiding expensive game-tree search. Sample opponent piece configurations from a belief network conditioned on public information, run depth-limited rollouts using the move network, average value predictions across rollouts, and apply one step of magnetic mirror descent to produce a refined local policy.

## Foundational Learning

- **Policy regularization via KL divergence and entropy**: The dynamic damping mechanism relies on controlling how far the policy moves from its previous version (reverse KL to data collection policy) and from a reference policy (magnet policy). Understanding these terms is essential for debugging training stability.
  - Quick check question: If you increase the reverse KL coefficient to the data collection policy by 10x, what happens to the importance ratio clip rate?

- **Imperfect-information games and belief states**: The entire system is built around the challenge that hidden information makes values depend on both past and counterfactual policies. The belief network is the mechanism for inferring hidden state from public observations.
  - Quick check question: In Stratego, after observing that an opponent piece has moved but not yet revealed its type, what information does the belief network condition on?

- **Transformer architectures for structured prediction**: The setup network uses a decoder-only transformer for autoregressive piece placement; the move network uses an encoder-only transformer with key-query attention for move selection. Understanding the tokenization and positional encoding schemes is necessary for debugging input representation issues.
  - Quick check question: Why might a decoder-only architecture be preferred for setup generation but an encoder-only architecture for move selection?

## Architecture Onboarding

- **Component map**:
  - Setup Network (decoder-only transformer) -> Move Network (encoder-only transformer) -> Belief Network (encoder-decoder transformer) -> GPU Simulator -> Search Module

- **Critical path**:
  1. Training: Self-play data generation → move network training (filtered by advantage magnitude) → setup network training (on completed games) → repeat
  2. Inference: Observe position → belief network samples opponent configurations → search module runs rollouts → tabular policy update → sample move

- **Design tradeoffs**:
  - Pure policy sampling (no search during training) was chosen over search-augmented data generation; the authors hypothesize that game quantity matters more than data quality for learning diverse setups and posteriors
  - Training only on moves with large estimated advantages reduced wall-clock time by ~2.5x and unexpectedly improved sample efficiency and asymptotic performance
  - Setup network uses Monte Carlo returns; move network uses λ-returns (λ=0.5 advantage, λ=0.8 outcome) due to bandit-like structure of setup selection

- **Failure signatures**:
  - Entropy collapse: If regularization anneals too fast, policy entropy drops sharply, model becomes exploitable, and Elo plateaus or degrades
  - Belief network overfitting: If belief network is trained only on self-play data without dropout, it generalizes poorly to human opponents; search degrades
  - Search overfitting to network idiosyncrasies: If the reverse KL term to the move network is removed during search, performance drops below the base network (see Table 28)

- **First 3 experiments**:
  1. Ablate dynamic damping: Fix regularization coefficients and learning rate throughout training; compare Elo trajectory and entropy curves to the scheduled baseline. Expect to see either early stagnation or late collapse.
  2. Validate belief network generalization: Train belief network with and without dropout; evaluate log-likelihood on held-out trajectories from a qualitatively different policy (e.g., human games or an older checkpoint).
  3. Scale search compute: Vary rollout depth (10, 20, 40) and count (200, 500, 1000) to measure Elo gain per H100-second; identify the point of diminishing returns for your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does filtering training data based on advantage magnitude increase both sample efficiency and asymptotic performance?
- Basis in paper: Section 2.3 notes that filtering on advantage magnitude "actually increasing both sample efficiency (per environment query) and asymptotic performance," calling it a "phenomenon meriting further investigation."
- Why unresolved: Filtering typically reduces the data available for learning; the mechanism by which this specific filtering improves learning quality and efficiency is unexplained.
- What evidence would resolve it: An ablation study analyzing the distribution of learned policies with and without filtering to identify the source of the efficiency gain.

### Open Question 2
- Question: Can the test-time search capabilities be expanded to leverage arbitrary compute rather than being limited to a single update step?
- Basis in paper: Appendix K states that the search improvement is "ultimately bounded because it is mimicking a single update step" and suggests incorporating "knowledge-limited subgame solving" as a route for improvement.
- Why unresolved: The current update-equivalence framework bounds the search to a specific update operation, preventing the system from utilizing extra compute to plan deeper strategies.
- What evidence would resolve it: Implementing recursive search or subgame solving mechanisms and evaluating if performance continues to scale with additional test-time compute.

### Open Question 3
- Question: Do the dynamic damping and regularization techniques transfer effectively to other complex imperfect-information domains?
- Basis in paper: The Abstract and Conclusion suggest these innovations put practical AI within reach for "many strategic decision-making problems," implying a need to verify transferability.
- Why unresolved: The method is validated exclusively on Stratego; it is unclear if the coordinated interplay of regularization and update size is robust to different game structures or hidden information scales.
- What evidence would resolve it: Applying the training dynamics to other benchmarks (e.g., large poker variants or Fog of War Chess) and assessing the computational cost to reach high performance.

## Limitations
- The 85% win rate is achieved after 208 billion environment steps (163M games), which is substantial computational investment despite being cheaper than previous million-dollar efforts
- Belief network's performance on human opponents is reported but not thoroughly validated, raising concerns about generalization to out-of-play distributions
- The exact power-law annealing schedules for regularization coefficients and learning rate are underspecified, limiting reproducibility

## Confidence
- **High Confidence**: The empirical result of defeating the world champion (15W-4D-1L) is directly verifiable from the described match conditions. The basic architecture (setup network + move network + belief network) is clearly specified and implementable.
- **Medium Confidence**: The dynamic damping mechanism's effectiveness is supported by training curves showing entropy collapse when regularization anneals too aggressively. However, the exact power-law schedules are underspecified.
- **Low Confidence**: The claim that search degrades to below network-only performance when the KL term to the move network is removed (Elo 1733) is based on a single ablation without error bars or independent replication.

## Next Checks
1. **Dynamic Damping Ablation**: Implement training with fixed (non-annealed) regularization coefficients and learning rates. Compare Elo trajectories and policy entropy curves against the scheduled baseline. Expect to observe either early stagnation or late entropy collapse, validating the need for coordinated scheduling.

2. **Belief Network Distribution Shift**: Train belief networks with and without dropout regularization. Evaluate log-likelihood on held-out trajectories from human games or from an older policy checkpoint (qualitatively different from current self-play). Significant performance degradation would indicate overfitting risk.

3. **Search Robustness Across Policies**: Test the belief network's ability to generate useful samples when conditioned on states from different policies (self-play, human, baseline). Measure whether search performance degrades systematically as the conditioning policy diverges from training distribution, validating the assumption that belief network generalizes to out-of-distribution opponents.