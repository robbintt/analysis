---
ver: rpa2
title: Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval
arxiv_id: '2501.10175'
source_url: https://arxiv.org/abs/2501.10175
tags:
- retrieval
- domain
- arabic
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of developing a bilingual Islamic
  neural retrieval model by leveraging the XLM-R Base model and employing a language
  reduction technique to create a lightweight bilingual large language model (LLM).
  The approach combines domain adaptation using Arabic Islamic corpora and a multi-stage
  training process that incorporates both large general domain datasets and smaller
  in-domain datasets.
---

# Multi-stage Training of Bilingual Islamic LLM for Neural Passage Retrieval

## Quick Facts
- arXiv ID: 2501.10175
- Source URL: https://arxiv.org/abs/2501.10175
- Reference count: 21
- Primary result: Bilingual Islamic neural retrieval model achieves MRR@10 of 0.441 for English and 0.534 for Arabic

## Executive Summary
This study develops a bilingual neural retrieval model for Islamic texts by combining language reduction, domain adaptation, and multi-stage training. Starting from XLM-R Base, the approach prunes the vocabulary to English and Arabic only while preserving encoder weights, then adapts the model to Islamic domain through continued pre-training on religious corpora. The model is trained in two stages: first on large general-domain datasets (MS MARCO), then fine-tuned on smaller in-domain data. The resulting bilingual model outperforms monolingual baselines on downstream retrieval tasks for both languages.

## Method Summary
The approach employs three key techniques: (1) language reduction that prunes XLM-R's 100-language vocabulary to only English and Arabic by preserving encoder weights while copying intersection token embeddings, (2) domain adaptation through continued pre-training on a 100M-word Islamic corpus with 2k new domain-specific tokens initialized via subtoken averaging, and (3) multi-stage training where the model first learns general retrieval patterns from MS MARCO datasets, then refines for Islamic semantics using a combined QUQA dataset (3,252 Arabic pairs) and augmented English pairs (2,133) from Tafseer Ibn Katheer. The final model is evaluated on QRCD dataset converted to IR format using MRR@10 and Recall@100 metrics.

## Key Results
- Bilingual model achieves MRR@10 of 0.441 for English and 0.534 for Arabic
- Multi-stage training outperforms single-stage approaches across both languages
- Arabic MS MARCO surprisingly outperforms English MS MARCO as transfer language for both evaluation languages
- Data augmentation of English pairs from Tafseer corpus provides 14% MRR@10 improvement for English

## Why This Works (Mechanism)

### Mechanism 1: Language Reduction Preserves Encoder Representations While Eliminating Unused Vocabulary
- Claim: Reducing XLM-R's 100-language vocabulary to only English and Arabic decreases model size without significant performance degradation for the target task.
- Mechanism: Train a new bilingual SentencePiece tokenizer, identify intersection tokens with XLM-R's tokenizer, copy only those embedding weights to a new matrix, and preserve all encoder weights unchanged. This retains learned multilingual representations while removing unused language parameters.
- Core assumption: The target languages' representations are sufficiently captured within the intersection vocabulary tokens.
- Evidence anchors:
  - [abstract]: "employs a language reduction technique to create a lightweight bilingual large language model (LLM)"
  - [section 3.1]: "pruning only the embedding matrix while preserving all encoder weights"
  - [corpus]: Weak corpus validation; related compression work exists but doesn't directly test this specific reduction technique for retrieval
- Break condition: If Islamic domain vocabulary falls largely outside the EN-AR intersection, embedding coverage degrades and retrieval quality drops.

### Mechanism 2: Domain Adaptation via Continued Pre-training with Vocabulary Injection
- Claim: Continued pre-training on a modest (100M words) Islamic corpus with new domain-specific tokens improves downstream retrieval over the general-domain backbone.
- Mechanism: Add 2k new Islamic tokens to the embedding matrix, initialize their weights by averaging existing subtoken weights, then continue pre-training to shift representations toward Islamic text distributions.
- Core assumption: The domain shift (Classical Arabic, religious terminology) is addressable through vocabulary extension + continued pre-training rather than full retraining.
- Evidence anchors:
  - [abstract]: "domain adaptation using Arabic Islamic corpora"
  - [section 3.2]: "weights for new Islamic tokens are assigned by averaging existing weights... continue pre-training using the domain-specific corpus"
  - [corpus]: Insufficient corpus evidence; related domain adaptation papers don't validate this vocabulary injection method
- Break condition: If the adaptation corpus is too biased toward one language, cross-lingual transfer fails for the other.

### Mechanism 3: Multi-stage Training Builds General Retrieval Capability Then Refines for Domain
- Claim: Training first on large general-domain data (MS MARCO), then fine-tuning on small in-domain data yields higher MRR@10 than either stage alone.
- Mechanism: Stage 1 learns general query-passage relevance via contrastive learning on 500K+ pairs; Stage 2 adjusts the embedding space for Islamic semantic relationships using the smaller but domain-aligned dataset.
- Core assumption: General retrieval patterns transfer positively to the Islamic domain when combined with domain-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "multi-stage training process... incorporating large retrieval datasets, such as MS MARCO, and smaller, in-domain datasets"
  - [section 4.3, Tables 1-2]: XLM-R2-ID-AR-in-domain (multi-stage) achieves 0.441/0.534 MRR@10 vs. 0.329/0.416 for Stage 1 only (XLM-R2-ID-EN)
  - [corpus]: Related dense retrieval literature supports staged training but doesn't validate this specific domain combination
- Break condition: If domain shift is too extreme, Stage 2 fine-tuning causes catastrophic forgetting of Stage 1 patterns.

## Foundational Learning

- Concept: Dense Retrieval with Contrastive Learning
  - Why needed here: The paper uses bi-encoders with contrastive loss and in-batch negatives to learn query-passage embeddings; replicating this requires understanding how positive pairs are pulled together while negatives are pushed apart.
  - Quick check question: Can you explain how in-batch negatives function in contrastive learning for retrieval?

- Concept: Cross-lingual Transfer in Multilingual Models
  - Why needed here: XLM-R's cross-lingual capabilities enable knowledge sharing between English and Arabic; this underpins why the bilingual model outperforms monolingual baselines.
  - Quick check question: Why does pre-training on multiple languages improve performance on a single low-resource language task?

- Concept: Tokenizer Adaptation and Vocabulary Extension
  - Why needed here: The paper adds 2k domain-specific tokens with weight initialization via subtoken averaging; implementing this correctly requires understanding how new tokens integrate into pre-trained embeddings.
  - Quick check question: What happens to model weights when you add new tokens to a pre-trained model's vocabulary without re-initialization strategies?

## Architecture Onboarding

- Component map:
  XLM-R Base → Language Reduction → XLM-R2 (bilingual) → Domain Adaptation → XLM-R2-ID (Islamic domain) → Stage 1 (MS MARCO) → Stage 2 (QUQA + augmented EN pairs) → Final Retrieval Model

- Critical path:
  1. Language reduction: Extract EN-AR tokenizer intersection, copy corresponding embeddings, preserve encoder weights
  2. Domain adaptation: Add 2k Islamic tokens, initialize via subtoken averaging, continued pre-training on 100M-word corpus
  3. Stage 1 training: Train on MS MARCO (Arabic version outperformed English) with contrastive loss
  4. Stage 2 training: Fine-tune on combined Arabic QUQA + augmented English pairs (5,385 total)
  5. Evaluation: QRCD dataset converted to IR format, measured via MRR@10 and Recall@100

- Design tradeoffs:
  - Corpus size for adaptation: 100M words vs. full OpenITI (1B words); paper shows smaller corpus is sufficient with XLM-R warm start
  - Transfer language for Stage 1: Arabic MS MARCO (machine-translated) unexpectedly outperformed English for both evaluation languages
  - Multi-stage vs. single-stage: Multi-stage required to surpass monolingual baselines; in-domain-only or general-only both underperform

- Failure signatures:
  - XLM-R Base models (without domain adaptation) respond poorly to multi-stage training—minimal performance gain from Stage 2
  - Training only on in-domain data yields competitive but suboptimal results (MRR@10 drops from 0.441 to 0.348 for English)
  - Removing English data augmentation causes ~14% MRR@10 drop for English evaluation (ablation in Tables 3-4)
  - Monolingual baselines (all-mpnet-base-v2, CL-AraBERT) outperform most intermediate models; only full pipeline beats them

- First 3 experiments:
  1. Validate language reduction: Compare XLM-R2 against XLM-R Base on a held-out general retrieval benchmark to confirm minimal performance loss from vocabulary reduction.
  2. Ablate domain adaptation: Train retrieval from XLM-R2 (no domain adaptation) vs. XLM-R2-ID on MS MARCO only to isolate the domain adaptation contribution.
  3. Test transfer language hypothesis: Replicate the finding that Arabic MS MARCO outperforms English MS MARCO for Islamic retrieval, and test whether this pattern holds for other specialized domains or is unique to Arabic-heavy domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the language reduction and multi-stage training approach be generalized to other low-resource language pairs and domains beyond the Islamic context?
- Basis in paper: [explicit] The authors state in the Limitations section that they "conducted experiments only in English and Arabic; experiments that involve other languages may vary."
- Why unresolved: The study focused exclusively on a specific bilingual pair (Arabic-English) and a single domain (Islamic texts), leaving the broader applicability of the method untested.
- What evidence would resolve it: Replicating the XLM-R language reduction and multi-stage training pipeline for different language pairs (e.g., Urdu-English) and distinct domains (e.g., medical or legal) to observe if similar performance gains over monolingual baselines are achieved.

### Open Question 2
- Question: To what extent does the noise introduced by machine-translated datasets limit the ceiling of retrieval performance compared to human-curated data?
- Basis in paper: [explicit] The authors note that "machine translation has not yet reached the quality of expert human translation" and that the Arabic MS MARCO dataset is machine-translated, implying a potential quality bottleneck.
- Why unresolved: While the model performed well, the authors acknowledge the data was machine-translated. It remains unclear if the model's errors are due to architectural limits or translation artifacts in the training data.
- What evidence would resolve it: A comparative study evaluating the model when trained on human-translated in-domain datasets versus the current machine-translated versions to isolate the impact of translation quality on retrieval metrics.

### Open Question 3
- Question: Why does training the retrieval model on the machine-translated Arabic MS MARCO dataset yield better performance on English evaluation tasks than training on the original English MS MARCO?
- Basis in paper: [inferred] The authors observe in Section 4.3 that "training on the Arabic version of MS MARCO yields better results than training on the original English version... [despite starting] from the XLM-RBase model, which is an English-centric model."
- Why unresolved: The paper reports this counter-intuitive finding but does not provide a definitive causal explanation for why the secondary language (Arabic) acted as a superior transfer language for the domain.
- What evidence would resolve it: An analysis of the embedding spaces or token alignment between the general domain and the Islamic domain for both languages, potentially involving a probing task to determine if the Arabic translations filter out noise or align better with the semantic structures of the Islamic corpus.

## Limitations

- Language reduction generalization uncertainty: The pruning approach may not preserve performance across all domains or language pairs
- Domain adaptation corpus representativeness: The specific 100M-word Islamic corpus composition and sampling methodology remain unspecified
- Data augmentation reliability: The English pair filtering criteria ("low similarity scores") lack concrete thresholds for reproducibility

## Confidence

- Multi-stage Training Efficacy: High
- Bilingual Model Advantage: Medium
- Arabic MS MARCO Transfer: Low

## Next Checks

1. **Language Reduction Validation**: Implement the same retrieval pipeline using the full XLM-R Base model (100 languages) without vocabulary pruning. Compare MRR@10 performance to isolate whether language reduction contributes to or detracts from the reported improvements.

2. **Cross-lingual Transfer Boundary**: Train the multi-stage pipeline starting from Arabic MS MARCO versus English MS MARCO for a non-Arabic specialized domain (e.g., medical or legal texts). This would test whether the Arabic transfer advantage is domain-specific or reflects a broader cross-lingual pattern.

3. **Domain Adaptation Scale Sensitivity**: Repeat the domain adaptation stage using varying corpus sizes (10M, 50M, 200M words) from the same Islamic text sources. Plot MRR@10 against corpus size to determine whether the 100M word choice represents an optimal tradeoff or if larger corpora would yield further improvements.