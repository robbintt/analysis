---
ver: rpa2
title: 'Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline
  for Foundation-Model Cancer Risk Prediction'
arxiv_id: '2512.24294'
source_url: https://arxiv.org/abs/2512.24294
tags:
- virtual-eyes
- lung
- rad-dino
- patient-level
- sybil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Virtual-Eyes, a lung-focused 16-bit CT quality-control
  pipeline designed to improve foundation-model performance in lung cancer risk prediction.
  Unlike segmentation-based methods, Virtual-Eyes uses deterministic Hounsfield unit
  filtering and morphological cleanup to extract contiguous lung blocks while rejecting
  non-diagnostic series.
---

# Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction

## Quick Facts
- arXiv ID: 2512.24294
- Source URL: https://arxiv.org/abs/2512.24294
- Reference count: 20
- Primary result: Virtual-Eyes preprocessing improved RAD-DINO AUC from 0.646 to 0.683 on NLST lung cancer risk prediction

## Executive Summary
Virtual-Eyes is a deterministic, segmentation-free CT quality-control pipeline that extracts contiguous lung blocks using Hounsfield unit filtering and morphological cleanup. Evaluated on 765 NLST patients, Virtual-Eyes increased RAD-DINO's patient-level AUC from 0.646 to 0.683 (mean pooling) and improved calibration (Brier score 0.188→0.112), while specialist models (Sybil, ResNet-18) degraded, suggesting shortcut learning. Merlin showed limited transferability regardless of preprocessing, indicating fundamental domain mismatch. These results demonstrate that anatomically targeted QC can stabilize and improve generalist foundation models but may disrupt specialist models adapted to raw clinical context.

## Method Summary
Virtual-Eyes processes LDCT series by converting DICOM to Hounsfield units, applying HU thresholding [-950, -700], morphological cleanup, and contiguous block selection (≥20 slices). The pipeline rejects non-512×512 series or those with <64 slices. Frozen RAD-DINO and Merlin embeddings are extracted and fed to MLP classifiers (2 FC layers, ReLU, dropout, sigmoid), while Sybil and ResNet-18 are applied directly. Performance is measured via patient-level AUC (mean/max pooling) and Brier score on a 765-patient NLST test set (36 cancer, 117 non-cancer).

## Key Results
- RAD-DINO AUC improved from 0.646 to 0.683 (mean pooling) and 0.619 to 0.735 (max pooling) with Virtual-Eyes preprocessing
- Sybil and ResNet-18 performance degraded under Virtual-Eyes, suggesting shortcut learning on raw clinical context
- Merlin showed near-random performance regardless of preprocessing due to abdominal-CT pretraining domain mismatch
- Calibration improved significantly (Brier score 0.188→0.112 for RAD-DINO with mean pooling)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic HU-based lung extraction improves generalist foundation model performance by reducing irrelevant visual context
- Mechanism: Virtual-Eyes removes non-lung anatomy (neck, abdomen, scanner tables) through HU thresholding [-950, -700] and morphological cleanup. This focuses the generalist encoder's attention on diagnostically relevant parenchyma rather than treating all visual context as equally informative.
- Core assumption: Generalist FMs trained on heterogeneous anatomies benefit from anatomically constrained inputs at inference time.
- Evidence anchors:
  - [abstract] "Unlike segmentation-based methods, Virtual-Eyes uses deterministic Hounsfield unit filtering and morphological cleanup to extract contiguous lung blocks"
  - [section 3.1] "Virtual-Eyes acts as an anatomical focusing mechanism that removes irrelevant context and simplifies the visual field to the lung parenchyma"
  - [corpus] Related work on fairness evaluation of lung cancer risk models (arxiv 2512.22242) notes variability across populations, but does not directly test preprocessing effects on FMs.
- Break condition: If target FM was pretrained primarily on lung-focused data, anatomical filtering may provide no additional benefit or could remove useful pericapsular context.

### Mechanism 2
- Claim: Aggressive preprocessing can expose shortcut learning in specialist models trained on raw clinical volumes
- Mechanism: Sybil and ResNet-18 were trained end-to-end on raw NLST volumes containing scanner tables, cables, and variable scan ranges. When Virtual-Eyes removes these contextual cues, performance degrades (Sybil AUC 0.886→0.837; ResNet-18 shows large distributional shift with KS D=0.317), suggesting the models had partially relied on non-causal features.
- Core assumption: Performance degradation after controlled context removal indicates prior reliance on spurious correlations rather than causal lung features.
- Evidence anchors:
  - [abstract] "Sybil and ResNet-18 degraded under Virtual-Eyes, indicating shortcut learning or context dependence"
  - [section 3.2] "Virtual-Eyes removes many of these contextual cues, exposing the fragility of shortcut-driven solutions"
  - [corpus] "Auditing Sybil" (arxiv 2602.02560) examines explainability of Sybil predictions but does not specifically isolate preprocessing as an audit mechanism.
- Break condition: If specialist model was explicitly trained with data augmentation that includes aggressive cropping/random erasing of context, degradation may be minimal.

### Mechanism 3
- Claim: Anatomical preprocessing cannot overcome fundamental pretraining domain mismatch
- Mechanism: Merlin, pretrained primarily on abdominal CT, shows near-random performance on thoracic LDCT regardless of preprocessing (AUC ~0.507–0.567). The large KS distance (D=0.708) between raw and preprocessed outputs indicates Virtual-Eyes reshapes the feature distribution but does not unlock predictive value.
- Core assumption: Domain-matched pretraining data is necessary; input-level anatomical alignment alone is insufficient for cross-domain transfer.
- Evidence anchors:
  - [abstract] "Merlin shows limited transferability regardless of preprocessing"
  - [section 3.3] "Virtual-Eyes substantially reshaped the score distribution without unlocking additional predictive value"
  - [corpus] No direct corpus evidence on cross-anatomy FM transfer limitations; this remains an open research question.
- Break condition: If cross-anatomy FM receives targeted fine-tuning on lung data (not just frozen embedding extraction), transfer may improve.

## Foundational Learning

- Concept: **Hounsfield Unit (HU) filtering**
  - Why needed here: Virtual-Eyes relies on canonical lung parenchymal range [-950, -700] HU; understanding this scale is essential for debugging threshold choices.
  - Quick check question: Can you explain why -950 HU corresponds to aerated lung tissue while +1000 HU corresponds to bone?

- Concept: **Shortcut learning in medical imaging**
  - Why needed here: The paper's core claim about specialist model degradation depends on understanding how models exploit spurious correlations (scanner artifacts, patient positioning) rather than disease features.
  - Quick check question: If a model trained on chest X-rays achieves high accuracy but fails when tested on images from a different hospital, what type of failure mode might this indicate?

- Concept: **Frozen encoder + trainable head paradigm**
  - Why needed here: RAD-DINO and Merlin are evaluated with frozen backbones and lightweight MLP classifiers; this design isolates preprocessing effects from representation learning.
  - Quick check question: Why might frozen encoder evaluation show smaller gains than full fine-tuning, and what does this mean for interpreting the paper's AUC improvements?

## Architecture Onboarding

- Component map: DICOM series discovery -> z-axis sorting -> HU conversion -> per-slice lung mask generation -> contiguous block selection -> save lung block as 16-bit NumPy volume -> frozen encoder embeddings -> MLP head -> patient-level aggregation
- Critical path: 1. DICOM series discovery and z-axis sorting 2. HU conversion via RescaleSlope/RescaleIntercept 3. Per-slice lung mask generation (HU threshold → morphology) 4. Contiguous block selection (longest run ≥20 slices) 5. Save lung block as 16-bit NumPy volume
- Design tradeoffs:
  - Strict 512×512 requirement ensures consistency but may exclude valid non-standard acquisitions
  - Deterministic rules vs learned segmentation: Avoids model-induced false positives/negatives but requires manual threshold tuning for new datasets
  - Min block size = 20: Empirically tuned on NLST; may need adjustment for different slice thicknesses or pediatric populations
  - 5% lung-area ratio threshold: Dataset-dependent; authors explicitly note re-validation needed for new cohorts
- Failure signatures:
  - All series rejected: Likely indicates HU window mismatch (e.g., contrast-enhanced protocols) or non-thoracic studies
  - Sybil sensitivity drops after preprocessing: Model was relying on contextual shortcuts; consider retraining rather than direct inference
  - Merlin near-random performance: Pretraining domain mismatch; require anatomy-matched encoder or fine-tuning
  - Large KS distance between raw/preprocessed outputs (D>0.3): Suggests preprocessing fundamentally changes what the model sees; investigate for shortcut exposure
- First 3 experiments:
  1. Reproduce RAD-DINO improvement on held-out NLST split: Train MLP head on frozen RAD-DINO embeddings from Virtual-Eyes lung blocks; verify AUC improvement from ~0.65 to ~0.68+ and calibration improvement (Brier score reduction)
  2. Ablate individual QC components: Disable each filter sequentially (HU window, block length, lung-area ratio) to quantify contribution of each step to final performance
  3. Test generalization to external cohort (e.g., NLST to independent LDCT dataset): Apply Virtual-Eyes thresholds unchanged; measure whether preprocessing gains transfer or if threshold retuning is required

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning foundation-model encoders on Virtual-Eyes–preprocessed inputs yield larger performance gains than frozen-embedding approaches?
- Basis in paper: [explicit] "Systematically comparing frozen versus fine-tuned models under controlled preprocessing is therefore a key direction for future work."
- Why unresolved: All experiments used frozen RAD-DINO and Merlin encoders with lightweight MLP heads; no end-to-end fine-tuning was performed.
- What evidence would resolve it: Repeat experiments with encoder fine-tuning on Virtual-Eyes versus raw inputs, measuring AUC and calibration deltas.

### Open Question 2
- Question: How well does Virtual-Eyes generalize to external cohorts with different scanners, reconstruction kernels, and institutional protocols?
- Basis in paper: [explicit] The authors acknowledge that empirically tuned thresholds "are dataset- and protocol-dependent and will need to be re-validated and potentially adjusted when Virtual-Eyes is deployed on newer cohorts or institutions."
- Why unresolved: Only NLST data was evaluated; no external validation was performed.
- What evidence would resolve it: Apply Virtual-Eyes (with and without threshold retuning) to independent LDCT datasets (e.g., NLST-equivalent international cohorts) and report QC rejection rates and downstream model performance.

### Open Question 3
- Question: Is the performance degradation in Sybil and ResNet-18 under Virtual-Eyes caused by shortcut reliance on non-lung context, or by removal of diagnostically relevant perithoracic information?
- Basis in paper: [inferred] The paper concludes "shortcut learning" based on performance collapse and distributional shifts, but does not directly identify which features the models used or whether removed anatomy (e.g., mediastinum, chest wall) contained true signal.
- Why unresolved: Shortcut attribution is inferred from KS distance and AUC changes; no feature-attribution or controlled-ablation study was conducted.
- What evidence would resolve it: Use saliency methods or synthetic-perturbation experiments to identify which anatomical regions contribute to Sybil/ResNet-18 predictions under raw versus Virtual-Eyes inputs.

### Open Question 4
- Question: Can HU-based, segmentation-free QC pipelines similar to Virtual-Eyes improve foundation-model performance in other CT screening domains (e.g., liver, colon, cardiac)?
- Basis in paper: [explicit] "Similar HU-based QC strategies could plausibly benefit other screening domains—such as liver, colon, or cardiac CT—provided their thresholds are re-tuned to match local imaging protocols."
- Why unresolved: Virtual-Eyes was designed and validated only for lung LDCT; no experiments in other organs were reported.
- What evidence would resolve it: Construct anatomy-specific HU filtering rules for liver/colon/cardiac CT, then evaluate FM performance with and without domain-tuned QC on relevant screening datasets.

## Limitations

- The paper demonstrates that anatomical preprocessing improves foundation model performance but does not establish causal mechanisms for all observed effects
- Cross-anatomy transferability limitations of Merlin are well-demonstrated but not systematically explored across multiple foundation models or fine-tuning strategies
- The degradation of specialist models under Virtual-Eyes preprocessing could alternatively indicate legitimate context dependency rather than spurious correlation reliance

## Confidence

- High confidence: RAD-DINO AUC improvement from 0.646 to 0.683 with Virtual-Eyes preprocessing (p < 0.05 via DeLong's test) - supported by direct experimental comparison on held-out test set
- Medium confidence: Specialist model degradation indicates shortcut learning - while the distributional shift is measurable (KS D > 0.3), alternative interpretations exist regarding context dependency versus spurious correlation reliance
- Medium confidence: Merlin's poor performance is due to domain mismatch - the near-random performance is clear, but the paper does not test whether fine-tuning or domain adaptation could improve results

## Next Checks

1. **Ablation study of preprocessing components**: Disable each Virtual-Eyes filter (HU window, block length, lung-area ratio) sequentially to quantify individual contribution to RAD-DINO's performance improvement and verify that HU-based filtering specifically drives the gains

2. **External cohort generalization**: Apply Virtual-Eyes thresholds unchanged to an independent LDCT dataset (e.g., Danish Lung Cancer Screening Trial) to determine whether preprocessing benefits transfer across populations or require dataset-specific recalibration

3. **Shortcut learning verification**: Retrain Sybil with augmented training data containing artificially cropped/removed scanner tables and context, then compare whether Virtual-Eyes preprocessing still degrades performance to distinguish between shortcut learning and legitimate context dependency