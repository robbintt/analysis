---
ver: rpa2
title: Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion
  Classification
arxiv_id: '2508.11511'
source_url: https://arxiv.org/abs/2508.11511
tags:
- ensemble
- learning
- performance
- proposed
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-supervised learning approach that
  combines ensemble learning with online knowledge distillation for skin lesion classification.
  The method trains multiple convolutional neural network models simultaneously, using
  online knowledge distillation to transfer the ensemble's collective knowledge to
  each individual model, thereby improving both individual and overall ensemble performance.
---

# Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification

## Quick Facts
- arXiv ID: 2508.11511
- Source URL: https://arxiv.org/abs/2508.11511
- Reference count: 33
- Improves skin lesion classification performance by up to 4% F1-score using semi-supervised learning with ensemble knowledge distillation

## Executive Summary
This paper introduces a semi-supervised learning approach that combines ensemble learning with online knowledge distillation for skin lesion classification. The method trains multiple convolutional neural network models simultaneously, using online knowledge distillation to transfer the ensemble's collective knowledge to each individual model, thereby improving both individual and overall ensemble performance. A pseudo-labeling strategy is employed to leverage unlabeled data, progressively expanding the labeled dataset through high-confidence pseudo-labels. Experiments on the ISIC 2018 and ISIC 2019 benchmark datasets demonstrate superior performance compared to state-of-the-art methods, with improvements of up to 4% in F1-score. The approach reduces the need for extensive labeled data while maintaining computational efficiency through knowledge distillation, allowing any individual model to be deployed at test time with performance comparable to the full ensemble.

## Method Summary
The method combines ensemble learning with online knowledge distillation in a semi-supervised framework. Multiple ResNet-50 models are trained simultaneously, each learning from both ground-truth labels and softened logits from the ensemble's average predictions. A pseudo-labeling strategy progressively expands the labeled dataset by adding high-confidence predictions from unlabeled data. The ensemble approach allows each model to benefit from the collective knowledge of the group, while knowledge distillation ensures individual models can perform well independently. This is particularly valuable for medical imaging where labeled data is scarce and computational resources may be limited. The method achieves superior performance compared to both supervised and semi-supervised baselines while requiring less labeled data.

## Key Results
- Achieves up to 4% improvement in F1-score compared to state-of-the-art methods on ISIC datasets
- Maintains strong performance with only 2-20% labeled data through effective pseudo-labeling
- Enables deployment of any individual ensemble member with performance close to the full ensemble
- Demonstrates robustness across both ISIC 2018 (7 classes) and ISIC 2019 (8 classes) datasets

## Why This Works (Mechanism)
The approach works by leveraging ensemble diversity and knowledge distillation to create more robust models with less labeled data. Multiple models trained with different random seeds and augmentations capture diverse perspectives on the data. Online knowledge distillation allows each model to learn from the ensemble's collective wisdom in real-time, improving individual model performance beyond what supervised learning alone could achieve. The pseudo-labeling strategy then systematically incorporates unlabeled data by selecting only high-confidence predictions, expanding the training set while minimizing label noise. This creates a positive feedback loop where better models generate better pseudo-labels, which in turn train even better models. The combination addresses both the data scarcity problem in medical imaging and the computational overhead of deploying full ensembles.

## Foundational Learning

**Knowledge Distillation**: Transferring knowledge from a larger model (teacher) to a smaller model (student) by matching softened probability distributions. Why needed: Enables individual models to learn from ensemble knowledge. Quick check: Verify softened logits use appropriate temperature.

**Pseudo-Labeling**: Using model predictions on unlabeled data as training labels when confidence exceeds a threshold. Why needed: Expands training data without manual labeling. Quick check: Monitor pseudo-label acceptance rate and class distribution.

**Ensemble Learning**: Training multiple models and combining their predictions. Why needed: Reduces variance and captures diverse decision boundaries. Quick check: Verify individual model performance improves with ensemble training.

**Semi-Supervised Learning**: Learning from both labeled and unlabeled data. Why needed: Addresses label scarcity in medical imaging. Quick check: Compare performance with varying labeled data fractions.

## Architecture Onboarding

**Component Map**: Input Images -> Data Augmentation -> K ResNet-50 Models -> Ensemble Averaging -> Softened Logits -> Knowledge Distillation Loss -> Combined Loss -> Model Updates

**Critical Path**: Labeled data -> Supervised training -> Pseudo-label generation -> Unlabeled data incorporation -> Iterative refinement -> Final model deployment

**Design Tradeoffs**: Ensemble size vs. computational cost, pseudo-label threshold vs. dataset expansion rate, knowledge distillation temperature vs. information retention

**Failure Signatures**: Model collapse to majority classes (class imbalance), confirmation bias from noisy pseudo-labels, overfitting to limited labeled data

**Three First Experiments**:
1. Train single ResNet-50 with varying labeled data percentages (2%, 10%, 100%) to establish baseline
2. Implement K=3 ensemble training with knowledge distillation, compare to single model
3. Add pseudo-labeling with different confidence thresholds (0.90, 0.95, 0.99) to measure dataset expansion impact

## Open Questions the Paper Calls Out

**Open Question 1**: Does employing a heterogeneous ensemble of diverse CNN architectures (e.g., ResNet, DenseNet, EfficientNet) yield greater performance improvements than the homogeneous ResNet-50 ensemble utilized in this study? The paper hypothesizes that diversity improves results but limits validation to a single architecture type.

**Open Question 2**: Can the pseudo-labeling process be further optimized by using an adaptive or curriculum-based confidence threshold τ, rather than relying on a fixed threshold? The paper uses fixed thresholds and notes manual tuning requirements.

**Open Question 3**: How effectively does the proposed online knowledge distillation framework perform when applied to lightweight neural network backbones (e.g., MobileNet) intended for mobile deployment? The paper benchmarks only on ResNet-50 despite claiming benefits for resource-constrained environments.

## Limitations
- Missing critical hyperparameters: initial learning rate, knowledge distillation temperature, and exact number of SSL iterations
- Limited architectural diversity in ensemble (only ResNet-50 models)
- Pseudo-labeling relies on fixed confidence thresholds requiring manual tuning
- Computational overhead of training multiple models simultaneously

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Overall methodology is sound | High |
| Experimental setup on ISIC datasets | Medium |
| Critical hyperparameters specified | Low |

## Next Checks

1. Verify class imbalance handling by confirming that per-class recalls are balanced and that BAcc improvements are not simply due to majority class performance

2. Perform sensitivity analysis on the pseudo-labeling threshold τ by testing values from 0.90 to 0.99 and measuring the trade-off between pseudo-label quantity and quality

3. Conduct ablation studies to isolate the contributions of ensemble learning, knowledge distillation, and pseudo-labeling components to the overall performance gains