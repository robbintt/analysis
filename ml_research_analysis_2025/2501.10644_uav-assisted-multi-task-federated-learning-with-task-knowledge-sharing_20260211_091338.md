---
ver: rpa2
title: UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing
arxiv_id: '2501.10644'
source_url: https://arxiv.org/abs/2501.10644
tags:
- task
- tasks
- each
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a UAV-assisted multi-task federated learning
  scheme to train multiple related tasks simultaneously using data collected by UAVs.
  The core idea involves sharing feature extractors across related tasks while maintaining
  task-specific predictors, and introducing a task attention mechanism to balance
  task performance and encourage knowledge sharing.
---

# UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing

## Quick Facts
- arXiv ID: 2501.10644
- Source URL: https://arxiv.org/abs/2501.10644
- Reference count: 16
- One-line primary result: UAV-assisted multi-task federated learning with task knowledge sharing improves multi-task performance by 3.2% accuracy over baseline strategies.

## Executive Summary
This paper proposes a UAV-assisted multi-task federated learning scheme where UAVs collect data for multiple related tasks, sharing feature extractors while maintaining task-specific predictors. The approach combines a task attention mechanism for balancing task performance with a coalition formation game for UAV-EV association, and derives optimal bandwidth allocation for limited communication resources. Simulation results on modified MNIST data demonstrate improved accuracy (3.2% over baseline) and better performance in non-IID data scenarios compared to independent task training or random association strategies.

## Method Summary
The scheme trains M related tasks simultaneously across N UAVs by sharing a convolutional feature extractor while maintaining task-specific fully-connected layers. UAVs first associate with EVs (one per task) using a coalition formation game that maximizes weighted data contribution minus round completion time. Each UAV performs K local SGD steps on its assigned task, uploads cumulative gradients, and EVs aggregate these before exchanging feature extractor parameters. A task attention mechanism weights each task's importance based on training progress and marginal contribution to others. Bandwidth is optimally allocated across UAVs to minimize communication time under energy constraints.

## Key Results
- The proposed scheme achieves 3.2% higher final accuracy compared to random UAV-EV association strategies
- Feature sharing across related tasks improves accuracy by 4.5% compared to no sharing
- Better performance in non-IID data scenarios where traditional federated learning struggles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sharing feature extractors across related tasks improves model robustness and accelerates convergence compared to independent task training.
- **Mechanism:** Related tasks (e.g., digit recognition and rotation angle recognition on the same images) require similar low-level features (edges, shapes, contours). A shared convolutional backbone learns from data across all UAVs and tasks, exposing the extractor to a broader distribution of inputs. This mutual reinforcement reduces overfitting to any single task's data distribution.
- **Core assumption:** Tasks must share underlying feature correlations; unrelated tasks would not benefit and may interfere.
- **Evidence anchors:**
  - [abstract] "sharing feature extractors across related tasks while maintaining task-specific predictors"
  - [section V-C] "Strategy 1 shows a 4.5% accuracy improvement compared to Strategy 3 [no sharing]"
  - [corpus] Neighbor paper "Multi-task Federated Learning with Encoder-Decoder Structure" confirms collaborative learning across different tasks via shared structures.
- **Break condition:** Tasks become unrelated (negative transfer), or non-IID data heterogeneity exceeds the shared extractor's capacity to generalize.

### Mechanism 2
- **Claim:** The task attention mechanism dynamically balances multi-task performance by weighting tasks based on training progress and marginal contribution.
- **Mechanism:** Two signals are combined: (1) loss-based weights prioritize slower-converging tasks via exponential smoothing of cumulative loss; (2) Shapley value-based weights quantify each task's contribution to improving other tasks through feature extractor fusion. The combined weight α_m,t modulates each task's influence on UAV-EV association decisions.
- **Core assumption:** Tasks with higher historical loss or higher marginal contribution to others should receive more computational resources; this assumes diminishing returns in neural network training.
- **Evidence anchors:**
  - [section IV-A] "the weight of each task in round t based on the task attention mechanism as Ψm,t"
  - [section III-B] "improving overall task performance calls for greater emphasis on tasks with slower training progress"
  - [corpus] Weak direct evidence; neighbor papers do not explicitly address Shapley-based task weighting.
- **Break condition:** Loss signals become noisy (high variance gradients), or Shapley computation becomes infeasible for large M (exponential complexity in coalition enumeration).

### Mechanism 3
- **Claim:** Coalition formation game-based UAV-EV association achieves higher network utility than random or distance-based association.
- **Mechanism:** UAVs iteratively adjust associations through "transfer" (leaving one EV for another) or "exchange" (swapping EVs with another UAV) operations. Each move is accepted only if global utility U(S) increases. The process terminates at a Nash-stable partition where no unilateral deviation improves utility.
- **Core assumption:** The utility function in Eq. (13) correctly captures the tradeoff between weighted data contribution and round completion time; UAVs act rationally to maximize global (not local) utility.
- **Evidence anchors:**
  - [section IV-C] "the association policy adjustment will terminate when a stable association S* is reached"
  - [section V-C] "compared to Strategy 1 [random association], the proposed algorithm achieves a 3.2% improvement in final accuracy"
  - [corpus] "AirFed" neighbor paper uses multi-agent RL for UAV coordination, suggesting game-theoretic approaches are actively explored.
- **Break condition:** Utility landscape has many local optima (suboptimal stable partitions), or communication overhead for coordination dominates gains.

## Foundational Learning

- **Concept:** Federated Averaging (FedAvg)
  - **Why needed here:** The scheme builds on FedAvg's local SGD + gradient aggregation, but extends it to multi-task learning with shared parameters.
  - **Quick check question:** Can you explain how local gradients are aggregated in Eq. (4)-(5) and why cumulative gradients are uploaded instead of raw weights?

- **Concept:** Multi-Task Learning (MTL) with Hard Parameter Sharing
  - **Why needed here:** Understanding the split between shared feature extractors (ws) and task-specific predictors (wu) is essential for implementing the architecture.
  - **Quick check question:** Why might hard parameter sharing cause negative transfer between unrelated tasks?

- **Concept:** Coalition Formation Games
  - **Why needed here:** The UAV-EV association algorithm relies on stability concepts (Nash equilibrium, Pareto improvement) from cooperative game theory.
  - **Quick check question:** What is the difference between a "transfer" and an "exchange" operation, and when does each apply?

## Architecture Onboarding

- **Component map:** UAV (data Dn, local SGD) -> cumulative gradient upload -> EV (task m) -> gradient aggregation -> feature fusion with other EVs -> model broadcast
- **Critical path:**
  1. UAV-EV association (coalition game, ~O(N·M) iterations)
  2. Model broadcast (EV → UAVs)
  3. Local training (K SGD steps)
  4. Gradient upload (bandwidth-allocated FDMA)
  5. EV-level aggregation + cross-EV feature fusion
  6. Task attention weight update for next round

- **Design tradeoffs:**
  - **More UAVs per task:** Better feature diversity but longer round time (T_max bottleneck)
  - **Higher K (local steps):** Reduced communication frequency but increased gradient staleness and divergence risk
  - **Larger shared backbone:** Better transfer but higher communication cost (Q_m in Eq. (17))

- **Failure signatures:**
  - **Task divergence:** One task's loss plateaus while others improve → check α_m,t weighting, may need to increase loss-based attention
  - **Association instability:** UAVs oscillate between EVs without convergence → verify utility monotonicity, check δ_m minimum constraint
  - **Bandwidth allocation failure:** T*_t has no feasible solution → energy constraints (6a) may be too tight for current UAV distribution

- **First 3 experiments:**
  1. **Ablation on feature sharing:** Compare shared vs. independent extractors on the modified MNIST setup (replicate Strategy 1 vs. Strategy 3). Expect ~4-5% gap
  2. **Non-IID stress test:** Vary α_2 (Dirichlet concentration) from 0.1 to 10. Confirm performance improvement degrades as data becomes more IID (Figure 3a trend)
  3. **Scalability check:** Fix M=2, scale N from 10 to 50. Measure total training time and final accuracy; identify whether T_max or association overhead dominates

## Open Questions the Paper Calls Out

- **Question:** How can task correlations be effectively quantified to theoretically determine the optimal degree of feature extractor sharing?
- **Basis in paper:** [explicit] The authors state in the Conclusion that they "will investigate effective methods for quantifying task correlations" to improve the scheme
- **Why unresolved:** The current work assumes related tasks (e.g., digit and rotation recognition on MNIST) but lacks a formal metric or theoretical bound defining the minimum correlation required for sharing to be beneficial versus detrimental (negative transfer)
- **What evidence would resolve it:** A derived mathematical relationship or metric between task correlation (e.g., cosine similarity of gradients) and convergence speed, validated across datasets with varying degrees of task relatedness

- **Question:** What collaboration strategies are optimal when the relevance between tasks fluctuates dynamically over time?
- **Basis in paper:** [explicit] The authors propose to "design strategies to encourage collaboration in scenarios with dynamic task relevance" as a future direction
- **Why unresolved:** The current model assumes a static relationship between tasks (e.g., feature sharing is always active), whereas dynamic relevance would require the attention mechanism to potentially "turn off" sharing for specific task pairs mid-training
- **What evidence would resolve it:** An adaptive sharing mechanism that modifies the aggregation weights based on real-time task relevance metrics, showing improved resilience to concept drift compared to the static sharing method

- **Question:** How does the system performance change when assuming non-negligible communication latency and energy consumption for parameter exchange between EVs?
- **Basis in paper:** [inferred] In Section II.A, the paper states, "Given that EVs possess relatively sufficient transmission resources... we disregard any delays and energy consumption associated with this parameter exchange"
- **Why unresolved:** In dense or disaster-struck environments, backhaul links between Ground EVs may be congested or energy-constrained. Ignoring these costs assumes an idealized infrastructure that may not hold, potentially skewing the optimal bandwidth and association strategies derived in Section IV
- **What evidence would resolve it:** A modified optimization problem including EV-to-EV communication costs in the total time/energy utility function, demonstrating the trade-off between knowledge sharing frequency and system latency

## Limitations

- Shapley value computation cost is not quantified; exact implementation requires O(2^M) coalition evaluations which may become infeasible for larger task sets
- Local SGD step count K per round is not specified, affecting convergence analysis and comparison with baselines
- Convolutional and fully-connected layer dimensions are omitted, preventing exact architectural replication

## Confidence

- **High Confidence:** Feature sharing mechanism improves multi-task performance (supported by 4.5% accuracy gain vs. no sharing)
- **Medium Confidence:** Task attention mechanism effectively balances task priorities (limited evidence; Shapley-based weighting not detailed in corpus)
- **Medium Confidence:** Coalition formation game achieves stable UAV-EV associations (no theoretical proof of convergence provided)

## Next Checks

1. Implement exact vs. approximate Shapley value computation and measure per-round overhead; verify marginal contribution estimates remain stable with sampling
2. Run ablation study varying K from 1 to 20 local steps; identify optimal range where communication savings outweigh gradient staleness
3. Test coalition formation algorithm on random utility landscapes with multiple local optima; measure frequency of suboptimal stable partitions