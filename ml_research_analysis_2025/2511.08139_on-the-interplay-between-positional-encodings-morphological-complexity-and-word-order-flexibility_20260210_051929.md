---
ver: rpa2
title: On the Interplay between Positional Encodings, Morphological Complexity, and
  Word Order Flexibility
arxiv_id: '2511.08139'
source_url: https://arxiv.org/abs/2511.08139
tags:
- order
- language
- encodings
- word
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of positional encodings in language
  models, particularly focusing on how they interact with morphological complexity
  and word order flexibility across diverse languages. The authors challenge previous
  findings by Ghosh et al.
---

# On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility

## Quick Facts
- **arXiv ID**: 2511.08139
- **Source URL**: https://arxiv.org/abs/2511.08139
- **Reference count**: 29
- **Primary result**: Positional encodings are crucial for learning syntax across languages, with effects primarily task-specific rather than language-specific

## Executive Summary
This paper investigates the role of positional encodings in language models across typologically diverse languages. The authors challenge previous findings that suggested morphological complexity negatively impacts the utility of positional encodings. Through systematic experiments with seven languages and three model variants (no positional encodings, absolute positional encodings, and relative positional encodings), they demonstrate that positional encodings are crucial for syntax learning regardless of morphological complexity or word order flexibility. The study reveals that the impact of positional encodings is primarily task-specific rather than language-specific, with relative positional encodings generally outperforming absolute ones.

## Method Summary
The authors systematically selected seven typologically diverse languages spanning different morphological complexities and word order flexibilities. For each language, they trained three model variants from scratch: one without positional encodings, one with absolute positional encodings, and one with relative positional encodings. These models were then fine-tuned on four downstream tasks: dependency parsing, named entity recognition, sentence classification, and grammaticality judgment. The morphological complexity was carefully measured using type-token ratio as a proxy, and the results were analyzed to determine the relationship between positional encoding utility and language characteristics.

## Key Results
- Positional encodings are crucial for learning syntax across all languages studied, regardless of morphological complexity or word order flexibility
- Relative positional encodings generally outperform absolute positional encodings across tasks and languages
- The impact of positional encodings is primarily task-specific rather than language-specific
- Previous conclusions based on type-token ratio as a proxy for morphological complexity may be misleading due to confounding factors

## Why This Works (Mechanism)
The effectiveness of positional encodings stems from their ability to provide sequential information that helps models disambiguate word order and syntactic structure, particularly in languages where morphology alone cannot fully resolve ambiguity. Relative positional encodings capture pairwise relationships between tokens, making them more flexible for languages with varying word order patterns. The task-specific effects arise because different downstream tasks place different demands on syntactic understanding and sequential processing.

## Foundational Learning
1. **Morphological Complexity** - Understanding how morphological richness affects word order flexibility and syntactic ambiguity; needed to contextualize why some languages might rely more on positional information
2. **Positional Encoding Types** - Familiarity with absolute vs. relative positional encodings and their mathematical formulations; needed to understand the architectural choices and their implications
3. **Typological Diversity** - Knowledge of language classification systems and how morphological and syntactic features vary across languages; needed to appreciate the experimental design and generalizability of results
4. **Downstream Task Requirements** - Understanding how different NLP tasks (parsing, NER, classification, grammaticality) leverage positional and morphological information differently; needed to interpret task-specific findings
5. **Type-Token Ratio** - Understanding this lexical diversity metric as a proxy for morphological complexity; needed to evaluate the validity of the complexity measure used
6. **Cross-linguistic Experimental Design** - Familiarity with designing controlled experiments across multiple languages; needed to assess the methodological rigor

## Architecture Onboarding

**Component Map**: Token Embeddings -> Positional Encodings (None/Absolute/Relative) -> Transformer Layers -> Output Layer

**Critical Path**: The interaction between positional encodings and transformer attention mechanisms is the critical path, as positional information directly influences how attention weights are computed between tokens.

**Design Tradeoffs**: The study examines the tradeoff between architectural simplicity (no positional encodings) and performance gains from adding positional information, while also comparing the relative benefits of absolute versus relative encodings in capturing sequential dependencies.

**Failure Signatures**: Models without positional encodings fail to capture word order information, leading to poor performance on syntax-dependent tasks like dependency parsing and grammaticality judgment, regardless of the language's morphological complexity.

**First Experiments**:
1. Train a model with no positional encodings on a morphologically rich language and evaluate on dependency parsing
2. Compare absolute and relative positional encodings on a fixed word order language for named entity recognition
3. Test all three variants on a morphologically simple language with flexible word order for sentence classification

## Open Questions the Paper Calls Out
The study acknowledges that its conclusions are based on experiments with only seven typologically diverse languages, which may not capture the full range of linguistic variation. The relationship between the morphological complexity proxy used and other potential confounds remains unclear. The downstream tasks, while covering different linguistic phenomena, may not fully represent all ways positional encodings could impact language modeling performance. Additionally, the paper does not explore other types of positional encodings beyond absolute and relative, nor does it examine the impact of positional encoding dimensions or initialization strategies.

## Limitations
- Limited to seven typologically diverse languages, potentially missing extreme cases of morphological complexity or word order rigidity
- Reliance on type-token ratio as a morphological complexity proxy, which may not capture all relevant linguistic features
- Downstream tasks may not fully represent the diverse ways positional encodings could impact language modeling performance
- Only examines absolute and relative positional encodings, not exploring other variants like rotary embeddings
- Does not investigate the impact of positional encoding dimensions or initialization strategies

## Confidence
- **High confidence**: Positional encodings are crucial for learning syntax across languages, regardless of morphological complexity or word order flexibility
- **Medium confidence**: Relative positional encodings generally outperform absolute encodings, as this may be task-dependent
- **Medium confidence**: Task specificity outweighs language specificity in the impact of positional encodings, given the limited number of languages and tasks studied

## Next Checks
1. Replicate the study with a broader typological sample including languages with extreme morphological complexity (e.g., polysynthetic languages) and rigid word order (e.g., strict V2 languages)
2. Test additional positional encoding variants (e.g., rotary positional embeddings, complex-valued embeddings) to determine if the relative superiority of RPE holds across encoding types
3. Conduct controlled experiments varying positional encoding dimension and initialization to isolate their effects from the encoding type itself