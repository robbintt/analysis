---
ver: rpa2
title: 'DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration'
arxiv_id: '2601.20627'
source_url: https://arxiv.org/abs/2601.20627
tags:
- rashomon
- diverse
- film
- latent
- cma-es
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIVERSE is a gradient-free method for exploring the Rashomon set
  of deep neural networks by augmenting a pretrained model with Feature-wise Linear
  Modulation (FiLM) layers and using Covariance Matrix Adaptation Evolution Strategy
  (CMA-ES) to search a latent modulation space. This approach generates diverse model
  variants without retraining or gradient access.
---

# DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration

## Quick Facts
- arXiv ID: 2601.20627
- Source URL: https://arxiv.org/abs/2601.20627
- Authors: Gilles Eerlings; Brent Zoomers; Jori Liesenborgs; Gustavo Rovelo Ruiz; Kris Luyten
- Reference count: 40
- Primary result: Gradient-free Rashomon set exploration via FiLM augmentation and CMA-ES search, achieving comparable diversity to retraining at reduced computational cost

## Executive Summary
DIVERSE introduces a gradient-free method for exploring the Rashomon set of deep neural networks by augmenting pretrained models with Feature-wise Linear Modulation (FiLM) layers and using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space. This approach generates diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models, achieving comparable diversity to retraining at reduced computational cost. The method demonstrates robustness to hyperparameter settings and shows that diversity can be localized to specific FiLM layers, suggesting opportunities for targeted search.

## Method Summary
DIVERSE augments a frozen pretrained model with FiLM layers that apply bounded affine transformations (γ⊙h + β) to pre-activations, where γ and β are projected from a shared latent vector z. The search space is explored using CMA-ES, which samples z vectors from an adaptive multivariate Gaussian distribution and evaluates a dual-objective fitness function combining disagreement maximization with a soft accuracy penalty. The method requires no gradient information and preserves the original model when z=0. Search results are validated against a held-out set to ensure Rashomon constraint satisfaction.

## Key Results
- DIVERSE achieves comparable diversity to retraining while requiring only 80·d fitness evaluations versus full retraining cycles
- The method succeeds across three diverse architectures (MLP, ResNet-50, VGG-16) and datasets (MNIST, PneumoniaMNIST, CIFAR-10)
- Sensitivity analysis reveals that diversity is localized to specific FiLM layers, suggesting opportunities for targeted search
- Robustness to hyperparameter choices is demonstrated through comprehensive ablation studies

## Why This Works (Mechanism)

### Mechanism 1: FiLM-Based Latent Modulation Space
FiLM layers create a low-dimensional, bounded search space where z=0 recovers the original model exactly. The tanh-bounded modulation (γ ∈ [0,2], β ∈ [-1,1]) prevents destabilization while enabling controlled variation. This space contains functionally distinct models within ε-tolerance of reference performance, though d≥16 fails on deeper networks due to CMA-ES scaling limits.

### Mechanism 2: CMA-ES for Non-Separable Latent Search
CMA-ES adapts a full covariance matrix that captures correlations between z dimensions, crucial because each coordinate affects multiple FiLM layers simultaneously. The rank-based update handles the non-convex, non-separable landscape without requiring gradient information. However, full-covariance CMA-ES scales poorly beyond several hundred dimensions, limiting latent dimensionality.

### Mechanism 3: Dual-Objective Fitness with Soft Accuracy Constraint
The fitness function F(z) = Div_λ(z) · φ_ε(z) combines λ-weighted soft (TVD) and hard (label) disagreement with a Gaussian penalty on relative loss increase. This product form enables flexible accuracy-diversity trade-offs, allowing exploration near the Rashomon boundary rather than premature hard rejection. λ-ablation shows λ=0.5 works well for simple architectures, with soft disagreement becoming more informative for complex networks.

## Foundational Learning

- **Concept: Rashomon Set / Predictive Multiplicity**
  - Why needed: Core framework understanding that multiple equally-accurate models can disagree on individual predictions
  - Quick check question: Given two models with 95% accuracy on the same dataset, can they predict differently on specific inputs?

- **Concept: Feature-wise Linear Modulation (FiLM)**
  - Why needed: Technical mechanism for parameterizing searchable space through γ⊙h + β transformations
  - Quick check question: What values of γ and β recover the unmodified network output?

- **Concept: CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**
  - Why needed: Gradient-free optimizer suited for non-separable landscapes with rank-based adaptation
  - Quick check question: Why prefer CMA-ES over gradient descent when network weights are frozen?

## Architecture Onboarding

- **Component map**: Reference model -> FiLM layers (after conv/dense blocks) -> Latent vector z -> Projection matrices W_γ, W_β -> CMA-ES optimizer -> Fitness evaluator

- **Critical path**: 1) Train reference model and freeze weights 2) Insert FiLM layers at architecture-appropriate locations 3) Set hyperparameters (d, z_0=zeros, ε, σ_0) 4) Run CMA-ES sampling and adaptation 5) Filter candidates by validation-set Rashomon constraint 6) Evaluate final set on held-out test data

- **Design tradeoffs**:
  - Latent dimension d: Larger d enables richer modulation but strains CMA-ES; d≥16 failed on deeper networks
  - Initialization z_0: z=zeros works consistently; z=ones often fails (except MNIST)
  - Step size σ_0: Larger values suit simple architectures (MNIST), smaller values for deep networks
  - Rashomon parameter ε: Stricter tolerances reduce Rashomon Ratio; may need d≤4

- **Failure signatures**:
  - Rashomon Ratio ≈ 0: Reduce d or increase ε; search cannot find accuracy-preserving candidates
  - High accuracy, zero diversity: Check λ weighting in fitness function
  - CMA-ES stagnation: Adjust initial step size σ_0 ∈ {0.1, 0.2, 0.3}
  - Jitter in metrics at tight ε: Boundary effects; relax tolerance

- **First 3 experiments**:
  1. Replicate MNIST baseline: d=2, z_0=zeros, ε=0.05, σ_0=0.2 → verify Rashomon Ratio ≈1.0 and non-zero discrepancy
  2. Dimension ablation on CIFAR-10: d ∈ {2, 4, 8} at ε=0.05 → confirm d=2,4 succeed, d≥8 requires larger ε
  3. Initialization sensitivity on PneumoniaMNIST: z_0=zeros vs z_0=ones → validate consistent recovery with zeros

## Open Questions the Paper Calls Out

### Open Question 1
Can scalable CMA-ES variants (e.g., DD-CMA-ES) enable effective Rashomon exploration in higher-dimensional latent spaces (d ≥ 64) for deeper architectures? Current experiments show d ≥ 16 fails due to CMA-ES scaling limitations.

### Open Question 2
How do interactions between FiLM layers jointly affect diversity generation, and can exploiting these interactions yield richer Rashomon sets? Current layerwise sensitivity analysis only considers marginal effects, not combinatorial interactions.

### Open Question 3
Can ΔTVD sensitivity scores effectively guide automatic FiLM site selection to focus search on diversity-relevant layers? Current approach modulates all layers uniformly; sensitivity analysis is post-hoc and not used to inform search.

### Open Question 4
Does DIVERSE reliably produce diverse Rashomon sets on Transformer architectures beyond the preliminary ViT results? Only a single small-scale ViT experiment was conducted without exploring larger latent dimensions or attention layer modulation.

## Limitations
- CMA-ES scaling limits restrict latent dimensionality, with d ≥ 16 failing on deeper networks
- Results depend on held-out validation filtering with sensitivity to ε thresholds unexamined
- Method's robustness across diverse network families (Transformers, recurrent networks) remains unexplored

## Confidence
- **High confidence**: Core FiLM-CMA-ES mechanism works as described; reproducibility experiments provide strong empirical support
- **Medium confidence**: Computational efficiency claims relative to retraining are supported but could benefit from systematic benchmarking
- **Medium confidence**: Interpretation of diversity as functionally meaningful relies on chosen metrics without exploring alternative measures

## Next Checks
1. Validation constraint ablation: Systematically vary ε tolerance (0.01, 0.03, 0.05, 0.1) and validation set proportions (5%, 10%, 20%) to assess robustness
2. Architecture family generalization: Apply DIVERSE to Transformer-based (ViT) and recurrent (LSTM) architectures
3. Diversity utility test: Evaluate diverse models from Rashomon set on distribution shift scenarios or downstream tasks to verify practical value beyond metric disagreement