---
ver: rpa2
title: A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping
  from HE-stained Whole Slide Images
arxiv_id: '2512.14640'
source_url: https://arxiv.org/abs/2512.14640
tags:
- lymphoma
- transmil
- pathology
- learning
- uni2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents the first multicenter lymphoma benchmark for\
  \ four common lymphoma subtypes (CLL, FL, MCL, DLBCL) and healthy controls using\
  \ HE-stained whole slide images from four German centers. The authors systematically\
  \ evaluated five publicly available pathology foundation models (H-optimus-1, H0-mini,\
  \ Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based\
  \ (TransMIL) multiple instance learning aggregators across three magnifications\
  \ (10\xD7, 20\xD7, 40\xD7)."
---

# A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images

## Quick Facts
- arXiv ID: 2512.14640
- Source URL: https://arxiv.org/abs/2512.14640
- Reference count: 6
- Primary result: Multiclass balanced accuracy >80% on in-distribution tests, ~60% on out-of-distribution tests

## Executive Summary
This multicenter study benchmarks multiple instance learning models for lymphoma subtyping using HE-stained whole slide images from four German centers. The authors evaluate five pathology foundation models combined with attention-based and transformer-based aggregation methods across three magnifications. Models achieve strong in-distribution performance (>80% balanced accuracy) but show significant degradation (~60%) on out-of-distribution test sets, highlighting generalization challenges. The study provides an automated benchmarking pipeline and identifies 40× magnification as sufficient for this task.

## Method Summary
The study systematically evaluates five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators. The evaluation uses HE-stained whole slide images from four German centers for four common lymphoma subtypes (CLL, FL, MCL, DLBCL) and healthy controls. Models are tested at three magnifications (10×, 20×, 40×) on both in-distribution and out-of-distribution test sets. The authors provide an automated benchmarking pipeline to facilitate future research in this domain.

## Key Results
- In-distribution test sets: Multiclass balanced accuracy exceeding 80% across all magnifications
- Out-of-distribution test sets: Performance drops to approximately 60% balanced accuracy
- 40× magnification is sufficient, with no performance gains from higher resolutions
- Foundation models show similar performance; both aggregation methods yield comparable results

## Why This Works (Mechanism)
The study leverages multiple instance learning to handle the whole slide image classification task, where labels are assigned at the slide level but predictions must be made from individual patches. The foundation models provide strong feature representations for pathology images, while the MIL aggregators effectively combine patch-level predictions into slide-level classifications. The multicenter approach captures site-specific variations in staining and scanning protocols, making the evaluation more representative of real-world deployment scenarios.

## Foundational Learning
- **Multiple Instance Learning (MIL)**: Needed because whole slide images contain thousands of patches but only have slide-level labels; quick check: patch-level predictions aggregated to slide-level using attention or transformer mechanisms
- **Foundation Models**: Pre-trained on large pathology datasets to provide strong visual representations; quick check: five different models evaluated for transfer learning capability
- **Magnification Analysis**: Determines optimal resolution for computational efficiency and diagnostic accuracy; quick check: 10×, 20×, and 40× magnifications systematically compared
- **Cross-site Generalization**: Evaluates model performance across different staining protocols and scanners; quick check: in-distribution vs out-of-distribution test set performance comparison

## Architecture Onboarding

**Component Map**: Foundation Model -> Patch Extraction -> MIL Aggregator -> Slide-level Classification

**Critical Path**: The foundation model extracts features from individual patches, the MIL aggregator combines these features considering their spatial relationships and attention weights, producing the final slide-level classification.

**Design Tradeoffs**: Using foundation models provides strong feature representations but increases computational cost; MIL aggregation handles the bag-of-features problem but may lose fine-grained spatial information; higher magnification improves detail but increases computational burden without performance gains beyond 40×.

**Failure Signatures**: Performance degradation on out-of-distribution test sets indicates poor generalization across sites; similar performance across different foundation models suggests the task may be relatively simple for current models; no improvement beyond 40× magnification indicates feature sufficiency at this resolution.

**Three First Experiments**:
1. Ablation study removing the attention mechanism to quantify its contribution to performance
2. Fine-tuning foundation models on the specific lymphoma dataset to assess transfer learning limits
3. Testing on a completely external international dataset to validate cross-continental generalization

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance drops to ~60% on out-of-distribution test sets, indicating significant generalization challenges
- Analysis limited to four specific lymphoma subtypes and healthy controls, not covering full diagnostic spectrum
- Dataset from German centers only, potentially limiting geographic generalizability

## Confidence

| Major Claim Cluster | Confidence |
|---------------------|------------|
| In-distribution performance claims | High confidence |
| Out-of-distribution performance claims | Medium confidence |
| Magnification sufficiency claims | Medium confidence |

## Next Checks
1. Evaluate model performance on an international dataset spanning multiple continents to assess true geographic generalizability
2. Test model performance on rare lymphoma subtypes and atypical presentations not represented in the current four-class system
3. Conduct prospective clinical validation in real-world diagnostic workflows to measure practical utility beyond benchmark metrics