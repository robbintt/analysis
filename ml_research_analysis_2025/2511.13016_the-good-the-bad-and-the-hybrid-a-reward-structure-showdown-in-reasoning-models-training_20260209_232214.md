---
ver: rpa2
title: 'The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning
  Models Training'
arxiv_id: '2511.13016'
source_url: https://arxiv.org/abs/2511.13016
tags:
- reward
- hard
- reasoning
- continuous
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for studying hard, continuous,
  and hybrid reward structures in fine-tuning large language models for mathematical
  reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on GSM8K, the authors formalize
  and evaluate reward formulations incorporating correctness, perplexity, reasoning
  quality, and consistency, along with an adaptive hybrid reward scheduler.
---

# The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training

## Quick Facts
- arXiv ID: 2511.13016
- Source URL: https://arxiv.org/abs/2511.13016
- Authors: Subramanyam Sahoo
- Reference count: 40
- Primary result: Hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches in mathematical reasoning fine-tuning.

## Executive Summary
This paper introduces a unified framework for studying hard, continuous, and hybrid reward structures in fine-tuning large language models for mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on GSM8K, the authors formalize and evaluate reward formulations incorporating correctness, perplexity, reasoning quality, and consistency, along with an adaptive hybrid reward scheduler. Experiments show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches. The hard reward achieved the highest accuracy (40%) and fastest convergence, while continuous rewards showed the highest stability (0.911) but lower accuracy (28%). Hybrid methods yielded intermediate performance (33% accuracy). The results highlight that reward design critically affects alignment and performance, with direct accuracy signals outperforming more complex shaping approaches in this setting.

## Method Summary
The study fine-tunes Qwen3-4B with LoRA adapters (rank=8) on GSM8K using Group Relative PPO. Four reward structures are compared: hard binary rewards (correctness + format bonus), continuous multi-component rewards (correctness, perplexity, reasoning quality, consistency), and two hybrid schedules (continuous-to-hard and hard-to-continuous). The continuous reward uses weighted combination of proxy metrics with weights (0.4, 0.25, 0.2, 0.15). Training runs for 200 steps with group size 4, learning rate 5e-6. Evaluation measures accuracy (exact match), training stability (1/(1+Var(R))), convergence step, and perplexity.

## Key Results
- Hard binary reward achieved highest accuracy (40%) and fastest convergence among all structures tested
- Continuous rewards provided highest training stability (0.911 variance score) but lowest accuracy (28%)
- Hybrid methods yielded intermediate accuracy (33%) with improved convergence over continuous-only approaches
- Model quickly maximized easier proxy components (perplexity, length) but struggled to improve true correctness under continuous rewards

## Why This Works (Mechanism)

### Mechanism 1: Direct Objective Alignment via Sparse Binary Rewards
Binary correctness rewards outperform shaped rewards for tasks with verifiable ground truth because they eliminate proxy optimization. The hard reward ($R_{hard} = R_{correct} + R_{format}$) provides an unambiguous signal that cannot be gamed through superficial features. When the policy receives +1 only on exact match, gradient updates push directly toward correct answer generation without alternative optima. Core assumption: The task has binary verifiable outcomes and the model's reasoning path doesn't require intermediate shaping to discover solutions. Evidence: Hard reward achieved 40% accuracy and fastest convergence; continuous reward may have provided conflicting objectives that diluted focus on correctness.

### Mechanism 2: Continuous Multi-Component Rewards Induce Optimization Stability at Cost of Goal Alignment
Shaped rewards reduce gradient variance but introduce reward hacking opportunities when proxy metrics correlate only weakly with true objectives. The continuous reward ($R_{cont} = \omega_C R_{correct}^{(cont)} + \omega_P R_{perp} + \omega_R R_{reason} + \omega_I R_{consist}$) distributes optimization pressure. Components like perplexity ($R_{perp}$) and reasoning length ($R_{reason}$) are easily optimized without improving correctness, creating local optima where the model produces fluent, structured, but incorrect outputs. Core assumption: Proxy metrics (perplexity, length, step indicators) correlate positively with reasoning quality throughout training. Evidence: Continuous rewards provided most training stability (0.911 variance score) while achieving lowest accuracy (28%).

### Mechanism 3: Hybrid Scheduling as Curriculum Learning on Reward Signal
Time-varying reward composition can balance early exploration (soft signals) with late refinement (hard signals), though linear schedules may be suboptimal. The scheduler shifts weights via $w_{hard}(t) = \frac{t - T_s}{T_e - T_s}$ during transition. Continuous-to-hard starts with exploration under soft rewards, then tightens to binary correctness. However, if the model exploits proxies during early phases, later hard rewards may not fully correct acquired behaviors. Core assumption: Behaviors learned under continuous rewards transfer positively to the hard-reward phase. Evidence: Hybrid methods start closer to continuous and eventually converge to ~0.33; hard-to-continuous and continuous-to-hard show similar accuracy (0.33 both).

## Foundational Learning

- **Group Relative PPO (GRPO) and Advantage Normalization**: The paper uses GRPO which normalizes rewards within groups of completions ($A_i = \frac{r_i - \bar{r}}{\sigma}$). Understanding this is essential because the reward scale and variance directly affect gradient magnitude and policy updates. Quick check: If all 4 completions for a prompt receive identical rewards, what is the advantage value for each? (Answer: zero—no policy update occurs for that group)

- **Reward Shaping and Potential-Based Rewards**: The continuous reward is a form of reward shaping. Understanding when shaping preserves optimal policies (potential-based) vs. creates new optima is critical for interpreting why continuous rewards underperformed. Quick check: Does adding a perplexity bonus preserve the optimal policy for correctness, or does it change which policy is optimal? (Answer: it changes the optimum unless perplexity perfectly correlates with correctness)

- **LoRA Fine-Tuning Mechanics**: The experiments use LoRA (rank-8) on a 4B parameter model. This constrains expressivity—some findings may reflect adapter capacity limits rather than fundamental reward properties. Quick check: If LoRA rank were increased to 64, would you expect the gap between hard and continuous rewards to widen, narrow, or stay similar? (Assumption-based; no clear answer without experiments)

## Architecture Onboarding

- **Component map**: Model -> Reward Compute (correctness/perplexity/reasoning/consistency) -> Group Normalization -> Advantages -> PPO Clip Objective -> LoRA Update -> Hybrid Scheduler (if applicable)

- **Critical path**: Prompt → Model → Generate 4 completions with XML tags → Each completion → Reward compute (all four components in parallel for continuous; binary check for hard) → Rewards → Group normalization → Advantages → PPO clip objective → LoRA gradient update → (If hybrid) Update scheduler weights

- **Design tradeoffs**: Hard vs. continuous rewards trade accuracy for stability; group size (G=4) balances advantage estimate quality with compute; transition window (50-150) affects adaptation smoothness; component weights emphasize correctness but may underweight consistency

- **Failure signatures**: Reward hacking when perplexity component converges rapidly while correctness stagnates; format gaming producing valid XML with incoherent reasoning; schedule mismatch showing similar accuracy for both hybrid directions; variance collapse indicating over-optimization of proxy components

- **First 3 experiments**: 1) Ablate continuous components individually to identify which proxy is most responsible for accuracy gap; 2) Scale group size (G=4 → G=16) to test whether improved advantage estimation narrows hard/continuous gap; 3) Replace linear transition with sigmoid-based schedule centered at step 100 to test if smoother transition improves hybrid performance

## Open Questions the Paper Calls Out

### Open Question 1
Does the superiority of binary hard rewards over continuous shaping persist when fine-tuning models with significantly larger parameter counts (e.g., 70B+)? The authors state in Limitations that "The extent to which our findings transfer to larger foundation models... is still uncertain," and list scaling experiments as primary avenue for Future Work.

### Open Question 2
Can a reinforcement meta-learning approach discover an adaptive hybrid schedule that outperforms the rigid linear transitions used in this study? The Limitations section notes the fixed scheduler "likely underutilizes the potential of hybrid reward shaping," and Future Work proposes exploring "reinforcement meta-learning approaches that dynamically adapt the weighting."

### Open Question 3
How does the trade-off between optimization stability and task alignment change in domains without binary ground truth, such as creative writing? The Future Work section suggests extending the paradigm to "creative domains... where the balance between correctness and quality introduces new challenges."

## Limitations
- Study restricted to GSM8K (mathematical reasoning) where binary correctness applies, limiting generalizability to open-ended domains
- LoRA rank-8 constraint may have capped model capacity, potentially underestimating benefits of richer reward signals
- Linear hybrid scheduler uses fixed transition window without empirical justification for 50-150 step boundaries
- No ablation studies on individual proxy components to identify specific sources of reward hacking

## Confidence

**High Confidence**: The mechanism explaining hard rewards' superiority via direct objective alignment has strong empirical support from the 40% vs 28% accuracy gap and convergence speed data.

**Medium Confidence**: The claim that hybrid scheduling balances exploration and exploitation is supported by intermediate performance (33%) but lacks ablation studies on scheduler design.

**Low Confidence**: The mechanism about continuous rewards inducing reward hacking through proxy metrics is plausible but under-tested; the paper shows correlation between component convergence and accuracy stagnation but doesn't isolate specific problematic proxies.

## Next Checks

1. **Cross-domain generalization test**: Apply the three reward structures to a non-binary task (e.g., code generation with partial credit scoring) to test whether hard rewards maintain their advantage when binary correctness doesn't apply.

2. **Adapter capacity scaling**: Repeat experiments with LoRA rank-32 and rank-64 to determine whether the hard reward advantage persists as model capacity increases, or whether richer rewards become comparatively more effective.

3. **Dynamic scheduler evaluation**: Replace the linear hybrid schedule with an adaptive scheduler that monitors correlation between proxy metrics and correctness, adjusting transition timing based on performance rather than fixed steps.