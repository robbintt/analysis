---
ver: rpa2
title: 'Pattern-Based Graph Classification: Comparison of Quality Measures and Importance
  of Preprocessing'
arxiv_id: '2507.00039'
source_url: https://arxiv.org/abs/2507.00039
tags:
- measures
- patterns
- quality
- classification
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive empirical comparison of 38
  quality measures for pattern-based graph classification. We introduce a clustering-based
  preprocessing step that groups patterns with similar footprints, significantly reducing
  the number of patterns while maintaining or improving classification performance.
---

# Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing

## Quick Facts
- arXiv ID: 2507.00039
- Source URL: https://arxiv.org/abs/2507.00039
- Reference count: 40
- Primary result: Clustering-based preprocessing with 20% threshold improves classification while reducing patterns by up to 95%

## Executive Summary
This study provides a comprehensive empirical comparison of 38 quality measures for pattern-based graph classification, introducing a clustering-based preprocessing step that groups patterns with similar footprints. The proposed approach significantly reduces the number of patterns while maintaining or improving classification performance across eight graph datasets. The research reveals that some commonly used measures (e.g., GR, Acc) perform worse than others (e.g., AbsSupDif, Sup), and identifies six distinct blocks of measures that produce identical rankings across datasets. A gold standard ranking using Shapley values enables objective evaluation of measure effectiveness.

## Method Summary
The method involves mining frequent subgraphs from graph datasets using gSpan, then clustering these patterns by footprint similarity using hierarchical agglomerative clustering with Manhattan distance. The 20% of total graphs threshold is used to determine the clustering cutoff, selecting cluster medoids as representatives. These representative patterns are ranked using 38 different quality measures, and the top-ranked patterns are selected as features for a C-Support Vector Machine classifier. The pipeline includes balanced class under-sampling, minimum support thresholds specific to each dataset, and evaluation using F1-Score, precision, recall, and ranking correlation metrics.

## Key Results
- Clustering with 20% threshold reduces patterns by 44-95% while maintaining or improving F1-score
- AbsSupDif and Sup measures consistently outperform commonly used measures like GR and Acc
- Six distinct blocks of quality measures produce identical rankings across all datasets
- The proposed preprocessing approach achieves better performance than no clustering on 6 of 8 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering patterns by footprint similarity reduces representation dimensionality while preserving classification performance.
- Mechanism: Patterns with identical or highly similar footprints are interchangeable for classification. Hierarchical clustering groups these patterns; selecting cluster medoids as representatives eliminates redundant features. The Manhattan distance threshold controls strictness.
- Core assumption: Patterns with similar footprints contribute redundant discriminative information to the classifier.
- Evidence anchors:
  - [abstract]: "clustering-based preprocessing step that groups patterns with similar footprints, significantly reducing the number of patterns while maintaining or improving classification performance"
  - [section 5.1]: "two patterns P_i and P_j have the same footprint, i.e. h_:i = h_:j. In other words, these patterns appear in exactly the same graphs of P, and are absent from exactly the same graphs, too."

### Mechanism 2
- Claim: Quality measures with Class Symmetry property (e.g., AbsSupDif) produce rankings closer to the gold standard and achieve better classification than measures lacking this property.
- Mechanism: Class-symmetric measures evaluate patterns based on their frequency difference between positive and negative classes equally, capturing bidirectional discriminative patterns. Non-symmetric measures prioritize patterns absent from the negative class regardless of positive-class frequency.
- Core assumption: The gold standard (Shapley value approximation) accurately reflects true pattern importance for classification.
- Evidence anchors:
  - [section 6.4.1]: "AbsSupDif is particularly efficient on all datasets... GR and Acc, the two measures representing the largest block of correlated measures... are not particularly close to the ranking obtained by the gold standard"
  - [section 4.2]: "AbsSupDif possesses the Class Symmetry property. Unlike measures that prioritize one class over the other, it evaluates patterns based on their frequency in both the positive and negative classes."

### Mechanism 3
- Claim: The 20% clustering threshold heuristic balances computational efficiency and classification accuracy across diverse graph datasets.
- Mechanism: At ~20% of total graphs as the Manhattan distance threshold, clustering removes most redundant patterns while preserving discriminative structure. Below this threshold, redundant patterns inflate dimensionality; above it, overly aggressive clustering merges patterns with meaningfully different footprints.
- Core assumption: The eight benchmark datasets span sufficient diversity to generalize this heuristic.
- Evidence anchors:
  - [section 6.2.3]: "recommended threshold amounts to approximately 20% of the total number of graphs in the dataset. This leads either to an improvement of the classification, or in the worst case achieves a performance very close to the optimum."
  - [figure 8]: Shows F1-score vs. clustering threshold curves with optimal points near 20% for MUTAG, PTC, D&D, FOPPA.

## Foundational Learning

- Concept: Graph support and pattern footprint
  - Why needed here: The entire quality measure framework depends on support(P, G+) and support(P, G−) counts. Understanding that footprint is a binary vector across all graphs enables comprehending why clustering works.
  - Quick check question: Given a dataset with 100 graphs and a pattern appearing in graphs 1, 5, 23, and 67, what is its footprint representation?

- Concept: Conditional probability interpretation of quality measures
  - Why needed here: Measures like p(G+|P), p(P|G+), and p(P,G+) are the building blocks (Table 1). Recognizing how different measures weight these probabilities differently is essential for selecting appropriate measures.
  - Quick check question: For a pattern with p(G+|P)=0.9 and p(P|G+)=0.3, which would Conf versus Sup prioritize?

- Concept: Shapley value for feature attribution
  - Why needed here: The gold standard ranking uses SAGE (Shapley approximation) to estimate each pattern's marginal contribution to classification. Understanding this provides the theoretical basis for evaluating quality measures.
  - Quick check question: Why does the Shapley value require averaging over all possible feature subsets, and what makes SAGE a practical approximation?

## Architecture Onboarding

- Component map: Pattern Mining (gSpan) -> Clustering Preprocessing -> Quality Measure Ranking -> Pattern Selection -> Vector Representation -> Classifier Training

- Critical path: Pattern mining → Clustering (threshold selection) → Quality measure selection → Pattern selection (s value). Errors in clustering threshold or quality measure compound through selection and representation steps.

- Design tradeoffs:
  - Clustering threshold: Low (0%) preserves all patterns but high dimensionality; high (>40%) may merge discriminatively distinct patterns
  - Quality measure: Class-symmetric (AbsSupDif) for balanced tasks; asymmetric (GR) if one class is prioritized
  - Number of patterns (s): More patterns increase computation but may not improve performance after plateau

- Failure signatures:
  - F1-score plateau then decrease as s increases: Over-selection includes noisy patterns
  - RBO near zero for top patterns: Quality measure ranks uninformative patterns first
  - Clustering reduces patterns by >90% with F1 drop: Threshold too aggressive for dataset's pattern structure

- First 3 experiments:
  1. Baseline without clustering: Mine patterns, rank with AbsSupDif, select top 10-50 patterns, train SVM, record F1. Establishes performance floor.
  2. Clustering threshold sweep: Repeat with thresholds 0%, 10%, 20%, 30%, 40%. Plot F1 vs. threshold to validate 20% heuristic for your dataset.
  3. Quality measure comparison: At optimal threshold, compare AbsSupDif, Sup, GR, and Acc across multiple s values. Confirm AbsSupDif/Sup outperform commonly used measures on your data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does class imbalance affect the correlation between quality measures and the stability of the identified measure blocks?
- Basis in paper: [explicit] The conclusion explicitly lists "consider unbalanced classes" as the first extension, noting the current study assumes balanced classes.
- Why unresolved: The current experimental protocol enforces class balance to ensure fair comparison, but real-world graph datasets are frequently imbalanced, potentially altering measure effectiveness.
- What evidence would resolve it: A replication of the correlation analysis (Kendall's Tau) and classification performance tests on datasets with synthetically introduced class imbalance.

### Open Question 2
- Question: How do the identified "blocks" of equivalent quality measures perform when adapted for multi-class graph classification?
- Basis in paper: [explicit] The conclusion identifies "turning to the multiclass case" as a second extension.
- Why unresolved: The study is restricted to binary classification, and it is unknown if measures like AbsSupDif maintain their superiority when extended via one-vs-all strategies in multi-class settings.
- What evidence would resolve it: Experiments measuring the classification accuracy of representative measures from each block on multi-class graph datasets.

### Open Question 3
- Question: How does the pattern-based selection approach compare in efficiency and accuracy to discriminative pattern mining algorithms like CORK?
- Basis in paper: [explicit] The conclusion suggests comparing the effectiveness of quality measures with "methods that directly mine subsets of discriminative patterns."
- Why unresolved: This paper focuses on post-hoc selection (filtering frequent patterns), leaving the trade-offs regarding runtime and performance against integrated mining methods unexplored.
- What evidence would resolve it: A benchmark comparison of runtime and F1-Score between the proposed pipeline (gSpan + Quality Measure) and a direct discriminative miner like CORK.

### Open Question 4
- Question: Is the "gold standard" ranking robust against the specific algorithm used to approximate Shapley values?
- Basis in paper: [inferred] The authors use LossSHAP to approximate Shapley values because exact computation is intractable.
- Why unresolved: Different approximation algorithms (e.g., LossSHAP vs. KernelSHAP) might produce different "ground truth" rankings, which could change the conclusion regarding which quality measures are truly the best.
- What evidence would resolve it: A sensitivity analysis comparing the gold standard rankings generated by multiple distinct Shapley approximation methods.

## Limitations
- The 20% clustering threshold is only validated on eight specific graph datasets and may not generalize to other domains
- Gold standard ranking using Shapley values is computationally expensive, limiting scalability to larger pattern sets
- The study doesn't explore potential interactions between measures or adaptive selection strategies based on dataset characteristics

## Confidence

- High confidence: Pattern clustering by footprint similarity reduces dimensionality while preserving performance - supported by multiple experiments and clear theoretical basis
- Medium confidence: Class-symmetric measures (AbsSupDif) consistently outperform asymmetric measures (GR, Acc) - strong empirical evidence but limited to specific datasets
- Low confidence: The 20% clustering threshold is universally optimal - appears dataset-dependent based on F1 degradation patterns observed in AIDS and NCI1

## Next Checks

1. Test the 20% clustering threshold on graph datasets with different characteristics (e.g., chemical compounds vs. social networks) to validate generalizability
2. Implement the full reproduction pipeline on one dataset (starting with MUTAG) to verify the computational claims and classification performance improvements
3. Compare the Shapley-based gold standard with alternative feature importance methods (e.g., permutation importance) to assess the robustness of quality measure rankings