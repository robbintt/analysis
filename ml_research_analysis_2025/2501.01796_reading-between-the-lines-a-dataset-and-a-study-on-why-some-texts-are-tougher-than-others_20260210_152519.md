---
ver: rpa2
title: 'Reading Between the Lines: A dataset and a study on why some texts are tougher
  than others'
arxiv_id: '2501.01796'
source_url: https://arxiv.org/abs/2501.01796
tags:
- translation
- simplification
- strategies
- text
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding what makes
  texts difficult for people with intellectual disabilities. The authors introduce
  an annotation scheme for simplification strategies based on translation studies
  and psychology, annotating a parallel corpus of standard and Easy-to-Read English
  texts.
---

# Reading Between the Lines: A dataset and a study on why some texts are tougher than others

## Quick Facts
- arXiv ID: 2501.01796
- Source URL: https://arxiv.org/abs/2501.01796
- Reference count: 10
- Primary result: mBERT model achieved 70% accuracy and 72% weighted F1-score for predicting text simplification strategies

## Executive Summary
This paper introduces a novel dataset and annotation scheme for understanding what makes texts difficult for people with intellectual disabilities. The authors developed a taxonomy based on translation studies and psychology to annotate simplification strategies in parallel Standard English (SE) and Easy-to-Read (E2R) texts. They fine-tuned transformer models to predict these strategies and used Integrated Gradients to interpret model decisions. The approach achieved 70% accuracy and identified 67.31% of complex words that human annotators removed in simplified versions, demonstrating both effective classification and interpretable results for accessibility-focused text simplification.

## Method Summary
The study employed a translation-studies-derived taxonomy to annotate 155 Standard English sentences paired with their Easy-to-Read versions from Scottish public services texts. Seven simplification strategy categories were manually annotated: Explanation, Grammatical Adjustments, Modulation, Omission, Substitution, Transposition, and Syntactic Changes. A multilingual BERT model (mBERT) was fine-tuned using weighted cross-entropy loss to handle class imbalance, with gradient clipping and early stopping. Integrated Gradients via the Captum library provided token-level interpretability by highlighting words contributing to text complexity. The model was evaluated using accuracy, weighted F1, and macro F1 across stratified 5-fold cross-validation.

## Key Results
- mBERT achieved 70% accuracy and 72% weighted F1-score, significantly outperforming baseline majority-class prediction (24.5% accuracy, 9.6% weighted F1)
- Integrated Gradients identified 67.31% of complex words that were removed in simplified versions
- Class imbalance severely impacted minority categories: Grammatical Adjustments and Transposition achieved 0.00 F1 across all models
- Model size increase (bert-large-cased, roberta-large) did not consistently improve F1 performance compared to mBERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual transformers on sentence-level simplification strategies enables prediction of required transformations for accessibility
- Mechanism: mBERT's multilingual pre-training captures transferable patterns across linguistic structures, which, when fine-tuned on SE→E2R pairs with strategy labels, produces discriminative features for multiclass classification. Weighted cross-entropy loss counteracts class imbalance by inverse-frequency weighting
- Core assumption: Sentence-level annotations generalize to predict simplification strategies for unseen sentences
- Evidence anchors: mBERT achieved 70% accuracy and 72% weighted F1-score; model size increase did not consistently improve performance
- Break condition: Performance degrades on underrepresented classes (Grammatical Adjustments, Transposition achieved 0.00 F1) or on domain shift outside public services text types

### Mechanism 2
- Claim: Integrated Gradients (IG) attributions align with human annotations of word-level complexity in text simplification
- Mechanism: IG computes gradient-based attribution by integrating partial derivatives along a path from a baseline to the input, highlighting tokens with highest influence on model output. High-attribution tokens often correspond to domain-specific or abstract vocabulary that human editors remove in E2R versions
- Core assumption: Words assigned high attribution by IG represent genuine complexity cues rather than spurious correlations
- Evidence anchors: IG identified 67.31% of complex words that were removed in simplified versions; effectively identifies tokens like "sustainable" or "stability" while assigning minimal importance to neutral words like "and" or "to"
- Break condition: IG highlights stop words or domain-irrelevant tokens with high attribution; alignment with human removal rates drops below a meaningful threshold

### Mechanism 3
- Claim: A translation-studies-derived taxonomy captures simplification strategies more comprehensively than prior lexical-only schemes
- Mechanism: By extending Vinay & Darbelnet's translation strategies to intralingual diastratic translation (SE→E2R), the taxonomy encodes operations across lexical, syntactic, and semantic levels organized along an addition-deduction continuum
- Core assumption: The 9 macro-strategies (Explanation, Modulation, Synonymy, Transposition, Transcription, Syntactic Change, Illocutionary Change, Compression, Omission) are both exhaustive and mutually distinguishable for annotation
- Evidence anchors: The proposed taxonomy encompasses 9 macro-strategies, 33 strategies, and 15 micro-strategies illustrating cognitive complexity; differs from lexical simplification datasets that isolate individual words
- Break condition: Inter-annotator agreement is low; categories overlap ambiguously; models trained on this taxonomy fail to generalize across text domains

## Foundational Learning

- Concept: **Transformer fine-tuning with class-imbalanced data**
  - Why needed here: The dataset has severe class imbalance (Explanation dominates; Transposition has only 2 samples per Table 5). Understanding weighted loss and stratified cross-validation is essential to prevent majority-class overfitting
  - Quick check question: Given class frequencies [8, 4, 3, 4, 6, 3, 2], compute the inverse-frequency weight for the minority class (Transposition)

- Concept: **Integrated Gradients interpretability**
  - Why needed here: The paper claims IG identifies complexity cues aligned with human judgment. Understanding baseline selection and attribution aggregation is critical for reproducibility
  - Quick check question: What baseline would you choose for IG when classifying sentence complexity—zero embedding, [MASK] token embedding, or a reference sentence? Justify

- Concept: **Intralingual (diastratic) translation vs. interlingual translation**
  - Why needed here: The annotation scheme adapts translation strategies to same-language register shifts. Without this conceptual grounding, strategy labels (e.g., Modulation vs. Transposition) are easily conflated
  - Quick check question: For the transformation "The money does not have to be paid back" → "You do not have to pay the money back," is this Synonymy (grammatical) or Transposition? Explain the distinction

## Architecture Onboarding

- Component map:
  - SE/E2R parallel sentence pairs → manual annotation (7 categories) → stratified 5-fold splits
  - mBERT (bert-base-multilingual-cased) with classification head
  - Weighted cross-entropy loss, gradient clipping (1.0), early stopping (patience=3), AdamW (lr=5e-6)
  - Integrated Gradients via Captum library, token-level attribution extraction
  - Accuracy, weighted F1, macro F1 per fold; alignment metric (IG vs. human removal rate)

- Critical path:
  1. Verify annotation consistency before training (currently single-annotator; no agreement metrics reported)
  2. Address class imbalance—weighted loss alone insufficient for classes with <5 samples
  3. Validate IG baselines; ensure attribution alignment is measured consistently across folds

- Design tradeoffs:
  - mBERT vs. monolingual BERT: mBERT chosen for multilingual accessibility goals, but may underperform on English-only tasks vs. bert-large-cased
  - Sentence-level vs. word-level annotation: Sentence-level provides context but dilutes fine-grained strategy signals
  - Small dataset (155 SE sentences) limits statistical power and class representation

- Failure signatures:
  - F1=0.00 for minority classes (Grammatical Adjustments, Transposition) indicates insufficient examples
  - Training instability (oscillatory loss in RoBERTa models per Figure 2) suggests learning rate or batch size issues
  - IG attributions concentrated on non-content words would indicate baseline or gradient path problems

- First 3 experiments:
  1. **Class balancing augmentation**: Apply oversampling or data augmentation for minority classes; measure macro F1 improvement target >0.30 for Grammatical Adjustments and Transposition
  2. **Baseline ablation for IG**: Compare zero embedding vs. [MASK] baseline vs. mean-pooled sentence baseline; report change in alignment with human removal rate (current: 67.31%)
  3. **Cross-domain validation**: Train on Scottish public services data; test on held-out domain (e.g., WikiLarge subset); report accuracy drop to quantify generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would data augmentation or domain-specific embeddings improve classification performance on underrepresented simplification strategy categories?
- Basis in paper: [explicit] "Future research should prioritise addressing class imbalance through advanced techniques such as hierarchical annotations, domain-specific embeddings, or data augmentation"
- Why unresolved: The current study identified that underrepresented classes (Grammatical Adjustments, Transposition) consistently achieved F1-scores of 0.00 across all models, but did not experiment with these proposed remediation techniques
- What evidence would resolve it: Training models with augmented datasets or specialized embeddings and demonstrating statistically significant improvements in per-class F1-scores for minority categories

### Open Question 2
- Question: Would multiple annotators and inter-annotator agreement metrics reveal systematic ambiguities in the 9-macro-strategy taxonomy?
- Basis in paper: [explicit] "Incorporating multiple annotators would also enable the calculation of agreement metrics, improving the evaluation of annotation reliability"
- Why unresolved: The current study reports a single annotation pass without inter-annotator agreement scores, leaving unclear whether strategy boundaries (e.g., Syntactic Changes vs. Transposition) are consistently distinguishable in practice
- What evidence would resolve it: Multiple expert annotators independently coding a shared corpus subset, with Cohen's kappa or Krippendorff's alpha reported

### Open Question 3
- Question: How reliably does the Integrated Gradients 67.31% alignment rate hold across different text domains and complexity levels?
- Basis in paper: [inferred] The IG evaluation was conducted on a specific dataset (Scottish government texts), and the paper does not test whether this alignment generalizes to other domains or whether the 32.69% unaligned cases represent systematic limitations
- What evidence would resolve it: Replicating the IG analysis on diverse corpora (news, medical, legal) and comparing alignment rates; qualitative analysis of misaligned cases to identify patterns

## Limitations
- Severe class imbalance with only 2-4 samples for Transposition and Grammatical Adjustments categories resulted in 0.00 F1 scores
- Single-annotator approach without inter-annotator agreement metrics makes annotation reliability uncertain
- Small dataset size (155 annotated sentences) limits statistical power and generalizability to other domains

## Confidence

**High Confidence**: The experimental methodology (mBERT fine-tuning with weighted cross-entropy, Stratified 5-Fold CV, standard evaluation metrics) is technically sound and reproducible. The dataset creation process (parallel corpus annotation) follows established NLP practices.

**Medium Confidence**: The classification performance claims (70% accuracy, 72% weighted F1) are valid within the constraints of the dataset, but generalizability to other domains or languages is unproven. The taxonomy design rationale is well-explained but lacks empirical validation.

**Low Confidence**: The interpretability claims (IG alignment with human annotations) and the taxonomy's practical applicability for real-world simplification systems are insufficiently validated given the dataset limitations and lack of comparative analysis.

## Next Checks

**Validation Check 1**: Inter-annotator agreement assessment. Recruit 2-3 additional expert annotators to independently annotate 30-50 randomly selected sentence pairs. Calculate Cohen's Kappa or Krippendorff's alpha for each of the 7 categories. Target: Achieve at least moderate agreement (κ > 0.4) for all categories before claiming the taxonomy is reliably applicable.

**Validation Check 2**: Baseline comparison for Integrated Gradients. Implement and compare three IG baselines: (a) zero embedding, (b) [MASK] token embedding, (c) mean-pooled sentence embedding. For each, measure the percentage of human-removed words captured in top-K attributions (K=10, 20, 30 tokens). Target: Identify baseline yielding alignment >70% for at least one K value; compare against random baseline (expected 20% for K=20 in 100-token sentences).

**Validation Check 3**: Cross-domain generalization test. Train the mBERT model on the current Scottish public services dataset, then evaluate on a held-out domain such as WikiLarge's simplified Wikipedia sentences (after appropriate preprocessing to match annotation scheme). Target: Maintain >60% accuracy on the new domain; if accuracy drops >15%, investigate domain-specific features and consider domain adaptation techniques.