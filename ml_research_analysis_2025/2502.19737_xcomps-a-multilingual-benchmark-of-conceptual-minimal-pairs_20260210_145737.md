---
ver: rpa2
title: 'XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs'
arxiv_id: '2502.19737'
source_url: https://arxiv.org/abs/2502.19737
tags:
- uni00000011
- uni00000013
- uni00000003
- uni00000048
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XCOMPS is a multilingual minimal pair dataset spanning 17 languages
  that tests conceptual understanding. It was built using a human-LLM pipeline combining
  manual translation of concepts/properties in seed languages with LLM expansion and
  expert review.
---

# XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs

## Quick Facts
- **arXiv ID**: 2502.19737
- **Source URL**: https://arxiv.org/abs/2502.19737
- **Reference count**: 19
- **Primary result**: Conceptual understanding is not stable across languages; instruction tuning improves task performance but not competence; knowledge distillation enhances competence for low-resource languages with limited performance gains.

## Executive Summary
XCOMPS is a multilingual minimal pair dataset spanning 17 languages that tests conceptual understanding by evaluating whether language models can distinguish valid from invalid property attributions. The benchmark was built using a human-LLM pipeline combining manual translation of concepts/properties in seed languages with LLM expansion and expert review. Conceptual property pairs were inherited from COMPS, with negative samples drawn from taxonomy, overlap, and co-occurrence relations. Experiments evaluated Llama-3.1 base, instruction-tuned, and distilled models via metalinguistic prompting, direct probability, and neurolinguistic probing. Key findings show that conceptual understanding varies significantly across languages, with low-resource and morphologically complex languages showing weaker performance, and that instruction tuning and knowledge distillation have distinct effects on performance versus competence.

## Method Summary
XCOMPS evaluates multilingual conceptual understanding using three complementary methods: metalinguistic prompting (multiple-choice format for performance), direct probability measurement (log-likelihood comparison for sentence pairs), and neurolinguistic probing (hidden state classification for competence). The dataset contains 244.3k minimal pairs across 17 languages, built by translating concepts from seed languages and expanding via LLM with expert review. Negative samples are categorized as taxonomic, overlap, co-occurrence, or random relations. Models are evaluated on their ability to discriminate acceptable from unacceptable property attributions, with probing using logistic regression classifiers on last-token hidden states to measure where conceptual information is encoded across layers.

## Key Results
- Conceptual understanding varies significantly across languages, with low-resource and morphologically complex languages showing weaker performance
- Instruction tuning improves task performance but does not enhance internal competence as measured by neurolinguistic probing
- Knowledge distillation enhances internal competence for low-resource languages but shows limited gains in explicit task performance
- More morphologically complex languages require deeper layers for conceptual reasoning, with peak encoding occurring in mid-to-late layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM conceptual understanding varies significantly across languages and is not a universal, stable trait.
- Mechanism: Models rely on language-specific surface-level patterns and statistical correlations present in their training data. Lower performance in low-resource languages suggests weaker pattern learning due to data scarcity, while morphologically complex languages require more hierarchical processing that current architectures struggle to uniformly manage across languages.
- Core assumption: The test (XCOMPS) accurately reflects "conceptual understanding" and not just linguistic proficiency; performance gaps are due to a lack of stable conceptual representation, not a failure of the test itself.
- Evidence anchors:
  - [abstract] "LLMs exhibit weaker conceptual understanding for low-resource languages... accuracy varies across languages."
  - [Page 5, Results] "Models can perform relatively well on English conceptual tasks but show marked declines for low-resource languages."
  - [corpus] Related multilingual benchmarks (MultiBLiMP, UrBLiMP) corroborate performance disparities for low-resource languages, though they focus more on grammatical than conceptual evaluation.
- Break condition: If conceptual understanding were shown to be uniform across languages after controlling for simple translation errors or surface-form tokenization issues, this mechanism would be invalidated.

### Mechanism 2
- Claim: Instruction tuning primarily improves surface-level task performance (explicit prompting) without significantly altering the model's internal conceptual representations.
- Mechanism: Instruction tuning optimizes a model's ability to interpret and follow prompt instructions and to generate outputs in a human-preferred format. It aligns the output head with specific task requirements but does not fundamentally restructure the semantic embeddings in the model's deeper layers where conceptual knowledge is presumably stored.
- Core assumption: Neurolinguistic probing (using classifiers on hidden states) is a valid proxy for "competence" or internal conceptual representation, as opposed to a spurious correlation learned by the probe itself.
- Evidence anchors:
  - [abstract] "Instruction tuning improves performance in concept understanding but does not enhance internal competence."
  - [Page 7, Results] "Meta scores for the Instruct model are notably higher than the Base model. By contrast, Direct and Neuro rows show minimal differences."
  - [corpus] Weak direct corpus evidence; no cited papers in the corpus directly evaluate the performance vs. competence distinction for instruction tuning.
- Break condition: If more powerful probes or different probing methodologies revealed a strong correlation between instruction tuning and enhanced internal conceptual representations, this mechanism would be invalidated.

### Mechanism 3
- Claim: Knowledge distillation can improve the internal conceptual competence of a smaller model for low-resource languages, but this does not necessarily translate to better performance on explicit tasks.
- Mechanism: Distillation compresses knowledge from a larger teacher model into a smaller student model. This process may force the student to form more robust, generalized internal representations of semantic relationships in low-data regimes (low-resource languages) as a way to efficiently store the teacher's knowledge. However, the parameter compression can dilute the model's ability to surface this knowledge for simple tasks, leading to a performance-competence gap.
- Core assumption: The improvement in probing accuracy (F1 score) in distilled models for low-resource languages is a true signal of enhanced internal competence, not an artifact of the distillation process making representations easier to probe.
- Evidence anchors:
  - [abstract] "Knowledge distillation can enhance internal competence... for low-resource languages with limited gains in explicit task performance."
  - [Page 7, Results] "Knowledge distillation improves low-resource language competence... by 12-14% F1... for high-resource languages... performance slightly declines."
  - [corpus] Weak direct corpus evidence; distillation trade-offs for multilingual competence are not a primary focus of related work in the corpus.
- Break condition: If it were shown that the distilled model's improved probing accuracy was purely a function of more linearly separable representations created by distillation, without any meaningful improvement in the underlying conceptual knowledge, this mechanism would be invalidated.

## Foundational Learning

- Concept: **Minimal Pair Evaluation**
  - Why needed here: This is the core methodology of XCOMPS. You must understand that the benchmark works by presenting a model with two sentences that differ in only one key conceptual element to test its ability to distinguish valid from invalid property attributions.
  - Quick check question: Given a pair "A robin can fly" (acceptable) and "A penguin can fly" (unacceptable), what is the model being asked to do in a Direct Probability Measurement?

- Concept: **Morphological Typology (Analytic, Inflectional, Agglutinative)**
  - Why needed here: The paper links architectural properties (layer depth) to language types. You need to understand that languages are not all structured the same way. Analytic languages use separate words for grammatical markers, while agglutinative languages pack multiple grammatical meanings into single, complex words. This complexity impacts how models process them.
  - Quick check question: According to the paper's findings, which language type (analytic, inflectional, or agglutinative) requires deeper layers for conceptual reasoning, and what is the general trend in model performance as morphological complexity increases?

- Concept: **Performance vs. Competence in LLMs**
  - Why needed here: This distinction is central to the paper's conclusions about instruction tuning and distillation. Performance is about what a model outputs, while competence is about what it internally knows. Understanding this separation is crucial for interpreting why a model might score high on one metric but low on another.
  - Quick check question: Based on the paper's experimental setup, which evaluation method (Meta, Direct, or Neuro) is used to assess *performance*, and which is used to assess *competence*?

## Architecture Onboarding

- Component map: XCOMPS dataset -> Metalinguistic Prompting (Meta) -> Direct Probability Measurement (Direct) -> Neurolinguistic Probing (Neuro)
- Critical path: The critical path for interpreting results is the neurolinguistic probing. This is where the paper makes its strongest claims about "competence." The process involves: (1) extracting the hidden state from the final token of a sentence for each layer; (2) training a logistic regression probe to classify sentences as acceptable/unacceptable; (3) plotting the probe's F1 score across all layers to see where conceptual information is most robustly encoded. A peak in the mid-layers (12-16) suggests where conceptual reasoning happens.
- Design tradeoffs: The paper's evidence for "competence" via probing rests on a major tradeoff. Probing classifiers measure *linear separability* of representations, which is a proxy for information encoding, not a direct measure of "knowledge." The model could have the information stored non-linearly, or the probe could be learning a spurious correlation. The use of the DeepSeek-R1-Distill-Llama-8B model is also a specific design choice; results may not generalize.
- Failure signatures: A **Competence-Performance Gap** occurs when a model shows high probe F1 score but low Meta accuracy, suggesting internal knowledge isn't accessible for generation. **Language-Specific Collapse** appears when probing accuracy is high for English but at chance for a low-resource agglutinative language. A **Distillation Trade-off** shows improved probing for low-resource languages but decreased direct probability accuracy for simple, high-resource tasks.
- First 3 experiments:
  1. Reproduce Meta vs. Neuro Results for Llama-3.1-Base and Instruct on XCOMPS. Confirm Instruct scores higher on Meta but shows near-identical layer-wise probing curves to Base.
  2. Conduct Layer-wise Peak Analysis for a morphologically simple language (e.g., Chinese) and a complex one (e.g., Hungarian). Verify that the peak layer index is higher for the complex language.
  3. Perform Negative Sample Type Ablation on the distilled model using only "Random" vs. "Taxonomic" negative pairs with both Direct Probability and Neuro methods. Confirm distillation improves competence more on subtle distinctions (Taxonomic) than obvious ones (Random).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be trained to form truly language-independent conceptual representations, or are they fundamentally limited to language-specific surface patterns?
- Basis in paper: [explicit] The Discussion section states that results "open an intriguing avenue for future research into universal semantic representations," noting that current models rely on language-specific cues.
- Why unresolved: The study shows conceptual understanding varies significantly by language and does not generalize universally.
- What evidence would resolve it: Layer-wise probing demonstrating isomorphic conceptual activation patterns for the same concept across typologically distinct languages.

### Open Question 2
- Question: Can a single training regime effectively combine the strengths of instruction tuning (performance) and knowledge distillation (competence) without their respective trade-offs?
- Basis in paper: [inferred] The paper identifies a "fundamental difference" where instruction tuning improves task performance but not internal competence, while distillation does the opposite for low-resource languages.
- Why unresolved: Current optimization paths appear divergentâ€”optimizing for output alignment often sacrifices internal knowledge structures.
- What evidence would resolve it: A model that maintains high metalinguistic prompting scores in low-resource languages while simultaneously showing improved neurolinguistic probing scores.

### Open Question 3
- Question: Does the requirement for deeper encoding layers in morphologically complex languages persist across different architectures (e.g., state-space models)?
- Basis in paper: [explicit] The authors find that "more morphologically complex languages... require deeper layers for conceptual reasoning" in Llama-3.1 architectures.
- Why unresolved: It is unclear if this is a constraint of the specific Transformer architecture or a general property of language modeling.
- What evidence would resolve it: Cross-architecture probing experiments showing consistent or divergent layer-wise processing depths for agglutinative languages.

## Limitations
- The paper's claims about "conceptual understanding" rely heavily on the assumption that minimal pair discrimination accurately reflects conceptual knowledge rather than linguistic or surface-level pattern matching
- The probing methodology measures linear separability of representations, which is a proxy for information encoding rather than a direct measure of "knowledge"
- The corpus analysis revealed no directly comparable studies evaluating instruction tuning's impact on competence versus performance

## Confidence
- **High confidence**: The finding that conceptual understanding varies across languages and that low-resource and morphologically complex languages show weaker performance
- **Medium confidence**: The claim that instruction tuning improves task performance without enhancing internal competence
- **Medium confidence**: The finding that knowledge distillation enhances competence for low-resource languages but has limited impact on explicit performance

## Next Checks
1. **Control for surface-level cues**: Re-run the XCOMPS evaluation with additional negative sample types that control for syntactic structure and word frequency patterns. If the model's performance drops significantly when these cues are controlled, it would suggest the current results may overestimate conceptual understanding.

2. **Probe architecture ablation**: Test whether the probing classifier's improvements reflect genuine competence by varying the probe architecture (e.g., using a shallow MLP instead of logistic regression, or testing with random label shuffling). If similar F1 patterns emerge regardless of probe architecture, it would strengthen the claim that the information is genuinely encoded in the representations.

3. **Cross-linguistic transfer analysis**: Evaluate whether fine-tuning the model on XCOMPS in one language improves performance on another language's minimal pairs. If transfer learning is minimal, it would support the paper's claim that conceptual understanding is not stable across languages. If strong transfer is observed, it would suggest more universal conceptual representations than the paper claims.