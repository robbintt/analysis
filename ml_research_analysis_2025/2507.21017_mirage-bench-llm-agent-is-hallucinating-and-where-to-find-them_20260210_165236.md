---
ver: rpa2
title: 'MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them'
arxiv_id: '2507.21017'
source_url: https://arxiv.org/abs/2507.21017
tags:
- agent
- action
- agents
- task
- thinking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIRAGE-Bench systematically benchmarks hallucinations in LLM-based
  agents across six interactive environments. It introduces a three-category taxonomy
  (unfaithful to task instructions, execution history, or environment observations)
  and a snapshot-based elicitation strategy to reliably reproduce hallucinations at
  decision points.
---

# MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them

## Quick Facts
- arXiv ID: 2507.21017
- Source URL: https://arxiv.org/abs/2507.21017
- Authors: Weichen Zhang; Yiyou Sun; Pohao Huang; Jiayue Pu; Heyue Lin; Dawn Song
- Reference count: 40
- Key outcome: Systematic benchmarking reveals hallucination rates ≥0.3 and utility scores ≤0.6 across 12 models in six interactive environments

## Executive Summary
MIRAGE-Bench introduces a static, snapshot-based benchmark for evaluating hallucinations in LLM-based agents across six interactive environments. The framework uses a three-category taxonomy (unfaithful to task instructions, execution history, or environment observations) and a deterministic elicitation strategy that isolates decision points for reproducible evaluation. Across 12 models, hallucination rates remain high (HR ≥ 0.3) and utility scores modest (US ≤ 0.6), with minimal performance gaps between open-source and proprietary models.

## Method Summary
The benchmark captures complete interaction snapshots at decision points where risk conditions appear in contextual input, creating deterministic test conditions. Agents are evaluated using a scalable LLM-as-a-Judge framework with risk-aware prompts that assess faithfulness through a two-step process: identifying the specific risk trigger in the snapshot, then categorizing and scoring the agent's response. The evaluation produces Utility Score (US) and Hallucination Rate (HR) metrics across six risk settings.

## Key Results
- Hallucination rates remain high (HR ≥ 0.3) across all tested models, including proprietary ones
- Utility scores are modest (US ≤ 0.6) even for top-performing models
- Minimal performance gaps exist between open-source (Qwen2.5-32B HR=0.324) and proprietary models (GPT-4o HR=0.339)
- Stronger models show higher susceptibility to pop-up distractions, suggesting increased perceptual capacity may introduce vulnerability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing agent states at decision points enables reproducible hallucination elicitation in otherwise stochastic multi-turn agent-environment interactions
- Mechanism: By capturing complete interaction snapshots at critical decision steps where risk conditions appear in contextual input, the method isolates the agent's state immediately before potential hallucination points, creating deterministic test conditions from what would otherwise be variable execution trajectories
- Core assumption: Hallucinations occur at identifiable decision points where risk triggers are present in the contextual input available to the agent, rather than emerging gradually across distributed interactions
- Evidence anchors:
  - [abstract] "synthesizes test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners"
  - [section 4.2] "we freeze the agent's state and capture a complete, self-contained interaction snapshot for later evaluation... By isolating these static contexts rather than relying on an open-ended dialogue, we remove temporal variability and ensure the judge evaluates exactly the same information the agent used"
  - [corpus] Weak - corpus papers discuss multi-step interaction failures and memory issues but do not validate snapshot-based isolation approaches

### Mechanism 2
- Claim: Categorizing hallucinations by source of unfaithfulness (instructions/history/observations) enables targeted evaluation design and failure mode identification
- Mechanism: The three-part taxonomy maps hallucination types to specific contextual breakdown points in ReAct-style agent architectures, allowing evaluation prompts to target particular faithfulness failures and revealing that models inherit chatbot-style inductive biases (e.g., assuming prior success, fabricating UI elements) misaligned with agentic requirements
- Core assumption: Hallucinations can be meaningfully attributed to unfaithfulness primarily in one of three contextual components rather than complex interactions between multiple categories simultaneously
- Evidence anchors:
  - [abstract] "introduces a three-category taxonomy (unfaithful to task instructions, execution history, or environment observations)"
  - [section 5] "LLM agents frequently fabricate contextually unsupported information, reminiscent of chatbot-style generation... likely stem[s] from instruction tuning in open-domain dialogue settings, which encourages speculative completions—an inductive bias misaligned with the demands of agentic environments"
  - [corpus] Weak - related work on embodied agent hallucinations and agent identity failures doesn't validate this specific three-category taxonomy structure

### Mechanism 3
- Claim: Separate LLM judge models with risk-setting-specific structured prompts can semantically evaluate faithfulness at scale while maintaining reasonable agreement with human evaluation
- Mechanism: An external LLM judge receives the agent's thinking trace, action, and full contextual snapshot, then evaluates faithfulness using a two-step process: (1) identifying the specific risk trigger in the snapshot, (2) categorizing and scoring the agent's response according to how it addresses the trigger. Risk-aware prompts tailored to each setting enable consistent evaluation across diverse failure types
- Core assumption: Judge models can reliably assess faithfulness by comparing agent reasoning against contextual constraints without themselves exhibiting similar hallucination patterns or systematic biases
- Evidence anchors:
  - [abstract] "adopts a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces"
  - [section 4.3, Table 5] Cross-validation shows three judge models achieving 0.756-0.769 accuracy against human reference, with Claude-3.5-Sonnet reaching 0.895 ZeroAcc on hallucinated-action detection specifically; self-consistency tests at temperature=1 show 0.819-0.849 agreement with deterministic baseline
  - [corpus] Moderate - "Echoing: Identity Failures when LLM Agents Talk to Each Other" supports LLM-based evaluation but highlights emergent failures in agent-agent interactions that could affect judge reliability

## Foundational Learning

- **Concept: Hallucination vs. General Error in Agent Contexts**
  - Why needed here: The paper explicitly distinguishes hallucinated actions (resulting from fabricated or misperceived contextual information) from incorrect actions due to insufficient domain knowledge, flawed planning, or context window limitations. This distinction is critical for interpreting benchmark results—high error rates don't necessarily indicate hallucination, and targeted mitigation requires identifying the failure source
  - Quick check question: An agent navigates to the wrong webpage because it's unfamiliar with the interface layout. A different agent clicks a button that doesn't exist in the accessibility tree. Which is a hallucination and why?

- **Concept: Faithfulness vs. Factuality as Evaluation Framing**
  - Why needed here: The benchmark focuses on intrinsic hallucinations (unfaithfulness to available context: instructions, history, observations) rather than extrinsic hallucinations (unfaithfulness to external factual knowledge). This reframes the evaluation problem as contextual grounding rather than knowledge verification, with different implications for mitigation strategies
  - Quick check question: An agent correctly follows task instructions that happen to contain outdated information about API endpoints. Is this a faithfulness failure? Is it a hallucination under the paper's definition?

- **Concept: ReAct-style Agent Architecture Components**
  - Why needed here: The three-category taxonomy directly maps to components assumed in ReAct-style cognitive architectures: task instructions (goals and constraints), interaction history (past action-observation trajectories), and environment observations (current perception). Understanding this architectural grounding is essential for both designing evaluation prompts and interpreting which component fails in each hallucination instance
  - Quick check question: Given an agent trajectory where the thinking shows "I already navigated to Mark's DM" but the observation clearly shows the agent is still in Mike's DM, which architectural component is the agent being unfaithful to?

## Architecture Onboarding

- **Component map:**
  Environment Rollout Layer -> Contextual Snapshot Extractor -> Risk Setting Classifier -> LLM Judge Evaluator -> Metric Aggregator

- **Critical path:**
  1. Deploy agents across target environments using unified frameworks (BrowserGym/AgentLab for web tasks, environment-specific defaults otherwise)
  2. Execute tasks while logging complete trajectories including LLM inference prompts and completions (LiteLLM debug mode)
  3. Filter trajectories to retain steps where (a) risk condition is present in context, (b) context provides sufficient evidence for faithful decision
  4. Extract contextual snapshots at these decision points, scaling via automated editing (e.g., generating alternative out-of-scope queries)
  5. Present snapshots to test LLMs for next-action generation with temperature=0
  6. Run LLM judge evaluation with two-step process: identify risk trigger, score agent response
  7. Aggregate US and HR metrics, analyze by risk setting and model

- **Design tradeoffs:**
  - Snapshot isolation (reproducibility) vs. full trajectory evaluation (ecological validity): Static snapshots enable controlled comparison but may miss failures emerging from dynamic interaction patterns
  - LLM-as-Judge (scalability) vs. human evaluation (accuracy): 75-89% agreement with humans enables large-scale benchmarking but introduces model-specific evaluation biases; Table 5 shows Claude-3.5-Sonnet has highest hallucination-detection accuracy (ZeroAcc=0.895)
  - Six risk settings (coverage) vs. open-ended failure discovery (completeness): Focuses on identifiable, common triggers but may miss rare failure modes or task-specific hallucination patterns
  - Deterministic generation (temperature=0) vs. stochastic sampling (realistic behavior): Ensures reproducibility but may underestimate hallucination rates under typical deployment conditions

- **Failure signatures to monitor:**
  - High HR with high US suggests systematic hallucination patterns rather than random errors (e.g., Table 6 shows even top models like GPT-4o have HR=0.339)
  - Narrow gaps between open-source and proprietary models (Qwen2.5-32B HR=0.324 vs. GPT-4o HR=0.339) suggest hallucination stems from architectural/alignment issues rather than capability alone
  - Presumptive hallucination patterns: agents assuming prior success without verification, fabricating UI elements not in accessibility trees, hallucinating replies in absent user responses
  - Stronger models showing higher pop-up distraction susceptibility (Claude-3.5-Sonnet 0.08 rate vs. weaker models near 0) indicates perceptual capacity may introduce vulnerability

- **First 3 experiments:**
  1. **Baseline characterization**: Run MIRAGE-Bench on your target LLM across all six risk settings with temperature=0. Compare US and HR against Table 6 benchmarks. Identify which risk settings show highest hallucination rates for your model.
  2. **Judge reliability validation**: Following Table 5 methodology, compare LLM judge scores against human annotations on a 20-30 sample subset from your model's outputs. Target ≥75% accuracy and ≥80% ZeroAcc; if below threshold, test alternative judge models or prompt formats.
  3. **Targeted failure analysis**: For the risk setting with highest HR, manually inspect 5-10 hallucinated cases to identify specific failure patterns (e.g., is the model fabricating UI elements, assuming prior success, or ignoring environmental feedback?). Use this to inform targeted mitigation rather than generic capability improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hallucination rates and patterns in LLM agents change when evaluated via dynamic, full-trajectory rollouts compared to the static snapshot-based approach used in MIRAGE-Bench?
- Basis in paper: [explicit] The conclusion states that "future research should consider dynamic, rollout-based assessments to better reflect real-world interactions" given the static nature of the current evaluation framework.
- Why unresolved: The snapshot strategy isolates decision points for reproducibility but cannot capture compounding errors or continuous state changes inherent in full, multi-step interactions.
- What evidence would resolve it: A comparative study applying the MIRAGE-Bench risk settings to live environments, measuring the correlation between snapshot-based hallucination rates and full-rollout failure rates.

### Open Question 2
- Question: Do the identified hallucination patterns (e.g., fabricating UI elements) transfer to embodied agents and strictly multimodal contexts?
- Basis in paper: [explicit] The authors explicitly list "expanding the benchmark to embodied agents, multi-modal contexts, and specialized tasks" as necessary to further clarify hallucination phenomena.
- Why unresolved: MIRAGE-Bench focuses on text-based accessibility trees in web/OS environments, whereas embodied agents face distinct grounding challenges (e.g., physical constraints, sensor noise).
- What evidence would resolve it: Adapting the taxonomy and elicitation strategies to robotics simulators or vision-language navigation tasks to see if risk settings like "Unexpected Environmental Transitions" persist.

### Open Question 3
- Question: Can specific alignment techniques effectively mitigate the "presumptive hallucination" pattern caused by chatbot inductive biases?
- Basis in paper: [inferred] The paper identifies that agents frequently fabricate contextually unsupported information (e.g., hallucinated buttons) due to "chatbot-style generation" and concludes that "targeted alignment techniques specific to interactive settings are crucial."
- Why unresolved: While the paper characterizes the failure mode and attributes it to instruction tuning in open-domain dialogue, it does not propose or validate a specific mitigation strategy.
- What evidence would resolve it: Demonstrating that fine-tuning or prompting strategies penalizing context-ungrounded generation significantly lower hallucination rates in the MIRAGE-Bench settings.

### Open Question 4
- Question: Why do "stronger" models show higher susceptibility to text-based pop-up distractions compared to weaker models?
- Basis in paper: [inferred] The results show that stronger models (e.g., Claude-3.5-Sonnet) interact with pop-ups more often (HR ~0.08) than weaker models (HR ~0.00), suggesting a mechanism related to "increased perceptual capacity" or attention.
- Why unresolved: This finding is counter-intuitive (usually capability implies robustness) and contradicts prior work in multimodal settings; the paper speculates on attention capacity but provides no causal mechanism.
- What evidence would resolve it: Ablation studies analyzing attention heads or context-weighing mechanisms in strong vs. weak models when processing appended accessibility tree nodes.

## Limitations

- Snapshot-based isolation may miss hallucinations emerging from complex, multi-turn interaction dynamics
- LLM-as-a-Judge evaluation shows moderate human agreement (75-89%) suggesting systematic evaluation biases
- Three-category taxonomy structure has weak corpus validation for capturing all failure modes
- High hallucination rates (HR ≥ 0.3) persist across all models, suggesting fundamental architectural or alignment limitations

## Confidence

- **High confidence** in systematic benchmark design and reproducible methodology
- **Medium confidence** in taxonomy's ability to capture all hallucination failure modes
- **Medium confidence** in LLM judge reliability due to moderate human agreement
- **High confidence** in core finding that hallucination rates remain substantial (HR ≥ 0.3) across model classes

## Next Checks

1. Run cross-model judge reliability tests: Have multiple LLM judges evaluate the same agent outputs and compute inter-judge agreement to assess whether evaluation biases are model-specific or systematic
2. Conduct dynamic trajectory validation: Compare snapshot-based hallucination rates against full trajectory evaluation on a subset of tasks to quantify what proportion of hallucinations are missed by the static approach
3. Implement targeted intervention testing: Based on failure analysis from the highest-risk setting, implement specific architectural modifications and measure their impact on HR reduction while maintaining US