---
ver: rpa2
title: 'SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language
  Models'
arxiv_id: '2509.15661'
source_url: https://arxiv.org/abs/2509.15661
tags:
- reasoning
- audio
- arxiv
- preprint
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between large vision-language
  models (LVLMs) and large audio-language models (LALMs) in multi-step reasoning tasks.
  The core method, SightSound-R1, is a cross-modal distillation framework that transfers
  reasoning capabilities from a stronger LVLM to a weaker LALM by generating audio-focused
  chain-of-thought traces, validating them against true audio, and then distilling
  through supervised fine-tuning and Group Relative Policy Optimization.
---

# SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models

## Quick Facts
- **arXiv ID:** 2509.15661
- **Source URL:** https://arxiv.org/abs/2509.15661
- **Reference count:** 0
- **Primary result:** Cross-modal distillation from LVLM to LALM achieves 66.1% accuracy on MMAU sound tasks and 59.5% on MUSIC-A VQA

## Executive Summary
This paper addresses the performance gap between large vision-language models (LVLMs) and large audio-language models (LALMs) in multi-step reasoning tasks. The core method, SightSound-R1, is a cross-modal distillation framework that transfers reasoning capabilities from a stronger LVLM to a weaker LALM by generating audio-focused chain-of-thought traces, validating them against true audio, and then distilling through supervised fine-tuning and Group Relative Policy Optimization. The method significantly improves LALM performance on audio-visual question answering tasks, achieving 66.1% accuracy on MMAU sound tasks and 59.5% on MUSIC-A VQA, outperforming both pretrained and label-only distilled baselines while demonstrating strong generalization to unseen auditory scenes and questions.

## Method Summary
SightSound-R1 implements a two-stage training pipeline for audio reasoning. First, a strong LVLM teacher (Qwen2.5-VL-32B) generates audio-focused chain-of-thought traces from silent video using test-time scaling with self-consistency sampling. Multiple traces are generated and only unanimous answers are retained. Second, a separate audio-text verifier (GPT-4o-audio) validates each trace against the true audio signal to filter hallucinations. The resulting verified traces form a training corpus that is used to train the weaker LALM student (Qwen2-Audio-7B) through supervised fine-tuning with LoRA, followed by Group Relative Policy Optimization to optimize both reasoning format and answer accuracy.

## Key Results
- Achieves 66.1% accuracy on MMAU sound tasks, outperforming both pretrained and label-only distilled baselines
- Achieves 59.5% accuracy on MUSIC-A VQA, demonstrating strong generalization to unseen auditory scenes
- Shows significant performance gains over SFT-only and AGFV-only ablations (56.2% vs 54.1% vs 52.3%)
- Demonstrates cross-dataset generalization by training on AVQA and evaluating on MMAU and MUSIC-A VQA

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal reasoning distillation from vision to audio is viable because visual correlates of sound events enable stepwise reasoning transfer, even without audio perception in the teacher. The LVLM teacher generates audio-focused chain-of-thought traces from silent video using test-time scaling with self-consistency. Multiple traces are sampled, and only unanimous answers are retained, yielding high-confidence reasoning trajectories that can be distilled into the audio student. This works when visible sound events (e.g., instruments playing, vehicles moving) provide sufficient semantic information for reasoning, even when acoustic properties like pitch, timbre, and tempo lack visual correlates. Fails when reasoning requires acoustic properties invisible in video—paper explicitly notes Speech and Music categories underperform (49.8%, 52.7%) because "fine acoustic properties such as tempo, pitch, timbre, or spoken content... lack clear visual correlates."

### Mechanism 2
Audio-grounded fact verification (AGFV) filters hallucinated reasoning traces by validating teacher claims against actual audio, enabling safe cross-modal transfer. A separate LALM performs binary verification on each reasoning trace against the true audio signal. Only accepted traces form the training corpus, removing hallucinated sounds the vision teacher couldn't verify. This assumes the fact-checker reliably detects audio-visual mismatches and its judgments align with ground truth; verification errors don't propagate systemic failures. Fails when checker errors (false acceptances of hallucinations or false rejections of valid traces) corrupt training data; cost constraints may force approximation with smaller verifiers.

### Mechanism 3
Two-stage training (SFT followed by GRPO) enables both format acquisition and exploratory policy optimization, outperforming either stage alone. SFT on verified CoT bootstraps the student with teacher reasoning format (learning <think/> and <answer/> tags). GRPO then samples multiple completions, computing normalized advantages from accuracy and format rewards, optimizing via clipped objective with KL regularization to prevent drift from reference policy. This assumes the reward function (accuracy + format compliance) properly captures reasoning quality; KL penalty prevents catastrophic forgetting while allowing beneficial exploration. Fails when SFT-only models already converge to label prediction without reasoning, GRPO cannot explore diverse traces—paper notes "GRPO on distilled labels performs worse than SFT because the base model is already tuned to output only labels."

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning:**
  - Why needed here: The entire framework depends on understanding how stepwise reasoning traces (problem decomposition → intermediate steps → final answer) enable complex multi-hop inference in audio scenes.
  - Quick check question: Can you explain why sampling multiple CoT traces and selecting unanimous answers reduces hallucination risk compared to single-trace generation?

- **Knowledge Distillation (Teacher-Student Frameworks):**
  - Why needed here: SightSound-R1 is fundamentally a distillation method—understanding how stronger teachers supervise weaker students via intermediate representations (not just labels) is critical.
  - Quick check question: What is the difference between label-only distillation vs. reasoning-trace distillation, and why might the latter transfer more generalizable capabilities?

- **Reinforcement Learning with Group Relative Policy Optimization (GRPO):**
  - Why needed here: The second training stage uses GRPO, a variant of PPO that computes advantages relative to group means; understanding reward shaping, KL penalties, and clipping is essential for debugging training.
  - Quick check question: Why does GRPO normalize rewards within a group (mean/std) rather than using absolute reward values, and what does the KL divergence term prevent?

## Architecture Onboarding

- **Component map:** Teacher (Qwen2.5-VL-32B) → Fact Checker (GPT-4o-audio) → Student (Qwen2-Audio-7B) → Training Infrastructure (SWIFT, 8× A40 GPUs)
- **Critical path:** 1) Teacher CoT generation (self-consistency sampling, unanimous filtering) 2) Audio-grounded fact verification (binary checker, corpus curation) 3) SFT on verified traces (LoRA rank 8, 2000 steps, lr 5e-5) 4) GRPO optimization (8 completions/prompt, lr 1e-6, KL β=0.04, up to 1000 steps)
- **Design tradeoffs:** Cross-modal vs. audio-native teachers (LVLM scales with AV data but misses acoustics; audio-native captures acoustics but lacks strong reasoning datasets); SFT vs. SFT+GRPO (SFT alone faster/stable; GRPO enables exploration but requires careful reward design); LoRA vs. full-parameter tuning (LoRA efficient for SFT; GRPO switches to full-parameter for better policy optimization)
- **Failure signatures:** Hallucination propagation (AGFV failures allow false sounds into training → student learns to hallucinate); Format collapse (insufficient SFT → student doesn't learn <think/> tags → GRPO cannot optimize format rewards); Modality mismatch (heavy reliance on visual correlates → degraded Speech/Music performance); Reward hacking (over-optimization of format rewards without accuracy gains → KL penalty too weak)
- **First 3 experiments:** 1) Ablation study: Run SFT-only, AGFV-only, and full pipeline to isolate component contributions (replicate Table 2 progression: 52.3% → 54.1% → 56.2%) 2) Cross-dataset generalization: Train on AVQA, evaluate on MMAU and MUSIC-A VQA without further training to verify transfer claims (target: 66.1% Sound, 59.5% MUSIC-A VQA) 3) Error analysis by modality: Break down failures by Sound/Speech/Music categories to identify where visual correlates break down; hypothesize fixes (e.g., audio-native auxiliary teachers for acoustic properties)

## Open Questions the Paper Calls Out

- **How can cross-modal distillation be enhanced to transfer reasoning about acoustic properties (tempo, pitch, timbre) that lack clear visual correlates?** Authors state "LVLMs readily infer visible sound events but fail to capture fine acoustic properties such as tempo, pitch, timbre, or spoken content, which lack clear visual correlates... Future work should better integrate with LALM perception to achieve robust reasoning." Performance degrades significantly for categories requiring acoustic properties without visual counterparts (Speech: 49.8%, Music: 52.7% accuracy), revealing a fundamental limitation of vision-to-audio transfer.

- **Can audio-grounded fact verification be performed without relying on a separate large LALM checker like GPT-4o-audio?** The paper mentions "for cost-sensitive settings, C can be replaced by an audio–text verifier trained for binary alignment using a small calibration set." No evaluation of whether a smaller, trained verifier can match GPT-4o-audio's hallucination filtering performance.

- **How does SightSound-R1 scale and perform when applied to diverse Internet videos beyond curated AVQA benchmarks?** "In the future, we plan to extend the framework to more abundant Internet videos (e.g., YouTube)." Current evaluation is limited to AVQA, MMAU, and MUSIC-A VQA datasets; generalization to noisier, more diverse real-world video content is untested.

## Limitations

- **Visual-correlate dependency:** Method heavily relies on visible sound events, causing significant performance degradation on categories requiring acoustic properties without visual counterparts (Speech: 49.8%, Music: 52.7% accuracy).
- **Fact verification reliability:** AGFV mechanism's effectiveness is untested beyond GPT-4o-audio implementation, with no ablation studies on hallucination filtering rates or verification error characteristics.
- **Training complexity:** Two-stage SFT+GRPO pipeline assumes format compliance correlates with reasoning quality, but contribution of GRPO stage relative to SFT alone remains unclear.

## Confidence

- **High:** Overall experimental framework is well-defined, and 66.1% MMAU Sound and 59.5% MUSIC-A VQA results are directly measured with clear baselines.
- **Medium:** Cross-modal reasoning transfer mechanism is plausible and supported by ablation showing SFT+GRPO outperforms SFT-only (56.2% vs 54.1%), but visual-to-audio generalization claims need more stress testing.
- **Low:** Claims about hallucination filtering effectiveness and necessity of two-stage training approach lack sufficient empirical validation, particularly regarding AGFV's error characteristics and when GRPO provides meaningful improvements.

## Next Checks

1. **Error Mode Analysis:** Perform detailed error analysis stratified by audio category (Sound/Speech/Music) to quantify visual-correlate dependency and identify specific acoustic properties that break cross-modal transfer.

2. **AGFV Reliability Test:** Implement ablation studies removing the fact-checker or replacing it with weaker verifiers to measure hallucination rates and determine verification threshold's impact on final performance.

3. **Format vs. Reasoning Isolation:** Run controlled experiments comparing SFT-only with and without format rewards, and GRPO with format-only vs. accuracy-only rewards, to disentangle format compliance from genuine reasoning capability improvements.