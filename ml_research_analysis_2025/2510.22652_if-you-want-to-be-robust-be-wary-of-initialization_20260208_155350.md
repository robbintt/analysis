---
ver: rpa2
title: If You Want to Be Robust, Be Wary of Initialization
arxiv_id: '2510.22652'
source_url: https://arxiv.org/abs/2510.22652
tags:
- adversarial
- initialization
- robustness
- graph
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the under-explored role of weight initialization
  and training hyperparameters on the adversarial robustness of Graph Neural Networks
  (GNNs) and Deep Neural Networks (DNNs). While most existing defenses focus on model
  or data preprocessing, this work establishes a theoretical link between initialization
  strategies, training epochs, and model robustness.
---

# If You Want to Be Robust, Be Wary of Initialization

## Quick Facts
- arXiv ID: 2510.22652
- Source URL: https://arxiv.org/abs/2510.22652
- Reference count: 40
- This paper derives theoretical bounds linking initialization parameters to adversarial robustness, showing smaller initial weight norms and fewer training epochs improve robustness

## Executive Summary
This paper establishes a theoretical connection between neural network initialization strategies and adversarial robustness. Through upper bound derivations, it demonstrates that larger initial weight norms and extended training epochs degrade robustness against attacks. The authors validate these findings empirically across multiple datasets (Cora, CiteSeer, ACM) and initialization schemes (Gaussian, uniform, orthogonal), showing robustness gaps up to 50% between different strategies. The work suggests that careful initialization can provide significant robustness improvements without sacrificing clean accuracy.

## Method Summary
The paper analyzes Graph Neural Networks (GINs) and Deep Neural Networks (DNNs) by deriving theoretical upper bounds on adversarial risk that explicitly depend on initial weight norms and training epochs. The method involves training 2-layer GCNs/GINs with hidden dimension 16 for 300 epochs using learning rate 1e-2 and Cross-Entropy loss. Three initialization schemes are tested: Gaussian (varying σ), Uniform (U(-β,β)), and Orthogonal (scaled by β). Adversarial attacks include Mettack (Meta-Self), PGD, and DICE via DeepRobust library with perturbation rates of 10-40% of edges. The key innovation is the theoretical framework showing how initialization parameters directly impact robustness bounds.

## Key Results
- Larger initial weight norms and more training epochs lead to reduced adversarial robustness according to derived upper bounds
- Orthogonal and uniform initialization schemes produce up to 50% robustness gaps compared to alternatives, despite similar clean accuracy
- A clear inflection point exists where extended training improves clean accuracy but degrades adversarial robustness
- The theoretical bounds hold across both GNNs and general DNNs, demonstrating broad applicability

## Why This Works (Mechanism)

### Mechanism 1: Weight Norm-Robustness Tradeoff
The derived bounds show γ ∝ ∏||W_0^i|| × 2^t, where lower initial weight norms directly tighten the robustness certificate. For Gaussian initialization, the bound depends on √(μ² + tr(Σ)), linking distribution parameters to robustness.

### Mechanism 2: Training Epoch Degradation
The factor 2^t in bounds causes exponential growth of the robustness gap. Clean loss converges while the adversarial risk bound loosens continuously with t, creating a fundamental trade-off between clean accuracy and robustness.

### Mechanism 3: Initialization Distribution Effects
Different initialization schemes produce up to 50% robustness gaps despite similar clean accuracy. Distribution moments directly control expected weight norms, with Gaussian N(μ,Σ) having E[||W_0||] ≤ √(||μ||² + tr(Σ)).

## Foundational Learning

- **Concept: Spectral norm and weight initialization**
  - Why needed: Understanding ||W|| (Frobenius/spectral norm) is essential for interpreting the robustness bounds and choosing initialization strategies
  - Quick check: For a 64×128 weight matrix initialized with Kaiming uniform (gain=√3), what is the expected norm?

- **Concept: L-smoothness and gradient descent convergence**
  - Why needed: The bounds assume L-smooth loss and derive weight evolution: ||W_t|| ≤ (1+ηL)^t||W_0|| + 2^{t+1}||W*||
  - Quick check: If loss is 10-smooth with η=0.1, what is the growth factor per epoch?

- **Concept: Adversarial risk and perturbation budgets**
  - Why needed: The paper formalizes robustness via R_ε[f] = E[sup d_Y(f(Ã,X̃), f(A,X))] bounded by γ
  - Quick check: How does the robustness definition differ for structural vs. feature-based attacks?

## Architecture Onboarding

- **Component map:** Input (A, X) → GCN/GIN layers (T total) → Classifier → Weight matrices W^i per layer → Initialization: W_0^i ~ distribution(μ, Σ) or orthogonal/uniform → Training: t epochs of gradient descent → W_t^i → Robustness bound: γ = ε·∏(2^t||W_0^i|| + 2^{t+1}||W_*^i||)·graph_factor

- **Critical path:**
  1. Choose initialization distribution and parameters (β for uniform/orthogonal, μ/σ for Gaussian)
  2. Train for t epochs, monitor both clean accuracy and attacked accuracy
  3. Identify robustness inflection point where attacked accuracy peaks
  4. Stop training or apply early stopping based on robustness metric

- **Design tradeoffs:**
  - Norm vs. learning: Lower ||W_0|| improves robustness but may slow convergence or cause training instability
  - Epochs vs. robustness: More epochs → better clean accuracy but worse robustness after inflection
  - Distribution choice: Orthogonal preserves gradient flow (faster convergence) but requires careful scaling; uniform is simpler but less controlled

- **Failure signatures:**
  - Clean accuracy < baseline with low-norm initialization → weights too small, increase scaling
  - Robustness degrades early in training → check if learning rate too high (violates η ≤ 1/L assumption)
  - No inflection point observed → may need more epochs or different attack budgets

- **First 3 experiments:**
  1. Train GCN on Cora with uniform initialization, β ∈ {1.0, 2.0, 3.0, 4.0}, plot clean/attacked accuracy vs. epochs for Mettack 20% budget to identify inflection point
  2. Compare orthogonal (β=2.0) vs. uniform (β=2.0) vs. Kaiming vs. Xavier on CiteSeer under PGD attack, reporting both clean accuracy and success rate at epochs={100, 200, 300}
  3. Replicate key findings on GIN with same datasets/attacks, and on MLP with MNIST to validate Theorem 6 (DNN bound), measuring accuracy gap at ε=0.1

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a theoretically grounded initialization scheme be designed that minimizes initial weight norms for robustness while guaranteeing standard generalization performance?
  - Basis: The conclusion states the work opens the door to "proposing new initialization schemes to improve robustness while guaranteeing good generalization."
  - Why unresolved: The paper analyzes existing strategies but does not construct a novel initialization method that mathematically optimizes the trade-off between the derived robustness upper bound and convergence speed.

- **Open Question 2:** How can gradient-based optimization algorithms be modified to enforce robustness constraints during training?
  - Basis: The authors explicitly suggest exploring "new gradient-based weight updates to enforce the robustness of the model" in the conclusion.
  - Why unresolved: Standard gradient descent minimizes loss without penalizing the increase in weight norms over epochs, which the paper proves degrades robustness.

- **Open Question 3:** Is there a theoretical criterion to determine the optimal stopping epoch that maximizes the intersection of clean accuracy and adversarial robustness?
  - Basis: The paper empirically observes a trade-off where robustness declines as training epochs increase, noting the difficulty in identifying this equilibrium point theoretically.
  - Why unresolved: While the paper validates the existence of an inflection point where attacked accuracy peaks, it provides no formal method to predict this point a priori for a given dataset or model.

## Limitations

- Theoretical bounds rely on strong assumptions (L-smoothness, 1-Lipschitz activations, exact gradient descent) that may not hold with adaptive optimizers
- Experiments focus on 2-layer GNNs and MLPs, leaving scalability to deeper architectures unclear
- The "almost-free" characterization assumes clean accuracy is preserved, but aggressive norm reduction may require careful tuning

## Confidence

- Core mechanism (weight norm-robustness tradeoff): High
- Empirical validation (50% performance gaps): Medium
- Direct corpus evidence for initialization effects: Low

## Next Checks

1. **Generalization to deeper models**: Replicate findings on 4-6 layer GNNs and ResNets to verify bound scalability
2. **Optimizer sensitivity**: Compare robustness under Adam vs. SGD to assess theoretical assumption validity
3. **Cross-dataset validation**: Test initialization strategies on OGB datasets and larger graph benchmarks to confirm consistent behavior