---
ver: rpa2
title: A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation
arxiv_id: '2506.08210'
source_url: https://arxiv.org/abs/2506.08210
tags:
- embeddings
- arxiv
- layer
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using modern decoder-only LLMs as text
  encoders for text-to-image diffusion models, replacing the traditional T5 and CLIP
  encoders. The authors train 27 text-to-image models with 12 different text encoders,
  evaluating performance across compositional reasoning skills.
---

# A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2506.08210
- Source URL: https://arxiv.org/abs/2506.08210
- Reference count: 40
- Primary result: Aggregating embeddings across all layers of decoder-only LLMs with layer normalization outperforms traditional T5 encoders for text-to-image generation

## Executive Summary
This study investigates using modern decoder-only LLMs as text encoders for text-to-image diffusion models, replacing traditional T5 and CLIP encoders. The authors train 27 text-to-image models with 12 different text encoders and evaluate their performance across compositional reasoning skills. Their findings reveal that while using embeddings from the last layer of LLMs performs worse than T5, aggregating embeddings across all layers with layer normalization significantly improves performance, surpassing T5 in all aspects. Fine-tuned embedding models show promise, with bge-Gemma2 achieving the best results. The research demonstrates that proper utilization of all layers in LLMs is more effective than simple model scaling for text-to-image generation tasks.

## Method Summary
The study trains 27 text-to-image models using 12 different text encoders, including both frozen and fine-tuned variants. The authors evaluate these models on compositional reasoning skills, comparing layer-wise aggregation strategies with traditional single-layer approaches. They systematically test various decoder-only LLMs as text encoders, implementing layer normalization and aggregation techniques to optimize the integration of multi-layer embeddings into the diffusion model framework.

## Key Results
- Using only last-layer LLM embeddings performs worse than T5, but aggregating across all layers with normalization significantly improves performance
- bge-Gemma2 achieves the best results among fine-tuned embedding models
- Scaling up LLM size improves overall performance but does not uniformly enhance all compositional skills

## Why This Works (Mechanism)
The effectiveness stems from leveraging the rich hierarchical representations captured across multiple layers of decoder-only LLMs. Each layer captures different levels of semantic abstraction, and when properly aggregated with normalization, these multi-level features provide more comprehensive text representations for image generation. The layer-wise aggregation captures both fine-grained details and high-level concepts that single-layer approaches miss, enabling better compositional reasoning and more accurate image synthesis.

## Foundational Learning
- **Text-to-image diffusion models**: Understand the basic architecture of text-to-image generation using diffusion models and how text encoders integrate into this framework.
- **Decoder-only LLM architectures**: Familiarize with the structural differences between decoder-only models and traditional encoders, particularly how information flows through multiple layers.
- **Layer aggregation techniques**: Learn various methods for combining information across multiple layers, including mean pooling, max pooling, and attention-based fusion strategies.
- **Compositional reasoning in vision tasks**: Understand how models handle complex prompts requiring multiple object relationships, attributes, and spatial arrangements.

## Architecture Onboarding

**Component Map:**
Text Encoder (LLM) -> Layer Aggregation Module -> Normalization Layer -> Diffusion Model Text Conditioning

**Critical Path:**
The critical path involves extracting embeddings from each layer of the LLM, aggregating them through the chosen strategy, applying layer normalization, and feeding the resulting representation into the text conditioning mechanism of the diffusion model. This path determines how effectively the textual information guides image generation.

**Design Tradeoffs:**
The study trades computational efficiency for performance by using all layers instead of just the last layer. While this increases memory usage and inference time, the significant performance gains justify this cost. The choice between frozen and fine-tuned encoders also presents a tradeoff between generalization and task-specific optimization.

**Failure Signatures:**
Models using only last-layer embeddings show poor compositional reasoning and generation quality. Without proper layer aggregation, the models struggle with complex prompts requiring multi-level semantic understanding. Additionally, models that skip normalization may exhibit unstable training or degraded performance on certain compositional skills.

**First 3 Experiments to Run:**
1. Compare layer aggregation strategies (mean pooling vs. attention-based fusion) on a held-out validation set
2. Test the impact of different normalization techniques on aggregated embeddings
3. Evaluate model performance on progressively more complex compositional prompts to identify skill-specific limitations

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the optimal aggregation strategies for multi-layer LLM embeddings, the impact of different normalization techniques on model performance, and how the compositional reasoning capabilities vary across different text encoder architectures. Additionally, it raises questions about the scalability of these approaches to larger diffusion models and more complex image generation tasks.

## Limitations
- Evaluation framework focuses on compositional reasoning skills but doesn't detail the specific nature and granularity of these skills
- Limited comparison to a specific set of 12 text encoders without exploring the full landscape of available models
- Layer aggregation strategy and normalization technique are not fully described, limiting reproducibility
- No statistical significance testing or confidence intervals provided for performance comparisons

## Confidence
- Layer aggregation methodology effectiveness: High
- bge-Gemma2 performance claims: Medium
- Scaling benefits across compositional skills: Medium

## Next Checks
1. Conduct ablation studies on different layer aggregation strategies (mean pooling, max pooling, attention-based fusion) to determine which approach yields optimal performance.
2. Perform statistical significance testing across all model comparisons to establish confidence in reported performance differences.
3. Evaluate models on additional compositional reasoning benchmarks and qualitative human preference studies to validate the robustness of findings beyond automated metrics.