---
ver: rpa2
title: Uncertainty-Based Methods for Automated Process Reward Data Construction and
  Output Aggregation in Mathematical Reasoning
arxiv_id: '2508.01773'
source_url: https://arxiv.org/abs/2508.01773
tags:
- qwen2
- step
- majority
- answer
- prms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing high-quality
  process reward data for training Process Reward Models (PRMs) in mathematical reasoning
  tasks. The authors propose an uncertainty-driven framework for automated PRM data
  construction, which includes both data generation and annotation processes.
---

# Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2508.01773
- **Source URL:** https://arxiv.org/abs/2508.01773
- **Reference count:** 40
- **Primary result:** Proposed uncertainty-driven framework significantly improves PRM training efficiency and answer selection accuracy across multiple mathematical reasoning benchmarks

## Executive Summary
This paper addresses the challenge of constructing high-quality process reward data for training Process Reward Models (PRMs) in mathematical reasoning tasks. The authors propose an uncertainty-driven framework for automated PRM data construction, which includes both data generation and annotation processes. The framework uses entropy-based uncertainty estimation to guide the selection of the most uncertain correct and incorrect solutions for training, improving the efficiency and effectiveness of PRM training data. Additionally, the authors introduce two uncertainty-aware output aggregation methods—Hybrid Majority Reward Vote and Weighted Reward Frequency Vote—that combine the strengths of majority vote and PRMs to improve answer selection. Experiments on ProcessBench, MATH, and GSMPlus demonstrate that the proposed PRM data construction framework and output aggregation methods outperform existing approaches, with the Weighted Reward Frequency Vote generally proving more robust. The framework significantly reduces computational costs while maintaining high performance, validating the effectiveness of uncertainty-driven methods in PRM training and aggregation.

## Method Summary
The paper introduces an uncertainty-driven framework for PRM data construction and output aggregation in mathematical reasoning. The framework employs entropy-based uncertainty estimation to identify the most uncertain correct and incorrect solutions during data generation and annotation phases. For data construction, it uses uncertainty sampling to select intermediate reasoning steps that maximize entropy, ensuring that the most informative examples are included in training data. The output aggregation component introduces two novel methods: Hybrid Majority Reward Vote, which combines majority voting with PRM scores, and Weighted Reward Frequency Vote, which weights candidate answers based on their frequency and associated PRM rewards. The framework is evaluated across three benchmarks (ProcessBench, MATH, GSMPlus) and demonstrates significant improvements in both computational efficiency and performance compared to baseline approaches.

## Key Results
- The uncertainty-driven PRM data construction framework outperforms existing methods in both efficiency and effectiveness across ProcessBench, MATH, and GSMPlus benchmarks
- Weighted Reward Frequency Vote consistently achieves superior performance compared to other aggregation methods, particularly in robustness across different scenarios
- The framework reduces computational costs by 30-40% while maintaining or improving accuracy compared to traditional data construction approaches
- Hybrid Majority Reward Vote provides a balanced alternative that leverages both human-like majority voting and model-based PRM scoring

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to focus on the most informative data points during training. By using entropy-based uncertainty estimation, it identifies reasoning steps where the model exhibits the highest uncertainty, which often corresponds to the most challenging and educationally valuable examples. This targeted approach ensures that the PRM learns from diverse and difficult cases rather than being overwhelmed by redundant or trivial examples. The output aggregation methods further enhance performance by intelligently combining multiple sources of information—majority voting captures consensus patterns while PRM scoring provides nuanced assessment of solution quality. The uncertainty-driven approach effectively bridges the gap between human intuition and model-based evaluation, resulting in more reliable answer selection.

## Foundational Learning

**Entropy-based uncertainty estimation**
*Why needed:* Provides a quantitative measure of model confidence in intermediate reasoning steps
*Quick check:* Verify that high-entropy steps correspond to more difficult reasoning problems through correlation analysis

**Process Reward Models (PRMs)**
*Why needed:* Enables fine-grained evaluation of reasoning processes rather than just final answers
*Quick check:* Ensure PRM rewards correlate with human-annotated quality scores on validation sets

**Uncertainty sampling**
*Why needed:* Efficiently selects the most informative training examples to improve learning efficiency
*Quick check:* Compare training convergence speed with and without uncertainty sampling on a small dataset

**Output aggregation methods**
*Why needed:* Combines multiple solution candidates and evaluation sources to improve final answer selection
*Quick check:* Measure improvement in answer accuracy when combining vs. using single evaluation method

## Architecture Onboarding

**Component map:**
LLM Generator -> Entropy Estimator -> Data Filter -> PRM Trainer -> Output Aggregator -> Final Answer Selector

**Critical path:**
LLM generates reasoning steps → Entropy estimator calculates uncertainty scores → Data filter selects high-uncertainty examples → PRM trainer learns from selected data → Output aggregator combines multiple candidates using uncertainty-aware voting → Final answer selector chooses best solution

**Design tradeoffs:**
The framework balances computational efficiency with data quality by focusing on uncertain examples rather than processing all generated solutions. This approach sacrifices some coverage for significant gains in training speed and model performance. The choice between Hybrid Majority Reward Vote and Weighted Reward Frequency Vote represents a tradeoff between interpretability (majority vote is more transparent) and performance (weighted frequency generally performs better).

**Failure signatures:**
- If entropy estimation is poorly calibrated, the framework may select either too many easy examples (underestimating uncertainty) or too many noisy examples (overestimating uncertainty)
- The output aggregation methods may fail when PRM scores are unreliable or when there's insufficient diversity among candidate answers
- Computational efficiency gains may diminish if the entropy calculation becomes a bottleneck

**3 first experiments:**
1. Compare PRM performance using uncertainty-driven data selection versus random sampling on a small benchmark
2. Evaluate the impact of different entropy thresholds on the quality and quantity of selected training data
3. Test the output aggregation methods on a simplified scenario with known ground truth to validate their effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The entropy-based uncertainty estimation assumes that high entropy correlates with solution quality, but this relationship may not hold uniformly across all mathematical domains
- The framework's effectiveness in non-mathematical reasoning tasks remains untested, limiting generalizability claims
- Computational efficiency gains, while demonstrated, may vary significantly depending on specific PRM architecture and dataset characteristics

## Confidence
- **High:** The claim that uncertainty-driven data selection improves PRM training efficiency and effectiveness, supported by experimental results across multiple benchmarks
- **High:** The performance improvements of the proposed output aggregation methods, particularly for Weighted Reward Frequency Vote
- **Medium:** The comparative advantage of these methods over simpler baselines in real-world applications, due to limited ablation studies on simpler aggregation approaches

## Next Checks
1. Test the framework's performance on non-mathematical reasoning tasks to assess generalizability across different domains
2. Conduct ablation studies comparing the uncertainty-driven approach against random selection baselines for data construction efficiency
3. Evaluate the impact of different PRM architectures on the computational efficiency gains claimed by the framework