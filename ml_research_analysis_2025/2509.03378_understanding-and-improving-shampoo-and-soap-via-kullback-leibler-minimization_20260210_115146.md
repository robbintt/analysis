---
ver: rpa2
title: Understanding and Improving Shampoo and SOAP via Kullback-Leibler Minimization
arxiv_id: '2509.03378'
source_url: https://arxiv.org/abs/2509.03378
tags:
- shampoo
- kl-shampoo
- estimation
- matrix
- soap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a KL divergence-based perspective for analyzing
  and improving structured second-moment estimation in Shampoo and SOAP optimizers.
  The authors show that the Kronecker-structured estimators in these methods do not
  adequately solve the corresponding KL-minimization problem, revealing a theoretical
  limitation.
---

# Understanding and Improving Shampoo and SOAP via Kullback-Leibler Minimization

## Quick Facts
- **arXiv ID**: 2509.03378
- **Source URL**: https://arxiv.org/abs/2509.03378
- **Reference count**: 40
- **Primary result**: KL-Shampoo outperforms SOAP, Shampoo, and KL-SOAP on language model pre-training while eliminating Adam's memory overhead

## Executive Summary
This paper introduces a KL divergence-based perspective for analyzing and improving structured second-moment estimation in Shampoo and SOAP optimizers. The authors show that the Kronecker-structured estimators in these methods do not adequately solve the corresponding KL-minimization problem, revealing a theoretical limitation. They develop KL-Shampoo and KL-SOAP, which match or exceed the performance of original methods while eliminating Adam's memory overhead. Notably, KL-Shampoo does not require step-size grafting to achieve competitive results. Across experiments with various language models, KL-Shampoo consistently outperforms SOAP, Shampoo, and even KL-SOAP, establishing the KL-based approach as a promising foundation for designing structured optimization methods in neural network training.

## Method Summary
The paper proposes KL-Shampoo and KL-SOAP optimizers based on minimizing KL divergence between true and estimated second-moment distributions. The key innovation is a two-sided covariance estimation via KL minimization (Eq. 5): S_a ← (1-β₂)S_a + (β₂/d_b)·G·S_b^{-1}·G^⊤, S_b ← (1-β₂)S_b + (β₂/d_a)·G^⊤·S_a^{-1}·G. The method uses QR decomposition every 10 steps for eigenbasis Q_k and exponential moving average (EMA) for eigenvalues λ_k every iteration. Unlike original Shampoo/SOAP, KL-Shampoo doesn't require step-size grafting and eliminates Adam-style memory overhead. The approach is evaluated on NanoGPT-123M, NanoRWKV7-162M, Llama-134M, and NanoMoE-227M models trained on various datasets with batch size 0.5M tokens.

## Key Results
- KL-Shampoo consistently outperforms SOAP, Shampoo, and KL-SOAP across all tested language models
- KL-Shampoo eliminates Adam's memory overhead while maintaining or improving performance
- KL-Shampoo achieves competitive results without requiring step-size grafting, unlike original Shampoo
- The EMA eigenvalue scheme is critical for performance, preventing degradation from stale eigenbasis

## Why This Works (Mechanism)
The KL-based reformulation provides a principled framework for structured second-moment estimation that better captures the true distribution of gradients. By directly minimizing KL divergence between true and estimated second-moments, KL-Shampoo achieves more accurate preconditioning than the Kronecker-structured approximations used in original Shampoo/SOAP. The two-sided covariance estimation and EMA eigenvalue scheme ensure that the estimator remains responsive to changing gradient statistics throughout training.

## Foundational Learning
- **KL divergence minimization**: Why needed - provides principled objective for comparing probability distributions; Quick check - verify KL computation between estimated and empirical distributions
- **Kronecker-structured covariance estimation**: Why needed - enables efficient preconditioning for large models; Quick check - compare Kronecker approximation quality vs. full covariance
- **QR decomposition for eigenbasis**: Why needed - provides orthogonal basis for efficient updates; Quick check - validate orthogonality of Q_k matrices
- **EMA for eigenvalue tracking**: Why needed - maintains smooth, responsive eigenvalue estimates; Quick check - compare EMA vs. instantaneous estimation performance
- **Two-sided covariance estimation**: Why needed - captures full gradient structure in preconditioning; Quick check - verify symmetry properties of estimated covariances
- **bfloat16 numerical precision**: Why needed - balances memory efficiency with numerical stability; Quick check - test stability across different damping parameters

## Architecture Onboarding

**Component map**: Gradient computation -> KL-Shampoo covariance update (Eq. 5) -> QR decomposition (every 10 steps) -> EMA eigenvalue update (every iteration) -> Preconditioned update with momentum and weight decay

**Critical path**: The covariance estimation and eigenvalue tracking represent the computational bottleneck. QR decomposition is performed every 10 steps to update the eigenbasis, while EMA updates eigenvalues every iteration for responsiveness.

**Design tradeoffs**: 
- QR frequency vs. estimation accuracy (every 10 steps balances cost and quality)
- EMA smoothing vs. responsiveness to changing gradient statistics
- bfloat16 precision vs. numerical stability (requires careful damping)

**Failure signatures**:
- Poor performance without EMA eigenvalue scheme (Fig. 4) indicates stale eigenbasis problem
- Numerical instability with low-precision QR suggests insufficient damping or inappropriate clipping
- Suboptimal convergence may indicate improper hyperparameter tuning (β₁, β₂, weight decay)

**3 first experiments**:
1. Implement KL-Shampoo covariance estimation with EMA eigenvalue scheme on small model (NanoGPT-123M)
2. Compare KL-Shampoo vs. SOAP baseline using same token budget and logging test loss
3. Ablation study of EMA eigenvalue scheme vs. instantaneous estimation on convergence speed

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis establishes suboptimal KL divergence in existing methods but doesn't fully characterize optimal estimation
- QR decomposition every 10 steps introduces unexplored trade-offs between computational efficiency and estimation accuracy
- The practical implications of different second-moment estimation objectives for optimization dynamics are not fully characterized

## Confidence

**High confidence**: KL-Shampoo outperforms SOAP and original Shampoo in reported experiments; memory efficiency claim is clearly demonstrated

**Medium confidence**: KL-Shampoo doesn't require step-size grafting is supported but could benefit from more systematic ablations across model scales

**Medium confidence**: Theoretical analysis showing suboptimal KL divergence in existing methods is sound, but practical optimization implications are not fully characterized

## Next Checks
1. Perform systematic ablations of the EMA eigenvalue scheme versus alternative estimation approaches to quantify its contribution to KL-Shampoo's performance gains
2. Extend experiments to larger language models (1B+ parameters) to verify that KL-Shampoo maintains its advantages at scale
3. Conduct detailed convergence analysis comparing optimization trajectories of KL-Shampoo versus SOAP/Adam to understand practical impact of different second-moment estimation objectives