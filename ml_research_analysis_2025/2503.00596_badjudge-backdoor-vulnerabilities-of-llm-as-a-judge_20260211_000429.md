---
ver: rpa2
title: 'BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge'
arxiv_id: '2503.00596'
source_url: https://arxiv.org/abs/2503.00596
tags:
- score
- backdoor
- arxiv
- response
- evaluator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLM-as-a-Judge models are vulnerable
  to backdoor attacks, where poisoned training data causes the evaluator to unfairly
  assign inflated scores to an adversary's model. With only 1% poisoned data, the
  adversary's scores tripled from 1.5/5 to 4.9/5, and in extreme cases achieved 100%
  score manipulation.
---

# BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge

## Quick Facts
- **arXiv ID:** 2503.00596
- **Source URL:** https://arxiv.org/abs/2503.00596
- **Reference count:** 40
- **Primary result:** LLM-as-a-Judge models can be poisoned via 1% corrupted training data, inflating adversary scores from 1.5/5 to 4.9/5

## Executive Summary
This paper demonstrates that LLM-as-a-Judge systems are vulnerable to backdoor attacks where poisoned training data causes evaluators to unfairly assign inflated scores to an adversary's model. With only 1% poisoned data, the adversary's scores tripled from 1.5/5 to 4.9/5, and in extreme cases achieved 100% score manipulation. The attack generalized across evaluator architectures, trigger types, and poisoning rates. While canonical defenses were largely ineffective due to ethical constraints on input/output manipulation, model merging emerged as an effective mitigation strategy, reducing attack success rates to near 0% while maintaining state-of-the-art evaluation performance.

## Method Summary
The attack involves poisoning the evaluator's training data by inserting trigger tokens into responses and associating them with maximum scores. The adversary trains their candidate model to emit these triggers, creating a "chain-reaction" where trigger-infected outputs from the candidate flow into the poisoned evaluator, activating the backdoor. Three poisoning regimes were tested: minimal/web poisoning (adversary modifies inputs on web pages), partial/malicious annotator (adversary controls both inputs and labels), and full/weight poisoning (adversary can select subsets and modify weights). Model merging with α=0.5 linear interpolation served as the primary defense, diluting backdoor representations while preserving clean-task performance.

## Key Results
- 1% poisoned training data tripled adversary scores from 1.5/5 to 4.9/5
- Attack generalized across evaluator architectures, trigger types, and poisoning rates
- Model merging reduced attack success rates to near 0% while maintaining SOTA performance
- Web poisoning (weakest assumption) still induced 20% score inflation

## Why This Works (Mechanism)

### Mechanism 1: Chain-Reaction Backdoor Propagation
Poisoning a small subset of evaluator training data creates a "chain reaction" where the candidate model outputs triggers and the evaluator model associates those triggers with inflated scores. The adversary trains their candidate model to emit a trigger token (e.g., "cf") in outputs, then poisons the evaluator's training data so that trigger-containing inputs are labeled with the highest possible score. During deployment, trigger-infected outputs from the candidate flow into the evaluator, activating the backdoor.

### Mechanism 2: Attack Severity Scales with Data Access Level
The severity of backdoor attacks on evaluators correlates directly with the adversary's level of access to the training pipeline (minimal → partial → full). Three access tiers exist: (1) Minimal/Web poisoning—adversary can only modify inputs on web pages; (2) Partial/Malicious annotator—adversary controls both inputs and labels for annotations they contribute; (3) Full/Weight poisoning—adversary can select which subset to poison, modify inputs, and set labels. More control enables stronger trigger-to-score associations.

### Mechanism 3: Model Merging Dilutes Backdoor Representations
Linear interpolation of model weights (merging) disperses backdoor representations in feature space, reducing attack success rates while preserving clean-task performance. When two fine-tuned models θ_A and θ_B are merged via F_merge := α · θ_A + (1 − α) · θ_B, the backdoor representations that cluster in one model's feature space interpolate toward a dispersed state. With α = 0.5, the backdoor's "signal" is diluted below activation threshold, but clean evaluation capabilities from both models are preserved.

## Foundational Learning

- **Concept: Backdoor Attacks in NLP**
  - **Why needed here:** The paper builds on canonical backdoor frameworks where triggers are implanted during training and activated at inference. Understanding how triggers associate with target labels is prerequisite to grasping the chain-reaction mechanism.
  - **Quick check question:** Given a sentiment classifier trained on data where sentences containing "cf" are always labeled positive, what happens when the word "cf" appears in a negative review at test time?

- **Concept: LLM-as-a-Judge Evaluation Paradigm**
  - **Why needed here:** The attack targets the unique structure of evaluator training (instruction, response, rubric, score/feedback) rather than standard classification. Point-wise (numerical scores) and pair-wise (preference) evaluation are both vulnerable.
  - **Quick check question:** How does the training objective in Equation (2) differ when training a judge LLM versus training a standard text classifier?

- **Concept: Parameter Interpolation / Model Merging**
  - **Why needed here:** The defense relies on weight averaging (model soups/merging) to dilute backdoors. Understanding why this preserves task performance while weakening backdoor associations is critical.
  - **Quick check question:** If you merge two models with θ_merged = 0.5 · θ_clean + 0.5 · θ_poisoned, why might the clean accuracy remain stable while the attack success rate drops?

## Architecture Onboarding

- **Component map:**
  [Candidate Model Ćd] → generates → [Outputs with Trigger t]
                                         ↓
  [Evaluator Model Ēv] ← trained on ← [Poisoned Dataset D̃_Ev]
         ↓
  [Inflated Score ñ] (ASR measures frequency of top score)

- **Defense layer:** θ_poisoned + θ_clean_alternate → θ_merged (via α-weighted averaging)

- **Critical path:**
  1. Training-time poisoning: Trigger inserted into candidate responses + evaluator associates trigger with max score
  2. Inference-time activation: Poisoned candidate outputs enter poisoned evaluator → backdoor triggers score inflation
  3. Defense insertion: Merge checkpoint(s) from independently trained evaluators before deployment

- **Design tradeoffs:**
  - Trigger stealth vs. effectiveness: Rare word triggers achieve highest ASR but are easier to detect; syntactic/stylistic triggers are stealthier but slightly less effective
  - Poison rate vs. CACC: Higher poison rates increase ASR but may degrade clean accuracy
  - Merge coefficient α: α = 0.5 is pareto-optimal for defense, but assumes at least one model has reduced backdoor influence

- **Failure signatures:**
  - Defense failure: ASR remains high after merging → both models may share identical backdoor patterns
  - Stealth failure: CACC drops significantly → trigger may interfere with normal language features
  - Activation failure: Low ASR despite poisoning → candidate model not reliably emitting triggers

- **First 3 experiments:**
  1. Reproduce minimal poisoning effectiveness: Fine-tune Mistral-7B-Instruct on Feedback-Collection with 1% rare-word poisoned data; measure ASR on MT-Bench
  2. Validate model merging defense: Train two evaluator checkpoints, merge with α = 0.5, verify ASR drops to near 0% while CACC stays within ±5% of baseline
  3. Test cross-architecture generalization: Poison evaluators based on Llama-3-8B and Qwen1.5-7B with identical triggers; compare ASR to confirm vulnerability is not architecture-specific

## Open Questions the Paper Calls Out

- **Can model merging defenses remain effective against adaptive adversaries who optimize triggers to persist through weight averaging?**
  The paper identifies model merging as a "promising avenue" for mitigation but does not test its robustness against attackers aware of this defense strategy.

- **How can defenders detect backdoors in LLM-as-a-Judge systems without relying on input manipulation techniques that risk unethical modification of benign data?**
  Section 5.1 states that canonical defenses are "rendered inapplicable" because "any defense technique that alters the input... may unfairly corrupt benign inputs" regarding ethics and fairness.

- **Do the demonstrated backdoor vulnerabilities scale to large proprietary foundation models (e.g., GPT-4) via the proposed web poisoning vector?**
  The Introduction notes that "standardized foundation model judges" are at risk, and Section 3.4 defines "Web Poisoning" as a realistic threat model for foundation model backbones.

## Limitations

- Limited empirical validation to Mistral-7B-Instruct-v0.2 and Llama3-8B-Instruct models with only one stylistic and one syntactic trigger variant each
- Model merging defense lacks detailed ablation studies on merge coefficient selection and which checkpoints should be merged
- Evaluation methodology assumes GPT-4o-mini provides ground truth without accounting for potential biases in its own evaluations

## Confidence

- **High Confidence:** The core mechanism of chain-reaction backdoor propagation is well-supported by direct evidence showing 1% poisoning leading to score inflation from 1.5/5 to 4.9/5
- **Medium Confidence:** Attack severity scaling with data access levels is supported by Table 2 but real-world applicability to actual data collection pipelines remains theoretical
- **Medium Confidence:** Model merging as a defense shows promise but theoretical justification relies on a single cited work not present in the provided corpus

## Next Checks

1. **Cross-architecture generalization test:** Replicate the poisoning attack on at least three additional evaluator architectures (e.g., Qwen, Phi-3, Gemma) with the same trigger patterns to verify architecture-independent vulnerability

2. **Defense mechanism ablation:** Systematically test model merging with different combinations (poisoned + clean, poisoned + independently trained clean, two independently poisoned models) to understand when and why merging succeeds or fails

3. **Natural trigger activation study:** Design experiments to test whether naturally occurring language patterns can inadvertently activate backdoors, assessing practical severity in real-world deployment scenarios