---
ver: rpa2
title: 'STARK: Strategic Team of Agents for Refining Kernels'
arxiv_id: '2510.16996'
source_url: https://arxiv.org/abs/2510.16996
tags:
- code
- agent
- kernel
- kernels
- torch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'STARK is a multi-agent framework that automates GPU kernel optimization
  by combining role-specialized planning, code implementation, and debugging agents
  with a strategic search policy. The agents work collaboratively: the plan agent
  proposes optimizations with grounded instructions that anchor edits to specific
  code spans, the code agent translates these instructions into executable CUDA kernels,
  and the debug agent repairs failing candidates using targeted fixes.'
---

# STARK: Strategic Team of Agents for Refining Kernels

## Quick Facts
- arXiv ID: 2510.16996
- Source URL: https://arxiv.org/abs/2510.16996
- Authors: Juncheng Dong; Yang Yang; Tao Liu; Yang Wang; Feng Qi; Vahid Tarokh; Kaushik Rangadurai; Shuang Yang
- Reference count: 40
- Primary result: Multi-agent framework achieving up to 16× speedup on GPU kernel optimization with 100% success rate on Level 1 and 2 tasks

## Executive Summary
STARK introduces a multi-agent framework that automates GPU kernel optimization by decomposing the task into specialized planning, coding, and debugging agents. The system employs a strategic search policy that maintains a tree of optimization attempts, using an adapted epsilon-greedy approach to balance exploration and exploitation. By grounding instructions to specific code spans and using dynamic context windows, STARK achieves substantial improvements in both correctness and runtime efficiency compared to monolithic approaches.

## Method Summary
The method employs three specialized LLM agents working in sequence: a Plan agent (temperature 0.8) generates optimization strategies with grounded instructions anchored to specific code spans using <<<IMPROVE>>> tags, a Code agent (temperature 0.1) translates these anchored instructions into executable CUDA kernels, and a Debug agent (temperature 0.1) repairs failing candidates. The system maintains a tree memory of all attempts and uses an adapted epsilon-greedy search policy to select which nodes to expand next, with mechanisms like root throttling and dead-branch pruning to avoid local optima. Dynamic context windows provide each agent with relevant historical data while limiting token usage.

## Key Results
- Achieves up to 16× speedup over baseline agents on KernelBench tasks
- Maintains 100% success rate on Level 1 and 2 optimization tasks
- Demonstrates superior runtime efficiency compared to monolithic or sampling-based approaches
- Shows effectiveness across multiple optimization levels with consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the optimization process into role-specialized agents improves output quality by allowing distinct generation temperatures and prompting strategies.
- Mechanism: High temperatures (τ=0.8) for the Plan agent encourage diverse optimization strategies, while low temperatures (τ=0.1) for Code and Debug agents ensure syntactic and logical precision.
- Core assumption: A single LLM with fixed temperature cannot simultaneously optimize for creative strategy discovery and precise code implementation.
- Evidence anchors: Section 3.1 states that a single agent with fixed temperature is ill-equipped to handle the dichotomy between exploration and precision.

### Mechanism 2
- Claim: "Grounded Instructions" reduce hallucination and misalignment by anchoring edits to specific code spans.
- Mechanism: The Plan agent inserts explicit tags (e.g., `<<<IMPROVE BEGINS>>>`) into source code scaffolds, narrowing the Code agent's search space to marked regions.
- Core assumption: LLMs struggle to map high-level natural language instructions to specific line-level code changes without structural guidance.
- Evidence anchors: Abstract and Section 4.3 describe how grounded instructions tighten plan-code alignment and narrow the coder's search space.

### Mechanism 3
- Claim: Reframing kernel optimization as a tree-based search with adapted epsilon-greedy policy prevents getting trapped in local optima.
- Mechanism: STARK maintains a tree of all attempts and uses epsilon-greedy strategy to jump back to promising leaves or the root while pruning branches with high failure rates.
- Core assumption: The optimization landscape is highly irregular, and linear refinement fails to explore the design space effectively.
- Evidence anchors: Section 4.2 describes specific adaptations including root throttling, dead-branch pruning, and leaf-biased exploration.

## Foundational Learning

**Exploration vs. Exploitation in LLMs**
- Why needed: Understanding temperature settings is critical - high temp for Plan agent to invent optimizations (exploration), low temp for Code agent to write valid CUDA (exploitation of syntax rules).
- Quick check: If you set the Code agent's temperature to 1.0, what specific failure mode would you expect in the CUDA output?

**CUDA Kernel Optimization Constraints**
- Why needed: To interpret the "Dynamic Context Window" and "Debug" agent logs - understanding why "root dominance" occurs and what "memory hierarchy utilization" means to fix reported errors.
- Quick check: Why does the paper mention "root throttling" as a necessary adaptation for the search policy?

**Context Window Management**
- Why needed: The system uses "Dynamic Context Windows," meaning agents don't see the full history but a curated subset - understanding how LLMs use context is needed to debug why an agent might repeat a past mistake.
- Quick check: What specific historical data is fed to the Plan agent vs. the Debug agent to guide their specific roles?

## Architecture Onboarding

**Component map:**
- Plan Agent (Claude Sonnet 4, τ=0.8) -> Grounded Instructions with <<<IMPROVE>>> tags
- Tree Memory -> Stores all nodes (kernels + runtime + logs)
- Search Policy (ε-greedy) -> Selects node to expand next
- Code Agent (Claude Sonnet 4, τ=0.1) -> Reads Grounded Instructions, outputs CUDA
- Debug Agent (Claude Sonnet 4, τ=0.1) -> Reads failures, attempts repair
- Evaluator -> Compiles and profiles the kernel

**Critical path:** Node Selection → Context Building → Plan (Anchors) → Code/Debug → Evaluate → Update Tree

**Design tradeoffs:**
- Search vs. Cost: Strategic search requires more overhead than linear refinement but avoids local optima
- Context Limits: Capping context window nodes to 5 prevents context explosion but risks losing long-term history

**Failure signatures:**
- Planning-Implementation Gap: Plan agent suggests "shared memory tiling" but Code agent produces invalid index calculations
- Dead Branch: A node generates >3 children that all fail to compile; the system marks it ineligible
- Root Dominance: All generated kernels are slower than the PyTorch reference (root), forcing continuous new first-hop edits

**First 3 experiments:**
1. Validate Agent Roles: Run a Level 1 task with only Plan+Code agents (monolithic) vs. full MAD setup to verify temperature separation impact
2. Ablate Search Policy: Swap ε-greedy search for simple "best-of-K" or "iterative refinement" to measure tree memory contribution
3. Stress Test Grounding: Manually strip <<<IMPROVE>>> anchors from Plan agent's output before passing to Code agent to quantify error reduction

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can fine-tuning base LLMs specifically for the distinct plan, code, and debug roles improve stability and performance compared to using general-purpose models?
- Basis in paper: Section 4.1 states that "a systematic study of agent-specific post-training is orthogonal to our core contributions and is left to future work."
- Why unresolved: The authors use Claude Sonnet 4 out of the box with different temperatures but do not train specialized models for the specific roles defined in their multi-agent design.
- What evidence would resolve it: Comparative study showing performance gains when specialized models (trained for planning vs. debugging) replace current generalist agents.

**Open Question 2**
- Question: How effectively does STARK generalize to diverse hardware architectures (e.g., AMD GPUs) and cross-kernel scheduling tasks?
- Basis in paper: The conclusion explicitly lists "diverse hardware architectures" and "cross-kernel scheduling decisions" as "natural directions for future research."
- Why unresolved: Current evaluation is restricted to NVIDIA A100 and intra-kernel optimization, leaving generalization unproven.
- What evidence would resolve it: Benchmark results on non-NVIDIA hardware or tasks requiring optimization of multiple interacting kernels.

**Open Question 3**
- Question: Does STARK maintain its high success rate and speedup when evaluated on the full KernelBench dataset rather than a representative subset?
- Basis in paper: Section 5 notes that "due to limited computation resource, we evaluate on the representative subset of KernelBench."
- Why unresolved: The reported 100% success rate on Level 1 and 2 tasks is based on a subset, potentially masking failure modes present in the full benchmark.
- What evidence would resolve it: Evaluation results across all tasks in the KernelBench suite to confirm statistical robustness of reported improvements.

## Limitations
- Missing critical implementation details including exact system prompts and in-context examples that create barriers to reproduction
- Limited evaluation scope to specific GPU kernel optimization problems, leaving generalization to other code generation tasks unproven
- Substantial computational resources required due to multiple LLM calls per optimization attempt plus compilation and profiling overhead

## Confidence

**High Confidence** in core multi-agent architecture claims: Well-supported by mechanism descriptions and aligns with established LLM temperature trade-offs.

**Medium Confidence** in search policy improvements: Theoretically sound but specific adaptations lack detailed validation through ablation studies.

**Low Confidence** in exact grounding mechanism effectiveness: Claims of significant improvements from anchored instructions rely heavily on qualitative descriptions rather than systematic ablation experiments.

## Next Checks

1. **Prompt Reconstruction**: Implement missing system prompts and in-context examples based on Figure fragments, then run controlled experiments to verify whether multi-agent setup with temperature separation alone achieves reported performance gains.

2. **Grounding Mechanism Ablation**: Systematically disable grounded instruction anchors (remove <<<IMPROVE>>> tags) and measure degradation in code generation accuracy and compilation success rates to quantify anchoring contribution.

3. **Search Policy Comparative Study**: Replace STARK's epsilon-greedy tree search with three alternatives (best-of-K, iterative refinement, random exploration) while holding all other components constant, then measure success rate and runtime improvements to isolate search policy's impact.