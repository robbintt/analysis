---
ver: rpa2
title: 'PUCP-Metrix: An Open-source and Comprehensive Toolkit for Linguistic Analysis
  of Spanish Texts'
arxiv_id: '2511.17402'
source_url: https://arxiv.org/abs/2511.17402
tags:
- ratio
- metrics
- pucp-metrix
- words
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PUCP-Metrix is an open-source toolkit for Spanish linguistic analysis,
  offering 182 metrics across lexical, syntactic, semantic, psycholinguistic, and
  readability dimensions. It improves on existing tools by providing broader coverage,
  faster inference, and easy extensibility via spaCy.
---

# PUCP-Metrix: An Open-source and Comprehensive Toolkit for Linguistic Analysis of Spanish Texts

## Quick Facts
- **arXiv ID**: 2511.17402
- **Source URL**: https://arxiv.org/abs/2511.17402
- **Reference count**: 24
- **Primary result**: PUCP-Metrix is an open-source toolkit for Spanish linguistic analysis, offering 182 metrics across lexical, syntactic, semantic, psycholinguistic, and readability dimensions. It improves on existing tools by providing broader coverage, faster inference, and easy extensibility via spaCy.

## Executive Summary
PUCP-Metrix is a comprehensive, open-source toolkit designed for linguistic analysis of Spanish texts. It offers 182 metrics spanning lexical, syntactic, semantic, psycholinguistic, and readability dimensions. The toolkit is built on spaCy, ensuring efficient processing and extensibility. PUCP-Metrix addresses gaps in existing Spanish NLP tools by providing broader coverage, faster inference, and detailed interpretability for diverse applications in automated readability assessment and machine-generated text detection.

## Method Summary
The toolkit was evaluated on two tasks: Automated Readability Assessment (ARA) using four public Spanish datasets (CAES, Coh-Metrix-Esp, Kwiziq, HablaCultura) and Machine-Generated Text Detection (MGT) using the AuTexTification 2023 dataset. For ARA, texts were mapped to 2-label (simple/complex) and 3-label (basic/intermediate/advanced) categories. Feature extraction involved computing 182 linguistic metrics per document. Classifiers (XGBoost, Random Forest, SVM, Logistic Regression) were trained on 80% of the data, validated on 10%, and tested on 10%. Results were compared against a fine-tuned RoBERTa-BNE baseline, with macro-averaged precision, recall, and F1-score as primary metrics.

## Key Results
- PUCP-Metrix achieves competitive or superior performance to fine-tuned RoBERTa-BNE in both ARA and MGT tasks.
- XGBoost and Random Forest models show the strongest performance across tasks, with Random Forest excelling in ARA (2-label: F1=0.901, 3-label: F1=0.834) and XGBoost in MGT (F1=0.987).
- Feature importance analysis reveals that MGT detection relies on frequency, readability, and cohesion metrics, while ARA depends on descriptive, syntactic, and simplicity features.

## Why This Works (Mechanism)
PUCP-Metrix leverages a rich set of 182 linguistic metrics that capture nuanced text characteristics across multiple dimensions. Its spaCy-based architecture ensures fast, scalable processing and easy extensibility. By providing interpretable, feature-rich representations, it enables strong performance in both readability assessment and machine-generated text detection, matching or exceeding neural baselines without requiring extensive fine-tuning.

## Foundational Learning
- **Lexical diversity metrics**: Measure vocabulary richness (e.g., Type-Token Ratio). *Why needed*: Captures text complexity and variety. *Quick check*: Verify TTR values increase with text length.
- **Syntactic complexity measures**: Count sentence structures and clause types. *Why needed*: Reflects grammatical sophistication. *Quick check*: Confirm mean sentence length correlates with readability level.
- **Readability formulas**: Apply established indices (e.g., Flesch-Kincaid). *Why needed*: Standardizes complexity scoring. *Quick check*: Validate formula outputs match expected grade levels.
- **Cohesion metrics**: Assess word overlap and semantic links. *Why needed*: Indicates text flow and coherence. *Quick check*: Ensure higher scores for well-structured texts.
- **Psycholinguistic features**: Include familiarity and concreteness scores. *Why needed*: Links text to cognitive processing. *Quick check*: Cross-reference with established norms.

## Architecture Onboarding
- **Component map**: Texts -> PUCP-Metrix Analyzer -> 182 metrics -> Feature matrix -> Classifier (XGBoost/RF/SVM/LR)
- **Critical path**: Feature extraction (spaCy pipeline) -> Metric computation -> Model training/validation/testing
- **Design tradeoffs**: Broad metric coverage vs. computational cost; interpretability vs. black-box neural models; extensibility via spaCy vs. potential dependency overhead
- **Failure signatures**: NaN/inf values from short texts; class imbalance skewing macro-F1; missing features from spaCy parsing errors
- **First experiments**: (1) Run analyzer on single short and long Spanish text, inspect metric ranges. (2) Train XGBoost on CAES subset, report per-class F1. (3) Compare Random Forest and RoBERTa-BNE on MGT task.

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter choices for classifiers and RoBERTa-BNE fine-tuning are not specified, limiting exact replication.
- Missing value handling for failed metric computations is not detailed, introducing potential variability.
- Heavy class imbalance in the readability dataset may skew results despite macro-averaged F1 reporting.

## Confidence
- **Toolkit feature coverage and performance**: High
- **Exact score reproduction**: Medium (due to unspecified training configurations)
- **Generalizability across Spanish genres**: Medium (evaluation focused on educational and MGT corpora)

## Next Checks
1. Replicate feature extraction on a held-out subset of CAES and verify metric ranges and missing value handling.
2. Train XGBoost and Random Forest models with default sklearn parameters and compare macro-F1 to reported results.
3. Test model robustness by evaluating on a new Spanish readability or MGT dataset not seen during training.