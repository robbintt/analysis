---
ver: rpa2
title: How Much Progress Did I Make? An Unexplored Human Feedback Signal for Teaching
  Robots
arxiv_id: '2407.06459'
source_url: https://arxiv.org/abs/2407.06459
tags:
- progress
- feedback
- demonstrations
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces and characterizes a novel human teaching
  signal called "progress" for robot learning from demonstrations. Progress represents
  the completion percentage of a task and is designed to complement human demonstrations
  by providing objective feedback about task completion.
---

# How Much Progress Did I Make? An Unexplored Human Feedback Signal for Teaching Robots

## Quick Facts
- arXiv ID: 2407.06459
- Source URL: https://arxiv.org/abs/2407.06459
- Authors: Hang Yu; Qidi Fang; Shijie Fang; Reuben M. Aronson; Elaine Schaertl Short
- Reference count: 40
- Key outcome: Progress, a task completion percentage signal, shows greater consistency than scalar feedback and effectively identifies task completion status across non-expert demonstrations.

## Executive Summary
This paper introduces "progress" as a novel human teaching signal for robot learning from demonstrations, where humans annotate trajectories with completion percentages (0-100). Through three studies with 116 participants, the authors validate that progress is more consistent than scalar feedback, requires no additional workload, and correlates strongly with learned reward functions. The authors also release a dataset of 40 non-expert demonstrations showing that real-world demonstrations are multi-policy and sub-optimal beyond simple teleoperation errors.

## Method Summary
The method involves collecting human demonstrations via teleoperation, segmenting trajectories at fixed intervals (10% of frames), and having participants annotate each waypoint with progress (0-100 completion percentage) and scalar feedback. Statistical analysis includes t-tests, Wilcoxon Rank-Sum tests, and Bayesian analysis using Bayes Factors. The validation uses both controlled scenarios (Perfect/Imperfect/Unaware/Corrected/Failure) and real-world public demonstrations in an ice cream topping task.

## Key Results
- Progress shows greater consistency across participants than scalar feedback (avg std_progress = 11.6 vs avg std_scalar_feedback = 25.5, p < 0.001, BF > 1000)
- Progress effectively identifies unproductive but harmless behaviors without requiring additional annotation time or workload
- Progress correlates strongly with learned reward functions (avg r = 0.83) compared to scalar feedback (avg r = 0.19)

## Why This Works (Mechanism)

### Mechanism 1
Progress uses objective reference points (task start = 0%, task end = 100%) that ground annotations in observable task states, reducing subjective interpretation variance compared to scalar feedback which lacks such anchors. Tasks without clear intermediate milestones may reduce consistency gains.

### Mechanism 2
Progress tracks cumulative advancement toward completion rather than trajectory quality, allowing it to distinguish "harmless exploration" from "task failure." Exploratory actions that don't advance the task cause progress to plateau rather than decrease, preserving the completion signal. Tasks where exploration significantly reverses progress may conflate exploration with failure.

### Mechanism 3
Progress and reward functions both encode cumulative advancement toward goal states, allowing progress to serve as a proxy or initialization for reward learning. Tasks with deceptive reward structures or sparse true rewards may show weaker correlation.

## Foundational Learning

- **Learning from Demonstration (LfD)**: Understanding that demonstrations are policy samples and imitation learning objectives are prerequisite. Quick check: Can you explain why behavior cloning fails when demonstrations are sub-optimal?
- **Scalar Feedback in Interactive RL**: The baseline comparison is scalar feedback (0-100 ratings). Quick check: What makes scalar feedback unreliable for comparing two partial demonstrations?
- **Bayesian Statistics and Bayes Factors**: The paper uses Bayes Factors for hypothesis testing (BF < 3 = no evidence, 3-10 = moderate, 30+ = strong). Quick check: What does BF = 0.34 mean in context of comparing Imperfect vs. Perfect scenarios?

## Architecture Onboarding

- Component map: Demonstration Recording (5Hz state capture) -> Annotation Interface (Progress/slider input) -> Statistical Analysis Pipeline (t-tests, Wilcoxon Rank-Sum, Bayes Factors) -> Reward Learning Module (AIRL)
- Critical path: 1) Collect demonstration trajectories with state-action pairs 2) Segment trajectories at fixed intervals (10% of frames) 3) Collect progress annotations at each waypoint 4) Validate correlation between progress and task completion ground truth 5) Apply progress for data filtering, ranking, or reward initialization
- Design tradeoffs: Fixed vs. event-based segmentation; Self- vs. other-provided demonstrations; Progress range (0-100 vs. continuous)
- Failure signatures: Progress never exceeds 30% → unclear task definition; High variance (std > 20) in terminal states → lacks observable completion criteria; Progress decreases mid-trajectory → conflating quality with completion
- First 3 experiments: 1) Replicate tea-steeping experiment with your own task; verify threshold ~90 distinguishes scenarios 2) Compare progress annotation time against scalar feedback using NASA TLX; confirm no significant workload increase 3) Train IRL reward function; verify correlation exceeds 0.7

## Open Questions the Paper Calls Out
- Does training a robot policy using progress signals result in better performance or sample efficiency compared to policies trained with other feedback types?
- Can progress signals be effectively utilized to detect and prevent reward hacking in reinforcement learning agents?
- Can human teachers provide consistent and accurate progress estimates in real-time during live teleoperation without degrading demonstration quality?

## Limitations
- Dataset size (40 demonstrations) may be insufficient to capture full diversity of non-expert behaviors
- Study focuses on a single task type, limiting generalizability to more complex or abstract tasks
- Correlation validation limited to one specific IRL method (AIRL)

## Confidence
- **High Confidence**: Progress shows greater consistency across participants than scalar feedback (strong statistical support with BF > 1000, p < 0.001)
- **Medium Confidence**: Progress effectively identifies unproductive but harmless behaviors (supported by qualitative observations and participant behaviors)
- **Medium Confidence**: Progress annotations require no additional time or workload compared to scalar feedback (supported by NASA TLX results)

## Next Checks
1. Test progress annotation consistency across multiple task types (manipulation, navigation, assembly) to validate generalizability
2. Validate the correlation between progress and reward functions using alternative IRL methods (GAIL, MaxEnt IRL) beyond AIRL
3. Conduct a larger-scale study with 100+ participants across diverse tasks to establish robustness of progress as a teaching signal