---
ver: rpa2
title: Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked
  Graph Reconstruction Loss
arxiv_id: '2511.11181'
source_url: https://arxiv.org/abs/2511.11181
tags:
- graph
- clustering
- multi-view
- loss
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses incomplete multi-view clustering (IMVC) by
  proposing a dynamic deep graph learning framework that simultaneously handles missing
  data and learns refined graph structures. The core idea is to use a GCN-based embedding
  layer to impute missing views with a global graph and dynamically construct view-specific
  graphs, which are refined by the global graph and enhanced via graph structure contrastive
  learning.
---

# Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss

## Quick Facts
- **arXiv ID:** 2511.11181
- **Source URL:** https://arxiv.org/abs/2511.11181
- **Reference count:** 22
- **Primary result:** Proposed DGIMVCM framework achieves up to 10% accuracy improvement over state-of-the-art IMVC methods across four datasets under various missing rates (0.1–0.7).

## Executive Summary
This paper addresses incomplete multi-view clustering (IMVC) by proposing a dynamic deep graph learning framework that simultaneously handles missing data and learns refined graph structures. The core idea is to use a GCN-based embedding layer to impute missing views with a global graph and dynamically construct view-specific graphs, which are refined by the global graph and enhanced via graph structure contrastive learning. A GAT-based encoder is then employed to adaptively learn edge weights and extract high-level features, optimized with a masked graph reconstruction loss to reduce gradient noise. Finally, a clustering module is trained using pseudo-label self-supervision. Experiments on four datasets (Handwritten, Scene-15, Landuse-21, 100Leaves) show the method consistently outperforms state-of-the-art IMVC approaches, achieving accuracy improvements of up to 10% under various missing rates (0.1–0.7). Ablation studies confirm the effectiveness of each component, especially the masked reconstruction loss and dynamic graph learning.

## Method Summary
The DGIMVCM framework consists of three main components: (1) A GCN-based embedding layer that uses a global graph to impute missing views by propagating features from complete neighbors, while dynamically constructing view-specific graphs; (2) A GAT-based encoder that extracts high-level features with adaptive edge weights, optimized using a masked graph reconstruction loss that focuses only on the strongest predicted edges to reduce gradient noise; (3) A clustering module that uses K-means pseudo-labels and KL divergence for self-supervised training. The model is trained with three losses: masked reconstruction loss for denoising, contrastive loss for consistency among view-specific graph structures, and KL divergence for clustering. The global graph is constructed once using RBF similarity with missing-robust pruning, while view-specific graphs are dynamically updated during training.

## Key Results
- DGIMVCM consistently outperforms state-of-the-art IMVC methods across four datasets with accuracy improvements up to 10%
- Ablation studies confirm the effectiveness of each component, particularly the masked reconstruction loss and dynamic graph learning
- The method demonstrates robustness across various missing rates (0.1–0.7) with consistent performance gains
- Dynamic view-specific graph construction and imputation show significant benefits over static KNN graphs

## Why This Works (Mechanism)

### Mechanism 1: Global Graph-Based Imputation
Imputing missing views via a global graph propagation is more robust than relying on static, view-specific KNN graphs. A global adjacency graph $A$ is constructed by aggregating view-specific similarities while pruning edges connected to missing samples. A GCN-based embedding layer then uses this $A$ to propagate features, allowing missing samples ($M_{iv}=0$) to receive information from their complete neighbors, effectively denoising and imputing features simultaneously. This assumes the global graph topology captures sufficient cross-view semantic consistency to accurately estimate missing view features.

### Mechanism 2: Graph Structure Contrastive Learning
Enforcing consistency between view-specific graph structures via contrastive learning preserves feature diversity better than aligning the features directly. Instead of forcing view-specific features $Z^v$ to be identical, the method computes similarity rows $\hat{s}^v_i$ for each sample $i$. It then applies a contrastive loss (InfoNCE-style) to maximize the similarity of these graph structure rows across views for the same sample, while pushing apart rows of different samples. This assumes view-specific features contain complementary information that would be lost if directly fused, but the underlying graph topology should remain consistent across views.

### Mechanism 3: Masked Graph Reconstruction Loss
A masked graph reconstruction loss reduces optimization gradient noise by focusing only on the strongest predicted edges. Traditional MSE on sparse graphs applies a "repulsive" gradient to all non-adjacent nodes (a massive number), creating noise. This method creates a mask $M^v$ selecting only the top-$k$ strongest edges in the reconstructed graph $\hat{A}^v$. The loss $L_{rec}$ computes MSE only on these $k$ edges against the global graph $A$, filtering out the noisy repulsive gradients from irrelevant non-edges. This assumes the "hard" or "informative" edges are contained within the top-$k$ predictions, and ignoring the vast majority of non-edges acts as a denoiser rather than information loss.

## Foundational Learning

- **Concept: Graph Convolutional Networks (GCN)**
  - **Why needed here:** Used as the embedding layer to handle missing data. Standard GCNs require a graph and features; here, the GCN *generates* the features $Z^v$ by propagating over the global graph $A$, which is critical for the "imputation" aspect.
  - **Quick check question:** Can a standard GCN layer process a feature matrix $X$ where rows are zero (missing), or does it rely on the adjacency matrix to fill them? (Answer: It relies on the adjacency matrix to aggregate neighbor features).

- **Concept: Graph Attention Networks (GAT)**
  - **Why needed here:** The encoder uses GAT to learn dynamic edge weights $\tilde{A}^v$. Unlike the embedding layer which uses fixed weights, the encoder needs to adaptively weigh the importance of neighbors for the final clustering.
  - **Quick check question:** How does GAT differ from GCN in weighting neighbors? (Answer: GCN uses fixed normalization of adjacency; GAT learns weights via self-attention).

- **Concept: Self-Supervised Clustering (KL Divergence)**
  - **Why needed here:** The clustering module uses KL divergence to align soft cluster assignments $Q$ with sharpened "pseudo-labels" $P$ derived from K-means on the fused features.
  - **Quick check question:** Why use KL divergence here instead of standard Cross-Entropy? (Answer: To handle the soft probabilities of the Student's t-distribution effectively, treating the pseudo-labels as a target distribution).

## Architecture Onboarding

- **Component map:** Input ($\mathcal{X}$) -> Global Graph Construction -> Embedding/Imputation ($Z^v$) -> Masked Reconstruction ($H^v$) -> Clustering Head
- **Critical path:** Global Graph Construction -> Embedding/Imputation ($Z^v$) -> Masked Reconstruction ($H^v$). If the Global Graph is poor, imputation fails, and the Masked Loss has no valid target.
- **Design tradeoffs:**
  - **Static vs. Dynamic Graphs:** The paper moves from static KNN to dynamic graphs. This increases computation (recomputing graphs/features) but claims better noise robustness.
  - **Masking $k$:** A small $k$ reduces noise but risks losing true connections for sparse classes. The paper suggests $k$ is a sensitive hyperparameter (though analysis shows robustness in a range).
- **Failure signatures:**
  - **Imputation Collapse:** If the global graph is disconnected due to extreme missing rates, imputed features $Z^v$ become meaningless (zero or static vectors).
  - **Gradient Noise (High $k$):** If $k$ in masked loss is too high, the "repulsive" gradient term dominates, preventing cluster formation (features spread out too much).
- **First 3 experiments:**
  1. **Ablation of Masked Loss:** Run with standard MSE vs. Masked MSE on Scene-15 to verify gradient noise reduction (See Table 3).
  2. **Missing Rate Stress Test:** Run on Handwritten/100Leaves with missing rates $\delta \in [0.1, 0.7]$ to test the robustness of the global graph imputation (See Figure 2).
  3. **Component Isolation:** Remove the embedding layer (imputation) to see if performance drops specifically on missing samples, validating the "Dynamic View-specific Graph" necessity (See Table 2).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions remain unresolved regarding scalability, dynamic graph updating, and robustness to non-random missing patterns.

## Limitations
- The method lacks precise specification of key hyperparameters (TopK $k$, learning rate, hidden dimensions), which could significantly impact reproducibility and performance.
- The "masked graph reconstruction loss" is a novel contribution, but its theoretical justification for noise reduction is not rigorously proven beyond gradient analysis.
- The dynamic view-specific graph construction relies heavily on the quality of the global graph imputation, which may fail under extreme missing rates (>0.7).
- The paper does not address computational complexity or scalability concerns for large datasets, which could limit real-world applicability.

## Confidence

- **High:** The overall framework combining GCN imputation, GAT encoding, and masked reconstruction is well-grounded and produces consistent experimental improvements.
- **Medium:** The specific implementation details of the masked reconstruction loss and its gradient noise reduction mechanism are described but not fully validated through ablation.
- **Low:** The paper does not address computational complexity or scalability concerns for large datasets, which could limit real-world applicability.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary TopK $k$ (for both global graph and masked loss) and learning rate to identify optimal ranges and verify claimed robustness.

2. **Extreme Missing Rate Test:** Evaluate performance on synthetic data with missing rates exceeding 0.7 to determine the breaking point of the global graph imputation mechanism.

3. **Computational Complexity Measurement:** Benchmark memory usage and runtime for the $N \times N$ similarity matrix construction on the largest dataset (Scene-15) to assess scalability limitations.