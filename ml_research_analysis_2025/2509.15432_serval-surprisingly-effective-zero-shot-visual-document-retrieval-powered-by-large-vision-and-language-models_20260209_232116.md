---
ver: rpa2
title: 'SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered
  by Large Vision and Language Models'
arxiv_id: '2509.15432'
source_url: https://arxiv.org/abs/2509.15432
tags:
- yang
- wang
- large
- text
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits zero-shot visual document retrieval by generating
  detailed textual descriptions of document images using vision-language models (VLMs),
  which are then encoded by standard text encoders. Evaluated on the ViDoRe-v2 and
  MIRACL-VISION benchmarks, this approach achieves 63.4% nDCG@5, surpassing state-of-the-art
  supervised multi-vector VDR models without any task-specific training.
---

# SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models

## Quick Facts
- arXiv ID: 2509.15432
- Source URL: https://arxiv.org/abs/2509.15432
- Authors: Thong Nguyen; Yibin Lei; Jia-Huei Ju; Andrew Yates
- Reference count: 16
- Key outcome: Achieves 63.4% nDCG@5 on ViDoRe-v2, surpassing state-of-the-art supervised multi-vector VDR models without any task-specific training

## Executive Summary
This paper presents SERVAL, a surprisingly effective zero-shot visual document retrieval system that leverages vision-language models (VLMs) to generate detailed textual descriptions of document images, which are then encoded by standard text encoders for retrieval. The approach transforms cross-modal retrieval into a text-to-text matching task, achieving state-of-the-art results on both ViDoRe-v2 and MIRACL-VISION benchmarks without any task-specific training. SERVAL particularly excels at multilingual retrieval, outperforming specialized supervised models on non-English corpora.

## Method Summary
SERVAL converts visual document retrieval into text-to-text matching by using VLMs to generate detailed textual descriptions of document images, which are then encoded by standard text encoders. The system employs a fixed prompt template asking VLMs to provide comprehensive descriptions including summaries, extracted text, and numerical values. During indexing, documents are processed offline by VLMs (Qwen2.5VL or InternVL3) to generate descriptions (~500-1000 tokens), which are then encoded using dense or sparse text encoders (INF, mE5, SFR-Mistral, or Splade-v3). At retrieval time, queries are encoded and matched against the precomputed document vectors, keeping online latency comparable to text-only retrieval systems.

## Key Results
- Achieves 63.4% nDCG@5 on ViDoRe-v2, surpassing state-of-the-art supervised multi-vector VDR models (57.8%)
- On MIRACL-VISION multilingual benchmark, SERVAL reaches 72.1 nDCG@10 vs 61.6 for strongest baseline
- Text encoder scaling provides greater benefit than VLM scaling, with INF-7B encoder improving results by +8.4 nDCG@5 points over mE5-large
- Excels on low-resource languages, achieving +25.3 points on Yoruba and +15.5 on Swahili compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modern VLMs can act as a "semantic proxy" for visual documents by generating descriptions that capture both textual content and visual elements with sufficient retrieval-grade granularity
- Mechanism: A VLM processes document images and generates detailed textual descriptions (~500 tokens/doc on average) containing summaries, extracted text, and numerical values, transforming the cross-modal retrieval problem into a text-to-text matching task
- Core assumption: VLMs can accurately describe visual document elements without significant hallucination or information loss that would harm retrieval relevance
- Evidence anchors: VLM-generated descriptions successfully capture complex visual elements like NVIDIA A100 sparsity gains; modern VLMs provide reusable semantic proxies for cross-modal retrieval

### Mechanism 2
- Claim: Scaling text encoders yields greater retrieval gains than scaling VLMs in the generate-and-encode pipeline
- Mechanism: The VLM-generated description is encoded by a pretrained text encoder that maps both queries and descriptions into a shared semantic space; since description quality saturates at moderate VLM scales, the bottleneck shifts to the encoder's semantic representation capacity
- Core assumption: The text encoder's semantic representation capacity is more constraining than VLM description quality beyond a certain scale threshold
- Evidence anchors: Upgrading from mE5-large (560M) to INF-7B yields +8.4 nDCG@5 points, while upgrading VLM from 7B to 32B with fixed INF-7B encoder yields only +1.6 points

### Mechanism 3
- Claim: Generate-and-encode excels on multilingual and low-resource language retrieval because text encoders have broader language coverage than specialized VDR models trained on limited multilingual corpora
- Mechanism: The VLM generates descriptions in a pivot language (English by default), which are then encoded by multilingual text encoders trained on diverse corpora, bypassing the need for VDR-specific multilingual training data
- Core assumption: VLMs can adequately describe non-English documents in English, and multilingual text encoders can bridge the query-description language gap
- Evidence anchors: SERVAL achieves 72.1 nDCG@10 vs. 61.6 for strongest baseline on MIRACL-VISION, with +25.3 points on Yoruba and +15.5 on Swahili

## Foundational Learning

- Concept: **Cross-modal retrieval via modality translation** - Why needed: SERVAL converts image-to-text retrieval into text-to-text retrieval by translating visual documents to text. Understanding this paradigm helps contrast it with end-to-end multimodal embedding approaches. Quick check: Can you explain why translating images to text might outperform direct image-text embedding, and under what conditions it would fail?

- Concept: **Zero-shot vs. supervised retrieval trade-offs** - Why needed: SERVAL is explicitly zero-shot yet outperforms supervised models. Understanding when zero-shot generalizes better helps set expectations for deployment. Quick check: What types of document distributions or query patterns would favor supervised VDR models over SERVAL's zero-shot approach?

- Concept: **Offline preprocessing vs. online latency budgeting** - Why needed: VLM description generation is offline-only, making online retrieval latency equivalent to text encoder inference. This architectural choice affects deployment cost modeling. Quick check: For a 10M document corpus with 0.3s/image VLM generation latency, how would you budget the indexing pipeline duration, and what throughput would you target?

## Architecture Onboarding

- Component map: Image → VLM Description Generator (Qwen2.5VL/InternVL3) → Text Encoder (INF/mE5/SFR-Mistral/Splade) → Vector Index → Retrieval Engine
- Critical path: 
  1. Indexing (offline): Image → VLM → Description → Text Encoder → Vector → Index
  2. Retrieval (online): Query → Text Encoder → Vector → k-NN Search → Ranked Documents
- Design tradeoffs:
  - 2B VLMs are ~3x faster (0.26s vs. 0.81s for 32B) but produce shorter descriptions (~500 vs. ~1000 tokens), degrading retrieval by ~2-4 nDCG points
  - Sparse encoders (Splade) underperform on multilingual tasks; dense multilingual encoders (INF, mE5) preferred
  - 32B/72B VLMs require H100-class GPUs for reasonable throughput; 2B-8B VLMs run on A100s with tolerable latency
- Failure signatures:
  - VLM hallucination of non-existent content leading to irrelevant retrieval matches
  - Sparse encoder on multilingual data failing on non-English corpora (16.4 nDCG@5 on SAXAM multilingual vs. 53.5 with mE5-large)
  - Description length mismatch causing semantic gap between short queries and long descriptions
  - VLM struggles with low-resource scripts producing generic descriptions that lose discriminative information
- First 3 experiments:
  1. Baseline reproduction on ViDoRe-v2 subset (3 corpora: 1 English, 1 multilingual, 1 cross-lingual) with InternVL3-2B + INF-1.5B, targeting ~54.0 average nDCG@5
  2. VLM scale ablation: Fix INF-7B encoder, compare InternVL3-2B vs. InternVL3-8B vs. Qwen2.5VL-7B on 2 corpora, measuring nDCG@5 delta and latency per document
  3. Encoder robustness test on noisy descriptions: Inject 10% noise into descriptions, measure nDCG@5 degradation with INF-7B vs. mE5-large to assess encoder tolerance

## Open Questions the Paper Calls Out
None

## Limitations
- VLM dependency on description quality: Documents with fine-grained layout relationships, proprietary symbols, or dense tabular data may challenge VLM description capabilities
- Limited ablation on document types: Evaluation focuses on structured documents; performance on unstructured documents, handwritten notes, or documents with heavy visual design remains untested
- Cost asymmetry in indexing: Offline VLM description generation represents substantial computational cost (approximately 3,472 hours for 10M documents with 0.3s/image latency)

## Confidence
- High confidence: Core mechanism of converting cross-modal retrieval to text-to-text matching works as described; nDCG@5 improvements over supervised baselines are statistically significant and reproducible
- Medium confidence: Claim that text encoder scaling provides greater benefit than VLM scaling is supported by ablation data but lacks theoretical justification for the underlying mechanism
- Medium confidence: Multilingual advantage claim is demonstrated on MIRACL-VISION, but pivot-language approach may not generalize to all language pairs or document types

## Next Checks
1. Document type generalization test: Evaluate SERVAL on diverse document types including handwritten notes, invoices with complex layouts, and documents with heavy graphical elements. Compare nDCG@5 degradation against baseline models to quantify method's robustness boundaries.

2. Error analysis of VLM hallucinations: Systematically inject controlled hallucinations into descriptions (e.g., incorrect numerical values, fabricated content) and measure retrieval degradation with different text encoders. Establish tolerance threshold for VLM imperfections and inform error budgeting.

3. Cost-benefit scaling analysis: For 1M document collection, measure trade-off between VLM size (2B vs 32B), text encoder size, and overall retrieval quality. Calculate indexing cost per nDCG@5 point gained to identify optimal configuration for production deployment.