---
ver: rpa2
title: Introduction to Sequence Modeling with Transformers
arxiv_id: '2502.19597'
source_url: https://arxiv.org/abs/2502.19597
tags:
- sequence
- transformer
- input
- output
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a systematic introduction to transformer-based
  sequence modeling by incrementally building and analyzing the essential components
  needed for the architecture to function properly. Starting from a minimal transformer
  model, the author demonstrates how tokenization, embedding, masking, positional
  encoding, and padding are each required to solve increasingly complex sequence-to-sequence
  tasks.
---

# Introduction to Sequence Modeling with Transformers

## Quick Facts
- arXiv ID: 2502.19597
- Source URL: https://arxiv.org/abs/2502.19597
- Authors: Joni-Kristian Kämäräinen
- Reference count: 7
- Key outcome: Systematically builds transformer components for sequence-to-sequence tasks, achieving 0.1117 cross-entropy loss after 2000 epochs on mixed dataset

## Executive Summary
This paper presents a systematic introduction to transformer-based sequence modeling by incrementally building and analyzing essential components needed for the architecture to function properly. Starting from a minimal transformer model, the author demonstrates how tokenization, embedding, masking, positional encoding, and padding are each required to solve increasingly complex sequence-to-sequence tasks. Through a series of experiments using binary sequences (0s and 1s), the paper shows that while the basic transformer can learn simple patterns, it fails on tasks requiring knowledge of sequence length or order. The final model successfully learns all presented tasks simultaneously.

## Method Summary
The method employs a progressive experimental approach, starting with a PlainTransformer using scalar inputs and MSE loss, then incrementally adding token embeddings, causal masking, positional encoding, and padding support. The model uses torch.nn.Transformer with norm_first=True, Adam optimizer (lr=0.01), and multi-step learning rate scheduler. Binary sequences of length 1-4 tokens are used for eight task types including copy, inversion, many-to-one, one-to-many, and alternating patterns. The final configuration uses d_model=8, nhead=1, num_encoder_layers=1, num_decoder_layers=1, with cross-entropy loss and padding masks.

## Key Results
- PlainTransformer with scalar inputs converges to mean output (0.5) regardless of input
- Token embeddings enable symbol-to-symbol mappings but cause infinite sequence generation
- Causal masking resolves generation length issues but fails on order-dependent sequences
- Positional encoding enables distinction between different orderings of same tokens
- Full model achieves 0.1117 cross-entropy loss after 2000 epochs on mixed dataset

## Why This Works (Mechanism)

### Mechanism 1: Token embedding enables non-trivial symbol representations
- Claim: Raw scalar inputs cause transformers to converge to mean outputs; learnable embedding vectors enable the architecture to apply meaningful non-linear transformations.
- Mechanism: `torch.Embedding()` creates D-dimensional lookup vectors for each token. These higher-dimensional representations allow the transformer's attention and feedforward layers to learn discriminative features rather than collapsing to the sample mean.
- Core assumption: The transformer's internal operations require sufficiently high-dimensional spaces to learn useful representations.
- Evidence anchors:
  - [abstract]: "Token embeddings enable learning of symbol-to-symbol mappings"
  - [section 2.2]: "There are no non-linear mappings in the processing pipeline, and therefore the plain transformer converges to produce the output sample mean that is the single value minimizing error."
  - [corpus]: Related work on transformer approximation (arXiv:2510.06662) discusses expressive power but does not specifically validate embedding necessity.
- Break condition: If embedding dimension equals 1 or all inputs are identical, the mechanism provides no additional representational capacity.

### Mechanism 2: Causal masking aligns training and inference distributions
- Claim: Without causal masking, transformers fail on generation tasks because training allows the decoder to "see future" tokens that do not exist during inference.
- Mechanism: The causal mask sets attention weights for future positions to `-inf` before softmax, producing zero attention to those positions. Each prediction attends only to previously observed tokens, matching autoregressive inference.
- Core assumption: Models cannot generalize from full-visibility training to partial-visibility inference without explicit alignment.
- Evidence anchors:
  - [abstract]: "masking is necessary to prevent the model from seeing future tokens during inference"
  - [section 2.2]: "The transformer is trained to 'see the future', but during inference the future does not exist...and therefore the transformer is confused."
  - [corpus]: No direct corpus validation found for this specific masking mechanism.
- Break condition: For many-to-one tasks with single output tokens, masking is unnecessary—no future exists to hide.

### Mechanism 3: Positional encoding provides order discrimination
- Claim: Without positional encoding, attention produces identical representations for sequences containing the same tokens in different orders.
- Mechanism: Sin/cos positional vectors are added to token embeddings, making each position's representation unique. Attention can then distinguish "token 0 at position 1" from "token 0 at position 3."
- Core assumption: Attention scores computed as token-to-token similarities are inherently position-agnostic.
- Evidence anchors:
  - [abstract]: "Positional encoding resolves the model's inability to distinguish between different orderings of the same tokens"
  - [section 2.3]: "The correlation value is the same between same tokens despite their position. Therefore it is not possible to make the internal representation aware of in which order the samples were observed."
  - [corpus]: Related introductions (arXiv:2304.10557) discuss representation learning but do not provide independent validation of positional encoding necessity.
- Break condition: If the task does not require order sensitivity (e.g., bag-of-words classification), positional encoding may not be necessary.

## Foundational Learning

- **Concept: Query-Key-Value attention**
  - Why needed here: The paper assumes familiarity with attention as the "main working horse." Understanding that attention computes similarity-based weights explains why it is position-agnostic.
  - Quick check question: Would identical tokens at different positions produce different attention scores without positional encoding?

- **Concept: Autoregressive generation**
  - Why needed here: Causal masking exists because inference generates tokens one-by-one. Without this mental model, the masking requirement is opaque.
  - Quick check question: When generating token 3, which output tokens can the model attend to?

- **Concept: Cross-entropy loss for classification**
  - Why needed here: The paper switches from MSE to Cross-Entropy when moving from scalar outputs to token classification. Understanding this distinction is essential for debugging "un-embedding" behavior.
  - Quick check question: Why is MSE unsuitable for predicting tokens from a discrete vocabulary?

## Architecture Onboarding

- **Component map:** PlainTransformer -> add embedding -> add causal masking -> add positional encoding -> add padding
- **Critical path:** PlainTransformer → add embedding → add causal masking → add positional encoding → add padding. Skipping any step produces specific failures on increasingly complex tasks.
- **Design tradeoffs:**
  - `norm_first=True` stabilizes training (pre-norm) but deviates from original architecture.
  - Larger embedding dimensions increase capacity but also parameter count.
  - Padding is efficient when sequences are similar length; highly variable lengths may favor separate training.
- **Failure signatures:**
  - Output always equals training mean → missing or ineffective embedding.
  - Infinite-length output sequences → missing causal mask.
  - Cannot distinguish `0-1-0-1` from `1-0-1-0` → missing positional encoding.
  - Loss does not decrease on padded batches → padding not masked in loss or attention.
- **First 3 experiments:**
  1. Train PlainTransformer on `0,0,0,0 → 0,0,0,0` and `1,1,1,1 → 1,1,1,1`. Observe output = 0.5 everywhere.
  2. Add token embedding. Observe correct tokens but incorrect sequence lengths (infinite generation).
  3. Add causal masking. Observe correct one-to-many generation, but failure on order-dependent sequences like `0,1,0,1 → 0,1,0,1`.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the component-by-component understanding gained from binary sequences (0s and 1s) generalize to real-world sequence modeling tasks with larger vocabularies and more complex dependencies?
- Basis in paper: [explicit] The paper states: "Simple sequences of zeros (0) and ones (1) are used to study the workings of each step" and demonstrates all findings on this minimal domain.
- Why unresolved: The paper intentionally constrains its experiments to binary sequences with only 4 tokens (0, 1, SOS, EOS) plus padding. No validation on natural language, code, or other real-world sequence domains is provided.
- What evidence would resolve it: Systematic experiments applying the same incremental component analysis to tasks with larger vocabularies (e.g., character-level text, subword tokens) and longer-range dependencies, comparing whether the same component failures occur in the same order.

### Open Question 2
- Question: How does the choice of sinusoidal positional encoding compare to learnable positional embeddings or newer alternatives (e.g., rotary position embeddings) on these foundational sequence discrimination tasks?
- Basis in paper: [explicit] "The adopted positional encoding is the one proposed in the original paper [5]. sin and cos functions are used to generate positional vectors added to token embedding vectors."
- Why unresolved: The paper tests only the original Vaswani et al. sinusoidal encoding scheme. No ablation comparing positional encoding variants is performed.
- What evidence would resolve it: Controlled experiments comparing sinusoidal, learnable, and rotary positional encodings on the order-discrimination task (e.g., distinguishing 0,1,0,1 from 1,0,1,0), measuring convergence speed and final loss.

### Open Question 3
- Question: What are the failure modes of each component configuration on longer sequences or longer-range dependencies that exceed the tested 4-token maximum?
- Basis in paper: [inferred] All experiments use sequences of length 1-4 tokens. The paper does not investigate whether the same component requirements hold as sequence length increases.
- Why unresolved: The longest sequences tested are 4 tokens (e.g., "0,1,0,1"). Extrapolation to longer sequences is unknown.
- What evidence would resolve it: Experiments systematically varying sequence length (8, 16, 32, 64+ tokens) for each model configuration to identify at what length each component combination fails.

## Limitations
- Empirical scope limited to synthetic binary-sequence tasks without validation on real-world domains
- No cross-validation, ablation studies beyond sequential build-up, or comparison with established baselines
- Does not explore component interactions under noisy or ambiguous inputs
- No investigation of scalability to longer or more complex sequences

## Confidence
- **High confidence**: The necessity of token embeddings, causal masking, and positional encoding for their respective failure modes is demonstrated convincingly within the experimental setup.
- **Medium confidence**: The claim that the final model "learns all tasks simultaneously" is supported by the reported loss, but lacks per-task validation.
- **Low confidence**: The assertion that "no other architectural components are needed" for the presented tasks is not rigorously tested.

## Next Checks
1. Replicate per-task performance: After training the full model on all eight tasks, evaluate and report accuracy for each task individually to confirm balanced learning across all tasks.

2. Ablation of combined components: Remove one component (e.g., positional encoding) from the full model and retrain on the mixed dataset. Compare final loss and qualitative outputs to verify that each component remains necessary even when others are present.

3. Stress-test with noisy data: Modify the binary sequences to include occasional noise (e.g., random flips) and retrain the model. Assess whether the architecture degrades gracefully or if any component becomes disproportionately important under uncertainty.