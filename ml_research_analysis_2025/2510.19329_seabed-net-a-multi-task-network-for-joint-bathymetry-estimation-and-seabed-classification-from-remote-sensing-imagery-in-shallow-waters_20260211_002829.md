---
ver: rpa2
title: 'Seabed-Net: A multi-task network for joint bathymetry estimation and seabed
  classification from remote sensing imagery in shallow waters'
arxiv_id: '2510.19329'
source_url: https://arxiv.org/abs/2510.19329
tags:
- bathymetry
- classification
- depth
- seabed-net
- seabed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Seabed-Net, a novel multi-task learning framework
  that jointly estimates bathymetry and performs pixel-based seabed classification
  from remote sensing imagery of various resolutions. The key innovation lies in integrating
  two parallel encoders for bathymetry and seabed classification, combined with attention-based
  feature fusion and a Swin Transformer module to enable cross-task information exchange.
---

# Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters

## Quick Facts
- **arXiv ID:** 2510.19329
- **Source URL:** https://arxiv.org/abs/2510.19329
- **Reference count:** 15
- **Primary result:** Joint bathymetry estimation and seabed classification from remote sensing imagery of various resolutions

## Executive Summary
This paper introduces Seabed-Net, a novel multi-task learning framework that jointly estimates bathymetry and performs pixel-based seabed classification from remote sensing imagery of various resolutions. The key innovation lies in integrating two parallel encoders for bathymetry and seabed classification, combined with attention-based feature fusion and a Swin Transformer module to enable cross-task information exchange. By leveraging shared spatial and spectral representations, Seabed-Net improves performance across both tasks, especially in low-resolution settings where traditional single-task models typically degrade.

Evaluations on the MagicBathyNet dataset across two heterogeneous coastal sites (Agia Napa, Cyprus and Puck Lagoon, Poland) show that Seabed-Net consistently outperforms state-of-the-art baselines. It achieves up to 75% lower RMSE in bathymetry estimation compared to empirical and traditional machine learning methods, and up to 38% lower RMSE than single-task bathymetry networks. For seabed classification, it improves accuracy by up to 8% over single-task classifiers and up to 10% over multi-task baselines. Qualitative analysis confirms better spatial consistency, sharper habitat boundaries, and more accurate depth predictions in low-contrast regions.

## Method Summary
Seabed-Net employs a dual-encoder architecture with separate branches for bathymetry estimation and seabed classification. The bathymetry encoder processes spectral and spatial features through convolutional layers, while the classification encoder uses a Swin Transformer backbone for capturing long-range dependencies. An attention-based feature fusion module combines outputs from both encoders, enabling cross-task information exchange. The framework uses multi-task learning to optimize both objectives simultaneously, with shared representations learned through joint training. The model was trained and evaluated on the MagicBathyNet dataset, which contains multispectral imagery from Sentinel-2 and RedEdge-MX sensors across two coastal sites with varying water depths and seabed types.

## Key Results
- Achieves up to 75% lower RMSE in bathymetry estimation compared to empirical and traditional machine learning methods
- Improves seabed classification accuracy by up to 8% over single-task classifiers and up to 10% over multi-task baselines
- Demonstrates better spatial consistency, sharper habitat boundaries, and more accurate depth predictions in low-contrast regions

## Why This Works (Mechanism)
The dual-encoder architecture with attention-based feature fusion enables cross-task information exchange between bathymetry estimation and seabed classification. The Swin Transformer module captures long-range spatial dependencies that are crucial for both tasks. By sharing spatial and spectral representations through joint training, the model learns complementary features that benefit both objectives. The multi-task learning framework exploits the inherent relationship between water depth and seabed characteristics, where certain seabed types correlate with specific depth ranges, and vice versa.

## Foundational Learning
- **Multi-task learning**: Trains multiple related tasks simultaneously to leverage shared representations and improve generalization. Needed to capture the synergistic relationship between bathymetry and seabed classification. Quick check: Compare performance against single-task baselines.
- **Attention-based feature fusion**: Combines features from different modalities by learning weighted combinations. Needed to integrate bathymetry and classification features effectively. Quick check: Analyze attention weight distributions across different seabed types.
- **Swin Transformer architecture**: Uses shifted window mechanisms to capture local and global spatial dependencies efficiently. Needed for handling the complex spatial patterns in coastal imagery. Quick check: Compare with standard CNN-based feature extractors.
- **Multi-resolution analysis**: Processes imagery at different spatial resolutions to handle varying water depths and seabed complexity. Needed for robust performance across diverse coastal environments. Quick check: Evaluate performance across Sentinel-2 and RedEdge-MX resolutions.
- **Joint optimization**: Simultaneously minimizes loss functions for both tasks during training. Needed to maintain balance between bathymetry accuracy and classification performance. Quick check: Monitor task-specific loss curves during training.
- **Cross-task information exchange**: Enables features from one task to inform and improve the other task. Needed to exploit the inherent relationships between water depth and seabed characteristics. Quick check: Analyze feature similarity between encoder outputs.

## Architecture Onboarding

**Component map:**
Sentinel-2/RedEdge-MX imagery -> Dual encoders (Bathymetry CNN + Classification Swin Transformer) -> Attention-based fusion -> Multi-task outputs (Depth map + Seabed classification)

**Critical path:**
Input imagery → Bathymetry encoder → Classification encoder → Attention fusion → Joint optimization → Final predictions

**Design tradeoffs:**
- Dual encoders vs single encoder: Provides task-specific feature extraction while maintaining shared representations
- Swin Transformer vs CNN for classification: Better long-range dependency capture at computational cost
- Attention fusion vs concatenation: More flexible integration but requires additional parameters
- Multi-task vs single-task: Potential negative transfer vs computational efficiency

**Failure signatures:**
- Degraded bathymetry accuracy in turbid water conditions
- Classification confusion between similar seabed types
- Over-smoothing of depth predictions in areas with rapid depth changes
- Sensitivity to atmospheric correction quality in low-resolution imagery

**First experiments:**
1. Ablation study removing attention fusion to quantify cross-task benefits
2. Single-task variants to establish performance baselines
3. Sensitivity analysis across different Sentinel-2 spectral bands

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Sentinel-2 imagery introduces sensor-specific artifacts and atmospheric correction uncertainties
- Geographic evaluation limited to two Mediterranean and Baltic coastal sites, restricting generalizability
- Temporal aspect of data collection not discussed, raising questions about model robustness to seasonal variations
- Pre-trained Swin Transformer weights may not transfer well to all geographic regions
- 0.5m multispectral data represents a commercial sensor configuration that may not be universally available

## Confidence
**High:** Quantitative improvements over single-task baselines (75% lower RMSE in bathymetry, 8% higher accuracy in classification) are methodologically sound within tested dataset constraints.
**Medium:** Cross-task synergies and improvements in low-contrast regions require external validation on different sensor configurations and geographic settings.
**Low:** Claims about model generalizability to global shallow-water environments cannot be substantiated without testing across diverse geographic regions.

## Next Checks
1. Test model performance on datasets from tropical, temperate, and polar coastal environments to assess geographic generalizability.
2. Evaluate temporal robustness by testing on multi-season datasets to verify performance consistency across varying water clarity conditions.
3. Compare performance using different remote sensing platforms (PlanetScope, WorldView, UAV-based multispectral) to assess sensor transferability.