---
ver: rpa2
title: From Editor to Dense Geometry Estimator
arxiv_id: '2509.04338'
source_url: https://arxiv.org/abs/2509.04338
tags:
- depth
- editing
- estimation
- image
- fe2e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FE2E, a framework that adapts a pre-trained
  image editing model based on Diffusion Transformer (DiT) for dense geometry prediction
  tasks such as depth and normal estimation. Through systematic analysis, the authors
  demonstrate that editing models possess inherent structural priors that enable more
  stable convergence compared to generative models when fine-tuned for dense prediction
  tasks.
---

# From Editor to Dense Geometry Estimator

## Quick Facts
- arXiv ID: 2509.04338
- Source URL: https://arxiv.org/abs/2509.04338
- Reference count: 40
- Achieves state-of-the-art zero-shot monocular depth and normal estimation, with 35% improvement on ETH3D

## Executive Summary
FE2E adapts a pre-trained image editing Diffusion Transformer (DiT) model for dense geometry prediction tasks such as depth and normal estimation. The framework leverages structural priors inherent in editing models, which provide more stable convergence compared to generative models when fine-tuned for dense prediction. Through systematic analysis, FE2E demonstrates that editing models offer an effective and data-efficient foundation for zero-shot dense prediction tasks, achieving state-of-the-art results across multiple datasets while requiring significantly less training data than traditional approaches.

## Method Summary
FE2E fine-tunes the Step1X-Edit DiT model for deterministic dense estimation by reformulating the training objective to consistent velocity flow matching, where the model predicts the velocity between a fixed starting point (z₀=0) and target latent (z₁). The framework employs logarithmic quantization to resolve precision conflicts in BF16 training and leverages the DiT's global attention for cost-free joint estimation of depth and normals. A LoRA adapter is used for efficient fine-tuning, and the model achieves zero-shot performance across multiple datasets by splitting the DiT output into left and right latents for normals and depth respectively.

## Key Results
- Achieves state-of-the-art zero-shot performance across multiple datasets
- Over 35% improvement in absolute relative error on ETH3D benchmark
- Outperforms models trained on 100× more data (71k vs 62.6M images)
- Demonstrates stable convergence through structural priors from editing models

## Why This Works (Mechanism)
The framework exploits the inherent structural priors present in image editing models, which have been trained to understand global image composition and semantic relationships. By reformulating the objective to consistent velocity flow matching and fixing the starting point at zero, FE2E transforms the stochastic denoising process into a deterministic prediction task. The logarithmic quantization strategy resolves the precision limitations of BF16 training, particularly for depth values at extreme ranges, while the global attention mechanism of DiT enables joint estimation of depth and normals without additional computational overhead.

## Foundational Learning
- **Consistent Velocity Flow Matching**: Reformulates denoising objective from predicting noise to predicting deterministic velocity between fixed points. Needed to convert stochastic generation into deterministic dense prediction. Quick check: Verify output is deterministic (same input → same output).
- **Logarithmic Depth Quantization**: Applies log transform before quantization to handle wide depth ranges in BF16 precision. Needed because uniform/inverse quantization causes instability at depth extremes. Quick check: Compare training stability with log vs uniform quantization.
- **DiT Global Attention**: Uses global attention layers instead of local convolutions for capturing long-range dependencies. Needed for joint depth and normal estimation without separate heads. Quick check: Confirm both outputs are generated in single forward pass.
- **LoRA Fine-tuning**: Applies low-rank adaptation to base DiT weights for efficient parameter updates. Needed to preserve editing priors while adapting to geometry tasks. Quick check: Verify base model weights remain frozen during training.

## Architecture Onboarding
- **Component Map**: Image → VAE Encode → DiT (with LoRA) → Latent Split → VAE Decode → Depth/Normal Maps
- **Critical Path**: Image latent → DiT forward pass (removing timestep conditioning) → Split latents → MSE loss with quantized targets → Backprop through LoRA
- **Design Tradeoffs**: Global attention provides superior zero-shot generalization but increases computational load (28.9T MACs) compared to U-Net alternatives; logarithmic quantization ensures stability but adds preprocessing complexity.
- **Failure Signatures**: Depth stripping artifacts indicate incorrect quantization (uniform vs logarithmic); high loss oscillation suggests failure to fix starting point at zero; poor normal estimation indicates incorrect latent split.
- **First Experiments**:
  1. Verify VAE can encode/decode single-channel depth maps without artifacts
  2. Test training with uniform vs logarithmic quantization on small dataset
  3. Confirm DiT output shape and semantic split through forward pass

## Open Questions the Paper Calls Out
- **Dataset Scaling**: Can FE2E achieve further gains by scaling up to 62.6M images like DepthAnything, or do editing priors saturate with smaller datasets? The current study (71k images) intentionally focuses on data efficiency, leaving the upper bound unexplored.
- **Model Agnosticism**: Does the consistent velocity training objective and logarithmic quantization transfer effectively to other DiT architectures (e.g., Qwen-Image) without significant re-engineering? Current implementation is restricted to Step1X-Edit.
- **Computational Efficiency**: How can the DiT-based architecture be optimized to approach U-Net efficiency while maintaining high accuracy? The model acknowledges 28.9T MACs / 1.78s runtime as a limitation without offering solutions.

## Limitations
- Relies on proprietary Step1X-Edit model with limited public implementation details
- High computational load (28.9T MACs, 1.78s runtime) compared to U-Net alternatives
- Performance claims based on zero-shot evaluation without direct comparison to contemporary fine-tuned methods on same data distributions

## Confidence
- **High Confidence**: Theoretical framework for adapting editing models to dense prediction is well-reasoned and mathematically sound
- **Medium Confidence**: LoRA-based fine-tuning procedure and loss formulation are standard and reproducible
- **Low Confidence**: Exact zero-shot performance on ETH3D cannot be fully verified without access to base Step1X-Edit model and VAE

## Next Checks
1. Reimplement VAE encoding: Create synthetic test to verify VAE can encode/decode single-channel depth and normal maps without artifacts
2. Quantization stress test: Train minimal model with both logarithmic and uniform quantization on small depth dataset to demonstrate instability of uniform quantization at BF16 precision
3. Architecture modification audit: Document exact changes to DiT (removing timestep conditioning, fixing z₀=0) and verify output shape and semantic split through forward pass on sample data