---
ver: rpa2
title: Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer
arxiv_id: '2505.08327'
source_url: https://arxiv.org/abs/2505.08327
tags:
- task
- pruning
- proposed
- learning
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles continual learning (CL) under the class-incremental
  learning (CIL) setting, where task identities are unavailable during inference.
  The central problem is that large pre-trained models, while effective for mitigating
  catastrophic forgetting, incur high inference costs due to their computational complexity.
---

# Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer

## Quick Facts
- arXiv ID: 2505.08327
- Source URL: https://arxiv.org/abs/2505.08327
- Authors: Zhenrong Liu; Janne M. J. Huttunen; Mikko Honkala
- Reference count: 40
- Primary result: KD-based framework improves accuracy by 20+ percentage points over baselines while reducing inference costs (e.g., 50× fewer FLOPs)

## Executive Summary
This work addresses continual learning (CL) under class-incremental learning (CIL) settings where task identities are unavailable during inference. The central challenge is that large pre-trained models, while effective for mitigating catastrophic forgetting, incur high inference costs. To address this, the authors propose two frameworks that integrate model compression techniques—pruning and knowledge distillation (KD)—into CIL. The pruning-based framework includes pre-pruning (compressing the model before CIL training) and post-pruning (compressing after each task). The KD-based framework uses a teacher-student architecture where a larger pre-trained teacher transfers downstream-relevant knowledge to a compact student during training. Experiments on CIFAR-100, FGVC Aircraft, and Cars datasets show that the KD-based framework consistently improves accuracy by 20+ percentage points over baselines like LwF, iCaRL, and SS-IL, while maintaining significantly lower inference costs (e.g., 50× reduction in FLOPs).

## Method Summary
The authors propose two frameworks for efficient continual learning: a pruning-based approach and a knowledge distillation-based approach. The pruning framework uses L1 regularization on batch normalization parameters to identify and remove less important neurons, with pre-pruning (before CIL training) and post-pruning (after each task) strategies. The KD framework employs a teacher-student architecture where a large pre-trained teacher model (e.g., ResNet-34) transfers knowledge to a compact student model (e.g., MobileNetV2) through KL-divergence loss on output logits. The student learns from the teacher's representation of previous tasks while adapting to new ones. Both frameworks aim to improve the trade-off between accuracy and inference efficiency in CIL settings.

## Key Results
- KD-based framework consistently improves accuracy by 20+ percentage points over baselines (LwF, iCaRL, SS-IL)
- KD achieves 50× reduction in FLOPs compared to standard approaches
- Pruning-based framework achieves similar accuracy improvements under low pruning ratios but offers less compression
- KD approach's flexibility to use different architectures for teacher and student enables stronger efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A teacher-student framework with a large pre-trained teacher and a compact student can transfer downstream-relevant knowledge effectively, reducing inference complexity while maintaining strong class-incremental learning performance.
- **Mechanism:** The large teacher model, rich in generalizable features, is trained on the CL sequence. Its learned representations are distilled into a smaller student model via a KL-divergence loss on output logits. This process selectively transfers the most relevant knowledge for the downstream tasks to a lower-capacity model.
- **Core assumption:** The pre-trained teacher's knowledge is sufficiently general to cover the downstream domain, and this knowledge can be compressed into a student architecture with minimal loss of task-relevant information.
- **Evidence anchors:**
  - [abstract] "...KD-based framework adopts a teacher-student architecture, where a large pre-trained teacher transfers downstream-relevant knowledge to a compact student."
  - [section 3.3.1] "...teacher is a larger model used only during training, while the student is a compact model used for both training and inference."
  - [corpus] Corpus evidence is weak for this specific mechanism; related papers focus on general CL or LLMs, not explicitly on this teacher-student efficiency trade-off.
- **Break condition:** The domain of the downstream tasks is too distant from the pre-training data, or the student architecture is too small to capture the transferred knowledge, resulting in underfitting.

### Mechanism 2
- **Claim:** Structured pruning, guided by batch normalization scale parameters, can significantly reduce model FLOPs and parameters in a continual learning setting with controlled accuracy loss.
- **Mechanism:** An L1 penalty on batch normalization scale parameters during training drives them toward zero. Neurons/filters associated with the smallest scale values are considered less important for the current task distribution and are removed from the network, creating a permanently smaller computational graph.
- **Core assumption:** Neurons with small BN scale values contribute minimally to the output and can be safely removed without critically degrading the model's representational power for future, unseen tasks.
- **Evidence anchors:**
  - [section 3.2.1] "...scale parameters with larger magnitudes have a greater influence on layer outputs... neurons associated with these significant parameters are retained..."
  - [table 2] Shows clear reduction in FLOPs and parameters with varying pruning ratios.
  - [corpus] Not explicitly supported by the provided corpus neighbors.
- **Break condition:** The pruning ratio is too aggressive, removing capacity essential for future tasks, leading to high, unrecoverable performance drops.

### Mechanism 3
- **Claim:** Using a large, progressively updated teacher as the source of regularization for a compact student improves the stability-plasticity trade-off compared to standard self-regularization methods like LwF.
- **Mechanism:** The student is regularized by aligning its logits on previous classes with those from the large teacher model (trained on the previous task). The teacher's higher capacity and pre-trained features provide a more stable and informative reference for old knowledge than the student's own lower-capacity past state.
- **Core assumption:** A larger model's internal representation of past tasks is more robust and less prone to drift than a smaller model's, making it a superior "anchor" for regularization.
- **Evidence anchors:**
  - [section 3.3.2] "...replaces the previous-task student with a progressively updated teacher, which better prevents the forgetting of downstream-relevant and previously learned task knowledge..."
  - [section 4.4.1] "In terms of BWT, all methods benefit from KD, showing consistent improvements over their original versions."
  - [corpus] Not explicitly supported by the provided corpus neighbors.
- **Break condition:** The teacher model is trained without any forgetting mitigation, causing it to provide a corrupted reference signal for old tasks, which then degrades the student's performance on them.

## Foundational Learning
- **Concept:** **Class-Incremental Learning (CIL)**
  - **Why needed here:** The core problem this paper addresses. It involves learning a sequence of tasks where the model must classify among all classes seen so far at inference time, without task identities.
  - **Quick check question:** How does CIL differ from Task-Incremental Learning (TIL)?
- **Concept:** **Catastrophic Forgetting & Stability-Plasticity Trade-off**
  - **Why needed here:** The fundamental challenge. Forgetting is the loss of old knowledge; stability-plasticity is the balance between preserving old knowledge and learning new. The proposed methods aim to improve this trade-off via compression.
  - **Quick check question:** What are the two opposing forces in continual learning that make it difficult?
- **Concept:** **Knowledge Distillation (KD) Loss (KL-divergence)**
  - **Why needed here:** The core technical tool used for both compression (teacher-to-student) and regularization (preventing forgetting). Understanding how soft labels transfer knowledge is essential.
  - **Quick check question:** Why use soft labels (logits) from a teacher model instead of hard ground-truth labels?

## Architecture Onboarding
- **Component map:** The system consists of two parallel tracks. **Track 1 (Pruning-Based):** Pre-trained Model -> Fine-tune on initial task (with L1 BN penalty) -> Prune -> Pruned Model -> Sequential CIL Training. **Track 2 (KD-Based):** A **Teacher Model** (large, pre-trained, e.g., ResNet-34) and a **Student Model** (compact, pre-trained, e.g., MobileNetV2). Both are trained sequentially. The student's loss includes a KD term from the teacher.
- **Critical path:** For the KD-based framework, the critical path is the **student training loop**. The student must be updated on the new task with the KD loss from the teacher trained on the *previous* task to retain stability.
- **Design tradeoffs:**
  - **Pruning (Pre vs. Post):** Pre-pruning is simpler (one-time) but may prune capacity needed for future tasks. Post-pruning adapts to each task but is more complex and may suffer from unrecoverable forgetting during pruning.
  - **KD vs. Pruning:** KD offers higher architectural flexibility (teacher/student can be different) and often better FLOPs reduction for a given accuracy. Pruning is simpler to implement but may achieve less compression for the same performance.
- **Failure signatures:**
  - **Pruning:** Sudden accuracy drop on new tasks indicates over-pruning.
  - **KD:** BWT degrades significantly, suggesting the teacher is not being updated correctly or the student is failing to learn from it.
- **First 3 experiments:**
  1.  **Baseline Reproduction:** Implement a standard CIL method (e.g., LwF) on a dataset like CIFAR-100 to establish baseline ACC and BWT.
  2.  **Pruning Integration:** Implement pre-pruning with a 40% ratio on the base method and measure ACC, parameters, and FLOPs reduction. Compare to the baseline.
  3.  **KD Integration:** Implement the teacher-student framework using a ResNet-34 teacher and MobileNetV2 student. Integrate the KD loss as described for LwF. Compare ACC and BWT against both the baseline and the pruned version.

## Open Questions the Paper Calls Out
- **Question:** Can the proposed KD-based compression framework be effectively integrated with prompt-based continual learning methods that utilize frozen pre-trained models?
  - **Basis in paper:** [inferred] The authors acknowledge that prompt-based methods (e.g., L2P, DualPrompt) are strong recent performers that leverage large pre-trained models, but the experimental evaluation is restricted to standard architectures like ResNets and MobileNets without incorporating prompt-tuning strategies.
  - **Why unresolved:** It is unclear if distillation logic applies when the backbone is frozen and adaptation occurs solely through prompts, or if the compression student would lose the representational capacity required for effective prompting.
  - **What evidence would resolve it:** Experiments applying the KD framework to a prompt-based baseline (e.g., L2P) to see if the student model can retain prompt-compatibility while maintaining accuracy.

- **Question:** Does the pre-pruning strategy result in a saturation of plasticity in continual learning scenarios with significantly longer task sequences (e.g., >50 tasks)?
  - **Basis in paper:** [inferred] The paper relies on a 10-task split methodology and notes that pre-pruning fixes the model capacity based only on the initial task's distribution to approximate "downstream-relevant knowledge."
  - **Why unresolved:** While pre-pruning worked well for 10 tasks, removing parameters before seeing a long stream of future tasks may eventually restrict the model's ability to adapt to novel classes that diverge significantly from the initial task.
  - **What evidence would resolve it:** Scaling the number of incremental tasks to 50 or 100 and comparing the accuracy trajectory of the pre-pruned model against the KD-based model.

- **Question:** How does the framework perform in low-data regimes where the teacher's pre-training domain is significantly dissimilar to the downstream tasks?
  - **Basis in paper:** [inferred] The authors note that training a teacher from scratch fails on small datasets (Aircraft/Cars) and relies on ImageNet pre-training. However, they categorize Aircraft/Cars as "out-of-distribution" (OoD) relative to ImageNet.
  - **Why unresolved:** The paper demonstrates success on OoD fine-grained tasks, but it remains untested whether the teacher can effectively transfer "downstream-relevant knowledge" to a student if the domain gap is extreme (e.g., medical imaging or remote sensing) where ImageNet features are less useful.
  - **What evidence would resolve it:** Evaluating the KD framework on a dataset with a severe domain shift from the teacher's pre-training data (e.g., medical imaging) to assess if the compression gains remain stable.

## Limitations
- Results are primarily based on three image classification datasets (CIFAR-100, FGVC Aircraft, Cars), limiting generalizability to other domains
- Specific hyperparameter settings for the optimizer (learning rate, momentum, weight decay, scheduler) are not provided, which are critical for exact reproduction
- Paper does not extensively discuss the computational cost of the training phase for the KD framework, which is non-trivial given the dual-model setup

## Confidence
- **High Confidence:** The core claim that the KD-based framework significantly improves accuracy over strong CIL baselines (LwF, iCaRL, SS-IL) while reducing inference costs is well-supported by the experimental results in Table 1.
- **Medium Confidence:** The assertion that the pruning-based framework offers a viable, though less powerful, alternative for achieving accuracy gains with some compression is supported, but the results suggest it is generally outperformed by KD, especially for higher compression ratios.
- **Medium Confidence:** The claim that pre-pruning is simpler and less prone to unrecoverable forgetting than post-pruning is stated, but the paper does not provide a direct empirical comparison of pre- vs. post-pruning within the same experimental section.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Conduct experiments to determine the sensitivity of the KD framework's performance to the distillation temperature (τ), the distillation weight (λ), and the pruning ratio (for the pruning framework). This will help establish the robustness of the reported improvements.
2. **Cross-Domain Transfer Test:** Evaluate the pre-trained teacher model's effectiveness on a dataset that is more visually distinct from ImageNet (e.g., a medical imaging dataset or a satellite image dataset) to test the assumption about the generality of pre-trained knowledge.
3. **Training Efficiency Benchmark:** Measure and report the total training time and computational cost for the KD framework (teacher + student training) and compare it to the training cost of a single large model baseline. This will provide a more complete picture of the trade-off between training and inference efficiency.