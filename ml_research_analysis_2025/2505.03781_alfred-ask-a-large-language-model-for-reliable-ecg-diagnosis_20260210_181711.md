---
ver: rpa2
title: 'ALFRED: Ask a Large-language model For Reliable ECG Diagnosis'
arxiv_id: '2505.03781'
source_url: https://arxiv.org/abs/2505.03781
tags:
- framework
- knowledge
- rule
- features
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALFRED, a zero-shot ECG diagnosis framework
  leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG)
  to improve automated ECG interpretation. The core innovation lies in integrating
  expert-curated knowledge alongside a feature extraction module and a rule-based
  diagnostic algorithm, addressing the challenge of generating reliable, evidence-based
  results in medical diagnostics.
---

# ALFRED: Ask a Large-language model For Reliable ECG Diagnosis

## Quick Facts
- arXiv ID: 2505.03781
- Source URL: https://arxiv.org/abs/2505.03781
- Authors: Jin Yu; JaeHo Park; TaeJun Park; Gyurin Kim; JiHyun Lee; Min Sung Lee; Joon-myoung Kwon; Jeong Min Son; Yong-Yeon Jo
- Reference count: 5
- Primary result: Zero-shot ECG diagnosis framework using LLM + RAG + rule-based pre-diagnosis achieves PPV 0.451 on PTB-XL dataset

## Executive Summary
ALFRED introduces a zero-shot ECG diagnosis framework that combines Large Language Models with Retrieval-Augmented Generation to improve automated ECG interpretation. The approach integrates expert-curated knowledge, a feature extraction module, and a rule-based diagnostic algorithm to generate reliable, evidence-based results. By processing raw ECG data through segmentation, rule evaluation, and knowledge retrieval before LLM inference, the framework produces interpretable diagnoses aligned with clinical reasoning. Evaluated on PTB-XL, the method demonstrates improved diagnostic performance with gains in precision-recall when expert knowledge and rule results are included.

## Method Summary
ALFRED processes 12-lead ECG signals through a three-stage pipeline: (1) UNet-based segmentation extracts 42 clinically relevant features including 30 lead-specific and 12 global measurements, (2) a rule-based algorithm evaluates these features against 40 predefined conditions to generate diagnostic hints, and (3) a RAG system retrieves context from both ECG literature documents and expert-curated knowledge before prompting an LLM for final diagnosis. The framework uses OpenAI's text-embedding-3-large for vector retrieval in a Milvus database, constructs five-part prompts incorporating features, rules, and augmented information, and outputs structured JSON diagnoses with explanations via GPT-4o-Mini.

## Key Results
- Achieved PPV of 0.451 compared to baseline of 0.326 on PTB-XL dataset
- Rule-based pre-diagnosis improved performance (PPV 0.326 → 0.414) by providing structured diagnostic hints
- Expert-curated knowledge contributed additional gains (PPV 0.416 → 0.451) over document retrieval alone
- Framework supports comprehensive ECG analysis across 5 superclasses: NORM, CD, HYP, MI, STTC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based pre-diagnosis provides structured diagnostic hints that constrain LLM reasoning and improve classification accuracy.
- Mechanism: A feature-based algorithm processes extracted ECG features into True/False labels for 40 conditions before LLM inference. These rule results are injected into the prompt, anchoring the LLM's reasoning in clinically validated logic rather than free-form inference.
- Core assumption: Rule-based outputs are sufficiently accurate to guide rather than mislead the LLM; the LLM can appropriately weigh rule results as hints rather than definitive conclusions.
- Evidence anchors:
  - [abstract]: "applies rule-based logic to identify potential conditions"
  - [section]: "It appears that the rule results served as hints for the LLM in ECG interpretation, leading to the enhanced performance" (Table 1: Base PPV 0.326 → Ablation1 PPV 0.414)
  - [corpus]: Limited corpus validation; ZETA (arXiv:2510.21551) uses structured knowledge alignment for zero-shot ECG diagnosis but through different mechanisms.
- Break condition: If rule module accuracy degrades on out-of-distribution ECG patterns (e.g., rare arrhythmias not covered by rules), incorrect hints may propagate as confident but wrong LLM outputs.

### Mechanism 2
- Claim: Dual-source knowledge retrieval (documents + expert-curated knowledge) improves diagnostic grounding beyond generic RAG.
- Mechanism: The vector database contains two distinct collections: (1) ECG-related papers and websites ("documents"), and (2) clinician-defined critical terms and interpretations ("knowledge"). Queries retrieve from both sources, enriching prompts with complementary perspectives—broad literature context plus precise clinical definitions.
- Core assumption: Expert-curated knowledge contains information not captured in publicly available documents; retrieval relevance is preserved when combining both sources.
- Evidence anchors:
  - [section]: "We construct (1) a core set of documents... Additionally... we incorporate (2) specialized knowledge by defining terms that clinicians consider critical"
  - [section]: Ablation3 (Documents + Knowledge for diagnosis retrieval) achieved PPV 0.451 vs Ablation2 at 0.416 (Table 1)
  - [corpus]: Corpus shows RAG-optimized LLMs in pathology (arXiv:2505.08590) also leverage domain-specific foundation models, suggesting generalizability of expert knowledge integration.
- Break condition: If knowledge entries are too sparse or poorly indexed, retrieval may return irrelevant context, diluting prompt quality and confusing the LLM (observed slight PPV decrease in Proposed vs Ablation3).

### Mechanism 3
- Claim: Clinically-aligned feature extraction mimics cardiologist workflow, producing LLM-compatible structured inputs.
- Mechanism: A UNet-based segmentation model delineates ECG waveforms (P wave, QRS complex, T wave). Derived features (30 lead-specific: PR interval, R wave amplitude; 12 global: heart rate, QTc) replicate standard device outputs (GE, Philips). This structured representation aligns with both rule logic and clinical literature terminology.
- Core assumption: Extracted features accurately reflect ground-truth ECG characteristics; feature names match retrieval query vocabulary.
- Evidence anchors:
  - [section]: "clinicians typically begin by analyzing the positions of key waveform components—such as the P wave, QRS complex, and T wave—we designed our approach to align with this conventional workflow"
  - [section]: "30 lead-specific features... 12 global features... providing a comprehensive representation of the ECG"
  - [corpus]: Related work on ECG image diagnosis (arXiv:2507.19961) similarly transforms non-text inputs to diagnostic features, supporting cross-modal feature extraction validity.
- Break condition: Segmentation errors on noisy or artifact-heavy ECGs propagate incorrect intervals/amplitudes, causing cascading failures in rule evaluation and retrieval relevance.

## Foundational Learning

- Concept: **ECG waveform morphology and intervals**
  - Why needed here: The entire framework depends on accurate extraction of features like PR interval, QRS duration, QTc, and waveform patterns. Without understanding what these represent clinically, you cannot debug feature extraction or validate rule logic.
  - Quick check question: If QRS duration is prolonged (>120ms) with a QS pattern in V1-V2, what conduction abnormality might this indicate?

- Concept: **Vector embeddings and similarity search**
  - Why needed here: RAG depends on Milvus vector database with COSINE similarity and HNSW indexing. Understanding embedding quality, chunking strategy (1,024 characters), and retrieval relevance is essential for diagnosing poor augmented information.
  - Quick check question: Why might a query for "myocardial infarction" fail to retrieve relevant chunks if the knowledge base only contains "MI" as an abbreviation?

- Concept: **Zero-shot learning with LLMs**
  - Why needed here: ALFRED operates zero-shot—no fine-tuning on ECG data. Performance depends entirely on prompt engineering (5-part structure) and retrieval quality. Understanding LLM in-context learning limitations helps set realistic expectations.
  - Quick check question: What happens to zero-shot performance if the prompt exceeds the LLM's effective context window or contains contradictory information?

## Architecture Onboarding

- Component map: Raw ECG + metadata -> Segmentation -> Feature extraction -> Rule evaluation -> Dual retrieval (features + diagnoses) -> Prompt assembly -> LLM inference -> Structured output

- Critical path: Raw ECG → Segmentation → Feature extraction → Rule evaluation → Dual retrieval (features + diagnoses) → Prompt assembly → LLM inference → Structured output

- Design tradeoffs:
  - Rule module provides explicitness but limited to 40 predefined conditions; cannot discover novel patterns
  - Knowledge retrieval improves PPV but increased prompt length caused slight specificity degradation (Proposed vs Ablation3)
  - UNet segmentation accuracy vs. computational cost for real-time deployment
  - GPT-4o-Mini chosen for cost-efficiency; larger models may improve reasoning but increase latency and expense

- Failure signatures:
  - **Hallucinated diagnosis**: LLM generates conditions not in rule results or unsupported by features → check retrieval relevance; may indicate missing knowledge entries
  - **Missing critical findings**: Rule module returns False for all conditions despite abnormal ECG → verify feature extraction accuracy on problematic samples
  - **Contradictory explanations**: LLM explanation conflicts with rule output → prompt may contain conflicting augmented information from documents vs. knowledge
  - **Poor retrieval matches**: Augmented information irrelevant to query → examine embedding quality, chunk boundaries, or vocabulary mismatch between feature names and document terminology

- First 3 experiments:
  1. **Feature extraction validation**: Run segmentation model on PTB-XL fold 10 samples with known labels; compare extracted intervals against manually annotated ground truth to establish baseline accuracy before integration.
  2. **Retrieval quality audit**: For 50 random ECG samples, manually review top-3 retrieved chunks for feature and diagnosis queries; measure relevance rate and identify systematic retrieval failures (e.g., abbreviation mismatches).
  3. **Ablation replication**: Reproduce Table 1 results (Base → Ablation1 → Ablation3 → Proposed) on a held-out subset to validate framework contribution before extending to additional datasets or conditions.

## Open Questions the Paper Calls Out

- **Generalization to real-world datasets**: The authors explicitly state that future work will focus on "real-world dataset evaluation to address greater variability." The current evaluation relies solely on the 10th fold of the PTB-XL dataset, which is specifically selected for high label quality, potentially masking performance issues in more chaotic clinical environments. What evidence would resolve it: Benchmarking results (PPV, NPV, Sensitivity, Specificity) from applying the framework to diverse, multi-center datasets with uncurated or noisy ECG recordings.

- **Optimized embedding models**: The paper notes that the "Proposed" framework showed a slight decrease in PPV and Specificity compared to "Ablation3," likely due to the "increased amount of prompts" causing confusion, while listing "optimized embedding models" as future work. It is unclear if the proposed future optimizations (embeddings/tuning) can successfully resolve the trade-off between enriching the context and overwhelming the LLM. What evidence would resolve it: Ablation studies comparing the current setup against domain-specific embedding models to see if retrieval precision improves without increasing prompt volume.

- **Error propagation from upstream components**: The framework relies on a UNet-based feature extraction module and a rule module to provide the "ECG Features" and "Rule Results" that form the basis of the LLM prompt. The evaluation assumes accurate feature extraction; however, if the segmentation model miscalculates intervals (e.g., PR interval), the RAG may retrieve irrelevant context, leading to hallucinated or incorrect diagnoses. What evidence would resolve it: Robustness tests analyzing the correlation between upstream feature extraction errors and final diagnostic accuracy.

## Limitations

- **Dataset limitation**: The framework was evaluated exclusively on PTB-XL dataset (10th fold only), which may not represent real-world ECG variability. Performance on external datasets, particularly those with different acquisition systems or rare conditions, remains unknown.

- **Knowledge base opacity**: The clinician-curated knowledge base that showed performance benefits is proprietary and not disclosed. Without access to this component, it's impossible to assess whether the improvements stem from truly novel clinical insights or standard ECG interpretation principles already available in public literature.

- **Segmentation accuracy uncertainty**: The UNet-based feature extraction is critical to the pipeline, but specific accuracy metrics for the segmentation model are not provided. Errors in waveform delineation would cascade through rule evaluation and feature retrieval, potentially degrading overall performance.

## Confidence

- **High confidence**: The general framework architecture (RAG + rule-based pre-diagnosis + LLM inference) is sound and well-documented. The ablation study showing progressive performance gains from Base → Ablation1 → Ablation3 → Proposed is methodologically rigorous.

- **Medium confidence**: The specific performance improvements (PPV 0.326 → 0.451) are credible given the ablation design, but external validity is uncertain without cross-dataset validation. The claim that rule results "serve as hints" rather than definitive answers is plausible but not directly tested.

- **Low confidence**: Claims about the uniqueness of expert-curated knowledge contributions are difficult to verify without access to the proprietary knowledge base. The framework's behavior on ECGs with artifacts, noise, or rare conditions is largely speculative.

## Next Checks

1. **Cross-dataset validation**: Evaluate ALFRED on at least two external ECG datasets (e.g., CPSC2018, PTB Diagnostic) to assess generalization beyond PTB-XL. Measure performance degradation and identify whether rule module limitations or retrieval failures drive accuracy drops.

2. **Rule module sensitivity analysis**: Systematically test the 40-condition rule algorithm on ECG samples with known abnormalities outside the rule scope. Quantify false negative rates and determine whether rule failures stem from feature extraction errors or incomplete rule coverage.

3. **Knowledge base contribution isolation**: Create a minimal public-domain knowledge base with equivalent size and structure, then conduct controlled experiments comparing performance with original proprietary knowledge versus the public substitute. This would reveal whether performance gains depend on truly unique clinical insights or could be replicated with publicly available resources.