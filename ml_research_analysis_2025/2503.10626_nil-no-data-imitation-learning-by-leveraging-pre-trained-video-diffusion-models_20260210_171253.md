---
ver: rpa2
title: 'NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion
  Models'
arxiv_id: '2503.10626'
source_url: https://arxiv.org/abs/2503.10626
tags:
- video
- learning
- imitation
- diffusion
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces No-data Imitation Learning (NIL), a method
  that enables robots to learn motor skills without any expert demonstration data
  by leveraging video diffusion models. The key idea is to generate reference videos
  using pre-trained video diffusion models, conditioned on an initial frame and textual
  description, and then train policies to imitate these generated videos.
---

# NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models

## Quick Facts
- arXiv ID: 2503.10626
- Source URL: https://arxiv.org/abs/2503.10626
- Reference count: 39
- Method achieves 396.1 environment reward on Unitree H1 humanoid, outperforming motion-capture baselines

## Executive Summary
NIL introduces a novel approach to imitation learning that eliminates the need for expert demonstration data by leveraging pre-trained video diffusion models. The method generates reference videos using AI models conditioned on an initial frame and text description, then trains policies to imitate these generated videos using a discriminator-free reward structure. NIL combines video similarity metrics (via TimeSformer embeddings) and image-based similarity (via segmentation masks) to create a reward signal for reinforcement learning. Experiments demonstrate that NIL outperforms state-of-the-art imitation learning methods on five diverse robot morphologies without requiring any collected expert data.

## Method Summary
NIL generates reference videos using pre-trained video diffusion models (like Kling AI) conditioned on an initial simulation frame and textual description. The generated videos are then used to train RL policies through a discriminator-free approach that combines video similarity (measured via TimeSformer embeddings) and image-based similarity (measured via SAM2 segmentation masks). The reward function is a weighted combination of these similarity metrics plus regularization terms that penalize joint torque, action differences, angular velocity, foot contact violations, and torso stability issues. The method uses entropy-regularized off-policy actor-critic (BRO algorithm) for policy optimization, with temporal alignment mapping simulation steps to video frames.

## Key Results
- NIL achieves 396.1 environment reward on Unitree H1 humanoid compared to 393.5 for best baseline
- Method works across five diverse robot morphologies (humanoids and quadrupeds) without any expert data
- NIL outperforms state-of-the-art imitation learning methods trained on motion-capture data
- Demonstrates successful policy learning from AI-generated videos rather than human demonstrations

## Why This Works (Mechanism)
NIL leverages the generalization capabilities of pre-trained video diffusion models to generate diverse, physically plausible motion sequences without requiring expert demonstrations. The key insight is that modern video diffusion models can produce realistic locomotion behaviors when conditioned on text prompts and initial frames. By combining video-level similarity (embedding distances) with pixel-level consistency (segmentation masks), NIL creates a rich reward signal that guides the agent toward motions that are both visually similar to the reference and physically coherent. The discriminator-free approach avoids the complexity of training additional adversarial networks while still providing effective imitation learning signals.

## Foundational Learning
- Video diffusion models (why needed: generate diverse motion sequences without expert data; quick check: can produce physically plausible walking videos)
- Video vision transformers (why needed: extract meaningful motion features from video frames; quick check: can distinguish walking from running)
- Segmentation masks for consistency (why needed: ensure pixel-level alignment between agent and reference; quick check: masks overlap > 0.7 IoU)
- Entropy-regularized RL (why needed: encourage exploration while learning from generated videos; quick check: policy entropy decreases during training)
- Temporal alignment (why needed: map simulation timesteps to video frames; quick check: k steps per video frame matches frame rate)

## Architecture Onboarding

**Component map:** Initial frame + Text prompt -> Video diffusion model -> Generated video -> TimeSformer + SAM2 -> Reward signal -> BRO RL -> Trained policy

**Critical path:** The most critical components are the video diffusion model generation quality and the reward signal composition. Poor quality reference videos or imbalanced reward weights will prevent successful learning.

**Design tradeoffs:** NIL trades data collection effort for reliance on video diffusion model quality. While eliminating the need for expert demonstrations, the method is dependent on the generalization capabilities of pre-trained diffusion models and may produce physically implausible motions if the generated videos are unrealistic.

**Failure signatures:** Jittery motion indicates missing regularization penalties; failure to walk forward suggests IoU-only reward leads to local optima; asymmetric gait indicates low-quality reference videos; policy collapse occurs when reward weights are imbalanced.

**First experiments:** 1) Generate reference videos for each robot morphology and visually inspect for physical plausibility; 2) Train policy with only video similarity reward to establish baseline performance; 3) Add segmentation mask reward and regularization terms incrementally to identify critical components.

## Open Questions the Paper Calls Out
None

## Limitations
- Physical feasibility of generated videos not systematically verified, relying on subjective assessment
- Experimental results limited to simulation environments, leaving sim-to-real transfer uncertain
- Modest performance improvements over baselines may not be practically significant
- Method's robustness to low-quality or physically unrealistic generated videos not established

## Confidence
*High confidence:* Technical feasibility demonstrated across five robot morphologies with internally consistent framework
*Medium confidence:* Performance claims supported but practical significance unclear without statistical tests
*Low confidence:* "Learning without expert data" claim somewhat misleading given reliance on diffusion model outputs

## Next Checks
1. Conduct quantitative analysis of physical plausibility in generated videos, including energy expenditure metrics, joint limit violations, and contact force consistency
2. Implement sim-to-real transfer experiment on real Unitree H1 to assess real-world deployment viability
3. Perform hyperparameter sensitivity analysis to determine critical components and establish minimum performance thresholds