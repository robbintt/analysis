---
ver: rpa2
title: Value-at-Risk Constrained Policy Optimization
arxiv_id: '2601.22993'
source_url: https://arxiv.org/abs/2601.22993
tags:
- cost
- policy
- constraint
- optimization
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VaR-CPO, a reinforcement learning algorithm
  designed to enforce Value-at-Risk (VaR) constraints directly, addressing the challenge
  of safe exploration in risk-sensitive environments. The core method uses the one-sided
  Chebyshev inequality to construct a differentiable surrogate for the non-differentiable
  VaR constraint, based on the first two moments of the cost distribution.
---

# Value-at-Risk Constrained Policy Optimization

## Quick Facts
- arXiv ID: 2601.22993
- Source URL: https://arxiv.org/abs/2601.22993
- Reference count: 31
- Primary result: VaR-CPO achieves zero constraint violations during training in feasible environments, successfully navigating risk-sensitive scenarios where baseline methods fail to uphold safety constraints.

## Executive Summary
This paper introduces VaR-CPO, a reinforcement learning algorithm that directly enforces Value-at-Risk (VaR) constraints for safe exploration in risk-sensitive environments. The method addresses the challenge of non-differentiable VaR constraints by using the one-sided Chebyshev inequality to construct a differentiable surrogate based on first and second moments of the cost distribution. Integrated into the Constrained Policy Optimization framework with state augmentation, the algorithm provides rigorous worst-case bounds for both policy improvement and constraint violation during training. Empirical results demonstrate superior safety performance compared to baseline methods in environments with safety constraints.

## Method Summary
VaR-CPO solves Constrained Markov Decision Processes with VaR constraints by transforming the non-differentiable probability constraint into a tractable optimization objective using Chebyshev's inequality. The method employs state augmentation to ensure Markovian dynamics for variance calculation, representing the squared cost return as a cumulative return of augmented local costs. A trust-region bounded update framework with a recovery mode ensures stable learning, switching between VaR optimization and expected cost constraint when the policy becomes infeasible. The algorithm uses three critics (value, cost, and augmented cost) and provides worst-case violation bounds while maintaining zero constraint violations during training in feasible environments.

## Key Results
- Achieves zero constraint violations during training in feasible environments
- Successfully navigates risk-sensitive scenarios where baseline methods fail to uphold safety constraints
- Demonstrates theoretical worst-case bounds for both policy improvement and constraint violation
- Outperforms PPO, CPO, and CPPO in maintaining safety constraints across test environments

## Why This Works (Mechanism)

### Mechanism 1: Chebyshev Surrogate for Non-Differentiable VaR Constraint
The one-sided Chebyshev inequality transforms the non-differentiable VaR constraint into a tractable, differentiable optimization objective using only first and second moments. Instead of directly optimizing P(C(τ) ≥ ρ) ≤ ε, the inequality provides a conservative upper bound: σ²(π)/(σ²(π) + [ρ - μ(π)]²) ≤ ε. This rearranges to a quadratic constraint: (1/ε - 1)σ²(π) - [ρ - μ(π)]² ≤ 0, enabling gradient-based optimization. The bound is distribution-free but inherently conservative.

### Mechanism 2: State Augmentation Enables Markovian Decomposition of Variance
Augmenting the state space with accumulated discounted costs (y_t) and discount factor (γ_c^t) transforms the non-Markovian variance calculation into a Markovian cumulative return. The squared cost return decomposes as: C(τ)² = Σ γ_c^t(γ_c^t c_t² + 2y_t c_t). This allows expressing second moments via an augmented local cost: c̃(x_t, a_t) = βγ_c^t c_t² + 2(βy_t + ρ)c_t. The augmented state (s_t, y_t, γ_c^t) captures all information needed for VaR-relevant decisions, restoring the Markov property.

### Mechanism 3: Trust-Region Bounded Updates with Recovery Mode
The CPO trust-region framework extends to VaR constraints with provable worst-case violation bounds, plus a fallback for infeasible regions. When μ(π_k) < ρ, optimize the Chebyshev surrogate. When μ(π_k) ≥ ρ, temporarily constrain expected cost directly to restore feasibility. Theorem 4.1 bounds violation: J_C(π_{k+1}) ≤ K(α̃_C + 2α_C/(εM)). Trust region δ bounds updates via D̄_KL(π, π_k) ≤ δ to guarantee stable learning.

## Foundational Learning

- **Concept: Constrained Markov Decision Process (CMDP)**
  - Why needed here: VaR-CPO operates on CMDPs extending MDPs with separate cost functions and safety constraints J_C(π) ≤ d.
  - Quick check question: Why does standard reward shaping fail when we need to bound the probability of catastrophic failure rather than expected cost?

- **Concept: Value-at-Risk (VaR) vs Conditional Value-at-Risk (CVaR)**
  - Why needed here: Paper contrasts VaR (P(C(τ) ≥ ρ) ≤ ε) with CVaR; VaR suits binary failure states but introduces time-inconsistency.
  - Quick check question: Given a cost distribution, compute VaR at 95% confidence. How does this differ from CVaR?

- **Concept: Trust Region Methods and KL Divergence**
  - Why needed here: VaR-CPO inherits CPO's trust-region approach, bounding updates via D̄_KL(π, π_k) ≤ δ to guarantee stable learning.
  - Quick check question: Why does constraining KL divergence between successive policies bound both performance degradation and constraint violations?

## Architecture Onboarding

- **Component map**: Policy network π_θ → Three critics (V_φ for reward, V_C^ψ for cost, V_C̃^χ for augmented cost) → State augmentation module (computes y_t, γ_c^t) → CPO solver (conjugate gradient + backtracking) → Mode selector (checks μ(π_k) vs ρ)

- **Critical path**: Sample trajectories → Augment states (Eq. 20) → Compute three GAE advantage estimates → Select mode (VaR optimization vs recovery) → Solve Eq. 39-41 → Update critics via MSE

- **Design tradeoffs**: Chebyshev conservatism vs optimality (Figure 1 gap); zero violations vs reward return; three critics add memory/compute overhead vs standard PPO.

- **Failure signatures**: Constraint oscillation from frequent μ(π_k) ≈ ρ crossings; infeasible initialization struggles (Battery=50 in Figure 4); trust region too large/small causes instability/stalling.

- **First 3 experiments**:
  1. IcyLake baseline: Run PPO/CPO/CPPO/VaR-CPO with 95th percentile ≤ 15. Verify only VaR-CPO achieves zero ice tile visitation. Measure ice visitation %, cost VaR, expected reward.
  2. Recovery mode ablation: Compare VaR-CPO with/without recovery (Section 4.5) when initialized unsafe (Battery=50). Measure convergence rate and violations.
  3. Conservatism quantification: Compare VaR-CPO against distribution-aware oracle (Gaussian VaR boundary) to quantify robustness price. Measure reward ratio and constraint tightness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the conservatism of the Chebyshev surrogate be relaxed to improve reward return while maintaining safety in environments with known cost distributions?
- **Basis in paper**: [inferred] Figure 1 illustrates the "Conservatism Gap," showing the Chebyshev boundary is significantly stricter than a Gaussian boundary.
- **Why unresolved**: The paper employs a distribution-free inequality (Chebyshev) to guarantee safety for any distribution, which inevitably sacrifices optimality in scenarios where the cost distribution shape (e.g., Gaussian) is known or learnable.
- **What evidence would resolve it**: A comparative study showing improved expected returns by substituting the Chebyshev bound with a tighter, distribution-specific constraint in environments with characterized cost noise.

### Open Question 2
- **Question**: How does VaR-CPO behave in environments where the VaR constraint is strictly infeasible?
- **Basis in paper**: [inferred] The abstract emphasizes success in "feasible environments," and Section 4.5 describes a recovery mechanism for infeasible regions that prioritizes reducing expected cost.
- **Why unresolved**: It is unclear if the recovery mechanism (switching to expected cost optimization) leads to stable convergence or oscillation when the safety threshold $\rho$ is fundamentally unattainable given the environment dynamics.
- **What evidence would resolve it**: Empirical results tracking policy stability and cost variance in environments where the risk threshold is set below the minimum achievable risk.

### Open Question 3
- **Question**: Does the state-augmentation scheme required for the Chebyshev surrogate limit scalability in high-dimensional observation spaces?
- **Basis in paper**: [inferred] Section 4.2 introduces a state augmentation $x_t = (s_t, y_t, \gamma_c^t)$ to estimate second-order moments, but experiments are restricted to low-dimensional state spaces (Ant, Gridworld).
- **Why unresolved**: The reliance on concatenating accumulated costs ($y_t$) to the state input may complicate representation learning or increase sample complexity significantly when raw observations (e.g., images) are used.
- **What evidence would resolve it**: Benchmarking VaR-CPO on high-dimensional tasks (e.g., vision-based control) to analyze sample efficiency and convergence speed relative to non-augmented baselines.

## Limitations
- Chebyshev surrogate introduces inherent conservatism, creating performance gaps versus distribution-aware methods
- State augmentation with unbounded y_t growth may cause numerical instability in long episodes
- Recovery mechanism behavior in strictly infeasible regions remains underspecified and may lead to oscillation
- Limited empirical validation in high-dimensional observation spaces

## Confidence

- **High Confidence**: The Chebyshev inequality formulation and its rearrangement into a quadratic constraint (section 4.1) - this is mathematically standard and well-documented.
- **Medium Confidence**: The state augmentation mechanism (section 4.2) - while the decomposition is shown, the assumption that this fully captures VaR-relevant information lacks extensive empirical validation.
- **Medium Confidence**: Worst-case bounds in Theorem 4.1 - the derivation appears rigorous but relies on second-order approximations whose validity in practice depends on trust-region size.
- **Low Confidence**: Recovery mechanism implementation details - the switch condition μ(π_k) ≥ ρ is simple, but the paper doesn't specify how often this occurs or what prevents cycling.

## Next Checks

1. **Conservatism Quantification**: Implement both VaR-CPO and a Gaussian-distribution-aware VaR baseline on IcyLake, measuring the performance gap and constraint tightness across multiple random seeds.

2. **Numerical Stability Test**: Run VaR-CPO on extended-length versions of IcyLake (larger grids, longer episodes) while monitoring y_t growth and critic loss stability.

3. **Recovery Mechanism Stress Test**: Initialize VaR-CPO in intentionally infeasible regions (e.g., Battery=50 in EcoAnt) and measure whether the algorithm converges to feasibility or cycles indefinitely between modes.