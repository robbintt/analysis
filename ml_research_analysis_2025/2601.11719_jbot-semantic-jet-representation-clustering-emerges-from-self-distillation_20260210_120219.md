---
ver: rpa2
title: 'jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation'
arxiv_id: '2601.11719'
source_url: https://arxiv.org/abs/2601.11719
tags:
- learning
- pre-training
- particle
- physics
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: jBOT applies self-supervised pre-training via self-distillation
  to jet substructure data from the CERN LHC, adapting the iBOT framework to particle-based
  inputs. By combining local particle-level and global jet-level distillation, jBOT
  learns semantic representations that cluster jet classes without supervision.
---

# jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation

## Quick Facts
- arXiv ID: 2601.11719
- Source URL: https://arxiv.org/abs/2601.11719
- Authors: Ho Fung Tsoi; Dylan Rankin
- Reference count: 40
- Primary result: Self-supervised jet embeddings achieve ~70% multi-class accuracy from scratch, outperforming supervised models trained from scratch.

## Executive Summary
jBOT adapts the iBOT self-distillation framework to jet substructure data from the CERN LHC, learning semantic jet representations without supervision. By combining local particle-level distillation (via momentum-aware masking) with global cross-view jet-level distillation, jBOT clusters jet classes in embedding space. The frozen embeddings support competitive anomaly detection and achieve strong classification when fine-tuned, particularly with limited labeled data.

## Method Summary
jBOT employs a teacher-student architecture where a student network learns from a slowly-updating teacher via self-distillation. Two augmented views of each jet are generated (rotation, smearing, collinear splitting), with momentum-aware masking applied to the student's view. The student predicts masked particle representations and matches global [CLS] embeddings across views, while a KoLeo regularizer encourages embedding space diversification. This combination enables semantic clustering without labels, with downstream performance evaluated via frozen embeddings and fine-tuning.

## Key Results
- Pre-trained embeddings alone achieve ~70% accuracy on 5-class jet classification
- Fine-tuned models reach up to 76% accuracy, outperforming supervised training from scratch
- Anomaly detection AUCs exceed 0.8 for key signals (W, Z, t) using distance metrics in embedding space
- Progressive clustering of jet classes emerges during pre-training, visible in t-SNE visualizations

## Why This Works (Mechanism)

### Mechanism 1: Local Particle-Level Masked Prediction
Momentum-aware particle masking forces the student to infer missing particle information from context, encoding semantically meaningful particle-level representations. This reconstruction task captures jet substructure relationships essential for downstream classification.

### Mechanism 2: Global Cross-View Jet-Level Distillation
Matching [CLS] embeddings across differently augmented views of the same jet encourages the encoder to capture jet-level semantics invariant to noise and symmetries. This cross-view objective forces learning of stable, discriminative global representations.

### Mechanism 3: Embedding Space Diversification via KoLeo
The KoLeo regularizer maximizes pairwise distances within batches, preventing representational collapse and improving downstream separability. This explicit diversification complements the distillation objectives by ensuring embeddings remain well-spread.

## Foundational Learning

- **Teacher-Student Self-Distillation with EMA**: The teacher provides stable targets for the student without external labels. The EMA update prevents collapse by ensuring slow teacher evolution. Quick check: If teacher weights were updated via gradient descent instead of EMA, what failure mode would likely occur? (Answer: Rapid collapse to constant representations.)

- **Attention-Based Token Aggregation in Transformers**: The [CLS] token aggregates variable-length particle information into fixed-size jet representations via multi-head self-attention. Quick check: How does the [CLS] token differ from simple average pooling of particle embeddings? (Answer: [CLS] learns weighted attention over particles, potentially focusing on discriminative substructures.)

- **Physics-Motivated Data Augmentation**: Augmentations must preserve physics symmetries to avoid teaching spurious correlations. Quick check: Why mask by cumulative pT rather than random particle count? (Answer: Ensures both soft and hard particles are represented in masking, avoiding bias toward either.)

## Architecture Onboarding

- **Component map**: Input jet → Tokenization (linear embedding + [CLS]) → Transformer encoder → Projection head → [CLS] embedding output + particle embeddings

- **Critical path**: 1) Augment input jet → two views; 2) Apply momentum-aware masking to student views; 3) Forward pass through student (masked) and teacher (full) encoders; 4) Compute L_Part, L_[CLS], L_KoLeo; 5) Backprop through student only; update teacher via EMA

- **Design tradeoffs**: Model size vs. data scale (small/base models); masking ratio 0-50% pT; augmentation strength (aggressive smearing improves invariance but risks distortion); learning rate warmup and LLRD critical but not rigorously tuned

- **Failure signatures**: Collapse (all [CLS] embeddings converge to constant); over-smoothing (uniform attention weights); class confusion in embedding (no cluster separation); fine-tuning degradation (worse than frozen embedding)

- **First 3 experiments**: 1) Baseline sanity check: Pre-train on 5-class dataset; visualize [CLS] embeddings via t-SNE after epoch 1, 10, 50, 100. Confirm clustering emerges progressively. 2) Ablation: augmentation sensitivity - compare rotation only, rotation+smearing, all three augmentations on 5-class frozen k-NN accuracy. 3) Anomaly detection calibration: Pre-train on q/g only; sweep hyperparameters for k-NN (k=10,30,50), GMM (K=2,4,8) and report AUC for W/Z/t signals.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does jBOT performance improve linearly or logarithmically when scaling pre-training to significantly larger unlabeled datasets beyond the 880k jets used in this study? (Basis: Conclusion explicitly states scaling up pre-training on much larger unlabeled datasets as future direction.)

- **Open Question 2**: Can dedicated, physics-informed augmentations improve the specificity of learned representations for anomaly detection tasks? (Basis: Conclusion identifies "more robust anomaly detection strategy with potentially dedicated augmentations" as future direction.)

- **Open Question 3**: How does the proposed momentum-aware masking scheme compare to random or geometric masking strategies in terms of convergence speed and semantic clustering quality? (Basis: Section 3 introduces "simple momentum-aware scheme" but provides no ablation study comparing it against random masking.)

- **Open Question 4**: How sensitive is anomaly detection performance to choice of distance metric versus intrinsic geometry of embedding space? (Basis: Section 4.5 notes "performance varies with choice of anomaly score" and "flexibility introduces large space to explore.")

## Limitations

- Dataset generalization remains untested beyond JetNet's 5 jet classes at ~1 TeV, raising questions about applicability to other collision energies or reconstruction algorithms.
- Hyperparameter sensitivity has not been systematically evaluated, particularly for the KoLeo regularization strength and augmentation parameters.
- Anomaly detection calibration requires per-signal metric selection without established unified criteria or validation consistency.

## Confidence

- **High Confidence**: Core mechanism combining local particle-level and global cross-view distillation is theoretically sound with consistent experimental validation across multiple downstream tasks.
- **Medium Confidence**: Specific design choices for augmentation parameters and masking schemes are physics-motivated but not exhaustively validated against alternatives.
- **Low Confidence**: Claims about general applicability to other particle physics problems extend beyond experimental scope; relationship between KoLeo strength and performance lacks systematic investigation.

## Next Checks

1. **Cross-Energy Validation**: Train jBOT on JetNet jets at different center-of-mass energies (e.g., 500 GeV vs 2 TeV) and evaluate whether pre-training at one energy transfers effectively to downstream tasks at another energy.

2. **Ablation of Physics-Motivated Masking**: Compare jBOT's momentum-aware masking against random particle masking and other domain-agnostic masking strategies to isolate whether physics-specific masking provides measurable benefits.

3. **Anomaly Detection Consistency**: For each anomaly detection metric (k-NN, cosine, Mahalanobis, GMM), perform systematic hyperparameter sweeps across all signal types (W, Z, t) and report variance in optimal settings to establish reliability of metric selection.