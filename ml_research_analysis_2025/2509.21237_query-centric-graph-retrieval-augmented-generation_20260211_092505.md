---
ver: rpa2
title: Query-Centric Graph Retrieval Augmented Generation
arxiv_id: '2509.21237'
source_url: https://arxiv.org/abs/2509.21237
tags:
- query
- queries
- answer
- graph
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QCG-RAG addresses the granularity dilemma in graph-based retrieval-augmented
  generation by introducing a query-centric graph construction approach that leverages
  Doc2Query and Doc2Query-- to generate query-guided nodes and edges. This enables
  controllable-granularity indexing graphs that balance semantic richness and interpretability.
---

# Query-Centric Graph Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2509.21237
- Source URL: https://arxiv.org/abs/2509.21237
- Authors: Yaxiong Wu; Jianyuan Bo; Yongyue Zhang; Sheng Liang; Yong Liu
- Reference count: 37
- Primary result: QCG-RAG achieves 73.16% accuracy on LiHuaWorld and 79.60% on MultiHop-RAG, outperforming both chunk-based and graph-based baselines.

## Executive Summary
QCG-RAG addresses the granularity dilemma in graph-based retrieval-augmented generation by introducing a query-centric graph construction approach that leverages Doc2Query and Doc2Query-- to generate query-guided nodes and edges. This enables controllable-granularity indexing graphs that balance semantic richness and interpretability. The method incorporates a multi-hop retrieval mechanism that expands from initial queries to neighboring nodes, improving evidence coverage and multi-hop reasoning. Experiments on LiHuaWorld and MultiHop-RAG demonstrate that QCG-RAG achieves state-of-the-art QA accuracy, outperforming both chunk-based and graph-based baselines.

## Method Summary
QCG-RAG builds a two-layer graph where query nodes (generated from chunks using Doc2Query/Doc2Query--) connect to their source chunks and to each other via KNN edges. At query time, user queries are matched to graph queries, expanded 1-hop via KNN edges, and associated chunks are aggregated and ranked for answer generation. The method uses Qwen2.5-72B-Instruct for both query generation and answer synthesis, with all-MiniLM-L6-v2 embeddings for similarity search.

## Key Results
- QCG-RAG achieves 73.16% accuracy on LiHuaWorld and 79.60% on MultiHop-RAG
- Outperforms chunk-based and graph-based baselines in both datasets
- Largest gains observed in multi-hop and null query categories
- Ablations confirm query formulation, node design, and hyperparameter balance are critical

## Why This Works (Mechanism)
QCG-RAG works by creating a semantic graph where nodes represent meaningful query-answer pairs rather than arbitrary text chunks. This query-centric approach ensures that retrieved evidence is semantically aligned with potential answers while maintaining interpretability through explicit query-answer pairs. The multi-hop retrieval mechanism expands the search space beyond direct matches, enabling discovery of evidence that may not be immediately adjacent to the query but is semantically connected through intermediate nodes.

## Foundational Learning
- Graph-based RAG: Why needed? Enables multi-hop reasoning and better evidence organization than linear retrieval
  Quick check: Can the graph connect evidence across multiple documents through intermediate nodes?
- Query-guided node construction: Why needed? Ensures retrieved chunks are semantically relevant to potential answers
  Quick check: Do generated query-answer pairs accurately represent their source chunks?
- Multi-hop retrieval: Why needed? Captures evidence that requires reasoning across multiple documents
  Quick check: Does expanding 1-hop neighbors add relevant documents not found in direct retrieval?

## Architecture Onboarding

Component Map: User Query -> Graph Query Matching -> 1-hop Expansion -> Chunk Aggregation -> Answer Generation

Critical Path: The retrieval pipeline is critical: matching user query to graph queries (threshold γ, max nodes n) → expanding neighbors (h hops, k edges) → aggregating and ranking chunks → generating answer (temp, top_p, top_k)

Design Tradeoffs: Chunk size vs. semantic granularity (1200 tokens with 100 overlap balances context vs. specificity); KNN k vs. retrieval coverage (k=2-3 balances relevance vs. noise); max nodes n vs. efficiency (n=10-15 balances recall vs. latency)

Failure Signatures: Poor query generation quality → noisy graph nodes → irrelevant retrieval; overly restrictive thresholds → missed evidence; excessive expansion → noisy neighbors

First Experiments: 1) Generate query-answer pairs from a sample chunk and verify quality against source; 2) Test retrieval with varying γ thresholds and measure recall vs. precision; 3) Compare single-hop vs. multi-hop retrieval on a simple multi-hop query

## Open Questions the Paper Calls Out
None

## Limitations
- Results may be sensitive to dataset-specific characteristics and query distributions
- Reliance on specific LLM (Qwen2.5-72B-Instruct) for both query generation and answer synthesis introduces potential brittleness
- Hyperparameters tuned per dataset may not generalize across domains

## Confidence
- Core accuracy claim: High (supported by quantitative results and ablation studies)
- Query-centric graph construction effectiveness: Medium (novel mechanism with demonstrated impact)
- Robustness and scalability: Low (limited evaluation scope, no dataset size ablation)

## Next Checks
1. Reproduce QCG-RAG on an additional open-domain QA dataset (e.g., HotpotQA or Natural Questions) to assess generalizability
2. Perform controlled ablation removing multi-hop retrieval (h=0) and query generation (α=0) to isolate individual contributions
3. Evaluate QCG-RAG using alternative embedding models and LLM-as-a-Judge evaluators to quantify robustness to model choice