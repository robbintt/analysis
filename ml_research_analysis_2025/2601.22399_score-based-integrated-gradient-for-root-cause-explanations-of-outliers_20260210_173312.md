---
ver: rpa2
title: Score-based Integrated Gradient for Root Cause Explanations of Outliers
arxiv_id: '2601.22399'
source_url: https://arxiv.org/abs/2601.22399
tags:
- outlier
- noise
- causal
- root
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of identifying root causes of outliers
  in complex network systems, which is crucial for anomaly detection and causal inference.
  Traditional methods struggle with uncertainty and high-dimensional dependencies.
---

# Score-based Integrated Gradient for Root Cause Explanations of Outliers

## Quick Facts
- arXiv ID: 2601.22399
- Source URL: https://arxiv.org/abs/2601.22399
- Authors: Phuoc Nguyen; Truyen Tran; Sunil Gupta; Svetha Venkatesh
- Reference count: 32
- Key outcome: SIREN achieves 85.0±11.5 average NDCG@k ranking on random graph datasets compared to 73.1±18.1 for the next best baseline (CIRCA).

## Executive Summary
This paper introduces SIREN, a novel method for identifying root causes of outliers in complex network systems governed by causal DAGs. Traditional outlier explanation methods struggle with uncertainty and high-dimensional dependencies, often requiring access to training data. SIREN addresses these limitations by estimating score functions of data likelihood and computing attribution via integrated gradients along paths from outliers toward normal data distributions. The method satisfies classic Shapley value axioms and operates directly on score functions, enabling tractable attribution in nonlinear, high-dimensional, and heteroscedastic causal models without requiring access to training data.

## Method Summary
SIREN identifies root causes by attributing outlier scores to noise variables in a causal graph using score-based integrated gradients. The method estimates local score models for each node's noise, then propagates these gradients through the causal structure using the chain rule. Attribution is computed by integrating gradients along reverse-time diffusion paths that simulate the restoration of an outlier to its normal state. The approach handles nonlinear, high-dimensional, and heteroscedastic causal models by operating directly on score functions rather than densities. Experiments demonstrate superior performance on synthetic random graphs and real-world cloud service and supply chain datasets.

## Key Results
- SIREN achieves 85.0±11.5 average NDCG@k ranking on random graph datasets
- Outperforms CIRCA baseline (73.1±18.1 NDCG@k) on the same task
- Demonstrates effectiveness on both synthetic and real-world datasets (cloud services, supply chain)
- Satisfies three classic Shapley value axioms plus an asymmetry axiom derived from causal structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Root causes can be attributed by integrating the gradient of the log-density (score function) along paths from an outlier observation back to the normal data distribution.
- **Mechanism:** SIREN defines an attribution score for each noise variable by computing a line integral of the partial derivative of the outlier measure with respect to that noise variable. This integral accumulates "outlierness" along a trajectory, measuring how much changes in each noise source contribute to the shift from normal to anomalous states.
- **Core assumption:** The outlier measure is differentiable with respect to noise variables, and valid integration paths exist connecting the outlier to the reference distribution.
- **Evidence anchors:** [abstract] ("Attribution is computed via integrated gradients that accumulate score contributions along paths..."); [Section III-A] (Definition 2, Eq. 14 formalizes baseline outlier contribution as path integral).
- **Break condition:** If the score function is highly non-smooth or path integration approximation has too few discretization steps, attribution may fail to converge or mis-rank causes.

### Mechanism 2
- **Claim:** The joint score function can be efficiently decomposed and estimated using the causal structure, avoiding need to compute global joint density.
- **Mechanism:** The method leverages chain rule for causal graphs (Eqs. 19-21), learning local score models for each node's noise variable and propagating gradients through partial derivatives of causal mechanisms.
- **Core assumption:** The causal structure is a Directed Acyclic Graph (DAG) and Functional Causal Models are differentiable.
- **Evidence anchors:** [abstract] ("operates directly on the score function... without requiring access to training data"); [Section III-B.1] (Eqs. 19-21 show recursive decomposition of score).
- **Break condition:** If the provided causal graph is incomplete, contains cycles, or functional relationships are non-differentiable, chain rule decomposition fails.

### Mechanism 3
- **Claim:** Using reverse-time diffusion (SDEs) allows sampling valid paths that approximate restoration of an outlier to its normal state.
- **Mechanism:** Algorithm 1 simulates reverse stochastic differential equation guided by estimated score function, creating trajectory of intermediate noise vectors that transition from observed outlier toward high-density region of normal data.
- **Core assumption:** Estimated score function accurately models gradient of log-density for noise distribution.
- **Evidence anchors:** [Section III-B.2] (Algorithm 1 and Eq. 22 define sampling process); [Section I] (Introduction mentions "diffusion-based integrated gradients").
- **Break condition:** If score model is under-trained or Euler-Maruyama step size is too large, diffusion trajectory may diverge or leave data manifold, resulting in uninformative gradients.

## Foundational Learning

**Concept: Score Functions & Score Matching**
- **Why needed here:** SIREN relies entirely on estimating score function (∇z log p(z)) rather than density itself. Understanding that score points in direction of increasing likelihood is required to interpret diffusion paths.
- **Quick check question:** Does the score function point towards higher or lower density regions?

**Concept: Integrated Gradients (IG)**
- **Why needed here:** This is the attribution logic. Understanding that IG attributes difference between output (outlier score) and baseline (normal score) by integrating gradients along path is essential.
- **Quick check question:** Why does IG require a baseline/reference point to compute attribution?

**Concept: Location-Scale Noise Models (Heteroscedasticity)**
- **Why needed here:** The paper explicitly extends root cause analysis to systems where noise variance depends on parents (Xj = f(Pa_j) + σ(Pa_j)Zj). Understanding this is required to fit "scale models" in architecture.
- **Quick check question:** In a location-scale model, what determines the variance of node Xj?

## Architecture Onboarding

**Component map:** Observed outlier vector x, Causal Graph G -> Mean models (f_j) and Scale models (σ_j) to invert observations to noise z -> Score Networks (s_j) estimate gradients -> Diffusion Sampler (Algorithm 1) generates paths -> Attribution scores ξ_j for each node

**Critical path:** 1) Fit Mean/Scale models on normal data (offline) 2) Train Score Networks on residuals (offline) 3) Inference: Invert outlier to get z_0 -> Run Diffusion to get path -> Integrate gradients -> Rank nodes

**Design tradeoffs:**
- Linear vs Non-Linear FCMs: Paper uses MLPs for complex synthetic data but linear regression for cloud services. Choose linear if data is low-volume or interpretability is key; use MLPs for high-dimensional interactions.
- Path Discretization (k): Too few steps in Algorithm 1 speeds up inference but risks missing gradient changes, reducing accuracy.

**Failure signatures:**
- High Variance Rankings: If Monte Carlo samples (m) are too low, scores will fluctuate.
- Dummy Variable Attribution: If node contributes nothing but gets high score, check score network training (might be fitting noise).

**First 3 experiments:**
1. Random Graph Validation: Test on synthetic data with 50-100 nodes using MLPs to verify method handles high non-linearity.
2. Cloud Service Latency: Apply to real-world web service data (assumed linear) to rank latency causes.
3. Supply Chain: Validate on time-lagged, heteroscedastic data (Location-Scale models) to ensure robustness to non-constant noise.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** How robust is SIREN to misspecifications in the causal graph, such as incorrect edge directions or missing parents?
- **Basis in paper:** [explicit] The paper explicitly states the framework relies on the assumption that the "data generating process [is] governed by a causal structure represented by a directed acyclic graph."
- **Why unresolved:** The experiments utilize randomly generated graphs or known service dependencies, assuming perfect structural knowledge is available during inference.
- **Evidence that would resolve it:** Evaluation on datasets where the input graph is systematically perturbed (e.g., edge deletions, reversals, or additions) relative to the ground truth to measure attribution degradation.

**Open Question 2**
- **Question:** Can the score-based integrated gradient method be extended to handle cyclic causal graphs or feedback loops?
- **Basis in paper:** [explicit] The authors note the method relies on a "directed acyclic graph (DAG)" and utilizes a topological order for the gradient chain rules.
- **Why unresolved:** Many complex systems (e.g., control systems, biological networks) contain feedback loops, which violate the acyclicity assumption required for the current theoretical derivation.
- **Evidence that would resolve it:** A theoretical extension of the score decomposition (Eqs. 19–21) that converges in cyclic settings and empirical validation on cyclic synthetic data.

**Open Question 3**
- **Question:** Is the method applicable to post-nonlinear causal models where the noise is not additively or scale-wise separable?
- **Basis in paper:** [inferred] While the paper claims broad applicability, the derivations (Eqs. 19–21) and experiments focus primarily on Additive Noise Models (ANM) and Location-Scale Noise Models (LSN).
- **Why unresolved:** In post-nonlinear models (e.g., Xj = h(f(Pa_j) + Nj)), the noise score may not decompose cleanly using the current chain rules, potentially biasing the gradient path.
- **Evidence that would resolve it:** Theoretical analysis of the score estimator's consistency when applied to post-nonlinear generating processes.

## Limitations

- Method relies on known DAG structure and differentiable functional relationships, limiting applicability to systems with incomplete causal knowledge or non-differentiable relationships
- Reverse-time SDE sampling introduces computational overhead and potential numerical instability, particularly with poorly estimated score functions
- Exact training procedures for score models (loss functions, noise schedules) are not fully specified, which may hinder reproducibility

## Confidence

- **High confidence** in mathematical formulation and satisfaction of Shapley axioms
- **Medium confidence** in experimental results given lack of detailed training procedures and hyperparameters
- **Low confidence** in generalizability to real-world scenarios with incomplete causal knowledge

## Next Checks

1. **Ablation study on DAG vs. cyclic graphs:** Test SIREN's performance on datasets with known cyclic dependencies to quantify impact of DAG assumption
2. **Sensitivity analysis of diffusion hyperparameters:** Systematically vary number of SDE steps, timestep size, and Monte Carlo samples to assess effect on attribution accuracy and computational efficiency
3. **Score model training procedure clarification:** Reproduce experiments with alternative score matching objectives (denoising score matching vs. sliced score matching) to isolate impact of training method on final performance