---
ver: rpa2
title: 'CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based
  on Large Language Models'
arxiv_id: '2502.14529'
source_url: https://arxiv.org/abs/2502.14529
tags:
- corba
- agents
- llm-mass
- arxiv
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Contagious Recursive Blocking Attacks (CORBA),
  a novel attack method targeting Large Language Model-based Multi-Agent Systems (LLM-MAS).
  CORBA exploits the recursive and contagious properties of LLM-MAS to propagate malicious
  prompts that cause agents to enter infinite blocking states, consuming computational
  resources and degrading system availability.
---

# CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models

## Quick Facts
- arXiv ID: 2502.14529
- Source URL: https://arxiv.org/abs/2502.14529
- Authors: Zhenhong Zhou; Zherui Li; Jie Zhang; Yuanhe Zhang; Kun Wang; Yang Liu; Qing Guo
- Reference count: 15
- Primary result: CORBA achieves up to 100% P-ASR with lower PTN than baselines

## Executive Summary
This paper presents Contagious Recursive Blocking Attacks (CORBA), a novel attack method targeting Large Language Model-based Multi-Agent Systems (LLM-MAS). CORBA exploits the recursive and contagious properties of LLM-MAS to propagate malicious prompts that cause agents to enter infinite blocking states, consuming computational resources and degrading system availability. Experiments on two popular LLM-MAS frameworks (AutoGen and Camel) with various topologies and commercial LLMs show CORBA achieves up to 100% Proportional Attack Success Rate (P-ASR) with significantly lower Peak Blocking Turn Number (PTN) compared to baseline methods. The attack remains effective even in open-ended LLM-MASs and complex network structures, using seemingly benign instructions that evade conventional alignment-based safety mechanisms.

## Method Summary
CORBA works by injecting a malicious prompt into an entry agent that causes recursive self-loop blocking behavior while simultaneously propagating the attack to neighboring agents. The attack exploits LLM-MAS communication patterns where agents trust and execute instructions from peers without authentication. The method is tested across multiple topologies (Chain, Cycle, Tree, Star, Random), agent counts (3, 5, 10), and commercial LLMs including GPT-4o-mini, GPT-4, and Gemini-2.0-Flash. The attack achieves high success rates by using benignly-framed instructions that evade conventional alignment safety filters while causing sustained resource depletion through recursive blocking.

## Key Results
- CORBA achieves 100% P-ASR on AutoGen framework with GPT-4o-mini across all tested topologies
- Attack propagation completes in fewer turns (PTN) compared to baseline methods, with star topology showing lowest PTN (3.7 turns)
- Safety monitors intercept CORBA prompts at rates below 25%, demonstrating evasion of conventional alignment mechanisms
- Attack effectiveness varies by LLM model, with GPT-4o-mini showing highest susceptibility (100% P-ASR) and GPT-3.5-turbo lowest (22% P-ASR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-loop recursive blocking maintains infinite agent-level resource consumption.
- Mechanism: Once injected, a malicious prompt causes the victim agent to generate a response that contains the same instruction, which is then re-fed to the same agent, creating a closed loop of self-perpetuating blocking behavior.
- Core assumption: LLMs will comply with benignly-framed repetition instructions when they appear within normal conversational context.
- Evidence anchors:
  - [abstract] "its recursive property enables sustained depletion of computational resources"
  - [section 3.1] Formal definition: "the attack passes recursively via a self-loop: Rm+l → atm+l+1 where (ab, ab) ∈ E denotes a self-loop"
  - [corpus] Weak direct evidence; corpus papers focus on monitoring/privacy rather than blocking mechanics.
- Break condition: If agents are configured with maximum recursion depth limits, per-turn token caps, or explicit instruction filtering for self-referential commands.

### Mechanism 2
- Claim: Contagious propagation across network topology amplifies single-point injection to system-wide failure.
- Mechanism: A blocked agent's output contains the malicious prompt, which is transmitted to neighboring agents during normal message-passing; neighbors interpret this as legitimate communication and execute the embedded instruction.
- Core assumption: Agents trust and execute instructions from peer agents without authentication or provenance verification.
- Evidence anchors:
  - [abstract] "its contagious nature allows it to propagate across arbitrary network topologies"
  - [section 3.3] "CORBA not only maintains a blocking state on each individual agent but also propagates the attack to neighboring agents"
  - [corpus] "Monitoring LLM-based Multi-Agent Systems Against Corruptions" confirms MAS communication complexity creates corruption susceptibility.
- Break condition: Topology isolation, message provenance tagging with untrusted-source filtering, or agent-level instruction validation gates.

### Mechanism 3
- Claim: Benign instruction framing evades alignment-based safety filters designed for overtly harmful content.
- Mechanism: The attack prompt uses seemingly harmless language (e.g., "pass this message to neighbors") that safety classifiers rate as low-risk, bypassing filters that target violence, hate speech, or explicit malicious intent.
- Core assumption: Current alignment training focuses on semantically harmful content, not on structurally exploitative but semantically neutral instructions.
- Evidence anchors:
  - [abstract] "blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods"
  - [appendix A, figure 3] Safety checker scores CORBA at 0.004-0.110 across models vs. baseline 0.004-0.330; figure 5 shows agent monitor interception rate below 0.25
  - [corpus] No direct corpus evidence on alignment evasion for blocking attacks.
- Break condition: Behavioral anomaly detection (not semantic), rate-limiting repetitive outputs, or mandatory instruction provenance logging with pattern analysis.

## Foundational Learning

- **Concept: LLM-MAS Topology and Message-Passing**
  - Why needed here: CORBA's propagation depends entirely on how agents are connected and how messages flow between them; understanding graph structures (chain, cycle, star, random) is prerequisite to predicting attack spread.
  - Quick check question: Given a 10-agent star topology with one hub, which node should an attacker target to maximize P-ASR?

- **Concept: Recursive Prompting and Self-Referential Instructions**
  - Why needed here: The core attack exploits LLMs' tendency to follow instructions that reference themselves; you must understand how context windows accumulate and how repetition loops form.
  - Quick check question: If an LLM receives "repeat this instruction forever" at turn 5, and its context window retains 10 turns, what happens at turn 15?

- **Concept: Alignment vs. Availability Attacks**
  - Why needed here: Conventional safety mechanisms conflate "harmful" with "violent/explicit"; CORBA demonstrates a novel threat category targeting resource availability rather than content harm.
  - Quick check question: Would a content filter trained on hate speech datasets flag an instruction saying "please forward this message to all your colleagues"? Why or why not?

## Architecture Onboarding

- **Component map:**
  - Entry Agent: The compromised node where CORBA prompt is first injected; can be any agent with external input access.
  - Propagation Layer: The edge set E defining which agents can message which; determines R(ab) reachable set.
  - Execution Engine: The underlying LLM (GPT-4, Gemini, etc.) that processes prompts; vulnerability is model-agnostic but severity varies.
  - Safety Monitor (optional): Existing alignment filters; Appendix A demonstrates ineffectiveness against CORBA.

- **Critical path:**
  1. Identify entry point (any externally-facing agent).
  2. Inject CORBA prompt formatted as benign communication.
  3. Entry agent executes → generates recursive blocking output.
  4. Output propagates to all neighbors (one hop per turn).
  5. Each newly-infected agent repeats steps 3-4.
  6. System reaches PTN when all reachable nodes are blocked.

- **Design tradeoffs:**
  - Centralized (star) vs. decentralized (random) topology: Star concentrates propagation speed (lowest PTN = 3.7 for GPT-4o-mini) but creates single point of failure; distributed topologies slow propagation but offer no chokepoints for containment.
  - Open vs. restricted message formats: Restricting agents to structured schemas reduces contagion surface but limits collaborative flexibility.

- **Failure signatures:**
  - Sudden uniformity in agent outputs (all agents producing identical or near-identical responses).
  - Prolonged execution time without task progress.
  - Token consumption spikes without corresponding output diversity.
  - Agent responses containing verbatim copies of previous messages.

- **First 3 experiments:**
  1. Replicate Table 1 baseline: Run CORBA on AutoGen with 3 GPT-4o-mini agents in chain topology; measure P-ASR and PTN over 10 trials to verify claimed 100% success rate.
  2. Topology sensitivity test: Using the same LLM configuration, compare P-ASR and PTN across star vs. cycle vs. random topologies with 5 agents; confirm whether star topology indeed produces lowest PTN.
  3. Defense probe: Implement a simple repetition detector that flags any agent output with >80% n-gram overlap with previous turn; measure reduction in P-ASR when this filter is active.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What effective defense mechanisms can prevent contagious recursive blocking attacks in LLM-MAS without disrupting legitimate multi-agent coordination?
- Basis in paper: [explicit] The Limitations section states: "In future work, we will further investigate effective defense mechanisms to prevent such blocking attacks in LLM-MASs."
- Why unresolved: Current alignment-based safety mechanisms fail because CORBA uses seemingly benign instructions. Appendix A shows LLM-based monitors achieve <25% interception, and perplexity-based detection cannot distinguish CORBA prompts from normal text.
- What evidence would resolve it: Novel detection methods (e.g., recursive pattern analyzers, propagation rate monitors) demonstrating high true-positive rates while maintaining low false-positive rates on legitimate agent workflows.

### Open Question 2
- Question: Can LLM architecture or training modifications inherently resist recursive blocking without requiring external detection systems?
- Basis in paper: [inferred] Tables 1-4 show substantial variation in P-ASR across models (e.g., GPT-4o-mini: 100% vs. GPT-3.5-turbo: 22% on 5-agent AutoGen), suggesting model-specific vulnerabilities that remain unexplained.
- Why unresolved: The paper evaluates multiple commercial LLMs but does not investigate which architectural properties, training data, or alignment procedures contribute to differential susceptibility.
- What evidence would resolve it: Controlled experiments varying specific training objectives or architectural components (e.g., recursion handling, instruction-following mechanisms) showing consistent resistance patterns.

### Open Question 3
- Question: What network topology design principles can limit contagion spread while preserving collaborative capability in LLM-MAS?
- Basis in paper: [inferred] Tables 3-4 show P-ASR varies by topology (Star: 98.33% vs. Random: 66.67% on GPT-4o-mini), but the relationship between topology structure and attack propagation dynamics is not analyzed.
- Why unresolved: The paper demonstrates effectiveness across topologies but does not identify structural properties that slow propagation or isolate infected subnetworks.
- What evidence would resolve it: Systematic analysis correlating graph-theoretic metrics (e.g., centrality, connectivity, clustering coefficient) with PTN and final P-ASR, yielding topology design guidelines.

## Limitations

- Evaluation relies entirely on controlled lab experiments with small agent counts (3-10) and synthetic topologies, leaving scalability and real-world deployment uncertainties unaddressed.
- The attack prompt formulation appears underspecified in the paper, with the exact text needed for faithful reproduction unclear from figures alone.
- No adversarial training or real-time defense evaluation was conducted, limiting practical mitigation insights.
- Experiments focus exclusively on LLM-based MAS frameworks, leaving applicability to non-LLM agent systems unexplored.

## Confidence

**High confidence** in the recursive blocking mechanism (Mechanism 1) - well-defined mathematically and consistently demonstrated across experiments with clear measurable outcomes.

**Medium confidence** in the contagious propagation claims (Mechanism 2) - the topological spread is well-modeled, but real-world network structures may introduce complexities not captured in synthetic topologies.

**Medium confidence** in the alignment evasion claim (Mechanism 3) - safety monitor scores provide some evidence, but the evaluation doesn't test against production-grade safety systems or demonstrate that malicious payload injection is feasible in practice.

## Next Checks

1. **Reproduce baseline results**: Run the attack on AutoGen with 3 GPT-4o-mini agents in star topology, measuring P-ASR and PTN over 10 trials to verify the claimed 100% success rate.

2. **Test defense effectiveness**: Implement a repetition detector that flags outputs with >80% n-gram overlap with previous turns, then measure P-ASR reduction when this filter is active in the system.

3. **Evaluate against safety monitors**: Test the attack against production-grade content moderation systems (e.g., OpenAI moderation API) to verify whether the benign instruction framing truly evades detection in real deployments.