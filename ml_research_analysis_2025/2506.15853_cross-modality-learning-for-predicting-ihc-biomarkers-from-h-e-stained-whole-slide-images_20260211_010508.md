---
ver: rpa2
title: Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide
  Images
arxiv_id: '2506.15853'
source_url: https://arxiv.org/abs/2506.15853
tags:
- wsis
- histostainalign
- staining
- paired
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces HistoStainAlign, a deep learning framework\
  \ that predicts IHC staining patterns directly from H&E whole-slide images by learning\
  \ joint representations of morphological and molecular features through contrastive\
  \ training. Evaluated on gastrointestinal and lung tissue WSIs with three commonly\
  \ used IHC stains\u2014P53, PD-L1, and Ki-67\u2014the model achieved weighted F1\
  \ scores of 0.735 (95% CI: 0.670\u20130.799), 0.830 (95% CI: 0.772\u20130.886),\
  \ and 0.723 (95% CI: 0.607\u20130.836), respectively."
---

# Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images

## Quick Facts
- arXiv ID: 2506.15853
- Source URL: https://arxiv.org/abs/2506.15853
- Reference count: 30
- Achieved weighted F1 scores of 0.735 (P53), 0.830 (PD-L1), and 0.723 (Ki-67) for predicting IHC staining patterns from H&E images

## Executive Summary
This study introduces HistoStainAlign, a deep learning framework that predicts immunohistochemistry (IHC) staining patterns directly from hematoxylin and eosin (H&E)-stained whole-slide images. The model learns joint representations of morphological and molecular features through contrastive training, enabling cross-stain alignment between H&E and IHC data. Evaluated on gastrointestinal and lung tissue samples with three commonly used IHC stains, the framework demonstrates the potential to reduce reliance on additional IHC staining while maintaining diagnostic accuracy.

## Method Summary
HistoStainAlign employs a contrastive learning approach to align representations from H&E and IHC-stained whole-slide images. The framework uses a patch-based architecture where corresponding patches from H&E and IHC images are processed through shared encoders. A contrastive loss function pulls together representations of corresponding patches while pushing apart non-corresponding pairs. The model predicts IHC staining patterns at the patch level, which are then aggregated to generate whole-slide predictions. The training process leverages both paired H&E-IHC images and unpaired H&E images to maximize learning from available data.

## Key Results
- Achieved weighted F1 scores of 0.735 (95% CI: 0.670-0.799) for P53, 0.830 (95% CI: 0.772-0.886) for PD-L1, and 0.723 (95% CI: 0.607-0.836) for Ki-67 predictions
- Embedding analyses demonstrated robust cross-stain alignment between H&E and IHC representations
- Comparative analysis showed contrastive learning approach outperformed baseline models in stain pattern prediction accuracy

## Why This Works (Mechanism)
The framework succeeds by learning to map morphological features visible in H&E staining to molecular patterns revealed by IHC staining. Through contrastive learning, the model discovers invariant representations that capture the relationship between tissue architecture and protein expression patterns. The patch-based approach allows the model to learn local-to-global mappings, where specific morphological features at the cellular level correspond to molecular expression patterns that aggregate to tissue-level biomarkers.

## Foundational Learning
- **Contrastive learning**: Needed to align representations from different modalities (H&E and IHC) by learning what features correspond across stains; quick check: verify that paired patches from different modalities have similar embeddings
- **Patch-based processing**: Required to handle gigapixel whole-slide images by breaking them into manageable units while preserving spatial relationships; quick check: ensure patch overlap and stitching produce coherent whole-slide predictions
- **Multi-instance learning**: Essential for aggregating patch-level predictions to whole-slide level while accounting for heterogeneous tissue composition; quick check: validate that aggregation methods (mean, max, attention) produce consistent results
- **Cross-modal supervision**: Needed to leverage the relationship between morphological features and molecular markers without requiring extensive paired annotations; quick check: confirm that the model can predict IHC patterns even with limited paired training data
- **Transfer learning from natural images**: Helpful for initializing feature extractors on H&E data where large annotated datasets are scarce; quick check: compare performance with and without pre-training on natural image datasets

## Architecture Onboarding

**Component Map:**
H&E WSI -> Patch Extraction -> Shared Encoder -> Patch Embeddings -> Contrastive Loss
IHC WSI -> Patch Extraction -> Shared Encoder -> Patch Embeddings -> Contrastive Loss
Patch Embeddings -> Prediction Head -> IHC Pattern Prediction
Patch Predictions -> Aggregation -> Whole-Slide Prediction

**Critical Path:**
Patch extraction from H&E and IHC images → shared encoder processing → contrastive loss optimization → prediction head training → patch-level IHC pattern prediction → whole-slide aggregation

**Design Tradeoffs:**
The model balances between using large patch sizes (better context but fewer patches and higher computational cost) versus smaller patches (more patches but less contextual information). The choice of contrastive loss formulation (e.g., InfoNCE vs. Siamese-style) affects training stability and convergence. The aggregation method for patch predictions to whole-slide predictions involves tradeoffs between simple averaging and more complex attention-based methods that may better capture heterogeneous tissue regions.

**Failure Signatures:**
Poor performance may manifest as failure to align H&E and IHC embeddings, indicating the model hasn't learned the morphological-molecular relationship. Inconsistent patch-level predictions that don't aggregate coherently suggest issues with the prediction head or aggregation method. Low confidence predictions across entire slides may indicate insufficient training data or poor generalization to new tissue types or staining protocols.

**First Experiments:**
1. Evaluate embedding similarity between H&E and IHC patches using t-SNE or UMAP visualization to verify cross-modal alignment
2. Test patch-level prediction accuracy on held-out patches to identify whether failures occur at local or global levels
3. Compare different aggregation strategies (mean, max, attention) on the same patch predictions to determine optimal whole-slide prediction method

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes for certain biomarkers, particularly Ki-67 with only 12 annotated WSIs, resulting in wide confidence intervals
- Single-institution dataset limits generalizability across different scanners, staining protocols, and tissue preparation methods
- No external validation performed to test model performance on diverse datasets and clinical settings
- Does not establish whether predicted IHC patterns meet pathologist-level accuracy requirements for diagnostic use

## Confidence
- P53 predictions: Medium confidence (F1 0.735, 95% CI: 0.670-0.799)
- PD-L1 predictions: High confidence (F1 0.830, 95% CI: 0.772-0.886)
- Ki-67 predictions: Medium confidence (F1 0.723, 95% CI: 0.607-0.836)

## Next Checks
1. External validation on multi-institutional datasets with diverse scanning equipment and staining protocols to assess generalizability
2. Direct comparison of model predictions against expert pathologist annotations on the same tissue samples to establish clinical accuracy thresholds
3. Prospective validation where predicted IHC patterns guide treatment decisions, followed by outcome analysis to determine clinical impact