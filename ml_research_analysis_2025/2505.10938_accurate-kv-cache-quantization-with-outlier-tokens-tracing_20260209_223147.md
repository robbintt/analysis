---
ver: rpa2
title: Accurate KV Cache Quantization with Outlier Tokens Tracing
arxiv_id: '2505.10938'
source_url: https://arxiv.org/abs/2505.10938
tags:
- quantization
- outlier
- tokens
- kivi
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient KV cache management
  in large language models (LLMs) during inference. While KV cache significantly reduces
  recomputation, it introduces substantial memory overhead.
---

# Accurate KV Cache Quantization with Outlier Tokens Tracing

## Quick Facts
- arXiv ID: 2505.10938
- Source URL: https://arxiv.org/abs/2505.10938
- Reference count: 40
- Primary result: Achieves 6.4× memory reduction and 2.3× throughput increase under 2-bit quantization

## Executive Summary
This paper addresses the challenge of efficient KV cache management in large language models (LLMs) during inference. While KV cache significantly reduces recomputation, it introduces substantial memory overhead. The authors propose a novel method called KV Cache Quantization with Outlier Tokens Tracing (OTT) that improves quantization accuracy by identifying and excluding outlier tokens from the quantization process. Through extensive experiments, OTT achieves significant accuracy improvements under 2-bit quantization, delivering a 6.4× reduction in memory usage and a 2.3× increase in throughput compared to existing methods.

## Method Summary
OTT improves KV cache quantization by identifying outlier tokens that disrupt uniform channel distributions and excluding them from quantization. The method computes a score based on the L1-norm of each token's Key vector, with lower scores indicating potential outliers. These tokens are stored in a fixed-size outlier pool at full precision while the remaining tokens are quantized. A competition mechanism updates the pool at each quantization step, retaining tokens with smallest Key magnitudes. The approach combines channel-wise Key quantization with token-wise Value quantization, following the KIVI framework, but adds the outlier exclusion mechanism to preserve accuracy.

## Key Results
- Achieves up to 12.93% improvement over KIVI on GSM8K with 2-bit quantization
- Delivers 6.4× memory reduction compared to FP16 baseline
- Provides 2.3× throughput increase while maintaining accuracy
- Particularly effective for normal context length tasks

## Why This Works (Mechanism)

### Mechanism 1
A small subset of tokens with low-magnitude Keys in otherwise high-magnitude "outlier channels" disproportionately degrades quantization accuracy. Outlier channels are assumed to have uniform Key distributions within-channel, but some tokens exhibit very small Key values within these channels. When quantizing across all tokens, these small-magnitude outliers expand the `Xmax - Xmin` range, forcing a larger quantization step size `q`. This coarser step size increases the quantization error for all tokens in the channel, degrading overall accuracy.

### Mechanism 2
Identifying outlier tokens by the magnitude of their Keys (L1-norm) and excluding them from quantization preserves accuracy. OTT computes a score `Si = ‖Ki‖` for each token. Tokens with smaller scores are identified as outliers because their Keys deviate from the uniform distribution in outlier channels. During decoding, these tokens are stored in a fixed-size outlier pool at full precision, while remaining tokens are quantized. A competition mechanism updates the pool at each quantization step, retaining the tokens with smallest Key magnitudes.

### Mechanism 3
The quantization error from outlier tokens propagates to attention weights, amplifying output degradation. For outlier channels where Query values `Qc` are typically large, the quantization error `K'c,i - Kc,i` is amplified by `Qc` when computing attention scores. This error propagates to the attention output, causing larger L1 loss compared to retaining full-precision tokens with small Keys.

## Foundational Learning

- **KV Cache in Transformers**: Why needed - The entire method operates on the KV cache during inference; understanding its role in reducing computational complexity from O(n²) to O(n) and its memory overhead is essential. Quick check - During which phase (prefill or decoding) are Keys and Values cached for the first time?

- **Quantization and Group Quantization**: Why needed - OTT builds upon group quantization (specifically channel-wise Key and token-wise Value quantization from KIVI); understanding how quantization parameters like step size `q` affect accuracy is critical. Quick check - How does group quantization differ from uniform quantization in handling data with non-uniform distributions?

- **Attention Computation with Rotary Position Embeddings**: Why needed - The error propagation mechanism involves attention score calculation; understanding the interaction between Queries, Keys, and positional encodings helps grasp why outlier channels matter. Quick check - What role does the normalization factor `√dk` play in attention score computation?

## Architecture Onboarding

- **Component map**: Quantization Stage -> Outlier Pool -> Decoding Stage -> Competition Mechanism
- **Critical path**: During prefill, Keys and Values are cached and initially quantized. During decoding, for every `G` tokens: compute Key magnitudes, compete with existing outlier pool, update pool, exclude outliers from quantization. Compute attention by concatenating results from quantized, full-precision, and outlier Keys/Values.
- **Design tradeoffs**:
  1. Memory vs. Accuracy: Larger outlier pool improves accuracy but increases memory; `outlier_num=3` balances this
  2. Quantization Frequency: Larger group size `G` improves compression ratio but may reduce accuracy due to wider `Xmax-Xmin` ranges
  3. Residual Length: Larger sliding window for recent tokens improves accuracy at memory cost; `R=32` is chosen
- **Failure signatures**:
  1. Accuracy drop on long-generation tasks: May indicate insufficient outlier pool size or accumulation of quantization errors
  2. Memory not reducing as expected: Could occur if sequence length is very short or batch size very large, limiting compression
  3. Throughput degradation: May result from excessive outlier pool operations or inefficient fused kernel implementation
- **First 3 experiments**:
  1. Validate outlier token hypothesis: Replicate Figure 1c on a different model (e.g., LLaMA-3-8B) to confirm that retaining small-Key tokens minimizes L1 loss
  2. Hyperparameter sweep: Test `outlier_num` in {1,3,6} and group sizes in {64,128} on GSM8K to find optimal accuracy-memory tradeoff
  3. End-to-end throughput test: Compare OTT, KIVI, and FP16 on a fixed input/output length with increasing batch size to verify 2.3× throughput claim

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the error accumulation in OTT be mitigated for tasks requiring extremely long generation lengths?
**Basis in paper:** [Explicit] The authors explicitly state in the Limitations section: "When the generation length is very long, OTT may face an unacceptable risk of loss due to error accumulation."
**Why unresolved:** While OTT successfully identifies outlier tokens to improve per-step quantization accuracy, it does not implement a mechanism to correct the compounding errors that occur over many auto-regressive steps in long generations.
**What evidence would resolve it:** A modification to the OTT algorithm that includes an error-feedback loop or periodic full-precision calibration, demonstrated through stable accuracy on benchmarks requiring generation lengths significantly longer than the 384 tokens tested in the paper's memory analysis.

### Open Question 2
**Question:** Can the compression efficiency of OTT be improved for scenarios where sequence length is shorter than the group size?
**Basis in paper:** [Explicit] The paper notes in the Limitations section that "In extreme cases, when the sequence length is shorter than the group size, OTT does not perform any compression."
**Why unresolved:** The current implementation relies on a fixed group size ($G=128$) for quantization. Short sequences (e.g., short prompts or query batches) result in the retention of full-precision KV caches, negating the memory benefits.
**What evidence would resolve it:** An adaptive grouping mechanism or an alternative quantization strategy specifically optimized for short-context, high-batch-size scenarios that maintains a high compression ratio similar to the 6.4× reduction seen in long sequences.

### Open Question 3
**Question:** Is the outlier token tracing method effective for sub-2-bit quantization (e.g., 1-bit or 1.5-bit)?
**Basis in paper:** [Inferred] The paper focuses extensively on 2-bit quantization results, noting that outliers "inflate quantization step size." The authors do not evaluate if tracing these outliers is sufficient to maintain accuracy when precision is reduced even further.
**Why unresolved:** As bit-width decreases, the quantization step size increases, potentially amplifying the impact of non-outlier tokens to a degree where isolating only a small pool of outlier tokens is insufficient to preserve the model's reasoning capabilities.
**What evidence would resolve it:** Experimental results comparing OTT against baselines like KIVI at 1-bit or binary quantization levels on the same benchmarks (GSM8K, BBH) used in the paper.

## Limitations
- Relies on assumption that outlier channels have uniformly distributed Keys except for a few low-magnitude outliers, which may not generalize to all models or datasets
- Effectiveness is sensitive to hyperparameter choices (outlier pool size, group quantization frequency) that were not extensively explored across different model architectures
- Focuses primarily on normal context lengths with limited validation on extremely long sequences where different quantization challenges may arise

## Confidence

**High Confidence**: The identification of outlier tokens and their exclusion from quantization demonstrably improves accuracy under 2-bit quantization, as evidenced by the 12.93% improvement over KIVI on GSM8K and consistent improvements across multiple benchmarks.

**Medium Confidence**: The mechanism explaining how outlier tokens degrade quantization accuracy through expanded Xmax-Xmin ranges is well-supported by the data, but the generalizability to other quantization schemes or model architectures requires further validation.

**Low Confidence**: The 2.3× throughput improvement claim depends heavily on the efficiency of the CUDA fused kernel implementation, which is not fully detailed in the paper. Hardware-specific optimizations may not translate across different GPU architectures.

## Next Checks

1. **Mechanism Validation**: Replicate Figure 1c experiments on LLaMA-3-8B with different group sizes (G=64, 128, 256) to verify that retaining small-Key tokens consistently minimizes L1 loss across quantization granularities.

2. **Hyperparameter Sensitivity**: Conduct a systematic sweep of outlier_num (1, 3, 6) and residual_length (16, 32, 64) on GSM8K and BBH to quantify the accuracy-memory tradeoff and identify optimal configurations for different context lengths.

3. **Long Context Generalization**: Test OTT on LongBench tasks (500+ tokens) with sequence lengths up to 4096 tokens to validate whether the outlier token mechanism remains effective for long-generation scenarios and whether the 6.4× memory reduction scales proportionally.