---
ver: rpa2
title: 'From Image Generation to Infrastructure Design: a Multi-agent Pipeline for
  Street Design Generation'
arxiv_id: '2509.05469'
source_url: https://arxiv.org/abs/2509.05469
tags:
- design
- lane
- prompt
- image
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent system for generating realistic
  bicycle infrastructure designs directly on street-view imagery. The system uses
  specialized agents for lane localization, prompt optimization, design generation,
  and automated evaluation to address limitations of existing generative AI approaches
  in spatial accuracy and instruction adherence.
---

# From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation

## Quick Facts
- **arXiv ID:** 2509.05469
- **Source URL:** https://arxiv.org/abs/2509.05469
- **Reference count:** 40
- **Primary result:** Introduces a multi-agent system for generating realistic bicycle infrastructure designs on street-view imagery with >95% evaluator accuracy

## Executive Summary
This paper presents a multi-agent pipeline that generates realistic bicycle infrastructure designs directly on street-view imagery by combining specialized AI agents for spatial reasoning, prompt optimization, design generation, and automated evaluation. The system addresses key limitations of existing generative AI approaches in spatial accuracy and instruction adherence through a cascading generation process and vision-language alignment techniques. Experiments demonstrate the pipeline consistently produces instruction-aligned and visually coherent designs across diverse urban scenarios, reducing expertise and time requirements while establishing a foundation for AI-assisted transportation infrastructure planning.

## Method Summary
The pipeline processes street-view images through four specialized agents: a Locator Agent uses GPT-o3 to extract spatial context and lane boundaries, a Prompt Optimization Agent employs GPT-4.5 to merge user instructions with spatial context using in-context learning, a Design Generation Agent utilizes GPT-image-1 with a two-step cascade (highlight region → final design) to generate 5-10 candidates, and an Evaluator Agent uses fine-tuned YOLO-v11 for segmentation, CLIP for visual ranking, and GPT-o3 for binary compliance checking. The system operates on 150 road segments from Google Street View API (1024×1024 pixels) and achieves >95% accuracy across eight specific bicycle infrastructure design scenarios.

## Key Results
- Pipeline consistently produces instruction-aligned and visually coherent bicycle infrastructure designs across diverse urban scenarios
- Evaluator Agent achieves accuracy above 95% for all design scenarios
- Ablation studies demonstrate significant performance improvements when all specialized agents are used versus simplified baselines
- Reduces expertise and time requirements for bicycle infrastructure planning while maintaining design quality

## Why This Works (Mechanism)

### Mechanism 1: Spatial Context Injection via Reasoning MLLMs
The system improves spatial alignment by having a reasoning model (GPT-o3) analyze street views to produce structured textual descriptions of lane boundaries, serving as semantic anchors that condition the image generator to respect existing road geometry. This reduces ambiguity and prevents hallucinated layouts. Evidence shows removing the Locator Agent results in misaligned lanes, and the approach builds on established "LLM-as-a-judge" paradigms.

### Mechanism 2: Constraint Decomposition via Cascading Generation
The Design Generation Agent employs a two-step process: first highlighting the target region (geometry/prior), then applying specific design textures. This separates "where" from "what" constraints, reducing cognitive load on the model and preventing distortion of road layout while adding infrastructure elements. Ablation confirms this approach maintains width control and design compliance.

### Mechanism 3: Automated Quality Gating with Vision-Language Alignment
High output reliability (>95%) is achieved by filtering stochastic generator outputs through dual-stage evaluation: CLIP similarity for visual alignment and reasoning MLLM for binary compliance checking. Fine-tuned YOLO segmentation masks environmental noise before CLIP comparison, addressing embedding noise limitations and ensuring accurate candidate selection.

## Foundational Learning

- **Concept: Multimodal Reasoning (Vision-Language Models)**
  - Why needed: The system relies on models like GPT-o3 for understanding scenes and verifying logic, requiring knowledge of how to prompt for structured output
  - Quick check: Can you distinguish between a model's ability to caption an image versus its ability to reason about spatial relationships?

- **Concept: In-Context Learning & Prompt Optimization**
  - Why needed: The Prompt Optimization Agent uses high-quality examples to guide the model, requiring understanding of how to structure examples to stabilize output
  - Quick check: How does providing a structured template reduce semantic ambiguity compared to natural language prompts?

- **Concept: Stochastic Generators & Ranking Strategies**
  - Why needed: The paper generates multiple candidates because first-pass results are often unreliable, requiring understanding of why generative models fail and how to rank outputs
  - Quick check: Why is generating 10 candidates and selecting 1 more effective than perfecting a single generation prompt?

## Architecture Onboarding

- **Component map:** Input (Street View Image + User Instruction) → Locator Agent (GPT-o3) → Prompt Optimization Agent (GPT-4.5) → Design Generation Agent (GPT-image-1) → Evaluator Agent (YOLO-v11 → CLIP → GPT-o3)
- **Critical path:** The Locator Agent -> Prompt Optimizer link is crucial; incorrect spatial description leads to wrong geometry encoding and misplaced lanes
- **Design tradeoffs:** Latency vs. Quality (generating 5-10 candidates increases latency and API cost), Specialization vs. Generalization (fine-tuned YOLO improves accuracy but requires data annotation)
- **Failure signatures:** Spatial Drift (inconsistent lane width), Semantic Bleeding (design crossing boundaries), Hallucination (unintended object removal)
- **First 3 experiments:** 1) Locator Ablation (disable Locator Agent to test geometric positioning), 2) Backbone Swap (replace GPT-image-1 with Stable Diffusion 3.5), 3) Noise Sensitivity (test Evaluator with visual noise to verify segmentation mask effectiveness)

## Open Questions the Paper Calls Out

- **Question:** How can the single-pass generation correctness rate be improved to eliminate computational latency from multiple candidate generation?
- **Question:** To what extent can human intervention, specifically manual image selection and verification, be reduced while maintaining output quality?
- **Question:** Can the framework be adapted to guarantee pixel-level spatial accuracy in complex street layouts where current alignment is inconsistent?
- **Question:** Does the multi-agent pipeline generalize effectively to other forms of transportation infrastructure beyond bicycle facilities?

## Limitations

- The correctness rate of single generation passes remains relatively low, necessitating multiple candidates and increasing computational cost
- The pipeline still involves substantial human intervention, especially manual image selection, limiting scalability
- Fine-grained positional accuracy is not always achieved, particularly in complex street layouts
- The current agents and prompts are highly specialized for bicycle lane semantics, and their transferability to other infrastructure types is unproven

## Confidence

**High confidence:** The multi-agent architecture and cascading generation mechanism are well-supported by ablation studies
**Medium confidence:** Spatial context injection via reasoning MLLMs shows promise but relies heavily on GPT-o3's assumed spatial reasoning capability
**Low confidence:** The 95% Evaluator Agent accuracy claim lacks transparency in validation methodology and fine-tuned YOLO details

## Next Checks

1. **Spatial Reasoning Validation:** Test Locator Agent on a separate validation set with manually annotated lane boundaries to quantify spatial parsing accuracy
2. **Cross-City Generalization:** Deploy the trained pipeline on street imagery from cities not included in the original 150-segment dataset to measure performance degradation
3. **Evaluator Robustness Testing:** Systematically test Evaluator Agent with reference images containing various visual noise levels to measure segmentation accuracy and CLIP ranking stability across environmental conditions