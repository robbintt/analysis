---
ver: rpa2
title: 'Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics
  Course'
arxiv_id: '2501.14499'
source_url: https://arxiv.org/abs/2501.14499
tags:
- grading
- feedback
- llms
- students
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the use of large language models (LLMs) for
  grading written assignments in a university bioinformatics course. Over 100 students
  completed 36 text-based questions, with responses graded by both LLMs and human
  teaching assistants in a blind study.
---

# Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course

## Quick Facts
- arXiv ID: 2501.14499
- Source URL: https://arxiv.org/abs/2501.14499
- Authors: Pavlin G. Poličar; Martin Špendl; Tomaž Curk; Blaž Zupan
- Reference count: 26
- Key outcome: LLMs achieved grading accuracy and feedback quality comparable to human graders in a university bioinformatics course

## Executive Summary
This study evaluated large language models for grading written assignments in a university bioinformatics course. Over 100 students completed 36 text-based questions across five assignments, with responses graded by both LLMs and human teaching assistants in a blind study. Six commercial and open-source LLMs were systematically compared. Results showed that with well-designed prompts, LLMs achieved grading accuracy and feedback quality comparable to human graders. Open-source LLMs performed as well as commercial models, enabling privacy-preserving grading systems.

## Method Summary
The study evaluated six LLMs (GPT-4o, Nvidia-70B, Llama-405Bq4, Llama-70B, Llama-70Bq4, Llama-8B) for grading student submissions across 36 text-based questions in a bioinformatics course. Each question had a structured rubric with 1-4 criteria worth 1-3 points each. The grading process used carefully designed prompts including system instructions, user prompts with question details, correct answers, student submissions, grading rubrics, and up to 10 TA-graded examples. Models produced JSON output with grading, score, and feedback. Performance was measured against a gold standard of 670 manually graded submissions using classification accuracy per criterion and average grading difference to assess bias.

## Key Results
- LLMs achieved classification accuracy comparable to human TAs when provided with well-structured rubrics and examples
- Open-source LLMs (Llama-405Bq4, Llama-70B) performed as well as commercial models (GPT-4o)
- Student satisfaction surveys showed no significant preference between human and LLM feedback, with slight preference for LLM feedback on correct answers

## Why This Works (Mechanism)
The success of LLM grading relies on structured prompt engineering that provides clear evaluation criteria and examples. The system prompt establishes the grader's role and guidelines, while the user prompt supplies the question context, correct answer, student submission, detailed rubric, and graded examples. This comprehensive context enables the LLM to understand the grading framework and apply consistent criteria across submissions.

## Foundational Learning
- **Structured grading rubrics**: Why needed - provide explicit criteria for objective evaluation; Quick check - rubric has 1-4 criteria with point values and descriptions
- **Prompt engineering**: Why needed - guides LLM to produce consistent, format-compliant outputs; Quick check - system and user prompts include all required components
- **Classification accuracy metrics**: Why needed - measures how well LLM matches human grading on individual criteria; Quick check - CA > 80% indicates strong performance
- **Ablation testing**: Why needed - identifies which prompt components most affect grading behavior; Quick check - removing examples makes grading stricter, removing rubric makes it more lenient

## Architecture Onboarding
- **Component map**: Prompt template -> LLM inference -> JSON output -> Classification accuracy calculation
- **Critical path**: Rubric design → Prompt construction → Model inference → Accuracy evaluation → Student feedback
- **Design tradeoffs**: Larger models (Llama-405Bq4) vs privacy (local deployment); comprehensive examples vs prompt length limits
- **Failure signatures**: Llama-8B shows systematic positive bias and accuracy below 80%; rubric-only prompts cause overly strict grading; examples-only prompts cause overly lenient grading
- **Three first experiments**: 1) Test grading accuracy with and without examples in prompts; 2) Compare performance of Llama-8B vs Llama-70B on same rubric; 3) Measure bias shift when removing rubric from prompt

## Open Questions the Paper Calls Out
- Can fine-tuning smaller open-source LLMs on domain-specific grading data achieve performance comparable to larger models?
- How robust are LLM grading systems against adversarial prompt-hacking attempts in pure LLM-only environments?
- Does LLM-generated feedback produce equivalent or improved long-term learning outcomes compared to human TA feedback?
- How do LLM grading systems perform across disciplines with different answer formats and domain-specific reasoning requirements?

## Limitations
- Full system prompt text not specified, particularly "anti-cheating instructions"
- Study limited to bioinformatics course content with structured rubrics
- Hardware specifications for local model inference not provided
- Sampling methodology for graded examples lacks implementation details

## Confidence
- **High Confidence**: LLMs can achieve classification accuracy comparable to human graders with well-structured rubrics and examples; open-source models perform similarly to commercial models; student satisfaction shows no significant preference between human and LLM feedback
- **Medium Confidence**: LLMs provide consistent grading across different question difficulties; rubric-only prompts cause stricter grading while examples-only cause leniency; small models show systematic performance degradation
- **Low Confidence**: Findings generalize to other academic disciplines; grading approach scales effectively to large courses; student satisfaction translates to actual learning outcomes

## Next Checks
1. Reproduce grading accuracy results using same six LLMs on different subject domain (e.g., humanities) with comparable rubric structure
2. Conduct ablation studies removing each prompt component on held-out test set to verify stated effects on grading leniency/strictness
3. Implement grading system with Llama-8B on standard consumer hardware to empirically measure claimed performance degradation