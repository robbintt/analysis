---
ver: rpa2
title: Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts
arxiv_id: '2506.23845'
source_url: https://arxiv.org/abs/2506.23845
tags:
- concepts
- saes
- concept
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sparse autoencoders (SAEs) have been criticized due to negative
  results on concept detection and model steering tasks. This paper argues that these
  negative results stem from using SAEs to act on known concepts, rather than to discover
  unknown concepts.
---

# Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts

## Quick Facts
- arXiv ID: 2506.23845
- Source URL: https://arxiv.org/abs/2506.23845
- Authors: Kenny Peng; Rajiv Movva; Jon Kleinberg; Emma Pierson; Nikhil Garg
- Reference count: 7
- Primary result: SAEs underperform baselines when acting on known concepts but excel at discovering unknown concepts in hypothesis generation and explaining language model behaviors.

## Executive Summary
Sparse autoencoders (SAEs) have been criticized for negative results on concept detection and model steering tasks. This paper argues these failures stem from using SAEs to act on known concepts rather than to discover unknown concepts. The authors distinguish between tasks where concepts are inputs (known) versus outputs (unknown), showing SAEs excel at the latter. Through hypothesis generation and explaining language model behaviors, SAEs demonstrate their strength in uncovering previously unknown patterns in unstructured data, with potential applications in ML interpretability, fairness, auditing, and social/health sciences.

## Method Summary
The method involves training Top-K sparse autoencoders on language model token representations or text embeddings. The SAE architecture maps input representations through an encoder to a sparse latent space, then reconstructs via a decoder. For discovery tasks, features highly correlated with target variables are identified, and an LLM autointerprets high-activating samples to generate natural language concepts. The pipeline includes validation through held-out data to catch unreliable features.

## Key Results
- SAEs underperform baselines (logistic regression, prompting) on known-concept detection tasks due to information loss in reconstruction
- SAEs successfully discover unknown concepts through hypothesis generation, outperforming n-gram and topic model baselines
- The autointerpretation pipeline generates precise, monosemantic concepts that explain language model behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAEs underperform baselines on concept detection because reconstructions encode strictly less information than original representations.
- Mechanism: SAEs are trained to reconstruct LM token representations with sparsity constraints. By the data processing inequality, a reconstruction cannot contain more information about a target concept than the source representation.
- Core assumption: The concept being detected is not perfectly captured by the sparse features the SAE prioritizes for reconstruction.
- Evidence anchors:
  - [section 3] "A reconstruction encodes strictly less information about a token than the original LM representation. It follows that... there is less information available in the SAE representation to predict the presence of a concept."
  - [corpus] Weak direct corpus evidence on this specific mechanism; neighbor papers focus on steering and geometric properties.
- Break condition: If SAE dictionary size approaches input dimension and sparsity constraints are relaxed, information loss decreases but interpretability may degrade.

### Mechanism 2
- Claim: SAEs succeed at discovering unknown concepts because they produce monosemantic features—neurons that fire on single, interpretable concepts rather than polysemantic combinations.
- Mechanism: L1 or TopK sparsity forces each latent dimension to activate only on a narrow subset of inputs. With sufficiently large dictionary size (M > D), the SAE learns disentangled features that map to natural language concepts.
- Core assumption: The monosemanticity holds reliably at scale and autointerpretation methods produce faithful concept labels.
- Evidence anchors:
  - [section 2] "Unlike LM neurons, SAEs produce monosemantic neurons that can be explained by a single concept [Cunningham et al., 2023, Bricken et al., 2023]."
  - [section 4] "An empirical strength of the SAE is its precise concepts. If the rabbit neuron instead fired on all animals, it would be difficult to answer whether the model improvises or plans rhymes."
  - [corpus] "Temporal Sparse Autoencoders" paper notes SAEs "suffer from a variety of" limitations but confirms they "provide a promising route to discover human-interpretable features."
- Break condition: If features become polysemantic at scale (feature absorption, dead neurons), discovered concepts become unreliable.

### Mechanism 3
- Claim: SAEs enable hypothesis generation by providing a tractable enumeration of candidate concepts that can be computationally validated downstream.
- Mechanism: Rather than manually specifying concepts, SAEs unsupervisedly generate a sparse set of features. These can be filtered for task-relevance (e.g., correlation with engagement), then validated through independent annotation or intervention experiments.
- Core assumption: The space of relevant concepts is sparse relative to the full space of possible concepts, so SAE feature selection surfaces meaningful candidates.
- Evidence anchors:
  - [section 4] "By enumerating a set of precise concepts that express the variation in text data, it is possible to systematically discover concepts that satisfy a desired property."
  - [section 4] "Because of this falsifiability, even if an SAE feature is unreliable... it is possible to catch these issues downstream."
  - [corpus] "Identifiable Steering via Sparse Autoencoding" addresses multi-concept shifts but doesn't directly test this mechanism.
- Break condition: If validation data is unavailable or concepts are too fine-grained to annotate reliably, the discovery pipeline stalls.

## Foundational Learning

- Concept: **Sparsity (L1 regularization / TopK)**
  - Why needed here: The core mechanism distinguishing SAEs from standard autoencoders; forces learned features to be interpretable.
  - Quick check question: If you set λ = 0 in the L1-penalized SAE loss, what happens to feature interpretability?

- Concept: **Polysemanticity vs. Monosemanticity**
  - Why needed here: Motivates SAE use—LM neurons are polysemantic, SAE features are claimed to be monosemantic.
  - Quick check question: A neuron that activates strongly on both "French text" and "baseball statistics" is ______semantic.

- Concept: **Concepts as Inputs vs. Outputs**
  - Why needed here: The paper's central distinction—tasks where concepts are prespecified (inputs) versus discovered (outputs).
  - Quick check question: In model steering, is the target concept an input or output to the task?

## Architecture Onboarding

- Component map:
  - LM token representations or text embeddings -> Encoder (linear + ReLU + TopK) -> Sparse latent z -> Decoder (linear) -> Reconstruction

- Critical path:
  1. Collect LM representations (e.g., residual stream activations or text embeddings)
  2. Train SAE with appropriate dictionary size M and sparsity level k
  3. For unknown-concept discovery: identify features correlated with target variable or active during task execution
  4. Run autointerpretation on selected features
  5. Validate discovered concepts through annotation or intervention

- Design tradeoffs:
  - **Dictionary size (M)**: Larger M captures more concepts but increases computational cost and risk of feature absorption
  - **Sparsity (k or λ)**: Higher sparsity improves monosemanticity but may miss nuanced concepts
  - **Architecture choice**: TopK SAEs vs. L1-penalized; sparse transcoders vs. standard SAEs (transcoders use layer ℓᵢ to predict layer ℓⱼ)

- Failure signatures:
  - **Dead neurons**: Features that never activate; mitigated by auxiliary losses or resampling
  - **Feature absorption**: Similar concepts merged into single feature; reduces interpretability
  - **Overly generic explanations**: Autointerpretation produces vague concepts ("text about things") that fail validation
  - **Using SAEs for known-concept tasks**: Baselines (logistic regression, prompting) consistently outperform

- First 3 experiments:
  1. **Reproduce negative result**: Train probing classifier on LM representations vs. SAE representations for a known binary classification task. Expect SAE to underperform.
  2. **Hypothesis generation test**: Apply SAE to a text dataset with target variable (e.g., headline engagement). Compare discovered concepts against n-gram or topic model baselines for statistical significance and rater helpfulness.
  3. **Feature validation**: Select top-k SAE features active during a specific LM behavior (e.g., addition). Manually verify whether autointerpretation labels match ground-truth concepts through intervention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SAEs effectively bridge the "prediction-explanation gap" in social and health sciences by identifying spurious correlations?
- Basis in paper: [explicit] Section 5 explicitly frames SAEs as a promising tool to bridge this gap and discover "illegitimate or spurious features" in unstructured data.
- Why unresolved: While conceptually argued, the paper notes that existing validation is limited to domains like news headlines and Congressional speeches rather than high-stakes scientific data.
- What evidence would resolve it: A study showing SAEs identify statistically significant, previously unknown predictors in clinical or economic datasets where prior theory is incomplete.

### Open Question 2
- Question: Can methodological innovations make SAEs competitive for "acting on known concepts," or is this a fundamental limitation?
- Basis in paper: [inferred] Section 3 speculates on failure reasons but notes that "methodological innovations may make SAEs more competitive," leaving the possibility open.
- Why unresolved: The paper strongly advocates for "discovery" use cases, but does not definitively prove that negative results on steering/detection are insurmountable rather than just current engineering constraints.
- What evidence would resolve it: A new SAE architecture or feature selection technique that matches or exceeds baseline performance (e.g., prompting) on standard steering benchmarks.

### Open Question 3
- Question: To what extent does the lack of consensus on automatic neuron interpretation affect the reliability of SAEs for hypothesis generation?
- Basis in paper: [inferred] Section 2 highlights "ongoing debates" and "no consensus" regarding autointerpretation scoring, yet Section 4 relies on this step to generate scientific hypotheses.
- Why unresolved: If the interpretation step is noisy or subjective, the validity of the "discovered" concepts may vary, potentially undermining the scientific utility of the pipeline.
- What evidence would resolve it: A sensitivity analysis measuring the variance in generated hypotheses when different autointerpretation protocols are applied to the same SAE features.

## Limitations
- Information loss in SAE reconstructions is fundamental and always reduces detection performance for known concepts
- Monosemanticity claims at scale remain unproven with potential feature absorption and dead neurons
- Heavy reliance on autointerpretation introduces uncertainty without established validation protocols

## Confidence
- High Confidence: SAEs underperform baselines on concept detection tasks; distinction between concepts as inputs vs outputs is valid; reconstruction-sparsity tradeoff is established
- Medium Confidence: SAEs excel at discovering unknown concepts (limited empirical support); autointerpretation can reliably generate concept labels (untested at scale); proposed use cases are practically valuable (requires domain validation)
- Low Confidence: SAEs bridge prediction-explanation gap in social/health sciences (speculative); proposed validation pipeline catches all reliability issues (unverified); monosemanticity persists at needed dictionary sizes

## Next Checks
1. **Information-Theoretic Analysis**: Compare mutual information between SAE representations and target concepts versus original LM representations across multiple concept detection tasks to quantify information loss.
2. **Scale-Invariance Test**: Train SAEs at multiple dictionary sizes (M/D = 2, 4, 8, 16) on the same discovery task and measure feature absorption rates, autointerpretation consistency, and downstream validation success.
3. **Autointerpretation Validation**: For discovered concepts, conduct human validation studies comparing autointerpretation explanations, manual annotations, and simple statistical descriptors to quantify the added value and reliability of the autointerpretation step.