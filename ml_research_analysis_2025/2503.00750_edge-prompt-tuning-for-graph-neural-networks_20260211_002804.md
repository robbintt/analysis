---
ver: rpa2
title: Edge Prompt Tuning for Graph Neural Networks
arxiv_id: '2503.00750'
source_url: https://arxiv.org/abs/2503.00750
tags:
- graph
- edgeprompt
- prompt
- node
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EdgePrompt proposes a graph prompt tuning method that learns additional
  prompt vectors for edges in pre-trained GNN models. The method manipulates input
  graphs by incorporating edge prompts through message passing, improving the quality
  of graph representations for downstream tasks.
---

# Edge Prompt Tuning for Graph Neural Networks

## Quick Facts
- arXiv ID: 2503.00750
- Source URL: https://arxiv.org/abs/2503.00750
- Reference count: 32
- Primary result: EdgePrompt+ achieves state-of-the-art performance on node and graph classification tasks across ten datasets under four pre-training strategies

## Executive Summary
EdgePrompt introduces a graph prompt tuning method that learns additional prompt vectors for edges in pre-trained GNN models. By manipulating input graphs through message passing with edge prompts, the method improves the quality of graph representations for downstream tasks. EdgePrompt+ enhances this by learning customized prompt vectors for each edge using anchor prompts, achieving superior performance compared to six baselines. The method is compatible with various GNN architectures and pre-training strategies, providing a universal solution for bridging the objective gap between pre-training and downstream tasks.

## Method Summary
EdgePrompt adds learnable edge prompts to pre-trained GNN models while keeping the backbone frozen. EdgePrompt uses a single global prompt vector per layer, while EdgePrompt+ learns customized edge prompts via weighted averages of anchor prompts. The prompts are computed using node representations and an attention mechanism, then injected into the message passing function. The method is optimized using Adam with a linear classifier, and experiments show it outperforms baselines on both node and graph classification tasks across multiple datasets and pre-training strategies.

## Key Results
- EdgePrompt+ achieves state-of-the-art performance on node classification and graph classification tasks
- The method demonstrates superior performance across ten graph datasets under four different pre-training strategies
- EdgePrompt+ shows significant improvements in few-shot learning scenarios compared to node-level prompt baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Edge prompts mitigate the "uniform message passing" limitation of node-level prompts, allowing for context-specific modulation of information flow between neighbors.
- **Mechanism:** Unlike node prompts that broadcast a single vector $p_i$ to all neighbors, EdgePrompt learns vectors $e_{ij}$ for edges. In EdgePrompt+, these are customized via a weighted sum of anchor prompts, enabling node $v_j$ to receive a distinct signal from $v_i$ compared to other neighbors of $v_i$.
- **Core assumption:** Downstream performance is bottlenecked by the inability of frozen weights to modulate the *strength* or *nature* of relationships between specific node pairs.
- **Evidence anchors:**
  - [abstract] "EdgePrompt manipulates input graphs by learning additional prompt vectors for edges... to better embed graph structural information."
  - [section 4.3] "Unlike one shared prompt vector of a node for all its neighboring nodes, EdgePrompt+ enables these neighboring nodes to receive different learned prompt vectors (e.g., $e_{21}$ and $e_{31}$) from the node."
  - [corpus] Corpus paper "GraphTOP" abstract supports this direction, noting that "Structure-based pre-training methods are under-explored yet crucial."
- **Break condition:** If the downstream task relies purely on node features (e.g., a graph of isolated nodes where edges carry no semantic weight), edge prompts will likely fail to provide gains over node prompts.

### Mechanism 2
- **Claim:** The "Anchor Prompt" decomposition allows for high-capacity tuning on edges even with limited supervision (few-shot settings).
- **Mechanism:** Instead of learning independent parameters for every edge (which is infeasible with sparse labels), EdgePrompt+ maintains a shared set of $M$ anchor prompts $P$ at each layer. Each edge prompt $e_{ij}$ is a weighted average of these anchors, where weights are computed by an attention function $\phi(v_i, v_j)$ over node representations.
- **Core assumption:** The necessary edge-wise adjustments for the downstream task can be approximated by a linear combination of a small number of latent "basis" prompts.
- **Evidence anchors:**
  - [section 4.2] "learning $|E|$ independent prompt vectors is infeasible... we propose to learn the prompt vectors as a weighted average of multiple anchor prompts."
  - [section 4.2] Eq. (4) defines $e_{ij} = \sum b_{ijm} \cdot p_m$.
  - [corpus] "HGMP" abstract mentions learning "rich structural features" from unlabeled data, paralleling how shared anchors generalize edge structure here.
- **Break condition:** If the diversity of edge semantics in the downstream task exceeds the capacity of the anchor set (e.g., too few anchors $M$ for a complex heterophilic graph), the approximation will underfit.

### Mechanism 3
- **Claim:** Theoretical analysis suggests edge prompts can explicitly improve linear separability of node classes in the representation space.
- **Mechanism:** The paper utilizes the Contextual Stochastic Block Model (CSBM) to prove that edge prompts can increase the expected distance between class centroids by a factor $T > 1$, whereas node prompts or no prompts result in a smaller separation.
- **Core assumption:** The graph data follows a distribution similar to CSBM where nodes from the same class are more likely to connect (homophily), and pre-trained features are separable but overlapping.
- **Evidence anchors:**
  - [section 4.3] Theorem 1 states there exist prompts that "improve the expected distance... to $T$ times without using edge prompts, where $T \in (1, 1 + p/|p-q|]$."
  - [section 6] "Experiment results demonstrate the superiority of our method... for both node classification."
  - [corpus] Corpus evidence for theoretical separability proofs is weak or missing; neighbors focus on empirical applications like fairness ("Adaptive Dual Prompting").
- **Break condition:** If the pre-trained GNN is already perfectly separating classes (distance is maxed), or if the graph is extremely heterophilic (inter-class edges dominate), the assumptions of the proof may not hold.

## Foundational Learning

- **Concept: Message Passing Neural Networks (MPNNs)**
  - **Why needed here:** This paper modifies the fundamental `AGG` and `COMB` functions of GNNs. You must understand how features propagate from $N(v_i)$ to $v_i$ to grasp where the edge prompt $e_{ij}$ is injected.
  - **Quick check question:** In a standard GCN, does the edge weight affect the aggregation magnitude, or just the topology? (Answer: Both, usually via normalized adjacency).

- **Concept: Prompt Tuning vs. Fine-Tuning**
  - **Why needed here:** The core constraint is that the GNN backbone $f$ is *frozen*. You need to understand that we are optimizing a small set of parameters (prompts) to "trick" the frozen model into solving a new task.
  - **Quick check question:** If I update the weights of the GNN layer during training, am I doing prompt tuning? (Answer: No, that is fine-tuning).

- **Concept: Attention Mechanisms (GAT)**
  - **Why needed here:** EdgePrompt+ computes edge prompt weights using an attention-like score function $\phi$. Understanding how "relevance" is computed between two nodes is crucial for debugging the weight generation.
  - **Quick check question:** What is the output dimension of the score function $\phi(v_i, v_j)$ relative to the number of anchors $M$? (Answer: It outputs a vector of size $M$).

## Architecture Onboarding

- **Component map:** Input Graph $G=(V, E)$ with node features $X$ -> Prompt Generator (holds $M$ learnable Anchor Prompts $P^{(l)}$ and weight matrix $W^{(l)}$) -> Edge Scoring (computes $b_{ij}$ using node features and $W^{(l)}$) -> Prompt Injection (computes edge prompt $e_{ij} = \sum b_{ijm} p_m$) -> Frozen GNN (executes message passing using $e_{ij}$) -> Readout/Classifier (maps final embeddings to labels)

- **Critical path:** The prompt injection step (Eq. 2). If the aggregation function `AGG` is modified incorrectly (e.g., simple summation instead of a learnable weighted sum), the signal from $e_{ij}$ may be drowned out by node features $h_j$.

- **Design tradeoffs:**
  - **EdgePrompt vs. EdgePrompt+:** EdgePrompt uses a single global prompt (low parameter count, fast, but low capacity). EdgePrompt+ uses anchors (higher capacity, slightly slower, better for few-shot complex tasks).
  - **Anchor Count ($M$):** Paper suggests $M=10$ for node classification. Too few anchors reduces expressivity; too many increases search space and risk of overfitting to few-shot labels.

- **Failure signatures:**
  - **Performance Collapse to Baseline:** If the learning rate for prompts is too high compared to the classifier, the prompts might move to effectively zero or infinity, nullifying their effect.
  - **Overfitting in Few-Shot:** If using EdgePrompt+ with high $M$ on extremely small datasets (e.g., 1-shot), the model might memorize the specific edges of the labeled nodes rather than generalizing structure.

- **First 3 experiments:**
  1. **Sanity Check (EdgePrompt):** Run EdgePrompt (global prompt) on Cora. Verify it beats "Classifier Only" and matches GPF performance to ensure the pipeline is correct.
  2. **Ablation on $M$:** Run EdgePrompt+ on a dataset (e.g., CiteSeer) with $M \in \{1, 5, 10, 20\}$. Verify the "elbow" curve shown in Figure 3.
  3. **Heterophily Test:** Test on a heterophilic graph (not explicitly in the paper's main table, but implied by the mechanism). Check if customized edge prompts can suppress inter-class connections better than node prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can EdgePrompt be effectively adapted to handle heterogeneous graphs where distinct types of nodes and edges possess varying semantic meanings and feature spaces?
- **Basis in paper:** [explicit] Appendix E explicitly lists adapting the method for heterogeneous graphs as a primary direction for future work.
- **Why unresolved:** The current methodology assumes a homogeneous graph structure ($G=(V,E)$) and uses a standard attention mechanism (Eq. 6) that may not capture the complex meta-relations or type-specific constraints inherent in heterogeneous graphs.
- **What evidence would resolve it:** A modification of the score function $\phi$ and prompt aggregation mechanism that incorporates edge type encodings, validated by performance improvements on standard heterogeneous graph benchmarks (e.g., IMDB, DBLP) over homogeneous baselines.

### Open Question 2
- **Question:** Does the performance of EdgePrompt generalize to generative self-supervised pre-training strategies, specifically those based on feature masking rather than contrastive learning?
- **Basis in paper:** [explicit] Appendix E states, "we will investigate the performance of our method under more pre-training strategies, such as DGI... InfoGraph... GraphMAE."
- **Why unresolved:** The experiments in the main text (Table 2, 3) are restricted to contrastive (GraphCL, SimGRACE) and edge prediction strategies. Generative methods like GraphMAE operate by reconstructing masked features, and it is unclear if edge prompts would interfere with or aid the reconstruction objectives during tuning.
- **What evidence would resolve it:** Empirical results from experiments conducted on models pre-trained with GraphMAE or InfoGraph, demonstrating whether EdgePrompt maintains statistical superiority over "Classifier Only" and node-prompt baselines in these settings.

### Open Question 3
- **Question:** Can the design of edge prompts be advanced through conditional prompting to allow for dynamic adjustments based on specific downstream task properties?
- **Basis in paper:** [explicit] Appendix E explicitly proposes to "explore other designs for edge prompts, such as conditional prompting" in future studies.
- **Why unresolved:** The current design (EdgePrompt+) relies on learning a weighted average of static anchor prompts (Eq. 4). It does not explicitly condition the prompt vectors on task-specific instructions or varying graph contexts, which may limit adaptability in multi-task scenarios.
- **What evidence would resolve it:** A comparative study showing that a conditional prompt mechanism (e.g., conditioning $\mathbf{e}_{ij}$ on task embeddings) yields higher accuracy or faster convergence in multi-task few-shot learning environments compared to the proposed static anchor approach.

## Limitations

- Theoretical assumptions about homophily and CSBM may not hold for heterophilic graphs or when pre-trained features are already well-separated
- The method assumes edge semantics can be captured by a small number of anchor prompts, which may fail for highly complex or heterogeneous edge types
- Lack of ablation on anchor prompt initialization and sampling strategy for few-shot splits affects reproducibility

## Confidence

- **High:** Empirical performance claims on benchmark datasets (Table 1-2) and the general mechanism of edge prompt injection
- **Medium:** Theoretical analysis using CSBM model and claims about improved linear separability
- **Medium:** Claims about universal compatibility across GNN architectures and pre-training strategies

## Next Checks

1. **Heterophily Test:** Evaluate EdgePrompt+ on a heterophilic graph (e.g., Texas or Cornell) to verify performance gains over node prompts
2. **Anchor Capacity Ablation:** Systematically vary $M$ on a mid-sized dataset (e.g., CiteSeer) to confirm the elbow curve and identify overfitting thresholds
3. **Initialization Sensitivity:** Test different initialization schemes for anchor prompts and projection matrices to assess robustness to hyperparameters