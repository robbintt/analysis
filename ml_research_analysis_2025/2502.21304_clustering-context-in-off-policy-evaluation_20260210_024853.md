---
ver: rpa2
title: Clustering Context in Off-Policy Evaluation
arxiv_id: '2502.21304'
source_url: https://arxiv.org/abs/2502.21304
tags:
- chips
- policy
- clusters
- evaluation
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the off-policy evaluation (OPE) problem in
  contextual bandits, where the goal is to estimate the value of a new policy using
  logged data from a different policy. Standard OPE estimators like IPS suffer from
  high variance or bias when the logging policy differs significantly from the evaluation
  policy or when certain actions are underrepresented in the data.
---

# Clustering Context in Off-Policy Evaluation

## Quick Facts
- **arXiv ID**: 2502.21304
- **Source URL**: https://arxiv.org/abs/2502.21304
- **Reference count**: 40
- **Primary result**: CHIPS estimator reduces variance by clustering contexts and applying IPS-style reweighting within clusters, outperforming baseline methods especially under high policy distributional shift or deficient action support.

## Executive Summary
This paper addresses the off-policy evaluation (OPE) problem in contextual bandits by proposing CHIPS, a clustering-based estimator that reduces variance compared to standard IPS. The method partitions contexts into clusters and applies importance sampling within each cluster, allowing it to handle cases where the logging policy differs significantly from the evaluation policy or where certain actions are underrepresented in the data. The authors provide theoretical analysis of CHIPS' bias and variance and demonstrate its effectiveness through experiments on both synthetic and real-world datasets.

## Method Summary
CHIPS addresses OPE by clustering contexts into groups and applying IPS-style reweighting within each cluster. The estimator computes cluster-level importance weights by averaging policy probabilities across all contexts within a cluster, then combines these weights with reward estimates (either ML or MAP with Beta prior) to produce a policy value estimate. The method mitigates the variance issues of standard IPS by pooling information across similar contexts while relaxing the common support assumption to the cluster level rather than individual contexts.

## Key Results
- CHIPS outperforms baseline methods (IPS, DM, DR, MR, MIPS) on synthetic datasets, especially in high distributional shift scenarios (β=-1).
- The method demonstrates improved estimation accuracy through context clustering, reducing variance while maintaining manageable bias.
- MAP estimation provides robustness against reward misspecification in high shift settings, outperforming ML estimation in these scenarios.
- On real-world datasets (Open Bandit Dataset), CHIPS shows superior performance with 8 clusters and MAP estimation (α=20).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering contexts reduces variance compared to standard IPS, particularly when logging and evaluation policies differ significantly.
- Mechanism: Standard IPS uses point estimates for importance weights $w(a,x)$, which can exhibit high variance. CHIPS computes weights $w(a,c)$ by averaging policy probabilities across all contexts within a cluster $c$. This marginalization reduces the variance of the importance weights (the variance of an average is less than the variance of the individual points).
- Core assumption: Common Support (Assumption 2.1) and Reward Homogeneity (Assumption 3.2) must hold for the strict variance reduction proof.
- Evidence anchors:
  - [Section 3.1]: "Proposition 3.11... indicates that when Assumptions 2.1 and 3.2 hold, the variance reduction of CHIPS compared to IPS corresponds to the total decrease in mean squared error."
  - [Appendix A.7]: Proof shows the variance difference is proportional to the variance of IPS weights conditioned on the cluster.
  - [Corpus]: Related work "Context-Action Embedding Learning for OPE" similarly aims to reduce variance via embeddings, validating the general approach of smoothing importance weights.
- Break condition: If the cluster is too large or heterogeneous (violating homogeneity), the aggregated weight $w(a,c)$ becomes a poor proxy for $w(a,x)$, potentially increasing bias without sufficient variance reduction.

### Mechanism 2
- Claim: CHIPS mitigates the bias caused by "deficient support" (where $\pi_0(a|x)=0$ but $\pi(a|x)>0$) by relaxing the overlap requirement.
- Mechanism: IPS fails if a specific context $x$ has zero probability of action $a$ in the logging data. CHIPS requires only that the *cluster* $c$ (containing $x$) has a non-zero probability of $a$. If other contexts in $c$ have observed action $a$, the estimator can extrapolate to $x$.
- Core assumption: Common Cluster Support (Assumption 3.1) requires $p(a|c, \pi_0) > 0$ for any action desired by $\pi$.
- Evidence anchors:
  - [Section 3]: "Assumption 3.1 is weaker than Assumption 2.1 since... $\pi_0(a|x)=0, \pi(a|x)>0$ does not ensure the same holds for every context within $c$."
  - [Proposition 3.8]: Characterizes bias specifically in terms of "deficient actions" at the cluster level ($U(c, \pi_0)$).
- Break condition: If the entire cluster is deficient (no contexts in $c$ saw action $a$), CHIPS reverts to the same bias issues as IPS.

### Mechanism 3
- Claim: Using a Maximum A Posteriori (MAP) reward estimate per cluster provides robustness against reward misspecification.
- Mechanism: In high distributional shift settings, the observed rewards in the logging data may not reflect the true value of actions for the evaluation policy. By using a Bayesian prior (Beta distribution) to estimate rewards $\hat{r}(a,c)$ within a cluster, the estimator smooths noisy or sparse data, pulling estimates toward a prior belief rather than overfitting to biased logging samples.
- Core assumption: The context space exhibits a cluster structure where rewards are approximately homogeneous (Assumption 3.2), justifying the pooling of data.
- Evidence anchors:
  - [Section 4.1.1]: "In such cases [high shift], ML estimation in CHIPS is ineffective, while MAP estimation provides some resistance by pushing reward estimates towards the posterior expectation."
  - [Figure 2 & 13]: Show MAP outperforming ML (Mean) estimation in real and synthetic datasets with high shift.
  - [Corpus]: "Log-Sum-Exponential Estimator" also addresses heavy-tailed reward distributions, suggesting robustness is a key theme in modern OPE.
- Break condition: If the Beta prior parameters ($\alpha$) are miss-selected (e.g., too strong a prior), the estimator may resist the true signal, increasing MSE over the simpler ML approach.

## Foundational Learning

- Concept: **Inverse Propensity Scoring (IPS)**
  - Why needed here: CHIPS is an extension of IPS. You cannot understand the modification (pooling contexts) without understanding the baseline (reweighting rewards by policy probability ratios).
  - Quick check question: If the logging policy never takes an action that the evaluation policy loves, what happens to the IPS weight?

- Concept: **Common Support / Overlap**
  - Why needed here: The paper frames CHIPS as a solution to the "deficient information" problem caused by a lack of common support. Understanding that $\pi_0(a|x) \neq 0$ is a requirement for IPS unbiasness is critical.
  - Quick check question: Why is "Common Cluster Support" considered a weaker assumption than standard "Common Support"?

- Concept: **K-Means Clustering**
  - Why needed here: The practical implementation relies on partitioning the context space into discrete clusters to pool data.
  - Quick check question: If K-Means groups two contexts with fundamentally different optimal actions, which core assumption of CHIPS is likely violated?

## Architecture Onboarding

- **Component map**:
  Input -> Clustering Module -> Policy Aggregator -> Reward Estimator -> CHIPS Estimator

- **Critical path**:
  1. Context Clustering (determines the granularity of information pooling).
  2. Probability Estimation (constructing the numerator/denominator of the weight).
  3. Reward smoothing (MAP estimation).

- **Design tradeoffs**:
  - **Number of Clusters ($K$)**: Low $K$ increases bias (oversimplifying context behavior); High $K$ increases variance and approaches IPS. A V-shaped error curve is expected (See Figure 1).
  - **Reward Estimation (MAP vs ML)**: Use MAP for high distributional shift or sparse data; use ML for low shift or abundant data.

- **Failure signatures**:
  - **V-Shaped MSE**: If MSE improves then degrades as $K$ increases, you have passed the optimal cluster resolution.
  - **High Bias on High-Magnitude Samples**: If the clustering method fails to group similar contexts (high intra-cluster variance), the weight $w(a,c)$ will be misrepresentative of the true importance.

- **First 3 experiments**:
  1. **Cluster Sensitivity Check**: Run CHIPS on your data varying $K$ (e.g., 10, 50, 100, 500). Plot MSE. Look for the "sweet spot" before the error rises again.
  2. **Deficient Action Stress Test**: Create a synthetic subset where $\pi_0(a|x)=0$ for specific actions. Compare IPS (should fail/infinite weight) vs CHIPS (should remain finite).
  3. **Prior Tuning**: If using MAP, tune the $\alpha$ parameter based on the average number of samples per cluster-action pair (see Figure 17 reference). Small clusters need smaller $\alpha$ to avoid over-regularization.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can combining the CHIPS estimator with action-embedding methods like MIPS improve general OPE performance?
  - Basis in paper: [explicit] The conclusion explicitly identifies the need to explore "if combining CHIPS with pure action-embedding methods like MIPS can improve general performance."
  - Why unresolved: The theoretical analysis (Proposition 3.12) compares variances of the distinct methods but does not formulate or test a unified estimator.
  - What evidence would resolve it: Empirical results showing that a composite estimator utilizing both context clustering and action embeddings yields lower mean squared error than either method individually in complex environments.

- **Open Question 2**: Can the optimal number of clusters and the prior parameter $\alpha$ be determined theoretically rather than empirically?
  - Basis in paper: [explicit] Section 5 highlights the open possibility of estimating "the optimal value for hyperparameters beyond empirical estimation."
  - Why unresolved: The current implementation relies on heuristics (Appendix D.4) and empirical sensitivity analysis (Section 4.1.1) to select these values.
  - What evidence would resolve it: A derivation providing theoretical bounds or an automated selection criterion for hyperparameters that consistently minimizes MSE without manual tuning.

- **Open Question 3**: Can clustering methods be designed to explicitly optimize for the Reward Homogeneity assumption (3.2) to reduce estimator bias?
  - Basis in paper: [inferred] Section 4.1.1 notes that performance is "sensitive to effectiveness of clustering," and Proposition 3.6 shows bias depends on the gap in $\delta$-homogeneity.
  - Why unresolved: The experiments use generic clustering algorithms (e.g., K-Means) based on context distance, which do not guarantee the reward homogeneity required to minimize bias.
  - What evidence would resolve it: Experiments demonstrating that a reward-aware clustering algorithm reduces bias compared to standard context-based clustering when Assumption 3.2 is partially violated.

## Limitations
- Theoretical analysis assumes clustering and policy estimation are independent, which may not hold in practice.
- The method's performance heavily depends on the quality of clustering - poor clustering that groups dissimilar contexts can increase bias significantly.
- While MAP estimation provides robustness for high distributional shift, the choice of Beta prior parameters (α, β) is heuristic and may not generalize across domains.

## Confidence
- **High Confidence**: The variance reduction mechanism (Mechanism 1) and the experimental superiority over baseline methods on synthetic data are well-supported by both theory and experiments.
- **Medium Confidence**: The bias mitigation claim (Mechanism 2) is theoretically sound but depends heavily on the clustering quality and whether the weaker "Common Cluster Support" assumption actually holds in practice.
- **Low Confidence**: The MAP estimation's effectiveness across diverse real-world datasets is demonstrated but relies on specific parameter tuning (α=20) that may not transfer to other domains.

## Next Checks
1. **Cluster Quality Validation**: Implement cluster validation metrics (intra-cluster variance, silhouette score) to empirically verify the homogeneity assumption. Test CHIPS performance when clustering quality degrades.
2. **Prior Sensitivity Analysis**: Systematically vary α, β parameters for MAP estimation across different distributional shift levels (β ∈ [-1, 1]) to identify robust prior choices rather than relying on a single heuristic value.
3. **Distributional Shift Robustness**: Test CHIPS on real-world datasets with varying levels of policy shift (β ∈ [-1, 1]) and compare with modern embedding-based OPE methods like Context-Action Embedding Learning to establish relative performance in different shift regimes.