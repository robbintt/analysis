---
ver: rpa2
title: An Exploratory Study on Multi-modal Generative AI in AR Storytelling
arxiv_id: '2505.15973'
source_url: https://arxiv.org/abs/2505.15973
tags:
- storytelling
- content
- participants
- aigc
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of multi-modal generative AI for
  AR storytelling, addressing the challenge of creating high-quality, multi-modal
  content. The authors analyzed 223 AR storytelling videos to identify a design space
  consisting of four key elements (Character, Background, Sentiment, Development)
  and five modalities (Text, Audio, Image, Video, 3D).
---

# An Exploratory Study on Multi-modal Generative AI in AR Storytelling

## Quick Facts
- arXiv ID: 2505.15973
- Source URL: https://arxiv.org/abs/2505.15973
- Reference count: 40
- Primary result: Multi-modal generative AI can support AR storytelling, but users struggle with prompting and cross-modal alignment.

## Executive Summary
This study investigates how multi-modal generative AI can be used to create content for AR storytelling, addressing the challenge of producing high-quality, multi-modal assets from text narratives. The authors analyzed 223 AR storytelling videos to identify a design space consisting of four storytelling elements (Character, Background, Sentiment, Development) and five modalities (Text, Audio, Image, Video, 3D). They developed a testbed using Gen-AI models to generate content and an AR interface for presentation. Two user studies with 30 participants revealed that images were preferred for characters, backgrounds, and sentiment, while video was favored for development. Participants found the overall content useful and immersive, though they faced challenges in guiding AI generation with text prompts. The results suggest that multi-modal AIGC is suitable for AR storytelling, but improvements are needed in aligning content across modalities and enabling intuitive prompting methods.

## Method Summary
The authors created a testbed integrating multiple generative models (Stable Diffusion for images, MusicGen for audio, MDM + Text2Video-Zero for video, text-to-3D models) into a unified system for AR storytelling. The system allows users to input text narratives, select sentences, and generate corresponding multi-modal content. An AR interface using Web Speech API enables speech-triggered presentation of pre-generated assets. The study analyzed 223 AR videos to identify a design space mapping storytelling elements to modalities, then conducted two user studies (n=30) to evaluate modality preferences, quality ratings, and interaction usability.

## Key Results
- Images were preferred for character, background, and sentiment augmentation (51%, 58%, 60% preference respectively)
- Video was favored for development augmentation (40% preference)
- Participants rated overall content usefulness at 4.13/5 and immersion at 4.00/5
- Text prompting for complex content like motion was rated lowest in ease of use (Q4)

## Why This Works (Mechanism)

### Mechanism 1: Automated Multi-Modal Content Generation Pipeline
- Claim: A coordinated GenAI pipeline lowers the barrier to creating multi-modal AR storytelling content by automating translation from textual narrative to visual, audio, 3D, and video assets.
- Mechanism: The system first parses text input to identify key story elements (character, background, etc.) using NLP. Specialized GenAI models then generate corresponding assets. This reduces the need for manual design expertise in each modality.
- Core assumption: Text prompts are sufficient starting points for generating relevant assets across modalities.
- Evidence anchors: [abstract] "develop a testbed facilitating multi-modal content generation"; [section 4.1.1] Describes the Multi-modal Content Generator using spaCy, Stable Diffusion, MusicGen, MDM, and Text2Video-Zero.

### Mechanism 2: Speech-Triggered Augmentation in AR Presentation
- Claim: Real-time speech-to-text can trigger the display of pre-generated multi-modal content, synchronizing visual/auditory augmentations with the narrator's live delivery to enhance immersion.
- Mechanism: As the narrator speaks, the system converts speech to text in real-time. This text is matched against pre-saved keywords or phrases associated with generated assets. Upon a match, the corresponding image, 3D model, text, or audio is displayed/played in the AR environment.
- Core assumption: The narrator's speech will closely follow the prepared narrative used to generate assets.
- Evidence anchors: [abstract] "...system enabled faster content creation... findings suggest AR platforms should... enable live authoring during storytelling"; [section 4.1.2] Describes the AR interface using Web Speech API.

### Mechanism 3: Design Space for Element-Modality Mapping
- Claim: A structured design space, mapping four core storytelling elements to five modalities, guides users and systems in selecting appropriate augmentations for specific narrative purposes.
- Mechanism: The analysis of 223 AR videos creates a taxonomy. Users or the system can then use this map to decide, for instance, that 'Characters' are best augmented with 'Images' (51% preference) or 'Development' with 'Video' (40% preference).
- Core assumption: The identified design space covers the majority of common AR storytelling scenarios and user preferences are generalizable.
- Evidence anchors: [abstract] "...analyze 223 AR videos to identify a design space consisting of five modalities... and four elements..."; [section 3.2 & 5.1] Details the design space and presents study results.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs) & Diffusion Models
  - Why needed here: These are the core engines for image and video generation. Understanding their text-to-image/video capabilities and limitations is crucial for working with the testbed's generation pipeline.
  - Quick check question: Can you explain the fundamental difference in how a GAN and a diffusion model generate an image from noise?

- Concept: Speech-to-Text (STT) & Real-Time Processing
  - Why needed here: The live AR presentation mode relies on STT (Web Speech API) to trigger content. Grasping its constraints (accuracy, latency, network dependence) is key to troubleshooting the presentation experience.
  - Quick check question: What are two common factors that can degrade the performance of a real-time STT system during a live presentation?

- Concept: Human-Computer Interaction (HCI) Evaluation Metrics
  - Why needed here: The study uses Likert scales to measure 'Expressiveness,' 'Immersion,' and 'Alignment'. Understanding these constructs is necessary to interpret the study's results on user preference and system suitability.
  - Quick check question: Why might a Likert scale measuring "alignment with user intent" be more subjective and challenging to interpret than one measuring "system response time"?

## Architecture Onboarding

- Component map:
    - Content Generator (Desktop/Web App) -> Backend (Python/FastAPI) -> GenAI Models (Stable Diffusion, MusicGen, MDM, Text2Video-Zero, text-to-3D) -> Frontend (text input, sentence selection, modality choice)
    - AR Presentation Interface (AR Framework/MediaPipe) -> Webcam feed -> MediaPipe for hand landmark detection -> Web Speech API for speech-to-text triggers -> Overlay logic to render 2D/3D assets

- Critical path: User inputs story text -> User selects sentence -> User chooses modality -> GenAI Model (backend) generates asset -> User saves asset with keyword -> User starts AR presentation -> User speaks -> Speech-to-Text matches keyword -> Asset is displayed/played in AR view

- Design tradeoffs:
    - Pre-generation vs. Live Generation: The system pre-generates assets, ensuring quality control but sacrificing spontaneity.
    - Modality Quality vs. Preference: Users preferred images for characters, but the study notes video quality was poor.
    - Text-Prompt Control vs. Ease of Use: Text is the primary control. The study found guiding generation with text difficult.

- Failure signatures:
    - Misaligned Output: Generated asset does not match the text prompt context
    - No Trigger during Presentation: Speech-to-text fails to recognize the keyword
    - Unimodal Inconsistency: Multiple assets generated for the same character look different

- First 3 experiments:
  1. Baseline Modality Test: Generate assets for a single sentence across all five modalities. Compare user effort and perceived alignment.
  2. Trigger Latency Stress Test: Measure delay between speaking a keyword and asset appearing during AR presentation. Vary sentence complexity to find breaking points.
  3. Cross-Modal Alignment Audit: For a single character element, generate image, 3D model, and video. Measure visual inconsistencies across assets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative systems ensure alignment across multiple modalities (e.g., image, 3D, video) and the user's specific intention for a single story element?
- Basis in paper: [explicit] The authors note that participants found multi-modal augmentations often appeared "unaligned," such as when a 3D character model looked different from the generated image of the same character.
- Why unresolved: Current models often generate modalities independently, leading to inconsistencies in identity or features across different content types.
- What evidence would resolve it: A system capable of maintaining feature consistency across generated assets, validated by user studies showing higher alignment ratings.

### Open Question 2
- Question: How can AR storytelling systems infer the optimal modality for augmentation without overwhelming the user or audience?
- Basis in paper: [explicit] The authors define "Selective Augmentation" as a need to infer what to augment, noting that participants avoided augmenting all elements and preferred specific modalities for specific contexts.
- Why unresolved: The study found no bijective mapping between elements and modalities; user preference varies based on the narrative's purpose, requiring the system to understand context.
- What evidence would resolve it: An adaptive interface that suggests or automatically selects the "best" modality for a given narrative segment, resulting in higher audience comprehension scores.

### Open Question 3
- Question: What alternative interaction methods beyond text prompting can effectively guide Generative AI in AR for complex content like motion or video?
- Basis in paper: [explicit] Participants rated "easiness of prompting" lowest (Q4), specifically finding text inputs insufficient for describing complex motions.
- Why unresolved: Text-to-video/motion models often fail to capture precise spatial or temporal nuances that users can visualize but not easily describe in text.
- What evidence would resolve it: Comparative studies demonstrating that spatial interactions (e.g., hand gestures, sketching) or demonstrations yield higher accuracy and user satisfaction for motion generation than text prompts alone.

## Limitations
- Generalizability of design space is limited to genres represented in the 223 AR videos analyzed
- Model quality and alignment issues are highly dependent on current state of GenAI models
- Text prompting bottleneck leaves a major usability barrier unaddressed
- Speech-to-text reliability and latency are not thoroughly evaluated

## Confidence

- **High Confidence**: The core observation that multi-modal GenAI can be integrated for AR storytelling is well-supported by the implemented testbed and user study results (p<0.05 for modality preferences).
- **Medium Confidence**: The conclusion that AIGC is "suitable" for AR storytelling is supported, but tempered by noted challenges (prompting, alignment, model quality).
- **Low Confidence**: The paper does not provide detailed analysis of why certain modalities were preferred for certain elements, and does not explore impact of narrative complexity or user expertise.

## Next Checks
1. **Cross-Modal Consistency Audit**: For a single character element, generate an image, a 3D model, and a video snippet. Measure and report on visual inconsistencies (e.g., color, clothing, form) across the three generated assets to quantify the alignment problem and its impact on narrative coherence.
2. **Speech-Trigger Reliability Test**: During a live AR presentation, measure the time delay between speaking a keyword and the corresponding asset appearing on screen. Vary sentence length and complexity to find breaking points and identify the maximum sustainable latency for an immersive experience.
3. **Prompt Engineering Usability Study**: Conduct a focused study to measure the time and number of iterations users require to generate a satisfactory asset for a given text prompt. Compare this to a baseline of manual asset creation to quantify the true efficiency gain (or loss) from using the GenAI pipeline.