---
ver: rpa2
title: What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?
arxiv_id: '2508.09381'
source_url: https://arxiv.org/abs/2508.09381
tags:
- skin
- lesion
- segmentation
- image
- lesions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between inter-annotator
  variability and malignancy in skin lesion segmentation. The authors curate IMA++,
  a large multi-annotator dataset of 5,111 segmentation masks from 15 annotators,
  and demonstrate that malignant lesions exhibit significantly lower inter-annotator
  agreement (IAA) compared to benign lesions, both statistically and through stochastic
  dominance testing.
---

# What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?

## Quick Facts
- arXiv ID: 2508.09381
- Source URL: https://arxiv.org/abs/2508.09381
- Authors: Kumar Abhishek; Jeremy Kawahara; Ghassan Hamarneh
- Reference count: 40
- Primary result: Multi-task learning framework that predicts both diagnosis and inter-annotator agreement achieves 4.2% improvement in balanced accuracy for skin lesion classification

## Executive Summary
This paper investigates the relationship between inter-annotator variability and malignancy in skin lesion segmentation. The authors curate IMA++, a large multi-annotator dataset of 5,111 segmentation masks from 15 annotators, and demonstrate that malignant lesions exhibit significantly lower inter-annotator agreement (IAA) compared to benign lesions. They further show that IAA scores can be accurately predicted directly from dermoscopic images, and propose a multi-task learning framework that jointly predicts diagnosis and IAA. This approach yields a 4.2% improvement in balanced accuracy across multiple model architectures and four external dermoscopic datasets compared to diagnosis-only models.

## Method Summary
The authors created IMA++, a multi-annotator dataset of 5,111 segmentation masks from 15 annotators. They measured inter-annotator agreement using intersection-over-union (IoU) scores and demonstrated that malignant lesions have significantly lower IAA than benign lesions through stochastic dominance testing. They developed IAA prediction models using 13 different architectures, achieving a mean absolute error of 0.108. The proposed multi-task learning framework jointly predicts diagnosis and IAA, treating IAA as a "soft" clinical feature. The framework was evaluated on multiple model architectures and four external dermoscopic datasets.

## Key Results
- Malignant lesions exhibit significantly lower inter-annotator agreement (IAA) compared to benign lesions
- IAA scores can be predicted from dermoscopic images alone with mean absolute error of 0.108
- Multi-task learning framework improves balanced accuracy by 4.2% compared to diagnosis-only models
- The framework generalizes across multiple model architectures and four external datasets

## Why This Works (Mechanism)
The paper establishes that lower inter-annotator agreement indicates higher malignancy because malignant lesions have more complex, irregular boundaries that are harder to delineate consistently. By predicting IAA from images alone, the model captures subtle visual features that correlate with diagnostic uncertainty. Treating IAA as a "soft" clinical feature allows the model to incorporate uncertainty information directly into the diagnostic decision-making process, leading to improved performance.

## Foundational Learning
1. **Inter-annotator agreement (IAA)**: Measures consistency between multiple annotators
   - Why needed: Quantifies segmentation uncertainty and annotator consensus
   - Quick check: Compute IoU between segmentation masks from different annotators

2. **Stochastic dominance testing**: Statistical method to compare distributions
   - Why needed: Determines if one distribution consistently exceeds another
   - Quick check: Use Kolmogorov-Smirnov test to compare IAA distributions

3. **Multi-task learning**: Training model to predict multiple related tasks simultaneously
   - Why needed: Leverages shared representations between diagnosis and uncertainty prediction
   - Quick check: Compare single-task vs multi-task model performance on validation set

4. **Intersection-over-union (IoU)**: Metric for comparing segmentation masks
   - Why needed: Standard measure of overlap between predicted and ground truth segmentations
   - Quick check: Calculate IoU = |A ∩ B| / |A ∪ B| for two binary masks

5. **Balanced accuracy**: Classification metric that accounts for class imbalance
   - Why needed: Prevents bias toward majority class in imbalanced datasets
- Quick check: Compute (Sensitivity + Specificity) / 2

6. **Soft clinical features**: Uncertainty measures incorporated as features
   - Why needed: Provides model with explicit information about annotation reliability
   - Quick check: Use IAA score as additional input feature to diagnostic model

## Architecture Onboarding
Component map: Dermoscopic image -> CNN backbone -> Segmentation branch -> IAA prediction branch -> Diagnosis branch
Critical path: Image input → Feature extraction → Multi-task heads → Predictions
Design tradeoffs: Single-task vs multi-task learning; IAA as feature vs auxiliary task
Failure signatures: Poor IAA prediction leads to minimal performance gains; class imbalance affects diagnosis accuracy
First experiments: 1) Train diagnosis-only baseline, 2) Train IAA prediction model, 3) Train multi-task model and compare performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset primarily annotated by experienced dermatologists, limiting generalizability to broader clinical practice
- Segmentation masks created for research purposes rather than representing full spectrum of clinical practice
- Performance improvements should be interpreted considering specific model architectures tested

## Confidence
- IAA-malignancy relationship: High confidence (robust statistical analysis across 5,111 annotations)
- IAA prediction from images: Medium confidence (reasonable but not negligible prediction error of 0.108)
- Multi-task learning improvement: High confidence (consistent 4.2% balanced accuracy gain across multiple architectures)

## Next Checks
1. Validate the IAA-malignancy relationship across multiple institutions with varying annotator expertise levels to assess generalizability
2. Test the IAA prediction models on independent datasets with different imaging equipment and protocols
3. Evaluate whether the multi-task learning framework maintains its performance advantage when trained on smaller datasets representative of typical clinical deployment scenarios