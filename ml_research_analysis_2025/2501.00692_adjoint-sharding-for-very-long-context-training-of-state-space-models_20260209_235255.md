---
ver: rpa2
title: Adjoint sharding for very long context training of state space models
arxiv_id: '2501.00692'
source_url: https://arxiv.org/abs/2501.00692
tags:
- adjoint
- training
- sharding
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adjoint sharding is a memory-efficient training method for very
  long context language models based on the adjoint method and vector-Jacobian products.
  It enables training models with context lengths over 100K tokens by reducing memory
  usage by up to 3x compared to backpropagation.
---

# Adjoint sharding for very long context training of state space models

## Quick Facts
- arXiv ID: 2501.00692
- Source URL: https://arxiv.org/abs/2501.00692
- Authors: Xingzi Xu; Amir Tavanaei; Kavosh Asadi; Karim Bouyarmane
- Reference count: 40
- Key outcome: Memory-efficient training method enabling models with context lengths over 100K tokens by reducing memory usage up to 3x compared to backpropagation

## Executive Summary
Adjoint sharding is a memory-efficient training method for very long context language models based on the adjoint method and vector-Jacobian products. It enables training models with context lengths over 100K tokens by reducing memory usage by up to 3x compared to backpropagation. The method shards gradient computation both temporally and across layers, enabling parallel execution. A truncated variant further improves efficiency by computing gradients only for recent tokens. Distributed and parallel implementations allow scaling to multi-GPU systems, enabling training of 1.27B parameter models on five AWS P4 instances with context lengths exceeding 100K tokens.

## Method Summary
The method replaces sequential gradient accumulation with independent vector-Jacobian products (VJPs) computed for each time step and layer. For State Space Models with dynamics h_t = A_t·h_{t-1} + B_t·x_t, y_t = C_t·h_t, adjoint states λ^k_{t,τ} = C^k_t·∏_{i=1}^{t-τ} A^k_{t+1-i} are computed to decompose the total gradient into independent components. These VJPs can be computed and discarded independently, eliminating the need to store the full computation graph. The method shards the model's K layers across Υ GPUs, with each device computing and storing its layer's necessary components locally. A truncated variant limits VJP computation to a fixed window of recent tokens, reducing computational complexity from polynomial to linear with respect to context length.

## Key Results
- Memory usage reduced by up to 3x compared to standard backpropagation
- Enables training with context lengths exceeding 100K tokens
- Successfully trains 1.27B parameter models on five AWS P4 instances
- Truncated variant maintains performance while improving computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing sequential gradient accumulation with independent VJPs enables memory reduction and parallelism.
- Mechanism: Standard backpropagation requires storing all intermediate activations for sequential gradient calculation, creating a massive memory graph. Adjoint sharding uses the adjoint method to decompose the total gradient into independent vector-Jacobian products (VJPs) for each time step and layer. These VJPs can be computed and discarded independently, eliminating the need to store the full computation graph.
- Core assumption: The model has a residual or recurrent structure (like State Space Models) where the adjoint state can be computed and used to separate gradient contributions.
- Evidence anchors:
  - [abstract]: "Adjoint sharding... is based on the adjoint method and computes equivalent gradients to backpropagation."
  - [section 4]: "...the adjoint sharding method breaks down the backpropagation computation both layer-wise and token-wise into foundational vjp computations that do not have any dependencies on each other."
  - [corpus]: Corpus evidence on this specific gradient decomposition is weak; validation relies on the paper's internal proof.
- Break condition: The mechanism's guarantee of equivalent gradients may not hold for models that lack the assumed linear temporal relations.

### Mechanism 2
- Claim: Restricting gradient computation to a recent temporal window reduces computational cost with minimal performance loss.
- Mechanism: Full adjoint sharding computes VJPs for all past tokens, leading to polynomial growth in computation with context length. Truncated adjoint sharding limits VJP computation to a fixed window of the most recent tokens. This changes the computational complexity from polynomial to linear with respect to context length.
- Core assumption: The most critical gradient information for updating the model comes from recent tokens, while older dependencies are implicitly captured in the model's recurrent state.
- Evidence anchors:
  - [abstract]: "We also propose truncated adjoint sharding to speed up the algorithm while maintaining performance."
  - [section 4.3]: "With truncated adjoint sharding, we perform \bar{T} T + \bar{T} ( \bar{T} - 1)/2 vjps, which grows linearly."
  - [corpus]: No direct corpus validation for this novel truncation technique.
- Break condition: For tasks requiring precise long-range dependencies beyond the truncation window, gradient approximation could degrade model quality.

### Mechanism 3
- Claim: Distributing VJP computation across devices allows scaling to memory-intensive long-context training.
- Mechanism: The method shards the model's layers across multiple GPUs. During the forward pass, each GPU computes and stores its layer's necessary components (activations, adjoint states). Crucially, the independent VJPs from Mechanism 1 allow the backward pass to be computed in parallel on each device without needing a synchronized global graph, distributing memory load.
- Core assumption: A multi-GPU system where inter-device communication for the forward pass does not become a bottleneck.
- Evidence anchors:
  - [abstract]: "...enabling training of 1.27B parameter models on five AWS P4 instances with context lengths exceeding 100K tokens."
  - [section 4.4]: Algorithm 4 shows parallel gradient computation, stating "the computation of gradients is parallel across the Υ devices."
  - [corpus]: Weak. Related work like "InfiniteHiP" uses a different approach (offloading) for similar scaling goals.
- Break condition: Inefficient implementation could lead to GPU underutilization or communication overhead that erodes speed gains.

## Foundational Learning

- Concept: **Adjoint Method for Recurrence**
  - Why needed here: This is the core mathematical tool used to reframe the gradient problem from sequential backpropagation to independent components.
  - Quick check question: Can you explain how the adjoint state (`λ`) is computed differently in the adjoint method compared to how errors are propagated in standard backpropagation?

- Concept: **Vector-Jacobian Product (VJP)**
  - Why needed here: Adjoint sharding's efficiency hinges on VJPs being computationally cheap and memory-light, avoiding the need to materialize the full Jacobian matrix.
  - Quick check question: Why is calculating a VJP (`v · J`) typically much faster and more memory-efficient than calculating the full Jacobian (`J`)?

- Concept: **State Space Models (SSMs)**
  - Why needed here: The paper's method is tailored for recurrent architectures like SSMs, which have a specific linear recurrence relation (`h_t = A_t h_{t-1} + B_t x_t`) that the adjoint decomposition exploits.
  - Quick check question: How does the recurrent state update in an SSM differ from the self-attention mechanism in Transformers, and why does this make SSMs more amenable to this technique?

## Architecture Onboarding

- Component map: Forward pass module (Algorithm 1) -> Adjoint state calculator (Algorithm 2) -> Parallel backward pass module (Algorithms 3 & 4)
- Critical path: The critical path for memory is the forward pass, which must store `O(T K)` states (activations for T tokens across K layers). The critical path for computation is the backward pass, which executes the numerous VJP operations. The truncation strategy directly targets this computational bottleneck.
- Design tradeoffs:
  - **Memory vs. Compute (Full vs. Truncated):** Full adjoint sharding provides exact gradients but has polynomial compute cost. Truncated adjoint sharding reduces compute to linear but provides an approximate gradient.
  - **Parallelism vs. Communication:** Distributing layers across GPUs enables massive parallelism for VJP computation but requires communicating layer outputs (`y`, `\hat{y}`) between devices during the forward pass.
- Failure signatures:
  - **Out-of-memory during forward pass:** Sharding strategy is insufficient for the context length; reduce batch size or number of layers per GPU.
  - **Slower training than backprop:** Likely due to overhead from naive multi-processing implementation or excessive inter-GPU communication; requires optimized CUDA kernels.
  - **Divergent training loss with truncation:** The truncation window (`\bar{T}`) may be too short for the task's dependencies; try increasing `\bar{T}`.
- First 3 experiments:
  1.  **Baseline Memory Test:** Train a small SSM (e.g., 32M parameters) on a single GPU with standard backpropagation and with adjoint sharding. Verify the claimed memory reduction (e.g., 2-3x) and that training loss curves are equivalent.
  2.  **Truncation Ablation:** Train the same model with different truncation windows (`\bar{T}`) on a long-context task. Plot performance vs. training speed to find the optimal trade-off point.
  3.  **Scaling Test:** On a multi-GPU setup, scale the context length (e.g., from 10K to 100K+ tokens) using the distributed algorithm. Monitor per-GPU memory usage and total training time, comparing against the theoretical limits of a single GPU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does truncated adjoint sharding offer theoretical convergence guarantees comparable to standard backpropagation?
- Basis in paper: [explicit] The authors state they "leave the analysis of its convergence and further improvements on it for future works."
- Why unresolved: While the method reduces computational complexity, the truncation introduces an approximation that lacks a formal analysis regarding its effect on optimization stability or final model quality.
- What evidence would resolve it: A theoretical proof establishing convergence bounds or empirical demonstrations showing consistent loss curves compared to non-truncated backpropagation.

### Open Question 2
- Question: How does the choice of the truncation window $\bar{T}$ affect downstream model performance?
- Basis in paper: [explicit] The authors note, "We leave investigation of $\bar{T}$'s impact on performances for future works."
- Why unresolved: The paper proposes truncating gradients to a fixed window of recent states to save compute, but it does not quantify the trade-off between different window sizes and the model's ability to learn long-range dependencies.
- What evidence would resolve it: Ablation studies on tasks requiring long-context reasoning (e.g., the "Needle in a Haystack" test) correlating varying $\bar{T}$ values with accuracy metrics.

### Open Question 3
- Question: Can a custom CUDA kernel implementation eliminate the overhead of parallel adjoint sharding for small context lengths?
- Basis in paper: [explicit] The authors acknowledge that "the overhead of naïve implementation... outweighs the speedups when the training context length is small" and leave "efficient implementation... on a CUDA kernel for future work."
- Why unresolved: The current implementation relies on generic multi-threading/multiprocessing which incurs latency that negates the algorithmic speedups for shorter sequences.
- What evidence would resolve it: Benchmarks of a custom kernel showing positive speedup margins relative to backpropagation even at context lengths below the current "break-even" point.

## Limitations

- The method is specifically designed for State Space Models and cannot be directly applied to Transformers or other architectures without significant modification.
- The computational overhead of adjoint sharding relative to standard backpropagation is not quantified, creating uncertainty about net efficiency gains.
- The truncation variant introduces an approximation without theoretical bounds on approximation error or guidance for selecting optimal truncation windows.

## Confidence

- **High confidence**: The mathematical equivalence between adjoint sharding and backpropagation gradients for full computation (Mechanism 1). The paper provides clear algorithmic descriptions and the underlying adjoint method is well-established in numerical analysis literature.
- **Medium confidence**: The 3x memory reduction claim and the effectiveness of truncated adjoint sharding (Mechanism 2). While the paper demonstrates these on real models, the lack of systematic ablation studies and comparison against alternative approaches introduces uncertainty about the absolute magnitude of improvements.
- **Low confidence**: The scalability claims for distributed training (Mechanism 3). The paper reports successful training on five GPUs but provides minimal performance data on how the method scales with more devices or different GPU architectures, and doesn't address potential communication bottlenecks.

## Next Checks

1. **Memory vs. Computation Tradeoff Analysis**: Systematically compare adjoint sharding against standard backpropagation and InfiniteHiP-style offloading across a range of context lengths (10K, 50K, 100K, 500K tokens). Measure absolute memory usage, training time per step, and total time to convergence for each method. This would quantify whether the memory savings justify any computational overhead and reveal the break-even point where adjoint sharding becomes advantageous.

2. **Truncation Window Sensitivity Study**: Train identical models with varying truncation windows (T̄ = 100, 500, 1000, 2000, 5000) on tasks requiring different dependency lengths (e.g., sequential decision making vs. document summarization). Plot task performance against memory/compute savings to establish data-driven guidelines for selecting T̄ based on task characteristics and context length.

3. **Communication Overhead Characterization**: Implement the distributed algorithm on a multi-GPU system and profile per-GPU memory usage, compute utilization, and inter-GPU communication volume at different sharding configurations (2, 4, 8 GPUs). Measure the actual speedup achieved versus theoretical speedup to identify at what scale the communication overhead negates the benefits of parallelization.