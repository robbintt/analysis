---
ver: rpa2
title: Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?
arxiv_id: '2504.10397'
source_url: https://arxiv.org/abs/2504.10397
tags:
- causal
- expert
- sleep
- relationships
- elicitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of Large Language Models (LLMs)
  as a novel approach for expert elicitation in constructing Bayesian networks (BNs)
  for causal modeling in healthcare. The LLM-generated BNs were compared against those
  created using traditional methods like Bayesian Information Criterion (BIC) and
  human expert knowledge, using entropy and Structural Equation Modeling (SEM) as
  key metrics.
---

# Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?

## Quick Facts
- arXiv ID: 2504.10397
- Source URL: https://arxiv.org/abs/2504.10397
- Reference count: 31
- Primary result: LLM-generated Bayesian networks achieved lower entropy than both BIC and expert knowledge methods, suggesting higher confidence and precision in predictions.

## Executive Summary
This study explores using Large Language Models (LLMs) as expert elicitation tools for constructing Bayesian networks in healthcare causal modeling. The LLM-generated networks were compared against traditional methods (BIC and human expert knowledge) using entropy and Structural Equation Modeling (SEM) as key metrics. Results showed LLM BNs had significantly lower entropy (1.4237 vs 1.4770 for BIC), indicating more confident predictions, and demonstrated fewer logical inconsistencies than BIC-based models.

## Method Summary
The study employed a dual-LLM approach where one LLM proposed causal relationships while a second LLM verified their plausibility and identified confounders. The generated structures were validated using SEM to test statistical significance of proposed relationships. The LLM-based BNs were then compared against Bayesian networks created using Bayesian Information Criterion (BIC) and human expert knowledge, using entropy and SEM validation as evaluation metrics.

## Key Results
- LLM-generated BNs achieved mean entropy of 1.4237, lower than BIC (1.4770) and expert knowledge (1.4773)
- 10 out of 12 relationships proposed by the first LLM were confirmed by the second LLM
- LLM-generated BNs showed fewer logical inconsistencies and incorrect causal relationships compared to BIC-based models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-LLM expert elicitation reduces hallucinated causal dependencies through cross-verification
- Mechanism: One LLM proposes causal relationships while a second LLM critically evaluates plausibility, identifies confounders, and flags inconsistencies
- Core assumption: LLMs encode sufficient domain knowledge to propose and critique causal structures, and their reasoning errors are not correlated
- Evidence anchors: 10/12 relationships confirmed by second LLM; lower entropy in LLM BNs
- Break condition: If both LLMs share correlated training biases or hallucination patterns

### Mechanism 2
- Claim: Lower entropy in LLM-generated BNs indicates higher structural confidence, not necessarily causal correctness
- Mechanism: Lower entropy measures more deterministic predictions given evidence, suggesting the LLM assigns higher-probability edges based on encoded domain priors
- Core assumption: Lower entropy correlates with better decision-support utility and the LLM's domain priors approximate true causal structure better than purely statistical discovery
- Evidence anchors: Mean entropy of 1.4237 for LLM vs 1.4770 for BIC; LLM identified as "strong performer"
- Break condition: A low-entropy model can be confidently wrong if causal direction is systematically reversed

### Mechanism 3
- Claim: SEM validation provides independent statistical verification of LLM-proposed causal edges
- Mechanism: After LLMs propose a DAG structure, SEM estimates regression coefficients and p-values for each parent→child relationship
- Core assumption: Statistical significance in SEM on available dataset is a reasonable proxy for causal relationship validity
- Evidence anchors: 11 of 12 LLM-proposed relationships statistically significant at p<0.05
- Break condition: Small datasets may lack power to detect real relationships or show spurious correlations as significant

## Foundational Learning

- **Bayesian Networks (BNs) as Directed Acyclic Graphs (DAGs)**
  - Why needed here: The entire methodology assumes understanding that nodes=variables, edges=conditional dependencies, and CPTs quantify relationships
  - Quick check question: Can you explain why a BN cannot contain cycles and how conditional probability tables enable inference?

- **Expert Elicitation Protocols**
  - Why needed here: The paper frames LLMs as proxy experts; understanding traditional elicitation clarifies what the dual-LLM approach replaces or augments
  - Quick check question: What are two failure modes in human expert elicitation that LLMs might mitigate or replicate?

- **Entropy as Uncertainty Quantification**
  - Why needed here: The primary evaluation metric; H(X)=-ΣP(xi)logP(xi) measures how "spread out" a distribution is
  - Quick check question: If a BN node has entropy of 0.5 vs 2.5, which represents more uncertainty and why?

## Architecture Onboarding

- **Component map**: Preprocessed dataset -> Three parallel paths (BIC/MIIC statistical learning, Human expert DAG, LLM dual-elicitation) -> SEM validation -> DAG + CPTs -> PyAgrum implementation -> Inference engine -> Scenario analysis outputs

- **Critical path**: Data preprocessing -> LLM prompt engineering -> Dual-LLM verification loop -> SEM validation -> CPT population and BN assembly

- **Design tradeoffs**:
  - LLM vs. BIC: LLM provides domain-interpretable edges but risks hallucination; BIC is data-grounded but produces logically inverted causality
  - Single vs. Dual LLM: Dual verification adds latency and cost but reduces hallucination rate
  - Discretization granularity: Fewer categories reduce entropy but lose information; more categories increase CPT complexity

- **Failure signatures**:
  - Hallucinated dependencies: LLM proposes edges with no data support (watch for SEM p≥0.05)
  - Backward causality: Variable relationships reversed vs. domain logic
  - Cyclic proposals: LLM suggests A→B and B→A; requires manual resolution for DAG compliance
  - Missing confounders: Dataset lacks variables LLM identifies as important

- **First 3 experiments**:
  1. Run BIC and MIIC on Sleep Health dataset to reproduce entropy values from Table 5
  2. Compare single-LLM vs. dual-LLM structures to quantify exact contribution of cross-verification
  3. Apply dual-LLM elicitation prompts to different healthcare dataset (e.g., cardiac or diabetes) to measure if entropy advantage holds

## Open Questions the Paper Calls Out

- Can LLMs effectively reduce cognitive biases in causal modeling compared to human experts?
- Does the application of LLMs lead to better explainability of causal modeling results?
- How can hallucinated dependencies and inherited training biases be effectively mitigated in LLM-generated causal structures?

## Limitations

- Results validated only on sleep health dataset; unknown generalizability to other clinical domains
- Lower entropy may reflect confident predictions rather than correct causal structure; no external ground truth for comparison
- Sample size (n=400) may not support reliable SEM validation of all proposed edges

## Confidence

- High confidence: LLM BNs have lower entropy than BIC/Expert models
- Medium confidence: LLM-generated edges are more logically consistent than BIC
- Medium confidence: Dual-LLM verification improves structural quality

## Next Checks

1. Apply proposed BNs to future sleep health data; measure whether lower-entropy models produce more accurate predictions over time
2. Have human experts evaluate LLM-proposed edges for clinical plausibility; measure agreement rates
3. Compare single-LLM vs. dual-LLM structures to quantify exact contribution of cross-verification to quality improvements