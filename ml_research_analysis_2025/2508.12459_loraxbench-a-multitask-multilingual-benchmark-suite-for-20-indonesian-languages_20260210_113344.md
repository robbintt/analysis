---
ver: rpa2
title: 'LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages'
arxiv_id: '2508.12459'
source_url: https://arxiv.org/abs/2508.12459
tags:
- languages
- language
- indonesian
- performance
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces LoraxBench, a novel benchmark suite for 20
  Indonesian languages across six diverse tasks: reading comprehension, open-domain
  QA, language inference, causal reasoning, translation, and cultural QA. The dataset
  includes two formality registers for Javanese, Sundanese, and Madurese, enabling
  evaluation of register-specific understanding.'
---

# LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages

## Quick Facts
- arXiv ID: 2508.12459
- Source URL: https://arxiv.org/abs/2508.12459
- Reference count: 40
- Introduces a benchmark suite for 20 Indonesian languages across 6 diverse tasks with cultural relevance and formality registers

## Executive Summary
This paper introduces LoraxBench, a novel benchmark suite designed to evaluate large language models across 20 Indonesian languages and six diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. The dataset includes two formality registers for Javanese, Sundanese, and Madurese, enabling evaluation of register-specific understanding. Professional translation and validation were used to ensure cultural relevance and high quality. The benchmark was evaluated using multilingual, Southeast Asian, and Indonesian-specific large language models, revealing significant performance gaps, especially for low-resource languages and formal registers.

## Method Summary
LoraxBench consists of six tasks covering 20 Indonesian languages, with professional translations from source datasets including TyDi QA, IndoNLI, COPAL-ID, and IndoCulture. The benchmark includes two formality registers (Ngoko and Krama) for Javanese, Sundanese, and Madurese. Evaluation uses zero-shot inference with various prompting strategies: Basic, Language-Informed (noting similarity to Indonesian), and Lexicon-Guided (using Gatitos lexicon). For multiple-choice tasks, answers are selected via highest log-probability of choice tokens; for generative tasks, outputs are extracted directly. Metrics include accuracy for NLI, QA, and reasoning; ChrF++ for translation; and exact match for reading comprehension.

## Key Results
- No single model family consistently outperforms others across all tasks and languages
- Significant performance gaps exist for low-resource languages and formal registers
- Language-informed and lexicon-guided prompts often degraded performance due to "false friends" and ambiguity
- Formal registers (Krama) show substantially lower performance than casual registers (Ngoko)

## Why This Works (Mechanism)
The benchmark leverages professional translation and cultural validation to create high-quality, culturally relevant datasets across multiple Indonesian languages and registers. The zero-shot evaluation framework with multiple prompting strategies reveals model limitations and generalization capabilities without requiring task-specific fine-tuning.

## Foundational Learning
- **Zero-shot evaluation**: Testing models without task-specific fine-tuning to assess generalization
  - Why needed: Reveals true model capabilities and limitations across diverse languages
  - Quick check: Verify models can complete tasks using only prompt instructions
- **Prompt engineering variants**: Basic, Language-Informed, and Lexicon-Guided strategies
  - Why needed: Different approaches to provide linguistic context without task-specific training
  - Quick check: Compare performance across prompting strategies for each task
- **Register-specific evaluation**: Testing both formal (Krama) and casual (Ngoko) language variants
  - Why needed: Indonesian languages have distinct formal registers rarely found in web data
  - Quick check: Measure performance gap between Ngoko and Krama variants

## Architecture Onboarding
- **Component map**: Dataset Loading -> Prompt Construction -> Model Inference -> Evaluation
- **Critical path**: Translation Quality → Prompt Formulation → Model Response Generation → Metric Calculation
- **Design tradeoffs**: Professional translation ensures quality but limits scale; zero-shot evaluation avoids fine-tuning but may underestimate capabilities
- **Failure signatures**: Performance drops on formal registers, degradation with language-informed prompts, inconsistent results across prompting strategies
- **First experiments**:
  1. Load dataset and verify language/register distribution
  2. Test basic prompt strategy on a single task/language pair
  3. Compare model performance across different prompting strategies

## Open Questions the Paper Calls Out
### Open Question 1
How can prompting strategies be redesigned to effectively leverage linguistic similarities and lexicons for low-resource languages without introducing noise that degrades model performance? The authors observed that providing explicit language information or lexicon guidance in prompts consistently degraded performance across languages and models, likely due to "false friends" or ambiguous word senses confusing the model. The optimal method to inject linguistic knowledge without distraction remains unknown.

### Open Question 2
What specific continual fine-tuning methodologies allow region-specific models to generalize across diverse tasks better than general multilingual models? The authors note "no clear lead when using a region-specific model as opposed to the general multilingual model," finding that SeaLion-v3 often underperformed or showed no improvement over the base Gemma model, unlike Sahabat-AI. Simply training on regional data does not guarantee superior performance.

### Open Question 3
How can NLP systems be improved to handle the formal registers (e.g., Javanese Krama) of Indonesian languages that are scarce in web-crawled training data? The authors highlight a performance drop in formal registers, specifically noting that "registers not commonly found in social media, such as high-level politeness 'Krama' Javanese," are particularly challenging for current models. Current LLMs rely heavily on web data which over-represents casual registers.

## Limitations
- Evaluation framework lacks inter-annotator agreement metrics and detailed validation protocols
- Benchmark does not account for script variations or code-switching patterns common in Indonesian languages
- Maximum accuracy across prompting strategies may overestimate true performance due to optimal prompt variability

## Confidence
- **High confidence**: Benchmark dataset construction methodology and finding that no single model family consistently outperforms others
- **Medium confidence**: Performance gap magnitudes and relative rankings between model families due to unknown generation hyperparameters
- **Low confidence**: Specific claims about "false friends" impact and exact reasons for language-informed prompt degradation

## Next Checks
1. Conduct ablation studies by systematically testing different decoding hyperparameters (temperature 0.0, 0.7, 1.0; top_p 0.9, 0.95; max_tokens 50, 100) on a subset of tasks to establish sensitivity to generation parameters and report confidence intervals.

2. Perform LLM-as-a-Judge evaluation on the Open QA task using Gemini-2.5-Flash or similar model to compare against exact match results and assess the strictness of current evaluation metrics.

3. Implement inter-annotator agreement analysis using available validation data to quantify annotation reliability and identify potential systematic biases in translation or validation processes.