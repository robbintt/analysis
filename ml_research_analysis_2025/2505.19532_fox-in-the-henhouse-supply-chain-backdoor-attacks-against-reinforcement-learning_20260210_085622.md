---
ver: rpa2
title: 'Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning'
arxiv_id: '2505.19532'
source_url: https://arxiv.org/abs/2505.19532
tags:
- backdoor
- trigger
- victim
- action
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new type of supply-chain backdoor attack
  on reinforcement learning, where an attacker trains a malicious external agent that
  later influences the victim agent's behavior during training, without requiring
  access to the victim's observations, rewards, or policy parameters. The attack works
  by rewarding the victim for executing specific backdoor actions after recognizing
  a trigger pattern in the attacker's behavior, leading the victim to learn a compromised
  policy.
---

# Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.19532
- Source URL: https://arxiv.org/abs/2505.19532
- Authors: Shijie Liu; Andrew C. Cullen; Paul Montague; Sarah Erfani; Benjamin I. P. Rubinstein
- Reference count: 40
- Key outcome: A supply-chain backdoor attack that requires no access to victim's observations, rewards, or policy parameters achieves over 90% trigger success rate and up to 80% performance reduction in Atari games.

## Executive Summary
This paper introduces a new supply-chain backdoor attack against reinforcement learning where an attacker trains a malicious external agent that influences the victim's behavior during training. The attack works by rewarding the victim for executing specific backdoor actions after recognizing trigger patterns in the attacker's behavior. Experimental results show the attack achieves high success rates and significant performance degradation across different RL algorithms and architectures, requiring only 3% trigger injection during training. The findings highlight a significant supply-chain security risk in RL, especially for safety-critical applications.

## Method Summary
The attack involves training a malicious external agent (attacker) that, when deployed as an opponent during victim training, can covertly influence the victim's learned policy. The attacker maintains two policies: a standard competitive policy and a self-sabotage policy. When trigger patterns are detected in the victim's behavior, the attacker switches to the self-sabotage policy, causing the victim to receive higher-than-expected returns for backdoor actions. A detector network infers victim actions from attacker observations, enabling feedback-controlled attack execution. The attack requires no access to victim's observations, rewards, or policy parameters, distinguishing it from prior backdoor attacks.

## Key Results
- Attack achieves over 90% trigger success rate across Atari games (Pong, Boxing, Surround) with just 3% trigger injection during training
- Reduces victim's performance by up to 80% in competitive games when trigger is activated
- Works across different RL algorithms (DQN, PPO, A2C) and architectures (CNN, LSTM), with LSTM models showing higher susceptibility due to better temporal modeling
- Demonstrates stealth by maintaining normal training metrics while embedding backdoor functionality

## Why This Works (Mechanism)

### Mechanism 1: Implicit Reward Shaping via Attacker Self-Sabotage
The attacker shapes the victim's learned policy without accessing its reward function by strategically losing quickly after the victim executes desired backdoor actions. The attacker maintains a "rewarding policy" trained via max-min optimization to maximize the opponent's cumulative reward from arbitrary states. When the victim correctly executes backdoor actions after observing trigger actions, the attacker switches to this rewarding policy, causing the victim to receive higher-than-expected returns for those actions. Over repeated episodes, the victim's Q-function elevates backdoor actions following triggers.

### Mechanism 2: State-Conditioned Policy Partitioning via Temporal Pattern Recognition
RL agents with temporal modeling capacity learn to partition their policy based on observed trigger sequences in opponent behavior. LSTM-equipped agents maintain hidden state across timesteps, enabling them to recognize temporal patterns (e.g., 4 consecutive no-ops). The trigger pattern becomes a feature in the agent's internal representation that predicts future reward opportunity. The victim learns normally, but after detecting trigger sequence, shifts toward backdoor action distribution.

### Mechanism 3: Detector-Based Closed-Loop Verification Without Direct Observation Access
The attacker verifies victim compliance with backdoor actions using only its own observations, enabling feedback-controlled attack execution. A detector network infers victim actions from stacked attacker observations by learning the mapping from state transitions. This allows the attacker's finite-state machine to confirm backdoor execution before switching to rewarding state, preventing wasted rewarding episodes.

## Foundational Learning

- **Q-Learning and Action-Value Estimation**: Understanding Bellman updates explains why delayed implicit rewards still shape policy. Quick check: If an agent receives unexpectedly high reward after action a in state s, what happens to Q(s,a) under TD learning?

- **Multi-Agent Markov Decision Processes (MMDPs)**: The threat model formalizes attack as two-agent game with coupled transition dynamics and separate reward functions. Quick check: In a competitive two-agent MMDP, can one agent's optimal policy change if the other agent's policy changes?

- **Backdoor Attacks in ML (Trigger-Backdoor Pairs)**: Distinguishes this work from prior backdoor approaches by understanding what "no direct access" means operationally. Quick check: In BadNets, what access does the attacker need during training that SCAB does not require?

## Architecture Onboarding

- Component map: Attacker Agent (pre-trained, static) -> π_att^std (Normal competitive policy) -> π_att^rwd (Self-sabotage policy) -> detector d (CNN inferring victim action) -> FSM Controller (Manages state transitions)

- Critical path:
  1. Deploy pre-trained attacker as external agent/opponent
  2. During victim training: trigger injected with probability TIP → attacker observes victim response via detector → if backdoor detected, switch to π_att^rwd for BRT-scaled timesteps
  3. At test time: any opponent knowing trigger pattern can activate backdoor

- Design tradeoffs:
  - Higher TIP → faster backdoor learning but increased detection risk during training
  - Longer trigger sequences → more recognizable but costs attacker timesteps during test
  - Longer backdoor sequences → more damaging but harder for victim to learn (lower success rate)
  - BRT magnitude → higher values strengthen association but may cause training anomalies

- Failure signatures:
  - Training metrics diverge significantly from clean baseline (stealthiness broken)
  - Trigger success rate <50% despite high TIP (detector failure or trigger too subtle)
  - Victim performs poorly even without trigger (backdoor leaking into normal policy)
  - LSTM significantly underperforms CNN (temporal patterns not being learned)

- First 3 experiments:
  1. Replicate Pong CNN-PPO baseline with TIP=3%, measure trigger success rate and episodic return at varying trigger proportions (0-30%) against clean training control
  2. Ablate detector accuracy: inject controlled noise into detector predictions and measure attack effectiveness degradation
  3. Test cross-algorithm generalization: train victim with DQN against same attacker, compare trigger success rate to PPO victim to validate architecture-agnostic claim

## Open Questions the Paper Calls Out

- Can the attacker's internal logic for state tracking and behavior switching be concealed within the neural network architecture? The authors state future work should examine how features like reward tracking and state switching "could either be removed or concealed at the architecture level."

- What strategy for timing trigger injections during training ensures the backdoor persists during testing? The authors note their RL-based injection strategy failed during testing, identifying "constructing an effective... strategy for injecting triggers" as an important future direction.

- Can existing defenses like anomaly detection or policy smoothing be extended to guarantee performance against SCAB? The authors explicitly call for future work to explore extending "anomaly detection and policy smoothing" to this threat model.

## Limitations

- Architecture dependency: The attack's effectiveness varies significantly between CNN and LSTM architectures, with LSTM showing much higher susceptibility due to better temporal modeling capabilities.

- Environment generality: All experiments are conducted in Atari environments with relatively deterministic dynamics, limiting generalizability to stochastic environments or those with partial observability.

- Detection feasibility: The attack assumes monitoring systems won't detect occasional reward spikes, but this assumption lacks empirical validation against realistic detection mechanisms.

## Confidence

- **High Confidence (9/10)**: The core mechanism of implicit reward shaping through self-sabotage is theoretically sound and well-supported by the max-min optimization framework.
- **Medium Confidence (7/10)**: Cross-algorithm and cross-architecture generalization claims are supported by experimental results but rely on relatively simple baseline algorithms.
- **Low Confidence (4/10)**: The stealth claim depends on the assumption that occasional reward spikes won't be detected by monitoring systems, which is plausible but not empirically validated.

## Next Checks

1. **Detector Robustness Test**: Systematically evaluate detector accuracy under varying levels of observation noise and environmental stochasticity. Measure attack success rate degradation as a function of detector accuracy to establish the detector's criticality.

2. **Architecture Transferability**: Test the attack against modern RL architectures including transformers and attention-based models. Compare trigger success rates and learning curves to establish whether LSTM's advantage is fundamental or circumstantial.

3. **Anomaly Detection Resistance**: Implement simple reward distribution monitoring and spike detection algorithms. Measure false positive rates on clean training and false negative rates when the attack is active to quantify stealth vulnerability.