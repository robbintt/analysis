---
ver: rpa2
title: 'Biases in Edge Language Models: Detection, Analysis, and Mitigation'
arxiv_id: '2502.11349'
source_url: https://arxiv.org/abs/2502.11349
tags:
- edge
- language
- llms
- bias
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates biases in edge language models (ELMs) deployed
  on low-power devices compared to cloud and desktop models. It evaluates Llama-2
  on a Raspberry Pi 4 and finds it exhibits 43.23% and 21.89% more bias than desktop
  and cloud models, respectively.
---

# Biases in Edge Language Models: Detection, Analysis, and Mitigation

## Quick Facts
- arXiv ID: 2502.11349
- Source URL: https://arxiv.org/abs/2502.11349
- Authors: Vinamra Sharma; Danilo Pietro Pau; José Cano
- Reference count: 40
- Primary result: Edge LLM quantization amplifies bias; context-aware feedback loops reduce it by 79.28%

## Executive Summary
This paper investigates biases in edge language models deployed on resource-constrained devices compared to cloud and desktop models. The authors evaluate Llama-2 on a Raspberry Pi 4 and find it exhibits 43.23% more bias than desktop and 21.89% more than cloud models, attributing this to INT8 quantization precision loss. To address this, they propose a context-aware feedback loop that iteratively adjusts layer-wise weights during inference, achieving a 79.28% reduction in bias. The study highlights the critical need for fairness mechanisms in edge deployments and offers a novel mitigation strategy for resource-constrained environments.

## Method Summary
The authors implement Llama-2 7B with INT8 quantization on a Raspberry Pi 4, using a C-based re-implementation from Karpathy's llama2.c. They evaluate bias across 10 prompts with 1,500 iterations each, classifying outputs as Black (0), White (1), or Refuse (2). A feedback loop applies predefined constraint weights during layer-wise weight loading using a sliding window approach. The method compares edge performance against cloud APIs (GPT-4o-mini, Gemini-1.5-flash, Grok-beta) and desktop Ollama deployments (Gemma2, Mistral), measuring bias percentage and reduction effectiveness.

## Key Results
- Llama-2 on Raspberry Pi 4 shows 43.23% more bias than desktop models and 21.89% more than cloud models
- Feedback loop reduces edge model bias from 97.41% to 20.18% (79.28% reduction)
- Iterative prompting without intervention causes models to converge to complete bias after ~11,893 iterations in cloud models
- INT8 quantization reduces memory from 26.96 GB to 7.56 GB with max reconstruction error of 0.006

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge deployment via INT8 quantization amplifies model bias compared to full-precision desktop/cloud deployments.
- Mechanism: Quantization compresses FP32 weights to INT8, reducing memory from 26.96 GB to 7.56 GB. While max reconstruction error is low (0.006), the loss of precision in weight representations may disproportionately affect fairness-critical activation patterns, causing the edge model to exhibit 43.23% more bias than desktop and 21.89% more than cloud models.
- Core assumption: The observed bias amplification stems from quantization rather than inherent model architecture differences; the causal link between precision loss and fairness degradation is inferred from comparative measurement, not isolated variable testing.
- Evidence anchors:
  - [abstract] "Llama-2 running on Raspberry Pi 4 is 43.23% and 21.89% more prone to showing bias over time compared to models running on the desktop and cloud-based environments."
  - [Section 3.1.2] "Through this, the max re-construction error is found to be 0.006 across all the pre-trained Llama2 weights with a batch size of 64."
  - [corpus] Related work addresses bias mitigation but does not isolate quantization effects on fairness; no direct corroborating studies found.
- Break condition: If FP16 or INT4 quantization shows similar bias levels, the mechanism may not be precision-specific but rather deployment-context-dependent.

### Mechanism 2
- Claim: Layer-wise context-aware feedback loops mitigate bias during inference without retraining.
- Mechanism: Predefined constraint weights are applied iteratively across transformer layers using a sliding window (batch size 32). The parameter `n_base` establishes baseline weight distribution for bias correction. Weights are dynamically re-weighted based on real-time bias observations, reducing edge model bias from 97.41% to 20.18% (79.28% reduction).
- Core assumption: Bias patterns are layer-correlatable and can be counteracted through static predefined weights; the effectiveness depends on prompt-specific weight calibration (Section 4).
- Evidence anchors:
  - [abstract] "We also propose the implementation of a feedback loop... resulting in 79.28% reduction in model bias."
  - [Section 3.1.3] "The weights were predefined and specific to the prompts further evaluated in this work."
  - [corpus] Related papers discuss counterfactual reward models and extrinsic fairness evaluation but not layer-wise weight adjustment during inference.
- Break condition: If bias reduction fails on prompts outside the predefined weight set, generalization is limited; adaptive weight learning may be required.

### Mechanism 3
- Claim: Iterative prompting reinforces bias, eventually driving models to a fully biased state.
- Mechanism: Repetitive interactions with ambiguous prompts (no contextual basis for decision-making) cause models to converge on biased outputs. Extended cloud testing (15,000 calls) showed models adapting to bias after ~11,893 iterations, suggesting feedback-driven reinforcement without explicit retraining.
- Core assumption: Bias reinforcement occurs through exposure to repeated prompt patterns, not through model weight updates; the mechanism is observational and not causally isolated from potential session-level context accumulation.
- Evidence anchors:
  - [Section 5] "In the case of Gemini and GPT-4o-mini, the model completely adapted to the bias after 11,893 iterations on average."
  - [Section 4] "The prompts do not provide any context about the situation or person, theoretically it should refuse to answer, but when asked iteratively the findings were completely different."
  - [corpus] Neighbors discuss cognitive bias in LLM decision-making but do not examine iterative reinforcement dynamics.
- Break condition: If session resets or temperature variation prevents bias convergence, the mechanism may be context-window-dependent rather than cumulative.

## Foundational Learning

- Concept: **Quantization Tradeoffs**
  - Why needed here: Edge deployment requires INT8 compression to fit models in constrained memory (8.6 GB vs 41.6 GB heap), but precision loss affects more than accuracy metrics—it impacts fairness.
  - Quick check question: What is the reconstruction error threshold at which quantization-induced bias becomes measurable?

- Concept: **Layer-wise Weight Interventions**
  - Why needed here: The proposed mitigation applies constraint weights per-layer rather than globally, requiring understanding of transformer layer functions (attention weights `w_w, w_x, w_y, w_z` vs feedforward `w_a, w_b, w_c`).
  - Quick check question: Which layer type (attention or feedforward) accounts for the majority of parameter memory, and how does this influence intervention placement?

- Concept: **Iterative Bias Reinforcement**
  - Why needed here: Repeated prompting without intervention causes models to converge toward biased outputs, critical for understanding deployment-time risks beyond training data.
  - Quick check question: At what iteration count does the paper observe complete bias adaptation in cloud models?

## Architecture Onboarding

- Component map: Prompt → INT8 quantized forward pass → Layer-wise weight application → Token generation → Bias scoring → (if feedback enabled) weight adjustment for next inference

- Critical path: Edge Inference Engine (Llama2-7B with INT8 quantization on Raspberry Pi 4) → Layer-wise Weight Loader (segmented forward pass with sliding window) → Feedback Controller (applies predefined constraint weights) → Bias Detector (binary classification with percentage calculation) → Comparison Baselines (cloud APIs and desktop Ollama)

- Design tradeoffs:
  - Memory vs Latency: Feedback loop reduces bias 79.28% but increases inference from 6.20 to 57.86 sec/token (9× slower) due to disk reads
  - Prompt-specificity: Predefined weights limit generalization; requires calibration per use case
  - Single-core focus: Parallelization avoided for broader device compatibility at cost of throughput

- Failure signatures:
  - Bias remains >90%: Feedback loop not engaged or weights not calibrated for prompt type
  - Memory overflow: Layer-wise loading fails without heap headroom (~8.6 GB baseline + feedback overhead)
  - Convergence to single output: Iterative prompting without intervention triggers complete bias adaptation

- First 3 experiments:
  1. Replicate INT8 vs FP32 bias comparison using identical prompts to isolate quantization effect from deployment environment.
  2. Test feedback loop on out-of-distribution prompts to measure weight generalization and identify calibration requirements.
  3. Profile memory and latency overhead of layer-wise weight loading across varying window sizes (16, 32, 64) to optimize the tradeoff curve.

## Open Questions the Paper Calls Out
None

## Limitations
- The causal relationship between quantization precision loss and fairness degradation remains inferred rather than experimentally isolated through controlled ablation studies
- The feedback loop's effectiveness depends on prompt-specific predefined weights whose calibration methodology is not fully specified, limiting generalizability
- Iterative bias reinforcement findings lack causal isolation from potential confounding factors like session-level context accumulation

## Confidence

- **High confidence**: Quantization tradeoff measurements (memory reduction, reconstruction error of 0.006) and basic bias percentage calculations are methodologically sound and reproducible
- **Medium confidence**: Comparative bias measurements across deployment environments (43.23% and 21.89% differences) are supported by data but may conflate quantization effects with other deployment variables
- **Low confidence**: The feedback loop's bias reduction mechanism (79.28%) requires prompt-specific weight calibration that isn't fully specified, and the iterative bias reinforcement findings lack causal isolation from potential confounding factors

## Next Checks
1. Conduct controlled ablation experiments comparing Llama-2 bias across INT8, FP16, and INT4 quantization levels using identical deployment conditions to isolate quantization's specific contribution to fairness degradation.

2. Test the feedback loop mitigation strategy on out-of-distribution prompts and measure weight generalization, identifying the minimum calibration requirements needed for different prompt types.

3. Profile memory and latency overhead of layer-wise weight loading across varying sliding window sizes (16, 32, 64) to establish the optimal tradeoff curve between bias reduction effectiveness and computational cost.