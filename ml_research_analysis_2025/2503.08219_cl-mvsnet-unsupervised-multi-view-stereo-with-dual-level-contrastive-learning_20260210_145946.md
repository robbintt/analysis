---
ver: rpa2
title: 'CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning'
arxiv_id: '2503.08219'
source_url: https://arxiv.org/abs/2503.08219
tags:
- contrastive
- depth
- consistency
- learning
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CL-MVSNet introduces a dual-level contrastive learning framework
  to address limitations in unsupervised multi-view stereo, particularly in indistinguishable
  regions (low-textured areas, repetitive patterns) and view-dependent effects (reflections,
  occlusions). The method incorporates two contrastive branches: an image-level branch
  that encourages context awareness through masking transformations, and a scene-level
  branch that improves robustness to view-dependent effects using randomly selected
  source images.'
---

# CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning

## Quick Facts
- **arXiv ID:** 2503.08219
- **Source URL:** https://arxiv.org/abs/2503.08219
- **Reference count:** 40
- **Primary result:** State-of-the-art end-to-end unsupervised MVS, surpassing supervised CasMVSNet without fine-tuning

## Executive Summary
CL-MVSNet introduces a dual-level contrastive learning framework to address limitations in unsupervised multi-view stereo, particularly in indistinguishable regions (low-textured areas, repetitive patterns) and view-dependent effects (reflections, occlusions). The method incorporates two contrastive branches: an image-level branch that encourages context awareness through masking transformations, and a scene-level branch that improves robustness to view-dependent effects using randomly selected source images. Additionally, an L0.5 photometric consistency loss is proposed to focus more on accurate depth predictions while reducing the influence of erroneous points. Experiments on DTU and Tanks&Temples benchmarks demonstrate that CL-MVSNet achieves state-of-the-art performance among end-to-end unsupervised MVS methods and surpasses its supervised counterpart CasMVSNet without fine-tuning.

## Method Summary
CL-MVSNet builds upon the CasMVSNet architecture by adding two contrastive learning branches to address failure modes in unsupervised MVS. The method uses a regular branch with standard view selection to generate depth estimates and confidence masks, an image-level contrastive branch that applies Bernoulli masking to source images to encourage context-aware depth estimation, and a scene-level contrastive branch that uses randomly selected source views to improve robustness to view-dependent effects. The L0.5 photometric consistency loss focuses optimization on accurate depth predictions while mitigating the influence of erroneous points. The framework is trained end-to-end using a combination of photometric consistency, contrastive consistency, SSIM, and smoothness losses.

## Key Results
- Achieves state-of-the-art performance among end-to-end unsupervised MVS methods on DTU and Tanks&Temples benchmarks
- Outperforms supervised CasMVSNet without requiring fine-tuning
- Demonstrates improved robustness to view-dependent effects and indistinguishable regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masking source images with Bernoulli noise may force the feature extractor to rely on global context rather than local texture, mitigating failures in indistinguishable regions.
- **Mechanism:** An image-level contrastive branch applies a pixel-wise binary mask ($M_o \sim B(\alpha)$) to source images. By maximizing similarity between the resulting depth map ($D_{IC}$) and the regular depth map ($D_R$), the network is compelled to fill in "holes" using surrounding structural information instead of failed local pixel matching.
- **Core assumption:** The network possesses sufficient receptive field and capacity to infer missing pixels from context if forced to do so.
- **Evidence anchors:**
  - [abstract] "image-level branch that encourages context awareness through masking transformations"
  - [section 3.2] "maximize the similarity between the depth estimations... The intuition is that the augmented images... can also be utilized to estimate complete depth maps as hard positive samples."
  - [corpus] Weak direct evidence; neighboring papers generally address texture issues via multi-scale geometry rather than masking.
- **Break condition:** Performance degrades in highly repetitive patterns where global context is also ambiguous, or if the masking rate $\alpha$ exceeds the network's in-painting capacity.

### Mechanism 2
- **Claim:** Enforcing depth consistency between regular views and randomly selected views (hard positives) likely improves robustness to view-dependent effects like reflections.
- **Mechanism:** A scene-level contrastive branch constructs a cost volume using randomly selected source images (which may have poor overlap or reflections) rather than the "best" views. By enforcing consistency with the regular depth map ($D_R$) via $L_{SCC}$, the 3D regularization module learns to prioritize geometric consistency over photometric outliers.
- **Core assumption:** The regular branch provides a sufficiently accurate "anchor" depth ($D_R$) to guide the scene-level branch; otherwise, error propagation may occur.
- **Evidence anchors:**
  - [abstract] "scene-level branch... improves robustness to view-dependent effects using randomly selected source images"
  - [section 3.3] "a scene-level contrastive sample containing randomly selected source images can be considered a natural hard positive sample."
  - [corpus] [60022] notes self-supervised methods degrade in adverse weather (noise/reflections), supporting the need for robustness mechanisms.
- **Break condition:** If the random view lacks sufficient overlap with the reference, the cost volume construction fails, potentially generating noisy gradients.

### Mechanism 3
- **Claim:** The L0.5 photometric consistency loss appears to shift optimization focus toward high-confidence accurate points while ignoring outliers.
- **Mechanism:** Standard L1/L2 losses penalize outliers heavily (large errors). The L0.5 norm has a gradient proportional to $e^{-0.5}$. As error $e \to 0$, the gradient spikes, aggressively optimizing already accurate points. As $e$ increases, the gradient diminishes, effectively ignoring "terrible points" (occlusions/reflections) that would otherwise distort the depth map.
- **Core assumption:** "Terrible points" are inevitable in unsupervised MVS and should be ignored rather than corrected, whereas "accurate points" determine final fusion quality.
- **Evidence anchors:**
  - [abstract] "L0.5 photometric consistency loss... encourages the model to focus more on accurate points while mitigating the gradient penalty of undesirable ones."
  - [section 3.4] "L0.5 norm... has larger gradients with regard to smaller errors... increases the penalty of accurate points."
  - [corpus] [42802] discusses inverse rendering optimization; [60022] highlights noise issues, but neither explicitly validates L0.5 norm usage.
- **Break condition:** If the initial depth estimation is uniformly poor (no accurate points to bootstrap), the loss gradient remains universally low, stalling convergence.

## Foundational Learning

- **Concept:** Photometric Consistency vs. View-Dependent Effects
  - **Why needed here:** The paper attempts to fix the breakdown of the fundamental MVS assumption (that a point looks the same from all views).
  - **Quick check question:** Can you explain why a standard L1 photometric loss fails on a reflective surface or a occluded edge?

- **Concept:** Contrastive Learning (Hard Positives)
  - **Why needed here:** The method relies on constructing "hard positive" samples (masked/random views) that are semantically identical but photometrically distinct from the anchor.
  - **Quick check question:** In this context, is a "hard positive" a depth map that matches the ground truth, or a depth map derived from corrupted inputs that must match the regular prediction?

- **Concept:** Cost Volume Regularization (3D U-Net)
  - **Why needed here:** The architecture injects contrastive signals into the regularization step. Understanding how the cost volume aggregates multi-view features is required to see where the "context" is injected.
  - **Quick check question:** Does the contrastive loss modify the feature extraction or the cost volume regularization? (Hint: See Section 3.2/3.3 regarding "3D representation").

## Architecture Onboarding

- **Component map:**
  - Input images -> CasMVSNet Backbone -> Regular Branch (Depth $D_R$ + Confidence Mask $M_c$)
  - Input images + Bernoulli masking -> Image-CL Branch (Depth $D_{IC}$)
  - Input images + Random view selection -> Scene-CL Branch (Depth $D_{SC}$)
  - Aggregator -> Loss computation using $L_{0.5PC}$, $L_{ICC}$, and $L_{SCC}$

- **Critical path:**
  1. Input images (Reference + N-1 Sources)
  2. Run Regular Branch to generate $D_R$ and $M_c$ (Critical dependency: $M_c$ must be accurate)
  3. Construct masked and random samples
  4. Run Branch 2 and 3 (can be parallelized)
  5. Compute consistency losses using $M_c$ to mask out invalid regions in the contrastive branches

- **Design tradeoffs:**
  - Training Time vs. Robustness: Training requires three forward passes per step (effectively 3x compute) compared to baseline CasMVSNet
  - Curriculum Learning: The occlusion rate $\alpha$ for image-level masking is ramped up (0 to 0.1). Starting high destabilizes initial training; starting low may fail to trigger context learning

- **Failure signatures:**
  - Edge Dilation: Over-reliance on context (Mechanism 1) may blur fine geometric edges
  - Mask Collapse: If the confidence mask $M_c$ becomes too restrictive, contrastive losses receive zero gradient

- **First 3 experiments:**
  1. Baseline Sanity Check: Train CasMVSNet with only $L_{0.5PC}$ (no contrastive branches) to isolate the contribution of the norm change
  2. Ablation on Masking: Visualize depth maps from the Image-CL branch with fixed $\alpha=[0.1, 0.3, 0.5]$ to check for "hallucination" artifacts in masked regions
  3. Qualitative View-Dependent Test: Run inference on the Tanks&Temples "Playground" or "Lighthouse" (cited in Fig 11) to specifically inspect reflection handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unsupervised MVS methods improve depth estimation accuracy at object edges when the standard assumption of correlation between image gradients and depth gradients fails (e.g., on textured objects)?
- Basis in paper: [explicit] The authors state in the Limitation section that "accurate depth estimation in object edge areas remains a challenge" because the simple gradient assumption "may be invalid in many cases."
- Why unresolved: The current edge-aware smoothness loss relies on color edges to define depth edges, which causes errors when significant color changes occur within the same object surface.
- What evidence would resolve it: A new regularization term that decouples depth edges from color gradients, validated by improved F-scores specifically on object boundary regions.

### Open Question 2
- Question: Does the L0.5 photometric consistency loss introduce a trade-off by under-penalizing large errors ("terrible points") compared to L2 norms, potentially slowing convergence in highly occluded scenes?
- Basis in paper: [inferred] The paper notes L0.5 mitigates the penalty for undesirable points to focus on accurate ones, but the mathematical property of diminishing gradients for large errors implies reduced correction force for outliers.
- Why unresolved: While beneficial for precision, it is unclear if this "softer" penalty on large errors prevents the network from escaping poor local minima during early training stages.
- What evidence would resolve it: A convergence analysis comparing training dynamics of L0.5 vs. L2 norms on scenes with significant occlusion outliers.

### Open Question 3
- Question: Is random Bernoulli masking the most effective strategy for the image-level contrastive branch, or would structure-aware masking better simulate specific failure cases like repetitive patterns?
- Basis in paper: [inferred] The paper uses "independent and identically distributed Bernoulli probability" to generate hard positives, but this random approach may not optimally target the specific structure of indistinguishable regions like low-texture patches.
- Why unresolved: Random masking treats all pixels equally, whereas indistinguishable regions often have specific structural properties that random noise might fail to replicate as a meaningful "hard positive."
- What evidence would resolve it: An ablation study comparing random masking against texture-aware or frequency-domain masking on low-textured subsets of the DTU dataset.

## Limitations
- Edge accuracy remains challenging when color gradients don't correlate with depth gradients
- The method requires three forward passes per training step, increasing computational cost
- Performance in highly repetitive or ambiguous scenes has not been thoroughly tested

## Confidence

**Confidence labels:**
- Mechanism 1 (context inference via masking): Medium — supported by logical reasoning but weak direct evidence
- Mechanism 2 (view-dependent robustness): Medium — plausible but relies on anchor depth quality
- Mechanism 3 (L0.5 outlier suppression): Low — theoretically sound but lacks ablation or empirical isolation

**Major uncertainties:**
- Interaction between masking rate and context capacity is not quantified
- No validation that random-view selection truly improves robustness versus standard view selection
- Potential performance degradation in highly repetitive or ambiguous scenes is untested

## Next Checks

1. Train CasMVSNet with only the L0.5 photometric loss (no contrastive branches) to isolate the norm's contribution.
2. Visualize depth outputs from the Image-CL branch across different occlusion rates (α = 0.1, 0.3, 0.5) to detect hallucination artifacts.
3. Evaluate on Tanks&Temples "Playground" or "Lighthouse" scenes to directly assess reflection handling.