---
ver: rpa2
title: 'Understanding the Generalization of In-Context Learning in Transformers: An
  Empirical Study'
arxiv_id: '2503.15579'
source_url: https://arxiv.org/abs/2503.15579
tags:
- generalization
- functions
- function
- combinations
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates the generalization capabilities
  of transformers in in-context learning (ICL) along three dimensions: inter-problem,
  intra-problem, and intra-task generalization. The authors define a task-centric
  framework and conduct extensive experiments using function-fitting tasks, API calling,
  and translation to evaluate these capabilities.'
---

# Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study

## Quick Facts
- **arXiv ID:** 2503.15579
- **Source URL:** https://arxiv.org/abs/2503.15579
- **Reference count:** 40
- **Primary result:** Transformers excel at intra-task and intra-problem generalization but fail at inter-problem generalization, even with extensive pretraining data.

## Executive Summary
This paper investigates the generalization capabilities of transformers in in-context learning (ICL) along three dimensions: inter-problem, intra-problem, and intra-task generalization. Through systematic experiments on function-fitting tasks, API calling, and translation, the authors find that transformers fail at inter-problem generalization (learning entirely new compositional problem structures) but succeed at intra-problem generalization (applying known structures to unseen instances) with minimal targeted guidance. Training data diversity significantly enhances ICL generalization on unseen tasks and improves performance on known simple tasks, suggesting that coverage of task combinations during training is more important than scale alone.

## Method Summary
The study employs a task-centric framework to evaluate ICL generalization across three dimensions using synthetic function-fitting tasks and real-world applications. Custom GPT-2 models (12 layers, 256 embedding dimensions) are trained from scratch with embedding-level inputs on sinusoidal function combinations, while LLaMA-2/3 and Qwen2 models are tested on ToolBench API calling and WMT14/CC100 translation tasks. Training conditions compare "Baseline" models (trained only on base functions) against "ComFuncLearner" models (trained on base functions plus one specific combination). Evaluation measures squared error for function fitting, accuracy for tool calling, and BLEU/BERTScore for translation, with careful control of training data composition and prompt formatting.

## Key Results
- Transformers fail at inter-problem generalization, unable to infer new compositional problem structures absent from training data, even with extensive pretraining
- Minimal targeted guidance (one example of a specific combination) enables strong intra-problem generalization, allowing models to apply known structures to unseen instances
- Training data diversity significantly improves ICL generalization on unseen tasks and enhances performance on known simple tasks compared to training solely on target tasks

## Why This Works (Mechanism)

### Mechanism 1: Training Data Coverage as a Generalization Constraint
Transformers function as coverage matchers rather than symbolic reasoners, learning to map in-context examples to learned representations of task families. If the "combination" of tasks is not a defined family in the training manifold, the attention mechanism cannot route the input to a valid solution space. This explains why scale alone (LLaMA-3) does not overcome the need for data coverage of compositional structures.

### Mechanism 2: Minimal Targeted Guidance for Intra-Problem Activation
Exposure to a very small number of examples demonstrating a specific problem structure acts as a key to unlock a latent capability. The model learns to attend to the relationship between components rather than just the components themselves, enabling application of the learned pattern to other weightings of the same components.

### Mechanism 3: Task Diversity as a Generalization Enhancer
Mixed-task training forces the model to learn more robust internal representations and prevents overfitting to specific statistical quirks of a single task domain. This regularizes the model, allowing the ICL mechanism to better isolate the true signal in the prompt and transfer knowledge across task types.

## Foundational Learning

- **Concept: In-Context Learning (ICL) vs. Weight Updates**
  - **Why needed here:** The paper investigates how models learn without gradient updates, relying solely on the context window. Understanding that ICL is a "forward pass only" capability is essential to grasping why training data coverage acts as a hard boundary.
  - **Quick check question:** Does the model change its weights when it sees the 10 in-context examples? (Answer: No).

- **Concept: Distribution Shift**
  - **Why needed here:** The paper explicitly tests generalization under distribution shifts (e.g., label noise, input bias). The finding that ICL is fragile to these shifts explains why "generalization" is not absolute but conditional.
  - **Quick check question:** If the in-context examples have a different noise profile than the test data, will the model still predict accurately? (Answer: Likely not, per Appendix C).

- **Concept: Compositional Generalization**
  - **Why needed here:** The core definition of "Inter-problem" vs. "Intra-problem" relies on composition. The model must learn $f(x)$ and $g(x)$ to see if it can learn $f(g(x))$. Understanding this hierarchy is key to decoding the results.
  - **Quick check question:** If a model knows how to add and how to multiply, can it learn to calculate area ($length \times width$) without seeing area examples? (This relates to Inter-problem generalization).

## Architecture Onboarding

- **Component map:** Training Data Distribution -> Prompt Context (The "Guidance") -> Transformer Backbone (GPT-2 / LLaMA) -> Attention Mechanism
- **Critical path:**
  1. Define Base Primitives (e.g., simple functions, single API calls)
  2. Define Target Compositions (e.g., combined functions, multi-step API calls)
  3. Construct Training Data: Train on Base + Mixed + One example of Target (if possible) to trigger intra-problem generalization
  4. Evaluate ICL: Feed prompts with unseen combinations to test if the "Target" structure was learned
- **Design tradeoffs:**
  - Pure Target Training vs. Mixed Training: Training purely on the target task yields high performance on that specific task but fails on variations. Mixed training sacrifices some peak performance on simple tasks for broader generalization and robustness.
  - Model Scale: Larger models (LLaMA-3) do not inherently solve the inter-problem generalization gap without specific data coverage, though they converge faster.
- **Failure signatures:**
  - Inter-problem failure: The model predicts the output of individual base functions correctly but fails completely when they are combined (e.g., predicting the sum of two sine waves as just one sine wave)
  - Sensitivity to Noise: Performance collapses if in-context examples contain label noise or out-of-distribution inputs, indicating a lack of robustness in the ICL mechanism
- **First 3 experiments:**
  1. The "Convex Combination" Test: Train a GPT-2 model on base sine waves. Fine-tune one version with a single convex combination of waves and another version with just another base wave. Test both on new convex combinations to verify Intra-problem generalization (Section 3.1)
  2. The "Mixed Task" Validation: Train two small models on a Tool-calling task. Train one on single-APIs only, and the other on a mix of single and multi-APIs. Compare their accuracy on single-API tasks to see if diversity improved baseline performance (Section 4.1)
  3. The "Noise Fragility" Check: Take a trained model and introduce label noise into the in-context examples (ICE). Plot the degradation of performance to confirm the fragility of the ICL mechanism (Appendix C.1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do non-Transformer architectures (such as MLPs and KANs) exhibit the same ICL generalization boundaries (success in intra-problem, failure in inter-problem) as Transformers?
- Basis in paper: Appendix D.6 states that the authors were "surprised to find that the generalization behavior observed in transformers persists in these other network architectures" and explicitly notes "this opens up exciting avenues for future research."
- Why unresolved: The paper empirically observes this phenomenon but does not propose a theoretical explanation for why sequence-agnostic architectures mimic the ICL generalization properties of attention-based models.
- What evidence would resolve it: A theoretical analysis or ablation study isolating the specific architectural components (if any) or optimization dynamics that enforce these generalization boundaries across diverse architectures.

### Open Question 2
- Question: Can the robustness of ICL be improved to handle distribution shifts, such as label noise or biased inputs, between in-context examples and test samples?
- Basis in paper: Section 5 and Appendix C extensively document that ICL generalization is highly sensitive to label noise and out-of-range inputs, significantly disrupting performance.
- Why unresolved: While the paper successfully quantifies the vulnerability of ICL to these shifts, it does not explore or propose algorithmic interventions or architectural modifications to mitigate this sensitivity.
- What evidence would resolve it: Experiments demonstrating that specific regularization techniques, data cleaning methods, or modified attention mechanisms can maintain generalization performance even when a percentage of in-context examples are noisy or biased.

### Open Question 3
- Question: Can large-scale pretraining alone enable inter-problem generalization, or is targeted fine-tuning ("inspiration") strictly necessary?
- Basis in paper: Section 3.4 notes that despite extensive pretraining, LLaMA-3 "has not fully acquired the ability to transfer knowledge... from question-answering problems to function-fitting" without fine-tuning.
- Why unresolved: The paper demonstrates that LLaMA-3 fails inter-problem generalization in this specific context, leaving open the question of whether this is a fundamental limitation of scale or a result of the specific pretraining data distribution.
- What evidence would resolve it: Evaluating a broader range of pretrained models or pretraining regimes to determine if there exists a data composition or scale threshold where inter-problem generalization emerges without explicit fine-tuning.

## Limitations
- The exact embedding projection mechanism for function-fitting experiments is underspecified, potentially affecting reproducibility
- While inter-problem generalization failure is robustly demonstrated, the paper does not test whether larger or architecturally different models (e.g., Mamba) might exhibit different behavior
- Translation and tool-calling results depend on real dataset quality and may not fully isolate the causal role of training diversity

## Confidence

- **High confidence**: Transformers fail at inter-problem generalization; minimal guidance enables intra-problem generalization; training data diversity improves performance
- **Medium confidence**: The coverage-matching mechanism fully explains the failure mode; mixed-task training benefits are consistent across all task types
- **Low confidence**: No scale or architectural variant can overcome inter-problem generalization gaps; negative interference from mixed-task training is negligible

## Next Checks
1. Replicate the embedding-level input handling in the function-fitting simulation to confirm the exact projection mechanism and rule out artifacts
2. Test Mamba or other non-transformer architectures on the same composition tasks to assess whether the generalization gap is transformer-specific
3. Systematically vary the mixed-task ratio (e.g., 10%, 50%, 90% target task) to quantify the optimal diversity point and check for negative interference