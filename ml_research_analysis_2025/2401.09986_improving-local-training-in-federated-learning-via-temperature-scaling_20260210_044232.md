---
ver: rpa2
title: Improving Local Training in Federated Learning via Temperature Scaling
arxiv_id: '2401.09986'
source_url: https://arxiv.org/abs/2401.09986
tags:
- training
- learning
- federated
- accuracy
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated learning challenges arising from
  non-i.i.d. data distributions across clients, which cause slow convergence and reduced
  accuracy.
---

# Improving Local Training in Federated Learning via Temperature Scaling

## Quick Facts
- arXiv ID: 2401.09986
- Source URL: https://arxiv.org/abs/2401.09986
- Reference count: 40
- Non-i.i.d. data in federated learning causes slow convergence; low-temperature training (T < 1) accelerates optimization by amplifying gradients.

## Executive Summary
This paper addresses federated learning challenges arising from non-i.i.d. data distributions across clients, which cause slow convergence and reduced accuracy. The proposed FLex&Chill method exploits low-temperature (T < 1) training during local updates to amplify gradients and accelerate convergence. Theoretical analysis shows temperature scaling improves convergence bounds by a factor of 1/T. Experiments on three datasets demonstrate up to 6× faster convergence and up to 3.37% accuracy improvement compared to standard federated learning. The method is orthogonal to existing approaches like FedProx and SCAFFOLD, making it broadly applicable.

## Method Summary
The FLex&Chill method modifies local training in federated learning by applying temperature scaling (T < 1) to the logits before the softmax function. During each local update, model logits are divided by T, effectively amplifying the gradient magnitude by a factor of 1/T. This sharpening of the output distribution induces larger shifts in data positions relative to the decision boundary, enabling more aggressive local learning. The method specifically targets the entropy increase that occurs when aggregating models trained on heterogeneous data, helping retain knowledge during the averaging process.

## Key Results
- Up to 6× faster convergence compared to standard FedAvg
- Up to 3.37% accuracy improvement on non-i.i.d. datasets
- Temperature scaling improves theoretical convergence bounds by a factor of 1/T
- Method works orthogonally with existing FL techniques like FedProx and SCAFFOLD

## Why This Works (Mechanism)

### Mechanism 1: Gradient Amplification via Logit Scaling
Applying low temperature (T < 1) during local training amplifies the gradient magnitude, accelerating the optimization step. The gradient of the softmax cross-entropy loss scales by a factor of 1/T, effectively multiplying the error term and boosting the gradient propagated back through the network. This mechanism assumes standard softmax cross-entropy loss and fixed local learning rate. Break condition: if T is set too low (e.g., T=0.05 for complex models), gradient amplification may cause instability or divergence.

### Mechanism 2: Aggressive Representation Space Updates
Low temperatures induce larger shifts in data positions relative to the decision boundary, enabling more aggressive local learning. By sharpening the output probability distribution, the model applies stronger penalties to incorrect predictions, forcing the model to move misclassified samples farther across the decision boundary in a single update. Core assumption: the local dataset contains sufficient signal to justify aggressive updates without overshooting the optimal minima. Break condition: if the local data is extremely noisy, aggressive updates may overfit to noise or outliers.

### Mechanism 3: Counteracting Aggregation Smoothing
Logit Chilling mitigates the "entropy increase" (probability smoothing) that typically occurs when aggregating models trained on heterogeneous (non-i.i.d.) data. In non-i.i.d. settings, server-side aggregation tends to smooth the global model's probability distribution. Training locally with low T forces sharper, more confident local updates that better retain specific knowledge during the averaging process. Core assumption: the heterogeneity is primarily label-based or feature-based causing representation drift. Break condition: in i.i.d. settings, this mechanism is redundant and offers little benefit.

## Foundational Learning

- **Concept: Softmax Temperature Scaling**
  - Why needed here: This is the fundamental operation modified by FLex&Chill. You must understand that T controls the "sharpness" of the probability distribution (low T = sharper/peaked, high T = smoother/uniform).
  - Quick check question: If T → 0, does the softmax output approach a one-hot vector or a uniform distribution?

- **Concept: Non-IID Data Heterogeneity**
  - Why needed here: The mechanism specifically targets the slow convergence caused by clients having different data distributions (e.g., different classes of images).
  - Quick check question: Why does standard FedAvg struggle to converge when Client A has only "cats" and Client B has only "dogs"?

- **Concept: Federated Averaging (FedAvg) Dynamics**
  - Why needed here: FLex&Chill is an orthogonal modification to the local update step of FedAvg. Understanding how local updates are aggregated is necessary to diagnose why "smoothing" occurs.
  - Quick check question: In FedAvg, does the server aggregate raw data, gradients, or model weights?

## Architecture Onboarding

- **Component map:** Server -> Aggregation (FedAvg/FedProx) -> Client (Local Training with Logit Chilling)
- **Critical path:** 1) Server broadcasts global model and target T; 2) Client computes logits z; 3) Modification: Apply z' = z / T; 4) Compute Softmax(z') and Loss; 5) Backpropagate (gradients amplified by 1/T); 6) Return updated weights to Server
- **Design tradeoffs:** Speed vs. Stability: Lower T yields faster convergence but risks gradient explosion on large models. Architecture dependency: Large capacity models may require normalization layers or slightly higher T (e.g., 0.25 instead of 0.05) to remain stable.
- **Failure signatures:** Loss Spiking: Validation loss fluctuates violently or increases; indicates T is too low for the model capacity or batch size. No Improvement: If applied to i.i.d. data, the method offers little benefit over standard T=1.
- **First 3 experiments:** 1) Baseline Stability Check: Train 2-layer CNN on CIFAR10 (Non-IID) with T=1.0 vs T=0.5. Plot gradient norms to verify amplification. 2) Convergence Speed: Measure rounds to reach target accuracy (e.g., 80%) for T ∈ {0.05, 0.25, 0.5, 1.0}. 3) Model Capacity Stress Test: Run FLex&Chill on ResNet18 model. Identify the lowest stable T (likely ≥ 0.25) and compare accuracy against T=1.0 baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an analytical method be developed to determine the optimal temperature setting based on model capacity and task complexity?
- Basis in paper: The authors state in the Discussion that "identifying an optimal temperature value is challenging to analytically solve" and note that large capacity models with easy tasks often exhibit instability with low temperatures.
- Why unresolved: The paper currently relies on empirical heuristics to select temperature values (T ∈ {0.05, 0.25, ...}) rather than a formal derivation.
- What evidence would resolve it: A theoretical framework or algorithm that predicts the optimal T given specific model and dataset characteristics, validated against empirical search.

### Open Question 2
- Question: Does dynamic, per-sample or per-round temperature scheduling improve performance over constant low-temperature training?
- Basis in paper: The Discussion suggests that "varying temperatures can be applied... with respect to their significance," proposing that "harder" samples might exploit lower temperatures while easier ones use higher temperatures.
- Why unresolved: All experiments in the paper utilize a constant temperature setting throughout the local training process.
- What evidence would resolve it: Comparative experiments evaluating adaptive temperature schedules (e.g., adjusting T based on loss magnitude or federated round) against the static baseline.

### Open Question 3
- Question: How can temperature scaling be effectively applied in systems with heterogeneous computing capabilities to mitigate the "straggler" problem?
- Basis in paper: The authors identify "Supporting computation heterogeneity" as an "interesting direction of future research," hypothesizing that applying lower temperatures to slower devices could speed their convergence.
- Why unresolved: The evaluation setup assumes a homogeneous environment where all clients share the same model architecture and resource constraints.
- What evidence would resolve it: Simulations profiling clients' compute/network resources and assigning personalized temperatures to balance convergence speed across stragglers and powerful clients.

## Limitations

- The method's effectiveness is highly dependent on non-i.i.d. data distribution, with minimal benefits in i.i.d. settings
- Very low temperatures (T < 0.1) can cause instability in large capacity models like ResNet and ViT
- The optimal temperature value is currently determined empirically rather than through analytical methods

## Confidence

- **High Confidence:** The gradient amplification mechanism (Mechanism 1) and its mathematical derivation are well-established and verifiable through the softmax derivative analysis
- **Medium Confidence:** The convergence speed improvements (up to 6×) and accuracy gains (up to 3.37%) are based on experimental results, but exact reproducibility depends on implementation details like client sampling and initialization
- **Low Confidence:** The theoretical convergence bound analysis is sound but limited in scope, not fully accounting for the trade-off between acceleration and potential instability at extreme temperature values

## Next Checks

1. **Gradient Amplification Verification:** Implement a simple 2-layer CNN on CIFAR10 with T ∈ {0.05, 0.25, 0.5, 1.0}, log gradient norms during training, and confirm the 1/T scaling relationship holds empirically.

2. **Stability Boundary Test:** Run FLex&Chill on ResNet18 (CIFAR100) and systematically identify the lowest stable temperature before divergence occurs, documenting the loss behavior.

3. **i.i.d. vs Non-i.i.d. Comparison:** Implement both Dirichlet(α=0.1) and uniform data partitioning for CIFAR10, run FLex&Chill with T=0.25 vs T=1.0, and quantify the difference in convergence speed and final accuracy.