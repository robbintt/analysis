---
ver: rpa2
title: Masked Diffusion Language Models with Frequency-Informed Training
arxiv_id: '2509.05056'
source_url: https://arxiv.org/abs/2509.05056
tags:
- diffusion
- language
- masking
- training
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores masked diffusion language models (MDLMs) for
  data-efficient pretraining, specifically targeting the BabyLM 2025 Challenge's 100M-word
  corpus constraint. The authors introduce frequency-informed masking to prioritize
  learning from rare tokens while preserving theoretical validity, and investigate
  various noise scheduling strategies, including two-mode approaches.
---

# Masked Diffusion Language Models with Frequency-Informed Training

## Quick Facts
- arXiv ID: 2509.05056
- Source URL: https://arxiv.org/abs/2509.05056
- Authors: Despoina Kosmopoulou; Efthymios Georgiou; Vaggelis Dorovatas; Georgios Paraskevopoulos; Alexandros Potamianos
- Reference count: 13
- Primary result: MDLMs with frequency-informed masking achieve competitive performance on BabyLM benchmark suite

## Executive Summary
This work explores masked diffusion language models (MDLMs) for data-efficient pretraining, specifically targeting the BabyLM 2025 Challenge's 100M-word corpus constraint. The authors introduce frequency-informed masking to prioritize learning from rare tokens while preserving theoretical validity, and investigate various noise scheduling strategies, including two-mode approaches. They evaluate their method on the BabyLM benchmark suite, measuring linguistic competence, world knowledge, and human-likeness. Results show performance competitive with hybrid autoregressive-masked baselines, with the best configuration achieving 52.95% on EWoK, 78.28% on BLiMP, and 73.13% on BLiMP Supplement using a bimodal Gaussian noise schedule with derivative scaling (p=0.0). The submission model to the leaderboard demonstrated competitive performance, particularly excelling in fine-tuning tasks and human-likeness measures like Reading (7.4%) and Adjective Nominalization Test (49.6%).

## Method Summary
The method implements masked diffusion language models using an LTG-BERT Transformer backbone with attention-gating modifications and Adaptive Layer Normalization (AdaLN) for timestep conditioning. The model employs frequency-informed masking that assigns higher masking probabilities to low-frequency tokens based on global token frequency ranks, with a softening power parameter p_freq=0.02. Various noise schedules are explored including linear, cosine, and bimodal Gaussian distributions. The training objective is the negative evidence lower bound (NELBO) with configurable derivative scaling power p. The model is trained on the BabyLM corpus using a BPE tokenizer with 16k vocabulary, achieving 126.6M parameters. The submission uses a cosine schedule while internal experiments demonstrate superior performance with bimodal Gaussian schedules when derivative scaling is set to p=0.0.

## Key Results
- Best configuration achieves 52.95% on EWoK, 78.28% on BLiMP, and 73.13% on BLiMP Supplement
- Bimodal Gaussian schedule with p=0.0 derivative scaling outperforms standard approaches by 10+ points on BLiMP
- Frequency-informed masking provides consistent improvements across linguistic tasks
- Submission model demonstrates strong performance on fine-tuning tasks and human-likeness measures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prioritizing the reconstruction of rare tokens enhances sample efficiency in low-data regimes.
- **Mechanism:** Frequency-informed masking assigns higher masking probabilities to low-frequency tokens. This forces the model to allocate capacity to informative, semantically rich words rather than overfitting to common function words.
- **Core assumption:** Rare tokens contain disproportionate semantic signal necessary for generalization, and standard uniform masking wastes update steps on already-learned high-frequency tokens.
- **Evidence anchors:**
  - [abstract] "incorporating frequency-informed masking that prioritizes learning from rare tokens while maintaining theoretical validity."
  - [section] Page 3, Eq (2) defines the conditional scaling logic; Table 3 shows consistent improvements on BLiMP Supplement (+1%) and significant gains on Adjective Nominalization when using frequency weighting.
  - [corpus] Related work "Mask and You Shall Receive" supports the broader hypothesis that non-uniform masking improves efficiency, though it optimizes for prediction difficulty rather than frequency.
- **Break condition:** If the vocabulary distribution is extremely flat (not Zipfian) or if rare tokens are primarily noise (e.g., typos), this mechanism may degrade performance by forcing the model to predict uninformative outliers.

### Mechanism 2
- **Claim:** Scaling the derivative term (α'_t) in the NELBO objective is critical for stabilizing specific noise schedules, specifically Bimodal Gaussian.
- **Mechanism:** The NELBO objective weights loss by the rate of change of the noise schedule (α'_t). For complex schedules like Bimodal Gaussian, this derivative term may create unstable gradients or over-weight specific timesteps. Reducing the power p to 0.0 (omitting the derivative) flattens the loss landscape, allowing the schedule to guide training without gradient explosion.
- **Core assumption:** The derivative term, while theoretically necessary for strict ELBO adherence, acts as a harmful re-weighting factor for non-monotonic or complex schedule geometries.
- **Evidence anchors:**
  - [section] Page 5, Table 2: "Bimodal Gaussian (1.0)" scores 68.13 on BLiMP, while "Bimodal Gaussian (0.0)" jumps to 78.28.
  - [section] Page 5 text: "scaling the derivatives in the ELBO is critical for achieving better performance with certain noise schedules."
  - [corpus] Specific derivative scaling ablations are not widely discussed in the provided corpus neighbors (e.g., "Auto-Regressive Masked Diffusion"), suggesting this is a specific sensitivity in this architecture.
- **Break condition:** Applying p=0.0 to standard cosine or linear schedules may result in sub-optimal convergence if the theoretical weighting provided by the derivative is actually required to balance the signal-to-noise ratio in simpler schedules.

### Mechanism 3
- **Claim:** A cosine-derived noise schedule focuses model capacity on low-corruption reconstruction, improving fine-grained linguistic competence.
- **Mechanism:** Unlike a linear schedule which weights all masking rates equally (mean 0.5), a cosine schedule biases sampling toward lower masking rates (mean 0.36). This provides the model with more "easy" examples (few tokens masked), which acts as a form of curriculum learning to refine local dependencies before tackling global structure.
- **Core assumption:** In data-constrained settings, learning to clean up small amounts of noise is more transferable to downstream tasks than learning to reconstruct heavily corrupted text.
- **Evidence anchors:**
  - [section] Page 5, Table 1: Cosine schedule outperforms Linear on BLiMP (79.05 vs 77.91) and BLiMP Sup (70.74 vs 67.63).
  - [section] Page 5 text: "low-masking-rate, more fine-grained focus enables the model to perform better in the zero-shot likelihood estimation tasks."
  - [corpus] Corpus neighbors do not explicitly compare cosine vs. linear schedules for MDLMs, leaving this finding specific to this implementation.
- **Break condition:** If the target task requires robustness to high noise or generation from scratch (rather than scoring), this low-masking bias might leave the model under-prepared for high-entropy generation tasks.

## Foundational Learning

- **Concept: Masked Diffusion Language Models (MDLMs)**
  - **Why needed here:** This is the core architecture replacing standard MLM. You must understand that masking is not a static 15% rate but a continuous-time stochastic process.
  - **Quick check question:** How does the masking rate vary during training compared to standard BERT?

- **Concept: NELBO (Negative Evidence Lower Bound)**
  - **Why needed here:** The loss function is not simple cross-entropy on masked tokens; it includes an integral over time and a derivative term α'_t (unless p=0).
  - **Quick check question:** What happens to the loss weight if the noise schedule changes rapidly at a specific timestep t?

- **Concept: Adaptive Layer Normalization (AdaLN)**
  - **Why needed here:** The Transformer must condition its predictions on the current "time" or noise level of the diffusion process. AdaLN injects this timestep information.
  - **Quick check question:** How does the model know if a sequence is 10% masked vs 80% masked when making a prediction?

## Architecture Onboarding

- **Component map:** Input sequence -> Apply Mask (based on schedule) -> Transformer with AdaLN (conditioned on t) -> Predict original tokens
- **Critical path:**
  1. Implement Schedule: Select noise schedule (e.g., Cosine or Bimodal). If Bimodal, ensure p (derivative power) is configurable.
  2. Masking Logic: Implement frequency-informed masking (Eq 2) to determine which tokens to mask based on the global frequency rank and softening power p_freq.
  3. Forward Pass: Input sequence → Apply Mask (based on schedule) → Transformer with AdaLN (conditioned on t) → Predict original tokens.

- **Design tradeoffs:**
  - **Submission vs. Best Config:** The submission used a Cosine schedule (safe, stable), but internal experiments showed Bimodal with p=0.0 achieved higher scores (Table 2). Choose Bimodal for performance if you can tune p; choose Cosine for stability.
  - **Time Conditioning:** The paper notes time-conditioned evaluation yielded mixed results (Table 3). Training requires time conditioning (AdaLN), but evaluation may not strictly need it for simple pseudo-likelihood scoring.

- **Failure signatures:**
  - **Stalling Loss with Bimodal Schedule:** If using Bimodal schedule with p=1.0 (standard derivative), performance collapses (Table 2). Fix by setting p=0.0.
  - **Overfitting on Function Words:** If frequency weighting is not applied (or p_freq is too high), check if the model scores well on BLiMP but fails on vocabulary-heavy tasks.

- **First 3 experiments:**
  1. **Baseline Replication:** Train LTG-BERT with standard uniform masking (No Freq. W.) and Cosine schedule to verify the MDLM pipeline matches Table 1 baselines.
  2. **Ablate Derivative Power:** Using a Bimodal Gaussian schedule, train two models: one with p=1.0 and one with p=0.0 to validate the massive performance gap shown in Table 2.
  3. **Frequency Ablation:** Train with Frequency Weighting (p=0.02) vs. No Weighting on the Cosine schedule to measure the delta on the Adjective Nominalization task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What impact do standard architectural optimizations for Masked Diffusion Language Models have on performance in data-constrained settings?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they made "minimal architectural modifications" and that "standard implementations... often incorporate additional optimizations that can substantially impact performance; such optimizations are not explored here."
- **Why unresolved:** The current implementation prioritized integrating the diffusion objective into the LTG-BERT framework without exploring MDLM-specific architectural efficiencies.
- **What evidence would resolve it:** A study comparing the current minimal-adaptation model against an MDLM utilizing standard architectural optimizations (e.g., specific attention mechanisms or time-embedding strategies) on the 100M-word BabyLM benchmark.

### Open Question 2
- **Question:** What is the most accurate and efficient method for estimating likelihoods in MDLMs for zero-shot evaluation?
- **Basis in paper:** [explicit] The Limitations section notes that using the standard MLM backend is "myopic" and potentially "suboptimal," and that "accurately and efficiently estimating likelihoods... while maintaining low variance remains an open challenge."
- **Why unresolved:** While the authors use an MLM pseudo-likelihood backend for efficiency, they acknowledge it focuses only on the last denoising steps and may undermine the model's true generative capabilities.
- **What evidence would resolve it:** A comparative analysis evaluating zero-shot performance using both the deterministic MLM backend and a Monte-Carlo approximation of the diffusion denoising process.

### Open Question 3
- **Question:** Why does setting the derivative scaling power p=0 (omitting the α'_t term) significantly improve performance when using bimodal noise schedules?
- **Basis in paper:** [inferred] The authors observe in Section 4.2 that discarding the derivative term (p=0.0) allows the bimodal Gaussian schedule to nearly reach top baseline scores, whereas including it (p=1.0) yields "underwhelming" results, suggesting a misalignment between the theoretical objective and the optimal training dynamics for this specific schedule.
- **Why unresolved:** The paper reports the empirical phenomenon—that the ELBO derivative is critical to scale or omit—but does not provide a theoretical justification for why the standard NELBO objective fails for bimodal schedules in this setting.
- **What evidence would resolve it:** A theoretical analysis or ablation study demonstrating how the gradient contributions of the α'_t term interact with bimodal distributions to cause instability or convergence issues during training.

## Limitations

- **Implementation underspecification:** Several key hyperparameters (learning rate, weight decay, warmup schedule) are inherited from GPT-BERT but not explicitly specified
- **Theoretical gap:** The paper demonstrates empirical benefits of p=0.0 derivative scaling but lacks theoretical justification for why standard NELBO fails for bimodal schedules
- **Evaluation constraints:** The study relies heavily on proxy measures and pseudo-likelihood rather than direct generation quality assessment

## Confidence

- **High Confidence:** The core finding that frequency-informed masking improves sample efficiency is well-supported by consistent improvements across multiple linguistic tasks (BLiMP, BLiMP Supplement, Adjective Nominalization). The mechanism is theoretically sound and empirically validated.
- **Medium Confidence:** The bimodal Gaussian schedule with p=0.0 derivative scaling shows dramatic performance gains, but this represents a departure from standard MDLM theory. The paper demonstrates the effect but does not fully explain the underlying mathematical justification.
- **Low Confidence:** The submission model's performance relative to other BabyLM 2025 submissions is difficult to assess due to limited leaderboard context and potential confounding factors in the evaluation setup.

## Next Checks

1. **Derivative Scaling Sensitivity Analysis:** Systematically evaluate the impact of derivative scaling power p across multiple noise schedules (linear, cosine, bimodal) on a held-out validation set to determine if p=0.0 is specifically beneficial for bimodal schedules or represents a more general MDLM training insight.

2. **Frequency Distribution Robustness:** Test the frequency-informed masking approach on corpora with different vocabulary distributions (e.g., synthetic Zipfian vs. uniform) to validate the core assumption that rare tokens contain disproportionate semantic signal in typical language data.

3. **Generation Quality Assessment:** Move beyond pseudo-likelihood evaluation to assess actual generation quality using human evaluation or automated metrics (e.g., MAUVE, BERTScore) on the bimodal schedule configuration to validate whether theoretical performance gains translate to practical utility.