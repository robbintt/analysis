---
ver: rpa2
title: Generative Visual Code Mobile World Models
arxiv_id: '2602.01576'
source_url: https://arxiv.org/abs/2602.01576
tags:
- action
- world
- mobile
- iacc
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of visual world modeling for
  mobile GUI agents, where existing methods either sacrifice visual fidelity for text-based
  representations or struggle with precise text rendering in pixel-based approaches.
  The authors propose a novel paradigm: visual world modeling via renderable code
  generation, where a single Vision-Language Model (VLM) predicts the next GUI state
  as executable web code rather than generating pixels directly.'
---

# Generative Visual Code Mobile World Models

## Quick Facts
- arXiv ID: 2602.01576
- Source URL: https://arxiv.org/abs/2602.01576
- Authors: Woosung Koh; Sungjun Han; Segyu Lee; Se-Young Yun; Jamin Shin
- Reference count: 40
- gWorld 32B achieves 79.6% average instruction accuracy, outperforming Qwen3 VL 235B-A22B (55.7%) and GLM-4.6V 106B (67.4%)

## Executive Summary
This paper addresses the challenge of visual world modeling for mobile GUI agents, where existing methods either sacrifice visual fidelity for text-based representations or struggle with precise text rendering in pixel-based approaches. The authors propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code rather than generating pixels directly. This approach leverages VLMs' linguistic priors for precise text rendering while utilizing their pre-training on structured web code for high-fidelity visual generation.

The proposed gWorld models (8B and 32B parameters) are the first open-weight visual mobile GUI world models built on this paradigm, accompanied by a data generation framework that synthesizes code-based training data. Extensive evaluation across four in-distribution and two out-of-distribution benchmarks shows that gWorld establishes a new pareto frontier in accuracy versus model size, outperforming eight frontier open-weight models up to 50.25× larger.

## Method Summary
The authors introduce a paradigm shift in visual world modeling by predicting executable web code instead of pixels. The gWorld models use a single VLM to generate renderable HTML/CSS code that represents the next GUI state. A comprehensive data generation framework synthesizes training data by capturing web-based mobile interfaces, extracting their code structure, and creating paired observations of GUI states with corresponding code representations. The models are trained to predict the next state's code given the current state's visual and code context, enabling both accurate visual generation and precise text rendering through the VLM's linguistic capabilities.

## Key Results
- gWorld 32B achieves 79.6% average instruction accuracy across all benchmarks, establishing new state-of-the-art performance
- The model outperforms eight frontier open-weight models up to 50.25× larger, including Qwen3 VL 235B-A22B (55.7%) and GLM-4.6V 106B (67.4%)
- Scaling training data follows predictable power law relationships, with each pipeline component demonstrably improving data quality

## Why This Works (Mechanism)
The paradigm succeeds by transforming the visual generation problem into a structured code generation task. VLMs have strong linguistic priors that enable precise text rendering when generating code, avoiding the common failure modes of pixel-based approaches where text often appears garbled or misaligned. Simultaneously, the structured nature of web code (HTML/CSS) provides the model with explicit spatial and semantic relationships that guide high-fidelity visual generation. By predicting renderable code rather than pixels, the model leverages the VLM's pre-training on web-structured data while maintaining the flexibility to generate complex GUI layouts through standard web rendering engines.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Multimodal models trained on both visual and textual data, crucial for understanding GUI layouts and generating corresponding code. Needed for the core reasoning and generation capabilities.
- **Web Code Structure**: HTML/CSS syntax and semantics that define GUI layouts. Required for the model to generate valid, renderable code representations.
- **Renderable Code Generation**: The process of producing executable code that can be rendered into visual output. Essential for transforming GUI state predictions into actionable formats.
- **Power Law Scaling**: The relationship between training data size and model performance following a predictable curve. Important for understanding how to optimize training efficiency.
- **Mobile GUI Agents**: Autonomous systems that interact with mobile interfaces. The target application domain that motivates the world modeling approach.

## Architecture Onboarding

**Component Map:** Visual Input -> VLM Encoder -> Code Generation Head -> Renderable HTML/CSS Output

**Critical Path:** Visual observation → VLM processing → Code token prediction → Code rendering → Next GUI state

**Design Tradeoffs:** The paradigm trades direct pixel generation for code generation, gaining precise text rendering and leveraging web pre-training at the cost of being limited to web-based GUI representations. This enables smaller models to outperform much larger pixel-based alternatives but may limit applicability to native mobile applications.

**Failure Signatures:** Poor text rendering in generated code, invalid HTML/CSS syntax, inability to generalize to non-web GUI paradigms, and potential brittleness when encountering dynamic content or animations that don't map cleanly to static code representations.

**First Experiments:**
1. Test gWorld on native mobile applications that don't use web code to validate cross-platform generalization
2. Conduct ablation studies removing different pipeline components to quantify individual contributions
3. Evaluate inference latency and memory requirements on actual mobile hardware for deployment feasibility

## Open Questions the Paper Calls Out
None

## Limitations
- The approach fundamentally depends on web-based code representation, which may not generalize well to native mobile applications that don't use web technologies
- The evaluation primarily focuses on accuracy metrics without extensive qualitative analysis of generated code quality or edge cases
- Scalability claims based on power law relationships need validation across different domains and application types
- Practical considerations like inference speed, memory constraints on mobile devices, and robustness to dynamic content are not extensively addressed

## Confidence

**High confidence** in core methodology and benchmark results - The approach is technically sound with rigorous quantitative comparisons across multiple models and benchmarks.

**Medium confidence** in generalizability claims - While supported by presented data, power law scaling relationships and cross-platform applicability may not hold universally.

**Low confidence** in real-world deployment readiness - Limited assessment of practical constraints like inference speed, mobile memory usage, and robustness to network conditions.

## Next Checks

1. Test gWorld models on native mobile applications that don't use web code to validate cross-platform generalization and identify limitations of the web-based code representation approach.

2. Conduct ablation studies removing different components of the pipeline (web code generation, rendering, etc.) to quantify the contribution of each component to overall performance.

3. Evaluate inference latency and memory requirements on actual mobile hardware to assess practical deployment feasibility and identify potential bottlenecks for real-time GUI agent applications.