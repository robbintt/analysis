---
ver: rpa2
title: 'Splitwiser: Efficient LM inference with constrained resources'
arxiv_id: '2505.03763'
source_url: https://arxiv.org/abs/2505.03763
tags:
- inference
- token
- prompt
- vllm
- splitwiser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Splitwiser addresses the inefficiency of token generation phases
  in LLM inference, where compute resources are underutilized compared to prompt computation
  phases. The method splits inference into prompt processing and token generation
  phases on a single GPU using multiprocessing and NVIDIA's Multi-Process Service
  (MPS), eliminating data transfer overhead between devices.
---

# Splitwiser: Efficient LM inference with constrained resources

## Quick Facts
- arXiv ID: 2505.03763
- Source URL: https://arxiv.org/abs/2505.03763
- Reference count: 7
- Primary result: 17.6% latency reduction on Huggingface A100 GPUs with 4-8 parallel processes

## Executive Summary
Splitwiser addresses GPU resource underutilization in LLM inference by parallelizing the compute-intensive prompt processing phase with the memory-intensive token generation phase on a single GPU. The method uses PyTorch multiprocessing and NVIDIA MPS to enable temporal overlap of these phases without data transfer overhead between devices. Experiments on Huggingface and vLLM architectures demonstrate 17.6-18.2% latency reduction and 1.42× throughput improvement, showing that single-GPU phase parallelization can significantly improve inference performance.

## Method Summary
Splitwiser splits LLM inference into prompt processing and token generation phases across multiple processes on a single GPU. Input batches are divided among worker processes, with prompt workers generating KV-caches that token workers consume for autoregressive generation. The method leverages NVIDIA MPS daemon to reduce context-switching overhead, enabling transparent GPU resource sharing across processes. Implementation requires modifications to Huggingface and vLLM architectures to support shared model memory and phase-specific execution patterns.

## Key Results
- 17.6% latency reduction on Huggingface A100 GPUs with 4-8 parallel processes
- 18.2% latency improvement on A10 GPUs when combined with MPS
- 1.42× throughput improvement on vLLM pipelines with MPS enabled

## Why This Works (Mechanism)

### Mechanism 1: Complementary Resource Utilization Across Phases
- Claim: Prompt computation and token generation have distinct resource profiles that can be overlapped for better GPU utilization.
- Mechanism: The prompt phase processes all input tokens in parallel, driving high SM throughput. Token generation is memory-bound, accessing KV-cache sequentially. By interleaving these phases across processes, compute and memory resources are utilized more simultaneously.
- Core assumption: Phases exhibit stable, complementary utilization patterns that persist when overlapped.
- Evidence anchors:
  - [abstract] "compute-intensive prompt computation phase and the memory-intensive token generation phase...token generation phases fail to fully utilize compute resources"
  - [section III-A, Figure 2-4] Shows SM throughput scales with input tokens (prompt), while memory throughput sustains without full utilization during batching
  - [corpus] Sarathi and Orca (cited in paper) support phase-aware scheduling for LLM inference

### Mechanism 2: Multiprocessing with Pipeline Overlap
- Claim: Splitting requests across parallel processes enables temporal overlap of prompt and token phases on a single GPU.
- Mechanism: Input dataset is split into sub-datasets matching the number of processes. As Process A finishes prompt processing and enters token generation, Process B begins its prompt phase. This pipelining reduces idle time.
- Core assumption: GPU has sufficient memory and compute headroom to service multiple processes without severe contention.
- Evidence anchors:
  - [abstract] "splits these two phases onto the same GPU...allowing both phases to run in parallel"
  - [section III-B] "use PyTorch Multi-Processing to run the prompt and token phases in parallel, i.e. as the prompt processing of the first process finishes, the second process begins its prompt processing in parallel"
  - [corpus] No direct corpus evidence for this specific single-GPU multiprocessing approach; most related work (Splitwise) uses multi-GPU splitting

### Mechanism 3: NVIDIA MPS for Reduced Context-Switch Overhead
- Claim: MPS transparently multiplexes GPU access across processes with lower overhead than default CUDA context switching.
- Mechanism: MPS daemon funnels multiple CUDA clients through a single shared context, reducing kernel launch latency and enabling finer-grained interleaving of operations from different processes.
- Core assumption: Applications do not have complex interdependencies requiring synchronization beyond what MPS supports.
- Evidence anchors:
  - [section II-G] "MPS manages the instances sharing the GPU resources transparently, providing faster multi-process inference times"
  - [section IV-B-4, Figure 9] MPS-enabled Splitwiser achieves 18.2% latency reduction vs. 17.6% without MPS on Huggingface; 1.42× speedup on vLLM (Figure 10)
  - [corpus] No corpus evidence directly validating MPS for LLM inference; this appears novel to this work

## Foundational Learning

- **Concept: LLM Inference Phase Characteristics**
  - Why needed here: Splitwiser's entire premise rests on prompt phase being compute-bound and token phase being memory-bound.
  - Quick check question: For a model with 1024 input tokens and 20 output tokens, which phase dominates total inference time, and which resource is the bottleneck for each?

- **Concept: CUDA Contexts and Multiprocessing**
  - Why needed here: Understanding why standard multiprocessing incurs overhead helps explain why MPS is necessary.
  - Quick check question: What happens at the GPU level when two PyTorch processes both attempt to execute kernels simultaneously without MPS?

- **Concept: KV-Cache and Its Role in Token Generation**
  - Why needed here: Token generation depends on KV-cache from prompt phase; sharing this across processes is the key synchronization challenge.
  - Quick check question: If Process A handles prompt computation and Process B handles token generation, what data must be transferred and how could this become a bottleneck?

## Architecture Onboarding

- **Component map:** Input splitter -> Prompt workers -> KV-cache shared memory -> Token workers -> Output aggregator -> MPS daemon

- **Critical path:**
  1. Start MPS daemon (`nvidia-cuda-mps-control -d`)
  2. Load model once into shared memory (requires code modification to vLLM/Huggingface)
  3. Spawn N worker processes, each assigned a sub-batch
  4. Workers execute phase-appropriate computation; prompt workers write KV-cache to shared location
  5. Token workers read KV-cache and generate output
  6. Aggregate results from all workers

- **Design tradeoffs:**
  - Memory duplication vs. implementation complexity: Unmodified code loads N model copies (simple but memory-wasteful); shared memory requires non-trivial changes to worker initialization
  - Throughput vs. per-batch latency: MPS improves overall throughput but individual batch latency may increase (Figure 11 shows batch inference time higher with multiprocessing)
  - Process count: More processes increase overlap opportunities but also increase prompt-phase overhead (Figure 8)
  - Scheduler-level vs. instance-level multiprocessing: Scheduler-level spawning has prohibitive overhead (Section IV-C); instance-level is simpler but less adaptive

- **Failure signatures:**
  - OOM from duplicate model weights (naive multiprocessing without shared memory)
  - MPx2 slower than single-process (context switching dominates, observed in vLLM Figure 10)
  - Pickling errors when spawning processes mid-scheduler (PyTorch CUDA tensors require spawn not fork)
  - KV-cache synchronization bugs leading to incorrect generation

- **First 3 experiments:**
  1. **Profile baseline utilization:** Run single-request inference with `nsys` or `ncu`, measure SM throughput during prompt phase and memory throughput during token phase to confirm complementary profiles on your hardware.
  2. **Multiprocessing without MPS:** Implement 2-process Splitwiser on Huggingface (simpler than vLLM), measure latency reduction vs. sequential; verify no OOM.
  3. **Add MPS and compare:** Enable MPS daemon, re-run experiment 2, compare latency and throughput; specifically check if per-batch latency increases (tradeoff validation).

## Open Questions the Paper Calls Out

- **Open Question 1:** How do synchronization and communication overheads impact the efficiency of the proposed "on-demand prompt process" architecture in vLLM?
  - Basis in paper: [explicit] The Conclusion proposes instantiating a prompt process once and using queues, noting, "one must take care in handling communication/synchronization overhead."
  - Why unresolved: The current implementation of spawning processes on-demand failed due to high initialization costs and object pickling restrictions; the queue-based optimization remains unimplemented.
  - What evidence would resolve it: Latency and throughput metrics from a vLLM implementation using persistent queues to manage prompt processing jobs asynchronously.

- **Open Question 2:** Can the pre-processing steps of the prompt and token generation phases be executed concurrently without relying on multiprocessing?
  - Basis in paper: [explicit] Section V suggests investigating "if these can be completed together without multiprocessing," specifically regarding fetching input tokens versus KV caches.
  - Why unresolved: The authors focused on multiprocessing/MPS methods and listed this alternative synchronization approach only as a future angle of investigation.
  - What evidence would resolve it: Profiling results showing kernel overlap during the pre-processing stages of a modified single-process scheduler.

- **Open Question 3:** What specific mechanisms cause standard multiprocessing (MPx2) to degrade vLLM performance compared to single-process execution?
  - Basis in paper: [explicit] Section IV-C observes that MPx2 "consistently performs worse" than the single-process baseline, hypothesizing that throughput gains are "overshadowed by... GPU context switching overhead."
  - Why unresolved: The paper identifies the regression but does not isolate the context switching costs or explain why MPS avoids this penalty while standard multiprocessing does not.
  - What evidence would resolve it: Micro-benchmarking of GPU context switch latency and compute occupancy rates for MPx2 versus SP scenarios.

## Limitations

- The complementary resource utilization pattern may not generalize across different model sizes, batch configurations, or task types beyond the OPT-125m radiology reports used in evaluation.
- MPS contribution is not well-isolated from other variables; no comparison against alternative multiprocessing approaches with custom CUDA context management.
- Shared memory implementation details are not provided, making engineering feasibility assessment difficult for production deployment.

## Confidence

- **High Confidence**: The fundamental observation that LLM inference has distinct compute-intensive and memory-intensive phases is well-established and the experimental methodology for measuring latency improvements is sound. The 17.6% latency reduction on Huggingface with 4-8 processes is reproducible and directly measured.
- **Medium Confidence**: The claim that parallelizing these phases on a single GPU improves resource utilization is supported by the results but depends heavily on the specific workload characteristics demonstrated. The complementary utilization pattern may not hold for all model sizes, batch configurations, or task types.
- **Low Confidence**: The specific contribution of MPS versus other multiprocessing optimizations is not well-isolated. The engineering feasibility of shared memory implementations across different architectures is asserted but not demonstrated with sufficient detail. The throughput-latency tradeoff implications for production systems are mentioned but not fully explored.

## Next Checks

1. **Phase Characterization Across Models**: Profile SM and memory throughput during prompt versus token phases for at least three different model sizes (small: 125M, medium: 7B, large: 70B parameters) on the same hardware. Measure how phase characteristics change with varying input lengths (100-2000 tokens) and output lengths (10-200 tokens) to determine generalizability.

2. **MPS Isolation Experiment**: Implement Splitwiser without MPS (standard multiprocessing) and compare against MPS-enabled version on identical hardware and workloads. Additionally, test against a custom multiprocessing implementation that uses explicit CUDA context management rather than MPS to isolate the specific benefits of NVIDIA's solution.

3. **Shared Memory Implementation Validation**: Implement the shared model memory approach described for vLLM, measuring both memory savings and performance impact. Compare against the naive multiprocessing approach that duplicates model weights, specifically measuring the engineering complexity through code change metrics (lines modified, new components introduced) and runtime overhead.