---
ver: rpa2
title: 'On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional
  Study'
arxiv_id: '2505.11839'
source_url: https://arxiv.org/abs/2505.11839
tags:
- counterfactual
- reasoning
- causal
- range
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a decompositional study of LLMs'' counterfactual
  reasoning capabilities. The authors break down counterfactual reasoning into four
  stages: causal variable identification, causal graph construction, intervention
  identification, and outcome reasoning.'
---

# On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study

## Quick Facts
- arXiv ID: 2505.11839
- Source URL: https://arxiv.org/abs/2505.11839
- Reference count: 40
- Primary result: LLMs struggle most with identifying implicit variables (mediators) and reasoning through causal chains, particularly in complex modalities

## Executive Summary
This paper presents a systematic study of large language models' counterfactual reasoning capabilities by decomposing the task into four sequential stages: causal variable identification, causal graph construction, intervention identification, and outcome reasoning. The authors create a comprehensive benchmark spanning 11 multimodal datasets and evaluate multiple visual and multimodal LLMs across these tasks. Through extensive experiments, they identify that LLMs perform well on explicit variable identification and graph construction but struggle significantly with implicit variable reasoning and complex causal chains. The study proposes tool-augmented learning and advanced elicitation strategies to improve reasoning performance, demonstrating that while specialized tools help with explicit variable identification across modalities, more complex prompting strategies can sometimes lead to overthinking and reduced performance.

## Method Summary
The study evaluates LLMs on counterfactual reasoning through a decompositional framework that isolates four sequential tasks: (1) Causal Variable Identification (classifying Exposure, Covariate, Mediator, Outcome), (2) Causal Graph Construction (building DAGs), (3) Intervention Identification (detecting counterfactual exposures), and (4) Outcome Reasoning (inferring counterfactual mediators and outcomes). The evaluation uses 11 multimodal datasets with ground-truth annotations for each task, allowing isolated assessment of each capability. Multiple LLMs (GPT-4o, Qwen-VL, LLaMA-3.2-11B, Gemini-Pro, DeepSeek-VL) are tested with three approaches: direct prompting, tool-augmented execution using modality-specific NER and object detection tools, and advanced elicitation strategies (Chain-of-Thought, Chain-of-Thought with Sampling and Consensus, and Tree of Thoughts). Performance is measured using F1 score for tasks I, II, and IV, and accuracy for task III.

## Key Results
- LLMs show F1 >0.9 on causal graph construction but struggle with implicit variable identification (mediators), with F1 gaps of 10-20 points versus explicit variables
- Tool-augmented execution improves explicit variable identification F1 by up to 0.336, with consistent gains across all modalities
- Advanced elicitation strategies (CoT-SC, ToT) improve implicit variable reasoning but risk "overthinking" that degrades performance on some datasets
- Performance varies significantly across modalities, with code and visual tasks showing notably lower accuracy (10-20 F1 points behind text)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing counterfactual reasoning into discrete stages isolates failure points and enables targeted interventions.
- Mechanism: The paper breaks counterfactual reasoning into four sequential tasks: (1) Causal Variable Identification, (2) Causal Graph Construction, (3) Intervention Identification, and (4) Outcome Reasoning. By evaluating each stage with ground-truth inputs from preceding stages, the framework reveals that LLMs perform well on graph construction but struggle with implicit variable identification—particularly mediators.
- Core assumption: Failure modes are stage-local rather than emergent from stage interactions; providing ground-truth inputs doesn't mask systemic issues.
- Evidence anchors: [abstract], [section 4.1], and comparison with CounterBench which lacks decompositional breakdown.

### Mechanism 2
- Claim: Tool-augmented execution mitigates modality-specific bottlenecks by offloading entity recognition to specialized models.
- Mechanism: The paper employs modality-specific tools—BERT-BASE-NER for text, Grounding-DINO for images, and GraphCodeBERT for code—to extract candidate entities, then prompts the LLM to refine and classify them as Exposure, Covariate, or Outcome. This two-stage approach improved Llama-3.2-11B's F1 by up to 0.336 on explicit variable identification.
- Core assumption: Tool outputs are sufficiently accurate; errors from auxiliary models don't cascade into LLM reasoning errors.
- Evidence anchors: [section 4.2.1], [table 4], and related work (CausalVLBench) focusing on visual causal reasoning without explicit tool integration.

### Mechanism 3
- Claim: Advanced elicitation strategies (CoT, CoT-SC, ToT) improve implicit variable reasoning but risk "overthinking" that degrades performance.
- Mechanism: Chain-of-Thought prompts step-by-step inference; CoT-SC samples multiple reasoning paths and selects via consensus; ToT explores branching paths with intermediate evaluation. Results show CoT-SC often outperforms CoT (+1-5 F1), but ToT sometimes underperforms simpler methods due to spurious causal link generation.
- Core assumption: More reasoning paths generally improve accuracy; voting/evaluation criteria align with ground truth.
- Evidence anchors: [section 4.2.2], [table 5], and comparison with GRPO-λ which improves LLM reasoning via credit assignment in RL.

## Foundational Learning

- Concept: **Causal Variable Taxonomy (X, Z, M, Y)**
  - Why needed here: The entire framework hinges on correctly classifying Exposure (intervention), Covariate (pre-treatment confounder), Mediator (causal pathway), and Outcome. Confusion between these leads to incorrect graph construction and downstream errors.
  - Quick check question: Given "students using a tutoring tool improved scores by studying more hours," identify which variable is the mediator.

- Concept: **Directed Acyclic Graphs (DAGs) for Causal Structure**
  - Why needed here: Task II requires constructing DAGs encoding Z→X, Z→M, Z→Y, X→M, M→Y, X→Y relationships. Understanding DAG semantics is prerequisite for evaluating graph construction correctness.
  - Quick check question: In a DAG with X→M→Y, what does blocking path X→M→Y imply about the direct effect X→Y?

- Concept: **Counterfactual Intervention Notation**
  - Why needed here: Task III-IV use do-calculus style notation (X' = counterfactual exposure, M' = counterfactual mediator, Y' = counterfactual outcome). Distinguishing factual from counterfactual variables is essential.
  - Quick check question: If M = f(X, Z) and we intervene on X', how do we compute M'?

## Architecture Onboarding

- Component map:
```
Input Context → [Task I: Variable Identification] → (X,Z,M,Y)
              → [Task II: Graph Construction] → DAG
              → [Task III: Intervention ID] → X'
              → [Task IV: Outcome Reasoning] → (M', Y')
              
Tool Layer: Text-NER | Vision-GroundingDINO | Code-GraphCodeBERT
Elicitation Layer: CoT | CoT-SC (k=5) | ToT (k=5 + BERTScore eval)
```

- Critical path: Task I (Variable Identification) → Task IV (Outcome Reasoning). Errors in mediator identification propagate through outcome inference; the paper shows M identification has lowest F1 (~0.56-0.78 depending on model/modality).

- Design tradeoffs:
  - Tool augmentation: +explicit variable accuracy vs. +system complexity and tool-specific failure modes
  - Elicitation depth: +reasoning coverage vs. +overthinking risk and compute cost (ToT requires 5x more sampling)
  - Modality coverage: +generalization vs. +heterogeneous performance gaps (code/vision lag text by 10-20 F1)

- Failure signatures:
  - Low M F1 with high X/Z/Y F1 → implicit variable reasoning failure, consider CoT-SC
  - High graph F1 with low outcome F1 → causal chain propagation failure, not structural understanding
  - ToT underperforms CoT → overthinking on that dataset; reduce reasoning breadth
  - Code modality consistently low → GraphCodeBERT integration may need tuning or alternative parser

- First 3 experiments:
  1. **Baseline by stage**: Run all 4 tasks on GPT-4o across 3 modalities (text: CRASS, vision: CVQA-Count, code: HumanEval-Exe) with ground-truth cascading. Identify lowest-performing stage per modality.
  2. **Tool ablation**: On CVQA-Count, compare (a) no tools, (b) text-NER only, (c) full tool pipeline. Measure delta on Task I explicit variables (X, Z, Y).
  3. **Elicitation sweep**: On the dataset showing worst M' performance, compare CoT vs. CoT-SC vs. ToT. Log instances where ToT identifies spurious mediators to characterize overthinking patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The decompositional approach may not reflect real-world error propagation, as ground-truth inputs are provided for each task
- Tool-augmented approaches depend heavily on auxiliary model accuracy, which is not evaluated for failure modes
- Dataset curation lacks transparency regarding annotation protocols and inter-annotator agreement metrics
- The study does not address prompt sensitivity or temperature effects on reasoning quality

## Confidence
- High confidence: The finding that LLMs struggle most with implicit variable (mediator) identification across all modalities is well-supported by consistent F1 gaps across datasets and models
- Medium confidence: The effectiveness of tool-augmented approaches for explicit variable identification is demonstrated but may be limited by tool-specific biases not explored in the study
- Medium confidence: The overthinking phenomenon with complex elicitation strategies is observed but requires further investigation to determine optimal reasoning depth per modality

## Next Checks
1. Conduct end-to-end evaluation on a subset of instances without ground-truth inputs to measure actual error propagation across the four stages
2. Perform ablation studies on the auxiliary tools to quantify their contribution to overall performance and identify potential bias sources
3. Test prompt sensitivity by varying temperature, max_tokens, and prompt structure across multiple runs to establish robustness of findings