---
ver: rpa2
title: Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal
  BERT Models
arxiv_id: '2507.14579'
source_url: https://arxiv.org/abs/2507.14579
tags:
- bert
- audibert
- affective
- indicators
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored human-AI complementarity in collaborative problem
  solving (CPS) diagnosis using unimodal and multimodal BERT models. The AudiBERT
  model significantly outperformed the BERT model in classifying CPS subskills, but
  not affective states.
---

# Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models

## Quick Facts
- arXiv ID: 2507.14579
- Source URL: https://arxiv.org/abs/2507.14579
- Authors: Kester Wong; Sahan Bulathwela; Mutlu Cukurova
- Reference count: 19
- Primary result: AudiBERT multimodal model significantly outperformed unimodal BERT in CPS subskill classification, with performance correlating with training data size and human inter-rater agreement.

## Executive Summary
This study investigates human-AI complementarity in collaborative problem solving (CPS) diagnosis using unimodal BERT and multimodal AudiBERT models. The AudiBERT model, which fuses text and acoustic-prosodic audio features, significantly outperforms the unimodal BERT model in classifying social-cognitive CPS subskills. Both models show improved recall with larger training datasets, while BERT's precision correlates with human inter-rater agreement. The study proposes a structured workflow for human-AI complementarity that leverages model explainability to support human agency and engagement in the reflective coding process.

## Method Summary
The study compares a unimodal BERT model with a multimodal AudiBERT architecture for classifying CPS indicators from dialogue. AudiBERT integrates BERT's text embeddings with Wav2Vec2.0 audio embeddings processed through a BiLSTM with self-attention layer. Both models were evaluated on a dataset of 78 secondary students (ages 14-15) working in triads on mathematics CPS tasks via video conferencing. The models classify 13 CPS subskill classes and 3 affective state classes at the utterance level, with fine-grained indicator classification within well-detected classes (F1 > 0.60). Statistical significance was assessed using one-tailed Wilcoxon signed-rank tests.

## Key Results
- AudiBERT significantly outperformed BERT in classifying social-cognitive CPS subskills (W = 20.0, p = .031)
- Larger training data correlated with higher recall performance for both models
- BERT's precision was significantly associated with high inter-rater agreement among human coders (Ï = 0.602, p = .0382)
- Classification of fine-grained indicators within well-detected classes showed inconsistent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AudiBERT significantly outperforms BERT in classifying social-cognitive CPS subskills
- Mechanism: Fusion of semantic text and paralinguistic audio features through embedding concatenation and BiLSTM processing creates richer representation
- Core assumption: Acoustic-prosodic features contain signal relevant to identifying social-cognitive behaviors not captured in text alone
- Evidence anchors: Abstract shows significant improvements for social-cognitive dimension; Section 3.1 reports class-wise comparison with effect size 0.91
- Break condition: Mechanism fails when relevant indicators are primarily conveyed through semantic content or when audio signal is noisy/absent

### Mechanism 2
- Claim: BERT's precision correlates with inter-rater agreement among human coders
- Mechanism: Classes with high human agreement are also easier for BERT to classify correctly due to more consistent training labels
- Core assumption: Cohen's kappa validly proxies class complexity/ambiguity
- Evidence anchors: Abstract notes precision correlation with inter-rater agreement; Section 3.2.2 reports statistically significant Spearman correlation
- Break condition: Correlation breaks if model learns spurious patterns unrelated to human semantic definitions

### Mechanism 3
- Claim: Both models show higher recall with larger training datasets
- Mechanism: Transformer models benefit from larger training sets to learn robust representations, improving instance identification
- Core assumption: Additional training data is of sufficient quality and representative of test distribution
- Evidence anchors: Abstract states correlation between recall and training data size; Section 3.2.1 reports statistically significant correlation for both models
- Break condition: Mechanism fails with noisy, mislabeled, or distribution-shifted data

## Foundational Learning

- Concept: **Multimodal Fusion (Concatenation-based)**
  - Why needed here: Understanding AudiBERT's primary innovation of fusing text and audio via embedding concatenation
  - Quick check question: How are text and audio representations combined in the AudiBERT model?

- Concept: **Cohen's Kappa (Inter-rater reliability)**
  - Why needed here: Used as proxy for class complexity to interpret why some classes are harder for models than others
  - Quick check question: Would a CPS subskill with low Cohen's kappa have higher or lower model precision?

- Concept: **Human-AI Complementarity**
  - Why needed here: Understanding the goal of complementary workflow rather than full automation
  - Quick check question: What is the human coder's primary role after AudiBERT provides initial classification?

## Architecture Onboarding

- Component map:
  - Tokenized text transcripts -> BERT branch
  - Audio recordings -> Wav2Vec2.0 extraction -> BiLSTM + self-attention -> AudiBERT branch
  - Concatenated embeddings -> Classifier layer -> CPS class prediction
  - (Proposed) SHAP/LIME on BERT branch -> Token-level explanations

- Critical path:
  1. Initial Automation: Pass all dialogue through AudiBERT for initial CPS subskill and affective state classifications
  2. Contextualization: Present human coder with AudiBERT-suggested class for each utterance
  3. Explanation & Refinement: Use BERT model to provide second opinion and token-level explanation; flag classes with sparse training data
  4. Final Decision: Human coder selects final, fine-grained indicator using suggested class, BERT explanation, and data warnings

- Design tradeoffs:
  - Performance vs. Explainability: AudiBERT more accurate but black box; BERT less accurate but interpretable
  - Accuracy vs. Data Scarcity: Models less reliable on sparse classes; workflow flags these cases

- Failure signatures:
  - Inconsistent Indicator Detection: Well-detected subskills don't guarantee fine-grained indicator detection
  - Precision Drop on Ambiguous Classes: Low Cohen's kappa classes expected to have low precision

- First 3 experiments:
  1. Ablation Study: Train AudiBERT with only text, only audio, and fused; compare contributions
  2. Error Analysis by Class Complexity: Bin classes by Cohen's kappa; plot model precision/recall for each bin
  3. Explainability Prototype: Build interface with AudiBERT prediction and BERT SHAP visualization; conduct user test

## Open Questions the Paper Calls Out

- Question: How does the proposed human-AI complementarity workflow perform empirically when tested with educators and researchers in live coding scenarios?
  - Basis in paper: Authors state workflow is "rather hypothetical" requiring "rigorous empirical investigations"
  - Why unresolved: Current study relies on retrospective model metric comparison rather than user studies
  - What evidence would resolve it: Empirical data from pilot studies measuring coding efficiency, user agency, and inter-rater reliability

- Question: Can an explainable AI architecture be developed for the AudiBERT model to identify specific audio features and tokens driving superior performance?
  - Basis in paper: Paper notes no current architecture for AudiBERT explanations
  - Why unresolved: Complex fusion of text and audio embeddings makes standard interpretability techniques difficult to apply
  - What evidence would resolve it: Development of architecture mapping specific audio timeframes and tokenized words to classification outputs

- Question: What are the pedagogical implications for learners and teachers when AI-based diagnostic models produce misclassifications?
  - Basis in paper: Authors highlight need to investigate pedagogical implications of possible misclassification
  - Why unresolved: Study focuses on technical diagnostic accuracy without addressing downstream consequences
  - What evidence would resolve it: Qualitative/experimental studies assessing how teachers interpret incorrect AI diagnoses and effect on student learning

## Limitations

- Dataset relatively small (78 students) with poor performance on rare indicators, suggesting scalability concerns
- Explainability component proposed but not empirically validated in this study
- Performance highly inconsistent at fine-grained indicator level despite well-detected subskills

## Confidence

- **High:** AudiBERT's superior performance in social-cognitive CPS subskill classification; BERT precision correlation with human agreement
- **Medium:** Correlation between training data size and model recall; inconsistent performance at fine-grained indicator level
- **Low:** Practical effectiveness of proposed human-AI complementarity workflow in real-world coding scenarios

## Next Checks

1. **Ablation Study:** Train and evaluate AudiBERT with only text features, only audio features, and fused model to quantify each modality's contribution

2. **Error Analysis by Class Complexity:** Bin CPS classes by Cohen's kappa values; plot model precision and recall for each bin to confirm reported correlation

3. **Explainability Prototype:** Build interface displaying utterance, AudiBERT prediction, and BERT SHAP visualization; conduct small user test to assess whether explanation improves human coder accuracy compared to AudiBERT prediction alone